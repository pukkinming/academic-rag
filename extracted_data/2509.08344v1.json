{
  "paper_id": "2509.08344v1",
  "title": "Few-Shot Personalization Via In-Context Learning For Speech Emotion Recognition Based On Speech-Language Model",
  "published": "2025-09-10T07:31:23Z",
  "authors": [
    "Mana Ihori",
    "Taiga Yamane",
    "Naotaka Kawata",
    "Naoki Makishima",
    "Tomohiro Tanaka",
    "Satoshi Suzuki",
    "Shota Orihashi",
    "Ryo Masumura"
  ],
  "keywords": [
    "speech emotion recognition",
    "in-context learning",
    "speech-language model",
    "meta-training for in-context learning"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "This paper proposes a personalization method for speech emotion recognition (SER) through in-context learning (ICL). Since the expression of emotions varies from person to person, speaker-specific adaptation is crucial for improving the SER performance. Conventional SER methods have been personalized using emotional utterances of a target speaker, but it is often difficult to prepare utterances corresponding to all emotion labels in advance. Our idea to overcome this difficulty is to obtain speaker characteristics by conditioning a few emotional utterances of the target speaker in ICL-based inference. ICL is a method to perform unseen tasks by conditioning a few inputoutput examples through inference in large language models (LLMs). We meta-train a speech-language model extended from the LLM to learn how to perform personalized SER via ICL. Experimental results using our newly collected SER dataset demonstrate that the proposed method outperforms conventional methods.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "Speech emotion recognition (SER) plays an important role in effectively understanding human-computer interaction and human communication. For example, SER is expected to be applied to applications such as estimating customer satisfaction in contact centers  [1]  and developing speech dialogue systems that can empathize and sympathize  [2] . On the other hand, the expression of emotions varies depending on various individual characteristics such as culture, environment, and personality, making SER a difficult problem to generalize  [3] -  [7] . In particular, the SER performance significantly degrades when a speaker who is not included in the training data is input during inference  [8] ,  [9] . Therefore, SER is a difficult task to adapt to unseen speakers.\n\nConventionally, to improve the SER performance for unseen speakers, methods using enrollment utterances, which are pre-collected and reflect speaker-specific emotional traits, have been proposed  [10] -  [13] . In these methods, enrollment utterances (e.g., neutral or each emotional utterance) are input into the SER model simultaneously with the target speech, and the utterances are used as guidance for speaker adaptation. In addition, it is reported that the method using enrollment utterances corresponding to all emotion labels has outperformed the method using only a neutral utterance  [13] . In these personalization methods, the SER models have been trained using a fixed number of enrollment utterances with fixed emotion labels. Thus, it is necessary to prepare enrollment utterances with the same emotion labels as those used during training for unseen speakers at inference time. However, in practice, it is difficult to always prepare enrollment utterances with the same emotion labels as those used during training because the enrollment utterances of the target speaker may have imbalanced emotion labels or may not be available.\n\nTo perform personalization for SER using an arbitrary number of enrollment utterances with any emotion labels, we focus on in-context learning (ICL) in large language models (LLMs). ICL is a method for adapting LLMs to unseen tasks by conditioning a few input-output examples through inference without updating their parameters  [14] . In ICL, we can flexibly adjust the number and types of examples and perform zero-shot inference without using examples. In this study, we consider performing ICL not to adapt the SER model to unseen tasks, but to adapt it to unseen speakers. By conditioning the SER model on a few enrollment utterancelabel pairs uttered by a target speaker, we expect the model to acquire the speaker characteristics necessary for speakeradaptive SER based on the correspondence between these utterances and these labels. In addition, the model can improve SER performance for unseen speakers by personalizing SER through ICL-based inference. Since the number of enrollment utterances and their emotion labels uttered by a target speaker do not need to be fixed in ICL, dynamic SER personalization becomes possible.\n\nIn this paper, we propose a method to perform personalized SER for unseen speakers by leveraging the ICL capability of LLMs. In the proposed method, we construct a speechlanguage model (speech LM) that can perform dynamic speaker adaptation for SER through ICL-based inference by conditioning a few enrollment utterance-label pairs uttered by a target speaker, as shown in Fig.  1 . The speech LM is constructed by extending the LLM to enable cross-modal input of speech and text, and conventional speech LMs for SER have not taken into account speaker characteristics, which is important for SER  [15] ,  [16] . To acquire speaker characteristics of unseen speakers using the ICL capability of the LLM, we meta-train the speech LM using meta-training for ICL (MetaICL), which is a fine-tuning method for training how to perform ICL  [17] . Specifically, we prepare multiple enrollment utterances of various speakers and fine-tune the LLM to predict other emotions given partial emotions of the target speaker in MetaICL. By modeling in this way, we expect that the speech LM can perform robust personalized SER for unseen speakers via ICL. Experimental results using a newly collected SER dataset demonstrate that the proposed method improves unweighted accuracy (%) by 2-3 pt regardless of which enrollment utterances were used up to the 2-shot setting and up to 8.2 pt compared to performing inference without ICL in the speech LM.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Related Work",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ser:",
      "text": "With the advancement of deep learning, neural networks are generally used to predict emotion labels given speech features in the SER task  [18] . In SER, there are two types of settings: speaker-dependent settings, which use the same speaker for training and inference  [19] -  [21] , and speaker-independent settings, which use different speakers for training and inference  [22] ,  [23] . In speaker-dependent settings, it has been reported that recognition performance can be improved by constructing a speaker-dependent SER model, and speaker information is considered to be an important factor in SER  [24] . In the speaker-independent setting, by using speaker information such as age  [25] , gender  [26] ,  [27] , and speaker vectors  [28] ,  [29] , SER performance has been improved. These methods demonstrated that speaker information is effective for SER, but it remains unclear which characteristics of speaker information are particularly effective. In addition, methods for removing speaker individuality have been proposed to perform generalized SER  [30] . However, this method embeds individual differences in speakers into a common space, which may reduce the accuracy of SER for speakers with large individual differences. Furthermore, SER methods using the enrollment utterances, which are precollected and reflect speaker-specific emotional traits, have been proposed  [10] -  [13] . In these methods, the model combines the emotional information in enrollment utterances with the target speech to improve SER performance. This paper aims to perform personalized SER for unseen speakers by using enrollment utterances and their labels in ICL.\n\nSpeech LM: Recently, many speech LMs that enable cross-modal input speech and text have appeared by extending the LLM, and these models can perform various speech tasks, including SER  [31] -  [33] . Also, speech LMs specialized for the SER task have appeared. In these models,  [15]  proposes a method to suppress hallucinations of emotion labels, and  [16]  proposes a method to generate emotion captions rather than to output emotion labels corresponding to the input speech. However, these methods do not leverage speaker characteristics such as age, gender, and enrollment utterances. With the success of ICL in LLMs, ICL is also performed in speech LMs. In ICL for speech LMs, it has been reported that performance can be improved by performing ICL-based inference for tasks such as automatic speech recognition (ASR) and speech translation  [34] -  [36] . In addition, there are several studies that perform unseen tasks using ICL-based inference in the speech LM. For example,  [37]  showed that speech LMs can perform speech translation of combinations that have not been learned during inference using ICL by learning mainly speech recognition and speech translation tasks. Also,  [38]  showed that unseen tasks can be performed through ICL by training various speech understanding tasks. In this study, we focus on SER, which is more speaker-dependent than ASR, and investigate whether it is possible to apply the speech LM to unseen speakers rather than to unseen tasks by using a few enrollment utterance-label pairs of a target speaker in ICL.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Iii. Preliminaries",
      "text": "In this section, we define the modeling method of a classifier model and speech LM for SER used in this paper.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "A. Classifier Model",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "In This Paper, The Classifier Model Predicts An Emotion Label",
      "text": "x m is the m-th speech feature and M is the number of speech features, as shown in Fig.  2  (a). The probability of y is defined as P (y|X; Θ cls ), where Θ cls is a trainable parameter set. P (y|X; Θ cls ) is computed with a transformer-based speech encoder and an output layer.\n\nNetwork Structure: The speech encoder converts X into speech representations H as\n\nwhere TransformerEnc() is a function of the transformer encoder blocks that consist of multi-head self-attention layers and position-wise feed-forward networks  [39] , and θ speech ∈ Θ cls are the trainable parameters. Next, the attentive pooling converts variable-length hidden vectors H into a fixed-size vector h as\n\nwhere AttentivePooling() is the attentive pooling function, and θ pool ∈ Θ cls are the trainable parameters. The probability of y is calculated by\n\nwhere Softmax() is a softmax-activation layer with a linear transformation, and θ soft ∈ Θ cls are the trainable parameters.\n\nTraining: The model parameter set",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "B. Speech Lm",
      "text": "In this paper, the speech LM predicts an emotion text\n\n, where z t is the t-th token and T is the number of tokens in the emotion text, given instruction text W and X, as shown in Fig.  2 (b ). The generation probability of Z is defined as\n\nwhere\n\n} and Θ sl is a trainable parameter set. P (Z|W , X; Θ sl ) is computed with the speech LM that combines a speech encoder with an LLM. In this paper, we use an instruction-tuned encoder-decoder type LLM, and the speech LM consists of a speech encoder, a conversion module, a text encoder, an LLM encoder, and an LLM decoder.\n\nInstruction-tuned LLM: In this paper, we construct a speech LM by extending an instruction-tuned LLM. Instruction-tuning is a fine-tuning method to improve the ability to respond according to instructions and the ICL capability  [40] , and the LLM is fine-tuned to predict a response text O given an instruction text W . The probabilities of O are defined as P (O|W ; Θ llm ), where Θ llm is a trainable parameter set, which is initialized by pretraining with a next token prediction objective. Θ llm is optimized from the training dataset\n\nSpeech encoder: In the speech encoder, X is converted into H as shown in Eq. (  1 ), and θ speech is fixed.\n\nConversion module: To input X into the LLM, a conversion module is required to align the input embedding space of the LLM and the speech representation. In this paper, we use Q-Former  [41] , which can convert input sequences of arbitrary length into fixed-length representations. In the Q-Former, fixed-length query representation U is outputted given the query that is trainable fixed-length embedding Q and H as\n\nwhere QFormer() is a function of the transformer decoder blocks without a causal attention mask that consist of multihead self-attention and cross-attention layers and position-wise feed-forward networks  [39] , and θ qf ∈ Θ sl are the trainable parameters.\n\nText encoder: In the text encoder, W is converted into a continuous representation R as\n\nwhere Embedding() is the function that converts a token into a continuous vector and Θllm ∈ Θ sl . LLM encoder: In the LLM encoder, R and U are converted hidden representation C as\n\nwhere PosEnc() is positional encoding, SeqPosEmb() is sequence position embedding that indicates the order of the input sequences, SegmentEmb() is segment embedding that distinguishes different modalities and {θ seg , θ seq } ∈ Θ sl are the trainable parameters. LLM decoder: In the LLM decoder, the generation probability of z t is calculated given C and z\n\nwhere TransformerDec() is a function of the transformer decoder blocks.\n\nTraining: The model parameter set\n\nW is a common instruction text across D text . During training, the speech encoder parameters are fixed, and the LLM parameters are updated so that the speech representation approaches the LLM embedding space.\n\nInference: The inference of Z is defined as",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Iv. Proposed Method",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "A. Strategy Of Personalization Via Icl",
      "text": "This paper proposes a method to perform personalized SER for unseen speakers via ICL. Fig.  2 (c ) shows the overview of the proposed method. We expect the speech LM extended from an LLM with ICL capability to perform personalization for a speaker through inference by conditioning a few enrollment utterance-label pairs uttered by the target speaker in ICL. In the proposed method, we construct the speech LM that can be adapted to unseen speakers through ICL by fine-tuning the LLM that can be adapted to unseen tasks through ICL. In MetaICL, the speech LM is fine-tuned in two stages. In the first stage, since training the model using long sequences including enrollment utterances may cause unstable learning, the model is fine-tuned without the enrollment utterance-label pair, as in Eq. (  15 ). In the second stage, the model is metatrained to generate emotion text given an instruction text, a few enrollment utterance-label pairs, and target speech features. At that stage, zero to seven enrollment utterance-label pairs are randomly selected from the dataset of the same speaker as the target speech, satisfying the following settings (TU+LU 0:7 ).\n\n• Target-Uncontrolled (TU): The same emotion as the target appears or not in the enrollment utterance-label pairs. • Label-Uncontrolled (LU): The enrollment utterancelabel pairs have the same or different emotions. By using enrollment utterance-label pairs selected under these settings in MetaICL, we expect the speech LM to perform ICL even when utterances with any emotion labels are used.\n\nIn addition, to investigate whether performance changes depending on the combination of emotions, we select enrollment utterance-label pairs from the above and the following settings in ICL-based inference.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "B. Metaicl",
      "text": "In MetaICL, the speech LM in Eq. (  6 ) is finetuned using k-shot enrollment utterance-label pairs S = {(X 1 , Z 1 ), • • • , (X k , Z k )} by optimizing a trainable parameter set Θ meta , which is initialized from Θsl in Eq. (  15 ). Θ meta is optimized using a set of datasets\n\n)} has a set of enrollment utterance-label pair uttered by the j-th speaker. Thus, Θ meta is optimized using D as Θmeta =  (18)  arg min Θmeta -J j=1 (Xj ,Zj )∈Dj log P (Z j |W , S j , X j ; Θ meta ),\n\nwhere SelectProcedure() randomly selects k samples from the set excluding X j and Z j from D j in accordance with TU+LU setting.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "C. Personalization Via Icl",
      "text": "In ICL-based inference, the speech LM personalizes SER by conditioning k-shot enrollment utterance-label pairs uttered by a target speaker. In k-shot ICL-based inference, the inference of Z is defined as\n\nwhere   and X  k tgt is the k-th speech features uttered by a target speaker. In addition, the speech LM can predict Z without giving S tgt , which is called \"zero-shot inference\", when Θmeta is used in Eq. (  17 ).",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "V. Experiments",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "A. Dataset",
      "text": "To evaluate personalized SER for unseen speakers, multiple emotional utterances of various speakers are required.\n\nHowever, the dataset used in conventional SER only includes about 10 speakers  [42] -  [44] , so it is difficult to evaluate in the speaker-independent setting with a sufficient number of speakers. Thus, this study newly collected a Japanese SER dataset that has many speakers. This dataset consists of 800 native Japanese speakers (368 males and 432 females) who performed 50 utterances each for seven emotions: anger, disgust, fear, joy, sadness, surprise, and neutral. The text read aloud is unrelated to the speaker's emotional state, so even if the text content is taken into consideration, it does not contribute to emotion estimation. We instructed the speakers to \"speak as clearly as possible so that your emotions come across\" for each text, and recorded speech using a headset microphone and a laptop computer in a conference room free of background noise. We divided this dataset into training, development, and test sets so that the gender ratio and age groups are evenly distributed, as shown in Table  I . In this dataset, we used a common instruction text: \"Please select the appropriate emotion for the input speech from the following: Neutral, Surprise, Sadness, Joy, Fear, Disgust, Anger.\"",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "B. Models",
      "text": "We compare the performance of four classifier models, two speech LMs, and the proposed method. In the classifier model and speech LM, we utilized an in-house transformerbased speech encoder, which has 42M parameters, and these parameters were pre-trained in various speech-understanding tasks. In the speech LM, we utilized an encoder-decoder style in-house LLM, which has 0.6B parameters. The parameters were pre-trained with a large amount of text based on unifying language learning paradigms  [45] , and instruction-tuned with various language tasks. Details of the comparison method are shown below.\n\nClassifier model:\n\n• Scratch model: a transformer-based model in which all parameters were randomly initialized, defined as Eq. (  4 ), • Pretrained model: a transformer-based model with the pretrained speech encoder. In this model, the parameters of the speech encoder were fixed. • Personalized model N  [11] : a model with a single neutral utterance added to the input of the pretrained model. • Personalized model A  [13] : a model with each emotional utterance of a target speaker added to the input of the pretrained model. Speech LM:\n\n• 0-shot: a model that predicts emotion text given an instruction text and speech features defined as Eq. (  15 ). • 1-shot N : a model that predicts emotion text for target speech by conditioning a single neutral speech of a target speaker and a text described as \"Neutral\".",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "C. Setup",
      "text": "These models were composed under the following conditions: For the speech encoder, we used 80 log mel-scale filterbank coefficients as speech features. The frame shift was 10 ms. The speech features passed two convolution and max pooling layers with a slide of 2, so we down-sampled them to 1/4 along with the time axis. After these layers, we stacked 6-layer transformer encoder blocks. For the Q-Former, we stacked 2-layer transformer decoder blocks without a causal attention mask. Based on preliminary experiments, the length of the query was set to 150, as it yielded the best performance. For the LLM encoder and decoder, we used 1024-dimensional token embeddings where the vocabulary size was set to 37,156 and stacked 32-layer transformer encoder blocks and 6-layer transformer decoder blocks. In the speech encoder, Q-Former, and LLM encoder and decoder, the dimensions of the output continuous representations were set to 512, 512, and 1024, those of the inner outputs in the position-wise feed-forward networks were set to 2048, 2048 and 4096, and the number of heads in the multi-head attention was set to 8, 8, and 16, respectively. For training, we used the RAdam optimizer. We set the mini-batch size to 64 utterances and the dropout rate in the transformer blocks to 0.1. We introduced label smoothing, where its smoothing parameter was set to 0.1. For inference with speech LM, we used a beam search algorithm in which the beam size was set to 4, and conditioned 0-7-shot enrollment utterance-label pairs for ICL.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "D. Evaluation",
      "text": "As evaluation metrics, we used unweighted accuracy for the speaker (UA spk ), which is the average of individual speaker accuracies, to evaluate whether personalized SER is performed for unseen speakers. In addition, we calculated standard statistics, such as standard deviation, median, maximum, and minimum of UA spk . Note that when evaluating the speech LM, which is a generative model, a generated result is correct if it exactly matches the correct answer.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "E. Results",
      "text": "Main result: Table  II  shows UA spk , standard deviation (σ(•)), median (µ 1/2 (•)), maximum (max(•)), and minimum (min(•)) values of UA spk in each model. The \"Neutral\" in the setting column of the table shows the performance when neutral utterances were only used in ICL-based inference. In the table, although the Personalized A , which uses enrollment speech of all emotions, outperformed other models in the classifier model, it performed worse than the 0-shot. This result indicates that the speech LM can learn the SER task better than classifier models. In addition, in the proposed method, the SER performance improved as the number of enrollment utterance-label pairs increased, and the highest performance was achieved when all emotional utterances were used. These results indicate that the proposed method can perform personalized SER via ICL.\n\nHow to select enrollment utterances in ICL: To investigate the impact of selecting methods of enrollment utterances on performance, we performed ICL using enrollment utterances that satisfy the following settings: TU+LD, TU+LU, TE+LD, TE+LU, and TU+LO settings.    On the other hand, in the TU+LO setting, the ICL performance was improved or decreased slightly when using three or more examples. These results indicate that if the same emotion is used in ICL, up to two shots are effective for personalization. In other settings, SER performance was improved with increased examples in ICL, and performance was improved in the order of TU+LD>TU+LU>TE+LD>TE+LU. These results show that in SER via ICL, personalization works well when utterances can be prepared under TU or LU settings.\n\nHow to select enrollment utterances in MetaICL: To investigate the effects of the types of enrollment utterances used in MetaICL, we prepared two meta-trained speech LM using enrollment utterances that satisfy the following settings: TO+LD 7 , which had only 7-shot examples using each emotion, and Neutral 0:7 , which had 0-7-shot examples using only neutral utterance. Table  IV  shows the results of ICL-based inference using TU+LD and TU+LO (neutral utterances only) settings. In the TU+LD setting, the performance of the speech LM meta-trained with TO+LD 7 was worse than that with Neutral 0:7 when 1-6-shot examples were used. In the TU+LO setting, the performance of the speech LM meta-trained with TO+LD 7 decreased with increased examples. These results show that the speech LM meta-trained with TO+LD 7 was optimized for input of 7-shot examples, so it cannot personalize SER via ICL. Also, in the TU+LO setting, the performance of the speech LM meta-trained with Neutral 0:7 was worse than that with TU+LU 0:7 . From the above, it is inferred that the speech LM can improve the personalization capability via ICL by meta-training using enrollment utterances expressing various emotions.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Vi. Conclusion",
      "text": "This paper proposed a method for personalizing speech emotion recognition (SER) via in-context learning (ICL). In the proposed method, the speech language model (speech LM), extended from a large language model with ICL capability, performs personalization for SER by conditioning a few enrollment utterance-label pairs uttered by a target speaker in ICLbased inference. Experimental results using our created SER dataset demonstrated that the proposed method had higher performance than conventional personalization methods, and SER performance was improved when utterances with arbitrary emotion labels were increased in ICL-based inference.",
      "page_start": 6,
      "page_end": 6
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Example of personalization for SER via ICL in speech LM.",
      "page": 1
    },
    {
      "caption": "Figure 1: The speech LM",
      "page": 1
    },
    {
      "caption": "Figure 2: (a). The probability of y is defined",
      "page": 2
    },
    {
      "caption": "Figure 2: Overview of classifier model, speech language model, and proposed method for SER.",
      "page": 3
    },
    {
      "caption": "Figure 2: (b). The generation probability",
      "page": 3
    },
    {
      "caption": "Figure 2: (c) shows the overview of",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Tomohiro Tanaka, Satoshi Suzuki, Shota Orihashi, Ryo Masumura": "Human Informatics Laboratories, NTT,\nInc., Japan"
        },
        {
          "Tomohiro Tanaka, Satoshi Suzuki, Shota Orihashi, Ryo Masumura": "Anger\n</s>"
        },
        {
          "Tomohiro Tanaka, Satoshi Suzuki, Shota Orihashi, Ryo Masumura": "Abstract—This paper proposes a personalization method for"
        },
        {
          "Tomohiro Tanaka, Satoshi Suzuki, Shota Orihashi, Ryo Masumura": "LLM Encoder \nLLM Decoder \nspeech emotion recognition (SER)\nthrough in-context\nlearning"
        },
        {
          "Tomohiro Tanaka, Satoshi Suzuki, Shota Orihashi, Ryo Masumura": "(ICL).\nSince\nthe\nexpression\nof\nemotions\nvaries\nfrom person\nText \nSpeech \nText \nText \nSpeech \nSpeech"
        },
        {
          "Tomohiro Tanaka, Satoshi Suzuki, Shota Orihashi, Ryo Masumura": "Anger\n<s>\n⋯"
        },
        {
          "Tomohiro Tanaka, Satoshi Suzuki, Shota Orihashi, Ryo Masumura": "Encoder"
        },
        {
          "Tomohiro Tanaka, Satoshi Suzuki, Shota Orihashi, Ryo Masumura": "Encoder \nEncoder \nEncoder \nEncoder \nEncoder \nto person,\nspeaker-specific adaptation is\ncrucial\nfor\nimproving"
        },
        {
          "Tomohiro Tanaka, Satoshi Suzuki, Shota Orihashi, Ryo Masumura": "Please estimate"
        },
        {
          "Tomohiro Tanaka, Satoshi Suzuki, Shota Orihashi, Ryo Masumura": "Neutral\nDisgust\nthe SER performance. Conventional SER methods have been\n: Parameters"
        },
        {
          "Tomohiro Tanaka, Satoshi Suzuki, Shota Orihashi, Ryo Masumura": "the emotion of the"
        },
        {
          "Tomohiro Tanaka, Satoshi Suzuki, Shota Orihashi, Ryo Masumura": "freeze\npersonalized using emotional utterances of a target speaker, but\nfollowing speech."
        },
        {
          "Tomohiro Tanaka, Satoshi Suzuki, Shota Orihashi, Ryo Masumura": "Target speaker’s utterances\nTarget Speech"
        },
        {
          "Tomohiro Tanaka, Satoshi Suzuki, Shota Orihashi, Ryo Masumura": "it\nis\noften difficult\nto prepare utterances\ncorresponding\nto\nall"
        },
        {
          "Tomohiro Tanaka, Satoshi Suzuki, Shota Orihashi, Ryo Masumura": "emotion labels in advance. Our idea to overcome this difficulty is"
        },
        {
          "Tomohiro Tanaka, Satoshi Suzuki, Shota Orihashi, Ryo Masumura": "Fig. 1.\nExample of personalization for SER via ICL in speech LM."
        },
        {
          "Tomohiro Tanaka, Satoshi Suzuki, Shota Orihashi, Ryo Masumura": "to obtain speaker characteristics by conditioning a few emotional"
        },
        {
          "Tomohiro Tanaka, Satoshi Suzuki, Shota Orihashi, Ryo Masumura": "utterances of\nthe target\nspeaker in ICL-based inference.\nICL is"
        },
        {
          "Tomohiro Tanaka, Satoshi Suzuki, Shota Orihashi, Ryo Masumura": "a method to perform unseen tasks by conditioning a few input-"
        },
        {
          "Tomohiro Tanaka, Satoshi Suzuki, Shota Orihashi, Ryo Masumura": "emotion labels. Thus,\nit\nis necessary to prepare\nenrollment"
        },
        {
          "Tomohiro Tanaka, Satoshi Suzuki, Shota Orihashi, Ryo Masumura": "output\nexamples\nthrough\ninference\nin\nlarge\nlanguage models"
        },
        {
          "Tomohiro Tanaka, Satoshi Suzuki, Shota Orihashi, Ryo Masumura": "utterances with the same emotion labels as those used during\n(LLMs). We meta-train a speech-language model extended from"
        },
        {
          "Tomohiro Tanaka, Satoshi Suzuki, Shota Orihashi, Ryo Masumura": "the LLM to learn how to perform personalized SER via ICL.\ntraining for unseen speakers\nat\ninference\ntime. However,\nin"
        },
        {
          "Tomohiro Tanaka, Satoshi Suzuki, Shota Orihashi, Ryo Masumura": "Experimental\nresults\nusing\nour\nnewly\ncollected\nSER dataset"
        },
        {
          "Tomohiro Tanaka, Satoshi Suzuki, Shota Orihashi, Ryo Masumura": "practice,\nit\nis difficult\nto always prepare enrollment utterances"
        },
        {
          "Tomohiro Tanaka, Satoshi Suzuki, Shota Orihashi, Ryo Masumura": "demonstrate that the proposed method outperforms conventional"
        },
        {
          "Tomohiro Tanaka, Satoshi Suzuki, Shota Orihashi, Ryo Masumura": "with the\nsame\nemotion labels\nas\nthose used during training"
        },
        {
          "Tomohiro Tanaka, Satoshi Suzuki, Shota Orihashi, Ryo Masumura": "methods."
        },
        {
          "Tomohiro Tanaka, Satoshi Suzuki, Shota Orihashi, Ryo Masumura": "because the enrollment utterances of\nthe target\nspeaker may"
        },
        {
          "Tomohiro Tanaka, Satoshi Suzuki, Shota Orihashi, Ryo Masumura": "Index Terms—speech emotion recognition,\nin-context learning,"
        },
        {
          "Tomohiro Tanaka, Satoshi Suzuki, Shota Orihashi, Ryo Masumura": "have imbalanced emotion labels or may not be available.\nspeech-language model, meta-training for in-context\nlearning"
        },
        {
          "Tomohiro Tanaka, Satoshi Suzuki, Shota Orihashi, Ryo Masumura": "To\nperform personalization\nfor\nSER using\nan\narbitrary"
        },
        {
          "Tomohiro Tanaka, Satoshi Suzuki, Shota Orihashi, Ryo Masumura": "I.\nINTRODUCTION"
        },
        {
          "Tomohiro Tanaka, Satoshi Suzuki, Shota Orihashi, Ryo Masumura": "number of enrollment utterances with any emotion labels, we"
        },
        {
          "Tomohiro Tanaka, Satoshi Suzuki, Shota Orihashi, Ryo Masumura": "focus on in-context\nlearning (ICL)\nin large language models\nSpeech emotion recognition (SER) plays an important\nrole"
        },
        {
          "Tomohiro Tanaka, Satoshi Suzuki, Shota Orihashi, Ryo Masumura": "(LLMs).\nICL is\na method\nfor\nadapting LLMs\nto\nunseen\nin effectively understanding human-computer\ninteraction and"
        },
        {
          "Tomohiro Tanaka, Satoshi Suzuki, Shota Orihashi, Ryo Masumura": "tasks by conditioning a\nfew input-output\nexamples\nthrough\nhuman communication. For example, SER is expected to be"
        },
        {
          "Tomohiro Tanaka, Satoshi Suzuki, Shota Orihashi, Ryo Masumura": "inference without updating their parameters [14].\nIn ICL, we\napplied to applications\nsuch as estimating customer\nsatisfac-"
        },
        {
          "Tomohiro Tanaka, Satoshi Suzuki, Shota Orihashi, Ryo Masumura": "can flexibly\nadjust\nthe\nnumber\nand\ntypes\nof\nexamples\nand\ntion in contact\ncenters\n[1]\nand developing speech dialogue"
        },
        {
          "Tomohiro Tanaka, Satoshi Suzuki, Shota Orihashi, Ryo Masumura": "perform zero-shot\ninference without using examples.\nIn this\nsystems that can empathize and sympathize [2]. On the other"
        },
        {
          "Tomohiro Tanaka, Satoshi Suzuki, Shota Orihashi, Ryo Masumura": "study, we\nconsider\nperforming\nICL not\nto\nadapt\nthe SER\nhand,\nthe expression of emotions varies depending on various"
        },
        {
          "Tomohiro Tanaka, Satoshi Suzuki, Shota Orihashi, Ryo Masumura": "model\nto unseen tasks, but\nto adapt\nit\nto unseen speakers. By\nindividual\ncharacteristics\nsuch\nas\nculture,\nenvironment,\nand"
        },
        {
          "Tomohiro Tanaka, Satoshi Suzuki, Shota Orihashi, Ryo Masumura": "conditioning the SER model on a few enrollment utterance-\npersonality, making SER a difficult problem to generalize [3]–"
        },
        {
          "Tomohiro Tanaka, Satoshi Suzuki, Shota Orihashi, Ryo Masumura": "label pairs uttered by a target\nspeaker, we expect\nthe model\n[7]. In particular,\nthe SER performance significantly degrades"
        },
        {
          "Tomohiro Tanaka, Satoshi Suzuki, Shota Orihashi, Ryo Masumura": "to acquire the speaker characteristics necessary for\nspeaker-\nwhen a\nspeaker who is not\nincluded in the\ntraining data\nis"
        },
        {
          "Tomohiro Tanaka, Satoshi Suzuki, Shota Orihashi, Ryo Masumura": "adaptive SER based\non\nthe\ncorrespondence\nbetween\nthese\ninput during inference [8],\n[9]. Therefore, SER is a difficult"
        },
        {
          "Tomohiro Tanaka, Satoshi Suzuki, Shota Orihashi, Ryo Masumura": "utterances and these labels. In addition, the model can improve\ntask to adapt\nto unseen speakers."
        },
        {
          "Tomohiro Tanaka, Satoshi Suzuki, Shota Orihashi, Ryo Masumura": "SER performance for unseen speakers by personalizing SER\nConventionally,\nto improve\nthe SER performance\nfor un-"
        },
        {
          "Tomohiro Tanaka, Satoshi Suzuki, Shota Orihashi, Ryo Masumura": "through ICL-based inference. Since the number of enrollment\nseen\nspeakers, methods\nusing\nenrollment\nutterances, which"
        },
        {
          "Tomohiro Tanaka, Satoshi Suzuki, Shota Orihashi, Ryo Masumura": "utterances and their emotion labels uttered by a target speaker\nare pre-collected and reflect speaker-specific emotional\ntraits,"
        },
        {
          "Tomohiro Tanaka, Satoshi Suzuki, Shota Orihashi, Ryo Masumura": "do not need to be fixed in ICL, dynamic SER personalization\nhave been proposed [10]–[13].\nIn these methods, enrollment"
        },
        {
          "Tomohiro Tanaka, Satoshi Suzuki, Shota Orihashi, Ryo Masumura": "becomes possible.\nutterances (e.g., neutral or each emotional utterance) are input"
        },
        {
          "Tomohiro Tanaka, Satoshi Suzuki, Shota Orihashi, Ryo Masumura": "into the SER model simultaneously with the target speech, and\nIn this paper, we propose a method to perform personalized"
        },
        {
          "Tomohiro Tanaka, Satoshi Suzuki, Shota Orihashi, Ryo Masumura": "the utterances\nare used as guidance\nfor\nspeaker\nadaptation.\nSER for\nunseen\nspeakers\nby\nleveraging\nthe\nICL capability"
        },
        {
          "Tomohiro Tanaka, Satoshi Suzuki, Shota Orihashi, Ryo Masumura": "In addition,\nit\nis\nreported that\nthe method using enrollment\nof LLMs.\nIn the proposed method, we\nconstruct\na\nspeech-"
        },
        {
          "Tomohiro Tanaka, Satoshi Suzuki, Shota Orihashi, Ryo Masumura": "utterances\ncorresponding\nto\nall\nemotion\nlabels\nhas\noutper-\nlanguage model\n(speech\nLM)\nthat\ncan\nperform dynamic"
        },
        {
          "Tomohiro Tanaka, Satoshi Suzuki, Shota Orihashi, Ryo Masumura": "formed the method using only a neutral utterance [13]. In these\nspeaker adaptation for SER through ICL-based inference by"
        },
        {
          "Tomohiro Tanaka, Satoshi Suzuki, Shota Orihashi, Ryo Masumura": "personalization methods,\nthe SER models have been trained\nconditioning\na\nfew enrollment\nutterance-label\npairs\nuttered"
        },
        {
          "Tomohiro Tanaka, Satoshi Suzuki, Shota Orihashi, Ryo Masumura": "using\na\nfixed\nnumber\nof\nenrollment\nutterances with\nfixed\nby\na\ntarget\nspeaker,\nas\nshown\nin Fig.\n1. The\nspeech LM"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "is constructed by extending the LLM to enable cross-modal": "input of\nspeech and text,\nand conventional\nspeech LMs\nfor",
          "speech. However,\nthese methods\ndo\nnot\nleverage\nspeaker": "characteristics such as age, gender, and enrollment utterances."
        },
        {
          "is constructed by extending the LLM to enable cross-modal": "SER have\nnot\ntaken\ninto\naccount\nspeaker\ncharacteristics,",
          "speech. However,\nthese methods\ndo\nnot\nleverage\nspeaker": "With the success of\nICL in LLMs,\nICL is also performed in"
        },
        {
          "is constructed by extending the LLM to enable cross-modal": "which is\nimportant\nfor SER [15],\n[16]. To acquire\nspeaker",
          "speech. However,\nthese methods\ndo\nnot\nleverage\nspeaker": "speech LMs. In ICL for speech LMs,\nit has been reported that"
        },
        {
          "is constructed by extending the LLM to enable cross-modal": "characteristics of unseen speakers using the ICL capability of",
          "speech. However,\nthese methods\ndo\nnot\nleverage\nspeaker": "performance\ncan be\nimproved by performing ICL-based in-"
        },
        {
          "is constructed by extending the LLM to enable cross-modal": "the LLM, we meta-train the speech LM using meta-training",
          "speech. However,\nthese methods\ndo\nnot\nleverage\nspeaker": "ference for tasks such as automatic speech recognition (ASR)"
        },
        {
          "is constructed by extending the LLM to enable cross-modal": "for ICL (MetaICL), which is a fine-tuning method for training",
          "speech. However,\nthese methods\ndo\nnot\nleverage\nspeaker": "and speech translation [34]–[36]. In addition,\nthere are several"
        },
        {
          "is constructed by extending the LLM to enable cross-modal": "how to perform ICL [17]. Specifically, we prepare multiple",
          "speech. However,\nthese methods\ndo\nnot\nleverage\nspeaker": "studies\nthat perform unseen tasks using ICL-based inference"
        },
        {
          "is constructed by extending the LLM to enable cross-modal": "enrollment utterances of various\nspeakers\nand fine-tune\nthe",
          "speech. However,\nthese methods\ndo\nnot\nleverage\nspeaker": "in the speech LM. For example, [37] showed that speech LMs"
        },
        {
          "is constructed by extending the LLM to enable cross-modal": "LLM to predict other emotions given partial emotions of\nthe",
          "speech. However,\nthese methods\ndo\nnot\nleverage\nspeaker": "can perform speech translation of combinations that have not"
        },
        {
          "is constructed by extending the LLM to enable cross-modal": "target speaker in MetaICL. By modeling in this way, we expect",
          "speech. However,\nthese methods\ndo\nnot\nleverage\nspeaker": "been learned during inference using ICL by learning mainly"
        },
        {
          "is constructed by extending the LLM to enable cross-modal": "that\nthe speech LM can perform robust personalized SER for",
          "speech. However,\nthese methods\ndo\nnot\nleverage\nspeaker": "speech recognition and speech translation tasks. Also,\n[38]"
        },
        {
          "is constructed by extending the LLM to enable cross-modal": "unseen speakers via ICL. Experimental\nresults using a newly",
          "speech. However,\nthese methods\ndo\nnot\nleverage\nspeaker": "showed that unseen tasks can be performed through ICL by"
        },
        {
          "is constructed by extending the LLM to enable cross-modal": "collected SER dataset demonstrate that\nthe proposed method",
          "speech. However,\nthese methods\ndo\nnot\nleverage\nspeaker": "training various speech understanding tasks.\nIn this study, we"
        },
        {
          "is constructed by extending the LLM to enable cross-modal": "improves unweighted accuracy (%) by 2—3 pt\nregardless of",
          "speech. However,\nthese methods\ndo\nnot\nleverage\nspeaker": "focus on SER, which is more speaker-dependent\nthan ASR,"
        },
        {
          "is constructed by extending the LLM to enable cross-modal": "which enrollment utterances were used up to the 2-shot setting",
          "speech. However,\nthese methods\ndo\nnot\nleverage\nspeaker": "and investigate whether it\nis possible to apply the speech LM"
        },
        {
          "is constructed by extending the LLM to enable cross-modal": "and up to 8.2 pt compared to performing inference without ICL",
          "speech. However,\nthese methods\ndo\nnot\nleverage\nspeaker": "to unseen speakers rather than to unseen tasks by using a few"
        },
        {
          "is constructed by extending the LLM to enable cross-modal": "in the speech LM.",
          "speech. However,\nthese methods\ndo\nnot\nleverage\nspeaker": "enrollment utterance-label pairs of a target speaker\nin ICL."
        },
        {
          "is constructed by extending the LLM to enable cross-modal": "II. RELATED WORK",
          "speech. However,\nthese methods\ndo\nnot\nleverage\nspeaker": "III. PRELIMINARIES"
        },
        {
          "is constructed by extending the LLM to enable cross-modal": "",
          "speech. However,\nthese methods\ndo\nnot\nleverage\nspeaker": "In this section, we define the modeling method of a classifier"
        },
        {
          "is constructed by extending the LLM to enable cross-modal": "SER: With\nthe\nadvancement\nof\ndeep\nlearning,\nneural",
          "speech. However,\nthese methods\ndo\nnot\nleverage\nspeaker": ""
        },
        {
          "is constructed by extending the LLM to enable cross-modal": "",
          "speech. However,\nthese methods\ndo\nnot\nleverage\nspeaker": "model and speech LM for SER used in this paper."
        },
        {
          "is constructed by extending the LLM to enable cross-modal": "networks are generally used to predict emotion labels given",
          "speech. However,\nthese methods\ndo\nnot\nleverage\nspeaker": ""
        },
        {
          "is constructed by extending the LLM to enable cross-modal": "speech\nfeatures\nin\nthe\nSER task\n[18].\nIn\nSER,\nthere\nare",
          "speech. However,\nthese methods\ndo\nnot\nleverage\nspeaker": ""
        },
        {
          "is constructed by extending the LLM to enable cross-modal": "",
          "speech. However,\nthese methods\ndo\nnot\nleverage\nspeaker": "A. Classifier model"
        },
        {
          "is constructed by extending the LLM to enable cross-modal": "two types of\nsettings:\nspeaker-dependent\nsettings, which use",
          "speech. However,\nthese methods\ndo\nnot\nleverage\nspeaker": ""
        },
        {
          "is constructed by extending the LLM to enable cross-modal": "",
          "speech. However,\nthese methods\ndo\nnot\nleverage\nspeaker": "In this paper,\nthe classifier model predicts an emotion label"
        },
        {
          "is constructed by extending the LLM to enable cross-modal": "the\nsame\nspeaker\nfor\ntraining and inference\n[19]–[21],\nand",
          "speech. However,\nthese methods\ndo\nnot\nleverage\nspeaker": ""
        },
        {
          "is constructed by extending the LLM to enable cross-modal": "",
          "speech. However,\nthese methods\ndo\nnot\nleverage\nspeaker": "y ∈ {y1, · · ·\n, yN }, where N is the number of emotion labels,"
        },
        {
          "is constructed by extending the LLM to enable cross-modal": "speaker-independent\nsettings, which\nuse\ndifferent\nspeakers",
          "speech. However,\nthese methods\ndo\nnot\nleverage\nspeaker": ""
        },
        {
          "is constructed by extending the LLM to enable cross-modal": "",
          "speech. However,\nthese methods\ndo\nnot\nleverage\nspeaker": "given speech features X = {x1, · · ·\n, xm, · · ·\n, xM }, where"
        },
        {
          "is constructed by extending the LLM to enable cross-modal": "for\ntraining\nand\ninference\n[22],\n[23].\nIn\nspeaker-dependent",
          "speech. However,\nthese methods\ndo\nnot\nleverage\nspeaker": ""
        },
        {
          "is constructed by extending the LLM to enable cross-modal": "",
          "speech. However,\nthese methods\ndo\nnot\nleverage\nspeaker": "xm is the m-th speech feature and M is the number of speech"
        },
        {
          "is constructed by extending the LLM to enable cross-modal": "settings,\nit has been reported that recognition performance can",
          "speech. However,\nthese methods\ndo\nnot\nleverage\nspeaker": ""
        },
        {
          "is constructed by extending the LLM to enable cross-modal": "",
          "speech. However,\nthese methods\ndo\nnot\nleverage\nspeaker": "features, as shown in Fig. 2 (a). The probability of y is defined"
        },
        {
          "is constructed by extending the LLM to enable cross-modal": "be improved by constructing a speaker-dependent SER model,",
          "speech. However,\nthese methods\ndo\nnot\nleverage\nspeaker": ""
        },
        {
          "is constructed by extending the LLM to enable cross-modal": "",
          "speech. However,\nthese methods\ndo\nnot\nleverage\nspeaker": "is\na\ntrainable\nparameter\nset.\nas P (y|X; Θcls), where Θcls"
        },
        {
          "is constructed by extending the LLM to enable cross-modal": "and\nspeaker\ninformation\nis\nconsidered\nto\nbe\nan\nimportant",
          "speech. However,\nthese methods\ndo\nnot\nleverage\nspeaker": ""
        },
        {
          "is constructed by extending the LLM to enable cross-modal": "",
          "speech. However,\nthese methods\ndo\nnot\nleverage\nspeaker": "is\ncomputed with a\ntransformer-based speech\nP (y|X; Θcls)"
        },
        {
          "is constructed by extending the LLM to enable cross-modal": "factor\nin SER [24].\nIn\nthe\nspeaker-independent\nsetting,\nby",
          "speech. However,\nthese methods\ndo\nnot\nleverage\nspeaker": ""
        },
        {
          "is constructed by extending the LLM to enable cross-modal": "",
          "speech. However,\nthese methods\ndo\nnot\nleverage\nspeaker": "encoder and an output\nlayer."
        },
        {
          "is constructed by extending the LLM to enable cross-modal": "using speaker information such as age [25], gender [26], [27],",
          "speech. However,\nthese methods\ndo\nnot\nleverage\nspeaker": ""
        },
        {
          "is constructed by extending the LLM to enable cross-modal": "",
          "speech. However,\nthese methods\ndo\nnot\nleverage\nspeaker": "Network Structure: The speech encoder converts X into"
        },
        {
          "is constructed by extending the LLM to enable cross-modal": "and speaker vectors\n[28],\n[29], SER performance has been",
          "speech. However,\nthese methods\ndo\nnot\nleverage\nspeaker": ""
        },
        {
          "is constructed by extending the LLM to enable cross-modal": "",
          "speech. However,\nthese methods\ndo\nnot\nleverage\nspeaker": "speech representations H as"
        },
        {
          "is constructed by extending the LLM to enable cross-modal": "improved. These methods demonstrated that speaker informa-",
          "speech. However,\nthese methods\ndo\nnot\nleverage\nspeaker": ""
        },
        {
          "is constructed by extending the LLM to enable cross-modal": "tion is effective for SER, but\nit\nremains unclear which char-",
          "speech. However,\nthese methods\ndo\nnot\nleverage\nspeaker": "(1)\nH = TransformerEnc(X; θspeech),"
        },
        {
          "is constructed by extending the LLM to enable cross-modal": "acteristics\nof\nspeaker\ninformation\nare\nparticularly\neffective.",
          "speech. However,\nthese methods\ndo\nnot\nleverage\nspeaker": ""
        },
        {
          "is constructed by extending the LLM to enable cross-modal": "",
          "speech. However,\nthese methods\ndo\nnot\nleverage\nspeaker": "where TransformerEnc()\nis\na\nfunction\nof\nthe\ntransformer"
        },
        {
          "is constructed by extending the LLM to enable cross-modal": "In addition, methods for\nremoving speaker\nindividuality have",
          "speech. However,\nthese methods\ndo\nnot\nleverage\nspeaker": ""
        },
        {
          "is constructed by extending the LLM to enable cross-modal": "",
          "speech. However,\nthese methods\ndo\nnot\nleverage\nspeaker": "encoder blocks that consist of multi-head self-attention layers"
        },
        {
          "is constructed by extending the LLM to enable cross-modal": "been proposed to perform generalized SER [30]. However,",
          "speech. However,\nthese methods\ndo\nnot\nleverage\nspeaker": ""
        },
        {
          "is constructed by extending the LLM to enable cross-modal": "",
          "speech. However,\nthese methods\ndo\nnot\nleverage\nspeaker": "and position-wise feed-forward networks\n[39], and θspeech ∈"
        },
        {
          "is constructed by extending the LLM to enable cross-modal": "this method embeds\nindividual differences\nin speakers\ninto",
          "speech. However,\nthese methods\ndo\nnot\nleverage\nspeaker": ""
        },
        {
          "is constructed by extending the LLM to enable cross-modal": "",
          "speech. However,\nthese methods\ndo\nnot\nleverage\nspeaker": "the attentive pooling\nΘcls are the trainable parameters. Next,"
        },
        {
          "is constructed by extending the LLM to enable cross-modal": "a\ncommon\nspace, which may\nreduce\nthe\naccuracy\nof SER",
          "speech. However,\nthese methods\ndo\nnot\nleverage\nspeaker": ""
        },
        {
          "is constructed by extending the LLM to enable cross-modal": "",
          "speech. However,\nthese methods\ndo\nnot\nleverage\nspeaker": "converts variable-length hidden vectors H into a fixed-size"
        },
        {
          "is constructed by extending the LLM to enable cross-modal": "for\nspeakers with large\nindividual differences. Furthermore,",
          "speech. However,\nthese methods\ndo\nnot\nleverage\nspeaker": ""
        },
        {
          "is constructed by extending the LLM to enable cross-modal": "",
          "speech. However,\nthese methods\ndo\nnot\nleverage\nspeaker": "vector h as"
        },
        {
          "is constructed by extending the LLM to enable cross-modal": "SER methods using the enrollment utterances, which are pre-",
          "speech. However,\nthese methods\ndo\nnot\nleverage\nspeaker": ""
        },
        {
          "is constructed by extending the LLM to enable cross-modal": "collected\nand\nreflect\nspeaker-specific\nemotional\ntraits,\nhave",
          "speech. However,\nthese methods\ndo\nnot\nleverage\nspeaker": "(2)\nh = AttentivePooling(H; θpool),"
        },
        {
          "is constructed by extending the LLM to enable cross-modal": "been proposed [10]–[13].\nIn these methods,\nthe model com-",
          "speech. However,\nthese methods\ndo\nnot\nleverage\nspeaker": ""
        },
        {
          "is constructed by extending the LLM to enable cross-modal": "",
          "speech. However,\nthese methods\ndo\nnot\nleverage\nspeaker": "where AttentivePooling()\nis\nthe attentive pooling function,"
        },
        {
          "is constructed by extending the LLM to enable cross-modal": "bines the emotional\ninformation in enrollment utterances with",
          "speech. However,\nthese methods\ndo\nnot\nleverage\nspeaker": ""
        },
        {
          "is constructed by extending the LLM to enable cross-modal": "",
          "speech. However,\nthese methods\ndo\nnot\nleverage\nspeaker": "and θpool ∈ Θcls are the trainable parameters. The probability"
        },
        {
          "is constructed by extending the LLM to enable cross-modal": "the\ntarget\nspeech to improve SER performance. This paper",
          "speech. However,\nthese methods\ndo\nnot\nleverage\nspeaker": ""
        },
        {
          "is constructed by extending the LLM to enable cross-modal": "",
          "speech. However,\nthese methods\ndo\nnot\nleverage\nspeaker": "of y is calculated by"
        },
        {
          "is constructed by extending the LLM to enable cross-modal": "aims\nto\nperform personalized SER for\nunseen\nspeakers\nby",
          "speech. However,\nthese methods\ndo\nnot\nleverage\nspeaker": ""
        },
        {
          "is constructed by extending the LLM to enable cross-modal": "using enrollment utterances and their\nlabels in ICL.",
          "speech. However,\nthese methods\ndo\nnot\nleverage\nspeaker": ""
        },
        {
          "is constructed by extending the LLM to enable cross-modal": "",
          "speech. However,\nthese methods\ndo\nnot\nleverage\nspeaker": "(3)\nP (y|X; Θcls) = Softmax(h; θsoft),"
        },
        {
          "is constructed by extending the LLM to enable cross-modal": "Speech LM:\nRecently, many\nspeech LMs\nthat\nenable",
          "speech. However,\nthese methods\ndo\nnot\nleverage\nspeaker": ""
        },
        {
          "is constructed by extending the LLM to enable cross-modal": "",
          "speech. However,\nthese methods\ndo\nnot\nleverage\nspeaker": "where Softmax()\nis a softmax-activation layer with a linear"
        },
        {
          "is constructed by extending the LLM to enable cross-modal": "cross-modal input speech and text have appeared by extending",
          "speech. However,\nthese methods\ndo\nnot\nleverage\nspeaker": ""
        },
        {
          "is constructed by extending the LLM to enable cross-modal": "",
          "speech. However,\nthese methods\ndo\nnot\nleverage\nspeaker": "transformation, and θsoft ∈ Θcls are the trainable parameters."
        },
        {
          "is constructed by extending the LLM to enable cross-modal": "the LLM, and these models can perform various speech tasks,",
          "speech. However,\nthese methods\ndo\nnot\nleverage\nspeaker": ""
        },
        {
          "is constructed by extending the LLM to enable cross-modal": "",
          "speech. However,\nthese methods\ndo\nnot\nleverage\nspeaker": "Training:\nThe model\nparameter\nis\noptimized\nset Θcls"
        },
        {
          "is constructed by extending the LLM to enable cross-modal": "including SER [31]–[33]. Also,\nspeech LMs\nspecialized for",
          "speech. However,\nthese methods\ndo\nnot\nleverage\nspeaker": ""
        },
        {
          "is constructed by extending the LLM to enable cross-modal": "",
          "speech. However,\nthese methods\ndo\nnot\nleverage\nspeaker": "· · ·\n,\n(X |Dcls|,\nfrom the training dataset Dcls = {(X 1, y1),"
        },
        {
          "is constructed by extending the LLM to enable cross-modal": "the SER task have appeared.\nIn these models,\n[15] proposes",
          "speech. However,\nthese methods\ndo\nnot\nleverage\nspeaker": ""
        },
        {
          "is constructed by extending the LLM to enable cross-modal": "",
          "speech. However,\nthese methods\ndo\nnot\nleverage\nspeaker": "y|Dcls|)} as"
        },
        {
          "is constructed by extending the LLM to enable cross-modal": "a method to suppress hallucinations of\nemotion labels,\nand",
          "speech. However,\nthese methods\ndo\nnot\nleverage\nspeaker": ""
        },
        {
          "is constructed by extending the LLM to enable cross-modal": "",
          "speech. However,\nthese methods\ndo\nnot\nleverage\nspeaker": "(cid:88)"
        },
        {
          "is constructed by extending the LLM to enable cross-modal": "[16] proposes a method to generate emotion captions\nrather",
          "speech. However,\nthese methods\ndo\nnot\nleverage\nspeaker": "ˆ"
        },
        {
          "is constructed by extending the LLM to enable cross-modal": "",
          "speech. However,\nthese methods\ndo\nnot\nleverage\nspeaker": "−\n(4)\nΘcls = arg min\nlog P (y|X; Θcls)."
        },
        {
          "is constructed by extending the LLM to enable cross-modal": "than\nto\noutput\nemotion\nlabels\ncorresponding\nto\nthe\ninput",
          "speech. However,\nthese methods\ndo\nnot\nleverage\nspeaker": "Θcls"
        },
        {
          "is constructed by extending the LLM to enable cross-modal": "",
          "speech. However,\nthese methods\ndo\nnot\nleverage\nspeaker": "(X,y)∈Dcls"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Inference: The inference of y is defined as": "",
          "the query that\nis trainable fixed-length embedding Q and H": "as"
        },
        {
          "Inference: The inference of y is defined as": "y = arg max\n(5)\nP (y|X; ˆΘcls).",
          "the query that\nis trainable fixed-length embedding Q and H": ""
        },
        {
          "Inference: The inference of y is defined as": "y",
          "the query that\nis trainable fixed-length embedding Q and H": "(8)\nU = QFormer(Q, H; θqf ),"
        },
        {
          "Inference: The inference of y is defined as": "B. Speech LM",
          "the query that\nis trainable fixed-length embedding Q and H": ""
        },
        {
          "Inference: The inference of y is defined as": "",
          "the query that\nis trainable fixed-length embedding Q and H": "where QFormer()\nis\na\nfunction of\nthe\ntransformer decoder"
        },
        {
          "Inference: The inference of y is defined as": "In this paper,\nthe speech LM predicts an emotion text Z =",
          "the query that\nis trainable fixed-length embedding Q and H": "blocks without a causal attention mask that consist of multi-"
        },
        {
          "Inference: The inference of y is defined as": "",
          "the query that\nis trainable fixed-length embedding Q and H": "head self-attention and cross-attention layers and position-wise"
        },
        {
          "Inference: The inference of y is defined as": "{z1, · · ·\n, zt, · · ·\n, zT }, where zt",
          "the query that\nis trainable fixed-length embedding Q and H": ""
        },
        {
          "Inference: The inference of y is defined as": "number of\ntokens\nin the emotion text, given instruction text",
          "the query that\nis trainable fixed-length embedding Q and H": "feed-forward networks\nare the trainable\n[39], and θqf ∈ Θsl"
        },
        {
          "Inference: The inference of y is defined as": "W and X, as shown in Fig. 2 (b). The generation probability",
          "the query that\nis trainable fixed-length embedding Q and H": "parameters."
        },
        {
          "Inference: The inference of y is defined as": "of Z is defined as",
          "the query that\nis trainable fixed-length embedding Q and H": "Text encoder:\nIn the text encoder, W is converted into a"
        },
        {
          "Inference: The inference of y is defined as": "",
          "the query that\nis trainable fixed-length embedding Q and H": "continuous representation R as"
        },
        {
          "Inference: The inference of y is defined as": "T(cid:89) t\n(6)\nP (Z|W , X; Θsl) =\nP (zt|z1:t−1, W , X; Θsl),",
          "the query that\nis trainable fixed-length embedding Q and H": ""
        },
        {
          "Inference: The inference of y is defined as": "",
          "the query that\nis trainable fixed-length embedding Q and H": "(9)\nR = Embedding(W ; ˆΘllm),"
        },
        {
          "Inference: The inference of y is defined as": "=1",
          "the query that\nis trainable fixed-length embedding Q and H": ""
        },
        {
          "Inference: The inference of y is defined as": "is a trainable param-\nwhere z1:t−1 = {z1, · · ·\n, zt−1} and Θsl",
          "the query that\nis trainable fixed-length embedding Q and H": "where Embedding() is the function that converts a token into"
        },
        {
          "Inference: The inference of y is defined as": "",
          "the query that\nis trainable fixed-length embedding Q and H": "ˆ"
        },
        {
          "Inference: The inference of y is defined as": "eter\nis computed with the speech LM\nset. P (Z|W , X; Θsl)",
          "the query that\nis trainable fixed-length embedding Q and H": "a continuous vector and\nΘllm ∈ Θsl."
        },
        {
          "Inference: The inference of y is defined as": "that combines a speech encoder with an LLM. In this paper, we",
          "the query that\nis trainable fixed-length embedding Q and H": "LLM encoder:\nIn\nthe\nLLM encoder, R and U are"
        },
        {
          "Inference: The inference of y is defined as": "use an instruction-tuned encoder-decoder\ntype LLM, and the",
          "the query that\nis trainable fixed-length embedding Q and H": "converted hidden representation C as"
        },
        {
          "Inference: The inference of y is defined as": "speech LM consists of a speech encoder, a conversion module,",
          "the query that\nis trainable fixed-length embedding Q and H": ""
        },
        {
          "Inference: The inference of y is defined as": "a text encoder, an LLM encoder, and an LLM decoder.",
          "the query that\nis trainable fixed-length embedding Q and H": "(10)\nC = TransformerEnc(A; ˆΘllm),"
        },
        {
          "Inference: The inference of y is defined as": "Instruction-tuned\nLLM:\nIn\nthis\npaper, we\nconstruct",
          "the query that\nis trainable fixed-length embedding Q and H": ""
        },
        {
          "Inference: The inference of y is defined as": "",
          "the query that\nis trainable fixed-length embedding Q and H": "A = PositionalEncoding([ ¯RT , ¯U T ]),\n(11)"
        },
        {
          "Inference: The inference of y is defined as": "a\nspeech\nLM by\nextending\nan\ninstruction-tuned\nLLM.",
          "the query that\nis trainable fixed-length embedding Q and H": ""
        },
        {
          "Inference: The inference of y is defined as": "Instruction-tuning\nis\na\nfine-tuning method\nto\nimprove\nthe",
          "the query that\nis trainable fixed-length embedding Q and H": ""
        },
        {
          "Inference: The inference of y is defined as": "",
          "the query that\nis trainable fixed-length embedding Q and H": "¯"
        },
        {
          "Inference: The inference of y is defined as": "ability\nto\nrespond\naccording\nto\ninstructions\nand\nthe\nICL",
          "the query that\nis trainable fixed-length embedding Q and H": "(12)\nR = SeqPosEmb(SegmentEmb(R; θseg); θseq),"
        },
        {
          "Inference: The inference of y is defined as": "capability [40], and the LLM is fine-tuned to predict a response",
          "the query that\nis trainable fixed-length embedding Q and H": ""
        },
        {
          "Inference: The inference of y is defined as": "",
          "the query that\nis trainable fixed-length embedding Q and H": "¯"
        },
        {
          "Inference: The inference of y is defined as": "text O given an instruction text W . The probabilities of O",
          "the query that\nis trainable fixed-length embedding Q and H": "(13)\nU = SeqPosEmb(SegmentEmb(U ; θseg); θseq),"
        },
        {
          "Inference: The inference of y is defined as": "are\ndefined\na\ntrainable\nas P (O|W ; Θllm), where Θllm is",
          "the query that\nis trainable fixed-length embedding Q and H": ""
        },
        {
          "Inference: The inference of y is defined as": "",
          "the query that\nis trainable fixed-length embedding Q and H": "SeqPosEmb()\nwhere PosEnc()\nis\npositional\nencoding,\nis"
        },
        {
          "Inference: The inference of y is defined as": "parameter set, which is initialized by pretraining with a next",
          "the query that\nis trainable fixed-length embedding Q and H": ""
        },
        {
          "Inference: The inference of y is defined as": "",
          "the query that\nis trainable fixed-length embedding Q and H": "sequence position embedding that\nindicates\nthe order of\nthe"
        },
        {
          "Inference: The inference of y is defined as": "token prediction objective. Θllm is optimized from the training",
          "the query that\nis trainable fixed-length embedding Q and H": ""
        },
        {
          "Inference: The inference of y is defined as": "",
          "the query that\nis trainable fixed-length embedding Q and H": "input\nsequences, SegmentEmb()\nis\nsegment embedding that"
        },
        {
          "Inference: The inference of y is defined as": ", (W |Dins|, O|Dins|)} as\ndataset Dins = {(W 1, O1), · · ·",
          "the query that\nis trainable fixed-length embedding Q and H": ""
        },
        {
          "Inference: The inference of y is defined as": "",
          "the query that\nis trainable fixed-length embedding Q and H": "are\ndistinguishes different modalities and {θseg, θseq} ∈ Θsl"
        },
        {
          "Inference: The inference of y is defined as": "(cid:88)",
          "the query that\nis trainable fixed-length embedding Q and H": ""
        },
        {
          "Inference: The inference of y is defined as": "ˆ",
          "the query that\nis trainable fixed-length embedding Q and H": "the trainable parameters."
        },
        {
          "Inference: The inference of y is defined as": "(7)\nΘllm = arg min\nP (O|W ; Θllm).",
          "the query that\nis trainable fixed-length embedding Q and H": ""
        },
        {
          "Inference: The inference of y is defined as": "Θllm",
          "the query that\nis trainable fixed-length embedding Q and H": ""
        },
        {
          "Inference: The inference of y is defined as": "(W ,O)∈Dins",
          "the query that\nis trainable fixed-length embedding Q and H": "LLM decoder:\nIn the LLM decoder, the generation proba-"
        },
        {
          "Inference: The inference of y is defined as": "",
          "the query that\nis trainable fixed-length embedding Q and H": "bility of zt\nis calculated given C and z1:t−1 = {z1, · · ·\n, zt−1}"
        },
        {
          "Inference: The inference of y is defined as": "Speech encoder:\nIn the speech encoder, X is converted",
          "the query that\nis trainable fixed-length embedding Q and H": ""
        },
        {
          "Inference: The inference of y is defined as": "",
          "the query that\nis trainable fixed-length embedding Q and H": "as"
        },
        {
          "Inference: The inference of y is defined as": "into H as shown in Eq.\n(1), and θspeech is fixed.",
          "the query that\nis trainable fixed-length embedding Q and H": ""
        },
        {
          "Inference: The inference of y is defined as": "Conversion module: To input X into the LLM, a con-",
          "the query that\nis trainable fixed-length embedding Q and H": "(14)\nP (zt|z1:t−1, W , X; Θsl) = Softmax(F ; ˆΘllm),"
        },
        {
          "Inference: The inference of y is defined as": "version module is required to align the input embedding space",
          "the query that\nis trainable fixed-length embedding Q and H": ""
        },
        {
          "Inference: The inference of y is defined as": "of\nthe LLM and the speech representation.\nIn this paper, we",
          "the query that\nis trainable fixed-length embedding Q and H": "(15)\nF = TransformerDec(C, z1:t−1; ˆΘllm),"
        },
        {
          "Inference: The inference of y is defined as": "use Q-Former\n[41], which\ncan\nconvert\ninput\nsequences\nof",
          "the query that\nis trainable fixed-length embedding Q and H": ""
        },
        {
          "Inference: The inference of y is defined as": "arbitrary length into fixed-length representations.\nIn the Q-",
          "the query that\nis trainable fixed-length embedding Q and H": "where TransformerDec()\nis\na\nfunction\nof\nthe\ntransformer"
        },
        {
          "Inference: The inference of y is defined as": "Former, fixed-length query representation U is outputted given",
          "the query that\nis trainable fixed-length embedding Q and H": "decoder blocks."
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Training: The model parameter set Θsl": "",
          "is optimized from": "",
          "TABLE I": ""
        },
        {
          "Training: The model parameter set Θsl": "training dataset Dtext = {(X 1, Z1), · · ·",
          "is optimized from": "",
          "TABLE I": ""
        },
        {
          "Training: The model parameter set Θsl": "",
          "is optimized from": "",
          "TABLE I": ""
        },
        {
          "Training: The model parameter set Θsl": "",
          "is optimized from": "",
          "TABLE I": ""
        },
        {
          "Training: The model parameter set Θsl": "",
          "is optimized from": "",
          "TABLE I": "EMOTIONS."
        },
        {
          "Training: The model parameter set Θsl": "(cid:88)",
          "is optimized from": "",
          "TABLE I": ""
        },
        {
          "Training: The model parameter set Θsl": "",
          "is optimized from": "",
          "TABLE I": ""
        },
        {
          "Training: The model parameter set Θsl": "−",
          "is optimized from": "(16)",
          "TABLE I": ""
        },
        {
          "Training: The model parameter set Θsl": "",
          "is optimized from": "",
          "TABLE I": "Train"
        },
        {
          "Training: The model parameter set Θsl": "(X,Z)∈Dtext",
          "is optimized from": "",
          "TABLE I": ""
        },
        {
          "Training: The model parameter set Θsl": "",
          "is optimized from": "",
          "TABLE I": "1,050"
        },
        {
          "Training: The model parameter set Θsl": "W is a common instruction text across Dtext. During training,",
          "is optimized from": "",
          "TABLE I": "24,850"
        },
        {
          "Training: The model parameter set Θsl": "",
          "is optimized from": "",
          "TABLE I": "14,000"
        },
        {
          "Training: The model parameter set Θsl": "the speech encoder parameters are fixed, and the LLM param-",
          "is optimized from": "",
          "TABLE I": ""
        },
        {
          "Training: The model parameter set Θsl": "",
          "is optimized from": "",
          "TABLE I": "23,100"
        },
        {
          "Training: The model parameter set Θsl": "",
          "is optimized from": "",
          "TABLE I": "22,750"
        },
        {
          "Training: The model parameter set Θsl": "",
          "is optimized from": "",
          "TABLE I": "17,150"
        },
        {
          "Training: The model parameter set Θsl": "",
          "is optimized from": "",
          "TABLE I": "1,050"
        },
        {
          "Training: The model parameter set Θsl": "Inference: The inference of Z is defined as",
          "is optimized from": "",
          "TABLE I": ""
        },
        {
          "Training: The model parameter set Θsl": "",
          "is optimized from": "",
          "TABLE I": "24,850"
        },
        {
          "Training: The model parameter set Θsl": "",
          "is optimized from": "",
          "TABLE I": "17,850"
        },
        {
          "Training: The model parameter set Θsl": "Z = arg max",
          "is optimized from": "(17)",
          "TABLE I": ""
        },
        {
          "Training: The model parameter set Θsl": "",
          "is optimized from": "",
          "TABLE I": "28,000"
        },
        {
          "Training: The model parameter set Θsl": "Z",
          "is optimized from": "",
          "TABLE I": ""
        },
        {
          "Training: The model parameter set Θsl": "",
          "is optimized from": "",
          "TABLE I": "28,000"
        },
        {
          "Training: The model parameter set Θsl": "",
          "is optimized from": "",
          "TABLE I": "22,050"
        },
        {
          "Training: The model parameter set Θsl": "IV. PROPOSED METHOD",
          "is optimized from": "",
          "TABLE I": ""
        },
        {
          "Training: The model parameter set Θsl": "",
          "is optimized from": "",
          "TABLE I": "350"
        },
        {
          "Training: The model parameter set Θsl": "",
          "is optimized from": "",
          "TABLE I": "225,050"
        },
        {
          "Training: The model parameter set Θsl": "This paper proposes a method to perform personalized SER",
          "is optimized from": "",
          "TABLE I": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "were pre-trained with a large amount of text based on unifying": ""
        },
        {
          "were pre-trained with a large amount of text based on unifying": "language learning paradigms [45], and instruction-tuned with"
        },
        {
          "were pre-trained with a large amount of text based on unifying": ""
        },
        {
          "were pre-trained with a large amount of text based on unifying": "various language tasks. Details of the comparison method are"
        },
        {
          "were pre-trained with a large amount of text based on unifying": ""
        },
        {
          "were pre-trained with a large amount of text based on unifying": "shown below."
        },
        {
          "were pre-trained with a large amount of text based on unifying": "Classifier model:"
        },
        {
          "were pre-trained with a large amount of text based on unifying": "•\nScratch model: a transformer-based model\nin which all"
        },
        {
          "were pre-trained with a large amount of text based on unifying": ""
        },
        {
          "were pre-trained with a large amount of text based on unifying": "parameters were randomly initialized, defined as Eq. (4),"
        },
        {
          "were pre-trained with a large amount of text based on unifying": "• Pretrained model: a transformer-based model with the"
        },
        {
          "were pre-trained with a large amount of text based on unifying": "pretrained speech encoder.\nIn this model,\nthe parameters"
        },
        {
          "were pre-trained with a large amount of text based on unifying": "of\nthe speech encoder were fixed."
        },
        {
          "were pre-trained with a large amount of text based on unifying": "• Personalized modelN [11]: a model with a single neutral"
        },
        {
          "were pre-trained with a large amount of text based on unifying": "utterance added to the input of\nthe pretrained model."
        },
        {
          "were pre-trained with a large amount of text based on unifying": "• Personalized modelA [13]: a model with each emotional"
        },
        {
          "were pre-trained with a large amount of text based on unifying": "utterance of\na\ntarget\nspeaker\nadded to the\ninput of\nthe"
        },
        {
          "were pre-trained with a large amount of text based on unifying": "pretrained model."
        },
        {
          "were pre-trained with a large amount of text based on unifying": "Speech LM:"
        },
        {
          "were pre-trained with a large amount of text based on unifying": ""
        },
        {
          "were pre-trained with a large amount of text based on unifying": "•\n0-shot:\na model\nthat\npredicts\nemotion\ntext\ngiven\nan"
        },
        {
          "were pre-trained with a large amount of text based on unifying": ""
        },
        {
          "were pre-trained with a large amount of text based on unifying": "instruction text and speech features defined as Eq.\n(15)."
        },
        {
          "were pre-trained with a large amount of text based on unifying": ""
        },
        {
          "were pre-trained with a large amount of text based on unifying": "•\na model\nthat predicts\nemotion text\nfor\ntarget\n1-shotN :"
        },
        {
          "were pre-trained with a large amount of text based on unifying": ""
        },
        {
          "were pre-trained with a large amount of text based on unifying": "speech by conditioning a single neutral speech of a target"
        },
        {
          "were pre-trained with a large amount of text based on unifying": ""
        },
        {
          "were pre-trained with a large amount of text based on unifying": "speaker and a text described as “Neutral”."
        },
        {
          "were pre-trained with a large amount of text based on unifying": ""
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "However,\nthe dataset used in conventional SER only includes": "about\n10\nspeakers\n[42]–[44],\nso\nit\nis\ndifficult\nto\nevaluate",
          "pooling layers with a slide of 2, so we down-sampled them to": "1/4 along with the time axis. After\nthese layers, we stacked"
        },
        {
          "However,\nthe dataset used in conventional SER only includes": "in the\nspeaker-independent\nsetting with a\nsufficient number",
          "pooling layers with a slide of 2, so we down-sampled them to": "6-layer\ntransformer\nencoder\nblocks. For\nthe Q-Former, we"
        },
        {
          "However,\nthe dataset used in conventional SER only includes": "of\nspeakers. Thus,\nthis\nstudy\nnewly\ncollected\na\nJapanese",
          "pooling layers with a slide of 2, so we down-sampled them to": "stacked 2-layer\ntransformer decoder blocks without a causal"
        },
        {
          "However,\nthe dataset used in conventional SER only includes": "SER dataset\nthat has many speakers. This dataset consists of",
          "pooling layers with a slide of 2, so we down-sampled them to": "attention mask. Based on preliminary experiments,\nthe length"
        },
        {
          "However,\nthe dataset used in conventional SER only includes": "800 native\nJapanese\nspeakers\n(368 males\nand 432 females)",
          "pooling layers with a slide of 2, so we down-sampled them to": "of the query was set to 150, as it yielded the best performance."
        },
        {
          "However,\nthe dataset used in conventional SER only includes": "who performed 50 utterances each for seven emotions: anger,",
          "pooling layers with a slide of 2, so we down-sampled them to": "For the LLM encoder and decoder, we used 1024-dimensional"
        },
        {
          "However,\nthe dataset used in conventional SER only includes": "disgust, fear,\njoy, sadness, surprise, and neutral. The text read",
          "pooling layers with a slide of 2, so we down-sampled them to": "token embeddings where the vocabulary size was set to 37,156"
        },
        {
          "However,\nthe dataset used in conventional SER only includes": "aloud is unrelated to the\nspeaker’s\nemotional\nstate,\nso even",
          "pooling layers with a slide of 2, so we down-sampled them to": "and stacked 32-layer\ntransformer encoder blocks and 6-layer"
        },
        {
          "However,\nthe dataset used in conventional SER only includes": "if\nthe\ntext\ncontent\nis\ntaken\ninto\nconsideration,\nit\ndoes\nnot",
          "pooling layers with a slide of 2, so we down-sampled them to": "transformer decoder blocks. In the speech encoder, Q-Former,"
        },
        {
          "However,\nthe dataset used in conventional SER only includes": "contribute to emotion estimation. We instructed the speakers",
          "pooling layers with a slide of 2, so we down-sampled them to": "and LLM encoder and decoder,\nthe dimensions of\nthe output"
        },
        {
          "However,\nthe dataset used in conventional SER only includes": "to “speak as clearly as possible so that your emotions come",
          "pooling layers with a slide of 2, so we down-sampled them to": "continuous\nrepresentations were\nset\nto 512, 512,\nand 1024,"
        },
        {
          "However,\nthe dataset used in conventional SER only includes": "across”\nfor\neach text,\nand recorded speech using a headset",
          "pooling layers with a slide of 2, so we down-sampled them to": "those of\nthe inner outputs\nin the position-wise feed-forward"
        },
        {
          "However,\nthe dataset used in conventional SER only includes": "microphone and a laptop computer in a conference room free",
          "pooling layers with a slide of 2, so we down-sampled them to": "networks were set\nto 2048, 2048 and 4096, and the number"
        },
        {
          "However,\nthe dataset used in conventional SER only includes": "of background noise. We divided this dataset\ninto training,",
          "pooling layers with a slide of 2, so we down-sampled them to": "of\nheads\nin\nthe multi-head\nattention was\nset\nto\n8,\n8,\nand"
        },
        {
          "However,\nthe dataset used in conventional SER only includes": "development,\nand test\nsets\nso that\nthe gender\nratio and age",
          "pooling layers with a slide of 2, so we down-sampled them to": "16,\nrespectively. For\ntraining, we used the RAdam optimizer."
        },
        {
          "However,\nthe dataset used in conventional SER only includes": "groups\nare\nevenly distributed,\nas\nshown in Table\nI.\nIn this",
          "pooling layers with a slide of 2, so we down-sampled them to": "We set\nthe mini-batch size to 64 utterances and the dropout"
        },
        {
          "However,\nthe dataset used in conventional SER only includes": "dataset, we used a common instruction text: “Please select\nthe",
          "pooling layers with a slide of 2, so we down-sampled them to": "rate\nin\nthe\ntransformer\nblocks\nto\n0.1. We\nintroduced\nlabel"
        },
        {
          "However,\nthe dataset used in conventional SER only includes": "appropriate emotion for\nthe input speech from the following:",
          "pooling layers with a slide of 2, so we down-sampled them to": "smoothing, where its smoothing parameter was set\nto 0.1. For"
        },
        {
          "However,\nthe dataset used in conventional SER only includes": "Neutral, Surprise, Sadness, Joy, Fear, Disgust, Anger.”",
          "pooling layers with a slide of 2, so we down-sampled them to": "inference with speech LM, we used a beam search algorithm"
        },
        {
          "However,\nthe dataset used in conventional SER only includes": "",
          "pooling layers with a slide of 2, so we down-sampled them to": "in which the beam size was set\nto 4, and conditioned 0–7-shot"
        },
        {
          "However,\nthe dataset used in conventional SER only includes": "B. Models",
          "pooling layers with a slide of 2, so we down-sampled them to": ""
        },
        {
          "However,\nthe dataset used in conventional SER only includes": "",
          "pooling layers with a slide of 2, so we down-sampled them to": "enrollment utterance-label pairs for\nICL."
        },
        {
          "However,\nthe dataset used in conventional SER only includes": "We\ncompare\nthe\nperformance\nof\nfour\nclassifier models,",
          "pooling layers with a slide of 2, so we down-sampled them to": ""
        },
        {
          "However,\nthe dataset used in conventional SER only includes": "two speech LMs, and the proposed method.\nIn the classifier",
          "pooling layers with a slide of 2, so we down-sampled them to": "D. Evaluation"
        },
        {
          "However,\nthe dataset used in conventional SER only includes": "model and speech LM, we utilized an in-house transformer-",
          "pooling layers with a slide of 2, so we down-sampled them to": ""
        },
        {
          "However,\nthe dataset used in conventional SER only includes": "",
          "pooling layers with a slide of 2, so we down-sampled them to": "As evaluation metrics, we used unweighted accuracy for the"
        },
        {
          "However,\nthe dataset used in conventional SER only includes": "based speech encoder, which has 42M parameters, and these",
          "pooling layers with a slide of 2, so we down-sampled them to": ""
        },
        {
          "However,\nthe dataset used in conventional SER only includes": "",
          "pooling layers with a slide of 2, so we down-sampled them to": "speaker\nthe average of\nindividual\nspeaker\n(UAspk), which is"
        },
        {
          "However,\nthe dataset used in conventional SER only includes": "parameters were pre-trained in various\nspeech-understanding",
          "pooling layers with a slide of 2, so we down-sampled them to": ""
        },
        {
          "However,\nthe dataset used in conventional SER only includes": "",
          "pooling layers with a slide of 2, so we down-sampled them to": "accuracies, to evaluate whether personalized SER is performed"
        },
        {
          "However,\nthe dataset used in conventional SER only includes": "tasks. In the speech LM, we utilized an encoder-decoder style",
          "pooling layers with a slide of 2, so we down-sampled them to": ""
        },
        {
          "However,\nthe dataset used in conventional SER only includes": "",
          "pooling layers with a slide of 2, so we down-sampled them to": "for\nunseen\nspeakers.\nIn\naddition, we\ncalculated\nstandard"
        },
        {
          "However,\nthe dataset used in conventional SER only includes": "in-house LLM, which has 0.6B parameters. The parameters",
          "pooling layers with a slide of 2, so we down-sampled them to": ""
        },
        {
          "However,\nthe dataset used in conventional SER only includes": "",
          "pooling layers with a slide of 2, so we down-sampled them to": "statistics, such as standard deviation, median, maximum, and"
        },
        {
          "However,\nthe dataset used in conventional SER only includes": "were pre-trained with a large amount of text based on unifying",
          "pooling layers with a slide of 2, so we down-sampled them to": ""
        },
        {
          "However,\nthe dataset used in conventional SER only includes": "",
          "pooling layers with a slide of 2, so we down-sampled them to": "minimum of UAspk. Note that when evaluating the speech LM,"
        },
        {
          "However,\nthe dataset used in conventional SER only includes": "language learning paradigms [45], and instruction-tuned with",
          "pooling layers with a slide of 2, so we down-sampled them to": ""
        },
        {
          "However,\nthe dataset used in conventional SER only includes": "",
          "pooling layers with a slide of 2, so we down-sampled them to": "which is a generative model, a generated result\nis correct\nif it"
        },
        {
          "However,\nthe dataset used in conventional SER only includes": "various language tasks. Details of the comparison method are",
          "pooling layers with a slide of 2, so we down-sampled them to": ""
        },
        {
          "However,\nthe dataset used in conventional SER only includes": "",
          "pooling layers with a slide of 2, so we down-sampled them to": "exactly matches the correct answer."
        },
        {
          "However,\nthe dataset used in conventional SER only includes": "shown below.",
          "pooling layers with a slide of 2, so we down-sampled them to": ""
        },
        {
          "However,\nthe dataset used in conventional SER only includes": "Classifier model:",
          "pooling layers with a slide of 2, so we down-sampled them to": "E. Results"
        },
        {
          "However,\nthe dataset used in conventional SER only includes": "•\nScratch model: a transformer-based model\nin which all",
          "pooling layers with a slide of 2, so we down-sampled them to": "Main result: Table II"
        },
        {
          "However,\nthe dataset used in conventional SER only includes": "",
          "pooling layers with a slide of 2, so we down-sampled them to": "shows UAspk,"
        },
        {
          "However,\nthe dataset used in conventional SER only includes": "parameters were randomly initialized, defined as Eq. (4),",
          "pooling layers with a slide of 2, so we down-sampled them to": "and minimum\n(σ(·)), median (µ1/2(·)), maximum (max(·)),"
        },
        {
          "However,\nthe dataset used in conventional SER only includes": "• Pretrained model: a transformer-based model with the",
          "pooling layers with a slide of 2, so we down-sampled them to": "in each model. The\n“Neutral”\nin\n(min(·)) values of UAspk"
        },
        {
          "However,\nthe dataset used in conventional SER only includes": "pretrained speech encoder.\nIn this model,\nthe parameters",
          "pooling layers with a slide of 2, so we down-sampled them to": "the setting column of\nthe table shows the performance when"
        },
        {
          "However,\nthe dataset used in conventional SER only includes": "of\nthe speech encoder were fixed.",
          "pooling layers with a slide of 2, so we down-sampled them to": "neutral utterances were only used in ICL-based inference.\nIn"
        },
        {
          "However,\nthe dataset used in conventional SER only includes": "• Personalized modelN [11]: a model with a single neutral",
          "pooling layers with a slide of 2, so we down-sampled them to": "the table, although the PersonalizedA, which uses enrollment"
        },
        {
          "However,\nthe dataset used in conventional SER only includes": "utterance added to the input of\nthe pretrained model.",
          "pooling layers with a slide of 2, so we down-sampled them to": "speech\nof\nall\nemotions,\noutperformed\nother models\nin\nthe"
        },
        {
          "However,\nthe dataset used in conventional SER only includes": "• Personalized modelA [13]: a model with each emotional",
          "pooling layers with a slide of 2, so we down-sampled them to": "classifier model,\nit\nperformed worse\nthan\nthe\n0-shot. This"
        },
        {
          "However,\nthe dataset used in conventional SER only includes": "utterance of\na\ntarget\nspeaker\nadded to the\ninput of\nthe",
          "pooling layers with a slide of 2, so we down-sampled them to": "result\nindicates\nthat\nthe speech LM can learn the SER task"
        },
        {
          "However,\nthe dataset used in conventional SER only includes": "pretrained model.",
          "pooling layers with a slide of 2, so we down-sampled them to": "better\nthan\nclassifier models.\nIn\naddition,\nin\nthe\nproposed"
        },
        {
          "However,\nthe dataset used in conventional SER only includes": "Speech LM:",
          "pooling layers with a slide of 2, so we down-sampled them to": "method,\nthe SER performance\nimproved\nas\nthe\nnumber\nof"
        },
        {
          "However,\nthe dataset used in conventional SER only includes": "",
          "pooling layers with a slide of 2, so we down-sampled them to": "enrollment\nutterance-label\npairs\nincreased,\nand\nthe\nhighest"
        },
        {
          "However,\nthe dataset used in conventional SER only includes": "•\n0-shot:\na model\nthat\npredicts\nemotion\ntext\ngiven\nan",
          "pooling layers with a slide of 2, so we down-sampled them to": ""
        },
        {
          "However,\nthe dataset used in conventional SER only includes": "",
          "pooling layers with a slide of 2, so we down-sampled them to": "performance was achieved when all emotional utterances were"
        },
        {
          "However,\nthe dataset used in conventional SER only includes": "instruction text and speech features defined as Eq.\n(15).",
          "pooling layers with a slide of 2, so we down-sampled them to": ""
        },
        {
          "However,\nthe dataset used in conventional SER only includes": "",
          "pooling layers with a slide of 2, so we down-sampled them to": "used. These\nresults\nindicate\nthat\nthe\nproposed method\ncan"
        },
        {
          "However,\nthe dataset used in conventional SER only includes": "•\na model\nthat predicts\nemotion text\nfor\ntarget\n1-shotN :",
          "pooling layers with a slide of 2, so we down-sampled them to": ""
        },
        {
          "However,\nthe dataset used in conventional SER only includes": "",
          "pooling layers with a slide of 2, so we down-sampled them to": "perform personalized SER via ICL."
        },
        {
          "However,\nthe dataset used in conventional SER only includes": "speech by conditioning a single neutral speech of a target",
          "pooling layers with a slide of 2, so we down-sampled them to": ""
        },
        {
          "However,\nthe dataset used in conventional SER only includes": "",
          "pooling layers with a slide of 2, so we down-sampled them to": "How to select enrollment utterances in ICL: To investi-"
        },
        {
          "However,\nthe dataset used in conventional SER only includes": "speaker and a text described as “Neutral”.",
          "pooling layers with a slide of 2, so we down-sampled them to": ""
        },
        {
          "However,\nthe dataset used in conventional SER only includes": "",
          "pooling layers with a slide of 2, so we down-sampled them to": "gate the impact of selecting methods of enrollment utterances"
        },
        {
          "However,\nthe dataset used in conventional SER only includes": "C.\nSetup",
          "pooling layers with a slide of 2, so we down-sampled them to": ""
        },
        {
          "However,\nthe dataset used in conventional SER only includes": "",
          "pooling layers with a slide of 2, so we down-sampled them to": "on performance, we performed ICL using enrollment utter-"
        },
        {
          "However,\nthe dataset used in conventional SER only includes": "These models were\ncomposed\nunder\nthe\nfollowing\ncon-",
          "pooling layers with a slide of 2, so we down-sampled them to": "ances\nthat\nsatisfy the\nfollowing settings: TU+LD, TU+LU,"
        },
        {
          "However,\nthe dataset used in conventional SER only includes": "ditions: For\nthe\nspeech encoder, we used 80 log mel-scale",
          "pooling layers with a slide of 2, so we down-sampled them to": "TE+LD, TE+LU,\nand TU+LO settings. Table\nIII\nshows\nthe"
        },
        {
          "However,\nthe dataset used in conventional SER only includes": "filterbank coefficients as speech features. The frame shift was",
          "pooling layers with a slide of 2, so we down-sampled them to": "results UAspk of each ICL setting. In the table, using 1–2-shot"
        },
        {
          "However,\nthe dataset used in conventional SER only includes": "10 ms. The speech features passed two convolution and max",
          "pooling layers with a slide of 2, so we down-sampled them to": "examples achieved equivalent performance under all settings."
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "TABLE II": ""
        },
        {
          "TABLE II": "(↓)\nσ(UAspk)"
        },
        {
          "TABLE II": "0.099"
        },
        {
          "TABLE II": "0.114"
        },
        {
          "TABLE II": "0.108"
        },
        {
          "TABLE II": "0.109"
        },
        {
          "TABLE II": "0.110"
        },
        {
          "TABLE II": "0.102"
        },
        {
          "TABLE II": "0.113"
        },
        {
          "TABLE II": "0.099"
        },
        {
          "TABLE II": "0.102"
        },
        {
          "TABLE II": "0.097"
        },
        {
          "TABLE II": "0.096"
        },
        {
          "TABLE II": "0.098"
        },
        {
          "TABLE II": "0.099"
        },
        {
          "TABLE II": "0.098"
        },
        {
          "TABLE II": "TABLE III"
        },
        {
          "TABLE II": ""
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "TABLE III": ""
        },
        {
          "TABLE III": ""
        },
        {
          "TABLE III": "Anger"
        },
        {
          "TABLE III": "0.695"
        },
        {
          "TABLE III": "0.705"
        },
        {
          "TABLE III": "0.709"
        },
        {
          "TABLE III": "0.712"
        },
        {
          "TABLE III": "–"
        },
        {
          "TABLE III": "–"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "TABLE IV": ""
        },
        {
          "TABLE IV": "2-shot"
        },
        {
          "TABLE IV": "0.717"
        },
        {
          "TABLE IV": "0.621"
        },
        {
          "TABLE IV": "0.638"
        },
        {
          "TABLE IV": "0.696"
        },
        {
          "TABLE IV": "0.512"
        },
        {
          "TABLE IV": "0.684"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "(Neutral)\n0.516\n0.512\nTO+LD7": "0.677\n0.684\nNeutral0:7",
          "0.508\n0.506\n–\n–\n–": "0.688\n0.689\n–\n–\n–"
        },
        {
          "(Neutral)\n0.516\n0.512\nTO+LD7": "On the other hand, in the TU+LO setting, the ICL performance",
          "0.508\n0.506\n–\n–\n–": "decreased with\nincreased\nexamples. These\nresults\nTO+LD7"
        },
        {
          "(Neutral)\n0.516\n0.512\nTO+LD7": "was improved or decreased slightly when using three or more",
          "0.508\n0.506\n–\n–\n–": "show that\nthe speech LM meta-trained with TO+LD7 was op-"
        },
        {
          "(Neutral)\n0.516\n0.512\nTO+LD7": "examples. These results\nindicate that\nif\nthe same emotion is",
          "0.508\n0.506\n–\n–\n–": "timized for input of 7-shot examples, so it cannot personalize"
        },
        {
          "(Neutral)\n0.516\n0.512\nTO+LD7": "used in ICL, up to two shots\nare\neffective\nfor personaliza-",
          "0.508\n0.506\n–\n–\n–": "SER via ICL. Also,\nin the TU+LO setting,\nthe performance"
        },
        {
          "(Neutral)\n0.516\n0.512\nTO+LD7": "tion.\nIn other\nsettings, SER performance was\nimproved with",
          "0.508\n0.506\n–\n–\n–": "of\nthe\nspeech LM meta-trained with Neutral0:7 was worse"
        },
        {
          "(Neutral)\n0.516\n0.512\nTO+LD7": "increased examples\nin ICL,\nand performance was\nimproved",
          "0.508\n0.506\n–\n–\n–": "it\nis inferred that\nthan that with TU+LU0:7. From the above,"
        },
        {
          "(Neutral)\n0.516\n0.512\nTO+LD7": "in\nthe\norder\nof TU+LD>TU+LU>TE+LD>TE+LU. These",
          "0.508\n0.506\n–\n–\n–": "the speech LM can improve the personalization capability via"
        },
        {
          "(Neutral)\n0.516\n0.512\nTO+LD7": "results show that\nin SER via ICL, personalization works well",
          "0.508\n0.506\n–\n–\n–": "ICL by meta-training using enrollment utterances expressing"
        },
        {
          "(Neutral)\n0.516\n0.512\nTO+LD7": "when utterances can be prepared under TU or LU settings.",
          "0.508\n0.506\n–\n–\n–": "various emotions."
        },
        {
          "(Neutral)\n0.516\n0.512\nTO+LD7": "",
          "0.508\n0.506\n–\n–\n–": "VI. CONCLUSION"
        },
        {
          "(Neutral)\n0.516\n0.512\nTO+LD7": "How to\nselect\nenrollment\nutterances\nin MetaICL:\nTo",
          "0.508\n0.506\n–\n–\n–": ""
        },
        {
          "(Neutral)\n0.516\n0.512\nTO+LD7": "investigate\nthe\neffects of\nthe\ntypes of\nenrollment utterances",
          "0.508\n0.506\n–\n–\n–": "This\npaper\nproposed\na method\nfor\npersonalizing\nspeech"
        },
        {
          "(Neutral)\n0.516\n0.512\nTO+LD7": "used in MetaICL, we prepared two meta-trained speech LM",
          "0.508\n0.506\n–\n–\n–": "emotion recognition (SER) via in-context\nlearning (ICL).\nIn"
        },
        {
          "(Neutral)\n0.516\n0.512\nTO+LD7": "using enrollment utterances that satisfy the following settings:",
          "0.508\n0.506\n–\n–\n–": "the proposed method, the speech language model (speech LM),"
        },
        {
          "(Neutral)\n0.516\n0.512\nTO+LD7": "TO+LD7, which had only 7-shot examples using each emo-",
          "0.508\n0.506\n–\n–\n–": "extended from a\nlarge\nlanguage model with ICL capability,"
        },
        {
          "(Neutral)\n0.516\n0.512\nTO+LD7": "tion, and Neutral0:7, which had 0–7-shot examples using only",
          "0.508\n0.506\n–\n–\n–": "performs personalization for SER by conditioning a few enroll-"
        },
        {
          "(Neutral)\n0.516\n0.512\nTO+LD7": "neutral utterance. Table\nIV shows\nthe\nresults of\nICL-based",
          "0.508\n0.506\n–\n–\n–": "ment utterance-label pairs uttered by a target speaker\nin ICL-"
        },
        {
          "(Neutral)\n0.516\n0.512\nTO+LD7": "inference using TU+LD and TU+LO (neutral utterances only)",
          "0.508\n0.506\n–\n–\n–": "based inference. Experimental\nresults using our created SER"
        },
        {
          "(Neutral)\n0.516\n0.512\nTO+LD7": "settings. In the TU+LD setting,\nthe performance of the speech",
          "0.508\n0.506\n–\n–\n–": "dataset\ndemonstrated\nthat\nthe\nproposed method\nhad\nhigher"
        },
        {
          "(Neutral)\n0.516\n0.512\nTO+LD7": "than\nthat with\nLM meta-trained with TO+LD7 was worse",
          "0.508\n0.506\n–\n–\n–": "performance than conventional personalization methods, and"
        },
        {
          "(Neutral)\n0.516\n0.512\nTO+LD7": "Neutral0:7 when 1–6-shot examples were used. In the TU+LO",
          "0.508\n0.506\n–\n–\n–": "SER performance was\nimproved when utterances with arbi-"
        },
        {
          "(Neutral)\n0.516\n0.512\nTO+LD7": "setting,\nthe performance of\nthe speech LM meta-trained with",
          "0.508\n0.506\n–\n–\n–": "trary emotion labels were increased in ICL-based inference."
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "REFERENCES": "",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": "tion Using Audio and Text,” in Proc. IEEE Spoken Language Technology"
        },
        {
          "REFERENCES": "",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": "Workshop (SLT), 2018, pp. 112–118."
        },
        {
          "REFERENCES": "[1] A. Ando, R. Masumura, H. Kamiyama, S. Kobashikawa, Y. Aono, and",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": ""
        },
        {
          "REFERENCES": "",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": "[20]\nJ. Wang, M. Xue, R. Culhane, E. Diao, J. Ding, and V. Tarokh, “Speech"
        },
        {
          "REFERENCES": "T. Toda,\n“Customer Satisfaction Estimation\nin Contact Center Calls",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": ""
        },
        {
          "REFERENCES": "",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": "Emotion Recognition with Dual-Sequence LSTM Architecture,” in Proc."
        },
        {
          "REFERENCES": "Based on a Hierarchical Multi-Task Model,” IEEE/ACM Transactions",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": ""
        },
        {
          "REFERENCES": "",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": "IEEE International Conference on Acoustics, Speech and Signal Pro-"
        },
        {
          "REFERENCES": "on Audio, Speech, and Language Processing (TASLPRO), pp. 715–728,",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": ""
        },
        {
          "REFERENCES": "",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": "cessing (ICASSP), 2020, pp. 6474–6478."
        },
        {
          "REFERENCES": "2020.",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": ""
        },
        {
          "REFERENCES": "",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": "[21] M. A. Jalal, R. Milner, T. Hain, and R. K. Moore, “Removing Bias with"
        },
        {
          "REFERENCES": "[2]\nJ. C. Acosta,\n“Using Emotion to Gain Rapport\nin a Spoken Dialog",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": ""
        },
        {
          "REFERENCES": "",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": "Residual Mixture of Multi-view Attention for Speech Emotion Recog-"
        },
        {
          "REFERENCES": "System,”\nin Proc. North American Chapter",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": ""
        },
        {
          "REFERENCES": "",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": "the International Speech Communication\nnition,” in Proc. Conference of"
        },
        {
          "REFERENCES": "Computational Linguistics (NAACL), 2009, p. 49–54.",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": ""
        },
        {
          "REFERENCES": "",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": "Association (INTERSPEECH), 2020, pp. 4084–4088."
        },
        {
          "REFERENCES": "[3] R. J. Larsen and E. Diener, “Affect\nintensity as an individual difference",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": ""
        },
        {
          "REFERENCES": "characteristic: A review,” Journal of Research in personality, pp. 1–39,",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": "[22] B. Schuller, R. M¨uller, M. Lang, and G. Rigoll, “Speaker\nIndependent"
        },
        {
          "REFERENCES": "1987.",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": "Emotion Recognition\nby\nEarly\nFusion\nof Acoustic\nand\nLinguistic"
        },
        {
          "REFERENCES": "",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": "Features Within Ensembles,” 2005."
        },
        {
          "REFERENCES": "[4]\nJ. A. Russell, “Is There Universal Recognition of Emotion from Facial",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": ""
        },
        {
          "REFERENCES": "Expression? A Review of\nthe Cross-Cultural Studies,” Psychological",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": "[23] B. Schuller, S. Reiter, R. Muller, M. Al-Hames, M. Lang, and G. Rigoll,"
        },
        {
          "REFERENCES": "bulletin, p. 102, 1994.",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": "“Speaker\nIndependent Speech Emotion Recognition by Ensemble Clas-"
        },
        {
          "REFERENCES": "",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": "IEEE International Conference on Multimedia and\nsification,” in Proc."
        },
        {
          "REFERENCES": "[5]\nJ. J. Gross, L. L. Carstensen, M. Pasupathi, J. Tsai, C. G¨otestam Skorpen,",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": ""
        },
        {
          "REFERENCES": "",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": "Expo (ICME), 2005, pp. 864–867."
        },
        {
          "REFERENCES": "and A. Y. Hsu,\n“Emotion\nand Aging: Experience, Expression,\nand",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": ""
        },
        {
          "REFERENCES": "Control.” Psychology and aging, p. 590, 1997.",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": "[24] M.\nSidorov,\nS. Ultes,\nand A.\nSchmitt,\n“Emotions\nare A Personal"
        },
        {
          "REFERENCES": "",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": "Thing: Towards Speaker-Adaptive Emotion Recognition,” in Proc. IEEE"
        },
        {
          "REFERENCES": "[6]\nT. M. Chaplin and A. Aldao, “Gender Differences in Emotion Expression",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": ""
        },
        {
          "REFERENCES": "",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": "International Conference on Acoustics, Speech and Signal Processing"
        },
        {
          "REFERENCES": "in Children: a Meta-Analytic Review,” Psychological bulletin, p. 735,",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": ""
        },
        {
          "REFERENCES": "",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": "(ICASSP), 2014, pp. 4803–4807."
        },
        {
          "REFERENCES": "2013.",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": ""
        },
        {
          "REFERENCES": "",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": "[25]\nZ.-Q. Wang and I. Tashev, “Learning Utterance-Level Representations"
        },
        {
          "REFERENCES": "[7] R. A. Sherman, J. F. Rauthmann, N. A. Brown, D. G. Serfass, and A. B.",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": ""
        },
        {
          "REFERENCES": "",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": "for Speech Emotion and Age/Gender Recognition Using Deep Neural"
        },
        {
          "REFERENCES": "Jones, “The Independent Effects of Personality and Situations on Real-",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": ""
        },
        {
          "REFERENCES": "",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": "Networks,” in Proc. IEEE International Conference on Acoustics, Speech"
        },
        {
          "REFERENCES": "time Expressions of Behavior and Emotion,” Journal of personality and",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": ""
        },
        {
          "REFERENCES": "",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": "and Signal Processing (ICASSP), 2017, pp. 5150–5154."
        },
        {
          "REFERENCES": "social psychology, p. 872, 2015.",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": ""
        },
        {
          "REFERENCES": "",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": "[26]\nL. Zhang, L. Wang,\nJ. Dang, L. Guo,\nand Q. Yu,\n“Gender-aware"
        },
        {
          "REFERENCES": "[8] C. Lu, Y. Zong, W. Zheng, Y. Li, C. Tang, and B. W. Schuller, “Domain",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": ""
        },
        {
          "REFERENCES": "",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": "International\nCNN-BLSTM for\nspeech emotion recognition,” in Proc."
        },
        {
          "REFERENCES": "Invariant Feature Learning\nfor Speaker-Independent Speech Emotion",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": ""
        },
        {
          "REFERENCES": "",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": "Conference on Artificial Neural Networks (ICANN), 2018, pp. 782–790."
        },
        {
          "REFERENCES": "Recognition,” IEEE/ACM Transactions on Audio, Speech, and Language",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": ""
        },
        {
          "REFERENCES": "Processing (TASLPRO), pp. 2217–2230, 2022.",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": "et\n[27] Y. Li, T. Zhao, T. Kawahara\nal.,\n“Improved End-to-End Speech"
        },
        {
          "REFERENCES": "",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": "Emotion Recognition using Self Attention Mechanism and Multitask"
        },
        {
          "REFERENCES": "[9] G. R. Doddington, W. Liggett, A. F. Martin, M. A. Przybocki,\nand",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": ""
        },
        {
          "REFERENCES": "",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": "the International Speech Communi-\nLearning,” in Proc. Conference of"
        },
        {
          "REFERENCES": "D. A. Reynolds, “SHEEP, GOATS, LAMBS and WOLVES: a statistical",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": ""
        },
        {
          "REFERENCES": "",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": "cation Association (INTERSPEECH), 2019, pp. 2803–2807."
        },
        {
          "REFERENCES": "analysis of speaker performance in the NIST 1998 speaker\nrecognition",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": ""
        },
        {
          "REFERENCES": "International Conference\non\nSpoken Language\nevaluation,”\nin Proc.",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": "[28] C. Le Moine, N. Obin,\nand A. Roebel,\n“Speaker Attentive Speech"
        },
        {
          "REFERENCES": "Processing (ICSLP), 1998, pp. 1351–1354.",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": "the International Speech\nEmotion Recognition,” in Proc. Conference of"
        },
        {
          "REFERENCES": "[10] W. Fan, X. Xu, B. Cai, and X. Xing, “ISNet: Individual Standardization",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": "Communication Association (INTERSPEECH), 2021, pp. 2866–2870."
        },
        {
          "REFERENCES": "Network for Speech Emotion Recognition,” IEEE/ACM Transactions on",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": "[29] Y.\nFang, X. Xing, Z. Chu, Y. Du,\nand X. Xu,\n“Individual-Aware"
        },
        {
          "REFERENCES": "Audio, Speech, and Language Processing (TASLPRO), pp. 1803–1814,",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": "Attention Modulation for Unseen Speaker Emotion Recognition,” IEEE"
        },
        {
          "REFERENCES": "2022.",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": "Transactions on Affective Computing, pp. 1–14, 2024."
        },
        {
          "REFERENCES": "[11] A. Triantafyllopoulos, S. Liu, and B. W. Schuller, “Deep speaker con-",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": "[30]\nI. Gat, H. Aronowitz, W. Zhu, E. Morais,\nand R. Hoory,\n“Speaker"
        },
        {
          "REFERENCES": "IEEE International\nditioning for speech emotion recognition,” in Proc.",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": "Normalization\nfor Self-Supervised Speech Emotion Recognition,”\nin"
        },
        {
          "REFERENCES": "Conference on Multimedia and Expo (ICME), 2021, pp. 1–6.",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": "Proc.\nIEEE International Conference on Acoustics, Speech and Signal"
        },
        {
          "REFERENCES": "[12] A. Triantafyllopoulos, M. Song, Z. Yang, X. Jing, and B. W. Schuller,",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": "Processing (ICASSP), 2022, pp. 7342–7346."
        },
        {
          "REFERENCES": "“Exploring speaker enrolment for few-shot personalisation in emotional",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": "[31]\nT. Changli, Y. Wenyi, S. Guangzhi, C. Xianzhao, T. Tian, L. Wei,"
        },
        {
          "REFERENCES": "vocalisation prediction,” arXiv preprint arXiv:2206.06680, 2022.",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": "L. Lu, M. Zejun, and Z. Chao, “SALMONN: Towards Generic Hearing"
        },
        {
          "REFERENCES": "[13] A. Triantafyllopoulos and B. Schuller, “Enrolment-based personalisation",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": "Abilities for Large Language Models,” arXiv preprint arXiv:2310.13289,"
        },
        {
          "REFERENCES": "for\nimproving individual-level\nfairness in speech emotion recognition,”",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": "2023."
        },
        {
          "REFERENCES": "the International Speech Communication Asso-\nin Proc. Conference of",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": "[32]\nZ. Kong, A. Goel, R. Badlani, W. Ping, R. Valle,\nand B. Catanzaro,"
        },
        {
          "REFERENCES": "ciation (INTERSPEECH), 2024, pp. 3729–3733.",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": "“Audio Flamingo: A Novel Audio Language Model with Few-Shot"
        },
        {
          "REFERENCES": "[14]\nT. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal,",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": "Learning and Dialogue Abilities,” in Proc. International Conference on"
        },
        {
          "REFERENCES": "A. Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": "Machine Learning (ICML), 2024, pp. 25 125–25 148."
        },
        {
          "REFERENCES": "Voss, G. Krueger, T. Henighan, R. Child, A. Ramesh, D. Ziegler,",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": "[33] Y. Chu, J. Xu, Q. Yang, H. Wei, X. Wei, Z. Guo, Y. Leng, Y. Lv, J. He,"
        },
        {
          "REFERENCES": "J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray,",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": "J. Lin, C. Zhou, and J. Zhou, “Qwen2-Audio Technical Report,” arXiv"
        },
        {
          "REFERENCES": "B. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever,",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": "preprint arXiv:2407.10759, 2024."
        },
        {
          "REFERENCES": "and D. Amodei, “Language Models are Few-Shot Learners,” in Proc.",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": "[34]\nZ. Chen, H. Huang, A. Andrusenko, O. Hrinchuk, K. C. Puvvada, J. Li,"
        },
        {
          "REFERENCES": "Advances\nin Neural\nInformation Processing Systems\n(NeurIPS), 2020,",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": "S. Ghosh,\nJ. Balam,\nand B. Ginsburg,\n“SALM: Speech-Augmented"
        },
        {
          "REFERENCES": "pp. 1877–1901.",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": "Language Model with\nIn-Context Learning\nfor\nSpeech Recognition"
        },
        {
          "REFERENCES": "[15]\nZ. Zhao, X. Zhu, X. Wang, S. Wang, X. Geng, W. Tian, and L. Xie,",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": "and Translation,” in Proc. IEEE International Conference on Acoustics,"
        },
        {
          "REFERENCES": "“Steering\nLanguage Model\nto\nStable\nSpeech\nEmotion Recognition",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": "Speech and Signal Processing (ICASSP), 2024, pp. 13 521–13 525."
        },
        {
          "REFERENCES": "arXiv\npreprint\nvia Contextual\nPerception\nand Chain\nof\nThought,”",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": "[35]\nS. Wang, C.-H. Yang,\nJ. Wu,\nand C. Zhang,\n“Can Whisper Perform"
        },
        {
          "REFERENCES": "arXiv:2502.18186, 2025.",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": "IEEE International Con-\nSpeech-Based In-Context Learning?” in Proc."
        },
        {
          "REFERENCES": "[16] Y. Xu, H. Chen,\nJ. Yu, Q. Huang,\nZ. Wu,\nS.-X.\nZhang, G.\nLi,",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": "ference on Acoustics, Speech and Signal Processing (ICASSP), 2024,"
        },
        {
          "REFERENCES": "Y\n. Luo, and R. Gu, “SECap: Speech Emotion Captioning with Large",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": "pp. 13 421–13 425."
        },
        {
          "REFERENCES": "Language Model,” in Proc. Association for the Advancement of Artificial",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": "[36] N. Roll, C. Graham, Y. Tatsumi, K. T. Nguyen, M. Sumner, and D. Ju-"
        },
        {
          "REFERENCES": "Intelligence (AAAI), 2024, pp. 19 323–19 331.",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": "rafsky,\n“In-Context Learning Boosts Speech Recognition via Human-"
        },
        {
          "REFERENCES": "[17]\nS. Min, M.\nLewis,\nL.\nZettlemoyer,\nand H. Hajishirzi,\n“MetaICL:",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": "like Adaptation to Speakers\nand Language Varieties,” arXiv preprint"
        },
        {
          "REFERENCES": "the\nLearning to learn in context,” in Proc. North American Chapter of",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": "arXiv:2505.14887, 2025."
        },
        {
          "REFERENCES": "Association for Computational Linguistics\n(NAACL), 2022, pp. 2791–",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": "[37]\nJ. Pan,\nJ. Wu, Y. Gaur, S. Sivasankaran, Z. Chen, S. Liu,\nand J. Li,"
        },
        {
          "REFERENCES": "2809.",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": "“COSMIC: Data Efficient\nInstruction-tuning\nFor\nSpeech\nIn-Context"
        },
        {
          "REFERENCES": "[18]\nJ. Wagner, A. Triantafyllopoulos, H. Wierstorf, M. Schmitt, F. Burkhardt,",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": "the\nInternational Speech Commu-\nLearning,”\nin Proc. Conference of"
        },
        {
          "REFERENCES": "F. Eyben, and B. W. Schuller, “Dawn of the Transformer Era in Speech",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": "nication Association (INTERSPEECH), 2024, pp. 4164–4168."
        },
        {
          "REFERENCES": "Emotion Recognition: Closing the Valence Gap,” IEEE Transactions on",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": "[38] K.-W. Chang, H. Ming-Hao, S.-W. Li, and H.-y. Lee, “Exploring In-"
        },
        {
          "REFERENCES": "Pattern Analysis and Machine Intelligence, pp. 10 745–10 759, 2023.",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": "Context Learning\nof Textless\nSpeech Language Model\nfor\nSpeech"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "REFERENCES": "",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": "tion Using Audio and Text,” in Proc. IEEE Spoken Language Technology"
        },
        {
          "REFERENCES": "",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": "Workshop (SLT), 2018, pp. 112–118."
        },
        {
          "REFERENCES": "[1] A. Ando, R. Masumura, H. Kamiyama, S. Kobashikawa, Y. Aono, and",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": ""
        },
        {
          "REFERENCES": "",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": "[20]\nJ. Wang, M. Xue, R. Culhane, E. Diao, J. Ding, and V. Tarokh, “Speech"
        },
        {
          "REFERENCES": "T. Toda,\n“Customer Satisfaction Estimation\nin Contact Center Calls",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": ""
        },
        {
          "REFERENCES": "",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": "Emotion Recognition with Dual-Sequence LSTM Architecture,” in Proc."
        },
        {
          "REFERENCES": "Based on a Hierarchical Multi-Task Model,” IEEE/ACM Transactions",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": ""
        },
        {
          "REFERENCES": "",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": "IEEE International Conference on Acoustics, Speech and Signal Pro-"
        },
        {
          "REFERENCES": "on Audio, Speech, and Language Processing (TASLPRO), pp. 715–728,",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": ""
        },
        {
          "REFERENCES": "",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": "cessing (ICASSP), 2020, pp. 6474–6478."
        },
        {
          "REFERENCES": "2020.",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": ""
        },
        {
          "REFERENCES": "",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": "[21] M. A. Jalal, R. Milner, T. Hain, and R. K. Moore, “Removing Bias with"
        },
        {
          "REFERENCES": "[2]\nJ. C. Acosta,\n“Using Emotion to Gain Rapport\nin a Spoken Dialog",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": ""
        },
        {
          "REFERENCES": "",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": "Residual Mixture of Multi-view Attention for Speech Emotion Recog-"
        },
        {
          "REFERENCES": "System,”\nin Proc. North American Chapter",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": ""
        },
        {
          "REFERENCES": "",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": "the International Speech Communication\nnition,” in Proc. Conference of"
        },
        {
          "REFERENCES": "Computational Linguistics (NAACL), 2009, p. 49–54.",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": ""
        },
        {
          "REFERENCES": "",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": "Association (INTERSPEECH), 2020, pp. 4084–4088."
        },
        {
          "REFERENCES": "[3] R. J. Larsen and E. Diener, “Affect\nintensity as an individual difference",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": ""
        },
        {
          "REFERENCES": "characteristic: A review,” Journal of Research in personality, pp. 1–39,",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": "[22] B. Schuller, R. M¨uller, M. Lang, and G. Rigoll, “Speaker\nIndependent"
        },
        {
          "REFERENCES": "1987.",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": "Emotion Recognition\nby\nEarly\nFusion\nof Acoustic\nand\nLinguistic"
        },
        {
          "REFERENCES": "",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": "Features Within Ensembles,” 2005."
        },
        {
          "REFERENCES": "[4]\nJ. A. Russell, “Is There Universal Recognition of Emotion from Facial",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": ""
        },
        {
          "REFERENCES": "Expression? A Review of\nthe Cross-Cultural Studies,” Psychological",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": "[23] B. Schuller, S. Reiter, R. Muller, M. Al-Hames, M. Lang, and G. Rigoll,"
        },
        {
          "REFERENCES": "bulletin, p. 102, 1994.",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": "“Speaker\nIndependent Speech Emotion Recognition by Ensemble Clas-"
        },
        {
          "REFERENCES": "",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": "IEEE International Conference on Multimedia and\nsification,” in Proc."
        },
        {
          "REFERENCES": "[5]\nJ. J. Gross, L. L. Carstensen, M. Pasupathi, J. Tsai, C. G¨otestam Skorpen,",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": ""
        },
        {
          "REFERENCES": "",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": "Expo (ICME), 2005, pp. 864–867."
        },
        {
          "REFERENCES": "and A. Y. Hsu,\n“Emotion\nand Aging: Experience, Expression,\nand",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": ""
        },
        {
          "REFERENCES": "Control.” Psychology and aging, p. 590, 1997.",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": "[24] M.\nSidorov,\nS. Ultes,\nand A.\nSchmitt,\n“Emotions\nare A Personal"
        },
        {
          "REFERENCES": "",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": "Thing: Towards Speaker-Adaptive Emotion Recognition,” in Proc. IEEE"
        },
        {
          "REFERENCES": "[6]\nT. M. Chaplin and A. Aldao, “Gender Differences in Emotion Expression",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": ""
        },
        {
          "REFERENCES": "",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": "International Conference on Acoustics, Speech and Signal Processing"
        },
        {
          "REFERENCES": "in Children: a Meta-Analytic Review,” Psychological bulletin, p. 735,",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": ""
        },
        {
          "REFERENCES": "",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": "(ICASSP), 2014, pp. 4803–4807."
        },
        {
          "REFERENCES": "2013.",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": ""
        },
        {
          "REFERENCES": "",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": "[25]\nZ.-Q. Wang and I. Tashev, “Learning Utterance-Level Representations"
        },
        {
          "REFERENCES": "[7] R. A. Sherman, J. F. Rauthmann, N. A. Brown, D. G. Serfass, and A. B.",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": ""
        },
        {
          "REFERENCES": "",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": "for Speech Emotion and Age/Gender Recognition Using Deep Neural"
        },
        {
          "REFERENCES": "Jones, “The Independent Effects of Personality and Situations on Real-",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": ""
        },
        {
          "REFERENCES": "",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": "Networks,” in Proc. IEEE International Conference on Acoustics, Speech"
        },
        {
          "REFERENCES": "time Expressions of Behavior and Emotion,” Journal of personality and",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": ""
        },
        {
          "REFERENCES": "",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": "and Signal Processing (ICASSP), 2017, pp. 5150–5154."
        },
        {
          "REFERENCES": "social psychology, p. 872, 2015.",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": ""
        },
        {
          "REFERENCES": "",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": "[26]\nL. Zhang, L. Wang,\nJ. Dang, L. Guo,\nand Q. Yu,\n“Gender-aware"
        },
        {
          "REFERENCES": "[8] C. Lu, Y. Zong, W. Zheng, Y. Li, C. Tang, and B. W. Schuller, “Domain",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": ""
        },
        {
          "REFERENCES": "",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": "International\nCNN-BLSTM for\nspeech emotion recognition,” in Proc."
        },
        {
          "REFERENCES": "Invariant Feature Learning\nfor Speaker-Independent Speech Emotion",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": ""
        },
        {
          "REFERENCES": "",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": "Conference on Artificial Neural Networks (ICANN), 2018, pp. 782–790."
        },
        {
          "REFERENCES": "Recognition,” IEEE/ACM Transactions on Audio, Speech, and Language",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": ""
        },
        {
          "REFERENCES": "Processing (TASLPRO), pp. 2217–2230, 2022.",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": "et\n[27] Y. Li, T. Zhao, T. Kawahara\nal.,\n“Improved End-to-End Speech"
        },
        {
          "REFERENCES": "",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": "Emotion Recognition using Self Attention Mechanism and Multitask"
        },
        {
          "REFERENCES": "[9] G. R. Doddington, W. Liggett, A. F. Martin, M. A. Przybocki,\nand",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": ""
        },
        {
          "REFERENCES": "",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": "the International Speech Communi-\nLearning,” in Proc. Conference of"
        },
        {
          "REFERENCES": "D. A. Reynolds, “SHEEP, GOATS, LAMBS and WOLVES: a statistical",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": ""
        },
        {
          "REFERENCES": "",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": "cation Association (INTERSPEECH), 2019, pp. 2803–2807."
        },
        {
          "REFERENCES": "analysis of speaker performance in the NIST 1998 speaker\nrecognition",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": ""
        },
        {
          "REFERENCES": "International Conference\non\nSpoken Language\nevaluation,”\nin Proc.",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": "[28] C. Le Moine, N. Obin,\nand A. Roebel,\n“Speaker Attentive Speech"
        },
        {
          "REFERENCES": "Processing (ICSLP), 1998, pp. 1351–1354.",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": "the International Speech\nEmotion Recognition,” in Proc. Conference of"
        },
        {
          "REFERENCES": "[10] W. Fan, X. Xu, B. Cai, and X. Xing, “ISNet: Individual Standardization",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": "Communication Association (INTERSPEECH), 2021, pp. 2866–2870."
        },
        {
          "REFERENCES": "Network for Speech Emotion Recognition,” IEEE/ACM Transactions on",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": "[29] Y.\nFang, X. Xing, Z. Chu, Y. Du,\nand X. Xu,\n“Individual-Aware"
        },
        {
          "REFERENCES": "Audio, Speech, and Language Processing (TASLPRO), pp. 1803–1814,",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": "Attention Modulation for Unseen Speaker Emotion Recognition,” IEEE"
        },
        {
          "REFERENCES": "2022.",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": "Transactions on Affective Computing, pp. 1–14, 2024."
        },
        {
          "REFERENCES": "[11] A. Triantafyllopoulos, S. Liu, and B. W. Schuller, “Deep speaker con-",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": "[30]\nI. Gat, H. Aronowitz, W. Zhu, E. Morais,\nand R. Hoory,\n“Speaker"
        },
        {
          "REFERENCES": "IEEE International\nditioning for speech emotion recognition,” in Proc.",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": "Normalization\nfor Self-Supervised Speech Emotion Recognition,”\nin"
        },
        {
          "REFERENCES": "Conference on Multimedia and Expo (ICME), 2021, pp. 1–6.",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": "Proc.\nIEEE International Conference on Acoustics, Speech and Signal"
        },
        {
          "REFERENCES": "[12] A. Triantafyllopoulos, M. Song, Z. Yang, X. Jing, and B. W. Schuller,",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": "Processing (ICASSP), 2022, pp. 7342–7346."
        },
        {
          "REFERENCES": "“Exploring speaker enrolment for few-shot personalisation in emotional",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": "[31]\nT. Changli, Y. Wenyi, S. Guangzhi, C. Xianzhao, T. Tian, L. Wei,"
        },
        {
          "REFERENCES": "vocalisation prediction,” arXiv preprint arXiv:2206.06680, 2022.",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": "L. Lu, M. Zejun, and Z. Chao, “SALMONN: Towards Generic Hearing"
        },
        {
          "REFERENCES": "[13] A. Triantafyllopoulos and B. Schuller, “Enrolment-based personalisation",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": "Abilities for Large Language Models,” arXiv preprint arXiv:2310.13289,"
        },
        {
          "REFERENCES": "for\nimproving individual-level\nfairness in speech emotion recognition,”",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": "2023."
        },
        {
          "REFERENCES": "the International Speech Communication Asso-\nin Proc. Conference of",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": "[32]\nZ. Kong, A. Goel, R. Badlani, W. Ping, R. Valle,\nand B. Catanzaro,"
        },
        {
          "REFERENCES": "ciation (INTERSPEECH), 2024, pp. 3729–3733.",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": "“Audio Flamingo: A Novel Audio Language Model with Few-Shot"
        },
        {
          "REFERENCES": "[14]\nT. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal,",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": "Learning and Dialogue Abilities,” in Proc. International Conference on"
        },
        {
          "REFERENCES": "A. Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": "Machine Learning (ICML), 2024, pp. 25 125–25 148."
        },
        {
          "REFERENCES": "Voss, G. Krueger, T. Henighan, R. Child, A. Ramesh, D. Ziegler,",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": "[33] Y. Chu, J. Xu, Q. Yang, H. Wei, X. Wei, Z. Guo, Y. Leng, Y. Lv, J. He,"
        },
        {
          "REFERENCES": "J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray,",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": "J. Lin, C. Zhou, and J. Zhou, “Qwen2-Audio Technical Report,” arXiv"
        },
        {
          "REFERENCES": "B. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever,",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": "preprint arXiv:2407.10759, 2024."
        },
        {
          "REFERENCES": "and D. Amodei, “Language Models are Few-Shot Learners,” in Proc.",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": "[34]\nZ. Chen, H. Huang, A. Andrusenko, O. Hrinchuk, K. C. Puvvada, J. Li,"
        },
        {
          "REFERENCES": "Advances\nin Neural\nInformation Processing Systems\n(NeurIPS), 2020,",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": "S. Ghosh,\nJ. Balam,\nand B. Ginsburg,\n“SALM: Speech-Augmented"
        },
        {
          "REFERENCES": "pp. 1877–1901.",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": "Language Model with\nIn-Context Learning\nfor\nSpeech Recognition"
        },
        {
          "REFERENCES": "[15]\nZ. Zhao, X. Zhu, X. Wang, S. Wang, X. Geng, W. Tian, and L. Xie,",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": "and Translation,” in Proc. IEEE International Conference on Acoustics,"
        },
        {
          "REFERENCES": "“Steering\nLanguage Model\nto\nStable\nSpeech\nEmotion Recognition",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": "Speech and Signal Processing (ICASSP), 2024, pp. 13 521–13 525."
        },
        {
          "REFERENCES": "arXiv\npreprint\nvia Contextual\nPerception\nand Chain\nof\nThought,”",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": "[35]\nS. Wang, C.-H. Yang,\nJ. Wu,\nand C. Zhang,\n“Can Whisper Perform"
        },
        {
          "REFERENCES": "arXiv:2502.18186, 2025.",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": "IEEE International Con-\nSpeech-Based In-Context Learning?” in Proc."
        },
        {
          "REFERENCES": "[16] Y. Xu, H. Chen,\nJ. Yu, Q. Huang,\nZ. Wu,\nS.-X.\nZhang, G.\nLi,",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": "ference on Acoustics, Speech and Signal Processing (ICASSP), 2024,"
        },
        {
          "REFERENCES": "Y\n. Luo, and R. Gu, “SECap: Speech Emotion Captioning with Large",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": "pp. 13 421–13 425."
        },
        {
          "REFERENCES": "Language Model,” in Proc. Association for the Advancement of Artificial",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": "[36] N. Roll, C. Graham, Y. Tatsumi, K. T. Nguyen, M. Sumner, and D. Ju-"
        },
        {
          "REFERENCES": "Intelligence (AAAI), 2024, pp. 19 323–19 331.",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": "rafsky,\n“In-Context Learning Boosts Speech Recognition via Human-"
        },
        {
          "REFERENCES": "[17]\nS. Min, M.\nLewis,\nL.\nZettlemoyer,\nand H. Hajishirzi,\n“MetaICL:",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": "like Adaptation to Speakers\nand Language Varieties,” arXiv preprint"
        },
        {
          "REFERENCES": "the\nLearning to learn in context,” in Proc. North American Chapter of",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": "arXiv:2505.14887, 2025."
        },
        {
          "REFERENCES": "Association for Computational Linguistics\n(NAACL), 2022, pp. 2791–",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": "[37]\nJ. Pan,\nJ. Wu, Y. Gaur, S. Sivasankaran, Z. Chen, S. Liu,\nand J. Li,"
        },
        {
          "REFERENCES": "2809.",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": "“COSMIC: Data Efficient\nInstruction-tuning\nFor\nSpeech\nIn-Context"
        },
        {
          "REFERENCES": "[18]\nJ. Wagner, A. Triantafyllopoulos, H. Wierstorf, M. Schmitt, F. Burkhardt,",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": "the\nInternational Speech Commu-\nLearning,”\nin Proc. Conference of"
        },
        {
          "REFERENCES": "F. Eyben, and B. W. Schuller, “Dawn of the Transformer Era in Speech",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": "nication Association (INTERSPEECH), 2024, pp. 4164–4168."
        },
        {
          "REFERENCES": "Emotion Recognition: Closing the Valence Gap,” IEEE Transactions on",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": "[38] K.-W. Chang, H. Ming-Hao, S.-W. Li, and H.-y. Lee, “Exploring In-"
        },
        {
          "REFERENCES": "Pattern Analysis and Machine Intelligence, pp. 10 745–10 759, 2023.",
          "[19]\nS. Yoon, S. Byun, and K. Jung, “Multimodal Speech Emotion Recogni-": "Context Learning\nof Textless\nSpeech Language Model\nfor\nSpeech"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "the International Speech\nClassification Tasks,” in Proc. Conference of": "Communication Association (INTERSPEECH), 2024, pp. 4139–4143."
        },
        {
          "the International Speech\nClassification Tasks,” in Proc. Conference of": "[39] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,"
        },
        {
          "the International Speech\nClassification Tasks,” in Proc. Conference of": "Ł. Kaiser,\nand I. Polosukhin,\n“Attention is All You Need,”\nin Proc."
        },
        {
          "the International Speech\nClassification Tasks,” in Proc. Conference of": "Advances\nin neural\ninformation processing systems\n(NIPS), 2017, pp."
        },
        {
          "the International Speech\nClassification Tasks,” in Proc. Conference of": "5998–6008."
        },
        {
          "the International Speech\nClassification Tasks,” in Proc. Conference of": "[40]\nJ. Wei, M. Bosma, V. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du,"
        },
        {
          "the International Speech\nClassification Tasks,” in Proc. Conference of": "A. M. Dai, and Q. V. Le, “Finetuned Language Models are Zero-Shot"
        },
        {
          "the International Speech\nClassification Tasks,” in Proc. Conference of": "International Conference on Learning Representa-\nLearners,” in Proc."
        },
        {
          "the International Speech\nClassification Tasks,” in Proc. Conference of": "tions (ICLR), 2022."
        },
        {
          "the International Speech\nClassification Tasks,” in Proc. Conference of": "[41]\nJ. Li, D. Li, S. Savarese, and S. Hoi, “BLIP-2: Bootstrapping Language-"
        },
        {
          "the International Speech\nClassification Tasks,” in Proc. Conference of": "Image Pre-training with Frozen Image Encoders and Large Language"
        },
        {
          "the International Speech\nClassification Tasks,” in Proc. Conference of": "Proc.\nInternational Conference\non Machine\nLearning\nModels,”\nin"
        },
        {
          "the International Speech\nClassification Tasks,” in Proc. Conference of": "(ICML), 2023, pp. 19 730–19 742."
        },
        {
          "the International Speech\nClassification Tasks,” in Proc. Conference of": "[42] C. Busso, M. Bulut, C.-C. Lee, A. Kazemzadeh, E. Mower, S. Kim, J. N."
        },
        {
          "the International Speech\nClassification Tasks,” in Proc. Conference of": "Chang, S. Lee, and S. S. Narayanan, “IEMOCAP: Interactive emotional"
        },
        {
          "the International Speech\nClassification Tasks,” in Proc. Conference of": "and\ndyadic motion\ncapture\ndatabase,”\nin Proc. Language Resources"
        },
        {
          "the International Speech\nClassification Tasks,” in Proc. Conference of": "Evaluation Conference (LREC), 2008, pp. 335–359."
        },
        {
          "the International Speech\nClassification Tasks,” in Proc. Conference of": "[43]\nF. Burkhardt, A. Paeschke, M. Rolfes, W. F. Sendlmeier, B. Weiss"
        },
        {
          "the International Speech\nClassification Tasks,” in Proc. Conference of": "et al., “A database of German emotional speech,” in Proc. Conference of"
        },
        {
          "the International Speech\nClassification Tasks,” in Proc. Conference of": "the International Speech Communication Association (INTERSPEECH),"
        },
        {
          "the International Speech\nClassification Tasks,” in Proc. Conference of": "2005, pp. 1517–1520."
        },
        {
          "the International Speech\nClassification Tasks,” in Proc. Conference of": "[44]\nS. Haq, P.\nJ.\nJackson, and J. Edge, “Speaker-Dependent Audio-Visual"
        },
        {
          "the International Speech\nClassification Tasks,” in Proc. Conference of": "International Conference on Auditory-\nEmotion Recognition,” in Proc."
        },
        {
          "the International Speech\nClassification Tasks,” in Proc. Conference of": "Visual Speech Processing (AVSP), vol. 2009, 2009, pp. 53–58."
        },
        {
          "the International Speech\nClassification Tasks,” in Proc. Conference of": "[45] Y. Tay, M. Dehghani, V. Q. Tran, X. Garcia,\nJ. Wei, X. Wang, H. W."
        },
        {
          "the International Speech\nClassification Tasks,” in Proc. Conference of": "Chung, D. Bahri, T. Schuster, S. Zheng et al., “Ul2: Unifying language"
        },
        {
          "the International Speech\nClassification Tasks,” in Proc. Conference of": "International Conference\non Learning\nlearning\nparadigms,”\nin Proc."
        },
        {
          "the International Speech\nClassification Tasks,” in Proc. Conference of": "Representations (ICLR), 2023."
        }
      ],
      "page": 8
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Customer Satisfaction Estimation in Contact Center Calls Based on a Hierarchical Multi-Task Model",
      "authors": [
        "A Ando",
        "R Masumura",
        "H Kamiyama",
        "S Kobashikawa",
        "Y Aono",
        "T Toda"
      ],
      "year": "2020",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "2",
      "title": "Using Emotion to Gain Rapport in a Spoken Dialog System",
      "authors": [
        "J Acosta"
      ],
      "year": "2009",
      "venue": "Proc. North American Chapter"
    },
    {
      "citation_id": "3",
      "title": "Affect intensity as an individual difference characteristic: A review",
      "authors": [
        "R Larsen",
        "E Diener"
      ],
      "year": "1987",
      "venue": "Journal of Research in personality"
    },
    {
      "citation_id": "4",
      "title": "Is There Universal Recognition of Emotion from Facial Expression? A Review of the Cross-Cultural Studies",
      "authors": [
        "J Russell"
      ],
      "year": "1994",
      "venue": "Psychological bulletin"
    },
    {
      "citation_id": "5",
      "title": "Emotion and Aging: Experience, Expression, and Control",
      "authors": [
        "J Gross",
        "L Carstensen",
        "M Pasupathi",
        "J Tsai",
        "C Götestam Skorpen",
        "A Hsu"
      ],
      "year": "1997",
      "venue": "Psychology and aging"
    },
    {
      "citation_id": "6",
      "title": "Gender Differences in Emotion Expression in Children: a Meta-Analytic Review",
      "authors": [
        "T Chaplin",
        "A Aldao"
      ],
      "year": "2013",
      "venue": "Psychological bulletin"
    },
    {
      "citation_id": "7",
      "title": "The Independent Effects of Personality and Situations on Realtime Expressions of Behavior and Emotion",
      "authors": [
        "R Sherman",
        "J Rauthmann",
        "N Brown",
        "D Serfass",
        "A Jones"
      ],
      "year": "2015",
      "venue": "Journal of personality and social psychology"
    },
    {
      "citation_id": "8",
      "title": "Domain Invariant Feature Learning for Speaker-Independent Speech Emotion Recognition",
      "authors": [
        "C Lu",
        "Y Zong",
        "W Zheng",
        "Y Li",
        "C Tang",
        "B Schuller"
      ],
      "year": "2022",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "9",
      "title": "SHEEP, GOATS, LAMBS and WOLVES: a statistical analysis of speaker performance in the NIST 1998 speaker recognition evaluation",
      "authors": [
        "G Doddington",
        "W Liggett",
        "A Martin",
        "M Przybocki",
        "D Reynolds"
      ],
      "year": "1998",
      "venue": "Proc. International Conference on Spoken Language Processing"
    },
    {
      "citation_id": "10",
      "title": "ISNet: Individual Standardization Network for Speech Emotion Recognition",
      "authors": [
        "W Fan",
        "X Xu",
        "B Cai",
        "X Xing"
      ],
      "year": "2022",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "11",
      "title": "Deep speaker conditioning for speech emotion recognition",
      "authors": [
        "A Triantafyllopoulos",
        "S Liu",
        "B Schuller"
      ],
      "year": "2021",
      "venue": "Proc. IEEE International Conference on Multimedia and Expo (ICME)"
    },
    {
      "citation_id": "12",
      "title": "Exploring speaker enrolment for few-shot personalisation in emotional vocalisation prediction",
      "authors": [
        "A Triantafyllopoulos",
        "M Song",
        "Z Yang",
        "X Jing",
        "B Schuller"
      ],
      "year": "2022",
      "venue": "Exploring speaker enrolment for few-shot personalisation in emotional vocalisation prediction",
      "arxiv": "arXiv:2206.06680"
    },
    {
      "citation_id": "13",
      "title": "Enrolment-based personalisation for improving individual-level fairness in speech emotion recognition",
      "authors": [
        "A Triantafyllopoulos",
        "B Schuller"
      ],
      "year": "2024",
      "venue": "Proc. Conference of the International Speech Communication Association (INTERSPEECH)"
    },
    {
      "citation_id": "14",
      "title": "Language Models are Few-Shot Learners",
      "authors": [
        "T Brown",
        "B Mann",
        "N Ryder",
        "M Subbiah",
        "J Kaplan",
        "P Dhariwal",
        "A Neelakantan",
        "P Shyam",
        "G Sastry",
        "A Askell",
        "S Agarwal",
        "A Herbert-Voss",
        "G Krueger",
        "T Henighan",
        "R Child",
        "A Ramesh",
        "D Ziegler",
        "J Wu",
        "C Winter",
        "C Hesse",
        "M Chen",
        "E Sigler",
        "M Litwin",
        "S Gray",
        "B Chess",
        "J Clark",
        "C Berner",
        "S Mccandlish",
        "A Radford",
        "I Sutskever",
        "D Amodei"
      ],
      "year": "2020",
      "venue": "Proc. Advances in Neural Information Processing Systems (NeurIPS)"
    },
    {
      "citation_id": "15",
      "title": "Steering Language Model to Stable Speech Emotion Recognition via Contextual Perception and Chain of Thought",
      "authors": [
        "Z Zhao",
        "X Zhu",
        "X Wang",
        "S Wang",
        "X Geng",
        "W Tian",
        "L Xie"
      ],
      "year": "2025",
      "venue": "Steering Language Model to Stable Speech Emotion Recognition via Contextual Perception and Chain of Thought",
      "arxiv": "arXiv:2502.18186"
    },
    {
      "citation_id": "16",
      "title": "SECap: Speech Emotion Captioning with Large Language Model",
      "authors": [
        "Y Xu",
        "H Chen",
        "J Yu",
        "Q Huang",
        "Z Wu",
        "S.-X Zhang",
        "G Li",
        "Y Luo",
        "R Gu"
      ],
      "year": "2024",
      "venue": "Proc. Association for the Advancement of Artificial Intelligence (AAAI)"
    },
    {
      "citation_id": "17",
      "title": "MetaICL: Learning to learn in context",
      "authors": [
        "S Min",
        "M Lewis",
        "L Zettlemoyer",
        "H Hajishirzi"
      ],
      "year": "2022",
      "venue": "Proc. North American Chapter"
    },
    {
      "citation_id": "18",
      "title": "Dawn of the Transformer Era in Speech Emotion Recognition: Closing the Valence Gap",
      "authors": [
        "J Wagner",
        "A Triantafyllopoulos",
        "H Wierstorf",
        "M Schmitt",
        "F Burkhardt",
        "F Eyben",
        "B Schuller"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "19",
      "title": "Multimodal Speech Emotion Recognition Using Audio and Text",
      "authors": [
        "S Yoon",
        "S Byun",
        "K Jung"
      ],
      "year": "2018",
      "venue": "Proc. IEEE Spoken Language Technology Workshop (SLT)"
    },
    {
      "citation_id": "20",
      "title": "Speech Emotion Recognition with Dual-Sequence LSTM Architecture",
      "authors": [
        "J Wang",
        "M Xue",
        "R Culhane",
        "E Diao",
        "J Ding",
        "V Tarokh"
      ],
      "year": "2020",
      "venue": "Proc. IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "21",
      "title": "Removing Bias with Residual Mixture of Multi-view Attention for Speech Emotion Recognition",
      "authors": [
        "M Jalal",
        "R Milner",
        "T Hain",
        "R Moore"
      ],
      "year": "2020",
      "venue": "Proc. Conference of the International Speech Communication Association (INTERSPEECH)"
    },
    {
      "citation_id": "22",
      "title": "Speaker Independent Emotion Recognition by Early Fusion of Acoustic and Linguistic Features Within Ensembles",
      "authors": [
        "B Schuller",
        "R Müller",
        "M Lang",
        "G Rigoll"
      ],
      "year": "2005",
      "venue": "Speaker Independent Emotion Recognition by Early Fusion of Acoustic and Linguistic Features Within Ensembles"
    },
    {
      "citation_id": "23",
      "title": "Speaker Independent Speech Emotion Recognition by Ensemble Classification",
      "authors": [
        "B Schuller",
        "S Reiter",
        "R Muller",
        "M Al-Hames",
        "M Lang",
        "G Rigoll"
      ],
      "year": "2005",
      "venue": "Proc. IEEE International Conference on Multimedia and Expo (ICME)"
    },
    {
      "citation_id": "24",
      "title": "Emotions are A Personal Thing: Towards Speaker-Adaptive Emotion Recognition",
      "authors": [
        "M Sidorov",
        "S Ultes",
        "A Schmitt"
      ],
      "year": "2014",
      "venue": "Proc. IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "25",
      "title": "Learning Utterance-Level Representations for Speech Emotion and Age/Gender Recognition Using Deep Neural Networks",
      "authors": [
        "Z.-Q Wang",
        "I Tashev"
      ],
      "year": "2017",
      "venue": "Proc. IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "26",
      "title": "Gender-aware CNN-BLSTM for speech emotion recognition",
      "authors": [
        "L Zhang",
        "L Wang",
        "J Dang",
        "L Guo",
        "Q Yu"
      ],
      "year": "2018",
      "venue": "Proc. International Conference on Artificial Neural Networks (ICANN)"
    },
    {
      "citation_id": "27",
      "title": "Improved End-to-End Speech Emotion Recognition using Self Attention Mechanism and Multitask Learning",
      "authors": [
        "Y Li",
        "T Zhao",
        "T Kawahara"
      ],
      "year": "2019",
      "venue": "Proc. Conference of the International Speech Communication Association (INTERSPEECH)"
    },
    {
      "citation_id": "28",
      "title": "Speaker Attentive Speech Emotion Recognition",
      "authors": [
        "C Le Moine",
        "N Obin",
        "A Roebel"
      ],
      "year": "2021",
      "venue": "Proc. Conference of the International Speech Communication Association (INTERSPEECH)"
    },
    {
      "citation_id": "29",
      "title": "Individual-Aware Attention Modulation for Unseen Speaker Emotion Recognition",
      "authors": [
        "Y Fang",
        "X Xing",
        "Z Chu",
        "Y Du",
        "X Xu"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "30",
      "title": "Speaker Normalization for Self-Supervised Speech Emotion Recognition",
      "authors": [
        "I Gat",
        "H Aronowitz",
        "W Zhu",
        "E Morais",
        "R Hoory"
      ],
      "year": "2022",
      "venue": "Proc. IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "31",
      "title": "SALMONN: Towards Generic Hearing Abilities for Large Language Models",
      "authors": [
        "T Changli",
        "Y Wenyi",
        "S Guangzhi",
        "C Xianzhao",
        "T Tian",
        "L Wei",
        "L Lu",
        "M Zejun",
        "Z Chao"
      ],
      "year": "2023",
      "venue": "SALMONN: Towards Generic Hearing Abilities for Large Language Models",
      "arxiv": "arXiv:2310.13289"
    },
    {
      "citation_id": "32",
      "title": "Audio Flamingo: A Novel Audio Language Model with Few-Shot Learning and Dialogue Abilities",
      "authors": [
        "Z Kong",
        "A Goel",
        "R Badlani",
        "W Ping",
        "R Valle",
        "B Catanzaro"
      ],
      "year": "2024",
      "venue": "Proc. International Conference on Machine Learning (ICML)"
    },
    {
      "citation_id": "33",
      "title": "",
      "authors": [
        "Y Chu",
        "J Xu",
        "Q Yang",
        "H Wei",
        "X Wei",
        "Z Guo",
        "Y Leng",
        "Y Lv",
        "J He",
        "J Lin",
        "C Zhou",
        "J Zhou"
      ],
      "year": "2024",
      "venue": "",
      "arxiv": "arXiv:2407.10759"
    },
    {
      "citation_id": "34",
      "title": "SALM: Speech-Augmented Language Model with In-Context Learning for Speech Recognition and Translation",
      "authors": [
        "Z Chen",
        "H Huang",
        "A Andrusenko",
        "O Hrinchuk",
        "K Puvvada",
        "J Li",
        "S Ghosh",
        "J Balam",
        "B Ginsburg"
      ],
      "year": "2024",
      "venue": "Proc. IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "35",
      "title": "Can Whisper Perform Speech-Based In-Context Learning?",
      "authors": [
        "S Wang",
        "C.-H Yang",
        "J Wu",
        "C Zhang"
      ],
      "year": "2024",
      "venue": "Proc. IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "36",
      "title": "Context Learning Boosts Speech Recognition via Humanlike Adaptation to Speakers and Language Varieties",
      "authors": [
        "N Roll",
        "C Graham",
        "Y Tatsumi",
        "K Nguyen",
        "M Sumner",
        "D Jurafsky"
      ],
      "year": "2025",
      "venue": "Context Learning Boosts Speech Recognition via Humanlike Adaptation to Speakers and Language Varieties",
      "arxiv": "arXiv:2505.14887"
    },
    {
      "citation_id": "37",
      "title": "COSMIC: Data Efficient Instruction-tuning For Speech In-Context Learning",
      "authors": [
        "J Pan",
        "J Wu",
        "Y Gaur",
        "S Sivasankaran",
        "Z Chen",
        "S Liu",
        "J Li"
      ],
      "year": "2024",
      "venue": "Proc. Conference of the International Speech Communication Association (INTERSPEECH)"
    },
    {
      "citation_id": "38",
      "title": "Exploring In-Context Learning of Textless Speech Language Model for Speech Classification Tasks",
      "authors": [
        "K.-W Chang",
        "H Ming-Hao",
        "S.-W Li",
        "H.-Y Lee"
      ],
      "year": "2024",
      "venue": "Proc. Conference of the International Speech Communication Association (INTERSPEECH)"
    },
    {
      "citation_id": "39",
      "title": "Attention is All You Need",
      "authors": [
        "A Vaswani",
        "N Shazeer",
        "N Parmar",
        "J Uszkoreit",
        "L Jones",
        "A Gomez",
        "Ł Kaiser",
        "I Polosukhin"
      ],
      "year": "2017",
      "venue": "Proc. Advances in neural information processing systems (NIPS)"
    },
    {
      "citation_id": "40",
      "title": "Finetuned Language Models are Zero-Shot Learners",
      "authors": [
        "J Wei",
        "M Bosma",
        "V Zhao",
        "K Guu",
        "A Yu",
        "B Lester",
        "N Du",
        "A Dai",
        "Q Le"
      ],
      "venue": "Proc. International Conference on Learning Representations (ICLR)"
    },
    {
      "citation_id": "41",
      "title": "BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models",
      "authors": [
        "J Li",
        "D Li",
        "S Savarese",
        "S Hoi"
      ],
      "year": "2023",
      "venue": "Proc. International Conference on Machine Learning (ICML)"
    },
    {
      "citation_id": "42",
      "title": "IEMOCAP: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Proc. Language Resources and Evaluation Conference (LREC)"
    },
    {
      "citation_id": "43",
      "title": "A database of German emotional speech",
      "authors": [
        "F Burkhardt",
        "A Paeschke",
        "M Rolfes",
        "W Sendlmeier",
        "B Weiss"
      ],
      "year": "2005",
      "venue": "Proc. Conference of the International Speech Communication Association (INTERSPEECH)"
    },
    {
      "citation_id": "44",
      "title": "Speaker-Dependent Audio-Visual Emotion Recognition",
      "authors": [
        "S Haq",
        "P Jackson",
        "J Edge"
      ],
      "year": "2009",
      "venue": "Proc. International Conference on Auditory-Visual Speech Processing (AVSP)"
    },
    {
      "citation_id": "45",
      "title": "Ul2: Unifying language learning paradigms",
      "authors": [
        "Y Tay",
        "M Dehghani",
        "V Tran",
        "X Garcia",
        "J Wei",
        "X Wang",
        "H Chung",
        "D Bahri",
        "T Schuster",
        "S Zheng"
      ],
      "venue": "Proc. International Conference on Learning Representations (ICLR)"
    }
  ]
}