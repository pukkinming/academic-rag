{
  "paper_id": "2508.12227v2",
  "title": "Arabic Multimodal Machine Learning: Datasets, Applications, Approaches, And Challenges",
  "published": "2025-08-17T03:59:27Z",
  "authors": [
    "Abdelhamid Haouhat",
    "Slimane Bellaouar",
    "Attia Nehar",
    "Hadda Cherroun",
    "Ahmed Abdelali"
  ],
  "keywords": [
    "Multimodal Machine Learning",
    "Modality Representation",
    "Deep Learning",
    "Cross-Modal",
    "Representation Learning",
    "Multimodal Fusion",
    "Multimedia",
    "Arabic Multimodal Learning"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Multimodal Machine Learning (MML) aims to integrate and analyze information from diverse modalities, such as text, audio, and visuals, enabling machines to address complex tasks like sentiment analysis, emotion recognition, and multimedia retrieval. Recently, Arabic MML has reached a certain level of maturity in its foundational development, making it time to conduct a comprehensive survey. This paper explores Arabic MML by categorizing efforts through a novel taxonomy and analyzing existing research. Our taxonomy organizes these efforts into four key topics: datasets, applications, approaches, and challenges. By providing a structured overview, this survey offers insights into the current state of Arabic MML, highlighting areas that have not been investigated and critical research gaps. Researchers will be empowered to build upon the identified opportunities and address challenges to advance the field.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Humans naturally perceive and interpret the world through a combination of senses such as sight, hearing, and touch.\n\nWithout the integration of these multiple senses, understanding the environment can become challenging. For example, sarcasm cannot be fully identified through spoken words alone; it requires additional context, such as speech tone and facial expressions. This demonstrates the essential role of multiple modalities in human understanding and expression.\n\nInspired by this natural multisensory perception, Multimodal Machine Learning (MML) integrates information from diverse modalities to address complex tasks such as sentiment analysis, emotion recognition, and multimedia retrieval.\n\nThese modalities are represented in various ways  [53] . For the textual modality, word or sentence embeddings are The remainder of the paper is organized as follows. In Section 2, we present the methodology and introduce our proposed taxonomy for Arabic MML efforts. Section 3 provides a comprehensive review of existing MML studies. Section 4 offers an in-depth discussion of the findings and their implications. Finally, Section 5 concludes the paper with insights and directions for future research.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Methodology And Taxonomy",
      "text": "We have reviewed 45 papers and studies on various aspects of Arabic Multimodal Machine Learning research. We have developed a taxonomy based on several classification criteria to present these studies effectively. The primary focus is to identify the key patterns and trends within the field.\n\nThe first criterion focuses on dataset-centric contributions, which play a key role in advancing research in this area.\n\nThese studies are described based on their intended applications (e.g., sentiment analysis, emotion recognition) and the types of modalities they involve (e.g., text, speech, image, video). Furthermore, we classify Arabic MML efforts based on their target applications, including but not limited to Sentiment Analysis, Emotion Recognition, Propaganda Detection, Speech Recognition, Remote Sensing, Image and Video Retrieval, Video Classification, and other specific applications such as multimodal interaction systems and accessibility tools.\n\nAdditionally, we analyze the techniques employed in Arabic multimodal research, spanning traditional methods, classical machine learning, and advanced deep learning architectures, including CNNs, RNNs, Transformers, and multimodal-specific models.\n\nFinally, under the last classification criterion, we explore the challenges these studies addressed, with a particular focus on modality representation. This includes examining how Arabic multimodal data are represented and processed, as well as the techniques used to extract meaningful features from modalities such as text, speech, images, and video. We also explore fusion techniques, emphasizing methods for integrating information across multiple modalities, including early fusion, late fusion, cross-attention mechanisms, and hybrid approaches. In addition, we discuss the translation of input modalities into other output modalities and the alignment between relevant input modalities to ensure seamless integration. This taxonomy is the foundation for our detailed review, offering a systematic framework to describe, analyze, and compare Arabic multimodal research efforts. It provides a comprehensive understanding of the advancements, limitations, and potential future directions in Arabic multimodal research.\n\nFigure  1  provides a visual representation of our proposed taxonomy, illustrating the categorized research efforts in Arabic multimodal studies across target applications, core challenges, machine learning approaches, and dataset-centric contributions.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Review Of Mml Work",
      "text": "In this section, we give a full study of our taxonomy, structured around four key topics: data-centric aspects, applications, approaches, and challenges. Each of these key topics will be examined in detail, offering a holistic grasp of the taxonomy and its ramifications. By delving into these dimensions, we seek to present a clear and methodical framework that not only organizes existing information but also offers potential for future study and innovation.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Dataset Centric",
      "text": "Data is a fundamental pillar of the machine learning era, serving as the foundation for learning. This importance has led researchers to focus on the creation of datasets for diverse machine learning tasks in the Arabic context. One of the earliest efforts in building multimodal Arabic datasets was undertaken by Ashraf et al.  [2]  in 2010 with the introduction of their Multimodal Arabic Corpus (MMAC). This later dataset was specifically designed to support Optical Character Recognition (OCR) development and linguistic research. The corpus includes a variety of Arabic text images, encompassing real-world, computer-generated, and noisy examples. To enhance the dataset's flexibility and richness, the authors incorporated not only full word sequences but also partial word sequences, categorized as 'Piece of Words' and 'Naked Piece of Words. ' Subsequently, Samar et al.  [16]  created the Audio-Visual Speech Recognition (AVAS) corpus to address audio-visual recognition tasks. Introduced in 2013, this dataset incorporates variations in illumination and head pose, making it suitable for advancing multimodal recognition systems. The AVAS corpus is designed to support the development of audio-visual speech recognition (AVSR) systems, audio-only speech recognition, lip-reading (visual-only) systems, and face recognition under diverse conditions. Its variability in illumination and pose ensures the dataset's utility for enhancing the performance and robustness of speech recognition systems in dynamic, real-world environments.\n\nSentiment Analysis and Emotion Recognition are among the most popular and captivating topics in research. In 2019, Alqarafi et al.  [11]  introduced the Arabic Multi-Modal Dataset (AMMD) for sentiment analysis, comprising 830 segments, each containing visual and language modalities along with sentiment labels. The dataset is labeled with four classes: objective, subjective, positive, and negative. The authors utilized Support Vector Machine (SVM)-based classification models to perform sentiment analysis on this dataset. Expanding on sentiment analysis datasets, Haouhat et al.  [32]  developed the Arabic Multimodal Sentiment Analysis (AMSA) dataset, a larger and more comprehensive resource. AMSA integrates all three available modalities commonly found in multimedia videos: visual, auditory, and textual transcriptions. It includes 60 long videos and 540 segments, providing a rich dataset for multimodal sentiment analysis.\n\nFocusing on emotion recognition, AbuShaqra et al.  [3]  conducted a study in 2019 and introduced the AVANEmo dataset for Audio-Visual Natural Emotions. This dataset comprises 3000 clips containing both audio and video data, annotated with six basic emotional labels: Happy, Sad, Angry, Surprise, Disgust, and Neutral. The authors also evaluated the performance of their emotion recognition models, achieving notable accuracy using both audio and visual modalities.\n\nIn 2021, Bellagha et Zrigui  [20]  developed a multimodal dataset specifically designed for recognizing speaker roles in Arabic TV broadcasts. The dataset, created using data from the Multi-genre Broadcast MGB-2 Challenge, comprises approximately 205 hours of audio data with corresponding transcriptions, and 8, 112 pairs of annotated speaker and speech turns. It is annotated for both text and audio modalities to identify speaker roles during TV show interactions.\n\nThe authors evaluated the dataset using baseline classifiers for both audio and text, highlighting the significance of integrating both modalities to accurately identify speaker roles in television shows. The dataset is further augmented by the AraFACT dataset  [62]  containing 1, 726 samples.\n\nIn 2022, Albalawi et al.  [9]  introduced a carefully curated dataset of 2, 299 samples, along with a novel approach to rumor detection. The dataset combines two modalities -text and images-extracted from social media posts, providing a valuable resource for advancing research in this domain.\n\nIn 2023, Luqman Hamza  [46]  introduced a novel Dataset and Benchmark, ArabSign, for Arabic Sign Language (ArSL).\n\nArabSign contains 9,335 samples recorded from six signers, amounting to approximately 10 hours of recorded sentences.\n\nEach sentence has an average length of 3.1 signs. The recordings were captured using a Kinect V2 camera, providing three types of information-color, depth, and skeleton joint points-all recorded simultaneously for each sentence. Samah Abbas et al.  [1]  later developed a multimodal dataset for ArSL, specifically in religious speech, such as Friday Sermons. The dataset comprises 1, 950 audio signals, 131 corresponding text phrases, and 262 annotated ArSL videos.\n\nAnnotations made using the ELAN tool based on gloss representation. Focusing on formal speech, this dataset provides a valuable resource for multimodal learning models that can bridge the gap between the audio, text, and sign language modalities. To evaluate annotation consistency, the authors employed the Jaccard Similarity metric between gloss representations across two different signers, achieving a similarity score of 85%.  [29]  is a novel benchmark developed by researchers at MBZUAI 1  to evaluate the Visual questionanswering (VQA) capabilities of Arabic Language Multimodal Models (LMMs). This benchmark comprises over 29,000 questions across eight diverse domains and 38 sub-domains, meticulously curated by native Arabic speakers to ensure cultural and linguistic relevance. CAMEL-Bench assesses LMM performance on a variety of tasks, including OCR, medical imaging analysis, remote sensing interpretation, and understanding culturally specific content. Table  1  provides an overview of key multimodal Arabic datasets, highlighting their modalities and applications.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Camel-Bench",
      "text": "",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Applications",
      "text": "Arabic MML frameworks have explored a wide variety of applications, from the most popular, such as sentiment analysis and emotion recognition, to more complex ones, such as computer vision image captioning and human-robot interactions. Hereafter, we categorize the reviewed studies based on their respective application domains.\n\n3.2.1 Sentiment Analysis. Researchers have shown that analyzing a speaker's tone alongside his facial expressions enables models to understand better emotional states, such as distinguishing sarcasm from sincerity. For example, a smiling face paired with a seemingly angry tone might indicate sarcasm, a nuance that single-modality systems often fail to capture  [18, 32, 55, 63, 69] . This integration reflects the human ability to infer sentiments more accurately by combining visual and auditory cues.\n\nIn 2020, Al-Azani and El-Alfy  [5]   processed by an LSTM decoder to produce output tokens. To evaluate their approach, the authors conducted experiments on two widely used datasets, Flickr8k and MSCOCO, comparing their method against existing models. Using BLEU-1\n\nto BLEU-4 metrics as the primary evaluation criteria, their method demonstrated improved performance for Arabic image captioning tasks. Recently, Emran and Dan  [28]  introduced a novel system for generating cross-lingual image descriptions, focusing on producing semantically coherent and culturally aligned translations from English to Arabic.\n\nTheir approach combines a neural network model with a semantic matching module to ensure both semantic and stylistic alignment. The system employs a multimodal semantic matching module, where input images are encoded using a pre-trained ResNet followed by fully connected layers (FCL). The resulting image features are then decoded into Arabic captions using an LSTM. To ensure cross-modal coherence, the model calculates semantic similarity between the mapped representations of the generated sentence and the image within a common knowledge space. This process is further enhanced by a language evaluation module that incorporates rewards based on cross-modal semantic matching and language evaluation. For evaluation, the authors introduced a dedicated dataset, AraImg2K, consisting of 2, 000 images, each annotated with five captions. While the proposed system does not introduce an efficient fusion method for integrating image-text modalities, it effectively addresses the challenges of modality alignment and the translation of image data into textual description. Muhy Za'ter and Talafha  [70]    Finally, the system maps existing visual materials to semantic conceptual graphs to retrieve illustrative images that match the textual input. Salah and Aljaam.  [60]  introduced a modular system designed to support children with learning difficulties by transforming user-provided textual stories into corresponding descriptive visual elements, such as images and animations. The system extracts keywords from the story text, selects relevant images, and generates captions for those images using pre-trained CNN and LSTM models. It then evaluates the similarity between the input text and the generated image captions, ranging sentences based on similarity scores to identify the most relevant matches. While this work effectively employs statistical methods to process textual inputs, there is a need for further automation of the workflow to reduce reliance on human interpretation, thereby enhancing scalability and efficiency. Moselhy et al.  [49]  developed a retrieval and recommendation system for Arabic content that utilizes multimodal data to enhance search accuracy and user experience. The system combines textual and visual modalities, focusing on aligning features extracted from both using advanced fusion techniques. These techniques generated robust cross-modal embeddings, significantly enhancing retrieval precision. This work underscores the critical role of sophisticated fusion methods in aligning multimodal data, ensuring seamless retrieval and recommendation processes across text and image modalities.\n\nFollowing the rise of Transformer  [66]  as a prominent technique in various research fields, AlRahhal et al.  [13, 59]  explored their application in image retrieval, proposing a multilanguage image-text model for remote sensing image retrieval. Their approach employs transformer-based encoders to extract modality features from image-text pairs and uses global average pooling to create a common similarity space. During training, the model optimizes contrastive classification losses for both text-to-image and image-to-text pairs, ensuring effective learning from both modalities.\n\nIn a related study  [13] , the authors addressed cross-modal retrieval challenges in remote sensing applications by using separate encoders for textual and visual features. Their system supports both Arabic and English captions in the text modality. They investigated two paradigms for language alignment: one where Arabic and English are learned independently, and another where both languages are learned jointly. Experiments on two cross-modal datasets showcased the promising capabilities of their approach.\n\n3.2.5 Speech Recognition. Since speech is the most effective and common form of human communication in daily life, academics have been concentrating on Automatic Speech Recognition (ASR) tasks over time, particularly in low-resource languages such as Arabic.\n\nAlrashoudi et al.  [12]  present a novel framework for mispronunciation detection and diagnosis (MDD) for nonnative Arabic speakers. As the first stage of their framework, they use an ASR model by fine-tuning Wav2Vec2.0  [38] ,\n\nHubert  [38] , and Whisper  [57]  to autoregressively predict phoneme-based transcriptions from the raw data, which includes audio and textual data. They are creating their L2-KSU dataset, which includes 4086 utterances and six hours and six minutes of audio recordings associated with its transcriptions.\n\nAnother application where speech is essential is speech recognition for robot control. Researchers enable robots to process speech as a natural medium for receiving human instructions. Sagheer  [7]  proposed an Audio-Visual Speech Recognition (AVSR) system composed of three main components: face-mouth feature detection, automatic user identification, and a visual speech recognition module. The face-mouth feature detection uses the Viola-Jones detection module  [67] , which relies on integral image concepts to reduce computational costs via attentional cascade classifiers. For user authentication, a machine learning-based automatic identification system is implemented to address security concerns.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Language Recognition.",
      "text": "In the context of language identification, Alharbi  [10]  introduced a multimodal framework combining audio and textual data for detecting Saudi dialects. The textual transcription is encoded using a BERT-based encoder, while the audio input is processed through a CNN-based acoustic model to extract relevant features. The authors utilize a self-attention mechanism to capture semantic relationships between the two modalities. Additionally, they design a neural network to align textual information with corresponding audio segments, ensuring effective representation and synchronization between modalities. This work adopts an early fusion approach to combine audio and textual embeddings, effectively bridging the modalities. The alignment mechanism between textual and audio representations is particularly noteworthy, as it enhances the framework's ability to handle the inherent temporal and semantic differences between the modalities.\n\n3.2.9 Speaker Role Recognition. Mehra et al.  [48]  introduced a novel strategy for spoken word recognition in Arabic, particularly under the constraints of a multilingual and low-resource setting. Their approach utilized the pre-trained Arabic Large xlsr-Wav2Vec2-53  [31]  transformer model for speech-to-text conversion, processing text in both Buckwalter transliterations and Arabic script. It leveraged phonetic representation through the CMU dictionary 3 and applied a grapheme-to-phoneme model for Buckwalter transliterations  [22] . For Arabic script, stemming and unigram-based conversions are followed by FastText  [39]  word embeddings for vector representation  [24] . These vectors are processed by a three-layer dense model with batch normalization, and the final output is an average of results from both forms. It is important to note that Arabic-based LMMs have benefited from benchmarking efforts aimed at understanding their performances. For instance, CAMEL-Bench  [29]  is an open-sourced multimodal framework that comprises around more than 29K questions that are filtered from a larger pool of samples, where the quality is manually verified by native speakers to ensure reliable model assessment.",
      "page_start": 11,
      "page_end": 12
    },
    {
      "section_name": "Approaches",
      "text": "The progression of multimodal learning techniques has transitioned from traditional probabilistic models to classical machine learning methods and, more recently, to the widespread adoption of deep learning architectures. Each methodological category has introduced distinct strengths and limitations, significantly influencing the design of multimodal datasets and their applications. This section examines the reviewed studies by organizing them into three categories: traditional, classical machine learning, and deep learning-based approaches. We highlight the foundational principles of each category and discuss specific studies that employ these techniques.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Traditional Methods.",
      "text": "Early studies relied on traditional methods because of their simplicity and effectiveness in structuring feature spaces. Rule-based approaches, in particular, use predefined logical rules or conditions-often derived from expert knowledge or heuristics-to make decisions or classify data. To the best of our knowledge, we found only one study that used traditional methods in the Arabic context, which is demonstrated in  [40] . Karkar et al. employ a heuristic mapping process (i.e., basic matching techniques) to retrieve relevant images. Additionally, they utilized cosine similarity measures to improve the retrieval accuracy.\n\nHidden Markov Models (HMMs) emerged as a prominent approach for capturing sequential dependencies by modeling systems as a series of observable events governed by hidden states. Their probabilistic state transitions enabled an effective representation of temporal patterns. For example, in viseme-based speech recognition, HMMs were combined with k-Nearest Neighbors (k-NN) to improve classification accuracy by utilizing hidden state transitions alongside neighborhood-based voting techniques  [7] .\n\nAlthough traditional methods laid a solid groundwork for multimodal research, they were limited by their dependence on manual feature engineering, which required extensive domain expertise. Moreover, these methods often lacked scalability and robustness in dynamic environments, prompting the adoption of classical machine-learning approaches to overcome these challenges.",
      "page_start": 13,
      "page_end": 14
    },
    {
      "section_name": "Classical Machine",
      "text": "Learning Methods. Classical machine learning methods marked a significant step forward by integrating advanced algorithms capable of handling various tasks. These methods leveraged statistical and supervised learning techniques, enabling the integration of structured features from different modalities.\n\nSupport Vector Machines (SVMs) are a popularly used technique that employs supervised learning to find an optimal hyperplane that separates data points into distinct classes. By maximizing the margin between the support vectors, the SVMs provided robust classification capabilities. These models were often combined with other classical methods, such as Logistic Regression and L2 norm regularization, to enhance multimodal data processing, as demonstrated in various studies  [6, 25, 69] .\n\nK-Nearest Neighbors (k-NN), a simple yet effective algorithm, classified data points based on the majority vote of their closest neighbors. When integrated with HMMs, as seen in viseme recognition applications, k-NN contributed to better classification accuracy in multimodal datasets  [7] .\n\nKernel methods, which map data into higher-dimensional spaces using kernel functions, proved highly effective in addressing nonlinear problems by making them linearly separable. Techniques such as kernelized SVMs (KSVMs), which employed linear kernels for efficient binary classification, demonstrated improved performance in multimodal contexts  [5] .\n\nLastly, Multilayer Perceptrons (MLPs) demonstrated their ability to capture nonlinear patterns in multimodal data. These models proved to be effective for tasks that require complex relationships between inputs from different modalities  [4, 21] . However, MLPs faced limitations when dealing with sequential or spatial data, where models such as Recurrent Neural Networks (RNNs) or Convolutional Neural Networks (CNNs) were more adept at capturing complex dependencies.\n\nWhile classical machine learning models expanded the scope of multimodal tasks, their reliance on feature engineering limited adaptability. This paved the way for deep learning architectures that automated feature extraction and supported end-to-end learning, marking a significant evolution in the field. addressed the challenges of sequential data, such as audio or text. LSTMs, in particular, excelled in capturing temporal dependencies by maintaining long-term context, which proved invaluable for tasks like image caption generation  [27, 28, 70] . Similarly, they were instrumental in speech and lip-reading tasks on the AVAS dataset, where their ability to handle variable-length sequences offered a robust framework for modeling temporal patterns.\n\nThe emergence of Transformers revolutionized the way sequential data was processed. By leveraging attention-based mechanisms, transformers efficiently captured long-term dependencies and complex correlations between multimodal inputs. This attention-centric approach was further enhanced through the integration of cross-attention modules, enabling nuanced interaction across modalities. Transformers have been widely adopted for fine-grained cross-modal understanding  [10, 71] .\n\nBuilding on the foundation of transformers, Large Language Models (LLMs) emerged as cutting-edge tools for multimodal research. Some studies employed LLMs as synthetic data generators, enhancing model performance through diverse and high-quality samples  [15, 61] . Additionally, autoregressive training paradigms enabled smoother and more coherent outputs  [58, 66] , especially in text generation tasks  [21] . Unique applications of transformers in Arabic contexts included efficient speech recognition, even under the constraints of multilingual and low-resource settings  [48] .\n\nDeep learning methods have revolutionized the multimodal research landscape, offering tools capable of learning complex representations and capturing intricate relationships across diverse data types. Despite these advancements, challenges persist, including significant computational demands, the scarcity of data in low-resource settings, and the ongoing need to enhance model interpretability. These areas remain at the forefront of active research.\n\n3.3.4 Hybrid Methods. Hybrid models take advantage of the strengths of multiple approaches, combining deep learning architectures with traditional methods to enhance multimodal analysis. For instance, some studies integrate CNNs for spatial feature extraction from images with LSTMs to process temporal dependencies in sequential data  [37] . Others employ Transformer layers to enable cross-modal interactions, such as combining advanced models like SAM and CLIP with LSTMs to tackle diverse multimodal tasks effectively. These combinations showcase the versatility of hybrid models in addressing the challenges posed by different data modalities  [15, 34] .\n\nA notable approach involves pairing deep learning feature extractors, such as Transformers, with traditional statistical methods for multimodal classification. For example, similarity matrices constructed from image-text contrastive learning have been employed to enhance classification accuracy  [8, 13, 59 ]. This combination allows for capturing nuanced relationships between modalities while benefiting from the structured decision-making process inherent in classical statistical methods.\n\nOther hybrid techniques include back-translation for augmenting multimodal datasets  [15] . Furthermore, some studies use frozen pre-trained models for feature extraction, subsequently integrating classical neural layers to adapt these features to task-specific requirements  [14] . Such strategies enable the reuse of large-scale pre-trained models, reducing computational costs while achieving high accuracy on specific tasks.\n\nHybrid methods have proven particularly effective in scenarios with limited resources or where single-model architectures fall short. By integrating diverse methodologies, these approaches push the boundaries of multimodal learning, offering flexible solutions to a range of tasks.",
      "page_start": 13,
      "page_end": 15
    },
    {
      "section_name": "Challenges",
      "text": "Multimodal Machine Learning has made remarkable strides and demonstrated broad applicability. Nevertheless, substantial challenges remain, particularly in Arabic language applications. Key aspects requiring further enhancement include fusion methods, modality translation, alignment strategies, and data representation. The reviewed papers have made progress in addressing some of these issues.",
      "page_start": 15,
      "page_end": 15
    },
    {
      "section_name": "Fusion Methods.",
      "text": "Multimodal learning tasks rely on the effective combination of modalities, such as visual, audio, and textual embeddings, to create a unified representation that drives system performance. In the literature, fusion techniques are classified into three main types: early, late, and hybrid.\n\nEarly fusion unifies the representations of multiple modalities at an initial stage, typically before the prediction phase. For example, features are often combined and fed into dense neural networks with batch normalization layers to ensure stability and efficient training  [48] . This method enables the model to capture interactions between modalities early in the pipeline, potentially improving the coherence of multimodal representations.\n\nEarly fusion employs techniques such as simple vector operations (e.g., concatenation, addition, or weighted vectors  [25] ). Furthermore, more sophisticated methods such as cross-modal fusion, which is based on mechanisms of cross-attention, are available to capture semantic relationships between modalities, facilitating dynamic and adaptive representations  [21] .\n\nIn contrast, late fusion combines predictions or high-level features from each modality after individual processing.\n\nDecision fusion is a prominent example of this approach, where the outcomes of unimodal predictors are integrated at the decision-making stage to improve performance  [4] . Late fusion is advantageous in scenarios where each modality's individual contribution needs to be preserved and analyzed independently.\n\nHybrid fusion blends the benefits of both early and late fusion methods within a unified framework. For example, Group Gated Fusion (GGF) dynamically integrates information from different modalities  [44] , ensuring effective interaction while leveraging the complementary strengths of early and late fusion  [8] . Similarly, hybrid multi-level fusion approaches capture cross-modal dependencies at various levels, enabling nuanced and context-aware representations  [5] .\n\nSome works classified multimodality fusion approaches into more detailed classification, as illustrated by Figure  2 . The latter highlighted two main categories: model-agnostic and model-based approaches  [18] . Model-agnostic approaches do not depend on a specific machine learning model. Instead, they leverage general techniques to integrate multimodal data as we described earlier.\n\nModel-based approaches explicitly incorporate multimodal fusion mechanisms into their architecture. These primary include, first, Kernel-based methods, extend support vector machines (SVMs) by utilizing different kernels for each modality, enabling better fusion of heterogeneous data (e.g., such as Multiple Kernel Learning (MKL)). Secondly, graphical models such as Bayesian networks, which are based on probabilistic frameworks, are especially well-suited for temporal and sequential multimodal tasks. Third, Neural Networks-based methods, which leverage modern deep neural architectures, have proven highly effective for multimodal fusion.\n\nIn general, all these fusion categories are designed to integrate information from all available modalities, even in the presence of missing or noisy inputs. Researchers have addressed these challenges through various approaches, such as imputing missing data using generative models, designing noise-resilient architectures, or employing robust late fusion techniques that weigh modality contributions based on reliability. Furthermore, exploring self-supervised and contrastive learning techniques to improve cross-modal alignment and representation learning significantly enhances fusion robustness in case of incomplete or noisy inputs.",
      "page_start": 15,
      "page_end": 16
    },
    {
      "section_name": "Modality",
      "text": "Translation. Modality translation refers to the process of switching or generating one modality from another. This technique plays a crucial role in bridging gaps between modalities, making it possible to derive enriched insights and enabling a broader spectrum of applications. The process can generally be classified into two categories:\n\nunimodal and multimodal settings.\n\nIn unimodal settings, the model operates on a specific input modality and transforms it into a different modality.\n\nFor example, textual inputs can be processed through an LSTM decoder to produce corresponding output tokens, as demonstrated in tasks like image captioning  [27, 28] . This category is often leveraged in scenarios where the transformation involves single-modality data, such as converting visual input into another structured textual text.\n\nIn contrast, multimodal settings embrace unified designs capable of representing and translating between multiple modalities. These approaches rely on advanced architectures that harmonize diverse modalities, such as textual and visual inputs. Models like CLIP  [56] , LLAMA-3  [30] , and ChatGPT-4  [54]  exemplify this paradigm. For instance, CLIP's visual output matrix has been utilized to match meme images with their corresponding textual descriptions, showcasing the effectiveness of multimodal translation in capturing nuanced relationships  [34] . This approach improves tasks Manuscript submitted to ACM such as image captioning, video analysis, and multimodal sentiment analysis by creating seamless interaction across modalities.\n\nSuch advancements in modality translation empower complex tasks like visual question answering, multimodal dialogue systems, and creative content generation, where the interplay of modalities enhances overall system performance.\n\nTadas et al.  [18]  classify multimodal translation techniques into two approaches: Example-based approaches:\n\nThese rely on similarity functions to identify the closest instance to the input modality within a source space. In essence, this is a retrieval-based system where paired input and target translated samples are used.\n\nGenerative-based approaches: These construct the target modality from scratch, often utilizing encoder-decoder architectures. This method extends language generation techniques to handle not just textual modalities but also multimodal translation tasks. For example, an encoder-decoder model might generate descriptive captions for images by encoding visual features and decoding them into coherent text. Despite significant progress, multimodal machine translation faces challenges, particularly subjectivity in translation, where multiple correct outputs may exist for tasks like image captioning, visual question answering, and others. Addressing this subjectivity remains a critical area for further research.  Researchers have employed diverse mechanisms to achieve alignment, including Neural Network Alignment, Crossmodal Mechanisms, and Graph Neural Network Alignment  [42, 52, 64] .\n\nNeural Network Alignment focuses on synchronizing textual information with corresponding audio segments. This approach addresses temporal mismatches and ensures synchronized multimodal representations. By integrating neural networks for alignment tasks, frameworks can better manage semantic and temporal differences, among the most significant challenges in multimodal tasks  [10] .\n\nCross-attention Mechanisms play a pivotal role in aligning visual and textual modalities. These mechanisms enable dynamic feature interactions, improving the model's ability to identify nuanced relationships between text and image representations. For example, cross-attention modules enhance accuracy by ensuring better alignment and integration of modalities, making them especially effective in tasks such as image captioning, video-text matching, and multimodal sentiment analysis  [21] .\n\nThrough these methods, researchers have significantly improved the alignment between modalities, enabling more accurate and context-aware multimodal systems. Fourier transform features are effective. For instance,  [40]  employs an entity-word matrix composition approach to identify semantic connections using statistical methods. For acoustic modalities, Authors of  [25, 26]  implement MFCC and spectral features to analyze audio data, while other studies explore facial expressions and movements to extract meaningful visual features  [41, 51] .\n\nIn contrast, learned representations leverage advanced machine learning techniques to provide more versatile and adaptive representations. These representations are derived by training unimodal or multimodal models on large datasets, ensuring context awareness and capturing complex, non-linear relationships in the data. This data-driven approach widely uses deep learning techniques such as deep neural networks, convolutional neural networks, recurrent neural networks, and transformers. This approach also enables transfer learning, reusing pre-trained models for related tasks. Researchers frequently use pre-trained models such as AraBERT, MARBERT, ResNet, and VGG to effectively represent data inputs  [27, 28, 40, 70] .",
      "page_start": 16,
      "page_end": 16
    },
    {
      "section_name": "Discussion",
      "text": "In Figure  4 , we illustrate chronologically the Arabic MML surveyed work to highlight the Arabic MML's progress over time.\n\nThe first insights from this overview reveal many:",
      "page_start": 17,
      "page_end": 17
    },
    {
      "section_name": "Datasets Are Prior",
      "text": "Around a quarter of the contributions focused on designing multimodal datasets, highlighting the scarcity of multimodal data available for Arabic AI-based multimodal systems. In fact, among the contributions, 12 out of 45 are dedicated to this area, emphasizing its significance in the field.\n\nThese efforts have also been complemented by developing various datasets to support multimodal research in the Arabic language. These studies are critical, given the lack of comprehensive resources available in comparison to other widely studied languages, such as English.",
      "page_start": 19,
      "page_end": 19
    },
    {
      "section_name": "Popular Targeted Applications",
      "text": "We observed a diverse range of tasks that have been explored. However, sentiment analysis, retrieval systems, and computer vision -video classification and image captioning-are the most commonly targeted tasks. This focus is likely due to their broad applicability in real-world applications.",
      "page_start": 19,
      "page_end": 19
    },
    {
      "section_name": "Mml Challenges Less Addressed",
      "text": "While the initiatives are valuable for establishing a baseline  [17, 34, 48, 69] , they fall short in addressing more complex and deep challenges that are central to advancing MML. In fact, despite these efforts, current research in Arabic MML remains largely intuitive and concentrated on relatively straightforward problems. For instance, Alignment Strategies:\n\naligning modalities such as text, audio, and visuals, especially in the context of semantic and temporal synchronization, is an area where there is a clear lack of robust solutions. Fusion Techniques: The critical task of combining multimodal inputs effectively, whether through early, late, or hybrid fusion, remains underexplored in the Arabic domain. Particularly in cases where data is missing or noisy in a real-world environment. Efficient Architectures: Developing optimized multimodal models, especially tailored for Arabic-specific large language models and transformers, remains a significant challenge that demands further exploration. Addressing these challenges could lead to more scalable and efficient solutions.\n\nThese gaps underscore the need for Arabic MML research to evolve beyond dataset curation and application-focused studies to tackle fundamental technical challenges. Advancements in this area would not only enhance performance on existing tasks but also enable researchers to address more complex real-world problems where multimodal data integration is essential.",
      "page_start": 20,
      "page_end": 20
    },
    {
      "section_name": "Reuse Of Existing Models",
      "text": "Most existing studies primarily rely on pretrained models-originally designed for other languages or general-purpose tasks-such as BERT, ResNet, or CLIP to represent Arabic content. These models are typically fine-tuned or adapted with minimal modifications for Arabic-specific contexts. Subsequently, simple combination methods, such as concatenation fusion or basic alignment mechanisms, are employed to integrate information from multiple modalities and perform downstream tasks.\n\nIn contrast, the Fanar team 5  adopted a comprehensive approach to developing Fanar Star from scratch, starting with data collection and progressing through the training stage. This method ensures a more robust learning process and deeper knowledge acquisition.",
      "page_start": 21,
      "page_end": 21
    },
    {
      "section_name": "Lmms Emergence And Deployment",
      "text": "Research on LMMs has progressed more slowly for Arabic compared to extensively studied languages like English.\n\nNotably, the first Arabic LMMs were introduced only in 2024. This delay can be attributed to the relatively late",
      "page_start": 21,
      "page_end": 21
    },
    {
      "section_name": "Conclusion",
      "text": "The field of Arabic Multimodal Machine Learning (MML) has reached a certain level of maturity, making this survey both timely and impactful. By introducing a novel taxonomy centered on four key axes: data-centric, applications, approaches, and challenges, we have outlined a comprehensive landscape of Arabic MML research. This framework, grounded in Arabic Language Technology, provides a systematic approach to analyzing and advancing work in this domain. Our comprehensive overview of this field serves as a foundational guide for readers to better understand this challenging and rapidly evolving field.\n\nOur analysis reveals a strong emphasis on the construction of multimodal datasets, with sentiment analysis, retrieval systems, and computer vision emerging as dominant applications. The transition from traditional methods to Machine\n\nLearning and Deep Learning techniques demonstrates the field's growing sophistication. However, most existing studies continue to rely on pre-trained models to represent Arabic content. Furthermore, despite recent advancements, core difficulties -such as alignment strategies, fusion methods, and the integration of complex multimodal interactionsremain underdeveloped. Addressing these gaps will require more inventive and resilient solutions to push the boundaries of Arabic MML research.\n\nFrom a scientific perspective, this work highlights the critical importance of addressing the core challenges in MML, notably in attaining effective alignment and fusion between modalities. The proposed taxonomy provides a clear roadmap, helping researchers identify gaps and prioritize efforts-especially in leveraging LMMs and advancing fusion techniques.\n\nOn a practical level, the insights from this survey can inform the development of more efficient and scalable multimodal systems, with substantial potential for applications in healthcare, education, and human-computer interaction, where Arabic multimodal data is becoming increasingly relevant.\n\nLooking ahead, the field must focus on overcoming the persistent challenges of alignment and fusion while exploring the revolutionary potential of LMMs for Arabic-specific tasks. Interdisciplinary collaboration will be essential to address real-world problems and drive innovation. By building on existing and tackling outstanding concerns, Arabic MML can continue to flourish, contributing not only to academic achievements but also to practical solutions that benefit the Arabic-speaking world and beyond.",
      "page_start": 21,
      "page_end": 22
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: provides a visual representation of our proposed taxonomy, illustrating the categorized research efforts in",
      "page": 3
    },
    {
      "caption": "Figure 1: An Overview of the Proposed Taxonomy for Arabic Multimodal Machine Learning Research.",
      "page": 4
    },
    {
      "caption": "Figure 2: An Overview of Fusion Mechanisms in Multimodal Machine Learning.",
      "page": 17
    },
    {
      "caption": "Figure 3: provides an overview of the multimodal translation process, illustrating the different approaches and",
      "page": 17
    },
    {
      "caption": "Figure 3: Overview of Multimodal Translation: This figure illustrates both approaches, where the input can consist of one or multiple",
      "page": 18
    },
    {
      "caption": "Figure 4: , we illustrate chronologically the Arabic MML surveyed work to highlight the Arabic MML’s progress over",
      "page": 19
    },
    {
      "caption": "Figure 4: Chronological Overview of the Surveyed Papers",
      "page": 20
    }
  ],
  "tables": [
    {
      "caption": "Table 1: The table summarizes various Arabic multimodal datasets, detailing their mod(alities), app(lications), and avail(ability). The",
      "page": 6
    },
    {
      "caption": "Table 2: highlights data representation methods, providing references pertinent to techniques used in Arabic multi-",
      "page": 19
    },
    {
      "caption": "Table 2: Overview of data representations in multimodal machine learning. This table categorizes various modalities—Textual, Visual,",
      "page": 19
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Establishing a multimodal dataset for arabic sign language (arsl) production",
      "authors": [
        "Samah Abbas",
        "Dimah Alahmadi",
        "Hassanin Al-Barhamtoshy"
      ],
      "year": "2024",
      "venue": "Journal of King Saud University -Computer and Information Sciences"
    },
    {
      "citation_id": "2",
      "title": "Building a multi-modal arabic corpus (MMAC)",
      "authors": [
        "Ashraf Abdelraouf",
        "Colin Higgins",
        "Tony Pridmore",
        "Mahmoud Khalil"
      ],
      "year": "2010",
      "venue": "International Journal on Document Analysis and Recognition (IJDAR)"
    },
    {
      "citation_id": "3",
      "title": "The audio-visual arabic dataset for natural emotions",
      "authors": [
        "Ftoon Abu Shaqra",
        "Rehab Duwairi",
        "Mahmoud Al-Ayyoub"
      ],
      "year": "2019",
      "venue": "2019 7th International Conference on Future Internet of Things and Cloud (FiCloud)"
    },
    {
      "citation_id": "4",
      "title": "A multi-modal deep learning system for arabic emotion recognition",
      "authors": [
        "Ftoon Abu Shaqra",
        "Rehab Duwairi",
        "Mahmoud Al-Ayyoub"
      ],
      "year": "2022",
      "venue": "Int. J. Speech Technol"
    },
    {
      "citation_id": "5",
      "title": "Enhanced video analytics for sentiment analysis based on fusing textual, auditory and visual information",
      "authors": [
        "Sadam Al",
        "-Azani El-Sayed",
        "M El-Alfy"
      ],
      "year": "2020",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "6",
      "title": "Multimodal arabic emotion recognition using deep learning",
      "authors": [
        "Noora Al",
        "Gerassimos Barlas"
      ],
      "year": "2023",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "7",
      "title": "Multimodal arabic speech recognition for human-robot interaction applications",
      "authors": [
        "Alaa Sagheer"
      ],
      "year": "2015",
      "venue": "Applied Mathematics & Information Sciences"
    },
    {
      "citation_id": "8",
      "title": "A novel deep learning multi-modal sentiment analysis model for english and egyptian arabic dialects using audio and text",
      "authors": [
        "Sohaila Alalem",
        "Mohamed Saad Zaghloul",
        "Osama Badawy"
      ],
      "year": "2023",
      "venue": "2023 24th International Arab Conference on Information Technology (ACIT)"
    },
    {
      "citation_id": "9",
      "title": "Multimodal arabic rumors detection",
      "authors": [
        "M Rasha",
        "Amani Albalawi",
        "Alaa Jamal",
        "Areej Khadidos",
        "Alhothali"
      ],
      "year": "2023",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "10",
      "title": "Mf-saudi: A multimodal framework for bridging the gap between audio and textual data for saudi dialect detection",
      "authors": [
        "Raed Alharbi"
      ],
      "year": "2024",
      "venue": "Journal of King Saud University -Computer and Information Sciences"
    },
    {
      "citation_id": "11",
      "title": "Toward's arabic multi-modal sentiment analysis",
      "authors": [
        "Abdulrahman Alqarafi",
        "Ahsan Adeel",
        "Mandar Gogate",
        "Kia Dashitpour",
        "Amir Hussain",
        "Tariq Durrani"
      ],
      "year": "2019",
      "venue": "Communications, Signal Processing, and Systems"
    },
    {
      "citation_id": "12",
      "title": "Improving mispronunciation detection and diagnosis for non-native learners of the arabic language",
      "authors": [
        "Norah Alrashoudi",
        "Hend Al-Khalifa",
        "Yousef Alotaibi"
      ],
      "year": "2025",
      "venue": "Discover Computing"
    },
    {
      "citation_id": "13",
      "title": "Learning to align arabic and english text to remote sensing images using transformers",
      "authors": [
        "Norah Alsharif",
        "Yakoub Bazi",
        "Mohamad Rahhal"
      ],
      "year": "2022",
      "venue": "2022 IEEE Mediterranean and Middle-East Geoscience and Remote Sensing Symposium (M2GARSS)"
    },
    {
      "citation_id": "14",
      "title": "Dallah: A dialect-aware multimodal large language model for Arabic",
      "authors": [
        "Fakhraddin Alwajih",
        "Gagan Bhatia",
        "Muhammad Abdul-Mageed"
      ],
      "year": "2024",
      "venue": "Proceedings of The Second Arabic Natural Language Processing Conference"
    },
    {
      "citation_id": "15",
      "title": "Peacock: A family of Arabic multimodal large language models and benchmarks",
      "authors": [
        "Fakhraddin Alwajih",
        "El Moatez",
        "Billah Nagoudi",
        "Gagan Bhatia",
        "Abdelrahman Mohamed",
        "Muhammad Abdul-Mageed"
      ],
      "year": "2024",
      "venue": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "16",
      "title": "AVAS: Speech database for multimodal recognition applications",
      "authors": [
        "Samar Antar",
        "Alaa Sagheer",
        "Saleh Aly",
        "Mohamed Tolba"
      ],
      "year": "2013",
      "venue": "13th International Conference on Hybrid Intelligent Systems (HIS 2013)"
    },
    {
      "citation_id": "17",
      "title": "Morphbpe: A morpho-aware tokenizer bridging linguistic complexity for efficient llm training across morphologies",
      "authors": [
        "Ehsaneddin Asgari",
        "Yassine Kheir",
        "Mohammad Ali",
        "Sadraei Javaheri"
      ],
      "year": "2025",
      "venue": "Morphbpe: A morpho-aware tokenizer bridging linguistic complexity for efficient llm training across morphologies"
    },
    {
      "citation_id": "18",
      "title": "Multimodal machine learning: A survey and taxonomy",
      "authors": [
        "Tadas Baltrusaitis",
        "Chaitanya Ahuja",
        "Louis-Philippe Morency"
      ],
      "year": "2019",
      "venue": "IEEE Trans. Pattern Anal. Mach. Intell"
    },
    {
      "citation_id": "19",
      "title": "A systematic literature review on multimodal machine learning: Applications, challenges, gaps and future directions",
      "authors": [
        "Arnab Barua",
        "Mobyen Uddin Ahmed",
        "Shahina Begum"
      ],
      "year": "2023",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "20",
      "title": "Using the MGB-2 challenge data for creating a new multimodal dataset for speaker role recognition in Arabic TV Broadcasts",
      "authors": [
        "Mohamed Lazhar",
        "Mounir Zrigui"
      ],
      "year": "2021",
      "venue": "Proceedings of the 25th International Conference KES2021"
    },
    {
      "citation_id": "21",
      "title": "Qalam: A multimodal LLM for Arabic optical character and handwriting recognition",
      "authors": [
        "Gagan Bhatia",
        "El Moatez",
        "Billah Nagoudi",
        "Fakhraddin Alwajih",
        "Muhammad Abdul-Mageed"
      ],
      "year": "2024",
      "venue": "Proceedings of The Second Arabic Natural Language Processing Conference"
    },
    {
      "citation_id": "22",
      "title": "Joint-sequence models for grapheme-to-phoneme conversion",
      "authors": [
        "Maximilian Bisani",
        "Hermann Ney"
      ],
      "year": "2008",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "23",
      "title": "Essentia: an open-source library for sound and music analysis",
      "authors": [
        "Dmitry Bogdanov",
        "Nicolas Wack",
        "Emilia Gómez",
        "Sankalp Gulati",
        "Perfecto Herrera",
        "Oscar Mayor",
        "Gerard Roma",
        "Justin Salamon",
        "José Zapata",
        "Xavier Serra"
      ],
      "year": "2013",
      "venue": "Proceedings of the 21st ACM International Conference on Multimedia, MM '13"
    },
    {
      "citation_id": "24",
      "title": "Enriching word vectors with subword information",
      "authors": [
        "Piotr Bojanowski",
        "Edouard Grave",
        "Armand Joulin",
        "Tomas Mikolov"
      ],
      "year": "2016",
      "venue": "Enriching word vectors with subword information",
      "arxiv": "arXiv:1607.04606"
    },
    {
      "citation_id": "25",
      "title": "Audio-visual video classification system design: For arabic news domain",
      "authors": [
        "Amal Dandashi",
        "Jihad Aljaam",
        "Sebti Foufou"
      ],
      "year": "2016",
      "venue": "2016 International Conference on Computational Science and Computational Intelligence (CSCI)"
    },
    {
      "citation_id": "26",
      "title": "Video classification methods: Multimodal techniques",
      "authors": [
        "Amal Dandashi",
        "Jihad Mohamad Alja'am"
      ],
      "year": "2018",
      "venue": "Recent Trends in Computer Applications"
    },
    {
      "citation_id": "27",
      "title": "Improved arabic image captioning model using feature concatenation with pre-trained word embedding",
      "authors": [
        "Samar Elbedwehy",
        "T Medhat"
      ],
      "year": "2023",
      "venue": "Neural Computing and Applications"
    },
    {
      "citation_id": "28",
      "title": "Enhancing cross-lingual image description: A multimodal approach for semantic relevance and stylistic alignment",
      "authors": [
        "Dan Wang",
        "Emran Al-Buraihy"
      ],
      "year": "2024",
      "venue": "Computers, Materials & Continua"
    },
    {
      "citation_id": "29",
      "title": "",
      "authors": [
        "Sara Ghaboura",
        "Ahmed Heakl",
        "Omkar Thawakar",
        "Ali Alharthi",
        "Ines Riahi",
        "Abduljalil Saif",
        "Jorma Laaksonen",
        "Fahad Khan",
        "Salman Khan",
        "Rao Anwer"
      ],
      "year": "2024",
      "venue": ""
    },
    {
      "citation_id": "30",
      "title": "",
      "authors": [
        "Aaron Grattafiori",
        "Abhimanyu Dubey",
        "Abhinav Jauhri",
        "Abhinav Pandey",
        "Abhishek Kadian",
        "Ahmad Al-Dahle",
        "Aiesha Letman",
        "Akhil Mathur",
        "Alan Schelten",
        "Alex Vaughan",
        "Amy Yang",
        "Angela Fan",
        "Anirudh Goyal",
        "Anthony Hartshorn",
        "Aobo Yang",
        "Archi Mitra",
        "Archie Sravankumar",
        "Artem Korenev",
        "Arthur Hinsvark",
        "Arun Rao",
        "Aston Zhang",
        "Aurelien Rodriguez",
        "Austen Gregerson",
        "Ava Spataru",
        "Baptiste Roziere",
        "Bethany Biron",
        "Binh Tang",
        "Bobbie Chern",
        "Charlotte Caucheteux",
        "Chaya Nayak",
        "Chloe Bi",
        "Chris Marra",
        "Chris Mcconnell",
        "Christian Keller",
        "Christophe Touret",
        "Chunyang Wu",
        "Corinne Wong",
        "Cristian Canton Ferrer",
        "Cyrus Nikolaidis",
        "Damien Allonsius",
        "Daniel Song",
        "Danielle Pintz",
        "Danny Livshits",
        "Danny Wyatt",
        "David Esiobu",
        "Dhruv Choudhary",
        "Dhruv Mahajan",
        "Diego Garcia-Olano",
        "Diego Perino",
        "Dieuwke Hupkes",
        "Egor Lakomkin",
        "Ehab Albadawy",
        "Elina Lobanova",
        "Emily Dinan",
        "Eric Smith",
        "Filip Radenovic",
        "Francisco Guzmán",
        "Frank Zhang",
        "Gabriel Synnaeve",
        "Gabrielle Lee",
        "Georgia Lewis Anderson",
        "Govind Thattai",
        "Graeme Nail",
        "Gregoire Mialon",
        "Guan Pang",
        "Guillem Cucurell",
        "Hailey Nguyen",
        "Hannah Korevaar",
        "Hu Xu",
        "Hugo Touvron",
        "Iliyan Zarov",
        "Arrieta Imanol",
        "Isabel Ibarra",
        "Ishan Kloumann",
        "Ivan Misra",
        "Jack Evtimov",
        "Jade Zhang",
        "Jaewon Copet",
        "Jan Lee",
        "Jana Geffert",
        "Jason Vranes",
        "Jay Park",
        "Jeet Mahadeokar",
        "Jelmer Shah",
        "Jennifer Van Der Linde",
        "Jenny Billock",
        "Jenya Hong",
        "Jeremy Lee",
        "Jianfeng Fu",
        "Jianyu Chi",
        "Jiawen Huang",
        "Jie Liu",
        "Jiecao Wang",
        "Joanna Yu",
        "Joe Bitton",
        "Jongsoo Spisak",
        "Joseph Park",
        "Joshua Rocca",
        "; Johnstun",
        "Laurens Yeary",
        "Lawrence Van Der Maaten",
        "Liang Chen",
        "Liz Tan",
        "Louis Jenkins",
        "Lovish Martin",
        "Lubo Madaan",
        "Lukas Malo",
        "Lukas Blecher",
        "Luke Landzaat",
        "Madeline De Oliveira",
        "Mahesh Muzzi",
        "Mannat Pasupuleti",
        "Manohar Singh",
        "Marcin Paluri",
        "Maria Kardas",
        "Mathew Tsimpoukelli",
        "Mathieu Oldham",
        "Maya Rita",
        "Melanie Pavlova",
        "Mike Kambadur",
        "Min Lewis",
        "Mitesh Si",
        "Mona Singh",
        "Naman Hassan",
        "Narjes Goyal",
        "Nikolay Torabi",
        "Nikolay Bashlykov",
        "Niladri Bogoychev",
        "Ning Chatterji",
        "Olivier Zhang",
        "Onur Duchenne",
        "Patrick Çelebi",
        "Pengchuan Alrassy",
        "Pengwei Zhang",
        "Petar Li",
        "Peter Vasic",
        "Prajjwal Weng",
        "Pratik Bhargava",
        "Praveen Dubal",
        "Punit Krishnan",
        "Puxin Singh Koura",
        "Qing Xu",
        "Qingxiao He",
        "Ragavan Dong",
        "Raj Srinivasan",
        "Ramon Ganapathy",
        "Ricardo Calderer",
        "Robert Cabral",
        "Roberta Stojnic",
        "Rohan Raileanu",
        "Rohit Maheswari",
        "Rohit Girdhar",
        "Romain Patel",
        "Ronnie Sauvestre",
        "Roshan Polidoro",
        "Ross Sumbaly",
        "Ruan Taylor",
        "Rui Silva",
        "Rui Hou",
        "Saghar Wang",
        "Sahana Hosseini",
        "Sanjay Chennabasappa",
        "Sean Singh",
        "Bell",
        "Sonia Seohyun",
        "Sergey Kim",
        "Shaoliang Edunov",
        "Sharan Nie",
        "Sharath Narang",
        "Sheng Raparthy",
        "Shengye Shen",
        "Shruti Wan",
        "Shun Bhosale",
        "Simon Zhang",
        "Soumya Vandenhende",
        "Spencer Batra",
        "Sten Whitman",
        "Stephane Sootla",
        "Suchin Collot",
        "Sydney Gururangan",
        "Tamar Borodinsky",
        "Tara Herman",
        "Tarek Fowler",
        "Thomas Sheasha",
        "Thomas Georgiou",
        "Tobias Scialom",
        "Todor Speckbacher",
        "Tong Mihaylov",
        "Ujjwal Xiao",
        "Vedanuj Karn",
        "Vibhor Goswami",
        "Vignesh Gupta",
        "Viktor Ramanathan",
        "Vincent Kerkez",
        "Virginie Gonguet",
        "Vish Do",
        "Vítor Vogeti",
        "Vladan Albiero",
        "Weiwei Petrovic",
        "Wenhan Chu",
        "Wenyin Xiong",
        "Whitney Fu",
        "Xavier Meers",
        "Xiaodong Martinet",
        "Xiaofang Wang",
        "Wang",
        "Ellen Xiaoqing",
        "Xide Tan",
        "Xinfeng Xia",
        "Xuchao Xie",
        "Xuewei Jia",
        "Yaelle Wang",
        "Yashesh Goldschlag",
        "Yasmine Gaur",
        "Yi Babaei",
        "Yiwen Wen",
        "Yuchen Song",
        "Yue Zhang",
        "Yuning Li",
        "Zacharie Mao",
        "Zheng Coudert",
        "Zhengxing Yan",
        "Zoe Chen",
        "Aaditya Papakipos",
        "Aayushi Singh",
        "Abha Srivastava",
        "Adam Jain",
        "Adam Kelsey",
        "Adithya Shajnfeld",
        "Adolfo Gangidi",
        "Ahuva Victoria",
        "Ajay Goldstand",
        "Ajay Menon",
        "Alex Sharma",
        "Alexei Boesenberg",
        "Allie Baevski",
        "Amanda Feinstein",
        "Amit Kallet",
        "Amos Sangani",
        "Anam Teo",
        "Andrei Yunus",
        "Andres Lupu",
        "Andrew Alvarado",
        "Andrew Caples",
        "Andrew Gu",
        "Andrew Ho",
        "Andrew Poulton",
        "Ankit Ryan",
        "Annie Ramchandani",
        "Annie Dong",
        "Anuj Franco",
        "Aparajita Goyal",
        "Arkabandhu Saraf",
        "Ashley Chowdhury",
        "Ashwin Gabriel",
        "Assaf Bharambe",
        "Azadeh Eisenman",
        "Beau Yazdan",
        "Ben James",
        "Benjamin Maurer",
        "Bernie Leonhardi",
        "Beth Huang",
        "Beto Loyd",
        "Bhargavi De Paola",
        "Bing Paranjape",
        "Bo Liu",
        "Boyu Wu",
        "Braden Ni",
        "Bram Hancock",
        "Brandon Wasti",
        "Brani Spence",
        "Brian Stojkovic",
        "Britt Gamido",
        "Carl Montalvo",
        "Carly Parker",
        "Catalina Burton",
        "Ce Mejia",
        "Changhan Liu",
        "Changkyu Wang",
        "Chao Kim",
        "Chester Zhou",
        "Ching-Hsiang Hu",
        "Chris Chu",
        "Chris Cai",
        "Christoph Tindal",
        "; Feichtenhofer",
        "Davide Xu",
        "Delia Testuggine",
        "Devi David",
        "Diana Parikh",
        "Didem Liskovich",
        "Dingkang Foss",
        "Duc Wang",
        "Dustin Le",
        "Edward Holland",
        "Eissa Dowling",
        "Elaine Jamil",
        "Eleonora Montgomery",
        "Emily Presani",
        "Emily Hahn",
        "Eric-Tuan Wood",
        "Erik Le",
        "Esteban Brinkman",
        "Evan Arcaute",
        "Evan Dunbar",
        "Fei Smothers",
        "Felix Sun",
        "Feng Kreuk",
        "Filippos Tian",
        "Firat Kokkinos",
        "Francesco Ozgenel",
        "Frank Caggioni",
        "Frank Kanayet",
        "Gabriela Seide",
        "Gabriella Florez",
        "Gada Schwarz",
        "Georgia Badeer",
        "Gil Swee",
        "Grant Halpern",
        "Grigory Herman",
        "Sizov",
        "Guangyi",
        "Guna Zhang",
        "Hakan Lakshminarayanan",
        "Hamid Inan",
        "Han Shojanazeri",
        "Hannah Zou",
        "Hanwen Wang",
        "Haroun Zha",
        "Harrison Habeeb",
        "Helen Rudolph",
        "Henry Suk",
        "Hunter Aspegren",
        "Hongyuan Goldman",
        "Ibrahim Zhan",
        "Igor Damlaj",
        "Igor Molybog",
        "Ilias Tufanov",
        "Irina-Elena Leontiadis",
        "Itai Veliche",
        "Jake Gat",
        "James Weissman",
        "James Geboski",
        "Janice Kohli",
        "Japhet Lam",
        "Jean-Baptiste Asher",
        "Jeff Gaya",
        "Jeff Marcus",
        "Jennifer Tang",
        "Jenny Chan",
        "Jeremy Zhen",
        "Jeremy Reizenstein",
        "Jessica Teboul",
        "Jian Zhong",
        "Jingyi Jin",
        "Joe Yang",
        "Jon Cummings",
        "Jon Carvill",
        "Jonathan Shepard",
        "Jonathan Mcphie",
        "Yanjun Torres ; Yaniv Kleinman",
        "Ye Chen",
        "Ye Hu",
        "Ye Jia",
        "Yenda Qi",
        "Yilin Li",
        "Ying Zhang",
        "Yossi Zhang",
        "Youngjin Adi",
        "Nam",
        "Wang Yu",
        "Yu Zhao",
        "Yuchen Hao",
        "Yundi Qian",
        "Yunlu Li",
        "Yuzi He",
        "Zach Rait",
        "Zachary Devito",
        "Zef Rosnbrick",
        "Zhaoduo Wen",
        "Zhenyu Yang",
        "Zhiwei Zhao",
        "Zhiyu Ma"
      ],
      "year": "2024",
      "venue": ""
    },
    {
      "citation_id": "31",
      "title": "Fine-tuned XLSR-53 large model for speech recognition in Arabic",
      "authors": [
        "Jonatas Grosman"
      ],
      "year": "2021",
      "venue": "Fine-tuned XLSR-53 large model for speech recognition in Arabic"
    },
    {
      "citation_id": "32",
      "title": "Towards arabic multimodal dataset for sentiment analysis",
      "authors": [
        "Abdelhamid Haouhat",
        "Slimane Bellaouar",
        "Attia Nehar",
        "Hadda Cherroun"
      ],
      "year": "2023",
      "venue": "2023 Fourth International Conference on Intelligent Data Science Technologies and Applications (IDSTA)"
    },
    {
      "citation_id": "33",
      "title": "Amd'saer: Arabic multimodal dataset for sentiment analysis and emotion recognition",
      "authors": [
        "Abdelhamid Haouhat",
        "Slimane Bellaouar",
        "Attia Nehar",
        "Hadda Cherroun"
      ],
      "year": "2024",
      "venue": "ACM Transactions on Asian and Low-Resource Language Information Processing"
    },
    {
      "citation_id": "34",
      "title": "MODOS at ArAIEval shared task: Multimodal propagandistic memes classification using weighted SAM, CLIP and ArabianGPT",
      "authors": [
        "Abdelhamid Haouhat",
        "Hadda Cherroun",
        "Slimane Bellaouar",
        "Attia Nehar"
      ],
      "year": "2024",
      "venue": "Proceedings of The Second Arabic Natural Language Processing Conference"
    },
    {
      "citation_id": "35",
      "title": "ArAIEval shared task: Propagandistic techniques detection in unimodal and multimodal Arabic content",
      "authors": [
        "Md Maram Hasanain",
        "Fatema Hasan",
        "Reem Ahmad",
        "Md Suwaileh",
        "Wajdi Biswas",
        "Firoj Zaghouani",
        "Alam"
      ],
      "year": "2024",
      "venue": "Proceedings of The Second Arabic Natural Language Processing Conference"
    },
    {
      "citation_id": "36",
      "title": "The arabic inclusive large multimodal model",
      "authors": [
        "Ahmed Heakl",
        "Sara Ghaboura",
        "Omkar Thawkar",
        "Fahad Shahbaz Khan",
        "Hisham Cholakkal",
        "Rao Muhammad Anwer",
        "Salman Khan",
        "Ain"
      ],
      "year": "2025",
      "venue": "The arabic inclusive large multimodal model",
      "arxiv": "arXiv:2502.00094"
    },
    {
      "citation_id": "37",
      "title": "Long short-term memory",
      "authors": [
        "Sepp Hochreiter",
        "Jürgen Schmidhuber"
      ],
      "year": "1997",
      "venue": "Neural Computation"
    },
    {
      "citation_id": "38",
      "title": "Self-supervised speech representation learning by masked prediction of hidden units",
      "authors": [
        "Wei-Ning Hsu",
        "Benjamin Bolte",
        "Hubert Yao-Hung",
        "Kushal Tsai",
        "Ruslan Lakhotia",
        "Abdelrahman Salakhutdinov",
        "Mohamed",
        "Hubert"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "39",
      "title": "Fasttext.zip: Compressing text classification models",
      "authors": [
        "Armand Joulin",
        "Edouard Grave",
        "Piotr Bojanowski",
        "Matthijs Douze",
        "Hérve Jégou",
        "Tomas Mikolov"
      ],
      "year": "2016",
      "venue": "Fasttext.zip: Compressing text classification models",
      "arxiv": "arXiv:1612.03651"
    },
    {
      "citation_id": "40",
      "title": "Jihad Mohamad Alja'am, and Arif Mahmood. Illustrate it! an arabic multimedia text-to-picture m-learning system",
      "authors": [
        "Abdel Ghani"
      ],
      "year": "2017",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "41",
      "title": "Automatic facial feature extraction and expression recognition based on neural network",
      "authors": [
        "S Khandait",
        "R Thool",
        "P Khandait"
      ],
      "year": "2012",
      "venue": "Automatic facial feature extraction and expression recognition based on neural network"
    },
    {
      "citation_id": "42",
      "title": "Cross-modal alignment learning of vision-language conceptual systems",
      "authors": [
        "Taehyeong Kim",
        "Hyeonseop Song",
        "Byoung-Tak Zhang"
      ],
      "year": "2022",
      "venue": "Cross-modal alignment learning of vision-language conceptual systems"
    },
    {
      "citation_id": "43",
      "title": "Foundations & trends in multimodal machine learning: Principles, challenges, and open questions",
      "authors": [
        "Paul Pu Liang",
        "Amir Zadeh",
        "Louis-Philippe Morency"
      ],
      "year": "2024",
      "venue": "ACM Comput. Surv"
    },
    {
      "citation_id": "44",
      "title": "Group gated fusion on attention-based bidirectional alignment for multimodal emotion recognition",
      "authors": [
        "Pengfei Liu",
        "Kun Li",
        "Helen Meng"
      ],
      "year": "2022",
      "venue": "Group gated fusion on attention-based bidirectional alignment for multimodal emotion recognition"
    },
    {
      "citation_id": "45",
      "title": "Distinctive image features from scale-invariant keypoints",
      "authors": [
        "G David",
        "Lowe"
      ],
      "year": "2004",
      "venue": "International Journal of Computer Vision"
    },
    {
      "citation_id": "46",
      "title": "Arabsign: A multi-modality dataset and benchmark for continuous arabic sign language recognition",
      "authors": [
        "Hamzah Luqman"
      ],
      "year": "2023",
      "venue": "2023 IEEE 17th International Conference on Automatic Face and Gesture Recognition (FG)"
    },
    {
      "citation_id": "47",
      "title": "No images, no problem: Retaining knowledge in continual vqa with questions-only memory",
      "authors": [
        "Enzo Imad Eddine Marouf",
        "Stephane Tartaglione",
        "Joost Lathuiliere",
        "Van De Weijer"
      ],
      "year": "2025",
      "venue": "No images, no problem: Retaining knowledge in continual vqa with questions-only memory"
    },
    {
      "citation_id": "48",
      "title": "Speaker independent recognition of low-resourced multilingual arabic spoken words through hybrid fusion",
      "authors": [
        "Sunakshi Mehra",
        "Virender Ranga",
        "Ritu Agarwal",
        "Seba Susan"
      ],
      "year": "2024",
      "venue": "Speaker independent recognition of low-resourced multilingual arabic spoken words through hybrid fusion"
    },
    {
      "citation_id": "49",
      "title": "Multimodal arabic retrieval and recommendation system",
      "authors": [
        "Mohamed Moselhy",
        "Abdallah Bastawy",
        "Osama Yousef",
        "Mohamed Khalil",
        "Mohamed Ashraf",
        "Ibrahim Elsayed",
        "Yasser Hifny"
      ],
      "year": "2023",
      "venue": "2023 International Mobile, Intelligent, and Ubiquitous Computing Conference (MIUCC)"
    },
    {
      "citation_id": "50",
      "title": "Crosslingual generalization through multitask finetuning",
      "authors": [
        "Niklas Muennighoff",
        "Thomas Wang",
        "Lintang Sutawika",
        "Adam Roberts",
        "Stella Biderman",
        "Teven Le Scao",
        "M Saiful Bari",
        "Sheng Shen",
        "Zheng Xin Yong",
        "Hailey Schoelkopf",
        "Xiangru Tang",
        "Dragomir Radev"
      ],
      "year": "2023",
      "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "51",
      "title": "Facial emotion recognition feature extraction: A survey",
      "authors": [
        "Michele Mukeshimana",
        "Abraham Niyongere",
        "Jérémie Ndikumagenge"
      ],
      "year": "2023",
      "venue": "Emotion Recognition, chapter"
    },
    {
      "citation_id": "52",
      "title": "Human alignment of neural network representations",
      "authors": [
        "Lukas Muttenthaler",
        "Jonas Dippel",
        "Lorenz Linhardt",
        "Robert Vandermeulen",
        "Simon Kornblith"
      ],
      "year": "2025",
      "venue": "Human alignment of neural network representations"
    },
    {
      "citation_id": "53",
      "title": "Multimodal deep learning",
      "authors": [
        "Jiquan Ngiam",
        "Aditya Khosla",
        "Mingyu Kim",
        "Juhan Nam",
        "Honglak Lee",
        "Andrew Ng"
      ],
      "year": "2011",
      "venue": "Proceedings of the 28th International Conference on International Conference on Machine Learning, ICML'11"
    },
    {
      "citation_id": "54",
      "title": "Felipe Petroski Such",
      "authors": [
        "Josh Openai",
        "Steven Achiam",
        "Sandhini Adler",
        "Lama Agarwal",
        "Ilge Ahmad",
        "Florencia Akkaya",
        "Diogo Leoni Aleman",
        "Janko Almeida",
        "Sam Altenschmidt",
        "Shyamal Altman",
        "Red Anadkat",
        "Igor Avila",
        "Suchir Babuschkin",
        "Valerie Balaji",
        "Paul Balcom",
        "Haiming Baltescu",
        "Mohammad Bao",
        "Jeff Bavarian",
        "Irwan Belgum",
        "Jake Bello",
        "Gabriel Berdine",
        "Christopher Bernadett-Shapiro",
        "Lenny Berner",
        "Oleg Bogdonoff",
        "Madelaine Boiko",
        "Anna-Luisa Boyd",
        "Greg Brakman",
        "Tim Brockman",
        "Miles Brooks",
        "Kevin Brundage",
        "Trevor Button",
        "Rosie Cai",
        "Andrew Campbell",
        "Brittany Cann",
        "Chelsea Carey",
        "Rory Carlson",
        "Brooke Carmichael",
        "Che Chan",
        "Fotis Chang",
        "Derek Chantzis",
        "Sully Chen",
        "Ruby Chen",
        "Jason Chen",
        "Mark Chen",
        "Ben Chen",
        "Chester Chess",
        "Casey Cho",
        "Hyung Chu",
        "Dave Chung",
        "Jeremiah Cummings",
        "Leo Currier",
        "Elie Gao",
        "Christian Georges",
        "Vik Gibson",
        "Tarun Goel",
        "Gabriel Gogineni",
        "Rapha Goh",
        "Jonathan Gontijo-Lopes",
        "Morgan Gordon",
        "Scott Grafstein",
        "Ryan Gray",
        "Joshua Greene",
        "Gross",
        "Shane Shixiang",
        "Yufei Gu",
        "Chris Guo",
        "Jesse Hallacy",
        "Jeff Han",
        "Yuchen Harris",
        "Mike He",
        "Johannes Heaton",
        "Chris Heidecke",
        "Alan Hesse",
        "Wade Hickey",
        "Peter Hickey",
        "Brandon Hoeschele",
        "Kenny Houghton",
        "Shengli Hsu",
        "Xin Hu",
        "Joost Hu",
        "Shantanu Huizinga",
        "Shawn Jain",
        "Joanne Jain",
        "Angela Jang",
        "Roger Jiang",
        "Haozhun Jiang",
        "Denny Jin",
        "Shino Jin",
        "Billie Jomoto",
        "Heewoo Jonn",
        "Tomer Jun",
        "Łukasz Kaftan",
        "Ali Kaiser",
        "Ingmar Kamali",
        "Nitish Kanitscheider",
        "Tabarak Shirish Keskar",
        "Logan Khan",
        "Jong Kilpatrick",
        "Christina Kim",
        "Yongjik Kim",
        "Jan Kim",
        "Jamie Kirchner",
        "Matt Kiros",
        "Daniel Knight",
        "Łukasz Kokotajlo",
        "Andrew Kondraciuk",
        "Aris Kondrich",
        "Kyle Konstantinidis",
        "Gretchen Kosic",
        "Vishal Krueger",
        "Michael Kuo",
        "Ikai Lampe",
        "Teddy Lan",
        "Jan Lee",
        "Jade Leike",
        "Daniel Leung",
        "Levy",
        "Ming Chak",
        "Rachel Li",
        "Molly Lim",
        "Stephanie Lin",
        "Mateusz Lin",
        "Theresa Litwin",
        "Ryan Lopez",
        "Patricia Lowe",
        "Anna Lue",
        "Kim Makanju",
        "Sam Malfacini",
        "Todor Manning",
        "Yaniv Markov",
        "Bianca Markovski",
        "Katie Martin",
        "Andrew Mayer",
        "Bob Mayne",
        "Scott Mcgrew",
        "Christine Mckinney",
        "Paul Mcleavey",
        "Jake Mcmillan",
        "David Mcneil",
        "Aalok Medina",
        "Jacob Mehta",
        "Luke Menick",
        "Andrey Metz",
        "Pamela Mishchenko",
        "Vinnie Mishkin",
        "Evan Monaco",
        "Daniel Morikawa",
        "Tong Mossing",
        "Mira Mu",
        "Oleg Murati",
        "David Murk",
        "Ashvin Mély",
        "Reiichiro Nair",
        "Rajeev Nakano",
        "Arvind Nayak",
        "Richard Neelakantan",
        "Hyeonwoo Ngo",
        "Long Noh",
        "Ouyang",
        "O' Cullen",
        "Jakub Keefe",
        "Alex Pachocki",
        "Joe Paino",
        "Ashley Palermo",
        "Giambattista Pantuliano",
        "Joel Parascandolo",
        "Emy Parish",
        "Alex Parparita",
        "Mikhail Passos",
        "Andrew Pavlov",
        "Adam Peng",
        "; Perelman",
        "Wei",
        "Akila Cj Weinmann",
        "Peter Welihinda",
        "Jiayi Welinder",
        "Lilian Weng",
        "Matt Weng",
        "Dave Wiethoff",
        "Clemens Willner",
        "Samuel Winter",
        "Hannah Wolrich",
        "Lauren Wong",
        "Sherwin Workman",
        "Jeff Wu",
        "Michael Wu",
        "Kai Wu",
        "Tao Xiao",
        "Sarah Xu",
        "Kevin Yoo",
        "Qiming Yu",
        "Wojciech Yuan",
        "Rowan Zaremba",
        "Chong Zellers",
        "Marvin Zhang",
        "Shengjia Zhang",
        "Tianhao Zhao",
        "Juntang Zheng",
        "William Zhuang",
        "Barret Zhuk",
        "Zoph"
      ],
      "year": "2024",
      "venue": "Felipe Petroski Such"
    },
    {
      "citation_id": "55",
      "title": "A review of affective computing: From unimodal analysis to multimodal fusion",
      "authors": [
        "Soujanya Poria",
        "Erik Cambria",
        "Rajiv Bajpai",
        "Amir Hussain"
      ],
      "year": "2017",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "56",
      "title": "Learning transferable visual models from natural language supervision",
      "authors": [
        "Alec Radford",
        "Jong Kim",
        "Chris Hallacy",
        "Aditya Ramesh",
        "Gabriel Goh",
        "Sandhini Agarwal",
        "Girish Sastry",
        "Amanda Askell",
        "Pamela Mishkin",
        "Jack Clark",
        "Gretchen Krueger",
        "Ilya Sutskever"
      ],
      "year": "2021",
      "venue": "Learning transferable visual models from natural language supervision"
    },
    {
      "citation_id": "57",
      "title": "Robust speech recognition via large-scale weak supervision",
      "authors": [
        "Alec Radford",
        "Jong Kim",
        "Tao Xu",
        "Greg Brockman",
        "Christine Mcleavey",
        "Ilya Sutskever"
      ],
      "year": "2022",
      "venue": "Robust speech recognition via large-scale weak supervision"
    },
    {
      "citation_id": "58",
      "title": "Improving language understanding by generative pre-training",
      "authors": [
        "Alec Radford",
        "Karthik Narasimhan"
      ],
      "year": "2018",
      "venue": "Improving language understanding by generative pre-training"
    },
    {
      "citation_id": "59",
      "title": "Multilanguage transformer for improved text to remote sensing image retrieval",
      "authors": [
        "Mohamad Al Rahhal",
        "Yakoub Bazi",
        "Norah Alsharif",
        "Laila Bashmal",
        "Naif Alajlan",
        "Farid Melgani"
      ],
      "year": "2022",
      "venue": "IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing"
    },
    {
      "citation_id": "60",
      "title": "Towards adaptive multimedia system for assisting children with arabic learning difficulties",
      "authors": [
        "Moutaz Saleh",
        "Jihad Mohamad Alja'am"
      ],
      "year": "2019",
      "venue": "2019 IEEE Jordan International Joint Conference on Electrical Engineering and Information Technology (JEEIT)"
    },
    {
      "citation_id": "61",
      "title": "MemeMind at ArAIEval shared task: Generative augmentation and feature fusion for multimodal propaganda detection in Arabic memes through advanced language and vision models",
      "authors": [
        "Uzair Shah",
        "Md Biswas",
        "Marco Agus",
        "Mowafa Househ",
        "Wajdi Zaghouani"
      ],
      "year": "2024",
      "venue": "Proceedings of The Second Arabic Natural Language Processing Conference"
    },
    {
      "citation_id": "62",
      "title": "AraFacts: The first large Arabic dataset of naturally occurring claims",
      "authors": [
        "Zien Sheikh",
        "Watheq Mansour",
        "Tamer Elsayed",
        "Abdulaziz Al-Ali"
      ],
      "year": "2021",
      "venue": "Proceedings of the Sixth Arabic Natural Language Processing Workshop"
    },
    {
      "citation_id": "63",
      "title": "A comprehensive survey of contemporary arabic sentiment analysis: Methods, challenges, and future directions",
      "authors": [
        "Zhiqiang Shi",
        "Ruchit Agrawal"
      ],
      "year": "2025",
      "venue": "A comprehensive survey of contemporary arabic sentiment analysis: Methods, challenges, and future directions"
    },
    {
      "citation_id": "64",
      "title": "Gann: Graph alignment neural network for semi-supervised learning",
      "authors": [
        "Linxuan Song",
        "Wenxuan Tu",
        "Sihang Zhou",
        "En Zhu"
      ],
      "year": "2024",
      "venue": "Pattern Recognition"
    },
    {
      "citation_id": "65",
      "title": "Fanar: An arabic-centric multimodal generative ai platform",
      "authors": [
        "Fanar Team",
        "Ummar Abbas",
        "Mohammad Shahmeer Ahmad",
        "Firoj Alam",
        "Enes Altinisik",
        "Ehsannedin Asgari",
        "Yazan Boshmaf",
        "Sabri Boughorbel",
        "Sanjay Chawla",
        "Shammur Chowdhury",
        "Fahim Dalvi",
        "Kareem Darwish",
        "Nadir Durrani",
        "Mohamed Elfeky",
        "Ahmed Elmagarmid",
        "Mohamed Eltabakh",
        "Masoomali Fatehkia",
        "Anastasios Fragkopoulos",
        "Maram Hasanain",
        "Majd Hawasly",
        "' Mus",
        "Soon-Gyo Husaini",
        "Ji Jung",
        "Walid Lucas",
        "Safa Magdy",
        "Messaoud"
      ],
      "year": "2025",
      "venue": "Fanar: An arabic-centric multimodal generative ai platform"
    },
    {
      "citation_id": "66",
      "title": "Attention is all you need",
      "authors": [
        "Ashish Vaswani",
        "Noam Shazeer",
        "Niki Parmar",
        "Jakob Uszkoreit",
        "Llion Jones",
        "Aidan Gomez",
        "Łukasz Kaiser",
        "Illia Polosukhin"
      ],
      "year": "2017",
      "venue": "Proceedings of the 31st International Conference on Neural Information Processing Systems, NIPS'17"
    },
    {
      "citation_id": "67",
      "title": "Rapid object detection using a boosted cascade of simple features",
      "authors": [
        "P Viola",
        "M Jones"
      ],
      "year": "2001",
      "venue": "Proceedings of the 2001 IEEE Computer Society Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "68",
      "title": "CLTL at ArAIEval shared task: Multimodal propagandistic memes classification using transformer models",
      "authors": [
        "Yeshan Wang",
        "Ilia Markov"
      ],
      "year": "2024",
      "venue": "Proceedings of The Second Arabic Natural Language Processing Conference"
    },
    {
      "citation_id": "69",
      "title": "Arabic language investigation in the context of unimodal and multimodal sentiment analysis",
      "authors": [
        "Fatima Zohra",
        "Fatiha Barigou"
      ],
      "year": "2021",
      "venue": "2021 22nd International Arab Conference on Information Technology (ACIT)"
    },
    {
      "citation_id": "70",
      "title": "Bench-marking and improving arabic automatic image captioning through the use of multi-task learning paradigm",
      "authors": [
        "Eddin Muhy",
        "' Za",
        "Bashar Talafha"
      ],
      "year": "2022",
      "venue": "Bench-marking and improving arabic automatic image captioning through the use of multi-task learning paradigm"
    },
    {
      "citation_id": "71",
      "title": "AlexUNLP-MZ at ArAIEval shared task: Contrastive learning, LLM features extraction and multi-objective optimization for Arabic multi-modal meme propaganda detection",
      "authors": [
        "Mohamed Zaytoon",
        "Nagwa El-Makky",
        "Marwan Torki"
      ],
      "year": "2024",
      "venue": "Proceedings of The Second Arabic Natural Language Processing Conference"
    }
  ]
}