{
  "paper_id": "2409.05015v2",
  "title": "Improving Multimodal Emotion Recognition By Leveraging Acoustic Adaptation And Visual Alignment",
  "published": "2024-09-08T07:56:51Z",
  "authors": [
    "Zhixian Zhao",
    "Haifeng Chen",
    "Xi Li",
    "Dongmei Jiang",
    "Lei Xie"
  ],
  "keywords": [
    "Multimodal Emotion Recognition",
    "Fine-tuning",
    "Contrastive Learning"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Multimodal Emotion Recognition (MER) aims to automatically identify and understand human emotional states by integrating information from various modalities. However, the scarcity of annotated multimodal data significantly hinders the advancement of this research field. This paper presents our solution for the MER-SEMI sub-challenge of MER 2024. First, to better adapt acoustic modality features for the MER task, we experimentally evaluate the contributions of different layers of the pre-trained speech model HuBERT in emotion recognition. Based on these observations, we perform Parameter-Efficient Fine-Tuning (PEFT) on the layers identified as most effective for emotion recognition tasks, thereby achieving optimal adaptation for emotion recognition with a minimal number of learnable parameters. Second, leveraging the strengths of the acoustic modality, we propose a feature alignment pre-training method. This approach uses large-scale unlabeled data to train a visual encoder, thereby promoting the semantic alignment of visual features within the acoustic feature space. Finally, using the adapted acoustic features, aligned visual features, and lexical features, we employ an attention mechanism for feature fusion. On the MER2024-SEMI test set, the proposed method achieves a weighted F1 score of 88.90%, ranking fourth among all participating teams, validating the effectiveness of our approach. \n CCS Concepts â€¢ Computing methodologies â†’ Artificial intelligence; â€¢ Humancentered computing â†’ Human computer interaction (HCI).",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Introduction",
      "text": "Automatic emotion recognition is crucial for human-computer interaction (HCI), allowing computers to detect and respond to users' emotional states  [4, 12] . Multimodal emotion recognition (MER), based on supervised learning, has shown promising results  [6, 10, 20] . However, the annotation process is costly and time-consuming, limiting the scale of labeled MER datasets and hindering performance. The MER-SEMI challenge, a sub-challenge of the Multimodal Emotion Recognition Challenge  [13, 14] , addresses this issue by providing a labeled emotional dataset alongside substantial unlabeled data, enabling participants to explore effective unsupervised or semi-supervised learning strategies.\n\nPre-trained transformer models  [1, 2, 9, 18]  have achieved notable success across various speech tasks, excelling in capturing phonetic structures, temporal dependencies, and acoustic features. Studies  [11, 19]  suggest that essential task-specific information may reside in the hidden representations of different transformer layers, yet there is limited exploration of their contributions to speech emotion recognition. Typically, current parameter fine-tuning methods  [7, 8, 22]  involve modifying the entire model, which overlooks the varying significance of features across layers. To improve the performance of pre-trained speech models in emotion recognition, it is beneficial to incorporate adapters when fine-tuning certain intermediate layers. This approach aligns features with emotion recognition requirements, reduces training parameters, and preserves the model's generalization capability.\n\nThe visual modality provides rich non-verbal information, such as facial expressions, body language, and gestures, essential for computer vision and natural language processing tasks  [21, 25] . To fully leverage multimodal information, comparative cross-modal pretraining methods  [5, 16, 23]  have been developed. For instance, CLIP  [17]  achieves a shared semantic space for visual and textual understanding by jointly training image and text encoders. Baseline results  [14]  indicate that the visual modality's performance in emotion recognition is relatively weaker compared to the acoustic modality. Therefore, after obtaining optimal acoustic features, the visual modality can be aligned with the acoustic feature space. Contrastive learning methods can establish relationships between acoustic and visual modalities, enhancing the visual modality's ability to capture emotional information more accurately.\n\nBased on the above discussions, we propose a semi-supervised multimodal emotion recognition method comprising three stages: acoustic feature adaptation, visual feature alignment, and multimodal feature fusion. The main flow is shown in Figure  1 . First, to optimize acoustic modality features for emotion recognition, we conducted an empirical study exploring the performance of different layers of the HuBERT-large model  [9]  and the effectiveness of multi-layer feature fusion. Guided by these empirical findings, we propose a simple and effective parameter-efficient fine-tuning method. This method enhances recognition performance by incorporating adapters into well-performing intermediate transformer layers and dynamically fusing hidden representations across these layers using learnable weights. Second, to enhance the emotional representation capability of visual modality features, we perform contrastive learning between the fine-tuned acoustic features and visual features processed by a multilayer perceptron (MLP). We leverage a substantial amount of unlabeled visual and audio data to pretrain the vision MLP in an unsupervised manner, ensuring the visual features adapt to the acoustic feature space. Finally, we employ an attention-based feature fusion module to integrate the acoustic, visual, and textual features, achieving a weighted F1 score of 88.90% on the test set.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Method 2.1 Acoustic Feature Adaptation",
      "text": "Based on the findings of the baseline study  [14] , we select the HuBERT-large model  [9] , which has demonstrated superior performance in emotion recognition tasks, as the feature extractor. Since pre-trained transformer models (e.g., HuBERT and wav2vec2.0  [2] ) capture unique hidden representations across different layers during audio processing and that these layers may contain complementary information, we utilize the temporal pooling features (ğ‘“ ğ‘– ğ‘ âˆˆ R d a , ğ‘– âˆˆ 1, 2, ..., ğ‘˜) from ğ‘˜ consecutive middle layers of the HuBERT-large model for the emotion recognition task.\n\nTo adapt these features more effectively to the emotion recognition task while maintaining the generalization capability of the pre-trained model, we introduce adapters in these ğ‘˜ transformer layers for efficient parameter fine-tuning. The implementation details of the adapters are depicted in Figure  1 (Stage 1). Each adapter consists of a bottleneck structure, including a dimension reduction projection layer that reduces the hidden dimension from d a to d, followed by a ReLU non-linear activation function, and then an up-projection layer that restores the dimension back to d a . Given an input feature ğ‘¥, the output ğ‘¦ of the adapter can be represented as:\n\nwhere\n\nTo account for the varying contributions of different layers to the emotion recognition task, we introduce learnable weights (ğ‘¤ ğ‘– , ğ‘– âˆˆ {1, 2, ..., ğ‘˜ }) to fuse the output features of these ğ‘˜ transformer layers, resulting in the fused feature ğ‘“\n\nWe use two types of loss functions to optimize the model. The first is an unsupervised masked reconstruction loss, inspired by  [9] , this approach predicts the masked portions of the acoustic features using contextual information to learn more robust acoustic representations. We generate masked features ğ‘“ ğ‘šğ‘ğ‘ ğ‘˜ğ‘’ğ‘‘ ğ‘ âˆˆ R d a through a multi-layer perceptron (MLP) consisting of two fully connected layers and a ReLU layer, and measure the difference between the original and masked features using the mean squared error (MSE) loss function. The second loss function is a supervised cross-entropy (CE) loss. After passing the masked features through a fully connected layer to obtain the predicted results ğ‘¥ ğ‘ğ‘Ÿğ‘’ğ‘‘ , we use the CE loss function to calculate the loss between the predicted results and the labels ğ‘¦ ğ‘™ğ‘ğ‘ğ‘’ğ‘™ . These two loss functions can be expressed as:\n\nThus, the overall objective function for fine-tuning the model is defined as:",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Visual Feature Alignment",
      "text": "Due to the semantic disparity between the visual features extracted by CLIP-large  [17]  and the audio features, we perform pre-training feature alignment for the visual modality prior to multimodal feature fusion, as illustrated in Figure  1 (Stage 2). Inspired by  [15] , we first pre-train a vision MLP to align visual representations with the semantic space of pre-trained speech emotion representations. During this process, the weights of the CLIP-large model and the finetuned HuBERT-large model are kept frozen, and only the weights of the vision MLP are trained.\n\nWe introduce an video-audio contrastive learning method. An input video ğ¼ is encoded by the CLIP-large model and then averagepooled to obtain ğ‘“ v âˆˆ R d v . Then, an MLP maps the visual embedding to the same dimensionality as the audio embedding, resulting in mapped visual embedding ğ‘“ ğ‘£ âˆˆ R d a . This transformation can be represented as:\n\nwhere ğ‘Š 1 âˆˆ R ğ‘‘ ğ‘£ Ã—d a and ğ‘Š 2 âˆˆ R d a Ã—d a , ğ‘ 1 âˆˆ R d a and ğ‘ 2 âˆˆ R d a are trainable parameters of the vision MLP. The goal of the MLP is to align the visual and audio features so they can operate within a unified feature space.\n\nTo compute the similarity between the visual and audio embeddings, we first use the fine-tuned HuBERT-large model to convert the input audio ğ´ into an embedding sequence ğ‘“ ğ‘ âˆˆ R d a . The videoaudio similarity can be defined as: ğ‘  (ğ¼, ğ´) = ğ‘“ âŠ¤ ğ‘£ ğ‘“ ğ‘ /âˆ¥ğ‘“ ğ‘£ âˆ¥ â€¢ âˆ¥ğ‘“ ğ‘ âˆ¥ and the audio-video similarity as:\n\nwhere ğœ is a learnable temperature parameter. Let ğ‘¦ i2a and ğ‘¦ a2i represent the ground truth one-hot encoded similarities, with a probability of 0 for mismatched video-audio pairs and 1 for matched pairs. The video-audio contrastive loss L ita is defined as the crossentropy H between ğ‘ and ğ‘¦:",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Multimodal Feature Fusion",
      "text": "Before conducting feature fusion, we outline the extraction processes for the three modalities. Acoustic Features: we utilize the fine-tuned Chinese-HuBERT-Large model  [9]  to extract the speech representation ğ‘“ ğ‘ for each audio sample. Visual Features: we input the facial images extracted using the OpenFace toolkit  [3]  into the CLIP-large model  [17]  to extract the visual features. Subsequently, we use the vision MLP introduced in Section 2.2 as the feature extractor to obtain the feature representation ğ‘“ ğ‘£ . Lexical Features: we use the Baichuan2  [24]  model to extract the feature representation ğ‘“ ğ‘™ from the checked transcripts of the Train&Val set, as well as the subtitle files of the unlabeled data. After obtaining the feature vectors for the three modalities ğ‘“ ğ‘š âˆˆ R d m , ğ‘š âˆˆ (ğ‘, ğ‘£, ğ‘™), we use an MLP composed of several fully connected layers and ReLU activation functions to map each modality's features to the same dimension. Considering the varying importance of each modality, we stack the three modality features together and calculate the attention score ğ›¼ for each modality:\n\nwhere ğ‘Š ğ›¼ and ğ‘ ğ›¼ are trainable parameters. The final fused feature is given by ğ‘§ = â„ğ›¼. Implementation Details: For acoustic features adaptation, we select the 16th to 21st layers (total ğ‘˜ = 6 layers) of the HuBERT-large model and incorporate adapters into these layers for fine-tuning (refer to Section 3.2.1). The feature dimension d within the Adapter is set to 128. During fused feature computation, we assign an initial weight of 1.0 to the 18th layer for its optimal performance, while the other layers are initially weighted at 0.0. Pre-training is conducted on the Train&Val set with a batch size of 16 and a learning rate of 1e-4. We use the Adam optimizer with a weight decay of 0.02. For visual feature alignment, the vision MLP is trained on unlabeled data with a batch size of 1024 and a learning rate of 1e-4. For the feature fusion module, features from the three modalities are mapped to 256 dimensions through an MLP for fusion, with a learning rate set to 1e-4. The dimensions of the visual, audio, and lexical features are d v = 768, d a = 1024, and d l = 5120, respectively.  To validate the performance of features from different transformer layers in emotion recognition tasks, we extract the features from the last 10 layers of the HuBERT-large model and evaluate their performance on the Train&Val set, as shown in Figure  2 . The rationale for focusing on the last 10 layers is that shallow layers typically encode low-level features, such as pitch and short-term energy fluctuations, while deeper layers are more adept at capturing high-level features and global semantic information, which are crucial for emotion recognition. As shown in the figure, features from the 18th layer demonstrate the best performance, significantly surpassing the baseline score, indicating that this layer captures more representative emotional features. Furthermore, features from the 16th through 21st layers consistently outperform the baseline, indicating that these intermediate layers are more suitable for emotion recognition tasks. These layers balance capturing both low-level and high-level information, providing rich audio content while minimizing noise interference. This finding is of significant importance for subsequent model fine-tuning efforts.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Result And Discussion",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Unimodal Recognition Results",
      "text": ". We present the experimental results for both unimodal and multimodal approaches in Table  2 . For the acoustic modality, HL represents features extracted using the baseline method  [14]  from the HuBERT-large model, with HL(ğ‘–) indicating the feature output from the ğ‘–-th layer. HLFT(ğ‘–) indicates the output from the ğ‘–-th layer after fine-tuning the HuBERT-large model with adapters. As shown in the table, the performance of the finetuned features demonstrates a notable improvement over the baseline model. The proposed parameter-efficient fine-tuning method achieves superior performance with multi-layer fused features compared to single-layer features. Specifically, HL  (16) (17) (18) (19) (20) (21)  surpasses HL  (18) , and HLFT(16-21) outperforms HLFT  (18) , suggesting that complementary information exists among features from different layers, resulting in more robust results. Moreover, the multi-layer fused features obtained through the proposed parameter-efficient fine-tuning method achieve the highest F1 score of 84.88% on the test set. This method also demonstrates performance improvements of 7.42% on the Train&Val set and 1.39% on the test set compared to the baseline model, validating its effectiveness.\n\nFor the visual modality, CLIPL represents the features extracted using the CLIP-Large model, whereas CLIPL-A denotes the features aligned through the proposed visual feature alignment strategy. As shown in Table  2 , in comparison to the CLIPL features, the CLIPL-A features exhibit comparable performance on the Train&Val set and show a 3.64% improvement on the test set. These results affirm the efficacy of the visual feature alignment strategy in enhancing performance within multimodal emotion recognition tasks.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Multimodal Recognition Results",
      "text": ". We conduct a comprehensive comparison of the multimodal fusion effects, with specific results presented in Table  2 . Initially, we fuse the best-performing acoustic modality features, HLFT  (16) (17) (18) (19) (20) (21) , with the visual modality features extracted by CLIP-large. This fusion improves the recognition accuracy from 84.88% to 86.78%. Furthermore, using the aligned features extracted by the pre-trained vision MLP, the recognition accuracy further increases to 87.01%, which provides additional validation for the effectiveness of the feature alignment pre-training method. Similarly, we evaluate the fusion of lexical modality and acoustic modality. When fusing Baichuan2 features with HLFT  (16) (17) (18) (19) (20) (21)  features, the performance on the test set reaches 85.78%. Ultimately, the fusion of all three modality features results in the highest performance of 88.90% on the test set. Additionally, the table illustrates that the effect of multimodal fusion on the test set surpasses that of any single modality.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Conclusion",
      "text": "In this study, we propose a multimodal emotion recognition framework for the MER2024-SEMI challenge. Initially, our focus is on fully leveraging acoustic modality features to enhance emotion recognition tasks. We evaluate the performance of different transformer layers of the HuBERT-large model in speech emotion recognition and employ an PEFT method to fine-tune the HuBERT-large model. Subsequently, to enhance the emotional representation of the visual modality, we introduce an unsupervised feature alignment scheme that employs contrastive learning to align visual embeddings with acoustic embeddings. Experimental results validate the effectiveness of the proposed methods, with our approach securing fourth place in the MER2024-SEMI sub-challenge.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Acknowledgments",
      "text": "",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: First, to",
      "page": 2
    },
    {
      "caption": "Figure 1: (Stage 1). Each adapter",
      "page": 2
    },
    {
      "caption": "Figure 1: (Stage 2). Inspired by [15], we",
      "page": 2
    },
    {
      "caption": "Figure 1: The proposed multimodal emotion recognition model framework",
      "page": 3
    },
    {
      "caption": "Figure 2: Comparison of the performance of features from",
      "page": 4
    },
    {
      "caption": "Figure 2: The rationale for focusing on the last 10 layers is that",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Northwestern Polytechnical University": "Xiâ€™an, China",
          "lxie@nwpu.edu.cn": ""
        },
        {
          "Northwestern Polytechnical University": "jiangdm@nwpu.edu.cn",
          "lxie@nwpu.edu.cn": ""
        },
        {
          "Northwestern Polytechnical University": "Abstract",
          "lxie@nwpu.edu.cn": "Keywords"
        },
        {
          "Northwestern Polytechnical University": "Multimodal Emotion Recognition (MER) aims to automatically iden-",
          "lxie@nwpu.edu.cn": "Multimodal Emotion Recognition, Fine-tuning, Contrastive Learn-"
        },
        {
          "Northwestern Polytechnical University": "tify and understand human emotional states by integrating infor-",
          "lxie@nwpu.edu.cn": "ing"
        },
        {
          "Northwestern Polytechnical University": "mation from various modalities. However, the scarcity of annotated",
          "lxie@nwpu.edu.cn": ""
        },
        {
          "Northwestern Polytechnical University": "",
          "lxie@nwpu.edu.cn": "ACM Reference Format:"
        },
        {
          "Northwestern Polytechnical University": "multimodal data significantly hinders the advancement of this re-",
          "lxie@nwpu.edu.cn": "Zhixian Zhao, Haifeng Chen, Xi Li, Dongmei Jiang, and Lei Xie. 2024. Im-"
        },
        {
          "Northwestern Polytechnical University": "search field. This paper presents our solution for the MER-SEMI",
          "lxie@nwpu.edu.cn": "proving Multimodal Emotion Recognition by Leveraging Acoustic Adapta-"
        },
        {
          "Northwestern Polytechnical University": "sub-challenge of MER 2024. First, to better adapt acoustic modality",
          "lxie@nwpu.edu.cn": "tion and Visual Alignment. In Proceedings of the 2nd International Workshop"
        },
        {
          "Northwestern Polytechnical University": "features for the MER task, we experimentally evaluate the contribu-",
          "lxie@nwpu.edu.cn": "on Multimodal and Responsible Affective Computing (MRAC â€™24), Novem-"
        },
        {
          "Northwestern Polytechnical University": "",
          "lxie@nwpu.edu.cn": "ber 1, 2024, Melbourne, VIC, Australia. ACM, New York, NY, USA, 5 pages."
        },
        {
          "Northwestern Polytechnical University": "tions of different layers of the pre-trained speech model HuBERT",
          "lxie@nwpu.edu.cn": ""
        },
        {
          "Northwestern Polytechnical University": "",
          "lxie@nwpu.edu.cn": "https://doi.org/10.1145/3689092.3689407"
        },
        {
          "Northwestern Polytechnical University": "in emotion recognition. Based on these observations, we perform",
          "lxie@nwpu.edu.cn": ""
        },
        {
          "Northwestern Polytechnical University": "Parameter-Efficient Fine-Tuning (PEFT) on the layers identified as",
          "lxie@nwpu.edu.cn": ""
        },
        {
          "Northwestern Polytechnical University": "",
          "lxie@nwpu.edu.cn": "1\nIntroduction"
        },
        {
          "Northwestern Polytechnical University": "most effective for emotion recognition tasks,\nthereby achieving",
          "lxie@nwpu.edu.cn": ""
        },
        {
          "Northwestern Polytechnical University": "optimal adaptation for emotion recognition with a minimal number",
          "lxie@nwpu.edu.cn": "Automatic emotion recognition is crucial\nfor human-computer"
        },
        {
          "Northwestern Polytechnical University": "of learnable parameters. Second,\nleveraging the strengths of the",
          "lxie@nwpu.edu.cn": "interaction (HCI), allowing computers to detect and respond to"
        },
        {
          "Northwestern Polytechnical University": "acoustic modality, we propose a feature alignment pre-training",
          "lxie@nwpu.edu.cn": "usersâ€™ emotional states [4, 12]. Multimodal emotion recognition"
        },
        {
          "Northwestern Polytechnical University": "method. This approach uses large-scale unlabeled data to train a",
          "lxie@nwpu.edu.cn": "(MER), based on supervised learning, has shown promising re-"
        },
        {
          "Northwestern Polytechnical University": "visual encoder, thereby promoting the semantic alignment of vi-",
          "lxie@nwpu.edu.cn": "sults [6, 10, 20]. However,\nthe annotation process is costly and"
        },
        {
          "Northwestern Polytechnical University": "sual features within the acoustic feature space. Finally, using the",
          "lxie@nwpu.edu.cn": "time-consuming, limiting the scale of labeled MER datasets and hin-"
        },
        {
          "Northwestern Polytechnical University": "adapted acoustic features, aligned visual features, and lexical fea-",
          "lxie@nwpu.edu.cn": "dering performance. The MER-SEMI challenge, a sub-challenge of"
        },
        {
          "Northwestern Polytechnical University": "tures, we employ an attention mechanism for feature fusion. On the",
          "lxie@nwpu.edu.cn": "the Multimodal Emotion Recognition Challenge [13, 14], addresses"
        },
        {
          "Northwestern Polytechnical University": "MER2024-SEMI test set, the proposed method achieves a weighted",
          "lxie@nwpu.edu.cn": "this issue by providing a labeled emotional dataset alongside sub-"
        },
        {
          "Northwestern Polytechnical University": "F1 score of 88.90%, ranking fourth among all participating teams,",
          "lxie@nwpu.edu.cn": "stantial unlabeled data, enabling participants to explore effective"
        },
        {
          "Northwestern Polytechnical University": "validating the effectiveness of our approach.",
          "lxie@nwpu.edu.cn": "unsupervised or semi-supervised learning strategies."
        },
        {
          "Northwestern Polytechnical University": "",
          "lxie@nwpu.edu.cn": "Pre-trained transformer models [1, 2, 9, 18] have achieved no-"
        },
        {
          "Northwestern Polytechnical University": "",
          "lxie@nwpu.edu.cn": "table success across various speech tasks, excelling in capturing"
        },
        {
          "Northwestern Polytechnical University": "CCS Concepts",
          "lxie@nwpu.edu.cn": ""
        },
        {
          "Northwestern Polytechnical University": "",
          "lxie@nwpu.edu.cn": "phonetic structures, temporal dependencies, and acoustic features."
        },
        {
          "Northwestern Polytechnical University": "â€¢ Computing methodologies â†’ Artificial intelligence; â€¢ Human-",
          "lxie@nwpu.edu.cn": ""
        },
        {
          "Northwestern Polytechnical University": "",
          "lxie@nwpu.edu.cn": "Studies [11, 19] suggest that essential task-specific information may"
        },
        {
          "Northwestern Polytechnical University": "centered computing â†’ Human computer interaction (HCI).",
          "lxie@nwpu.edu.cn": ""
        },
        {
          "Northwestern Polytechnical University": "",
          "lxie@nwpu.edu.cn": "reside in the hidden representations of different transformer layers,"
        },
        {
          "Northwestern Polytechnical University": "",
          "lxie@nwpu.edu.cn": "yet there is limited exploration of their contributions to speech emo-"
        },
        {
          "Northwestern Polytechnical University": "âˆ—Corresponding author.",
          "lxie@nwpu.edu.cn": ""
        },
        {
          "Northwestern Polytechnical University": "",
          "lxie@nwpu.edu.cn": "tion recognition. Typically, current parameter fine-tuning methods"
        },
        {
          "Northwestern Polytechnical University": "",
          "lxie@nwpu.edu.cn": "[7, 8, 22] involve modifying the entire model, which overlooks the"
        },
        {
          "Northwestern Polytechnical University": "Permission to make digital or hard copies of all or part of this work for personal or",
          "lxie@nwpu.edu.cn": ""
        },
        {
          "Northwestern Polytechnical University": "",
          "lxie@nwpu.edu.cn": "varying significance of features across layers. To improve the per-"
        },
        {
          "Northwestern Polytechnical University": "classroom use is granted without fee provided that copies are not made or distributed",
          "lxie@nwpu.edu.cn": ""
        },
        {
          "Northwestern Polytechnical University": "for profit or commercial advantage and that copies bear this notice and the full citation",
          "lxie@nwpu.edu.cn": "formance of pre-trained speech models in emotion recognition,"
        },
        {
          "Northwestern Polytechnical University": "on the first page. Copyrights for components of this work owned by others than the",
          "lxie@nwpu.edu.cn": ""
        },
        {
          "Northwestern Polytechnical University": "",
          "lxie@nwpu.edu.cn": "it is beneficial to incorporate adapters when fine-tuning certain"
        },
        {
          "Northwestern Polytechnical University": "author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or",
          "lxie@nwpu.edu.cn": ""
        },
        {
          "Northwestern Polytechnical University": "republish, to post on servers or to redistribute to lists, requires prior specific permission",
          "lxie@nwpu.edu.cn": "intermediate layers. This approach aligns features with emotion"
        },
        {
          "Northwestern Polytechnical University": "and/or a fee. Request permissions from permissions@acm.org.",
          "lxie@nwpu.edu.cn": "recognition requirements, reduces training parameters, and pre-"
        },
        {
          "Northwestern Polytechnical University": "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.",
          "lxie@nwpu.edu.cn": ""
        },
        {
          "Northwestern Polytechnical University": "",
          "lxie@nwpu.edu.cn": "serves the modelâ€™s generalization capability."
        },
        {
          "Northwestern Polytechnical University": "Â© 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",
          "lxie@nwpu.edu.cn": ""
        },
        {
          "Northwestern Polytechnical University": "",
          "lxie@nwpu.edu.cn": "The visual modality provides rich non-verbal information, such"
        },
        {
          "Northwestern Polytechnical University": "ACM ISBN 979-8-4007-1203-6/24/11",
          "lxie@nwpu.edu.cn": ""
        },
        {
          "Northwestern Polytechnical University": "https://doi.org/10.1145/3689092.3689407",
          "lxie@nwpu.edu.cn": "as facial expressions, body language, and gestures, essential\nfor"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "computer vision and natural language processing tasks [21, 25]. To",
          "Zhixian Zhao, Haifeng Chen, Xi Li, Dongmei Jiang, & Lei Xie": "as:"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "fully leverage multimodal information, comparative cross-modal",
          "Zhixian Zhao, Haifeng Chen, Xi Li, Dongmei Jiang, & Lei Xie": "ğ‘¦ = ğ‘¥ + ReLU(ğ‘Šğ‘¢ğ‘ ReLU(ğ‘Šğ‘‘ğ‘œğ‘¤ğ‘›ğ‘¥ + ğ‘ğ‘‘ğ‘œğ‘¤ğ‘›) + ğ‘ğ‘¢ğ‘ ),\n(1)"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "pretraining methods [5, 16, 23] have been developed. For instance,",
          "Zhixian Zhao, Haifeng Chen, Xi Li, Dongmei Jiang, & Lei Xie": ""
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Zhixian Zhao, Haifeng Chen, Xi Li, Dongmei Jiang, & Lei Xie": "where ğ‘Šğ‘‘ğ‘œğ‘¤ğ‘› âˆˆ Rda Ã— Ë†d, ğ‘Šğ‘¢ğ‘ âˆˆ RË†dÃ—da , ğ‘ğ‘‘ğ‘œğ‘¤ğ‘› âˆˆ RË†d, ğ‘ğ‘¢ğ‘ âˆˆ Rda are"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "CLIP [17] achieves a shared semantic space for visual and textual",
          "Zhixian Zhao, Haifeng Chen, Xi Li, Dongmei Jiang, & Lei Xie": ""
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Zhixian Zhao, Haifeng Chen, Xi Li, Dongmei Jiang, & Lei Xie": "trainable parameters."
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "understanding by jointly training image and text encoders. Base-",
          "Zhixian Zhao, Haifeng Chen, Xi Li, Dongmei Jiang, & Lei Xie": ""
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Zhixian Zhao, Haifeng Chen, Xi Li, Dongmei Jiang, & Lei Xie": "To account\nfor\nthe varying contributions of different\nlayers"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "line results [14] indicate that the visual modalityâ€™s performance in",
          "Zhixian Zhao, Haifeng Chen, Xi Li, Dongmei Jiang, & Lei Xie": ""
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Zhixian Zhao, Haifeng Chen, Xi Li, Dongmei Jiang, & Lei Xie": "to the emotion recognition task, we introduce learnable weights"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "emotion recognition is relatively weaker compared to the acous-",
          "Zhixian Zhao, Haifeng Chen, Xi Li, Dongmei Jiang, & Lei Xie": ""
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Zhixian Zhao, Haifeng Chen, Xi Li, Dongmei Jiang, & Lei Xie": "(ğ‘¤ğ‘–, ğ‘–\nâˆˆ {1, 2, ..., ğ‘˜ })\nto fuse the output\nfeatures of\nthese ğ‘˜ trans-"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "tic modality. Therefore, after obtaining optimal acoustic features,",
          "Zhixian Zhao, Haifeng Chen, Xi Li, Dongmei Jiang, & Lei Xie": ""
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Zhixian Zhao, Haifeng Chen, Xi Li, Dongmei Jiang, & Lei Xie": "Â· ğ‘“ ğ‘–\nğ‘¤ğ‘–\n= (cid:205)ğ‘˜\nformer layers, resulting in the fused feature ğ‘“ ğ‘“ ğ‘¢ğ‘ ğ‘–ğ‘œğ‘›\nğ‘ ,"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "the visual modality can be aligned with the acoustic feature space.",
          "Zhixian Zhao, Haifeng Chen, Xi Li, Dongmei Jiang, & Lei Xie": "ğ‘–=1"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "Contrastive learning methods can establish relationships between",
          "Zhixian Zhao, Haifeng Chen, Xi Li, Dongmei Jiang, & Lei Xie": "where ğ‘“ ğ‘“ ğ‘¢ğ‘ ğ‘–ğ‘œğ‘›\nâˆˆ Rda . We use two types of loss functions to opti-"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "acoustic and visual modalities, enhancing the visual modalityâ€™s",
          "Zhixian Zhao, Haifeng Chen, Xi Li, Dongmei Jiang, & Lei Xie": "mize the model. The first is an unsupervised masked reconstruc-"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "ability to capture emotional information more accurately.",
          "Zhixian Zhao, Haifeng Chen, Xi Li, Dongmei Jiang, & Lei Xie": "tion loss, inspired by [9], this approach predicts the masked por-"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "Based on the above discussions, we propose a semi-supervised",
          "Zhixian Zhao, Haifeng Chen, Xi Li, Dongmei Jiang, & Lei Xie": "tions of the acoustic features using contextual information to learn"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "multimodal emotion recognition method comprising three stages:",
          "Zhixian Zhao, Haifeng Chen, Xi Li, Dongmei Jiang, & Lei Xie": "more robust acoustic representations. We generate masked features"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "acoustic feature adaptation, visual feature alignment, and multi-",
          "Zhixian Zhao, Haifeng Chen, Xi Li, Dongmei Jiang, & Lei Xie": "ğ‘“ ğ‘šğ‘ğ‘ ğ‘˜ğ‘’ğ‘‘\nâˆˆ Rda\nthrough a multi-layer perceptron (MLP) consisting"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Zhixian Zhao, Haifeng Chen, Xi Li, Dongmei Jiang, & Lei Xie": "ğ‘"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "modal feature fusion. The main flow is shown in Figure 1. First, to",
          "Zhixian Zhao, Haifeng Chen, Xi Li, Dongmei Jiang, & Lei Xie": "of two fully connected layers and a ReLU layer, and measure the"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "optimize acoustic modality features for emotion recognition, we",
          "Zhixian Zhao, Haifeng Chen, Xi Li, Dongmei Jiang, & Lei Xie": "difference between the original and masked features using the mean"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "conducted an empirical study exploring the performance of differ-",
          "Zhixian Zhao, Haifeng Chen, Xi Li, Dongmei Jiang, & Lei Xie": "squared error (MSE) loss function. The second loss function is a"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "ent layers of the HuBERT-large model [9] and the effectiveness",
          "Zhixian Zhao, Haifeng Chen, Xi Li, Dongmei Jiang, & Lei Xie": "supervised cross-entropy (CE) loss. After passing the masked fea-"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "of multi-layer feature fusion. Guided by these empirical findings,",
          "Zhixian Zhao, Haifeng Chen, Xi Li, Dongmei Jiang, & Lei Xie": "tures through a fully connected layer to obtain the predicted results"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "we propose a simple and effective parameter-efficient fine-tuning",
          "Zhixian Zhao, Haifeng Chen, Xi Li, Dongmei Jiang, & Lei Xie": "ğ‘¥ğ‘ğ‘Ÿğ‘’ğ‘‘ , we use the CE loss function to calculate the loss between"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "method. This method enhances recognition performance by incor-",
          "Zhixian Zhao, Haifeng Chen, Xi Li, Dongmei Jiang, & Lei Xie": "the predicted results and the labels ğ‘¦ğ‘™ğ‘ğ‘ğ‘’ğ‘™ . These two loss functions"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "porating adapters into well-performing intermediate transformer",
          "Zhixian Zhao, Haifeng Chen, Xi Li, Dongmei Jiang, & Lei Xie": "can be expressed as:"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "layers and dynamically fusing hidden representations across these",
          "Zhixian Zhao, Haifeng Chen, Xi Li, Dongmei Jiang, & Lei Xie": ""
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Zhixian Zhao, Haifeng Chen, Xi Li, Dongmei Jiang, & Lei Xie": "Lğ‘ğ‘’ = ğ¶ğ¸ (ğ‘¥ğ‘ğ‘Ÿğ‘’ğ‘‘, ğ‘¦ğ‘™ğ‘ğ‘ğ‘’ğ‘™ ),\n(2)"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "layers using learnable weights. Second, to enhance the emotional",
          "Zhixian Zhao, Haifeng Chen, Xi Li, Dongmei Jiang, & Lei Xie": ""
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "representation capability of visual modality features, we perform",
          "Zhixian Zhao, Haifeng Chen, Xi Li, Dongmei Jiang, & Lei Xie": ", ğ‘“ ğ‘“ ğ‘¢ğ‘ ğ‘–ğ‘œğ‘›\n),\nLğ‘šğ‘™ğ‘š = ğ‘€ğ‘†ğ¸ (ğ‘“ ğ‘šğ‘ğ‘ ğ‘˜ğ‘’ğ‘‘\nğ‘\n(3)"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "contrastive learning between the fine-tuned acoustic features and",
          "Zhixian Zhao, Haifeng Chen, Xi Li, Dongmei Jiang, & Lei Xie": ""
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Zhixian Zhao, Haifeng Chen, Xi Li, Dongmei Jiang, & Lei Xie": "Thus, the overall objective function for fine-tuning the model\nis"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "visual features processed by a multilayer perceptron (MLP). We",
          "Zhixian Zhao, Haifeng Chen, Xi Li, Dongmei Jiang, & Lei Xie": ""
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Zhixian Zhao, Haifeng Chen, Xi Li, Dongmei Jiang, & Lei Xie": "defined as:"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "leverage a substantial amount of unlabeled visual and audio data",
          "Zhixian Zhao, Haifeng Chen, Xi Li, Dongmei Jiang, & Lei Xie": ""
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Zhixian Zhao, Haifeng Chen, Xi Li, Dongmei Jiang, & Lei Xie": "L = Lğ‘ğ‘’ + Lğ‘šğ‘™ğ‘š .\n(4)"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "to pretrain the vision MLP in an unsupervised manner, ensuring",
          "Zhixian Zhao, Haifeng Chen, Xi Li, Dongmei Jiang, & Lei Xie": ""
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "the visual features adapt to the acoustic feature space. Finally, we",
          "Zhixian Zhao, Haifeng Chen, Xi Li, Dongmei Jiang, & Lei Xie": "2.2\nVisual Feature Alignment"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "employ an attention-based feature fusion module to integrate the",
          "Zhixian Zhao, Haifeng Chen, Xi Li, Dongmei Jiang, & Lei Xie": "Due to the semantic disparity between the visual features extracted"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "acoustic, visual, and textual features, achieving a weighted F1 score",
          "Zhixian Zhao, Haifeng Chen, Xi Li, Dongmei Jiang, & Lei Xie": "by CLIP-large [17] and the audio features, we perform pre-training"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "of 88.90% on the test set.",
          "Zhixian Zhao, Haifeng Chen, Xi Li, Dongmei Jiang, & Lei Xie": "feature alignment for the visual modality prior to multimodal fea-"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Zhixian Zhao, Haifeng Chen, Xi Li, Dongmei Jiang, & Lei Xie": "ture fusion, as illustrated in Figure 1(Stage 2). Inspired by [15], we"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Zhixian Zhao, Haifeng Chen, Xi Li, Dongmei Jiang, & Lei Xie": "first pre-train a vision MLP to align visual representations with the"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "2\nMethod",
          "Zhixian Zhao, Haifeng Chen, Xi Li, Dongmei Jiang, & Lei Xie": ""
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Zhixian Zhao, Haifeng Chen, Xi Li, Dongmei Jiang, & Lei Xie": "semantic space of pre-trained speech emotion representations. Dur-"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "2.1\nAcoustic Feature Adaptation",
          "Zhixian Zhao, Haifeng Chen, Xi Li, Dongmei Jiang, & Lei Xie": ""
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Zhixian Zhao, Haifeng Chen, Xi Li, Dongmei Jiang, & Lei Xie": "ing this process, the weights of the CLIP-large model and the fine-"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "Based on the findings of\nthe baseline study [14], we select\nthe",
          "Zhixian Zhao, Haifeng Chen, Xi Li, Dongmei Jiang, & Lei Xie": "tuned HuBERT-large model are kept frozen, and only the weights"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "HuBERT-large model [9], which has demonstrated superior perfor-",
          "Zhixian Zhao, Haifeng Chen, Xi Li, Dongmei Jiang, & Lei Xie": "of the vision MLP are trained."
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "mance in emotion recognition tasks, as the feature extractor. Since",
          "Zhixian Zhao, Haifeng Chen, Xi Li, Dongmei Jiang, & Lei Xie": "We introduce an video-audio contrastive learning method. An"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "pre-trained transformer models (e.g., HuBERT and wav2vec2.0",
          "Zhixian Zhao, Haifeng Chen, Xi Li, Dongmei Jiang, & Lei Xie": "input video ğ¼\nis encoded by the CLIP-large model and then average-"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "[2]) capture unique hidden representations across different layers",
          "Zhixian Zhao, Haifeng Chen, Xi Li, Dongmei Jiang, & Lei Xie": ""
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Zhixian Zhao, Haifeng Chen, Xi Li, Dongmei Jiang, & Lei Xie": "pooled to obtain ğ‘“ Ë†ğ‘£ âˆˆ Rdv . Then, an MLP maps the visual embedding"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "during audio processing and that these layers may contain com-",
          "Zhixian Zhao, Haifeng Chen, Xi Li, Dongmei Jiang, & Lei Xie": ""
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Zhixian Zhao, Haifeng Chen, Xi Li, Dongmei Jiang, & Lei Xie": "to the same dimensionality as the audio embedding, resulting in"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "plementary information, we utilize the temporal pooling features",
          "Zhixian Zhao, Haifeng Chen, Xi Li, Dongmei Jiang, & Lei Xie": ""
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Zhixian Zhao, Haifeng Chen, Xi Li, Dongmei Jiang, & Lei Xie": "mapped visual embedding ğ‘“ ğ‘£ âˆˆ Rda . This transformation can be"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "(ğ‘“ ğ‘–\nâˆˆ Rda, ğ‘– âˆˆ 1, 2, ..., ğ‘˜) from ğ‘˜ consecutive middle layers of the",
          "Zhixian Zhao, Haifeng Chen, Xi Li, Dongmei Jiang, & Lei Xie": ""
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Zhixian Zhao, Haifeng Chen, Xi Li, Dongmei Jiang, & Lei Xie": "represented as:"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "HuBERT-large model for the emotion recognition task.",
          "Zhixian Zhao, Haifeng Chen, Xi Li, Dongmei Jiang, & Lei Xie": ""
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Zhixian Zhao, Haifeng Chen, Xi Li, Dongmei Jiang, & Lei Xie": "(5)\nğ‘“ğ‘£ = ReLU(ğ‘Š1ReLU(ğ‘Š2 ğ‘“ Ë†ğ‘£ + ğ‘1) + ğ‘2),"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "To adapt these features more effectively to the emotion recog-",
          "Zhixian Zhao, Haifeng Chen, Xi Li, Dongmei Jiang, & Lei Xie": ""
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "nition task while maintaining the generalization capability of the",
          "Zhixian Zhao, Haifeng Chen, Xi Li, Dongmei Jiang, & Lei Xie": "where ğ‘Š1 âˆˆ Rğ‘‘ğ‘£ Ã—da and ğ‘Š2 âˆˆ Rda Ã—da , ğ‘1 âˆˆ Rda and ğ‘2 âˆˆ Rda are"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "pre-trained model, we introduce adapters in these ğ‘˜ transformer",
          "Zhixian Zhao, Haifeng Chen, Xi Li, Dongmei Jiang, & Lei Xie": "trainable parameters of the vision MLP. The goal of the MLP is to"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "layers for efficient parameter fine-tuning. The implementation de-",
          "Zhixian Zhao, Haifeng Chen, Xi Li, Dongmei Jiang, & Lei Xie": "align the visual and audio features so they can operate within a"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "tails of the adapters are depicted in Figure 1(Stage 1). Each adapter",
          "Zhixian Zhao, Haifeng Chen, Xi Li, Dongmei Jiang, & Lei Xie": "unified feature space."
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "consists of a bottleneck structure, including a dimension reduction",
          "Zhixian Zhao, Haifeng Chen, Xi Li, Dongmei Jiang, & Lei Xie": "To compute the similarity between the visual and audio embed-"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "projection layer that reduces the hidden dimension from da to Ë†d,",
          "Zhixian Zhao, Haifeng Chen, Xi Li, Dongmei Jiang, & Lei Xie": "dings, we first use the fine-tuned HuBERT-large model to convert"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "followed by a ReLU non-linear activation function, and then an",
          "Zhixian Zhao, Haifeng Chen, Xi Li, Dongmei Jiang, & Lei Xie": "the input audio ğ´ into an embedding sequence ğ‘“ğ‘ âˆˆ Rda . The video-"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "up-projection layer that restores the dimension back to da. Given",
          "Zhixian Zhao, Haifeng Chen, Xi Li, Dongmei Jiang, & Lei Xie": "audio similarity can be defined as: ğ‘  (ğ¼, ğ´) = ğ‘“ âŠ¤\nğ‘“ğ‘/âˆ¥ğ‘“ğ‘£ âˆ¥ Â· âˆ¥ğ‘“ğ‘ âˆ¥ and"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "an input feature ğ‘¥, the output ğ‘¦ of the adapter can be represented",
          "Zhixian Zhao, Haifeng Chen, Xi Li, Dongmei Jiang, & Lei Xie": "the audio-video similarity as: ğ‘  (ğ´, ğ¼ ) = ğ‘“ âŠ¤\nğ‘“ğ‘£/âˆ¥ğ‘“ğ‘ âˆ¥ Â· âˆ¥ ğ‘“ğ‘£ âˆ¥. For each"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Train&Val\n5030\n0\n05:56:39": ""
        },
        {
          "Train&Val\n5030\n0\n05:56:39": ""
        },
        {
          "Train&Val\n5030\n0\n05:56:39": "MER-SEMI\n0\n1169/115595\n100:38:49"
        },
        {
          "Train&Val\n5030\n0\n05:56:39": ""
        },
        {
          "Train&Val\n5030\n0\n05:56:39": "MER-NOISE\n0\n1170/115595\n100:38:49"
        },
        {
          "Train&Val\n5030\n0\n05:56:39": ""
        },
        {
          "Train&Val\n5030\n0\n05:56:39": "3\nExperiments and Results"
        },
        {
          "Train&Val\n5030\n0\n05:56:39": ""
        },
        {
          "Train&Val\n5030\n0\n05:56:39": "3.1\nDataset and Implementation Details"
        },
        {
          "Train&Val\n5030\n0\n05:56:39": ""
        },
        {
          "Train&Val\n5030\n0\n05:56:39": "Dataset: We conduct experiments using the MER-SEMI dataset,"
        },
        {
          "Train&Val\n5030\n0\n05:56:39": ""
        },
        {
          "Train&Val\n5030\n0\n05:56:39": "as detailed in Table 1. The Train&Val set consists of 5,030 video"
        },
        {
          "Train&Val\n5030\n0\n05:56:39": ""
        },
        {
          "Train&Val\n5030\n0\n05:56:39": "clips with both discrete and dimensional emotion labels. Given"
        },
        {
          "Train&Val\n5030\n0\n05:56:39": ""
        },
        {
          "Train&Val\n5030\n0\n05:56:39": "the absence of a predefined training/validation split, we utilize"
        },
        {
          "Train&Val\n5030\n0\n05:56:39": ""
        },
        {
          "Train&Val\n5030\n0\n05:56:39": "five-fold cross-validation on the Train&Val set [14]. To evaluate"
        },
        {
          "Train&Val\n5030\n0\n05:56:39": ""
        },
        {
          "Train&Val\n5030\n0\n05:56:39": "model generalization, the MER-SEMI track includes 1,169 unlabeled"
        },
        {
          "Train&Val\n5030\n0\n05:56:39": ""
        },
        {
          "Train&Val\n5030\n0\n05:56:39": "video clips in a test set, drawn from a total of 115,595 unlabeled"
        },
        {
          "Train&Val\n5030\n0\n05:56:39": "data. Participants must predict discrete emotion labels across all"
        },
        {
          "Train&Val\n5030\n0\n05:56:39": "unlabeled data, not just the test set. The set of discrete emotion"
        },
        {
          "Train&Val\n5030\n0\n05:56:39": "labels includes six categories: neutral, anger, happiness, sadness,"
        },
        {
          "Train&Val\n5030\n0\n05:56:39": "worry, and surprise. We use a weighted average F1 score as the"
        },
        {
          "Train&Val\n5030\n0\n05:56:39": "evaluation metric, aligning with the official baseline."
        },
        {
          "Train&Val\n5030\n0\n05:56:39": "Implementation Details: For acoustic features adaptation, we"
        },
        {
          "Train&Val\n5030\n0\n05:56:39": "select the 16th to 21st layers (total ğ‘˜ = 6 layers) of the HuBERT-large"
        },
        {
          "Train&Val\n5030\n0\n05:56:39": "model and incorporate adapters into these layers for fine-tuning"
        },
        {
          "Train&Val\n5030\n0\n05:56:39": "(refer to Section 3.2.1). The feature dimension Ë†d within the Adapter"
        },
        {
          "Train&Val\n5030\n0\n05:56:39": "is set to 128. During fused feature computation, we assign an initial"
        },
        {
          "Train&Val\n5030\n0\n05:56:39": "weight of 1.0 to the 18th layer for its optimal performance, while the"
        },
        {
          "Train&Val\n5030\n0\n05:56:39": "other layers are initially weighted at 0.0. Pre-training is conducted"
        },
        {
          "Train&Val\n5030\n0\n05:56:39": "on the Train&Val set with a batch size of 16 and a learning rate of 1e-"
        },
        {
          "Train&Val\n5030\n0\n05:56:39": "4. We use the Adam optimizer with a weight decay of 0.02. For visual"
        },
        {
          "Train&Val\n5030\n0\n05:56:39": "feature alignment, the vision MLP is trained on unlabeled data with"
        },
        {
          "Train&Val\n5030\n0\n05:56:39": "a batch size of 1024 and a learning rate of 1e-4. For the feature"
        },
        {
          "Train&Val\n5030\n0\n05:56:39": "fusion module, features from the three modalities are mapped to"
        },
        {
          "Train&Val\n5030\n0\n05:56:39": "256 dimensions through an MLP for fusion, with a learning rate set"
        },
        {
          "Train&Val\n5030\n0\n05:56:39": ""
        },
        {
          "Train&Val\n5030\n0\n05:56:39": "to 1e-4. The dimensions of the visual, audio, and lexical features"
        },
        {
          "Train&Val\n5030\n0\n05:56:39": ""
        },
        {
          "Train&Val\n5030\n0\n05:56:39": "are dv = 768, da = 1024, and dl = 5120, respectively."
        },
        {
          "Train&Val\n5030\n0\n05:56:39": ""
        },
        {
          "Train&Val\n5030\n0\n05:56:39": "3.2\nResult and Discussion"
        },
        {
          "Train&Val\n5030\n0\n05:56:39": ""
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Zhixian Zhao, Haifeng Chen, Xi Li, Dongmei Jiang, & Lei Xie": "output from the ğ‘–-th layer after fine-tuning the HuBERT-large model"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Zhixian Zhao, Haifeng Chen, Xi Li, Dongmei Jiang, & Lei Xie": "with adapters. As shown in the table, the performance of the fine-"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Zhixian Zhao, Haifeng Chen, Xi Li, Dongmei Jiang, & Lei Xie": "tuned features demonstrates a notable improvement over the base-"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Zhixian Zhao, Haifeng Chen, Xi Li, Dongmei Jiang, & Lei Xie": "line model. The proposed parameter-efficient fine-tuning method"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Zhixian Zhao, Haifeng Chen, Xi Li, Dongmei Jiang, & Lei Xie": "achieves superior performance with multi-layer fused features com-"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Zhixian Zhao, Haifeng Chen, Xi Li, Dongmei Jiang, & Lei Xie": "pared to single-layer features. Specifically, HL(16-21) surpasses"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Zhixian Zhao, Haifeng Chen, Xi Li, Dongmei Jiang, & Lei Xie": "HL(18), and HLFT(16-21) outperforms HLFT(18), suggesting that"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Zhixian Zhao, Haifeng Chen, Xi Li, Dongmei Jiang, & Lei Xie": "complementary information exists among features from different"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Zhixian Zhao, Haifeng Chen, Xi Li, Dongmei Jiang, & Lei Xie": "layers, resulting in more robust results. Moreover, the multi-layer"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Zhixian Zhao, Haifeng Chen, Xi Li, Dongmei Jiang, & Lei Xie": "fused features obtained through the proposed parameter-efficient"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Zhixian Zhao, Haifeng Chen, Xi Li, Dongmei Jiang, & Lei Xie": "fine-tuning method achieve the highest F1 score of 84.88% on the"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Zhixian Zhao, Haifeng Chen, Xi Li, Dongmei Jiang, & Lei Xie": "test set. This method also demonstrates performance improvements"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Zhixian Zhao, Haifeng Chen, Xi Li, Dongmei Jiang, & Lei Xie": "of 7.42% on the Train&Val set and 1.39% on the test set compared"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Zhixian Zhao, Haifeng Chen, Xi Li, Dongmei Jiang, & Lei Xie": "to the baseline model, validating its effectiveness."
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "Figure 2: Comparison of the performance of features from",
          "Zhixian Zhao, Haifeng Chen, Xi Li, Dongmei Jiang, & Lei Xie": ""
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Zhixian Zhao, Haifeng Chen, Xi Li, Dongmei Jiang, & Lei Xie": "For the visual modality, CLIPL represents the features extracted"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "different layers of HuBERT-large.",
          "Zhixian Zhao, Haifeng Chen, Xi Li, Dongmei Jiang, & Lei Xie": ""
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Zhixian Zhao, Haifeng Chen, Xi Li, Dongmei Jiang, & Lei Xie": "using the CLIP-Large model, whereas CLIPL-A denotes the features"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "Table 2: Evaluation of Unimodal and Multimodal Feature",
          "Zhixian Zhao, Haifeng Chen, Xi Li, Dongmei Jiang, & Lei Xie": "aligned through the proposed visual feature alignment strategy. As"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "Performance.",
          "Zhixian Zhao, Haifeng Chen, Xi Li, Dongmei Jiang, & Lei Xie": "shown in Table 2, in comparison to the CLIPL features, the CLIPL-A"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Zhixian Zhao, Haifeng Chen, Xi Li, Dongmei Jiang, & Lei Xie": "features exhibit comparable performance on the Train&Val set and"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "Features\nTrain&Val\nMER-SEMI",
          "Zhixian Zhao, Haifeng Chen, Xi Li, Dongmei Jiang, & Lei Xie": "show a 3.64% improvement on the test set. These results affirm"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "A\nV\nT\nF1-Score(â†‘)\nF1-Score(â†‘)",
          "Zhixian Zhao, Haifeng Chen, Xi Li, Dongmei Jiang, & Lei Xie": "the efficacy of the visual feature alignment strategy in enhancing"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "Unimodal Results",
          "Zhixian Zhao, Haifeng Chen, Xi Li, Dongmei Jiang, & Lei Xie": "performance within multimodal emotion recognition tasks."
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "HL\n-\n-\n72.82Â±0.30\n83.49",
          "Zhixian Zhao, Haifeng Chen, Xi Li, Dongmei Jiang, & Lei Xie": ""
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Zhixian Zhao, Haifeng Chen, Xi Li, Dongmei Jiang, & Lei Xie": "3.2.3\nMultimodal Recognition Results. We conduct a comprehen-"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "HL(18)\n-\n-\n74.40Â±0.46\n84.78",
          "Zhixian Zhao, Haifeng Chen, Xi Li, Dongmei Jiang, & Lei Xie": ""
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Zhixian Zhao, Haifeng Chen, Xi Li, Dongmei Jiang, & Lei Xie": "sive comparison of\nthe multimodal\nfusion effects, with specific"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "HL(16-21)\n-\n-\n73.98Â±0.43\n84.79",
          "Zhixian Zhao, Haifeng Chen, Xi Li, Dongmei Jiang, & Lei Xie": ""
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Zhixian Zhao, Haifeng Chen, Xi Li, Dongmei Jiang, & Lei Xie": "results presented in Table 2. Initially, we fuse the best-performing"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "HLFT(18)\n-\n-\n76.30Â±0.33\n84.69",
          "Zhixian Zhao, Haifeng Chen, Xi Li, Dongmei Jiang, & Lei Xie": ""
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Zhixian Zhao, Haifeng Chen, Xi Li, Dongmei Jiang, & Lei Xie": "acoustic modality features, HLFT (16-21), with the visual modality"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "HLFT(16-21)\n-\n-\n80.24Â±0.21\n84.88",
          "Zhixian Zhao, Haifeng Chen, Xi Li, Dongmei Jiang, & Lei Xie": ""
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "66.22Â±0.43\n60.95\n-\nCLIPL\n-",
          "Zhixian Zhao, Haifeng Chen, Xi Li, Dongmei Jiang, & Lei Xie": "features extracted by CLIP-large. This fusion improves the recogni-"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "-\nCLIPL-A\n-\n66.05Â±0.37\n64.59",
          "Zhixian Zhao, Haifeng Chen, Xi Li, Dongmei Jiang, & Lei Xie": "tion accuracy from 84.88% to 86.78%. Furthermore, using the aligned"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "Multimodal Results",
          "Zhixian Zhao, Haifeng Chen, Xi Li, Dongmei Jiang, & Lei Xie": "features extracted by the pre-trained vision MLP, the recognition"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "HLFT(16-21)\nCLIPL\n-\n84.34Â±0.25\n86.78",
          "Zhixian Zhao, Haifeng Chen, Xi Li, Dongmei Jiang, & Lei Xie": "accuracy further increases to 87.01%, which provides additional val-"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "HLFT(16-21)\nCLIPL-A\n-\n84.70Â±0.19\n87.01",
          "Zhixian Zhao, Haifeng Chen, Xi Li, Dongmei Jiang, & Lei Xie": ""
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Zhixian Zhao, Haifeng Chen, Xi Li, Dongmei Jiang, & Lei Xie": "idation for the effectiveness of the feature alignment pre-training"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "HLFT(16-21)\n-\nBaichuan2\n79.66Â±0.13\n85.78",
          "Zhixian Zhao, Haifeng Chen, Xi Li, Dongmei Jiang, & Lei Xie": ""
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Zhixian Zhao, Haifeng Chen, Xi Li, Dongmei Jiang, & Lei Xie": "method. Similarly, we evaluate the fusion of lexical modality and"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "HLFT(16-21)\nCLIPL-A\nBaichuan2\n83.85Â±0.35\n88.90",
          "Zhixian Zhao, Haifeng Chen, Xi Li, Dongmei Jiang, & Lei Xie": ""
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Zhixian Zhao, Haifeng Chen, Xi Li, Dongmei Jiang, & Lei Xie": "acoustic modality. When fusing Baichuan2 features with HLFT"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Zhixian Zhao, Haifeng Chen, Xi Li, Dongmei Jiang, & Lei Xie": "(16-21) features, the performance on the test set reaches 85.78%."
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "3.2.1\nEmpirical study. To validate the performance of features from",
          "Zhixian Zhao, Haifeng Chen, Xi Li, Dongmei Jiang, & Lei Xie": ""
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Zhixian Zhao, Haifeng Chen, Xi Li, Dongmei Jiang, & Lei Xie": "Ultimately, the fusion of all three modality features results in the"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "different transformer layers in emotion recognition tasks, we ex-",
          "Zhixian Zhao, Haifeng Chen, Xi Li, Dongmei Jiang, & Lei Xie": ""
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Zhixian Zhao, Haifeng Chen, Xi Li, Dongmei Jiang, & Lei Xie": "highest performance of 88.90% on the test set. Additionally, the"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "tract the features from the last 10 layers of the HuBERT-large model",
          "Zhixian Zhao, Haifeng Chen, Xi Li, Dongmei Jiang, & Lei Xie": ""
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Zhixian Zhao, Haifeng Chen, Xi Li, Dongmei Jiang, & Lei Xie": "table illustrates that the effect of multimodal fusion on the test set"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "and evaluate their performance on the Train&Val set, as shown",
          "Zhixian Zhao, Haifeng Chen, Xi Li, Dongmei Jiang, & Lei Xie": ""
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Zhixian Zhao, Haifeng Chen, Xi Li, Dongmei Jiang, & Lei Xie": "surpasses that of any single modality."
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "in Figure 2. The rationale for focusing on the last 10 layers is that",
          "Zhixian Zhao, Haifeng Chen, Xi Li, Dongmei Jiang, & Lei Xie": ""
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "shallow layers typically encode low-level features, such as pitch and",
          "Zhixian Zhao, Haifeng Chen, Xi Li, Dongmei Jiang, & Lei Xie": ""
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Zhixian Zhao, Haifeng Chen, Xi Li, Dongmei Jiang, & Lei Xie": "4\nConclusion"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "short-term energy fluctuations, while deeper layers are more adept",
          "Zhixian Zhao, Haifeng Chen, Xi Li, Dongmei Jiang, & Lei Xie": ""
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Zhixian Zhao, Haifeng Chen, Xi Li, Dongmei Jiang, & Lei Xie": "In this study, we propose a multimodal emotion recognition frame-"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "at capturing high-level features and global semantic information,",
          "Zhixian Zhao, Haifeng Chen, Xi Li, Dongmei Jiang, & Lei Xie": ""
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Zhixian Zhao, Haifeng Chen, Xi Li, Dongmei Jiang, & Lei Xie": "work for the MER2024-SEMI challenge. Initially, our focus is on fully"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "which are crucial for emotion recognition. As shown in the figure,",
          "Zhixian Zhao, Haifeng Chen, Xi Li, Dongmei Jiang, & Lei Xie": ""
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Zhixian Zhao, Haifeng Chen, Xi Li, Dongmei Jiang, & Lei Xie": "leveraging acoustic modality features to enhance emotion recogni-"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "features from the 18th layer demonstrate the best performance, sig-",
          "Zhixian Zhao, Haifeng Chen, Xi Li, Dongmei Jiang, & Lei Xie": ""
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Zhixian Zhao, Haifeng Chen, Xi Li, Dongmei Jiang, & Lei Xie": "tion tasks. We evaluate the performance of different transformer"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "nificantly surpassing the baseline score, indicating that this layer",
          "Zhixian Zhao, Haifeng Chen, Xi Li, Dongmei Jiang, & Lei Xie": ""
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Zhixian Zhao, Haifeng Chen, Xi Li, Dongmei Jiang, & Lei Xie": "layers of the HuBERT-large model in speech emotion recognition"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "captures more representative emotional features. Furthermore, fea-",
          "Zhixian Zhao, Haifeng Chen, Xi Li, Dongmei Jiang, & Lei Xie": ""
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Zhixian Zhao, Haifeng Chen, Xi Li, Dongmei Jiang, & Lei Xie": "and employ an PEFT method to fine-tune the HuBERT-large model."
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "tures from the 16th through 21st layers consistently outperform the",
          "Zhixian Zhao, Haifeng Chen, Xi Li, Dongmei Jiang, & Lei Xie": ""
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Zhixian Zhao, Haifeng Chen, Xi Li, Dongmei Jiang, & Lei Xie": "Subsequently, to enhance the emotional representation of the visual"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "baseline, indicating that these intermediate layers are more suitable",
          "Zhixian Zhao, Haifeng Chen, Xi Li, Dongmei Jiang, & Lei Xie": ""
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Zhixian Zhao, Haifeng Chen, Xi Li, Dongmei Jiang, & Lei Xie": "modality, we introduce an unsupervised feature alignment scheme"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "for emotion recognition tasks. These layers balance capturing both",
          "Zhixian Zhao, Haifeng Chen, Xi Li, Dongmei Jiang, & Lei Xie": ""
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Zhixian Zhao, Haifeng Chen, Xi Li, Dongmei Jiang, & Lei Xie": "that employs contrastive learning to align visual embeddings with"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "low-level and high-level information, providing rich audio content",
          "Zhixian Zhao, Haifeng Chen, Xi Li, Dongmei Jiang, & Lei Xie": ""
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Zhixian Zhao, Haifeng Chen, Xi Li, Dongmei Jiang, & Lei Xie": "acoustic embeddings. Experimental results validate the effective-"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "while minimizing noise interference. This finding is of significant",
          "Zhixian Zhao, Haifeng Chen, Xi Li, Dongmei Jiang, & Lei Xie": ""
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Zhixian Zhao, Haifeng Chen, Xi Li, Dongmei Jiang, & Lei Xie": "ness of the proposed methods, with our approach securing fourth"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "importance for subsequent model fine-tuning efforts.",
          "Zhixian Zhao, Haifeng Chen, Xi Li, Dongmei Jiang, & Lei Xie": ""
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Zhixian Zhao, Haifeng Chen, Xi Li, Dongmei Jiang, & Lei Xie": "place in the MER2024-SEMI sub-challenge."
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "3.2.2\nUnimodal Recognition Results. We present the experimental",
          "Zhixian Zhao, Haifeng Chen, Xi Li, Dongmei Jiang, & Lei Xie": ""
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Zhixian Zhao, Haifeng Chen, Xi Li, Dongmei Jiang, & Lei Xie": "5\nAcknowledgments"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "results for both unimodal and multimodal approaches in Table 2. For",
          "Zhixian Zhao, Haifeng Chen, Xi Li, Dongmei Jiang, & Lei Xie": ""
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "the acoustic modality, HL represents features extracted using the",
          "Zhixian Zhao, Haifeng Chen, Xi Li, Dongmei Jiang, & Lei Xie": "This work is supported by the National Natural Science Foundation"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "baseline method [14] from the HuBERT-large model, with HL(ğ‘–) in-",
          "Zhixian Zhao, Haifeng Chen, Xi Li, Dongmei Jiang, & Lei Xie": "of China (grant 62236006, grant 62306172), the Key Research and"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "dicating the feature output from the ğ‘–-th layer. HLFT(ğ‘–) indicates the",
          "Zhixian Zhao, Haifeng Chen, Xi Li, Dongmei Jiang, & Lei Xie": "Development Program of Shaanxi (No. 2022ZDLGY06-03)."
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Improving Multimodal Emotion Recognition by Leveraging Acoustic Adaptation and Visual Alignment\nMRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "[13] Zheng Lian, Haiyang Sun, Licai Sun, Kang Chen, Mngyu Xu, Kexin Wang, Ke\nReferences"
        },
        {
          "Improving Multimodal Emotion Recognition by Leveraging Acoustic Adaptation and Visual Alignment\nMRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "Xu, Yu He, Ying Li, Jinming Zhao, Ye Liu, Bin Liu, Jiangyan Yi, Meng Wang, Erik"
        },
        {
          "Improving Multimodal Emotion Recognition by Leveraging Acoustic Adaptation and Visual Alignment\nMRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "[1] Alexei Baevski, Arun Babu, Wei-Ning Hsu, and Michael Auli. 2023. Efficient"
        },
        {
          "Improving Multimodal Emotion Recognition by Leveraging Acoustic Adaptation and Visual Alignment\nMRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "Cambria, Guoying Zhao, BjÃ¶rn W. Schuller, and Jianhua Tao. 2023. MER 2023:"
        },
        {
          "Improving Multimodal Emotion Recognition by Leveraging Acoustic Adaptation and Visual Alignment\nMRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "Self-supervised Learning with Contextualized Target Representations for Vision,"
        },
        {
          "Improving Multimodal Emotion Recognition by Leveraging Acoustic Adaptation and Visual Alignment\nMRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "Multi-label Learning, Modality Robustness, and Semi-Supervised Learning. In"
        },
        {
          "Improving Multimodal Emotion Recognition by Leveraging Acoustic Adaptation and Visual Alignment\nMRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "Speech and Language.\nIn Proceedings of\nthe 40th International Conference on"
        },
        {
          "Improving Multimodal Emotion Recognition by Leveraging Acoustic Adaptation and Visual Alignment\nMRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "Proceedings of the 31st ACM International Conference on Multimedia (MM â€™23)."
        },
        {
          "Improving Multimodal Emotion Recognition by Leveraging Acoustic Adaptation and Visual Alignment\nMRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "Machine Learning. 1416â€“1429."
        },
        {
          "Improving Multimodal Emotion Recognition by Leveraging Acoustic Adaptation and Visual Alignment\nMRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "9610â€“9614.\nhttps://doi.org/10.1145/3581783.3612836"
        },
        {
          "Improving Multimodal Emotion Recognition by Leveraging Acoustic Adaptation and Visual Alignment\nMRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "[2] Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed, and Michael Auli. 2020."
        },
        {
          "Improving Multimodal Emotion Recognition by Leveraging Acoustic Adaptation and Visual Alignment\nMRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "[14] Zheng Lian, Haiyang Sun, Licai Sun, Zhuofan Wen, Siyuan Zhang, Shun Chen,"
        },
        {
          "Improving Multimodal Emotion Recognition by Leveraging Acoustic Adaptation and Visual Alignment\nMRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representa-"
        },
        {
          "Improving Multimodal Emotion Recognition by Leveraging Acoustic Adaptation and Visual Alignment\nMRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "Hao Gu, Jinming Zhao, Ziyang Ma, Xie Chen, Jiangyan Yi, Rui Liu, Kele Xu,"
        },
        {
          "Improving Multimodal Emotion Recognition by Leveraging Acoustic Adaptation and Visual Alignment\nMRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "tions. In Advances in Neural Information Processing Systems, Vol. 33. 12449â€“12460."
        },
        {
          "Improving Multimodal Emotion Recognition by Leveraging Acoustic Adaptation and Visual Alignment\nMRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "Bin Liu, Erik Cambria, Guoying Zhao, BjÃ¶rn W. Schuller, and Jianhua Tao. 2024."
        },
        {
          "Improving Multimodal Emotion Recognition by Leveraging Acoustic Adaptation and Visual Alignment\nMRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "[3] Tadas Baltrusaitis, Amir Zadeh, Yao Chong Lim, and Louis-Philippe Morency."
        },
        {
          "Improving Multimodal Emotion Recognition by Leveraging Acoustic Adaptation and Visual Alignment\nMRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "MER 2024: Semi-Supervised Learning, Noise Robustness, and Open-Vocabulary"
        },
        {
          "Improving Multimodal Emotion Recognition by Leveraging Acoustic Adaptation and Visual Alignment\nMRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "2018. OpenFace 2.0: Facial Behavior Analysis Toolkit. In 2018 13th IEEE Inter-"
        },
        {
          "Improving Multimodal Emotion Recognition by Leveraging Acoustic Adaptation and Visual Alignment\nMRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "Multimodal Emotion Recognition.\nhttps://arxiv.org/abs/2404.17113"
        },
        {
          "Improving Multimodal Emotion Recognition by Leveraging Acoustic Adaptation and Visual Alignment\nMRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "national Conference on Automatic Face & Gesture Recognition (FG 2018). 59â€“66."
        },
        {
          "Improving Multimodal Emotion Recognition by Leveraging Acoustic Adaptation and Visual Alignment\nMRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "[15] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2023. Visual In-"
        },
        {
          "Improving Multimodal Emotion Recognition by Leveraging Acoustic Adaptation and Visual Alignment\nMRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "https://doi.org/10.1109/FG.2018.00019"
        },
        {
          "Improving Multimodal Emotion Recognition by Leveraging Acoustic Adaptation and Visual Alignment\nMRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "struction Tuning. In Advances in Neural Information Processing Systems, Vol. 36."
        },
        {
          "Improving Multimodal Emotion Recognition by Leveraging Acoustic Adaptation and Visual Alignment\nMRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "[4]\nFelipe Zago Canal, Tobias Rossi MÃ¼ller, Jhennifer Cristine Matias, Gustavo Gino"
        },
        {
          "Improving Multimodal Emotion Recognition by Leveraging Acoustic Adaptation and Visual Alignment\nMRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "34892â€“34916."
        },
        {
          "Improving Multimodal Emotion Recognition by Leveraging Acoustic Adaptation and Visual Alignment\nMRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "Scotton, Antonio Reis de Sa Junior, Eliane Pozzebon, and Antonio Carlos So-"
        },
        {
          "Improving Multimodal Emotion Recognition by Leveraging Acoustic Adaptation and Visual Alignment\nMRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "[16]\nYu Pan, Yanni Hu, Yuguang Yang, Wen Fei, Jixun Yao, Heng Lu, Lei Ma, and Jianjun"
        },
        {
          "Improving Multimodal Emotion Recognition by Leveraging Acoustic Adaptation and Visual Alignment\nMRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "bieranski. 2022. A survey on facial emotion recognition techniques: A state-"
        },
        {
          "Improving Multimodal Emotion Recognition by Leveraging Acoustic Adaptation and Visual Alignment\nMRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "Zhao. 2024. GEmo-CLAP: Gender-Attribute-Enhanced Contrastive Language-"
        },
        {
          "Improving Multimodal Emotion Recognition by Leveraging Acoustic Adaptation and Visual Alignment\nMRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "of-the-art literature review.\nhttps:\nInformation Sciences 582 (2022), 593â€“617."
        },
        {
          "Improving Multimodal Emotion Recognition by Leveraging Acoustic Adaptation and Visual Alignment\nMRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "Audio Pretraining for Accurate Speech Emotion Recognition. In ICASSP 2024"
        },
        {
          "Improving Multimodal Emotion Recognition by Leveraging Acoustic Adaptation and Visual Alignment\nMRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "//doi.org/10.1016/j.ins.2021.10.005"
        },
        {
          "Improving Multimodal Emotion Recognition by Leveraging Acoustic Adaptation and Visual Alignment\nMRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "- 2024 IEEE International Conference on Acoustics, Speech and Signal Processing"
        },
        {
          "Improving Multimodal Emotion Recognition by Leveraging Acoustic Adaptation and Visual Alignment\nMRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "[5] Benjamin Elizalde, Soham Deshmukh, Mahmoud Al Ismail, and Huaming Wang."
        },
        {
          "Improving Multimodal Emotion Recognition by Leveraging Acoustic Adaptation and Visual Alignment\nMRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "https://doi.org/10.1109/ICASSP48485.2024.10448394\n(ICASSP). 10021â€“10025."
        },
        {
          "Improving Multimodal Emotion Recognition by Leveraging Acoustic Adaptation and Visual Alignment\nMRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "2023. CLAP Learning Audio Concepts from Natural Language Supervision. In"
        },
        {
          "Improving Multimodal Emotion Recognition by Leveraging Acoustic Adaptation and Visual Alignment\nMRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "[17] Alec Radford,\nJong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh,"
        },
        {
          "Improving Multimodal Emotion Recognition by Leveraging Acoustic Adaptation and Visual Alignment\nMRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal"
        },
        {
          "Improving Multimodal Emotion Recognition by Leveraging Acoustic Adaptation and Visual Alignment\nMRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark,"
        },
        {
          "Improving Multimodal Emotion Recognition by Leveraging Acoustic Adaptation and Visual Alignment\nMRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "https://doi.org/10.1109/ICASSP49357.2023.10095889\nProcessing (ICASSP). 1â€“5."
        },
        {
          "Improving Multimodal Emotion Recognition by Leveraging Acoustic Adaptation and Visual Alignment\nMRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "Gretchen Krueger, and Ilya Sutskever. 2021. Learning Transferable Visual Models"
        },
        {
          "Improving Multimodal Emotion Recognition by Leveraging Acoustic Adaptation and Visual Alignment\nMRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "[6] Ruijia Fan, Hong Liu, Yidi Li, Peini Guo, Guoquan Wang, and Ti Wang. 2024. AttA-"
        },
        {
          "Improving Multimodal Emotion Recognition by Leveraging Acoustic Adaptation and Visual Alignment\nMRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "From Natural Language Supervision.\nIn Proceedings of\nthe 38th International"
        },
        {
          "Improving Multimodal Emotion Recognition by Leveraging Acoustic Adaptation and Visual Alignment\nMRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "NET: Attention Aggregation Network for Audio-Visual Emotion Recognition."
        },
        {
          "Improving Multimodal Emotion Recognition by Leveraging Acoustic Adaptation and Visual Alignment\nMRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "Conference on Machine Learning, Vol. 139. PMLR, 8748â€“8763."
        },
        {
          "Improving Multimodal Emotion Recognition by Leveraging Acoustic Adaptation and Visual Alignment\nMRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "In ICASSP 2024 - 2024 IEEE International Conference on Acoustics, Speech and"
        },
        {
          "Improving Multimodal Emotion Recognition by Leveraging Acoustic Adaptation and Visual Alignment\nMRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "[18] Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine Mcleavey, and"
        },
        {
          "Improving Multimodal Emotion Recognition by Leveraging Acoustic Adaptation and Visual Alignment\nMRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "https://doi.org/10.1109/ICASSP48485.\nSignal Processing (ICASSP). 8030â€“8034."
        },
        {
          "Improving Multimodal Emotion Recognition by Leveraging Acoustic Adaptation and Visual Alignment\nMRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "Ilya Sutskever. 2023. Robust Speech Recognition via Large-Scale Weak Super-"
        },
        {
          "Improving Multimodal Emotion Recognition by Leveraging Acoustic Adaptation and Visual Alignment\nMRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "2024.10447640"
        },
        {
          "Improving Multimodal Emotion Recognition by Leveraging Acoustic Adaptation and Visual Alignment\nMRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "vision. In Proceedings of the 40th International Conference on Machine Learning,"
        },
        {
          "Improving Multimodal Emotion Recognition by Leveraging Acoustic Adaptation and Visual Alignment\nMRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "Yuan Gao, Hao Shi, Chenhui Chu, and Tatsuya Kawahara. 2024. Enhancing Two-\n[7]"
        },
        {
          "Improving Multimodal Emotion Recognition by Leveraging Acoustic Adaptation and Visual Alignment\nMRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "Vol. 202. PMLR, 28492â€“28518."
        },
        {
          "Improving Multimodal Emotion Recognition by Leveraging Acoustic Adaptation and Visual Alignment\nMRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "Stage Finetuning for Speech Emotion Recognition Using Adapters. In ICASSP 2024"
        },
        {
          "Improving Multimodal Emotion Recognition by Leveraging Acoustic Adaptation and Visual Alignment\nMRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "[19] Hassan Sajjad, Fahim Dalvi, Nadir Durrani, and Preslav Nakov. 2023. On the"
        },
        {
          "Improving Multimodal Emotion Recognition by Leveraging Acoustic Adaptation and Visual Alignment\nMRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "- 2024 IEEE International Conference on Acoustics, Speech and Signal Processing"
        },
        {
          "Improving Multimodal Emotion Recognition by Leveraging Acoustic Adaptation and Visual Alignment\nMRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "effect of dropping layers of pre-trained transformer models. Computer Speech &"
        },
        {
          "Improving Multimodal Emotion Recognition by Leveraging Acoustic Adaptation and Visual Alignment\nMRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "https://doi.org/10.1109/ICASSP48485.2024.10446645\n(ICASSP). 11316â€“11320."
        },
        {
          "Improving Multimodal Emotion Recognition by Leveraging Acoustic Adaptation and Visual Alignment\nMRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "https://doi.org/10.1016/j.csl.2022.101429\nLanguage 77 (2023), 101429."
        },
        {
          "Improving Multimodal Emotion Recognition by Leveraging Acoustic Adaptation and Visual Alignment\nMRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "[8]\nErik Goron, Lena Asai, Elias Rut, and Martin Dinov. 2024.\nImproving Domain"
        },
        {
          "Improving Multimodal Emotion Recognition by Leveraging Acoustic Adaptation and Visual Alignment\nMRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "[20] Xin Sun, Xiangyu Ren, and Xiaohao Xie. 2024. A Novel Multimodal Sentiment"
        },
        {
          "Improving Multimodal Emotion Recognition by Leveraging Acoustic Adaptation and Visual Alignment\nMRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "Generalization in Speech Emotion Recognition with Whisper. In ICASSP 2024"
        },
        {
          "Improving Multimodal Emotion Recognition by Leveraging Acoustic Adaptation and Visual Alignment\nMRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "Analysis Model Based on Gated Fusion and Multi-Task Learning. In ICASSP 2024"
        },
        {
          "Improving Multimodal Emotion Recognition by Leveraging Acoustic Adaptation and Visual Alignment\nMRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "- 2024 IEEE International Conference on Acoustics, Speech and Signal Processing"
        },
        {
          "Improving Multimodal Emotion Recognition by Leveraging Acoustic Adaptation and Visual Alignment\nMRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "- 2024 IEEE International Conference on Acoustics, Speech and Signal Processing"
        },
        {
          "Improving Multimodal Emotion Recognition by Leveraging Acoustic Adaptation and Visual Alignment\nMRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "https://doi.org/10.1109/ICASSP48485.2024.10446997\n(ICASSP). 11631â€“11635."
        },
        {
          "Improving Multimodal Emotion Recognition by Leveraging Acoustic Adaptation and Visual Alignment\nMRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "https://doi.org/10.1109/ICASSP48485.2024.10446040\n(ICASSP). 8336â€“8340."
        },
        {
          "Improving Multimodal Emotion Recognition by Leveraging Acoustic Adaptation and Visual Alignment\nMRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "[9] Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia,"
        },
        {
          "Improving Multimodal Emotion Recognition by Leveraging Acoustic Adaptation and Visual Alignment\nMRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "Juan Terven, Diana-Margarita CÃ³rdova-Esparza, and Julio-Alejandro Romero-\n[21]"
        },
        {
          "Improving Multimodal Emotion Recognition by Leveraging Acoustic Adaptation and Visual Alignment\nMRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "Ruslan Salakhutdinov, and Abdelrahman Mohamed. 2021.\nHuBERT: Self-"
        },
        {
          "Improving Multimodal Emotion Recognition by Leveraging Acoustic Adaptation and Visual Alignment\nMRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "GonzÃ¡lez. 2023. A Comprehensive Review of YOLO Architectures in Computer"
        },
        {
          "Improving Multimodal Emotion Recognition by Leveraging Acoustic Adaptation and Visual Alignment\nMRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "Supervised Speech Representation Learning by Masked Prediction of Hidden"
        },
        {
          "Improving Multimodal Emotion Recognition by Leveraging Acoustic Adaptation and Visual Alignment\nMRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "Vision: From YOLOv1 to YOLOv8 and YOLO-NAS. Machine Learning and Knowl-"
        },
        {
          "Improving Multimodal Emotion Recognition by Leveraging Acoustic Adaptation and Visual Alignment\nMRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "Units.\nIEEE/ACM Transactions on Audio, Speech, and Language Processing 29"
        },
        {
          "Improving Multimodal Emotion Recognition by Leveraging Acoustic Adaptation and Visual Alignment\nMRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "https://doi.org/10.3390/make5040083\nedge Extraction 5, 4 (2023), 1680â€“1716."
        },
        {
          "Improving Multimodal Emotion Recognition by Leveraging Acoustic Adaptation and Visual Alignment\nMRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "(2021), 3451â€“3460.\nhttps://doi.org/10.1109/TASLP.2021.3122291"
        },
        {
          "Improving Multimodal Emotion Recognition by Leveraging Acoustic Adaptation and Visual Alignment\nMRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "[22] Yingzhi Wang, Abdelmoumene Boumadane, and Abdelwahab Heba. 2022. A"
        },
        {
          "Improving Multimodal Emotion Recognition by Leveraging Acoustic Adaptation and Visual Alignment\nMRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "[10] Qi Huang, Pingting Cai, Tanyue Nie, and Jinshan Zeng. 2024. CLIP-MSA: In-"
        },
        {
          "Improving Multimodal Emotion Recognition by Leveraging Acoustic Adaptation and Visual Alignment\nMRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "Fine-tuned Wav2vec 2.0/HuBERT Benchmark For Speech Emotion Recognition,"
        },
        {
          "Improving Multimodal Emotion Recognition by Leveraging Acoustic Adaptation and Visual Alignment\nMRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "corporating Inter-Modal Dynamics and Common Knowledge to Multimodal"
        },
        {
          "Improving Multimodal Emotion Recognition by Leveraging Acoustic Adaptation and Visual Alignment\nMRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "Speaker Verification and Spoken Language Understanding.\nhttps://arxiv.org/"
        },
        {
          "Improving Multimodal Emotion Recognition by Leveraging Acoustic Adaptation and Visual Alignment\nMRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "Sentiment Analysis With Clip.\nIn ICASSP 2024 - 2024 IEEE International Con-"
        },
        {
          "Improving Multimodal Emotion Recognition by Leveraging Acoustic Adaptation and Visual Alignment\nMRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "abs/2111.02735"
        },
        {
          "Improving Multimodal Emotion Recognition by Leveraging Acoustic Adaptation and Visual Alignment\nMRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "https:\nference on Acoustics, Speech and Signal Processing (ICASSP). 8145â€“8149."
        },
        {
          "Improving Multimodal Emotion Recognition by Leveraging Acoustic Adaptation and Visual Alignment\nMRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "[23]\nYusong Wu, Ke Chen, Tianyu Zhang, Yuchen Hui, Taylor Berg-Kirkpatrick, and"
        },
        {
          "Improving Multimodal Emotion Recognition by Leveraging Acoustic Adaptation and Visual Alignment\nMRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "//doi.org/10.1109/ICASSP48485.2024.10446825"
        },
        {
          "Improving Multimodal Emotion Recognition by Leveraging Acoustic Adaptation and Visual Alignment\nMRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "Shlomo Dubnov. 2023. Large-Scale Contrastive Language-Audio Pretraining with"
        },
        {
          "Improving Multimodal Emotion Recognition by Leveraging Acoustic Adaptation and Visual Alignment\nMRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "Pratik Kumar, Vrunda N. Sukhadia, and S. Umesh. 2022.\nInvestigation of Robust-\n[11]"
        },
        {
          "Improving Multimodal Emotion Recognition by Leveraging Acoustic Adaptation and Visual Alignment\nMRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "Feature Fusion and Keyword-to-Caption Augmentation. In ICASSP 2023 - 2023"
        },
        {
          "Improving Multimodal Emotion Recognition by Leveraging Acoustic Adaptation and Visual Alignment\nMRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "ness of Hubert Features from Different Layers to Domain, Accent and Language"
        },
        {
          "Improving Multimodal Emotion Recognition by Leveraging Acoustic Adaptation and Visual Alignment\nMRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)."
        },
        {
          "Improving Multimodal Emotion Recognition by Leveraging Acoustic Adaptation and Visual Alignment\nMRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "Variations. In ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech"
        },
        {
          "Improving Multimodal Emotion Recognition by Leveraging Acoustic Adaptation and Visual Alignment\nMRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "1â€“5.\nhttps://doi.org/10.1109/ICASSP49357.2023.10095969"
        },
        {
          "Improving Multimodal Emotion Recognition by Leveraging Acoustic Adaptation and Visual Alignment\nMRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "https://doi.org/10.1109/ICASSP43922.\nand Signal Processing (ICASSP). 6887â€“6891."
        },
        {
          "Improving Multimodal Emotion Recognition by Leveraging Acoustic Adaptation and Visual Alignment\nMRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "[24] Aiyuan Yang, Bin Xiao, Bingning Wang, Borong Zhang, Ce Bian, Chao Yin, and"
        },
        {
          "Improving Multimodal Emotion Recognition by Leveraging Acoustic Adaptation and Visual Alignment\nMRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "2022.9746250"
        },
        {
          "Improving Multimodal Emotion Recognition by Leveraging Acoustic Adaptation and Visual Alignment\nMRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "Chenxu Lv. 2023.\nBaichuan 2: Open Large-scale Language Models.\nhttps:"
        },
        {
          "Improving Multimodal Emotion Recognition by Leveraging Acoustic Adaptation and Visual Alignment\nMRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "[12] Xiang Li, Yazhou Zhang, Prayag Tiwari, Dawei Song, Bin Hu, Meihong Yang,"
        },
        {
          "Improving Multimodal Emotion Recognition by Leveraging Acoustic Adaptation and Visual Alignment\nMRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "//arxiv.org/abs/2309.10305"
        },
        {
          "Improving Multimodal Emotion Recognition by Leveraging Acoustic Adaptation and Visual Alignment\nMRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "Zhigang Zhao, Neeraj Kumar, and Pekka Marttinen. 2022. EEG Based Emotion"
        },
        {
          "Improving Multimodal Emotion Recognition by Leveraging Acoustic Adaptation and Visual Alignment\nMRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "[25]\nJingyi Zhang, Jiaxing Huang, Sheng Jin, and Shijian Lu. 2024. Vision-Language"
        },
        {
          "Improving Multimodal Emotion Recognition by Leveraging Acoustic Adaptation and Visual Alignment\nMRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "Recognition: A Tutorial and Review. ACM Comput. Surv. 55, 4, Article 79 (nov"
        },
        {
          "Improving Multimodal Emotion Recognition by Leveraging Acoustic Adaptation and Visual Alignment\nMRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "Models for Vision Tasks: A Survey.\nIEEE Transactions on Pattern Analysis and"
        },
        {
          "Improving Multimodal Emotion Recognition by Leveraging Acoustic Adaptation and Visual Alignment\nMRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "2022), 57 pages.\nhttps://doi.org/10.1145/3524499"
        },
        {
          "Improving Multimodal Emotion Recognition by Leveraging Acoustic Adaptation and Visual Alignment\nMRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "Machine Intelligence 46, 8 (2024), 5625â€“5644. https://doi.org/10.1109/TPAMI.2024."
        },
        {
          "Improving Multimodal Emotion Recognition by Leveraging Acoustic Adaptation and Visual Alignment\nMRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "3369699"
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Efficient Self-supervised Learning with Contextualized Target Representations for Vision, Speech and Language",
      "authors": [
        "Alexei Baevski",
        "Arun Babu",
        "Wei-Ning Hsu",
        "Michael Auli"
      ],
      "year": "2023",
      "venue": "Proceedings of the 40th International Conference on Machine Learning"
    },
    {
      "citation_id": "2",
      "title": "wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations",
      "authors": [
        "Alexei Baevski",
        "Yuhao Zhou",
        "Abdelrahman Mohamed",
        "Michael Auli"
      ],
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "3",
      "title": "OpenFace 2.0: Facial Behavior Analysis Toolkit",
      "authors": [
        "Tadas Baltrusaitis",
        "Amir Zadeh",
        "Chong Lim",
        "Louis-Philippe Morency"
      ],
      "year": "2018",
      "venue": "2018 13th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2018)",
      "doi": "10.1109/FG.2018.00019"
    },
    {
      "citation_id": "4",
      "title": "A survey on facial emotion recognition techniques: A stateof-the-art literature review",
      "authors": [
        "Felipe Zago Canal",
        "Tobias Rossi MÃ¼ller",
        "Cristine Jhennifer",
        "Gustavo Matias",
        "Antonio Gino Scotton",
        "Junior Reis De Sa",
        "Eliane Pozzebon",
        "Antonio Sobieranski"
      ],
      "year": "2022",
      "venue": "Information Sciences",
      "doi": "10.1016/j.ins.2021.10.005"
    },
    {
      "citation_id": "5",
      "title": "CLAP Learning Audio Concepts from Natural Language Supervision",
      "authors": [
        "Benjamin Elizalde",
        "Soham Deshmukh",
        "Mahmoud Ismail",
        "Huaming Wang"
      ],
      "year": "2023",
      "venue": "ICASSP 2023 -2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
      "doi": "10.1109/ICASSP49357.2023.10095889"
    },
    {
      "citation_id": "6",
      "title": "AttA-NET: Attention Aggregation Network for Audio-Visual Emotion Recognition",
      "authors": [
        "Ruijia Fan",
        "Hong Liu",
        "Yidi Li",
        "Peini Guo",
        "Guoquan Wang",
        "Ti Wang"
      ],
      "year": "2024",
      "venue": "ICASSP 2024 -2024 IEEE International Conference on Acoustics, Speech and Signal Processing",
      "doi": "10.1109/ICASSP48485.2024.10447640"
    },
    {
      "citation_id": "7",
      "title": "Enhancing Two-Stage Finetuning for Speech Emotion Recognition Using Adapters",
      "authors": [
        "Yuan Gao",
        "Hao Shi",
        "Chenhui Chu",
        "Tatsuya Kawahara"
      ],
      "year": "2024",
      "venue": "ICASSP 2024 -2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
      "doi": "10.1109/ICASSP48485.2024.10446645"
    },
    {
      "citation_id": "8",
      "title": "Improving Domain Generalization in Speech Emotion Recognition with Whisper",
      "authors": [
        "Erik Goron",
        "Lena Asai",
        "Elias Rut",
        "Martin Dinov"
      ],
      "year": "2024",
      "venue": "ICASSP 2024 -2024 IEEE International Conference on Acoustics, Speech and Signal Processing",
      "doi": "10.1109/ICASSP48485.2024.10446997"
    },
    {
      "citation_id": "9",
      "title": "HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units",
      "authors": [
        "Wei-Ning Hsu",
        "Benjamin Bolte",
        "Hubert Yao-Hung",
        "Kushal Tsai",
        "Ruslan Lakhotia",
        "Abdelrahman Salakhutdinov",
        "Mohamed"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing",
      "doi": "10.1109/TASLP.2021.3122291"
    },
    {
      "citation_id": "10",
      "title": "CLIP-MSA: Incorporating Inter-Modal Dynamics and Common Knowledge to Multimodal Sentiment Analysis With Clip",
      "authors": [
        "Qi Huang",
        "Pingting Cai",
        "Tanyue Nie",
        "Jinshan Zeng"
      ],
      "year": "2024",
      "venue": "ICASSP 2024 -2024 IEEE International Conference on Acoustics, Speech and Signal Processing",
      "doi": "10.1109/ICASSP48485.2024.10446825"
    },
    {
      "citation_id": "11",
      "title": "Investigation of Robustness of Hubert Features from Different Layers to Domain, Accent and Language Variations",
      "authors": [
        "Pratik Kumar",
        "N Vrunda",
        "S Sukhadia",
        "Umesh"
      ],
      "year": "2022",
      "venue": "ICASSP 2022 -2022 IEEE International Conference on Acoustics, Speech and Signal Processing",
      "doi": "10.1109/ICASSP43922.2022.9746250"
    },
    {
      "citation_id": "12",
      "title": "EEG Based Emotion Recognition: A Tutorial and Review",
      "authors": [
        "Xiang Li",
        "Yazhou Zhang",
        "Prayag Tiwari",
        "Dawei Song",
        "Bin Hu",
        "Meihong Yang",
        "Zhigang Zhao",
        "Neeraj Kumar",
        "Pekka Marttinen"
      ],
      "year": "2022",
      "venue": "ACM Comput. Surv",
      "doi": "10.1145/3524499"
    },
    {
      "citation_id": "13",
      "title": "Multi-label Learning, Modality Robustness, and Semi-Supervised Learning",
      "authors": [
        "Zheng Lian",
        "Haiyang Sun",
        "Licai Sun",
        "Kang Chen",
        "Mngyu Xu",
        "Kexin Wang",
        "Ke Xu",
        "Yu He",
        "Ying Li",
        "Jinming Zhao",
        "Ye Liu",
        "Bin Liu",
        "Jiangyan Yi",
        "Meng Wang",
        "Erik Cambria",
        "Guoying Zhao",
        "BjÃ¶rn Schuller",
        "Jianhua Tao"
      ],
      "year": "2023",
      "venue": "Proceedings of the 31st ACM International Conference on Multimedia (MM '23",
      "doi": "10.1145/3581783.3612836"
    },
    {
      "citation_id": "14",
      "title": "MER 2024: Semi-Supervised Learning, Noise Robustness, and Open-Vocabulary Multimodal Emotion Recognition",
      "authors": [
        "Zheng Lian",
        "Haiyang Sun",
        "Licai Sun",
        "Zhuofan Wen",
        "Siyuan Zhang",
        "Shun Chen",
        "Hao Gu",
        "Jinming Zhao",
        "Ziyang Ma",
        "Xie Chen",
        "Jiangyan Yi",
        "Rui Liu",
        "Kele Xu",
        "Bin Liu",
        "Erik Cambria",
        "Guoying Zhao",
        "BjÃ¶rn Schuller",
        "Jianhua Tao"
      ],
      "year": "2024",
      "venue": "MER 2024: Semi-Supervised Learning, Noise Robustness, and Open-Vocabulary Multimodal Emotion Recognition"
    },
    {
      "citation_id": "15",
      "title": "Visual Instruction Tuning",
      "authors": [
        "Haotian Liu",
        "Chunyuan Li",
        "Qingyang Wu",
        "Yong Jae Lee"
      ],
      "year": "2023",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "16",
      "title": "GEmo-CLAP: Gender-Attribute-Enhanced Contrastive Language-Audio Pretraining for Accurate Speech Emotion Recognition",
      "authors": [
        "Yu Pan",
        "Yanni Hu",
        "Yuguang Yang",
        "Wen Fei",
        "Jixun Yao",
        "Heng Lu",
        "Lei Ma",
        "Jianjun Zhao"
      ],
      "year": "2024",
      "venue": "ICASSP 2024 -2024 IEEE International Conference on Acoustics, Speech and Signal Processing",
      "doi": "10.1109/ICASSP48485.2024.10448394"
    },
    {
      "citation_id": "17",
      "title": "Learning Transferable Visual Models From Natural Language Supervision",
      "authors": [
        "Alec Radford",
        "Jong Kim",
        "Chris Hallacy",
        "Aditya Ramesh",
        "Gabriel Goh",
        "Sandhini Agarwal",
        "Girish Sastry",
        "Amanda Askell",
        "Pamela Mishkin",
        "Jack Clark",
        "Gretchen Krueger",
        "Ilya Sutskever"
      ],
      "year": "2021",
      "venue": "Proceedings of the 38th International Conference on Machine Learning"
    },
    {
      "citation_id": "18",
      "title": "Robust Speech Recognition via Large-Scale Weak Supervision",
      "authors": [
        "Alec Radford",
        "Jong Kim",
        "Tao Xu",
        "Greg Brockman",
        "Christine Mcleavey",
        "Ilya Sutskever"
      ],
      "year": "2023",
      "venue": "Proceedings of the 40th International Conference on Machine Learning"
    },
    {
      "citation_id": "19",
      "title": "On the effect of dropping layers of pre-trained transformer models",
      "authors": [
        "Hassan Sajjad",
        "Fahim Dalvi",
        "Nadir Durrani",
        "Preslav Nakov"
      ],
      "year": "2023",
      "venue": "Computer Speech & Language",
      "doi": "10.1016/j.csl.2022.101429"
    },
    {
      "citation_id": "20",
      "title": "A Novel Multimodal Sentiment Analysis Model Based on Gated Fusion and Multi-Task Learning",
      "authors": [
        "Xin Sun",
        "Xiangyu Ren",
        "Xiaohao Xie"
      ],
      "year": "2024",
      "venue": "ICASSP 2024 -2024 IEEE International Conference on Acoustics, Speech and Signal Processing",
      "doi": "10.1109/ICASSP48485.2024.10446040"
    },
    {
      "citation_id": "21",
      "title": "A Comprehensive Review of YOLO Architectures in Computer Vision: From YOLOv1 to YOLOv8 and YOLO-NAS",
      "authors": [
        "Juan Terven",
        "Diana-Margarita CÃ³rdova-Esparza",
        "Julio-Alejandro Romero-GonzÃ¡lez"
      ],
      "year": "2023",
      "venue": "Machine Learning and Knowledge Extraction",
      "doi": "10.3390/make5040083"
    },
    {
      "citation_id": "22",
      "title": "A Fine-tuned Wav2vec 2.0/HuBERT Benchmark For Speech Emotion Recognition, Speaker Verification and Spoken Language Understanding",
      "authors": [
        "Yingzhi Wang",
        "Abdelmoumene Boumadane",
        "Abdelwahab Heba"
      ],
      "year": "2022",
      "venue": "A Fine-tuned Wav2vec 2.0/HuBERT Benchmark For Speech Emotion Recognition, Speaker Verification and Spoken Language Understanding"
    },
    {
      "citation_id": "23",
      "title": "Large-Scale Contrastive Language-Audio Pretraining with Feature Fusion and Keyword-to-Caption Augmentation",
      "authors": [
        "Yusong Wu",
        "Ke Chen",
        "Tianyu Zhang",
        "Yuchen Hui",
        "Taylor Berg-Kirkpatrick",
        "Shlomo Dubnov"
      ],
      "year": "2023",
      "venue": "ICASSP 2023 -2023 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "25",
      "title": "Open Large-scale Language Models",
      "authors": [
        "Aiyuan Yang",
        "Bin Xiao",
        "Bingning Wang",
        "Borong Zhang",
        "Ce Bian",
        "Chenxu Chao Yin",
        "Lv"
      ],
      "year": "2023",
      "venue": "Open Large-scale Language Models"
    },
    {
      "citation_id": "26",
      "title": "Vision-Language Models for Vision Tasks: A Survey",
      "authors": [
        "Jingyi Zhang",
        "Jiaxing Huang",
        "Sheng Jin",
        "Shijian Lu"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
      "doi": "10.1109/TPAMI.2024.3369699"
    }
  ]
}