{
  "paper_id": "2502.06803v1",
  "title": "Emotion Recognition And Generation: A Comprehensive Review Of Face, Speech, And Text Modalities A Preprint",
  "published": "2025-02-02T00:11:19Z",
  "authors": [
    "Rebecca Mobbs",
    "Dimitrios Makris",
    "Vasileios Argyriou"
  ],
  "keywords": [
    "Artificial Intelligence",
    "AI",
    "Generative AI",
    "Emotion Recognition",
    "Sentiment Recognition",
    "Face Emotion Recognition",
    "Facial Expression Recognition",
    "Speech Emotion Recognition",
    "Text Emotion Recognition",
    "Text Sentiment Recognition",
    "Survey",
    "Speech to Animation",
    "Speech to Speech",
    "Text Generation",
    "Large Language Models",
    "Facial Expression Generation",
    "Speech Emotion Generation",
    "Text Emotion Generation",
    "Survey",
    "Review"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Emotion recognition and generation have emerged as crucial topics in Artificial Intelligence research, playing a significant role in enhancing human-computer interaction within healthcare, customer service, and other fields. Although several reviews have been conducted on emotion recognition and generation as separate entities, many of these works are either fragmented or limited to specific methodologies, lacking a comprehensive overview of recent developments and trends across different modalities. In this survey, we provide a holistic review aimed at researchers beginning their exploration in emotion recognition and generation. We introduce the fundamental principles underlying emotion recognition and generation across facial, vocal, and textual modalities. This work categorises recent state-of-the-art research into distinct technical approaches and explains the theoretical foundations and motivations behind these methodologies, offering a clearer understanding of their application. Moreover, we discuss evaluation metrics, comparative analyses, and current limitations, shedding light on the challenges faced by researchers in the field. Finally, we propose future research directions to address these challenges and encourage further exploration into developing robust, effective, and ethically responsible emotion recognition and generation systems.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Emotions are central to human communication, shaping interactions through body language, facial expressions, vocal intonations, and textual cues  [1] . Psychological research suggests recognition of emotions is innate in humans, with newborns able to replicate facial expressions and vocal tones as early as two days old  [2] . Understanding emotions aids in teamwork and cooperation, a concept recognised by Darwin's theories on survival mechanisms  [3] . This significance has led to the development of emotion models like Ekman and Friesen's Facial Action Coding System (FACS), which categorises emotions such as anger, disgust, fear, happiness, sadness, surprise, and contempt  [4, 5] , forming the basis for many contemporary emotion recognition systems.\n\nAs interest in artificial intelligence (AI) grows, emotion recognition and generation technologies have gained traction in fields such as healthcare, customer service, education, and entertainment  [6, 7, 8, 9, 10, 11] . AI systems can now analyse and simulate emotional responses, allowing machines to engage in more meaningful human-computer interactions. Emotion recognition is used in applications such as driver fatigue detection  [12]  and lie detection  [13] , while generative models create realistic emotional content in apps like FaceApp  [14] , HeadSpace  [15] , and Wysa  [16] .\n\nThis survey provides a comprehensive review of State-of-the-Art (SOTA) methodologies in AI for emotion recognition and emotion generation, addressing the gap in the literature regarding the integration of these two domains and their applications across multiple modalities. The generation of emotions on faces, Facial Expression Generation (FEG) systems, are termed in the literature as Talking Face or Speech/Text-to-Animation models, while Speech Emotion Generation (SEG) involves Speech-to-Speech or Speech Reenactment methods, and Text Sentiment Generation (TSG) relies on Large Language Models (LLMs). Existing reviews have typically focused on either emotion recognition  [17, 18, 19]  or emotion generation  [20, 21] , without addressing their intersection. Additionally, Facial Expression Recognition (FER) and FEG have not yet been discussed alongside Speech Emotion Recognition (SER), SEG, or Text Sentiment Recognition (TSR). Research tends to prioritise facial systems due to heightened public interest and the relative ease with which facial expressions are interpreted by both humans and machines  [22] . These systems also benefit from extensive pretrained models and datasets derived from computer vision research  [23] . By exploring both emotion recognition and generation across modalities, this survey aims to offer insights into current techniques, highlight areas for improvement, and guide future research directions. This survey is structured to provide a holistic examination of the field. Section 1.1 explores various applications of emotion recognition and generation models. Section 2 discusses preprocessing techniques to improve model accuracy and efficiency. Section 3 reviews the datasets commonly used, detailing their characteristics. Sections 4 and 5 present state-of-the-art methods for emotion recognition and generation, respectively, across faces, speech, and text. Section 5.4 discusses emotion control methods accross modalities. Section 6 provides a comparative analysis of evaluation metrics to assess SOTA performance. Section 7 outlines current challenges and future research directions. Finally, Section 9 concludes with a synthesis of key findings and contributions to the development of emotion recognition and generation technologies.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Applications",
      "text": "Emotion recognition systems are used across various fields. In customer service, they are utilised to discern customers' emotions and evaluate the effectiveness of sales assistants' communication strategies through assessment of transcripts  [7] . Similarly, at self-service checkouts, FER is used to gauge customer satisfaction based on their facial cues  [6] . In healthcare, these systems assist in tracking the progression of Alzheimer's disease  [8] , facilitating therapy sessions  [9] , and supporting individuals with Asperger's Syndrome in recognising emotions  [24] . They are also used in robotics to interpret human emotions during interactions with machines  [10] , and in educational settings to evaluate students' engagement and learning  [11] . Other applications include lie detection  [13]  and monitoring driver fatigue levels  [12] .\n\nEmotion recognition systems can also serve as foundational tools for training models capable of generating realistic emotional content  [22] . These models can be used to create visual virtual assistants and avatars for virtual calls  [25] . As reliance on chatbots for social interactions and advice increases  [26] , there is a growing opportunity for the development of talking head chatbots. Such chatbots would use speech or text input-whether from a customer service representative, therapist, or a text generation model-to produce animated faces with lifelike emotions in real-time. These animated avatars could integrate with AI models such as Character.AI  [27] , ChatGPT  [28] , Llama  [29] , or Gemini  [30]  to function as therapeutic or customer service bots. This technology has the potential to provide users with a highly immersive and personalised experience, enhancing or even replacing current customer service chatbots.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Preprocessing For Er And Eg Systems",
      "text": "Preprocessing is an important stage in deep learning pipelines, particularly when handling data obtained from uncontrolled or 'in-the-wild' environments, such as facial and speech data extracted from movies or textual data from social media. Such data often exhibit significant variability compared to controlled laboratory settings, with variations in background, lighting, noise, and other artefacts. To address these challenges, preprocessing typically involves standard steps like data normalisation, noise reduction, and feature extraction to ensure data consistency and optimise model performance. Below, we explore the specific preprocessing techniques used for processing face, speech, and textual data.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Preprocessing For Face Systems",
      "text": "Preprocessing for facial emotion recognition systems aims to enhance image quality, standardise data, and extract critical features for accurate model predictions. The initial step involves resizing and cropping facial images to create uniform input dimensions, ensuring consistency across the dataset. By eliminating background elements and focusing on the region of interest, these techniques enable models to concentrate on key facial features. Normalisation, through scaling pixel values to a common range (e.g., 0 to 1 or -1 to 1), ensures uniform pixel intensity across different samples, thereby enhancing the model's capacity to learn relevant patterns. Common methods such as mean subtraction  [31]  and standard deviation normalisation  [32]  are frequently used. Noise reduction techniques, like Gaussian blurring  [33]  and median filtering  [34] , are used to minimise the impact of noise introduced during image acquisition or transmission.\n\nTechniques such as histogram equalisation  [35]  improve contrast by redistributing pixel intensities, enhancing visibility in images captured under challenging conditions. Data augmentation, involving transformations like rotation, scaling, and flipping, increases training data diversity and mitigates overfitting  [36] . Furthermore, advanced algorithms such as Haar cascades  [37]  and deep learning-based facial landmark detection methods  [38]  are applied to extract and align facial regions, standardising poses and reducing variability. Feature extraction models, such as VGG  [39] , ResNet  [40] , and MobileNet  [41] , are widely used for extracting high-level features. Colour space transformations and quality control measures help streamline data preparation, ensuring only high-quality data is fed into the models  [33, 42] .",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Preprocessing For Speech Systems",
      "text": "The primary goals of preprocessing in speech systems are noise reduction, normalisation, segmentation, and feature extraction from raw audio signals. Noise reduction methods like spectral subtraction  [43] , Wiener filtering  [44] , and adaptive filtering  [45]  are used to eliminate background noise which can degrade speech signal quality. Normalisation adjusts amplitude and dynamic range to maintain consistency across recordings  [46] . Speech segmentation techniques, such as endpoint detection  [47]  and silence removal  [48] , isolate speech segments within continuous audio streams, enabling more targeted analysis.\n\nFeature extraction captures the salient characteristics of speech, using Mel-Frequency Cepstral Coefficients (MFCCs)  [49] , which represent spectral properties in a compact form, and Linear Predictive Coding (LPC)  [50] , which models the spectral envelope. Other methods like pitch estimation  [51]  and anti-aliasing filtering  [52]  help preserve signal integrity. Techniques such as de-reverberation  [53]  and pre-emphasis  [54]  further refine the signal quality. For segmentation, windowing techniques like frame blocking divide speech signals into shorter frames, facilitating computational efficiency  [55] . Mean and variance normalisation standardises feature scales, improving model robustness to variability in input data  [56] .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Preprocessing For Text Systems",
      "text": "Text preprocessing begins with tokenisation, which breaks down text into smaller units, such as words or characters. This is followed by lowercasing, which standardises the text by treating uppercase and lowercase versions of words identically, thereby reducing vocabulary size and simplifying the learning process  [57] . Punctuation and special character removal further eliminate noise which could interfere with learning. Stopwords-such as \"and\" or \"the\"-are often removed, as they carry little semantic value  [58] . Stemming and lemmatisation techniques group words with similar meanings, helping models understand linguistic variations  [59, 60] .\n\nNumerical values are encoded or replaced with placeholders to maintain the semantic integrity of the text  [61] . Outof-vocabulary words are managed through tokenisation or character-level representations  [62] , while padding and truncation ensure uniform sequence lengths, which is crucial for text classification  [63] . Pretrained word embeddings, such as Word2Vec  [64] , can be used to initialise the embedding layers of deep learning models or be fine-tuned during training. Encoding methods like one-hot or integer encoding convert textual data into numerical representations, while pretrained tokenisers accelerate this conversion  [65] . Text augmentation techniques, such as synonym replacement and paraphrasing, diversify training data and reduce overfitting, improving generalisation  [66] .\n\n3 Datasets for Face, Text, and Speech ER and EG Systems High-quality, diverse datasets are essential for training emotion recognition and generation models. These datasets provide labelled examples from facial expressions, speech, and text, enabling models to learn emotional cues in varied contexts. Some datasets are captured in controlled environments, while others are collected in the wild, offering more complex real-world variations. This section highlights the most widely used datasets across facial, speech, and text systems, focusing on those with comprehensive emotional labelling and diversity (see 1). This section will discuss deep learning methodologies for emotion recognition for faces, speech, and text. We will discuss the strengths and limitations of current literature. Most emotion recognition systems use the 8 primary emotions anger, disgust, fear, happiness, sadness, surprise, contempt, and neutral  [5] . Unlike traditional methodologies where feature extraction and classification are treated as distinct stages  [67] , deep learning frameworks for emotion detection enable end-to-end pipelines. A key component in classification is the use of a loss layer, which regulates the backpropagation error, for estimating prediction probabilities for each sample. For example, in CNNs the softmax loss function is typically used to minimise the difference between the predicted class probabilities and the ground-truth. Some models simultaneously predict both discrete emotions and continuous affect dimensions, such as arousal, valence, and strength of emotion  [23]  (see Fig.  1 ). This aims to minimise data mislabelling and improve overall prediction accuracy.\n\nFigure  1 : The EmoFAN pipeline integrates facial landmark detection, discrete emotion classification, and continuous valence-arousal estimation in a single neural network. This unified model performs all tasks in one pass, using a face-alignment network and an attention mechanism to focus on key facial regions, enhancing accuracy. Joint prediction of both emotion types, combined with knowledge distillation, improves robustness.  [23]  4",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": ".1 Facial Expression Recognition",
      "text": "FER systems begin with facial feature detection, whereby the face is identified and isolated. Methods such as the Viola-Jones algorithm, Histogram of Oriented Gradients (HOG), and Convolutional Neural Networks (CNNs) are used. Facial landmark detection identifies key points on the face, then feature extraction focuses on geometric features and appearance features. Traditional machine learning algorithms and deep learning models, especially CNNs, classify these features into emotional categories. CNNs are effective as they automatically learn and extract hierarchical features from raw pixel data  [68] . The following section will discuss state-of-the-art research in FER with an emphasis on novelty, recurring themes, strengths, and limitations of current research.\n\nFER systems are classified into two categories: static image and dynamic sequence. While static methods encode spatial information from individual images, dynamic techniques use temporal relationships across frames within sequences  [17] . Historically, FER heavily relied on handcrafted features or shallow learning techniques such as Decision Trees  [69] , K-Nearest Neighbors (K-NN)  [70] , and Support Vector Machines (SVM)  [71] . However, with the rise in emotion detection competitions such as FER2013  [72] , EMOCA  [73] , and ABAW 2023  [74]  a shift towards the use of deep learning techniques occurred. This has coincided with improvements in processing capabilities and network architectures, enabling the widespread adoption of deep learning methodologies.\n\nModels using pretrained Contrastive Language-Image Pretrained (CLIP)  [75]  achieve remarkable results in FER. Using the joint embedding space of text and images, CLIP models can understand contextual information across modalities.\n\nBy training on large datasets containing images paired with descriptions of emotions, CLIP learns to associate visual patterns with their emotional description. One such model which uses CLIP is DFER-CLIP  [76] . This method combines both modalities, using a temporal model atop the CLIP image encoder. Temporal facial features are captured while using descriptions of facial behaviour instead of class names for the text encoder. It uses learnable prompts as context for descriptors of each facial expression class, enabling automatic learning of relevant context information during training. The model's pipeline involves extracting features from facial images or frames, and predicting facial expression descriptions. Furthermore, DFER-CLIP automates the generation of textual descriptors by prompting a language model with queries about useful visual features for each expression, culminating in comprehensive descriptions for classification.\n\nAttention is a key topic in FER with approaches such as self-attention, patch attention, and cross attention being utilised. EmoFan (see Fig.  1 ) uses attention mechanisms on facial landmarks and facial heat maps and achieves SOTA results.\n\n[77] uses patch attention and a pretrained ResNet-18 to extract the facial feature maps to overcome issues caused by occlusion for improved performance.  [78]  uses a similar approach by making use of window-based cross-attention mechanisms in conjunction with landmark detection, and multi-scale feature extraction. In comparison,  [79]  uses self-attention and a transformer to identify facial expressions in images or videos where the face is difficult to see.  [73]  addresses a shortfall in labelled datasets by incorporating an emotion recognition model into the 3D face reconstruction framework DECA  [80]  This enables improved emotion reconstruction and classification, along with the use of their Emotion Consistency Loss.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Speech Emotion Recognition",
      "text": "Figure  2 : The SER model processes frame-level speech features as input, using a 2-layer LSTM to generate outputs aligned with each frame's corresponding time. The LSTM's internal forget gate has been replaced by an attention gate. To differentiate emotional nuances across time and feature dimensions, the model applies a weighting operation separately on the LSTM's output along both the time and feature dimensions. These two weighted outputs are then fed into fully connected layers, and the final output from the softmax layer provides the classification result.  [81]  Recognising emotions in speech involves a multidisciplinary approach, integrating linguistics, psychology, and computer science  [82] . Acoustic feature analysis, focusing on prosody and voice quality, plays a key role. Prosodic features, such as pitch, intensity, and speech rate, effectively indicate emotions. For example, happiness or excitement use higher pitch and greater variability, while sad voices use lower pitch and slower speech. Voice quality, including elements such as breathiness and tension, can also signal different emotions. Word choices and sentence structures, provide additional clues. Short, abrupt sentences can indicate anger, while longer, complex sentences might suggest calmness. Contextual analysis, considering the situational context and dialog history, is vital, as the same utterance can convey different emotions depending on the context  [83] .\n\nTransformer based model ESCM  [84] , achieved state-of-the-art results in SER by adjusting emotions and semantics based on context. They achieve this by using Graph Convolutional Network (GCN) to find correlations between words in spoken coversations. In contrast,  [81]  (see Fig.  2 ) introduces a novel approach to speech emotion recognition by integrating attention mechanisms into Long Short Term Memory (LSTM) models. By prioritising relevant information across both time and feature dimensions, the attention-based LSTM architecture improves performance in SER. The use of frame-level features provide a comprehensive representation of emotional content, contributing to the model's accuracy.  [79]  use Large Language Models (LLMs) and weakly-supervised learning to label the emotions in speech data, which contributes to the effectiveness of their SER model.\n\nFurther innovations in time-frequency analysis have also improved SER. For instance, the fast Continuous Wavelet Transform (fCWT) enables high-resolution analysis of non-stationary speech signals, balancing temporal and spectral features. When combined with Deep Convolutional Neural Networks (DCNNs), this approach enhances the extraction of paralinguistic information, offering robust real-time performance while overcoming limitations of traditional methods like the Short-Term Fourier Transform (STFT)  [85] .",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Text Sentiment Recognition",
      "text": "TER focuses on the identification and classification of emotions expressed in textual data using Natural Language Processing models (NLP). NLP models enable machines to understand, interpret, and generate text  [87] . Bidirectional Encoder Representations from Transformers (BERT)  [88]  are used in most modern NLP models  [89] . These models are useful for TER due to their ability to capture contextual data and decipher emotions in text, enabling SOTA performance. Campagnano et al.  [90]  combines BERT encodings with bidirectional LSTM layers to achieve robust emotion classification, particularly in semantic role labelling tasks.  [91]  use a modified BERT-based architecture to classify emotions for individual sentences and entire texts.  [92]  use a BERT model trained on data from 100 languages as well as X (formerly Twitter), to detect emotions on social media platforms. In contrast,  [86]  (see Fig.  3 ) use LSTM and a CNN based model for TER. The use of CNN-LSTM channels extracts both local and global contextual information from input text, working for diverse text inputs.  [91, 92]  address multilingual emotion recognition, developing models and datasets capable of working across languages. As seen in this analysis there is a distinct lack of recent research into TER, highlighting the need for updated studies to address current challenges and advancements in the field.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Emotion Generation For Faces, Speech, And Text",
      "text": "This section will discuss generated content for faces -which will focus on animated face generation, speech -taking the nuances of audio from one speaker and converting to another voice, and text -the generation of realistic text. Emotion recognition models are sometimes used for training  [93] , and evaluating  [94]  these models to generate accurate emotional content. Emotion recognition datasets are also utilised for emotion generation models  [95] . A recent challenge with creating emotionally realistic generated content comes from negativity in public's perception due to media hype surrounding stealing of identities  [96] , deepfakes  [97] , and the rapid rate in which models are being released  [75] . This consideration has the capacity to hinder research in these fields due to restrictions on the availability of models for researchers  [98] , due to the fear they will fall into the wrong hands. This section will discuss SOTA methods for these modalities, and will discuss the strengths and limitations of current research.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Facial Expression Generation",
      "text": "FEG and face manipulation techniques have been around for years, present on mobile phone apps such as Instagram  [100] , SnapChat [101], and AI photo editors such as FaceApp  [102]  and others. The release of visually appealing talking-head models such as VASA  [103]  and EMO-Live  [99] , have further bolstered public interest in this research area. Talking-head animation refers to models which take as input an image of a person, and generates new frames using audio  [99]  (see Fig.  4 ), video  [104] , or text  [22]  to guide the facial expressions. The manipulation of facial expressions through prompts is another new area of research  [105, 106, 107] . FEG models often focus on prioritising the manipulation of the mouth, eyes, or poses  [108, 109, 110, 111] , while others focus on overall realism  [99, 103, 112] .\n\nWith mouth movements now achieving realism pretrained SOTA models such as Wav2Lip  [109]  are incorporated into larger models to guide the lip movements, while the model focuses on poses and facial expressions  [112] .  [99, 104]  use 3D face modelling techniques and reconstruction methods to capture detailed facial geometry. This allows for accurate expression synthesis and emotion manipulation. Similarly, other methods use 3D registration and mesh-based representations to achieve realistic Face Expression generation  [113, 114] . Generative Adversarial Networks (GAN) are used for generating animations  [105, 115]  due to their ability to create realistic synthetic content. GANs are trained by generating content through a generator network, then using a discriminator network to predict if the generated content is real or not. For Face Expression generation, GANs are combined with other models to generate realistic facial expressions in talking-head animation generation  [116, 117, 118, 106, 113] . For example,  [106]  employ LSTM networks and a GAN for speech-driven animation.\n\n[116] use a GAN to guide the generation process of emotional animations, and preserve the identity of the target face.  [117]  focuses on facial expression manipulation using a modified U-Net structure with GANs and achieves precise emotion manipulation.  [119]  use GANs and attention mechanisms as the backbone of their text to talking-head generation framework. Meanwhile,  [120]  and  [105]  utilise GANs in their methodologies for efficient emotional manipulation. Additionally,  [113]  use a GAN for personalised facial expression manipulation.  [99]  use Diffusion models for generative power and extensive control over the generation of animations. Diffusion models iteratively refine a noisy image into a high-quality sample. This refinement allows for the generation of highly realistic facial expressions, while maintaining control over intensity, duration, and subtle movements. By conditioning the diffusion process on desired expression labels or latent codes, these models produce specific facial expressions with remarkable realism. As diffusion models capture uncertainty during generation, this enables the synthesis of realistic variations.\n\nAttention's ability to focus on important facial regions and generate realistic facial expressions has enabled them to become a key part of face generation architectures. In  [107] , attention mechanisms ensure the generated facial animations accurately capture the speaker's gestures and facial expressions.  [99]  (see Fig.  4 ) integrates attention mechanisms into the pipeline to improve the quality and synchronisation of talking portrait videos, attention mechanisms are utilised to refine motion dynamics and speed adjustments. This method achieves realistic talking portrait videos which closely align with the input audio content.  [119]  use attention gate and self-attention mechanisms in their text-based talking-head generation framework. By incorporating these mechanisms their model manipulates Action Unit-related embeddings, leading for accurate and expressive facial animations synchronised with input text. CLIP with its multimodal capabilities is useful for facial animation generation tasks. By inputting textual prompts to describing desired emotional states along with images associated with those emotions, CLIP can generate images reflecting the specified emotions. This allows the model to learn associations between text and images which improves its ability to generate content with realistic emotions. TalkCLIP by  [121]  generates realistic talking head videos of a target speaker with specific speaking styles. Their model utilises CLIP embeddings and an adaptor network to map text descriptions, to speaking style codes.\n\nFurthermore, researchers have explored the ability to control the generation of emotions on the faces through various inputs such as speech, video, facial reenactment, and text. Speech data is the most common input medium whereby an animated face video is generated using the emotions in the speech  [106, 105, 99]  (see Fig.  4 ). Video is used as an input in architectures where the face is changed to a target face using facial reenactment methods  [104] , or the emotions are manipulated via facial reenactment from a static image  [116] . However, the synchronisation of speech and facial animations rely on robust phoneme processing within the architectures  [119] . Using text as a input is a relatively unexplored field which enables the generation of Face Expression generation based on the emotion content of textual dialogue  [114] . Other researchers have explored methods to directly control the emotions on the output videos using CLIP text prompts  [105, 121] .",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Speech Emotion Generation",
      "text": "One element of SEG, known as voice conversion, speech-to-speech synthesis, or speech reenactment, involves the transformation of speech signals to modify the vocal characteristics of one speaker to resemble another or to produce entirely synthetic voices. These methods form the basis of SEG, whereby the emotions in a target voice can be changed through prompts  [122] (see Fig.  5 ), or by the emotions in a target voice through using a emotional reference voice  [123] .\n\nRecent advancements in AI have led to the development of synthetic voices that are almost indistinguishable from human speech. Achieving realism in generated speech involves capturing natural intonation, rhythm, and emotion. Advanced systems, such as those by ElevenLabs  [124] , use SOTA deep learning techniques to produce high-quality, realistic speech. These systems generate voices that sound authentic and carry unique characteristics associated with individual speakers. This section reviews recent advancements in SEG methodologies. Efficient fine-tuning is achieved using methods such as QLoRA, which significantly reduce computational requirements.\n\nThe model is iteratively optimised and evaluated to attain state-of-the-art performance across various applications.  [29]  SEG models use phonetic content, including emotional cues, from a source voice to synthesize audio in a target voice while retaining desired stylistic characteristics  [123] . A common approach in SEG involves using language models like BERT  [88]  for extracting contextualized representations of linguistic content, thereby enabling precise alignment between source and target voices. BERT embeddings contribute to the controllability and realism of SEG systems, facilitating accurate transformations in speech style and characteristics, which allows synthetic speech to be tailored to specific emotions  [125, 123, 126, 127] . Traditional approaches often rely on text-based conditioning using transcripts  [128] ; however, recent methods, such as that by  [127] , employ discrete representations for phonetic content. This enables the capture of non-textual cues, such as laughter, and supports diverse linguistic applications. Additionally,  [126]  propose an architecture that integrates source and target encoders with a decoder, preserving critical linguistic and speaker features throughout the conversion process to ensure the synthesized speech remains natural and true to the source. SEG also benefits from adversarial training techniques inspired by GANs  [129] . In these frameworks, a discriminator differentiates between target voice samples and synthesized speech, prompting the model to generate speech that convincingly reconstructs the source content while mimicking the target speaker's characteristics.The DDDM-VC model  [130]  introduces a novel approach for SEG, enhancing controllability by decoupling and independently processing attributes such as content, pitch, and timbre. Through attribute-specific denoising, DDDM-VC achieves high-precision voice style transformations, while the inclusion of prior mixup techniques strengthens robustness in voice adaptation, especially in zero-shot scenarios. This disentangled structure enables DDDM-VC to maintain speaker fidelity and naturalness in synthesized voices across a variety of speaker styles . Similarly, PromptVC by  [122]  (see Fig.  5 ), uses a latent diffusion model for voice style conversion using natural language prompts. This enables precise control over the attributes in the generated speech. Another method uses Contrastive Predictive Coding (CPC) features to enhance the quality of synthesised speech  [123] , which is a self-supervised learning technique for predicting future utterances in latent space. Similarly,  [131]  preserves time-synchronisation and fundamental frequency information to maintain the naturalness of converted speech. Finally, two-stage training schemes are frequently used to align hidden representations between source and target speech. The initial stage focuses on reconstructing single utterances to establish alignment, followed by a second stage where multiple utterances refine the conversion process  [132] . This progressive refinement enhances the model's adaptability, improving performance in scenarios with significant divergence between source and target speech characteristics.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Text Sentiment Generation",
      "text": "TSG models work from a user interface by taking input text, and generating a response (see Fig.  6 ). TSG have the ability to alter the emotional content in existing text. Large Language Models (LLMs) such as ChatGPT  [28] , Llama  [29]  (see Fig.  6 ), Gemini  [133]  can create text with emotions and personality which can pass for human writing. Ensuring accurate grammar and syntax, a diverse and contextually appropriate vocabulary, and consistency in style, tone, and information are all important for TSG. Additionally, typographical errors, realistic mistakes, smooth transitions between ideas and a deep understanding of context also contribute to the text's realism.\n\nUntil recently Recurrent Neural Networks (RNNs)  [134]  were used extensively in text generation due to their ability to handle sequential data by maintaining an internal memory. However, traditional RNNs suffer from the vanishing gradient problem, which impedes long-range dependencies. They also struggled to work on long sentences  [135] .\n\nResearchers attempted to combat this by running the RNNs both forward and backward over the textual data  [136] , which did not rectify the problem. These limitations led to the development of Long Short-Term Memory (LSTM) networks, a variant of RNNs. LSTMs employ architectures with gated mechanisms, including input, output, and forget gates, enabling them to learn and retain long-term dependencies in sequential data  [135] . This feature makes LSTMs particularly ideal for tasks requiring memory over extended sequences, such as text generation. Another architecture used for text generation are Sequence-to-Sequence (Seq2Seq) models  [137] , which consist of an encoder and a decoder. Seq2Seq models have shown proficiency in generating coherent and contextually relevant text, making them valuable for emotional text generation tasks. Generative Adversarial Networks (GANs)  [138] , used mostly in computer vision, have also emerged as useful for text generation tasks. The generator produces synthetic text data, while the discriminator evaluates the authenticity of the generated text. Used in conjunction with the above algorithms, attention mechanisms enable models to focus on relevant parts of the input text sequence when generating a response. Attention mechanisms allow models to weigh the importance of each word in the input sequence dynamically as they generate each word in the output sequence  [139] . For example, in the Seq2Seq model, attention mechanisms help align the encoder hidden states with the decoder hidden states at each time point, ensuring the model attends to the most relevant parts of the input sequence when generating each word in the output sequence  [139] .\n\nTo address these challenges researchers are exploring various approaches. One approach involves fine-tuning pretrained language models such as ChatGPT  [28]  for emotion-specific tasks  [140] . This approach uses datasets annotated with emotional labels to train the model to associate linguistic patterns with emotional states. During fine-tuning, adjustments are made to the model's parameters through additional training iterations on emotional text datasets. Developing models with an understanding of contextual cues is essential for accurate emotional text generation. This involves considering factors such as the broader narrative, speaker intent, and audience context to generate realistic text.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Generative Models With Emotion Control",
      "text": "This section will examine methodologies for implementing emotion control within FEG, SEG, and TSG. Emotion control, in this context, pertains to the systematic generation of content-spanning animations, speech, and textual outputs-characterised by realistic and contextually appropriate emotional expressions. These emotions are elicited or guided through specific prompts or control mechanisms, ensuring that the generated outputs align with intended affective states. The discussion will encompass techniques used to encode, manipulate, and render emotions, as well as the underlying computational models that enable nuanced emotional dynamics across various modalities.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Audio Driven Face Expression Generation",
      "text": "Fig.  4  shows audio driven Face Expression generation by  [99] . This method for Face Expression generation takes a reference image as input which is put through a frames encoder. Next, a feature extraction network, called ReferenceNet extracts detailed features from the reference image and after the first iteration, the motion frames, to preserve the identity from the reference image. The architecture then progresses to the diffusion stage where a pretrained audio encoder processes the input voice audio clip, extracting voice features which influence the facial movements and expressions. The Backbone Network, using reference-attention and audio-attention mechanisms, denoises the input data and generating realistic video frames. This comprehensive network architecture ensures the generated video frames sync with the provided audio content. Speed layers fine-tune temporal modules and control head motion across clips, improving consistency and stability in the generated videos.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Text Driven Face Expression Generation",
      "text": "The text-based talking-head generation framework by  [114]  uses neural networks tailored to different aspects of generating Face Expression animations from textual inputs. Gmou, dedicated to animating mouth movements from phonemes, uses a structure based on CNNs for efficient parallel computation and is trained using a combination of L1 loss and Least Squares Generative Adversarial Network (LSGAN) loss. Similarly, Gupp and Ghed utilise encoder-decoder network structures to synthesize upper face parameters and head pose, respectively, from input words, training with analogous loss functions to ensure realistic outputs. The Style-Preserving Landmark Generator, Gldmk, uses a multi-linear 3D Morphable Model (3DMM) and a fully-connected network to ensure consistency and accuracy in facial expressions, incorporating a unique mapping technique to preserve speaker-specific styles.",
      "page_start": 11,
      "page_end": 12
    },
    {
      "section_name": "Video Driven Face Expression Generation",
      "text": "NED by  [104]  allows manipulation of Face Expressions in in-the-wild videos while preserving natural speech-related mouth motion. The Face Analysis module incorporates preprocessing steps such as face landmark detection, segmentation, and resizing, alongside 3D Morphable Models (3DMMs) for accurate estimation of 3D face geometry. The Expressions Translator, a GAN, utilises a recurrent network with LSTM units to convert sequences of facial expressions into desired emotions, while maintaining the original mouth motion. A encoder extracts emotion-related style vectors from the input sequences, while the Mapping Network generates style vectors associated with target emotions. A neural face renderer generates realistic frames, incorporating techniques such as multi-band blending for seamless integration of generated faces into the original backgrounds. This ensures the manipulated facial expressions seamlessly blend into real-world scenarios. During testing, N-length sliding windows are applied frame by frame, with the sequences processed through the Expressions Translator. The conditional style vector is either generated by the Mapping Network or extracted from a reference video, allowing for flexible manipulation of emotions in facial videos.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Emotion Prompted Face Expression Generation",
      "text": "EAT by  [105]  takes in an image of a target face, speech, and an emotion prompt such as happy, sad, or angry, to generate animated videos. The model first trains the CLIP model on emotion labelled datasets to learn audio-visual correlations. This pre-training phase uses enhanced latent representations and a transformer model. Enhanced latent representations capture intricate facial expressions, incorporating identity-specific canonical keypoints, rotation, translation, and expression deformation components. The transformer model predicts synchronised expression deformations from audio inputs and predicts head pose features, and latent source image representations. Next, three primary modules-Deep Emotional Prompts, Emotional Deformation Network (EDN), and Emotional Adaptation Module (EAM)-play integral roles in the emotional adaptation. Deep Emotional Prompts inject emotion-guided expression generation into the model, using latent codes sampled from a Gaussian distribution to provide crucial emotional guidance. EDN complements this by predicting emotion-related expression deformations. EAM further refines the visual quality of generated videos by generating emotion-conditioned features. The architecture also accommodates zero-shot expression editing, which allows text-guided manipulation of talking-head videos without the need for extensive emotional training data. Using the CLIP model, the system aligns generated expressions with textual descriptions, offering users control over the emotional content of the videos.",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "Speech Emotion Generation Model",
      "text": "The architecture in  [123]  comprises three main components: source encoder, target encoder, and a decoder. The source encoder uses Wav2Vec 2.0  [141] , a pretrained feature extractor, to capture speech representations from the source utterance. The target encoder processes log mel-spectrograms of utterances from the target speaker, and the decoder consists of transformer layers using both self-attention and cross-attention. A linear projection layer contributes to the final prediction of the desired output voice, following a non-autoregressive approach. The model is trained using a two-stage approach. In the first stage, single utterances from both the source and target speakers are used to reconstruct the log mel-spectrogram of the utterance. In the second stage, multiple utterances, typically 10, from the target speaker are concatenated and fed into the target encoder. Simultaneously, a single utterance from the source speaker is fed into the source encoder.",
      "page_start": 15,
      "page_end": 15
    },
    {
      "section_name": "Text Sentiment Generation Model",
      "text": "A model  [140]  built upon ChatGPT2  [142] , has been trained to generate text with specific emotions. The ChatGPT2 model is fine-tuned with text samples annotated with affective labels or sentiment scores. The Plug and Play Language Model (PPLM) framework is integrated into the ChatGPT2 architecture to enable attribute-controlled text generation.\n\nPPLM incorporates perturbation and optimisation mechanisms during training, enhancing the model's ability to generate text with specific affective attributes. The model's loss functions include terms which encourage the generation of text with desired emotional attributes and intensity levels. Users specify the desired emotional tone or topic, and the intensity of the emotion desired. The model uses specified attributes and intensity levels to control the content and tone during the text generation process.",
      "page_start": 16,
      "page_end": 16
    },
    {
      "section_name": "Text Sentiment Generation Chatbot",
      "text": "The Empathetic Semantic Correlation Model (ESCM) by  [84]  generates empathetic responses in dialogues by understanding emotions and semantics. It includes three components: a context encoder, a dynamic correlation encoding module, and an emotion and response predicting module. The dynamic correlation encoding module features dynamic emotion-semantic vectors and a correlation Graph Convolutional Network, adjusting emotions and semantics based on contextual cues. The emotion and response predicting module uses context semantics and correlations to predict emotions and generate empathetic responses. During training, ESCM optimises parameters using multiple loss functions and supervised learning on annotated datasets. In use, ESCM processes dialogue context, adjusts to contextual cues, and continuously learns to provide accurate, empathetic responses.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Evaluation",
      "text": "This section provides an overview of the metrics used to evaluate ER and EG models across facial, speech, and textual modalities. It explores various evaluation techniques to determine their effectiveness in measuring model performance and accuracy. Furthermore, the comparative analysis within this section examines state-of-the-art methods to identify the most effective approaches. By synthesising findings from recent studies, this evaluation aims to uncover the strengths and limitations of current evaluation frameworks, thereby highlighting which models are most proficient at recognising and generating emotional expressions across different modalities.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Evaluation Metrics",
      "text": "Evaluation metrics are essential for assessing the performance of emotion recognition and generation models across different modalities. This section highlights the most widely used metrics in facial, speech, and text, emotion recognition and generation.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Common Metrics",
      "text": "â€¢ Accuracy: This metric measures the proportion of correctly classified instances among the total instances. It provides a basic overview of model performance but does not account for class imbalances, which can lead to misleading results.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Accuracy = Number Of Correct Predictions Total Number Of Predictions",
      "text": "â€¢ F1 Score: The harmonic mean of precision and recall, providing a balanced measure of a classifier's performance, particularly in cases with imbalanced datasets. The F1 score is crucial for understanding the trade-off between precision and recall.\n\nâ€¢ Precision: Measures the proportion of true positive predictions out of all positive predictions, indicating the accuracy of positive predictions in identifying emotional expressions.\n\nâ€¢ Recall: Measures the proportion of true positive predictions out of all actual positives, reflecting the model's ability to identify relevant instances. High recall is essential in applications where missing a positive instance can have significant consequences.\n\nâ€¢ Mean Opinion Score (MOS): Often used in evaluating generated speech and facial expressions, this metric assesses perceived quality by averaging ratings given by human evaluators on a numerical scale, providing a subjective measure of output quality.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Metrics For Face Systems",
      "text": "â€¢ Structural Similarity Index (SSIM, 5, is used to assess the similarity between two images. It takes into account luminance, contrast, and structure of the images. SSIM is defined as:\n\nâ€¢ FrÃ©chet Inception Distance score (FID), 6, evaluates the quality of generated images in generative adversarial networks (GANs). It measures the similarity between the distribution of real images and generated images in a feature space learned by a pretrained deep convolutional neural network. FID is defined as:\n\nâ€¢ Cumulative Probability Blur Detection (CPBD) CPBD quantifies image blur by analysing edge sharpness and comparing edge gradient profiles to perceptual thresholds. A higher CPBD score indicates a clearer image with less blur.\n\nP(e i )\n\nâ€¢ Cosine Similarity (CSIM) CSIM measures the similarity between two vectors, such as feature embeddings of source and generated faces.\n\nValues range from -1 to 1, where 1 indicates identical direction and maximum similarity.\n\nM-LMD evaluates the average difference in lip keypoint positions between reference and generated videos. It reflects the overall accuracy of lip synchronisation in generated content.\n\nâ€¢ Equal Error Rate (EER), 9, is a point where the false acceptance rate (FAR) and false rejection rate (FRR) are equal in a speaker systems. It represents the operating point where the system's performance is balanced.\n\nâ€¢ Mel-cepstral distortion (MCD), 10, quantifies the difference between two sets of mel-frequency cepstral coefficients (MFCCs) for speech tasks.\n\nâ€¢ Perplexity: A key metric in text generation, perplexity measures how well a language model predicts a sample of text. It reflects the average branching factor of the model, with lower perplexity indicating better performance.\n\nâ€¢ Sentiment Accuracy: For text-based emotion recognition, sentiment accuracy measures how accurately a model classifies the overall emotional tone or sentiment of a text (e.g., positive, negative, neutral). This metric is widely used in applications such as sentiment analysis and Text Sentiment generation.\n\nâ€¢ BLEU (Bilingual Evaluation Understudy Score): Commonly used in text generation systems, BLEU compares the generated text to a reference by measuring how many n-grams in the generated text appear in the reference. It is particularly useful for evaluating the fluency and relevance of generated text.\n\nEvaluation metrics for assessing LLMs include: Massive Multitask Language Understanding (MMLU), Generalized Question-Answering Performance (GPQA), MATH, HumanEval, Multi-Genre Social Media (MGSM), and Discrete Reasoning Over Paragraphs (DROP). MMLU evaluates the models ability to understand and generate text across 57 subjects using multiple choice questions. GPQU evaluates text generation in question answering tasks. MATH tests the models ability to understand mathematical concepts, problem-solving skills, and ability to generate accurate solutions to mathematical queries. HumanEval assesses performance on tasks which require a high level of language comprehension and expression, such as essay writing, and summarisation. MGSM assesses the generation of text for social media across various formats, including tweets, posts, and comments. DROP is used to assess the models ability to extract information from longer texts such as performing logical reasoning and answering questions regarding the text. The F1 score is the measure of models precision and recall in these tasks. All of these metrics are obtained from user studies. Additional metrics include, Recall-Oriented Understudy for Gisting Evaluation (ROUGE) used for evaluating the quality of summaries produced by text systems. The ROUGE score is typically calculated as the F1 score between the generated and reference summaries using the respective metric.",
      "page_start": 14,
      "page_end": 15
    },
    {
      "section_name": "Comparative Analysis For Emotion Recognition And Generation Models",
      "text": "This section presents a comparative analysis of SOTA methods in ER and EG for faces, speech, and text. We will discuss the most effective methods based on their performance in recognising and generating emotions across these modalities. The performance of these models will be evaluated through experiments and the corresponding results. However, comparing these methods poses challenges due to a lack of uniformity in evaluation metrics, complicating the assessment process. By conducting this comparative analysis of SOTA models, we aim to highlight the most effective methods for emotion recognition and generation.",
      "page_start": 15,
      "page_end": 15
    },
    {
      "section_name": "Facial Expression Recognition Comparative Analysis",
      "text": "2 summarises the evaluation of FER models, showcasing their performance across various datasets, with accuracy (ACC) as the primary metric. EmoFAN  [23]  achieves the highest accuracy of 75% on the AffectNet dataset, demonstrating exceptional capabilities in recognising Facial Expressions. Likewise, models such as Poster++  [78]  display impressive performance with an accuracy of 92% on the RAF-DB dataset. The variability in performance across different datasets highlights the unique challenges each dataset presents. For example, ESTLNet  [79]  exhibits lower performance on the FERV39K dataset, attaining an accuracy of 58.70%, yet it achieves a remarkable 99% accuracy on the CK+ dataset. The Sun 2023  [145]  model obtains SOTA scores across the JAFFE, CK+, and KDEF datasets, with an accuracy of 98.00% in each case.",
      "page_start": 15,
      "page_end": 15
    },
    {
      "section_name": "Facial Expression Generation Comparative Analysis",
      "text": "Both quantitative and qualitative methods are used to evaluate FEG models. However, the absence of a universal evaluation framework complicates comparisons across different studies. Most researchers omit estimating the accuracy   [165]  on the RAVDESS dataset attain a high accuracy of 92.88%. This comparison underscores the importance of developing versatile models capable of maintaining high performance across diverse datasets. The results also highlight the ongoing challenges and the necessity for further research to enhance the generalisability and robustness of SER on models across varying emotional contexts.",
      "page_start": 15,
      "page_end": 16
    },
    {
      "section_name": "Speech Emotion Generation Comparative Analysis",
      "text": "Comparative analyses of SEG methods remain limited, as many researchers choose not to compare their approaches against competitors, SEG techniques are evaluated based on their ability to reconstruct and generate voices.   The analysis also highlights variability in model performance across different datasets, underscoring the complexity of the task. For example, the AutoVC  [167]  model on the ESD dataset exhibits a high WER (87.0%) and CER (31.8%), reflecting difficulties in maintaining accuracy.   [178]  0.59 -EmotionLines *Multi-turn dialogue analysis  [179]  0.70 -EmotionLines *Kumar 2022  [86]  0.81 0.79 ISEAR *Feature selection  [180]  -0.73 ISEAR *Emotion distribution learning  [181]  0.67 0.67 ISEAR *XLM-T Barbieri 2021  [182]  0.67 0.79 Sem-EVAL 17 Ohman 2020  [183]  0.83 0.84 XED Accuracy and F1 score are the most commonly used metrics for TSR. Table  6  presents a comparison of SOTA methods evaluated on these metrics across different datasets. Notably, the Emotion BERT  [176]  model achieves the highest F1 score of 0.88 on the EmotionLines dataset, indicating its effectiveness in accurately recognising emotions from text. Similarly, the Ohman 2020  [183]  model demonstrates high F1 score (0.83) and accuracy (0.84) on the XED dataset, reflecting its robustness in TSR. In contrast, models such as AutoVC  [167] \" on the ESD dataset show significantly lower performance, with an F1 score of 0.47 and accuracy of 0.5, suggesting potential limitations in effectively recognising Text Sentiments. Models such as Kumar 2022  [86]  and XLM-EMO  [92]  demonstrate robust performance with F1 scores and accuracies around 0.85 across multiple datasets, showcasing their adaptability and effectiveness. Conversely, models evaluated on more complex datasets, such as the FERV39K dataset, exhibit lower performance. This comparative analysis emphasises the advancements achieved in TSR while also highlighting the need to enhance model accuracy and generalisability across text datasets.",
      "page_start": 16,
      "page_end": 17
    },
    {
      "section_name": "Text Sentiment Recognition Comparative Analysis",
      "text": "The GPT-4 model  [28]  achieves the highest MMLU score of 88.7%, demonstrating its strong performance in multi-task learning. This model also secures the highest HumanEval score of 90.2%, indicating its capability to generate realistic text. In contrast, models such as Gemini Ultra 1.0  [30]  display significantly lower performance, with an MMLU score of 83.7% and low scores across several other metrics. The table illustrates the varying performance across different tasks, reflecting the strengths and weaknesses of each model. For instance, the Claude 3 Opus model  [184]  achieves high scores in MMLU (86.8%) and HumanEval (86.7%), indicating its balanced proficiency in both multi-task learning and text generation.",
      "page_start": 18,
      "page_end": 18
    },
    {
      "section_name": "Challenges And Future Directions",
      "text": "Despite significant advances in ER and EG across faces, speech, and text, several key challenges remain. The inherent complexity of emotions-often difficult for humans to interpret reliably-creates challenges for machines, especially in speech and text ER and EG, where non-verbal cues are absent. Subtle expressions of emotions, such as microexpressions, in FEG add further complexity to emotion recognition and generation processes. A promising direction is to integrate multiple modalities, such as facial cues, speech, text, and body language, to create more robust systems.\n\nAdvancements in natural language processing (NLP), particularly through transformer models like GPT and BERT, are also essential for capturing linguistic nuances and cultural differences in emotional expression. Generating subtle and dynamic emotions in real time is another challenge, especially for interactive applications like virtual reality. Improved real-time emotion tracking is essential to make ER and EG systems more responsive and functional in dynamic environments.\n\nA shortage of large, diverse datasets limits progress in ER and EG. Current datasets often contain biases or labelling errors and lack generalisability, which hampers model performance. Efforts to collect \"in-the-wild\" datasets that reflect real-world emotional dynamics and include multiple languages would improve model effectiveness and fairness. Standardised evaluation metrics are also needed to enable consistent assessment and comparison of models. Openaccess benchmarks would provide clear standards for evaluating models, measuring both accuracy and emotional appropriateness, and fostering progress across the field. Ethical concerns, such as the misuse of deepfake technology, indicate the need for ethical guidelines and detection mechanisms without hindering technological progress. Finally, techniques like model compression and the use of pretrained models as a foundation for new applications can reduce computational costs.",
      "page_start": 19,
      "page_end": 20
    },
    {
      "section_name": "Conclusion",
      "text": "This survey explored state-of-the-art methods in emotion recognition and generation across facial, vocal, and textual modalities. With advances in AI, deep learning techniques have enhanced both the accuracy of emotional analysis and the realism of generated content. In particular, deep learning models, such as CNNs and attention-based architectures, have improved FER by learning features directly from raw data. Likewise, SER has advanced through models that integrate linguistic and acoustic features, enhancing classification accuracy through prosodic and contextual analysis.\n\nDespite progress, challenges remain in FEG, SEG and TSG. In FEG, accurately capturing the nuances of facial muscle movements and micro-expressions presents substantial difficulty, while ensuring emotional coherence across frames adds further complexity. Similarly, generating realistic emotions in speech and text requires addressing the intricate subtleties of tone, intonation, context, and emotional consistency. Limited labelled data, especially for in-the-wild systems, also impedes model robustness and generalizability. Future research should focus on expanding dataset diversity and improving models for under-explored modalities like speech and text. Multimodal approaches, enabling emotion analysis and generation across faces, speech, and text, hold promise. Ethical considerations, such as preventing misuse in deepfakes, should also guide future developments, paving the way for more empathetic and context-aware AI applications.",
      "page_start": 19,
      "page_end": 20
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The EmoFAN pipeline integrates facial landmark detection, discrete emotion classification, and continuous",
      "page": 5
    },
    {
      "caption": "Figure 2: The SER model processes frame-level speech features as input, using a 2-layer LSTM to generate outputs",
      "page": 6
    },
    {
      "caption": "Figure 3: The TER system by[86] uses a BERT-based dual-channel pipeline for text emotion recognition. First, input",
      "page": 7
    },
    {
      "caption": "Figure 4: In the place of 3D modelling, EMO utilising Stable Diffusion for generating new frames. The pipeline",
      "page": 8
    },
    {
      "caption": "Figure 5: The PromptVC pipeline uses a latent diffusion model for voice style conversion using natural language",
      "page": 9
    },
    {
      "caption": "Figure 6: The LLaMA pipeline involves pre-training transformer-based models on large textual datasets, followed",
      "page": 10
    },
    {
      "caption": "Figure 6: ). TSG have the ability",
      "page": 11
    },
    {
      "caption": "Figure 4: shows audio driven Face Expression generation by [99]. This method for Face Expression generation takes a",
      "page": 11
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Name": "AffectNet",
          "Description": "Extensive facial\nimagery dataset anno-\ntated with discrete and continuous emo-\ntion labels.",
          "Type": "Image",
          "Size": "450,000 images",
          "Emotions": "Surprise,\nfear, disgust,\nhappiness,\nsadness,\nanger, neutral, contempt"
        },
        {
          "Name": "RAF-DB",
          "Description": "Diverse facial expression dataset featur-\ning multiple genders, ages, and ethnici-\nties.",
          "Type": "Image",
          "Size": "29,672 images",
          "Emotions": "Surprise,\nfear, disgust,\nhappiness,\nsadness,\nanger, neutral"
        },
        {
          "Name": "FERPlus",
          "Description": "Derived from the FER2013 dataset, en-\nhancing expression annotations through\ncrowdsourcing.",
          "Type": "Image",
          "Size": "Unlimited",
          "Emotions": "Surprise,\nfear, disgust,\nhappiness,\nsadness,\nanger, neutral, contempt"
        },
        {
          "Name": "AFEW",
          "Description": "High-resolution videos from YouTube\nwith over 300 subjects and 10,000 sen-\ntences.",
          "Type": "Video",
          "Size": "16 hours",
          "Emotions": "Surprise,\nfear, disgust,\nhappiness,\nsadness,\nanger, neutral"
        },
        {
          "Name": "HDTF",
          "Description": "Video clips gathered from TV shows and\nmovies,\nincluding various head poses\nand occlusions.",
          "Type": "Video",
          "Size": "1,809 clips",
          "Emotions": "Surprise,\nfear, disgust,\nhappiness,\nsadness,\nanger, neutral"
        },
        {
          "Name": "AFEW-VA",
          "Description": "Video clips annotated for valence and\narousal levels, with 68 facial landmarks\nper frame.",
          "Type": "Video",
          "Size": "600 clips",
          "Emotions": "Surprise,\nfear, disgust,\nhappiness,\nsadness,\nanger, neutral"
        },
        {
          "Name": "DFEW",
          "Description": "Facial expression dataset created from\nmore than 1,500 movies.",
          "Type": "Video",
          "Size": "12,059 clips",
          "Emotions": "Happiness,\nanger,\nsad-\nness,\nfear, disgust, sur-\nprise, neutral"
        },
        {
          "Name": "CK+",
          "Description": "Laboratory-controlled video data captur-\ning transitions from neutral to peak ex-\npression.",
          "Type": "Video",
          "Size": "593 sequences",
          "Emotions": "Surprise,\nfear, disgust,\nhappiness,\nsadness,\nanger, contempt"
        },
        {
          "Name": "MEAD",
          "Description": "High-resolution emotional audiovisual\ndataset with 60 actors.",
          "Type": "Video & audio",
          "Size": "16,800 hours",
          "Emotions": "Surprise,\nfear, disgust,\nhappiness,\nsadness,\nanger, contempt"
        },
        {
          "Name": "LRW",
          "Description": "Video\nsequences\nof\npeople\nspeaking\nwords in uncontrolled conditions.",
          "Type": "Video & audio",
          "Size": "1,000 utterances",
          "Emotions": "Unlabeled"
        },
        {
          "Name": "LibriTTS",
          "Description": "Multi-speaker English corpus of\nread\nspeech at 24kHz for TTS research.",
          "Type": "Audio",
          "Size": "585 hours",
          "Emotions": "Unlabeled"
        },
        {
          "Name": "VCC2018",
          "Description": "Dataset\nfor speech-to-speech systems,\nconsisting of male and female speakers.",
          "Type": "Audio",
          "Size": "464 sentences",
          "Emotions": "Unlabeled"
        },
        {
          "Name": "ESD",
          "Description": "Collection of audio recordings for study-\ning emotions expressed through speech.",
          "Type": "Audio",
          "Size": "7,000\nutter-\nances",
          "Emotions": "Neutral,\nhappy,\nangry,\nsad, surprise"
        },
        {
          "Name": "Empathetic Dialogues",
          "Description": "Open-domain\nconversations\nbetween\nspeakers and listeners for empathic re-\nsponses.",
          "Type": "Audio",
          "Size": "24,850\nconver-\nsations",
          "Emotions": "32 emotion labels"
        },
        {
          "Name": "EMO-DB",
          "Description": "German emotional speech recorded by\nten professional speakers.",
          "Type": "Audio",
          "Size": "535 utterances",
          "Emotions": "7 emotions"
        },
        {
          "Name": "CASIA",
          "Description": "Mandarin emotional speech dataset.",
          "Type": "Audio",
          "Size": "1,200 snippets",
          "Emotions": "6 emotions"
        },
        {
          "Name": "Amazon Reviews",
          "Description": "Large dataset of product\nreviews pro-\nvided by Amazon.",
          "Type": "Text",
          "Size": "Unlimited",
          "Emotions": "-"
        },
        {
          "Name": "Twitter",
          "Description": "Collection of\ntweets\nfor\nsocial media\ntext analysis.",
          "Type": "Text",
          "Size": "Unlimited",
          "Emotions": "-"
        },
        {
          "Name": "Reddit",
          "Description": "Comments and posts from Reddit\nfor\nunderstanding informal language.",
          "Type": "Text",
          "Size": "Unlimited",
          "Emotions": "-"
        }
      ],
      "page": 4
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Text Sentiment Generation Comparative Analysis Table 7: TSG Comparative Analysis: *Results derived from",
      "venue": "Text Sentiment Generation Comparative Analysis Table 7: TSG Comparative Analysis: *Results derived from"
    },
    {
      "citation_id": "2",
      "title": "",
      "authors": [
        "Model"
      ],
      "venue": ""
    },
    {
      "citation_id": "3",
      "title": "utilising metrics such as MMLU, GQPA, MATH, HumanEval, MGSM, and DROP (F1) across various tasks. Due to potential biases inherent in user studies, there is considerable variability in the performance of TSG models across different experiments. This variability may stem from the of the questions posed, the diversity in answers generated by the TSG models, and the subjective opinions of the respondents. For consistency, we have selected the results from [28]. Table 7 evaluates the performance of large language models (LLMs) in text generation across multiple tasks",
      "venue": "utilising metrics such as MMLU, GQPA, MATH, HumanEval, MGSM, and DROP (F1) across various tasks. Due to potential biases inherent in user studies, there is considerable variability in the performance of TSG models across different experiments. This variability may stem from the of the questions posed, the diversity in answers generated by the TSG models, and the subjective opinions of the respondents. For consistency, we have selected the results from [28]. Table 7 evaluates the performance of large language models (LLMs) in text generation across multiple tasks"
    },
    {
      "citation_id": "4",
      "title": "Recognizing action units for facial expression analysis",
      "authors": [
        "Y-I Tian",
        "Takeo Kanade",
        "Jeffrey Cohn"
      ],
      "year": "2001",
      "venue": "IEEE Transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "5",
      "title": "Face validity",
      "authors": [
        "Ellen Johnson"
      ],
      "year": "2021",
      "venue": "Encyclopedia of autism spectrum disorders"
    },
    {
      "citation_id": "6",
      "title": "The expression of the emotions in man and animals",
      "authors": [
        "C Darwin",
        "Prodger"
      ],
      "year": "1998",
      "venue": "The expression of the emotions in man and animals"
    },
    {
      "citation_id": "7",
      "title": "Constants across cultures in the face and emotion",
      "authors": [
        "Paul Ekman",
        "Wallace Friesen"
      ],
      "year": "1971",
      "venue": "Journal of personality and social psychology"
    },
    {
      "citation_id": "8",
      "title": "Strong evidence for universals in facial expressions: A reply to russell's mistaken critique",
      "authors": [
        "Paul Ekman"
      ],
      "year": "1994",
      "venue": "Strong evidence for universals in facial expressions: A reply to russell's mistaken critique"
    },
    {
      "citation_id": "9",
      "title": "",
      "authors": [
        "Einfochips",
        "Einfochips"
      ],
      "year": "2024",
      "venue": ""
    },
    {
      "citation_id": "10",
      "title": "",
      "authors": [
        "Elevate Ai",
        "A Elevate"
      ],
      "year": "2024",
      "venue": ""
    },
    {
      "citation_id": "11",
      "title": "Isabel Barbeito Lacerda, and Marcia Cristina Nascimento Dourado. Facial expression recognition of emotional situations in mild and moderate alzheimer's disease",
      "authors": [
        "Michelle Brandt",
        "Felipe De Oliveira",
        "JosÃ© Silva",
        "Pedro SimÃµes",
        "Maria Neto",
        "Alice Baptista",
        "Tatiana Belfort"
      ],
      "year": "2024",
      "venue": "Journal of Geriatric Psychiatry and Neurology"
    },
    {
      "citation_id": "12",
      "title": "Mental states and personality based on real-time physical activity and facial expression recognition",
      "authors": [
        "Yating Huang",
        "Dengyue Zhai",
        "Jingze Song",
        "Xuanheng Rao",
        "Xiao Sun",
        "Jin Tang"
      ],
      "year": "2023",
      "venue": "Frontiers in Psychiatry"
    },
    {
      "citation_id": "13",
      "title": "Using a social robot to evaluate facial expressions in the wild",
      "authors": [
        "Silvia Ramis",
        "Jose Buades",
        "Francisco Perales"
      ],
      "year": "2020",
      "venue": "Sensors"
    },
    {
      "citation_id": "14",
      "title": "Classroom teaching evaluation based on facial expression recognition",
      "authors": [
        "Xiao-Yu Tang",
        "Wang-Yue Peng",
        "Si-Rui Liu",
        "Jian-Wen Xiong"
      ],
      "year": "2020",
      "venue": "Proceedings of the 2020 9th International Conference on Educational and Information Technology"
    },
    {
      "citation_id": "15",
      "title": "Driver fatigue detection based on deeply-learned facial expression representation",
      "authors": [
        "Zhongmin Liu",
        "Yuxi Peng",
        "Wenjin Hu"
      ],
      "year": "2020",
      "venue": "Journal of Visual Communication and Image Representation"
    },
    {
      "citation_id": "16",
      "title": "An accurate facial expression detector using multilandmarks selection and local transform features",
      "authors": [
        "Ahmad Syeda Amna Rizwan",
        "Kibum Jalal",
        "Kim"
      ],
      "year": "2020",
      "venue": "2020 3rd International conference on advancements in computational sciences (ICACS)"
    },
    {
      "citation_id": "17",
      "title": "Societies becoming the same: Visual representation of the individual via the faceapp: Application",
      "authors": [
        "Hilal Sansar"
      ],
      "year": "2023",
      "venue": "International Symposium on Intelligent Manufacturing and Service Systems"
    },
    {
      "citation_id": "18",
      "title": "Is there an app for that? a review of popular apps for depression, anxiety, and well-being",
      "authors": [
        "Emma Akash R Wasil",
        "Lorenzo Palermo",
        "Robert Lorenzo-Luaces",
        "Derubeis"
      ],
      "year": "2022",
      "venue": "Cognitive and Behavioral Practice"
    },
    {
      "citation_id": "19",
      "title": "Evaluating the therapeutic alliance with a free-text cbt conversational agent (wysa): a mixed-methods study",
      "authors": [
        "Clare Beatty",
        "Tanya Malik",
        "Saha Meheli",
        "Chaitali Sinha"
      ],
      "year": "2022",
      "venue": "Frontiers in Digital Health"
    },
    {
      "citation_id": "20",
      "title": "A comprehensive review of facial expression recognition techniques",
      "authors": [
        "R Rashmi Adyapady",
        "B Annappa"
      ],
      "venue": "Multimedia Systems"
    },
    {
      "citation_id": "21",
      "title": "A survey of textual emotion recognition and its challenges",
      "authors": [
        "Jiawen Deng",
        "Fuji Ren"
      ],
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "22",
      "title": "Speech emotion recognition: a comprehensive survey",
      "authors": [
        "Mohammed Jawad",
        "Abbas Ebrahimi-Moghadam"
      ],
      "year": "2023",
      "venue": "Wireless Personal Communications"
    },
    {
      "citation_id": "23",
      "title": "A comprehensive survey of ai-generated content",
      "authors": [
        "Yihan Cao",
        "Siyu Li",
        "Yixin Liu",
        "Zhiling Yan",
        "Yutong Dai",
        "Philip Yu",
        "Lichao Sun"
      ],
      "year": "2023",
      "venue": "A history of generative ai from gan to chatgpt",
      "arxiv": "arXiv:2303.04226"
    },
    {
      "citation_id": "24",
      "title": "A comprehensive survey and analysis of generative models in machine learning",
      "authors": [
        "Mahendra Gm Harshvardhan",
        "Manjusha Kumar Gourisaria",
        "Siddharth Pandey",
        "Rautaray Swarup"
      ],
      "year": "2020",
      "venue": "Computer Science Review"
    },
    {
      "citation_id": "25",
      "title": "Deep facial expression recognition: A survey",
      "authors": [
        "Shan Li",
        "Weihong Deng"
      ],
      "year": "2018",
      "venue": "Deep facial expression recognition: A survey"
    },
    {
      "citation_id": "26",
      "title": "Estimation of continuous valence and arousal levels from faces in naturalistic conditions",
      "authors": [
        "Antoine Toisoul",
        "Jean Kossaifi",
        "Adrian Bulat",
        "Georgios Tzimiropoulos",
        "Maja Pantic"
      ],
      "year": "2021",
      "venue": "Nature Machine Intelligence"
    },
    {
      "citation_id": "27",
      "title": "Training and profiling a pediatric facial expression classifier for children on mobile devices: machine learning study",
      "authors": [
        "Agnik Banerjee",
        "Onur Cezmi Mutlu",
        "Aaron Kline",
        "Saimourya Surabhi",
        "Peter Washington",
        "Dennis Wall"
      ],
      "year": "2023",
      "venue": "JMIR formative research"
    },
    {
      "citation_id": "28",
      "title": "Effects of digital avatar on perceived social presence and co-presence in business meetings between the managers and their co-workers",
      "authors": [
        "Mika Yasuoka",
        "Marko Zivko",
        "Hiroshi Ishiguro",
        "Yuichiro Yoshikawa",
        "Kazuki Sakai"
      ],
      "year": "2022",
      "venue": "International Conference on Collaboration Technologies and Social Computing"
    },
    {
      "citation_id": "29",
      "title": "\"i'm here for you\": Can social chatbots truly support their users? a literature review",
      "authors": [
        "Emmelyn Aj Marloes Mc Van Wezel",
        "Marjolijn Croes",
        "Antheunis"
      ],
      "year": "2020",
      "venue": "Chatbot Research and Design: 4th International Workshop, CONVERSATIONS 2020, Virtual Event"
    },
    {
      "citation_id": "30",
      "title": "",
      "authors": [
        "Characterai",
        "Characterai"
      ],
      "year": "2024",
      "venue": ""
    },
    {
      "citation_id": "31",
      "title": "Shyamal Anadkat, et al. Gpt-4 technical report",
      "authors": [
        "Josh Achiam",
        "Steven Adler",
        "Sandhini Agarwal",
        "Lama Ahmad",
        "Ilge Akkaya",
        "Florencia Leoni Aleman",
        "Diogo Almeida",
        "Janko Altenschmidt",
        "Sam Altman"
      ],
      "year": "2023",
      "venue": "Shyamal Anadkat, et al. Gpt-4 technical report",
      "arxiv": "arXiv:2303.08774"
    },
    {
      "citation_id": "32",
      "title": "Meta. Llama",
      "year": "2024",
      "venue": "Meta. Llama"
    },
    {
      "citation_id": "33",
      "title": "a family of highly capable multimodal models",
      "authors": [
        "Gemini Team",
        "Rohan Anil",
        "Sebastian Borgeaud",
        "Yonghui Wu",
        "Jean-Baptiste Alayrac",
        "Jiahui Yu",
        "Radu Soricut",
        "Johan Schalkwyk",
        "Andrew Dai",
        "Anja Hauth"
      ],
      "year": "2023",
      "venue": "a family of highly capable multimodal models",
      "arxiv": "arXiv:2312.11805"
    },
    {
      "citation_id": "34",
      "title": "Imagenet classification with deep convolutional neural networks",
      "authors": [
        "Alex Krizhevsky",
        "Ilya Sutskever",
        "Geoffrey Hinton"
      ],
      "year": "2012",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "35",
      "title": "Gradient-based learning applied to document recognition",
      "authors": [
        "Yann Lecun",
        "LÃ©on Bottou",
        "Yoshua Bengio",
        "Patrick Haffner"
      ],
      "year": "1998",
      "venue": "Proceedings of the IEEE"
    },
    {
      "citation_id": "36",
      "title": "Digital Image Processing",
      "authors": [
        "C Rafael",
        "Richard Gonzalez",
        "Woods"
      ],
      "year": "2002",
      "venue": "Digital Image Processing"
    },
    {
      "citation_id": "37",
      "title": "Exploratory Data Analysis",
      "authors": [
        "John Tukey"
      ],
      "year": "1977",
      "venue": "Exploratory Data Analysis"
    },
    {
      "citation_id": "38",
      "title": "Adaptive histogram equalization and its variations",
      "authors": [
        "Stephen Pizer",
        "E Philip Amburn",
        "John Austin",
        "Robert Cromartie",
        "Alan Geselowitz",
        "Trey Greer",
        "Bartter Ter Haar",
        "John Romeny",
        "Karel Zimmerman",
        "Zuiderveld"
      ],
      "year": "1987",
      "venue": "Computer Vision, Graphics, and Image Processing"
    },
    {
      "citation_id": "39",
      "title": "A survey on image data augmentation for deep learning",
      "authors": [
        "Connor Shorten",
        "M Taghi",
        "Khoshgoftaar"
      ],
      "year": "2019",
      "venue": "Journal of Big Data"
    },
    {
      "citation_id": "40",
      "title": "Rapid object detection using a boosted cascade of simple features",
      "authors": [
        "Paul Viola",
        "Michael Jones"
      ],
      "year": "2001",
      "venue": "Proceedings of the 2001 IEEE Computer Society Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "41",
      "title": "Facial landmark detection by deep multi-task learning",
      "authors": [
        "Zhifeng Zhang",
        "Ping Luo"
      ],
      "year": "2014",
      "venue": "European Conference on Computer Vision"
    },
    {
      "citation_id": "42",
      "title": "Very deep convolutional networks for large-scale image recognition",
      "authors": [
        "Karen Simonyan",
        "Andrew Zisserman"
      ],
      "year": "2014",
      "venue": "Very deep convolutional networks for large-scale image recognition",
      "arxiv": "arXiv:1409.1556"
    },
    {
      "citation_id": "43",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "Kaiming He",
        "Xiangyu Zhang",
        "Shaoqing Ren",
        "Jian Sun"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "44",
      "title": "Mobilenets: Efficient convolutional neural networks for mobile vision applications",
      "authors": [
        "Andrew Howard",
        "Menglong Zhu",
        "Bo Chen",
        "Dmitry Kalenichenko",
        "Weijun Wang",
        "Tobias Weyand",
        "Marco Andreetto",
        "Hartwig Adam"
      ],
      "year": "2017",
      "venue": "Mobilenets: Efficient convolutional neural networks for mobile vision applications",
      "arxiv": "arXiv:1704.04861"
    },
    {
      "citation_id": "45",
      "title": "Image quality assessment: from error visibility to structural similarity",
      "authors": [
        "Zhou Wang",
        "Alan Bovik",
        "Hamid Sheikh",
        "Eero Simoncelli"
      ],
      "year": "2004",
      "venue": "IEEE Transactions on Image Processing"
    },
    {
      "citation_id": "46",
      "title": "Suppression of acoustic noise in speech using spectral subtraction",
      "authors": [
        "F Steven",
        "Boll"
      ],
      "year": "1979",
      "venue": "IEEE Transactions on Acoustics, Speech, and Signal Processing"
    },
    {
      "citation_id": "47",
      "title": "Enhancement and bandwidth compression of noisy speech",
      "authors": [
        "Jae Lim",
        "Alan Oppenheim"
      ],
      "year": "1979",
      "venue": "Proceedings of the IEEE"
    },
    {
      "citation_id": "48",
      "title": "Adaptive Signal Processing",
      "authors": [
        "Bernard Widrow",
        "Samuel Stearns"
      ],
      "year": "1985",
      "venue": "Adaptive Signal Processing"
    },
    {
      "citation_id": "49",
      "title": "Fundamentals of Speech Recognition",
      "authors": [
        "Lawrence Rabiner",
        "Biing-Hwang Juang"
      ],
      "year": "1993",
      "venue": "Fundamentals of Speech Recognition"
    },
    {
      "citation_id": "50",
      "title": "An algorithm for determining the endpoints of isolated utterances. The Bell System Technical",
      "authors": [
        "Lawrence Rabiner",
        "Myron Sambur"
      ],
      "year": "1975",
      "venue": "Journal"
    },
    {
      "citation_id": "51",
      "title": "Unsupervised speech activity detection using voicing measures and perceptual spectral flux",
      "authors": [
        "O Shahin",
        "John Sadjadi",
        "Hansen"
      ],
      "year": "2013",
      "venue": "IEEE Signal Processing Letters"
    },
    {
      "citation_id": "52",
      "title": "Comparison of parametric representations for monosyllabic word recognition in continuously spoken sentences",
      "authors": [
        "Steven Davis",
        "Paul Mermelstein"
      ],
      "year": "1980",
      "venue": "IEEE Transactions on Acoustics, Speech, and Signal Processing"
    },
    {
      "citation_id": "53",
      "title": "Linear prediction: A tutorial review",
      "authors": [
        "John Makhoul"
      ],
      "year": "1975",
      "venue": "Proceedings of the IEEE"
    },
    {
      "citation_id": "54",
      "title": "Accurate short-term analysis of the fundamental frequency and the harmonics-to-noise ratio of a sampled sound",
      "authors": [
        "Paul Boersma"
      ],
      "year": "1193",
      "venue": "Proceedings of the Institute of Phonetic Sciences"
    },
    {
      "citation_id": "55",
      "title": "Multirate Digital Signal Processing",
      "authors": [
        "Ronald Crochiere",
        "Lawrence Rabiner"
      ],
      "year": "1983",
      "venue": "Multirate Digital Signal Processing"
    },
    {
      "citation_id": "56",
      "title": "",
      "authors": [
        "A Patrick",
        "Nikolay Naylor",
        "Gaubitch"
      ],
      "year": "2010",
      "venue": ""
    },
    {
      "citation_id": "57",
      "title": "Speech enhancement using vector quantization and a formant distance measure",
      "authors": [
        "O' Douglas",
        "Shaughnessy"
      ],
      "year": "1988",
      "venue": "International Conference on Acoustics, Speech, and Signal Processing"
    },
    {
      "citation_id": "58",
      "title": "Frame blocking and windowing speech signal",
      "authors": [
        "Oday Kamil"
      ],
      "year": "2018",
      "venue": "Journal of Information, Communication, and Intelligence Systems (JICIS)"
    },
    {
      "citation_id": "59",
      "title": "A recursive feature vector normalization approach for robust speech recognition in noise",
      "authors": [
        "Olli Viikki",
        "David Bye",
        "Kari Laurila"
      ],
      "year": "1998",
      "venue": "Proceedings of the 1998 IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP'98 (Cat. No. 98CH36181)"
    },
    {
      "citation_id": "60",
      "title": "Efficient estimation of word representations in vector space",
      "authors": [
        "Tomas Mikolov",
        "Kai Chen",
        "Greg Corrado",
        "Jeffrey Dean"
      ],
      "year": "2013",
      "venue": "Efficient estimation of word representations in vector space",
      "arxiv": "arXiv:1301.3781"
    },
    {
      "citation_id": "61",
      "title": "Introduction to Information Retrieval",
      "authors": [
        "D Christopher",
        "Prabhakar Manning",
        "Hinrich Raghavan",
        "SchÃ¼tze"
      ],
      "year": "2008",
      "venue": "Introduction to Information Retrieval"
    },
    {
      "citation_id": "62",
      "title": "An algorithm for suffix stripping",
      "authors": [
        "Martin Porter"
      ],
      "year": "1980",
      "venue": "Program"
    },
    {
      "citation_id": "63",
      "title": "Natural Language Processing with Python",
      "authors": [
        "Steven Bird",
        "Ewan Klein",
        "Edward Loper"
      ],
      "year": "2009",
      "venue": "Natural Language Processing with Python"
    },
    {
      "citation_id": "64",
      "title": "Credit card fraud detection with a neural-network",
      "authors": [
        "S Ghosh",
        "D Reilly"
      ],
      "year": "1994",
      "venue": "Proceedings of the 27th Annual Hawaii International Conference on System Sciences"
    },
    {
      "citation_id": "65",
      "title": "Glove: Global vectors for word representation",
      "authors": [
        "Jeffrey Pennington",
        "Richard Socher",
        "Christopher Manning"
      ],
      "year": "2014",
      "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)"
    },
    {
      "citation_id": "66",
      "title": "Neural Network Methods for Natural Language Processing",
      "authors": [
        "Yoav Goldberg"
      ],
      "year": "2017",
      "venue": "Neural Network Methods for Natural Language Processing"
    },
    {
      "citation_id": "67",
      "title": "Distributed representations of words and phrases and their compositionality",
      "authors": [
        "Tomas Mikolov",
        "Ilya Sutskever",
        "Kai Chen",
        "Greg Corrado",
        "Jeffrey Dean"
      ],
      "year": "2013",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "68",
      "title": "Deep pyramid convolutional neural networks for text categorization",
      "authors": [
        "Rie Johnson",
        "Tong Zhang"
      ],
      "year": "2017",
      "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "69",
      "title": "Eda: Easy data augmentation techniques for boosting performance on text classification tasks",
      "authors": [
        "Jason Wei",
        "Kai Zou"
      ],
      "year": "2019",
      "venue": "Eda: Easy data augmentation techniques for boosting performance on text classification tasks",
      "arxiv": "arXiv:1901.11196"
    },
    {
      "citation_id": "70",
      "title": "The interspeech 2010 paralinguistic challenge",
      "authors": [
        "BjÃ¶rn Schuller",
        "Stefan Steidl",
        "Andreas Batliner",
        "Felix Burkhardt",
        "Laurence Devillers",
        "Christian MÃ¼ller",
        "Shrikanth Narayanan"
      ],
      "year": "2010",
      "venue": "Proceedings of INTERSPEECH 2010"
    },
    {
      "citation_id": "71",
      "title": "Deep learning",
      "authors": [
        "Yann Lecun",
        "Yoshua Bengio",
        "Geoffrey Hinton"
      ],
      "year": "2015",
      "venue": "Nature"
    },
    {
      "citation_id": "72",
      "title": "The computer expression recognition toolbox (cert)",
      "authors": [
        "G Littlewort",
        "J Whitehill",
        "T Wu",
        "I Fasel",
        "M Frank",
        "J Movellan",
        "M Bartlett"
      ],
      "year": "2011",
      "venue": "Proceedings of the IEEE International Conference on Automatic Face & Gesture Recognition and Workshops (FG)"
    },
    {
      "citation_id": "73",
      "title": "Multi-modal face and audio-visual emotion recognition in development",
      "authors": [
        "Ziheng Zhang",
        "Michael Lyons"
      ],
      "year": "2011",
      "venue": "Proceedings of the IEEE International Conference on Automatic Face & Gesture Recognition (FG)"
    },
    {
      "citation_id": "74",
      "title": "Recognizing facial expression: Machine learning and application to spontaneous behavior",
      "authors": [
        "M Bartlett",
        "G Littlewort",
        "M Frank",
        "C Lainscsek",
        "I Fasel",
        "J Movellan"
      ],
      "year": "2006",
      "venue": "Neural Networks"
    },
    {
      "citation_id": "75",
      "title": "Challenges in representation learning: A report on three machine learning contests",
      "authors": [
        "Ian Goodfellow",
        "Dumitru Erhan",
        "Pierre Carrier",
        "Aaron Courville",
        "Mehdi Mirza",
        "Ben Hamner",
        "Will Cukierski",
        "Yichuan Tang",
        "David Thaler",
        "Dong-Hyun Lee"
      ],
      "year": "2013",
      "venue": "International Conference on Neural Information Processing"
    },
    {
      "citation_id": "76",
      "title": "Emoca: Emotion driven monocular face capture and animation",
      "authors": [
        "Radek Danecek",
        "Michael Black",
        "Timo Bolkart"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "77",
      "title": "Abaw: Valence-arousal estimation, expression recognition, action unit detection & emotional reaction intensity estimation challenges",
      "authors": [
        "Dimitrios Kollias",
        "Panagiotis Tzirakis",
        "Hume Ai",
        "Alice Hume",
        "Alice Hume Ai",
        "Alan Cowen",
        "Alan Hume",
        "Stefanos Zafeiriou"
      ],
      "year": "2023",
      "venue": "Abaw: Valence-arousal estimation, expression recognition, action unit detection & emotional reaction intensity estimation challenges"
    },
    {
      "citation_id": "78",
      "title": "Learning transferable visual models from natural language supervision",
      "authors": [
        "Alec Radford",
        "Jong Kim",
        "Chris Hallacy",
        "Aditya Ramesh",
        "Gabriel Goh",
        "Sandhini Agarwal",
        "Girish Sastry",
        "Amanda Askell",
        "Pamela Mishkin",
        "Jack Clark",
        "Gretchen Krueger",
        "Ilya Sutskever"
      ],
      "year": "2021",
      "venue": "Learning transferable visual models from natural language supervision",
      "arxiv": "arXiv:2103.00020"
    },
    {
      "citation_id": "79",
      "title": "Prompting visual-language models for dynamic facial expression recognition",
      "authors": [
        "Zengqun Zhao",
        "Ioannis Patras"
      ],
      "year": "2023",
      "venue": "Prompting visual-language models for dynamic facial expression recognition",
      "arxiv": "arXiv:2308.13382"
    },
    {
      "citation_id": "80",
      "title": "Patch attention convolutional vision transformer for facial expression recognition with occlusion",
      "authors": [
        "Chang Liu",
        "Kaoru Hirota",
        "Yaping Dai"
      ],
      "venue": "Information Sciences"
    },
    {
      "citation_id": "81",
      "title": "Poster++: A simpler and stronger facial expression recognition network",
      "authors": [
        "Jiawei Mao",
        "Rui Xu",
        "Xuesong Yin",
        "Yuanqi Chang",
        "Binling Nie",
        "Aibin Huang"
      ],
      "venue": "Poster++: A simpler and stronger facial expression recognition network"
    },
    {
      "citation_id": "82",
      "title": "Enhanced spatial-temporal learning network for dynamic facial expression recognition",
      "authors": [
        "Weijun Gong",
        "Yurong Qian",
        "Weihang Zhou",
        "Hongyong Leng"
      ],
      "year": "2024",
      "venue": "Biomedical Signal Processing and Control"
    },
    {
      "citation_id": "83",
      "title": "Towards accurate marker-less 3D facial performance capture",
      "authors": [
        "Ayush Tewari",
        "Michael ZollhÃ¶fer",
        "Justus Thies",
        "Pablo Garrido",
        "Florian Bernard",
        "Derek Bradley",
        "Thabo Beeler",
        "Patrick Perez",
        "Christian Theobalt"
      ],
      "year": "2020",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "84",
      "title": "Speech emotion classification using attention-based lstm",
      "authors": [
        "Yue Xie",
        "Ruiyu Liang",
        "Zhenlin Liang",
        "Chengwei Huang",
        "Cairong Zou",
        "BjÃ¶rn Schuller"
      ],
      "year": "2023",
      "venue": "Speech emotion classification using attention-based lstm"
    },
    {
      "citation_id": "85",
      "title": "Hidden markov model-based speech emotion recognition",
      "authors": [
        "BjÃ¶rn Schuller",
        "Gerhard Rigoll",
        "Manfred Lang"
      ],
      "year": "2003",
      "venue": "Proceedings 2003 IEEE International Conference on Acoustics, Speech, and Signal Processing"
    },
    {
      "citation_id": "86",
      "title": "Vocal communication of emotion: A review of research paradigms",
      "authors": [
        "Klaus Scherer"
      ],
      "year": "2003",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "87",
      "title": "Exploiting emotion-semantic correlations for empathetic response generation",
      "authors": [
        "Zhou Yang",
        "Zhaochun Ren",
        "Yufeng Wang",
        "Xiaofei Zhu",
        "Zhihao Chen",
        "Tiecheng Cai",
        "Yunbing Wu",
        "Yisong Su",
        "Sibo Ju",
        "Xiangwen Liao"
      ],
      "year": "2023",
      "venue": "Findings of the Association for Computational Linguistics: EMNLP 2023"
    },
    {
      "citation_id": "88",
      "title": "Speech emotion recognition using deep convolutional neural networks improved by the fast continuous wavelet transform",
      "authors": [
        "E BjÃ¶rn",
        "Mathijs Van Zwol",
        "Lukas Langezaal",
        "Albert Arts",
        "Egon L Gatt",
        "Van Den",
        "Broek"
      ],
      "year": "2023",
      "venue": "Workshop Proceedings of the 19th International Conference on Intelligent Environments (IE2023)"
    },
    {
      "citation_id": "89",
      "title": "A bert based dual-channel explainable text emotion recognition system",
      "authors": [
        "Puneet Kumar",
        "Balasubramanian Raman"
      ],
      "venue": "Neural Networks"
    },
    {
      "citation_id": "90",
      "title": "Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition",
      "authors": [
        "Daniel Jurafsky",
        "James Martin"
      ],
      "year": "2009",
      "venue": "Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition"
    },
    {
      "citation_id": "91",
      "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "Jacob Devlin",
        "Ming-Wei Chang",
        "Kenton Lee",
        "Kristina Toutanova"
      ],
      "year": "2019",
      "venue": "BERT: Pre-training of deep bidirectional transformers for language understanding"
    },
    {
      "citation_id": "92",
      "title": "Attention is all you need",
      "authors": [
        "Vaswani"
      ],
      "year": "2017",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "93",
      "title": "Srl4e-semantic role labeling for emotions: A unified evaluation framework",
      "authors": [
        "Cesare Campagnano",
        "Simone Conia",
        "Roberto Navigli"
      ],
      "year": "2022",
      "venue": "Srl4e-semantic role labeling for emotions: A unified evaluation framework"
    },
    {
      "citation_id": "94",
      "title": "Clarin-emo: Training emotion recognition models using human annotation and chatgpt",
      "authors": [
        "BartÅ‚omiej Koptyra",
        "Anh Ngo",
        "Åukasz RadliÅ„ski",
        "Jan KocoÅ„"
      ],
      "year": "2023",
      "venue": "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics"
    },
    {
      "citation_id": "95",
      "title": "Xlm-emo: Multilingual emotion prediction in social media text",
      "authors": [
        "Federico Bianchi",
        "Debora Nozza",
        "Dirk Hovy"
      ],
      "year": "2022",
      "venue": "Xlm-emo: Multilingual emotion prediction in social media text"
    },
    {
      "citation_id": "96",
      "title": "Dual attention networks for multimodal reasoning and matching",
      "authors": [
        "Zhong-Yuan",
        "Jing-Yi Li",
        "Ming Duan",
        "Yu-Gang Zhou",
        "Zhao"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Image Processing"
    },
    {
      "citation_id": "97",
      "title": "Acoustic emotion recognition: A benchmark comparison of performances",
      "authors": [
        "BjÃ¶rn Schuller",
        "Bogdan Vlasenko",
        "Florian Eyben",
        "Martin WÃ¶llmer",
        "Gerhard Rigoll"
      ],
      "year": "2004",
      "venue": "Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "98",
      "title": "A survey of affect recognition methods: Audio, visual, and spontaneous expressions",
      "authors": [
        "Zhihua Zeng",
        "Maja Pantic",
        "Glenn Roisman"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "99",
      "title": "A review of deepfakes and an analysis of detection methods",
      "authors": [
        "Michael Matton",
        "Marin Ferecatu",
        "Nozha Boujemaa"
      ],
      "year": "2019",
      "venue": "Journal of Imaging"
    },
    {
      "citation_id": "100",
      "title": "The deepfake detection challenge dataset",
      "authors": [
        "Balazs Dolhansky",
        "Alexander Howie",
        "Hui Zheng",
        "Ser-Nam Lim",
        "Charles Nicholas"
      ],
      "year": "2020",
      "venue": "The deepfake detection challenge dataset",
      "arxiv": "arXiv:2006.07397"
    },
    {
      "citation_id": "101",
      "title": "Bengio Yoshua and Bengio Samy. Deep Learning",
      "authors": [
        "Courville Aaron",
        "Goodfellow Ian"
      ],
      "year": "2016",
      "venue": "Bengio Yoshua and Bengio Samy. Deep Learning"
    },
    {
      "citation_id": "102",
      "title": "Emo: Emote portrait alive -generating expressive portrait videos with audio2video diffusion model under weak conditions",
      "authors": [
        "Linrui Tian",
        "Qi Wang",
        "Bang Zhang",
        "Liefeng Bo"
      ],
      "venue": "Emo: Emote portrait alive -generating expressive portrait videos with audio2video diffusion model under weak conditions"
    },
    {
      "citation_id": "103",
      "title": "Meta. Instagram",
      "year": "2024",
      "venue": "Meta. Instagram"
    },
    {
      "citation_id": "104",
      "title": "",
      "authors": [
        "Faceapp",
        "Faceapp"
      ],
      "year": "2017",
      "venue": ""
    },
    {
      "citation_id": "105",
      "title": "Vasa-1: Lifelike audio-driven talking faces generated in real time",
      "authors": [
        "Sicheng Xu",
        "Guojun Chen",
        "Yu-Xiao Guo",
        "Jiaolong Yang",
        "Chong Li",
        "Zhenyu Zang",
        "Yizhong Zhang",
        "Xin Tong",
        "Baining Guo"
      ],
      "year": "2024",
      "venue": "Vasa-1: Lifelike audio-driven talking faces generated in real time",
      "arxiv": "arXiv:2404.10667"
    },
    {
      "citation_id": "106",
      "title": "Neural emotion director: Speech-preserving semantic control of facial expressions in",
      "authors": [
        "Foivos Paraperas Papantoniou",
        "P Panagiotis",
        "Filntisis"
      ],
      "year": "2021",
      "venue": "Petros Maragos, and Anastasios Roussos"
    },
    {
      "citation_id": "107",
      "title": "Efficient emotional adaptation for audiodriven talking-head generation",
      "authors": [
        "Yuan Gan",
        "Zongxin Yang",
        "Xihang Yue",
        "Lingyun Sun",
        "Yi Yang"
      ],
      "year": "2023",
      "venue": "Efficient emotional adaptation for audiodriven talking-head generation"
    },
    {
      "citation_id": "108",
      "title": "Emotion guided speech-driven facial animation",
      "authors": [
        "Sewhan Chun",
        "Daegeun Choe",
        "Shindong Kang",
        "Shounan An",
        "Youngbak Jo",
        "Insoo Oh"
      ],
      "venue": "Emotion guided speech-driven facial animation"
    },
    {
      "citation_id": "109",
      "title": "Expressive speech-driven facial animation with controllable emotions",
      "authors": [
        "Yutong Chen",
        "Junhong Zhao",
        "Wei-Qiang Zhang"
      ],
      "year": "2023",
      "venue": "Expressive speech-driven facial animation with controllable emotions",
      "arxiv": "arXiv:2301.02008"
    },
    {
      "citation_id": "110",
      "title": "Pose-controllable talking face generation by implicitly modularized audio-visual representation",
      "authors": [
        "Hang Zhou",
        "Yasheng Sun",
        "Wayne Wu",
        "Chen Loy",
        "Xiaogang Wang",
        "Ziwei Liu"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "111",
      "title": "A lip sync expert is all you need for speech to lip generation in the wild",
      "authors": [
        "Rudrabha Kr Prajwal",
        "Mukhopadhyay",
        "P Vinay",
        "Namboodiri",
        "Jawahar"
      ],
      "year": "2020",
      "venue": "Proceedings of the 28th ACM international conference on multimedia"
    },
    {
      "citation_id": "112",
      "title": "Seeing what you said: Talking face generation guided by a lip reading expert",
      "authors": [
        "Jiadong Wang",
        "Xinyuan Qian",
        "Malu Zhang",
        "Robby Tan",
        "Haizhou Li"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "113",
      "title": "Synctalkface: Talking face generation with precise lip-syncing via audio-lip memory",
      "authors": [
        "Jin Se",
        "Minsu Park",
        "Joanna Kim",
        "Jeongsoo Hong",
        "Yong Choi",
        "Ro"
      ],
      "year": "2022",
      "venue": "Proceedings of the 36th AAAI Conference on Artificial Intelligence, AAAI 2022"
    },
    {
      "citation_id": "114",
      "title": "Learning realistic 3d motion coefficients for stylized audio-driven single image talking face animation",
      "authors": [
        "Wenxuan Zhang",
        "Xiaodong Cun",
        "Xuan Wang",
        "Yong Zhang",
        "Xi Shen",
        "Yu Guo",
        "Ying Shan",
        "Fei Wang",
        "Sadtalker"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "115",
      "title": "Facial expression manipulation for personalized facial action estimation",
      "authors": [
        "Koichiro Niinuma",
        "Itir Onal Ertugrul",
        "Jeffrey Cohn",
        "LÃ¡szlÃ³ Jeni"
      ],
      "year": "2022",
      "venue": "Frontiers in Signal Processing"
    },
    {
      "citation_id": "116",
      "title": "Starganv2-vc: A diverse, unsupervised, non-parallel framework for natural-sounding voice conversion",
      "authors": [
        "Aaron Yinghao",
        "Ali Li",
        "Nima Zare",
        "Mesgarani"
      ],
      "year": "2021",
      "venue": "Proceedings of the Annual Conference of the International Speech Communication Association"
    },
    {
      "citation_id": "117",
      "title": "Realistic speech-driven facial animation with gans",
      "authors": [
        "Konstantinos Vougioukas",
        "Stavros Petridis",
        "Maja Pantic"
      ],
      "year": "2020",
      "venue": "International Journal of Computer Vision"
    },
    {
      "citation_id": "118",
      "title": "Dualpathgan: Facial reenacted emotion synthesis",
      "authors": [
        "Jiahui Kong",
        "Haibin Shen",
        "Kejie Huang"
      ],
      "year": "2021",
      "venue": "IET Computer Vision"
    },
    {
      "citation_id": "119",
      "title": "Toward fine-grained facial expression manipulation",
      "authors": [
        "Jun Ling",
        "Han Xue",
        "Li Song",
        "Shuhui Yang",
        "Rong Xie",
        "Xiao Gu"
      ],
      "year": "2020",
      "venue": "Toward fine-grained facial expression manipulation"
    },
    {
      "citation_id": "120",
      "title": "Combining gan with reverse correlation to construct personalized facial expressions",
      "authors": [
        "Sen Yan",
        "Catherine SoladiÃ©",
        "Jean Julien Aucouturier",
        "Renaud Seguier"
      ],
      "venue": "PLoS ONE"
    },
    {
      "citation_id": "121",
      "title": "Attention based facial expression manipulation",
      "authors": [
        "Feng Wang",
        "Suncheng Xiang",
        "Ting Liu",
        "Yuzhuo Fu"
      ],
      "year": "2021",
      "venue": "Attention based facial expression manipulation"
    },
    {
      "citation_id": "122",
      "title": "Photorealistic and identity-preserving image-based emotion manipulation with latent diffusion models",
      "authors": [
        "Ioannis Pikoulis",
        "P Panagiotis",
        "Petros Filntisis",
        "Maragos"
      ],
      "venue": "Photorealistic and identity-preserving image-based emotion manipulation with latent diffusion models"
    },
    {
      "citation_id": "123",
      "title": "Dreamtalk: When expressive talking head generation meets diffusion probabilistic models",
      "authors": [
        "Yifeng Ma",
        "Shiwei Zhang",
        "Jiayu Wang",
        "Xiang Wang",
        "Yingya Zhang",
        "Zhidong Deng"
      ],
      "year": "2023",
      "venue": "Dreamtalk: When expressive talking head generation meets diffusion probabilistic models",
      "arxiv": "arXiv:2312.09767"
    },
    {
      "citation_id": "124",
      "title": "Promptvc: Flexible stylistic voice conversion in latent space driven by natural language prompts",
      "authors": [
        "Jixun Yao",
        "Yuguang Yang",
        "Yi Lei",
        "Ziqian Ning",
        "Yanni Hu",
        "Yu Pan",
        "Jingjing Yin",
        "Hongbin Zhou",
        "Heng Lu",
        "Lei Xie"
      ],
      "year": "2024",
      "venue": "ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "125",
      "title": "Triaan-vc: Triple adaptive attention normalization for any-to-any voice conversion",
      "authors": [
        "Hyun Joon Park",
        "Seok Yang",
        "Jin Kim",
        "Wooseok Shin",
        "Sung Han"
      ],
      "year": "2023",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing -Proceedings"
    },
    {
      "citation_id": "126",
      "title": "",
      "authors": [
        "Elevenlabs",
        "Elevenlabs"
      ],
      "year": "2024",
      "venue": ""
    },
    {
      "citation_id": "127",
      "title": "Controllable residual speaker representation for voice conversion",
      "authors": [
        "Le Xu",
        "Jiangyan Yi",
        "Jianhua Tao",
        "Tao Wang",
        "Yong Ren",
        "Rongxiu Zhong"
      ],
      "year": "2023",
      "venue": "Controllable residual speaker representation for voice conversion"
    },
    {
      "citation_id": "128",
      "title": "Stylebert: Text-audio sentiment analysis with bi-directional style enhancement",
      "authors": [
        "Fei Lin",
        "Shengqiang Liu",
        "Cong Zhang",
        "Jin Fan",
        "Zizhao Wu"
      ],
      "venue": "Information Systems"
    },
    {
      "citation_id": "129",
      "title": "Speaking style conversion in the waveform domain using discrete self-supervised units",
      "authors": [
        "Gallil Maimon",
        "Yossi Adi"
      ],
      "venue": "Speaking style conversion in the waveform domain using discrete self-supervised units"
    },
    {
      "citation_id": "130",
      "title": "The state of the art in text-to-speech synthesis",
      "authors": [
        "Paul Taylor",
        "Alan Black"
      ],
      "year": "2009",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "131",
      "title": "Ganspeech: Adversarial training for high-fidelity multi-speaker speech synthesis",
      "authors": [
        "Jinhyeok Yang",
        "Jae-Sung Bae",
        "Taejun Bak",
        "Youngik Kim",
        "Hoon-Young Cho"
      ],
      "year": "2021",
      "venue": "Ganspeech: Adversarial training for high-fidelity multi-speaker speech synthesis",
      "arxiv": "arXiv:2106.15153"
    },
    {
      "citation_id": "132",
      "title": "Dddm-vc: Decoupled denoising diffusion models with disentangled representation and prior mixup for verified robust voice conversion",
      "authors": [
        "Ha-Yeong Choi",
        "Sang-Hoon Lee",
        "Seong-Whan Lee"
      ],
      "year": "2024",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "133",
      "title": "Sequence-to-sequence voice conversion using f0 and time conditioning and adversarial learning",
      "authors": [
        "Frederik Bous",
        "Laurent Benaroya",
        "Nicolas Obin",
        "Axel Roebel"
      ],
      "year": "2021",
      "venue": "Sequence-to-sequence voice conversion using f0 and time conditioning and adversarial learning"
    },
    {
      "citation_id": "134",
      "title": "S2vc: A framework for any-to-any voice conversion with self-supervised pretrained representations",
      "authors": [
        "Jheng Hao Lin",
        "Yist Lin",
        "Chung Ming Chien",
        "Hung Yi"
      ],
      "year": "2021",
      "venue": "Proceedings of the Annual Conference of the International Speech Communication Association"
    },
    {
      "citation_id": "135",
      "title": "Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context",
      "authors": [
        "Machel Reid",
        "Nikolay Savinov",
        "Denis Teplyashin",
        "Dmitry Lepikhin",
        "Timothy Lillicrap",
        "Jean-Baptiste Alayrac",
        "Radu Soricut",
        "Angeliki Lazaridou",
        "Orhan Firat",
        "Julian Schrittwieser"
      ],
      "year": "2024",
      "venue": "Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context",
      "arxiv": "arXiv:2403.05530"
    },
    {
      "citation_id": "136",
      "title": "Learning representations by back-propagating errors",
      "authors": [
        "Geoffrey David E Rumelhart",
        "Ronald Hinton",
        "Williams"
      ],
      "year": "1986",
      "venue": "Nature"
    },
    {
      "citation_id": "137",
      "title": "Long short-term memory",
      "authors": [
        "Sepp Hochreiter",
        "JÃ¼rgen Schmidhuber"
      ],
      "year": "1997",
      "venue": "Neural Computation"
    },
    {
      "citation_id": "138",
      "title": "Bidirectional recurrent neural networks",
      "authors": [
        "Mike Schuster",
        "Kuldip Paliwal"
      ],
      "year": "1997",
      "venue": "IEEE Transactions on Signal Processing"
    },
    {
      "citation_id": "139",
      "title": "Sequence to sequence learning with neural networks",
      "authors": [
        "Ilya Sutskever",
        "Oriol Vinyals",
        "V Quoc",
        "Le"
      ],
      "year": "2014",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "140",
      "title": "Generative adversarial nets",
      "authors": [
        "Ian Goodfellow",
        "Jean Pouget-Abadie",
        "Mehdi Mirza",
        "Bing Xu",
        "David Warde-Farley",
        "Sherjil Ozair",
        "Aaron Courville",
        "Yoshua Bengio"
      ],
      "year": "2014",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "141",
      "title": "Neural machine translation by jointly learning to align and translate",
      "authors": [
        "Dzmitry Bahdanau",
        "Kyunghyun Cho",
        "Yoshua Bengio"
      ],
      "year": "2014",
      "venue": "Neural machine translation by jointly learning to align and translate",
      "arxiv": "arXiv:1409.0473"
    },
    {
      "citation_id": "142",
      "title": "Adapting a language model for controlled affective text generation",
      "authors": [
        "Tushar Goswamy",
        "Ishika Singh",
        "Ahsan Barkati",
        "Ashutosh Modi"
      ],
      "year": "2020",
      "venue": "Adapting a language model for controlled affective text generation"
    },
    {
      "citation_id": "143",
      "title": "Wav2vec 2.0: A framework for self-supervised learning of speech representations",
      "authors": [
        "Alexei Baevski",
        "Yuhao Zhou",
        "Abdelrahman Mohamed",
        "Michael Auli"
      ],
      "year": "2020",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "144",
      "title": "Language models are unsupervised multitask learners",
      "authors": [
        "Alec Radford",
        "Jeff Wu",
        "Rewon Child",
        "David Luan",
        "Dario Amodei",
        "Ilya Sutskever"
      ],
      "year": "2019",
      "venue": "OpenAI Blog"
    },
    {
      "citation_id": "145",
      "title": "Libreface: An open-source toolkit for deep facial expression analysis",
      "authors": [
        "Di Chang",
        "Yufeng Yin",
        "Zongjian Li",
        "Minh Tran",
        "Mohammad Soleymani"
      ],
      "year": "2024",
      "venue": "Libreface: An open-source toolkit for deep facial expression analysis"
    },
    {
      "citation_id": "146",
      "title": "End-to-end modeling and transfer learning for audiovisual emotion recognition in-the-wild",
      "authors": [
        "Denis Dresvyanskiy",
        "Elena Ryumina",
        "Heysem Kaya",
        "Maxim Markitantov",
        "Alexey Karpov",
        "Wolfgang Minker"
      ],
      "year": "2022",
      "venue": "End-to-end modeling and transfer learning for audiovisual emotion recognition in-the-wild"
    },
    {
      "citation_id": "147",
      "title": "A discriminatively deep fusion approach with improved conditional gan (im-cgan) for facial expression recognition",
      "authors": [
        "Zhe Sun",
        "Hehao Zhang",
        "Jiatong Bai",
        "Mingyang Liu",
        "Zhengping Hu"
      ],
      "venue": "Pattern Recognition"
    },
    {
      "citation_id": "148",
      "title": "Fine-grained image analysis for facial expression recognition using deep convolutional neural networks with bilinear pooling",
      "authors": [
        "Sanoar Hossain",
        "Saiyed Umer",
        "Ranjeet Kr Rout",
        "M Tanveer"
      ],
      "year": "2023",
      "venue": "Applied Soft Computing"
    },
    {
      "citation_id": "149",
      "title": "Talkclip: Talking head generation with text-guided expressive speaking styles",
      "authors": [
        "Yifeng Ma",
        "Suzhen Wang",
        "Yu Ding",
        "Bowen Ma",
        "Tangjie Lv",
        "Changjie Fan",
        "Zhipeng Hu",
        "Zhidong Deng",
        "Xin Yu"
      ],
      "year": "2023",
      "venue": "Talkclip: Talking head generation with text-guided expressive speaking styles",
      "arxiv": "arXiv:2304.00334"
    },
    {
      "citation_id": "150",
      "title": "Styletalk: One-shot talking head generation with controllable speaking styles",
      "authors": [
        "Yifeng Ma",
        "Suzhen Wang",
        "Zhipeng Hu",
        "Changjie Fan",
        "Tangjie Lv",
        "Yu Ding",
        "Zhidong Deng",
        "Xin Yu"
      ],
      "year": "2023",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "151",
      "title": "One-shot talking face generation from single-speaker audio-visual correlation learning",
      "authors": [
        "Suzhen Wang",
        "Lincheng Li",
        "Yu Ding",
        "Xin Yu"
      ],
      "year": "2022",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "152",
      "title": "Makelttalk: speaker-aware talking-head animation",
      "authors": [
        "Yang Zhou",
        "Xintong Han",
        "Eli Shechtman",
        "Jose Echevarria",
        "Evangelos Kalogerakis",
        "Dingzeyu Li"
      ],
      "year": "2020",
      "venue": "ACM Transactions On Graphics (TOG)"
    },
    {
      "citation_id": "153",
      "title": "Eamm: One-shot emotional talking face via audio-based emotion-aware motion model",
      "authors": [
        "Xinya Ji",
        "Hang Zhou",
        "Kaisiyuan Wang",
        "Qianyi Wu",
        "Wayne Wu",
        "Feng Xu",
        "Xun Cao"
      ],
      "year": "2022",
      "venue": "ACM SIGGRAPH 2022 Conference Proceedings"
    },
    {
      "citation_id": "154",
      "title": "Expressive talking head generation with granular audio-visual control",
      "authors": [
        "Borong Liang",
        "Yan Pan",
        "Zhizhi Guo",
        "Hang Zhou",
        "Zhibin Hong",
        "Xiaoguang Han",
        "Junyu Han",
        "Jingtuo Liu",
        "Errui Ding",
        "Jingdong Wang"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "155",
      "title": "Audio2head: Audio-driven one-shot talkinghead generation with natural head motion",
      "authors": [
        "Suzhen Wang",
        "Lincheng Li",
        "Yu Ding",
        "Changjie Fan",
        "Xin Yu"
      ],
      "year": "2021",
      "venue": "Audio2head: Audio-driven one-shot talkinghead generation with natural head motion",
      "arxiv": "arXiv:2107.09293"
    },
    {
      "citation_id": "156",
      "title": "Hierarchical cross-modal talking face generation with dynamic pixel-wise loss",
      "authors": [
        "Lele Chen",
        "Ross Maddox",
        "Zhiyao Duan",
        "Chenliang Xu"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "157",
      "title": "Clstm: Deep feature-based speech emotion recognition using the hierarchical convlstm network",
      "authors": [
        "Soonil Kwon"
      ],
      "year": "2020",
      "venue": "Mathematics"
    },
    {
      "citation_id": "158",
      "title": "Speech emotion recognition from 3d log-mel spectrograms with deep learning network",
      "authors": [
        "Tianhao Hao Meng",
        "Fei Yan",
        "Hongwei Yuan",
        "Wei"
      ],
      "year": "2019",
      "venue": "IEEE access"
    },
    {
      "citation_id": "159",
      "title": "Decision tree svm model with fisher feature selection for speech emotion recognition",
      "authors": [
        "Linhui Sun",
        "Sheng Fu",
        "Fu Wang"
      ],
      "year": "2019",
      "venue": "EURASIP Journal on Audio, Speech, and Music Processing"
    },
    {
      "citation_id": "160",
      "title": "Speech emotion recognition with deep convolutional neural networks",
      "authors": [
        "Dias Issa",
        "M Fatih Demirci",
        "Adnan Yazici"
      ],
      "year": "2020",
      "venue": "Biomedical Signal Processing and Control"
    },
    {
      "citation_id": "161",
      "title": "Optimal feature selection speech emotion recognition using two-stream deep convolutional neural network",
      "authors": [
        "Soonil Mustaqeem",
        "Kwon"
      ],
      "year": "2021",
      "venue": "International Journal of Intelligent Systems"
    },
    {
      "citation_id": "162",
      "title": "Speech emotion recognition based on an improved brain emotion learning model",
      "authors": [
        "Zhen-Tao Liu",
        "Qiao Xie",
        "Min Wu",
        "Wei-Hua Cao",
        "Ying Mei",
        "Jun-Wei Mao"
      ],
      "year": "2018",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "163",
      "title": "Lanser: Language-model supported speech emotion recognition",
      "authors": [
        "Taesik Gong",
        "Josh Belanich",
        "Krishna Somandepalli",
        "Arsha Nagrani",
        "Brian Eoff",
        "Brendan Jou"
      ],
      "year": "2023",
      "venue": "Lanser: Language-model supported speech emotion recognition"
    },
    {
      "citation_id": "164",
      "title": "An iterative emotion interaction network for emotion recognition in conversations",
      "authors": [
        "Xin Lu",
        "Yanyan Zhao",
        "Yang Wu",
        "Yijian Tian",
        "Huipeng Chen",
        "Bing Qin"
      ],
      "year": "2020",
      "venue": "Proceedings of the 28th international conference on computational linguistics"
    },
    {
      "citation_id": "165",
      "title": "Speech emotion classification from affective dimensions: Limitation and advantage",
      "authors": [
        "Meysam Shamsi"
      ],
      "year": "2023",
      "venue": "11th International Conference on Affective Computing and Intelligent Interaction Workshops and Demos"
    },
    {
      "citation_id": "166",
      "title": "Emotion recognition from speech using wav2vec 2.0 embeddings",
      "authors": [
        "Leonardo Pepino",
        "Pablo Riera",
        "Luciana Ferrer"
      ],
      "year": "2021",
      "venue": "Emotion recognition from speech using wav2vec 2.0 embeddings",
      "arxiv": "arXiv:2104.03502"
    },
    {
      "citation_id": "167",
      "title": "Emotion recognition from speech using artificial neural networks and recurrent neural networks",
      "authors": [
        "Shambhavi Sharma"
      ],
      "year": "2021",
      "venue": "2021 11th International Conference on Cloud Computing, Data Science & Engineering (Confluence)"
    },
    {
      "citation_id": "168",
      "title": "Any-to-many voice conversion with location-relative sequence-to-sequence modeling",
      "authors": [
        "Songxiang Liu",
        "Yuewen Cao",
        "Disong Wang",
        "Xixin Wu",
        "Xunying Liu",
        "Helen Meng"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "169",
      "title": "Autovc: Zero-shot voice style transfer with only autoencoder loss",
      "authors": [
        "Kaizhi Qian",
        "Yang Zhang",
        "Shiyu Chang",
        "Xuesong Yang",
        "Mark Hasegawa-Johnson"
      ],
      "year": "2019",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "170",
      "title": "Global prosody style transfer without text transcriptions",
      "authors": [
        "Kaizhi Qian",
        "Yang Zhang",
        "Shiyu Chang",
        "Jinjun Xiong",
        "Chuang Gan",
        "David Cox",
        "Mark Hasegawa-Johnson"
      ],
      "year": "2021",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "171",
      "title": "Vqmivc: Vector quantization and mutual information-based unsupervised speech representation disentanglement for one-shot voice conversion",
      "authors": [
        "Disong Wang",
        "Liqun Deng",
        "Yu Yeung",
        "Xiao Chen",
        "Xunying Liu",
        "Helen Meng"
      ],
      "year": "2021",
      "venue": "Vqmivc: Vector quantization and mutual information-based unsupervised speech representation disentanglement for one-shot voice conversion",
      "arxiv": "arXiv:2106.10132"
    },
    {
      "citation_id": "172",
      "title": "Voice conversion for stuttered speech, instruments, unseen languages and textually described voices",
      "authors": [
        "Matthew Baas",
        "Herman Kamper"
      ],
      "year": "2023",
      "venue": "Southern African Conference for Artificial Intelligence Research"
    },
    {
      "citation_id": "173",
      "title": "Freevc: Towards high-quality text-free one-shot voice conversion",
      "authors": [
        "Jingyi Li",
        "Weiping Tu",
        "Li Xiao"
      ],
      "year": "2023",
      "venue": "ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "174",
      "title": "Yourtts: Towards zero-shot multi-speaker tts and zero-shot voice conversion for everyone",
      "authors": [
        "Edresson Casanova",
        "Julian Weber",
        "Christopher Shulby",
        "Arnaldo Candido Junior",
        "Eren GÃ¶lge",
        "Moacir Ponti"
      ],
      "year": "2022",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "175",
      "title": "Phoneme hallucinator: One-shot voice conversion via set expansion",
      "authors": [
        "Siyuan Shan",
        "Yang Li",
        "Amartya Banerjee",
        "Junier B Oliva"
      ],
      "year": "2024",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "176",
      "title": "Voicemixer: Adversarial voice style mixup",
      "authors": [
        "Sang-Hoon Lee",
        "Ji-Hoon Kim",
        "Hyunseung Chung",
        "Seong-Whan Lee"
      ],
      "year": "2021",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "177",
      "title": "An analysis of annotated corpora for emotion classification in text",
      "authors": [
        "Laura Ana",
        "Maria OberlÃ¤nder",
        "Roman Klinger"
      ],
      "year": "2018",
      "venue": "Proceedings of the 27th international conference on computational linguistics"
    },
    {
      "citation_id": "178",
      "title": "Emotionx-idea: Emotion bert-an affectional model for conversation",
      "authors": [
        "Yen-Hao Huang",
        "Ssu-Rui Lee",
        "Mau-Yun Ma",
        "Yi-Hsin Chen",
        "Ya-Wen Yu",
        "Yi-Shin Chen"
      ],
      "year": "2019",
      "venue": "Emotionx-idea: Emotion bert-an affectional model for conversation",
      "arxiv": "arXiv:1908.06264"
    },
    {
      "citation_id": "179",
      "title": "Multimodal approach of speech emotion recognition using multi-level multi-head fusion attention-based recurrent neural network",
      "authors": [
        "Ngoc-Huynh Ho",
        "Hyung-Jeong Yang",
        "Soo-Hyung Kim",
        "Gueesang Lee"
      ],
      "year": "2020",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "180",
      "title": "Modeling both context-and speaker-sensitive dependence for emotion detection in multi-speaker conversations",
      "authors": [
        "Dong Zhang",
        "Liangqing Wu",
        "Changlong Sun",
        "Shoushan Li",
        "Qiaoming Zhu",
        "Guodong Zhou"
      ],
      "year": "2019",
      "venue": "IJCAI"
    },
    {
      "citation_id": "181",
      "title": "Model of multi-turn dialogue in emotional chatbot",
      "authors": [
        "Chien-Hao Kao",
        "Chih-Chieh Chen",
        "Yu-Tza Tsai"
      ],
      "year": "2019",
      "venue": "2019 International Conference on Technologies and Applications of Artificial Intelligence (TAAI)"
    },
    {
      "citation_id": "182",
      "title": "Two-stage text feature selection method for human emotion recognition",
      "authors": [
        "Lovejit Singh",
        "Sarbjeet Singh",
        "Naveen Aggarwal"
      ],
      "year": "2018",
      "venue": "Proceedings of 2nd International Conference on Communication, Computing and Networking: ICCCN 2018"
    },
    {
      "citation_id": "183",
      "title": "Text emotion distribution learning via multi-task convolutional neural network",
      "authors": [
        "Yuxiang Zhang",
        "Jiamei Fu",
        "Dongyu She",
        "Ying Zhang",
        "Senzhang Wang",
        "Jufeng Yang"
      ],
      "year": "2018",
      "venue": "In IJCAI"
    },
    {
      "citation_id": "184",
      "title": "Xlm-t: Multilingual language models in twitter for sentiment analysis and beyond",
      "authors": [
        "Francesco Barbieri",
        "Luis Espinosa Anke",
        "Jose Camacho-Collados"
      ],
      "venue": "Xlm-t: Multilingual language models in twitter for sentiment analysis and beyond"
    },
    {
      "citation_id": "185",
      "title": "Xed: A multilingual dataset for sentiment analysis and emotion detection",
      "authors": [
        "Emily Ã–hman",
        "Marc PÃ mies",
        "Kaisla Kajava",
        "JÃ¶rg Tiedemann"
      ],
      "year": "2020",
      "venue": "Xed: A multilingual dataset for sentiment analysis and emotion detection",
      "arxiv": "arXiv:2011.01612"
    },
    {
      "citation_id": "186",
      "title": "",
      "authors": [
        "Anthropic",
        "Claude"
      ],
      "year": "2024",
      "venue": ""
    }
  ]
}