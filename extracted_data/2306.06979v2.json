{
  "paper_id": "2306.06979v2",
  "title": "A Weakly Supervised Approach To Emotion-Change Prediction And Improved Mood Inference",
  "published": "2023-06-12T09:21:34Z",
  "authors": [
    "Soujanya Narayana",
    "Ibrahim Radwan",
    "Ravikiran Parameshwara",
    "Iman Abbasnejad",
    "Akshay Asthana",
    "Ramanathan Subramanian",
    "Roland Goecke"
  ],
  "keywords": [
    "Mood inference",
    "Emotion change",
    "Siamese network",
    "Contrastive Loss",
    "Teacher-student network",
    "Unimodal",
    "Multimodal"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Whilst a majority of affective computing research focuses on inferring emotions, examining mood or understanding the mood-emotion interplay has received significantly less attention. Building on prior work, we (a) deduce and incorporate emotion-change (∆) information for inferring mood, without resorting to annotated labels, and (b) attempt mood prediction for long duration video clips, in alignment with the characterisation of mood. We generate the emotion-change (∆) labels via metric learning from a pre-trained Siamese Network, and use these in addition to mood labels for mood classification. Experiments evaluating unimodal (training only using mood labels) vs multimodal (training using mood plus ∆ labels) models show that mood prediction benefits from the incorporation of emotion-change information, emphasising the importance of modelling the moodemotion interplay for effective mood inference.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "Over the past two decades, there has been an enormous increase in the research on inferring affective states (characterised by emotions, moods, etc.) from unimodal and multimodal data. Several studies emphasise on the importance of emotional regulation for the successful functioning of human mind  [2] ,  [3] , as they play an indispensable role in rational decision-making, perception, attention, and other diverse cognitive functions  [4] . While the terms emotion and mood are often used synonymously, the two affective phenomena are distinct in terms of duration, intensity, attribute, and behavioural impact. Emotion is a short-term affective state, lasting for at most a few minutes, and is typically elicited by a contextual event/stimulus. On the contrary, mood is considered to be a long-term diffuse affective state lasting for hours, which may emerge without an apparent cause  [5] .\n\nAkin to emotions, mood has an impact on cognitive processes like human creativity, evaluative judgement, and memory retrieval, etc  [4] . Mood is also known to generate cognitive bias and influence human emotion recognition  [6] . Going further, mood disorders like depression and bipolar disorder result in the impairment of emotional processing abilities  [7] , altered facial expression understanding  [8] , and olfactory perception  [9] . Recently, the focus of numerous studies is on building emotionally-aware systems to better understand human behaviour, and facilitate enhanced human-computer interactions. Advancements in machine learning techniques have enabled automatic emotion recognition from unimodal and multimodal data, for instance, facial expressions  [10] , physiological signals  [11] -  [13] , videos  [14] , etc. Although substantial progress has been made in psychology to understand mood, negligible work has focused on computationally inferring mood. Furthermore, the psychology literature recognises an association between emotions and mood  [15] ; theories state that despite being distinct mechanisms, they affect one another repeatedly and continuously. Nevertheless, hardly any research has been devoted towards computationally modelling the mood-emotion interplay for mood inference.\n\nPreliminary studies on mood recognition  [16] ,  [17]  infer mood via body posture and 3D pose data in a controlled setting. As a step towards in-the-wild mood prediction,  [18]  uses deep learning to perform mood classification on affective videos. It observes that mood prediction improves on utilising emotion-change information. The premise in  [18]  is the existence of continuous emotion (valence) labels along with mood annotations during the training and testing phases. Reliance on continuous emotion labels represents a significant overhead, as these labels may not be available in real-world settings.\n\nThis work is inspired by and extends the idea proposed in  [18] , and obviates the need for ground-truth emotion labels by deducing emotional change information and utilising it for mood inference. Our mood inference framework is illustrated in Fig.  1 . Specifically, emotion change is modelled in terms of emotional (dis)similarity between a pair of video frames via a Siamese Network with contrastive loss.\n\nWe present mood inference results on the AffWild2 database  [1] , where (a) video mood labels are derived as in  [18] ; (b) pseudo emotion-change (∆) labels are derived via a pre-trained Siamese Network; (c) a 3-dimensional Convolutional Neural Network (3D-CNN) with a ResNet18 backbone and projection head is trained with mood labels; (d) a 3D-CNN with ResNet18 backbone and branched projection heads is trained with both mood and ∆ labels, and (e) a Teacher-Student (TS) network  [19]  is employed, where the teacher distills the privileged ∆-specific knowledge to the student for mood inference.\n\nConsistent with  [18] , we observe that mood prediction performance improves when emotion similarity information is incorporated, emphasising the prominence of short-term affect (emotion-change) for long-term affect (mood) inference. To summarise, the main contributions of this work are as follows:\n\n1) We propose to infer mood employing emotional similarity, which models emotion change between a pair of images. To this end, we train a 3D-CNN with two branches, which are respectively trained via video frames annotated with mood and ∆ labels. 2) We weakly label the AffWild2  [1]  dataset, by generating ∆ labels for video frame pairs using a pre-trained Siamese Network with contrastive loss. 3) Assessing various models, we demonstrate that incorporating emotion-change (∆) information via emotion similarity benefits mood recognition and enhances mood prediction performance. Similar trends are observed in the experiments employing ground truth ∆ GT labels. 4) Through an ablation study, we verify the effectiveness of the various components of the proposed mood classification framework outlined in Fig.  1 .",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Related Work",
      "text": "In this section, we present studies examining the affective phenomena of emotion and mood (Sec. II-A), the various affective databases available for affect inference, and machine learning studies examining mood inference (Sec. II-B). The motivation for this study given the literature context is presented in Sec. II-C.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "A. Emotion, Mood And Their Interconnectedness",
      "text": "While there are multiple definitions of emotion, the following characterisation appears to be consistent. Emotion is considered to be an episode of neurophysiological and cognitive change in response to an external or internal stimulus  [20] . The concepts of emotion and mood are distinguished based on the factors of duration, trigger, intensity, and behavioural impact  [21] . The former are short-term, lasting for a few seconds and are elicited based on stimulus events. The level of response to the stimuli and the corresponding emotional expression is recognised to be of relatively high intensity  [22] .\n\nIn contrast, moods are considered to be enduring affective states, lasting for hours or even days without being instantiated by a stimulus. They are regarded to be diffuse with low levels of intensity  [21] .\n\nThe human physiological state and mind are both influenced by and reflective of mood, as it directly influences human health and well-being. Besides having an impact on evaluative judgements, mood also governs memory retrieval  [23] . Moodcongruency, which refers to the match between a person's mood and his/her thoughts  [24] , is observed in  [25] , where the authors examine mood effects on emotion recognition. Happy mood impedes the recognition of mood-incongruent sad emotions, while sad mood obstructs the recognition of happy emotions. The interplay between mood and emotion is described by the mood-emotion loop  [23] , a theory which proposes that mood and emotion are distinct mechanisms forming a loop, and are reciprocally influencing one another.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "B. Computational Studies On Mood",
      "text": "Most research on affective state inference has focussed on emotions, as opposed to mood. Likewise, the prevailing affective databases to aid behavioural and computational studies with various modes of data predominantly target emotions.\n\n1) Databases: Audio-visual databases, such as AFEW  [26] , DECAF  [27] , Ascertain  [13]  etc., comprise videos of emotional episodes with categorical emotion annotations, namely, happy, sad, fear, disgust, anger, surprise, and neutral. RECOLA  [28] , AFEW-VA  [29] , AffWild2  [1] , etc., manifest continuous emotion annotations of valence (degree of pleasantness or unpleasantness) and arousal (degree of excitement or calmness). These databases, designed by considering factors such as recording environment, duration, and annotation type, best suit emotion inference tasks. The test set of the AVEC 2013 challenge, annotated for the level of depression, is one of the few databases with mood annotations. However, depression, a mood disorder, is not a commonly observed mood state. EMMA  [30]  is an acted video database recorded in a controlled setting with mood annotations.\n\n2) Computational Approaches: In  [17] , the authors use body posture and head movement features to capture the affective state while listening music. A vertical position of the head is observed in a positive mood and a downward position otherwise. A 3-dimensional pose tracker is used in  [16]  to infer physical attributes and the mood of the person by capturing walking motions. The authors of  [31]  perform automatic mood recognition from recognised emotions, and show that clustered emotions in the valence-arousal space are better predictors of a single mood as compared to multiple moods within a video.\n\nMood prediction using various 3D-CNNs are performed in  [18]  using the AFEW-VA  [29]  dataset. Utilising the valence annotations, the authors compute the valence differential to infer mood and demonstrate that incorporating valence change improves mood prediction performance. Although this study is a promising step towards automatic mood inference, only video clips of very short duration (≈ 0.04 seconds) are considered, which may not adequately depict subject mood in the video.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "C. Novelty Of Our Study",
      "text": "A thorough examination of the literature reveals the following: (a) While significant research has been conducted to infer emotions automatically, mood inference and modelling the mood-emotion interplay have been neglected from a computational perspective; (b) Existing affective databases are richly annotated for emotions, while labelled data for mood inference are sparse; and (c) Existing studies, which infer mood using emotions, require continuous valence annotations and consider clips of very short duration (≈ 0.04 seconds) for this purpose.\n\nDifferently, this study uses deduced emotional similarity information in lieu of valence-differential (∆) labels, obviating the need for ground truth ∆ annotations. This setting resembles real-world scenarios where valence annotations may not be accessible for inferring mood. This work proposes to (1) deduce emotion change (∆) labels using a Siamese Network trained with contrastive loss, and (2) incorporate ∆ labels for inferring mood. In addition, ablation studies are performed to empirically examine the effect of different components of the proposed mood inference framework.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Iii. Databases And Label Generation",
      "text": "This section describes the databases considered in this study as well as the labelling procedure.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "A. Affwild2 Database",
      "text": "We consider AffWild2  [1] , a publicly available affective video database with continuous dimensional (valence, arousal) and categorical emotion annotations for performing mood classification. AffWild2 comprises 564 in-the-wild videos collected from YouTube. There are a total of 2,816,832 frames with 455 subjects (277 male, 178 female). The videos are annotated by four experts for continuous valence and arousal values, and the average of the four raters is considered as the final rating, while three experts annotated the categorical emotion labels. The valence and arousal annotations are in the range [-1, 1], and the emotional categories are happy, sad, disgust, anger, fear, surprise, neutral, and other. The frames with annotated values outside this range are discarded as suggested by the dataset providers. The partitioning of the database into training, validation and test sets is done in a subject-independent manner, so that every subject is present in one of the three partitions. This partitioning results in 341, 71 and 152 videos respectively in the training, validation and test sets. The validation set is utilized for evaluating our proposed approach, as the test set has not been released.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "B. Mood Labels",
      "text": "Since current in-the-wild affective video databases lack mood annotations, as the closest alternative, we utilize valence annotations to assign mood labels for each video in the AffWild2 database, as done in  [18] . The three mood categories considered are positive (+1), negative (-1), and neutral (0). As mood denotes a long-term affective state  [4] , the most persistent valence value (maximum number of consecutive frames) is considered for assigning a mood label. Mood label is assigned to -1, 0, or +1 if the valence values for maximum number of consecutive frames respectively lie in the range [-1, -0.3), [-0.3, 0.3], and (0.3, 1].",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "C. Affectnet Database",
      "text": "In order to automatically generate the emotion change (∆) labels, we employ the AffectNet  [32]  dataset to train a Siamese Network (described in Sec. III-D). AffectNet, an affective database curated for automatic facial emotion recognition tasks, comprises around 420,300 facial images captured under natural conditions. Twelve experts annotated the data with continuous valence and arousal values, and eight emotion categories (happy, sad, disgust, anger, fear, surprise, neutral, and contempt). Partitioning the dataset with the criteria of having 500 images for each of the nine emotion categories in the validation set, results in 287,151 and 4500 images in the training and test sets respectively.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "D. Emotion Change (∆) Labels",
      "text": "This work seeks to eliminate the need for valence annotations (as proposed in  [18] ) to generate ∆ labels. Differently, we propose to use a Siamese Network with contrastive loss to deduce emotion-change (∆) between video frames in terms of similarity (little change in emotion) or dissimilarity (significant change in emotion). A Siamese network is a neural network that discriminates if a pair of input data samples are similar or dissimilar  [33] . Multiple emotion-related tasks have employed a Siamese network, and observed promising performance  [34] ,  [35] . We use contrastive loss in the Siamese network, which learns a similarity metric that minimises the distance between similar image pairs, while maximising the distance between dissimilar pairs. Distance between the pairs is compared to a margin value, and the contrastive loss function enforces a smaller distance between similar pairs, and a larger distance between dissimilar pairs. 1) Siamese Network: In this study, similarity refers to little or no change in the emotional facial expression between a pair of frames. Fig.  2  shows the architecture of the Siamese network, which comprises identical sub-networks for classifying the input frames as similar or dissimilar. Each sub-network involves an EmoFAN  [36]  network as the encoder, E(•), which maps the input image to a vector. We employ a pre-trained EmoFAN model as it has demonstrated high performance in emotion recognition tasks. For two images x 1 and x 2 we obtain,\n\n, where D C = 512. v is fed into a projection head P (•), which maps it to a vector, u = P (v) ∈ R D P , where D P = 2. P (•) is a Multi-Layer Perceptron (MLP) with three fully connected (fc) layers, comprising 256, 128, and 2 neurons, respectively. The neurons in the last fc layer refer to the two classes, similar and dissimilar. The inputs to the fc layers are normalised with zero mean and unit variance, before feeding to the ReLU activation.\n\n2) Contrastive Loss: As opposed to using the cross-entropy loss, L B , alone for binary classification (similar/dissimilar), we additionally consider using the contrastive loss, L C , given by,\n\nwhere N is the batch size, y i is the label indicating whether the two input samples are similar  (1)  or dissimilar (0), d i is the cosine distance between the embeddings v 1 and v 2 , and m is the margin. The total loss is given by,\n\nwhere λ is a training hyperparameter.\n\n3) Deducing the ∆ Label: The Siamese network is trained on the images in the AffectNet database. The groundtruth labels for the Siamese network y i are derived as 1 if the emotion categories of the input pair are the same, 0 otherwise. The network is trained for 40 epochs, using the Adam optimiser with the learning rate decreased by a factor of 10 for every 10 epochs, with the initial learning rate as 0.0001. The batch size is set to 64, with the dropout rate as 0.3, margin m in the contrastive loss set to 0.25, and λ set to 0.5.\n\nThe Siamese network achieves an accuracy of 68%. This model is used to generate ∆ labels for the AffWild2 video frame pairs.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "E. Generating Input Samples For Mood Inference",
      "text": "The AffWild2 dataset comprises videos of long duration, with an average length of ≈ 3 minutes (minimum duration of 0.03 minutes, maximum duration of 26.22 minutes). From each video, using a sliding window approach, we generate clips with a stride s, where each clip is a collection of sampled frames. Each clip c is of temporal length t, which refers to the duration of the clip (number of frames). Constructing clips by including all the frames in c increases the computational load and time substantially. Hence, to address this computational impediment, we significantly reduce the number of frames in c, and sample n frames at equal intervals of time. Clips generated from each video contain frames from the parent video alone, and no frames from other videos.\n\nc is assigned the mood label of its parent video, implying all clips generated from a source video are assigned the same mood label. To generate the ∆ label for c, the first frame and the last frame of c are fed to the trained Siamese Network (described in Sec. III-C). The model returns 0 or 1 as the ∆ label, by evaluating the dissimilarity or similarity of the emotion displayed in this pair of frames. Hence, each clip is associated with a mood label ∈ {0, +1, -1}, and a ∆ label ∈ {0, 1}.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Iv. Mood Classification Approach",
      "text": "The capability of 3D-CNNs to capture the temporal dependencies in the input data, along with spatial information, has resulted in their extensive usage. ResNet18-3D  [37]  (R3D), a 3D variant of the ResNet architecture, is commonly used as a backbone network in many 3D-CNN architectures for facial emotion inference  [38] ,  [39] . Leveraging the interplay between mood and emotions, we utilise the mood and ∆ labels to perform mood classification. The various models used in this study are described as follows.\n\nA. ResMood Fig.  3  (left) shows ResMood, a model trained with mood labels alone, which consists of a ResNet18-3D, R3D(•), as the backbone and a projection head, P (•). The backbone maps each input sample x to a representation vector, v = R3D(x) ∈ R D B , where D B = 1024. The projection head P (•) further maps v to a vector z = P (v) ∈ R D P , where D P = 3. P (•) is instantiated as a Multi-Layer Perceptron (MLP), with three fully-connected (fc) layers comprising 512, 256, and 3 neurons, respectively. The 3 neurons in the last fc layer denote the three mood classes, positive, negative, and neutral. The inputs to each layer in the MLP are normalised batch-wise with zero mean and unit variance before feeding them to the ReLU activation function.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "B. Resmoodemo",
      "text": "Fig.  3  (left) shows ResMoodEmo, a model for performing mood classification trained with both mood and ∆ labels. Distinct from ResMood, ResMoodEmo is composed of R3D(•) as the backbone, and two projection heads P M (•) and P ∆ (•), branching out for mood and ∆ classification, respectively.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "C. Teacher-Student Network",
      "text": "Similar to  [18] , we employ knowledge distillation  [19] , a technique used to transfer knowledge from a larger (teacher) model to a smaller (student) model. In this method, the goal is to train the student model to mimic the output probabilities of the teacher model, in addition to the predicting the true labels. Fig.  3  (right) presents the Teacher-Student Network (TS-Net), where ResMoodEmo is used as the teacher model (see Sec. IV-B), and ResMood is used as the student model (see Sec. IV-A). The teacher model, trained with both mood and ∆ labels, distills knowledge to the student, which is only trained with mood labels. Since the performance of the student model alone is evaluated, ∆ labels are not utilised during the testing phase. The SoftMax layer of the student model has a hyper-parameter called temperature (T ), which regulates the softness of the output class probabilities. Using low temperature values produces a sharper probability distribution, facilitating the student to focus on the relative differences in the probabilities of the classes. A weighted sum of the distillation loss, L D , measuring the difference between the outputs of the teacher and student models, and the student loss, L S , a typical supervised loss is optimised in the TS-Net, L = αL S + (1 -α)L D , where α is a training hyperparameter.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "D. Implementation Details",
      "text": "All experiments are based on using the open-source library PyTorch. The models are trained on Nvidia GeForce RTX 3090 GPU with 24GB memory. We use the videos with cropped and aligned faces provided in the AffWild2 database. To generate the input samples (see Sec. III-E), we set the temporal length t = 100, with the number of frames in each sample n = 5, and the stride s = 3. The models ResMood, ResMoodEmo, and the TS-Net are trained using the Adam optimiser with the learning rate reduced by a factor of 10 for every 10 epochs, and the base learning rate set to 0.0001. The models are trained for 30 epochs with a batch size of 128 and the dropout rate is 0.5. In the TS-Net, we validated with the temperature values ∈ {3, 5, 7} and α ∈ {0.05, 0.1, 0.15, 0.2}, as shown in Table  VI .",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "V. Results And Discussion",
      "text": "Due to an imbalance in mood classes in the test set, we use weighted F1-score as the performance evaluation metric in all our experiments. Table  I  shows the results of ResMood, ResMoodEmo, and TS-Net. While ResMood is trained with mood labels alone, ResMoodEmo is trained with both mood and ∆ labels. In the TS-net, ResMoodEmo is the teacher model, and ResMood is the student model, implying that the teacher is pre-trained with both mood and ∆ labels, while the student is trained with mood labels alone.\n\nResMoodEmo yields a higher F-score as compared to ResMood, indicating that the ResMoodEmo learning temporal short-term emotion changes, better predicts mood than ResMood, which only employs mood labels. Without resorting to the valence differential for gathering emotion change information as done in  [18] , training a Siamese Network with contrastive loss, and generating the ∆ labels results in a competitive performance in mood prediction. Further, by using the emotion change labels, we are capturing variations over a short duration simultaneously for characterising mood. The results indicate that (local) emotion variations contribute towards understanding the (global) mood.\n\nA similar trend is observed in the TS-Net, as it yields a higher F-score than the ResMood model. This shows that  With the ∆ labels being implicitly given as soft labels from ResMoodEmo, the performance of the student increases, as compared to the standalone ResMood model. Cumulatively, these results show that using the pseudo-emotion-change information enhances the mood prediction performance.\n\nFor the Siamese Network trained on the AffectNet dataset, generating effective ∆ labels from AffWild2 is crucial. The obtained results show that the ∆ labels generated are reliable as they improve mood prediction performance; the investigation of an optimal architecture for the Siamese network is left to future work. Overall, our results confirm that despite not using the valence differential labels to denote emotion changes, performing weak supervision using the ∆ labels in ResMoodEmo and TS-Net improves mood prediction performance.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "A. Ablation Studies",
      "text": "To corroborate the above findings, we perform the following ablation studies and examine the effectiveness of various components of our approach.\n\n1) Using Ground Truth Emotion-change (∆ GT ) Labels: The increase in the F-score using ResMoodEmo as compared to ResMood, could be attributed to the contribution of the ∆ labels for mood inference or the implicit efficiency of the Siamese Network. For better comprehension of the results, we use the eight emotion labels available for each video frame in the AffWild2 dataset to obtain ∆ GT (similar or dissimilar) labels. However, since not all videos have the categorical emotion annotations, the total number of samples reduced from 191,552 to 53,026. These samples are obtained as described in Sec. III-E. For each video sample, there is an assigned mood label and ∆ GT label (similarity between first frame and last frame). The results of ResMood and ResMoodEmo are shown in Table  II . It is noteworthy that a similar trend to Table  I  is observed here. This indicates that, (a) ∆ labels generated by the Siamese Network are effective, as they result in a competitive performance, (b) emotion information positively contributes in mood inference in a dataset-agnostic manner (∆ labels are deduced from the Siamese Network trained using AffectNet, whereas the ∆ GT labels are obtained from AffWild2.), and (c) achieving a comparable result without using the ∆ GT highlights the robustness of the proposed mood inference approach.\n\n2) Number of Frames in the Input Video Sample: Table  III  shows the results of varying the number of frames in the input samples, while fixing the temporal length (t) to 100. Temporal information plays a crucial role in examining mood, and variation in the number of frames in each sample clarifies if increasing the information provided to the model facilitates mood inference. The performance of ResMoodEmo increases when the number of frames are set to 3, 5, or 7, but decreases when the number of frames is increased to 9. Using the TS-Net, when the input samples have 3 frames, no change is observed, while with 7 and 9 frames, the performance reduces. Overall, the comparison shows that 5 frames in the input sample maximally increases mood prediction performance.\n\n3) Temporal Length (t) of the Sample: Table  III  presents the results of varying t of the input samples while fixing the number of frames (n) in each sample to be 5. Since mood is an enduring affect, it is important to consider long sequences of data for automatic mood inference. For t = 50, and t = 150, the F-score obtained with ResMood increases, while the Fscore remains the same for t = 200, as compared to using t = 100. Although varying t results in a comparable F-score with ResMood, it decreases in ResMoodEmo and TS-Net with lengths of 50, 150, and 200, as shown in the respective % increase to ResMood columns of Table  IV . The highest F-score with ResMood and the least F-score with ResMoodEmo is observed using t = 150. Since emotion is a short-term affect, observing the changes in the emotion over longer sequence of time causes a detrimental effect on mood inference.\n\n4) Varying the Backbone Architecture: Table V reports the results when the backbone architecture in the models is changed. The F-score for ResMood increases slightly as the depth of the ResNet increases. With ResNet18 and ResNet50, a general trend of increase in the mood prediction performance using ResMoodEmo and TS-Net is observed. Using ResNet34, an increase in F-score is observed with ResMoodEmo as compared to ResMood, but with TS-Net, the F-score remains the same. ResNet18, a lighter architecture as compared to its counterparts, results in the maximum F-score for ResMood-Emo and TS-Net, and largest % increase from ResMood.\n\n5) Temperature and α: The results with varying temperature (T ) and α values are shown in Table  VI . For varying α, as T increases, the F-score reduces. This is due to the fact that high values of temperature soften the output class probabilities, while with low temperature values, the relative differences in the probabilities are captured. For T = 5 and T = 7, the F-score either remains the same or increases as α increases, while for T = 3, no general trend is observed. As described in Sec. IV-C, α is the weight of the student loss function, indicating that lower values of α imply a higher weighting for the distillation loss, enabling the student model to get closer    Overall, the ablation study results confirm that the generated ∆ labels are an effective alternative to ∆ GT and produce a comparable effect, as incorporating these labels improves mood prediction performance.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Vi. Conclusion",
      "text": "The aim of this study is to examine mood from a computational perspective by incorporating emotion similarity information. Different from prior studies, without using the valence differential, this study proposes to use emotion change information by employing a metric learning approach. To this end, a Siamese network is trained using the AffectNet database and the trained model is used to generate pseudo-∆ labels for a pair of frames in the AffWild2 database. For mood classification, we employ ResMood, a model trained with mood labels alone, ResMoodEmo trained with mood labels and ∆ labels, and TS-Net, a teacher-student network with ResMoodEmo as the teacher to distil knowledge to ResMood. Higher F-scores are observed with models trained with both mood and ∆ labels as compared to models trained with mood labels alone. This indicates that the emotion change labels are generated effectively and contribute positively to the mood prediction performance. Our claim is further confirmed by similar trends when performing corresponding experiments employing ∆ GT labels.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Ethical Impact Statement",
      "text": "This study aims at examining mood from a computational perspective by using emotional similarity information. The data for the study reuses publicly available databases, Af-fectNet and AffWild2, to conduct computational modeling experiments. This study is designed towards answering a theoretical question regarding the interaction between mood and emotion. While facial information is revealed from images and videos in the databases, we neither use identity-specific information, nor base our claims on a specific religion, race or gender. The proposed framework is non-obtrusive, using the images, and videos present in the databases. One of the most crucial applications of this framework is in healthcare, to detect early signs of mood disorders such as depression, and monitoring the mood of the patients by observing their emotional patterns. Other application include education, gaming technology, marketing, etc  [4] .\n\nAlthough we aim at developing robust mood inference technology, as with any other affect recognition system, there could be potential ethical concerns. Mood inference could reveal sensitive information about an individual's mental state, and could be used against the person. Mood detection system could be also used inappropriately to influence or manipulate individuals' behavior or emotions. Additionally, the use of mood detection in contexts such as employment could lead to discrimination or bias. Finally, we acknowledge that there could be intrinsic bias, as we train our models on the databases which may be biased towards facial expressions of individuals from a specific location/culture.",
      "page_start": 7,
      "page_end": 8
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Specifically, emotion change is modelled in terms of",
      "page": 1
    },
    {
      "caption": "Figure 1: Study Overview: (1) We consider the publicly available AffWild2 [1] video dataset for automated mood inference. (2) A mood label is derived for the",
      "page": 2
    },
    {
      "caption": "Figure 1: II. RELATED WORK",
      "page": 2
    },
    {
      "caption": "Figure 2: Siamese network for modelling emotion-change (∆).",
      "page": 3
    },
    {
      "caption": "Figure 2: shows the architecture of the Siamese net-",
      "page": 4
    },
    {
      "caption": "Figure 3: (left) shows ResMood, a model trained with mood",
      "page": 4
    },
    {
      "caption": "Figure 3: (left) shows ResMoodEmo, a model for performing",
      "page": 4
    },
    {
      "caption": "Figure 3: (Left) The architecture of ResMood is shown within the inner dashed rectangle. The outer dashed rectangle represents ResMoodEmo. (Right) The",
      "page": 5
    },
    {
      "caption": "Figure 3: (right) presents the Teacher-Student Network",
      "page": 5
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Number of frames": "",
          "ResMood": "F-score",
          "ResMoodEmo": "F-score",
          "Column_4": "% increase to ResMood",
          "TS-Net": "F-score",
          "Column_6": "% increase to ResMood"
        },
        {
          "Number of frames": "3\n5\n7\n9",
          "ResMood": "0.67\n0.65\n0.68\n0.70",
          "ResMoodEmo": "0.68\n0.78\n0.72\n0.63",
          "Column_4": "+1.49\n+20\n+5.88\n-10",
          "TS-Net": "0.67\n0.78\n0.65\n0.52",
          "Column_6": "0\n+20\n-4.41\n-25.71"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Temporal length": "",
          "ResMood": "F-score",
          "ResMoodEmo": "F-score",
          "Column_4": "% increase to ResMood",
          "TS-Net": "F-score",
          "Column_6": "% increase to ResMood"
        },
        {
          "Temporal length": "50\n100\n150\n200",
          "ResMood": "0.67\n0.65\n0.69\n0.65",
          "ResMoodEmo": "0.65\n0.78\n0.65\n0.64",
          "Column_4": "-2.99\n+20\n-5.80\n-1.54",
          "TS-Net": "0.66\n0.78\n0.64\n0.64",
          "Column_6": "-1.49\n+20\n-7.25\n-1.54"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Backbone": "",
          "ResMood": "F-score",
          "ResMoodEmo": "F-score",
          "Column_4": "% increase to ResMood",
          "TS-Net": "F-score",
          "Column_6": "% increase to ResMood"
        },
        {
          "Backbone": "ResNet18\nResNet34\nResNet50",
          "ResMood": "0.65\n0.66\n0.68",
          "ResMoodEmo": "0.78\n0.70\n0.75",
          "Column_4": "+20\n+6.06\n+7",
          "TS-Net": "0.78\n0.66\n0.75",
          "Column_6": "+20\n0\n+7"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "T/α": "3\n5\n7",
          "0.05": "0.78\n0.64\n0.64",
          "0.1": "0.72\n0.64\n0.66",
          "0.15": "0.69\n0.68\n0.66",
          "0.2": "0.72\n0.69\n0.71"
        }
      ],
      "page": 7
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Expression, affect, action unit recognition: Aff-wild2, multi-task learning and arcface",
      "authors": [
        "D Kollias",
        "S Zafeiriou"
      ],
      "year": "2019",
      "venue": "30th British Machine Vision Conference"
    },
    {
      "citation_id": "2",
      "title": "Review of computational models of emotion",
      "authors": [
        "E Hudlicka",
        "J.-M Fellous"
      ],
      "year": "1996",
      "venue": "Review of computational models of emotion"
    },
    {
      "citation_id": "3",
      "title": "Affective science and health: the importance of emotion and emotion regulation",
      "authors": [
        "D Desteno",
        "J Gross",
        "L Kubzansky"
      ],
      "year": "2013",
      "venue": "Health Psychology"
    },
    {
      "citation_id": "4",
      "title": "Affective computing",
      "authors": [
        "R Picard"
      ],
      "year": "2000",
      "venue": "Affective computing"
    },
    {
      "citation_id": "5",
      "title": "What are emotions? and how can they be measured?",
      "authors": [
        "K Scherer"
      ],
      "year": "2005",
      "venue": "Social science information"
    },
    {
      "citation_id": "6",
      "title": "Mood-specific effects on appraisal and emotion judgements",
      "authors": [
        "M Siemer"
      ],
      "year": "2001",
      "venue": "Cognition & Emotion"
    },
    {
      "citation_id": "7",
      "title": "Cognitive emotional processing across mood disorders",
      "authors": [
        "P Panchal",
        "A Kaltenboeck",
        "C Harmer"
      ],
      "year": "2019",
      "venue": "CNS spectrums"
    },
    {
      "citation_id": "8",
      "title": "Facial expression perception: an objective outcome measure for treatment studies in mood disorders?",
      "authors": [
        "H Venn",
        "S Watson",
        "P Gallagher",
        "A Young"
      ],
      "year": "2006",
      "venue": "International Journal of Neuropsychopharmacology"
    },
    {
      "citation_id": "9",
      "title": "Odor perception in patients with mood disorders",
      "authors": [
        "S Lombion-Pouthier",
        "P Vandel",
        "S Nezelof",
        "E Haffen",
        "J.-L Millot"
      ],
      "year": "2006",
      "venue": "Journal of affective disorders"
    },
    {
      "citation_id": "10",
      "title": "Emotion recognition using facial expressions",
      "authors": [
        "P Tarnowski",
        "M Kołodziej",
        "A Majkowski",
        "R Rak"
      ],
      "year": "2017",
      "venue": "Procedia Computer Science"
    },
    {
      "citation_id": "11",
      "title": "Gender and emotion recognition with implicit user signals",
      "authors": [
        "M Bilalpur",
        "S Kia",
        "M Chawla",
        "T.-S Chua",
        "R Subramanian"
      ],
      "year": "2017",
      "venue": "ICMI '17: Proceedings of the 19th ACM International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "12",
      "title": "Affect Recognition in Ads with Application to Computational Advertising",
      "authors": [
        "A Shukla",
        "S Gullapuram",
        "H Katti",
        "K Yadati",
        "M Kankanhalli",
        "R Subramanian"
      ],
      "year": "2017",
      "venue": "MM '17: Proceedings of the 25th ACM international conference on Multimedia"
    },
    {
      "citation_id": "13",
      "title": "Ascertain: Emotion and personality recognition using commercial sensors",
      "authors": [
        "R Subramanian",
        "J Wache",
        "M Abadi",
        "R Vieriu",
        "S Winkler",
        "N Sebe"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "14",
      "title": "Examining Subject-Dependent and Subject-Independent Human Affect Inference from Limited Video Data",
      "authors": [
        "R Parameshwara",
        "I Radwan",
        "R Subramanian",
        "R Goecke"
      ],
      "year": "2023",
      "venue": "2023 IEEE 17th International Conference on Automatic Face and Gesture Recognition (FG)"
    },
    {
      "citation_id": "15",
      "title": "A functional analysis of the role of mood in affective systems",
      "authors": [
        "W Morris"
      ],
      "year": "1992",
      "venue": "Emotion"
    },
    {
      "citation_id": "16",
      "title": "Human attributes from 3d pose tracking",
      "authors": [
        "L Sigal",
        "D Fleet",
        "N Troje",
        "M Livne"
      ],
      "year": "2010",
      "venue": "Computer Vision-ECCV 2010: 11th European Conference on Computer Vision"
    },
    {
      "citation_id": "17",
      "title": "Mood recognition based on upper body posture and movement features",
      "authors": [
        "M Thrasher",
        "M Van Der Zwaag",
        "N Bianchi-Berthouze",
        "J Westerink"
      ],
      "year": "2011",
      "venue": "Affective Computing and Intelligent Interaction: 4th International Conference, ACII 2011"
    },
    {
      "citation_id": "18",
      "title": "To improve is to change: Towards improving mood prediction by learning changes in emotion",
      "authors": [
        "S Narayana",
        "R Subramanian",
        "I Radwan",
        "R Goecke"
      ],
      "year": "2022",
      "venue": "To improve is to change: Towards improving mood prediction by learning changes in emotion"
    },
    {
      "citation_id": "19",
      "title": "Distilling the knowledge in a neural network",
      "authors": [
        "G Hinton",
        "O Vinyals",
        "J Dean"
      ],
      "year": "2015",
      "venue": "Distilling the knowledge in a neural network",
      "arxiv": "arXiv:1503.02531"
    },
    {
      "citation_id": "20",
      "title": "Toward a dynamic theory of emotion: The component process model of affective states",
      "authors": [
        "K Scherer"
      ],
      "year": "1987",
      "venue": "Geneva studies in Emotion and Communication"
    },
    {
      "citation_id": "21",
      "title": "Expression and the nature of emotion",
      "authors": [
        "P Ekman"
      ],
      "year": "1984",
      "venue": "Approaches to emotion"
    },
    {
      "citation_id": "22",
      "title": "Understanding emotions. Blackwell publishing",
      "authors": [
        "K Oatley",
        "D Keltner",
        "J Jenkins"
      ],
      "year": "2006",
      "venue": "Understanding emotions. Blackwell publishing"
    },
    {
      "citation_id": "23",
      "title": "The mood-emotion loop",
      "authors": [
        "M Wong"
      ],
      "year": "2016",
      "venue": "Philosophical Studies"
    },
    {
      "citation_id": "24",
      "title": "Moodcongruent judgment is a general effect",
      "authors": [
        "J Mayer",
        "Y Gaschke",
        "D Braverman",
        "T Evans"
      ],
      "year": "1992",
      "venue": "Journal of personality and social psychology"
    },
    {
      "citation_id": "25",
      "title": "Mood effects on emotion recognition",
      "authors": [
        "P Schmid",
        "M Mast"
      ],
      "year": "2010",
      "venue": "Motivation and Emotion"
    },
    {
      "citation_id": "26",
      "title": "Collecting large, richly annotated facial-expression databases from movies",
      "authors": [
        "A Dhall",
        "R Goecke",
        "S Lucey",
        "T Gedeon"
      ],
      "year": "2012",
      "venue": "IEEE Multimedia"
    },
    {
      "citation_id": "27",
      "title": "DECAF: MEG-based multimodal database for decoding affective physiological responses",
      "authors": [
        "M Abadi",
        "R Subramanian",
        "S Kia",
        "P Avesani",
        "I Patras",
        "N Sebe"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "28",
      "title": "Introducing the RECOLA multimodal corpus of remote collaborative and affective interactions",
      "authors": [
        "F Ringeval",
        "A Sonderegger",
        "J Sauer",
        "D Lalanne"
      ],
      "year": "2013",
      "venue": "2013 10th IEEE international conference and workshops on automatic face and gesture recognition (FG)"
    },
    {
      "citation_id": "29",
      "title": "Afew-va database for valence and arousal estimation in-the-wild",
      "authors": [
        "J Kossaifi",
        "G Tzimiropoulos",
        "S Todorovic",
        "M Pantic"
      ],
      "year": "2017",
      "venue": "Image and Vision Computing"
    },
    {
      "citation_id": "30",
      "title": "Crowdsourcing empathetic intelligence: The case of the annotation of emma database for emotion and mood recognition",
      "authors": [
        "C Katsimerou",
        "J Albeda",
        "A Huldtgren",
        "I Heynderickx",
        "J Redi"
      ],
      "year": "2016",
      "venue": "ACM Transactions on Intelligent Systems and Technology (TIST)"
    },
    {
      "citation_id": "31",
      "title": "Predicting mood from punctual emotion annotations on videos",
      "authors": [
        "C Katsimerou",
        "I Heynderickx",
        "J Redi"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "32",
      "title": "Affectnet: A database for facial expression, valence, and arousal computing in the wild",
      "authors": [
        "A Mollahosseini",
        "B Hasani",
        "M Mahoor"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "33",
      "title": "Signature verification using a \"siamese\" time delay neural network",
      "authors": [
        "J Bromley",
        "I Guyon",
        "Y Lecun",
        "E Säckinger",
        "R Shah"
      ],
      "year": "1993",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "34",
      "title": "Speech emotion recognition via contrastive loss under siamese networks",
      "authors": [
        "Z Lian",
        "Y Li",
        "J Tao",
        "J Huang"
      ],
      "year": "2018",
      "venue": "Proceedings of the Joint Workshop of the 4th Workshop on Affective Social Multimedia Computing and First Multi-Modal Affective Computing of Large-Scale Multimedia Data"
    },
    {
      "citation_id": "35",
      "title": "Facial expression intensity estimation using siamese and triplet networks",
      "authors": [
        "M Sabri",
        "T Kurita"
      ],
      "year": "2018",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "36",
      "title": "Estimation of continuous valence and arousal levels from faces in naturalistic conditions",
      "authors": [
        "A Toisoul",
        "J Kossaifi",
        "A Bulat",
        "G Tzimiropoulos",
        "M Pantic"
      ],
      "year": "2021",
      "venue": "Nature Machine Intelligence"
    },
    {
      "citation_id": "37",
      "title": "Multiple spatio-temporal feature learning for video-based emotion recognition in the wild",
      "authors": [
        "C Lu",
        "W Zheng",
        "C Li",
        "C Tang",
        "S Liu",
        "S Yan",
        "Y Zong"
      ],
      "year": "2018",
      "venue": "Proceedings of the 20th ACM International Conference on Multimodal Interaction (ICMI2018)"
    },
    {
      "citation_id": "38",
      "title": "Emotion-aware Multi-view Contrastive Learning for Facial Emotion Recognition",
      "authors": [
        "D Kim",
        "B Song"
      ],
      "year": "2022",
      "venue": "Computer Vision-ECCV 2022: 17th European Conference"
    },
    {
      "citation_id": "39",
      "title": "Exploring emotion features and fusion strategies for audio-video emotion recognition",
      "authors": [
        "H Zhou",
        "D Meng",
        "Y Zhang",
        "X Peng",
        "J Du",
        "K Wang",
        "Y Qiao"
      ],
      "year": "2019",
      "venue": "2019 International Conference on Multimodal Interaction"
    }
  ]
}