{
  "paper_id": "2011.00043v1",
  "title": "Pose-Based Body Language Recognition For Emotion And Psychiatric Symptom Interpretation",
  "published": "2020-10-30T18:45:16Z",
  "authors": [
    "Zhengyuan Yang",
    "Amanda Kay",
    "Yuncheng Li",
    "Wendi Cross",
    "Jiebo Luo"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Inspired by the human ability to infer emotions from body language, we propose an automated framework for body language based emotion recognition starting from regular RGB videos. In collaboration with psychologists, we further extend the framework for psychiatric symptom prediction. Because a specific application domain of the proposed framework may only supply a limited amount of data, the framework is designed to work on a small training set and possess a good transferability. The proposed system in the first stage generates sequences of body language predictions based on human poses estimated from input videos. In the second stage, the predicted sequences are fed into a temporal network for emotion interpretation and psychiatric symptom prediction. We first validate the accuracy and transferability of the proposed body language recognition method on several public action recognition datasets. We then evaluate the framework on a proposed URMC dataset, which consists of conversations between a standardized patient and a behavioral health professional, along with expert annotations of body language, emotions, and potential psychiatric symptoms. The proposed framework outperforms other methods on the URMC dataset.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "Humans have shown a remarkable ability to infer emotions in face-to-face conversations, and much of the inference is made through body language. For example, among people in similar culture, \"touching one's nose\" implies disbelief, and \"holding one's head in the hands\" expresses upset. It seems a natural ability for humans to understand the \"meaning\" of body language. To help machines acquire a similar ability, we propose a two-stage framework that predicts emotions based on body language with regular RGB video inputs. In the first stage, the model predicts body language from input videos based on estimated human poses. The predicted body language are then fed into the second stage for emotion interpretation. We define a body language as a certain maintained posture or a period with repeated short actions. It is similar to but different from human actions defined in previous studies  [1] ,  [2] , which are usually shorter in time and contain dynamic motions. For example, \"holding one's head in the hands\" is a typical body language and is informative for emotion recognition, but it is not an action according to the definition in action recognition  [1] ,  [2] .\n\nAutomated body language based emotion recognition is useful in various application domains, such as health care, online chat, and computer-mediated communication  [3] . Despite the shared automated body language and emotion recognition techniques in different application domains, the body language and emotion of interest vary. For example, online chatting systems are concerned with detecting people's moods, i.e., whether they are happy or not, while applications in healthcare scenarios focus on identifying potential signs of mental disorders such as depression or panic attacks. Since a specific emotion can only be reflected by the corresponding body language 1 , different applications require the annotation of different body language and emotions. Annotating videos for each application potentially lead to a high annotation cost. To alleviate the data annotation problem, we design our framework to learn from a small training set together with the expert knowledge, instead of being purely data-driven.\n\nSpecifically, we improve the method's transferability and reduce the required amount of annotations by 1). using the abstracted human poses as the framework input, 2). proposing a KNN based approach for body language recognition, and 3). conducting emotion recognition purely based on the predicted body language sequences. In the first-stage body language recognition, instead of directly predicting body language from RGB videos  [4] ,  [5] , we use human poses as the input for body language recognition. Human poses are groups of human joint coordinates that provide abstracted high-level human structural information. Because of the abstractness, pose-based methods require less training data and have better transferrabilities, as proved empirically in our study. The recently avabile robust pose estimation methods  [6] ,  [7] ,  [8]  make our posebased attempt more feasible. We adopt Openpose  [6]  for pose estimation, and propose a Spatio-Temporal Convolutional Pose Feature (ST-ConvPose) to encode the spatial-temporal relationships among the joints. With the learned pose representation, we propose a K-Nearest-Neighbors-based classifier for body language recognition. In the second-stage emotion recognition, the model predicts the emotion based on the predicted body language sequences. Furthermore, as a concrete example of application, we adopt the proposed framework to help psychiatrists understand psychiatric symptoms from patients' body language and emotions. In collaboration with psychologists, we collected the URMC dataset under the mental healthcare scenario. The recorded videos are the conversations between a standardized patient 1 Examples of body language and their meanings can be found in this website. https://www.enkiverywell.com/body-language-examples.html and a psychiatrist or psychologist. Health experts define the body language, emotion, and psychiatric symptom classes adopted in this study. Multiple psychologists annotate the recorded videos in the URMC dataset with the defined classes. Experiments on the URMC dataset prove the effectiveness of the proposed framework.\n\nOur main contributions are two-fold:\n\n• We propose a framework to infer emotions from the body language. We design the framework to be interpretable and transferrable. • We adopt the proposed framework to help mental health professionals predict psychiatric symptoms. A new URMC dataset is built for the study.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Related Work",
      "text": "Human action recognition from videos is an important area in computer vision. Most of the RGB video based action recognition studies  [4] ,  [9] ,  [5]  start from low-level features. Intuitively though, high-level features such as human joints should be informative and beneficial for boosting the recognition accuracy. Joint-based action recognition  [10] ,  [11] ,  [12] ,  [13] ,  [14]  acquires reliable 3D joints with Kinect or similar RGB-D sensors and generates action predictions based on the joint sequences. However, one inherent problem in applications is the cost and inconvenience of depth sensors. Furthermore, depth sensors are difficult to integrate into the application system.\n\nTo better capture human-related features, Jhuang et al.  [15]  propose the scheme of using both RGB videos and poses for action recognition. Pose-CNN  [16]  proposes to use joint information to generate body parts sub-images, which are fed into a two-stream network for action recognition. Studies in JHMDB  [15]  and Pose-CNN  [16]  suggest that high-level information is rewarding for action recognition, while subject to the limited performance of pose estimation. The recent improvements in human pose estimation  [6] ,  [17]  make posebased recognition feasible and attractive. Several recent studies  [18] ,  [19] ,  [20] ,  [21]  propose various further improvements on the RGB+Pose task. We conduct body language prediction from RGB videos with poses calculated by OpenPose  [6] ,  [7] .\n\nThis study is also related to emotion recognition  [22] ,  [23] ,  [24] ,  [25] ,  [26] ,  [27] ,  [28] ,  [29] ,  [30] ,  [31] ,  [32] ,  [33] ,  [34] ,  [35] . Several previous studies  [36] ,  [37]  propose to detect psychological stresses with multi-modal contents. Xu et al.  [38]  recognize effects with body movements. Furthermore, this paper shares a similar goal with several previous studies on predicting human emotion from body motions and facial expressions. Traditional studies  [39] ,  [40]  focus on analyzing facial expressions for emotion interpretation. De et al.  [41]  first propose to adopt gestures as a channel for emotion interpretation, and a motion capture system is built to record children's behaviors in the scenario of playing network games. Gunes et al. and Shan et al.  [42] ,  [43]  propose to extract multimodality information from videos for emotion recognition, which is more closely related to this paper. Shan et al.  [43]  propose to fuse body gestures and facial expressions with Canonical Correlation Analysis (CCA). Furthermore, Gunes et al.  [42]  combine features from facial expressions, hand poses, body parts location, and orientation with a late fusion method. Although previous studies have achieved good performances on a bi-modal face and body gesture database (FABO)  [44] , there are problems in real applications since the dataset is collected under ideal conditions. FABO contains face and body images in a front view with no body part occlusion, which simplifies the problem.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Iii. Methodology",
      "text": "In this study, we propose a framework that recognizes body language and human emotions with a small amount of data. The input of the framework is a long sequence of estimated human poses p = {p t } where t ∈ 0, . . . , T -1 is the length of an untrimmed video. The framework contains two stages: the pose-based body language recognition stage and the body language based emotion interpretation stage. In the first stage, the model takes the pose sequence p as the input and outputs two body language sequences that represent the upper-and lower-body body language, respectively. The classifier is built with an easily-transferrable pose feature and an example based classifier, instead of being purely data-driven. The second stage takes the two predicted body language sequences as inputs and learns the emotions expressed by the person of interest. The framework is shown in Figure  1 .\n\nIn this section, we first discuss the pose estimation and preprocessing methods, together with several previous high-level pose feature representations, i.e., the NTraj and NTraj+  [15] . Then we introduce the proposed Spatial-Temporal Convolutional Pose Feature (ST-ConvPose). We also discuss the methods for processing incorrect or missing pose estimations. After getting the pose feature representation, we predict the body language with an example-based classifier. Finally, we propose a temporal network for emotion interpretation based on the detected body language sequences.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "A. Pose Feature Generation",
      "text": "The body language prediction stage uses only the poses as inputs to achieve the desired transferability and to reduce the required amount of data. We adopt a reliable pose estimator called OpenPose  [6] ,  [7]  to generate 2D human poses from RGB videos. We then encode the estimated poses as the highlevel pose features for body language recognition. In this section, we first introduce the pose pre-processing techniques and two previous pose features  [15] . The proposed pose feature is introduced in the next section.\n\nPre-processing. For each frame, we calculate 18-point human poses with OpenPose. In the pre-processing step, we first normalize the predicted poses with the torso size. In previous works, a puppet mask is used to estimate the torso size, but it is not available in our task. Therefore, we propose to normalize the poses by the distance between the neck joint and the center of two hip joints, such that the length is fixed to 240 pixels. After the size normalization, all joints are \"centered\" with a reference point. The reference point is defined as the averaged neck joints in several adjacent frames. NTraj. The NTraj feature  [15]  is one of the most basic highlevel pose features, which only represents the joint position information. Five features are encoded for each joint. The centered x-and y-coordinates are selected as the first two dimensions that represent spatial structures. The differences in pixel locations between two frames are encoded to represent the temporal motion. The direction of motion arctan( dy dx ) is also included. Furthermore, the motion is calculated with a temporal gap s, i.e. dx = x t+s -x t , dy = y t+s -y t , which helps eliminate jitters and provide reliable motion. The gap lengths of s = 1, 2, 3 are computed, and the trajectory length T is set to T = 5 based on experiments. Finally, each dimension in the calculated NTraj features is normalized by the absolute sum value on a training set, i.e.\n\nThe normalization is calculated 5 times for each feature F j i = {x j , y j , dx j , dy j , arctan( dy dx ) j }, where j represents the timestamps.\n\nNTraj+. Besides the 5 kinds of position-based features in NTraj calculated at each joint independently, the NTraj+ feature further includes the relationship information among different joints. The orientations between every two joints and the inner angles of all permutations in a three-joint group are encoded to represent the relationship information.\n\nThe bag-of-features is calculated to encode the pose feature representations. For each feature type, a codebook of size N is formed by running k-means 10 times on all features available in a training set and pick the one with the lowest error. With a small feature dimensionality, the codebook size N is tested among N = 10, 20, 50, 100, 200, 500.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "B. St-Convpose Feature",
      "text": "In previous high-level pose features, the spatial relations among joints are encoded with pre-defined features such as body part lengths, orientations, inner angles, and so on. Many RGB+Pose action recognition studies  [16] ,  [18]  also only use the pre-defined spatial relation information among joints that can not be adaptively learned. Inspired by the recent studies on 3D skeleton representations  [10] ,  [45] ,  [46] , we propose a spatial-temporal convolutional pose feature (ST-ConvPose) that learns the spatial and temporal relations among joints simultaneously with 2D CNNs. In the proposed feature, the coordinates of poses are arranged as 2D matrices where each row is the chaining of joint coordinates at time-stamp t, and the column lists the chains in all timestamps. To arrange the joints in each row, we experiment with three different orders proposed by J-HMDB  [15] , PennAction  [47] , and NTU RGB+D  [48] . The order defined in NTU RGB+D works the best on action recognition datasets. Therefore, we arrange the joints in each row following this order that joints in each body part are grouped first and then chained from upper left to lower right. To be specific, the left shoulder to the left wrist are placed in column 2 to 4, the right shoulder to the right wrist are in column 5 to 7, and so on. The encoded 2D matrices are then scaled into 0 to 255 and resized to a fixed width and height. The formatted multi-channel 2D matrices are referred to as pose images. Examples of the generated pose images are shown in Figure  2 (a) . The pose images are then fed into CNNs to generate human activity representation in an end-toend manner. The ResNet-50  [49]  is used as the CNN structure for high-level pose feature encoding. The feature encoding structure is shown in Figure  2 (b) .\n\nWith the ability to jointly learn the spatio-temporal joint relations, the proposed ST-ConvPose features show a better performance in modeling human activities compared to previous high-level pose features. Furthermore, the ST-ConvPose feature can be trained with a limited amount of training data and possesses a good transferability. With these desired properties provided by pose features, we do not include RGB frames as inputs to the framework and learn purely based on estimated poses. More details are discussed in the experiment section.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "C. Body Language Sequence Prediction",
      "text": "The proposed body language recognition stage is designed to work on untrimmed videos, and a fixed-length sliding window is adopted to convert the localization and recognition problem into a regular classification task.\n\nIn practice, different body parts could perform different body language simultaneously, and the required joints for recognition are different. For example, people might keep a catapult posture while having their legs crossed. Because of this, we divide body language into two label sets and predict them separately. The lower body pose set contains leg postures, including locking ankles, leg crossed, walking around, and so on. The upper body pose set includes both arm motions and hand movements. A background class is added to both pose sets. Full label name list is shown in Figure  3 . The split of the labeling set not only solves the multi-label problem but also provides classifiers the prior knowledge on which groups of joints should be focused on to recognize a specific body language. Joints on the torso and both legs are used for lower-body body language recognition, and the upper body recognition task takes the joints on the arms and head as input.\n\nAfter obtaining the feature representations with either NTraj+ or ST-ConvPose, we adopt a k-nearest neighbor classifier with manually selected label examples. For each body language, we manually select five to seven video clips and use their pose features as the KNN data points. Classification is conducted on each video clip under the sliding window. The output of the model is the two sequences of upper and lower body language predictions. The body language prediction framework is shown in the left part of Figure  1 .",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "D. Emotion Interpretation",
      "text": "After getting the body language sequence predictions, we propose an emotion RNN network for emotion interpretation as the second stage of the framework. We predict the body language sequences for all video clips in a dataset. Each output contains the lower-and upper-body body language prediction sequences with length K, where K is the number of sliding windows in an input video. We apply another sliding window on the predicted body language sequences and calculate the body language histogram under each window. The histogram sequences are fed into an LSTM network in the temporal order. The LSTM outputs at all timestamps are fed into another dense layer for the final video-level emotion prediction. The output is an N-hot vector representing the predicted emotions.\n\nWe have also experimented with the end-to-end emotion recognition model. We replace the KNN classifier with dense layers and teach the model to simultaneously predict body language and emotions. Unsurprisingly, due to the limited size of the available emotion labels, the end-to-end framework fails to learn effectively.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Iv. The Urmc Dataset",
      "text": "Among the various applications of the proposed body language and emotion recognition framework, psychotherapy is an area especially suitable for model evaluation. There exist solid and complete theoretical proofs in the medical field for the emotions of interest and the corresponding body language. Therefore, the proposed framework is employed to predict psychiatric symptoms. We construct a new URMC dataset under realistic simulated mental health care scenarios with expert annotations. A typical scenario involves the conversation between a standardized patient and a psychiatrist, with an initial intention to infer the standardized patient's potential symptoms by analyzing their body language. There are 144 30-second long video segments cropped from 12 20-minute videos.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "A. Dataset Collection",
      "text": "Scene Recording. We collaborated with psychologists to collect 12 videos recording the diagnostic sessions between standardized patients (highly trained actors) and mental health professionals, with the view focused on the standardized patients (SPs). The mental health professionals complete a survey after each diagnostic session to control the quality of the dataset. Positive feedbacks on questions, including the confidence level of diagnosis and other session-related questions, prove the quality of the recordings. All participants in the recordings provided permission for the data collection for research. Both RGB videos and conversation audios are recorded, and poses are later estimated using RGB videos. Example frames and estimated poses are shown in Figure  1 . Four standardized patients and eight mental health professionals (psychologists, psychiatrists) are involved. For the 12 videos, either the standardized patient or the psychiatrist is different. Also, the subjects' clothing is different in all videos.\n\nDataset Labeling. Three mental health professionals are involved in the dataset labeling process. In the first step, three mental health professionals go through each of the 12 recorded long videos, remove less informative segments, select 12 30second video segments, and then make the symptom decisions. Removing the less informative segments can reduce the labeling cost, and also reduce the noises in the dataset. After getting the 144 video segments, three mental health experts label the video clips with discernible body language and symptoms. Each video clip is labeled by all three health professionals, and conflicts are resolved in consensus discussions. Both the video recordings and the conversation audios are used for labeling, as certain symptoms can only be reflected from the content of the conversation. It is worth mentioning that instead of generating body language and emotion labels based on previous studies in the computer science field, the labels for annotation in our dataset are designed by the mental health experts with evidence in the medical field. Based on the psychiatry theories, 32 body language, 24 emotions, and 24 symptoms are selected for labeling. A final psychiatric verdict of whether the standardized patient has the major depressive disorder (MDD) or manic episode (ME) is also included. This dataset is built to help inferring emotions and psychiatric symptoms from body language. The 32 body language includes arm crossed, finger tapping, ear pulling and etc. Once a body language appears in a 30-second video, the video clip is marked with that label. Because certain body language rarely appear and several of them are extremely similar, we merge a number of body language classes. The final 14 body language classes are shown in Figure  3 , which are arm crossed, biting nails, and etc. Furthermore, a subset of the videos clips has frame-level body language labels that are annotated under sliding windows with a length of 6 frames.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "B. Dataset Summary",
      "text": "There are 144 30-second video clips cropped from 12 20minute videos. Each clip has multiple labels from the 14 body language, 24+1 emotion labels, and 24+1 symptom labels. Among the 144 videos, 48 videos are selected for training and have frame-level body language labels. Additionally, 48 videos are for validation, and 48 videos are for testing. The split follows a cross-subject setting. We will release the collected and processed poses.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "V. Experiments",
      "text": "In this section, we first experiment with the proposed ST-ConvPose feature. We then evaluate the proposed body language and emotion recognition framework on the URMC dataset. Section V-C presents the results of body language prediction, and Section V-D shows the performance of emotion recognition and psychiatric symptom prediction.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "A. St-Convpose Feature Evaluation",
      "text": "Because there lack public benchmarks on body language recognition, we evaluate the proposed ST-ConvPose feature on a similar task of action recognition and compare our method to state-of-the-art methods. It is important to note that although body language, as defined earlier, are different from actions, it is the best comparison we could perform due to the lack of existing body language datasets. Experiments prove that the proposed ST-ConvPose feature outperforms both RGB-based and pose-based state-of-the-art with only the pose information. Furthermore, experiments show that the ST-ConvPose feature achieves the following two desired properties:\n\n1) The pose feature generates good results when the amount of training data is limited.\n\n2) The pose feature captures more dataset-invariant human representations instead of learning the dataset bias, thus has a better transferability. We detail the analyses as follows.\n\nAction Recognition Results. We evaluate the proposed ST-ConvPose feature on the PennAction  [47]  and UCF-Motion  [14]  datasets. PennAction contains 1,212 videos for training and 1,020 videos for testing, with 2D full body joints manually labeled on each frame. Half of the training videos are used for training, and the rest is the validation set. UCF-Motion dataset extends UCF-101  [2]  by including estimated poses  [50]  on all video frames. UCF-Motion contains 23 classes from UCF-101 with 3172 videos in total. Table  I  shows the action recognition accuracy on the PennAction dataset. Our ST-ConvPose feature outperforms both the RGB-based and the pose-based state-of-the-art on the PennAction dataset. It is gratifying even when compared with the methods using both RGB and pose information, the proposed pose feature can achieve comparable results. The experiments show the effectiveness of the proposed ST-ConvPose feature and prove that poses can adequately represent the information needed to distinguish human actions. A similar improvement is also observed on UCF-Motion, as shown in Table  II . TABLE I: Recognition accuracy compared to the state-of-the-art on PennAction. The input data format is also shown.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "State-Of-The-Art",
      "text": "Acc. Pose RGB C3D  [5]  86.0 idt-fv  [20]  92.0 -NTraj+  [20]  79.0 -JDD  [18]  87.4 Pose+idt-fv  [20]  92.9 ST-ConvPose 94.4 -",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Table Ii:",
      "text": "The action recognition accuracy compared to the stateof-the-art methods on the UCF-Motion dataset.\n\nState-of-the-art Acc. Pose RGB Flow HLPF  [15]  71.4 --C3D  [5]  75.2 --Flow CNN  [51]  85. As shown in Table  III , the ST-ConvPose feature achieves an 82.4% accuracy using the pose features pretrained on NTU RGB+D without fine-tuning, which is already better than the 79.0% accuracy generated by previous N T raj+ pose feature. The high recognition accuracy proves that the pose feature captures invariant information to represent actions and has an excellent ability for domain transfer even without fine-tuning. The results can be further improved with fine-tuning.\n\nIn this experiment, we show that the proposed ST-ConvPose feature outperforms the state-of-the-art action recognition methods on PennAction and UCF-Motion. Furthermore, it requires less training data and has good transferability.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "B. Evaluation Settings On The Urmc Dataset",
      "text": "We evaluate the proposed body language and emotion recognition framework on the URMC dataset. For the first stage of the framework, we treat the body language sequence prediction as a video-level multi-label classification task. In the experiment, an N-hot vector is generated from the predicted body language sequence and compared with the video-level ground truth body language labels. Metrics for multi-label classification are used for evaluation, i.e., multi-label accuracy, precision, recall, and F1-score.\n\nThe second stage of the framework is emotion interpretation and symptom prediction. For emotion interpretation, we compare the proposed temporal network with other approaches as a multi-label classification task. Instead of predicting all 24 labeled psychiatric symptoms, the model only learns to distinguish major symptoms of Major Depressive Disorder (MDD) with Manic Episode (ME). As mentioned in Section IV-A, certain symptoms can only be reflected with other modalities, such as the audio track. Although predicting detailed psychiatric symptoms is a very interesting problem of great importance, we focus on a more feasible task of inferring emotions and major psychiatric symptoms in this study.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "C. Body Language Recognition",
      "text": "Table  IV  shows the body language recognition accuracy. Other than the accuracy metrics, we also indicate the models' interpretability, transferability, and the required amount of training data. We first compare the model with two end-to-end methods: the two-stream action recognition and the proposed ST-ConvPose feature trained end-to-end with dense layers. The two-stream action recognition is pretrained on UCF101  [2]  and HMDB51  [1] . Since both methods are trained end-to-end, they lack the interpretability. The ST-ConvPose feature requires less data since it is trained from pose information instead of RGB frames. Furthermore, the ST-ConvPose feature provides a good ability for domain transfer. For SVM, we adopt the one-vsone approach for multi-class classification with linear kernels. Overall, the KNN classifier provides interpretable results and requires less data for training. The proposed ST-ConvPose feature provides the desired transferability and further reduces the required amount of training data.\n\nIn Table  IV , N T raj + + SVM and ST-ConvPose + Dense layers have the lowest recognition accuracy because of the limited training data size of 48 video clips. Although the two-stream network requires even more training data, the model is transferred from the one pretrained on UCF101 and HMDB51. In contrast, our KNN based methods have the best performance with a small training set. Overall, our proposed ST-ConvPose and the KNN classifier has the best recognition performance.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "D. Emotion And Symptom Prediction",
      "text": "The ultimate goal of the framework is to understand emotions and infer psychiatric symptoms from human body language. We conduct emotion recognition purely based on the predicted body language sequence to achieve the desired transferability. A LSTM-based temporal network is proposed for the task. The output of the network is an N-hot vector with a length of 25, representing the 24 labeled emotions and a background class. For baseline comparisons, we implement a network with a 1D convolution for temporal information learning. The idea of adopting a convolutional layer for sequence learning is inspired by works in natural language processing, where CNN are used for sentence analysis  [52] ,  [53] . The body language sequences predicted by the proposed ST-ConvPose feature is also compared with the one predicted with NTraj+. We also consider an end-to-end design of the framework. However, the result is not promising due to the limited amount of body language labels and emotion labels. The experiment results on emotion interpretation are shown in Table  V . Two special cases are worth noting. First, when the sliding window length and stride both equal to one, the predicted vector sequences are directly fed into the temporal network without forming a histogram. Second, when the window length is the entire video, a video-level body language histogram is calculated with no temporal information used, similar to directly adopting a dense layer at the top. The limited performance of the two special cases proves the effectiveness of the proposed temporal structure and histogram features. The experiment results in Table V also indicate that the LSTM structure has a better ability in representing the temporal information in body language sequences, compared to 1D convolutional neural networks. Additionally, using the proposed ST-ConvPose feature generates a better overall performance compared to previous high-level pose features.\n\nFurthermore, we try to infer the psychiatric symptoms based on the body language predictions. The result of the binary classification between Major Depressive Disorder (MDD) or Manic Episode (ME) is promising. We achieve an accuracy of 90.3% with the ground truth body language sequences and 79.9% with the predicted sequences.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Vi. Conclusion And Future Work",
      "text": "Teaching machines to recognize human emotions from body language is a challenging but useful task that has various potential applications. In this paper, we propose a framework starting from regular RGB videos for body language prediction and emotion interpretation. Aiming at building a system capable of easy transfer between different application scenarios and producing interpretable results, we design a body language recognition system with the ST-ConvPose feature and the KNN classifier. The proposed pose feature shows good performances on both public datasets and the new URMC dataset, while requiring less training data and showing good transferability. Finally, we show that emotions or psychiatric symptoms can be predicted reliably from recognized human body language. Next on our agenda is to apply the framework to more domains and continue exploring detailed psychiatric symptom detection based on body language.",
      "page_start": 7,
      "page_end": 7
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: In this section, we ﬁrst discuss the pose estimation and pre-",
      "page": 2
    },
    {
      "caption": "Figure 1: Overview of the proposed framework. This is a two-stage framework for body language prediction and emotion interpretation. The",
      "page": 3
    },
    {
      "caption": "Figure 2: (a). The pose images are then fed into",
      "page": 3
    },
    {
      "caption": "Figure 2: (a). Pose image examples. (b). The framework for the spatio-",
      "page": 4
    },
    {
      "caption": "Figure 3: The split",
      "page": 4
    },
    {
      "caption": "Figure 1: D. Emotion Interpretation",
      "page": 4
    },
    {
      "caption": "Figure 1: Four standardized patients and eight mental health profes-",
      "page": 4
    },
    {
      "caption": "Figure 3: Examples from the URMC dataset. First row: examples from the upper body set. Second row: examples from the lower body set.",
      "page": 5
    },
    {
      "caption": "Figure 3: , which are arm crossed, biting nails, and",
      "page": 5
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Upper \nLower \nWindow\nindex\nSet\nSet": "0\n4\n2"
        },
        {
          "Upper \nLower \nWindow\nindex\nSet\nSet": "1\n2\n2"
        },
        {
          "Upper \nLower \nWindow\nindex\nSet\nSet": "2\n2\n2"
        },
        {
          "Upper \nLower \nWindow\nindex\nSet\nSet": "......"
        },
        {
          "Upper \nLower \nWindow\nindex\nSet\nSet": "k\n5\n3"
        },
        {
          "Upper \nLower \nWindow\nindex\nSet\nSet": "......"
        },
        {
          "Upper \nLower \nWindow\nindex\nSet\nSet": "K\n9\n2"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Anger": "Rage",
          "0": "0"
        },
        {
          "Anger": "Disgust",
          "0": "0"
        },
        {
          "Anger": "Fear",
          "0": "0"
        },
        {
          "Anger": "Joy",
          "0": "1"
        },
        {
          "Anger": "Distraction",
          "0": "1"
        },
        {
          "Anger": "......",
          "0": ""
        },
        {
          "Anger": "Trust\n0",
          "0": ""
        }
      ],
      "page": 3
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Hmdb: a large video database for human motion recognition",
      "authors": [
        "H Kuehne",
        "H Jhuang",
        "E Garrote",
        "T Poggio",
        "T Serre"
      ],
      "year": "2011",
      "venue": "ICCV"
    },
    {
      "citation_id": "2",
      "title": "Ucf101: A dataset of 101 human actions classes from videos in the wild",
      "authors": [
        "K Soomro",
        "A Zamir",
        "M Shah"
      ],
      "year": "2012",
      "venue": "CoRR"
    },
    {
      "citation_id": "3",
      "title": "Moving as a leader: Detecting emergent leadership in small groups using body pose",
      "authors": [
        "C Beyan",
        "V.-M Katsageorgiou",
        "V Murino"
      ],
      "year": "2017",
      "venue": "ACM MM"
    },
    {
      "citation_id": "4",
      "title": "Two-stream convolutional networks for action recognition in videos",
      "authors": [
        "K Simonyan",
        "A Zisserman"
      ],
      "year": "2014",
      "venue": "Neurips"
    },
    {
      "citation_id": "5",
      "title": "Learning spatiotemporal features with 3d convolutional networks",
      "authors": [
        "D Tran",
        "L Bourdev",
        "R Fergus",
        "L Torresani",
        "M Paluri"
      ],
      "year": "2015",
      "venue": "ICCV"
    },
    {
      "citation_id": "6",
      "title": "Realtime multi-person 2d pose estimation using part affinity fields",
      "authors": [
        "Z Cao",
        "T Simon",
        "S.-E Wei",
        "Y Sheikh"
      ],
      "year": "2017",
      "venue": "CVPR"
    },
    {
      "citation_id": "7",
      "title": "Hand keypoint detection in single images using multiview bootstrapping",
      "authors": [
        "T Simon",
        "H Joo",
        "I Matthews",
        "Y Sheikh"
      ],
      "year": "2017",
      "venue": "CVPR"
    },
    {
      "citation_id": "8",
      "title": "Convolutional pose machines",
      "authors": [
        "S.-E Wei",
        "V Ramakrishna",
        "T Kanade",
        "Y Sheikh"
      ],
      "year": "2016",
      "venue": "CVPR"
    },
    {
      "citation_id": "9",
      "title": "Long-term recurrent convolutional networks for visual recognition and description",
      "authors": [
        "J Donahue",
        "L Hendricks",
        "S Guadarrama",
        "M Rohrbach",
        "S Venugopalan",
        "K Saenko",
        "T Darrell"
      ],
      "year": "2015",
      "venue": "CVPR"
    },
    {
      "citation_id": "10",
      "title": "A new representation of skeleton sequences for 3d action recognition",
      "authors": [
        "Q Ke",
        "M Bennamoun",
        "S An",
        "F Sohel",
        "F Boussaid"
      ],
      "year": "2017",
      "venue": "CVPR"
    },
    {
      "citation_id": "11",
      "title": "Spatio-temporal lstm with trust gates for 3d human action recognition",
      "authors": [
        "J Liu",
        "A Shahroudy",
        "D Xu",
        "G Wang"
      ],
      "year": "2016",
      "venue": "European Conference on Computer Vision"
    },
    {
      "citation_id": "12",
      "title": "Spatial temporal graph convolutional networks for skeleton-based action recognition",
      "authors": [
        "S Yan",
        "Y Xiong",
        "D Lin"
      ],
      "year": "2018",
      "venue": "AAAI"
    },
    {
      "citation_id": "13",
      "title": "Action recognition with visual attention on skeleton images",
      "authors": [
        "Z Yang",
        "Y Li",
        "J Yang",
        "J Luo"
      ],
      "year": "2018",
      "venue": "2018 24th International Conference on Pattern Recognition (ICPR)"
    },
    {
      "citation_id": "14",
      "title": "Action recognition with spatiotemporal visual attention on skeleton image sequences",
      "authors": [
        "Z Yang",
        "Y Li",
        "J Yang",
        "J Luo"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Circuits and Systems for Video Technology"
    },
    {
      "citation_id": "15",
      "title": "Towards understanding action recognition",
      "authors": [
        "H Jhuang",
        "J Gall",
        "S Zuffi",
        "C Schmid",
        "M Black"
      ],
      "year": "2013",
      "venue": "CVPR"
    },
    {
      "citation_id": "16",
      "title": "P-cnn: Pose-based cnn features for action recognition",
      "authors": [
        "G Chéron",
        "I Laptev",
        "C Schmid"
      ],
      "year": "2015",
      "venue": "ICCV"
    },
    {
      "citation_id": "17",
      "title": "Towards accurate multi-person pose estimation in the wild",
      "authors": [
        "G Papandreou",
        "T Zhu",
        "N Kanazawa",
        "A Toshev",
        "J Tompson",
        "C Bregler",
        "K Murphy"
      ],
      "year": "2017",
      "venue": "CVPR"
    },
    {
      "citation_id": "18",
      "title": "Action recognition with jointspooled 3d deep convolutional descriptors",
      "authors": [
        "C Cao",
        "Y Zhang",
        "C Zhang",
        "H Lu"
      ],
      "year": "2016",
      "venue": "IJCAI"
    },
    {
      "citation_id": "19",
      "title": "Chained multistream networks exploiting pose, motion, and appearance for action classification and detection",
      "authors": [
        "M Zolfaghari",
        "G Oliveira",
        "N Sedaghat",
        "T Brox"
      ],
      "year": "2017",
      "venue": "Computer Vision (ICCV)"
    },
    {
      "citation_id": "20",
      "title": "Pose for action-action for pose",
      "authors": [
        "U Iqbal",
        "M Garbade",
        "J Gall"
      ],
      "year": "2017",
      "venue": "Automatic Face & Gesture Recognition"
    },
    {
      "citation_id": "21",
      "title": "Rpan: An end-to-end recurrent poseattention network for action recognition in videos",
      "authors": [
        "W Du",
        "Y Wang",
        "Y Qiao"
      ],
      "year": "2017",
      "venue": "CVPR"
    },
    {
      "citation_id": "22",
      "title": "The semaine database: Annotated multimodal records of emotionally colored conversations between a person and a limited agent",
      "authors": [
        "G Mckeown",
        "M Valstar",
        "R Cowie",
        "M Pantic",
        "M Schroder"
      ],
      "year": "2012",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "23",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "24",
      "title": "The enterface'05 audiovisual emotion database",
      "authors": [
        "O Martin",
        "I Kotsia",
        "B Macq",
        "I Pitas"
      ],
      "year": "2006",
      "venue": "Data Engineering Workshops, 2006. Proceedings. 22nd International Conference on"
    },
    {
      "citation_id": "25",
      "title": "Building a large scale dataset for image emotion recognition: The fine print and the benchmark",
      "authors": [
        "Q You",
        "J Luo",
        "H Jin",
        "J Yang"
      ],
      "year": "2016",
      "venue": "AAAI"
    },
    {
      "citation_id": "26",
      "title": "Learning multi-level deep representations for image emotion classification",
      "authors": [
        "T Rao",
        "M Xu",
        "D Xu"
      ],
      "year": "2016",
      "venue": "CoRR"
    },
    {
      "citation_id": "27",
      "title": "From hard to soft: Towards more human-like emotion recognition by modelling the perception uncertainty",
      "authors": [
        "J Han",
        "Z Zhang",
        "M Schmitt",
        "M Pantic",
        "B Schuller"
      ],
      "year": "2017",
      "venue": "Proceedings of the 2017 ACM on Multimedia Conference"
    },
    {
      "citation_id": "28",
      "title": "Learning visual emotion distributions via multi-modal features fusion",
      "authors": [
        "S Zhao",
        "G Ding",
        "Y Gao",
        "J Han"
      ],
      "year": "2017",
      "venue": "Proceedings of the 2017 ACM on Multimedia Conference"
    },
    {
      "citation_id": "29",
      "title": "Emotion in context: Deep semantic feature fusion for video emotion recognition",
      "authors": [
        "C Chen",
        "Z Wu",
        "Y.-G Jiang"
      ],
      "year": "2016",
      "venue": "Proceedings of the 2016 ACM on Multimedia Conference"
    },
    {
      "citation_id": "30",
      "title": "Affective image classification using features inspired by psychology and art theory",
      "authors": [
        "J Machajdik",
        "A Hanbury"
      ],
      "year": "2010",
      "venue": "Proceedings of the 18th ACM international conference on Multimedia"
    },
    {
      "citation_id": "31",
      "title": "Human-centered emotion recognition in animated gifs",
      "authors": [
        "Z Yang",
        "Y Zhang",
        "J Luo"
      ],
      "year": "2019",
      "venue": "2019 IEEE International Conference on Multimedia and Expo (ICME)"
    },
    {
      "citation_id": "32",
      "title": "Affective body expression perception and recognition: A survey",
      "authors": [
        "A Kleinsmith",
        "N Bianchi-Berthouze"
      ],
      "year": "2012",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "33",
      "title": "Body movements for affective expression: A survey of automatic recognition and generation",
      "authors": [
        "M Karg",
        "A.-A Samadani",
        "R Gorbet",
        "K Kühnlenz",
        "J Hoey",
        "D Kulić"
      ],
      "year": "2013",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "34",
      "title": "Towards unsupervised detection of affective body posture nuances",
      "authors": [
        "P De Silva",
        "A Kleinsmith",
        "N Bianchi-Berthouze"
      ],
      "year": "2005",
      "venue": "International Conference on Affective Computing and Intelligent Interaction"
    },
    {
      "citation_id": "35",
      "title": "Grounding affective dimensions into posture features",
      "authors": [
        "A Kleinsmith",
        "P De Silva",
        "N Bianchi-Berthouze"
      ],
      "year": "2005",
      "venue": "International Conference on Affective Computing and Intelligent Interaction"
    },
    {
      "citation_id": "36",
      "title": "User-level psychological stress detection from social media using deep neural network",
      "authors": [
        "H Lin",
        "J Jia",
        "Q Guo",
        "Y Xue",
        "Q Li",
        "J Huang",
        "L Cai",
        "L Feng"
      ],
      "year": "2014",
      "venue": "Proceedings of the 22nd ACM international conference on Multimedia"
    },
    {
      "citation_id": "37",
      "title": "Emotion recognition and depression diagnosis by acoustic and visual features: A multimodal approach",
      "authors": [
        "M Sidorov",
        "W Minker"
      ],
      "year": "2014",
      "venue": "Proceedings of the 4th International Workshop on Audio/Visual Emotion Challenge"
    },
    {
      "citation_id": "38",
      "title": "Temporal fusion approach using segment weight for affect recognition from body movements",
      "authors": [
        "J Xu",
        "S Sakazawa"
      ],
      "year": "2014",
      "venue": "Proceedings of the 22nd ACM international conference on Multimedia"
    },
    {
      "citation_id": "39",
      "title": "Visual inference of human emotion and behaviour",
      "authors": [
        "S Gong",
        "C Shan",
        "T Xiang"
      ],
      "year": "2007",
      "venue": "Proceedings of the 9th international conference on Multimodal interfaces"
    },
    {
      "citation_id": "40",
      "title": "Deep learning using linear support vector machines",
      "authors": [
        "Y Tang"
      ],
      "year": "2013",
      "venue": "CoRR"
    },
    {
      "citation_id": "41",
      "title": "Towards recognizing emotion with affective dimensions through body gestures",
      "authors": [
        "P De Silva",
        "M Osano",
        "A Marasinghe",
        "A Madurapperuma"
      ],
      "year": "2006",
      "venue": "Automatic Face and Gesture Recognition"
    },
    {
      "citation_id": "42",
      "title": "Bi-modal emotion recognition from expressive face and body gestures",
      "authors": [
        "H Gunes",
        "M Piccardi"
      ],
      "year": "2007",
      "venue": "Journal of Network and Computer Applications"
    },
    {
      "citation_id": "43",
      "title": "Beyond facial expressions: Learning human emotion from body gestures",
      "authors": [
        "C Shan",
        "S Gong",
        "P Mcowan"
      ],
      "year": "2007",
      "venue": "BMVC"
    },
    {
      "citation_id": "44",
      "title": "A bimodal face and body gesture database for automatic analysis of human nonverbal affective behavior",
      "authors": [
        "H Gunes",
        "M Piccardi"
      ],
      "year": "2006",
      "venue": "Pattern Recognition, 2006. ICPR 2006. 18th International Conference on"
    },
    {
      "citation_id": "45",
      "title": "Interpretable 3d human action analysis with temporal convolutional networks",
      "authors": [
        "T Kim",
        "A Reiter"
      ],
      "year": "2017",
      "venue": "2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)"
    },
    {
      "citation_id": "46",
      "title": "Modeling temporal dynamics and spatial configurations of actions using two-stream recurrent neural networks",
      "authors": [
        "H Wang",
        "L Wang"
      ],
      "year": "2017",
      "venue": "CVPR"
    },
    {
      "citation_id": "47",
      "title": "From actemes to action: A strongly-supervised representation for detailed action understanding",
      "authors": [
        "W Zhang",
        "M Zhu",
        "K Derpanis"
      ],
      "year": "2013",
      "venue": "Proceedings of the IEEE International Conference on Computer Vision"
    },
    {
      "citation_id": "48",
      "title": "Ntu rgb+ d: A large scale dataset for 3d human activity analysis",
      "authors": [
        "A Shahroudy",
        "J Liu",
        "T.-T Ng",
        "G Wang"
      ],
      "year": "2016",
      "venue": "Ntu rgb+ d: A large scale dataset for 3d human activity analysis"
    },
    {
      "citation_id": "49",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "K He",
        "X Zhang",
        "S Ren",
        "J Sun"
      ],
      "year": "2016",
      "venue": "CVPR"
    },
    {
      "citation_id": "50",
      "title": "RMPE: Regional multi-person pose estimation",
      "authors": [
        "H.-S Fang",
        "S Xie",
        "Y.-W Tai",
        "C Lu"
      ],
      "year": "2017",
      "venue": "ICCV"
    },
    {
      "citation_id": "51",
      "title": "The kinetics human action video dataset",
      "authors": [
        "W Kay",
        "J Carreira",
        "K Simonyan",
        "B Zhang",
        "C Hillier",
        "S Vijayanarasimhan",
        "F Viola",
        "T Green",
        "T Back",
        "P Natsev"
      ],
      "year": "2017",
      "venue": "The kinetics human action video dataset",
      "arxiv": "arXiv:1705.06950"
    },
    {
      "citation_id": "52",
      "title": "Convolutional neural networks for sentence classification",
      "authors": [
        "Y Kim"
      ],
      "year": "2014",
      "venue": "CoRR"
    },
    {
      "citation_id": "53",
      "title": "A convolutional neural network for modelling sentences",
      "authors": [
        "N Kalchbrenner",
        "E Grefenstette",
        "P Blunsom"
      ],
      "year": "2014",
      "venue": "CoRR"
    }
  ]
}