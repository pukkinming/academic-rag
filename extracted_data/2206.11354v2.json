{
  "paper_id": "2206.11354v2",
  "title": "Continual Learning For Affective Robotics: A Proof Of Concept For Wellbeing",
  "published": "2022-06-22T20:01:22Z",
  "authors": [
    "Nikhil Churamani",
    "Minja Axelsson",
    "Atahan Caldir",
    "Hatice Gunes"
  ],
  "keywords": [
    "Continual Learning",
    "Affective Robotics",
    "Wellbeing",
    "Facial Affect",
    "Human-Robot Interaction"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Sustaining real-world human-robot interactions requires robots to be sensitive to human behavioural idiosyncrasies and adapt their perception and behaviour models to cater to these individual preferences. For affective robots, this entails learning to adapt to individual affective behaviour to offer a personalised interaction experience to each individual. Continual Learning (CL) has been shown to enable real-time adaptation in agents, allowing them to learn with incrementally acquired data while preserving past knowledge. In this work, we present a novel framework for real-world application of CL for modelling personalised human-robot interactions using a CL-based affect perception mechanism. To evaluate the proposed framework, we undertake a proof-of-concept user study with 20 participants interacting with the Pepper robot using three variants of interaction behaviour: static and scripted, using affect-based adaptation without personalisation, and using affect-based adaptation with continual personalisation. Our results demonstrate a clear preference in the participants for CL-based continual personalisation with significant improvements observed in the robot's anthropomorphism, animacy and likeability ratings as well as the interactions being rated significantly higher for warmth and comfort as the robot is rated as significantly better at understanding how the participants feel.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction And Related Work",
      "text": "Social and affective robots are designed to interact in human-centred environments, learning to sustain closed-loop interactions with humans. Acting as assistants  [24] , tutors or coaches  [6, 18] , and companions that offer personalised conversational capabilities  [37] , they need to sense and understand human behaviour and learn to support them with context-appropriate social interactions, aiding and even fostering their cognitive and socio-emotional wellbeing  [15] . This entails socio-emotional adaptability in affective robots that may allow them to not only adapt to real-world interactions with individuals but also provide them personalised interaction experiences by adapting to individual context and behaviour  [1, 10, 37] . Developing sensitivity towards specific verbal and non-verbal human behaviours  [9, 12]  while learning to respond dynamically and appropriately enables social robots to offer personalised and naturalistic human-robot interactions  [17] .\n\nHumans use affect to convey meaning and intent in conversations using several outward signals  [19]  such as facial expressions  [27] , body gestures  [34]  or speech intonations  [39] . For affective robots, it is essential to interpret such affective signalling to gather a contextual understanding of their interactions with humans. This may enable them to adapt their own behaviours during interactions to appropriately reflect the users' affective state, engaging them in meaningful conversations  [13, 17] . However, this may not be easy in all situations owing to the unpredictability of human-centred environments resulting from changing environmental conditions and interaction contexts and the inherent uncertain and subjective nature of human affective expression  [16] .\n\nAffective Robotics research explores how embedding robots with an understanding of human affective behaviour may allow them to facilitate meaningful interactions that enhance an individual's experience with the robot  [21] . Yet, most approaches make use of off-the-shelf affect perception models that, despite providing state-of-the-art results on benchmarks, are not able to adapt to the dynamics of real-time interactions. Any adaptation in the model requires a large amount of computational resources as they need to be trained, from scratch in most cases, with large amounts of data. This may not be feasible, especially when applied on resource-constrained devices such as robots in real-world situations  [12, 41] .\n\nContinual Learning (CL)  [26, 36]  for robots focuses, instead, on adapting to changing contexts and shifts in the inherent data distributions. It aims to instil long-term adaptability in agents integrating novel information, acquired incrementally by the agent as it interacts with the environment, while balancing past knowledge  [26] . For affective robots, this entails adapting and personalising to individual socio-emotional behaviours over repeated interactions with the users, learning to adapt their own behaviours in the process. The desiderata for continually adapting affective robots thus becomes to model personalisation capabilities that allow robots to be sensitive to individual differences in affective behaviours  [10, 2]  and enable them to model context-appropriate behaviours during interactions  [12, 20, 32] .\n\nIn our previous work  [12] , we provide a theoretical framework for CL for affective robotics, proposing two specific components of such interactions; personalised affect perception and context-appropriate behavioural learning, that allow robots to continually learn and adapt in real-world situations. Enabling personalised affect perception requires robots to adapt their perception towards individual users, accounting for individual differences in expression and other characteristic attributes  [10] . Several approaches achieve this by considering contextual attributes such as gender and culture  [37]  to learn person-specific feature representations  [8, 37] , or learning individualised affective memories  [2]  that encode the robot's affective experience interacting with an individual, or simultaneously learning personalised (forming an episodic memory) and generalised (forming a semantic memory) representations  [10]  that may allow robots to adapt towards each individual while preserving a generalised understanding of human affective expression across individuals and contexts. Furthermore, using such personalised models to encode the users' affective state as a contextual affordance, robots may dynamically model and adapt their interactions with the users instead of following the same static and scripted interaction for each user  [11, 20] . Such personalised interactions are expected to enhance the users' experience interacting with the robot, offering a naturalistic interaction experience.\n\nExtending our theoretical formulations presented in  [12] , in this work, we present a practical framework for realworld application of continual learning for enabling personalised Human-Robot Interaction (HRI) for affective robots. The presented framework enables a multi-modal interaction, with speech recognition capabilities to parse user responses, and a CL-based facial affect perception model  [10]  that personalises its learning towards each individual's expressions. A dialogue manager allows to structure the interactions into different interaction states based on the interaction context. The robot uses its evaluation of participants' affective behaviour to model state-transitions, personalising interactions to individual preferences and employs Natural Language Generation (NLG) to generate naturalistic responses towards the participants. To evaluate the contribution of CL-based personalisation towards how the participants' impressions of the robot, we implement another version of the framework, for comparison, switching the CL-based model with the FaceChannel  [3, 4] , an off-the-shelf state-of-the-art facial affect perception model that still encodes the participants' facial affect, albeit without any personalisation. Furthermore, a control condition is implemented by completely 'switching off' affective adaptation and always following a static and scripted interaction flow, ignoring the participants' affective responses. For the proof-of-concept user study, 20 participants interact with three versions of the Pepper robot, in a between-subjects design; (i) following a static and non-adaptive interaction script, (ii) using affective adaptation without personalisation, or (iii) using CL-based affective adaptation with personalisation. Our results demonstrate that using CL-based personalisation improves participants' impressions of the robot for anthropomorphism, animacy and likeability while offering warm and comfortable interactions by being sensitive to the participants feelings.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "The Interaction Scenario",
      "text": "Enabling personalised interaction capabilities in affective robots can be beneficial for HRI across a variety of interaction contexts. In particular, we explore sensing and adapting to human affective behaviour in a one-off Positive Psychology (PP)-based  [14, 30]  interaction session where the Pepper Robot 2  requests participants to reflect upon recent events in their lives that may invoke positive feelings in them.\n\nWe develop a script for the interaction scenario in collaboration with a professional psychologist to mitigate situations or utterances from the robot that may risk invalidating or patronising the participants. This also allows us to improve the interaction, clarifying prompts for participants to share their experiences, and determine the appropriate amount to select appropriate robot responses using empathetic utterances  [25]  that are reflective of the participants' affective state. Each interaction with the participants consists of three exercises or tasks:\n\n1. The robot asks participants to talk about 2 impactful things or events in their lives from the past two weeks, why these events may have happened and how the events made them feel.\n\n2. Focusing on developing gratitude to increase positive affect and subjective happiness, the robot asks the participant to recall 2 things that they felt grateful for in the recent past.\n\n3. The robot then asks the participants to describe 2 recent accomplishments, the strengths applied to accomplish these, and how these made them feel.\n\nAfter the interaction, the robot asks participants for verbal and survey-based feedback using a tablet.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "The Proposed Framework",
      "text": "To facilitate interactions with Pepper, we implement a modular framework that enables multi-modal interaction capabilities in the robot (see Fig.  1 ). This allows us to selectively employ different functionalities as needed to implement different variations in robot behaviours. All modules are implemented on Pepper as Robot Operating System (ROS)  3 modules communicating with each other.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Face Detection And Obtaining Ground Truth",
      "text": "We use Pepper's on-board RGB-camera (from the forehead) to record participants' behaviour at 30 FPS with a resolution of 640 × 480. OpenCV Face Detection 4  is used to detect and crop-out the participants' face from the recorded",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Continual Learning For Personalised Affect Perception",
      "text": "For personalised affect perception we use the CLIFER framework  [10] , initially evaluated only on benchmark datasets, by extending its imagination model for dimensional facial expression editing  [28]  and applying it in the context of HRI. CLIFER implements a Growing Dual-Memory (GDM)-based architecture  [35]  that allows for incrementally personalising the robot's perception model towards an individual. It consists of an episodic (GDM-E) and a semantic (GDM-S) memory, both of which use a Growing When Required (GWR) neural network with Gamma-filtering  [35]  for a semisupervised processing of facial features for affect perception  [10] . While the episodic memory learns instance-level variations, personalising to the participants' current expressions during the interactions, the semantic memory learns concept-level abstractions for long-term retention of knowledge over repeated interactions with a participant. Imagination in CLIFER allows for the simulation of additional facial images for a participant by conditionally translating their face to represent different affective annotations using a Conditional Adversarial Auto-Encoder (CAAE)-based adversarial learning model  [10, 28] . These generated images augment learning in the dual-memory GDM model that personalises towards each individuals' expressions.\n\nDuring the interactions, from each response by the participant, we randomly sample 10 FaceChannel-annotated images of the participant, and for each of these images, CLIFER imagines or simulates 49 additional facial images (see Fig.  2 ), corresponding to arousal and valence values ranging from (-0.75 to 0.75) (values capped at ||0.75|| to avoid extremes). The additional 10 × 49 = 490 images, along with the original 10 images, are then passed through the encoder to generate feature representations that are used to update the GDM-based learning model. As the robot interacts with the participant, it is able to simulate imagined contact  [42]  over a wide-range of affective contexts, personalising towards their expressions. For real-time affect perception, we use the episodic memory representations to summarise the arousal-valence predictions over the last 5 seconds (5 × 30 = 150 frames) of each dialogue and publish them as ROS parameters to be used by the dialogue manager to modulate the interaction flow.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Dialogue Management",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Interaction State Manager",
      "text": "We model a bi-directional interaction where Pepper listens to the participants' and responds appropriately by generating naturalistic and congruous responses. For this, we implement a Finite-state Machine (FSM)-based dialogue manager using the SMACH  5  ROS library. The interactions consist of 7 different dialogue states, starting with Introduction (S 1 ) followed by the three interaction states tagged as Impactful (S 2 ), Grateful (S 3 ) and Accomplishments (S 4 ) (see Section 2). After the three interactions, the participants provide a verbal Feedback (S 5 ), fill out the Survey (S 6 ) questionnaires and finally end the session by saying GoodBye (S 7 ).\n\nUnder each of the interaction states (S 2 , S 3 and S 4 ), participants respond to open-ended descriptive questions about their recent experiences. The robot encodes their facial affect using arousal-valence predictions from the CLIFER (or the FaceChannel, depending on the condition) model and divides the dialogue states further, mapping them to the four quadrants (Q 1 : positive valence & positive arousal, Q 2 : negative valence & positive arousal, Q 3 : negative valence & negative arousal, and Q 4 : positive valence & negative arousal) of the Circumplex Model of affect  [38] , to determine robot responses. An additional Neutral dialogue-state is mapped for arousal-valence values ∈ [-0.10, 0.10]. These sub-states extend the dialogue with the robot adapting its responses based on the participants' affective behaviour. State transitions are adapted to generate appropriate robot responses by traversing these sub-states for each interaction state. For instance, if a participant expresses positive arousal and valence (Q 1 ) while talking about impactful events (S 2 ), the robot utters an additional positive response such as, \"That sounds great, I'm happy for you.\" to express its understanding of their affective state, before returning to the interaction script (S2 Q1 --→ S3). For the static and scripted interactions, such affective dialogue is not held and the robot strictly follows the interaction script, transiting from one interaction state to the other directly (S2 -→ S3).",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Speech Recognition",
      "text": "To capture participant responses during the interactions, we use the Python Speech Recognition Library 6  , built on top of Google ASR. For the 'Yes/No' questions, we implement keyword-spotting to capture variations of affirmative (yes, yeah, yep, OK, fine, aye, definitely, certainly, exactly, of course, positive, sure) and negative (no, nope, na, never, nah, nay) user responses. These responses are used by the state manager to model state-transitions between dialogue states (S 1 -S 7 ). Other user responses to the open-ended descriptive questions are also captured and logged but do not affect the interaction flow.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Natural Language Generation (Nlg)",
      "text": "For each interaction state in the dialogue manager, pre-defined sentence dictionaries are used by the NLG module to generate corresponding robot responses. The dialogue manager is tasked with determining the interaction state (and the affective sub-state, depending on the condition) and selecting the sentence to be uttered by the robot. A total of 120+ robot responses are scripted, split into several sentence dictionaries, each mapped to a given dialogue state (see Fig.  1  for some examples). Pepper's in-built Text-to-Speech (TTS) Python library  7  is used to generate robot responses.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Gesture Modelling",
      "text": "During the interactions, Pepper is designed to generate certain upper-body gestures by manipulating its joints (head, shoulders, elbows, wrists and hands) to make the interactions seem more naturalistic  [29, 33] . These gestures are performed as the robot welcomes the participants, asks a question, responds to their affective expressions (only for the adaptive interactions), and at the end of the experiment to say goodbye. All gestures (see Fig.  3  for examples) are pre-defined in Choregraphe 8  after consulting the psychologist to mitigate negative feelings in the participants.\n\n4 Proof-of-Concept: User Study",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Set-Up",
      "text": "The participants are sat in front of Pepper with a low table separating the two, in order to keep the participants' personal space (see Fig.  4 ). Two cameras capture the interaction, placed facing the participant and Pepper, respectively, while Pepper's on-board RGB camera is used to capture the participants' facial affect. An external microphone is placed on the table in front of the participant to accurately record their voice. Pepper's on-board speakers are used to communicate with the participants. Additionally, a tablet is placed on the table for the participants to fill out the survey questionnaires.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Participants",
      "text": "A total of 22 participants were recruited for the user study amongst the students and members of the university. However, some technical issues during the interaction led to the data from 2 participants being dropped from the analyses. Thus, the evaluations presented here consist of data from N = 20 participants (12 female, 5 male, 3 not disclosed) aged 26.70 ± 3.68 years from 12 different nationalities. The majority of the participants (N = 13) indicated little to no prior experience with humanoid robots. As the interaction was modelled around wellbeing and PP exercises, only a non-clinical population was sought making sure that none of the participants were undertaking any mental health treatment or medication. All participants were asked to fill in the General Anxiety Disorder (GAD7)  [40]  and Participant Health Questionnaire (PHQ9)  [22]  before they were enrolled in the experiments to make sure no participants were experiencing high anxiety or depression. This screening resulted in several participants not being able to participate in the study creating an imbalance in the gender distribution as well as the condition groups. Before the study, participants were also asked to watch 2 videos of Pepper showing its interaction capabilities, in order to familiarise them with the robot. All participants provided informed consent for their participation and the usage of their data for post-study analyses. The participants were compensated in the form of Amazon vouchers. The consent form, study-design and the experimental protocol was approved by the Departmental Ethics Committee.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Experiment Conditions",
      "text": "Following a between-subjects design, each participant is randomly allocated to one of the three conditions:\n\nC1 -Static and Scripted Interaction: In this condition, the interaction between the participant and the robot follows the pre-defined script where the robot's responses do not take into account the participants' affective responses and the robot always responds in an anodyne manner. A total of 5 participants were allocated to this condition.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "C2 -Affect-Based Adaptation Without Personalisation:",
      "text": "To evaluate if embedding adaptation in the robot influences its interactions with the participants, in this condition, we employ the FaceChannel affect perception model, to determine participants' affective expressions. Starting from the same initial state in the dialogue manager as C1, the affect perception outcome is used to modulate robot responses in the interaction states (S 2 , S 3 and S 4 ) to adapt to the interaction. A total of 9 participants were allocated to this condition.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "C3 -Affect-Based Adaptation With Continual Personalisation:",
      "text": "In this condition, instead of the pre-trained FaceChannel model, a CL-based CLIFER model is employed to determine the affective state expressed by the participant during the interactions (see Section 3.2 for details). As the interaction progresses, the model continually personalises towards the individual participant's facial expressions. This personalised affect prediction is used to modulate the robot's responses towards each participant. A total of 6 participants were allocated to this condition.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Questionnaires",
      "text": "To evaluate interactions with the robot, the participants completed a three-part questionnaire consisting of questions from the GODSPEED  [5]  (measuring robot anthropomorphism, animacy, likeability, perceived intelligence and perceived safety) and RoSAS  [7]  (measuring people's perceptions of the robot for warmth, competence, and (dis) comfort) questionnaires, along with customised questions on whether Pepper understood what they said or how they felt and adapted its behaviour accordingly.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Results",
      "text": "To evaluate whether affective adaptation with continual personalisation enhances the participants' interactions with Pepper, we examine how they rate the robot during the interactions, compared to affective adaptation without personalisation as well as the control condition following a static interaction script. Following a between-subjects design, we compare the participants' evaluation of the robot under the three conditions. First, the three conditions are compared using the Kruskal-Wallis H Test  [23]  to measure significant differences between the three conditions in terms of the participants' ratings for the robot. For the the dimensions with significant differences, as a post-hoc analysis, pair-wise non-parametric one-tailed Mann-Whitney U Test  [31]  is conducted comparing C1 vs. C2, C2 vs. C3, and C1 vs. C3. Bonferroni correction is applied to all the statistical comparisons conducted. Here we present the main outcomes of these evaluations:",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Godspeed",
      "text": "Our results show that both C2 and C3 are, on average, rated higher than C1 across all the GODSPEED evaluations (see Fig.  5 ) indicating a clear preference with the participants towards an adaptive Pepper that is sensitive to their affective expressions. Furthermore, we see that continual personalisation (C3) is rated significantly higher than the static interaction (C1) for the anthropomorphism (U = 0.5, p = 0.004), animacy (U = 1.0, p = 0.006), and likeability (U = 2.0, p = 0.011) of the robot. Investigating the underlying dimensions for GODSPEED, C3 is rated significantly higher than C1 on conscious (U = 2.5, p = 0.007), pleasant (U = 2.0, p = 0.005) and nice (U = 2.0, p = 0.005) dimensions while also being rated higher than C2 on the pleasant (U = 5.5, p = 0.004) and nice (U = 8.5, p = 0.012) dimensions. C2, on the other hand, is also rated higher than C1 on the conscious (U = 5.0, p = 0.008) and responsive (U = 5.5, p = 0.011) dimensions.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Rosas",
      "text": "Similar results are witnessed on the RoSAS evaluations as well, where C2 and C3 are rated better than C1 across all evaluations. Here too C3 performs the best being rated significantly higher than C1 for warmth ratings (U = 2.0, p = 0.010) and significantly lower for discomfort ratings (U = 29.0, p = 0.006). C2, on the other hand, is rated significantly higher for warmth ratings (U =, 4.5, p = 0.009) than C1. A deeper look into the underlying dimensions for RoSAS tells a similar story as the GODSPEED evaluations as C3 is consistently rated higher, on average, than both C1 and C2. It is rated significant less awkward (U = 30.0, p = 0.003) and more feeling (U = 2.0, p = 0.008) than C1. On the other hand, C2 is rated significantly more responsive (U = 5.5, p = 0.011) and more feeling (U = 5.5, p = 0.008) than C1.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Customised Questions",
      "text": "Customised questions are used to measure specific aspects of the interactions with Pepper such as its ability to understand what the user said, how they felt, and whether it adapted towards the participant. Pepper under C3 is rated higher than C2 and C1 across all dimensions with a significant difference witnessed in terms of understanding how the participants felt (U = 1.0, p = 0.005), during the interactions. C2 is also rated significantly higher than C1 in understanding how the participants felt (U = 5.5, p = 0.009). These results underscore the robot's ability, under C2 and C3 to be sensitive towards participants' affective behaviour during the interactions.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Conclusion",
      "text": "In this work, we design and implement a novel CL-based framework on a physical robot to enable continual personalisation capabilities, and undertake a proof-of-concept study for wellbeing coaching. To the best of our knowledge, this is the first study evaluating how continual personalisation capabilities in affective robots can improve participants' evaluation of them, improving their experiences with the robots.\n\nOur results from the proof-of-concept user study show that a robot that adapts its behaviour by taking into account the affective behaviour of the participants (C2 and C3) is preferred, on average, over a static, non-adaptive (C1) one. Furthermore, embedding continual personalisation capabilities in the robot (C3) significantly improves participants' impressions of the robot across several dimensions such as anthropomorphism, animacy, likeability, warmth and comfort and improves its perceived understanding of the participants' affective experience during the interactions. For PP-oriented wellbeing coaching, these are promising results as the expectation from such robots would be to offer positive experiences to the participants. Offering warm and comfortable interactions where the robot empathises with how the participants feel allows for them to open up to the coaching provided, especially with the enhanced likeability of the robot. This is evidenced in the reduced awkwardness during the interactions with C3 being rated significantly higher in terms of being pleasant, nice and feeling towards the participants. Starting from a generalised understanding of human facial affect, CL-based personalisation towards each individual user ensures that the robot is able to accurately sense and understand their affective state during the interactions, enabling a dynamic and robust interaction with the participants, essential for wellbeing coaching.\n\nEven though adapting the interactions based on the participants' affective responses under both C2 and C3 offers improvements over making the robot always follow a static script, using CL to personalise the robots' affect perception towards each participant makes these improvements significant. This is witnessed across evaluations where C3, on average, achieves the highest ratings. Thus, continual personalisation may become an important capability for robots interacting with individuals, especially under wellbeing scenarios, where the robot is expected to be sensitive to their affective behaviour and adapt its behaviour accordingly. Such adaptation can have a positive impact on the interactions with the robot 'empathising'  [25]  with the participants and adapting the interaction based on their affective behaviour.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Limitations And Future Work",
      "text": "This proof-of-concept evaluation constitutes preliminary results from a larger study aimed at investigating interactions with personalisable affective robots over repeated interactions. Here, we only investigate a one-off interaction to determine whether CL-based personalisation improves the participants' interaction experience under wellbeing settings. Even though our evaluations provide promising results in favour of continual personalisation, further analyses will focus on evaluating longitudinal interactions over multiple sessions to determine whether these effects hold under long-term HRI settings.\n\nThe screening of participants to make sure we were working with only a non-clinical population meant that several potential participants needed to be dropped from the user-study, leading to an imbalance in the gender distribution of the participants with more females participating in the study. Furthermore, technical issues during the experiments meant the data from some participants needed to be excluded from the analyses. This led to further imbalance in the distribution of the populations assigned randomly to each condition. Future work would also focus working on balancing these distributions for the participants for the longitudinal experiments.\n\nFurthermore, we use a uni-modal affect perception for the robot evaluating only their facial expressions. This may not be sufficient as participants' verbal responses and their body gestures may also include important information about their affective responses during the interaction. Thus, future work should explore multi-modal affect perception to better evaluate the participants' affective responses. Furthermore, integrating more advanced Natural Language Processing (NLP) capabilities will strengthen active listening capabilities for the robot, improving its contextual understanding of the interaction.",
      "page_start": 9,
      "page_end": 10
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The Proposed Framework: Continual Learning for Personalised Human-Robot Interactions.",
      "page": 3
    },
    {
      "caption": "Figure 1: ). This allows us to selectively employ different functionalities as needed to implement",
      "page": 3
    },
    {
      "caption": "Figure 2: Adapting CLIFER [10] for Imagining participant faces for a range of arousal-valence values.",
      "page": 4
    },
    {
      "caption": "Figure 2: ), corresponding to arousal and valence values ranging from (−0.75 to 0.75) (values capped at ||0.75|| to avoid",
      "page": 4
    },
    {
      "caption": "Figure 3: Pepper displaying gestures during the interactions.",
      "page": 5
    },
    {
      "caption": "Figure 1: for some examples). Pepper’s in-built Text-to-Speech (TTS) Python library7 is used to generate robot responses.",
      "page": 5
    },
    {
      "caption": "Figure 4: Setup: Pepper interacting with the Participant.",
      "page": 6
    },
    {
      "caption": "Figure 3: for examples) are",
      "page": 6
    },
    {
      "caption": "Figure 4: ). Two cameras capture the interaction, placed facing the participant and Pepper, respectively,",
      "page": 6
    },
    {
      "caption": "Figure 5: GODSPEED [5] Scores for C1, C2, and C3 conditions. ∗represents p < 0.05 and ∗∗represents p < 0.01.",
      "page": 7
    },
    {
      "caption": "Figure 6: RoSAS [7] Scores for C1, C2, and C3 conditions. ∗represents p < 0.05 and ∗∗represents p < 0.01.",
      "page": 8
    },
    {
      "caption": "Figure 5: ) indicating a clear preference with the participants towards an adaptive Pepper that is sensitive to their",
      "page": 8
    },
    {
      "caption": "Figure 7: Customised Questions scores under C1, C2, and C3 conditions. ∗∗represents p < 0.01.",
      "page": 9
    }
  ],
  "tables": [],
  "citations": [
    {
      "citation_id": "1",
      "title": "What Am I Allowed to Do Here?: Online Learning of Context-Specific Norms by Pepper",
      "authors": [
        "Ali Ayub",
        "Alan Wagner"
      ],
      "year": "2020",
      "venue": "Social Robotics ICSR 2020"
    },
    {
      "citation_id": "2",
      "title": "A Personalized Affective Memory Model for Improving Emotion Recognition",
      "authors": [
        "Pablo Barros",
        "German Parisi",
        "Stefan Wermter"
      ],
      "year": "2019",
      "venue": "Proceedings of the 36th International Conference on Machine Learning"
    },
    {
      "citation_id": "3",
      "title": "The FaceChannel: A fast and furious deep neural network for facial expression recognition",
      "authors": [
        "Pablo Barros",
        "Nikhil Churamani",
        "Alessandra Sciutti"
      ],
      "year": "2020",
      "venue": "SN Computer Science"
    },
    {
      "citation_id": "4",
      "title": "The FaceChannel: A Light-weight Deep Neural Network for Facial Expression Recognition",
      "authors": [
        "Pablo Barros",
        "Nikhil Churamani",
        "Alessandra Sciutti"
      ],
      "year": "2020",
      "venue": "Proceedings of the 15th International Conference on Automatic Face and Gesture Recognition (FG)"
    },
    {
      "citation_id": "5",
      "title": "Measurement instruments for the anthropomorphism, animacy, likeability, perceived intelligence, and perceived safety of robots",
      "authors": [
        "Christoph Bartneck",
        "Dana Kulić",
        "Elizabeth Croft",
        "Susana Zoghbi"
      ],
      "year": "2009",
      "venue": "International Journal of Social Robotics"
    },
    {
      "citation_id": "6",
      "title": "Teleoperated Robot Coaching for Mindfulness Training: A Longitudinal Study",
      "authors": [
        "P Indu",
        "Nikhil Bodala",
        "Hatice Churamani",
        "Gunes"
      ],
      "year": "2021",
      "venue": "30th IEEE International Conference on Robot and Human Interactive Communication"
    },
    {
      "citation_id": "7",
      "title": "The robotic social attributes scale (rosas) development and validation",
      "authors": [
        "Colleen Carpinella",
        "Alisa Wyman",
        "Michael Perez",
        "Steven Stroessner"
      ],
      "year": "2017",
      "venue": "Proceedings of the ACM/IEEE International Conference on Human-Robot Interaction"
    },
    {
      "citation_id": "8",
      "title": "Selective Transfer Machine for Personalized Facial Expression Analysis",
      "authors": [
        "W Chu",
        "F Torre",
        "J Cohn"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "9",
      "title": "Continual Learning for Affective Computing",
      "authors": [
        "Nikhil Churamani"
      ],
      "year": "2020",
      "venue": "Doctoral Consortium Proceedings of the 15th International Conference on Automatic Face and Gesture Recognition (FG)",
      "arxiv": "arXiv:2006.06113"
    },
    {
      "citation_id": "10",
      "title": "CLIFER: Continual Learning with Imagination for Facial Expression Recognition",
      "authors": [
        "Nikhil Churamani",
        "Hatice Gunes"
      ],
      "year": "2020",
      "venue": "Proceedings of the 15th IEEE International Conference on Automatic Face and Gesture Recognition (FG)"
    },
    {
      "citation_id": "11",
      "title": "The impact of personalisation on human-robot interaction in learning scenarios",
      "authors": [
        "Nikhil Churamani",
        "Paul Anton",
        "Marc Brügger",
        "Erik Fließ Wasser",
        "Thomas Hummel",
        "Julius Mayer",
        "Waleed Mustafa",
        "Geok Hwei",
        "Thi Ng",
        "Chi Linh",
        "Quan Nguyen",
        "Marcus Nguyen",
        "Sebastian Soll",
        "Sascha Springenberg",
        "Stefan Griffiths",
        "Nicolás Heinrich",
        "Erik Navarro-Guerrero",
        "Johannes Strahl",
        "Cornelius Twiefel",
        "Stefan Weber",
        "Wermter"
      ],
      "year": "2017",
      "venue": "Proceedings of the 5th International Conference on Human Agent Interaction"
    },
    {
      "citation_id": "12",
      "title": "Continual learning for affective robotics: Why, what and how?",
      "authors": [
        "Nikhil Churamani",
        "Sinan Kalkan",
        "Hatice Gunes"
      ],
      "year": "2020",
      "venue": "29th IEEE International Conference on Robot and Human Interactive Communication"
    },
    {
      "citation_id": "13",
      "title": "Affect-driven learning of robot behaviour for collaborative human-robot interactions",
      "authors": [
        "Nikhil Churamani",
        "Pablo Barros",
        "Hatice Gunes",
        "Stefan Wermter"
      ],
      "year": "2022",
      "venue": "Frontiers in Robotics and AI"
    },
    {
      "citation_id": "14",
      "title": "Positive psychology and gratitude interventions: A randomized clinical trial",
      "authors": [
        "Lúzie Fofonka Cunha",
        "Lucia Pellanda",
        "Caroline Tozzi"
      ],
      "year": "2019",
      "venue": "Frontiers in Psychology"
    },
    {
      "citation_id": "15",
      "title": "Robots we like to live with?! -a developmental perspective on a personalized, life-long robot companion",
      "authors": [
        "Kerstin Dautenhahn"
      ],
      "year": "2004",
      "venue": "IEEE International Workshop on Robot and Human Interactive Communication"
    },
    {
      "citation_id": "16",
      "title": "Long-Term Cohabitation with a Social Robot: A Case Study of the Influence of Human Attachment Patterns",
      "authors": [
        "Michał Dziergwa",
        "Mirela Kaczmarek",
        "Paweł Kaczmarek",
        "Jan Kedzierski",
        "Karolina Wadas-Szydłowska"
      ],
      "year": "2018",
      "venue": "International Journal of Social Robotics"
    },
    {
      "citation_id": "17",
      "title": "Promoting Interactions Between Humans and Robots Using Robotic Emotional Behavior",
      "authors": [
        "Maurizio Ficocelli",
        "Junichi Terao",
        "Goldie Nejat"
      ],
      "year": "2016",
      "venue": "IEEE Transactions on Cybernetics"
    },
    {
      "citation_id": "18",
      "title": "Exercise with social robots: Companion or coach?",
      "authors": [
        "Sascha Griffiths",
        "Tayfun Alpay",
        "Alexander Sutherland",
        "Matthias Kerzel",
        "Manfred Eppe",
        "Erik Strahl",
        "Stefan Wermter"
      ],
      "year": "2018",
      "venue": "Proceedings of Workshop on Personal Robots for Exercising and Coaching at the ACM/IEEE Interanational Conference on Human-Robot Interaction"
    },
    {
      "citation_id": "19",
      "title": "Emotion representation, analysis and synthesis in continuous space: A survey",
      "authors": [
        "Hatice Gunes",
        "Björn Schuller",
        "Maja Pantic",
        "Roddy Cowie"
      ],
      "year": "2011",
      "venue": "2011 IEEE International Conference on Automatic Face and Gesture Recognition (FG)"
    },
    {
      "citation_id": "20",
      "title": "Towards adaptive social behavior generation for assistive robots using reinforcement learning",
      "authors": [
        "Jacqueline Hemminghaus",
        "Stefan Kopp"
      ],
      "year": "2017",
      "venue": "Proceedings of the 2017 ACM/IEEE International Conference on Human-Robot Interaction"
    },
    {
      "citation_id": "21",
      "title": "Affective social robots",
      "authors": [
        "Rachel Kirby",
        "Jodi Forlizzi",
        "Reid Simmons"
      ],
      "year": "2010",
      "venue": "Robotics and Autonomous Systems"
    },
    {
      "citation_id": "22",
      "title": "The PHQ-9: validity of a brief depression severity measure",
      "authors": [
        "Kurt Kroenke",
        "Robert Spitzer",
        "Janet Bw Williams"
      ],
      "year": "2001",
      "venue": "Journal of General Internal Medicine"
    },
    {
      "citation_id": "23",
      "title": "Use of ranks in one-criterion variance analysis",
      "authors": [
        "William Kruskal",
        "W Wallis"
      ],
      "year": "1952",
      "venue": "Journal of the American Statistical Association"
    },
    {
      "citation_id": "24",
      "title": "Social robots for long-term interaction: A survey",
      "authors": [
        "Iolanda Leite",
        "Carlos Martinho",
        "Ana Paiva"
      ],
      "year": "2013",
      "venue": "International Journal of Social Robotics"
    },
    {
      "citation_id": "25",
      "title": "The influence of empathy in human-robot relations",
      "authors": [
        "Iolanda Leite",
        "André Pereira",
        "Samuel Mascarenhas",
        "Carlos Martinho",
        "Rui Prada",
        "Ana Paiva"
      ],
      "year": "2013",
      "venue": "International Journal of Human-Computer Studies"
    },
    {
      "citation_id": "26",
      "title": "Continual learning for robotics: Definition, framework, learning strategies, opportunities and challenges",
      "authors": [
        "Timothée Lesort",
        "Vincenzo Lomonaco",
        "Andrei Stoian",
        "Davide Maltoni",
        "David Filliat",
        "Natalia Díaz-Rodríguez"
      ],
      "year": "2020",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "27",
      "title": "Deep facial expression recognition: A survey",
      "authors": [
        "S Li",
        "W Deng"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "28",
      "title": "Facial Expression Editing with Continuous Emotion Labels",
      "authors": [
        "A Lindt",
        "P Barros",
        "H Siqueira",
        "S Wermter"
      ],
      "year": "2019",
      "venue": "Proceedings of the 14th IEEE International Conference on Automatic Face and Gesture Recognition (FG)"
    },
    {
      "citation_id": "29",
      "title": "Enriching the Human-Robot Interaction Loop with Natural",
      "authors": [
        "Katrin Solveig Lohan",
        "Hagen Lehmann",
        "Christian Dondrup",
        "Frank Broz",
        "Hatice Kose"
      ],
      "year": "2016",
      "venue": "Semantic, and Symbolic Gestures"
    },
    {
      "citation_id": "30",
      "title": "Positive psychology: The scientific and practical explorations of human strengths",
      "authors": [
        "J Shane",
        "Jennifer Lopez",
        "Charles Pedrotti",
        "Snyder"
      ],
      "year": "2018",
      "venue": "Positive psychology: The scientific and practical explorations of human strengths"
    },
    {
      "citation_id": "31",
      "title": "On a test of whether one of two random variables is stochastically larger than the other",
      "authors": [
        "H B Mann",
        "D R Whitney"
      ],
      "year": "1947",
      "venue": "The Annals of Mathematical Statistics"
    },
    {
      "citation_id": "32",
      "title": "Learning Socially Appropriate Robo-Waiter Behaviours through Real-Time User Feedback",
      "authors": [
        "Emily Mcquillin",
        "Nikhil Churamani",
        "Hatice Gunes"
      ],
      "year": "2022",
      "venue": "Proceedings of the 2022 ACM/IEEE International Conference on Human-Robot Interaction (HRI)"
    },
    {
      "citation_id": "33",
      "title": "Embodiment, Situatedness, and Morphology for Humanoid Robots Interacting with People",
      "authors": [
        "Blanca Miller",
        "David Feil-Seifer"
      ],
      "year": "2016",
      "venue": "Embodiment, Situatedness, and Morphology for Humanoid Robots Interacting with People"
    },
    {
      "citation_id": "34",
      "title": "Survey on emotional body gesture recognition",
      "authors": [
        "F Noroozi",
        "C Corneanu",
        "D Kaminska",
        "T Sapinski",
        "S Escalera",
        "G Anbarjafari"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "35",
      "title": "Lifelong Learning of Spatiotemporal Representations With Dual-Memory Recurrent Self-Organization",
      "authors": [
        "German Parisi",
        "Jun Tani",
        "Cornelius Weber",
        "Stefan Wermter"
      ],
      "year": "2018",
      "venue": "Frontiers in Neurorobotics"
    },
    {
      "citation_id": "36",
      "title": "Continual Lifelong Learning with Neural Networks: A review",
      "authors": [
        "G Parisi",
        "R Kemker",
        "J Part",
        "C Kanan",
        "S Wermter"
      ],
      "year": "2019",
      "venue": "Neural Networks"
    },
    {
      "citation_id": "37",
      "title": "Personalized machine learning for robot perception of affect and engagement in autism therapy",
      "authors": [
        "Ognjen Rudovic",
        "Jaeryoung Lee",
        "Miles Dai",
        "Björn Schuller",
        "Rosalind Picard"
      ],
      "year": "2018",
      "venue": "Science Robotics"
    },
    {
      "citation_id": "38",
      "title": "A circumplex model of affect",
      "authors": [
        "Russell James"
      ],
      "year": "1980",
      "venue": "Journal of Personality and Social Psychology"
    },
    {
      "citation_id": "39",
      "title": "Speech emotion recognition: Two decades in a nutshell, benchmarks, and ongoing trends",
      "authors": [
        "W Björn",
        "Schuller"
      ],
      "year": "2018",
      "venue": "Commun. ACM"
    },
    {
      "citation_id": "40",
      "title": "A brief measure for assessing generalized anxiety disorder: the GAD-7",
      "authors": [
        "Kurt Robert L Spitzer",
        "Janet Bw Kroenke",
        "Bernd Williams",
        "Löwe"
      ],
      "year": "2006",
      "venue": "Archives of Internal Medicine"
    },
    {
      "citation_id": "41",
      "title": "",
      "authors": [
        "Sahar Voghoei",
        "Navid Hashemi Tonekaboni",
        "Jason Wallace",
        "Hamid Arabnia"
      ],
      "year": "2019",
      "venue": ""
    },
    {
      "citation_id": "42",
      "title": "Imagine how to behave: the influence of imagined contact on human-robot interaction",
      "authors": [
        "Ricarda Wullenkord",
        "Friederike Eyssel"
      ],
      "year": "1771",
      "venue": "Philosophical Transactions of the Royal Society B: Biological Sciences"
    }
  ]
}