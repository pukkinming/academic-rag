{
  "paper_id": "2107.07740v1",
  "title": "Ms-Mda: Multisource Marginal Distribution Adaptation For Cross-Subject And Cross-Session Eeg Emotion Recognition",
  "published": "2021-07-16T07:19:54Z",
  "authors": [
    "Hao Chen",
    "Ming Jin",
    "Zhunan Li",
    "Cunhang Fan",
    "Jinpeng Li",
    "Huiguang He"
  ],
  "keywords": [
    "brain-computer interface",
    "deep learning",
    "EEG",
    "emotion recognition",
    "affective computing",
    "domain adaptation",
    "transfer learning"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "As an essential element for the diagnosis and rehabilitation of psychiatric disorders, the electroencephalogram (EEG) based emotion recognition has achieved significant progress due to its high precision and reliability. However, one obstacle to practicality lies in the variability between subjects and sessions. Although several studies have adopted domain adaptation (DA) approaches to tackle this problem, most of them treat multiple EEG data from different subjects and sessions together as a single source domain for transfer, which either fails to satisfy the assumption of domain adaptation that the source has a certain marginal distribution, or increases the difficulty of adaptation. We therefore propose the multi-source marginal distribution adaptation (MS-MDA) for EEG emotion recognition, which takes both domain-invariant and domain-specific features into consideration. First, we assume that different EEG data share the same low-level features, then we construct independent branches for multiple EEG data source domains to adopt oneto-one domain adaptation and extract domain-specific features. Finally, the inference is made by multiple branches. We evaluate our method on SEED and SEED-IV for recognizing three and four emotions, respectively. Experimental results show that the MS-MDA outperforms the comparison methods and state-of-theart models in cross-session and cross-subject transfer scenarios in our settings. Codes at https://github.com/VoiceBeer/MS-MDA.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Related Work",
      "text": "In recent years, the research of affective computing has become one of the trends of machine learning, neural systems, and rehabilitation study. Among those works, emotions are usually characterized into two types of emotion model: discrete categories (basic emotional states, e.g., happy, sad, neutral  [19] ) or continuous values (e.g., in 3D space of arousal, valence and dominance  [20] ). With domain adaptation techniques, many works have achieved significant performance in the field of affective computing.\n\nZheng et al.  [21]  first applies Transfer Component Analysis  [22]  and Kernel Principle Analysis based methods on SEED dataset to personalize EEG-based affective models and demonstrates the feasibility of adopting DA in EEG-based aBCIs. Chai et al. proposes adaptive subspace feature matching  [23]  to decrease the marginal distribution discrepancy between two domains, which requires no labeled samples in the target domain. To solve cross-day binary classification, Lin et al.  [24]  extends robust principal component analysis (rPCA)  [25]  to their filtering strategy which can capture EEG oscillations of relatively consistent emotional responses. Li et al., different from the above, considering the multi-source scenario, and proposes a Multi-source Style Transfer Mapping (MS-STM)  [26]  framework for cross-subject transfer. They first take a few labeled training data to learn multiple STMs, which are then being used to map the target domain distribution to the space of the sources. Their method is similar to our MS-MDA, but they do not take the domain-invariant features into consideration, thus losing the low-level information.\n\nIn recent years, with the development of deep learning techniques and its usability, many works of EEG-based decoding with neural networks have been proposed. Jin et al.  [27] , and Li et al.  [28]  adopts deep adaptation network (DAN)  [29]  to EEG-based emotion recognition, which takes maximum mean discrepancy (MMD)  [30]  as a measure of the distance between the source and the target domain, and training to reduce it on multiple layers. Extending the original method, Chai et al. proposes subspace alignment auto-encoder (SAAE)  [31]  which first projects both source and target domains into a domain-invariant subspace using an auto-encoder, and then kernel PCA, graph regularization and MMD are used to align the feature distribution. To adapt the joint distribution, Li et al.  [32]  propose a domain adaptation method for EEG-based emotion recognition by simultaneously adapting marginal distributions and conditional distributions, they also present a fast online instance transfer (FOIT) for improved EEG emotion recognition  [33] . Zheng et al. extends SEED dataset to SEED-IV dataset and presents EmotionMeter  [34] , a multi-modal emotion recognition framework that combines two modalities of eye movements and EEG waves. With the concept of attention-based convolutional neural network (CNN)  [35] , Fahimi et al.  [36]  develops an end-to-end deep CNN for crosssubject transfer and fine-tunes it by using some calibration data from the target domain. To tackle the requirement of amassing extensive EEG data, Zhao et al.  [37]  proposes a plug-andplay domain adaptation method for shortening the calibration time within a minute while maintaining the accuracy. Wang et al.  [38]  present a domain adaptation SPD matrix network (daSPDnet) to help cut the demand of calibration data for BCIs.\n\nThese aBCI works have gained significant improvement in their respective directions, transfer scenarios, and on multiple benchmark databases. However, many of them focus on combing multiple sources into one and adopt one-to-one DA, which ignores the differences of the marginal distribution of different EEG domains (source-combine DA in Fig.  1 ). This operation may compromise the effectiveness of downstream tasks, and although it somehow extends the training data, the trained models do not generalize well enough. Therefore, inspired by  [39] , a novel multi-source transfer framework, we propose MS-MDA (multi-source marginal distribution alignment for EEG-based emotion recognition), which transfers multiple source domains to the target domain separately, thus avoiding the destruction of the marginal distribution of the multiple EEG source domains; and also takes the domain-invariant features into consideration. Due to the sensitivity of the EEG data and intuition, we do not adopt complex networks, but just a combination of few multi-layer perceptrons (MLPs)  [40] , and thus makes our method computationally efficient, and easy to expand.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Iii. Materials",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "A. Diagram",
      "text": "The flow of one EEG-based aBCI for emotion recognition is shown in Fig.  2 , which involves five steps:\n\n• Stimulating emotions. The subjects are first stimulated with stimuli that correspond to a target emotion. The most commonly used stimuli are movie clips with sound, which can better stimulate the desired emotion because they mix sound with images and actions. After each clip, selfassessment is also applied for the subject to ensure the consistency of the evoked emotion and the target emotion. • EEG signal acquisition and recording. The EEG data are collected using the dry electrodes on the BCI, and then be labeled with the target emotion.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "B. Scenarios",
      "text": "Considering the sensitivity of the EEG, domain adaptation in emotion recognition can be divided into several cases: 1) Cross-subject transfer. In one session, new EEG data from a new subject is taken as the target domain, and the rest of existing EEG data from other subjects are taken as the source domains for DA. 2) Cross-session transfer. For one subject, data collected in the previous sessions can be used as the source domain for DA, and data collected in the new session are taken as the target domain.\n\nIn our work, since the datasets we evaluate on contains 3 session and 15 subjects (refer to Section III-C for details), we take the first 2 session data from one subject as the source domains for cross-session transfer, and take the first 14 subjects data from one session as the source domains for cross-subject transfer. The results of cross-session scenarios are averaged over 15 subjects, and the results of cross-subject are averaged over 3 sessions. Standard deviations are also calculated.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "C. Datasets",
      "text": "The database we evaluate on are: SEED  [19]  [41] and SEED-IV  [34] , both are established by the BCMI laboratory led by Prof. Bao-Liang Lu from Shanghai Jiao Tong University.\n\nThe SEED database contains emotion-related EEG signals that are evoked by 15 film clips (with positive, neutral, and negative emotions) from 15 subjects with 3 sessions each. The signals are recorded by a 62-channel ESI neuroscan system.\n\nThe SEED-IV is an evolution of SEED, which contains 3 sessions, each has 15 subjects and 24 film clips. Comparing to the SEED with EEG signals only, this database also includes eye movement features recorded by SMI eye-tracking glasses.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "D. Pre-Processing",
      "text": "After collecting EEG raw data, pre-processing on signals and feature extractions will be adopted. For both SEED and SEED-IV, to increase the SNR, the raw EEG signals are first down-sampled to a 200 Hz sampling rate, then been processed with a band-pass filter between 1 Hz to 75 Hz. After that, features are then being extracted.\n\nRecent works extract features from EEG data on the time domain, frequency domain, and time-frequency domain. Among them, Differential Entropy (DE) as in  (1) , has the ability to distinguish patterns from different bands  [42] , thus we choose to take DE features as the input data of our model. For SEED and SEED-IV, extracted DE features at five frequency bands of delta (1-4 Hz), theta (4-8 Hz), alpha (8-14 Hz) and gamma (31-50 Hz) are provided.\n\nOne data from one subject in one session for both databases is in the form of channel (62) × trial (15 for SEED, 24 for SEED-IV) × band (5), we then merge the channel with the band, and the form becomes trial × 310 (62 × 5). For SEED, 15 trials contain 3394 samples in total for each session. For SEED-IV, 24 trials contain 851/832/822 samples for three sessions, respectively. In the end, all data are formed into 3394 × 310 (SEED), or 851/832/822 × 310 (SEED-IV) with corresponding generated label vectors in the form of 3394 × 1, or 851/832/822 × 1.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Iv. Method",
      "text": "For simplicity of demonstration, we list the symbols and their definition in Table I that will be used in the following sections.\n\nGiven a set of pre-existing EEG data and a newly collected EEG data, our goal is to learn a model φ that is trained on these multiple independent source domain data using DA, and thus has a better prediction on the newly collected data than simply combining the existed data into one source domain. The architecture of the proposed method is illustrated in Fig.  3 .\n\nAs shown in the figure, the input to the MS-MDA are N independent source domain data {(X S i , Y S i )} N i=1 and a target domain data {X T }, and then these data are fed into a common feature extractor module to get the domain-invariance features {Q S i } N i=1 and {Q T }. Then for each domain-specific feature extractor, extracted common features {Q S i } N i=1 will be fed into one branch with {Q T } and get their domain-specific features:\n\n, and on top of that, the MMD extracted from the last step will get to the domain-specific classifiers to get the corresponding classification predictions:\n\n, then the results of the source domain are taken to calculate the classification loss. Since the target domain will be fed into all the source domain classifiers, multiple target domain predictions are generated. These predictions are taken to calculate the discrepancy loss. In the end, the average of these target-domain predictions is taken as the output of the model. Details of these modules are given below.\n\nCommon Feature Extractor in the MS-MDA is used to map the source and target domain data from the original feature spaces to a common sharing latent space, and then common representations of all domains are extracted. This module can help to extract some low-level domain-invariant features.\n\nDomain-specific Feature Extractor follows the Common Feature Extractor (CFE). After obtaining the features of all domains, we set up N single fully connected layers to correspond to N source domains. For each pair of source and target domain, we map the data to a unique latent space via the corresponding Domain-specific Feature Extractor (DSFE), respectively, and then obtain the domain-specific features in each branch. To apply DA and bring the two domains close in the latent space, we choose the MMD to estimate the distance between these two domains. MMD is widely used in the DA and can be formulated in  (2) . In the process of training, MMD loss is decreased to narrow the source domain and the target domain in the feature space, which helps make better predictions for the target domain. This module aims to learn multiple domain-specific features. Domain-specific Classifier uses the features extracted from the DSFE to predict the result. In Domain-specific Classifier (DSC), there are N single softmax classifiers that correspond to each source domain. For each classifier training, we choose cross-entropy to estimate the classification loss, as shown in  (3) . Besides, since there are N classifiers in this module, and these N classifiers are trained on N source domains, if their predictions are simply averaged as the final result, the variance will be high, especially when the target domain samples are at the decision boundary, which will have a significant negative impact on the results. To reduce this variance, a metric called discrepancy loss is introduced to make the predictions of the N classifiers converge, which is shown in  (4) . The average of the predictions of the N classifiers is taken as the final result.\n\nIn summary, MS-MDA accepts N source domain EEG data and one target domain EEG data, and then includes a common feature extractor to get N source domain features and one target domain feature. Next, N domain-specific feature extractors are used to pairwise compute the MMD loss of one individual source with the target domain and extract their domain-specific features. Finally, a domain-specific classifier is used to do the classification task, which also calculates the classification loss of the N classifiers using the features, with the discrepancy loss of the N classifiers for the features of the target domain data after the previous N feature extractors.\n\nThe training is based on the (5) and following the algorithm as shown in Algorithm. 1. For the three losses, minimizing MMD loss can get domain-invariant features for each pair of the source and target domains; minimizing classification loss will bring more accurate classifiers for predicting the source domain data; minimizing discrepancy loss will get more convergent multiple classifiers.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Algorithm 1 Overview Of Ms-Mda Input:",
      "text": "Iteration T , source domain data {(X S i , Y S i )} N i=1 and target domain data {X T } 1: for t = 1,..., T do",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "2:",
      "text": "Take m samples {x Si j , y Si j } m j=1 from source domains and {x T j } m j=1 from target domain.\n\n3:\n\n{r Si j } m j=1 , {r T j } m j=1 ← DSF E({q Si j } m j=1 , {q T })\n\n5:\n\nUpdate model by minimizing the total loss 9: end for 10: return { Ŷ T }; Output:\n\nPrediction of target domain data, { Ŷ T };",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "V. Experiments",
      "text": "We perform substantial experiments in the task of classification of emotions on two datasets SEED and SEED-IV, with the normalization study to the EEG data for domain adaptation. Besides, we also conduct some exploratory experiments in addition to the evaluation of our proposed methods and comparison methods.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "A. Implementation Details",
      "text": "As mentioned in the Section. IV, there are many details in the three modules of MS-MDA. First, for the Common Feature Extractor (CFE), since we do not take raw data (i. e. EEG signals) but the extracted DE features as vectors, complex deep models such as deep convolutional neural networks are not suitable for this module, thus we choose 3-layer MLP for simplicity which reduces feature dimensions from 310dimension (62 × 5, channel × band) to 64-D. In CFE, every linear layer is followed by a LeakyReLU  [43]  layer. We also evaluate the effort of the ReLU  [44]  activation function, but due to the sensitivity of the EEG data, much information would be lost if using ReLU since the value less than zero would be dropped, so we choose LeakyReLU as a compromise.\n\nNext, for both domain-specific feature extractor (DSFE) and domain-specific classifier (DSC), there is a single linear which reduces 64-D to 32-D and 32-D to the corresponding number of categories (3 for SEED, 4 for SEED-IV), respectively. In DSFE, same as the settings in CFE, a LeakyReLU layer is followed after the linear layer, while in DSC, there is only one linear layer without any activation function. The network is trained using an Adam  [45]  optimizer with an initial learning rate of 0.01, and train for 200 epoch. The batch size we choose is 256, which means we take 256 samples from each domain in every iteration (we also evaluate different settings of batch size and epoch in Section V-E). The whole model is trained under the (5), for domain adaptation loss, we choose MMD as the metric of the distance between two domains in the feature space (CORAL loss has a similar effect). As for the discrepancy loss, L1 regularization is being used, we also evaluate this loss in Section V-E. Besides, we dynamically adjust the α coefficients to achieve the effect of focusing on the classification results first, and then start aligning MMD and the convergence between the classifiers (α = 2 1+e -10 * i/epoch -1). As for the training data, we take the DE features and reform one sample to a 310-D vector as illustrated in the Section III-D. Before feeding into the model, we normalize all the data in electrode-wise, refer to Section V-D for details.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "B. Results",
      "text": "Experiment results of comparison methods and our proposed method on SEED and SEED-IV are listed in Table . \nII, all the hyper-parameters are the same, except for those results taken directly from the original papers. It should be noticed that since many previous works do not make their codes public available, we then customize the comparison methods (in the deep learning domain adaptation field) that are described in their papers with our settings, and also including some typical deep learning domain adaptation models for better comparison (DDC  [46] , DCORAL  [47] ). The results indicate that our method largely outperforms the comparison methods in most transfer scenarios. For SEED dataset, our method has a minimum of 7% and 3% improvement in cross-session and cross-subject scenarios, respectively. While in SEED-IV dataset, our method has a minimum of 7% and 18% for two transfer scenarios. The results also show that our method outperforms comparison methods significantly in cross-subject, the reason for that may be that in the crosssubject scenario, the number of sources is 14, much bigger than the number of 2 in cross-session, and thus maximizes the effect of taking multiple sources as multiple individuals in domain adaptation rather than concatenating them.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "C. Ablation Study",
      "text": "To understand the effect of each module in the MS-MDA, we remove them one at a time and evaluate the performance of the ablated model, the results are shown in Table . III. The first row of SEED and SEED-IV shows the performance of the full model (the same as in Table  II ). The second row ablates the MMD loss in the training process, which makes the model focuses only on the classification loss and discrepancy loss. The significant drop compared to the full model indicates the important effect of domain adaptation. Notice that even the results without MMD loss are better than many comparison methods, showing the importance of taking multiple sources as multiple individuals during training. The third row of taking out the discrepancy loss shows that this loss will affect the performance but the impact is minimal, the reason is that we want this discrepancy loss to be the icing on the cake rather than having a dominant effect on the model. The fourth row only considers the classification loss, thus reduces losses (2) and (4).",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "D. Normalization",
      "text": "During the experiments, we also find that different normalization to data can significantly impact the outcomes, and also the order of whether first concatenating multiple sources or first normalize each session individually. Thus we design diagrams and conduct extensive experiments to investigate the effects of different normalization strategies on the input data, i. e., extracted feature vectors from two datasets. Since we have reformed the origin 4-D matrices (session × channel × trial × band) into 3-D matrices (session × trial × (channel*band)), for each session, there is a 2-D matrix of trial × 310. Following the common machine learning normalization approaches and the prior knowledge and intuition of EEG data (i. e., the data acquired by the same electrode are more consistent with\n\n(2) Fig.  5 . Small blue matrices are data from different subjects, and (1), (  2 ) are two operations. The basic process is: multi-source data →(1) →(2). In order A, (1) in the figure stands for the normalization, and (2) stands for the concatenate.\n\nIn order B: (1) stands for concatenating while (  2 ) is for normalization.\n\nthe same distribution), the normalization methods to these 2-D matrices can be categorized into three, as shown in Fig.  4 . Besides, since we also take the multi-source situation into consideration, the order of normalization may also influence the performance, as shown in Fig.  5 . We evaluate three normalization methods and two normalization orders on SEED and SEED-IV with our proposed method MS-MDA and representative domain adaptation model DAN  [29] . The results are listed in Table . IV. In all three sets, the normalization of electrode-wise outperforms the other three normalization types significantly. Comparing DAN 1 with DAN 2 , the results indicate that the first normalization order of normalizing the data first and then concatenating them is better. In the third set of MS-MDA, we find that all the results of four normalization types are better than those in the first and second sets, and the improvement is significant. Row w/o normalization in MS-MDA, for example, has a top of 47% improvement, which also indicates the generalization of our proposed method in different normalization types, and the positive effects of taking multiple sources as individual branches for DA.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "E. Additions",
      "text": "1) Coefficient Study: After multiple sets of experiments, we find that easy to control the MMD loss and it plays an influential role in the training as shown in Table  III . However, for the disc. loss, it remains many problems. Adding this loss to the model too early will affect the overall effect, and too late will lose the impact of learning convergence. Too large a weight would cause the training to focus on convergence, thus the few correct ones might follow the many incorrect ones; too small may not have enough influence on the model. Also, for better use and simplicity mentioned earlier, we do not make many tests on the β, but simply compared the effects on only a few sets of β, and the results are shown in Table  V . From which we can see that compared to row one (w/o disc. loss), introducing discrepancy loss increases the performance in most cases, especially when training for the whole process in cross-subject for SEED-IV. We then choose the weight of 0.01 and training discrepancy loss for the whole process according to the results.\n\n2) Hyper-parameters and Data Visualization: To better investigating our proposed method, we evaluate it with different hyper-parameters, besides, we also take the representative method DAN as the comparison. The results are shown in Fig.  6  and Fig.  7 . From them we can see that, with the increase of batch size, both models show a drop in performance, especially when the batch size is 512, which has a significant decrease compared to 256 on SEED-IV. Besides, with the training epoch increases, neither model has a substantial improvement, especially MS-MDA, but our method achieves moderate accuracy and converges faster. Comparing cross-subject experiments on two datasets, it can be significantly seen that MS-MDA has a clear advantage over DAN, which indirectly shows that our approach has a more significant performance improvement for multiple source domain adaptation in EEG-based emotion recognition.\n\nFor a better understanding of the effect of our proposed method, we randomly pick 100 EEG samples from each subject (domain) in the scenario of cross-subject to visualize with t-SNE  [48] , as displayed in Fig.  8 . We only plot the crosssubject since this transfer scenario has more sources that will maximize visualization. In the Fig.  8 , each color stands for a source domain, and the target domain are in black. To better plotting, we transparent the target sample to avoid overlap. It should be noticed that in the lower left figure, we pick 1400 samples since we concatenate all sources into one.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Vi. Discussion",
      "text": "As can be seen from Table  II , comparing the results of selective methods and prior works, our proposed method has a significant improvement, especially for cross-subject DA in which the number of source domains is large. The ablation experiments from Table V-C also show that our proposed method requires both MMD and discrepancy loss  in most cases. Eliminating the MMD loss has a significant performance drop on both datasets, confirming the importance of DA, and eliminating disc. loss does not have as large an impact as MMD loss, but also verifies the help of multisource convergence. Also, during the experiments, we find that the type of normalization of the data has a significant impact on the overall results, so we also design experiments and explore the normalization of EEG data in DA to help improve the performance of our model. As can be seen in Table  V -D, there is not much difference between the two normalization orders, and it is most appropriate to do data normalization on the electrode-wise, which has a crushing performance improvement compared to the other three methods; for our method, which does not concatenate data, electrode normalization is also the most effective. This conclusion is in line with our intuition that data collected from the same electrode are relatively more regular or conform to a certain distribution, while data collected from different electrodes are very different. In addition, during the experiments, we find that the disc. loss needs to be carefully adjusted, otherwise it is easy to cause harmful effects, which we guess is because this loss introduces a convergence effect on multiple classifiers in the model (in other words, smooth the inferences made from multiple classifiers), and if most of the classifiers are wrong, this convergence effect will cause the correct classifiers to error. Therefore, we also test and evaluate the impact of the disc. loss coefficients on the model at different settings, and from Table  V , we can see that the disc. loss achieves the best results if it is set to 0.01 times the MMD loss coefficient and is being used in the full model training.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "一•",
      "text": "After exploring the internal details of the model, we also evaluated the performance of the model under different hyperparameters. For better comparison, we chose a representative DAN as the comparison method. From Fig.  6  and Fig.  7 , we can see that both models have a significant decrease as the batch size is increased. The reason for this we assume is that small batch size tends to fall into local optimal overfitting. The performance of both models increases slightly with epoch. From Figs.  6  and 7 , we can also clearly see that MS-MDA has a significant advantage over DAN in cross-subject DA where the number of multiple source domains is large, which also confirms the importance of constructing multiple branches for multiple source domains to adopt DA separately.\n\nAlthough it is from the results that our proposed method has a significant performance improvement, we also found that the training time consumed increases linearly with the number of source domains, i.e., the larger the number of source domains and the larger the model, the longer the training takes, unlike concatenating all source data into one, where there is only additional time due to the increase in the amount of data. For this problem, our current idea is to discard some less relevant source domains selectively and not build DA branches for them, allowing the disc. loss to play a more prominent role because there is less negative information. In addition, the encoders in the current model are the simplest MLP, and many literature and works have verified the usability of LSTM for EEG data  [49] ,  [50] ,  [51] , and we will consider switching to use LSTM as the encoders in future works.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Vii. Conclusion",
      "text": "In this paper, we propose MS-MDA, an EEG-based emotion recognition domain adaptation method, which is applicable to multiple source domain situations. Through experimental evaluation, we find that this method has a better ability to adapt to multiple source domains, which is validated by comparison with the selective approaches and the SOTA models, especially for cross-subject experiments where our proposed method consists of up to 20% improvement. In addition, we also explore the impact of different normalization methods for EEG data in domain adaptation, which we believe can serve as an inspiration for other EEG-based works while improving the effectiveness of the models. As for our future work, the current model for multiple source domains is to construct a DA branch for each of them without selection, which will increase the model size and training time exponentially, and also introduces information from the source domain that is not relevant to the target into the model. A more efficient approach may be to selectively build DA branches from a reservoir of source domains, allowing the model to be more efficient while only focusing on the source domain information that is relevant to the target domain.",
      "page_start": 9,
      "page_end": 9
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Two strategies of multi-source domain adaptation. a) is a single-",
      "page": 2
    },
    {
      "caption": "Figure 1: ). First, we assume all the EEG data share low-level",
      "page": 2
    },
    {
      "caption": "Figure 1: ). This operation",
      "page": 3
    },
    {
      "caption": "Figure 2: , which involves ﬁve steps:",
      "page": 3
    },
    {
      "caption": "Figure 2: The ﬂowchart of EEG-based BCI for emotion recognition. The",
      "page": 3
    },
    {
      "caption": "Figure 3: As shown in the ﬁgure, the input to the MS-MDA are N",
      "page": 4
    },
    {
      "caption": "Figure 3: The architecture of our proposed method. Our network consists of a common feature extractor, domain-speciﬁc feature extractor, and domain-speciﬁc",
      "page": 5
    },
    {
      "caption": "Figure 4: Three normalization meth-",
      "page": 7
    },
    {
      "caption": "Figure 5: Small blue matrices are",
      "page": 7
    },
    {
      "caption": "Figure 4: Besides, since we also take the multi-source situation into",
      "page": 7
    },
    {
      "caption": "Figure 5: We evaluate three normalization methods and two normal-",
      "page": 7
    },
    {
      "caption": "Figure 6: Evaluation of MS-MDA and DAN with different batch size and epochs.",
      "page": 8
    },
    {
      "caption": "Figure 6: and Fig. 7. From them we can see that, with the increase of",
      "page": 8
    },
    {
      "caption": "Figure 8: We only plot the cross-",
      "page": 8
    },
    {
      "caption": "Figure 8: , each color stands for a",
      "page": 8
    },
    {
      "caption": "Figure 7: Evaluation of MS-MDA and DAN with different settings of batch",
      "page": 8
    },
    {
      "caption": "Figure 8: Visualization with t-SNE for raw data (upper left), normalization data",
      "page": 8
    },
    {
      "caption": "Figure 6: and Fig. 7, we",
      "page": 9
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "32-D": "Domain-specific \nFeature Extractor",
          "mmd \nloss\nN ×": "",
          "3-D/4-D": "Domain-specific \nClassifier",
          "disc.\ncls. \nloss\nloss\nN×\nground-truth \nlabels": ""
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Emotion, cognition, and behavior",
      "authors": [
        "R Dolan"
      ],
      "year": "2002",
      "venue": "science"
    },
    {
      "citation_id": "2",
      "title": "The influences of emotion on learning and memory",
      "authors": [
        "C Tyng",
        "H Amin",
        "M Saad",
        "A Malik"
      ],
      "year": "2017",
      "venue": "Frontiers in psychology"
    },
    {
      "citation_id": "3",
      "title": "Emotions and affect in human factors and human-computer interaction: Taxonomy, theories, approaches, and methods",
      "authors": [
        "M Jeon"
      ],
      "year": "2017",
      "venue": "Emotions and affect in human factors and human-computer interaction"
    },
    {
      "citation_id": "4",
      "title": "Breaking the silence: brain-computer interfaces (bci) for communication and motor control",
      "authors": [
        "N Birbaumer"
      ],
      "year": "2006",
      "venue": "Psychophysiology"
    },
    {
      "citation_id": "5",
      "title": "Towards an eeg-based intuitive bci communication system using imagined speech and visual imagery",
      "authors": [
        "S.-H Lee",
        "M Lee",
        "J.-H Jeong",
        "S.-W Lee"
      ],
      "year": "2019",
      "venue": "2019 IEEE International Conference on Systems, Man and Cybernetics"
    },
    {
      "citation_id": "6",
      "title": "A new gaze-bci-driven control of an upper limb exoskeleton for rehabilitation in real-world tasks",
      "authors": [
        "A Frisoli",
        "C Loconsole",
        "D Leonardis",
        "F Banno",
        "M Barsotti",
        "C Chisari",
        "M Bergamasco"
      ],
      "year": "2012",
      "venue": "IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)"
    },
    {
      "citation_id": "7",
      "title": "Knowing what you're feeling and knowing what to do about it: Mapping the relation between emotion differentiation and emotion regulation",
      "authors": [
        "L Barrett",
        "J Gross",
        "T Christensen",
        "M Benvenuto"
      ],
      "year": "2001",
      "venue": "Cognition & Emotion"
    },
    {
      "citation_id": "8",
      "title": "Emotion regulation in depression: Relation to cognitive inhibition",
      "authors": [
        "J Joormann",
        "I Gotlib"
      ],
      "year": "2010",
      "venue": "Cognition and Emotion"
    },
    {
      "citation_id": "9",
      "title": "Emotion processing in alzheimer's disease",
      "authors": [
        "R Bucks",
        "S Radford"
      ],
      "year": "2004",
      "venue": "Aging & mental health"
    },
    {
      "citation_id": "10",
      "title": "Facial expression and emotion",
      "authors": [
        "P Ekman"
      ],
      "year": "1993",
      "venue": "American psychologist"
    },
    {
      "citation_id": "11",
      "title": "Automated depression detection using deep representation and sequence learning with eeg signals",
      "authors": [
        "B Ay",
        "O Yildirim",
        "M Talo",
        "U Baloglu",
        "G Aydin",
        "S Puthankattil",
        "U Acharya"
      ],
      "year": "2019",
      "venue": "Journal of medical systems"
    },
    {
      "citation_id": "12",
      "title": "EEG signal processing",
      "authors": [
        "S Sanei",
        "J Chambers"
      ],
      "year": "2013",
      "venue": "EEG signal processing"
    },
    {
      "citation_id": "13",
      "title": "Computer-aided diagnosis of depression using eeg signals",
      "authors": [
        "U Acharya",
        "V Sudarshan",
        "H Adeli",
        "J Santhosh",
        "J Koh",
        "A Adeli"
      ],
      "year": "2015",
      "venue": "European neurology"
    },
    {
      "citation_id": "14",
      "title": "A boosting-based spatialspectral model for stroke patients' eeg analysis in rehabilitation training",
      "authors": [
        "Y Liu",
        "H Zhang",
        "M Chen",
        "L Zhang"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on Neural Systems and Rehabilitation Engineering"
    },
    {
      "citation_id": "15",
      "title": "Enhancing eeg-based classification of depression patients using spatial information",
      "authors": [
        "C Jiang",
        "Y Li",
        "Y Tang",
        "C Guan"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Neural Systems and Rehabilitation Engineering"
    },
    {
      "citation_id": "16",
      "title": "Brain functional networks based on resting-state eeg data for major depressive disorder analysis and classification",
      "authors": [
        "B Zhang",
        "G Yan",
        "Z Yang",
        "Y Su",
        "J Wang",
        "T Lei"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Neural Systems and Rehabilitation Engineering"
    },
    {
      "citation_id": "17",
      "title": "Classifying depression patients and normal subjects using machine learning techniques and nonlinear features from eeg signal",
      "authors": [
        "B Hosseinifard",
        "M Moradi",
        "R Rostami"
      ],
      "year": "2013",
      "venue": "Computer methods and programs in biomedicine"
    },
    {
      "citation_id": "18",
      "title": "A survey on transfer learning",
      "authors": [
        "S Pan",
        "Q Yang"
      ],
      "year": "2009",
      "venue": "IEEE Transactions on knowledge and data engineering"
    },
    {
      "citation_id": "19",
      "title": "Investigating critical frequency bands and channels for eeg-based emotion recognition with deep neural networks",
      "authors": [
        "W.-L Zheng",
        "B.-L Lu"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on Autonomous Mental Development"
    },
    {
      "citation_id": "20",
      "title": "Deap: A database for emotion analysis; using physiological signals",
      "authors": [
        "S Koelstra",
        "C Muhl",
        "M Soleymani",
        "J.-S Lee",
        "A Yazdani",
        "T Ebrahimi",
        "T Pun",
        "A Nijholt",
        "I Patras"
      ],
      "year": "2011",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "21",
      "title": "Personalizing eeg-based affective models with transfer learning",
      "authors": [
        "W.-L Zheng",
        "B.-L Lu"
      ],
      "year": "2016",
      "venue": "Proceedings of the twenty-fifth international joint conference on artificial intelligence"
    },
    {
      "citation_id": "22",
      "title": "Domain adaptation via transfer component analysis",
      "authors": [
        "S Pan",
        "I Tsang",
        "J Kwok",
        "Q Yang"
      ],
      "year": "2010",
      "venue": "IEEE transactions on neural networks"
    },
    {
      "citation_id": "23",
      "title": "A fast, efficient domain adaptation technique for cross-domain electroencephalography (eeg)-based emotion recognition",
      "authors": [
        "X Chai",
        "Q Wang",
        "Y Zhao",
        "Y Li",
        "D Liu",
        "X Liu",
        "O Bai"
      ],
      "year": "2017",
      "venue": "Sensors"
    },
    {
      "citation_id": "24",
      "title": "Improving cross-day eeg-based emotion classification using robust principal component analysis",
      "authors": [
        "Y.-P Lin",
        "P.-K Jao",
        "Y.-H Yang"
      ],
      "year": "2017",
      "venue": "Frontiers in computational neuroscience"
    },
    {
      "citation_id": "25",
      "title": "Robust principal component analysis?",
      "authors": [
        "E Candès",
        "X Li",
        "Y Ma",
        "J Wright"
      ],
      "year": "2011",
      "venue": "Journal of the ACM (JACM)"
    },
    {
      "citation_id": "26",
      "title": "Multisource transfer learning for cross-subject eeg emotion recognition",
      "authors": [
        "J Li",
        "S Qiu",
        "Y.-Y Shen",
        "C.-L Liu",
        "H He"
      ],
      "year": "2019",
      "venue": "IEEE transactions on cybernetics"
    },
    {
      "citation_id": "27",
      "title": "Eeg-based emotion recognition using domain adaptation network",
      "authors": [
        "Y.-M Jin",
        "Y.-D Luo",
        "B.-L Lu"
      ],
      "year": "2017",
      "venue": "2017 international conference on orange technologies (ICOT)"
    },
    {
      "citation_id": "28",
      "title": "Cross-subject emotion recognition using deep adaptation networks",
      "authors": [
        "H Li",
        "Y.-M Jin",
        "W.-L Zheng",
        "B.-L Lu"
      ],
      "year": "2018",
      "venue": "International conference on neural information processing"
    },
    {
      "citation_id": "29",
      "title": "Learning transferable features with deep adaptation networks",
      "authors": [
        "M Long",
        "Y Cao",
        "J Wang",
        "M Jordan"
      ],
      "year": "2015",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "30",
      "title": "Integrating structured biological data by kernel maximum mean discrepancy",
      "authors": [
        "K Borgwardt",
        "A Gretton",
        "M Rasch",
        "H.-P Kriegel",
        "B Schölkopf",
        "A Smola"
      ],
      "year": "2006",
      "venue": "Bioinformatics"
    },
    {
      "citation_id": "31",
      "title": "Unsupervised domain adaptation techniques based on auto-encoder for non-stationary eeg-based emotion recognition",
      "authors": [
        "X Chai",
        "Q Wang",
        "Y Zhao",
        "X Liu",
        "O Bai",
        "Y Li"
      ],
      "year": "2016",
      "venue": "Computers in biology and medicine"
    },
    {
      "citation_id": "32",
      "title": "Domain adaptation for eeg emotion recognition based on latent representation similarity",
      "authors": [
        "J Li",
        "S Qiu",
        "C Du",
        "Y Wang",
        "H He"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Cognitive and Developmental Systems"
    },
    {
      "citation_id": "33",
      "title": "Foit: Fast online instance transfer for improved eeg emotion recognition",
      "authors": [
        "J Li",
        "H Chen",
        "T Cai"
      ],
      "year": "2020",
      "venue": "2020 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)"
    },
    {
      "citation_id": "34",
      "title": "Emotionmeter: A multimodal framework for recognizing human emotions",
      "authors": [
        "W.-L Zheng",
        "W Liu",
        "Y Lu",
        "B.-L Lu",
        "A Cichocki"
      ],
      "year": "2018",
      "venue": "IEEE transactions on cybernetics"
    },
    {
      "citation_id": "35",
      "title": "Abcnn: Attention-based convolutional neural network for modeling sentence pairs",
      "authors": [
        "W Yin",
        "H Schütze",
        "B Xiang",
        "B Zhou"
      ],
      "year": "2016",
      "venue": "Transactions of the Association for Computational Linguistics"
    },
    {
      "citation_id": "36",
      "title": "Inter-subject transfer learning with an end-to-end deep convolutional neural network for eeg-based bci",
      "authors": [
        "F Fahimi",
        "Z Zhang",
        "W Goh",
        "T.-S Lee",
        "K Ang",
        "C Guan"
      ],
      "year": "2019",
      "venue": "Journal of neural engineering"
    },
    {
      "citation_id": "37",
      "title": "Plug-and-play domain adaptation for cross-subject eeg-based emotion recognition",
      "authors": [
        "L.-M Zhao",
        "X Yan",
        "B.-L Lu"
      ],
      "year": "2021",
      "venue": "Proceedings of the 35th AAAI Conference on Artificial Intelligence. sn"
    },
    {
      "citation_id": "38",
      "title": "A prototype-based spd matrix network for domain adaptation eeg emotion recognition",
      "authors": [
        "Y Wang",
        "S Qiu",
        "X Ma",
        "H He"
      ],
      "year": "2021",
      "venue": "Pattern Recognition"
    },
    {
      "citation_id": "39",
      "title": "Aligning domain-specific distribution and classifier for cross-domain classification from multiple sources",
      "authors": [
        "Y Zhu",
        "F Zhuang",
        "D Wang"
      ],
      "year": "2019",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "40",
      "title": "Artificial neural networks (the multilayer perceptron)-a review of applications in the atmospheric sciences",
      "authors": [
        "M Gardner",
        "S Dorling"
      ],
      "year": "1998",
      "venue": "Atmospheric environment"
    },
    {
      "citation_id": "41",
      "title": "Differential entropy feature for eeg-based emotion classification",
      "authors": [
        "R.-N Duan",
        "J.-Y Zhu",
        "B.-L Lu"
      ],
      "year": "2013",
      "venue": "2013 6th International IEEE/EMBS Conference on Neural Engineering (NER)"
    },
    {
      "citation_id": "42",
      "title": "Analysis of eeg signals and facial expressions for continuous emotion detection",
      "authors": [
        "M Soleymani",
        "S Asghari-Esfeden",
        "Y Fu",
        "M Pantic"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "43",
      "title": "Empirical evaluation of rectified activations in convolutional network",
      "authors": [
        "B Xu",
        "N Wang",
        "T Chen",
        "M Li"
      ],
      "year": "2015",
      "venue": "Empirical evaluation of rectified activations in convolutional network",
      "arxiv": "arXiv:1505.00853"
    },
    {
      "citation_id": "44",
      "title": "Rectified linear units improve restricted boltzmann machines",
      "authors": [
        "V Nair",
        "G Hinton"
      ],
      "year": "2010",
      "venue": "Icml"
    },
    {
      "citation_id": "45",
      "title": "Adam: A method for stochastic optimization",
      "authors": [
        "D Kingma",
        "J Ba"
      ],
      "year": "2014",
      "venue": "Adam: A method for stochastic optimization",
      "arxiv": "arXiv:1412.6980"
    },
    {
      "citation_id": "46",
      "title": "Deep domain confusion: Maximizing for domain invariance",
      "authors": [
        "E Tzeng",
        "J Hoffman",
        "N Zhang",
        "K Saenko",
        "T Darrell"
      ],
      "year": "2014",
      "venue": "Deep domain confusion: Maximizing for domain invariance",
      "arxiv": "arXiv:1412.3474"
    },
    {
      "citation_id": "47",
      "title": "Deep coral: Correlation alignment for deep domain adaptation",
      "authors": [
        "B Sun",
        "K Saenko"
      ],
      "year": "2016",
      "venue": "European conference on computer vision"
    },
    {
      "citation_id": "48",
      "title": "Visualizing data using t-sne",
      "authors": [
        "L Van Der Maaten",
        "G Hinton"
      ],
      "year": "2008",
      "venue": "Journal of machine learning research"
    },
    {
      "citation_id": "49",
      "title": "Driver sleepiness detection from eeg and eog signals using gan and lstm networks",
      "authors": [
        "Y Jiao",
        "Y Deng",
        "Y Luo",
        "B.-L Lu"
      ],
      "year": "2020",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "50",
      "title": "Emotion recognition under sleep deprivation using a multimodal residual lstm network",
      "authors": [
        "L.-Y Tao",
        "B.-L Lu"
      ],
      "year": "2020",
      "venue": "2020 International Joint Conference on Neural Networks (IJCNN)"
    },
    {
      "citation_id": "51",
      "title": "Emotion recognition using multimodal residual lstm network",
      "authors": [
        "J Ma",
        "H Tang",
        "W.-L Zheng",
        "B.-L Lu"
      ],
      "year": "2019",
      "venue": "Proceedings of the 27th ACM international conference on multimedia"
    }
  ]
}