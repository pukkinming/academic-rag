{
  "paper_id": "2202.02611v1",
  "title": "Privacy-Preserving Speech Emotion Recognition Through Semi-Supervised Federated Learning",
  "published": "2022-02-05T18:30:23Z",
  "authors": [
    "Vasileios Tsouvalas",
    "Tanir Ozcelebi",
    "Nirvana Meratnia"
  ],
  "keywords": [
    "deep learning",
    "emotion classification",
    "federated learning",
    "semi-supervised learning",
    "speech emotion recognition"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Speech Emotion Recognition (SER) refers to the recognition of human emotions from natural speech. If done accurately, it can offer a number of benefits in building humancentered context-aware intelligent systems. Existing SER approaches are largely centralized, without considering users' privacy. Federated Learning (FL) is a distributed machine learning paradigm dealing with decentralization of privacy-sensitive personal data. In this paper, we present a privacy-preserving and data-efficient SER approach by utilizing the concept of FL. To the best of our knowledge, this is the first federated SER approach, which utilizes self-training learning in conjunction with federated learning to exploit both labeled and unlabeled on-device data. Our experimental evaluations on the IEMOCAP dataset shows that our federated approach can learn generalizable SER models even under low availability of data labels and highly non-i.i.d. distributions. We show that our approach with as few as 10% labeled data, on average, can improve the recognition rate by 8.67% compared to the fully-supervised federated counterparts.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "Speech Emotion Recognition (SER) has attracted growing attention due to its beneficial role in building human-centered context-aware intelligent systems in many fields, such as customer support call review and analysis  [1] , mental health surveillance  [2] , multimedia retrieval  [3]  and smart vehicles  [4] . The main task of SER is to automatically recognize the human emotional states by analyzing utterances. Since, linguists' emotion sets often exceed 300 states, researchers have agreed to use the 'palette theory' to compose a number of core emotions (i.e., Anger, Disgust, Fear, Joy, Sadness, and Surprise) from which any emotional state is originated  [5] .\n\nFor many years, SER approaches has focused on recognizing these core emotions by extracting frame-based features from utterances followed by a classification or regression algorithm, such as Support Vector Machines (SVMs)  [6] . With the advent of deep learning, several deep learning models have been proposed to perform SER by learning from raw audio data directly  [7] . In particular, the recent success of neural network-based attention mechanisms (AMs) across various machine learning tasks, such as image classification  [8] , automatic speech recognition  [9]  and text translation  [10] , has attracted a growing interest in SER  [11]  community. Firstly introduced in the Natural Language Processing (NLP) field  [12] , AMs \"help\" the model learn where to \"look for\" information that is meaningful for performing a particular classification task. Extending this concept to SER, the AM can help determine the emotional state of the speaker, by focusing on specific parts of a spoken utterance that contain emotional information, while disregarding noisy or irrelevant data.\n\nA common limitation of existing SER approaches is their centralized approach, with the implicit assumption that audio data originated from distributed devices can be aggregated in a centralized repository before performing further processing. However, the rapidly increasing size of available data, in combination with the high communication costs, and possible bandwidth limitations, render the accumulation of data in a cloud server unfeasible  [13] . Additionally, such centralized data processing schemes do not consider the privacy concerns and regulations like the General Data Protection Regulation. These limitations and the growing computational and storage capabilities of distributed devices make it appealing to perform SER directly on the device that collects the data by utilizing local computational resources and local learning models.\n\nFederated Learning (FL) is particularly suitable for this purpose thanks to its unique characteristic of collaboratively training machine learning models without sharing local data and compromising users' privacy  [14] . The most popular paradigm in FL is the Federated Averaging (FedAvg) algorithm  [15] , in which minimal updates to local models are performed entirely on-device and only a few model parameter updates are communicated to the central server to aggregate all updates to produce a unified global model. This strategy has recently been applied to a wide range of acoustic tasks  [16] -  [18] . Motivated by their success, in this paper, we investigate the feasibility of SER using FL aiming at preserving users' privacy. Applying traditional FL approaches to SER is undesirable since these FL approach assume that on-device labeled data is sufficiently available, or data can be easily labeled through user interaction or labeling functions, as in keyword prediction. This is clearly an unrealistic assumption in SER. In practice, the ever-growing on-device speech data is largely unlabeled due to the prohibitive cost of annotation and little to no incentives (or expertise) for users to label their data  [19] .\n\nTo utilize on-device unlabeled data, semi-supervised learning (SSL) under federated settings can be explored. Our hypothesis is that this will significantly boost SER models' performance  [20] . In SSL, size of the labeled data is generally much smaller than the unlabeled data  [21] . While there is a wide range of SSL methods, we focus on self-training or pseudo-labeling approach  [22] . In particular, self-training uses the prediction on unlabeled data to supervise the model's training in combination with a small percentage of labeled data  [22] . This simple, yet, effective approach has been shown to achieve great results in centralized regimes  [23] ,  [24] . In contrast with various data augmentation techniques proposed for SER to increase the training data  [25] , our FL approach utilizes largely available on-device unlabeled data. This way, we boost the data availability, without losing the emotional content through unfitting deformation on existing data.\n\nIn this regard, we propose a privacy-preserving and dataefficient SER approach through federated self-training, which unifies semi-supervision with federated learning. By doing so, it addresses significant challenges regarding scarcity of data labels and privacy regulations faced by SER. As distributed devices often operate under energy and computation constraints, we utilize a spectro-temporal-channel attention mechanism to effectively capture relationships across the different segments of utterances without increasing the model's complexity, thus accelerating the training process. To the best of our knowledge, this work is the first federated SER approach that learns models by utilizing not only labeled but also unlabeled samples on user devices, while not being dependent on any data (labeled or unlabeled) on the server side. To this end, our main contributions are as follows:\n\n• We present a privacy-preserving and data-efficient SER approach based on semi-supervised federated learning, which exploits both labeled and unlabeled on-device data. • We introduce an attention mechanism to improve the representation power of SER model without increasing its complexity. • We demonstrate, through extensive evaluations on the IEMOCAP dataset  [26] , that our approach effectively learns generalizable SER models under federated settings, even in case of low availability of labeled data. • We show that our approach with as few as 10% labeled data, on average, can improve the recognition rate by 8.67% compared to a fully-supervised federated regime.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Ii. Related Work",
      "text": "Authors of  [27]  first investigated the use of deep learning in SER by utilizing a Bi-directional Long Short-Term Memory (BLSTM) architecture to capture time-related relationships across data. This was followed by several papers that showed the contribution of attention models in the SER field. Authors of  [28]  used a BLSTM architecture in conjunction with a local attention scheme, which replaced mean pooling with a weighted time-pooling algorithm to compute a weighted sum of the attention outputs. In  [29]  a similar AM on top of a CNN model, showing that convolutions solve SER tasks with similar performance. Along this path,  [30]  utilized two types of convolution filters for extracting time-specific and spectral-specific features from spectrograms, after which a CNN architecture for modelling high-level representation was used. Subsequently, to further improve SER performance,  [30]  proposed an attention-based poling method, combining two attention maps -a class-specific top-down and a classagnostic bottom-up attention -on top of the spectral-time feature extraction, to replace Global Average Pooling (GAP). Recently,  [31]  proposed an attention mechanism, which considers the temporal, spectral, and channel (STC) dimensions simultaneously to tackle the limited ability of CNNs to capture relative importance of features across all 3 axes. With respect to model inputs, all previous works relied on segmenting the utterance into smaller uniformly-shaped chunks, which are then fed to the model. Alternatively,  [32]  designed a fully convolutional network (FCN) architecture, which can preserve the information from variable length utterances as a whole without the need for segmentation. To further distinguish between important and non-speech parts of the input, an attention mechanism, similar to  [29] , was used before the classifier. Apart from the attention-based approaches, transfer learning  [33]  and various augmentation techniques  [25]  have been developed to deal with the limited amount of available natural speech data. None of the existing SER models considers the users' privacy challenge, which can significantly affect their applicability in real-life applications. The requirement to preserve users' privacy often results in a limited amount of natural speech data, which augmentations can only increase to a certain extent before deforming the emotional information present in those data.\n\nSemi-Supervised Federated Learning: To realistically move away from a centralized approach to a federated approach, the unrealistic assumption of existing FL approaches that labeled data are largely available on devices needs to be eliminated  [19] . Existing semi-supervised federated learning (SSFL) approaches have only recently started to be examined in the vision domain to exploit unlabeled data  [34] ,  [35] . FedMatch  [34]  uses an inter-device consistency loss to enforce consistency between the pseudo-labeling predictions made across multiple devices. In  [35] , FedSemi adapts a mean teacher approach to harvest the unlabeled data during the training process. Nevertheless, none of the discussed approaches focuses on learning audio models by utilizing devices' unlabeled audio samples. Furthermore, these approaches also introduce additional communication overhead to utilize the available on-device unlabeled data.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Iii. Methodology",
      "text": "Problem Formulation: Formally, when performing SER in a semi-supervised FL setting, each of the K devices holds a labeled set,\n\ni=1 , where N l,k is the number of labeled data samples, x li is an input instance, y i {1, • • • , C} is the corresponding label, and C is the number of label categories for the C-way multi-class classification problem. Besides, each device holds a set of unlabeled samples denoted by\n\ni=1 , where, N u,k is the number of unlabeled data samples. Here, N k = N l,k + N u,k is the total number of data samples stored on the k -th device and N l,k N u,k .",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Global Model",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Federated Training Metadata",
      "text": "is the loss term from supervised learning on the labeled data held by the k-th device, and L u θ (D k U ) represents the loss term from unsupervised learning on the unlabeled data of the same device. We add the parameter β to control the effect of unlabeled data on the training procedure, while γ k is the relative impact of the k-th device on the construction of the global model G. For the FedAvg algorithm, the parameter γ k is equal to the ratio of device's local data N k over all training samples γ k = N k N . Model Architecture: We use log-Mel spectrograms as the model's input, which we compute by applying a short-time Fourier transform on the two-second audio segment with a window size of 25 ms and a hop size equal to 10 ms to extract 64 Mel-spaced frequency bins for each window. Inspired by  [36] , to make a prediction on an audio sample, we average over all the predictions of non-overlapping segments of this sample.\n\nOur model architecture consists of four blocks. In each block, we perform two separate convolutions (i.e., one on the temporal and another on the frequency dimension), whose outputs we concatenate afterward to perform a joint 1 × 1 convolution. By doing this, the model can capture fine-grained features from each dimension and learn high-level features from their shared output. To accelerate the training process, while avoiding overfitting, we employ group normalization and spatial dropout after each convolution layer. Furthermore, we apply L2 regularization with a rate of 0.0001 in each convolution layer and utilize max-pooling to reduce the timefrequency dimensions by a factor of two between blocks, effectively reducing the number of models parameters. In addition to max-pooling, a spatial dropout rate of 0.1 was used to avoid further over-fitting. We apply ReLU as a non-linear activation function and use Adam optimizer with the learning rate of 0.001 to optimize categorical cross-entropy loss.\n\nAttention Mechanism: As emotional cues can be sparsely scattered across natural speech utterances, we utilize a spectrotemporal-channel attention mechanism, inspired by  [31] , in conjunction with our CNN blocks to improve the representational power of our model without increasing its complexity. The STC attention mechanism consists of two major components, a spectro-temporal (ST) attention to capture prosody (e.g., rhythm, pitch, and intonations) and spectral (e.g., formants and harmonics) patterns, and a channel attention to discover interactions across CNN channels.\n\nThe channel attention is obtained by first aggregating the features from each channel though GAP, and then passing them through a two-layer perceptron to construct the channel attention map, where we adjust its scale using the tanh function. Concurrently, the spectro-temporal (ST) attention is computed by concatenating two feature maps extracted from average pooling and max pooling along the channel dimension, in which we perform two separate convolutions across both spectral and temporal dimensions. To extract the STC attention weights, the three attention maps are multiplied and scaled using a SoftMax function. We produce the attention-weighted features by element-wise multiplying the extracted STC attention weights with the originally-extracted CNN features, which we then feed to the classifier. Figure  1   , where z i is the logits produced for the input sample x ui by the k-th device model p θ k before the softmax layer. In essence, Algorithm 1 Federated Self-training for SER. η is the learning rate, l and u are equally sized on-device labeled and unlabeled batches, respectively.\n\n1: Server initialization of model G with model weights θ G 0 2: for i = 1, . . . , R do 3:\n\nRandomly select K devices to participate in round i 4:\n\nfor each device k ∈ K in parallel do 5:\n\nend for 8: for batch l ∈ D L and u ∈ D U do 13:\n\nend for 16:\n\nend for 17: end procedure Φ produces categorical labels for the given \"soften\" softmax values, in which temperature scaling is applied with a constant scalar temperature T . With the help of a confidence threshold, τ , we retain only high-confidence predictions, for which we then perform standard cross-entropy minimization while using y as targets,\n\nIn contrast to  [20] , to ensure the proper utilization of unlabeled samples in SER, we present a device-specific threshold τ , which follows a cosine scheduler that considers both the total number of rounds being completed and the device's specific participation in the FL procedure. Formally, the device-specific confidence threshold is defined as:\n\n, where R is the total number of rounds the federated algorithm will be performed, C and C s are the current total number of completed rounds of the federated procedure globally, and the current number of completed rounds by the specific device, respectively. Additionally, we control the effect of devices' participation in τ with δ, which we set to 0.5. Thus, we ensure that we utilize solely high-confidence predictions when learning from unlabeled dataset D k U , even under low participation rate of devices. Further details and an overview of our approach for the federated self-training procedure can be found in Algorithm 1.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Iv. Evaluation",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Datasets:",
      "text": "We conducted experiments with the IEMOCAP database  [26] , a widely used dataset across literature  [11] . It contains around 12 hours of audio data, sampled at 16 kHz and is composed of five dyadic sessions where actors perform improvisations or scripted scenarios, specifically to represent the emotional expressions. Since the improvised corpus is closer to natural speech and can elicit more intense emotions, we use only the improvised raw audio samples from the dataset. Additionally, as most papers on SER have targeted the improvised corpus, with a focus on the detection of four core emotion -Neutral, Happy, Sad and Angry  [28] ,  [32] ,  [33] , we use these 4 emotions to be able to compare our results. Simulation Environment and Setup: To simulate a federated environment, we use the Flower framework  [37]  and utilize FedAvg  [15]  as an optimization algorithm to construct the global model from devices' local updates. We select a number of parameters to control the federated settings of our experiments. These parameters are: 1) K -number of devices, 2) R -number of rounds, 3) q -devices' participation percentage in each round, 4) E -number of local training steps per round, 5) σ -variance of data distribution across devices, 6) L -dataset's percentage to be used as labeled samples, 7) β -influence of unlabeled data over training process, 8) T -temperature scaling parameter, and 9) τ -predictions' confidence threshold. Across all semisupervised experiments, we utilized the federated self-training algorithm, as presented in Section III, where we fixed T = 2 and set τ to initialize from 0.5 and gradually increase to a maximum of 0.9 during training, following Equation  3 .\n\nData partitioning across devices and into labeled and unlabeled subsets during our semi-supervised experiments plays a key role in performing a realistic evaluation  [38] . In our experiments where the creation of a labeled subset from the original is required (L <100%), we keep the dataset's initial class distribution ratio to avoid tempering with dataset characteristics. Likewise, in our semi-supervised federated approach, the unlabeled subset consists of the dataset's remaining samples after extracting the labeled samples.\n\nExperiments: To evaluate performance of our SER model in a federated setting, we performed four distinct experiments, each of which having a different splitting of on-device data. In particular, based on the placement (or ownership) of the distributed devices, the data partitioning among devices can be either random or in a per-speaker basis. The first corresponds to devices situated in common areas, while the latter to a scenario where devices belong to specific users (e.g., smartphones and computers). Furthermore, SER tasks can also be categorized based on the overlap of speakers between the training and test sets. The case of no overlap of speaker, namely speaker independent SER, has proven especially challenging, since we evaluate the ability of the models to generalize to speakers with different characteristics  [11] . In addition, an overlapping scenario can frequently occur in workplaces (e.g., offices and laboratories) where the number of speakers is fixed. Combining these aforementioned criteria (i.e., placement of distributed devices and speaker overlap between train and test To compare our work with recent state-of-the-art approaches, we followed an evaluation strategy similar to  [28] ,  [32] ,  [39] . In particular, for speaker independent experiments, we utilized leave-one-session-out (LOSO) cross-validation strategy. In speaker overlapping experiments, we randomly divide our data into training and test sets using an 80:20 ratio with a 5-fold Cross-Validation (5-fold CV) strategy, resulting in 5 distinct train-test sets. It is important to note that since the number of utterances is large enough, each subset will retain the same class distribution as the original data set. We further manage any randomness during the data partitioning and training procedures by passing a specific seed, while we perform five distinct trials (or runs, i.e., training a model from scratch) in each particular splitting, and the average Unweighted Accuracy (UA) over all five runs is reported across our results. The key characteristics of our federated experiments are presented in Table  I .\n\nCentralized vs. Federated SER: Since, to the best of our knowledge, no federated learning-based SER exists, we compare our model with state-of-the-art centralized SER approaches to establish an upper bound of performance of our model and to determine the feasibility of performing SER in a federated setting. To this end, we use attentionbased state-of-the-art architectures, such as recurrent neural networks (BLSTM and CRNN) and CNN's, which often rely on transfer learning and augmentations to improve their performance. However, these approaches require long training procedures and large-scale models. Thus, they are unfit for federated settings, where devices operate under low energy and computation constrains. We perform experiments in fullysupervised settings, both in a centralized and federated fashion,  to construct a high-quality supervised baseline. In federated settings, the FL parameters were set to R=100, q=80%, E=1, σ=25%,L=100%, and K=10. The resulting unweighted accuracy on the test set for centralized and federated settings are presented in Table  II .\n\nIn Table  II , we observe that our model, despite its simplicity, is within 2.5% from recent state-of-the art approaches, which rely on additional techniques (e.g., transfer learning, data augmentations) to improve their performance. We note that in Speaker Overlap experiments, the accuracy in both centralized and supervised federated settings is increased substantially, verifying that generalization is still an open research question in SER. Moving from centralized to federated settings, we notice that our model is able to retain high level of accuracy, with an accuracy gap between the centralized and federated models of approximately 4% and 2% for Speaker Independent and Speaker Overlap experiments, respectively.This indicates that utilizing FL to develop highly accurate privacy-preserving SER systems is feasible. Furthermore, to clearly demonstrate the performance of federated SER models, a confusion matrix for speaker independent experiments is presented in Figure  2 . From the 4 emotions, we note that the angry emotion has the largest misclassification rate. Considering that audio samples expressing an angry emotion are on average over 4.5 seconds long and since our model utilizes audio segments of two seconds, there might be difficulties to detect the particular emotional state.\n\nEffect of Low On-Device Label Data on Federated SER: We also evaluated performance of our SER model when ondevice labeled data are scarce. For this, we employed federated self-training to determine the obtained improvements versus a fully-supervised federated approach, where we only utilized 10% of the available data as labeled samples (L=10%). To illustrate the performance gain of our federated self-training approach over the supervised FL regime, we performed fullysupervised experiments with identical labeled subsets, where, the unlabeled instances remained unexploited.\n\nIn Table  III , comparing the two approaches for L=10%, we note that our method utilize on-device unlabeled data to improve the model's performance significantly across all four experiments, with an average increase on accuracy by 8.67%. Additionally, for the non-i.i.d. experiments, we notice that both supervised and semi-supervised models retain their accuracy, with the latter to improve the recognition rate by 1.75% on average. These results suggest that SER is feasible in federated settings, even under highly non-i.i.d. speakers distributions. If we recollect that in FL, such non-i.i.d. distributions pose significant challenges during training  [13] , being unrestrained by those challenges is yet another reason to learn federated SER models and build privacy-preserving systems.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "V. Conclusion",
      "text": "We proposed a privacy-preserving SER model by utilizing Federated Learning. To eliminate the assumption of abundant labeled data availability on devices, we utilize a data-efficient federated self-training method to learn SER models with few on-device labeled samples. From our evaluation, we demonstrate that our models' accuracy is consistently superior to fully supervised federated settings under the same labeled data availability. In addition, contrary to other audio recognition tasks, we show that highly non-i.i.d. speaker distributions have minor effect in federated SER models'performance.",
      "page_start": 6,
      "page_end": 6
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Proposed attention-based CNN architecture and Federated Self-Training",
      "page": 3
    },
    {
      "caption": "Figure 1: illustrates an overview",
      "page": 3
    },
    {
      "caption": "Figure 2: Confusion Matrix of Speaker Independent Federated SER",
      "page": 5
    },
    {
      "caption": "Figure 2: From the 4 emotions, we note that the angry emotion has the",
      "page": 5
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Raw 1-sec Audio Data\nLog-Mel Spectogram\nConvolution Block\nSpartialDropOut2D\n     STC Attention\nSoftMax Classifier\nProbability\nModel Architecture\n4x\nPredictions\nSTFT\nClasses": ""
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Federated\nSelf-Training": "",
          "Federated\nCentral \nLearning\nServer\nGlobal\nModel\nFederated Training  \nMetadata\nOn-Device \nLearning\nAudio\n...\n...\n...\n...\n...\nData\nLocal \nAudio \nModels": ""
        }
      ],
      "page": 3
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Two-stream emotion recognition for call center monitoring",
      "authors": [
        "P Gupta",
        "N Rajput"
      ],
      "year": "2007",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "2",
      "title": "Deep features-based speech emotion recognition for smart affective services",
      "authors": [
        "A Badshah",
        "N Rahim",
        "N Ullah",
        "J Ahmad",
        "K Muhammad",
        "M Lee",
        "S Kwon",
        "S Baik"
      ],
      "year": "2019",
      "venue": "Deep features-based speech emotion recognition for smart affective services"
    },
    {
      "citation_id": "3",
      "title": "Automatic curation of sports highlights using multimodal excitement features",
      "authors": [
        "M Merler",
        "K.-N Mac",
        "D Joshi",
        "Q.-B Nguyen",
        "S Hammer",
        "J Kent",
        "J Xiong",
        "M Do",
        "J Smith",
        "R Feris"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "4",
      "title": "Emotion-awareness for intelligent vehicle assistants: A research agenda",
      "authors": [
        "H.-J Vögel",
        "R Troncy",
        "B Huet",
        "M Önen",
        "A Ksentini",
        "J Conradt",
        "A Adi",
        "A Zadorojniy",
        "J Terken",
        "J Beskow",
        "A Morrison",
        "C Süß",
        "K Eng",
        "F Eyben",
        "S Al Moubayed",
        "S Müller",
        "T Hubregtsen",
        "V Ghaderi",
        "R Chadowitz",
        "J Härri"
      ],
      "year": "2018",
      "venue": "Emotion-awareness for intelligent vehicle assistants: A research agenda"
    },
    {
      "citation_id": "5",
      "title": "The hourglass model revisited",
      "authors": [
        "Y Susanto",
        "A Livingstone",
        "B Ng",
        "E Cambria"
      ],
      "year": "2020",
      "venue": "IEEE Intelligent Systems"
    },
    {
      "citation_id": "6",
      "title": "Survey on speech emotion recognition: Features, classification schemes, and databases",
      "authors": [
        "M Ayadi",
        "M Kamel",
        "F Karray"
      ],
      "year": "2011",
      "venue": "Pattern Recognition"
    },
    {
      "citation_id": "7",
      "title": "Speech emotion recognition using deep neural network and extreme learning machine",
      "authors": [
        "K Han",
        "D Yu",
        "I Tashev"
      ],
      "year": "2014",
      "venue": "Speech emotion recognition using deep neural network and extreme learning machine"
    },
    {
      "citation_id": "8",
      "title": "Residual attention network for image classification",
      "authors": [
        "F Wang",
        "M Jiang",
        "C Qian",
        "S Yang",
        "C Li",
        "H Zhang",
        "X Wang",
        "X Tang"
      ],
      "year": "2017",
      "venue": "CoRR"
    },
    {
      "citation_id": "9",
      "title": "Weak-attention suppression for transformer based speech recognition",
      "authors": [
        "Y Shi",
        "Y Wang",
        "C Wu",
        "C Fuegen",
        "F Zhang",
        "D Le",
        "C.-F Yeh",
        "M Seltzer"
      ],
      "year": "2020",
      "venue": "Weak-attention suppression for transformer based speech recognition"
    },
    {
      "citation_id": "10",
      "title": "Attention is all you need",
      "authors": [
        "A Vaswani",
        "N Shazeer",
        "N Parmar",
        "J Uszkoreit",
        "L Jones",
        "A Gomez",
        "L Kaiser",
        "I Polosukhin"
      ],
      "year": "2017",
      "venue": "Attention is all you need"
    },
    {
      "citation_id": "11",
      "title": "A review on speech emotion recognition using deep learning and attention mechanism",
      "authors": [
        "E Lieskovska",
        "M Jakubec",
        "R Jarina",
        "M Chmulik"
      ],
      "venue": "Electronics"
    },
    {
      "citation_id": "12",
      "title": "Neural machine translation by jointly learning to align and translate",
      "authors": [
        "D Bahdanau",
        "K Cho",
        "Y Bengio"
      ],
      "year": "2015",
      "venue": "CoRR"
    },
    {
      "citation_id": "13",
      "title": "Federated learning: Challenges, methods, and future directions",
      "authors": [
        "T Li",
        "A Sahu",
        "A Talwalkar",
        "V Smith"
      ],
      "year": "2020",
      "venue": "IEEE Signal Processing Magazine"
    },
    {
      "citation_id": "14",
      "title": "Federated learning: Strategies for improving communication efficiency",
      "authors": [
        "J Konečný",
        "H Mcmahan",
        "F Yu",
        "P Richtárik",
        "A Suresh",
        "D Bacon"
      ],
      "year": "2017",
      "venue": "Federated learning: Strategies for improving communication efficiency"
    },
    {
      "citation_id": "15",
      "title": "Communication-efficient learning of deep networks from decentralized data",
      "authors": [
        "H Mcmahan",
        "E Moore",
        "D Ramage",
        "S Hampson",
        "B Arcas"
      ],
      "year": "2017",
      "venue": "AISTATS"
    },
    {
      "citation_id": "16",
      "title": "Federated learning for keyword spotting",
      "authors": [
        "D Leroy",
        "A Coucke",
        "T Lavril",
        "T Gisselbrecht",
        "J Dureau"
      ],
      "year": "2019",
      "venue": "Federated learning for keyword spotting"
    },
    {
      "citation_id": "17",
      "title": "Training keyword spotting models on non-iid data with federated learning",
      "authors": [
        "A Hard",
        "K Partridge",
        "C Nguyen",
        "N Subrahmanya",
        "A Shah",
        "P Zhu",
        "I Moreno",
        "R Mathews"
      ],
      "year": "2020",
      "venue": "Training keyword spotting models on non-iid data with federated learning"
    },
    {
      "citation_id": "18",
      "title": "Applied federated learning: Improving google keyboard query suggestions",
      "authors": [
        "T Yang",
        "G Andrew",
        "H Eichner",
        "H Sun",
        "W Li",
        "N Kong",
        "D Ramage",
        "F Beaufays"
      ],
      "year": "2018",
      "venue": "Applied federated learning: Improving google keyboard query suggestions"
    },
    {
      "citation_id": "19",
      "title": "Towards utilizing unlabeled data in federated learning: A survey and prospective",
      "authors": [
        "Y Jin",
        "X Wei",
        "Y Liu",
        "Q Yang"
      ],
      "year": "2020",
      "venue": "Towards utilizing unlabeled data in federated learning: A survey and prospective"
    },
    {
      "citation_id": "20",
      "title": "Federated self-training for semi-supervised audio recognition",
      "authors": [
        "V Tsouvalas",
        "A Saeed",
        "T Ozcelebi"
      ],
      "year": "2021",
      "venue": "CoRR"
    },
    {
      "citation_id": "21",
      "title": "Introduction to Semi-Supervised Learning, 01",
      "authors": [
        "X Zhu",
        "A Goldberg"
      ],
      "year": "2009",
      "venue": "Introduction to Semi-Supervised Learning, 01"
    },
    {
      "citation_id": "22",
      "title": "Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks",
      "authors": [
        "D.-H Lee"
      ],
      "year": "2013",
      "venue": "Workshop on challenges in representation learning"
    },
    {
      "citation_id": "23",
      "title": "Pseudo-labeling and confirmation bias in deep semi-supervised learning",
      "authors": [
        "E Arazo",
        "D Ortego",
        "P Albert",
        "N O'connor",
        "K Mcguinness"
      ],
      "venue": "2020 International Joint Conference on Neural Networks (IJCNN)"
    },
    {
      "citation_id": "24",
      "title": "Distilling the knowledge in a neural network",
      "authors": [
        "G Hinton",
        "O Vinyals",
        "J Dean"
      ],
      "year": "2015",
      "venue": "Distilling the knowledge in a neural network"
    },
    {
      "citation_id": "25",
      "title": "Copypaste: An augmentation method for speech emotion recognition",
      "authors": [
        "R Pappagari",
        "J Villalba",
        "P Zelasko",
        "L Moro-Velázquez",
        "N Dehak"
      ],
      "year": "2020",
      "venue": "CoRR"
    },
    {
      "citation_id": "26",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Provost",
        "J Kim",
        "S Chang",
        "S Lee",
        "Narayanan"
      ],
      "year": "2008",
      "venue": "Language Resources and Evaluation"
    },
    {
      "citation_id": "27",
      "title": "High-level feature representation using recurrent neural network for speech emotion recognition",
      "authors": [
        "J Lee",
        "I Tashev"
      ],
      "venue": "Interspeech 2015. ISCA -International Speech Communication Association"
    },
    {
      "citation_id": "28",
      "title": "Automatic speech emotion recognition using recurrent neural networks with local attention",
      "authors": [
        "S Mirsamadi",
        "E Barsoum",
        "C Zhang"
      ],
      "year": "2017",
      "venue": "2017 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "29",
      "title": "Attentive convolutional neural network based speech emotion recognition: A study on the impact of input features, signal length, and acted speech",
      "authors": [
        "M Neumann",
        "N Vu"
      ],
      "year": "2017",
      "venue": "CoRR"
    },
    {
      "citation_id": "30",
      "title": "An attention pooling based representation learning method for speech emotion recognition",
      "authors": [
        "P Li",
        "Y Song",
        "I Mcloughlin",
        "W Guo",
        "L Dai"
      ],
      "year": "2018",
      "venue": "An attention pooling based representation learning method for speech emotion recognition"
    },
    {
      "citation_id": "31",
      "title": "Representation learning with spectro-temporal-channel attention for speech emotion recognition",
      "authors": [
        "L Guo",
        "L Wang",
        "C Xu",
        "J Dang",
        "E Chng",
        "H Li"
      ],
      "venue": "Representation learning with spectro-temporal-channel attention for speech emotion recognition"
    },
    {
      "citation_id": "32",
      "title": "Attention based fully convolutional network for speech emotion recognition",
      "authors": [
        "Y Zhang",
        "J Du",
        "Z Wang",
        "J Zhang"
      ],
      "year": "2018",
      "venue": "CoRR"
    },
    {
      "citation_id": "33",
      "title": "Improved speech emotion recognition using transfer learning and spectrogram augmentation",
      "authors": [
        "S Padi",
        "S Sadjadi",
        "D Manocha",
        "R Sriram"
      ],
      "year": "2021",
      "venue": "Improved speech emotion recognition using transfer learning and spectrogram augmentation"
    },
    {
      "citation_id": "34",
      "title": "Federated semisupervised learning with inter-client consistency",
      "authors": [
        "W Jeong",
        "J Yoon",
        "E Yang",
        "S Hwang"
      ],
      "year": "2020",
      "venue": "Federated semisupervised learning with inter-client consistency"
    },
    {
      "citation_id": "35",
      "title": "Fedsemi: An adaptive federated semi-supervised learning framework",
      "authors": [
        "Z Long",
        "L Che",
        "Y Wang",
        "M Ye",
        "J Luo",
        "J Wu",
        "H Xiao",
        "F Ma"
      ],
      "year": "2020",
      "venue": "Fedsemi: An adaptive federated semi-supervised learning framework"
    },
    {
      "citation_id": "36",
      "title": "Selfsupervised audio representation learning for mobile devices",
      "authors": [
        "M Tagliasacchi",
        "B Gfeller",
        "F Quitry",
        "D Roblek"
      ],
      "year": "2019",
      "venue": "Selfsupervised audio representation learning for mobile devices",
      "arxiv": "arXiv:1905.11796"
    },
    {
      "citation_id": "37",
      "title": "Flower: A friendly federated learning research framework",
      "authors": [
        "D Beutel",
        "T Topal",
        "A Mathur",
        "X Qiu",
        "T Parcollet",
        "N Lane"
      ],
      "year": "2020",
      "venue": "ArXiv"
    },
    {
      "citation_id": "38",
      "title": "Realistic evaluation of deep semi-supervised learning algorithms",
      "authors": [
        "A Oliver",
        "A Odena",
        "C Raffel",
        "E Cubuk",
        "I Goodfellow"
      ],
      "year": "2019",
      "venue": "Realistic evaluation of deep semi-supervised learning algorithms"
    },
    {
      "citation_id": "39",
      "title": "Self-attention for speech emotion recognition",
      "authors": [
        "L Tarantino",
        "P Garner",
        "A Lazaridis"
      ],
      "year": "2019",
      "venue": "Self-attention for speech emotion recognition"
    }
  ]
}