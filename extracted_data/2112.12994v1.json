{
  "paper_id": "2112.12994v1",
  "title": "Toeplitz Least Squares Problems, Fast Algorithms And Big Data",
  "published": "2021-12-24T08:32:09Z",
  "authors": [
    "Ali Eshragh",
    "Oliver Di Pietro",
    "Michael A. Saunders"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "In time series analysis, when fitting an autoregressive model, one must solve a Toeplitz ordinary least squares problem numerous times to find an appropriate model, which can severely affect computational times with large data sets. Two recent algorithms (LSAR and Repeated Halving) have applied randomized numerical linear algebra (RandNLA) techniques to fitting an autoregressive model to big time-series data. We investigate and compare the quality of these two approximation algorithms on largescale synthetic and real-world data. While both algorithms display comparable results for synthetic datasets, the LSAR algorithm appears to be more robust when applied to real-world time series data. We conclude that RandNLA is effective in the context of big-data time series.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Advancements in technology and computation have led to enormous data sets being generated from various fields of research including science, internet datasets and business. These data sets, commonly described as Big Data, are stored in the form of vectors and matrices, allowing us to draw on our knowledge of linear algebra to analyze them. The enormity of Big Data matrices has mandated the search for large-scale matrix algorithms with improved run times and stability  [8] .\n\nRandomised Numerical Linear Algebra (RandNLA) is a new tool to deal with big data  [24] . RandNLA utilises random sampling of elements, rows, or columns of a matrix to Definition 1 (Ordinary Least Squares Problem). An Ordinary Least Squares (OLS) problem with inputs A∈ IR n×d and b ∈ IR d solves the minimisation problem\n\nThe solution to this minimisation problem is well known and shown in Theorem 1  [17] .\n\nTheorem 1 (Solution to OLS problem). The optimal solution of the OLS minimisation problem in Definition 1 satisfies the normal equation\n\nwhich always has a solution. If A A is nonsingular, x is unique. Otherwise, the unique solution of minimum norm x can be found via the singular value decomposition of A.",
      "page_start": 1,
      "page_end": 3
    },
    {
      "section_name": "Solving Large Ols Problems Via Randnla",
      "text": "Randomised Numerical Linear Algebra (RandNLA) is a new tool to deal with big data. It utilises random sampling of the elements or rows or columns of a matrix to produce a second smaller (compressed) matrix that is similar to the first matrix in some way, yet computationally easier to deal with  [9] .\n\nIn this section we look at the application of RandNLA to OLS regression. We consider again the system of linear equations Ax = b with A ∈ IR n×d , x ∈ IR d , b ∈ IR n and n > d. In essence, RandNLA methods for OLS problems involve the appropriate choice of matrix S ∈ IR c×n to perform some form of sampling (according to a chosen distribution) and/or pre-processing operation. We are able to compress our data matrix A ∈ IR n×d into a smaller matrix SA ∈ IR c×d that will ideally lead to similar results. We replace the OLS problem (Definition 1) by a compressed least squares approximation problem  [27]  min\n\n(1)\n\nA solution x s to the smaller problem (1) can be found using a direct method such as QR factorization of the matrix SA, giving an approximation to x such that\n\nwhere ε > 0. Since we have a randomised algorithm, there is some probability δ < 1 (depending on ε > 0) with which the algorithm will fail, i.e.,  (2)  will not be satisfied. As mentioned, RandNLA methods for OLS problems depend on the choice of S. One may argue that the simplest choice for S performs uniform random sampling on the rows of A  [17] . Unfortunately, while this can be achieved quite easily and quickly, uniform sampling strategies perform poorly because of nonuniformity in the rows of A.\n\nThere are two ways to address this problem  [27] . A data-independent (or data-oblivious) approach, such as Algorithm 1, involves some kind of preprocessing (or preconditioning) of matrix A that transforms it in order to make it more uniform. Random sampling can then be applied to this uniform, transformed A. A second data-aware approach (such as using leverage scores) involves weighting the rows of A so that rows with more information, in some sense, are randomly sampled with higher probability.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Data-Oblivious Approach: Sampled Randomised Hadamard Transform",
      "text": "Drineas and Mahoney  [9]  present a data-oblivious method for OLS called Sampled Randomised Hadamard Transform (SRHT). As discussed, a data-oblivious method overcomes nonuniformity in the rows of A by preprocessing A in some way. The Randomised Hadamard Transform (RHT) H m D performs this role. This is the product of H m ∈ IR n×n (defined in Definition 2) and the diagonal matrix D ∈ IR n×n with D ii equal to 1 or -1 with probability 1 2 . Using the RHT to uniformise A has the advantage of being quite fast: O (n log 2 n) time to compute a vector H m Dx, or O (n log 2 c) if we only want to access c elements of vector H m Dx (as we do when sampling).\n\nDefinition 2 (Hadamard Transform). For some m > 0, the Hadamard transform (normalised), denoted H m ∈ IR n×n , n = 2 m+1 , is defined recursively with H 0 = 1 and\n\nFollowing preprocessing, a uniform sampling matrix S ∈ IR c×n is applied. This matrix is given in the sampling-and-rescaling form, but it may be implemented implicitly in practice through simply sampling the rows. Thus, when sampling and preprocessing are applied, we arrive at a smaller OLS problem min\n\nTheorem 2 (Number of Rows to Sample in SRHT Algorithm  [9] ). Let x * s be an optimal solution to (3). If the ideal number of sampled rows is given by c = max 48 2 d ln (40nd) ln 100 2 d ln (40nd) , 40d ln 40nd /ε ,\n\nthen for some\n\nTheorem 2 ensures that x s satisfies (2) with probability at least 0.8. The SRHT algorithm is described in Algorithm 1.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Algorithm 1 Srht Algorithm [9]",
      "text": "Require: A ∈ IR n×d , b ∈ IR n , error parameter ε ∈ (0, 1) 1: Let c be given by (4); 2: Let S be an empty matrix; Select uniformly i at random an integer from 1, 2, . . . , n;",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "5:",
      "text": "Append the row vector ( n/c)e i to S. 6: end for 7: Let H m ∈ IR n×n be the normalised Hadamard transform matrix; 8: Let D ∈ IR n×n be a diagonal matrix with\n\n+1, with probability 1 2 ; -1, with probability 1 2 ;\n\nReturn: x s , the solution of the OLS problem min",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Data-Aware Approach: Leverage Scores-Based Random Sampling",
      "text": "As discussed in Section 2.1, an alternative to the data-oblivious approach is data-aware approaches, in which information from the data matrix is assessed before sampling to determine which rows are deemed (in some sense) more important and thus more ideal to be selected in the sampling procedure. In particular, leverage score sampling is a common way to assess the importance of a row. In general terms, a statistical leverage score measures how far the values of an observation are from other observations. Definition 3 presents a more precise definition of leverage scores.\n\nDefinition 3 (Leverage Score). Given matrix A ∈ IR n×d with n ≥ d and rank(A) = d, the i th leverage score corresponding to the i th row of A is given by the i th diagonal entry of A(A A) -1 A ; that is,\n\nIt can be shown that (i) ≥ 0 for all i and m i=1 (i) = d, and so we can construct a probability distribution π over the rows of A by\n\nSampling according to leverage scores thus involves randomly selecting and rescaling rows of A proportional to their leverage scores. In terms of our sampling and rescaling formalism (1), S ∈ IR c×n is constructed by randomly choosing each row (with replacement) from the n × n identity matrix according to the nonuniform distribution  (6)   Computing leverage scores as in Definition 3 is more costly than solving the original OLS problem. However, as we see in the coming section, one can find approximate leverage scores cheaply. The LSAR algorithm  [17]  utilises approximate leverage scores, and the REPEATEDHALVING  [25]  algorithm utilises generalised leverage scores with respect to a smaller approximate matrix. The downside of approximate leverage scores is that they mis-estimate the true leverage scores by some factor 0 < β ≤ 1, that is (i) ≥ β (i), for i = 1, . . . , m. This leads to a trade-off between speed and accuracy.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Toeplitz Ols Problems For Time-Series Data",
      "text": "A Toeplitz Ordinary Least Squares (TOLS) problem is an OLS problem min\n\nin which T is a Toeplitz matrix as given in Definition 4. As we can see, there are only n + p -1 distinct numbers. This is useful for computation and storage of Toeplitz matrices.\n\nMotivation. Toeplitz matrices arise in a wide range of problems in both pure mathematics (such as algebra, combinatorics, differential geometry, etc.) and applied mathematics (approximation theory, image processing, time series analysis, etc.)  [29] . In particular, fitting an AR(p) model (see  Definition 5)  to time series data requires solving a TOLS problem for multiple possible orders of p. Given the present ubiquity of data, one is often required to fit an AR model to very large time series data sets, referred to as big time-series data. This TOLS problem quickly becomes a computational bottleneck because of the need to solve (7) repeatedly for different values of p. However, by utilising the unique structure of Toeplitz matrices and methods from RandNLA, some superfast (i.e., faster than O(np)) algorithms have been developed recently for approximating solutions to TOLS problems. In the following sections we examine some of these algorithms.\n\nDefinition 4 (Toeplitz Matrix). A Toeplitz matrix has the form\n\nwhere Y t is a stationary time series with mean zero, φ 1 , φ 2 , . . . , φ p are the regression parameters with φ p = 0, and W t is a Gaussian white noise series, i.e., each W t is an independent and identically distributed normal random with mean 0 and variance σ 2 W .\n\nGiven a set of time series observations y 1 , . . . , y n , if we wish to fit an AR(p) model, we need to find auto-regression parameters φ 1 , φ 2 , . . . , φ p . Finding these parameters exactly by the method of maximum likelihood estimates (i.e., by maximising the log-likelihood function) can be shown to be intractable and must be solved numerically  [21] . However, finding the log-likelihood function of the parameters conditional (CMLE) on the first p observations is an alternative approach for large samples, equivalent to obtaining the parameters from an OLS problem that regresses y t on p of its own lagged values  [21] .\n\nParameters\n\n. . .\n\nwhere X n,p is a Toeplitz matrix, often referred to as the data matrix. Thus we arrive at a TOLS problem, where the order p is an unknown parameter that must be estimated, typically using the partial autocorrelation function (PACF).\n\nDefinition 6 (Partial Autocorrelation Function). The PACF of a stationary time series {Y t ; t = 0, ±1, ±2, ...} at lag h is defined by\n\nwhere ρ denotes the correlation function and Y t and Y t+h are the linear regression of Y t and Y t+h on {X t+1 , . . . , X t+h-1 }.\n\nOrder p is estimated by selecting the last lag at which the PACF is nonzero. In the algorithm, τ h denotes the PACF value estimated at lag h using the CMLE of the parameters φ 1 , φ 2 , . . . , φ p (i.e., through solving the associated TOLS problem). Also, τ h denotes the PACF value using the CMLE of the parameters when based on the compressed (sampled and re-scaled) OLS problem.\n\nRemark 1. Estimating the order p requires solving the TOLS regression problem  (7)  multiple times with different p. In the context of big time-series data, these TOLS problems are the computational bottleneck.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Lsar Algorithm",
      "text": "Eshragh et al.  [17]  develop a fast algorithm called LSAR for estimating the leverage scores of an autoregressive model in big time-series data. To mitigate the computational complexity of solving numerous OLS problems, LSAR utilises a data-aware RandNLA sampling routine based on leverage scores. In this section we discuss how LSAR relates to Toeplitz OLS problems and then how to perform the algorithm.\n\nAs stated in Section 2.1.2, calculating leverage scores exactly could be computationally costly. However, Eshragh et al.  [17]  developed an efficient approximation to estimate the leverage scores recursively, as presented in Definition 7.\n\nDefinition 7 (Approximate Leverage Scores). For an AR(p) model with p ≥ 1, the fully-approximate leverage scores are given by the recursion\n\nwhere\n\nand φ n-1,p-1 is the solution of the OLS problem with inputs X n-1,p-1 ∈ IR c×(p-1) and y n-1,p-1 ∈ IR c . Here, X n-1,p-1 and y n-1,p-1 are compressed data from (8a) sampled according to the leverage score distribution\n\nThe first and second cases are given respectively by the exact leverage score,\n\nand the quasi-approximate leverage scores,\n\nand the remainder are evaluated recursively. Note that rn,p := X n,p φn,py n,p , where φn,p is the vector of OLS parameters computed from the sampled and re-scaled problem.\n\nIt can be shown that the approximate leverage scores misestimate the true leverage scores by some factor 0 < β ≤ 1, that is, (i) ≥ β (i), for i = 1, . . . , m. Hence, the choice of c (the number of rows to sample from X n-1,p-1 ) is given by\n\nwhere β can be shown to be 1 -O (p √ ε), and ε and δ are given by (2). The LSAR algorithm is given in Algorithm 3.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Repeated Halving Algorithm",
      "text": "The Repeated Halving (RH) algorithm was first given in  [6] . It is a data-aware, samplingbased procedure that returns C ∈ IR c×(d+1) , a spectral approximation of X = [T , b] where T is a Toeplitz matrix. A spectral approximation preserves the magnitude of matrix-vector multiplication, and also preserves the singular values of the matrix  [6] . Estimate PACF at lag p, i.e., τ p ;",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "5:",
      "text": "Compute the approximate leverage scores m,p (i) for i = 1, . . . , m -p as in (10a);",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "6:",
      "text": "Compute the sampling distribution π m,p (i) for i = 1, . . . , m -p as in (10c);",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "7:",
      "text": "Set c as in  (11) ;\n\nForm S ∈ IR c×m by randomly choosing c rows of the corresponding identity matrix according to the probability distribution π with replacement and rescaling factor 1/ √ cπ i ;\n\n9:\n\nConstruct the sampled data matrix X m,p = SX m,p and response vector y m,p = Sy m,p ;\n\nThe RH algorithm recursively computes a spectral approximation C of C , using the steps outlined in  [6]  and shown in Algorithm 4. The algorithm utilises generalised leverage scores with respect to a spectral approximation as a way mitigate the cost of calculating leverage scores in full.\n\nDefinition 9 (Generalised Leverage Score  [6] ). Let C and B be two matrices with the same number of columns, where B has full column rank. The i th generalised leverage score corresponding to the i th row of C with respect to B is defined to be\n\nFurthermore, the approximate generalised leverage score is given by\n\nwhere G is a random Gaussian matrix with O (log n) rows.\n\nNote that the i th generalised leverage score of A with respect to A is just the leverage score as defined in Definition 3.\n\nUniform sampling to approximate a matrix leads to approximate leverage scores that are good enough for sampling  [6] . Following recursive computation of the spectral approximation, a standard leverage score sampling procedure using approximate generalised leverage scores { ˜ B (i)} for i = 1, . . . , n in (  13 ) samples c = O ((d log d)/ε 2 ) rows of C with probability proportional to its leverage score to form C. We utilise the sampling and rescaling formulation of the sampling procedure to form C = SC, where the t th row of S ∈ IR c×n is e i / √ p i(t)\n\nif the i th row of C is sampled in the t th trial. Rows of C are sampled with probability\n\nThe validity of Algorithm 4 follows from Theorem 3. If we set m = n/2 we achieve a uniformly sampled matrix A u with n/2 rows, which if used to calculate approximate leverage scores Au (i) will lead to a spectral approximation Ã with O (d log d) rows. As A u may still be quite large, in the same manner we can recursively sample n/2 rows of A u to produce spectral approximations in an iterative manner. The RH algorithm is given in Algorithm 4.\n\nTheorem 4 (Time Complexity of RH Algorithm  [25] ). Given T ∈ IR n×d , b ∈ IR n , accuracy 0 < ε < 1, and probability of failure 0 < δ < 1, x * s satisfying (2) with probability of at least 1 -δ can be found in total time O (n log 2 (n) + poly(d log(n/ε))) log(1/δ) , where poly is a polynomial function.\n\nNote that to fit an AR(p) model, we need to solve a TOLS problem repeatedly O (p) times. Using the RH algorithm Algorithm 4 to fit an AR(p) model is considered by  [25]  and is achieved Algorithm 4 Repeated Halving Algorithm  [6]  Require: X = [T , b] ∈ IR n×(d+1) , where T is a Toeplitz matrix;\n\n1: Uniformly sample n/2 rows of X to form X 1 ; 2: Set i = 1; 3: while X i has greater than O (d log d) rows do",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "4:",
      "text": "Set i = i + 1;",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "5:",
      "text": "Uniformly sample n/2 rows of X i-1 to form X i ; 6: end while 7: while i ≥ 1 do 8:\n\nApproximate generalised leverage scores of X i w.r.t. Xi+1 by replacing B with Xi+1 in (13); Use these estimates to sample rows of X i to form Xi ; 11: end while Return: X ∈ IR c×(d+1) , a spectral approximation consisting of c = O (d log d) re-scaled rows of X.\n\nby first running Algorithm 4 on the data matrix X n-p,p (as in (  8 )) to obtain leverage scores to form a spectral approximation of X. Then we run the LSAR algorithm (Algorithm 3) except we replace the leverage scores obtained in step 5 with leverage scores obtained by Algorithm 4.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Numerical Results",
      "text": "In this section, we implement the LSAR and RH algorithms (often referred to as compressed algorithms) on some time series data, both synthetically generated and real, to investigate the quality and run time of the algorithms. Calculations utilising the full data matrix (referred to as the exact computation or algorithm) are also used to compare run time and error. The algorithms are implemented in MATLAB R2020b on a 64-bit Windows operating system with a 1.8GHz processor and 16GB of RAM. All numerical experiments are performed with double precision. Code is included to measure the computation time (using the tic and toc MATLAB functions) and accuracy.\n\nAll numerical results show the potential of both algorithms, which use compressed data, to provide comparable accuracy and utility to that of the existing alternative using the entire data set. Further, by using compressed data matrices, the algorithms are able to produce these results in considerably less time then the alternative, exact method.\n\nThe numerical results are discussed in three subsections where we report the computation time of each algorithm and the accuracy of the estimated parameters, and compare the PACF generated by each algorithm. In Section 4.1 we present numerical analysis on synthetic data generated without outliers from a range of sizes of AR(p) time series models. In Section 4.2 we present numerical analysis on synthetic data generated with outliers. Finally, in Section 4.3 we examine the performance of these algorithms on a real data set.",
      "page_start": 12,
      "page_end": 13
    },
    {
      "section_name": "Synthetic Data Without Outliers",
      "text": "Two million realisations from six AR(p) time series models were randomly generated for p = {5, 10, 20, 50, 100, 150}.\n\nFor each AR(p) model, coefficients corresponding to a stationary time series model for each order were obtained randomly. Synthetic data was generated with a zero constant and variance of one, using the simulate function of MATLAB's econometrics toolbox. When fitting an AR model we run the algorithms over a number of lags up to some maximum value p, which we choose to be large enough to detect order p. For the synthetically generated data sets we choose p = {50, 50, 50, 100, 200, 250} to correspond respectively to the AR(p) models with p = {5, 10, 20, 50, 100, 150}.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Computational Time Of The Pacf",
      "text": "We compare the time it takes for each algorithm to find the PACF (Definition 6) for a range of lag values, h.\n\nFor each lag, finding the PACF involves solving a TOLS problem. To compare the computational time of the algorithms, the associated TOLS problem is solved at each lag using a compressed data matrix based the approximate leverage scores of the LSAR algorithm, using a compressed data matrix based on leverage scores generated by the RH algorithm and using the entire data matrix to calculate solve the TOLS problem exactly. We choose c = 2, 000 as the number of sampled rows for each of the compressed algorithms.\n\nFigure  1  compares the run time to compute the PACF by the LSAR algorithm, the RH algorithm and the exact calculation, for each AR(p) model. Time is plotted cumulatively over each lag. We can clearly verify the speed of both compressed algorithms when compared to exact computation. In particular, the difference in computation time is exemplified by Figure  1f , which presents a 700-second difference between the exact method and the two compressd methods.\n\nFor the RH and LSAR algorithms, we see that they have similar computation times, separated by a constant that is due to the RH algorithm computing leverage scores prior to the algorithms' iteration over the lag.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Estimation Quality",
      "text": "To look at the estimation quality of each algorithm we compare how well they find the maximum likelihood estimates φ of the models' parameters at each lag.\n\nWhen fitting an AR(p) model, we find the maximum likelihood estimates of φ at each lag. Each algorithm derives estimates of φ s p,h based on the compressed data matrices. We can also calculate the estimate of φ p,h exactly using the full data matrix. To examine the quality of the algorithms, we wish to look at the relative difference between each algorithm's   maximum likelihood estimates of the parameters (based on the compressed data matrices) and the estimate based on the full data matrix (the exact algorithm). For this purpose we define the relative percentage error as\n\nFigure  2  compares the average relative percentage error, at each lag, between the LSAR algorithm and the RH algorithm for each AR(p) model. Once again we have used 2 million synthetically generated data points, and the hyper-parameter c (the number of sampled rows) was 2, 000 (0.1% of the data). To smooth out the error curves, the algorithms were repeated 50 times and the mean of error at each lag was computed after excluding 5% of the data values at each end of the data set. This was done to remove outliers.\n\nAs we can see in Figure  2 , despite the LSAR and RH algorithms taking very different approaches to obtaining leverage scores for sampling the data, the difference in the resultant estimated parameters in negligible.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Pacf Plots",
      "text": "In this section, we compare the PACF plots for each AR(p) synthetic data set. The PACF plot is of primary importance in the time series analysis process. We discussed in Section 3 that in order to estimate the order p of a time series, we can use the PACF (Definition 6). The PACF plot displays the PACF at each lag as a bar graph. The order p is estimated by choosing the largest lag in the PACF plot where the corresponding PACF is outside the 95% zero confidence boundary.\n\nFigures  3  and 4  display the PACF plots generated by each algorithm, for all synthetic data sets. We estimate the PACF for each lag up to p, first using the full data matrix, then twice more using the PACF obtained by each of the compressed data matrices of the LSAR and RH algorithms respectively. For each AR(p) model we use the same data sets from Section 4.1.1, with n = 2, 000, 000 and number of sampled rows c = 2, 000. The dashed red error lines indicate the 95% zero confidence boundary.\n\nIn Figures  3  and 4  we are able to obtain a correct estimate of the order p for the generated synthetic data from each of the exact, LSAR and RH algorithms. All PACF plots generated by the compressed data matrices appear to be quite similar to the corresponding exact PACF plots. This is by sampling only 0.1% of the rows of the data matrix.\n\nThere is clearly some error at each lag of the compressed algorithms. This is particularly evident at lags greater than the order of the model, which should be closer to zero. However, while we should be aware of this error, it must be noted that it does not affect the estimation of the order in the synthetic data examples that we present. Furthermore, we are able to obtain these reasonable approximations of the PACF in a significantly reduced time when compared to the exact alternative.",
      "page_start": 16,
      "page_end": 17
    },
    {
      "section_name": "The Effect Of Sample Size",
      "text": "We recall that the LSAR and RH algorithms have hyper-parameter c, the number of rows sampled from the full data matrix to construct the compressed data matrices used by these algorithms. Choosing different values of c leads to a trade-off between run time and accuracy of the compressed algorithms. For example, if we used a larger value of c we would expect our accuracy to increase (or our error to decrease) at the expense of computation time. Figure  5  displays the computation time and relative percentage error (given by (  14 )) as c changes for a synthetically generated AR(100) data set with n = 2, 000, 000. Maximum point-wise time and error over the lags was taken for each instance of the algorithm running for c = {1000, 2000, . . . , 20, 000} and is displayed in Figures  5a  and 5b  respectively.\n\nFigure  5  confirms the discussed trade-off between time and error. Computation time appears to increase linearly as a function of c, while the error decreases.",
      "page_start": 19,
      "page_end": 19
    },
    {
      "section_name": "Synthetic Data With Outliers",
      "text": "This section follows a similar pattern to Section 4.1, except this time with the addition of outliers in the data. Once again, two million realisations from six AR(p) time series models were randomly generated for p = 5, 10, 20, 50, 100, 150 with coefficients corresponding to a stationary time series model for each order obtained randomly. Synthetic data was generated with constant 0 and variance 1. One thousand data points were randomly selected and replaced with the sum of the data point, a randomly generated number from a uniform distribution over  [-3, 3] , and a normally distributed variable with a mean of 0 and variance of 100.",
      "page_start": 19,
      "page_end": 19
    },
    {
      "section_name": "Computational Time Of The Pacf",
      "text": "Similarly to Section 4.1.1, Figure  6  compares the run time to compute the PACF by the LSAR algorithm, the RH algorithm, and the exact calculation, for each AR(p) model. Time is plotted cumulatively over each lag. The computation times of the algorithms, including the exact computations, are unaffected by the presence of outliers. We note that computation times are approximately the same as those in Section 4.1.1 because we are performing calculations on matrices of the same size. The RH and LSAR algorithms appear to have similar computation times, separated by a constant, which is due to the RH algorithm computing leverage scores prior to the algorithm's iteration over the lag. In particular, the difference in computation time is exemplified by Figure  6f , which presents a 700-second difference between the exact method and the two compressed methods.",
      "page_start": 20,
      "page_end": 20
    },
    {
      "section_name": "Estimation Quality",
      "text": "We examine the estimation quality of each algorithm, as in Section 4.1.2, by comparing how well they find the maximum likelihood estimates φ of the models' parameters at each lag. This time, the models include outliers.\n\nFigure  7  compares the relative percentage error according to  (14) , at each lag, between the LSAR algorithm and the RH algorithm for each AR(p) model. We have used 2 million synthetically generated data points with 1000 replaced by outliers as discussed in Section 4.2. The number of sampled rows c was 2, 000 (0.1% of the data). To smooth out the error curves, the algorithms were repeated 50 times and the mean of error at each lag was computed after excluding 5% of the data values at each end of the data set. This was done to remove outliers. Thus each graph pertains to the average relative percentage error.\n\nIn Figure  7 , we can again observe that despite LSAR and RH taking very different approaches to obtaining leverage scores for sampling the data, the difference between the resultant estimated parameters is negligible. There also appears to be negligible difference between the errors of the algorithms for the data sets with and without outliers. This would suggest that the presence of outliers has very little effect on the estimated parameters for each algorithm.",
      "page_start": 21,
      "page_end": 21
    },
    {
      "section_name": "Pacf Plots",
      "text": "Figures  8  and 9  display the PACF plots generated by each algorithm, for all synthetic data sets with included outliers. We estimate the PACF using the full data matrix and each of the compressed data matrices as we did in Section 4.1.3. For each AR(p) model we use the same data sets from Section 4.2.1, with n = 2, 000, 000 and number of sampled rows c = 2, 000.\n\nWe are able use the PACF plots of the compressed algorithms to correctly identify the order p of the data sets, notwithstanding some error. The plots were obtained using only 0.1% of the data. As we saw in Section 4.2.1, the algorithms took significantly less time than the exact method, and were unaffected by outliers.",
      "page_start": 22,
      "page_end": 22
    },
    {
      "section_name": "Real-World Data",
      "text": "We now test the quality and run time of the algorithms on some real-world data. We turn to data collected by Huerta et al.  [23]  studying the accuracy of electronic nose measurements. An electronic nose is an array of metal-oxide sensors capable of detecting chemicals in the air as a way of mimicking how a human or animal nose works. In this study the nose was constructed from eight different metal-oxide sensors, as well as humidity and temperature sensors. Measurements from each of these sensors were taken simultaneously at a rate of one observation per second for a period of almost two years in one of the author's home. Huerta et al. were able to use a statistical model utilising measurements from the nose to discriminate between different gasses with an R-squared close to 1.\n\nThe data was obtained from the UCI machine learning repository  [22] . We look specifically at measurements of the eighth metal-oxide sensor (column R8 in the data set). The data set has n = 919, 438 observations, and we must transform the data by taking the logarithm and difference in one lag to obtain a stationary data set.\n\nAs we have for our synthetic data sets, we compare the run time, error and PACF plots for each of the LSAR algorithm, the RH algorithm and the exact calculation. The algorithms are run with p = 100, and the number of rows sampled for each of the compressed algorithms was s = 0.01n = 9194.\n\nFigure  10  displays the run time and the error of the estimated maximum likelihood error for the LSAR algorithm, the RH algorithm, and the exact calculation on the gas sensor data.\n\nTo smooth out the error curves in Figure  10b , the algorithms were repeated 50 times and the mean of error at each lag was computed after excluding 5% of the data values at each end of the data set. This was done to remove outliers.  Each plot corresponds to the gas sensor data of  [23] .\n\nThe run times of all three algorithms, shown in Figure  10a , tell a similar story to that of Sections 4.1.1 and 4.2.1. However, Figure  10b  displays a different pattern of error from what we have previously seen. Instead of steadily rising and being of similar magnitude to the LSAR algorithm, the RH algorithm jumps to 10% error in the first lag before steadily rising. On the other hand, error in the parameters of LSAR algorithm is robust and consistent with the numerical results that we have presented in the synthetically generated data sections.\n\nThis pattern of errors in the estimated parameters is reflected in the PACF plots produced by each of the algorithms (displayed in Figure  11 ). From Figure  11b , the PACF plot generated by the LSAR algorithm excellently replicates the PACF plot of the exact algorithm, and it would seem that AR(18) would be a good fit for this data set according to both the exact and LSAR algorithms. Figure  11c  on the other hand reaches the 95% zero confidence bounds much earlier, suggesting that it would incorrectly estimate the order to fit to the data set.",
      "page_start": 25,
      "page_end": 26
    },
    {
      "section_name": "Conclusion",
      "text": "We have examined the application of RandNLA to large time-series data. To do this we compared the LSAR and RH algorithms over a range of problem sizes with synthetic data and also some real data. As expected, because the algorithms solve subproblems with significantly smaller data matrices, the time to solve OLS problems associated with fitting an AR model was considerably reduced. In addition, the errors of the estimated model parameters were small and similar for both algorithms for each of the synthetic data sets. When applied to real time-series data, the two algorithms again had comparable run time; however, the LSAR algorithm elicited less error when estimating parameters.\n\nThe low error in the estimated parameters speaks to the utility of the algorithms for fitting an AR model. The AR fitting process involves two steps: estimating the order (from the PACF), then obtaining parameters of the model with best fitting order. Utility here refers to how the PACF plots generated by the algorithms provide it with accurate and usable information. For all synthetic models the PACF plot generated could be used to identify the order of the model, give or take some noise.\n\nOverall, this paper displays the effectiveness of RandNLA in a time series context. We also see how the LSAR algorithm could provide a framework with which to adapt another Toeplitz least squares solver, the RH algorithm, to a time series context. Future work could look at adapting other Toeplitz least squares solvers such as those in  [26, 28]  to a time series context and also compare the accuracy of these solvers when the problem involves ill-conditioned matrices.",
      "page_start": 26,
      "page_end": 27
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: compares the run time to compute the PACF by the LSAR algorithm, the RH",
      "page": 13
    },
    {
      "caption": "Figure 1: f, which presents a 700-second diﬀerence between the exact method and the two",
      "page": 13
    },
    {
      "caption": "Figure 1: Figures (a) to (f) corresponding to the labeled AR(p) models, show the comparison",
      "page": 14
    },
    {
      "caption": "Figure 2: Figures (a) to (f) corresponding to the labeled AR(p) models, show the percentage",
      "page": 15
    },
    {
      "caption": "Figure 2: compares the average relative percentage error, at each lag, between the LSAR",
      "page": 16
    },
    {
      "caption": "Figure 2: , despite the LSAR and RH algorithms taking very diﬀerent",
      "page": 16
    },
    {
      "caption": "Figure 3: Figures (a) to (c), (d) to (f) and (g) to (i) correspond to randomly generated data",
      "page": 17
    },
    {
      "caption": "Figure 4: Figures (a) to (c), (d) to (f) and (g) to (i) correspond to randomly generated data",
      "page": 18
    },
    {
      "caption": "Figure 5: Figure (a) displays the run time of the LSAR algorithm (in blue) and the RH",
      "page": 19
    },
    {
      "caption": "Figure 5: displays the computation time and relative percentage error (given by (14)) as",
      "page": 19
    },
    {
      "caption": "Figure 5: conﬁrms the discussed trade-oﬀbetween time and error. Computation time",
      "page": 19
    },
    {
      "caption": "Figure 6: Figures (a) to (f) corresponding to the labeled AR(p) models, compare the compu-",
      "page": 20
    },
    {
      "caption": "Figure 6: compares the run time to compute the PACF by the",
      "page": 21
    },
    {
      "caption": "Figure 6: f, which presents a 700-second diﬀerence between the exact method",
      "page": 21
    },
    {
      "caption": "Figure 7: compares the relative percentage error according to (14), at each lag, between",
      "page": 21
    },
    {
      "caption": "Figure 7: , we can again observe that despite LSAR and RH taking very diﬀerent ap-",
      "page": 21
    },
    {
      "caption": "Figure 7: Figures (a) to (f) corresponding to the labeled AR(p) models show the percentage",
      "page": 22
    },
    {
      "caption": "Figure 8: Figures (a) to (c), (d) to (f) and (g) to (i) correspond to randomly generated data",
      "page": 23
    },
    {
      "caption": "Figure 9: Figures (a) to (c), (d) to (f) and (g) to (i) correspond to randomly generated data",
      "page": 24
    },
    {
      "caption": "Figure 10: Figure (a) corresponding to the gas sensor data, shows the comparison between",
      "page": 25
    },
    {
      "caption": "Figure 10: displays the run time and the error of the estimated maximum likelihood error",
      "page": 25
    },
    {
      "caption": "Figure 10: b, the algorithms were repeated 50 times and",
      "page": 25
    },
    {
      "caption": "Figure 11: Figures (a), (b) and (c) display the exact PACF plot, the PACF plot computed",
      "page": 26
    },
    {
      "caption": "Figure 10: a, tell a similar story to that of",
      "page": 26
    },
    {
      "caption": "Figure 10: b displays a diﬀerent pattern of error from what",
      "page": 26
    },
    {
      "caption": "Figure 11: ). From Figure 11b, the PACF plot",
      "page": 26
    },
    {
      "caption": "Figure 11: c on the other hand reaches the 95% zero conﬁdence",
      "page": 26
    }
  ],
  "tables": [],
  "citations": [
    {
      "citation_id": "1",
      "title": "Demand forecasting in the presence of systematic events: Cases in capturing sales promotions",
      "authors": [
        "M Abolghasemi",
        "J Hurley",
        "A Eshragh",
        "B Fahimnia"
      ],
      "year": "2020",
      "venue": "International Journal of Production Economics"
    },
    {
      "citation_id": "2",
      "title": "On transition matrices of Markov chains corresponding to hamiltonian cycles",
      "authors": [
        "K Avrachenkov",
        "A Eshragh",
        "J Filar"
      ],
      "year": "2016",
      "venue": "Annals of Operations Research"
    },
    {
      "citation_id": "3",
      "title": "On binomial observation of continuous-time Markovian population models",
      "authors": [
        "N Bean",
        "R Elliott",
        "A Eshragh",
        "J Ross"
      ],
      "year": "2015",
      "venue": "Journal of Applied Probability"
    },
    {
      "citation_id": "4",
      "title": "Communications in statistics: Theory and methods",
      "authors": [
        "N Bean",
        "A Eshragh",
        "J Ross"
      ],
      "year": "2016",
      "venue": "Annals of Operations Research"
    },
    {
      "citation_id": "5",
      "title": "Low-rank approximation and regression in input sparsity time",
      "authors": [
        "K Clarkson",
        "D Woodruff"
      ],
      "year": "2017",
      "venue": "Journal of the ACM (JACM)"
    },
    {
      "citation_id": "6",
      "title": "Uniform sampling for matrix approximation",
      "authors": [
        "M Cohen",
        "Y Lee",
        "C Musco",
        "R Peng",
        "A Sidford"
      ],
      "year": "2015",
      "venue": "Proceedings of the 2015 Conference on Innovations in Theoretical Computer Science"
    },
    {
      "citation_id": "7",
      "title": "Fast approximation of matrix coherence and statistical leverage",
      "authors": [
        "P Drineas",
        "M Magdon-Ismail",
        "M Mahoney",
        "D Woodruff"
      ],
      "year": "2012",
      "venue": "Journal of Machine Learning Research"
    },
    {
      "citation_id": "8",
      "title": "RandNLA: randomized numerical linear algebra",
      "authors": [
        "P Drineas",
        "M Mahoney"
      ],
      "year": "2016",
      "venue": "Communications of the ACM"
    },
    {
      "citation_id": "9",
      "title": "Lectures on randomized numerical linear algebra",
      "authors": [
        "P Drineas",
        "M Mahoney"
      ],
      "year": "2017",
      "venue": "Lectures on randomized numerical linear algebra"
    },
    {
      "citation_id": "10",
      "title": "Fisher information, stochastic processes and generating functions",
      "authors": [
        "A Eshragh"
      ],
      "year": "2015",
      "venue": "Proceedings of the 21 st International Congress on Modeling and Simulation"
    },
    {
      "citation_id": "11",
      "title": "Modeling the dynamics of the COVID-19 population in Australia: A probabilistic analysis",
      "authors": [
        "A Eshragh",
        "S Alizamir",
        "P Howley",
        "E Stojanovski"
      ],
      "year": "2020",
      "venue": "Modeling the dynamics of the COVID-19 population in Australia: A probabilistic analysis"
    },
    {
      "citation_id": "12",
      "title": "Hamiltonian cycles, random walks and the geometry of the space of discounted occupational measures",
      "authors": [
        "A Eshragh",
        "J Filar"
      ],
      "year": "2011",
      "venue": "Mathematics of Operations Research"
    },
    {
      "citation_id": "13",
      "title": "A hybrid simulation-optimization algorithm for the Hamiltonian cycle problem",
      "authors": [
        "A Eshragh",
        "J Filar",
        "M Haythorpe"
      ],
      "year": "2011",
      "venue": "Annals of Operations Research"
    },
    {
      "citation_id": "14",
      "title": "A projection-adapted Cross Entropy (PACE) method for transmission network planning",
      "authors": [
        "A Eshragh",
        "J Filar",
        "A Nazari"
      ],
      "year": "2011",
      "venue": "Energy Systems"
    },
    {
      "citation_id": "15",
      "title": "The importance of environmental factors in forecasting Australian power demand. Environmental Modeling & Assessment",
      "authors": [
        "A Eshragh",
        "B Ganim",
        "T Perkins",
        "K Bandara"
      ],
      "year": "2021",
      "venue": "The importance of environmental factors in forecasting Australian power demand. Environmental Modeling & Assessment"
    },
    {
      "citation_id": "16",
      "title": "Rollage: Efficient rolling average algorithm to estimate ARMA models for big time series data",
      "authors": [
        "A Eshragh",
        "G Livingston",
        "T Mccann",
        "L Yerbury"
      ],
      "year": "2021",
      "venue": "Rollage: Efficient rolling average algorithm to estimate ARMA models for big time series data",
      "arxiv": "arXiv:2103.09175"
    },
    {
      "citation_id": "17",
      "title": "LSAR: Efficient leverage score sampling algorithm for the analysis of big time series data",
      "authors": [
        "A Eshragh",
        "F Roosta",
        "A Nazari",
        "M Mahoney"
      ],
      "year": "2019",
      "venue": "LSAR: Efficient leverage score sampling algorithm for the analysis of big time series data",
      "arxiv": "arXiv:1911.12321"
    },
    {
      "citation_id": "18",
      "title": "Performance comparison of three meta-heuristic algorithms for planning of a complex supply chain",
      "authors": [
        "B Fahimnia",
        "H Davarzani",
        "A Eshragh"
      ],
      "year": "2018",
      "venue": "Computers and Operations Research"
    },
    {
      "citation_id": "19",
      "title": "Tactical supply chain planning under a carbon tax policy scheme: A case study",
      "authors": [
        "B Fahimnia",
        "J Sarkis",
        "A Choudhary",
        "A Eshragh"
      ],
      "year": "2015",
      "venue": "International Journal of Production Economics"
    },
    {
      "citation_id": "20",
      "title": "Trade-off model for green supply chain planning: A leannessversus-greenness analysis",
      "authors": [
        "B Fahimnia",
        "J Sarkis",
        "A Eshragh"
      ],
      "year": "2015",
      "venue": "International Journal of Production Economics"
    },
    {
      "citation_id": "21",
      "title": "A new approach to the economic analysis of nonstationary time series and the business cycle",
      "authors": [
        "J Hamilton"
      ],
      "year": "1989",
      "venue": "Econometrica"
    },
    {
      "citation_id": "22",
      "title": "Gas sensors for home activity monitoring data set",
      "authors": [
        "F Huerta",
        "R Huerta"
      ],
      "year": "2016",
      "venue": "Gas sensors for home activity monitoring data set"
    },
    {
      "citation_id": "23",
      "title": "Online humidity and temperature decorrelation of chemical sensors for continuous monitoring",
      "authors": [
        "R Huerta",
        "T Mosqueiro",
        "J Fonollosa",
        "N Rulkov",
        "I Rodríguez-Luján"
      ],
      "year": "2016",
      "venue": "Online humidity and temperature decorrelation of chemical sensors for continuous monitoring"
    },
    {
      "citation_id": "24",
      "title": "Randomized algorithms for matrices and data",
      "authors": [
        "M Mahoney"
      ],
      "year": "2011",
      "venue": "Foundations and Trends® in Machine Learning"
    },
    {
      "citation_id": "25",
      "title": "Sublinear time numerical linear algebra for structured matrices",
      "authors": [
        "X Shi",
        "D Woodruff"
      ],
      "year": "2019",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "26",
      "title": "A superfast method for solving Toeplitz linear least squares problems",
      "authors": [
        "M Van Barel",
        "G Heinig",
        "P Kravanja"
      ],
      "year": "2003",
      "venue": "Linear Algebra and its Applications"
    },
    {
      "citation_id": "27",
      "title": "Sketching as a tool for numerical linear algebra",
      "authors": [
        "D Woodruff"
      ],
      "year": "2014",
      "venue": "Foundations and Trends® in Theoretical Computer Science"
    },
    {
      "citation_id": "28",
      "title": "Superfast and stable structured solvers for Toeplitz least squares via randomized sampling",
      "authors": [
        "Y Xi",
        "J Xia",
        "S Cauley",
        "V Balakrishnan"
      ],
      "venue": "SIAM Journal on Matrix Analysis and Applications"
    },
    {
      "citation_id": "29",
      "title": "Every matrix is a product of Toeplitz matrices",
      "authors": [
        "K Ye",
        "L Lim"
      ],
      "year": "2016",
      "venue": "Foundations of Computational Mathematics"
    }
  ]
}