{
  "paper_id": "2410.15403v2",
  "title": "Mmds: A Multimodal Medical Diagnosis System Integrating Image Analysis And Knowledgebased Departmental Consultation",
  "published": "2024-10-20T14:31:05Z",
  "authors": [
    "Yi Ren",
    "HanZhi Zhang",
    "Weibin Li",
    "Jun Fu",
    "Diandong Liu",
    "Tianyi Zhang",
    "Jie He",
    "Licheng Jiao"
  ],
  "keywords": [
    "Facial Paralysis Detection",
    "Multimodal Medical Model",
    "Large Language Model",
    "RAG",
    "Agent"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "We present MMDS, a system capable of recognizing medical images and patient facial details, and providing professional medical diagnoses. The system consists of two core components:The first component is the analysis of medical images and videos. We trained a specialized multimodal medical model capable of interpreting medical images and accurately analyzing patients' facial emotions and facial paralysis conditions. The model achieved an accuracy of 72.59% on the FER2013 facial emotion recognition dataset, with a 91.1% accuracy in recognizing the \" happy \" emotion. In facial paralysis recognition, the model reached an accuracy of 92%, which is 30% higher than that of GPT-4o. Based on this model, we developed a parser for analyzing facial movement videos of patients with facial paralysis, achieving precise grading of the paralysis severity. In tests on 30 videos of facial paralysis patients, the system demonstrated a grading accuracy of 83.3%.The second component is the generation of professional medical responses. We employed a large language model, integrated with a medical knowledge base, to generate professional diagnoses based on the analysis of medical images or videos. The core innovation lies in our development of a department-specific knowledge base routing management mechanism, in which the large language model categorizes data by medical departments and, during the retrieval process, determines the appropriate knowledge base to query. This significantly improves retrieval accuracy in the RAG (retrievalaugmented generation) process. This mechanism led to an average increase of 4 percentage points in accuracy for various large language models on the MedQA dataset.Our code is open-sourced and available at: https://github.com/renllll/MMDS.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Dataset Creation",
      "text": "This study aims to explore how large language models can be better applied in the medical field. In actual medical scenarios, multiple modalities of information sources are often integrated, and conclusions are drawn based on analysis and reasoning of past case experiences. Therefore, to simulate the real diagnostic process of doctors, our research focuses on the construction of medical multimodal large models and the development of a hierarchical knowledge base mechanism.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "A Training Data",
      "text": "Our training data for the medical multimodal large model comprises the following components:\n\n(1)LLaVA-Med  [27]  We utilized the llava_med_instruct_60k dataset from the LLaVA-Med project as part of our training data. This dataset is based on biomedical image and text pairs from the PubMed Central  [28]  (PMC) database. It has been rigorously screened and processed, containing 60,000 high-quality image-text pairs.\n\n(2)XrayGLM We selected the OpenI-zh dataset from the XrayGLM project, which was created by preprocessing the chest X-ray dataset from Indiana University Hospital. This dataset includes 6,423 chest medical images and their corresponding Chinese and English diagnostic reports.\n\n(3)Facial detail capture dataset  [29]  To enhance the model's ability to capture facial details of patients, we constructed a facial dataset from multiple sources. First, we selected 500 facial images of Asian individuals from the CASIA-Face dataset, 500 facial images of normal individuals from the CelebA-Dialog dataset  [30] , and 1,000 facial images of facial palsy patients from the Facial Nerve Palsy Database  [31] . These images were initially annotated using GPT-4o, followed by detailed annotation and data cleaning by specialists, resulting in our Facial Detail Capture Dataset.\n\nTo improve the model's accuracy in facial emotion recognition, we trained it using the training set of FER2013. This dataset is divided into seven emotion categories: sadness, anger, surprise, fear, happiness, disgust, and neutral, containing a total of 28,709 data samples.\n\nBy integrating the above datasets, we constructed a training dataset containing 97,132 image-text pairs, which was used to train our medical multimodal large model.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "B Multi-Department Knowledge Base Dataset",
      "text": "To enable the large language model to function as specialized doctors from different departments for professional medical Q&A, we collected up to 1GB of highquality doctor-patient dialogue data from the internet. We then used the large language model to precisely categorize the data according to different medical departments, ultimately constructing our multi-department knowledge base dataset.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "C Evaluation Datasets",
      "text": "To validate the performance of our system, we used the following evaluation datasets:\n\n1. Facial Palsy Evaluation Dataset: This dataset consists of 50 facial images from different facial palsy patients and 150 facial images of different normal individuals displaying various emotions such as Anger, Contempt, Disgust, Fear, Happy, Neutral, Sad, and Surprised. Since intense emotions can easily cause facial muscle distortion, this dataset effectively tests the model's ability to distinguish between facial palsy patients and normal individuals.\n\n2. Facial Palsy Video Evaluation Dataset: Due to the difficulty of collecting videos of facial palsy patients, we only managed to gather 30 video datasets from online sources. These videos primarily feature patients performing actions such as laughing, smiling, frowning, puckering, showing lower teeth, and closing eyes, with patients typically evaluated at levels II to III on the House-Brackmann  [32]  (H-B) scale (As shown in Table  1 ). This dataset is used to verify whether our system can accurately grade the severity of facial palsy based on patient videos. No movement at all, loss of tone, asymmetry at rest 3.FER2013: FER2013 is a dataset containing 35,887 grayscale facial images at a resolution of 48x48 pixels, labeled with seven emotion categories: anger, disgust, fear, happiness, sadness, surprise, and neutral. The dataset suffers from class imbalance, and emotion classification is challenging due to variations in facial pose, lighting, and age.\n\nThe dataset is divided into a training set, a public test set, and a private test set, and is primarily used for facial expression recognition tasks. We used 7,178 images from the test set to evaluate our model's performance on the facial expression recognition task.\n\n4. MedQA: MedQA is a large-scale open-domain question-answering dataset for the medical field, mainly composed of multiple-choice questions from professional medical qualification exams (such as the United States Medical Licensing Examination (USMLE) and other multiple-choice questions from mainland China and Taiwan. The dataset covers three languages: English, Simplified Chinese, and Traditional Chinese, with 12,723, 34,251, and 14,123 questions respectively. MedQA requires high levels of logical reasoning and prior knowledge integration, including many complex questions that need multi-hop reasoning. To provide the necessary background information for answering these questions, the dataset also includes extensive medical textbook text. We selected the test dataset with 3,426 multiple-choice questions applicable to mainland China to evaluate the performance of our system.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Iii. Proposed Methods",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "A Mmds System Architecture",
      "text": "This section describes the complete architecture of the Multimodal Hierarchical Department Consultation System, which consists of two stages:\n\nFirst Stage: When a user inputs medical images or patient videos, these inputs are processed by a medical image parser and patient video parser built around the core medical multimodal large model. This stage involves a detailed analysis of the user's facial and body conditions as well as the medical images. The detailed analysis results are then passed on to the second stage.\n\nSecond Stage: The Medical Long Agent receives and summarizes the analysis results from the first stage. The Medical Long Agent is powered by a local large language model combined with specific prompt templates. Its responsibilities include:\n\n1. Analyzing the medical image and video analysis results and the user's queries.\n\n2. Determining whether further detailed descriptions from the user are needed or guiding the medical multimodal large model to analyze additional medical image information.\n\n3. Identifying the appropriate department knowledge base to be used.\n\n4. Retrieving relevant past informationp  [33]  from the department's knowledge base based on the user's symptoms and queries.\n\n5. Acting as a specialist doctor from the relevant department, providing comprehensive answers and generating professional medical reports by combining case information, user symptoms, and user queries.\n\nThe medical report generated will be stored on blockchain nodes, sealed as historical information. When the user visits and asks questions again, the medical large model will Fig1. MMDS system architecture diagram combine historical information to provide a more professional medical report. The entire process will be recorded in an execution log, allowing doctors to intervene and verify at any time.\n\nThe complete architecture of the system is illustrated in Figure  1  B",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Mmds Analysis Of Medical Images",
      "text": "The core of the medical image parser is the medical multimodal large model. This model was fine-tuned on the InternLM-XComposer2-VL  [34]  model using LoRA  [35]  (Low-Rank Adaptation) training on a single A100 GPU, based on our collected training data for medical multimodal large models. The training parameters are listed in Table  2 .\n\nThis model has the capability to analyze medical images, analyze users' facial emotions, and interpret facial images of patients to identify the presence of facial palsy.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "C Mmds Analysis Of Medical Videos",
      "text": "Figure  2  shows the detailed process of our medical video parser, using the example of analyzing a video uploaded by a facial palsy patient. Our medical video parser consists of five modules: (i) multimodal preprocessing, (ii) external data collection, (iii) second-level frame video description generation, (iv) generation of complete video description script, and (v) generation of professional medical report. Each module is described in detail below.\n\n(i) Multimodal Preprocessing  [36] : Starting with the input video file, the video parser automatically uses ASR  [37, 38]  tools to transcribe the speech in the video into text. Then, the video parser extracts 1 to 2 frames per second from the input video, which are then analyzed by the medical multimodal large model.\n\n(ii) External Data Collection: For the incoming video, we collect the user's historical information, including previous conversation and symptom data, as well as some descriptive information about the video as external knowledge sources. This integrated information is included in the input prompts for both the analysis of images by the medical multimodal large model and the final generation of the complete video description by the medical large language model.\n\n(iii) Second-level Frame Video Description Generation: For each second-level image, we designed specific prompt templates and used multiple queries to analyze each image's information.\n\n(iv) Generation of Complete Video Description Script: At this stage, the video parser uses the medical large language Fig2. MMDS analysis process for medical videos model to integrate detailed analyses of each image, the corresponding time information, and some external video description data to generate a comprehensive summary of the user's video. (v) Generation of Professional Medical Report: At this stage, the video parser uses the medical large language model to generate a detailed medical report by integrating the user ' s video description  [39] , user requests, historical information, and specific prompt templates.Additionally, these second-level video descriptions are integrated into a video script for easy query and verification in the future.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "D Multi-Department Knowledge Base Routing Management Mechanism Driven By Large Models",
      "text": "This section introduces another core component of the MMDS system: a multi-department knowledge base routing management mechanism driven by large models.\n\nThe process begins with using a large language model to generate a medical question-and-answer dataset based on medical text data. This dataset is then classified according to a predefined list of medical departments. Once classified, the data is stored in knowledge bases named after each corresponding department. After analyzing medical images and videos, the analysis results are sent to our locally deployed large language model. The model first determines whether the current data is sufficient for making a decision or if more detailed information is needed. Once the necessary data is collected, the model autonomously identifies which department's knowledge base should be consulted.\n\nOnce the appropriate department is identified, our system automatically retrieves relevant medical cases from the corresponding knowledge base. The local large model then acts as the department's doctor, reading through the relevant medical cases and the user's detailed data. Step by step, the model produces a comprehensive medical report.\n\nThe process flow is illustrated in Figure  3 , and the algorithm workflow is depicted in Figure  4 .\n\nIn this system, we fine-tuned the bge-large-zh-v1.5  [40]  and BERT  [41]  models based on the collected multidepartment knowledge base dataset. These models serve as the medical recall and rank models, respectively. They are used to finely retrieve the necessary cases based on the user's queries.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Iv. Experiments And Analysis",
      "text": "",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "A Emotion Recognition Task",
      "text": "We evaluated the accuracy of our medical large model on the FER2013 test dataset, which contains a total of 7,178 images. The confusion matrix results are shown in Figure  5 , where A, B, C, D, E, F, and G represent the emotion categories of sadness, anger, surprise, fear, happiness, disgust, and neutral, respectively. Additionally, we compared various classic models with our model on the emotion recognition task, and the results are shown in Table  3 .   [45]  71.38 Inception  [46]  71.60 ResNet  [47]  72.40",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Mmds",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "72.59",
      "text": "In this experiment, we compared the performance of different methods on the emotion recognition task. As shown in Table  3 , we tested a variety of models, including CNN, GoogleNet, VGG+SVM, Conv+Inception layer, Bag of Words, Attentional ConvNet, CNN+SVM, ARM (ResNet-18), Inception, ResNet, and MMDS. Among these models, MMDS achieved the highest accuracy rate, reaching 72.59%.\n\nSpecifically, the traditional Convolutional Neural Network (CNN) method achieved an accuracy of 62.44% on the emotion recognition task, demonstrating relatively weak performance. As the network structure became more complex, the performance of GoogleNet and VGG+SVM improved, reaching 65.20% and 66.31%, respectively. The Conv model with an added Inception layer further improved the accuracy to 66.40%, indicating that modular designs in deep networks (such as the Inception layer) can better capture emotionrelated features. In more advanced models, the Bag of Words and Attentional ConvNet models achieved accuracy rates of 67.40% and 70.02%, respectively. Notably, the Attentional In deeper network models, ARM (ResNet-18) and ResNet achieved accuracy rates of 71.38% and 72.40%, respectively. In particular, the deeper ResNet model effectively avoided gradient vanishing issues, leading to more stable and precise performance in complex emotion recognition tasks. The Inception model achieved an accuracy of 71.60%, further demonstrating the effectiveness of modular network structures. Finally, the proposed MMDS system, which includes a large multimodal model, achieved the highest accuracy of 72.59% in this experiment, surpassing all other compared models. The success of the MMDS model can be attributed to its multimodal fusion design, which integrates different feature types, demonstrating exceptional generalization capabilities and precision in emotion recognition tasks.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Fig5. The Confusion Matrix Of Mmds In Recognizing Various Emotions.",
      "text": "To further analyze the classification performance of each emotion category, we plotted the confusion matrix (see Figure  5 ). From the confusion matrix, we can see that MM performed best in recognizing the emotion categories of \"happiness\" and \"surprise,\" achieving accuracy rates of 91.1% and 80.27%, respectively. This indicates that the MMDS model is highly sensitive and accurate in recognizing emotions with high intensity.\n\nHowever, the model's performance was slightly weaker in recognizing emotions like \"fear\" and \"anger,\" with accuracy rates of 49.62% and 72.095%, respectively. This may be due to the complexity of these emotions' expressions in real-world scenarios, making them more prone to confusion with other emotions. In particular, in the classification of \"fear,\" the confusion matrix shows a high misclassification rate, with many \"fear\" samples being misclassified as \"sadness\" or \"neutral,\" further highlighting the challenges of feature extraction for this emotion category.\n\nIn addition, the model performed reasonably well in the \"neutral\" category, achieving an accuracy rate of 77.445%, indicating that MMDS can accurately capture features associated with non-emotional states. As for the \"disgust\" category, although the sample size for this category was relatively small, the model still achieved an accuracy of 61.33%, demonstrating its strong generalization capabilities.\n\nOverall, the MMDS model achieved excellent performance in the emotion recognition task, especially in the \"happiness\" and \"surprise\" categories. Despite some limitations in recognizing the \"fear\" category, the MMDS model is capable of accurately capturing facial details and successfully completing multiple facial analysis tasks.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "B Mmds Facial Palsy Recognition Accuracy",
      "text": "We evaluated our medical multimodal large model, along with a batch of the most advanced multimodal large models, on our constructed facial palsy evaluation dataset. The results are shown in Table  4 .\n\nAs seen in the table, among the untrained models, GPT-4o performed the best, with an overall accuracy of 63%. It had an accuracy of 88% in recognizing facial palsy patients but only 54.67% accuracy in identifying normal individuals, often mistakenly identifying normal individuals displaying different emotional expressions as having facial palsy. In contrast, our model achieved an overall accuracy of 92%, with errors occurring only when identifying emotional expressions in normal individuals, but still maintaining a 90.67% accuracy rate in this regard, surpassing GPT-4o by 30% overall.  In this section, we will detail the architecture and experimental results of our medical video parser. For useruploaded videos, the medical video parser extracts frames per second and then analyzes each image to determine whether the patient has facial palsy. If more than half of the frames indicate facial palsy, the system proceeds with a detailed analysis of the video. The parsing process for videos of facial palsy patients is shown in Figure  6 .\n\nTo further validate the performance of our system, we tested it using videos collected from 30 patients with facial paralysis. The overall accuracy of our system on the test dataset was 83.33%, with the accuracy for each level shown in Figure  7 .\n\nAs illustrated in the figure, the accuracy for HBGS I and HBGS VI levels was 100%. However, there were errors in recognizing levels II to V, with level IV showing an accuracy of only 60%. This indicates that video descriptions for levels II to V have many similarities, making it challenging to distinguish between these levels accurately. Detailed and comprehensive video descriptions are required for better differentiation. Using video descriptions for grading facial palsy patients is more logical, explanatory, and provides a more intuitive basis for medical diagnosis and recommendations.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "D Accuracy Of Multi-Department Knowledge Base Management Mechanism On Medqa",
      "text": "In this study, we evaluated a series of state-of-the-art large language models using the mainland China dataset from MedQA to test the graded management mechanism of medical knowledge bases. These models include various models from different research institutions and companies, covering both API and weight access methods. The evaluation results are shown in Table  5 .\n\nAs can be seen from the table, the performance of the Qwen2-7B-instruct model improved from 80.64 to 84.41 after introducing the classified RAG mechanism, demonstrating the effectiveness of this mechanism in complex knowledge base scenarios. Additionally, the InternLM2-Chat-7B model also showed improvement after introducing the classified RAG, with its score increasing from 53.47 to 57.78. Compared to closed-source models accessed via API, locally managed models under the graded knowledge base mechanism showed comparable accuracy. For example, the accuracy of GLM-4 reached 84.73, only 0.32 higher than the local model, while the best closed-source model, GPT-4O, achieved an accuracy of 86.05, only 1.64 higher than the local model. The accuracy of DeepSeekv2 was even 1.71 lower than the local model. However, when using the traditional RAG mechanism without graded management, the performance of the Qwen2-7B-instruct  [49]  model decreased from 80.64 to 76.07.  To analyze the reasons behind this result, we projected the knowledge base data of each department and the test data from MedQA_zh into the vector space using our trained medical embedding model. The results are shown in Figure  8 .\n\nIn the figure, the red represents the test data, while the other colors represent the datasets from different departments. As observed, there is a significant overlap among many departmental datasets, indicating semantic similarity in the vector space. Given that our knowledge base dataset is as large as 1GB, this overlap can easily lead to substantial data conflicts. This is why, under the traditional RAG mechanism, it is difficult to recall high-quality data. Conversely, the graded management of the knowledge base effectively alleviates data conflicts and enables the recall of relatively high-quality data, thereby significantly improving the quality of the model's responses.As seen in the figure, our model equipped with the department-managed knowledge base achieves higher scores in almost every department compared to the baseline models, further proving the advantage of our mechanism.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "V. Conclusion",
      "text": "In this paper, we introduced MMDS, a comprehensive system capable of recognizing medical images and patient facial details to provide professional medical diagnoses. The system comprises two core components: a specialized multimodal medical model for analyzing medical images and videos, and a large language model integrated with a medical knowledge base for generating expert medical responses.\n\nOur multimodal medical model demonstrated high accuracy in facial emotion recognition, achieving a 72.59% accuracy rate on the FER2013 dataset and excelling particularly in recognizing the \"happy\" emotion with a 91.1% accuracy. In facial paralysis recognition, the model reached an impressive 92% accuracy, outperforming GPT-4o by 30%. By developing a parser to analyze facial movement videos of patients with facial paralysis, we achieved precise grading of paralysis severity, with the system demonstrating an 83.3% grading accuracy in tests on 30 patient videos.\n\nThe second component of MMDS leverages a large language model augmented with a department-specific knowledge base routing management mechanism. This innovation allows the model to categorize data by medical departments and determine the appropriate knowledge base during retrieval, significantly improving retrieval accuracy in the RAG (retrieval-augmented generation) process. This mechanism led to an average increase of 4 percentage points in accuracy for various large language models on the MedQA dataset.\n\nThe results highlight MMDS's potential as a valuable tool in medical diagnostics, offering accurate image analysis and professional diagnostic capabilities. The system's ability to integrate multimodal data and generate expert-level responses can assist healthcare professionals in making informed decisions. Future work will focus on expanding the system's capabilities to cover a broader range of medical conditions and further refining the knowledge base routing mechanism to enhance diagnostic accuracy.\n\nOur code is open-sourced and available at https://github.com/renllll/MMDS. We encourage the research community to utilize and build upon our work to advance the field of medical image analysis and automated diagnosis.",
      "page_start": 10,
      "page_end": 10
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: B MMDS Analysis of Medical Images",
      "page": 4
    },
    {
      "caption": "Figure 2: shows the detailed process of our medical video",
      "page": 4
    },
    {
      "caption": "Figure 4: In this system, we fine-tuned the bge-large-zh-v1.5[40]",
      "page": 5
    },
    {
      "caption": "Figure 5: ). From the confusion matrix, we can see that MM",
      "page": 7
    },
    {
      "caption": "Figure 6: To further validate the performance of our system, we",
      "page": 8
    },
    {
      "caption": "Figure 7: As illustrated in the figure, the accuracy for HBGS I and",
      "page": 8
    },
    {
      "caption": "Figure 8: In the figure, the red represents the test data, while the",
      "page": 9
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Grade": "I",
          "Description": "Normal Function",
          "Characteristics": "Normal facial\nfunction in all areas"
        },
        {
          "Grade": "II",
          "Description": "Mild Dysfunction",
          "Characteristics": "Slight weakness\nnoticeable only on\nclose inspection,\nmay have very slight\nsynkinesis"
        },
        {
          "Grade": "III",
          "Description": "Moderate\nDysfunction",
          "Characteristics": "Obvious but not\ndisfiguring\ndifference between\nthe two sides,\nnoticeable but not\nsevere synkinesis"
        },
        {
          "Grade": "IV",
          "Description": "Moderately Severe\nDysfunction",
          "Characteristics": "Obvious weakness\nand/or disfiguring\nasymmetry, severe\nsynkinesis,\nincomplete eye\nclosure"
        },
        {
          "Grade": "V",
          "Description": "Severe Dysfunction",
          "Characteristics": "Barely perceptible\nmovement,\nasymmetry at rest,\nno effective\nmovement of the\nface"
        },
        {
          "Grade": "VI",
          "Description": "Total Paralysis",
          "Characteristics": "No movement at all,\nloss of tone,\nasymmetry at rest"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Hyper parameter": "Precision",
          "Value": "fp16"
        },
        {
          "Hyper parameter": "Epochs",
          "Value": "6"
        },
        {
          "Hyper parameter": "Max length",
          "Value": "4096"
        },
        {
          "Hyper parameter": "Batch size",
          "Value": "8"
        },
        {
          "Hyper parameter": "Weight_decay",
          "Value": "0.1"
        },
        {
          "Hyper parameter": "Warmup_ratio",
          "Value": "0.01"
        },
        {
          "Hyper parameter": "Learning rate",
          "Value": "5e-5"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 3: The confusion matrix of MMDS in recognizing",
      "data": [
        {
          "Method": "CNN",
          "Accuracy Rate": "62.44"
        },
        {
          "Method": "GoogleNet[42]",
          "Accuracy Rate": "65.20"
        },
        {
          "Method": "VGG+SVM",
          "Accuracy Rate": "66.31"
        },
        {
          "Method": "Conv+Inception layer",
          "Accuracy Rate": "66.40"
        },
        {
          "Method": "Bag of Words [43]",
          "Accuracy Rate": "67.40"
        },
        {
          "Method": "Attentional ConvNet 44]",
          "Accuracy Rate": "70.02"
        },
        {
          "Method": "CNN + SVM",
          "Accuracy Rate": "71.20"
        },
        {
          "Method": "ARM (ResNet-18)[45]",
          "Accuracy Rate": "71.38"
        },
        {
          "Method": "Inception [46]",
          "Accuracy Rate": "71.60"
        },
        {
          "Method": "ResNet [47]",
          "Accuracy Rate": "72.40"
        },
        {
          "Method": "MMDS",
          "Accuracy Rate": "72.59"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Model": "MMDS",
          "Facial\nPalsy\nPatients": "100",
          "Normal\nIndividuals": "90.67",
          "Overall\nAccuracy": "93"
        },
        {
          "Model": "Gpt4o",
          "Facial\nPalsy\nPatients": "88",
          "Normal\nIndividuals": "54.67",
          "Overall\nAccuracy": "63"
        },
        {
          "Model": "Gpt4o-mini",
          "Facial\nPalsy\nPatients": "80",
          "Normal\nIndividuals": "26",
          "Overall\nAccuracy": "39.5"
        },
        {
          "Model": "Glm4[48]",
          "Facial\nPalsy\nPatients": "96",
          "Normal\nIndividuals": "6.67",
          "Overall\nAccuracy": "29"
        },
        {
          "Model": "Internlm-\nXcomposer2-\nVl-7b",
          "Facial\nPalsy\nPatients": "20",
          "Normal\nIndividuals": "66.67",
          "Overall\nAccuracy": "55"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Model": "GPT-4o-mini",
          "Creater": "OpenAI",
          "#Parameters": "undisclosed",
          "Access": "API",
          "MedQA\n(Mainland)": "75.37"
        },
        {
          "Model": "GPT-4o",
          "Creater": "OpenAI",
          "#Parameters": "undisclosed",
          "Access": "API",
          "MedQA\n(Mainland)": "86.05"
        },
        {
          "Model": "DeepSeek-V2[50]",
          "Creater": "DeepSeek-AI",
          "#Parameters": "236B",
          "Access": "API",
          "MedQA\n(Mainland)": "82.70"
        },
        {
          "Model": "GLM-4",
          "Creater": "Zhipu AI",
          "#Parameters": "undisclosed",
          "Access": "API",
          "MedQA\n(Mainland)": "84.73"
        },
        {
          "Model": "GLM-4-9B-chat",
          "Creater": "Zhipu AI",
          "#Parameters": "9B",
          "Access": "Weights",
          "MedQA\n(Mainland)": "80.93"
        },
        {
          "Model": "InternLM2-Chat-7B",
          "Creater": "Shanghai AI Lab",
          "#Parameters": "7B",
          "Access": "Weights",
          "MedQA\n(Mainland)": "53.47"
        },
        {
          "Model": "InternLM2-Chat-7B-\nclassification-RAG",
          "Creater": "Shanghai AI Lab",
          "#Parameters": "7B",
          "Access": "Weights",
          "MedQA\n(Mainland)": "57.78"
        },
        {
          "Model": "Qwen2-7B-instruct",
          "Creater": "Alibaba Group",
          "#Parameters": "7B",
          "Access": "Weights",
          "MedQA\n(Mainland)": "80.64"
        },
        {
          "Model": "Qwen2-7B-instruct+rag",
          "Creater": "Alibaba Group",
          "#Parameters": "7B",
          "Access": "Weights",
          "MedQA\n(Mainland)": "76.07"
        },
        {
          "Model": "Qwen2-7B-\ninstruct+classification_rag",
          "Creater": "Alibaba Group",
          "#Parameters": "7B",
          "Access": "Weights",
          "MedQA\n(Mainland)": "84.41"
        }
      ],
      "page": 9
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Xi'an Science and Technology Plan Project",
      "venue": "Xi'an Science and Technology Plan Project"
    },
    {
      "citation_id": "2",
      "title": "Training language models to follow instructions with human feedback[J]. Advances in neural information processing systems",
      "authors": [
        "L Ouyang",
        "J Wu",
        "X Jiang"
      ],
      "year": "2022",
      "venue": "Training language models to follow instructions with human feedback[J]. Advances in neural information processing systems"
    },
    {
      "citation_id": "3",
      "title": "The llama 3 herd of models",
      "authors": [
        "A Dubey",
        "A Jauhri",
        "A Pandey"
      ],
      "year": "2024",
      "venue": "The llama 3 herd of models",
      "arxiv": "arXiv:2407.21783"
    },
    {
      "citation_id": "4",
      "title": "Benchmarking of Commercial Large Language Models: ChatGPT, Mistral, and Llama",
      "authors": [
        "G Hou",
        "Q Lian"
      ],
      "year": "2024",
      "venue": "Benchmarking of Commercial Large Language Models: ChatGPT, Mistral, and Llama"
    },
    {
      "citation_id": "5",
      "title": "Qwen technical report",
      "authors": [
        "J Bai",
        "S Bai",
        "Y Chu"
      ],
      "year": "2023",
      "venue": "Qwen technical report",
      "arxiv": "arXiv:2309.16609"
    },
    {
      "citation_id": "6",
      "title": "WaterGPT: Training a large language model to become a hydrology expert",
      "authors": [
        "Y Ren",
        "T Zhang",
        "X Dong"
      ],
      "year": "2024",
      "venue": "SSRN"
    },
    {
      "citation_id": "7",
      "title": "Super-resolution water body extraction based on MF-SegFormer",
      "authors": [
        "T Zhang",
        "W Li",
        "X Feng"
      ],
      "year": "2024",
      "venue": "IGARSS 2024-2024 IEEE International Geoscience and Remote Sensing Symposium"
    },
    {
      "citation_id": "8",
      "title": "RemoteCLIP: A vision language foundation model for remote sensing",
      "authors": [
        "F Liu",
        "D Chen",
        "Z Guan"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Geoscience and Remote Sensing"
    },
    {
      "citation_id": "9",
      "title": "RS5M: A large scale vision-language dataset for remote sensing visionlanguage foundation model",
      "authors": [
        "Z Zhang",
        "T Zhao",
        "Y Guo"
      ],
      "year": "2023",
      "venue": "RS5M: A large scale vision-language dataset for remote sensing visionlanguage foundation model",
      "arxiv": "arXiv:2306.11300"
    },
    {
      "citation_id": "10",
      "title": "Geochat: Grounded large vision-language model for remote sensing",
      "authors": [
        "K Kuckreja",
        "M Danish",
        "M Naseer"
      ],
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "11",
      "title": "A Novel Adaptive Fine-Tuning Algorithm for Multimodal Models: Self-Optimizing Classification and Selection of High-Quality Datasets in Remote Sensing",
      "authors": [
        "Y Ren",
        "T Zhang",
        "Z Han"
      ],
      "year": "2024",
      "venue": "A Novel Adaptive Fine-Tuning Algorithm for Multimodal Models: Self-Optimizing Classification and Selection of High-Quality Datasets in Remote Sensing",
      "arxiv": "arXiv:2409.13345"
    },
    {
      "citation_id": "12",
      "title": "Large language models encode clinical knowledge",
      "authors": [
        "K Singhal"
      ],
      "year": "2022",
      "venue": "Nature"
    },
    {
      "citation_id": "13",
      "title": "ChatDoctor: A Medical Chat Model Fine-Tuned on a Large Language Model Meta-AI (LLaMA) Using Medical Domain Knowledge",
      "authors": [
        "Yunxiang Li"
      ],
      "year": "2023",
      "venue": "Cureus"
    },
    {
      "citation_id": "14",
      "title": "HuaTuo: Tuning LLaMA Model with Chinese Medical Knowledge",
      "authors": [
        "Hao Wang"
      ],
      "year": "2023",
      "venue": "HuaTuo: Tuning LLaMA Model with Chinese Medical Knowledge"
    },
    {
      "citation_id": "15",
      "title": "MedGPT: Medical Concept Prediction from Clinical Narratives",
      "authors": [
        "Zeljko Kraljevic"
      ],
      "year": "2021",
      "venue": "MedGPT: Medical Concept Prediction from Clinical Narratives"
    },
    {
      "citation_id": "16",
      "title": "Direct Preference Optimization: Your Language Model is Secretly a Reward Model",
      "authors": [
        "Rafael Rafailov"
      ],
      "year": "2023",
      "venue": "Direct Preference Optimization: Your Language Model is Secretly a Reward Model"
    },
    {
      "citation_id": "17",
      "title": "",
      "authors": [
        "Rongsheng Wang",
        "Yaofei Duan",
        "Junrong Li",
        "Patrick Pang",
        "Tao Tan",
        "Xrayglm"
      ],
      "venue": ""
    },
    {
      "citation_id": "18",
      "title": "MIMIC-CXR-JPG, a large publicly available database of labeled chest radiographs",
      "authors": [
        "Alistair Johnson"
      ],
      "year": "2019",
      "venue": "MIMIC-CXR-JPG, a large publicly available database of labeled chest radiographs"
    },
    {
      "citation_id": "19",
      "title": "Design and Development of a Multimodal Biomedical Information Retrieval System",
      "authors": [
        "Dina Demner-Fushman"
      ],
      "year": "2012",
      "venue": "J. Comput. Sci. Eng"
    },
    {
      "citation_id": "20",
      "title": "LLaVA-Med: Training a Large Language-and-Vision Assistant for Biomedicine in One Day",
      "authors": [
        "Chunyuan Li"
      ],
      "year": "2023",
      "venue": "LLaVA-Med: Training a Large Language-and-Vision Assistant for Biomedicine in One Day"
    },
    {
      "citation_id": "21",
      "title": "Towards Generalist Foundation Model for Radiology",
      "authors": [
        "Chaoyi Wu"
      ],
      "year": "2023",
      "venue": "Towards Generalist Foundation Model for Radiology"
    },
    {
      "citation_id": "22",
      "title": "Challenges in representation learning: A report on three machine learning contests",
      "authors": [
        "I J Goodfellow",
        "D Erhan",
        "P L Carrier"
      ],
      "year": "2013",
      "venue": "Neural information processing: 20th international conference"
    },
    {
      "citation_id": "23",
      "title": "Retrieval-augmented generation for knowledge-intensive nlp tasks[J]",
      "authors": [
        "P Lewis",
        "E Perez",
        "A Piktus"
      ],
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "24",
      "title": "What Disease does this Patient Have? A Largescale Open Domain Question Answering Dataset from Medical Exams",
      "authors": [
        "Di Jin"
      ],
      "year": "2020",
      "venue": "What Disease does this Patient Have? A Largescale Open Domain Question Answering Dataset from Medical Exams"
    },
    {
      "citation_id": "25",
      "title": "MedAgents: Large Language Models as Collaborators for Zero-shot Medical",
      "authors": [
        "Xiangru Tang"
      ],
      "venue": "MedAgents: Large Language Models as Collaborators for Zero-shot Medical"
    },
    {
      "citation_id": "26",
      "title": "ArXiv abs/2311",
      "year": "2023",
      "venue": "ArXiv abs/2311"
    },
    {
      "citation_id": "27",
      "title": "Agent Hospital: A Simulacrum of Hospital with Evolvable Medical Agents",
      "authors": [
        "Junkai Li"
      ],
      "year": "2024",
      "venue": "Agent Hospital: A Simulacrum of Hospital with Evolvable Medical Agents"
    },
    {
      "citation_id": "28",
      "title": "Aloe: A Family of Finetuned Open Healthcare LLMs",
      "authors": [
        "Ashwin Gururajan",
        "Kumar"
      ],
      "year": "2024",
      "venue": "Aloe: A Family of Finetuned Open Healthcare LLMs"
    },
    {
      "citation_id": "29",
      "title": "LLaVA-Med: Training a Large Language-and-Vision Assistant for Biomedicine in One Day",
      "authors": [
        "Chunyuan Li"
      ],
      "year": "2023",
      "venue": "LLaVA-Med: Training a Large Language-and-Vision Assistant for Biomedicine in One Day"
    },
    {
      "citation_id": "30",
      "title": "Large-scale domain-specific pretraining for biomedical vision-language processing",
      "authors": [
        "Sheng Zhang",
        "Yanbo Xu",
        "Naoto Usuyama",
        "Jaspreet Bagga",
        "Robert Tinn",
        "Sam Preston",
        "Rajesh Rao",
        "Mu Wei",
        "Naveen Valluri",
        "Cliff Wong"
      ],
      "year": "2023",
      "venue": "Large-scale domain-specific pretraining for biomedical vision-language processing",
      "arxiv": "arXiv:2303.00915"
    },
    {
      "citation_id": "31",
      "title": "CASIA-Face-Africa: A Large-Scale African Face Image Database",
      "authors": [
        "Jawad Muhammad"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Information Forensics and Security"
    },
    {
      "citation_id": "32",
      "title": "Talk-to-Edit: Fine-Grained Facial Editing via Dialog",
      "authors": [
        "Yuming Jiang"
      ],
      "year": "2021",
      "venue": "2021 IEEE/CVF International Conference on Computer Vision (ICCV)"
    },
    {
      "citation_id": "33",
      "title": "Deep Hierarchical Network With Line Segment Learning for Quantitative Analysis of Facial Palsy",
      "authors": [
        "G. -S Hsu",
        "J. -H Kang",
        "W. -F Huang"
      ],
      "year": "2019",
      "venue": "IEEE Access",
      "doi": "10.1109/ACCESS.2018.2884969"
    },
    {
      "citation_id": "34",
      "title": "Significance and reliability of the House-Brackmann grading system for regional facial nerve function",
      "authors": [
        "Shari Reitzen"
      ],
      "year": "2009",
      "venue": "Otolaryngology-Head and Neck Surgery"
    },
    {
      "citation_id": "35",
      "title": "A Survey on RAG Meeting LLMs: Towards Retrieval-Augmented Large Language Models",
      "authors": [
        "Wenqi Fan"
      ],
      "year": "2024",
      "venue": "A Survey on RAG Meeting LLMs: Towards Retrieval-Augmented Large Language Models"
    },
    {
      "citation_id": "36",
      "title": "InternLM-XComposer2: Mastering Free-form Text-Image Composition and Comprehension in Vision-Language Large Model",
      "authors": [
        "Xiao-Wen Dong"
      ],
      "year": "2024",
      "venue": "InternLM-XComposer2: Mastering Free-form Text-Image Composition and Comprehension in Vision-Language Large Model"
    },
    {
      "citation_id": "37",
      "title": "LoRA: Low-Rank Adaptation of Large Language Models",
      "authors": [
        "J Hu",
        "Edward"
      ],
      "year": "2021",
      "venue": "LoRA: Low-Rank Adaptation of Large Language Models"
    },
    {
      "citation_id": "38",
      "title": "VideoLLaMA 2: Advancing Spatial-Temporal Modeling and Audio Understanding in Video-LLMs",
      "authors": [
        "Zesen Cheng"
      ],
      "year": "2024",
      "venue": "VideoLLaMA 2: Advancing Spatial-Temporal Modeling and Audio Understanding in Video-LLMs"
    },
    {
      "citation_id": "39",
      "title": "Robust Speech Recognition via Large-Scale Weak Supervision",
      "authors": [
        "Alec Radford"
      ],
      "year": "2022",
      "venue": "Robust Speech Recognition via Large-Scale Weak Supervision"
    },
    {
      "citation_id": "40",
      "title": "Coqui TTS (Version 1.4)",
      "authors": [
        "G Eren",
        "The Coqui",
        "Tts Team"
      ],
      "year": "2021",
      "venue": "Coqui TTS (Version 1.4)",
      "doi": "10.5281/zenodo.6334862."
    },
    {
      "citation_id": "41",
      "title": "MiniGPT-Med: Large Language Model as a General Interface for Radiology Diagnosis",
      "authors": [
        "Asma Alkhaldi"
      ],
      "year": "2024",
      "venue": "MiniGPT-Med: Large Language Model as a General Interface for Radiology Diagnosis"
    },
    {
      "citation_id": "42",
      "title": "C-Pack: Packaged Resources To Advance General Chinese Embedding",
      "authors": [
        "Xiao",
        "Shitao"
      ],
      "venue": "C-Pack: Packaged Resources To Advance General Chinese Embedding",
      "arxiv": "arXiv:2309.07597[cs.CL"
    },
    {
      "citation_id": "43",
      "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
      "authors": [
        "Jacob Devlin"
      ],
      "year": "2019",
      "venue": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"
    },
    {
      "citation_id": "44",
      "title": "Going deeper with convolutions",
      "authors": [
        "Christian Szegedy"
      ],
      "year": "2014",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR"
    },
    {
      "citation_id": "45",
      "title": "Efficient Estimation of Word Representations in Vector Space",
      "authors": [
        "Tomas Mikolov"
      ],
      "year": "2013",
      "venue": "International Conference on Learning Representations"
    },
    {
      "citation_id": "46",
      "title": "A ConvNet for the 2020s",
      "authors": [
        "Zhuang Liu"
      ],
      "year": "2022",
      "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "47",
      "title": "CMSIS-NN: Efficient Neural Network Kernels for Arm Cortex-M CPUs",
      "authors": [
        "Liangzhen Lai"
      ],
      "year": "2018",
      "venue": "CMSIS-NN: Efficient Neural Network Kernels for Arm Cortex-M CPUs"
    },
    {
      "citation_id": "48",
      "title": "Going deeper with convolutions",
      "authors": [
        "Christian Szegedy"
      ],
      "year": "2014",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR"
    },
    {
      "citation_id": "49",
      "title": "Deep Residual Learning for Image Recognition",
      "authors": [
        "Kaiming He"
      ],
      "year": "2015",
      "venue": "2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR"
    },
    {
      "citation_id": "50",
      "title": "ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools",
      "authors": [
        "Team Zeng",
        "Glm Aohan"
      ],
      "year": "2024",
      "venue": "ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools"
    },
    {
      "citation_id": "51",
      "title": "Qwen2 Technical Report",
      "authors": [
        "An Yang"
      ],
      "year": "2024",
      "venue": "Qwen2 Technical Report"
    },
    {
      "citation_id": "52",
      "title": "DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language",
      "authors": [
        "Zhihong Shao"
      ],
      "venue": "DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language"
    },
    {
      "citation_id": "53",
      "title": "",
      "authors": [
        "Model"
      ],
      "year": "2024",
      "venue": ""
    }
  ]
}