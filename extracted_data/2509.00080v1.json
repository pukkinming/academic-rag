{
  "paper_id": "2509.00080v1",
  "title": "Wrong Face, Wrong Move: The Social Dynamics Of Emotion Misperception In Agent-Based Models",
  "published": "2025-08-26T22:42:46Z",
  "authors": [
    "David Freire-Obregón"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "The ability of humans to detect and respond to others' emotions is fundamental to understanding social behavior. Here, agents are instantiated with emotion classifiers of varying accuracy to study the impact of perceptual accuracy on emergent emotional and spatial behavior. Agents are visually represented with face photos from the KDEF database and endowed with one of three classifiers trained on the JAFFE (poor), CK+ (medium), or KDEF (high) datasets. Agents communicate locally on a 2D toroidal lattice, perceiving neighbors' emotional state based on their classifier and responding with movement toward perceived positive emotions and away from perceived negative emotions. Note that the agents respond to perceived, instead of ground-truth, emotions, introducing systematic misperception and frustration. A battery of experiments is carried out on homogeneous and heterogeneous populations and scenarios with repeated emotional shocks. Results show that low-accuracy classifiers on the part of the agent reliably result in diminished trust, emotional disintegration into sadness, and disordered social organization. By contrast, the agent that develops high accuracy develops hardy emotional clusters and resilience to emotional disruptions. Even in emotionally neutral scenarios, misperception is enough to generate segregation and disintegration of cohesion. These findings underscore the fact that biases or imprecision in emotion recognition may significantly warp social processes and disrupt emotional integration.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Recognizing and responding to the emotional states of others is a fundamental component of human social interaction. Successful emotion perception promotes trust, cooperation, and group cohesion, and misperceiving emotions leads to misunderstandings, conflict, or social exclusion. Artificial entities and social robots that increasingly interact with humans and with other robots need the ability to perceive and respond to emotional information.\n\nStudies on emotional contagion in social networks have long interested social and psychological researchers  (Hatfield et al., 1993; Barsade, 2002) . The rapid explosion of the Internet and, in particular, the spreading of social network websites exposed the possibility of the transmission of emotional states through virtual contacts as well  (Bollen et al.,  Figure  1 : Lattice configuration of emotional agents of the simulation. Each agent is represented as a colored circle, where color encodes the agent's current emotional state (e.g., happy, sad, angry, etc.). 2011;  Kramer, 2012) . For instance,  Kramer et al. (2014)  conducted a large-scale controlled experiment on Facebook and uncovered that emotional displays on posts influence others' emotions even without direct contact. In the same tradition of work,  Ferrara and Yang (2015)  also investigated the emotional tone of messages before people's tweets and uncovered apparent emotional influence patterns.\n\nOut of such findings, several computational models have been proposed to capture the temporal dynamics of emotional contagion  (Bosse et al., 2015; Wang et al., 2015) . To generalize the susceptible-infected-susceptible (SIS) model so that it can accommodate spontaneous emotional adoption,  Hill et al. (2010)  proposed the so-called SISa model in this arXiv:2509.00080v1 [cs.AI] 26 Aug 2025 regard. To capture the integration of emotion weighting into the transmission probability of the message so that contagious emotions impact the dissemination process of information,  Wang et al. (2015)  proposed the ESIS model. In reality, however, most models assume the existence of perfect or homogeneous emotional perception skills of the agents and do not take into consideration the variability and imperfection present in human and artificial recognition systems.\n\nThis assumption also introduces a critical flaw: emotional understanding is never absolute, even in humans, and certainly not in computer programs. Classifiers trained from limited or culture-bound datasets typically generalize poorly even in the usage environment. These defects can deflect an agent's comprehension of others' feelings and thus influence its decisions about approaching, avoiding, or mismatching with peers. An agent-based modeling (ABM) framework is presented in which each agent possesses a vision system with distinctive perceptual accuracy given by a convolutional neural network (CNN) trained on one of three facial expression datasets. Agents are depicted visually with images from the KDEF dataset and act locally on a 2D toroidal lattice. Their social behavior, given by movement towards or away from others, is governed not by the actual emotional state of neighbors but rather the agent's understanding of neighbors' emotions. This allows us to study the impact of systematic misperception on the emergence, breakdown, and stability of emotional and spatial patterns.\n\nThrough experiments with homogeneous and heterogeneous populations, we show that low-accuracy classifiers distort emotions, reduce trust, amplify negative convergence (especially sadness), and lead to social breakdown. In contrast, perceptually accurate agents foster stability and harmonious clusters. Perceptual biases (arising from data, culture, or technology) critically affect social structure and emotional health. Our model, a stylized abstraction, illustrates how systematic misrecognition of emotions can destabilize cooperative societies.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Background",
      "text": "Trust and cooperation are the basis of human social life, and the ability to recognize the trustworthiness of others is thought to be a key evolutionary advantage in the growth of cooperative behavior  (Boone and Buck, 2003) . Facial signals long thought to be connected with physiological arousal can be relatively reliable indicators of internal states connected with reliability  (Schug et al., 2010) . Those who freely express emotion are typically judged to be more cooperative and more trustworthy  (Lount, 2010) . Even though considerable work has been conducted looking at trust from the perspective of rational behavior and economic decisionmaking, much less focus has been placed on the role of emotional expression in the establishment of interpersonal trust  (Lount, 2010) . Combining emotional and rational perspectives on trust may offer a more comprehensive understand-ing of the mechanics underlying social cooperation.\n\nThis fundamental function of emotional expression in social cognition has also aroused much interest in the recognition and interpretation of such expressions in humans and machine systems.\n\nDetection of Emotions in Humans and Machines. Facial expression recognition has been deeply studied in psychology and computer science. We adopt the widely used Ekman set of six basic emotions plus neutral  (Ekman, 1993) , enabling comparability across JAFFE  (Lyons et al., 1998) , CK+  (Lucey et al., 2010) , and KDEF  (Lundqvist et al., 1998) , datasets widely employed for training deep models  (Li and Deng, 2022; Salas-Cáceres et al., 2024) . Although the models show very high accuracy on in-dataset testing, cross-dataset verification leads to drastically reduced performance  (Yang et al., 2020) , a sign of weak generality. More recent psychology work also calls into question the assumption that facial expressions map reliably to discrete emotions  (Barrett, 2012) . These questions about the models' validity raise concerns about the practicality of using emotion AI in interactive systems.\n\nAgent-Based Models (ABMs). ABMs have been popular tools for exploring complex social phenomena, including opinion formation, cooperation, and segregation. Historically, two principal limitations have plagued ABMs: computational inefficiencies on large scales and restricted behavioral sophistication  (Bonabeau, 2002) . More recent advancements in the domains of differential programming and deep learning have served to overthrow performance bottlenecks through the possibility of vectorized computation and neural-inspired models of large populations of agents  (Andelfinger, 2023) . Parallel efforts to elevate behavioral realism have aimed to leverage large language models (LLMs), drawing on the latter's human-like reasoning capabilities in order to model more subtle agent behavior  (Kerr et al., 2021) . LLMs' principal disadvantage continues to be the computationally expensive cost of inference  (Vezhnevets et al., 2023) . Here, we present an alternative solution: behavioral expressiveness based on the use of various perceptual modeling, entirely avoiding language generation and still producing rich emergent behavior on the population level.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Methodology",
      "text": "We model a population of N agents {a 1 , a 2 , . . . , a N } situated on a two-dimensional toroidal grid G ⊂ Z 2 . Time evolves in discrete steps t ∈ N. Each agent interacts locally with its Moore neighborhood and adjusts its internal state based on perceived emotional stimuli, classifier confidence, and accumulated trust.\n\nEmotion Classification via CNN. Each agent is equipped with a pretrained convolutional neural network (CNN) f θ : R H×W ×C → R |E| for facial emotion recognition. Given an input image x ∈ R H×W ×C , the output is a vector of logits:\n\nMoreover, the predicted emotion label is obtained as\n\nWhere ê ∈ E and the label set is defined as E = {happy, sad, angry, fear, disgust, surprise, neutral}.\n\nEach model is trained using a cross-entropy loss over a labeled dataset:\n\nWhere y i is the one-hot encoded ground-truth emotion.\n\nThree CNN classifiers, denoted f\n\n(1)\n\nθ , f\n\nθ , are pretrained independently on the CK+, JAFFE, and KDEF facial expression datasets, respectively. Each agent is assigned one of these models, f (k) θ , according to the experimental configuration defined for the simulation.\n\nAgent Specification. The tuple describes each agent a i :\n\nWhere:\n\n• e i (t) ∈ E is the emotional state at time t,\n\n• I i is the set of facial images for agent a i , one per emotion in E, taken from the KDEF dataset. At each timestep, the agent's displayed image is updated according to its current emotional state e i (t),\n\n• f",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Interaction Dynamics",
      "text": "At each timestep t, the agent performs the following sequence:\n\n(1) Emotional Perception. For each neighbor a j ∈ N i (t), the agent perceives an emotion:\n\n(2) Trust Update. Let δ j (t) = I ê(i) j (t) ̸ = e j (t) be the prediction error. Perceptual reliability (hereafter referred to as trust for brevity) is updated using an exponential moving average:\n\nwhere α ∈ (0, 1) is a smoothing parameter. Note that the comparison with ground-truth labels in δ j (t) is an evaluation device in the simulation rather than an observable feature available to agents. Agents only act based on their perceptions, but the ground-truth reference allows us to quantify the effect of perceptual mismatches on emergent trust dynamics. This mechanism captures the idea that repeated misperceptions reduce the agent's confidence in the reliability of emotional information from neighbors.\n\n(3) Environmental Valence. We define sets of positive and negative emotions:\n\nThe perceived valence of the local environment is:\n\nAgents enter an avoidance mode whenever the perceived emotional valence falls below a threshold, that is, when\n\nIn this state, the agent attempts to relocate to an adjacent unoccupied cell in the grid, effectively moving away from a negatively perceived neighborhood. Here only avoidance is implemented, but adding attraction to similar emotions (e.g., clustering negative states) would be a natural extension.\n\n(4) Frustration-Based Emotion Switching If trust falls below certain thresholds, an agent may switch to a negative or confused state:\n\nWe model sadness as the default frustrated state, as repeated social misperceptions typically elicit low affect and withdrawal rather than anger  (Carver and Harmon, 2009) .\n\n(5) Emotion Contagion Let F i (t) denote the multiset of perceived emotional states in the neighborhood N i (t) of agent a i at time t, that is,\n\nWe define emotional contagion as occurring when a particular emotion e ⋆ ∈ E dominates the local neighborhood. Specifically, if there exists an emotion e ⋆ such that\n\nand e ⋆ belongs to the agent's available expression set E, then agent a i may adopt emotion e ⋆ at the next timestep. This adoption is subject to a hysteresis condition: the candidate emotion e ⋆ must not have appeared more than a fixed number of times in the agent's recent emotional history H i (t), thereby preventing excessive emotional switching or oscillation.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Experimental Setup",
      "text": "The experimental setup consists of two main components: the training data for the emotion classifiers and the simulation parameters that define agent behavior and interaction dynamics.\n\nDatasets. The emotion classifiers were trained on three widely used facial expression datasets: JAFFE, CK+, and KDEF. The Japanese Female Facial Expression (JAFFE) dataset  (Lyons et al., 1998)  contains 213 grayscale images of 10 Japanese women posing six basic emotions (angry, disgust, fear, happy, sad, surprise) plus neutral. Its cultural and demographic homogeneity makes it a suitable test case for cross-population generalization. The Extended Cohn-Kanade (CK+) dataset  (Lucey et al., 2010) , an extension of CK, includes 593 sequences and still images from 123 subjects (81% Euro-American, 13% Afro-American, 6% other), annotated with eight categories: neutral, angry, contempt, disgust, fear, happy, sad, and surprise. Since contempt is absent from JAFFE and KDEF, we omitted it to ensure consistency. The Karolinska Directed Emotional Faces (KDEF) dataset  (Lundqvist et al., 1998)  provides high-resolution frontal face images of 70 actors under controlled conditions, each performing the seven basic emotions. For our experiments, we selected a balanced subset of 40 identities (20 male, 20 female). We extracted two frames per video (neutral and peak expression), yielding a balanced dataset across the six non-neutral emotions plus neutral.\n\nImplementation Details. To test the model proposed above, we ran simulations on a controlled parameter and condition set. In our experimental scenario, we chose a population of N = 40 agents, matching the number of distinct identities used in the KDEF facial expression dataset, and every agent has the complete set of seven basic emotions. The simulation world is a toroidal grid of size 9 × 9 (G ⊂ Z 2 ), resulting in a total of 81 cells. With randomly located 40 agents, the design ensures roughly 50% coverage of the surface of the grid. It provides an even spatial density with enough social contact coupled with room for agent movement and dispersal under the influence of emotional stress.\n\nThese face images for emotion classification were preprocessed uniformly so that the images were resized to be 96×96 pixels in size, converted to grayscale, and normalized to the interval [-1, 1], as is standard input to CNNs. The trust update mechanism uses a smoothing value of α = 0.1, and this controls the rate at which the agents update the trust based on the accuracy of their perception. The agent also maintains a rolling buffer of emotional history of size h = 5, so that some history of the previous states influences its behavior now.\n\nTo control behavior in response to perceived emotional context, the valence threshold was set fixed as τ valence = -1, so the agents move when perceiving a strongly negative state. Change of emotions due to lacking trust is controlled based on a threshold of τ sad = 0.3, and emotional transmission via contagion requires that at least 70% of the neighbors of an agent share a common emotion, i.e., τ contagion = 0.7. Each simulation ran for T = 100 discrete time-steps, and to make the results more robust, each parameter setting ran R = 10 times in order to capture stochastic variability from run to run.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Experimental Evaluation",
      "text": "To measure the influence of accuracy in emotional recognition on group emotional dynamics quantitatively, we model agent populations with varying perceptual skills. As previously specified, the agents perceive neighbors with the help of one of three trained emotion classifiers on data sets varying in generality to face images from KDEF. Agents change emotional state and spatial position based on perceived, not the ground-truth, emotions. Our design allows us to study the effects of misperception, trust decay, and affective feedback loops on group behavior and cohesion.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Baseline: Emotional Recognition Accuracy",
      "text": "Before analyzing emergent dynamics, we establish a baseline evaluation of the perceptual quality of each classifier in isolation. We test the JAFFE, CK+, and KDEF trained models on the full KDEF dataset (unseen during training for JAFFE and CK+) to simulate real-world generalization. This allows us to quantify classifier performance when interpreting agents represented by KDEF images, as used in all subsequent experiments. Since all our simulation agents possess face images from the KDEF dataset used for visually representing all the agents in our simulations, we evaluated each classifier on this domain with the standard 80/20 train-test split. The directly KDEF-trained classifier performed pretty well with 96% accuracy and macro-averaged precision and recall of 97% and 96%, respectively, and F1-score of 96%, evidencing near-perfect training-test data alignment. The CK+ classifier, on the other hand, has minimal generalization capa-bility and only achieves 37% accuracy on KDEF with macro precision, recall, and F1-score of 36%, 37%, and 30%, respectively. Worse performance still is observed for the classifier trained on the JAFFE dataset, which only achieves 19% accuracy on KDEF and a macro F1-score of 10%. The results indicate that the models of emotion recognition are highly dependent on the domain and demonstrate the way cultural and demographic differences between datasets can penalize cross-dataset generalization significantly. The all-Japanese female subject dataset JAFFE and the highly North American subject-based CK+ dataset equally struggle to interpret faces from the KDEF dataset, the latter having a more diverse collection of European faces. Simulation agents, then, with culturally mismatched datasets used for training, develop distorted concepts about the environment in which they exist and directly impact the social behavior and group emotional dynamics in the simulation.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Experiment 1: Homogeneous Classifier Populations",
      "text": "We start with entirely homogeneous populations where all the agents equally rely on the same facial expression recognition model. Although the same initial state (forty agents, random selection of emotional faces from the KDEF dataset), the emergent classifiers produced drastically contrasting emergent group behavior. The disparities arise from the varying perceptual accuracies of the models while interpreting facial expressions from the KDEF faces.\n\nWhen using only agents with the KDEF-trained classifier (see Figure  2 ), the emotional landscape remains diverse and balanced throughout the simulation. No single emotion dominates: \"sad\", \"neutral\", \"happy\", and \"fear\" each appear with comparable frequency (between approximately 6 and 10 agents on average). Agent trust remains high at 0.96, reflecting accurate emotional perception and stable interactions. Clustering patterns are consistent with this emotional diversity: agents form moderately sized groups for most emotions, such as \"happy\" (average cluster size: 4.17) and \"neutral\" (2.45), with no overwhelming dominance or fragmentation.\n\nThe ABM with only the CK+ classifier yields a much more polarized outcome (see Figure  3 ). The \"sad\" emotion becomes dominant, accounting for nearly 60% of the population on average (23.4 agents), while other emotions remain marginal. Trust drops significantly to 0.25, indicating frequent perceptual errors and agent frustration. This shift is also reflected in the cluster structure: although \"sad\" forms some larger clusters (up to 7.3 agents), other emotions tend to cluster weakly or not at all. The emotional drift toward sadness, despite a neutral or positive initialization, demonstrates how moderate perceptual inaccuracies can lead to unstable social dynamics.\n\nThe configuration with only the JAFFE-based agents displays the most extreme outcome (see Figure  4 ). Nearly all agents converge to the \"sad\" state (average 34.7), with virtually no representation of other emotions. Trust falls to near zero (0.038), signaling near-constant misperception. The emotional homogeneity is reinforced by dense and large \"sad\" and \"fear\" clusters (maximum sizes over 16 agents), indicating runaway emotional contagion and the collapse of social differentiation. These findings show that internal emotion models have a key role in determining emergent emotional and structural properties of agent populations. Clustering diversity and emotional stability are facilitated by high-fidelity classifiers (e.g., classifiers trained on the same data used for the visual inputs), and mismatched models create instability, miscommunication, and emotional polarization.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Experiment 2: Mixed Classifier Populations",
      "text": "To assess how agents with differing perceptual capacities influence each other, we simulated mixed populations consisting of multiple classifier types. Three specific configurations were tested: a balanced mix of 20 KDEF and 20  JAFFE agents; a uniform distribution of 13 KDEF, 14 CK+, and 13 JAFFE agents; and a population skewed toward CK+ with 10 KDEF, 20 CK+, and 10 JAFFE agents. All agents were initialized with random emotional expressions from the KDEF dataset, but the internal classifier used to interpret others' emotions varied by group.\n\nIn the 20 KDEF + 20 JAFFE scenario (see Figure  5 ), emotional outcomes are dominated by sadness, which accounts for more than 93% of the population (mean = 37.4 out of 40). Other emotions are nearly absent, each averaging fewer than one agent. This result is accompanied by a complete collapse of trust among JAFFE agents (mean trust = 0.000), while KDEF agents maintain high confidence (trust = 0.945). The emotional clustering patterns reflect this imbalance, with large and dense clusters of \"sad\" agents (up to 15.5 agents), suggesting that the poor perceptual accuracy of JAFFE agents triggers a feedback loop of misinterpretation and contagion.\n\nThe 13 KDEF + 14 CK+ + 13 JAFFE configuration also converges heavily toward \"sad\" (mean = 33.8), although with slightly more diversity than the previous case (see Figure  6 ). Emotions such as \"neutral\" and \"surprise\" appear sporadically. However, no emotion other than \"sad\" reaches a mean higher than three agents. Trust levels show a Cluster structures reinforce this emotional dominance, with sadness forming the most significant and numerous clusters, although some presence of \"neutral\" and \"surprise\" groupings is also observed. The most diverse pattern appears in the 10 KDEF + 20 CK+ + 10 JAFFE condition (see Figure  7 ). While \"sad\" still dominates (mean = 26.2), other negative emotions like \"fear\" (6.1) and \"surprise\" (5.0) also emerge more clearly. Small clusters of \"happy\" and \"disgust\" appear as well. Trust levels mirror classifier quality: KDEF remains high (0.948), CK+ moderate (0.223), and JAFFE low (0.063). Interestingly, this setting also yields a broader emotional distribution and more balanced cluster structures, \"happy\" and \"surprise\" form distinct and moderately sized groups (average cluster sizes of 7.0 and 7.6, respectively), suggesting that even limited perceptual improvement in the population can foster local zones of emotional variation.\n\nTogether, these findings demonstrate that population-level emotional dynamics are shaped not only by the overall perceptual quality of agents but also by their distribution within the social fabric. High-accuracy agents (KDEF) can maintain emotional stability in isolation or the presence of a minority. However, large proportions of low-performing classifiers (e.g., JAFFE) drive convergence toward negative affect and suppress emotional diversity across the population.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Experiment 3: Emotional Resilience Under Perturbation",
      "text": "Finally, we evaluate the system's resilience to repeated external emotional perturbations. In this setting, agents begin in randomly sampled emotional states, and at every ten simulation steps, 20% of the population is forcibly assigned a negative emotion. The periodic assignment of negative emotions to 20% of the agents serves as an exogenous shock to probe system resilience. This mechanism is intended as a proxy for external stressors (e.g., crises or hostile signals) and should not be interpreted as a literal psychological process. We explore homogeneous and heterogeneous populations to check the impact of perceptual fidelity on recovery from emotional states and long-term stability.\n\nHomogeneous Populations. In purely homogeneous settings, we observe significant differences in emotional resilience across classifiers. Populations composed entirely of KDEF agents display a gradual but incomplete erosion of positive affect (see Figure  8 ). While the proportion of positive emotions declines from 1.0 at step 10 to 0.08 at step 100, a small core of positivity persists. This is consistent with the classifier's strong perceptual accuracy and high final trust level (0.933), which appear to slow the spread of negative emotion and maintain partial recovery between shocks.\n\nIn contrast, CK+ agents degrade more rapidly (see Figure  9 ). Although they begin with a similarly high proportion of positive affect (0.93 at step 10), their emotional state deteriorates faster, reaching zero positive emotions by step 70. Their final trust score is extremely low (0.034), indicating that inaccurate perception undermines agents' confidence in others, thereby accelerating emotional collapse. JAFFE agents are unable to resist perturbation at all (see Figure  10 ). Even at step 10, no positive emotions remain in the system, and trust plummets to near-zero levels (0.001). This result underscores the fragility of populations reliant on poor perception: once injected, negativity enters and becomes dominant and irreversible.\n\nHeterogeneous Populations. Mixed-agent populations show outcomes that depend strongly on the relative proportions of perceptual capacity. In the setting with 8 KDEF, 16 CK+, and 16 JAFFE agents, the emotional trajectory mirrors the worst-performing components (see Figure  11 ). Positive affect drops sharply from 0.41 at step 10 to zero by step 70, with final trust scores confirming this collapse (JAFFE: 0.005, CK+: 0.039, KDEF: 0.961). The small number of accurate agents is insufficient to stabilize the group in the presence of overwhelming misperception.\n\nIn contrast, the configuration with 24 KDEF, 8 CK+, and 8 JAFFE agents retains substantially more emotional stability (see Figure  12 ). Positive emotions remain at 0.15 by step 100, and trust in the KDEF agents remains high (0.956). This demonstrates that when perceptually reliable agents form a substantial majority, emotional stability can be partially preserved despite repeated perturbations. These findings show that emotional resilience depends not only on perceptual accuracy but also on its distribution: a few reliable agents cannot offset widespread misperception, whereas a majority can stabilize the population under stress.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Conclusions",
      "text": "This study demonstrates that the quality of emotion perception critically shapes emergent social dynamics in agentbased simulations. Populations with high-accuracy classifiers maintained trust, emotional balance, and stable clusters of positive states. In contrast, those with poor classifiers rapidly converged toward negative affect (especially sadness), producing fragmented structures and minimal trust. Even small proportions of inaccurate agents destabilized otherwise cohesive societies, showing how perceptual biases (whether from data, culture, or algorithms) spread through local interactions and undermine collective coherence. Under repeated emotional shocks, heterogeneous populations deteriorated more rapidly than homogeneous ones, with only those dominated by accurate classifiers retaining stability. More broadly, our results resonate with Schelling's classic segregation model  (Schelling, 1971) : just as mild preferences can yield large-scale segregation, here systematic misperception drives avoidance and trust decay that accumulate into strong emotional clustering and social fragmentation. Beyond simulation, biased emotion classifiers may erode trust in human-AI interaction, underscoring the need for reliable perception in social technologies.",
      "page_start": 8,
      "page_end": 8
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Lattice configuration of emotional agents of the",
      "page": 1
    },
    {
      "caption": "Figure 2: Emotional evolution in homogeneous agent popu-",
      "page": 5
    },
    {
      "caption": "Figure 3: Emotional evolution in homogeneous agent popu-",
      "page": 5
    },
    {
      "caption": "Figure 4: Emotional evolution in homogeneous agent popu-",
      "page": 5
    },
    {
      "caption": "Figure 2: ), the emotional landscape remains diverse",
      "page": 5
    },
    {
      "caption": "Figure 3: ). The “sad” emotion",
      "page": 5
    },
    {
      "caption": "Figure 5: Emotional dynamics in mixed-agent populations",
      "page": 6
    },
    {
      "caption": "Figure 6: Emotional dynamics in mixed-agent populations",
      "page": 6
    },
    {
      "caption": "Figure 7: Emotional dynamics in mixed-agent populations",
      "page": 6
    },
    {
      "caption": "Figure 6: ). Emotions such as “neutral” and “surprise”",
      "page": 6
    },
    {
      "caption": "Figure 7: ). While “sad”",
      "page": 7
    },
    {
      "caption": "Figure 8: Emotional resilience of homogeneous KDEF",
      "page": 7
    },
    {
      "caption": "Figure 9: Emotional resilience of homogeneous CK+ agents",
      "page": 7
    },
    {
      "caption": "Figure 8: ). While the proportion of posi-",
      "page": 7
    },
    {
      "caption": "Figure 10: Emotional resilience of homogeneous JAFFE",
      "page": 8
    },
    {
      "caption": "Figure 11: Emotional stability under repeated negative",
      "page": 8
    },
    {
      "caption": "Figure 10: ). Even at step 10, no positive emotions remain in",
      "page": 8
    },
    {
      "caption": "Figure 12: Emotional resilience in a classifier-dominant sce-",
      "page": 8
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "happy\nsad\nangry": "fear\ndisgust\nsurprise\nneutral"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "happy\nsad\nangry": ""
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "happy\nsad\nangry": "fear\ndisgust\nsurprise\nneutral"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "happy\nsad\nangry": ""
        }
      ],
      "page": 8
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Towards differentiable agent-based simulation",
      "authors": [
        "P Andelfinger"
      ],
      "year": "2023",
      "venue": "ACM Transactions on Modeling and Computer Simulation"
    },
    {
      "citation_id": "2",
      "title": "Emotions are real",
      "authors": [
        "L Barrett"
      ],
      "year": "2012",
      "venue": "Emotion"
    },
    {
      "citation_id": "3",
      "title": "The ripple effect: Emotional contagion and its influence on group behavior",
      "authors": [
        "S Barsade"
      ],
      "year": "2002",
      "venue": "Administrative Science Quarterly"
    },
    {
      "citation_id": "4",
      "title": "Happiness is assortative in online social networks",
      "authors": [
        "J Bollen",
        "B Gonc ¸alves",
        "G Ruan",
        "H Mao"
      ],
      "year": "2011",
      "venue": "Artificial Life"
    },
    {
      "citation_id": "5",
      "title": "Agent-based modeling: Methods and techniques for simulating human systems",
      "authors": [
        "E Bonabeau"
      ],
      "year": "2002",
      "venue": "Proceedings of the National Academy of Sciences"
    },
    {
      "citation_id": "6",
      "title": "Emotional expressivity and trustworthiness: The role of nonverbal behavior in the evolution of cooperation",
      "authors": [
        "R Boone",
        "R Buck"
      ],
      "year": "2003",
      "venue": "Journal of Nonverbal Behavior"
    },
    {
      "citation_id": "7",
      "title": "Agent-based modeling of emotion contagion in groups",
      "authors": [
        "T Bosse",
        "R Duell",
        "Z Memon",
        "J Treur",
        "C Van Der Wal"
      ],
      "year": "2015",
      "venue": "Cognitive Computation"
    },
    {
      "citation_id": "8",
      "title": "Anger is an approach-related affect: Evidence and implications",
      "authors": [
        "C Carver",
        "E Harmon"
      ],
      "year": "2009",
      "venue": "Psychological Bulletin"
    },
    {
      "citation_id": "9",
      "title": "Facial expression and emotion",
      "authors": [
        "P Ekman"
      ],
      "year": "1993",
      "venue": "American psychologist"
    },
    {
      "citation_id": "10",
      "title": "Measuring emotional contagion in social media",
      "authors": [
        "E Ferrara",
        "Z Yang"
      ],
      "year": "2015",
      "venue": "PLOS ONE"
    },
    {
      "citation_id": "11",
      "title": "Emotional contagion",
      "authors": [
        "E Hatfield",
        "J Cacioppo",
        "R Rapson"
      ],
      "year": "1993",
      "venue": "Current Directions in Psychological Science"
    },
    {
      "citation_id": "12",
      "title": "Emotions as infectious diseases in a large social network: the sisa model",
      "authors": [
        "A Hill",
        "D Rand",
        "M Nowak",
        "N Christakis"
      ],
      "year": "2010",
      "venue": "Proceedings of the Royal Society B: Biological Sciences"
    },
    {
      "citation_id": "13",
      "title": "Covasim: An agent-based model of covid-19 dynamics and interventions",
      "authors": [
        "C Kerr",
        "R Stuart",
        "D Mistry",
        "R Abeysuriya",
        "K Rosenfeld",
        "G Hart",
        "R Núñez",
        "J Cohen",
        "P Selvaraj",
        "B Hagedorn",
        "L George",
        "M Jastrzebski",
        "A Izzo",
        "G Fowler",
        "A Palmer",
        "D Delport",
        "N Scott",
        "S Kelly",
        "C Bennette",
        "B Wagner",
        "S Chang",
        "A Oron",
        "E Wenger",
        "J Panovska-Griffiths",
        "M Famulare",
        "D Klein"
      ],
      "year": "2021",
      "venue": "PLOS Computational Biology"
    },
    {
      "citation_id": "14",
      "title": "The spread of emotion via facebook",
      "authors": [
        "A Kramer"
      ],
      "year": "2012",
      "venue": "Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, CHI '12"
    },
    {
      "citation_id": "15",
      "title": "Experimental evidence of massive-scale emotional contagion through social networks",
      "authors": [
        "A Kramer",
        "J Guillory",
        "J Hancock"
      ],
      "year": "2014",
      "venue": "Proceedings of the National Academy of Sciences"
    },
    {
      "citation_id": "16",
      "title": "Deep facial expression recognition: A survey",
      "authors": [
        "S Li",
        "W Deng"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "17",
      "title": "The impact of positive mood on trust in interpersonal and intergroup interactions",
      "authors": [
        "Robert Lount"
      ],
      "year": "2010",
      "venue": "Journal of Personality and Social Psychology"
    },
    {
      "citation_id": "18",
      "title": "The extended cohn-kanade dataset (ck+): A complete dataset for action unit and emotionspecified expression",
      "authors": [
        "P Lucey",
        "J Cohn",
        "T Kanade",
        "J Saragih",
        "Z Ambadar",
        "I Matthews"
      ],
      "year": "2010",
      "venue": "2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition -Workshops (CVPRW)"
    },
    {
      "citation_id": "19",
      "title": "CD-ROM from Department of Clinical Neuroscience, Psychology section, Karolinska Institutet",
      "authors": [
        "D Lundqvist",
        "A Flykt",
        "A Öhman"
      ],
      "year": "1998",
      "venue": "CD-ROM from Department of Clinical Neuroscience, Psychology section, Karolinska Institutet"
    },
    {
      "citation_id": "20",
      "title": "The japanese female facial expression (jaffe) dataset",
      "authors": [
        "M Lyons",
        "M Kamachi",
        "J Gyoba"
      ],
      "year": "1998",
      "venue": "The japanese female facial expression (jaffe) dataset"
    },
    {
      "citation_id": "21",
      "title": "Multimodal emotion recognition based on a fusion of audiovisual information with temporal dynamics",
      "authors": [
        "J Salas-Cáceres",
        "J Lorenzo-Navarro",
        "D Freire-Obregón",
        "M Castrillón-Santana"
      ],
      "year": "2024",
      "venue": "Multimodal emotion recognition based on a fusion of audiovisual information with temporal dynamics"
    },
    {
      "citation_id": "22",
      "title": "Dynamic models of segregation",
      "authors": [
        "T Schelling"
      ],
      "year": "1971",
      "venue": "The Journal of Mathematical Sociology"
    },
    {
      "citation_id": "23",
      "title": "Emotional expressivity as a signal of cooperation",
      "authors": [
        "J Schug",
        "D Matsumoto",
        "Y Horita",
        "T Yamagishi",
        "K Bonnet"
      ],
      "year": "2010",
      "venue": "Evolution and Human Behavior"
    },
    {
      "citation_id": "24",
      "title": "Generative agent-based modeling with actions grounded in physical, social, or digital space using concordia",
      "authors": [
        "A Vezhnevets",
        "J Agapiou",
        "A Aharon",
        "R Ziv",
        "J Matyas",
        "E Duéñez-Guzmán",
        "W Cunningham",
        "S Osindero",
        "D Karmon",
        "J Leibo"
      ],
      "year": "2023",
      "venue": "Generative agent-based modeling with actions grounded in physical, social, or digital space using concordia"
    },
    {
      "citation_id": "25",
      "title": "Esis: Emotion-based spreader ignorant stifler model for information diffusion",
      "authors": [
        "Q Wang",
        "Z Lin",
        "Y Jin",
        "S Cheng",
        "T Yang"
      ],
      "year": "2015",
      "venue": "Knowledge-Based Systems"
    },
    {
      "citation_id": "26",
      "title": "Effects of region features on the accuracy of cross-database facial expression recognition",
      "authors": [
        "Y Yang",
        "B Vuksanovic",
        "H Ma"
      ],
      "year": "2020",
      "venue": "Proceedings of the 12th International Conference on Agents and Artificial Intelligence"
    }
  ]
}