{
  "paper_id": "2410.13282v1",
  "title": "End-To-End Integration Of Speech Emotion Recognition With Voice Activity Detection Using Self-Supervised Learning Features",
  "published": "2024-10-17T07:18:19Z",
  "authors": [
    "Natsuo Yamashita",
    "Masaaki Yamamoto",
    "Yohei Kawaguchi"
  ],
  "keywords": [
    "speech emotion recognition",
    "voice activity detection",
    "self-supervised learning",
    "end-to-end training",
    "deep learning"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Speech Emotion Recognition (SER) often operates on speech segments detected by a Voice Activity Detection (VAD) model. However, VAD models may output flawed speech segments, especially in noisy environments, resulting in degraded performance of subsequent SER models. To address this issue, we propose an end-to-end (E2E) method that integrates VAD and SER using Self-Supervised Learning (SSL) features. The VAD module first receives the SSL features as input, and the segmented SSL features are then fed into the SER module. Both the VAD and SER modules are jointly trained to optimize SER performance. Experimental results on the IEMOCAP dataset demonstrate that our proposed method improves SER performance. Furthermore, to investigate the effect of our proposed method on the VAD and SSL modules, we present an analysis of the VAD outputs and the weights of each layer of the SSL encoder.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "Speech Emotion Recognition (SER) is the task of identifying and classifying emotional states expressed in spoken language  [1] . It is an essential field within the expansive domain of affective computing  [2]  and human-computer interaction  [3] , with a variety of real-world applications, including healthcare  [4] , customer service  [5] , and marketing  [6] .\n\nTypical SER methods  [7, 8, 9, 10]  begin by extracting low-level descriptive features, such as prosodic characteristics and spectral features from speech signals, which are then fed into machine learning models. With recent advances in deep learning, especially Transformer framework  [11] , a considerable number of studies  [12, 13, 14]  have focused on utilizing pre-trained self-supervised learning (SSL) models such as wav2vec 2.0  [15] , HuBERT  [16] , and WavLM  [17]  as feature extractors. The SER models using these SSL features (SSL-SER) have shown greatly improved performance in SER and other downstream tasks through fine-tuning  [18, 19, 20, 21] .\n\nIn practical applications of SER, it is common practice to employ a Voice Activity Detection (VAD) model, which detects the speech segments in a given audio sequence, before feeding the audio into a SER model  [22] . This preprocessing step aims to identify when specific emotions are expressed, mitigate the effect of noise on the SER model, and reduce the computational costs for post-processing. Though deep learning-based VAD models  [23, 24, 25]  have improved performance compared to traditional schemes based on statistics of speech  [26, 27, 28] , they can still generate flawed output, especially in noisy environments. This output may be fragmented, miss emotional features, or contain noise at the beginning, in the intervals, or at the end, resulting in degraded SER performance. In the context of Automatic Speech Recognition (ASR), this is a less crucial issue, because ASR, which is designed to extract meaningful linguistic information, can often compensate for minor segmentation errors. In contrast, SER is more susceptible to flawed VAD outputs because it relies on detecting subtle variations and nuances in speech, such as tone, pitch, rhythm, intensity, and speed  [29] . Since VAD models are trained to distinguish speech from non-speech but are not optimized for SER performance, they sometimes fail to generate optimal output for the subsequent SER models. Even if speech segments are correctly identified, some parts may contain rich emotional parts while others may not, and the latter can lead to a decline in the accuracy of SER.\n\nIn this paper, to address the problem of degraded SER performance due to flawed VAD outputs in noisy environments, we propose a method that integrates VAD and SER modules using SSL features in an end-to-end (E2E) manner. SSL features are first input into the VAD module, and then the segmented SSL features are fed into the SER module. Both the VAD and SER modules are jointly trained to optimize SER loss. This approach allows the VAD module to be trained to include more emotional speech segments that are important for SER, while the SER module is trained to be robust against flawed segments from the VAD module. As our experimental results demonstrate, the proposed method improves SER performance in noisy environments on the IEMOCAP dataset.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Ii. Proposed Approach",
      "text": "The overall architecture of the proposed approach is shown in Fig.  1 , which consists of the SSL, VAD, and SER modules. It is worth noting that the choice of network architecture for each module is not restricted to any specific one.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "A. Ssl Module",
      "text": "In this study, we employ SSL models as common feature extractors for both the following VAD and SER modules, due to their well-known generalizability and accessibility across various speech processing tasks  [21] . We denote the feature extraction process of the SSL module as follows:\n\nwhere X is an input utterance, F is the SSL features, and θ ssl and θ feat represent the parameters of the SSL encoder and the Featurizer, respectively. Given an input waveform, the SSL encoder, consisting of a Convolutional Neural Network (CNN) block and 12 Transformer encoder blocks, extracts a frame sequence of 768-dimensional speech features with a frame shift of 20 ms.\n\nResearch  [17, 21, 30]  has shown that intermediate representations of such foundation models contain information useful for different tasks. Therefore, the Featurizer computes the weighted-sum of embeddings from the 13 hidden states of the SSL encoder.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "B. Vad Module",
      "text": "While conventional VAD methods  [23, 24]  often use spectrogram features such as log Mel-Filterbanks (Fbank) and Mel-Frequency Cepstral Coefficients (MFCC), very recent work  [31]  has shown that a VAD architecture based on wav2vec 2.0 outperforms previous works  [24, 32] . In this study, we employ the VAD module using SSL features (SSL-VAD) and investigate the use of not only wav2vec 2.0 but also HuBERT and WavLM for our end-to-end approach. If we denote the VAD outputs indicating speech/non-speech as S, the SSL features as F, and the segmented SSL features as F ′ , we can write the process of the VAD module:\n\nwhere θ vad represents the parameters of the VAD module and ⊙ is the Hadamard product operator.\n\nThe VAD module consists of four 1D convolution layers with a hidden dimension of 256 and leaky ReLU activation, and a Fully-Connected (FC) layer with softmax activation. It detects frame-level speech/non-speech, assigning a label of 1 or 0 to each frame using an argmax operation. The segmented SSL features are then calculated by the Hadamard product between the SSL features and the broadcast outputs of the VAD process.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "C. Ser Module",
      "text": "We use the segmented SSL features from the VAD module as input for the SER module. The SER process can be written as:\n\nwhere Ŷ is a predicted emotion label and θ ser represents the parameters of the SER module. Given the segmented 768-dimensional SSL features, the SER module first applies dimensionality reduction from 768 to 256, followed by average pooling. The representations are then processed through three 1D convolution layers and a FC layer with ReLU activation.\n\nA subsequent self-attention pooling layer  [33]  aggregates features along the time axis, which are then fed into a FC layer with ReLU activation and a FC layer with softmax activation for emotion classification.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "D. End-To-End Training",
      "text": "Some studies attempted to combine VAD and SER modules in a cascade manner  [34]  or using a multi-task approach  [35] , but end-to-end training of VAD and SER modules (E2E SSL-VAD-SER) has not been fully investigated. In this study, we propose an end-to-end approach that jointly optimizes the VAD and SER modules for SER loss using the SSL features, to address the issue of degradation in SER performance due to flawed VAD outputs in noisy environments. Our entire endto-end approach can be described as follows:\n\n(5) Ŷ = SER( SSL(X; θ ssl , θ feat ) ⊙ VAD( SSL(X; θ ssl , θ feat ); θ vad ); θ ser ),\n\nTo give effective feedback between the VAD and SER modules from the start of end-to-end training, we initialize each parameter with θssl from publicly available pre-trained models and θfeat , θvad , and θser obtained by pre-training the SSL-VAD and SSL-SER, respectively. Then, θfeat , θvad , and θser are fine-tuned for SER loss to achieve better performance, while θssl is frozen in this study to reduce computational costs. Note that this end-to-end approach allows for fine-tuning modules even on speech data that has only emotion labels without speech/non-speech annotations for VAD.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Iii. Experimental Setup",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "A. Dataset",
      "text": "We used the IEMOCAP dataset  [36]  which has approximately 12 hours of English speech including 5 dyadic conversational sessions between two actors. There are in total 151 dialogues, including 10,039 utterances. The IEMOCAP provides the timestamps for each utterance in a dialogue, as well as word-level alignments for each utterance. The utterances contain silence at the beginning, between words, and at the end. The VAD module is trained based on these speech/nonspeech alignments, with 40 % of the segments being nonspeech. For SER, we merged emotion class \"excited\" with \"happy\" and used audio annotated with one of four labels, which are happy, sad, neutral, and angry.\n\nDuring evaluation, to simulate a real-world scenario, we extended the original utterances by concatenating the intervals before and after them. Specifically, we used the timestamps from the end of the previous utterance to the start of the next utterance, ensuring there was no overlap by subtracting 0.1 s. These extended utterances averaged 11.7 s, while the original utterances averaged 4.5 s. The original and extended utterances were contaminated with noise, randomly selected from the 37 recordings annotated as background noise in the MUSAN corpus  [37]  at Signal-to-Noise Ratio (SNR) levels of {10, 5, 0, -5, -10} dB. Speakers in the test set were excluded from the training and validation sets. Additionally, we made sure that utterances from the same dialogue were either all in the training set or all in the validation set.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "B. Model Configuration",
      "text": "We used pre-trained SSL models including wav2vec 2.0 BASE  [15] , HuBERT BASE  [16] , and WavLM BASE+  [17] , which are publicly available. For simplicity, the term \"BASE\" will be omitted hereafter. Each model has approximately 95 million parameters. Wav2vec 2.0 and HuBERT were trained with the concatenation of the train-clean-100, train-clean-360, and train-other-500 subsets from the LibriSpeech dataset  [38] . WavLM+ was trained with the Libri-Light  [39] , GigaSpeech  [40] , and VoxPopuli datasets  [41] . The SSL-VAD and SSL-SER were pre-trained and initialized using these SSL models. They were trained using the Adam optimizer  [42]  with a fixed learning rate of 1 × 10 -4 and a batch size of 8, without data augmentation, for at most 100 epochs. In the proposed approach, the modules were then fine-tuned using SER loss with the same optimizer algorithm and learning rate. During the training of each system, the parameters of the SSL encoder were not updated.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Iv. Results",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "A. Performance On Ser",
      "text": "In this section, we present the evaluation results on the IEMOCAP dataset. We computed Unweighted Accuracy (UA) and Weighted Accuracy (WA) on the test sets, as shown in Table  I . UA is the average recall across all categories and WA is the total number of correct predictions divided by the total number of samples. First, we report the performance of the SSL-SER, which does not include the VAD module, as a reference (Condition 1). It was evaluated using the original utterances, as in previous studies  [18, 21] . For the evaluation of Conditions 2-7, we used the extended utterances containing intervals before and after, as described in section III-A. Conditions 2 and 3 serve as baselines, while Conditions 4-7 represent our proposed approaches. The performance of Condition 2, which was based on the same architecture as Condition 1 but was evaluated with the extended utterances, shows a dramatic degradation compared to Condition 1 due to the effect of the noisy extended utterances. In Condition 3, we implemented MarbleNet  [23] , one of the common deep learning-based VAD models, as a pre-processing VAD step for the SSL-SER. MarbleNet utilized MFCC with 64 dimensions of mel-filter bank, a 25 ms window size and a 10 ms overlap. MarbleNet and the SSL-SER were individually trained and the segmented speeches detected by MarbleNet were fed into the SSL-SER. The results of Condition 3 show that SER performance was degraded when MarbleNet was used, compared to Condition 2. This indicates that simply combining a VAD model with the SSL-SER in noisy environments can cause detrimental effects on SER performance due to flawed VAD outputs.\n\nIn Conditions 4-7, we explored four different fine-tuning strategies for our approach. Condition 4 consists of the individually pre-trained SSL-VAD and SSL-SER without finetuning. The results of Conditions 3 and 4 show that replacing the MFCC-based MarbleNet with the SSL-VAD improved SER performance, especially when using wav2vec 2.0 and HuBERT. In Conditions 5-7, all trainable parameters were directly optimized for SER loss in an end-to-end manner. In Condition 5, θfeat and θvad were fine-tuned while the other parameters were frozen. The performance of Condition 5, on all the SSL models, shows improvement compared to Condition 4. This suggests that the VAD module optimized only for speech/non-speech detection is not always optimal for SER, whereas our approach successfully improved the VAD outputs for SER. In Condition 6, θfeat and θser were fine-tuned, while the other parameters were frozen. Condition 6, with any SSL model, also improved SER performance compared to Condition 4, indicating the importance of finetuning on the segmented SSL features rather than only on the original utterances. In Condition 7, θfeat , θvad , and θser were fine-tuned and θssl was frozen. We observe that Condition 7   further improved SER performance on all the SSL models, indicating that our end-to-end approach successfully jointly fine-tuned both the VAD and SER modules for SER. For example, comparing Condition 3 and 7 on WavLM+, the UA increased from 42.8 % to 51.4 % and the WA increased from 42.1 % to 54.9 %.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "B. Analysis Of Vad Outputs",
      "text": "To investigate the impact of VAD outputs on SER performance, we analyzed both VAD performance and its outputs. Table  II  shows accuracy, precision and recall scores for VAD, and Fig.  2  offers visualized examples of the VAD outputs, using HuBERT and WavLM+ under Conditions 3, 4, and 7. Accuracy is the ratio of correctly predicted speech and nonspeech segments to all segments. Precision is the ratio of correctly identified speech segments to all segments predicted as speech. Recall is the ratio of correctly identified speech segments to all actual speech segments.\n\nFrom the results, although MarbleNet exhibited higher precision, it often missed speech segments as depicted in Fig.  2 (a), which can lead to degrade SER performance. In Condition 4 and 7, we cannot see a significant difference  in the VAD performance as shown in Table  II . However, from the analysis of visualizations, it was observed that the fine-tuned VAD modules sometimes focused on important emotional parts as shown in Fig.  2(a) , or broadly detected segments to include more emotion despite containing more noise as shown in Fig.  2 (b), depending on the input utterances and the SSL models. These findings suggest that optimizing the VAD module for SER in an end-to-end manner does not necessarily lead to improvement in VAD performance. Additionally, we observed that the SSL-VAD models tended to produce fragmented outputs as shown in Fig.  2(b ). This indicates that fine-tuning the SER module, which was pretrained only on the original unsegmented utterances, with such flawed outputs likely contributed to improving SER performance in Conditions 6 and 7.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "C. Analysis Of Weights Of The Featurizer",
      "text": "We investigated the weights of the Featurizer in Conditions 4, 2, and 7, which were trained for VAD, SER, and both VAD and SER, respectively. Fig.  3  shows the weights of different layers of the Featurizer on HuBERT and WavLM+. The results indicate that Layers 0-4 are useful for VAD, while Layers 8-10 are more effective for SER. These patterns are consistent with previous findings  [17, 21, 35] . We observe that our end-to-end method successfully emphasizes the features of Layers 8 and 9 for SER, as well as of Layers 0-4 for VAD, enabling the VAD module to take emotional features into account.\n\nV. CONCLUSIONS In this study, we presented a method that integrates VAD and SER modules using SSL features in an end-to-end manner. Our approach allows the VAD module to capture effective speech segments for SER, while making the SER module robust against flawed segments from the VAD module. Experimental results on the IEMOCAP dataset showed that our proposed method significantly improved SER performance in noisy environments. Future work will include investigating the generalization ability of our proposed approach.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: , which consists of the SSL, VAD, and SER modules.",
      "page": 1
    },
    {
      "caption": "Figure 1: Overview of the proposed end-to-end approach composed of SSL,",
      "page": 2
    },
    {
      "caption": "Figure 2: Visualizations of the oracle segments annotated in the IEMOCAP",
      "page": 4
    },
    {
      "caption": "Figure 2: offers visualized examples of the VAD outputs,",
      "page": 4
    },
    {
      "caption": "Figure 2: (a), which can lead to degrade SER performance. In",
      "page": 4
    },
    {
      "caption": "Figure 3: Visualization of the Featurizer weights on HuBERT and WavLM+.",
      "page": 4
    },
    {
      "caption": "Figure 2: (a), or broadly detected",
      "page": 4
    },
    {
      "caption": "Figure 2: (b), depending on the input utterances",
      "page": 4
    },
    {
      "caption": "Figure 3: shows the weights of different",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "natsuo.yamashita.gh@hitachi.com": "Abstract—Speech Emotion Recognition (SER) often operates",
          "masaaki.yamamoto.af@hitachi.com\nyohei.kawaguchi.xk@hitachi.com": "expressed, mitigate the effect of noise on the SER model, and"
        },
        {
          "natsuo.yamashita.gh@hitachi.com": "on\nspeech\nsegments\ndetected\nby\na Voice Activity Detection",
          "masaaki.yamamoto.af@hitachi.com\nyohei.kawaguchi.xk@hitachi.com": ""
        },
        {
          "natsuo.yamashita.gh@hitachi.com": "",
          "masaaki.yamamoto.af@hitachi.com\nyohei.kawaguchi.xk@hitachi.com": "reduce\nthe\ncomputational costs\nfor post-processing. Though"
        },
        {
          "natsuo.yamashita.gh@hitachi.com": "(VAD) model. However, VAD models may output ﬂawed speech",
          "masaaki.yamamoto.af@hitachi.com\nyohei.kawaguchi.xk@hitachi.com": ""
        },
        {
          "natsuo.yamashita.gh@hitachi.com": "",
          "masaaki.yamamoto.af@hitachi.com\nyohei.kawaguchi.xk@hitachi.com": "deep learning-based VAD models [23, 24, 25] have improved"
        },
        {
          "natsuo.yamashita.gh@hitachi.com": "segments, especially in noisy environments, resulting in degraded",
          "masaaki.yamamoto.af@hitachi.com\nyohei.kawaguchi.xk@hitachi.com": ""
        },
        {
          "natsuo.yamashita.gh@hitachi.com": "",
          "masaaki.yamamoto.af@hitachi.com\nyohei.kawaguchi.xk@hitachi.com": "performance compared to traditional schemes based on statis-"
        },
        {
          "natsuo.yamashita.gh@hitachi.com": "performance of subsequent SER models. To address this issue, we",
          "masaaki.yamamoto.af@hitachi.com\nyohei.kawaguchi.xk@hitachi.com": ""
        },
        {
          "natsuo.yamashita.gh@hitachi.com": "propose an end-to-end (E2E) method that\nintegrates VAD and",
          "masaaki.yamamoto.af@hitachi.com\nyohei.kawaguchi.xk@hitachi.com": "tics\nof\nspeech\n[26,\n27,\n28],\nthey\ncan\nstill\ngenerate ﬂawed"
        },
        {
          "natsuo.yamashita.gh@hitachi.com": "SER using Self-Supervised Learning (SSL)\nfeatures. The VAD",
          "masaaki.yamamoto.af@hitachi.com\nyohei.kawaguchi.xk@hitachi.com": "output, especially in noisy environments. This output may be"
        },
        {
          "natsuo.yamashita.gh@hitachi.com": "module ﬁrst receives the SSL features as input, and the segmented",
          "masaaki.yamamoto.af@hitachi.com\nyohei.kawaguchi.xk@hitachi.com": ""
        },
        {
          "natsuo.yamashita.gh@hitachi.com": "",
          "masaaki.yamamoto.af@hitachi.com\nyohei.kawaguchi.xk@hitachi.com": "fragmented, miss emotional\nfeatures, or contain noise at\nthe"
        },
        {
          "natsuo.yamashita.gh@hitachi.com": "SSL features are then fed into the SER module. Both the VAD and",
          "masaaki.yamamoto.af@hitachi.com\nyohei.kawaguchi.xk@hitachi.com": ""
        },
        {
          "natsuo.yamashita.gh@hitachi.com": "",
          "masaaki.yamamoto.af@hitachi.com\nyohei.kawaguchi.xk@hitachi.com": "beginning, in the intervals, or at\nthe end, resulting in degraded"
        },
        {
          "natsuo.yamashita.gh@hitachi.com": "SER modules are jointly trained to optimize SER performance.",
          "masaaki.yamamoto.af@hitachi.com\nyohei.kawaguchi.xk@hitachi.com": ""
        },
        {
          "natsuo.yamashita.gh@hitachi.com": "",
          "masaaki.yamamoto.af@hitachi.com\nyohei.kawaguchi.xk@hitachi.com": "SER performance. In the context of Automatic Speech Recog-"
        },
        {
          "natsuo.yamashita.gh@hitachi.com": "Experimental results on the IEMOCAP dataset demonstrate that",
          "masaaki.yamamoto.af@hitachi.com\nyohei.kawaguchi.xk@hitachi.com": ""
        },
        {
          "natsuo.yamashita.gh@hitachi.com": "our proposed method improves SER performance. Furthermore,",
          "masaaki.yamamoto.af@hitachi.com\nyohei.kawaguchi.xk@hitachi.com": "nition (ASR),\nthis is a less crucial\nissue, because ASR, which"
        },
        {
          "natsuo.yamashita.gh@hitachi.com": "to investigate the effect of our proposed method on the VAD and",
          "masaaki.yamamoto.af@hitachi.com\nyohei.kawaguchi.xk@hitachi.com": "is designed to extract meaningful\nlinguistic information, can"
        },
        {
          "natsuo.yamashita.gh@hitachi.com": "SSL modules, we present an analysis of\nthe VAD outputs and",
          "masaaki.yamamoto.af@hitachi.com\nyohei.kawaguchi.xk@hitachi.com": ""
        },
        {
          "natsuo.yamashita.gh@hitachi.com": "",
          "masaaki.yamamoto.af@hitachi.com\nyohei.kawaguchi.xk@hitachi.com": "often compensate for minor segmentation errors.\nIn contrast,"
        },
        {
          "natsuo.yamashita.gh@hitachi.com": "the weights of each layer of\nthe SSL encoder.",
          "masaaki.yamamoto.af@hitachi.com\nyohei.kawaguchi.xk@hitachi.com": ""
        },
        {
          "natsuo.yamashita.gh@hitachi.com": "",
          "masaaki.yamamoto.af@hitachi.com\nyohei.kawaguchi.xk@hitachi.com": "SER is more susceptible to ﬂawed VAD outputs because\nit"
        },
        {
          "natsuo.yamashita.gh@hitachi.com": "Index Terms—speech emotion recognition, voice activity detec-",
          "masaaki.yamamoto.af@hitachi.com\nyohei.kawaguchi.xk@hitachi.com": ""
        },
        {
          "natsuo.yamashita.gh@hitachi.com": "",
          "masaaki.yamamoto.af@hitachi.com\nyohei.kawaguchi.xk@hitachi.com": "relies on detecting subtle variations and nuances\nin speech,"
        },
        {
          "natsuo.yamashita.gh@hitachi.com": "tion, self-supervised learning, end-to-end training, deep learning",
          "masaaki.yamamoto.af@hitachi.com\nyohei.kawaguchi.xk@hitachi.com": ""
        },
        {
          "natsuo.yamashita.gh@hitachi.com": "",
          "masaaki.yamamoto.af@hitachi.com\nyohei.kawaguchi.xk@hitachi.com": "such as\ntone, pitch,\nrhythm,\nintensity, and speed [29]. Since"
        },
        {
          "natsuo.yamashita.gh@hitachi.com": "",
          "masaaki.yamamoto.af@hitachi.com\nyohei.kawaguchi.xk@hitachi.com": "VAD models are trained to distinguish speech from non-speech"
        },
        {
          "natsuo.yamashita.gh@hitachi.com": "I.\nINTRODUCTION",
          "masaaki.yamamoto.af@hitachi.com\nyohei.kawaguchi.xk@hitachi.com": ""
        },
        {
          "natsuo.yamashita.gh@hitachi.com": "",
          "masaaki.yamamoto.af@hitachi.com\nyohei.kawaguchi.xk@hitachi.com": "but are not optimized for SER performance,\nthey sometimes"
        },
        {
          "natsuo.yamashita.gh@hitachi.com": "Speech Emotion Recognition (SER)\nis\nthe task of\nidenti-",
          "masaaki.yamamoto.af@hitachi.com\nyohei.kawaguchi.xk@hitachi.com": "fail to generate optimal output for the subsequent SER models."
        },
        {
          "natsuo.yamashita.gh@hitachi.com": "fying and classifying emotional\nstates\nexpressed in\nspoken",
          "masaaki.yamamoto.af@hitachi.com\nyohei.kawaguchi.xk@hitachi.com": "Even if\nspeech segments are correctly identiﬁed, some parts"
        },
        {
          "natsuo.yamashita.gh@hitachi.com": "language\n[1].\nIt\nis\nan\nessential ﬁeld within\nthe\nexpansive",
          "masaaki.yamamoto.af@hitachi.com\nyohei.kawaguchi.xk@hitachi.com": "may contain rich emotional parts while others may not, and"
        },
        {
          "natsuo.yamashita.gh@hitachi.com": "domain of affective computing [2] and human-computer inter-",
          "masaaki.yamamoto.af@hitachi.com\nyohei.kawaguchi.xk@hitachi.com": "the latter can lead to a decline in the accuracy of SER."
        },
        {
          "natsuo.yamashita.gh@hitachi.com": "action [3], with a variety of real-world applications,\nincluding",
          "masaaki.yamamoto.af@hitachi.com\nyohei.kawaguchi.xk@hitachi.com": "In\nthis\npaper,\nto\naddress\nthe\nproblem of\ndegraded SER"
        },
        {
          "natsuo.yamashita.gh@hitachi.com": "healthcare [4], customer service [5], and marketing [6].",
          "masaaki.yamamoto.af@hitachi.com\nyohei.kawaguchi.xk@hitachi.com": "performance due\nto ﬂawed VAD outputs\nin\nnoisy environ-"
        },
        {
          "natsuo.yamashita.gh@hitachi.com": "Typical SER methods\n[7,\n8,\n9,\n10]\nbegin\nby\nextracting",
          "masaaki.yamamoto.af@hitachi.com\nyohei.kawaguchi.xk@hitachi.com": "ments, we propose a method that\nintegrates VAD and SER"
        },
        {
          "natsuo.yamashita.gh@hitachi.com": "low-level descriptive features, such as prosodic characteristics",
          "masaaki.yamamoto.af@hitachi.com\nyohei.kawaguchi.xk@hitachi.com": "modules using SSL features in an end-to-end (E2E) manner."
        },
        {
          "natsuo.yamashita.gh@hitachi.com": "and spectral\nfeatures from speech signals, which are then fed",
          "masaaki.yamamoto.af@hitachi.com\nyohei.kawaguchi.xk@hitachi.com": "SSL features are ﬁrst input into the VAD module, and then the"
        },
        {
          "natsuo.yamashita.gh@hitachi.com": "into machine learning models. With recent advances in deep",
          "masaaki.yamamoto.af@hitachi.com\nyohei.kawaguchi.xk@hitachi.com": "segmented SSL features are fed into the SER module. Both"
        },
        {
          "natsuo.yamashita.gh@hitachi.com": "learning, especially Transformer\nframework [11], a consider-",
          "masaaki.yamamoto.af@hitachi.com\nyohei.kawaguchi.xk@hitachi.com": "the VAD and SER modules\nare\njointly trained to optimize"
        },
        {
          "natsuo.yamashita.gh@hitachi.com": "able number of studies [12, 13, 14] have focused on utilizing",
          "masaaki.yamamoto.af@hitachi.com\nyohei.kawaguchi.xk@hitachi.com": "SER loss.\nThis\napproach\nallows\nthe VAD module\nto\nbe"
        },
        {
          "natsuo.yamashita.gh@hitachi.com": "pre-trained\nself-supervised\nlearning\n(SSL) models\nsuch\nas",
          "masaaki.yamamoto.af@hitachi.com\nyohei.kawaguchi.xk@hitachi.com": "trained to include more emotional\nspeech segments\nthat are"
        },
        {
          "natsuo.yamashita.gh@hitachi.com": "wav2vec 2.0 [15], HuBERT [16], and WavLM [17] as feature",
          "masaaki.yamamoto.af@hitachi.com\nyohei.kawaguchi.xk@hitachi.com": "important\nfor SER, while\nthe SER module is\ntrained to be"
        },
        {
          "natsuo.yamashita.gh@hitachi.com": "extractors. The SER models using these SSL features\n(SSL-",
          "masaaki.yamamoto.af@hitachi.com\nyohei.kawaguchi.xk@hitachi.com": "robust\nagainst ﬂawed segments\nfrom the VAD module. As"
        },
        {
          "natsuo.yamashita.gh@hitachi.com": "SER) have shown greatly improved performance in SER and",
          "masaaki.yamamoto.af@hitachi.com\nyohei.kawaguchi.xk@hitachi.com": "our\nexperimental\nresults\ndemonstrate,\nthe\nproposed method"
        },
        {
          "natsuo.yamashita.gh@hitachi.com": "other downstream tasks through ﬁne-tuning [18, 19, 20, 21].",
          "masaaki.yamamoto.af@hitachi.com\nyohei.kawaguchi.xk@hitachi.com": "improves SER performance\nin\nnoisy\nenvironments\non\nthe"
        },
        {
          "natsuo.yamashita.gh@hitachi.com": "In\npractical\napplications\nof SER,\nit\nis\ncommon\npractice",
          "masaaki.yamamoto.af@hitachi.com\nyohei.kawaguchi.xk@hitachi.com": "IEMOCAP dataset."
        },
        {
          "natsuo.yamashita.gh@hitachi.com": "to employ a Voice Activity Detection (VAD) model, which",
          "masaaki.yamamoto.af@hitachi.com\nyohei.kawaguchi.xk@hitachi.com": ""
        },
        {
          "natsuo.yamashita.gh@hitachi.com": "",
          "masaaki.yamamoto.af@hitachi.com\nyohei.kawaguchi.xk@hitachi.com": "II. PROPOSED APPROACH"
        },
        {
          "natsuo.yamashita.gh@hitachi.com": "detects\nthe speech segments\nin a given audio sequence, be-",
          "masaaki.yamamoto.af@hitachi.com\nyohei.kawaguchi.xk@hitachi.com": ""
        },
        {
          "natsuo.yamashita.gh@hitachi.com": "fore\nfeeding\nthe\naudio\ninto\na SER model\n[22]. This\npre-",
          "masaaki.yamamoto.af@hitachi.com\nyohei.kawaguchi.xk@hitachi.com": "The overall architecture of the proposed approach is shown"
        },
        {
          "natsuo.yamashita.gh@hitachi.com": "processing step aims\nto identify when speciﬁc emotions are",
          "masaaki.yamamoto.af@hitachi.com\nyohei.kawaguchi.xk@hitachi.com": "in Fig. 1, which consists of the SSL, VAD, and SER modules."
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "The VAD module consists of\nfour 1D convolution layers": "with a hidden dimension of 256 and leaky ReLU activation,"
        },
        {
          "The VAD module consists of\nfour 1D convolution layers": ""
        },
        {
          "The VAD module consists of\nfour 1D convolution layers": "and a Fully-Connected (FC)\nlayer with softmax activation.\nIt"
        },
        {
          "The VAD module consists of\nfour 1D convolution layers": ""
        },
        {
          "The VAD module consists of\nfour 1D convolution layers": "detects frame-level speech/non-speech, assigning a label of 1"
        },
        {
          "The VAD module consists of\nfour 1D convolution layers": "or 0 to each frame using an argmax operation. The segmented"
        },
        {
          "The VAD module consists of\nfour 1D convolution layers": ""
        },
        {
          "The VAD module consists of\nfour 1D convolution layers": "SSL features\nare\nthen calculated by the Hadamard product"
        },
        {
          "The VAD module consists of\nfour 1D convolution layers": ""
        },
        {
          "The VAD module consists of\nfour 1D convolution layers": ""
        },
        {
          "The VAD module consists of\nfour 1D convolution layers": "between the SSL features\nand the broadcast outputs of\nthe"
        },
        {
          "The VAD module consists of\nfour 1D convolution layers": ""
        },
        {
          "The VAD module consists of\nfour 1D convolution layers": "VAD process."
        },
        {
          "The VAD module consists of\nfour 1D convolution layers": ""
        },
        {
          "The VAD module consists of\nfour 1D convolution layers": ""
        },
        {
          "The VAD module consists of\nfour 1D convolution layers": "C. SER module"
        },
        {
          "The VAD module consists of\nfour 1D convolution layers": ""
        },
        {
          "The VAD module consists of\nfour 1D convolution layers": "We use the segmented SSL features from the VAD module"
        },
        {
          "The VAD module consists of\nfour 1D convolution layers": "as input for the SER module. The SER process can be written"
        },
        {
          "The VAD module consists of\nfour 1D convolution layers": ""
        },
        {
          "The VAD module consists of\nfour 1D convolution layers": "as:"
        },
        {
          "The VAD module consists of\nfour 1D convolution layers": ""
        },
        {
          "The VAD module consists of\nfour 1D convolution layers": "(4)\nY = SER(F′\n; θser),"
        },
        {
          "The VAD module consists of\nfour 1D convolution layers": ""
        },
        {
          "The VAD module consists of\nfour 1D convolution layers": "θser\nwhere\nY is\na\npredicted\nemotion\nlabel\nand\nrepresents"
        },
        {
          "The VAD module consists of\nfour 1D convolution layers": ""
        },
        {
          "The VAD module consists of\nfour 1D convolution layers": "the\nparameters\nof\nthe\nSER module. Given\nthe\nsegmented"
        },
        {
          "The VAD module consists of\nfour 1D convolution layers": "768-dimensional SSL features,\nthe SER module ﬁrst applies"
        },
        {
          "The VAD module consists of\nfour 1D convolution layers": ""
        },
        {
          "The VAD module consists of\nfour 1D convolution layers": "dimensionality reduction from 768 to 256, followed by average"
        },
        {
          "The VAD module consists of\nfour 1D convolution layers": ""
        },
        {
          "The VAD module consists of\nfour 1D convolution layers": "pooling. The representations are then processed through three"
        },
        {
          "The VAD module consists of\nfour 1D convolution layers": ""
        },
        {
          "The VAD module consists of\nfour 1D convolution layers": "1D convolution layers and a FC layer with ReLU activation."
        },
        {
          "The VAD module consists of\nfour 1D convolution layers": ""
        },
        {
          "The VAD module consists of\nfour 1D convolution layers": "A subsequent\nself-attention\npooling\nlayer\n[33]\naggregates"
        },
        {
          "The VAD module consists of\nfour 1D convolution layers": ""
        },
        {
          "The VAD module consists of\nfour 1D convolution layers": "features along the time axis, which are then fed into a FC layer"
        },
        {
          "The VAD module consists of\nfour 1D convolution layers": ""
        },
        {
          "The VAD module consists of\nfour 1D convolution layers": "with ReLU activation and a FC layer with softmax activation"
        },
        {
          "The VAD module consists of\nfour 1D convolution layers": "for emotion classiﬁcation."
        },
        {
          "The VAD module consists of\nfour 1D convolution layers": "D. End-to-end training"
        },
        {
          "The VAD module consists of\nfour 1D convolution layers": ""
        },
        {
          "The VAD module consists of\nfour 1D convolution layers": "Some studies attempted to combine VAD and SER modules"
        },
        {
          "The VAD module consists of\nfour 1D convolution layers": ""
        },
        {
          "The VAD module consists of\nfour 1D convolution layers": "in a cascade manner [34] or using a multi-task approach [35],"
        },
        {
          "The VAD module consists of\nfour 1D convolution layers": ""
        },
        {
          "The VAD module consists of\nfour 1D convolution layers": "but end-to-end training of VAD and SER modules (E2E SSL-"
        },
        {
          "The VAD module consists of\nfour 1D convolution layers": ""
        },
        {
          "The VAD module consists of\nfour 1D convolution layers": "VAD-SER) has not been fully investigated.\nIn this study, we"
        },
        {
          "The VAD module consists of\nfour 1D convolution layers": ""
        },
        {
          "The VAD module consists of\nfour 1D convolution layers": "propose\nan\nend-to-end\napproach\nthat\njointly\noptimizes\nthe"
        },
        {
          "The VAD module consists of\nfour 1D convolution layers": ""
        },
        {
          "The VAD module consists of\nfour 1D convolution layers": "VAD and SER modules for SER loss using the SSL features,"
        },
        {
          "The VAD module consists of\nfour 1D convolution layers": ""
        },
        {
          "The VAD module consists of\nfour 1D convolution layers": "to address the issue of degradation in SER performance due"
        },
        {
          "The VAD module consists of\nfour 1D convolution layers": ""
        },
        {
          "The VAD module consists of\nfour 1D convolution layers": "to ﬂawed VAD outputs in noisy environments. Our entire end-"
        },
        {
          "The VAD module consists of\nfour 1D convolution layers": ""
        },
        {
          "The VAD module consists of\nfour 1D convolution layers": "to-end approach can be described as follows:"
        },
        {
          "The VAD module consists of\nfour 1D convolution layers": ""
        },
        {
          "The VAD module consists of\nfour 1D convolution layers": ""
        },
        {
          "The VAD module consists of\nfour 1D convolution layers": "Y = SER( SSL(X; θssl, θfeat)"
        },
        {
          "The VAD module consists of\nfour 1D convolution layers": "(5)"
        },
        {
          "The VAD module consists of\nfour 1D convolution layers": "⊙ VAD( SSL(X; θssl, θfeat); θvad); θser),"
        },
        {
          "The VAD module consists of\nfour 1D convolution layers": "To\ngive\neffective\nfeedback\nbetween\nthe VAD and\nSER"
        },
        {
          "The VAD module consists of\nfour 1D convolution layers": "modules\nfrom the\nstart of\nend-to-end training, we\ninitialize"
        },
        {
          "The VAD module consists of\nfour 1D convolution layers": "θssl\neach parameter with\nfrom publicly available pre-trained"
        },
        {
          "The VAD module consists of\nfour 1D convolution layers": "models and\nθfeat,\nθvad,\nand\nθser obtained by pre-training the"
        },
        {
          "The VAD module consists of\nfour 1D convolution layers": "SSL-VAD and SSL-SER,\nrespectively. Then,\nθfeat,\nθvad,\nand"
        },
        {
          "The VAD module consists of\nfour 1D convolution layers": "θser are ﬁne-tuned for SER loss to achieve better performance,"
        },
        {
          "The VAD module consists of\nfour 1D convolution layers": "θssl\nwhile\nis\nfrozen\nin\nthis\nstudy\nto\nreduce\ncomputational"
        },
        {
          "The VAD module consists of\nfour 1D convolution layers": "costs. Note that this end-to-end approach allows for ﬁne-tuning"
        },
        {
          "The VAD module consists of\nfour 1D convolution layers": "modules\neven on speech data\nthat has only emotion labels"
        },
        {
          "The VAD module consists of\nfour 1D convolution layers": "without speech/non-speech annotations for VAD."
        },
        {
          "The VAD module consists of\nfour 1D convolution layers": ""
        },
        {
          "The VAD module consists of\nfour 1D convolution layers": "III. EXPERIMENTAL SETUP"
        },
        {
          "The VAD module consists of\nfour 1D convolution layers": ""
        },
        {
          "The VAD module consists of\nfour 1D convolution layers": "A. Dataset"
        },
        {
          "The VAD module consists of\nfour 1D convolution layers": ""
        },
        {
          "The VAD module consists of\nfour 1D convolution layers": ""
        },
        {
          "The VAD module consists of\nfour 1D convolution layers": "We used the\nIEMOCAP dataset\n[36] which has\napproxi-"
        },
        {
          "The VAD module consists of\nfour 1D convolution layers": "mately 12 hours of English speech including 5 dyadic conver-"
        },
        {
          "The VAD module consists of\nfour 1D convolution layers": "sational\nsessions between two actors. There are in total 151"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "6\nE2E SSL-VAD-FT.SER\n50.2\n53.1\n48.9\n52.1\n49.0\n53.0": ""
        },
        {
          "6\nE2E SSL-VAD-FT.SER\n50.2\n53.1\n48.9\n52.1\n49.0\n53.0": "50.6\n53.7\n51.6\n54.2\n51.4\n54.9\n7\nE2E SSL-FT.VAD-FT.SER"
        },
        {
          "6\nE2E SSL-VAD-FT.SER\n50.2\n53.1\n48.9\n52.1\n49.0\n53.0": ""
        },
        {
          "6\nE2E SSL-VAD-FT.SER\n50.2\n53.1\n48.9\n52.1\n49.0\n53.0": ""
        },
        {
          "6\nE2E SSL-VAD-FT.SER\n50.2\n53.1\n48.9\n52.1\n49.0\n53.0": ""
        },
        {
          "6\nE2E SSL-VAD-FT.SER\n50.2\n53.1\n48.9\n52.1\n49.0\n53.0": "III-A. Conditions 2 and 3 serve as baselines, while Conditions"
        },
        {
          "6\nE2E SSL-VAD-FT.SER\n50.2\n53.1\n48.9\n52.1\n49.0\n53.0": ""
        },
        {
          "6\nE2E SSL-VAD-FT.SER\n50.2\n53.1\n48.9\n52.1\n49.0\n53.0": "4-7 represent our proposed approaches. The performance of"
        },
        {
          "6\nE2E SSL-VAD-FT.SER\n50.2\n53.1\n48.9\n52.1\n49.0\n53.0": ""
        },
        {
          "6\nE2E SSL-VAD-FT.SER\n50.2\n53.1\n48.9\n52.1\n49.0\n53.0": "Condition 2, which was based on the\nsame\narchitecture\nas"
        },
        {
          "6\nE2E SSL-VAD-FT.SER\n50.2\n53.1\n48.9\n52.1\n49.0\n53.0": ""
        },
        {
          "6\nE2E SSL-VAD-FT.SER\n50.2\n53.1\n48.9\n52.1\n49.0\n53.0": "Condition 1 but was evaluated with the extended utterances,"
        },
        {
          "6\nE2E SSL-VAD-FT.SER\n50.2\n53.1\n48.9\n52.1\n49.0\n53.0": ""
        },
        {
          "6\nE2E SSL-VAD-FT.SER\n50.2\n53.1\n48.9\n52.1\n49.0\n53.0": "shows a dramatic degradation compared to Condition 1 due"
        },
        {
          "6\nE2E SSL-VAD-FT.SER\n50.2\n53.1\n48.9\n52.1\n49.0\n53.0": ""
        },
        {
          "6\nE2E SSL-VAD-FT.SER\n50.2\n53.1\n48.9\n52.1\n49.0\n53.0": "to the\neffect of\nthe noisy extended utterances.\nIn Condition"
        },
        {
          "6\nE2E SSL-VAD-FT.SER\n50.2\n53.1\n48.9\n52.1\n49.0\n53.0": ""
        },
        {
          "6\nE2E SSL-VAD-FT.SER\n50.2\n53.1\n48.9\n52.1\n49.0\n53.0": "3, we implemented MarbleNet [23], one of the common deep"
        },
        {
          "6\nE2E SSL-VAD-FT.SER\n50.2\n53.1\n48.9\n52.1\n49.0\n53.0": ""
        },
        {
          "6\nE2E SSL-VAD-FT.SER\n50.2\n53.1\n48.9\n52.1\n49.0\n53.0": "learning-based VAD models, as a pre-processing VAD step for"
        },
        {
          "6\nE2E SSL-VAD-FT.SER\n50.2\n53.1\n48.9\n52.1\n49.0\n53.0": "the SSL-SER. MarbleNet utilized MFCC with 64 dimensions"
        },
        {
          "6\nE2E SSL-VAD-FT.SER\n50.2\n53.1\n48.9\n52.1\n49.0\n53.0": "of mel-ﬁlter bank, a 25 ms window size and a 10 ms overlap."
        },
        {
          "6\nE2E SSL-VAD-FT.SER\n50.2\n53.1\n48.9\n52.1\n49.0\n53.0": "MarbleNet\nand the SSL-SER were\nindividually trained and"
        },
        {
          "6\nE2E SSL-VAD-FT.SER\n50.2\n53.1\n48.9\n52.1\n49.0\n53.0": "the\nsegmented\nspeeches\ndetected\nby MarbleNet were\nfed"
        },
        {
          "6\nE2E SSL-VAD-FT.SER\n50.2\n53.1\n48.9\n52.1\n49.0\n53.0": "into\nthe\nSSL-SER. The\nresults\nof Condition\n3\nshow that"
        },
        {
          "6\nE2E SSL-VAD-FT.SER\n50.2\n53.1\n48.9\n52.1\n49.0\n53.0": "SER performance was degraded when MarbleNet was used,"
        },
        {
          "6\nE2E SSL-VAD-FT.SER\n50.2\n53.1\n48.9\n52.1\n49.0\n53.0": "compared to Condition 2. This indicates that simply combining"
        },
        {
          "6\nE2E SSL-VAD-FT.SER\n50.2\n53.1\n48.9\n52.1\n49.0\n53.0": "a VAD model with the SSL-SER in noisy environments can"
        },
        {
          "6\nE2E SSL-VAD-FT.SER\n50.2\n53.1\n48.9\n52.1\n49.0\n53.0": "cause detrimental effects on SER performance due to ﬂawed"
        },
        {
          "6\nE2E SSL-VAD-FT.SER\n50.2\n53.1\n48.9\n52.1\n49.0\n53.0": "VAD outputs."
        },
        {
          "6\nE2E SSL-VAD-FT.SER\n50.2\n53.1\n48.9\n52.1\n49.0\n53.0": "In Conditions 4-7, we\nexplored four different ﬁne-tuning"
        },
        {
          "6\nE2E SSL-VAD-FT.SER\n50.2\n53.1\n48.9\n52.1\n49.0\n53.0": "strategies\nfor our\napproach. Condition 4 consists of\nthe\nin-"
        },
        {
          "6\nE2E SSL-VAD-FT.SER\n50.2\n53.1\n48.9\n52.1\n49.0\n53.0": "dividually pre-trained SSL-VAD and SSL-SER without ﬁne-"
        },
        {
          "6\nE2E SSL-VAD-FT.SER\n50.2\n53.1\n48.9\n52.1\n49.0\n53.0": "tuning. The results of Conditions 3 and 4 show that replacing"
        },
        {
          "6\nE2E SSL-VAD-FT.SER\n50.2\n53.1\n48.9\n52.1\n49.0\n53.0": "the MFCC-based MarbleNet with\nthe SSL-VAD improved"
        },
        {
          "6\nE2E SSL-VAD-FT.SER\n50.2\n53.1\n48.9\n52.1\n49.0\n53.0": "SER performance,\nespecially when using wav2vec 2.0 and"
        },
        {
          "6\nE2E SSL-VAD-FT.SER\n50.2\n53.1\n48.9\n52.1\n49.0\n53.0": "HuBERT.\nIn Conditions\n5-7,\nall\ntrainable\nparameters were"
        },
        {
          "6\nE2E SSL-VAD-FT.SER\n50.2\n53.1\n48.9\n52.1\n49.0\n53.0": "directly optimized for SER loss\nin an end-to-end manner.\nIn"
        },
        {
          "6\nE2E SSL-VAD-FT.SER\n50.2\n53.1\n48.9\n52.1\n49.0\n53.0": ""
        },
        {
          "6\nE2E SSL-VAD-FT.SER\n50.2\n53.1\n48.9\n52.1\n49.0\n53.0": "θfeat\nCondition 5,\nand\nθvad were ﬁne-tuned while\nthe other"
        },
        {
          "6\nE2E SSL-VAD-FT.SER\n50.2\n53.1\n48.9\n52.1\n49.0\n53.0": ""
        },
        {
          "6\nE2E SSL-VAD-FT.SER\n50.2\n53.1\n48.9\n52.1\n49.0\n53.0": "parameters were\nfrozen. The\nperformance\nof Condition\n5,"
        },
        {
          "6\nE2E SSL-VAD-FT.SER\n50.2\n53.1\n48.9\n52.1\n49.0\n53.0": "on\nall\nthe\nSSL models,\nshows\nimprovement\ncompared\nto"
        },
        {
          "6\nE2E SSL-VAD-FT.SER\n50.2\n53.1\n48.9\n52.1\n49.0\n53.0": "Condition 4. This\nsuggests\nthat\nthe VAD module optimized"
        },
        {
          "6\nE2E SSL-VAD-FT.SER\n50.2\n53.1\n48.9\n52.1\n49.0\n53.0": "only for\nspeech/non-speech detection is not\nalways optimal"
        },
        {
          "6\nE2E SSL-VAD-FT.SER\n50.2\n53.1\n48.9\n52.1\n49.0\n53.0": "for SER, whereas\nour\napproach\nsuccessfully\nimproved\nthe"
        },
        {
          "6\nE2E SSL-VAD-FT.SER\n50.2\n53.1\n48.9\n52.1\n49.0\n53.0": "θfeat\nVAD outputs\nfor SER.\nIn Condition 6,\nand\nθser were"
        },
        {
          "6\nE2E SSL-VAD-FT.SER\n50.2\n53.1\n48.9\n52.1\n49.0\n53.0": "ﬁne-tuned, while the other parameters were frozen. Condition"
        },
        {
          "6\nE2E SSL-VAD-FT.SER\n50.2\n53.1\n48.9\n52.1\n49.0\n53.0": "6, with\nany SSL model,\nalso\nimproved SER performance"
        },
        {
          "6\nE2E SSL-VAD-FT.SER\n50.2\n53.1\n48.9\n52.1\n49.0\n53.0": "compared to Condition 4,\nindicating the importance of ﬁne-"
        },
        {
          "6\nE2E SSL-VAD-FT.SER\n50.2\n53.1\n48.9\n52.1\n49.0\n53.0": "tuning on the segmented SSL features rather than only on the"
        },
        {
          "6\nE2E SSL-VAD-FT.SER\n50.2\n53.1\n48.9\n52.1\n49.0\n53.0": "original utterances.\nIn Condition 7,\nθfeat,\nθvad, and ˆθser were"
        },
        {
          "6\nE2E SSL-VAD-FT.SER\n50.2\n53.1\n48.9\n52.1\n49.0\n53.0": "ﬁne-tuned and ˆθssl was\nfrozen. We observe that Condition 7"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "dialogues,\nincluding 10,039 utterances. The IEMOCAP pro-": "",
          "TABLE I": "SER PERFORMANCE (%UA, %WA) FOR THE REFERENTIAL CONDITION"
        },
        {
          "dialogues,\nincluding 10,039 utterances. The IEMOCAP pro-": "vides the timestamps for each utterance in a dialogue, as well",
          "TABLE I": ""
        },
        {
          "dialogues,\nincluding 10,039 utterances. The IEMOCAP pro-": "",
          "TABLE I": "(CONDITION 1) USING THE ORIGINAL UTTERANCES (ORIGINAL UTT.),"
        },
        {
          "dialogues,\nincluding 10,039 utterances. The IEMOCAP pro-": "as word-level alignments\nfor each utterance. The utterances",
          "TABLE I": ""
        },
        {
          "dialogues,\nincluding 10,039 utterances. The IEMOCAP pro-": "",
          "TABLE I": "AND FOR THE BASELINES (CONDITIONS 2 AND 3) AND PROPOSED"
        },
        {
          "dialogues,\nincluding 10,039 utterances. The IEMOCAP pro-": "contain silence at\nthe beginning, between words, and at\nthe",
          "TABLE I": "FINE-TUNING (FT) APPROACHES (CONDITIONS 4-7) USING THE"
        },
        {
          "dialogues,\nincluding 10,039 utterances. The IEMOCAP pro-": "",
          "TABLE I": "EXTENDED UTTERANCES."
        },
        {
          "dialogues,\nincluding 10,039 utterances. The IEMOCAP pro-": "end. The VAD module is trained based on these speech/non-",
          "TABLE I": ""
        },
        {
          "dialogues,\nincluding 10,039 utterances. The IEMOCAP pro-": "speech\nalignments, with\nthe\nsegments\nbeing\nnon-\n40 % of",
          "TABLE I": ""
        },
        {
          "dialogues,\nincluding 10,039 utterances. The IEMOCAP pro-": "",
          "TABLE I": "wav2vec 2.0\nHuBERT\nWavLM+"
        },
        {
          "dialogues,\nincluding 10,039 utterances. The IEMOCAP pro-": "speech. For SER, we merged emotion class\n“excited” with",
          "TABLE I": ""
        },
        {
          "dialogues,\nincluding 10,039 utterances. The IEMOCAP pro-": "",
          "TABLE I": "ID\nCondition\nUA\nWA\nUA\nWA\nUA\nWA"
        },
        {
          "dialogues,\nincluding 10,039 utterances. The IEMOCAP pro-": "“happy” and used audio annotated with one of\nfour\nlabels,",
          "TABLE I": ""
        },
        {
          "dialogues,\nincluding 10,039 utterances. The IEMOCAP pro-": "which are happy, sad, neutral, and angry.",
          "TABLE I": "1\nSSL-SER (original utt.)\n58.0\n59.5\n58.7\n60.2\n63.3\n64.7"
        },
        {
          "dialogues,\nincluding 10,039 utterances. The IEMOCAP pro-": "During evaluation,\nto simulate\na\nreal-world scenario, we",
          "TABLE I": "2\nSSL-SER\n46.0\n49.9\n42.6\n47.4\n43.8\n48.2"
        },
        {
          "dialogues,\nincluding 10,039 utterances. The IEMOCAP pro-": "extended the original utterances by concatenating the intervals",
          "TABLE I": "3\nMarbleNet-SSL-SER\n40.3\n39.6\n40.2\n39.4\n42.8\n42.1"
        },
        {
          "dialogues,\nincluding 10,039 utterances. The IEMOCAP pro-": "before and after\nthem. Speciﬁcally, we used the timestamps",
          "TABLE I": "4\nSSL-VAD-SER\n49.3\n47.0\n45.5\n49.0\n44.6\n41.7"
        },
        {
          "dialogues,\nincluding 10,039 utterances. The IEMOCAP pro-": "",
          "TABLE I": "5\nE2E SSL-FT.VAD-SER\n49.7\n48.0\n47.4\n50.5\n46.2\n43.6"
        },
        {
          "dialogues,\nincluding 10,039 utterances. The IEMOCAP pro-": "from the\nend of\nthe\nprevious utterance\nto\nthe\nstart\nof\nthe",
          "TABLE I": ""
        },
        {
          "dialogues,\nincluding 10,039 utterances. The IEMOCAP pro-": "",
          "TABLE I": "6\nE2E SSL-VAD-FT.SER\n50.2\n53.1\n48.9\n52.1\n49.0\n53.0"
        },
        {
          "dialogues,\nincluding 10,039 utterances. The IEMOCAP pro-": "next utterance, ensuring there was no overlap by subtracting",
          "TABLE I": ""
        },
        {
          "dialogues,\nincluding 10,039 utterances. The IEMOCAP pro-": "",
          "TABLE I": "50.6\n53.7\n51.6\n54.2\n51.4\n54.9\n7\nE2E SSL-FT.VAD-FT.SER"
        },
        {
          "dialogues,\nincluding 10,039 utterances. The IEMOCAP pro-": "extended utterances\nthe\n0.1 s. These\naveraged 11.7 s, while",
          "TABLE I": ""
        },
        {
          "dialogues,\nincluding 10,039 utterances. The IEMOCAP pro-": "original utterances averaged 4.5 s. The original and extended",
          "TABLE I": ""
        },
        {
          "dialogues,\nincluding 10,039 utterances. The IEMOCAP pro-": "utterances were contaminated with noise,\nrandomly selected",
          "TABLE I": ""
        },
        {
          "dialogues,\nincluding 10,039 utterances. The IEMOCAP pro-": "",
          "TABLE I": "III-A. Conditions 2 and 3 serve as baselines, while Conditions"
        },
        {
          "dialogues,\nincluding 10,039 utterances. The IEMOCAP pro-": "from the 37 recordings annotated as background noise in the",
          "TABLE I": ""
        },
        {
          "dialogues,\nincluding 10,039 utterances. The IEMOCAP pro-": "",
          "TABLE I": "4-7 represent our proposed approaches. The performance of"
        },
        {
          "dialogues,\nincluding 10,039 utterances. The IEMOCAP pro-": "MUSAN corpus [37] at Signal-to-Noise Ratio (SNR) levels of",
          "TABLE I": ""
        },
        {
          "dialogues,\nincluding 10,039 utterances. The IEMOCAP pro-": "",
          "TABLE I": "Condition 2, which was based on the\nsame\narchitecture\nas"
        },
        {
          "dialogues,\nincluding 10,039 utterances. The IEMOCAP pro-": "{10, 5, 0, −5, −10} dB. Speakers in the test set were excluded",
          "TABLE I": ""
        },
        {
          "dialogues,\nincluding 10,039 utterances. The IEMOCAP pro-": "",
          "TABLE I": "Condition 1 but was evaluated with the extended utterances,"
        },
        {
          "dialogues,\nincluding 10,039 utterances. The IEMOCAP pro-": "from the training and validation sets. Additionally, we made",
          "TABLE I": ""
        },
        {
          "dialogues,\nincluding 10,039 utterances. The IEMOCAP pro-": "",
          "TABLE I": "shows a dramatic degradation compared to Condition 1 due"
        },
        {
          "dialogues,\nincluding 10,039 utterances. The IEMOCAP pro-": "sure that utterances from the same dialogue were either all\nin",
          "TABLE I": ""
        },
        {
          "dialogues,\nincluding 10,039 utterances. The IEMOCAP pro-": "",
          "TABLE I": "to the\neffect of\nthe noisy extended utterances.\nIn Condition"
        },
        {
          "dialogues,\nincluding 10,039 utterances. The IEMOCAP pro-": "the training set or all\nin the validation set.",
          "TABLE I": ""
        },
        {
          "dialogues,\nincluding 10,039 utterances. The IEMOCAP pro-": "",
          "TABLE I": "3, we implemented MarbleNet [23], one of the common deep"
        },
        {
          "dialogues,\nincluding 10,039 utterances. The IEMOCAP pro-": "B. Model conﬁguration",
          "TABLE I": ""
        },
        {
          "dialogues,\nincluding 10,039 utterances. The IEMOCAP pro-": "",
          "TABLE I": "learning-based VAD models, as a pre-processing VAD step for"
        },
        {
          "dialogues,\nincluding 10,039 utterances. The IEMOCAP pro-": "We\nused pre-trained SSL models\nincluding wav2vec 2.0",
          "TABLE I": "the SSL-SER. MarbleNet utilized MFCC with 64 dimensions"
        },
        {
          "dialogues,\nincluding 10,039 utterances. The IEMOCAP pro-": "BASE [15], HuBERT BASE [16], and WavLM BASE+ [17],",
          "TABLE I": "of mel-ﬁlter bank, a 25 ms window size and a 10 ms overlap."
        },
        {
          "dialogues,\nincluding 10,039 utterances. The IEMOCAP pro-": "which are publicly available. For simplicity,\nthe term “BASE”",
          "TABLE I": "MarbleNet\nand the SSL-SER were\nindividually trained and"
        },
        {
          "dialogues,\nincluding 10,039 utterances. The IEMOCAP pro-": "will be omitted hereafter. Each model has approximately 95",
          "TABLE I": "the\nsegmented\nspeeches\ndetected\nby MarbleNet were\nfed"
        },
        {
          "dialogues,\nincluding 10,039 utterances. The IEMOCAP pro-": "million parameters. Wav2vec 2.0 and HuBERT were trained",
          "TABLE I": "into\nthe\nSSL-SER. The\nresults\nof Condition\n3\nshow that"
        },
        {
          "dialogues,\nincluding 10,039 utterances. The IEMOCAP pro-": "with the concatenation of the train-clean-100, train-clean-360,",
          "TABLE I": "SER performance was degraded when MarbleNet was used,"
        },
        {
          "dialogues,\nincluding 10,039 utterances. The IEMOCAP pro-": "and train-other-500 subsets from the LibriSpeech dataset [38].",
          "TABLE I": "compared to Condition 2. This indicates that simply combining"
        },
        {
          "dialogues,\nincluding 10,039 utterances. The IEMOCAP pro-": "WavLM+ was\ntrained with the Libri-Light\n[39], GigaSpeech",
          "TABLE I": "a VAD model with the SSL-SER in noisy environments can"
        },
        {
          "dialogues,\nincluding 10,039 utterances. The IEMOCAP pro-": "[40], and VoxPopuli datasets\n[41]. The SSL-VAD and SSL-",
          "TABLE I": "cause detrimental effects on SER performance due to ﬂawed"
        },
        {
          "dialogues,\nincluding 10,039 utterances. The IEMOCAP pro-": "SER were pre-trained and initialized using these SSL models.",
          "TABLE I": "VAD outputs."
        },
        {
          "dialogues,\nincluding 10,039 utterances. The IEMOCAP pro-": "They were trained using the Adam optimizer [42] with a ﬁxed",
          "TABLE I": "In Conditions 4-7, we\nexplored four different ﬁne-tuning"
        },
        {
          "dialogues,\nincluding 10,039 utterances. The IEMOCAP pro-": "learning\nrate\nof\nand\na\nbatch\nsize\nof\n8, without\n1 × 10−4",
          "TABLE I": "strategies\nfor our\napproach. Condition 4 consists of\nthe\nin-"
        },
        {
          "dialogues,\nincluding 10,039 utterances. The IEMOCAP pro-": "data augmentation,\nfor at most 100 epochs.\nIn the proposed",
          "TABLE I": "dividually pre-trained SSL-VAD and SSL-SER without ﬁne-"
        },
        {
          "dialogues,\nincluding 10,039 utterances. The IEMOCAP pro-": "approach,\nthe modules were then ﬁne-tuned using SER loss",
          "TABLE I": "tuning. The results of Conditions 3 and 4 show that replacing"
        },
        {
          "dialogues,\nincluding 10,039 utterances. The IEMOCAP pro-": "with the same optimizer algorithm and learning rate. During",
          "TABLE I": "the MFCC-based MarbleNet with\nthe SSL-VAD improved"
        },
        {
          "dialogues,\nincluding 10,039 utterances. The IEMOCAP pro-": "the training of each system, the parameters of the SSL encoder",
          "TABLE I": "SER performance,\nespecially when using wav2vec 2.0 and"
        },
        {
          "dialogues,\nincluding 10,039 utterances. The IEMOCAP pro-": "were not updated.",
          "TABLE I": "HuBERT.\nIn Conditions\n5-7,\nall\ntrainable\nparameters were"
        },
        {
          "dialogues,\nincluding 10,039 utterances. The IEMOCAP pro-": "",
          "TABLE I": "directly optimized for SER loss\nin an end-to-end manner.\nIn"
        },
        {
          "dialogues,\nincluding 10,039 utterances. The IEMOCAP pro-": "IV. RESULTS",
          "TABLE I": ""
        },
        {
          "dialogues,\nincluding 10,039 utterances. The IEMOCAP pro-": "",
          "TABLE I": "θfeat\nCondition 5,\nand\nθvad were ﬁne-tuned while\nthe other"
        },
        {
          "dialogues,\nincluding 10,039 utterances. The IEMOCAP pro-": "A. Performance on SER",
          "TABLE I": ""
        },
        {
          "dialogues,\nincluding 10,039 utterances. The IEMOCAP pro-": "",
          "TABLE I": "parameters were\nfrozen. The\nperformance\nof Condition\n5,"
        },
        {
          "dialogues,\nincluding 10,039 utterances. The IEMOCAP pro-": "In\nthis\nsection, we\npresent\nthe\nevaluation results\non\nthe",
          "TABLE I": "on\nall\nthe\nSSL models,\nshows\nimprovement\ncompared\nto"
        },
        {
          "dialogues,\nincluding 10,039 utterances. The IEMOCAP pro-": "IEMOCAP dataset. We computed Unweighted Accuracy (UA)",
          "TABLE I": "Condition 4. This\nsuggests\nthat\nthe VAD module optimized"
        },
        {
          "dialogues,\nincluding 10,039 utterances. The IEMOCAP pro-": "and Weighted Accuracy (WA) on the test\nsets, as\nshown in",
          "TABLE I": "only for\nspeech/non-speech detection is not\nalways optimal"
        },
        {
          "dialogues,\nincluding 10,039 utterances. The IEMOCAP pro-": "Table\nI. UA is\nthe\naverage\nrecall\nacross\nall\ncategories\nand",
          "TABLE I": "for SER, whereas\nour\napproach\nsuccessfully\nimproved\nthe"
        },
        {
          "dialogues,\nincluding 10,039 utterances. The IEMOCAP pro-": "WA is the total number of correct predictions divided by the",
          "TABLE I": "θfeat\nVAD outputs\nfor SER.\nIn Condition 6,\nand\nθser were"
        },
        {
          "dialogues,\nincluding 10,039 utterances. The IEMOCAP pro-": "total\nnumber of\nsamples. First, we\nreport\nthe\nperformance",
          "TABLE I": "ﬁne-tuned, while the other parameters were frozen. Condition"
        },
        {
          "dialogues,\nincluding 10,039 utterances. The IEMOCAP pro-": "of\nthe SSL-SER, which does not\ninclude the VAD module,",
          "TABLE I": "6, with\nany SSL model,\nalso\nimproved SER performance"
        },
        {
          "dialogues,\nincluding 10,039 utterances. The IEMOCAP pro-": "as\na\nreference\n(Condition\n1).\nIt was\nevaluated\nusing\nthe",
          "TABLE I": "compared to Condition 4,\nindicating the importance of ﬁne-"
        },
        {
          "dialogues,\nincluding 10,039 utterances. The IEMOCAP pro-": "original utterances,\nas\nin previous\nstudies\n[18, 21]. For\nthe",
          "TABLE I": "tuning on the segmented SSL features rather than only on the"
        },
        {
          "dialogues,\nincluding 10,039 utterances. The IEMOCAP pro-": "evaluation of Conditions 2-7, we used the extended utterances",
          "TABLE I": "original utterances.\nIn Condition 7,\nθfeat,\nθvad, and ˆθser were"
        },
        {
          "dialogues,\nincluding 10,039 utterances. The IEMOCAP pro-": "containing intervals before and after, as described in section",
          "TABLE I": "ﬁne-tuned and ˆθssl was\nfrozen. We observe that Condition 7"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "VAD PERFORMANCE WITH ACCURACY (%ACC), PRECISION (%PREC),": ""
        },
        {
          "VAD PERFORMANCE WITH ACCURACY (%ACC), PRECISION (%PREC),": ""
        },
        {
          "VAD PERFORMANCE WITH ACCURACY (%ACC), PRECISION (%PREC),": ""
        },
        {
          "VAD PERFORMANCE WITH ACCURACY (%ACC), PRECISION (%PREC),": "SSL Model"
        },
        {
          "VAD PERFORMANCE WITH ACCURACY (%ACC), PRECISION (%PREC),": ""
        },
        {
          "VAD PERFORMANCE WITH ACCURACY (%ACC), PRECISION (%PREC),": ""
        },
        {
          "VAD PERFORMANCE WITH ACCURACY (%ACC), PRECISION (%PREC),": ""
        },
        {
          "VAD PERFORMANCE WITH ACCURACY (%ACC), PRECISION (%PREC),": "HuBERT"
        },
        {
          "VAD PERFORMANCE WITH ACCURACY (%ACC), PRECISION (%PREC),": ""
        },
        {
          "VAD PERFORMANCE WITH ACCURACY (%ACC), PRECISION (%PREC),": ""
        },
        {
          "VAD PERFORMANCE WITH ACCURACY (%ACC), PRECISION (%PREC),": "WavLM+"
        },
        {
          "VAD PERFORMANCE WITH ACCURACY (%ACC), PRECISION (%PREC),": ""
        },
        {
          "VAD PERFORMANCE WITH ACCURACY (%ACC), PRECISION (%PREC),": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "SSL-VAD-SER\n90.2\n74.3\n92.6": "WavLM+"
        },
        {
          "SSL-VAD-SER\n90.2\n74.3\n92.6": "E2E SSL-FT.VAD-FT.SER\n89.6\n73.8\n93.0"
        },
        {
          "SSL-VAD-SER\n90.2\n74.3\n92.6": ""
        },
        {
          "SSL-VAD-SER\n90.2\n74.3\n92.6": "Oracle Segments\nOracle Segments"
        },
        {
          "SSL-VAD-SER\n90.2\n74.3\n92.6": ""
        },
        {
          "SSL-VAD-SER\n90.2\n74.3\n92.6": ""
        },
        {
          "SSL-VAD-SER\n90.2\n74.3\n92.6": "0\n2\n4\n6\n0\n2\n4\n6\n8"
        },
        {
          "SSL-VAD-SER\n90.2\n74.3\n92.6": "(cid:0)me [s]\n(cid:1)me [s]"
        },
        {
          "SSL-VAD-SER\n90.2\n74.3\n92.6": ""
        },
        {
          "SSL-VAD-SER\n90.2\n74.3\n92.6": "MarbleNet-SSL-SER\nMarbleNet-SSL-SER"
        },
        {
          "SSL-VAD-SER\n90.2\n74.3\n92.6": "0\n2\n4\n6\n0\n2\n4\n6\n8"
        },
        {
          "SSL-VAD-SER\n90.2\n74.3\n92.6": "(cid:0)me [s]\n(cid:1)me [s]"
        },
        {
          "SSL-VAD-SER\n90.2\n74.3\n92.6": ""
        },
        {
          "SSL-VAD-SER\n90.2\n74.3\n92.6": "SSL-VAD-SER\nSSL-VAD-SER"
        },
        {
          "SSL-VAD-SER\n90.2\n74.3\n92.6": ""
        },
        {
          "SSL-VAD-SER\n90.2\n74.3\n92.6": ""
        },
        {
          "SSL-VAD-SER\n90.2\n74.3\n92.6": "0\n2\n4\n6\n0\n2\n4\n6\n8"
        },
        {
          "SSL-VAD-SER\n90.2\n74.3\n92.6": "(cid:0)me [s]\n(cid:1)me [s]"
        },
        {
          "SSL-VAD-SER\n90.2\n74.3\n92.6": ""
        },
        {
          "SSL-VAD-SER\n90.2\n74.3\n92.6": "E2E SSL-FT.VAD-FT.SER\nE2E SSL-FT.VAD-FT.SER"
        },
        {
          "SSL-VAD-SER\n90.2\n74.3\n92.6": ""
        },
        {
          "SSL-VAD-SER\n90.2\n74.3\n92.6": ""
        },
        {
          "SSL-VAD-SER\n90.2\n74.3\n92.6": "6\n8\n0\n2\n4\n6\n0\n2\n4"
        },
        {
          "SSL-VAD-SER\n90.2\n74.3\n92.6": "(cid:1)me [s]\n(cid:0)me [s]"
        },
        {
          "SSL-VAD-SER\n90.2\n74.3\n92.6": ""
        },
        {
          "SSL-VAD-SER\n90.2\n74.3\n92.6": "(a) HuBERT\n(b) WavLM+"
        },
        {
          "SSL-VAD-SER\n90.2\n74.3\n92.6": ""
        },
        {
          "SSL-VAD-SER\n90.2\n74.3\n92.6": ""
        },
        {
          "SSL-VAD-SER\n90.2\n74.3\n92.6": "Fig. 2.\nVisualizations\nof\nthe oracle\nsegments\nannotated in the\nIEMOCAP"
        },
        {
          "SSL-VAD-SER\n90.2\n74.3\n92.6": ""
        },
        {
          "SSL-VAD-SER\n90.2\n74.3\n92.6": "and the VAD outputs\nin Conditions 3, 4, and 7 on HuBERT and WavLM+,"
        },
        {
          "SSL-VAD-SER\n90.2\n74.3\n92.6": "sampled from results with an SNR of 0 dB."
        },
        {
          "SSL-VAD-SER\n90.2\n74.3\n92.6": ""
        },
        {
          "SSL-VAD-SER\n90.2\n74.3\n92.6": ""
        },
        {
          "SSL-VAD-SER\n90.2\n74.3\n92.6": "further\nimproved SER performance on all\nthe SSL models,"
        },
        {
          "SSL-VAD-SER\n90.2\n74.3\n92.6": "indicating that our\nend-to-end approach successfully jointly"
        },
        {
          "SSL-VAD-SER\n90.2\n74.3\n92.6": "ﬁne-tuned\nboth\nthe VAD and SER modules\nfor SER. For"
        },
        {
          "SSL-VAD-SER\n90.2\n74.3\n92.6": "example, comparing Condition 3 and 7 on WavLM+,\nthe UA"
        },
        {
          "SSL-VAD-SER\n90.2\n74.3\n92.6": "increased from 42.8 % to 51.4 % and the WA increased from"
        },
        {
          "SSL-VAD-SER\n90.2\n74.3\n92.6": "42.1 % to 54.9 %."
        },
        {
          "SSL-VAD-SER\n90.2\n74.3\n92.6": ""
        },
        {
          "SSL-VAD-SER\n90.2\n74.3\n92.6": "B. Analysis of VAD outputs"
        },
        {
          "SSL-VAD-SER\n90.2\n74.3\n92.6": ""
        },
        {
          "SSL-VAD-SER\n90.2\n74.3\n92.6": "To investigate the impact of VAD outputs on SER perfor-"
        },
        {
          "SSL-VAD-SER\n90.2\n74.3\n92.6": ""
        },
        {
          "SSL-VAD-SER\n90.2\n74.3\n92.6": "mance, we analyzed both VAD performance and its outputs."
        },
        {
          "SSL-VAD-SER\n90.2\n74.3\n92.6": ""
        },
        {
          "SSL-VAD-SER\n90.2\n74.3\n92.6": "Table II shows accuracy, precision and recall scores for VAD,"
        },
        {
          "SSL-VAD-SER\n90.2\n74.3\n92.6": ""
        },
        {
          "SSL-VAD-SER\n90.2\n74.3\n92.6": "and Fig. 2 offers visualized examples of\nthe VAD outputs,"
        },
        {
          "SSL-VAD-SER\n90.2\n74.3\n92.6": ""
        },
        {
          "SSL-VAD-SER\n90.2\n74.3\n92.6": "using HuBERT and WavLM+ under Conditions 3, 4, and 7."
        },
        {
          "SSL-VAD-SER\n90.2\n74.3\n92.6": "Accuracy is\nthe ratio of correctly predicted speech and non-"
        },
        {
          "SSL-VAD-SER\n90.2\n74.3\n92.6": "speech\nsegments\nto\nall\nsegments. Precision\nis\nthe\nratio\nof"
        },
        {
          "SSL-VAD-SER\n90.2\n74.3\n92.6": "correctly identiﬁed speech segments to all segments predicted"
        },
        {
          "SSL-VAD-SER\n90.2\n74.3\n92.6": "as\nspeech. Recall\nis\nthe\nratio of\ncorrectly identiﬁed speech"
        },
        {
          "SSL-VAD-SER\n90.2\n74.3\n92.6": "segments to all actual speech segments."
        },
        {
          "SSL-VAD-SER\n90.2\n74.3\n92.6": "From the\nresults,\nalthough MarbleNet\nexhibited\nhigher"
        },
        {
          "SSL-VAD-SER\n90.2\n74.3\n92.6": "precision,\nit\noften missed\nspeech\nsegments\nas\ndepicted\nin"
        },
        {
          "SSL-VAD-SER\n90.2\n74.3\n92.6": "Fig. 2(a), which can lead to degrade SER performance.\nIn"
        },
        {
          "SSL-VAD-SER\n90.2\n74.3\n92.6": "Condition\n4\nand\n7, we\ncannot\nsee\na\nsigniﬁcant\ndifference"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "REFERENCES": "",
          "separable convolutional neural network for voice activity detection,” in": "Proc.\nICASSP, 2021, pp. 6818–6822."
        },
        {
          "REFERENCES": "",
          "separable convolutional neural network for voice activity detection,” in": "[24] N. Wilkinson\nand\nT. Niesler,\n“A hybrid\ncnn-bilstm voice\nactivity"
        },
        {
          "REFERENCES": "[1] V. Dimitrios\nand K. Constantine,\n“Emotional\nspeech recognition: Re-",
          "separable convolutional neural network for voice activity detection,” in": ""
        },
        {
          "REFERENCES": "",
          "separable convolutional neural network for voice activity detection,” in": "detector,” in Proc.\nICASSP, 2021, pp. 6803–6807."
        },
        {
          "REFERENCES": "sources, features, and methods,” Computer Speech & Language, vol. 48,",
          "separable convolutional neural network for voice activity detection,” in": ""
        },
        {
          "REFERENCES": "",
          "separable convolutional neural network for voice activity detection,” in": "[25] Q. Yang, Q. Liu, N. Li, M. Ge, Z. Song, and H. Li, “Svad: A robust,"
        },
        {
          "REFERENCES": "no. 9, pp. 1162–1181, 2006.",
          "separable convolutional neural network for voice activity detection,” in": ""
        },
        {
          "REFERENCES": "",
          "separable convolutional neural network for voice activity detection,” in": "low-power, and light-weight voice activity detection with spiking neural"
        },
        {
          "REFERENCES": "[2] C.-H. Wu, Y.-M. Huang, and J.-P. Hwang, “Review of affective\ncom-",
          "separable convolutional neural network for voice activity detection,” in": ""
        },
        {
          "REFERENCES": "",
          "separable convolutional neural network for voice activity detection,” in": "networks,” in Proc.\nICASSP, 2024, pp. 221–225."
        },
        {
          "REFERENCES": "puting in education/learning: Trends and challenges,” British Journal of",
          "separable convolutional neural network for voice activity detection,” in": ""
        },
        {
          "REFERENCES": "",
          "separable convolutional neural network for voice activity detection,” in": "[26]\nJ. Sohn, N. S. Kim,\nand W. Sung,\n“A statistical model-based\nvoice"
        },
        {
          "REFERENCES": "Educational Technology, vol. 47, no. 6, pp. 1304–1323, 2016.",
          "separable convolutional neural network for voice activity detection,” in": ""
        },
        {
          "REFERENCES": "",
          "separable convolutional neural network for voice activity detection,” in": "IEEE signal processing\nactivity\ndetection,”\nletters,\nvol. 6, no. 1, pp."
        },
        {
          "REFERENCES": "[3] R. Cowie, E. Douglas-Cowie, N. Tsapatsoulis, G. Votsis, S. Kollias,",
          "separable convolutional neural network for voice activity detection,” in": ""
        },
        {
          "REFERENCES": "",
          "separable convolutional neural network for voice activity detection,” in": "1–3, 1999."
        },
        {
          "REFERENCES": "W. Fellenz, and J. G. Taylor, “Emotion recognition in human-computer",
          "separable convolutional neural network for voice activity detection,” in": ""
        },
        {
          "REFERENCES": "",
          "separable convolutional neural network for voice activity detection,” in": "[27]\nJ.-H. Chang, N. S. Kim, and S. K. Mitra, “Voice activity detection based"
        },
        {
          "REFERENCES": "interaction,” IEEE Signal processing magazine, vol. 18, no. 1, pp. 32–80,",
          "separable convolutional neural network for voice activity detection,” in": ""
        },
        {
          "REFERENCES": "",
          "separable convolutional neural network for voice activity detection,” in": "on multiple statistical models,” IEEE Transactions on Signal Processing,"
        },
        {
          "REFERENCES": "2001.",
          "separable convolutional neural network for voice activity detection,” in": ""
        },
        {
          "REFERENCES": "",
          "separable convolutional neural network for voice activity detection,” in": "vol. 54, no. 6, pp. 1965–1976, 2006."
        },
        {
          "REFERENCES": "[4] N. A. Vaidyam, H. Wisniewski, J. D. Halamka, M. S. Kashavan, and J. B.",
          "separable convolutional neural network for voice activity detection,” in": ""
        },
        {
          "REFERENCES": "",
          "separable convolutional neural network for voice activity detection,” in": "[28]\n“WebRTC VAD.” [Online]. Available: https://webrtc.org/"
        },
        {
          "REFERENCES": "Torous, “Chatbots and conversational agents in mental health: a review",
          "separable convolutional neural network for voice activity detection,” in": ""
        },
        {
          "REFERENCES": "",
          "separable convolutional neural network for voice activity detection,” in": "[29] Y. Huang,\nJ. Xiao, K. Tian, A. Wu, and G. Zhang, “Research on ro-"
        },
        {
          "REFERENCES": "The Canadian\nJournal\nof\nthe\npsychiatric\nlandscape,”\nof Psychiatry,",
          "separable convolutional neural network for voice activity detection,” in": ""
        },
        {
          "REFERENCES": "",
          "separable convolutional neural network for voice activity detection,” in": "bustness of emotion recognition under environmental noise conditions,”"
        },
        {
          "REFERENCES": "vol. 64, no. 7, pp. 456–464, 2019.",
          "separable convolutional neural network for voice activity detection,” in": ""
        },
        {
          "REFERENCES": "",
          "separable convolutional neural network for voice activity detection,” in": "IEEE Access, vol. 7, pp. 142 009–142 021, 2019."
        },
        {
          "REFERENCES": "[5] V. Petrushin, “Emotion in speech: Recognition and application to call",
          "separable convolutional neural network for voice activity detection,” in": ""
        },
        {
          "REFERENCES": "",
          "separable convolutional neural network for voice activity detection,” in": "[30] A. Pasad,\nJ.-C. Chou, and K. Livescu, “Layer-wise analysis of a self-"
        },
        {
          "REFERENCES": "centers,” in Proc. ANNIE, vol. 710, 1999, p. 22.",
          "separable convolutional neural network for voice activity detection,” in": ""
        },
        {
          "REFERENCES": "",
          "separable convolutional neural network for voice activity detection,” in": "supervised speech representation model,” in Proc. ASRU, 2021, pp. 914–"
        },
        {
          "REFERENCES": "[6] R. P. Bagozzi, M. Gopinath, and P. U. Nyer, “The role of emotions\nin",
          "separable convolutional neural network for voice activity detection,” in": ""
        },
        {
          "REFERENCES": "",
          "separable convolutional neural network for voice activity detection,” in": "921."
        },
        {
          "REFERENCES": "marketing,” Journal of\nthe academy of marketing science, vol. 27, no. 2,",
          "separable convolutional neural network for voice activity detection,” in": ""
        },
        {
          "REFERENCES": "",
          "separable convolutional neural network for voice activity detection,” in": "[31] B. Karan,\nJ.\nJ. van V¨uren, F. de Wet, and T. Niesler, “A transformer-"
        },
        {
          "REFERENCES": "pp. 184–206, 1999.",
          "separable convolutional neural network for voice activity detection,” in": ""
        },
        {
          "REFERENCES": "",
          "separable convolutional neural network for voice activity detection,” in": "based voice\nactivity\ndetector,”\nin Proc.\nInterspeech,\n2024, pp. 3819–"
        },
        {
          "REFERENCES": "[7]\nT. L. Nwe, S. W. Foo, and L. C. De Silva, “Speech emotion recognition",
          "separable convolutional neural network for voice activity detection,” in": ""
        },
        {
          "REFERENCES": "",
          "separable convolutional neural network for voice activity detection,” in": "3823."
        },
        {
          "REFERENCES": "using hidden markov models,” Speech communication, vol. 41, no. 4,",
          "separable convolutional neural network for voice activity detection,” in": ""
        },
        {
          "REFERENCES": "",
          "separable convolutional neural network for voice activity detection,” in": "[32]\n“. V. p.-t.\ne.-g. v.\na. d. V. n. d. Silero Team and language\nclassiﬁer,"
        },
        {
          "REFERENCES": "pp. 603–623, 2003.",
          "separable convolutional neural network for voice activity detection,” in": ""
        },
        {
          "REFERENCES": "",
          "separable convolutional neural network for voice activity detection,” in": "2021.\n[Online]. Available: https://github.com/snakers4/silero-vad"
        },
        {
          "REFERENCES": "[8] A. Milton, S. S. Roy, and S. T. Selvi, “Svm scheme for speech emotion",
          "separable convolutional neural network for voice activity detection,” in": ""
        },
        {
          "REFERENCES": "",
          "separable convolutional neural network for voice activity detection,” in": "[33]\nP.\nSafari, M.\nIndia,\nand\nJ. Hernando,\n“Self-attention\nencoding\nand"
        },
        {
          "REFERENCES": "International\nJournal\nof Computer\nrecognition\nusing mfcc\nfeature,”",
          "separable convolutional neural network for voice activity detection,” in": ""
        },
        {
          "REFERENCES": "",
          "separable convolutional neural network for voice activity detection,” in": "pooling for\nspeaker\nrecognition,” in Proc.\nInterspeech, 2020, pp. 941–"
        },
        {
          "REFERENCES": "Applications, vol. 69, no. 9, pp. 34–39, 2013.",
          "separable convolutional neural network for voice activity detection,” in": ""
        },
        {
          "REFERENCES": "",
          "separable convolutional neural network for voice activity detection,” in": "945."
        },
        {
          "REFERENCES": "[9] W. Lim, D.\nJang,\nand T. Lee,\n“Speech\nemotion\nrecognition\nusing",
          "separable convolutional neural network for voice activity detection,” in": ""
        },
        {
          "REFERENCES": "",
          "separable convolutional neural network for voice activity detection,” in": "[34] M. F. Alghifari, T. S. Gunawan, M. A. b. W. Nordin, S. A. A. Qadri,"
        },
        {
          "REFERENCES": "convolutional\nand recurrent neural networks,”\nin Proc. APSIPA, 2016,",
          "separable convolutional neural network for voice activity detection,” in": ""
        },
        {
          "REFERENCES": "",
          "separable convolutional neural network for voice activity detection,” in": "M. Kartiwi,\nand Z.\nJanin,\n“On the use of voice\nactivity detection\nin"
        },
        {
          "REFERENCES": "pp. 1–4.",
          "separable convolutional neural network for voice activity detection,” in": ""
        },
        {
          "REFERENCES": "",
          "separable convolutional neural network for voice activity detection,” in": "of Electrical Engineering\nand\nspeech\nemotion\nrecognition,” Bulletin"
        },
        {
          "REFERENCES": "[10] Y. Xie, R. Liang,\nZ. Liang, C. Huang, C. Zou,\nand B.\nSchuller,",
          "separable convolutional neural network for voice activity detection,” in": ""
        },
        {
          "REFERENCES": "",
          "separable convolutional neural network for voice activity detection,” in": "Informatics, vol. 8, no. 4, pp. 3607–3611, 2019."
        },
        {
          "REFERENCES": "“Speech emotion classiﬁcation using attention-based lstm,” IEEE/ACM",
          "separable convolutional neural network for voice activity detection,” in": ""
        },
        {
          "REFERENCES": "",
          "separable convolutional neural network for voice activity detection,” in": "[35] W. u, C. Zhang, and P. C. Woodland, “Integrating emotion recognition"
        },
        {
          "REFERENCES": "Transactions\non Audio,\nSpeech,\nand\nLanguage Processing,\nvol.\n27,",
          "separable convolutional neural network for voice activity detection,” in": ""
        },
        {
          "REFERENCES": "",
          "separable convolutional neural network for voice activity detection,” in": "with speech recognition and speaker diarisation for conversations,”\nin"
        },
        {
          "REFERENCES": "no. 11, pp. 1675–1685, 2019.",
          "separable convolutional neural network for voice activity detection,” in": ""
        },
        {
          "REFERENCES": "",
          "separable convolutional neural network for voice activity detection,” in": "Proc.\nInterspeech, 2023, pp. 941–945."
        },
        {
          "REFERENCES": "[11] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkorei, L. Jones, A. N. Gomez,",
          "separable convolutional neural network for voice activity detection,” in": ""
        },
        {
          "REFERENCES": "",
          "separable convolutional neural network for voice activity detection,” in": "[36] C. Busso, M. Bulut, C.-C. Lee, A. Kazemzadeh, E. Mower, S. Kim, J. N."
        },
        {
          "REFERENCES": "L. Kaiser,\nand\nI. Polosukhin,\n“Attention\nis\nall\nyou\nneed,”\nin Proc.",
          "separable convolutional neural network for voice activity detection,” in": ""
        },
        {
          "REFERENCES": "",
          "separable convolutional neural network for voice activity detection,” in": "Chang, S. Lee, and S. S. Narayanan, “IEMOCAP: Interactive emotional"
        },
        {
          "REFERENCES": "APSIPA, 2016, pp. 1–4.",
          "separable convolutional neural network for voice activity detection,” in": ""
        },
        {
          "REFERENCES": "",
          "separable convolutional neural network for voice activity detection,” in": "dyadic motion capture database,” Language\nresources and evaluation,"
        },
        {
          "REFERENCES": "[12]\nL. Pepino, P. Riera, and L. Ferrer, “Emotion recognition from speech",
          "separable convolutional neural network for voice activity detection,” in": ""
        },
        {
          "REFERENCES": "",
          "separable convolutional neural network for voice activity detection,” in": "vol. 42, pp. 335–359, 2008."
        },
        {
          "REFERENCES": "Proc.\nIEEE Spoken\nLanguage\nusing wav2vec\n2.0\nembeddings,”\nin",
          "separable convolutional neural network for voice activity detection,” in": ""
        },
        {
          "REFERENCES": "",
          "separable convolutional neural network for voice activity detection,” in": "[37] D. Snyder, G. Chen, and D. Povey, “MUSAN: A music,\nspeech,\nand"
        },
        {
          "REFERENCES": "Technology Workshop, 2021, pp. 3400–3404.",
          "separable convolutional neural network for voice activity detection,” in": ""
        },
        {
          "REFERENCES": "",
          "separable convolutional neural network for voice activity detection,” in": "noise corpus,” arXiv:1510.08484, 2015."
        },
        {
          "REFERENCES": "[13] M. Macary, M. Tahon, Y. Est`eve, and A. Rousseau, “On the use of self-",
          "separable convolutional neural network for voice activity detection,” in": ""
        },
        {
          "REFERENCES": "",
          "separable convolutional neural network for voice activity detection,” in": "[38] V. Panayotov, G. Chen, D. Povey, and S. Khudanpur, “LibriSpeech: An"
        },
        {
          "REFERENCES": "supervised\npre-trained\nacoustic\nand linguistic\nfeatures\nfor\ncontinuous",
          "separable convolutional neural network for voice activity detection,” in": ""
        },
        {
          "REFERENCES": "",
          "separable convolutional neural network for voice activity detection,” in": "ASR corpus based on public domain audio books,” in Proc.\nICASSP,"
        },
        {
          "REFERENCES": "speech emotion recognition,” in Proc. IEEE Spoken Language Technol-",
          "separable convolutional neural network for voice activity detection,” in": ""
        },
        {
          "REFERENCES": "",
          "separable convolutional neural network for voice activity detection,” in": "2015, pp. 5206–5210."
        },
        {
          "REFERENCES": "ogy Workshop, 2020, pp. 373–380.",
          "separable convolutional neural network for voice activity detection,” in": ""
        },
        {
          "REFERENCES": "",
          "separable convolutional neural network for voice activity detection,” in": "[39]\nJ. Kahn, M. Riviere, W. Zheng, E. Kharitonov, Q. Xu, P.-E. Mazar´e,"
        },
        {
          "REFERENCES": "[14] H. Zou, Y. Si, C. Chen, D. Rajan, and E. S. Chng, “Speech emotion",
          "separable convolutional neural network for voice activity detection,” in": ""
        },
        {
          "REFERENCES": "",
          "separable convolutional neural network for voice activity detection,” in": "J. Karadayi, V. Liptchinsky, R. Collobert, C. Fuegen et al., “Libri-Light:"
        },
        {
          "REFERENCES": "recognition with co-attention based multi-level acoustic information,” in",
          "separable convolutional neural network for voice activity detection,” in": ""
        },
        {
          "REFERENCES": "",
          "separable convolutional neural network for voice activity detection,” in": "A benchmark for ASR with limited or no supervision,” in Proc. ICASSP,"
        },
        {
          "REFERENCES": "Proc.\nICASSP, 2022, pp. 7367–7371.",
          "separable convolutional neural network for voice activity detection,” in": ""
        },
        {
          "REFERENCES": "",
          "separable convolutional neural network for voice activity detection,” in": "2020, pp. 7669–7673."
        },
        {
          "REFERENCES": "[15] A. Baevski, H. Zhou, A. Mohamed,\nand M. Auli,\n“wav2vec\n2.0: A",
          "separable convolutional neural network for voice activity detection,” in": ""
        },
        {
          "REFERENCES": "",
          "separable convolutional neural network for voice activity detection,” in": "[40] G. Chen, S. Chai, G. Wang,\nJ. Du, W.-Q. Zhang, C. Weng, D. Su,"
        },
        {
          "REFERENCES": "framework\nfor\nselfsupervised\nlearning\nof\nspeech\nrepresentations,”\nin",
          "separable convolutional neural network for voice activity detection,” in": ""
        },
        {
          "REFERENCES": "",
          "separable convolutional neural network for voice activity detection,” in": "D. Povey, J. Trmal, J. Zhang et al., “GigaSpeech: An evolving, multi-"
        },
        {
          "REFERENCES": "Proc. NeurIPS, vol. 33, 2020, pp. 12 449–12 460.",
          "separable convolutional neural network for voice activity detection,” in": ""
        },
        {
          "REFERENCES": "",
          "separable convolutional neural network for voice activity detection,” in": "domain ASR corpus with 10,000 hours of\ntranscribed audio,” in Proc."
        },
        {
          "REFERENCES": "[16] W.-N. Hsu, B. Bolte, Y.-H. H. Tsai, K. Lakhotia, R. Salakhutdinov, and",
          "separable convolutional neural network for voice activity detection,” in": ""
        },
        {
          "REFERENCES": "",
          "separable convolutional neural network for voice activity detection,” in": "INTERSPEECH, 2021, pp. 3670–3674."
        },
        {
          "REFERENCES": "A. Mohamed, “HuBERT: Self-supervised speech representation learning",
          "separable convolutional neural network for voice activity detection,” in": ""
        },
        {
          "REFERENCES": "",
          "separable convolutional neural network for voice activity detection,” in": "[41] C. Wang, M.\nRiviere,\nA.\nLee, A. Wu,\nC.\nTalnikar,\nD. Haziza,"
        },
        {
          "REFERENCES": "by masked prediction of hidden units,” IEEE/ACM TASLP, vol. 29, pp.",
          "separable convolutional neural network for voice activity detection,” in": ""
        },
        {
          "REFERENCES": "",
          "separable convolutional neural network for voice activity detection,” in": "M. Williamson,\nJ. Pino,\nand E. Dupoux,\n“VoxPopuli: A large-scale"
        },
        {
          "REFERENCES": "3451–3460, 2021.",
          "separable convolutional neural network for voice activity detection,” in": ""
        },
        {
          "REFERENCES": "",
          "separable convolutional neural network for voice activity detection,” in": "multilingual\nspeech corpus for\nrepresentation learning, semi-supervised"
        },
        {
          "REFERENCES": "[17]\nS. Chen, C. Wang, Z. Chen, Y. Wu, S. Liu, Z. Chen, J. Li, N. Kanda,",
          "separable convolutional neural network for voice activity detection,” in": ""
        },
        {
          "REFERENCES": "",
          "separable convolutional neural network for voice activity detection,” in": "learning and interpretation,”\nin Proc. ACL-IJCNLP, 2021, pp. 993—-"
        },
        {
          "REFERENCES": "T. Yoshioka, X. Xiao et al., “WavLM: Large-scale self-supervised pre-",
          "separable convolutional neural network for voice activity detection,” in": ""
        },
        {
          "REFERENCES": "",
          "separable convolutional neural network for voice activity detection,” in": "1003."
        },
        {
          "REFERENCES": "IEEE Journal\nof\nSelected\ntraining\nfor\nfull\nstack\nspeech\nprocessing,”",
          "separable convolutional neural network for voice activity detection,” in": ""
        },
        {
          "REFERENCES": "",
          "separable convolutional neural network for voice activity detection,” in": "[42] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,”"
        },
        {
          "REFERENCES": "Topics in Signal Processing, vol. 16, no. 6, pp. 1505–1518, 2022.",
          "separable convolutional neural network for voice activity detection,” in": ""
        },
        {
          "REFERENCES": "",
          "separable convolutional neural network for voice activity detection,” in": "in Proc.\nICLR, 2015."
        },
        {
          "REFERENCES": "[18] Y. Wang, A. Boumadane, and A. Heba, “A ﬁne-tuned wav2vec 2.0/hubert",
          "separable convolutional neural network for voice activity detection,” in": ""
        },
        {
          "REFERENCES": "benchmark\nfor\nspeech\nemotion\nrecognition,\nspeaker\nveriﬁcation\nand",
          "separable convolutional neural network for voice activity detection,” in": ""
        },
        {
          "REFERENCES": "spoken language understanding,” 2021.",
          "separable convolutional neural network for voice activity detection,” in": ""
        },
        {
          "REFERENCES": "[19]\nL.-W. Chen and A. Rudnicky, “Exploring wav2vec 2.0 ﬁne tuning for",
          "separable convolutional neural network for voice activity detection,” in": ""
        },
        {
          "REFERENCES": "improved speech emotion recognition,” in Proc. ICASSP, 2023, pp. 1–5.",
          "separable convolutional neural network for voice activity detection,” in": ""
        },
        {
          "REFERENCES": "[20] W. Chen, X. Xing, P. Chen, and X. Xu, “Vesper: A compact and effective",
          "separable convolutional neural network for voice activity detection,” in": ""
        },
        {
          "REFERENCES": "pretrained model for speech emotion recognition,” IEEE Transactions on",
          "separable convolutional neural network for voice activity detection,” in": ""
        },
        {
          "REFERENCES": "Affective Computing, 2024.",
          "separable convolutional neural network for voice activity detection,” in": ""
        },
        {
          "REFERENCES": "[21]\nS.-w. Yang, P.-H. Chi, Y.-S. Chuang, C.-I. J. Lai, K. Lakhotia, Y. Y. Lin,",
          "separable convolutional neural network for voice activity detection,” in": ""
        },
        {
          "REFERENCES": "A. T. Liu,\nJ. Shi, X. Chang, G.-T. Lin et al., “SUPERB: Speech pro-",
          "separable convolutional neural network for voice activity detection,” in": ""
        },
        {
          "REFERENCES": "cessing universal performance benchmark,” in Proc. NeurIPS, vol. 33,",
          "separable convolutional neural network for voice activity detection,” in": ""
        },
        {
          "REFERENCES": "2020, pp. 12 449–12 460.",
          "separable convolutional neural network for voice activity detection,” in": ""
        },
        {
          "REFERENCES": "[22] M. Sharma, S.\nJoshi, T. Chatterjee,\nand R. Hamid, “A comprehensive",
          "separable convolutional neural network for voice activity detection,” in": ""
        },
        {
          "REFERENCES": "empirical\nreview of modern\nvoice\nactivity\ndetection\napproaches\nfor",
          "separable convolutional neural network for voice activity detection,” in": ""
        },
        {
          "REFERENCES": "movies and TV shows,” Neurocomputing, vol. 494, pp. 116–131, 2022.",
          "separable convolutional neural network for voice activity detection,” in": ""
        },
        {
          "REFERENCES": "[23]\nF. Jia, S. Majumdar, and B. Ginsburg, “Marblenet: Deep 1d time-channel",
          "separable convolutional neural network for voice activity detection,” in": ""
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "This figure \"fig1.png\" is available in \"png\"(cid:10) format from:": "http://arxiv.org/ps/2410.13282v1"
        }
      ],
      "page": 6
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Emotional speech recognition: Resources, features, and methods",
      "authors": [
        "V Dimitrios",
        "K Constantine"
      ],
      "year": "2006",
      "venue": "Computer Speech & Language"
    },
    {
      "citation_id": "2",
      "title": "Review of affective computing in education/learning: Trends and challenges",
      "authors": [
        "C.-H Wu",
        "Y.-M Huang",
        "J.-P Hwang"
      ],
      "year": "2016",
      "venue": "British Journal of Educational Technology"
    },
    {
      "citation_id": "3",
      "title": "Emotion recognition in human-computer interaction",
      "authors": [
        "R Cowie",
        "E Douglas-Cowie",
        "N Tsapatsoulis",
        "G Votsis",
        "S Kollias",
        "W Fellenz",
        "J Taylor"
      ],
      "year": "2001",
      "venue": "IEEE Signal processing magazine"
    },
    {
      "citation_id": "4",
      "title": "Chatbots and conversational agents in mental health: a review of the psychiatric landscape",
      "authors": [
        "N Vaidyam",
        "H Wisniewski",
        "J Halamka",
        "M Kashavan",
        "J Torous"
      ],
      "year": "2019",
      "venue": "The Canadian Journal of Psychiatry"
    },
    {
      "citation_id": "5",
      "title": "Emotion in speech: Recognition and application to call centers",
      "authors": [
        "V Petrushin"
      ],
      "year": "1999",
      "venue": "Proc. ANNIE"
    },
    {
      "citation_id": "6",
      "title": "The role of emotions in marketing",
      "authors": [
        "R Bagozzi",
        "M Gopinath",
        "P Nyer"
      ],
      "year": "1999",
      "venue": "Journal of the academy of marketing science"
    },
    {
      "citation_id": "7",
      "title": "Speech emotion recognition using hidden markov models",
      "authors": [
        "T Nwe",
        "S Foo",
        "L Silva"
      ],
      "year": "2003",
      "venue": "Speech communication"
    },
    {
      "citation_id": "8",
      "title": "Svm scheme for speech emotion recognition using mfcc feature",
      "authors": [
        "A Milton",
        "S Roy",
        "S Selvi"
      ],
      "year": "2013",
      "venue": "International Journal of Computer Applications"
    },
    {
      "citation_id": "9",
      "title": "Speech emotion recognition using convolutional and recurrent neural networks",
      "authors": [
        "W Lim",
        "D Jang",
        "T Lee"
      ],
      "year": "2016",
      "venue": "Proc. APSIPA"
    },
    {
      "citation_id": "10",
      "title": "Speech emotion classification using attention-based lstm",
      "authors": [
        "Y Xie",
        "R Liang",
        "Z Liang",
        "C Huang",
        "C Zou",
        "B Schuller"
      ],
      "year": "2019",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "11",
      "title": "Attention is all you need",
      "authors": [
        "A Vaswani",
        "N Shazeer",
        "N Parmar",
        "J Uszkorei",
        "L Jones",
        "A Gomez",
        "L Kaiser",
        "I Polosukhin"
      ],
      "year": "2016",
      "venue": "Proc. APSIPA"
    },
    {
      "citation_id": "12",
      "title": "Emotion recognition from speech using wav2vec 2.0 embeddings",
      "authors": [
        "L Pepino",
        "P Riera",
        "L Ferrer"
      ],
      "year": "2021",
      "venue": "Proc. IEEE Spoken Language Technology Workshop"
    },
    {
      "citation_id": "13",
      "title": "On the use of selfsupervised pre-trained acoustic and linguistic features for continuous speech emotion recognition",
      "authors": [
        "M Macary",
        "M Tahon",
        "Y Estève",
        "A Rousseau"
      ],
      "year": "2020",
      "venue": "Proc. IEEE Spoken Language Technology Workshop"
    },
    {
      "citation_id": "14",
      "title": "Speech emotion recognition with co-attention based multi-level acoustic information",
      "authors": [
        "H Zou",
        "Y Si",
        "C Chen",
        "D Rajan",
        "E Chng"
      ],
      "year": "2022",
      "venue": "Proc. ICASSP"
    },
    {
      "citation_id": "15",
      "title": "wav2vec 2.0: A framework for selfsupervised learning of speech representations",
      "authors": [
        "A Baevski",
        "H Zhou",
        "A Mohamed",
        "M Auli"
      ],
      "year": "2020",
      "venue": "Proc. NeurIPS"
    },
    {
      "citation_id": "16",
      "title": "HuBERT: Self-supervised speech representation learning by masked prediction of hidden units",
      "authors": [
        "W.-N Hsu",
        "B Bolte",
        "Y.-H Tsai",
        "K Lakhotia",
        "R Salakhutdinov",
        "A Mohamed"
      ],
      "year": "2021",
      "venue": "IEEE/ACM TASLP"
    },
    {
      "citation_id": "17",
      "title": "WavLM: Large-scale self-supervised pretraining for full stack speech processing",
      "authors": [
        "S Chen",
        "C Wang",
        "Z Chen",
        "Y Wu",
        "S Liu",
        "Z Chen",
        "J Li",
        "N Kanda",
        "T Yoshioka",
        "X Xiao"
      ],
      "year": "2022",
      "venue": "IEEE Journal of Selected Topics in Signal Processing"
    },
    {
      "citation_id": "18",
      "title": "A fine-tuned wav2vec 2.0/hubert benchmark for speech emotion recognition, speaker verification and spoken language understanding",
      "authors": [
        "Y Wang",
        "A Boumadane",
        "A Heba"
      ],
      "year": "2021",
      "venue": "A fine-tuned wav2vec 2.0/hubert benchmark for speech emotion recognition, speaker verification and spoken language understanding"
    },
    {
      "citation_id": "19",
      "title": "Exploring wav2vec 2.0 fine tuning for improved speech emotion recognition",
      "authors": [
        "L.-W Chen",
        "A Rudnicky"
      ],
      "venue": "Proc. ICASSP, 2023"
    },
    {
      "citation_id": "20",
      "title": "Vesper: A compact and effective pretrained model for speech emotion recognition",
      "authors": [
        "W Chen",
        "X Xing",
        "P Chen",
        "X Xu"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "21",
      "title": "SUPERB: Speech processing universal performance benchmark",
      "authors": [
        "S.-W Yang",
        "P.-H Chi",
        "Y.-S Chuang",
        "C.-I Lai",
        "K Lakhotia",
        "Y Lin",
        "A Liu",
        "J Shi",
        "X Chang",
        "G.-T Lin"
      ],
      "year": "2020",
      "venue": "Proc. NeurIPS"
    },
    {
      "citation_id": "22",
      "title": "A comprehensive empirical review of modern voice activity detection approaches for movies and TV shows",
      "authors": [
        "M Sharma",
        "S Joshi",
        "T Chatterjee",
        "R Hamid"
      ],
      "year": "2022",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "23",
      "title": "Marblenet: Deep 1d time-channel separable convolutional neural network for voice activity detection",
      "authors": [
        "F Jia",
        "S Majumdar",
        "B Ginsburg"
      ],
      "venue": "Proc. ICASSP, 2021"
    },
    {
      "citation_id": "24",
      "title": "A hybrid cnn-bilstm voice activity detector",
      "authors": [
        "N Wilkinson",
        "T Niesler"
      ],
      "venue": "Proc. ICASSP, 2021"
    },
    {
      "citation_id": "25",
      "title": "Svad: A robust, low-power, and light-weight voice activity detection with spiking neural networks",
      "authors": [
        "Q Yang",
        "Q Liu",
        "N Li",
        "M Ge",
        "Z Song",
        "H Li"
      ],
      "year": "2024",
      "venue": "Proc. ICASSP"
    },
    {
      "citation_id": "26",
      "title": "A statistical model-based voice activity detection",
      "authors": [
        "J Sohn",
        "N Kim",
        "W Sung"
      ],
      "year": "1999",
      "venue": "IEEE signal processing letters"
    },
    {
      "citation_id": "27",
      "title": "Voice activity detection based on multiple statistical models",
      "authors": [
        "J.-H Chang",
        "N Kim",
        "S Mitra"
      ],
      "year": "2006",
      "venue": "IEEE Transactions on Signal Processing"
    },
    {
      "citation_id": "28",
      "title": "WebRTC VAD",
      "venue": "WebRTC VAD"
    },
    {
      "citation_id": "29",
      "title": "Research on robustness of emotion recognition under environmental noise conditions",
      "authors": [
        "Y Huang",
        "J Xiao",
        "K Tian",
        "A Wu",
        "G Zhang"
      ],
      "year": "2019",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "30",
      "title": "Layer-wise analysis of a selfsupervised speech representation model",
      "authors": [
        "A Pasad",
        "J.-C Chou",
        "K Livescu"
      ],
      "venue": "Proc. ASRU, 2021"
    },
    {
      "citation_id": "31",
      "title": "A transformerbased voice activity detector",
      "authors": [
        "B Karan",
        "J Van Vüren",
        "F De Wet",
        "T Niesler"
      ],
      "year": "2024",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "32",
      "title": "Silero Team and language classifier",
      "year": "2021",
      "venue": "Silero Team and language classifier"
    },
    {
      "citation_id": "33",
      "title": "Self-attention encoding and pooling for speaker recognition",
      "authors": [
        "P Safari",
        "M India",
        "J Hernando"
      ],
      "year": "2020",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "34",
      "title": "On the use of voice activity detection in speech emotion recognition",
      "authors": [
        "M Alghifari",
        "T Gunawan",
        "M Nordin",
        "S Qadri",
        "M Kartiwi",
        "Z Janin"
      ],
      "year": "2019",
      "venue": "Bulletin of Electrical Engineering and Informatics"
    },
    {
      "citation_id": "35",
      "title": "Integrating emotion recognition with speech recognition and speaker diarisation for conversations",
      "authors": [
        "C Zhang",
        "P Woodland"
      ],
      "year": "2023",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "36",
      "title": "IEMOCAP: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "37",
      "title": "MUSAN: A music, speech, and noise corpus",
      "authors": [
        "D Snyder",
        "G Chen",
        "D Povey"
      ],
      "year": "2015",
      "venue": "MUSAN: A music, speech, and noise corpus",
      "arxiv": "arXiv:1510.08484"
    },
    {
      "citation_id": "38",
      "title": "LibriSpeech: An ASR corpus based on public domain audio books",
      "authors": [
        "V Panayotov",
        "G Chen",
        "D Povey",
        "S Khudanpur"
      ],
      "year": "2015",
      "venue": "Proc. ICASSP"
    },
    {
      "citation_id": "39",
      "title": "Libri-Light: A benchmark for ASR with limited or no supervision",
      "authors": [
        "J Kahn",
        "M Riviere",
        "W Zheng",
        "E Kharitonov",
        "Q Xu",
        "P.-E Mazaré",
        "J Karadayi",
        "V Liptchinsky",
        "R Collobert",
        "C Fuegen"
      ],
      "year": "2020",
      "venue": "Proc. ICASSP"
    },
    {
      "citation_id": "40",
      "title": "GigaSpeech: An evolving, multidomain ASR corpus with 10,000 hours of transcribed audio",
      "authors": [
        "G Chen",
        "S Chai",
        "G Wang",
        "J Du",
        "W.-Q Zhang",
        "C Weng",
        "D Su",
        "D Povey",
        "J Trmal",
        "J Zhang"
      ],
      "venue": "Proc. INTERSPEECH, 2021"
    },
    {
      "citation_id": "41",
      "title": "VoxPopuli: A large-scale multilingual speech corpus for representation learning, semi-supervised learning and interpretation",
      "authors": [
        "C Wang",
        "M Riviere",
        "A Lee",
        "A Wu",
        "C Talnikar",
        "D Haziza",
        "M Williamson",
        "J Pino",
        "E Dupoux"
      ],
      "year": "2021",
      "venue": "Proc. ACL-IJCNLP"
    },
    {
      "citation_id": "42",
      "title": "Adam: A method for stochastic optimization",
      "authors": [
        "D Kingma",
        "J Ba"
      ],
      "year": "2015",
      "venue": "Proc. ICLR"
    }
  ]
}