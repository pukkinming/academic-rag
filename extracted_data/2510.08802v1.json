{
  "paper_id": "2510.08802v1",
  "title": "Edu-Emotionnet: Cross-Modality Attention Alignment With Temporal Feedback Loops",
  "published": "2025-10-09T20:33:52Z",
  "authors": [
    "S M Rafiuddin"
  ],
  "keywords": [
    "Multimodal Emotion Recognition",
    "Temporal Modeling",
    "Modality Reliability",
    "Educational Affective Computing",
    "Cross-Modal Attention",
    "Robust Fusion",
    "Emotion Dynamics",
    "Online Learning Environments"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Understanding learner emotions in online education is critical for improving engagement and personalized instruction. While prior work in emotion recognition has explored multimodal fusion and temporal modeling, existing methods often rely on static fusion strategies and assume that modality inputs are consistently reliable, which is rarely the case in real-world learning environments. We introduce Edu-EmotionNet, a novel framework that jointly models temporal emotion evolution and modality reliability for robust affect recognition. Our model incorporates three key components-a Cross-Modality Attention Alignment (CMAA) module for dynamic cross-modal context sharing, a Modality Importance Estimator (MIE) that assigns confidence-based weights to each modality at every time step, and a Temporal Feedback Loop (TFL) that leverages previous predictions to enforce temporal consistency. Evaluated on educational subsets of IEMOCAP and MOSEI, re-annotated for confusion, curiosity, boredom, and frustration, Edu-EmotionNet achieves state-of-the-art performance and demonstrates strong robustness to missing or noisy modalities. Visualizations confirm its ability to capture emotional transitions and adaptively prioritize reliable signals, making it well suited for deployment in real-time learning systems 1 .",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "The widespread adoption of virtual and hybrid learning platforms has transformed the educational landscape by enabling scalable, remote access to quality instruction. Platforms such as MOOCs, video lectures, and intelligent tutoring systems have democratized education globally. However, this digital shift has introduced a critical limitation: the absence of realtime, affective feedback that human instructors naturally rely on to monitor student engagement, comprehension, and emotional state. Emotions like confusion, frustration, curiosity, and boredom are key indicators of learning effectiveness and dropout risk  [1] . In traditional classrooms, instructors can respond to these cues dynamically, but such responsiveness is largely absent in online platforms.\n\nTo address this gap, researchers have turned to emotion recognition technologies that use facial expressions, vocal tones, and textual interactions to infer learners' affective states  [3] -  [6] . While these unimodal systems have shown 1 Accepted as a Regular Research Paper at ICMLA 2025 promise, they often fail under real-world conditions where any single modality may be noisy, ambiguous, or missing. For instance, background noise may degrade audio quality, camera occlusions may impair facial expression detection, and sparse textual input may limit linguistic cues. Therefore, robust emotion recognition in educational environments demands a multimodal approach that can effectively integrate and reason over complementary information from multiple sources.\n\nRecent advances in multimodal machine learning have introduced sophisticated fusion architectures that combine visual, audio, and textual signals for improved performance in tasks such as sentiment analysis, sarcasm detection, and emotion classification  [7] -  [9] . However, most existing models apply static fusion strategies, such as simple concatenation or fixedattention schemes, that fail to account for the varying importance and reliability of modalities across instances. Moreover, they often overlook the temporal nature of emotion, treating it as a static label rather than a dynamic state that evolves throughout the learning session.\n\nIn this paper, we propose Edu-EmotionNet, a novel deep learning architecture for real-time multimodal emotion recognition in educational platforms. Edu-EmotionNet incorporates several innovative components tailored to the educational domain: a Cross-Modality Attention Alignment (CMAA) mechanism that enables each modality (audio, visual, text) to attend to the others and compute agreement-aligned features, thereby facilitating contextual reasoning and mitigating contradictory or noisy inputs; a Modality Importance Estimator (MIE) that predicts dynamic, instance-level confidence weights for each modality, allowing the model to suppress unreliable signals (e.g., poor audio) and emphasize stronger ones; and a Temporal Feedback Loop (TFL) that treats emotion as a temporal sequence by incorporating soft pseudolabels from previous timesteps into current predictions, thereby regularizing temporal consistency and enhancing sensitivity to the evolution of emotional states.\n\nTo validate our approach, we evaluate Edu-EmotionNet on a benchmark constructed from publicly available multimodal datasets, re-annotated for educationally relevant emotions such as confusion, boredom, curiosity, and frustration. Our model outperforms strong unimodal and fusion-based baselines, demonstrating improved robustness and interpretability in emotionally complex learning scenarios.\n\nOur contributions are threefold: first, we introduce Edu-EmotionNet, the first multimodal emotion recognition architecture explicitly designed for educational platforms, which integrates cross-modal alignment and temporal modeling; second, we develop a dynamic fusion strategy that combines attention-based alignment with confidence-weighted modality selection to enhance robustness under real-world noise and missing data; and third, we demonstrate that emotion trajectories can be effectively learned through a self-supervised temporal feedback mechanism, thereby improving temporal coherence and enabling real-time emotion understanding in learning environments.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Related Work",
      "text": "Unimodal emotion recognition has leveraged large-scale visual datasets such as AffectNet  [3] , FER2013  [14] , and RAF-DB  [15]  with convolutional and attention-based encoders, audio features like MFCCs and pitch in deep recurrent networks  [5] ,  [16] , and text sentiment and emotion classification via pretrained transformers  [1] ,  [17] ,  [18] , though these methods often fail under noisy or missing inputs. Classical early and late fusion have been outperformed by attentionbased architectures such as MulT  [7]  and MISA  [8] , as well as recent models like CMEM  [10]  and HybridFusion  [11] , but most assume full modality availability and lack dynamic adaptation to noise or dropout. Temporal-aware methods TAT  [12] , Emobert  [19] , Self-MM  [9]  and graph-based fusion  [13]  capture sequential emotion evolution yet typically overlook domain-specific dynamics and do not integrate modality reliability for real-world educational settings. Edu-EmotionNet addresses these gaps by jointly tackling cross-modal reasoning, dynamic fusion, and temporal adaptation through Cross-Modality Attention Alignment, a Modality Importance Estimator, and a Temporal Feedback Loop, evaluated on re-annotated subsets of IEMOCAP and MOSEI for confusion, boredom, curiosity, and frustration.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Iii. Method",
      "text": "Let a student interaction session be represented by a timeindexed multimodal sequence D = {(A t , V t , T t )} T t=1 , where A t , V t , and T t denote the audio, visual, and textual inputs at timestep t, respectively. The goal is to predict a sequence of emotional states {ŷ t } T t=1 over K classes, e.g., confused, bored, curious.\n\nWe define three modular components: modality-specific encoders, cross-modal alignment, and temporally regularized fusion. The entire framework is end-to-end differentiable and trained via backpropagation.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "A. Modality-Specific Encoders",
      "text": "Each modality is first projected into a latent space using deep pretrained encoders followed by temporal modeling:\n\nwhere ϕ m (•) is the feature extractor for modality m ∈ {A, V, T } (e.g., Wav2Vec2.0, ResNet, BERT), and Trans m (•) is a Transformer that captures modality-specific temporal dynamics.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "B. Cross-Modality Attention Alignment (Cmaa)",
      "text": "We define a symmetric cross-attention operator between modality i and j at timestep t:\n\nwhere\n\nis the alignment-enhanced feature from modality j as viewed by i.\n\nLet g i t be the aggregate aligned representation for modality i:\n\n, where {j, k} = {A, V, T } \\ {i}",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "C. Modality Importance Estimator (Mie)",
      "text": "To enhance robustness under noisy conditions, we introduce a confidence-weighted fusion mechanism. For each modality i, a small neural network predicts a scalar confidence score:\n\nwith i w i t = 1 enforced via normalization. The final fused feature is:",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "D. Temporal Feedback Loop (Tfl)",
      "text": "We incorporate pseudo-label feedback from prior predictions to enforce temporal smoothness. Let ŷt-1 be the softmax probability output at t-1. We define:\n\nThe final emotion prediction is:",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "E. Loss Functions",
      "text": "We use a combined loss:\n\ntemporal smoothness  (10)  where L CE is the cross-entropy loss, and KL divergence penalizes sharp transitions in adjacent predictions.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "F. Theoretical Properties",
      "text": ", where x a t , x v t , and x t t denote the audio, visual, and textual feature vectors at time t, respectively, and at most one modality input is missing (set to 0) at each t. Then the mapping\n\ndefined by Edu-EmotionNet is Lipschitz continuous with respect to any one modality input when the others are held fixed.\n\nProof. We will show that for each modality m ∈ {a, v, t}, the function\n\nEach encoder ϕ m : R dm → R h is a feed-forward network with bounded weights and Lipschitz activations, so\n\nThe Cross-Modality Attention Alignment (CMAA) block is a composition of affine maps and elementwise softmax/QKV projections, all with bounded operator norms. Hence it is Lipschitz:\n\nThe Modality Importance Estimator (MIE), which applies a small feed-forward net plus a softmax, is Lipschitz:\n\nwe can take its Lipschitz constant to be 1.\n\nThe Temporal Feedback Loop (TFL) is another feedforward/looped module with bounded weights:\n\nFinally, the classifier head is a Lipschitz map with constant L clf . Now, fix t and two values x m t , x ′m t for modality m, and keep the other two modalities identical (one of them possibly being the default 0 if missing). Denote\n\nand since this holds for any modality m, the network is Lipschitz continuous with respect to any remaining modality input.\n\nTheorem 1. Assuming that emotion-state transitions can be well-approximated by a first-order Markov process (as empirically validated by the Temporal Feedback Loop ablation study in Section V.G), and that the sequence of predictions (ŷ t ) converges, then the Temporal Feedback Loop (TFL) enforces a unique fixed point ŷ * = arg min\n\nwhere ∆ C-1 is the probability simplex in R C , and\n\ndenotes the forward Kullback-Leibler divergence (as used in Eq. (  10 )).\n\nProof. We adopt the forward KL divergence KL(p ∥ q) = i p i log(p i /q i ) consistently with our loss in Eq.  (10) . Under the Markov assumption, at each step the TFL update solves ŷt = arg min\n\nwhere ℓ(y; x t ) is convex in y and λ > 0. Since KL(ŷ t-1 ∥ y) is strictly convex in y over the compact convex set ∆ C-1 , the total objective admits a unique minimizer for each t.\n\nBy hypothesis, ŷt → ŷ * . Taking the limit in the optimality condition,\n\nbecause as t → ∞, the data-term and previous pseudo-label coincide, reducing the objective to the KL term alone. Finally,\n\nsince the forward KL divergence is uniquely minimized (to zero) at y = ŷ * . Hence, the TFL has a unique fixed point ŷ * .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Iv. Experiments",
      "text": "All experiments were conducted using Python 3.9, PyTorch 1.12, and CUDA 11.6 on a machine with NVIDIA A100 GPU (40 GB HBM2, NVLink). We evaluated our model on the custom educational emotion dataset described in Section V.A over four classes (confused, bored, curious, frustrated). Audio features are 40-dimensional MFCCs (25 ms window, 10 ms hop) normalized per session; video inputs are 224×224 RGB frames at 30 fps, resized and normalized to ImageNet mean/std; text inputs use BERT-base token embeddings (padded/truncated to 128 tokens). Each modality is encoded to d = 256 via a 4-layer Transformer (4 heads, d k = 64), then fused by CMAA (scaled dot-product attention), MIE (2-layer MLP), and TFL (1-layer MLP). We trained for up to 50 epochs (batch size 128; AdamW with lr = 1e-4, weight decay = 1e-5, 5-epoch linear warm-up, step LR decay ×0.1 at epochs 30/40; dropout 0.2), applying early stopping (patience 5, triggered at epoch 35) in approximately 8 h. We retained the checkpoint with the highest validation macro-F1 for final evaluation, reporting overall accuracy and macro-F1 on the test set.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "V. Results",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "A. Datasets",
      "text": "We evaluate on a custom educational emotion dataset derived from IEMOCAP (10 speakers) and CMU-MOSEI, re-annotated and filtered for four learning-specific emotions (confused, bored, curious, frustrated). Three annotators with backgrounds in educational psychology labeled each session according to a detailed guideline; disagreements were resolved by majority vote and consultation with a fourth senior reviewer, yielding an overall Cohen's κ = 0.78 (perclass range: 0.75-0.81). From an initial pool of 6,200 sessions, we removed 1,200 sessions that contained a gap exceeding 2 s in any modality (audio, video, or transcript), resulting in 5,000 sessions (average duration 30 s), balanced at 1,250 sessions per emotion. To prevent speaker/session leakage, we maintain a speaker-independent split over 50 unique speakers drawn from both corpora: 70% train (3,500 sessions, 35 speakers), 10% validation (500 sessions, 5 speakers), and 20% test (1,000 sessions, 10 speakers).",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "B. Comparison With Recent Baselines",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Model",
      "text": "Accuracy Macro-F1 MulT  [7]  0.81 ± 0.02 0.79 ± 0.02 Self-MM  [9]  0.82 ± 0.018 0.80 ± 0.017 CFN-ESA  [10]  0.83 ± 0.015 0.81 ± 0.014 HybridFusion  [11]  0.84 ± 0.012 0.82 ± 0.013 Edu-EmotionNet (ours) 0.88 ± 0.009 0.86 ± 0.008 Table I reports the mean and standard deviation of accuracy and macro-F1 over three independent runs with different random seeds. Edu-EmotionNet achieves 0.88 ± 0.009 accuracy and 0.86 ± 0.008 macro-F1, outperforming all baselines while exhibiting low variance and robust, reliable improvements. Notably, the 4 pp accuracy gain over HybridFusion exceeds its own standard deviation (±0.012), indicating that our improvement is unlikely to be due to random initialization. Paired t-tests across the three runs confirm statistical significance for both accuracy and macro-F1 (p < 0.05). reliance on secondary cues when primary signals falter. Together, these dynamics underscore the effectiveness of our uncertainty-driven fusion: by dynamically down-weighting unreliable inputs and momentarily boosting textual context, Edu-EmotionNet maintains robust emotion recognition even amidst fluctuating signal quality.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "C. Dynamic Modality Confidence Analysis",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "D. Per-Class Performance Analysis",
      "text": "We compare against HybridFusion, the multi-attention fusion baseline described in Table  I   Figure  4  shows that Edu-EmotionNet consistently outperforms HybridFusion across all four classes, with the largest improvements on \"Confused\" (+3 pp) and \"Curious\" (+3 pp), and tighter confidence intervals, demonstrating our model's superior sensitivity to nuanced learning-focused emotional states.  Figure  5  highlights Edu-EmotionNet's remarkable resilience when modalities become unavailable: unlike HybridFusion and Late Fusion, which suffer steep performance drops beyond 40% missing data, our model's accuracy declines only marginally (from 0.88 to 0.85 at 60% missing), demonstrating effective uncertainty-driven fusion and redundancy across modalities.  Table III reports mean and standard deviation of accuracy and macro-F1 over three independent runs. Ablating the Temporal Fusion Layer (TFL) causes a drop from 0.88 ± 0.009 to 0.83 ± 0.012 accuracy (5 pp) and from 0.86 ± 0.008 to 0.81 ± 0.013 macro-F1 (5 pp), removing the Cross-Modal Attention Alignment (CMAA) yields a decline of 4 pp, and omitting the Modality Importance Estimator (MIE) results in a 3 pp decrease. The fact that each performance loss exceeds the corresponding standard deviation underscores the unique, synergistic contribution of each module to Edu-EmotionNet's robust emotion recognition.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "E. Robustness To Missing Modalities",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "F. Main Results",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "H. Training Dynamics Analysis",
      "text": "",
      "page_start": 5,
      "page_end": 5
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Online platforms lack real-time affective feedback. Edu-EmotionNet",
      "page": 1
    },
    {
      "caption": "Figure 2: Overview of Edu-EmotionNet’s end-to-end pipeline. Raw audio, visual, and text inputs are first encoded (Wav2Vec2→Trans A, ResNet→Trans V,",
      "page": 3
    },
    {
      "caption": "Figure 3: reveals a clear hierarchical weighting of modalities,",
      "page": 4
    },
    {
      "caption": "Figure 3: Dynamic modality confidence weights over time with 95% confidence",
      "page": 5
    },
    {
      "caption": "Figure 4: Per-class F1 score comparison between the hybrid multi-attention",
      "page": 5
    },
    {
      "caption": "Figure 4: shows that Edu-EmotionNet consistently outper-",
      "page": 5
    },
    {
      "caption": "Figure 5: Accuracy under increasing missing modality rates.",
      "page": 5
    },
    {
      "caption": "Figure 5: highlights Edu-EmotionNet’s remarkable resilience",
      "page": 5
    },
    {
      "caption": "Figure 6: Representative loss curves for a full 50-epoch training run. In practice,",
      "page": 6
    },
    {
      "caption": "Figure 6: shows training and validation losses over a com-",
      "page": 6
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "0\n0.8",
          "Column_2": "",
          "Column_3": ".87\n4\n0.8",
          "Column_4": "",
          "Column_5": "",
          "Column_6": "0\n0.85 0.85\n2",
          "Column_7": "",
          "Column_8": "",
          "Column_9": ".88\n0\n0.8",
          "Column_10": "",
          "Column_11": "",
          "Column_12": ".86\n3",
          "Column_13": ""
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Affect detection: An interdisciplinary review of models, methods, and their applications",
      "authors": [
        "R Calvo",
        "S Mello"
      ],
      "year": "2010",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "2",
      "title": "Affective computing and intelligent interaction",
      "authors": [
        "J Tan",
        "R Picard"
      ],
      "year": "2007",
      "venue": "Proc. 2nd International Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "3",
      "title": "AffectNet: A database for facial expression, valence, and arousal computing in the wild",
      "authors": [
        "A Mollahosseini",
        "B Hasani",
        "M Mahoor"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "4",
      "title": "The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS): A dynamic, multimodal set of facial and vocal expressions in North American English",
      "authors": [
        "S Livingstone",
        "F Russo"
      ],
      "year": "2018",
      "venue": "PLOS ONE"
    },
    {
      "citation_id": "5",
      "title": "A deep learning approach for online learning emotion recognition",
      "authors": [
        "C Ma",
        "C Sun",
        "D Song",
        "X Li",
        "H Xu"
      ],
      "year": "2018",
      "venue": "Proc. 2018 13th International Conference on Computer Science & Education (ICCSE)"
    },
    {
      "citation_id": "6",
      "title": "Applying segment-level attention on bimodal transformer encoder for audio-visual emotion recognition",
      "authors": [
        "J Hsu",
        "C Wu"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "7",
      "title": "Multimodal Transformer for unaligned multimodal language sequences",
      "authors": [
        "Y.-H Tsai"
      ],
      "year": "2019",
      "venue": "Proc. ACL"
    },
    {
      "citation_id": "8",
      "title": "Tmmda: A new token mixup multimodal data augmentation for multimodal sentiment analysis",
      "authors": [
        "X Zhao",
        "Y Chen",
        "S Liu",
        "X Zang",
        "Y Xiang",
        "B Tang"
      ],
      "year": "2023",
      "venue": "Proc. ACM Web Conf. (WWW '23)"
    },
    {
      "citation_id": "9",
      "title": "Learning modality-specific representations with self-supervised multi-task learning for multimodal sentiment analysis",
      "authors": [
        "W Yu",
        "H Xu",
        "Z Yuan",
        "J Wu"
      ],
      "year": "2021",
      "venue": "Proc. AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "10",
      "title": "CFN-ESA: A cross-modal fusion network with emotion-shift awareness for dialogue emotion recognition",
      "authors": [
        "J Li",
        "X Wang",
        "Y Liu",
        "Z Zeng"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "11",
      "title": "Hybrid multi-attention network for audio-visual emotion recognition through multimodal feature fusion",
      "authors": [
        "S Moorthy",
        "Y Moon"
      ],
      "year": "2025",
      "venue": "Mathematics"
    },
    {
      "citation_id": "12",
      "title": "Valence and arousal estimation based on multimodal temporal-aware features for videos in the wild",
      "authors": [
        "L Meng",
        "Y Liu",
        "X Liu",
        "Z Huang",
        "W Jiang",
        "T Zhang"
      ],
      "year": "2022",
      "venue": "Proc. IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "13",
      "title": "An adaptive multi-graph neural network with multimodal feature fusion learning for MDD detection",
      "authors": [
        "T Xing",
        "Y Dou",
        "X Chen",
        "J Zhou",
        "X Xie",
        "S Peng"
      ],
      "year": "2024",
      "venue": "Scientific Reports"
    },
    {
      "citation_id": "14",
      "title": "Challenges in representation learning: A report on three machine learning contests",
      "authors": [
        "I Goodfellow",
        "D Erhan",
        "P Carrier",
        "A Courville",
        "M Mirza",
        "B Hamner"
      ],
      "year": "2013",
      "venue": "Proc. International Conference on Neural Information Processing (ICONIP)"
    },
    {
      "citation_id": "15",
      "title": "Reliable crowdsourcing and deep localitypreserving learning for expression recognition in the wild",
      "authors": [
        "S Li",
        "W Deng"
      ],
      "year": "2017",
      "venue": "Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "16",
      "title": "Adieu features? End-to-end speech emotion recognition using a deep convolutional recurrent network",
      "authors": [
        "G Trigeorgis",
        "F Ringeval",
        "R Brueckner",
        "E Marchi",
        "M Nicolaou",
        "B Schuller",
        "S Zafeiriou"
      ],
      "year": "2016",
      "venue": "Proc. IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "17",
      "title": "SemEval-2018 task 1: Affect in tweets",
      "authors": [
        "S Mohammad",
        "F Bravo-Marquez",
        "M Salameh",
        "S Kiritchenko"
      ],
      "year": "2018",
      "venue": "Proc. International Workshop on Semantic Evaluation (SemEval)"
    },
    {
      "citation_id": "18",
      "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "J Devlin",
        "M.-W Chang",
        "K Lee",
        "K Toutanova"
      ],
      "year": "2019",
      "venue": "Proc. NAACL-HLT"
    },
    {
      "citation_id": "19",
      "title": "Emotion-aware multimodal fusion for meme emotion detection",
      "authors": [
        "S Sharma",
        "S Ramaneswaran",
        "M Akhtar",
        "T Chakraborty"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Affective Computing"
    }
  ]
}