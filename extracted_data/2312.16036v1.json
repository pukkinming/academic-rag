{
  "paper_id": "2312.16036v1",
  "title": "Ensemble Learning To Assess Dynamics Of Affective Experience Ratings And Physiological Change",
  "published": "2023-12-26T12:53:57Z",
  "authors": [
    "Felix Dollack",
    "Kiyoshi Kiyokawa",
    "Huakun Liu",
    "Monica Perusquia-Hernandez",
    "Chirag Raman",
    "Hideaki Uchiyama",
    "Xin Wei"
  ],
  "keywords": [
    "affective computing",
    "continuous ratings",
    "biosignal processing",
    "machine learning",
    "data analysis challenge"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "The congruence between affective experiences and physiological changes has been a debated topic for centuries. Recent technological advances in measurement and data analysis provide hope to solve this epic challenge. Open science and open data practices, together with data analysis challenges open to the academic community, are also promising tools for solving this problem. In this entry to the Emotion Physiology and Experience Collaboration (EPiC) challenge, we propose a data analysis solution that combines theoretical assumptions with data-driven methodologies. We used feature engineering and ensemble selection. Each predictor was trained on subsets of the training data that would maximize the information available for training. Late fusion was used with an averaging step. We chose to average considering a \"wisdom of crowds\" strategy. This strategy yielded an overall RMSE of 1.19 in the test set. Future work should carefully explore if our assumptions are correct and the potential of weighted fusion.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "Understanding human emotion is instrumental for applications in mental healthcare, education, and communication  [6] . These applications aim to automatically assess and generate affective cues by relying on an assumed relationship between affective experience and physiological changes. However, the debate on the precedence of body changes or subjective experience started in the previous century and remains current  [7] . Recent research has discussed whether demand characteristics affect bias in our understanding of the relationship between facial expression and affective experience  [9, 2] ; and explored the relationship between dynamic Autonomic Nervous System responses and affective experiences  [12, 22] . Physiological sensing technologies have been popular in studying the physiological changes correlating with affective experiences  [5, 13] . Each physiological measurement type gives a different piece of information regarding the functioning of the sympathetic and parasympathetic nervous systems  [1] , leading to a popular multidimensional dataset collection. Traditional data analysis techniques require extensive knowledge about physiology characteristics, signal processing, and domain knowledge in affective sciences. This domain knowledge gave birth to hand-crafted feature engineering that improves data interpretability and reduces the number of comparisons All authors contributed equally to this work. to be made when analyzing the data. Recent advances in Machine Learning (ML) and data-driven analyses have brought a new perspective. Purely data-driven analyses with end-to-end automated processing have become popular  [16] . In end-to-end approaches, a machine learning network learns an intermediate representation of the input, thereby reducing manual work, and potentially enhancing the results  [15] . However, the evidence does not always support this claim. A previous study showed that convolutional and recurrent neural networks yielded better results than other state-of-the-art methods  [15] . Another study used a deep-learning approach to estimate momentary emotional states from multi-modal physiological data; and reported a higher correlation than traditional methods. Still, their mean absolute error (MAE) was higher (a lower MAE is better)  [14] . Finally, another study found that end-to-end processes are suitable for predicting stress states with abrupt changes, but not as good when assessing subtle affective states like enjoyment  [10] . Hence, end-to-end learning only provides a marginal improvement over feature engineering for physiological signal-based affect recognition. This is different from camera-based recognition, where performance is radically improved. One possible explanation is the limited amount of physiological data publicly available. Therefore, public data sets and multi-laboratory collaborations are necessary to assess the effectiveness of different training methods and cross-validation strategies.\n\nThe EPiC challenge aims to overcome the limitations in data availability and motivates researchers to work on the affect-embodiment coherence problem. Our team used theorydriven analysis and data engineering techniques to address the EPiC challenge. We opted for feature engineering and ensemble learning for our final submission. Also, we report an exploratory analysis validating our assumptions for the challenge submission.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Ii. Related Work",
      "text": "ML has been used to model emotion recognition mechanisms from data following the public release of benchmark databases. The basic procedure of classical ML-based methods consists of four steps: physiological signal collection while eliciting participants' emotions, feature extraction from the signals, training a classification model with the features, and emotion recognition based on the trained model  [16] . Research issues include the design of discriminative features and the selection of the optimal classification technique. For instance, hypothesis testing is performed over some features followed by a predictive model that makes feature selection to see if the tested features were still relevant when all features were considered together  [29] . It has been suggested that group synchrony improved arousal and valence classification  [3] . Electrocardiograms (ECG) and Electrodermal Activity (EDA) have been used as tabular data with AutoGluon-Tabular to arousal and valence across individuals and datasets  [8]  with similar accuracy (around 56 -62% respectively) to previous works. Nevertheless, the subject-independent classification remains only slightly above chance level.\n\nA crucial challenge surrounding continuous-time annotation of emotions is the lag between observed features and the reported emotion measures  [26, 24] . This lag arises from the time the rater requires to provide feedback about the experienced emotion. Such a temporal misalignment between features and labels has consequences for ML methods. Consequently, several compensation techniques have been investigated  [20, 21, 18, 19] . These methods involve estimating the reaction lag from the data, by maximizing the correlation coefficient  [20, 21, 18, 26]  or the mutual information between the multimodal features and emotional ratings  [19] . Others have used a recurrent neural network to handle the asynchronous dependencies  [24] .\n\nDeep learning (DL) has also been used in affective computing. Handcrafted feature design is not always necessary in DL  [16] , also, less modalities seem to be required to achieve equal performance. For example, Hssayeni and Ghoraani  [14]  presented a DL approach to estimate momentary emotional states from multi-modal physiological data. Used modalities included respiration, ECG, electromyography (EMG), EDA, and acceleration. The best emotion classification was achieved by a traditional method with 79% F1-score when all four physiological modalities were used. In contrast, using only two modalities, DL achieved 78% F1-score. Furthermore, there are several fusion strategies for multi-modal data: feature-level fusion, decision-level fusion, model-level fusion, and hybridlevel fusion. Late fusion by averaging class probabilities has performed well in the past  [14] .\n\nIn the case of continuous detection of valence and arousal, a previous work obtained 0.43 and 0.59 RMSE for valence and arousal, respectively  [28] , in the WESAD dataset. When dividing the continuous annotation into binary valence-arousal categories (high-low), another group of researchers reported subject-independent accuracy of 76.37% and 74.03% for valence and arousal, respectively  [30] , on the Continuously Annotated Signals of Emotion (CASE) dataset  [27] .",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Iii. Challenge Corpora",
      "text": "The challenge corpora is an open dataset that collected six physiological signals while 30 participants (15 female, age range: 22 -37 years) watched eight videos  [27] . The videos aimed to elicit a range of emotions and were rated with continuous self-reported valence and arousal in the range  of 0.5 to 9.5 using a joystick. Visual feedback was provided using the Self-Assessment Manikin  [4] . Two videos were chosen per quadrant in the valence-arousal space, often called affect grid  [25] , and were presented in pseudo-random order to the participants. Figure  1  shows the video distribution. The physiological sensing was logged at 1000 Hz, and the continuous annotation was done at 20 Hz. The physiological sensors included are: 1) Cardiac activity as measured from Electrocardiography (ECG) and Photoplethysmography (BVP). 2) Muscle activity (EMG) recorded from three muscles: the corrugator supercilii (emg coru), the zygomaticus major (emg zygo), and the trapezius (emg trap). 3) Electrodermal activity (EDA) measured from the nondominant hand. 4) Respiration (RSP) recorded from the chest. 5) Skin Temperature (SKT) recorded from the little finger of the non-dominant hand. For the EPiC Challenge, the Dataset was arranged in four scenarios to test four assumptions about the relationship between affective experiences and embodied cues. Each scenario is divided into training and test sets, as prepared by the organizers 1  .",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "A. Across-Time Scenario",
      "text": "This scenario evaluates subject-dependent and affective context-dependent model performance. The model is trained and tested over different durations of one data file (sub vid). In this scenario, each of the 240 data files, that is, 30 participants watching eight videos, is divided into training and test parts based on the time series. For each data file, the earlier part is the training data, and the latter is the test data. The training and test data are not consecutive but are spaced by an unknown length of time. The length of the training data ranged from 48 s to 127 s depending on the size of each video, with an average of 88 s. All test data files were 50 s in length.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "B. Across-Subject Scenario",
      "text": "This scenario evaluates subject-independent model performance. The model is trained on data from some participants and tested on data from another set of different, unseen, participants to verify the model's generalization ability to new people. In this scenario, the data of 30 participants were divided into five groups. Each group contains six participants watching eight videos for a total of 48 data files. This scenario consisted of five folds to use the cross-validation strategy. In each fold, the data of four groups of participants were set as the training data (192 data files in total). The 48 data files from the remaining six participants were set as the test data. The length of the training data files ranged from 50 s to 128 s, with an average of 90 s. All test data files lasted 50 s.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "C. Across-Elicitor Scenario",
      "text": "This scenario evaluates affective context-independent model performance. The model is trained on data from several affective contexts and then tested on data from a different affective context. Each affective context represents one quadrant in the valence-arousal affect grid. There were two elicitors (i.e., videos) per affective context. This verifies whether the model can infer from the physiological signals triggered by one affective context to the physiological features triggered by another affectivity. In this scenario, the data from eight videos were divided into four groups, according to the video's affective context. This resulted in four categories: low valence, high arousal; high valence, high arousal; high valence, low arousal; and low valence, low arousal. Each group contains 60 data files, which corresponds to 30 participants with two videos each. The two videos in one group are considered to trigger the same type of affectivity. By adopting a crossvalidation strategy, this scenario contains four folds. In each fold, three groups, totaling 180 data files, were chosen as the training data for the challenge. The remaining two videos for a total of 60 data were chosen as the test data.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "D. Across-Version Scenario",
      "text": "This scenario evaluates affective context-dependent model performance. The model is trained on data from a specific affective state instantiation and then tested on the other version of the same affective contexts to verify the model's generalizability across similar affective contexts. The data from eight videos were divided into two groups. Each group contains 30 users watching four video types covering all quadrants in the affect grid, for a total of 120 data files. This scenario consists of two folds. In each fold, one group is the training data, while the other is the test data.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Iv. Tackling The Challenge",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "A. Preprocessing",
      "text": "We used NeuroKit2 to preprocess the physiological signals and feature extraction  [17] . In the case of EMG, we opted to write a custom preprocessing pipeline based on  [23]  in combination with Neurokit.\n\nThe EMG pipeline to clean the signal consisted of a series of notch filters at frequencies of 60, 120, 180, and 240 Hz with a notch width of 3 Hz, followed by a bandpass filter with cutoff frequencies of 5 Hz and 250 Hz, and a detrending step. A ztransform is applied to the clean signal, before calculating the root mean square (RMS) over windows of 100 ms. To further smoothen the envelope, a Savitzky-Golay filter of third order   I . Feature extraction was performed over the whole signal using windows of different sizes as shown in Figure  2 . These windows are described in samples at 1 KHz. ECG and PPG's window size is longer to sample heart-rate variability. A similar reason applies to respiration. Regarding EDA, a mediumsized window is recommended to decompose the signal into tonic and phasic components. In contrast, the EMG window is shorter, because changes in facial expressions' EMG can happen in the order of milliseconds. As a curious fact, we found a heart-rate artifact in the trapezius EMG. This artifact might be misinterpreted as an EMG feature, but we decided to leave it in because we did not formally distinguish between data types when modeling the data.\n\nThe feature vectors were sampled at 20 Hz to match the sampling frequency of the annotations. Additionally, the preprocessed (clean) data temporally surrounding each annotation datapoint were flattened and input as additional features to the modeling block.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "B. Model Training",
      "text": "We used a consistent architecture across all four scenarios, but adopted unique strategies for designing the input of model training and generating predictions to accommodate the distinct requirements of various scenarios.\n\nFor the architecture, we employed AutoGluon, an opensource AutoML framework developed by AWS, to train our model  [11] . This framework expedites the development of machine learning models by automating model training, hyperparameter optimization, and model selection and ensembling. The AutoGluon-Tabular fits a total of 11 models that includes gradient boosting methods (CatBoost, LightGBM, LightGBMLarge, LightGBMXT, XGBoost), extra trees (Ex-traTreesMSE), K-nearest neighbors algorithm (KNeighbors-Dist, KNeighborsUnif), neural networks (NeuralNetFastAI, NeuralNetTorch), and random forests (RandomForestMSE). Furthermore, a weighted ensemble model (WeightedEnsemble L2) is fitted and employed to combine the previouslytrained models for generating predictions. To achieve optimal performance with AutoGluon, we designate the parameter presets as 'best quality', allowing AutoGluon to automatically construct robust model ensembles while allocating sufficient training time.\n\n1) Across-time scenario: We aimed to capture the unique characteristics and nuances of each subject's emotional responses and the specific stimuli embedded within the emotional context. Therefore, we trained the model on discrete datasets originating per participant and per video. In this scenario, both training and test sets comprise 240 subsets.\n\nThe training-test pairs were all collected from the same pool of participants and emotion elicitors that exhibit a one-toone correspondence. We trained 240 models on each training dataset and subsequently selected the corresponding model to yield predictions on the test sets.\n\n2) Across-subject scenario: The substantial inter-subject variability in physiological responses to stimuli and the inherent limitations of self-report labels, such as subjectivity, bias, and emotional granularity, presents a considerable challenge in developing one-size-fits-all-subjects effective models for affective computing. To generate plausible predictions, we assume that a single video should elicit similar emotions in most participants. Guided by this assumption, we trained a dedicated model for each video. In this scenario, every fold consists of 24 subjects in the training dataset and six subjects in the test dataset, with all participants having watched the same eight videos. We combined data from different subjects of each video as input when training the model. In total, we trained eight models and employed the respective models  for affective state estimation when generating predictions for corresponding test set files.\n\n3) Across-elicitor scenario: In this scenario, using only the data from the three quadrants available for estimating arousal and valence could compromise our ability to predict affective states associated with the missing quadrant accurately. To mitigate this concern, we performed a meta-analysis of the training data. We first calculated the mean value of user ratings per video file in the training set to categorize videos into the four affect grid quadrants systematically. By doing so, we could make well-founded assumptions regarding each video's quadrant affiliation.\n\nWithin this scenario, a total of eight videos were provided. By analyzing the composition of the test dataset across fourfolds, we categorized the eight videos into four groups: (0, 3), (4, 21),  (10, 22) , and  (16, 20) , see Figure  3 . Among these ratings, it is evident that videos (0, 3) and  (16, 20)  belong to the high valence high arousal (HV, HA) and low valence high arousal (LV, HA) quadrants, respectively. The categorization of the other two groups is less apparent; therefore, our hypothesis relies on the video with the more prominent rating within each of the two video groups. Consequently, we assumed that video (0, 3) belongs to the (HV, HA) quadrant,  (16, 20)  belongs to the (LV, HA) quadrant,  (10, 22)  belongs to the (LV, LA) quadrant, and (4, 21) belongs to the (HV, LA) quadrant. Based on this assumption, we employed only two relevant quadrants to achieve sample balance and maximize the variance along the valence and arousal axes. For instance, when the videos in the test set belong to the (HV, HA) quadrant, we train the valence predictor on the dataset comprising videos from the (LV, LA) and (HV, LA) quadrants, and the arousal predictor on the dataset containing videos from the (LV, HA) and (LV, LA) quadrants. This approach effectively minimizes input bias and ensures more accurate emotional state estimations for the missing quadrant.\n\n4) Across-version scenario: Given that instances of all the affective quadrants are available, albeit in only one version, we aimed to develop a general model that yields robust results by harnessing collective intelligence by applying late fusion. In other words, we assumed there is a \"wisdom of crowds\" effect when combining multiple weak classifiers into one. To this aim, we developed four separate models, each trained on a distinct set of four videos from the training dataset. During the testing process, we input the preprocessed and featureextracted data from the test set into each of these four models to generate predictions. Then we applied a late fusion strategy to obtain the final estimation. The four predictions from each model were fused by calculating their mean predicted rating values.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "V. Validation Results",
      "text": "The models were assessed using the root mean square error (RMSE) metric. A lower RMSE is better. It has the same units as the valence and arousal annotations. The final score for the EPiC challenge for our team was 1.19. Additionally, we report the detailed performance for each scenario on the test set, as reported by the workshop organizers. Our training was done on the full train set to maximize data availability. For each test data, i.e., the data of one subject and one video, the RMSE is calculated for arousal and valence, respectively. Then in each fold, the performance is assessed by averaging the RMSE values among all test data. Similarly, the model performance in each scenario is evaluated by averaging all RMSE values within the scenario. The final RMSE result was obtained by calculating the mean score on all scenarios and two prediction targets, i.e., arousal and valence. The scenarios-level RMSE and folds-level RMSE are shown in Figure  4 , Figure  5 , and Table  II .",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "A. Across-Time Scenario",
      "text": "The RMSE of predicted arousal and valence are 0.91 and 0.95, with a standard deviation of 0.83 and 0.93, respectively. Among the 240 test data, the lowest RMSE of arousal and valence is 0 and 0, and the highest is 3.87 and 4.33. For the predicted results of arousal and valence, the RMSE of 69% and 66% of the test results were below 1. Overall, the prediction error for arousal is slightly lower than that for valence.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "B. Across-Subject Scenario",
      "text": "The RMSE for the predicted arousal and valence are 1 and 1.1, respectively, with standard deviations of 0.16 and 0.04 across the five folds. In each fold, the predicted arousal RMSE is consistently lower than the predicted valence RMSE. This difference is especially noticeable in fold 4, where the RMSE and standard deviation for predicted arousal are 0.74 and 0.48, respectively, in contrast to the valence RMSE and standard deviation, which are 1.13 and 0.86, respectively.\n\nDespite the seemingly positive results in this scenario, there is a limitation on how we predicted the final ratings. We assumed that training and testing across elicitors would help to predict better the outcomes in the ratings done by other people not seen in the dataset. However, in the real world, we do not have information about the type of stimuli used. Therefore, the performance will probably be reduced, as exemplified in the following section.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "C. Across-Elicitor Scenario",
      "text": "Our model yielded the highest RMSE values, with the RMSE for arousal and valence being 1.44 and 1.42, respectively, and standard deviations of 0.51 and 0.55. We note that the high RMSE was due to poor prediction results in fold 0, where the RMSE values were twice those observed in other folds, reaching 2.29 and 2.36 for arousal and valence, respectively. By analyzing the data for this scenario, we noticed that a potential cause for the high RMSE lies in the   6 , the test data in fold 0 contains videos 16 and 20 in the upper left quadrant of the affective grid. The rating patterns of arousal and valence differ from the data in the other three quadrants, which were used as training data. This suggests that the larger variations in ratings characteristic of negative, high-arousing emotions are not present in the other types of emotion. Furthermore, these results also demonstrate the reliance of our model on data similarity, indicating a weaker generalization capability for novel data patterns.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "D. Across-Version Scenario",
      "text": "In this scenario, the prediction RMSE using our model is 1.30 for both arousal and valence, with standard deviations of 0.30 and 0.24, respectively. Although each of the two groups' data in this scenario covered all four emotional states, the cross-validation results revealed that our model's RMSE in fold 0 was 0.5 lower than in fold 1. This suggests that an appropriately balanced training set, encompassing all four emotional states, can significantly enhance the model's generalization capabilities.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Vi. Revisiting Assumptions",
      "text": "",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "A. Lag Between Physiological Signals And Ratings",
      "text": "When preparing the data to train our models, we assumed that the physiological changes happen at different speeds depending on the measurement metrics used. Here we investigate the effect of the time delay in reporting emotions. In particular, we followed the general procedure described by Schmitt, Ringeval, and Schuller  [26] . We shifted the features forward in time in steps of 0.005 s up to a maximum of 0.05 s and trained a Gated Recurrent Unit (GRU) model to predict arousal and valence. Note that the annotations were performed at 20 Hz while the physiological signals were sampled at 1000 Hz. Then, we experimented with using each individual signal as input to the model in isolation before combining all signals as input. The analysis results are in Table  III . The results suggest that predictive performance generally improves when accounting for annotation delays. However, the delay yielding the most empirical gains varies for each biosignal, and we often found several minimum values. A further investigation of the timing relationships between physiological change, experience, and annotation is needed to understand when the differences significantly disrupt the predictions.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "B. The Gradual Nature Of Changes In Emotion",
      "text": "Qualitatively, we noticed that the predictions from our models were characterized by more high-frequency changes than the annotations provided in the training data, which change more gradually over time. Consequently, we utilized a moving average window comprising 10 samples, equivalent to a 2 Hz low-pass filter. The choice of the window length follows from the observation that the annotations were provided at 20 Hz, so the Nyquist frequency is 10 Hz -only changes at 10 Hz can be measured with the joystick described in the dataset. Furthermore, we assumed people would not make more than two abrupt changes per second. Further investigation is required to establish whether the nature of emotion changes is a gradual process, or whether the relative smoothness of the ground-truth ratings is an annotation artifact.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "C. Single-And Multi-Label Predictors",
      "text": "Considering that participants simultaneously rated their emotions for valence (X-axis) and arousal (Y-axis) using a two-dimensional joystick, we speculated on the potential connection between these two values, despite the orthogonal nature of the valence-arousal model's axes. To evaluate this hypothesis, we extracted 24 datasets from scenario 1, each containing data from the combination of six participants and four videos. We compared the following approaches: (1) independent prediction of valence and arousal, (2) predicting valence first and then using both physiological signals and predicted valence values for arousal prediction, and (3) predicting arousal first and subsequently using the predicted arousal values for valence prediction. This comparison investigated any potential connections between valence and arousal predictions. As demonstrated in Figure  7 , the performance differences among these three strategies are minimal. As a result, owing to the slower training process associated with multilabel predictors compared to single-label cases, we ultimately chose to employ the independent prediction strategy for this competition.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Vii. Discussion And Future Directions",
      "text": "We introduced an attempt to address the EPiC Challenge. We predicted continuous valence and arousal ratings from several biosignals, across four scenarios. We used readily available algorithms, with our novelty being (a) the window choices for feature calculation; (b) the use of data around each annotation point; and (c) our use of theoretical assumptions to maximize data variance in each scenario's training. Our overall RMSE for the four validation scenarios was 1.19, as provided by the competition organizers. This result still has considerable room for improvement compared to previous work on   predicting continuous valence and arousal annotations from physiological data, and can be used as a baseline for future regression studies on the CASE dataset. As expected from the literature, fitting a personalized model to predict a single persons's reaction at a future point (across-time) is an easier problem than in the other scenarios. To extend our model to other, unseen people, we used information about the stimuli, and capitalized in our knowledge of the affective context in which the data was collected. By training several models per stimuli, we reduced the RMSE in the across-subject scenario. However, this strategy is unlikely to work in the real world, as we typically would not have information about the stimulus. A similar approach was used in the across-elicitor scenario. By examining the results (Figure  6 ), we hypothesize that high arousal and low valence emotions display abrupt physiological changes not present in other affective quadrants. Future work should explore whether this is consistently true, and devise a method to tackle the lack of information during training. Furthermore, the results of the across-version validation signal that expected affective messages depicted in a stimulus might not produce the same effect in different individuals. Future work should validate if this is the case, and assess if a weighted late fusion provides improvements with respect to an averaging function. This would also validate or refute whether our bet for a \"wisdom of a crowd classifier\" is suitable. Finally, future work should be done to formally assess if end-to-end methods outperform ensemble methods and feature engineering similar to those used in this work.\n\nETHICAL IMPACT STATEMENT This research was a data analysis of the CASE dataset  [27] . All data provided is anonymous, and obtained following the Declaration of Helsinki. Our results have several limitations. The sample size is only 30 people, and no mentions of the cultural background of the participants are made in the dataset. The interpretation of the stimuli, and therefore the ratings and physiological changes, might differ from person to person. Therefore, our results need to be replicated in other corpora. Moreover, we built our model based on certain assumptions that are described, but not formally validated. These assumptions might not yield the same performance in other datasets. Finally, our results are biased to better predict the situations in the dataset. Therefore, our model should be used with caution.",
      "page_start": 6,
      "page_end": 7
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Diagram depicting the dataset structure utilized in this competition.",
      "page": 2
    },
    {
      "caption": "Figure 1: shows the video distribution.",
      "page": 2
    },
    {
      "caption": "Figure 2: The pipeline used to process each signal, and the post-processing",
      "page": 3
    },
    {
      "caption": "Figure 3: The meta-analysis for determining the quadrant affiliation of videos",
      "page": 4
    },
    {
      "caption": "Figure 3: Among these",
      "page": 4
    },
    {
      "caption": "Figure 4: Scenarios-level RMSE. Error bars represent standard deviation.",
      "page": 5
    },
    {
      "caption": "Figure 5: Folds-level RMSE. Error bars represent standard deviation.",
      "page": 5
    },
    {
      "caption": "Figure 4: , Figure 5, and",
      "page": 5
    },
    {
      "caption": "Figure 6: , the test data in fold",
      "page": 6
    },
    {
      "caption": "Figure 7: , the performance dif-",
      "page": 6
    },
    {
      "caption": "Figure 6: Plot of the rating averages per each stimuli video. Shaded areas represent the standard deviation among participants.",
      "page": 7
    },
    {
      "caption": "Figure 7: The comparison between single and multi-label predictors. The “single",
      "page": 7
    },
    {
      "caption": "Figure 6: ), we hypothesize that high",
      "page": 7
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "raw signal": "clean signal",
          "Column_2": ""
        },
        {
          "raw signal": "features1\n(69 dim)",
          "Column_2": ""
        },
        {
          "raw signal": "input data2\n(776 dim)",
          "Column_2": ""
        },
        {
          "raw signal": "model",
          "Column_2": ""
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "",
          "Column_2": "21 21",
          "0": "",
          "Column_4": "",
          "Column_6": "",
          "0, 3 (m": "",
          "edian)": "ean)\nmedian)",
          "Column_9": ""
        },
        {
          "Column_1": "22\n10",
          "Column_2": "",
          "0": "",
          "Column_4": "",
          "Column_6": "",
          "0, 3 (m": "4, 21 (\n10, 22",
          "edian)": "mean)\n(median)",
          "Column_9": ""
        },
        {
          "Column_1": "",
          "Column_2": "22 10",
          "0": "16\n1",
          "Column_4": "",
          "Column_6": "6",
          "0, 3 (m": "10, 22\n16, 20\n16, 20",
          "edian)": "(mean)\n(median)\n(mean)",
          "Column_9": ""
        },
        {
          "Column_1": "",
          "Column_2": "",
          "0": "",
          "Column_4": "",
          "Column_6": "",
          "0, 3 (m": "",
          "edian)": "20",
          "Column_9": ""
        }
      ],
      "page": 4
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Capturing emotion reactivity through physiology measurement as a foundation for affective engineering in engineering design science and engineering practices",
      "authors": [
        "Stephanie Balters",
        "Martin Steinert"
      ],
      "year": "2017",
      "venue": "Journal of Intelligent Manufacturing"
    },
    {
      "citation_id": "2",
      "title": "Emotional Expressions Reconsidered: Challenges to Inferring Emotion From Human Facial Movements",
      "authors": [
        "Lisa Feldman"
      ],
      "year": "2019",
      "venue": "Psychological Science in the Public Interest"
    },
    {
      "citation_id": "3",
      "title": "Group Synchrony for Emotion Recognition using Physiological Signals",
      "authors": [
        "Patrícia Bota"
      ],
      "year": "2023",
      "venue": "IEEE Trans. Affect"
    },
    {
      "citation_id": "4",
      "title": "Measuring emotion: The self-assessment manikin and the semantic differential",
      "authors": [
        "Margaret Bradley",
        "Peter Lang"
      ],
      "year": "1994",
      "venue": "Journal of Behavior Therapy and Experimental Psychiatry"
    },
    {
      "citation_id": "5",
      "title": "Inferring psychological significance from physiological signals",
      "authors": [
        "John Cacioppo",
        "Louis Tassinary"
      ],
      "year": "1990",
      "venue": "American Psychologist"
    },
    {
      "citation_id": "6",
      "title": "The Oxford Handbook of Affective Computing",
      "authors": [
        "A Rafael",
        "Calvo"
      ],
      "year": "2015",
      "venue": "The Oxford Handbook of Affective Computing"
    },
    {
      "citation_id": "7",
      "title": "The James-Lange Theory of Emotions: A Critical Examination and an Alternative Theory",
      "authors": [
        "B Walter",
        "Cannon"
      ],
      "year": "1987",
      "venue": "The American Journal of Psychology"
    },
    {
      "citation_id": "8",
      "title": "An Evaluation of Tabular Neural Network Approaches for Human Affective State Classification from Physiological Signals",
      "authors": [
        "David Chhan",
        "Vernon J Lawhern"
      ],
      "year": "2022",
      "venue": "An Evaluation of Tabular Neural Network Approaches for Human Affective State Classification from Physiological Signals"
    },
    {
      "citation_id": "9",
      "title": "Fact or artifact? Demand characteristics and participants' beliefs can moderate, but do not fully account for, the effects of facial feedback on emotional experience",
      "authors": [
        "Nicholas Coles"
      ],
      "year": "2022",
      "venue": "Journal of Personality and Social Psychology"
    },
    {
      "citation_id": "10",
      "title": "Can We Ditch Feature Engineering? End-to-End Deep Learning for Affect Recognition from Physiological Sensor Data",
      "authors": [
        "Maciej Dzieżyc"
      ],
      "year": "2020",
      "venue": "Sensors"
    },
    {
      "citation_id": "11",
      "title": "AutoGluon-Tabular: Robust and Accurate AutoML for Structured Data",
      "authors": [
        "Nick Erickson"
      ],
      "year": "2020",
      "venue": "AutoGluon-Tabular: Robust and Accurate AutoML for Structured Data"
    },
    {
      "citation_id": "12",
      "title": "Studying the dynamics of autonomic activity during emotional experience",
      "authors": [
        "Yulia Golland"
      ],
      "year": "2014",
      "venue": "Psychophysiology"
    },
    {
      "citation_id": "13",
      "title": "Is automatic facial expression recognition of emotions coming to a dead end? The rise of the new kids on the block",
      "authors": [
        "Hatice Gunes",
        "Hayley Hung"
      ],
      "year": "2016",
      "venue": "Image and Vision Computing"
    },
    {
      "citation_id": "14",
      "title": "Multi-Modal Physiological Data Fusion for Affect Estimation Using Deep Learning",
      "authors": [
        "D Murtadha",
        "Behnanumbersz Hssayeni",
        "Ghoraani"
      ],
      "year": "2021",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "15",
      "title": "End-to-end learning for dimensional emotion recognition from physiological signals",
      "authors": [
        "Gil Keren"
      ],
      "year": "2017",
      "venue": "End-to-end learning for dimensional emotion recognition from physiological signals"
    },
    {
      "citation_id": "16",
      "title": "Deep Facial Expression Recognition: A Survey",
      "authors": [
        "Shan Li",
        "Weihong Deng"
      ],
      "year": "2022",
      "venue": "IEEE Trans. Affect"
    },
    {
      "citation_id": "17",
      "title": "NeuroKit2: A Python toolbox for neurophysiological signal processing",
      "authors": [
        "Dominique Makowski"
      ],
      "year": "2021",
      "venue": "Behavior Research Methods"
    },
    {
      "citation_id": "18",
      "title": "Analysis and compensation of the reaction lag of evaluators in continuous emotional annotations",
      "authors": [
        "Soroosh Mariooryad",
        "Carlos Busso"
      ],
      "year": "2013",
      "venue": "Humaine Association Conference on ACII"
    },
    {
      "citation_id": "19",
      "title": "Correcting time-continumbersnuous emotional labels by modeling the reaction lag of evaluators",
      "authors": [
        "Soroosh Mariooryad",
        "Carlos Busso"
      ],
      "year": "2014",
      "venue": "IEEE Trans. Affect"
    },
    {
      "citation_id": "20",
      "title": "Automatic segmentation of spontaneous data using dimensional labels from multiple coders",
      "authors": [
        "A Mihalis",
        "Nicolaou"
      ],
      "year": "2010",
      "venue": "Proc. of LREC Int. Workshop on Multimodal Corpora: Advances in Capturing, Coding and Analyzing Multimodality"
    },
    {
      "citation_id": "21",
      "title": "Robust continuous prediction of human emotions using multiscale dynamic cues",
      "authors": [
        "Jérémie Nicolle"
      ],
      "year": "2012",
      "venue": "Proceedings of the 14th ACM ICMI"
    },
    {
      "citation_id": "22",
      "title": "Dynamic autonomic nervous system states arise during emotions and manifest in basal physiology",
      "authors": [
        "Lorenzo Pasquini"
      ],
      "year": "2023",
      "venue": "Psychophysiology"
    },
    {
      "citation_id": "23",
      "title": "Smile Action Unit detection from distal wearable Electromyography and Computer Vision",
      "authors": [
        "Monica Perusquía-Hernández"
      ],
      "year": "2021",
      "venue": "16th IEEE International Conference on Automatic Face and Gesture Recognition"
    },
    {
      "citation_id": "24",
      "title": "Prediction of Asynchronous Dimensional Emotion Ratings from Audiovisual and Physiological Data",
      "authors": [
        "Fabien Ringeval"
      ],
      "year": "2014",
      "venue": "Pattern Recognition Letters"
    },
    {
      "citation_id": "25",
      "title": "Affect Grid: A Single-Item Scale of Pleasure and Arousal",
      "authors": [
        "Russell James"
      ],
      "year": "1989",
      "venue": "Journal of Personality and Social Psychology"
    },
    {
      "citation_id": "26",
      "title": "At the Border of Acoustics and Linguistics: Bag-of-Audio-Words for the Recognition of Emotions in Speech",
      "authors": [
        "Maximilian Schmitt"
      ],
      "year": "2016",
      "venue": "At the Border of Acoustics and Linguistics: Bag-of-Audio-Words for the Recognition of Emotions in Speech"
    },
    {
      "citation_id": "27",
      "title": "A dataset of continuous affect annotations and physiological signals for emotion analysis",
      "authors": [
        "Karan Sharma"
      ],
      "year": "2019",
      "venue": "Scientific Data"
    },
    {
      "citation_id": "28",
      "title": "Predicting Emotion with Biosignals: A Comparison of Classification and Regression Models for Estimating Valence and Arousal Level Using Wearable Sensors",
      "authors": [
        "Pekka Siirtola"
      ],
      "year": "2023",
      "venue": "Sensors"
    },
    {
      "citation_id": "29",
      "title": "Visual attention in schizophrenia: Eye contact and gaze aversion during clinical interactions",
      "authors": [
        "Alexandria Vail"
      ],
      "year": "2017",
      "venue": "Seventh International Conference on ACII"
    },
    {
      "citation_id": "30",
      "title": "CorrNet: Fine-Grained Emotion Recognition for Video Watching Using Wearable Physiological Sensors",
      "authors": [
        "Tianyi Zhang"
      ],
      "year": "2021",
      "venue": "Sensors"
    }
  ]
}