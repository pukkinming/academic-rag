{
  "paper_id": "2411.00822v1",
  "title": "Eeg-Based Multimodal Representation Learning For Emotion Recognition",
  "published": "2024-10-29T01:35:17Z",
  "authors": [
    "Kang Yin",
    "Hye-Bin Shin",
    "Dan Li",
    "Seong-Whan Lee"
  ],
  "keywords": [
    "brain-computer interface",
    "electroencephalogram",
    "multimodal training",
    "emotion recognition;"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Multimodal learning has been a popular area of research, yet integrating electroencephalogram (EEG) data poses unique challenges due to its inherent variability and limited availability. In this paper, we introduce a novel multimodal framework that accommodates not only conventional modalities such as video, images, and audio, but also incorporates EEG data. Our framework is designed to flexibly handle varying input sizes, while dynamically adjusting attention to account for feature importance across modalities. We evaluate our approach on a recently introduced emotion recognition dataset that combines data from three modalities, making it an ideal testbed for multimodal learning. The experimental results provide a benchmark for the dataset and demonstrate the effectiveness of the proposed framework. This work highlights the potential of integrating EEG into multimodal systems, paving the way for more robust and comprehensive applications in emotion recognition and beyond.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "Multimodal representation learning has gained significant attention in the field of artificial intelligence, particularly in tasks that involve complex human behaviors such as emotion recognition  [1] . By combining data from multiple modalities, such as video, audio, and physiological signals, multimodal systems can capture diverse and complementary information to improve model performance  [2] -  [4] . However, integrating electroencephalogram (EEG) data into such frameworks introduces unique challenges due to the inherent variability, noise, and limited availability of EEG datasets compared to other modalities. Despite its potential to provide direct insights into brain activity, the effective utilization of EEG data in multimodal settings remains an open research problem. EEG data, widely used in neuroscience and clinical research  [5] , offer a non-invasive window into the electrical activity of the brain. This modality has the advantage of capturing cognitive and emotional states in real-time, making it particularly valuable for emotion recognition tasks  [6] . However, EEG signals are often noisy  [7] , highly variable across subjects and sessions, and recorded at a higher dimensionality than traditional modalities like video and audio. These characteristics complicate the feature extraction process and hinder the straightforward integration of EEG into multimodal learning frameworks  [8] . Existing work on emotion recognition has largely focused on conventional modalities, with many studies opting to exclude EEG due to these challenges  [9] . As a result, the full potential of EEG data in improving emotion recognition systems remains underexplored  [10] ,  [11] . Recent advances in multimodal learning have explored various fusion strategies to integrate heterogeneous data sources, such as feature concatenation  [12] , attention mechanisms  [13] , and joint embedding spaces  [14] . These methods have shown promise in combining modalities like video and audio, where data characteristics are more homogeneous. However, the integration of EEG data presents a distinct set of limitations  [15] . Current models often struggle to dynamically adjust to the variability in feature importance across different modalities  [16] , especially when EEG is involved. Additionally, existing multimodal systems may not be flexible enough to handle the varying input sizes and feature distributions that arise when combining EEG with other sensory data  [17] . This often results in suboptimal performance or requires manual tuning to accommodate the unique nature of EEG signals  [18] ,  [19] .\n\nIn this paper, we propose a novel multimodal framework for emotion recognition that integrates EEG, video, and audio data. Our approach dynamically adjusts attention weights to prioritize key features from each modality and adapts to varying input sizes.  By addressing the limitations of existing multimodal frameworks and incorporating EEG data, our work paves the way for more robust and versatile applications in emotion recognition. The proposed framework highlights the value of using diverse data sources to capture complex emotional states, offering a more comprehensive understanding of human emotions than unimodal systems alone.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Ii. Methodology A. Framework Overview",
      "text": "The proposed multimodal framework, illustrated in Fig.  1 , begins by processing video, audio, and EEG data through specifically tailored transformers to extract unique features from each modality. The video input is divided into frames, which are fed into a Vision Transformer (ViT)  [20]  to capture spatial and temporal visual patterns. Similarly, audio is transformed into a spectrogram and passed through an Audio Spectrogram Transformer (AST)  [21] , designed to extract meaningful frequency and temporal features. EEG data, represented as temporal signals, are processed by the EEGformer  [22] , a transformer-based architecture equipped with a 1-D channel-wise convolutional neural network (CNN) to handle the high-dimensional, channel-specific nature of EEG signals.\n\nOnce each modality's distinct features are extracted-denoted as h vis for video, h aud for audio, and h eeg for EEG-the features are concatenated and sent into a shared multi-head attention module. This attention mechanism allows the model to learn the importance of each modality's features in a dynamic and context-dependent manner. Through this process, the framework is able to weigh the contributions of visual, auditory, and brainwave data appropriately for emotion recognition. Finally, the fused features are passed through a multi-layer perceptron (MLP) to output the final emotion prediction, classifying emotions such as anger, sadness, neutrality, calmness, and happiness.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "B. Network And Implementation",
      "text": "In the pre-training stage, each modality's transformer is trained separately on its own input domain. This modalityspecific training phase allows the network to focus on learning the unique representations inherent to each modality without interference from other data sources. By pre-training the ViT on video frames, the AST on audio spectrograms, and the EEGformer on brainwave signals, we ensure that each feature extractor captures the most salient features relevant to its input type. This modular approach helps mitigate the complexity that arises from combining heterogeneous data sources.\n\nThe fine-tuning stage involves fusing the extracted features from all three modalities and processing them together in a joint learning framework. At this stage, we freeze the weights of the pretrained feature extractors to retain their modalityspecific learned representations and introduce a shared multihead attention decoder for further fine-tuning. This attentionbased fusion mechanism dynamically adjusts the importance of each modality by attending to the most relevant features based on the context of the emotion recognition task. The modality-specific features h vis , h aud and h eeg are first flattened and then input into the multi-head attention decoder, which learns to combine and prioritize these features for optimal emotion classification.\n\nThe shared multi-head attention decoder effectively handles the interplay between visual, auditory, and brainwave signals by using query, key, and value representations for each modality, allowing the network to focus on the most informative features for the task. This fusion process ensures that complementary information from the different modalities is leveraged in a synergistic manner, enhancing the overall model's performance in emotion recognition. The output of the multi-head attention layer is a fused multimodal feature representation, which is then passed through a MLP for final emotion classification.\n\nFor both the pre-training and fine-tuning stages, we adopt a simple yet effective cross-entropy loss function for classification, given by the following equation:\n\nwhere h represents the feature vector (either unimodal or multimodal), f is its corresponding transformer decoder, and y h is the ground truth class label. This objective ensures that the model learns to minimize the difference between the predicted and actual emotion classes during training. The implementation is available at https://github.com/Kang1121/bciwinter-2025.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Iii. Experiment",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "A. Dataset",
      "text": "The EAV  [23]  dataset, recently released, includes 42 subjects with 30-channel EEG, video, and audio recordings, each contributing 200 interactions, with 20-second trials during both listening and speaking tasks. While 200 EEG trials and video clips are available, only 100 audio files correspond to speaking-only interactions. This is the first public dataset combining EEG, audio, and video for emotion recognition in a conversational setting.\n\nFollowing the authors' preprocessing methods, we evaluate the performance of classic transformer encoders and our proposed multimodal framework at the subject level.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "B. Subject-Wise Task Performance",
      "text": "Table  I  presents the performance of each unimodal model alongside our proposed multimodal framework, evaluated on a subject-wise basis. We benchmark the EAV dataset using multimodal inputs, achieving a performance of 70.86 % in accuracy. This represents an improvement of 3.64 % over the vision-only modality, 12.69 % over the audio modality, and 17.35 % over the EEG modality. These results highlight the advantage of integrating multiple modalities, especially given the limitations of unimodal approaches. As previously reported in  [23] , using unimodal data for emotion recognition-particularly EEG and audio-does not yield highly satisfying results. Our experiments show that EEG data alone achieves an average accuracy of 53.51 %, while audio data reaches 58.17 %. In contrast, video data performs significantly better, achieving an accuracy of 67.22 %, underscoring its prominent role in emotion recognition tasks. The discrepancy in performance across modalities suggests that visual information carries more distinctive cues for emotional state classification, as video captures a broad range of non-verbal signals such as facial expressions and gestures. From our case studies, we observed that despite the experimental design, which required subjects to remain fully engaged during both speaking and listening tasks, the emotional content in the audio recordings appeared subdued. In contrast, video clips revealed more nuanced differences across emotional classes, such as micro-expressions and body language, which were more reliably captured by the vision-  based model. This may explain the superior performance of the video modality relative to audio and EEG data.\n\nFig.  2  provides a visual comparison of the subject-wise performance across modalities. The figure illustrates the variability in performance between subjects, reinforcing the idea that emotion recognition is subject-dependent to some extent. Importantly, our proposed multimodal framework consistently outperforms the vision-only transformer across nearly all subjects, demonstrating the effectiveness of our fusion strategy in capturing and integrating the complementary information provided by each modality. By leveraging the strengths of video, audio, and EEG data, the proposed model can make more informed predictions, resulting in overall improved accuracy. These findings confirm that multimodal approaches are crucial in addressing the limitations of unimodal systems and enhancing the robustness of emotion recognition tasks.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Iv. Conclusion",
      "text": "We propose a two-stage, end-to-end multimodal framework that integrates EEG, video, and audio inputs, enabling the joint learning of different modalities for emotion recognition tasks. Leveraging transformers as the backbone for each modality, our framework effectively captures and fuses modality-specific features, allowing for a more comprehensive understanding of emotional states. The effectiveness of this approach is demonstrated through extensive benchmarking on the newly introduced EAV dataset, which is specifically designed to support multimodal learning for emotion recognition.\n\nOur proposed framework is both simple and highly effective, offering a strong baseline for future researchers to build upon. By presenting this framework, we hope to inspire further exploration into EEG-based multimodal learning, encouraging the research community to not only benchmark on this dataset but also delve deeper into the rich possibilities offered by integrating diverse modalities. Through this work, we aim to contribute a foundational model that can guide future advancements in multimodal emotion recognition and EEGbased research.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Overview of the proposed attention-based multimodal emotion recognition framework, extracting EEG, audio, and visual features with specifically",
      "page": 2
    },
    {
      "caption": "Figure 2: Barplot illustration of the subject-wise performance across modalities.",
      "page": 4
    },
    {
      "caption": "Figure 2: provides a visual comparison of the subject-wise",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "",
          "Avg. Vision\nAvg. Audio\nAvg. EEG\nAvg. Multimodal\nVision\nAudio": "EEG\nMultimodal"
        }
      ],
      "page": 4
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Abstract representations of associated emotions in the human brain",
      "authors": [
        "J Kim"
      ],
      "year": "2015",
      "venue": "J. Neurosci"
    },
    {
      "citation_id": "2",
      "title": "Cross-modal credibility modelling for EEG-based multimodal emotion recognition",
      "authors": [
        "Y Zhang"
      ],
      "year": "2024",
      "venue": "J. Neural Eng"
    },
    {
      "citation_id": "3",
      "title": "eRAD-Fe: Emotion recognition-assisted deep learning framework",
      "authors": [
        "S.-H Kim",
        "N Nguyen",
        "H.-J Yang",
        "S.-W Lee"
      ],
      "year": "2021",
      "venue": "IEEE Trans. Instrum. Meas"
    },
    {
      "citation_id": "4",
      "title": "Cross-modal diversity-based active learning for multimodal emotion estimation",
      "authors": [
        "Y Xu"
      ],
      "year": "2023",
      "venue": "Proc. IEEE Int. Jt. Conf. Neural Netw. (IJCNN)"
    },
    {
      "citation_id": "5",
      "title": "Continuous EEG decoding of pilots' mental states using multiple feature blockbased convolutional neural network",
      "authors": [
        "D.-H Lee",
        "J.-H Jeong",
        "K Kim",
        "B.-W Yu",
        "S.-W Lee"
      ],
      "year": "2020",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "6",
      "title": "Functional emotion transformer for EEG-assisted cross-modal emotion recognition",
      "authors": [
        "W.-B Jiang",
        "Z Li",
        "W.-L Zheng",
        "B.-L Lu"
      ],
      "year": "2024",
      "venue": "Proc. IEEE Int. Conf. Acoust. Speech Signal Process"
    },
    {
      "citation_id": "7",
      "title": "Classification of drowsiness levels based on a deep spatio-temporal convolutional bidirectional LSTM network using electroencephalography signals",
      "authors": [
        "J.-H Jeong",
        "B.-W Yu",
        "D.-H Lee",
        "S.-W Lee"
      ],
      "year": "2019",
      "venue": "Brain Sci"
    },
    {
      "citation_id": "8",
      "title": "GANSER: A self-supervised data augmentation framework for EEG-based emotion recognition",
      "authors": [
        "Z Zhang",
        "Y Liu",
        "S Zhong"
      ],
      "year": "2022",
      "venue": "IEEE Trans. Affect. Comput"
    },
    {
      "citation_id": "9",
      "title": "A multiview CNN with novel variance layer for motor imagery brain computer interface",
      "authors": [
        "R Mane",
        "N Robinson",
        "A Vinod",
        "S.-W Lee",
        "C Guan"
      ],
      "venue": "Proc. Int. Conf"
    },
    {
      "citation_id": "10",
      "title": "Visual-to-EEG cross-modal knowledge distillation for continuous emotion recognition",
      "authors": [
        "S Zhang",
        "C Tang",
        "C Guan"
      ],
      "year": "2022",
      "venue": "Pattern Recognit"
    },
    {
      "citation_id": "11",
      "title": "WeDea: A new EEG-based framework for emotion recognition",
      "authors": [
        "S.-H Kim",
        "H.-J Yang",
        "N Nguyen",
        "S Prabhakar",
        "S.-W Lee"
      ],
      "year": "2021",
      "venue": "IEEE J. Biomed. Health Inform"
    },
    {
      "citation_id": "12",
      "title": "A supervised information enhanced multi-granularity contrastive learning framework for EEG based emotion recognition",
      "authors": [
        "X Li"
      ],
      "year": "2024",
      "venue": "Proc. IEEE Int. Conf. Acoust. Speech Signal Process"
    },
    {
      "citation_id": "13",
      "title": "EmT: A novel transformer for generalized cross-subject EEG emotion recognition",
      "authors": [
        "Y Ding"
      ],
      "year": "2024",
      "venue": "EmT: A novel transformer for generalized cross-subject EEG emotion recognition",
      "arxiv": "arXiv:2406.18345"
    },
    {
      "citation_id": "14",
      "title": "Generalized contrastive partial label learning for cross-subject EEG-based emotion recognition",
      "authors": [
        "W Li",
        "L Fan",
        "S Shao",
        "A Song"
      ],
      "year": "2024",
      "venue": "IEEE Trans. Instrum. Meas"
    },
    {
      "citation_id": "15",
      "title": "Classification of pilots' mental states using a multimodal deep learning network",
      "authors": [
        "S.-Y Han",
        "N.-S Kwak",
        "T Oh",
        "S.-W Lee"
      ],
      "year": "2020",
      "venue": "Biocybern. Biomed. Eng"
    },
    {
      "citation_id": "16",
      "title": "An attention-enhanced retentive broad learning system for subject-generic emotion recognition from EEG signals",
      "authors": [
        "X Zhong",
        "F Wu",
        "Z Yin",
        "G Liu"
      ],
      "year": "2024",
      "venue": "Proc. IEEE Int. Conf. Acoust. Speech Signal Process"
    },
    {
      "citation_id": "17",
      "title": "Weighted graph regularized sparse brain network construction for MCI identification",
      "authors": [
        "R Yu"
      ],
      "year": "2019",
      "venue": "Pattern Recognit"
    },
    {
      "citation_id": "18",
      "title": "CEDNet: A continuous emotion detection network for naturalistic stimuli using MEG signals",
      "authors": [
        "Z He",
        "G Zhang"
      ],
      "year": "2024",
      "venue": "Proc. IEEE Int. Conf. Acoust. Speech Signal Process"
    },
    {
      "citation_id": "19",
      "title": "A framework for schizophrenia EEG signal classification with nature inspired optimization algorithms",
      "authors": [
        "S Prabhakar",
        "H Rajaguru",
        "S.-W Lee"
      ],
      "year": "2020",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "20",
      "title": "ViViT: A video vision transformer",
      "authors": [
        "A Arnab"
      ],
      "year": "2021",
      "venue": "Proc. IEEE Int. Conf. Comput. Vis. (ICCV)"
    },
    {
      "citation_id": "21",
      "title": "AST: Audio spectrogram transformer",
      "authors": [
        "Y Gong",
        "Y.-A Chung",
        "J Glass"
      ],
      "year": "2021",
      "venue": "AST: Audio spectrogram transformer",
      "arxiv": "arXiv:2104.01778"
    },
    {
      "citation_id": "22",
      "title": "EEGformer: A transformer-based brain activity classification method using EEG signal",
      "authors": [
        "Z Wan"
      ],
      "year": "2023",
      "venue": "Front. Neurosci"
    },
    {
      "citation_id": "23",
      "title": "EAV: EEG-audio-video dataset for emotion recognition in conversational contexts",
      "authors": [
        "M.-H Lee"
      ],
      "year": "2024",
      "venue": "Sci. Data"
    }
  ]
}