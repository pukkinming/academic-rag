{
  "paper_id": "2407.03640v1",
  "title": "Generative Technology For Human Emotion Recognition: A Scope Review",
  "published": "2024-07-04T05:22:55Z",
  "authors": [
    "Fei Ma",
    "Yucheng Yuan",
    "Yifan Xie",
    "Hongwei Ren",
    "Ivan Liu",
    "Ying He",
    "Fuji Ren",
    "Fei Richard Yu",
    "Shiguang Ni"
  ],
  "keywords": [
    "Emotion Recognition",
    "Generative Technology",
    "Autoencoder",
    "Generative Adversarial Network",
    "Diffusion Model",
    "Large Language Model"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Affective computing stands at the forefront of artificial intelligence (AI), seeking to imbue machines with the ability to comprehend and respond to human emotions. Central to this field is emotion recognition, which endeavors to identify and interpret human emotional states from different modalities, such as speech, facial images, text, and physiological signals. In recent years, important progress has been made in generative models, including Autoencoder, Generative Adversarial Network, Diffusion Model, and Large Language Model. These models, with their powerful data generation capabilities, emerge as pivotal tools in advancing emotion recognition. However, up to now, there remains a paucity of systematic efforts that review generative technology for emotion recognition. This survey aims to bridge the gaps in the existing literature by conducting a comprehensive analysis of over 320 research papers until June 2024. Specifically, this survey will firstly introduce the mathematical principles of different generative models and the commonly used datasets. Subsequently, through a taxonomy, it will provide an in-depth analysis of how generative techniques address emotion recognition based on different modalities in several aspects, including data augmentation, feature extraction, semi-supervised learning, cross-domain, etc. Finally, the review will outline future research directions, emphasizing the potential of generative models to advance the field of emotion recognition and enhance the emotional intelligence of AI systems.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "In the field of affective computing  [1, 2, 3] , researchers are actively engaged in developing technologies that can simulate, recognize, and understand human emotions. The primary objective of these endeavors is to imbue computers with a profound level of emotional perception, thereby facilitating more intelligent and human-like interactive experiences  [4, 5] . There are extensive applications of affective computing in numerous fields. For example, affective computing can be employed in healthcare to monitor patients' emotional changes, assist in diagnosing mental disorders, and evaluate treatment outcomes  [6, 7] . In education, it is employed to develop emotionally intelligent educational systems that adjust teaching strategies based on students' emotional states, thereby enhancing learning effectiveness  [8, 9] . In the field of autonomous driving, affective computing can monitor drivers' emotional conditions and provide timely warnings for risks such as fatigue driving  [10, 11] . In marketing, it enables the analysis of consumers' emotional preferences, providing insights for businesses to optimize marketing strategies and customer service  [12, 13] .\n\nAs a core component of affective computing, emotion recognition  [14, 15, 16, 17]  focuses on identifying emotional states from the data that conveys human emotions. Due to the inherent heterogeneity of human emotional expressions, these data can come from various modalities, such as speech, facial images, text, and physiological signals. Notably, speech emotion recognition (SER) identifies the speaker's emotion by extracting features such as pitch, volume, and speech rate  [18] . Facial emotion recognition (FER) predicts human emotional states by detecting and tracking facial feature points  [19] . Textual emotion recognition (TER) focuses on identifying the emotional content expressed in written language, such as social media posts, customer reviews, or personal messages  [20] . Physiological signals, such as electroencephalogram (EEG), electrocardiogram (ECG), heart rate variability (HRV), electrodermal activity (EDA), has become more prevalent in recent years due to their ability to objectively measure an individual's emotional state  [21] . In addition, multimodal emotion recognition (MER) also receives increasing attention  [22, 23] . Compared with emotion recognition based on a single modality, it comprehensively utilizes emotional information from different modalities to improve the performance of emotion recognition.\n\nOver the past few decades, researchers have conducted extensive work on emotion recognition based on machine learning and deep learning  [16, 24] , among which generative technology-based methods have shown tremendous potential. In contrast to discriminative models that directly learn the decision boundaries between different emotion categories, generative methods focus on learning the intrinsic distribution and representation of the emotional data  [25, 26] . Generative models, with their powerful generative capabilities, can generate samples highly similar to real emotional data, effectively enhancing the performance and generalization ability of emotion recognition. However, to the best of our knowledge, there is currently a lack of a systematical review that summarizes the work of generative technology for emotion recognition.\n\nIn view of this, this paper aims to provide a comprehensive overview of the research progress in generative technology for emotion recognition, based on more than 320 technical papers up to June 2024. The overall flow is shown in Figure  1 . Specifically, this survey will focus on several representative models, such as Au- toencoder (AE)  [27] , Generative Adversarial Network (GAN)  [28] , Diffusion Model (DM)  [29] , and Large Language Model (LLM)  [30, 31] . AEs enable the modeling of data distribution by learning a compressed representation of the input data and reconstructing the input from the compressed representation. Among them, the Variational Autoencoder (VAE) can generate new samples similar to the input data by introducing hidden variables and variational inference. GANs learn through the adversarial learning of the generator and the discriminator, where the generator tries to generate samples that are as similar as possible to the real data distribution, while the discriminator tries to differentiate between the generated samples and the real samples. The game between the two finally enables the generator to generate high-quality samples. DMs achieve sample generation by learning the process of gradual perturbation of the data distribution and by reversing the process, which has excellent performance in terms of generation quality and diversity. LLMs, such as the Generative Pre-trained Transformer (GPT) series  [32] , learn the statistical patterns and semantic representations of language by pre-training on vast amounts of text data. This pre-training process involves exposing the models to diverse and extensive corpora, allowing them to capture the intricate nuances, contextual dependencies, and latent structures inherent in natural language. In terms of the structure of this survey, we will elucidate the mathematical principles underlying these models to help readers gain a comprehensive understanding of the development and evolution of generative techniques.\n\nThen, this survey will cover datasets commonly used in the field of generative technology for emotion recognition. For emotion recognition based on speech, facial images, and textual information, a variety of datasets will be discussed. Particularly, FER2013  [36]  provides a wide range of facial expressions collected via the Google search engine and labeled for seven emotional states. AFEW  [37] , derived from movies, serves as a dynamic, real-world dataset capturing both auditory and visual expressions. IEMOCAP  [38]  consists of audio-visual recordings of acted emotional states. RAVDESS  [39]  includes both voice and video recordings of actors performing emotional expressions in a controlled environment. Additionally, CMU-MOSEI  [40]  integrates audio, video, and text annotations, making it one of the most versatile datasets available for studying emotion recognition based on multiple modalities. For emotion recognition based on physiological signals, several datasets will be introduced. For example, DEAP  [41]  includes EEG and peripheral physiological signals of participants as they respond to music videos, providing valuable insights into emotional responses. DREAMER  [42]  is another significant dataset that provides both EEG and ECG signals while subjects watch emotional video clips. By exploring these diverse datasets, readers will gain a better understanding of the characteristics and challenges associated with different emotion recognition tasks.\n\nBased on the introduction of the principles of generative models and the datasets used, this review will show the research progress of generative models in the field of emotion recognition from various perspectives, including data augmentation, feature extraction, semi-supervised learning, cross-domain, and so on. The taxonomy is shown in Figure  2 . Regarding data augmentation, it is an important way to improve the generalization performance of the model. However, traditional data augmentation methods, such as rotation and cropping, are difficult to portray the intrinsic attributes of emotional data  [43] . Generative models open a new path for sentiment data augmentation. For example, in SER, researchers  [44, 45]  use GAN to generate samples similar to real speech emotion data, enhancing the model's ability to adapt to different speech conditions. In FER, researchers  [46, 47]  integrate GAN and VAE to generate facial expression images under different pose, light, and occlusion conditions, improving the generalization ability of the model. In TER, researchers  [48, 49]  utilize LLMs to generate text samples with different emotional characteristics, expanding the diversity of training data. Feature extraction here refers to the use of generative models to learn effective feature representations from emotional data  [50] . For instance, AEs and GANs can be used to learn compact representations from speech and facial expressions. These learned features can be used in downstream emotion recognition tasks to improve recognition performance  [51, 52] . Semi-supervised learning  [53, 54]  is a paradigm for jointly training models using a large amount of unlabeled data and a small amount of labeled data, which is important for alleviating the scarcity of labeled data. Generative models provide new technical tools for semi-supervised emotion recognition. For example, some researchers try to apply GANs to semi-supervised learning by synthesizing unlabeled samples using a generator, then predicting pseudo-labels using a discriminator, and finally co-training classifiers with pseudo-labeled samples and real labeled samples  [55, 56] . Cross-domain refers to the phenomenon where the performance of emotion recognition models significantly degrades in different domains  [57, 58] . This is usually caused by the differences in data distribution between the source domain and the target domain. By capturing the commonalities between different domains, generative models can achieve cross-domain emotion recognition and improve the adaptability of models in new domains  [35, 59] .\n\nFinally, this review will present the main findings and provide an outlook on future research directions based on the above systematic analysis. The main findings include: (i) Generative models are most widely used in FER compared to emotion recognition based on other modalities. (ii) Generative models are primarily applied to emotion recognition in the form of data augmentation and feature extraction, although the specific working mechanisms may differ. (iii) AEs and GANs are more widely used than other generative models. Looking ahead, there are still many directions worth exploring, which include: (i) Investigating how to combine DMs and Transformer architecture for emotion recognition. This is expected to be a hotspot for future research, as these have already achieved significant success in a number of domains and may provide new opportunities to improve emotion recognition performance  [60, 61] . (ii) Combining techniques such as reinforcement learning  [62]  and federated learning  [63]  with generative models to better model the complex dependencies and temporal evolution of emotions. This highly promising direction has the potential to capture the intricate dynamics and context-dependent nature of emotional expressions, leading to more accurate and nuanced emotion recognition systems. (iii) Further expanding the combination of generative technology in emotion recognition, particularly in virtual reality (VR) and augmented reality (AR) applications  [64, 65] , to enhance user experiences. (iv) Based on the generative models discussed in this paper, it is possible to directly generate natural, realistic content with rich emotional expressions, such as emotionally charged speech, images, text, and videos  [66, 67] . To sum up, this relatively unexplored area presents a wealth of opportunities for innovation and creative applications of generative models, with the potential to improve how emotions are detected, interpreted, and responded to in immersive environments. Through the above work, this survey aims to deepen the understanding of generative technology in the context of emotion recognition and provide inspiration and guidance for future research. Overall, the main contributions of this survey include:\n\n(1) To the best of our knowledge, this paper is the first work to systematically review the research progress of generative technology in the field of emotion recognition, filling a gap in the existing literature.\n\n(2) By analyzing more than 320 research papers, this paper gives a taxonomy of generative models for emotion recognition from different perspectives, including data augmentation, feature extraction, semi-supervised learning, cross-damain, etc. This in-depth analysis enables readers to gain a thorough understanding of the diverse applications and methodologies of generative technology in emotion recognition.\n\n(3) We summarize the benchmark datasets used according to the different modalities, highlighting key characteristics such as sample size, number of subjects, and emotion categories. Furthermore, we present the performance of different generative models on emotion recognition based on these datasets.\n\n(4) Finally, we discuss some findings of generative models for emotion recognition and further point out potential future research directions.\n\nThe rest of the paper is structured as follows: Section 2 introduces the difference between this review and other existing reviews, Section 3 gives the mathematical principles of different generative models, Section 4 describes the datasets used, Sections 5 -9 present a series of work on SER, FER, TER, emotion recognition based on physiological signals, and MER based on generative models, respectively, Section 10 gives the main findings and future outlook, and Section 11 gives the conclusion. A list of abbreviations is given in Table  1 .\n\n[20] provide a comprehensive overview of TER research, focusing on deep learning approaches, which categorizes TER methods based on different stages of implementation, such as word embeddings, architectures, and training levels. Zhao et al.  [71]  present the history of FER, emotion representation models, well-known datasets, and applications of FER in fields such as transportation, healthcare, and business. Khare et al.  [17]  present emotion models based on different modalities, stimuli used for emotion elicitation, and existing automatic emotion recognition systems. Cîrneanu et al.  [72]  discuss different neural network architectures used in FER, including Multilayer Perceptrons (MLPs), Convolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs), and emphasize the key elements and performance of each architecture. Younis et al.  [73]  provide an overview of the advancements in emotion recognition from the broad perspective of machine learning, but lack a detailed analysis of the progress made using generative models.\n\nThrough the above analysis, it can be seen that existing works conduct separate reviews of generative technology and emotion recognition. However, due to the superior characteristics of generative models, they begin to be widely applied in the field of emotion recognition, such as data augmentation  [74] , feature extraction  [75] , etc. Nonetheless, so far, there is no work that systematically reviews generative models for emotion recognition. This paper aims to fill this gap by comprehensively summarizing the latest progress and applications of generative models in the field of emotion recognition, providing references and insights for future research. Table  2  shows how our work differs from other reviews.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Generative Model",
      "text": "Generative models are a powerful class of machine learning models that aim to learn the underlying probability distribution of a given dataset. By capturing the intricate patterns and structures within the data, these models can generate new samples that closely resemble the original data distribution. The ability to create realistic and diverse examples has led to a wide range of applications across various fields  [76, 77] . Generative models can be achieved in different approaches to model the probability distribution of data. Common generative modeling methods include AEs, GANs, DMs, and LLMs. These methods have their own unique characteristics in modeling complex data distributions and enhancing model generation capabilities  [78] . The following describes each of these models in detail.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Autoencoder (Ae)",
      "text": "AE  [27]  is an unsupervised generative model that learns the compressed representation of data and reconstructs the input data. It consists of an encoder that maps the original input data to a low-dimensional latent space and a decoder that maps the latent representation back to the original data space. Typical variants of AE include Adversarial Autoencoder (AAE) and Variational Autoencoder (VAE). Unlike traditional AE, AAE  [79]  introduces a discriminator network that evaluates the authenticity of reconstructed data by comparing the encoded latent representation with randomly sampled vectors from a prior distribution.\n\nVAE  [80]  is a generative model that combines the ideas of AE and variational inference. It works as follows: First, the input data is passed through an encoder, which maps the input data to the mean and variance parameters in the latent space. Then, a hidden variable is sampled from the latent space, usually using a normal distribution. Next, the sampled hidden variable is passed through a decoder, which maps the hidden variable back to the space of the original input data to generate a reconstructed sample. To constrain the latent representation, VAE uses the Kullback-Leibler (KL) divergence as a regularization term to make the learned latent representation closer to a prior distribution. Therefore, the VAE's loss function consists of a reconstruction error term and a KL divergence term, given by:\n\nwhere E q(z|x) indicates the calculation of the expectation over the posterior distribution q(z|x) of the latent variable z generated by the encoder. log p(x|z) refers to the reconstruction error term, which indicates the log-likelihood of generating data x given the latent variable z. KL(q(z|x)||p(z)) represents the KL divergence term, which is used to evaluate the dissimilarity between the latent variable distribution generated by the encoder q(z|x) and the prior distribution p(z).",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Generative Adversarial Network (Gan)",
      "text": "GAN  [28]  is a generative model consisting of two components: a generator and a discriminator. They learn from each other in an adversarial process, where the generator aims to generate more realistic samples, and the discriminator aims to distinguish between the generated samples and real samples. In addition to the traditional GAN model, there are several variants of GANs, such as Conditional GANs (CGANs)  [81] , Deep Convolutional GANs (DCGANs)  [82] , Wasserstein GANs  [83] , and CycleGANs  [84] .\n\nTo be specific, the training process of GAN involves an iterative optimization procedure. The generator takes a random noise vector as input and generates a sample. The discriminator then takes both real and generated samples as input, and outputs a probability that represents the likelihood of the input being a real sample. The parameters of the generator and discriminator are updated separately to improve their performance. The generator's loss function is designed to maximize the probability of the discriminator classifying the generated samples as real, which can be formulated as:\n\nwhere G represents the generator, D represents the discriminator, z is a random noise vector sampled from a prior distribution p z (z) (e.g., a Gaussian distribution), and E denotes the expected value. On the other hand, the discriminator's loss function is designed to maximize the probability of correctly classifying both real and generated samples, which can be expressed as:\n\nwhere x represents real samples from the training data distribution p data (x). This dynamic equilibrium results in the generator being able to generate samples that resemble real data closely, making GAN capable of generation tasks.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Diffusion Model (Dm)",
      "text": "DMs  [29]  are a class of generative models which have gained significant attention in recent years due to their ability to generate high-quality and diverse samples. Unlike other generative models that directly learn the data distribution, DMs learn to transform a simple noise distribution into the desired data distribution through a gradual diffusion process. The core idea behind DMs is to define a forward diffusion process that gradually adds noise to the data, and then learn a reverse diffusion process that removes the noise step by step, eventually generating clean samples. The diffusion process is typically defined as a Markov chain, where each step corresponds to a small amount of Gaussian noise being added to the data.\n\nThe forward diffusion process can be described by a sequence of latent variables z 0 , . . . , z T , where z 0 represents the clean data and z T represents the fully noised data. The transition from z t to z t+1 is modeled by a Gaussian transition kernel q(z t+1 |z t ), which adds Gaussian noise with a predetermined variance schedule. The reverse diffusion process, also known as the denoising process, is learned by training a neural network to predict the noise that was added in each step of the forward diffusion process. Given a noisy sample z t at time step t, the goal is to learn a function f θ (z t , t) that predicts the noise ϵ t , such that the clean data can be recovered by subtracting the predicted noise from the noisy sample:\n\nwhere β t is a time-dependent noise scaling factor. The training objective of DMs is to minimize the weighted sum of the squared error between the predicted noise and the actual noise added in each step of the forward diffusion process. This is typically achieved using a variant of the variational lower bound, such as the evidence lower bound (ELBO) or the simplified loss proposed in  [29] .\n\nDuring inference, the generation process starts with a sample from the noise distribution z T and iteratively applies the learned denoising function f θ (z t , t) to remove the noise and obtain increasingly cleaner samples. The generated sample at each step is obtained by:\n\nBy repeating this process for a sufficient number of steps, DMs can generate highquality samples that closely resemble the training data.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Large Language Model (Llm)",
      "text": "LLMs are a type of deep learning model based on the principles of generative modeling, aiming to learn the probability distribution of words in text data to generate coherent and natural text sequences  [30, 31] . The basic principle involves pre-training on large-scale text data to capture semantic relationships and syntactic structures between words, followed by fine-tuning on specific tasks to adapt to particular application scenarios  [30] . They typically employ neural network architectures like the Transformer  [85]  model. During pre-training, these models utilize unsupervised learning on unlabeled text data to learn rich language representations  [86] . In the fine-tuning stage, LLMs improve their performance on specific tasks through supervised learning  [87] . LLMs, such as T5 (Text-to-Text Transfer Transformer)  [87] , XLnet  [88] , LLaMA  [89] , and GPT (Generative Pre-trained Transformer)  [32] , demonstrate significant potential in various natural language processing tasks, leading to advancements in text generation  [90] .\n\nThrough the above discussion, we explore the mathematical principles behind several representative generative models, including AE, GAN, DM, and LLM. These models exhibit unique characteristics in learning complex data distributions and generating realistic samples, providing powerful tools for applications in various domains, including emotion recognition  [69, 77] .",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Databases",
      "text": "This section focuses on the datasets commonly used in generative technology for emotion recognition. These datasets are crucial in this field because the performance of related algorithms largely depends on the quality and richness of the data. Table  3  lists these datasets, which capture and represent emotional information in various ways, enabling researchers to develop and evaluate generative models that can effectively understand and generate emotional content. Specifically, as shown in Table  3 , these datasets can be divided into two main categories: spontaneous datasets and in-the-wild datasets  [91] . Spontaneous datasets include expressions simulated by participants, where emotions are expressed naturally despite participants' awareness that they are being monitored. The acquisition environment for these datasets is typically a controlled laboratory setting. On the other hand, inthe-wild datasets capture emotions in real-world scenarios without the need for controlled or processed collection, and participants are filmed in their natural environments. In addition, \"Modalities\" in Table  3  refers to the modalities contained in the dataset, such as visual or physiological data. \"Samples\" indicates the type and number of samples in the dataset. \"Subjects\" refers to the number of individuals included in the dataset. \"Categories\" shows the categories for emotion recognition, indicating the range of emotions that the dataset covers. A slash indicates that the specific information is not explicitly mentioned in the dataset description. For example, the IAPS dataset  [92]  contains data in the form of visual modality, including over 9,000 images, and the emotion category information includes valence, arousal, and dominance. In the following sections, we will show the performance of different generative models for emotion recognition based on the datasets listed in Table  3 . SER is a crucial branch of emotion recognition that aims to identify and understand the emotional state of a speaker by analyzing the features in speech signals  [18] . Speech signals contain rich emotional information, such as intonation, volume, and speaking rate, which can reflect the speaker's emotional changes  [135] . In recent years, generative models have contributed to the development of SER in multiple aspects. In the following sections, we will delve into the specific applications of generative models in SER. Section 5.1 will focus on discussing speech data augmentation based on generative models. Section 5.2 will explore the application of generative models for speech feature extraction. Section 5.3 will describe the application of generative models in conjunction with semi-supervised learning, Section 5.4 will present cross-domain SER based on generative models, and Section 5.5 will provide an introduction to the use of generative models for adversarial sample generation and defense.",
      "page_start": 11,
      "page_end": 12
    },
    {
      "section_name": "Speech Data Augmentation",
      "text": "Data augmentation is a crucial topic in emotion recognition, as it addresses the problem of limited training data  [43, 136] . In many real-world scenarios, acquiring and annotating large amounts of emotional data can be time-consuming, expensive, and labor-intensive. Moreover, emotional data may also suffer from the problem of class imbalance, where the number of labeled samples for certain emotional categories is relatively small, resulting in poor performance of the model on these categories  [137] . The scarcity of labeled data described above can hinder the development of robust and generalizable emotion recognition models. Data augmentation techniques aim to mitigate this issue by artificially increasing the size and diversity of the training dataset, thereby improving the model's ability to learn and generalize from the available data. Traditionally, researchers have proposed various strategies for data augmentation in SER  [138] . These strategies include simple techniques such as noise addition, pitch shifting, time stretching, and speed perturbation. These methods aim to introduce variations in the speech signal while preserving the emotional content. However, these traditional approaches often rely on handcrafted rules and may not capture the complex and nuanced patterns of emotional speech.\n\nIn recent years, generative models have emerged as a promising approach for data augmentation in SER  [44, 139] . By leveraging the power of generative models, researchers can create realistic and diverse emotional speech samples, effectively expanding the training dataset. Figure  3  illustrates a general architecture. Table  4  presents different generative methods used for data augmentation in SER and their performance, which is typically evaluated using metrics such as Accuracy (ACC) and Unweighted Average Recall (UAR), where ACC provides an overall measure of the model's correctness, indicating its general performance, while UAR, calculates the average recall score across all classes, making it particularly useful when dealing with imbalanced datasets.\n\nTo be specific, generative models can effectively augment speech data by directly generating synthetic samples. For example, Chatziagapi et al.  [44]  adapt and improve the Balancing GAN architecture to generate realistic emotional data for minority classes. Heracleous et al.  [139]  propose a data augmentation framework that utilizes CycleGAN to transfer natural speech to emotional speech and employs bringing more variations to the training data through deep learning  [45, 46] . Our proposed multimodal conditional GAN is a generalization of existing GANs for data augmentation to improve the performance of audio-visual emotion recognition. Figure  1  shows the architecture of our data augmentation approach. We first use our multimodal conditional GAN to generate new data, then augment the training set with the generated data, and finally train a DNN classifier with the augmented training set for the emotion classification task.  To be specific, our multimodal conditional GAN builds generators and discriminators for audio and visual modalities. Additional category information is used as their shared input to generate fake data of different categories. It is shown in  [17, [47] [48] [49]  that in the real multimodal data, the audio modality and the visual modality are highly dependent, which is beneficial to emotion recognition. Inspired by this, we measure the correlation between the audio modality and the visual modality in the generated multimodal data based on Hirschfeld-Gebelein-Rényi (HGR) maximal correlation  [50] [51] [52] , which makes the generated multimodal data close enough to the real multimodal data to help improve the performance of audio-visual emotion recognition. In such a way, our multimodal conditional GAN can (1) effectively generate the multimodal data with audio and visual modalities, and (2) fully consider the correlation between the audio modality and the visual modality in the generated multimodal data. Then, the generated multimodal data are used to augment our training set. Finally, a DNN model is used as the classifier to perform the emotion classification task with the augmented training set. Three public multimodal datasets, including eNTERFACE'05  [53] , RAVDESS  [54] , and CMEW  [55] [56] [57] , are used to conduct a series of experiments. The experiment results show that our multimodal conditional GAN can significantly enhance the performance of audio-visual emotion recognition.\n\nTo summarize, the main contributions of this paper lie in the following aspects:\n\n• We design an efficient multimodal conditional GAN to augment the multimodal data with audio and visual modalities for emotion recognition.",
      "page_start": 17,
      "page_end": 17
    },
    {
      "section_name": "•",
      "text": "We model the correlation between the audio modality and the visual modality in the generated multimodal data to approximate the real multimodal data.",
      "page_start": 18,
      "page_end": 18
    },
    {
      "section_name": "•",
      "text": "We conduct experiments on three public multimodal datasets to show that our multimodal conditional GAN can be effectively used for data augmentation of audio-visual emotion recognition. a Vision Transformer (ViT)  [141]  as the classifier for SER. Wang et al.  [142]  proposes a GAN-based augmentation strategy with triplet loss to increase and stabilize the augmentation performance for the SER task.\n\nIn addition to directly performing augmentation in the data space for SER, some researchers attempt to conduct data augmentation in the feature space. This approach aims to manipulate and transform the features extracted from the raw speech data, generating new feature representations to increase the diversity and robustness of the training data. For example, in  [45] , a CycleGAN model is employed for data augmentation, and two feature selection networks, Fisher and Linear Discriminant Analysis, are used to enhance SER performance. Yi and Mak  [143]  combine GANs and AEs to design an adversarial data augmentation network to generate emotional feature vectors that share a common latent representation. Latif et al.  [144]  utilize a data augmentation technique called mixup  [145]  to enhance GAN's ability in representation learning and synthesis of feature vectors. In another study, Sahu et al.  [146]  employ GANs to learn the distribution of low-dimensional representations of high-dimensional feature vectors and CGAN to learn the distribution of high-dimensional feature vectors conditioned on labels.\n\nParticularly, it is worth noting that apart from directly performing data augmentation on the speech modality, another approach is to transfer information from other modalities to the speech modality for data augmentation, thereby assisting in improving the performance of SER. This cross-modality information transfer-based data augmentation method has received increasing attention from researchers in recent years. For instance, He et al.  [147]  employ GAN and VAE to convert facial images into spectrograms to increase the number of sample data, thereby enhancing SER. Ma et al.  [148]  use a LLM, GPT-4, to generate emotional text, then use Azure emotional TTS to synthesize emotional speech. Malik et al.  [149]  utilize an improved DM model to generate synthesized emotional data. They use text embeddings represented by bidirectional encoders from transformers (BERT)  [150]  to adjust the DM model to generate high-quality synthesized emotional samples in the voices of different speakers.",
      "page_start": 19,
      "page_end": 19
    },
    {
      "section_name": "Speech Feature Extraction",
      "text": "Feature extraction plays a vital role in SER, as it involves deriving meaningful representations from speech signals to capture emotional cues effectively. Traditionally, handcrafted features such as Mel-Frequency Cepstral Coefficients (MFCCs)  [151]  and prosodic features have been extensively utilized in SER systems. However, these handcrafted features have limitations in capturing subtle emotional variations and can be sensitive to noise and speaker variability  [152] . To address these lim- above-mentioned works do not consider the significance of a priori knowledge. To address this problem, our method does not rely on basic autoencoder architecture. In this work, we increase the modeling capacity by designing a new autoencoder architecture, emotion-embedded autoencoder. Emotion embedding layers in our method lead the model to efficiently learn a priori emotion information from the label, which allows the autoencoder to focus more on deep emotion features during the reconstruction process. Experimental results demonstrate that the proposed method can present performance improvement.",
      "page_start": 18,
      "page_end": 18
    },
    {
      "section_name": "Iii. Proposed Method",
      "text": "In this section, we describe our proposed method. There are three parts: input speech feature, autoencoder with emotion embedding and emotion classification network. Fig.  1  depicts the model framework, which includes an autoencoder, an emotion embedding path, and an emotion classification  the detailed spectr work  [36] .\n\n2) IS10 FEATURE S itations, generative models, have emerged as a promising approach to learn and extract expressive representations directly from the raw speech signals  [153] . Table  5  provides an overview of the literature on feature extraction and other applications in SER. For example, Latif et al.  [51]  employ VAEs to learn latent representations of speech emotion and utilize Long Short-Term Memory (LSTM) networks for classifying phonological emotions. Zhang and Xue  [154]  propose a hybrid approach that combines the potential representations learned by an AE with acoustic features obtained using the openSMILE toolkit, as shown in Figure  4 . The concatenated feature vectors are then used for emotion classification. Sahu et al.  [155]  leverage an Adversarial AE (AAE) framework to learn low-dimensional code vectors that retain class discriminability from the higher-dimensional feature space for SER. Ying et al.  [156]  employ Denoising AEs (DAEs) and AAEs in conjunction with unsupervised learning to extract features for model pre-training. Almotlak et al.  [157]  extend the VAE framework to disentangle speaker-dependent information, such as identity and gender, from speaker-independent features like emotions, providing a more nuanced representation of speech data. Fonnegra and Díaz  [158]  use paralinguistic features and a deep convolutional Stacked AE (SAE) network to transform a set of 1582 IN-TERSPEECH 2010 features  [159]  extracted from speech signals into a higher-level representation for SER. Sahu et al.  [160]  use two methods to generate emotionspecific feature vectors with GAN variants: training a generator using samples from a mixture prior and providing a one-hot emotion vector to the generator to explicitly generate features for the specified emotion.  in  [39] . The generator G generates fake examples from noise samples z drawn from a prior distribution p(z), and the discriminator D is a classifier. However, the predictions of D comprise K emotion states and an additional pseudoclass (or false) rather than the binary classes (i.e., true or false). Therefore, the SSGAN not only captures the distribution of the input but also models the discriminated information to perform emotion recognition, simultaneously.\n\nMathematically, we assume that p model (y = K + 1|x) denotes the probability of the input belonging to the fake category, which corresponds to the probability 1 -D(x) in the traditional GAN. Since the minibatch consists of half of the training data and half of the generated data in  [39] , the loss function of the discriminator D can be expressed as:\n\nIn Eq. (  1 ), the first term is the expectation of the input under the class labels, which can be applied to perform emotion recognition. In the training phase, the parameters of the discriminator D are updated to minimize Eq. (  1 ), which is maximized by updating the parameters of the generator G.\n\nRecently, a more advanced SSGAN was proposed in  [25]  that not only exploits a small amount of real labeled data   [55] : A generator creates synthetic audio descriptors from noise. These descriptors, along with real ones from the openSMILE toolkit, are fed to a discriminator. The discriminator is trained with supervised and unsupervised loss functions to better distinguish real from fake audio cues.",
      "page_start": 19,
      "page_end": 22
    },
    {
      "section_name": "Semi-Supervised Learning",
      "text": "Unlabeled data refers to samples that lack corresponding labels, which may be due to the failure to obtain appropriate labels during the data collection process or the inability to cover the entire dataset due to the high cost of manual labeling. In the field of emotion recognition, the presence of unlabeled data can hinder the performance of models in emotion classification tasks  [170] . To address this issue, semi-supervised learning emerges as a promising approach that leverages both labeled and unlabeled data to improve the generalization and performance of emotion recognition models  [55, 162] . Common semi-supervised learning methods include strategies based on label propagation, self-training, and generative models  [53] . Among these approaches, generative models have proven to be particularly effective.\n\nTo be specific, Zhao et al.  [55]  propose a semi-supervised GAN for SER, which is designed to capture underlying knowledge from both labeled and unlabeled data, as shown in Figure  5 . In their approach, a generator creates synthetic audio descriptors from noise, while a discriminator is trained to distinguish between real and fake audio cues using both supervised and unsupervised loss functions. The discriminator not only classifies input samples as real or fake but also learns to identify the emotional class of real samples. Chang and Scherer  [161]  present a multitask deep convolutional GAN (DCGAN) approach for emotional valence classification in speech, which leverages a multi-task annotated corpus and a large unlabeled conference corpus. In addition to GANs, AEs have also been widely used by researchers for semi-supervised learning in SER. For example, Deng et al.  [162]  propose a semi-supervised AE model for SER that combines both generative and discriminative perspectives, enabling the model to distill knowledge from unlabeled data and incorporate it into the supervised learning process, thus enhancing the overall performance of the SER system. Xiao et al.  [163]  propose a novel semi-supervised adversarial VAE that learns the distribution of shared hidden features of labeled and unlabeled data. Neumann and Vu  [164]  employ a recurrent sequence-to-sequence AE trained on unlabeled data to generate representations for the labeled data. These generated representations  1 3  tures. Also, as mentioned in Sect. 2, m and n are the numbers of sources ( X s ) and target ( X t ) samples, respectively. The reconstruction loss of each AE is computed as Eq. 5.\n\nFig.  2  The proposed method for domain adaptation using auto-encoders Figure  6 : A framework  [35]  which leverages AEs to address the cross-domain problem in SER:\n\nThe key idea is to utilize the latent representations learned by the AE to align and compare the features of the source and target speech domains.\n\nserve as auxiliary information to the classifier during training, helping to improve the emotion recognition performance. Latif et al.  [34]  design an unsupervised AAE and combine it with a supervised classification network to achieve semi-supervised learning. Zhou et al.  [165]  extend the multi-path deep neural network to a generative model based on semi-supervised VAE, which enables simultaneous training on both labeled and unlabeled data.",
      "page_start": 22,
      "page_end": 23
    },
    {
      "section_name": "Cross-Domain Ser",
      "text": "Here, we discuss cross-domain SER, which refers to the ability of a model trained on one speech emotion domain (source domain) to be extended and applied to another different speech emotion domain (target domain). This cross-domain generalization is a major challenge faced by SER because different speech emotion domains may have significant differences, such as language, culture, speaking style, recording conditions, and so on  [57, 58] . These differences form distribution inconsistencies between different domains, so that the performance of a model trained on one domain often decreases significantly on another domain. Generative models provide a new perspective for solving this problem. By introducing domain adaptation techniques, generative models can learn to map samples from the source dataset and target dataset to a shared feature space, thereby eliminating the distribution differences between them.\n\nFor example, Nasersharif et al.  [35]  employ separate AEs for each source and target domain dataset and introduce a domain adaptation loss in addition to the conventional loss to achieve domain-invariant feature extraction, as shown in Figure  6 . Xiao et al.  [59]  design a framework by incorporating a VAE to extract generalized and domain-invariant representations from latent feature distributions. Das et al.  [166]  propose a VAE-based method with KL annealing and semi-supervised learning to achieve more consistent latent embedding distributions across datasets. Latif et al.  [167]  introduce an Adversarial Dual Discriminator (ADDi) network, which generates domain-invariant representations through a three-player adversarial game between a generator and two discriminators. Su et al.  [168]  introduce corpus-aware emotional CycleGAN, which incorporates a corpus-aware attention mechanism to aggregate each source dataset and generate synthetic target samples. Su and Lee  [169]  propose a conditional cycle GAN to generate samples similar to the source domain and assign the original labels of the source samples.",
      "page_start": 23,
      "page_end": 24
    },
    {
      "section_name": "Adversarial Sample Generation And Defense",
      "text": "Adversarial examples refer to samples that are created by adding carefully designed perturbations to the original samples, causing the model to misclassify them  [171] . In the SER task, adversarial examples can be speech with specific noise or perturbations added, which can deceive the SER model and lead to incorrect emotion predictions  [172] . The existence of adversarial examples highlights the vulnerability of SER models, as these models may overly rely on certain specific acoustic features while ignoring the overall features of emotional expression. Adversarial example defense refers to improving the ability of SER models to resist adversarial attacks, enabling them to maintain high emotion recognition accuracy even when faced with adversarial examples.\n\nGenerative models can play an important role in the generation and defense of adversarial examples in SER. On the one hand, generative models can be used to generate realistic adversarial examples. For example, Chang et al.  [173]  introduce STAANet, a novel method based on WaveNet  [174]  for generating sparse and transferable adversarial examples to spoof SER systems. Wang et al.  [175]  propose a GAN-based attack approach that can efficiently generate adversarial examples with pre-specified target labels. On the other hand, generative models can also be used to construct adversarial example defense systems. For example, Latif et al.  [176]  introduce a two-step defense strategy for SER systems. In their approach, perturbed utterances are first cleaned using GANs, and then a classifier is applied to the cleaned data. By removing the adversarial perturbations from the input samples, the classifier can make more accurate predictions and maintain robust performance. Chang et al.  [177]  propose a framework based on federated adversarial learning to protect data privacy and defend against adversarial attacks in SER systems. Their approach leverages the distributed nature of federated learning to train models on decentralized data while incorporating adversarial training techniques to enhance robustness against adversarial examples. In summary, generative models have made significant contributions to SER by enabling data augmentation, feature extraction, semi-supervised learning, and adversarial sample generation and defense. These applications have greatly enhanced the performance of SER systems. As research in this area continues to advance, generative models are expected to play an increasingly crucial role in pushing the boundaries of SER.",
      "page_start": 24,
      "page_end": 25
    },
    {
      "section_name": "Facial Emotion Recognition (Fer)",
      "text": "FER is a research field that utilizes computer vision techniques to recognize and understand the emotional states of individuals by analyzing facial expressions in facial images or video frames  [19, 178] . In recent years, generative models have made impressive progress in the field of FER, which is similar to SER, but presents some unique research focuses and development directions. This section will provide a detailed discussion on the applications of generative models in FER. Firstly, Section 6.1 will discuss facial data augmentation. Secondly, Section 6.2 will focus on facial Where λgp is a trade-off parameter, I real indicates the source image, p img is the distribution of real images and p img represents random interpolation distribution. The generator G generates high realism images to deceive discriminator D as much as possible. Also, the discriminator D distinguishes the generated image from the real image as much as possible.\n\nReconstruction loss. Considering no ground-truth supervision for the im-cGAN to simultaneously change the desired AUs and keep identity information, we construct a novel loss function to penalize the difference between the source image I real and its reconstructed image I real . Also, motivated by the method  [28]  that can remit the problem of images structure deformation and blurring caused by L 1 -norm, we therefore design the novel reconstruction loss as follows:\n\nWhere ξ represents a trade-off parameter and the calculation of SSIM ( •) is measured the similarity of two images.\n\nClassification loss. Due to the images generated by the im-cGAN should not only be realistic but contain the desired AUs, we define the following AUs classification loss to optimize both G and D in Eq. (  5 ) :\n\n(5)\n\nWhere the AUs regression loss of source image I real uses to optimize D and the G produces fake image G(I real |v tar ) minimizing the AUs regression loss.\n\nAttention loss. The attention masks tend to converge to the maximum value during training, thus losing the role of regulating weights. To address this problem, we further regularize the attention mask A and propose the attention loss as follows:\n\nWhere λtv is trade-off parameter and A i,j indicates the i th row and j th column element of matrix A .\n\nTotal loss. Combining the adversarial loss, reconstruction loss, classification loss and attention loss, the total loss function is finally constructed as follows:\n\nwhere λ class , λatt and λrec are trade-off parameters that balance each loss function.",
      "page_start": 24,
      "page_end": 25
    },
    {
      "section_name": "Discriminative Features Extraction Based On D-Loss Function",
      "text": "Although the proposed im-cGAN succeeded in enlarging expression samples, it is worth considering how to promote better classification results from the generated images and regional patches. Given this, we further introduce the global-region network (GRN) based on D-loss function to extract discriminative features. The detailed framework is illustrated in Fig.  2  , which consists of four parts: shallow convolution module, global-based module, regionbased module and D-loss. We will introduce the above modules detailly in the following.\n\nSince facial expressions are normally associated with the movements of the overall face and several critical regions, we first take the cropped critical patches (eyes, nose, eyebrows, mouth and chin) that capture the local dynamic variations as input to the region-based module. Subsequently, we employ the global-based module that considers the overall facial information by taking facial images as input to extract global information. Finally, we integrate the features above to obtain the fused feature representation. Consequently, we propose to obtain the fused features by a simple weighted sum of local representation and global information as:\n\nWhere P patch (i) and P global (i) denote the feature outputs from the region-based module and global-based module, respectively. τ is a trade-off parameter and we empirically set it to 0.5.\n\nGiven that the inter-class distance in island loss  [20]  (see Eq. (  9 ) ) is not particularly discriminant when the cluster centers are close to the origin, we design the D-loss function that adds a constrained term based on L2-norm to enlarge the inter-expression distance. Specifically, the D-loss function is written as Eq. (  10 ) and the optimization procedure of D-loss function are introduced in 3 Figure  7 : A facial data generation framework based on GANs from  [180] : The original image undergoes facial landmarks detection using Dlib, after which the im-cGAN generates augmented images and regional patches. These patches are then cropped into specific regions. In the second step, discriminative features are extracted. The cropped regional patches are fed into a globalregion network, which extracts discriminative features based on the D-loss function.\n\nfeature extraction. Next, Section 6.4 will introduce semi-supervised learning for FER. Furthermore, Section 6.5 will focus on FER in typical noisy environments. Finally, Section 6.3 will discuss dynamic FER.",
      "page_start": 26,
      "page_end": 26
    },
    {
      "section_name": "Facial Data Augmentation",
      "text": "Similar to speech data, acquiring high-quality facial image data is an expensive and time-consuming task, which hinders the development of FER  [95, 179] . With generative models, it becomes possible to generate synthetic facial images that align with the desired emotion labels, thus providing a data augmentation method to overcome the scarcity of labeled data. Table  6  shows the literature on generative models for data augmentation in FER.\n\nSpecifically, a series of GAN-based methods are proposed. Porcu et al.  [46]  reveal that GAN-based data augmentation substantially outperforms geometric transformations in improving emotion recognition accuracy of a VGG16 CNN-based FER system  [181] , highlighting the significance of large training datasets and the potential of combining GAN with less complex techniques to mitigate computational complexity. Zhu et al.  [182]  propose a data augmentation method using a framework combining a CNN classifier and a CycleGAN generator with least-squared adversarial loss to complement and complete the data manifold, find better margins between neighboring classes, and avoid the gradient vanishing problem. As shown in Figure  7 , Sun et al.  [180]  propose an improved CGAN (im-cGAN) model trained on facial images with Action Units (AUs) to generate more labeled samples for data augmentation. Similarly, Wang et al.  [183]  introduce an end-to-end synthetic Compositional GAN (Comp-GAN) that employs six task-driven loss functions to generate more realistic and natural facial images. Kusunose et al.  [184]  propose a data augmentation method using StyleGAN2  [185]  to generate artificial facial expression images for seven emotions, which are used as additional training data to train a VGG16-based emotion recognition model through transfer learning. Yang et al.  [186]  introduce an end-to-end deep learning framework for generating high-quality facial images. The framework is built upon DCGAN and incorporates an Ensemble Networks (Ens-Net) model for network integration. Han et al.  [187]  improve upon the image conversion model StarGAN V2  [188]  by integrating the   [191]  2020 starGAN KDEF/MMI ACC: 95.97/98.30 Li et al.  [47]  2023 GAN + VAE RAF-DB ACC: 74.43 Pons et al.  [192]  2020 CycleGAN USTC-NVIE ACC: 51\n\nSqueeze-and-Excitation Network (SENet)  [189]  into the generator. They also introduce hinge losses  [190]  to enhance the authenticity of the generated images. These modifications aim to produce more realistic and diverse facial expressions across multiple domains. Wang et al.  [191]  propose an improvement to the StarGAN model for data augmentation in FER by introducing context loss and an attention mechanism. These modifications enable the model to generate more realistic and expressive facial images with fine details in crucial regions, such as eyes and mouth.\n\nIn addition to GAN-based methods, there are also some approaches that combine AEs for data augmentation in FER. For example, Li et al.  [47]  introduce a novel approach that integrates GAN and VAE in the latent space for data augmentation in FER. The proposed method aims to generate diverse facial images that carry rich semantic information, enhancing the quality and variety of the augmented dataset. Pons et al.  [192]  propose using CycleGAN to generate thermal facial images from visual spectrum images, thereby augmenting the training dataset for thermal emotion recognition. This can be seen as a cross-modality data augmentation approach.",
      "page_start": 25,
      "page_end": 26
    },
    {
      "section_name": "Facial Feature Extraction",
      "text": "Traditional FER methods often rely on handcrafted features or predefined facial landmarks to describe facial expressions  [19, 178] . These features typically include the locations of facial key points, geometric relationships, and local texture descriptors  [193] . However, due to the complexity and diversity of facial expressions, these handcrafted features may not adequately capture the subtle differences between different expressions  [194] . Moreover, the variations in facial features among different individuals also pose challenges to feature design  [195, 196] . In recent years, with the development of deep learning, generative models provide new ideas for solving the feature extraction problem in FER by automatically learning compact and informative representations of facial expressions. Table  7  shows a series of research works, where CCC (Concordance Correlation Coefficient) assesses the agreement between predicted and actual values, taking into account both the correlation and the deviation from perfect agreement.\n\nFor example, Yang et al.  [52]  propose the Exchange-GAN model, which achieves the separation of expression-related and expression-irrelevant features through partial feature exchange and various constraints, such as adversarial loss, classification loss, content loss, and central loss. As illustrated in Figure  8 , this approach effectively disentangles the facial expression information from identity-related features. Khemakhem and Ltifi  [197]  introduce a neural style transfer GAN (NST-GAN) to remove identity information from facial images. By converting each image into a synthetic \"average\" identity, NST-GAN focuses on extracting expression-specific features while minimizing the influence of individual variations. Analogously, Yang et al.  [198]  propose a de-expression residue learning method by training a CGAN to generate neutral face images from input expressions, and then learn the expressive information residue from the intermediate layers of the generative model. Zhang and Tang  [199]  use a GAN to generate a neutral face from an emotional face, extract features from both faces using separate convolutional layers, and then subtract the neutral features from the emotional features to obtain pure \"emotion features\" for FER. Xie et al.  [200]  propose the two-branch divergent GAN, which consists of two generators and discriminators. One branch is dedicated to facial expression feature extraction and classification, while the other focuses on facial feature extraction and identity discrimination. This architecture allows for a more targeted approach to capturing expression-related information. Ali and Hughes  [201]  propose a novel disentangled expression learning GAN that decouples expression representation from identity components. By explicitly providing identity codes to the decoder, this approach enables the model to learn expression features independently of subjectspecific characteristics. Similarly, Tiwary et al.  [202]  design an expression GAN to separate shape, skin color, and other identity-related information from specific muscle movements that are crucial for emotion recognition. Sima et al.  [203]  tackle the challenges of category overlap and facial expression variation due to individual differences using CGAN. By generating neutral expressions while retaining identityrelated information, their method aims to normalize the facial images and facilitate more accurate expression recognition. Similar GAN-based approaches for addressing facial feature extraction include  [204, 205, 206, 207] .\n\nIn addition to the aforementioned methods based on GANs, several AE-based approaches are proposed to address the feature extraction problem in FER. For example, Kim et al.  [208]  design an AE network with CNNs to learn a contrastive representation of facial expressions by comparing a query image with a generative reference image, estimated from the given image itself. Wu et al.  [209]  propose Cross-VAE, an extension of conditional VAE that disentangles expression from identity by assuming orthogonal latent representations and employing a symmetric training procedure, ensuring disentangled and expressive encodings. Chatterjee et al.  [210]  propose a residual VAE model to significantly reduce class overlapping in the feature space by transforming facial images into latent vector. They  [211]  also apply various resampling techniques after feature extraction to solve the class imbalance problem. Zhou et al.  [212]  use Masked Autoencoders (MAEs) for model pre-training and improve the ability to extract facial features through masked-reconstruction method, as shown in Figure  8 .",
      "page_start": 26,
      "page_end": 26
    },
    {
      "section_name": "Cross-Domain Fer",
      "text": "Similar to cross-dataset SER, generative models can also provide effective solutions for cross-dataset FER through domain adaptation. For instance, Wang et al.  [213]  propose an unsupervised domain adaptation method based on GANs. Their approach generates new samples to fine-tune the pre-trained FER model, enabling it to better adapt to the target domain and achieve higher accuracy across different datasets. Fan et al.  [214]  employ CGANs to learn domain-invariant feature representations of facial expressions, allowing for effective knowledge transfer from The Transformer encoder only models the context within a single segment, thereby ignoring the dependencies between frames across segments. To account for the context of different frames, overlapping between consecutive segments can be employed, thus enabling the capture of the dependencies between frames across segments, which means s ≤ w.\n\nWe use another type of neural network that can learn the relationships and interactions among the features within each segment. The transformer encoder takes the input feature vector and applies a series of self-attention layers and feed-forward layers to produce an output feature vector. The output feature vector has more semantic meaning and representation power than the input feature vector. For example, the transformer encoder can learn how different parts of the image relate to each other in each segment of the video. However, the transformer encoder does not consider how different segments of the video are connected or influenced by each other. To solve this problem, we can make some segments overlap with each other so that some frames are shared by two or more segments. This way, we can capture some information about how different segments affect each other. The degree of overlap is controlled by two parameters: s is the length of a segment and w is the sliding window size. If s is smaller than or equal to w, then there will be some overlap between consecutive segments.",
      "page_start": 27,
      "page_end": 28
    },
    {
      "section_name": "Prediction",
      "text": "After the temporal encoder, the features h i are finally fed into MLP for regression, which can be formulated as follows:\n\nwhere y i are the predictions of i-th segment. For VA challenge, y i ∈ R l×2 . For Expr challenge, y i ∈ R l×8 . For AU challenge, y i ∈ R l×12 .\n\nThe prediction vector contains the values we want to estimate for each segment. The MLP consists of several layers of neurons that can learn non-linear transformations of the input. The MLP can be trained to minimize the error between the prediction vector and the ground truth vector. The ground truth vector is the values we want to predict for each segment. Depending on what kind of challenge we are solving, we have different types of ground truth vectors and prediction vectors. For the VA challenge, we want to predict two values: valence and arousal. Valence measures how positive or negative an emotion is. Arousal measures how active or passive an emotion is. For the Expr challenge, we want to predict eight values: one for each basic expression (anger, disgust, fear, happiness, sadness, and surprise) plus neutral and other expressions. For the AU challenge, we want to predict twelve values: one for each action unit (AU1, AU2, AU4, AU6, AU7, AU10, AU12, AU15, AU23, AU24, AU25, AU26).",
      "page_start": 29,
      "page_end": 29
    },
    {
      "section_name": "Loss Functions",
      "text": "VA challenge: We use the Concordance Correlation Coefficient (CCC) between the predictions and the ground truth labels as the measure, which is defined as in Eq 1. It measures the correlation between two sequences x and y and ranges between -1 and 1, where -1 means perfect anticorrelation, 0 means no correlation, and 1 means perfect correlation. The loss is calculated as Eq 2.     [199]  2021 GAN CK+ ACC: 98.04 Xie et al.  [200]  2021 GAN CK+/TFEID ACC: 97.53/97.20 Ali and Hughes  [201]  2019 GAN CK+/Oulu-CASIA /MMI ACC: 97.28/89.17/72.97 Tiwary et al.  [202]  2023 labeled source domains to unlabeled target domains. Zhang et al.  [215]  propose a domain adaptation model for FER, which utilizes unlabeled web facial images as an auxiliary domain to generate labeled facial images in the target domain using GANs with an attention transfer module, preserving structural consistency through pixel cycle-consistency and discriminative loss, and leveraging attention maps from the source domain classifier to enhance the target domain classifier's performance.",
      "page_start": 30,
      "page_end": 30
    },
    {
      "section_name": "Fer In Noisy Environments",
      "text": "In real-world applications, facial images are often affected by various noisy environments, such as occlusions, complex backgrounds, non-frontal views, and other factors that reduce the performance of FER  [178] . In recent years, generative models demonstrate powerful capabilities in effectively addressing these noisy environments. Table  8  presents the literature on generative models for FER in noisy environments.\n\nFor example, to cope with occlusion and complex backgrounds, Du et al.  [216]  introduce an expression associative network that learns the associations between interand intra-class expressions, along with a GAN-based auxiliary module designed to suppress changes caused by occlusion, illumination, and pose variations. Peng et al.  [217]  propose a method based on ACGAN  [218]  to restore occluded facial images, thereby enhancing the data manifold and improving FER performance. Similarly, Lu et al.  [219]  employ a Wasserstein GAN-based method to generate unoccluded facial images, enabling effective FER even in the presence of partial occlusion. To mitigate the influence of background clutter on FER, Tang et al.  [220]  introduce an expression CGAN that incorporates a novel mask loss. This loss function helps to reduce the impact of background information, allowing the model to focus more on the facial regions relevant to expression recognition. Building upon the idea of masking, Li et al.  [221]  propose a mask generation network (MGN) inspired by GANs. The MGN effectively filters out background noise and interference from facial images by generating masks that cover only the key areas of each face required for accurate expression classification. This targeted masking approach preserves the most informative regions while suppressing irrelevant information.\n\nApart from occlusion and complex backgrounds, the presence of non-frontal facial images in wild datasets poses another significant challenge for FER due to variations in facial posture. To address this issue, researchers have proposed different solutions based on generative models, aiming to generate high-quality frontal-face images from non-frontal views. For example, Han and He  [222]  introduce a GANbased three-stage training algorithm for multi-view FER. They utilize a pre-trained classification model to construct a new GAN that effectively synthesizes frontal faces while preserving facial expression features. This approach ensures that the generated frontal images retain the emotional information necessary for accurate FER. Instead of explicitly generating frontal faces, Zhang et al.  [223]  propose a GANbased method that leverages the geometric information inherent in facial shapes to achieve pose-invariant FER. This innovative perspective offers an alternative solution to the challenge of non-frontal images in wild datasets. Similar GAN-based methods for non-frontal FER also include  [224, 225, 226] .\n\nMoreover, generative models can be employed to combat other noise factors that degrade the performance of FER. For example, Yang et al.  [227]  propose a GAN-based method for recognizing expressions in low-intensity facial images. The concatenated training critic combines both the general adversarial loss, which regulates the intensity of facial expressions, and the FER penalty, which directs the model to generate visually enhanced facial images. Nan et al.  [228]  propose a novel GAN-based feature-level super-resolution method for robust FER that reduces the risk of privacy leakage. This approach transforms low-resolution image features into more discriminative ones using a pre-trained FER model as a feature extractor. To address the issue of image degradation caused by under-display cameras (UDC), Wang et al.  [229]  employ a method based on DM to overcome inherent noise and distortion obstacles, thereby improving the accuracy of expression recognition, as shown in Figure  9 .",
      "page_start": 28,
      "page_end": 28
    },
    {
      "section_name": "Dynamic Fer",
      "text": "The previously discussed FER mainly focuses on static images, i.e., singleframe facial expression recognition. However, in real-world scenarios, emotions often change dynamically, requiring consideration of the temporal evolution of facial expressions. Dynamic FER aims to analyze the changes in facial expressions across video sequences, capturing the temporal dynamic features of emotions  [230] . Compared to static FER, dynamic FER can provide richer and more accurate emotional information, but it also presents greater challenges due to the complexity of temporal dynamics and the need for robust algorithms that can handle variations in facial ing from noisy labels through feature extraction. Our goal is to create a diffusion-based FER system that utilises the distribution mapping capabilities of diffusion models (DMs) to effectively restore noise labels and their corresponding images. For this purpose, we introduce LRDif. Considering the transformer's proficiency in capturing long-distance pixel dependencies, we choose transformer blocks as the foundational components of our LRDif architecture. Dynamic transformer blocks are layered in a U-Net style to construct the Under-Display Camera Transformer (UDCformer), aimed at multilevel feature extraction. The UDCformer comprises two parallel networks: the DTnetwork, which extracts latent features from label and UDC images at multiple levels, and the DILnetwork, which learns the similarities between facial landmarks and UDC images. LRDif follows a two-stage training process: (1) Initially, as shown in Fig.  2  (a), we construct a condensed preliminary extraction network (FPEN) that extracts an Emotion Prior Representation (EPR) from latent label and UDC images. This EPR is then utilized to guide the UDCformer in restoring labels. (2) Subsequently, shown in Fig.  2 (b ), the diffusion model (DM) is trained to directly infer the precise EPR from UDC images. Owing to the lightweight nature of EPR Z, the DM can achieve highly accurate EPR predictions, leading to significant improvements in test accuracy after a few iterations.\n\nThe main contributions of this work are summarized as follows: 1) We introduce a novel approach to address the challenges posed by under-display cameras (UDC) in facial expression recognition, offering a diffusion-based solution aimed at mitigating the effects of extra noise and image distortions. 2) LRDif focuses on the powerful mapping capabilities of DMs to deduce a concise emotion prior representation (EPR), enhancing both the accuracy and consistency of FER predictions. This method distinguishes itself from previous approaches by not relying on the knowledge of the dataset's uncertainty distribution. 3) Extensive experiments demonstrate that LRDif achieves state-of-the-art performance in emotion recognition tasks on three synthesized UDC-FER datasets and several standard FER datasets.",
      "page_start": 29,
      "page_end": 30
    },
    {
      "section_name": "Related Work",
      "text": "Facial Expression Recognition. A typical FER system comprises three main phases: face detection, feature extraction, movements, head poses, and lighting conditions across frames  [231, 232, 233] . Table  9  shows the literature on generative models for dynamic FER, where UF1 stands for unweighted F1 score that combines precision and recall, providing a balanced measure of a model's performance.\n\nFor example, Cai et al.  [234]  introduce a Masked Autoencoder (MAE) for learning robust and generic facial embeddings, as depicted in Figure  10 . The MAE reconstructs spatio-temporal details of the face from densely masked facial regions, capturing both local and global aspects to encode transferable features. Priyanka A. Gavade et al.  [235]  propose the Taylor-Chicken Swarm Optimization-based Deep GAN (Taylor-CSO-based Deep GAN), which provides a new method for recognizing video expressions. The Deep GAN, trained by the Taylor CSO approach, is adopted to efficiently complete the recognition process on the basis of the feature matrix. Guo et al.  [236]  propose a novel approach that leverages GANs to generate inter-class optical flow images for FER. They calculate the difference between static fully expressive samples and neutral expression samples to obtain the optical flow information. By training the GAN on these optical flow images, the model learns to capture the subtle motion patterns associated with different facial expressions. Similarly, Liong et al.  [237]  propose an improved FER system that utilizes GANs to generate inter-class optical flow images by computing variations of optical flow. They utilize a self-attention mechanism within the GAN architecture, which enables the generator to integrate image information from all feature locations when generating image details. Other similar works based on GANs for dynamic FER include   nents across temporal axis for each spatio-temporal cube.\n\nOur facial regions based tube-masking strategy ensures the same facial region is masked throughout the temporal cube, thus posing a challenging reconstruction task and promoting learning local and global facial details (See Alg. 1).\n\nAs the masked spatio-temporal cubes look like deformable bending tubes, we termed it as Facial region-guided tube masking aka Fasking.\n\nWe begin with face parsing using FaceXZoo  [75]  library which divides facial regions into the following parts {left-eye, right-eye, nose, mouth, hair, skin, background} (Fig.  2 (b) ). Among the facial regions, we prioritize the following set P = {left-eye, right-eye, nose, mouth, hair} over skin and background to preserve face specific local and sparse features. In order to maintain pre-defined masking ratio r, facial regions from the priority set P are masked across frames first followed by {background, skin} masking. Thus, Fasking generates n masked and (kn) visible tokens. Across all the frames of the input v, we track specific facial regions from the pre-defined set to encode and reconstruct spatio-temporal changes to the model facial motion. The fasking strategy thus poses more challenges to the reconstruction while encoding subject specific appearance and fine-grained details. b) Masked Autoencoder. After Fasking, (kn) visible tokens are given input to the Encoder F ϕE which maps the tokens to the latent space z. The visible tokens serve as a reference to generate the masked counterpart of the face. Thus, the decoder F ϕD maps the latent space z to the re-constructed masked tokens X ′ m . Please note that similar to VideoMAE  [71] , we adopt ViT  [28]  architecture as a backbone for MARLIN. A reconstruction loss (L recon ) is imposed between masked cubes X m and their reconstructed counterparts X ′ m to guide the learning objective. c) Adversarial Adaptation Strategy. To enhance the generation quality for rich representation learning, we incorporate adversarial adaptation on top of the masked autoencoder backbone. According to the prior literature  [27, 60] , adversarial training enhances generation quality which in turn results in rich latent feature z. The discriminator F ϕΓ as shown in Fig.  2  is an MLP based network which imposes adversarial loss L adv between X m and their reconstructed counterparts X ′ m . (a) Reconstruction Loss. Given an input masked tokens Xm , the masked auto-encoder module reconstruct it back to X ′ m . To this end, we minimize mean squared error loss in the 3D token space to update the weights of the (F ϕΓ •F ϕE • F ϕf ) branch. The loss is defined as",
      "page_start": 31,
      "page_end": 31
    },
    {
      "section_name": "Overall Marlin Loss",
      "text": "where N is the total number of data in D, X\n\nm and X\n\nare the masked token and reconstruction of i-th data in D.\n\nFigure  10 : A MAE framework for dynamic FER from  [234] : It begins by detecting faces in each video frame, followed by a temporal sampling and tokenization process that divides the detected faces into segments. These segments are then fed into a face parser, which identifies and segments various facial features, such as eyes, nose, and mouth. The segmented features are patched together to create a complete representation of the face. To enhance the learning process, the framework employs a selective masking strategy. It randomly masks some of the patches, creating a mix of visible and masked segments.  [238, 239] .\n\nIn summary, generative models show tremendous advantages in the field of FER. First, generative models can effectively augment training data by generating realistic and diverse new facial images. Second, generative models demonstrate unique advantages in facial feature extraction. Moreover, generative models provide effective solutions for cross-domain FER. When dealing with noisy environments, generative models also exhibit promising performance. Finally, the effectiveness of generative models in dynamic FER is fully validated. In addition, it is worth noting that apart from these aspects, generative models can also play a role in semisupervised scenarios for FER, although there are relatively fewer related works. For example, Chen et al.  [56]  propose an improved classification method based on semisupervised GANs. They replace the output layer of the traditional unsupervised GAN with a Softmax layer, enabling the model to simultaneously learn from labeled and unlabeled data.",
      "page_start": 32,
      "page_end": 32
    },
    {
      "section_name": "Reference",
      "text": "Year Model Dataset Performance (%) Cai et al.  [234]  2023 MAE CMU-MOSEI ACC: 80.60 Gavade et al.  [235]  2023 DGAN CK+/RAVDESS/SAVEE ACC: 82.14/80/79 Guo et al.  [236]  2023 GAN SAMM/AFEW ACC: 56.98/60.20/44.72 Liong et al.  [237]  2020 GAN CASME II/SMIC/SAMM ACC: 70.29 Li et al.  [238]  2021 GAN CASME II/SAMM/SMIC UF1: 84.52, UAR: 84.65 Mazen et al.  [239]  2021 CycleGAN FER2013 ACC: 91.76",
      "page_start": 33,
      "page_end": 33
    },
    {
      "section_name": "Textual Emotion Recognition (Ter)",
      "text": "TER is a highly focused and challenging research direction in the field of natural language processing  [240] . It is dedicated to automatically identifying and extracting the emotional information contained in textual data, providing insights into the emotional states and subjective attitudes conveyed by humans in their written expressions  [241, 242] . The core task of TER is to identify the specific emotion categories expressed in the text, such as happiness, sadness, anger, surprise, etc.  [241] . Compared to textual sentiment analysis  [243, 244, 245, 246 ] (e.g., positive, negative, neutral), TER offers a more fine-grained characterization of emotional semantics, capturing richer and more subtle emotional expressions in the text  [20, 247] . Table  10  illustrates the main differences between TER and textual sentiment analysis. In recent years, generative models have shown great potential in TER tasks. By learning the intrinsic patterns and distributions of textual data, generative models, especially LLMs or their predecessors, can generate emotionally expressive text, providing new perspectives and methods for emotion recognition  [248] . Based on this, this section reviews a series of works of generative models in TER tasks. For instance, in terms of data augmentation, Nedilko  [48]  utilizes ChatGPT to augment the dataset by translating the data into different languages for the task of TER. Similarly, Koptyra et al.  [49]  investigate the feasibility of employing ChatGPT for automatic emotional text generation and annotation in the context of TER. Pico et al.  [249]  study the application of text-generating LLMs for TER with the aim of generating more emotional knowledge, in the form of beliefs, that can be utilized by an emotional agent.\n\nFurthermore, in terms of feature extraction, Ghosal et al.  [250]  leverage COMET  [251] , a GPT-based model, to extract emotional textual features, such as mental states, events, and causal relations, for the task of TER. These features are then incorporated into their proposed framework, COSMIC, which aims to enhance the understanding of emotional dynamics in conversations by capturing the underlying commonsense knowledge that influences the emotional states of the interlocutors. Hama et al.  [252]  demonstrate that the LLMs can achieve excellent performance for emotion labels with a limited number of training samples. Additionally, they introduce a multi-step prompting technique to further enhance the discriminative capacity of the LLMs for different emotion labels. InstructERC  [253]  reformulates the TER task by transitioning from a discriminative framework to a generative framework based on LLMs, as illustrated in Figure  11 . CKERC  [254]  is a joint LLMs with commonsense knowledge framework for TER, which utilizes commonsense knowledge for LLM pre-training to finetune implicit clues information.",
      "page_start": 32,
      "page_end": 33
    },
    {
      "section_name": "Physiological Signal-Based Emotion Recognition",
      "text": "In addition to the emotion recognition based on speech, facial images, and text mentioned above, physiological signals have garnered significant attention in the field of emotion recognition because they directly reflect an individual's bodily changes during emotional experiences. These changes are usually not influenced by an individual's subjective consciousness and can therefore provide an objective reflection of emotional states  [21, 255] . The physiological signals used in emotion recognition include EEG, ECG, HRV, EDA, and others, each with its own unique characteristics. For instance, EEG signals have the advantages of high temporal resolution and directly reflecting neural activity  [256] , enabling them to capture subtle changes in brain activity. As a result, EEG is widely used in emotion recognition research  [257, 258] . On the other hand, ECG signals reflect changes in cardiac activity and are closely related to emotional states  [259, 260] . HRV signals indicate the variability of heart rate, which can reflect the activity of the autonomic nervous system  [261] . EDA signals reflect changes in skin conductance and are associated with emotional arousal  [262] . These signals provide valuable insights into the physiological changes associated with different emotional experiences  [255] . In recent years, generative models have shown great advantages in physiological signal-based emotion recognition  [263] , similar to their applications in the fields of SER, FER, and TER. These models have demonstrated their potential in various aspects, such as data augmentation, feature extraction, and cross-domain, providing new impetus for the development of this field. Table  11  presents the literature on generative models for physiological signal-based emotion recognition.",
      "page_start": 33,
      "page_end": 33
    },
    {
      "section_name": "Vae",
      "text": "In this section, we introduce the structure and function of VAE  (Kingma and Welling, 2014) . In the proposed VAE-D2GAN model, VAE learns the latent information from real samples through the encoder and the decoder.\n\nThe TP-DE topological maps extracted from real EEG data were extended into one-dimensional feature vectors x real as input for VAE. Input x real into encoder E to return the estimation of posterior data distribution q(z |x real ), input the low dimensional latent vector z into decoder G, and reconstruct conditional distribution p(x real |z ) of data under the constraint of prior distribution p(z), where q(z |x real ) and p(x real |z ) are usually represented as follows:\n\nwhere E(•) represents the encoder, G(•) represents the decoder (or generator), and x real represents the reconstruction samples.\n\nThe latent vector z is a combination of the mean value µ and the standard deviation σ output by the encoder E, and is expressed as follows:\n\nwhere γ ∼ N(0, I) is a random vector which obeys Gaussian distribution, denotes multiplication by elements, therefore we assume that the latent vector z approximately confirms to Gaussian distribution z ∼ N(µ, exp(σ) 2 ).\n\nThe Kullback-Leibler (KL) divergence is introduced to optimize the parameters of the encoder, and the KL divergence loss formula is as follows:\n\nwhere KL(•) represents the calculation of KL divergence distance. Data augmentation is the primary application of generative models in physiological signal-based emotion recognition. Researchers conduct a series of studies in this direction. For example, Luo et al.  [264]  propose a Conditional Wasserstein GAN (CWGAN) framework for EEG data augmentation in emotion recognition, which generates high-quality realistic-like EEG data in differential entropy (DE) form, judged by three indicators, to supplement the data manifold and improve emotion classification performance. As shown in Figure  12 , Bao et al.  [265]  propose VAE-D2GAN, a data augmentation model for EEG emotion recognition, which combines a VAE model with a dual discriminator GAN to learn the distributions of DE features under five classical frequency bands and generate diverse artificial EEG samples for training. Bhat and Hortal  [266]  present a model based on Wasserstein GAN with gradient penalty, capable of generating new data features to augment the original dataset. Zhang et al.  [267]  propose a GAN-based framework with self-supervised learning to generate EEG samples by learning an EEG generator, masking parts of EEG signals to synthesize potential signals, and utilizing masking possibility as prior knowledge to extract distinguishable features and generalize the classifier. Zhang et al.  [268]  propose a emotional subspace constrained GAN model that addresses the limitations of existing GANs in generating reliable augmentations for underrepresented minority emotions in imbalanced EEG datasets by introducing an EEG editing paradigm and incorporating diversity-aware and boundary-aware losses to constrain the augmented emotional subspace. Similar works on EEG data augmentation based on GANs also include  [269, 270, 271, 272] . Apart from GAN-based models, other generative approaches have also been explored. Siddhad et al.  [273]  employ a conditional DM to generate raw synthetic EEG data, while Tosato et al.  [274]  and Yi et al.  [275]  also utilize DMs for EEG data generation.\n\nIn addition to data augmentation, generative models can be employed for feature extraction in physiological signal-based emotion recognition, among which AE-based models are widely used. For example, Lan et al.  [276]  use an AE model to learn subject-specific salient frequency components from EEG signals' power spectral density, automatically identifying the most discriminative features without predefined frequency band ranges. Rajpoot et al.  [277]  employ an unsupervised LSTM with channel-attention AEs to obtain subject-invariant latent vector subspace representations and a CNN with attention framework to perform EEG emotion recognition on the encoded latent space representations. Similar AE-based methods also include  [278, 279, 280, 281, 282, 283] . In addition, some researchers attempt to combine GAN-based approaches. For example, Gu et al.  [284]  integrates generative adversarial learning into a hybrid model of Graph Convolutional Neural Network (GCNN) and LSTM, which utilizes a GAN to generate latent representations of EEG signals, aiming to extract more discriminative features and improve classification performance.\n\nTo effectively utilize both unlabeled and labeled data, semi-supervised learning approaches are explored. Notably, Zhang et al.  [285]  introduced a semi-supervised framework for EEG emotion recognition using an attention-based recurrent AE model. Their approach consists of two components: an unsupervised component that maximizes the consistency between the original and reconstructed input data, and a supervised component that minimizes the cross-entropy between the input and output labels. To address the cross-domain problem, researchers propose a series of methods. For example, Chai et al.  [286]  introduce a subspace-aligned AE framework to obtain a universal representation across training and testing domains for EEG emotion recognition. Wang et al.  [287]  propose a multimodal VAE to utilize the multi-modal data (EEG and eye movement data) from the source domain to assist in EEG emotion recognition in the target domain. Huang et al.  [288]  propose a GAN-based domain adaptation method with a knowledge-free mechanism, which converts the source domain in the data space into the target domain through a novel EEG content loss function.",
      "page_start": 34,
      "page_end": 34
    },
    {
      "section_name": "Multimodal Emotion Recognition (Mer)",
      "text": "Unlike unimodal emotion recognition mentioned in Sections 5 -11 above, Multimodal Emotion Recognition (MER) is a research field that aims to identify emotional states by combing the information from different modalities, including speech, facial images, text, EEG, eye movements, and functional magnetic resonance imaging (fMRI), and so on  [3, 16, 289, 290] . By integrating these heterogeneous data, MER can more comprehensively capture and analyze subtle differences in emotional expressions, thereby significantly improving the performance of emotion recognition. In recent years, generative models have been increasingly applied in the field of MER, showing potential in various aspects such as data augmentation, feature extraction, semi-supervised learning, and modality completion. Table  12  summarizes a series of representative works on generative models for MER.\n\nSimilar to unimodal emotion recognition, the primary application of generative models in MER is also data augmentation. For example, Ma et al.  [140]  design a multimodal CGAN based on Hirschfeld-Gebelein-Rényi (HGR) maximal correlation  [291, 292, 293]  to generate audio-visual data with different emotion categories. Luo et al.  [294]  implement a conditional boundary Equilibrium GAN  [295]  to generate artificial differential entropy features for EEG data, eye movement data, and their direct connections. Yan et al.  [296]  propose a CGAN-based framework that uses eye movement signals as the sole input to map information onto EEG-eye movement features. Chao et al.  [297]  employ a GAN framework with a cyclic loss to learn the bi-directional generative relationship between acoustic features and fMRI signals, enabling the derivation of fMRI-enriched acoustic features for emotion classification.\n\nIn addition to data augmentation, feature extraction is an important aspect of MER. For example, Yan et al.  [298]  propose a method that uses a bimodal deep AE and a fuzzy integral-based technique  [299]  to extract high-level fused features from EEG signals and eye movements, which are then input into a SVM classifier framework to obtain an emotion classification model. Similar works based on the bimodal deep AE also include  [300, 301, 302, 303] . Nguyen et al.  [304]  propose a MER model that combines a two-stream AE with a LSTM network to perform endto-end trainable joint learning of temporal and compact-representative features from visual and audio signals.  Ma et al. [305]  propose a deep framework with CNN and LSTM for MER that learns feature mappings from multimodal data by introducing a correlation loss based on HGR maximal correlation and a reconstruction loss of AEs to capture common and private information among different modalities, respectively. Zheng et al.  [306]  propose a novel Multi-channel Weight-sharing AE model with a convolutional encoder network for improved feature extraction and a scalable feature fusion method based on multi-head attention mechanism (CMHA) to fuse the output features of multiple encoders for MER.\n\nSemi-supervised learning is another aspect where generative models are considered in MER. For example, Du et al.  [308]  propose a semi-supervised MER framework based on a multi-view VAE that imposes a mixture of Gaussians assumption on the posterior approximation of latent variables. This framework can leverage both labeled and unlabeled samples from multiple modalities and automatically learn the weight factor for each modality. Liang et al.  [309]  propose a semi-supervised learning approach for MER based on an improved GAN, CT-GAN  [310] , which utilizes unlabeled data from acoustic and visual modalities.\n\nIn addition to the aforementioned aspects, another direction that requires atten- The overview framework is illustrated in Fig.  2 . It mainly consists of three parts: shallow feature extractor, missing modality diffused network, and multimodal fusion and prediction. Due to the discrepancy of the original dimensional space between modalities, we first align the dimensionality of each modality by extracting multimodal shallow features to facilitate subsequent modality recovery. Then, to alleviate the distribution gap between recovered modalities and original ones, we design a missing modality diffused network to model the distribution of each missing modality and sample from the modeled distribution space to generate missing data. At the same time, to reduce the semantic ambiguity of the recovered modalities, the available modalities are embedded as conditions in the recovering process so that we can recover semantically consistent samples from a distribution conditioned on the available modalities. Finally, to perform the MER, the multimodal fusion and prediction part receives recovered complete multimodal data and uses multimodal transformers to fuse multimodal representation for emotion recognition. The detail is introduced in the next subsections.",
      "page_start": 37,
      "page_end": 37
    },
    {
      "section_name": "Shallow Feature Extractor",
      "text": "We consider three heterogeneous modalities: language (l), vision (v), and acoustic (a) for the MER task. Since the original dimensional spaces of the three modalities are often inconsistent, they are not suitable for direct use in missing modality recovering. To address this problem, we design a shallow feature extractor that contains three independent encoders E k , k ∈ {l, v, a}, to extract the shallow features of three modalities and map them into the same dimensional space. Each encoder contains a 1D convolutional layer. Thus, the subsequent recovery task aims to recover missing modalities from the distribution space of shallow features.\n\nGiven an input example (aka sequence data), we can extract the shallow features, X = {x k }, x k ∈ R L×D , L and D indicate the sequence length and the feature dimensionality. In the incomplete multimodal case, some modalities are missing either fixedly or randomly by guaranteeing at least one modality is available in X . For the three modalities mentioned above, a total of seven missing combinational cases are included, which are reported in Tab. 1. For a convenient statement but without loss of generality, below we recover the missed acoustic modality x a , as shown in Fig.  2 .",
      "page_start": 38,
      "page_end": 38
    },
    {
      "section_name": "Missing Modality Diffused Network",
      "text": "We consider a score network s m to model the distribution of missing modality m ∈ I miss by perturbing x m with a common SDE: dx = σ t dw, t ∈ [0, 1], like score-based diffusion  [24] , the corresponding reverse-time SDE can be derived from Eq. (2) and Eq. (  8 ) as:\n\n5\n\nFigure  13 : The IMDer framework for modality completion in MER from  [307] : It handles incomplete data (e.g., missing acoustic modality) by extracting shallow features from the available modalities. Then the diffused network samples the missing modality data from a noise distribution and then recovers it through a reverse diffusion process. Finally, the recovered data is combined with the other modality data for multimodal fusion and prediction to regress emotional labels.\n\ntion in MER is modality completion  [311, 312] . It refers to situations where some modalities in multimodal data are missing or incomplete due to reasons such as device failure, signal loss, or data corruption. By completing the missing modalities, incomplete multimodal data can be fully utilized to ensure the performance of emotion recognition. Generative models provide an effective approach for modal completion. For example, Liu et al.  [313]  employ multiple AEs to generate high-quality missing modality data by reconstructing it from the available modality features and the invariant features learned through contrastive learning. As shown in Figure  13 , Wang et al.  [307]  propose an Incomplete Multimodality-Diffused emotion recognition (IMDer) method that exploits a score-based DM to recover missing modalities while maintaining distribution consistency and semantic disambiguation by using available modalities as a condition to guide the diffusion-based recovering process.  10. Discussion 10.1. Summary Findings Firstly, we conduct a statistical analysis of the above research papers, as shown in Figure  14 , which includes two subfigures 14a and 14b. From subfigure 14a, it can be seen that among various modalities, facial images are the most commonly used by generative models for emotion recognition, reflecting that facial information can express emotions more intuitively and richly. Next are speech signals, while physiological signals, multimodal signals, and text modalities are less common. Subfigure 14b shows the usage trends of various generative models in emotion recognition research during different periods. It can be seen that AEs are most common initially, and over time, the use of GANs is increased. Recently, DMs and LLMs have also begun to gradually appear in research. This evolution reflects the progress of generative technology, with new and more complex models continuously being developed and applied to the field of emotion recognition.\n\nSecondly, generative models have applications in various aspects of emotion recognition, including data augmentation, feature extraction, semi-supervised learning, cross-domain, etc., among which data augmentation and feature extraction are the most common. This may be because the acquisition and annotation of emotion data is quite expensive, especially for facial images and speech data. Utilizing generative models, more samples can be synthesized from existing data to expand the training set. At the same time, the feature representations learned by generative models can also be easily used for downstream emotion recognition tasks to improve performance.\n\nThirdly, although there are common integration points of generative models in emotion recognition based on different modalities, the specific usage still needs to consider the characteristics of each modality. For example, when generative models are used for data augmentation in SER, the temporal information of speech data can be preserved in the feature space or sample space, while in FER, it almost always operates directly in the sample space to generate image data. Meanwhile, in terms of enhancing robustness, SER pays attention to adversarial sample generation, while FER conducts more research directly from the perspective of dealing with noisy environments such as occlusion and complex backgrounds during sample acquisition. These differences indicate that applying generative models to emotion recognition based on different modalities requires fully considering the unique attributes of their data and designing and optimizing models in a targeted manner.\n\nFourthly, it can be seen that among various emotion recognition models, those based on AE and GAN are used the most, possibly because they appeared earlier, and both theory and practice are relatively mature, with widespread applications. The use of DM and LLM is relatively less, possibly because as of now, the application of DM in emotion recognition is still in its infancy, while LLM is mainly used with text modalities. However, due to their excellent generative capabilities and ability to model prior knowledge, it is believed that they will play a greater role in the field of emotion recognition in the future.\n\nFifthly, it is worth mentioning that we also find relatively few literature on generative models in TER, which may be due to the fact that there has been less mining of fine-grained emotional attributes in text in the past. However, we also note that the application of LLM in TER is increasing. Its powerful semantic understanding and generative capabilities are expected to promote the further development of TER.\n\nFinally, it can be seen that eye movement signals are often combined with other modalities for emotion recognition. However, since the ability of eye movement signals alone to comprehensively represent emotions is limited, they are usually not used independently for emotion recognition, but rather integrated with other modalities to improve overall performance, indicating that multimodal fusion is a major trend in emotion recognition.",
      "page_start": 39,
      "page_end": 39
    },
    {
      "section_name": "Future Prospects",
      "text": "In the above discussion, we explore various applications of generative models in the field of emotion recognition. Looking forward, research on using generative models for emotion recognition remains full of opportunities, including the following aspects:\n\nFirstly, an exciting development direction is to combine advanced generative models such as DMs with Transformer architectures  [60, 61] . DMs excel at generating high-quality, diverse data samples and have shown more remarkable results than AEs and GANs in an increasing number of domains  [314, 315] . On the other hand, Transformer architectures, with their powerful sequence modeling capabilities and strong contextual modeling abilities, have brought about a paradigm shift  [85] . Combining the two can produce a more powerful generative model capable of simultaneously processing multimodal data such as images, speech, and text, thereby capturing multiple aspects of human emotions more comprehensively and accurately. This combination can significantly enhance the performance of emotion recognition, enabling systems to understand more subtle and complex emotional expressions.\n\nSecondly, combining generative models with other cutting-edge AI technologies such as reinforcement learning  [62]  and federated learning  [63, 316]  can further push the boundaries of emotion recognition. Reinforcement learning specializes in modeling and optimizing multi-stage decision-making processes, and emotion recognition and generative models are often dynamic processes involving multiple steps such as environment perception, strategy determination, and performance evaluation. Combining the two can achieve more interactive and adaptive emotional systems  [317] .\n\nFurthermore, due to the privacy nature of emotional data, it is often difficult to collect on a large scale, which limits the improvement of model performance. Federated learning can allow distributed data to contribute to model training while protecting privacy  [318, 319] . By training models on personal devices and then only uploading parameter updates instead of raw data, it is hoped that more powerful and personalized emotion models can be obtained.\n\nThirdly, with the continuous development of generative models, their applications in emotion recognition are expanding to new scenarios such as virtual reality (VR) and augmented reality (AR)  [64, 65] . In these immersive environments, user's emotions and experiences are closely linked. For example, in VR games or social interactions, generating responsive scenes, characters, and dialogues in real-time based on the user's emotional state can create a more immersive experience  [320] . In AR navigation or shopping, overlaying emotion-related information and recommendations can provide more thoughtful services  [321] . Since VR/AR devices can collect multimodal user data such as gaze and actions, they provide richer clues for emotion recognition. Generative models can leverage this data to create more diverse and dynamic emotional responses.\n\nFinally, this survey mainly discusses using generative models to assist emotion recognition, but another promising direction is to directly synthesize emotionally rich content, such as emotional speech, text, images, and videos  [66, 67] . This has important application value in many fields. For example, in film and animation production, automatically generating dubbing and background music with specific emotional tones can reduce the cost of manual recording  [322] . In intelligent customer service systems, empathetic and personalized responses can be synthesized in real-time based on the conversation content and user emotions, improving user satisfaction  [323] . In the field of education, emotionally rich learning materials (such as stories and dialogues) can be automatically generated to create a more vivid and interesting learning experience  [324] . This puts forward higher requirements for the design of generative models and requires more refined conditional control mechanisms. A beneficial prior work is the data augmentation discussed in this survey, i.e., using generative models to synthesize samples with different emotions to expand the dataset. In the future, data augmentation is expected to provide a data foundation and prior knowledge for further high-quality emotional content generation.",
      "page_start": 41,
      "page_end": 41
    },
    {
      "section_name": "Conclusion",
      "text": "In this paper, we conduct the first systematic survey of the progress of generative technology in the field of human emotion recognition. By analyzing over 320 relevant papers, we delve into the extensive impact of generative models on various aspects and modalities of emotion recognition. This work fills the gap in existing reviews and provides valuable references for researchers in this field.\n\nIn Section 1, we introduce the background knowledge of emotion recognition and affective computing, clarifying the close relationship between the two. Emotion recognition is one of the key technologies for realizing affective computing and is crucial for building intelligent, natural, and friendly human-computer interaction systems. Generative models, with their powerful data modeling and generation capabilities, provide new ideas for emotion recognition in multiple aspects. Based on this, we propose the taxonomy for the application of generative models in emotion recognition and clarify the main contributions of this review, which is to systematically summarize the current state of generative models in emotion recognition and to outlook future research directions.\n\nIn Section 2, we make a detailed comparison between this review and other relevant reviews. Although there have been some reviews on emotion recognition or generative models, no work has systematically discussed the combination of the two. This review fills this gap and helps researchers comprehensively understand the latest developments in this interdisciplinary field. In Section 3, in order to help readers better understand the subsequent content, we briefly introduce the mathematical principles of several common generative models, including AE, GAN, DM, and LLM. These models have their own characteristics and play important roles in different tasks of emotion recognition. Section 4 outlines the commonly used datasets for generative models in emotion recognition tasks, covering multiple modalities such as speech, facial images, text, and physiological signals.\n\nThen, from multiple perspectives such as data augmentation, feature extraction, semi-supervised learning, and cross-domain, we elaborate on the applications of generative models in emotion recognition based on different modalities, including SER in Section 5, FER in Section 6, TER in Section 7, physiological signal-based emotion recognition in Section 11, and MER in Section 9. Through comprehensive analysis, we demonstrate that generative models can (1) generate high-quality emotional data to alleviate the problem of data scarcity, (2) learn high-level feature representations of emotional data to enhance the discriminative power of features, (3) utilize unlabeled data for semi-supervised learning to reduce the dependence on large amounts of labeled data, (4) generate cross-domain data to enhance the generalization ability of models, and other aspects. These applications greatly promote the development of emotion recognition. In Section 10, we analyze the characteristics of different generative models in emotion recognition based on different modalities, and emphasize the necessity of continuous innovation of generative models to further enhance their role in affective computing in the future.\n\nIn summary, generative models have brought new breakthroughs to emotion recognition, greatly expanding the depth and breadth of research. However, how to further tap the potential of generative models and build more intelligent, natural, and humanized affective computing systems still requires the joint efforts of academia and industry. We believe that with the continuous development of artificial intelligence technology, especially the increasing maturity of generative models, emotion recognition and the entire field of affective computing will usher in a broader development prospect and bring a better future for human-computer interaction. This review provides useful references and inspiration for achieving this goal.",
      "page_start": 42,
      "page_end": 43
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Specifically, this survey will focus on several representative models, such as Au-",
      "page": 2
    },
    {
      "caption": "Figure 1: Schematic Diagram of Generation technology for Emotion Recognition. 2",
      "page": 3
    },
    {
      "caption": "Figure 2: Regarding data augmentation, it is an important way to",
      "page": 4
    },
    {
      "caption": "Figure 2: Taxonomy of This Survey.",
      "page": 6
    },
    {
      "caption": "Figure 3: illustrates a general architecture. Table 4",
      "page": 17
    },
    {
      "caption": "Figure 3: The pipeline of data augmentation from [140]: First, a generative model is employed to",
      "page": 18
    },
    {
      "caption": "Figure 4: The framework designed for extracting speech features from [154]: It introduces instance",
      "page": 19
    },
    {
      "caption": "Figure 4: The concatenated fea-",
      "page": 19
    },
    {
      "caption": "Figure 5: A semi-supervised learning framework in SER from [55]: A generator creates synthetic",
      "page": 22
    },
    {
      "caption": "Figure 5: In their approach, a generator creates synthetic audio descriptors",
      "page": 22
    },
    {
      "caption": "Figure 6: A framework [35] which leverages AEs to address the cross-domain problem in SER:",
      "page": 23
    },
    {
      "caption": "Figure 6: Xiao et al. [59] design a framework by incorporating a VAE to extract generalized",
      "page": 23
    },
    {
      "caption": "Figure 7: A facial data generation framework based on GANs from [180]: The original image",
      "page": 25
    },
    {
      "caption": "Figure 7: , Sun et al. [180] propose an improved CGAN (im-cGAN) model",
      "page": 25
    },
    {
      "caption": "Figure 8: , this approach effec-",
      "page": 26
    },
    {
      "caption": "Figure 8: 6.3. Cross-domain FER",
      "page": 27
    },
    {
      "caption": "Figure 8: A Masked Autoencoder (MAE)-based facial feature extraction framework from [212]:",
      "page": 28
    },
    {
      "caption": "Figure 9: 6.5. Dynamic FER",
      "page": 29
    },
    {
      "caption": "Figure 9: A DM-based framework for FER in noisy environments proposed by [229]: It overcomes",
      "page": 30
    },
    {
      "caption": "Figure 10: A MAE framework for dynamic FER from [234]: It begins by detecting faces in each",
      "page": 31
    },
    {
      "caption": "Figure 11: A LLM-based framework for TER proposed by [253]: It introduces an effective retrieval",
      "page": 33
    },
    {
      "caption": "Figure 11: CKERC [254] is a joint",
      "page": 33
    },
    {
      "caption": "Figure 12: The framework of VAE-D2GAN model for data augmentation in physiological signal-",
      "page": 34
    },
    {
      "caption": "Figure 12: , Bao et al. [265] propose VAE-",
      "page": 34
    },
    {
      "caption": "Figure 13: The IMDer framework for modality completion in MER from [307]: It handles in-",
      "page": 38
    },
    {
      "caption": "Figure 14: (a) The proportion of different modalities being used, (b) The proportion of different",
      "page": 40
    },
    {
      "caption": "Figure 14: , which includes two subfigures 14a and 14b. From subfigure 14a, it can",
      "page": 40
    },
    {
      "caption": "Figure 14: b shows the usage trends of various generative models in emotion recognition re-",
      "page": 40
    }
  ],
  "tables": [
    {
      "caption": "Table 1: Table 1: Main Acronyms.",
      "data": [
        {
          "Acronym": "FER",
          "Full Form": "Facial Emotion Recognition"
        },
        {
          "Acronym": "MER",
          "Full Form": "Multimodal Emotion Recognition"
        },
        {
          "Acronym": "DAE",
          "Full Form": "Denoising Autoencoder"
        },
        {
          "Acronym": "VAE",
          "Full Form": "Variational Autoencoder"
        },
        {
          "Acronym": "CAE",
          "Full Form": "Convolutional Autoencoder"
        },
        {
          "Acronym": "DCGAN",
          "Full Form": "Deep Convolutional GAN"
        },
        {
          "Acronym": "ACGAN",
          "Full Form": "Auxiliary Classifier GAN"
        },
        {
          "Acronym": "GCNN",
          "Full Form": "Graph Convolutional Neural Network"
        },
        {
          "Acronym": "LLM",
          "Full Form": "Large Language Model"
        },
        {
          "Acronym": "ViT",
          "Full Form": "Vision Transformer"
        },
        {
          "Acronym": "T5",
          "Full Form": "Text-to-Text Transfer Transformer"
        },
        {
          "Acronym": "CNN",
          "Full Form": "Convolutional Neural Networks"
        },
        {
          "Acronym": "LSTM",
          "Full Form": "Long Short-Term Memory"
        },
        {
          "Acronym": "AU",
          "Full Form": "Action Unit"
        },
        {
          "Acronym": "ECG",
          "Full Form": "Electrocardiogram"
        },
        {
          "Acronym": "HRV",
          "Full Form": "Heart Rate Variability"
        },
        {
          "Acronym": "fMRI",
          "Full Form": "functional Magnetic Resonance Imaging"
        },
        {
          "Acronym": "CCC",
          "Full Form": "Consistency Correlation Coefficient"
        },
        {
          "Acronym": "UAR",
          "Full Form": "Unweighted Average Recall"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table 2: shows how our work differs from other reviews.",
      "data": [
        {
          "Reference": "Ours",
          "Focus on\ngenerative models": "✓",
          "Related to\nemotion recognition": "✓",
          "Focus on\nmultiple modalities": "✓"
        },
        {
          "Reference": "Kammoun et al.\n[68]",
          "Focus on\ngenerative models": "✓",
          "Related to\nemotion recognition": "",
          "Focus on\nmultiple modalities": ""
        },
        {
          "Reference": "Wali et al.\n[69]",
          "Focus on\ngenerative models": "✓",
          "Related to\nemotion recognition": "✓",
          "Focus on\nmultiple modalities": ""
        },
        {
          "Reference": "Hajarolasvadi et al.\n[70]",
          "Focus on\ngenerative models": "✓",
          "Related to\nemotion recognition": "",
          "Focus on\nmultiple modalities": ""
        },
        {
          "Reference": "Deng and Ren [20]",
          "Focus on\ngenerative models": "",
          "Related to\nemotion recognition": "✓",
          "Focus on\nmultiple modalities": ""
        },
        {
          "Reference": "Zhao et al.\n[71]",
          "Focus on\ngenerative models": "",
          "Related to\nemotion recognition": "✓",
          "Focus on\nmultiple modalities": ""
        },
        {
          "Reference": "Khare et al.\n[17]",
          "Focus on\ngenerative models": "",
          "Related to\nemotion recognition": "✓",
          "Focus on\nmultiple modalities": "✓"
        },
        {
          "Reference": "Cîrneanu et al.\n[72]",
          "Focus on\ngenerative models": "✓",
          "Related to\nemotion recognition": "✓",
          "Focus on\nmultiple modalities": ""
        },
        {
          "Reference": "Younis et al.\n[73]",
          "Focus on\ngenerative models": "",
          "Related to\nemotion recognition": "✓",
          "Focus on\nmultiple modalities": "✓"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Database": "IAPS [92]",
          "Year": "1997",
          "Modalities": "Visual",
          "Spontaneous/\nin-the-wild": "In the Wild",
          "Samples": "More than 9000\nimages",
          "Subjects": "/",
          "Categories": "Valence, Arousal, and Dominance"
        },
        {
          "Database": "TFD [93]",
          "Year": "2010",
          "Modalities": "Visual",
          "Spontaneous/\nin-the-wild": "In the Wild",
          "Samples": "4178 images",
          "Subjects": "/",
          "Categories": "Happiness, Sadness, Surprise, Fear, Anger,\nDisgust, Neutral"
        },
        {
          "Database": "SFEW [94]",
          "Year": "2011",
          "Modalities": "Visual",
          "Spontaneous/\nin-the-wild": "In the Wild",
          "Samples": "1739 images",
          "Subjects": "330",
          "Categories": "Happiness, Sadness, Surprise, Fear, Anger,\nDisgust, Neutral"
        },
        {
          "Database": "FER2013\n[36]",
          "Year": "2013",
          "Modalities": "Visual",
          "Spontaneous/\nin-the-wild": "In the Wild",
          "Samples": "35887 images",
          "Subjects": "/",
          "Categories": "Happiness, Sadness, Surprise, Fear, Anger,\nDisgust, Neutral"
        },
        {
          "Database": "AffectNet\n[95]",
          "Year": "2017",
          "Modalities": "Visual",
          "Spontaneous/\nin-the-wild": "In the Wild",
          "Samples": "450000 images",
          "Subjects": "/",
          "Categories": "Happiness, Sadness, Surprise, Fear, Anger,\nDisgust, Neutral, Contempt"
        },
        {
          "Database": "RAF-DB\n[96]",
          "Year": "2017",
          "Modalities": "Visual",
          "Spontaneous/\nin-the-wild": "In the Wild",
          "Samples": "29672 images",
          "Subjects": "/",
          "Categories": "Happiness, Sadness, Surprise, Fear, Anger,\nDisgust, Neutral, Contempt"
        },
        {
          "Database": "JAFFE\n[97]",
          "Year": "1998",
          "Modalities": "Visual",
          "Spontaneous/\nin-the-wild": "Spontaneous",
          "Samples": "219 images",
          "Subjects": "10",
          "Categories": "Happiness, Sadness, Surprise, Fear, Anger,\nDisgust, Neutral"
        },
        {
          "Database": "BU-3DFE\n[98]",
          "Year": "2006",
          "Modalities": "Visual",
          "Spontaneous/\nin-the-wild": "Spontaneous",
          "Samples": "2500 images",
          "Subjects": "100",
          "Categories": "Happiness, Sadness, Surprise, Fear, Anger,\nDisgust, Neutral"
        },
        {
          "Database": "TFEID[99]",
          "Year": "2007",
          "Modalities": "Visual",
          "Spontaneous/\nin-the-wild": "Spontaneous",
          "Samples": "7200 images",
          "Subjects": "40",
          "Categories": "Happiness, Sadness, Surprise, Fear, Anger,\nDisgust, Neutral, Contempt"
        },
        {
          "Database": "Multi-PIE\n[100]",
          "Year": "2008",
          "Modalities": "Visual",
          "Spontaneous/\nin-the-wild": "Spontaneous",
          "Samples": "750000 images",
          "Subjects": "337",
          "Categories": "Happiness, Surprise, Sadness, Anger, Fear,\nDisgust"
        },
        {
          "Database": "BU-4DFE\n[101]",
          "Year": "2008",
          "Modalities": "Visual",
          "Spontaneous/\nin-the-wild": "Spontaneous",
          "Samples": "606 sequences",
          "Subjects": "101",
          "Categories": "Happiness, Surprise, Sadness, Anger, Fear,\nDisgust"
        },
        {
          "Database": "FAU Aibo\n[102]",
          "Year": "2008",
          "Modalities": "Audio",
          "Spontaneous/\nin-the-wild": "Spontaneous",
          "Samples": "9.2 hours of\nspeech",
          "Subjects": "31",
          "Categories": "Joyful, Surprised, Emphatic, Helpless, Touchy,\nRest"
        }
      ],
      "page": 12
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Database": "CHB-MIT\n[103]",
          "Year": "2009",
          "Modalities": "EEG",
          "Spontaneous/\nIn The Wild": "Spontaneous",
          "Samples": "/",
          "Subjects": "23",
          "Category": "Arousal and Valence"
        },
        {
          "Database": "USTC-\nNVIE[104]",
          "Year": "2010",
          "Modalities": "Visual",
          "Spontaneous/\nIn The Wild": "Spontaneous",
          "Samples": "236 images",
          "Subjects": "215",
          "Category": "Happiness, Sadness, Surprise, Fear, Anger,\nDisgust, Neutral"
        },
        {
          "Database": "CK+ [105]",
          "Year": "2010",
          "Modalities": "Visual",
          "Spontaneous/\nIn The Wild": "Spontaneous",
          "Samples": "593 images",
          "Subjects": "123",
          "Category": "Happiness, Sadness, Surprise, Fear, Anger,\nDisgust, Neutral, Contempt"
        },
        {
          "Database": "FACES\n[106]",
          "Year": "2010",
          "Modalities": "Visual",
          "Spontaneous/\nIn The Wild": "Spontaneous",
          "Samples": "1026 videos",
          "Subjects": "171",
          "Category": "Happiness, Surprise, Sadness, Anger, Fear,\nDisgust"
        },
        {
          "Database": "Oulu-\nCASIA[107]",
          "Year": "2011",
          "Modalities": "Visual",
          "Spontaneous/\nIn The Wild": "Spontaneous",
          "Samples": "480 sequences",
          "Subjects": "80",
          "Category": "Happiness, Surprise, Sadness, Anger, Fear,\nDisgust"
        },
        {
          "Database": "SMIC\n[108]",
          "Year": "2013",
          "Modalities": "Visual",
          "Spontaneous/\nIn The Wild": "Spontaneous",
          "Samples": "164 sequences",
          "Subjects": "16",
          "Category": "Positive, Negative, Surprise"
        },
        {
          "Database": "SEED\n[109]",
          "Year": "2013",
          "Modalities": "EEG",
          "Spontaneous/\nIn The Wild": "Spontaneous",
          "Samples": "/",
          "Subjects": "15",
          "Category": "Positive, Neutral, Negative"
        },
        {
          "Database": "CFEE\n[110]",
          "Year": "2014",
          "Modalities": "Visual",
          "Spontaneous/\nIn The Wild": "Spontaneous",
          "Samples": "229 images",
          "Subjects": "230",
          "Category": "Happiness, Surprise, Sadness, Anger, Fear,\nDisgust"
        },
        {
          "Database": "CASME II\n[111]",
          "Year": "2014",
          "Modalities": "Visual",
          "Spontaneous/\nIn The Wild": "Spontaneous",
          "Samples": "255 sequences",
          "Subjects": "35",
          "Category": "Happiness, Surprise, Disgust, Repression, and\nOthers"
        },
        {
          "Database": "ECG 200\n[112]",
          "Year": "2001",
          "Modalities": "ECG",
          "Spontaneous/\nIn The Wild": "Spontaneous",
          "Samples": "200 samples",
          "Subjects": "/",
          "Category": "Happiness, Surprise, Sadness, Anger, Fear,\nDisgust"
        },
        {
          "Database": "SAMM\n[113]",
          "Year": "2016",
          "Modalities": "Visual",
          "Spontaneous/\nIn The Wild": "Spontaneous",
          "Samples": "159 sequences",
          "Subjects": "32",
          "Category": "Happiness, Sadness, Surprise, Fear, Anger,\nDisgust, Contempt"
        },
        {
          "Database": "ADFES-\nBIV [114]",
          "Year": "2016",
          "Modalities": "Visual",
          "Spontaneous/\nIn The Wild": "Spontaneous",
          "Samples": "320 videos",
          "Subjects": "12",
          "Category": "Anger, Contempt, Disgust, Embarrassment, Fear,\nPride, Joy, Neutral, Sad, Surprise"
        }
      ],
      "page": 13
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Database": "BP4D+\n[115]",
          "Year": "2016",
          "Modalities": "Visual",
          "Spontaneous/\nIn The Wild": "Spontaneous",
          "Samples": "about 1.4\nmillion frames",
          "Subjects": "140",
          "Category": "12 Different Categories and 5-point Likert-type\nScales"
        },
        {
          "Database": "KDEF\n[116]",
          "Year": "2018",
          "Modalities": "Visual",
          "Spontaneous/\nIn The Wild": "Spontaneous",
          "Samples": "490 images",
          "Subjects": "272",
          "Category": "Happiness, Sadness, Surprise, Fear, Anger,\nDisgust, Neutral"
        },
        {
          "Database": "AudiofMRI\nParallel\nSet [117]",
          "Year": "2018",
          "Modalities": "fMRI",
          "Spontaneous/\nIn The Wild": "Spontaneous",
          "Samples": "/",
          "Subjects": "18",
          "Category": "Sad, Happy, Excited, Surprise, Neutral, Angry,\nDistress, Frustrated"
        },
        {
          "Database": "F2ED\n[118]",
          "Year": "2019",
          "Modalities": "Visual",
          "Spontaneous/\nIn The Wild": "Spontaneous",
          "Samples": "219719 images",
          "Subjects": "119",
          "Category": "54 Different Facial Expression Classes"
        },
        {
          "Database": "Affwild\n[119]",
          "Year": "2019",
          "Modalities": "Visual",
          "Spontaneous/\nIn The Wild": "In the Wild",
          "Samples": "298 videos",
          "Subjects": "/",
          "Category": "Arousal and Valence"
        },
        {
          "Database": "AFEW\n[37]",
          "Year": "2012",
          "Modalities": "Visual + Audio",
          "Spontaneous/\nIn The Wild": "In the Wild",
          "Samples": "1426 sequences",
          "Subjects": "330",
          "Category": "Happiness, Sadness, Surprise, Fear, Anger,\nDisgust, Neutral"
        },
        {
          "Database": "EMO-DB\n[120]",
          "Year": "2005",
          "Modalities": "Audio + Text",
          "Spontaneous/\nIn The Wild": "Spontaneous",
          "Samples": "500 utterances",
          "Subjects": "10",
          "Category": "Neutral, Anger, Fear, Joy, Sadness, Disgust,\nBoredom"
        },
        {
          "Database": "eNTERF-\nACE’05 [121]",
          "Year": "2006",
          "Modalities": "Visual + Audio",
          "Spontaneous/\nIn The Wild": "Spontaneous",
          "Samples": "1166 sequences",
          "Subjects": "42",
          "Category": "Happiness, Sadness, Surprise, Fear, Anger,\nDisgust, Neutral"
        },
        {
          "Database": "SAVEE[122]",
          "Year": "2008",
          "Modalities": "Visual + Audio",
          "Spontaneous/\nIn The Wild": "Spontaneous",
          "Samples": "480 utterances",
          "Subjects": "4",
          "Category": "Happiness, Sadness, Surprise, Fear, Anger,\nDisgust, Neutral"
        },
        {
          "Database": "VAM [123]",
          "Year": "2008",
          "Modalities": "Audio + Text",
          "Spontaneous/\nIn The Wild": "Spontaneous",
          "Samples": "12 hours of data",
          "Subjects": "47",
          "Category": "Happiness, Surprise, Sadness, Anger, Fear,\nDisgust"
        },
        {
          "Database": "MMI [124]",
          "Year": "2010",
          "Modalities": "Visual + Audio",
          "Spontaneous/\nIn The Wild": "Spontaneous",
          "Samples": "2900 videos",
          "Subjects": "75",
          "Category": "Happiness, Surprise, Sadness, Anger, Fear,\nDisgust"
        },
        {
          "Database": "RECOLA\n[125]",
          "Year": "2013",
          "Modalities": "Visual + Audio",
          "Spontaneous/\nIn The Wild": "Spontaneous",
          "Samples": "1308 utterances",
          "Subjects": "23",
          "Category": "Valence and Arousal"
        }
      ],
      "page": 14
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Database": "CREMA-\nD [126]",
          "Year": "2014",
          "Modalities": "Visual + Audio",
          "Spontaneous/\nIn The Wild": "Spontaneous",
          "Samples": "7442 utterances",
          "Subjects": "91",
          "Category": "Happiness, Surprise, Sadness, Anger, Fear,\nDisgust"
        },
        {
          "Database": "EMOVO\n[127]",
          "Year": "2014",
          "Modalities": "Audio + Text",
          "Spontaneous/\nIn The Wild": "Spontaneous",
          "Samples": "84 sentences",
          "Subjects": "6",
          "Category": "Happiness, Surprise, Sadness, Anger, Fear,\nDisgust"
        },
        {
          "Database": "JTES [128]",
          "Year": "2016",
          "Modalities": "Audio + Text",
          "Spontaneous/\nIn The Wild": "Spontaneous",
          "Samples": "20,000 speech\nsamples",
          "Subjects": "100",
          "Category": "Neutral, Angry, Joy, Sad"
        },
        {
          "Database": "MSP-\nIMPROV\n[129]",
          "Year": "2017",
          "Modalities": "Visual + Audio",
          "Spontaneous/\nIn The Wild": "Spontaneous",
          "Samples": "8348 samples",
          "Subjects": "12",
          "Category": "Sadness, Happiness, Anger, Neutrality"
        },
        {
          "Database": "DREAMER\n[42]",
          "Year": "2017",
          "Modalities": "EEG + ECG",
          "Spontaneous/\nIn The Wild": "Spontaneous",
          "Samples": "/",
          "Subjects": "25",
          "Category": "Arousal and Valence"
        },
        {
          "Database": "RAVDESS\n[39]",
          "Year": "2018",
          "Modalities": "Visual + Audio",
          "Spontaneous/\nIn The Wild": "Spontaneous",
          "Samples": "7356 viseos",
          "Subjects": "24",
          "Category": "Happiness, Sadness, Surprise, Fear, Anger,\nDisgust, Neutral, Contempt"
        },
        {
          "Database": "URDU\n[130]",
          "Year": "2018",
          "Modalities": "Audio + Text",
          "Spontaneous/\nIn The Wild": "Spontaneous",
          "Samples": "400 utterances",
          "Subjects": "38",
          "Category": "Angry, Happy, Sad, Neutral"
        },
        {
          "Database": "Emoti-W\n[131]",
          "Year": "2017",
          "Modalities": "Visual + Audio +\nText",
          "Spontaneous/\nIn The Wild": "In the Wild",
          "Samples": "More than 100\nmovies",
          "Subjects": "/",
          "Category": "Happiness, Sadness, Surprise, Fear, Anger,\nDisgust, Neutral"
        },
        {
          "Database": "MELD\n[132]",
          "Year": "2018",
          "Modalities": "Visual + Audio +\nText",
          "Spontaneous/\nIn The Wild": "In the Wild",
          "Samples": "more than 1400\ndialogues",
          "Subjects": "304",
          "Category": "Happiness, Sadness, Surprise, Fear, Anger,\nDisgust, Neutral"
        },
        {
          "Database": "CMU-\nMOSEI[40]",
          "Year": "2018",
          "Modalities": "Visual + Audio +\nText",
          "Spontaneous/\nIn The Wild": "In the Wild",
          "Samples": "videos and\n23453 sentences",
          "Subjects": "1000",
          "Category": "Happiness, Surprise, Sadness, Anger, Fear,\nDisgust"
        },
        {
          "Database": "OMG [133]",
          "Year": "2018",
          "Modalities": "Visual + Audio +\nText",
          "Spontaneous/\nIn The Wild": "In the Wild",
          "Samples": "7371 utterances",
          "Subjects": "/",
          "Category": "Happiness, Sadness, Surprise, Fear, Anger,\nDisgust, Neutral"
        },
        {
          "Database": "IEMOCAP\n[38]",
          "Year": "2008",
          "Modalities": "Visual + Audio +\nText",
          "Spontaneous/\nIn The Wild": "Spontaneous",
          "Samples": "10039 samples",
          "Subjects": "10",
          "Category": "Arousal and Valence"
        }
      ],
      "page": 15
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Database": "DEAP [41]",
          "Year": "2011",
          "Modalities": "EEG + EOG +\nEMG + GSR",
          "Spontaneous/\nIn The Wild": "Spontaneous",
          "Samples": "/",
          "Subjects": "32",
          "Category": "Arousal and Valence"
        },
        {
          "Database": "UlmTSST\n[134]",
          "Year": "2021",
          "Modalities": "EDA + ECG +\nRESP + BPM",
          "Spontaneous/\nIn The Wild": "Spontaneous",
          "Samples": "/",
          "Subjects": "69",
          "Category": "Arousal and Valence"
        }
      ],
      "page": 16
    },
    {
      "caption": "Table 4: Literature on Generative Models for Data Augmentation in SER.",
      "data": [
        {
          "Reference": "Chatziagapi et al.\n[44]",
          "Year": "2019",
          "Augmentation Domain": "Sample Space",
          "Model": "GAN",
          "Dataset": "IEMOCAP",
          "Performance (%)": "UAR: 53.6"
        },
        {
          "Reference": "Heracleous et al.\n[139]",
          "Year": "2022",
          "Augmentation Domain": "Sample Space",
          "Model": "CycleGAN",
          "Dataset": "RAVDESS/JTES",
          "Performance (%)": "ACC: 77/66.1"
        },
        {
          "Reference": "Wang et al.\n[142]",
          "Year": "2022",
          "Augmentation Domain": "Sample Space",
          "Model": "GAN",
          "Dataset": "IEMOCAP",
          "Performance (%)": "UAR: 58.19"
        },
        {
          "Reference": "Shilandari et al.\n[45]",
          "Year": "2022",
          "Augmentation Domain": "Feature Space",
          "Model": "CycleGAN",
          "Dataset": "EMO-DB/SAVEE/IEMOCAP",
          "Performance (%)": "ACC: 86.32/73.82/61.67"
        },
        {
          "Reference": "Yi and Mak [143]",
          "Year": "2020",
          "Augmentation Domain": "Feature Space",
          "Model": "GAN",
          "Dataset": "EMO-DB/IEMOCAP",
          "Performance (%)": "ACC: 84.49/71.45"
        },
        {
          "Reference": "Latif et al.\n[144]",
          "Year": "2020",
          "Augmentation Domain": "Feature Space",
          "Model": "GAN",
          "Dataset": "IEMOCAP/MSP-IMPROV",
          "Performance (%)": "UAR: 59.6/46.60"
        },
        {
          "Reference": "Sahu et al.\n[146]",
          "Year": "2018",
          "Augmentation Domain": "Feature Space",
          "Model": "GAN + CGAN",
          "Dataset": "IEMOCAP",
          "Performance (%)": "UAR: 60.29"
        },
        {
          "Reference": "He et al.\n[147]",
          "Year": "2020",
          "Augmentation Domain": "Sample Space\n(Cross-modality)",
          "Model": "GAN + VAE",
          "Dataset": "Multi-PIE/CK+/MMI/\nOulu-CASIA",
          "Performance (%)": "ACC: 61.3"
        },
        {
          "Reference": "Ma et al.\n[148]",
          "Year": "C. Zhang, L. Xue: Autoencoder With Emotion Embedding for SER\n2023",
          "Augmentation Domain": "Sample Space\n(Cross-modality)",
          "Model": "GPT-4",
          "Dataset": "IEMOCAP",
          "Performance (%)": "ACC: 68.85"
        },
        {
          "Reference": "Malik et al.\n[149]",
          "Year": "2023",
          "Augmentation Domain": "Sample Space\n(Cross-modality)",
          "Model": "DM",
          "Dataset": "IEMOCAP/MSP-IMPROV/\nCREMA-D/RAVDESS",
          "Performance (%)": "UAR: 61.22"
        }
      ],
      "page": 19
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Reference": "Latif et al.\n[51]",
          "Year": "2017",
          "Model": "VAE",
          "Application": "Feature Extraction",
          "Dataset": "IEMOCAP",
          "Performance (%)": "ACC: 64.93"
        },
        {
          "Reference": "Zhang and Xue [154]",
          "Year": "2021",
          "Model": "AE",
          "Application": "Feature Extraction",
          "Dataset": "EmoDB/IEMOCAP",
          "Performance (%)": "ACC: 95.6/71.2"
        },
        {
          "Reference": "Sahu et al.\n[155]",
          "Year": "2018",
          "Model": "AAE",
          "Application": "Feature Extraction",
          "Dataset": "IEMOCAP",
          "Performance (%)": "UAR: 58.38"
        },
        {
          "Reference": "Ying et al.\n[156]",
          "Year": "2021",
          "Model": "DAE + AAE",
          "Application": "Feature Extraction",
          "Dataset": "IEMOCAP",
          "Performance (%)": "ACC: 76.89"
        },
        {
          "Reference": "Almotlak et al.\n[157]",
          "Year": "2020",
          "Model": "VAE",
          "Application": "Feature Extraction",
          "Dataset": "OMG",
          "Performance (%)": "ACC: 99.86"
        },
        {
          "Reference": "Fonnegra and Díaz [158]",
          "Year": "2018",
          "Model": "DCAE",
          "Application": "Feature Extraction",
          "Dataset": "eNTERFACE’05",
          "Performance (%)": "ACC: 91.4"
        },
        {
          "Reference": "Sahu et al.\n[160]",
          "Year": "2022",
          "Model": "GAN + AE",
          "Application": "Feature Extraction",
          "Dataset": "IEMOCAP/MSP-IMPROV",
          "Performance (%)": "ACC: 45.56"
        },
        {
          "Reference": "Zhao et al.\n[55]",
          "Year": "2020",
          "Model": "GAN",
          "Application": "Semi-Supervised Learning",
          "Dataset": "IEMOCAP",
          "Performance (%)": "UAR: 58.7"
        },
        {
          "Reference": "Chang and Scherer [161]",
          "Year": "2017",
          "Model": "DCGAN",
          "Application": "Semi-Supervised Learning",
          "Dataset": "IEMOCAP",
          "Performance (%)": "ACC: 49.80"
        },
        {
          "Reference": "Deng et al.\n[162]",
          "Year": "2017",
          "Model": "AE",
          "Application": "Semi-Supervised Learning",
          "Dataset": "FAU Aibo",
          "Performance (%)": "UAR: 63.6"
        },
        {
          "Reference": "Xiao et al.\n[163]",
          "Year": "2023",
          "Model": "VAE",
          "Application": "Semi-Supervised Learning",
          "Dataset": "FAU Aibo",
          "Performance (%)": "UAR: 42.9"
        },
        {
          "Reference": "Neumann and Vu [164]",
          "Year": "2019",
          "Model": "AE",
          "Application": "Semi-Supervised Learning",
          "Dataset": "IEMOCAP/MSP-IMPROV",
          "Performance (%)": "UAR: 59.54/45.76"
        },
        {
          "Reference": "Latif et al.\n[34]",
          "Year": "2020",
          "Model": "AAE",
          "Application": "Semi-Supervised Learning",
          "Dataset": "IEMOCAP/MSP-IMPROV",
          "Performance (%)": "ACC: 66.7/60.2"
        },
        {
          "Reference": "Zhou et al.\n[165]",
          "Year": "2018",
          "Model": "VAE",
          "Application": "Semi-Supervised Learning",
          "Dataset": "IEMOCAP",
          "Performance (%)": "ACC: 76.7"
        },
        {
          "Reference": "Nasersharif et al.\n[35]",
          "Year": "2023",
          "Model": "AE",
          "Application": "Cross-domain SER",
          "Dataset": "EMODB/EMOVO",
          "Performance (%)": "ACC: 57.71"
        },
        {
          "Reference": "Xiao et al.\n[59]",
          "Year": "2020",
          "Model": "VAE",
          "Application": "Cross-domain SER",
          "Dataset": "IEMOCAP/MSP-IMPROV",
          "Performance (%)": "UAR: 44.1/56.2"
        },
        {
          "Reference": "Das et al.\n[166]",
          "Year": "2022",
          "Model": "VAE",
          "Application": "Cross-domain SER",
          "Dataset": "IEMOCAP",
          "Performance (%)": "ACC: 52.09"
        },
        {
          "Reference": "Latif et al.\n[167]",
          "Year": "2022",
          "Model": "ADDi",
          "Application": "Cross-domain SER",
          "Dataset": "EmoDB/IEMOCAP",
          "Performance (%)": "UAR: 47.1/49.8"
        },
        {
          "Reference": "Su et al.\n[168]",
          "Year": "2023",
          "Model": "CycleGAN",
          "Application": "Cross-domain SER",
          "Dataset": "IEMOCAP/VAM/MSP-Podcast",
          "Performance (%)": "ACC: 61.64"
        },
        {
          "Reference": "Su and Lee [169]",
          "Year": "2021",
          "Model": "CycleGAN",
          "Application": "Cross-domain SER",
          "Dataset": "IEMOCAP/MSP-IMPROV",
          "Performance (%)": "ACC: 51.13/65.7"
        }
      ],
      "page": 21
    },
    {
      "caption": "Table 6: Literature on Generative Models for Data Augmentation in FER.",
      "data": [
        {
          "Reference": "Porcu et al.\n[46]",
          "Year": "2022",
          "Model": "GAN",
          "Dataset": "CK+",
          "Performance (%)": "ACC: 83.3"
        },
        {
          "Reference": "Zhu et al.\n[182]",
          "Year": "2018",
          "Model": "CycleGAN",
          "Dataset": "FER2013",
          "Performance (%)": "ACC: 94.65"
        },
        {
          "Reference": "Sun et al.\n[180]",
          "Year": "2023",
          "Model": "CGAN",
          "Dataset": "JAFFE/CK+/Oulu-CASIA/KDEF",
          "Performance (%)": "ACC: 98.37/98.10/93.34/98.30"
        },
        {
          "Reference": "Wang et al.\n[183]",
          "Year": "2019",
          "Model": "GAN",
          "Dataset": "F2ED",
          "Performance (%)": "ACC: 44.19"
        },
        {
          "Reference": "Kusunose et al.\n[184]",
          "Year": "2022",
          "Model": "StyleGAN2",
          "Dataset": "CFEE",
          "Performance (%)": "ACC: 82.04"
        },
        {
          "Reference": "Yang et al.\n[186]",
          "Year": "2022",
          "Model": "GAN",
          "Dataset": "FER2013/CK+/JAFFE",
          "Performance (%)": "ACC: 82.1/84.8/91.5"
        },
        {
          "Reference": "Han et al.\n[187]",
          "Year": "2023",
          "Model": "starGAN",
          "Dataset": "CK+/MMI",
          "Performance (%)": "ACC: 99.20/98.14"
        },
        {
          "Reference": "Wang et al.\n[191]",
          "Year": "2020",
          "Model": "starGAN",
          "Dataset": "KDEF/MMI",
          "Performance (%)": "ACC: 95.97/98.30"
        },
        {
          "Reference": "Li et al.\n[47]",
          "Year": "2023",
          "Model": "GAN + VAE",
          "Dataset": "RAF-DB",
          "Performance (%)": "ACC: 74.43"
        },
        {
          "Reference": "Pons et al.\n[192]",
          "Year": "2020",
          "Model": "CycleGAN",
          "Dataset": "USTC-NVIE",
          "Performance (%)": "ACC: 51"
        }
      ],
      "page": 26
    },
    {
      "caption": "Table 8: presents the literature on generative models for FER in noisy environments.",
      "data": [
        {
          "Reference": "Yang et al.\n[52]\nw.",
          "pendencies between frames across segments, which means": "2020",
          "timate for each segment. The MLP consists of several\nlay-\nPerformance (%)": "ers of neurons that can learn non-linear transformations of\nACC: 91.08/95.24/86.33"
        },
        {
          "Reference": "≤\nKhemakhem and Ltifi [197]",
          "pendencies between frames across segments, which means": "type of neural network that can learn",
          "timate for each segment. The MLP consists of several\nlay-\nPerformance (%)": "the input. The MLP can be trained to minimize the error\nACC: 98.14/95.12/85.93\nbetween the prediction vector and the ground truth vector."
        },
        {
          "Reference": "Yang et al.",
          "pendencies between frames across segments, which means": "the relationships and interactions among the features within",
          "timate for each segment. The MLP consists of several\nlay-\nPerformance (%)": "ACC: 97.30/88.0/73.23/84.17\nThe ground truth vector is the values we want to predict for"
        },
        {
          "Reference": "The\nZhang and Tang [199]",
          "pendencies between frames across segments, which means": "transformer",
          "timate for each segment. The MLP consists of several\nlay-\nPerformance (%)": "ACC: 98.04\neach segment. Depending on what kind of challenge we"
        },
        {
          "Reference": "Xie et al.\n[200]",
          "pendencies between frames across segments, which means": "feature vector and applies a series of self-attention layers\n2021",
          "timate for each segment. The MLP consists of several\nlay-\nPerformance (%)": "ACC: 97.53/97.20\nare solving, we have different types of ground truth vectors"
        },
        {
          "Reference": "Ali and Hughes [201]",
          "pendencies between frames across segments, which means": "and feed-forward layers to produce an output\n2019",
          "timate for each segment. The MLP consists of several\nlay-\nPerformance (%)": "ACC: 97.28/89.17/72.97\nand prediction vectors. For the VA challenge, we want\nto"
        },
        {
          "Reference": "Tiwary et al.\n[202]",
          "pendencies between frames across segments, which means": "tor. The output feature vector has more semantic meaning\n2023",
          "timate for each segment. The MLP consists of several\nlay-\nPerformance (%)": "ACC: 94.67/92.57/92.85\npredict two values: valence and arousal. Valence measures"
        },
        {
          "Reference": "Sima et al.\n[203]\nexample,",
          "pendencies between frames across segments, which means": "and representation power than the input feature vector. For\n2021\nthe transformer encoder can learn how different",
          "timate for each segment. The MLP consists of several\nlay-\nPerformance (%)": "how positive or negative an emotion is. Arousal measures"
        },
        {
          "Reference": "Yang et al.\n[204]",
          "pendencies between frames across segments, which means": "2018\nparts of the image relate to each other in each segment of",
          "timate for each segment. The MLP consists of several\nlay-\nPerformance (%)": "ACC: 96.57/88.92/76.83/89.55\nhow active or passive an emotion is. For the Expr challenge,"
        },
        {
          "Reference": "Wang [205]",
          "pendencies between frames across segments, which means": "2021\nthe video. However, the transformer encoder does not con-",
          "timate for each segment. The MLP consists of several\nlay-\nPerformance (%)": "we want to predict eight values: one for each basic expres-"
        },
        {
          "Reference": "Abiram et al.\n[206]",
          "pendencies between frames across segments, which means": "2021\nsider how different segments of the video are connected or",
          "timate for each segment. The MLP consists of several\nlay-\nPerformance (%)": "sion (anger, disgust, fear, happiness, sadness, and surprise)"
        },
        {
          "Reference": "Dharanya et al.\n[207]\ninfluenced by each other.",
          "pendencies between frames across segments, which means": "2021",
          "timate for each segment. The MLP consists of several\nlay-\nPerformance (%)": "plus neutral and other expressions. For the AU challenge,\nACC: 93.6/97.39/96.33"
        },
        {
          "Reference": "Kim et al.\n[208]",
          "pendencies between frames across segments, which means": "2017\nmake some segments overlap with each other so that some",
          "timate for each segment. The MLP consists of several\nlay-\nPerformance (%)": "twelve values: one for each action unit\nACC: 97.93/81.53/86.11"
        },
        {
          "Reference": "Wu et al.",
          "pendencies between frames across segments, which means": "frames are shared by two or more segments. This way, we",
          "timate for each segment. The MLP consists of several\nlay-\nPerformance (%)": "(AU1, AU2, AU4, AU6, AU7, AU10, AU12, AU15, AU23,\nACC: 94.96/86.87/84.81"
        },
        {
          "Reference": "Chatterjee et al.",
          "pendencies between frames across segments, which means": "can capture some information about how different segments",
          "timate for each segment. The MLP consists of several\nlay-\nPerformance (%)": "ACC: 95"
        },
        {
          "Reference": "Zhou et al.",
          "pendencies between frames across segments, which means": "affect each other. The degree of overlap is controlled by two",
          "timate for each segment. The MLP consists of several\nlay-\nPerformance (%)": "CCC: 60.57"
        }
      ],
      "page": 28
    },
    {
      "caption": "Table 8: Literature on Generative Models for FER in Noisy Environments.",
      "data": [
        {
          "Type": "Occlusion and\nComplex Backgrounds",
          "Reference": "Du et al.\n[216]",
          "Year": "2021",
          "Model": "GAN",
          "Dataset": "RAF-DB/FER2013Plus",
          "Performance (%)": "ACC: 87.03/90.07"
        },
        {
          "Type": "",
          "Reference": "Peng et al.\n[217]",
          "Year": "2020",
          "Model": "ACGAN",
          "Dataset": "FER2013",
          "Performance (%)": "ACC: 73"
        },
        {
          "Type": "",
          "Reference": "Lu et al.\n[219]",
          "Year": "2019",
          "Model": "Wasserstein GAN",
          "Dataset": "RAF-DB/AffectNet",
          "Performance (%)": "ACC: 83.49/59.73"
        },
        {
          "Type": "",
          "Reference": "Tang et al.\n[220]",
          "Year": "2019",
          "Model": "CGAN",
          "Dataset": "JAFFE",
          "Performance (%)": "ACC: 80.32"
        },
        {
          "Type": "",
          "Reference": "Li et al.\n[221]",
          "Year": "2021",
          "Model": "GAN",
          "Dataset": "RAF-DB/FER2013Plus",
          "Performance (%)": "ACC: 87.91/88.88"
        },
        {
          "Type": "Non-frontal Views",
          "Reference": "Han and He [222]",
          "Year": "2021",
          "Model": "GAN",
          "Dataset": "KDEF/Multi-PIE",
          "Performance (%)": "ACC: 90.42/92.63"
        },
        {
          "Type": "",
          "Reference": "Zhang et al.\n[223]",
          "Year": "2020",
          "Model": "GAN",
          "Dataset": "Multi-PIE/BU-3DFE",
          "Performance (%)": "ACC: 92.09/81.95"
        },
        {
          "Type": "",
          "Reference": "Lai and Lai\n[224]",
          "Year": "2018",
          "Model": "GAN",
          "Dataset": "Multi-PIE",
          "Performance (%)": "ACC: 86.76"
        },
        {
          "Type": "",
          "Reference": "Li et al.\n[225]",
          "Year": "2019",
          "Model": "GAN",
          "Dataset": "Multi-PIE",
          "Performance (%)": "ACC: 86.9"
        },
        {
          "Type": "",
          "Reference": "Dong et al.\n[226]",
          "Year": "2023",
          "Model": "GAN",
          "Dataset": "KDEF",
          "Performance (%)": "ACC: 92.7"
        },
        {
          "Type": "Other Factors",
          "Reference": "Yang et al.\n[227]",
          "Year": "2021",
          "Model": "GAN",
          "Dataset": "BU-3DFE/BU-4DFE",
          "Performance (%)": "ACC: 88.89/80.03"
        },
        {
          "Type": "",
          "Reference": "Nan et al.\n[228]",
          "Year": "2022",
          "Model": "GAN",
          "Dataset": "RAF-DB/SFEW2.0",
          "Performance (%)": "ACC: 84.09/55.14"
        },
        {
          "Type": "",
          "Reference": "Wang et al.\n[229]",
          "Year": "2024",
          "Model": "DM",
          "Dataset": "UDC-RAF-DB/\nUDC-FERPlus",
          "Performance (%)": "ACC: 88.55/84.89"
        }
      ],
      "page": 31
    },
    {
      "caption": "Table 9: Literature on Generative Models for Dynamic FER.",
      "data": [
        {
          "Reference": "Cai et al.\n[234]",
          "Year": "2023",
          "Model": "MAE",
          "Dataset": "CMU-MOSEI",
          "Performance (%)": "ACC: 80.60"
        },
        {
          "Reference": "Gavade et al.\n[235]",
          "Year": "2023",
          "Model": "DGAN",
          "Dataset": "CK+/RAVDESS/SAVEE",
          "Performance (%)": "ACC: 82.14/80/79"
        },
        {
          "Reference": "Guo et al.\n[236]",
          "Year": "2023",
          "Model": "GAN",
          "Dataset": "SAMM/AFEW",
          "Performance (%)": "ACC: 56.98/60.20/44.72"
        },
        {
          "Reference": "Liong et al.\n[237]",
          "Year": "2020",
          "Model": "GAN",
          "Dataset": "CASME II/SMIC/SAMM",
          "Performance (%)": "ACC: 70.29"
        },
        {
          "Reference": "Li et al.\n[238]",
          "Year": "2021",
          "Model": "GAN",
          "Dataset": "CASME II/SAMM/SMIC",
          "Performance (%)": "UF1: 84.52, UAR: 84.65"
        },
        {
          "Reference": "Mazen et al.\n[239]",
          "Year": "2021",
          "Model": "CycleGAN",
          "Dataset": "FER2013",
          "Performance (%)": "ACC: 91.76"
        }
      ],
      "page": 32
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Reference": "Luo et al.\n[264]",
          "Year": "2018",
          "Model": "CWGAN",
          "Application": "Data Augmentation",
          "Modality": "EEG",
          "Dataset": "SEED",
          "Performance (%)": "ACC: 86.96"
        },
        {
          "Reference": "Bao et al.\n[265]",
          "Year": "2021",
          "Model": "VAE + GAN",
          "Application": "Data Augmentation",
          "Modality": "EEG",
          "Dataset": "SEED/SEED-IV",
          "Performance (%)": "ACC: 92.5/82.3"
        },
        {
          "Reference": "Bhat and Hortal\n[266]",
          "Year": "2021",
          "Model": "Wasserstein GAN",
          "Application": "Data Augmentation",
          "Modality": "EEG",
          "Dataset": "DEAP",
          "Performance (%)": "ACC: 71.50"
        },
        {
          "Reference": "Zhang et al.\n[267]",
          "Year": "2022",
          "Model": "GAN",
          "Application": "Data Augmentation",
          "Modality": "EEG",
          "Dataset": "DEAP/SEED/DREAMER",
          "Performance (%)": "ACC: 94.38/97.71/85.28"
        },
        {
          "Reference": "Zhang et al.\n[268]",
          "Year": "2024",
          "Model": "GAN",
          "Application": "Data Augmentation",
          "Modality": "EEG",
          "Dataset": "DEAP/SEED",
          "Performance (%)": "ACC: 96.68/97.14"
        },
        {
          "Reference": "Haradal et al.\n[269]",
          "Year": "2018",
          "Model": "GAN",
          "Application": "Data Augmentation",
          "Modality": "ECG",
          "Dataset": "ECG 200",
          "Performance (%)": "ACC: 76.20"
        },
        {
          "Reference": "Pan and Zheng [270]",
          "Year": "2021",
          "Model": "GAN",
          "Application": "Data Augmentation",
          "Modality": "EEG",
          "Dataset": "DEAP",
          "Performance (%)": "ACC: 90.26"
        },
        {
          "Reference": "Luo et al.\n[271]",
          "Year": "2020",
          "Model": "GAN",
          "Application": "Data Augmentation",
          "Modality": "EEG",
          "Dataset": "SEED/DEAP",
          "Performance (%)": "ACC: 67.7/50.8"
        },
        {
          "Reference": "Kalashami et al.\n[272]",
          "Year": "2022",
          "Model": "CWGAN",
          "Application": "Data Augmentation",
          "Modality": "EEG",
          "Dataset": "DEAP",
          "Performance (%)": "ACC: 71.9"
        },
        {
          "Reference": "Siddhad et al.\n[273]",
          "Year": "2024",
          "Model": "DM",
          "Application": "Data Augmentation",
          "Modality": "EEG",
          "Dataset": "DEAP",
          "Performance (%)": "ACC: 71.50"
        },
        {
          "Reference": "Tosato et al.\n[274]",
          "Year": "2023",
          "Model": "DM",
          "Application": "Data Augmentation",
          "Modality": "EEG",
          "Dataset": "SEED",
          "Performance (%)": "ACC: 92.98"
        },
        {
          "Reference": "Lan et al.\n[276]",
          "Year": "2017",
          "Model": "AE",
          "Application": "Feature Extraction",
          "Modality": "EEG",
          "Dataset": "SEED",
          "Performance (%)": "ACC: 59.03"
        },
        {
          "Reference": "Rajpoot et al.\n[277]",
          "Year": "2022",
          "Model": "AE",
          "Application": "Feature Extraction",
          "Modality": "EEG",
          "Dataset": "DEAP/SEED/CHB-MIT",
          "Performance (%)": "ACC: 69.5/76.7/72.3"
        },
        {
          "Reference": "Jiarayucharoensak et al.\n[278]",
          "Year": "2014",
          "Model": "SAE",
          "Application": "Feature Extraction",
          "Modality": "EEG",
          "Dataset": "DEAP",
          "Performance (%)": "ACC: 49.52"
        },
        {
          "Reference": "Li et al.\n[279]",
          "Year": "2019",
          "Model": "VAE",
          "Application": "Feature Extraction",
          "Modality": "EEG",
          "Dataset": "DEAP/SEED",
          "Performance (%)": "ACC: 72.43/84.29"
        },
        {
          "Reference": "Bethge et al.\n[280]",
          "Year": "2022",
          "Model": "CAE",
          "Application": "Feature Extraction",
          "Modality": "EEG",
          "Dataset": "SEED",
          "Performance (%)": "ACC: 69"
        },
        {
          "Reference": "Liu et al.\n[281]",
          "Year": "2020",
          "Model": "SAE",
          "Application": "Feature Extraction",
          "Modality": "EEG",
          "Dataset": "DEAP/SEED",
          "Performance (%)": "ACC: 92.86/96.77"
        },
        {
          "Reference": "Qing et al.\n[282]",
          "Year": "2019",
          "Model": "SAE",
          "Application": "Feature Extraction",
          "Modality": "EEG",
          "Dataset": "DEAP/SEED",
          "Performance (%)": "ACC: 63.09/75"
        },
        {
          "Reference": "Li et al.\n[283]",
          "Year": "2020",
          "Model": "VAE",
          "Application": "Feature Extraction",
          "Modality": "EEG",
          "Dataset": "DEAP/SEED",
          "Performance (%)": "ACC: 72.43/84.29"
        },
        {
          "Reference": "Gu et al.\n[284]",
          "Year": "2023",
          "Model": "GAN",
          "Application": "Feature Extraction",
          "Modality": "EEG",
          "Dataset": "DEAP/SEED",
          "Performance (%)": "ACC: 94.78/97.28"
        },
        {
          "Reference": "Zhang et al.\n[285]",
          "Year": "2021",
          "Model": "AE",
          "Application": "Semi-supervised Learning",
          "Modality": "EEG",
          "Dataset": "SEED",
          "Performance (%)": "ACC: 91.17"
        },
        {
          "Reference": "Chai et al.\n[286]",
          "Year": "2016",
          "Model": "AE",
          "Application": "Cross-domain",
          "Modality": "EEG",
          "Dataset": "SEED",
          "Performance (%)": "ACC: 62.50"
        },
        {
          "Reference": "Wang et al.\n[287]",
          "Year": "2022",
          "Model": "VAE",
          "Application": "Cross-domain",
          "Modality": "EEG",
          "Dataset": "SEED/SEED-IV",
          "Performance (%)": "ACC: 89.64/73.82"
        },
        {
          "Reference": "Huang et al.\n[288]",
          "Year": "2022",
          "Model": "AE",
          "Application": "Cross-domain",
          "Modality": "EEG",
          "Dataset": "DEAP",
          "Performance (%)": "ACC: 63.85"
        }
      ],
      "page": 36
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Reference": "Ma et al.\n[140]",
          "Year": "2022",
          "Model": "CGAN",
          "Application": "Data Augmentation",
          "Modalities": "Speech + Image",
          "Dataset": "eNTERFACE’05/RAVDESS/CMEW",
          "Performance (%)": "ACC: 49.48/65.90/46.19"
        },
        {
          "Reference": "Luo et al.\n[294]",
          "Year": "2019",
          "Model": "CGAN",
          "Application": "Data Augmentation",
          "Modalities": "EEG + Eye Movement",
          "Dataset": "SEED/SEED-V",
          "Performance (%)": "ACC: 90.33/68.32"
        },
        {
          "Reference": "Yan et al.\n[296]",
          "Year": "2021",
          "Model": "CGAN",
          "Application": "Data Augmentation",
          "Modalities": "EEG + Eye Movement",
          "Dataset": "SEED/SEED-IV/SEED-V",
          "Performance (%)": "ACC: 93.05/86.55/80.37"
        },
        {
          "Reference": "Chao et al.\n[297]",
          "Year": "2018",
          "Model": "GAN",
          "Application": "Data Augmentation",
          "Modalities": "Speech + fMRI",
          "Dataset": "AudiofMRI Parallel Set/IEMOCAP",
          "Performance (%)": "ACC: 49.58"
        },
        {
          "Reference": "Yan et al.\n[298]",
          "Year": "2017",
          "Model": "AE",
          "Application": "Feature Extraction",
          "Modalities": "EEG + Eye Movement",
          "Dataset": "Self",
          "Performance (%)": "ACC: 64.26"
        },
        {
          "Reference": "Guo et al.\n[300]",
          "Year": "2019",
          "Model": "AE",
          "Application": "Feature Extraction",
          "Modalities": "Eye movements + Eye images + EEG",
          "Dataset": "SEED V",
          "Performance (%)": "ACC: 79.63"
        },
        {
          "Reference": "Zhang [301]",
          "Year": "2020",
          "Model": "DAE",
          "Application": "Feature Extraction",
          "Modalities": "Image + EEG",
          "Dataset": "DEAP",
          "Performance (%)": "ACC: 85.71"
        },
        {
          "Reference": "Peng et al.\n[302]",
          "Year": "2022",
          "Model": "AE",
          "Application": "Feature Extraction",
          "Modalities": "Speech + Text",
          "Dataset": "IEMOCAP/MELD/CMU-MOSI",
          "Performance (%)": "ACC: 74.8/63.64/79.85"
        },
        {
          "Reference": "Hamieh et al.\n[303]",
          "Year": "2021",
          "Model": "AE",
          "Application": "Feature Extraction",
          "Modalities": "Speech + Image + Text",
          "Dataset": "UlmTSST",
          "Performance (%)": "ACC: 72.95"
        },
        {
          "Reference": "Nguyen et al.\n[304]",
          "Year": "2021",
          "Model": "AE",
          "Application": "Feature Extraction",
          "Modalities": "Speech + Image",
          "Dataset": "RECOLA",
          "Performance (%)": "ACC: 47.4"
        },
        {
          "Reference": "Ma et al.\n[305]",
          "Year": "2019",
          "Model": "AE",
          "Application": "Feature Extraction",
          "Modalities": "Speech + Image",
          "Dataset": "eNTERFACE’05",
          "Performance (%)": "ACC: 85.43"
        },
        {
          "Reference": "Zheng et al.\n[306]",
          "Year": "2022",
          "Model": "AE",
          "Application": "Feature Extraction",
          "Modalities": "Speech + Image + Text",
          "Dataset": "IEMOCAP/MSP-IMPROV",
          "Performance (%)": "ACC: 85.5/70.9"
        },
        {
          "Reference": "Du et al.[308]",
          "Year": "2017",
          "Model": "VAE",
          "Application": "Semi-supervised Learning",
          "Modalities": "EEG + Eye Movement",
          "Dataset": "DEAP/SEED",
          "Performance (%)": "ACC: 45.6/96.8"
        },
        {
          "Reference": "Liang et al.\n[309]",
          "Year": "2019",
          "Model": "GAN",
          "Application": "Semi-supervised Learning",
          "Modalities": "Speech + Image",
          "Dataset": "IEMOCAP",
          "Performance (%)": "UAR: 63.98"
        },
        {
          "Reference": "Liu et al.\n[313]",
          "Year": "2024",
          "Model": "AE",
          "Application": "Modality Completion",
          "Modalities": "Speech + Image + Text",
          "Dataset": "IEMOCAP/MSP-IMPROV",
          "Performance (%)": "ACC: 65.13/62.43"
        },
        {
          "Reference": "Wang et al.\n[307]",
          "Year": "2024",
          "Model": "DM",
          "Application": "Modality Completion",
          "Modalities": "Speech + Image",
          "Dataset": "CMU-MOSI/CMU-MOSEI",
          "Performance (%)": "ACC: 79.6/80.9"
        }
      ],
      "page": 39
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "100%\n90%\n80%\n70%\n60%\n50%\n40%\n30%\n20%\n10%\n0%": ""
        }
      ],
      "page": 40
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Affective computing",
      "authors": [
        "R Picard"
      ],
      "year": "2000",
      "venue": "Affective computing"
    },
    {
      "citation_id": "2",
      "title": "Affective computing: challenges",
      "authors": [
        "R Picard"
      ],
      "year": "2003",
      "venue": "International Journal of Human-Computer Studies"
    },
    {
      "citation_id": "3",
      "title": "A review of affective computing: From unimodal analysis to multimodal fusion",
      "authors": [
        "S Poria",
        "E Cambria",
        "R Bajpai",
        "A Hussain"
      ],
      "year": "2017",
      "venue": "Information fusion"
    },
    {
      "citation_id": "4",
      "title": "Affective computing: A review",
      "authors": [
        "J Tao",
        "T Tan"
      ],
      "year": "2005",
      "venue": "International Conference on Affective computing and intelligent interaction"
    },
    {
      "citation_id": "5",
      "title": "Affect detection: An interdisciplinary review of models, methods, and their applications",
      "authors": [
        "R Calvo",
        "S Mello"
      ],
      "year": "2010",
      "venue": "IEEE Transactions on affective computing"
    },
    {
      "citation_id": "6",
      "title": "Automatic emotion recognition in clinical scenario: a systematic review of methods",
      "authors": [
        "L Pepa",
        "L Spalazzi",
        "M Capecci",
        "M Ceravolo"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "7",
      "title": "Affective computing in psychotherapy",
      "authors": [
        "R Khanna",
        "N Robinson",
        "M O'donnell",
        "H Eyre",
        "E Smith"
      ],
      "year": "2022",
      "venue": "Advances in Psychiatry and Behavioral Health"
    },
    {
      "citation_id": "8",
      "title": "Affective computing in education: A systematic review and future research",
      "authors": [
        "E Yadegaridehkordi",
        "N Noor",
        "M Affal",
        "N Hussin"
      ],
      "year": "2019",
      "venue": "Computers & Education"
    },
    {
      "citation_id": "9",
      "title": "Trends in the use of affective computing in e-learning environments, Education and Information Technologies",
      "authors": [
        "N Mejbri",
        "F Essalmi",
        "M Jemni",
        "B Alyoubi"
      ],
      "year": "2022",
      "venue": "Trends in the use of affective computing in e-learning environments, Education and Information Technologies"
    },
    {
      "citation_id": "10",
      "title": "Driver emotion recognition for intelligent vehicles: A survey",
      "authors": [
        "S Zepf",
        "J Hernandez",
        "A Schmitt",
        "W Minker",
        "R Picard"
      ],
      "year": "2020",
      "venue": "ACM Computing Surveys (CSUR)"
    },
    {
      "citation_id": "11",
      "title": "Driver emotion recognition with a hybrid attentional multimodal fusion framework",
      "authors": [
        "L Mou",
        "Y Zhao",
        "C Zhou",
        "B Nakisa",
        "M Rastgoo",
        "L Ma",
        "T Huang",
        "B Yin",
        "R Jain",
        "W Gao"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "12",
      "title": "Artificial empathy in marketing interactions: Bridging the human-ai gap in affective and social customer experience",
      "authors": [
        "Y Liu-Thompkins",
        "S Okazaki",
        "H Li"
      ],
      "year": "2022",
      "venue": "Journal of the Academy of Marketing Science"
    },
    {
      "citation_id": "13",
      "title": "Winning your customers' minds and hearts: disentangling the effects of lock-in and affective customer experience on retention",
      "authors": [
        "L Gao",
        "E De Haan",
        "I Melero-Polo",
        "F Sese"
      ],
      "year": "2023",
      "venue": "Journal of the Academy of Marketing Science"
    },
    {
      "citation_id": "14",
      "title": "Multimodal approaches for emotion recognition: a survey",
      "authors": [
        "N Sebe",
        "I Cohen",
        "T Gevers",
        "T Huang"
      ],
      "year": "2005",
      "venue": "Internet Imaging VI"
    },
    {
      "citation_id": "15",
      "title": "A systematic review on affective computing: Emotion models, databases, and recent advances",
      "authors": [
        "Y Wang",
        "W Song",
        "W Tao",
        "A Liotta",
        "D Yang",
        "X Li",
        "S Gao",
        "Y Sun",
        "W Ge",
        "W Zhang"
      ],
      "year": "2022",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "16",
      "title": "Emotion recognition from unimodal to multimodal analysis: A review",
      "authors": [
        "K Ezzameli",
        "H Mahersia"
      ],
      "year": "2023",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "17",
      "title": "Emotion recognition and artificial intelligence: A systematic review (2014-2023) and research recommendations",
      "authors": [
        "S Khare",
        "V Blanes-Vidal",
        "E Nadimi",
        "U Acharya"
      ],
      "year": "2023",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "18",
      "title": "Speech emotion recognition: Emotional models, databases, features, preprocessing methods, supporting modalities, and classifiers",
      "authors": [
        "M Akçay",
        "K Oğuz"
      ],
      "year": "2020",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "19",
      "title": "A survey on facial emotion recognition techniques: A state-of-the-art literature review",
      "authors": [
        "F Canal",
        "T Müller",
        "J Matias",
        "G Scotton",
        "A De Sa Junior",
        "E Pozzebon",
        "A Sobieranski"
      ],
      "year": "2022",
      "venue": "Information Sciences"
    },
    {
      "citation_id": "20",
      "title": "A survey of textual emotion recognition and its challenges",
      "authors": [
        "J Deng",
        "F Ren"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "21",
      "title": "Emotion recognition from physiological signal analysis: A review",
      "authors": [
        "M Egger",
        "M Ley",
        "S Hanke"
      ],
      "year": "2019",
      "venue": "Electronic Notes in Theoretical Computer Science"
    },
    {
      "citation_id": "22",
      "title": "Multimodal machine learning: A survey and taxonomy",
      "authors": [
        "T Baltrušaitis",
        "C Ahuja",
        "L.-P Morency"
      ],
      "year": "2018",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "23",
      "title": "A systematic survey on multimodal emotion recognition using learning algorithms",
      "authors": [
        "N Ahmed",
        "Z Al Aghbari",
        "S Girija"
      ],
      "year": "2023",
      "venue": "Intelligent Systems with Applications"
    },
    {
      "citation_id": "24",
      "title": "Human emotion recognition: Review of sensors and methods",
      "authors": [
        "A Dzedzickis",
        "A Kaklauskas",
        "V Bucinskas"
      ],
      "year": "2020",
      "venue": "Sensors"
    },
    {
      "citation_id": "25",
      "title": "Machine learning: discriminative and generative",
      "authors": [
        "T Jebara"
      ],
      "year": "2012",
      "venue": "Machine learning: discriminative and generative"
    },
    {
      "citation_id": "26",
      "title": "A survey on generative adversarial networks: Variants, applications, and training",
      "authors": [
        "A Jabbar",
        "X Li",
        "B Omar"
      ],
      "year": "2021",
      "venue": "ACM Computing Surveys (CSUR)"
    },
    {
      "citation_id": "27",
      "title": "Learning representations by back-propagating errors",
      "authors": [
        "D Rumelhart",
        "G Hinton",
        "R Williams"
      ],
      "year": "1986",
      "venue": "nature"
    },
    {
      "citation_id": "28",
      "title": "Generative adversarial nets",
      "authors": [
        "I Goodfellow",
        "J Pouget-Abadie",
        "M Mirza",
        "B Xu",
        "D Warde-Farley",
        "S Ozair",
        "A Courville",
        "Y Bengio"
      ],
      "year": "2014",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "29",
      "title": "Denoising diffusion probabilistic models",
      "authors": [
        "J Ho",
        "A Jain",
        "P Abbeel"
      ],
      "year": "2020",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "30",
      "title": "Language models are few-shot learners",
      "authors": [
        "T Brown",
        "B Mann",
        "N Ryder",
        "M Subbiah",
        "J Kaplan",
        "P Dhariwal",
        "A Neelakantan",
        "P Shyam",
        "G Sastry",
        "A Askell"
      ],
      "year": "2020",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "31",
      "title": "On the opportunities and risks of foundation models",
      "authors": [
        "R Bommasani",
        "D Hudson",
        "E Adeli",
        "R Altman",
        "S Arora",
        "S Arx",
        "M Bernstein",
        "J Bohg",
        "A Bosselut",
        "E Brunskill"
      ],
      "year": "2021",
      "venue": "On the opportunities and risks of foundation models",
      "arxiv": "arXiv:2108.07258"
    },
    {
      "citation_id": "32",
      "title": "Gpt-4 technical report",
      "authors": [
        "J Achiam",
        "S Adler",
        "S Agarwal",
        "L Ahmad",
        "I Akkaya",
        "F Aleman",
        "D Almeida",
        "J Altenschmidt",
        "S Altman",
        "S Anadkat"
      ],
      "year": "2023",
      "venue": "Gpt-4 technical report",
      "arxiv": "arXiv:2303.08774"
    },
    {
      "citation_id": "33",
      "title": "Seemo: A computational approach to see emotions",
      "authors": [
        "Z Liu",
        "A Xu",
        "Y Guo",
        "J Mahmud",
        "H Liu",
        "R Akkiraju"
      ],
      "year": "2018",
      "venue": "Seemo: A computational approach to see emotions",
      "doi": "10.1145/3173574.3173938"
    },
    {
      "citation_id": "34",
      "title": "Multitask semi-supervised adversarial autoencoding for speech emotion recognition",
      "authors": [
        "S Latif",
        "R Rana",
        "S Khalifa",
        "R Jurdak",
        "J Epps",
        "B Schuller"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective computing"
    },
    {
      "citation_id": "35",
      "title": "Multi-layer maximum mean discrepancy in auto-encoders for cross-corpus speech emotion recognition",
      "authors": [
        "B Nasersharif",
        "M Ebrahimpour",
        "N Naderi"
      ],
      "year": "2023",
      "venue": "Journal of Supercomputing",
      "doi": "10.1007/s11227-023-05161-y"
    },
    {
      "citation_id": "36",
      "title": "Challenges in representation learning: A report on three machine learning contests",
      "authors": [
        "I Goodfellow",
        "D Erhan",
        "P Carrier",
        "A Courville",
        "M Mirza",
        "B Hamner",
        "W Cukierski",
        "Y Tang",
        "D Thaler",
        "D.-H Lee"
      ],
      "year": "2013",
      "venue": "Neural Information Processing: 20th International Conference"
    },
    {
      "citation_id": "37",
      "title": "Collecting large, richly annotated facial-expression databases from movies",
      "authors": [
        "A Dhall",
        "R Goecke",
        "S Lucey",
        "T Gedeon"
      ],
      "year": "2012",
      "venue": "IEEE MultiMedia",
      "doi": "10.1109/MMUL.2012.26"
    },
    {
      "citation_id": "38",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "39",
      "title": "The ryerson audio-visual database of emotional speech and song (ravdess): A dynamic, multimodal set of facial and vocal expressions in north american english",
      "authors": [
        "S Livingstone",
        "F Russo"
      ],
      "year": "2018",
      "venue": "PloS one"
    },
    {
      "citation_id": "40",
      "title": "Multimodal language analysis in the wild: Cmu-mosei dataset and interpretable dynamic fusion graph",
      "authors": [
        "A Zadeh",
        "P Liang",
        "S Poria",
        "E Cambria",
        "L.-P Morency"
      ],
      "year": "2018",
      "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "41",
      "title": "Deap: A database for emotion analysis; using physiological signals",
      "authors": [
        "S Koelstra",
        "C Muhl",
        "M Soleymani",
        "J.-S Lee",
        "A Yazdani",
        "T Ebrahimi",
        "T Pun",
        "A Nijholt",
        "I Patras"
      ],
      "year": "2011",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "42",
      "title": "Dreamer: A database for emotion recognition through eeg and ecg signals from wireless low-cost off-the-shelf devices",
      "authors": [
        "S Katsigiannis",
        "N Ramzan"
      ],
      "year": "2017",
      "venue": "IEEE journal of biomedical and health informatics"
    },
    {
      "citation_id": "43",
      "title": "A review: Data pre-processing and data augmentation techniques",
      "authors": [
        "K Maharana",
        "S Mondal",
        "B Nemade"
      ],
      "year": "2022",
      "venue": "Global Transitions Proceedings"
    },
    {
      "citation_id": "44",
      "title": "Data augmentation using gans for speech emotion recognition",
      "authors": [
        "A Chatziagapi",
        "G Paraskevopoulos",
        "D Sgouropoulos",
        "G Pantazopoulos",
        "M Nikandrou",
        "T Giannakopoulos",
        "A Katsamanis",
        "A Potamianos",
        "S Narayanan"
      ],
      "year": "2019",
      "venue": "Data augmentation using gans for speech emotion recognition"
    },
    {
      "citation_id": "45",
      "title": "Effective feature selection in speech emotion recognition systems using generative adversarial networks",
      "year": "2022",
      "venue": "Effective feature selection in speech emotion recognition systems using generative adversarial networks",
      "doi": "10.21203/RS.3.RS-2244414/V1"
    },
    {
      "citation_id": "47",
      "title": "Evaluation of data augmentation techniques for facial expression recognition systems",
      "authors": [
        "S Porcu",
        "A Floris",
        "L Atzori"
      ],
      "year": "2020",
      "venue": "Electronics"
    },
    {
      "citation_id": "48",
      "title": "Semantic data augmentation for long-tailed facial expression recognition",
      "authors": [
        "Z Li",
        "Y Wang",
        "B Guan",
        "J Yin"
      ],
      "year": "2023",
      "venue": "2023 8th International Conference on Computer and Communication Systems (ICCCS)"
    },
    {
      "citation_id": "49",
      "title": "Generative pretrained transformers for emotion detection in a code-switching setting",
      "authors": [
        "A Nedilko"
      ],
      "year": "2023",
      "venue": "Proceedings of the 13th Workshop on Computational Approaches to Subjectivity, Sentiment, & Social Media Analysis"
    },
    {
      "citation_id": "50",
      "title": "Clarin-emo: Training emotion recognition models using human annotation and chatgpt",
      "authors": [
        "B Koptyra",
        "A Ngo",
        "Ł Radliński",
        "J Kocoń"
      ],
      "year": "2023",
      "venue": "International Conference on Computational Science"
    },
    {
      "citation_id": "51",
      "title": "A comprehensive survey and analysis of generative models in machine learning",
      "authors": [
        "G Harshvardhan",
        "M Gourisaria",
        "M Pandey",
        "S Rautaray"
      ],
      "year": "2020",
      "venue": "Computer Science Review"
    },
    {
      "citation_id": "52",
      "title": "Variational autoencoders for learning latent representations of speech emotion: A preliminary study",
      "authors": [
        "S Latif",
        "R Rana",
        "J Qadir",
        "J Epps"
      ],
      "year": "2017",
      "venue": "Variational autoencoders for learning latent representations of speech emotion: A preliminary study"
    },
    {
      "citation_id": "53",
      "title": "A novel feature separation model exchange-gan for facial expression recognition",
      "authors": [
        "L Yang",
        "Y Tian",
        "Y Song",
        "N Yang",
        "K Ma",
        "L Xie"
      ],
      "year": "2020",
      "venue": "Knowledge-Based Systems",
      "doi": "10.1016/J.KNOSYS.2020.106217"
    },
    {
      "citation_id": "54",
      "title": "A survey on semi-supervised learning",
      "authors": [
        "J Van Engelen",
        "H Hoos"
      ],
      "year": "2020",
      "venue": "Machine learning"
    },
    {
      "citation_id": "55",
      "title": "A survey on deep semi-supervised learning",
      "authors": [
        "X Yang",
        "Z Song",
        "I King",
        "Z Xu"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Knowledge and Data Engineering"
    },
    {
      "citation_id": "56",
      "title": "Robust semisupervised generative adversarial networks for speech emotion recognition via distribution smoothness",
      "authors": [
        "H Zhao",
        "Y Xiao",
        "Z Zhang"
      ],
      "year": "2020",
      "venue": "IEEE Access",
      "doi": "10.1109/ACCESS.2020.3000751"
    },
    {
      "citation_id": "57",
      "title": "Emotion interaction recognition based on deep adversarial network in interactive design for intelligent robot",
      "authors": [
        "X Chen",
        "L Xu",
        "H Wei",
        "Z Shang",
        "T Zhang",
        "L Zhang"
      ],
      "year": "2019",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "58",
      "title": "A survey on domain adaptation theory: learning bounds and theoretical guarantees",
      "authors": [
        "I Redko",
        "E Morvant",
        "A Habrard",
        "M Sebban",
        "Y Bennani"
      ],
      "year": "2020",
      "venue": "A survey on domain adaptation theory: learning bounds and theoretical guarantees",
      "arxiv": "arXiv:2004.11829"
    },
    {
      "citation_id": "59",
      "title": "A brief review of domain adaptation, Advances in data science and information engineering: proceedings from ICDATA 2020 and IKE",
      "authors": [
        "A Farahani",
        "S Voghoei",
        "K Rasheed",
        "H Arabnia"
      ],
      "year": "2020",
      "venue": "A brief review of domain adaptation, Advances in data science and information engineering: proceedings from ICDATA 2020 and IKE"
    },
    {
      "citation_id": "60",
      "title": "Learning class-aligned and generalized domaininvariant representations for speech emotion recognition",
      "authors": [
        "Y Xiao",
        "H Zhao",
        "T Li"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Emerging Topics in Computational Intelligence",
      "doi": "10.1109/TETCI.2020.2972926"
    },
    {
      "citation_id": "61",
      "title": "Scalable diffusion models with transformers",
      "authors": [
        "W Peebles",
        "S Xie"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision"
    },
    {
      "citation_id": "62",
      "title": "All are worth words: A vit backbone for diffusion models",
      "authors": [
        "F Bao",
        "S Nie",
        "K Xue",
        "Y Cao",
        "C Li",
        "H Su",
        "J Zhu"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "63",
      "title": "Deep reinforcement learning: A brief survey",
      "authors": [
        "K Arulkumaran",
        "M Deisenroth",
        "M Brundage",
        "A Bharath"
      ],
      "year": "2017",
      "venue": "IEEE Signal Processing Magazine"
    },
    {
      "citation_id": "64",
      "title": "A review of applications in federated learning",
      "authors": [
        "L Li",
        "Y Fan",
        "M Tse",
        "K.-Y Lin"
      ],
      "year": "2020",
      "venue": "Computers & Industrial Engineering"
    },
    {
      "citation_id": "65",
      "title": "Emotion recognition in immersive virtual reality: From statistics to affective computing",
      "authors": [
        "J Marín-Morales",
        "C Llinares",
        "J Guixeres",
        "M Alcañiz"
      ],
      "year": "2020",
      "venue": "Sensors"
    },
    {
      "citation_id": "66",
      "title": "Virtual and augmented reality for developing emotional intelligence skills",
      "authors": [
        "C Papoutsi",
        "A Drigas",
        "C Skianis"
      ],
      "year": "2021",
      "venue": "Int. J. Recent Contrib. Eng. Sci. IT (IJES)"
    },
    {
      "citation_id": "67",
      "title": "Emotional intelligence of large language models",
      "authors": [
        "X Wang",
        "X Li",
        "Z Yin",
        "Y Wu",
        "J Liu"
      ],
      "year": "2023",
      "venue": "Journal of Pacific Rim Psychology"
    },
    {
      "citation_id": "68",
      "title": "Gpt-4v with emotion: a zero-shot benchmark for multimodal emotion understanding",
      "authors": [
        "Z Lian",
        "L Sun",
        "H Sun",
        "K Chen",
        "Z Wen",
        "H Gu",
        "S Chen",
        "B Liu",
        "J Tao"
      ],
      "year": "2023",
      "venue": "Gpt-4v with emotion: a zero-shot benchmark for multimodal emotion understanding",
      "arxiv": "arXiv:2312.04293"
    },
    {
      "citation_id": "69",
      "title": "Generative adversarial networks for face generation: A survey",
      "authors": [
        "A Kammoun",
        "R Slama",
        "H Tabia",
        "T Ouni",
        "M Abid"
      ],
      "year": "2022",
      "venue": "ACM Computing Surveys"
    },
    {
      "citation_id": "70",
      "title": "Generative adversarial networks for speech processing: A review",
      "authors": [
        "A Wali",
        "Z Alamgir",
        "S Karim",
        "A Fawaz",
        "M Ali",
        "M Adan",
        "M Mujtaba"
      ],
      "year": "2022",
      "venue": "Computer Speech & Language"
    },
    {
      "citation_id": "71",
      "title": "Generative adversarial networks in human emotion synthesis: A review",
      "authors": [
        "N Hajarolasvadi",
        "M Ramirez",
        "W Beccaro",
        "H Demirel"
      ],
      "year": "2020",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "72",
      "title": "Survey on facial expression recognition: History, applications, and challenges",
      "authors": [
        "X Zhao",
        "J Zhu",
        "B Luo",
        "Y Gao"
      ],
      "year": "2021",
      "venue": "IEEE MultiMedia"
    },
    {
      "citation_id": "73",
      "title": "New trends in emotion recognition using image analysis by neural networks, a systematic review",
      "authors": [
        "A.-L Cîrneanu",
        "D Popescu",
        "D Iordache"
      ],
      "year": "2023",
      "venue": "Sensors"
    },
    {
      "citation_id": "74",
      "title": "Machine learning for human emotion recognition: a comprehensive review",
      "authors": [
        "E Younis",
        "S Mohsen",
        "E Houssein",
        "O Ibrahim"
      ],
      "year": "2024",
      "venue": "Neural Computing and Applications"
    },
    {
      "citation_id": "75",
      "title": "A survey on data augmentation techniques, in: 2023 7th International Conference on Computing Methodologies and Communication",
      "authors": [
        "K Nanthini",
        "D Sivabalaselvamani",
        "K Chitra",
        "P Gokul",
        "S Kavinkumar",
        "S Kishore"
      ],
      "year": "2023",
      "venue": "A survey on data augmentation techniques, in: 2023 7th International Conference on Computing Methodologies and Communication"
    },
    {
      "citation_id": "76",
      "title": "Deep generative models for synthetic sequential data: A survey",
      "authors": [
        "P Eigenschink",
        "T Reutterer",
        "S Vamosi",
        "R Vamosi",
        "C Sun",
        "K Kalcher"
      ],
      "year": "2023",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "77",
      "title": "Deep generative models: Survey, in: 2018 International conference on intelligent systems and computer vision (ISCV)",
      "authors": [
        "A Oussidi",
        "A Elhassouny"
      ],
      "year": "2018",
      "venue": "Deep generative models: Survey, in: 2018 International conference on intelligent systems and computer vision (ISCV)"
    },
    {
      "citation_id": "78",
      "title": "A survey on generative diffusion models",
      "authors": [
        "H Cao",
        "C Tan",
        "Z Gao",
        "Y Xu",
        "G Chen",
        "P.-A Heng",
        "S Li"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Knowledge and Data Engineering"
    },
    {
      "citation_id": "79",
      "title": "Deep generative modelling: A comparative review of vaes, gans, normalizing flows, energy-based and autoregressive models",
      "authors": [
        "S Bond-Taylor",
        "A Leach",
        "Y Long",
        "C Willcocks"
      ],
      "year": "2021",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "80",
      "title": "Adversarial autoencoders",
      "authors": [
        "A Makhzani",
        "J Shlens",
        "N Jaitly",
        "I Goodfellow",
        "B Frey"
      ],
      "year": "2015",
      "venue": "Adversarial autoencoders",
      "arxiv": "arXiv:1511.05644"
    },
    {
      "citation_id": "81",
      "title": "Auto-encoding variational bayes",
      "authors": [
        "D Kingma",
        "M Welling"
      ],
      "year": "2013",
      "venue": "Auto-encoding variational bayes",
      "arxiv": "arXiv:1312.6114"
    },
    {
      "citation_id": "82",
      "title": "Conditional generative adversarial nets",
      "authors": [
        "M Mirza",
        "S Osindero"
      ],
      "year": "2014",
      "venue": "Conditional generative adversarial nets",
      "arxiv": "arXiv:1411.1784"
    },
    {
      "citation_id": "83",
      "title": "Unsupervised representation learning with deep convolutional generative adversarial networks",
      "authors": [
        "A Radford",
        "L Metz",
        "S Chintala"
      ],
      "year": "2015",
      "venue": "Unsupervised representation learning with deep convolutional generative adversarial networks",
      "arxiv": "arXiv:1511.06434"
    },
    {
      "citation_id": "84",
      "title": "Wasserstein generative adversarial networks",
      "authors": [
        "M Arjovsky",
        "S Chintala",
        "L Bottou"
      ],
      "year": "2017",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "85",
      "title": "Unpaired image-to-image translation using cycle-consistent adversarial networks",
      "authors": [
        "J.-Y Zhu",
        "T Park",
        "P Isola",
        "A Efros"
      ],
      "year": "2017",
      "venue": "Unpaired image-to-image translation using cycle-consistent adversarial networks"
    },
    {
      "citation_id": "86",
      "title": "Attention is all you need, Advances in neural information processing systems",
      "authors": [
        "A Vaswani",
        "N Shazeer",
        "N Parmar",
        "J Uszkoreit",
        "L Jones",
        "A Gomez",
        "Ł Kaiser",
        "I Polosukhin"
      ],
      "year": "2017",
      "venue": "Attention is all you need, Advances in neural information processing systems"
    },
    {
      "citation_id": "87",
      "title": "Improving language understanding by generative pre-training",
      "authors": [
        "A Radford",
        "K Narasimhan",
        "T Salimans",
        "I Sutskever"
      ],
      "year": "2018",
      "venue": "Improving language understanding by generative pre-training"
    },
    {
      "citation_id": "88",
      "title": "Exploring the limits of transfer learning with a unified text-totext transformer",
      "authors": [
        "C Raffel",
        "N Shazeer",
        "A Roberts",
        "K Lee",
        "S Narang",
        "M Matena",
        "Y Zhou",
        "W Li",
        "P Liu"
      ],
      "year": "2020",
      "venue": "Journal of machine learning research"
    },
    {
      "citation_id": "89",
      "title": "Generalized autoregressive pretraining for language understanding",
      "authors": [
        "Z Yang",
        "Z Dai",
        "Y Yang",
        "J Carbonell",
        "R Salakhutdinov",
        "Q Le"
      ],
      "year": "2019",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "90",
      "title": "Llama: Open and efficient foundation language models",
      "authors": [
        "H Touvron",
        "T Lavril",
        "G Izacard",
        "X Martinet",
        "M.-A Lachaux",
        "T Lacroix",
        "B Rozière",
        "N Goyal",
        "E Hambro",
        "F Azhar"
      ],
      "year": "2023",
      "venue": "Llama: Open and efficient foundation language models",
      "arxiv": "arXiv:2302.13971"
    },
    {
      "citation_id": "91",
      "title": "A survey of large language models",
      "authors": [
        "W Zhao",
        "K Zhou",
        "J Li",
        "T Tang",
        "X Wang",
        "Y Hou",
        "Y Min",
        "B Zhang",
        "J Zhang",
        "Z Dong"
      ],
      "year": "2023",
      "venue": "A survey of large language models",
      "arxiv": "arXiv:2303.18223"
    },
    {
      "citation_id": "92",
      "title": "Macro-and micro-expressions facial datasets: A survey",
      "authors": [
        "H Guerdelli",
        "C Ferrari",
        "W Barhoumi",
        "H Ghazouani",
        "S Berretti"
      ],
      "year": "2022",
      "venue": "Sensors"
    },
    {
      "citation_id": "93",
      "title": "International affective picture system (iaps): Technical manual and affective ratings",
      "authors": [
        "P Lang",
        "M Bradley",
        "B Cuthbert"
      ],
      "year": "1997",
      "venue": "International affective picture system (iaps): Technical manual and affective ratings"
    },
    {
      "citation_id": "94",
      "title": "The toronto face dataset",
      "authors": [
        "J Susskind",
        "A Anderson",
        "G Hinton"
      ],
      "year": "2010",
      "venue": "The toronto face dataset"
    },
    {
      "citation_id": "95",
      "title": "Static facial expression analysis in tough conditions: Data, evaluation protocol and benchmark",
      "authors": [
        "A Dhall",
        "R Goecke",
        "S Lucey",
        "T Gedeon"
      ],
      "year": "2011",
      "venue": "IEEE international conference on computer vision workshops (ICCV workshops)"
    },
    {
      "citation_id": "96",
      "title": "Affectnet: A database for facial expression, valence, and arousal computing in the wild",
      "authors": [
        "A Mollahosseini",
        "B Hasani",
        "M Mahoor"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "97",
      "title": "Reliable crowdsourcing and deep locality-preserving learning for unconstrained facial expression recognition",
      "authors": [
        "L Shan",
        "W Deng"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Image Processing"
    },
    {
      "citation_id": "98",
      "title": "Coding facial expressions with gabor wavelets",
      "authors": [
        "M Lyons",
        "S Akamatsu",
        "M Kamachi",
        "J Gyoba"
      ],
      "year": "1998",
      "venue": "Proceedings Third IEEE international conference on automatic face and gesture recognition"
    },
    {
      "citation_id": "99",
      "title": "A 3d facial expression database for facial behavior research",
      "authors": [
        "L Yin",
        "X Wei",
        "Y Sun",
        "J Wang",
        "M Rosato"
      ],
      "year": "2006",
      "venue": "th international conference on automatic face and gesture recognition (FGR06)"
    },
    {
      "citation_id": "100",
      "title": "Taiwanese facial expression image database",
      "authors": [
        "L.-F Chen",
        "Y.-S Yen"
      ],
      "year": "2007",
      "venue": "Taiwanese facial expression image database"
    },
    {
      "citation_id": "101",
      "title": "Multi-pie",
      "authors": [
        "R Gross",
        "I Matthews",
        "J Cohn",
        "T Kanade",
        "S Baker"
      ],
      "year": "2010",
      "venue": "Image and vision computing"
    },
    {
      "citation_id": "102",
      "title": "A high-resolution spontaneous 3d dynamic facial expression database",
      "authors": [
        "X Zhang",
        "L Yin",
        "J Cohn",
        "S Canavan",
        "M Reale",
        "A Horowitz",
        "P Liu"
      ],
      "year": "2013",
      "venue": "2013 10th IEEE international conference and workshops on automatic face and gesture recognition (FG)"
    },
    {
      "citation_id": "103",
      "title": "Releasing a thoroughly annotated and processed spontaneous emotional database: the fau aibo emotion corpus",
      "authors": [
        "A Batliner",
        "S Steidl",
        "E Nöth"
      ],
      "year": "2008",
      "venue": "Releasing a thoroughly annotated and processed spontaneous emotional database: the fau aibo emotion corpus"
    },
    {
      "citation_id": "104",
      "title": "Application of machine learning to epileptic seizure onset detection and treatment",
      "authors": [
        "A Shoeb"
      ],
      "year": "2009",
      "venue": "Application of machine learning to epileptic seizure onset detection and treatment"
    },
    {
      "citation_id": "105",
      "title": "A natural visible and infrared facial expression database for expression recognition and emotion inference",
      "authors": [
        "S Wang",
        "Z Liu",
        "S Lv",
        "Y Lv",
        "G Wu",
        "P Peng",
        "F Chen",
        "X Wang"
      ],
      "year": "2010",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "106",
      "title": "The extended cohn-kanade dataset (ck+): A complete dataset for action unit and emotion-specified expression",
      "authors": [
        "P Lucey",
        "J Cohn",
        "T Kanade",
        "J Saragih",
        "Z Ambadar",
        "I Matthews"
      ],
      "year": "2010",
      "venue": "2010 ieee computer society conference on computer vision and pattern recognition-workshops"
    },
    {
      "citation_id": "107",
      "title": "Faces-a database of facial expressions in young, middle-aged, and older women and men: Development and validation",
      "authors": [
        "N Ebner",
        "M Riediger",
        "U Lindenberger"
      ],
      "year": "2010",
      "venue": "Behavior research methods"
    },
    {
      "citation_id": "108",
      "title": "Facial expression recognition from near-infrared videos",
      "authors": [
        "G Zhao",
        "X Huang",
        "M Taini",
        "S Li",
        "M Pietikäinen"
      ],
      "year": "2011",
      "venue": "Image and vision computing"
    },
    {
      "citation_id": "109",
      "title": "A spontaneous microexpression database: Inducement, collection and baseline",
      "authors": [
        "X Li",
        "T Pfister",
        "X Huang",
        "G Zhao",
        "M Pietikäinen"
      ],
      "year": "2013",
      "venue": "2013 10th IEEE International Conference and Workshops on Automatic face and gesture recognition (fg)"
    },
    {
      "citation_id": "110",
      "title": "Differential entropy feature for eeg-based emotion classification",
      "authors": [
        "R.-N Duan",
        "J.-Y Zhu",
        "B.-L Lu"
      ],
      "year": "2013",
      "venue": "Differential entropy feature for eeg-based emotion classification"
    },
    {
      "citation_id": "111",
      "title": "Compound facial expressions of emotion",
      "authors": [
        "S Du",
        "Y Tao",
        "A Martinez"
      ],
      "year": "2014",
      "venue": "Proceedings of the national academy of sciences"
    },
    {
      "citation_id": "112",
      "title": "Casme ii: An improved spontaneous micro-expression database and the baseline evaluation",
      "authors": [
        "W.-J Yan",
        "X Li",
        "S.-J Wang",
        "G Zhao",
        "Y.-J Liu",
        "Y.-H Chen",
        "X Fu"
      ],
      "year": "2014",
      "venue": "PloS one"
    },
    {
      "citation_id": "113",
      "title": "feature extraction for structural pattern recognition in time-series data",
      "authors": [
        "R Olszewski"
      ],
      "year": "2001",
      "venue": "feature extraction for structural pattern recognition in time-series data"
    },
    {
      "citation_id": "114",
      "title": "Samm: A spontaneous micro-facial movement dataset",
      "authors": [
        "A Davison",
        "C Lansley",
        "N Costen",
        "K Tan",
        "M Yap"
      ],
      "year": "2016",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "115",
      "title": "Validation of the amsterdam dynamic facial expression set-bath intensity variations (adfes-biv): A set of videos expressing low, intermediate, and high intensity emotions",
      "authors": [
        "T Wingenbach",
        "C Ashwin",
        "M Brosnan"
      ],
      "year": "2016",
      "venue": "PloS one"
    },
    {
      "citation_id": "116",
      "title": "Multimodal spontaneous emotion corpus for human behavior analysis",
      "authors": [
        "Z Zhang",
        "J Girard",
        "Y Wu",
        "X Zhang",
        "P Liu",
        "U Ciftci",
        "S Canavan",
        "M Reale",
        "A Horowitz",
        "H Yang"
      ],
      "year": "2016",
      "venue": "Multimodal spontaneous emotion corpus for human behavior analysis"
    },
    {
      "citation_id": "117",
      "title": "Human observers and automated assessment of dynamic emotional facial expressions: Kdef-dyn database validation",
      "authors": [
        "M Calvo",
        "A Fernández-Martín",
        "G Recio",
        "D Lundqvist"
      ],
      "year": "2018",
      "venue": "Frontiers in psychology"
    },
    {
      "citation_id": "118",
      "title": "Toward a universal decoder of linguistic meaning from brain activation",
      "authors": [
        "F Pereira",
        "B Lou",
        "B Pritchett",
        "S Ritter",
        "S Gershman",
        "N Kanwisher",
        "M Botvinick",
        "E Fedorenko"
      ],
      "year": "2018",
      "venue": "Nature communications"
    },
    {
      "citation_id": "119",
      "title": "A finegrained facial expression database for end-to-end multi-pose facial expression recognition",
      "authors": [
        "W Wang",
        "Q Sun",
        "T Chen",
        "C Cao",
        "Z Zheng",
        "G Xu",
        "H Qiu",
        "Y Fu"
      ],
      "year": "2019",
      "venue": "A finegrained facial expression database for end-to-end multi-pose facial expression recognition",
      "arxiv": "arXiv:1907.10838"
    },
    {
      "citation_id": "120",
      "title": "Deep affect prediction in-the-wild: Aff-wild database and challenge, deep architectures, and beyond",
      "authors": [
        "D Kollias",
        "P Tzirakis",
        "M Nicolaou",
        "A Papaioannou",
        "G Zhao",
        "B Schuller",
        "I Kotsia",
        "S Zafeiriou"
      ],
      "year": "2019",
      "venue": "International Journal of Computer Vision"
    },
    {
      "citation_id": "121",
      "title": "A database of german emotional speech",
      "authors": [
        "F Burkhardt",
        "A Paeschke",
        "M Rolfes",
        "W Sendlmeier",
        "B Weiss"
      ],
      "year": "2005",
      "venue": "A database of german emotional speech"
    },
    {
      "citation_id": "122",
      "title": "The enterface'05 audio-visual emotion database",
      "authors": [
        "O Martin",
        "I Kotsia",
        "B Macq",
        "I Pitas"
      ],
      "year": "2006",
      "venue": "22nd international conference on data engineering workshops (ICDEW'06)"
    },
    {
      "citation_id": "123",
      "title": "Audio-visual feature selection and reduction for emotion classification",
      "authors": [
        "S Haq",
        "P Jackson",
        "J Edge"
      ],
      "year": "2008",
      "venue": "Proc. Int. Conf. on Auditory-Visual Speech Processing (AVSP'08)"
    },
    {
      "citation_id": "124",
      "title": "The vera am mittag german audiovisual emotional speech database",
      "authors": [
        "M Grimm",
        "K Kroschel",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "IEEE international conference on multimedia and expo"
    },
    {
      "citation_id": "125",
      "title": "Induced disgust, happiness and surprise: an addition to the mmi facial expression database",
      "authors": [
        "M Valstar",
        "M Pantic"
      ],
      "year": "2010",
      "venue": "Proc. 3rd Intern. Workshop on EMOTION (satellite of LREC"
    },
    {
      "citation_id": "126",
      "title": "Introducing the recola multimodal corpus of remote collaborative and affective interactions",
      "authors": [
        "F Ringeval",
        "A Sonderegger",
        "J Sauer",
        "D Lalanne"
      ],
      "year": "2013",
      "venue": "2013 10th IEEE international conference and workshops on automatic face and gesture recognition (FG)"
    },
    {
      "citation_id": "127",
      "title": "Crema-d: Crowd-sourced emotional multimodal actors dataset",
      "authors": [
        "H Cao",
        "D Cooper",
        "M Keutmann",
        "R Gur",
        "A Nenkova",
        "R Verma"
      ],
      "year": "2014",
      "venue": "Crema-d: Crowd-sourced emotional multimodal actors dataset"
    },
    {
      "citation_id": "128",
      "title": "Emovo corpus: an italian emotional speech database",
      "authors": [
        "G Costantini",
        "I Iaderola",
        "A Paoloni",
        "M Todisco"
      ],
      "year": "2014",
      "venue": "Proceedings of the ninth international conference on language resources and evaluation (LREC'14)"
    },
    {
      "citation_id": "129",
      "title": "Construction and analysis of phonetically and prosodically balanced emotional speech database",
      "authors": [
        "E Takeishi",
        "T Nose",
        "Y Chiba",
        "A Ito"
      ],
      "year": "2016",
      "venue": "Conference of The Oriental Chapter of International Committee for Coordination and Standardization of Speech Databases and Assessment Techniques"
    },
    {
      "citation_id": "130",
      "title": "Msp-improv: An acted corpus of dyadic interactions to study emotion perception",
      "authors": [
        "C Busso",
        "S Parthasarathy",
        "A Burmania",
        "M Abdelwahab",
        "N Sadoughi",
        "E Provost"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing",
      "doi": "10.1109/TAFFC.2016.2515617"
    },
    {
      "citation_id": "131",
      "title": "Cross lingual speech emotion recognition: Urdu vs. western languages",
      "authors": [
        "S Latif",
        "A Qayyum",
        "M Usman",
        "J Qadir"
      ],
      "year": "2018",
      "venue": "2018 International conference on frontiers of information technology (FIT)"
    },
    {
      "citation_id": "132",
      "title": "From individual to group-level emotion recognition: Emotiw 5",
      "authors": [
        "A Dhall",
        "R Goecke",
        "S Ghosh",
        "J Joshi",
        "J Hoey",
        "T Gedeon"
      ],
      "year": "2017",
      "venue": "Proceedings of the 19th ACM international conference on multimodal interaction"
    },
    {
      "citation_id": "133",
      "title": "A multimodal multi-party dataset for emotion recognition in conversations",
      "authors": [
        "S Poria",
        "D Hazarika",
        "N Majumder",
        "G Naik",
        "E Cambria",
        "R Mihalcea"
      ],
      "year": "2018",
      "venue": "A multimodal multi-party dataset for emotion recognition in conversations",
      "arxiv": "arXiv:1810.02508"
    },
    {
      "citation_id": "134",
      "title": "The omg-emotion behavior dataset",
      "authors": [
        "P Barros",
        "N Churamani",
        "E Lakomkin",
        "H Siqueira",
        "A Sutherland",
        "S Wermter"
      ],
      "year": "2018",
      "venue": "2018 International Joint Conference on Neural Networks (IJCNN)"
    },
    {
      "citation_id": "135",
      "title": "The muse 2021 multimodal sentiment analysis challenge: sentiment, emotion, physiological-emotion, and stress",
      "authors": [
        "L Stappen",
        "A Baird",
        "L Christ",
        "L Schumann",
        "B Sertolli",
        "E.-M Messner",
        "E Cambria",
        "G Zhao",
        "B Schuller"
      ],
      "year": "2021",
      "venue": "Proceedings of the 2nd on Multimodal Sentiment Analysis Challenge"
    },
    {
      "citation_id": "136",
      "title": "Speech emotion recognition using deep learning techniques: A review",
      "authors": [
        "R Khalil",
        "E Jones",
        "M Babar",
        "T Jan",
        "M Zafar",
        "T Alhussain"
      ],
      "year": "2019",
      "venue": "IEEE access"
    },
    {
      "citation_id": "137",
      "title": "Data augmentation: A comprehensive survey of modern approaches",
      "authors": [
        "A Mumuni",
        "F Mumuni"
      ],
      "year": "2022",
      "venue": "Array"
    },
    {
      "citation_id": "138",
      "title": "Learning from imbalanced data: open challenges and future directions",
      "authors": [
        "B Krawczyk"
      ],
      "year": "2016",
      "venue": "Progress in artificial intelligence"
    },
    {
      "citation_id": "139",
      "title": "Audio augmentation for speech recognition",
      "authors": [
        "T Ko",
        "V Peddinti",
        "D Povey",
        "S Khudanpur"
      ],
      "year": "2015",
      "venue": "Audio augmentation for speech recognition"
    },
    {
      "citation_id": "140",
      "title": "Applying generative adversarial networks and vision transformers in speech emotion recognition",
      "authors": [
        "P Heracleous",
        "S Fukayama",
        "J Ogata",
        "Y Mohammad"
      ],
      "year": "2022",
      "venue": "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics) 13519 LNCS",
      "doi": "10.1007/978-3-031-17618-0_6"
    },
    {
      "citation_id": "141",
      "title": "Data augmentation for audio-visual emotion recognition with an efficient multimodal conditional gan",
      "authors": [
        "F Ma",
        "Y Li",
        "S Ni",
        "S.-L Huang",
        "L Zhang"
      ],
      "year": "2022",
      "venue": "Applied Sciences"
    },
    {
      "citation_id": "142",
      "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
      "authors": [
        "A Dosovitskiy",
        "L Beyer",
        "A Kolesnikov",
        "D Weissenborn",
        "X Zhai",
        "T Unterthiner",
        "M Dehghani",
        "M Minderer",
        "G Heigold",
        "S Gelly"
      ],
      "year": "2020",
      "venue": "An image is worth 16x16 words: Transformers for image recognition at scale",
      "arxiv": "arXiv:2010.11929"
    },
    {
      "citation_id": "143",
      "title": "Generative data augmentation guided by triplet loss for speech emotion recognition",
      "authors": [
        "S Wang",
        "H Hemati",
        "J Guðnason",
        "D Borth"
      ],
      "year": "2022",
      "venue": "Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH 2022-Septe",
      "doi": "10.21437/Interspeech.2022-10667"
    },
    {
      "citation_id": "144",
      "title": "Improving speech emotion recognition with adversarial data augmentation network",
      "authors": [
        "L Yi",
        "M.-W Mak"
      ],
      "year": "2020",
      "venue": "IEEE transactions on neural networks and learning systems"
    },
    {
      "citation_id": "145",
      "title": "Augmenting generative adversarial networks for speech emotion recognition",
      "authors": [
        "S Latif",
        "M Asim",
        "R Rana",
        "S Khalifa",
        "R Jurdak",
        "B Schuller"
      ],
      "year": "2020",
      "venue": "Augmenting generative adversarial networks for speech emotion recognition",
      "arxiv": "arXiv:2005.08447"
    },
    {
      "citation_id": "146",
      "title": "mixup: Beyond empirical risk minimization, 6th International Conference on Learning Representations, ICLR 2018 -Conference Track Proceedings",
      "authors": [
        "H Zhang",
        "M Cisse",
        "Y Dauphin",
        "D Lopez-Paz"
      ],
      "year": "2017",
      "venue": "mixup: Beyond empirical risk minimization, 6th International Conference on Learning Representations, ICLR 2018 -Conference Track Proceedings"
    },
    {
      "citation_id": "147",
      "title": "On enhancing speech emotion recognition using generative adversarial networks",
      "authors": [
        "S Sahu",
        "R Gupta",
        "C Espy-Wilson"
      ],
      "year": "2018",
      "venue": "On enhancing speech emotion recognition using generative adversarial networks",
      "arxiv": "arXiv:1806.06626"
    },
    {
      "citation_id": "148",
      "title": "Image2audio: Facilitating semi-supervised audio emotion recognition with facial expression image",
      "authors": [
        "G He",
        "X Liu",
        "F Fan",
        "J You"
      ],
      "year": "2020",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops"
    },
    {
      "citation_id": "149",
      "title": "Leveraging speech ptm, text llm, and emotional tts for speech emotion recognition",
      "authors": [
        "Z Ma",
        "W Wu",
        "Z Zheng",
        "Y Guo",
        "Q Chen",
        "S Zhang",
        "X Chen"
      ],
      "year": "2023",
      "venue": "Leveraging speech ptm, text llm, and emotional tts for speech emotion recognition"
    },
    {
      "citation_id": "150",
      "title": "A preliminary study on augmenting speech emotion recognition using a diffusion model",
      "authors": [
        "M Malik",
        "S Latif",
        "R Jurdak",
        "B Schuller"
      ],
      "year": "2023",
      "venue": "Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH 2023-August",
      "doi": "10.21437/Interspeech.2023-1080"
    },
    {
      "citation_id": "151",
      "title": "Pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "J Devlin",
        "M.-W Chang",
        "K Lee",
        "K Toutanova"
      ],
      "year": "2018",
      "venue": "Pre-training of deep bidirectional transformers for language understanding",
      "arxiv": "arXiv:1810.04805"
    },
    {
      "citation_id": "152",
      "title": "Comparison of parametric representations for monosyllabic word recognition in continuously spoken sentences",
      "authors": [
        "S Davis",
        "P Mermelstein"
      ],
      "year": "1980",
      "venue": "IEEE transactions on acoustics, speech, and signal processing"
    },
    {
      "citation_id": "153",
      "title": "A cnn-assisted enhanced audio signal processing for speech emotion recognition",
      "authors": [
        "S Mustaqeem",
        "Kwon"
      ],
      "year": "2019",
      "venue": "Sensors"
    },
    {
      "citation_id": "154",
      "title": "Survey of deep representation learning for speech emotion recognition",
      "authors": [
        "S Latif",
        "R Rana",
        "S Khalifa",
        "R Jurdak",
        "J Qadir",
        "B Schuller"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "155",
      "title": "Autoencoder with emotion embedding for speech emotion recognition",
      "authors": [
        "C Zhang",
        "L Xue"
      ],
      "year": "2021",
      "venue": "IEEE Access",
      "doi": "10.1109/ACCESS.2021.3069818"
    },
    {
      "citation_id": "156",
      "title": "Adversarial auto-encoders for speech based emotion recognition",
      "authors": [
        "S Sahu",
        "R Gupta",
        "G Sivaraman",
        "W Abdalmageed",
        "C Espy-Wilson"
      ],
      "year": "2018",
      "venue": "Adversarial auto-encoders for speech based emotion recognition",
      "arxiv": "arXiv:1806.02146"
    },
    {
      "citation_id": "157",
      "title": "Unsupervised feature learning for speech emotion recognition based on autoencoder",
      "authors": [
        "Y Ying",
        "Y Tu",
        "H Zhou"
      ],
      "year": "2021",
      "venue": "Electronics"
    },
    {
      "citation_id": "158",
      "title": "Variational autoencoder with global-and medium timescale auxiliaries for emotion recognition from speech",
      "authors": [
        "H Almotlak",
        "C Weber",
        "L Qu",
        "S Wermter"
      ],
      "year": "2020",
      "venue": "Artificial Neural Networks and Machine Learning-ICANN 2020: 29th International Conference on Artificial Neural Networks"
    },
    {
      "citation_id": "159",
      "title": "Speech emotion recognition integrating paralinguistic features and auto-encoders in a deep learning model",
      "authors": [
        "R Fonnegra",
        "G Díaz"
      ],
      "year": "2018",
      "venue": "International Conference on Human-Computer Interaction"
    },
    {
      "citation_id": "160",
      "title": "The interspeech 2010 paralinguistic challenge",
      "authors": [
        "B Schuller",
        "S Steidl",
        "A Batliner",
        "F Burkhardt",
        "L Devillers",
        "C Müller",
        "S Narayanan"
      ],
      "year": "2010",
      "venue": "Proc. IN-TERSPEECH 2010, Makuhari, Japan"
    },
    {
      "citation_id": "161",
      "title": "Modeling feature representations for affective speech using generative adversarial networks",
      "authors": [
        "S Sahu",
        "R Gupta",
        "C Espy-Wilson"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing",
      "doi": "10.1109/TAFFC.2020.2998118"
    },
    {
      "citation_id": "162",
      "title": "Learning representations of emotional speech with deep convolutional generative adversarial networks",
      "authors": [
        "J Chang",
        "S Scherer"
      ],
      "year": "2017",
      "venue": "2017 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "163",
      "title": "Semisupervised autoencoders for speech emotion recognition",
      "authors": [
        "J Deng",
        "X Xu",
        "Z Zhang",
        "S Frühholz",
        "B Schuller"
      ],
      "year": "2017",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "164",
      "title": "Speech emotion recognition based on semisupervised adversarial variational autoencoder",
      "authors": [
        "Y Xiao",
        "Y Bo",
        "Z Zheng"
      ],
      "year": "2023",
      "venue": "Proceedings -2023 IEEE 10th International Conference on Cyber Security and Cloud Computing and 2023 IEEE 9th International Conference on Edge Computing and Scalable Cloud, CSCloud-EdgeCom 2023",
      "doi": "10.1109/CSCLOUD-EDGECOM58631.2023.00054"
    },
    {
      "citation_id": "165",
      "title": "Improving speech emotion recognition with unsupervised representation learning on unlabeled speech",
      "authors": [
        "M Neumann",
        "N Vu"
      ],
      "year": "2019",
      "venue": "ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "166",
      "title": "Inferring emotion from conversational voice data: A semi-supervised multi-path generative neural network approach",
      "authors": [
        "S Zhou",
        "J Jia",
        "Q Wang",
        "Y Dong",
        "Y Yin",
        "K Lei"
      ],
      "year": "2018",
      "venue": "Proceedings of the AAAI conference on artificial intelligence"
    },
    {
      "citation_id": "167",
      "title": "Towards transferable speech emotion representation: On loss functions for cross-lingual latent representations",
      "authors": [
        "S Das",
        "N Lønfeldt",
        "A Pagsberg",
        "L Clemmensen"
      ],
      "year": "2022",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing -Proceedings",
      "doi": "10.1109/ICASSP43922.2022.9746450"
    },
    {
      "citation_id": "168",
      "title": "Self supervised adversarial domain adaptation for cross-corpus and cross-language speech emotion recognition",
      "authors": [
        "S Latif",
        "R Rana",
        "S Khalifa",
        "R Jurdak",
        "B Schuller"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "169",
      "title": "Unsupervised cross-corpus speech emotion recognition using a multi-source cycle-gan",
      "authors": [
        "B Su",
        "C Lee"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Affective Computing",
      "doi": "10.1109/TAFFC.2022.3146325"
    },
    {
      "citation_id": "170",
      "title": "A conditional cycle emotion gan for cross corpus speech emotion recognition",
      "authors": [
        "B.-H Su",
        "C.-C Lee"
      ],
      "year": "2021",
      "venue": "2021 IEEE Spoken Language Technology Workshop (SLT)"
    },
    {
      "citation_id": "171",
      "title": "Semi-supervised speech emotion recognition with ladder networks",
      "authors": [
        "S Parthasarathy",
        "C Busso"
      ],
      "year": "2020",
      "venue": "IEEE/ACM transactions on audio, speech, and language processing"
    },
    {
      "citation_id": "172",
      "title": "Explaining and harnessing adversarial examples",
      "authors": [
        "I Goodfellow",
        "J Shlens",
        "C Szegedy"
      ],
      "year": "2014",
      "venue": "Explaining and harnessing adversarial examples",
      "arxiv": "arXiv:1412.6572"
    },
    {
      "citation_id": "173",
      "title": "Crafting adversarial examples for speech paralinguistics applications",
      "authors": [
        "Y Gong",
        "C Poellabauer"
      ],
      "year": "2017",
      "venue": "Crafting adversarial examples for speech paralinguistics applications",
      "doi": "http://arxiv.org/abs/1711.0328010.1145/3306195.3306196"
    },
    {
      "citation_id": "174",
      "title": "Staa-net: A sparse and transferable adversarial attack for speech emotion recognition",
      "authors": [
        "Y Chang",
        "Z Ren",
        "Z Zhang",
        "X Jing",
        "K Qian",
        "X Shao",
        "B Hu",
        "T Schultz",
        "B Schuller"
      ],
      "year": "2024",
      "venue": "Staa-net: A sparse and transferable adversarial attack for speech emotion recognition",
      "arxiv": "arXiv:2402.01227"
    },
    {
      "citation_id": "175",
      "title": "Wavenet: A generative model for raw audio",
      "authors": [
        "A Van Den",
        "S Oord",
        "H Dieleman",
        "K Zen",
        "O Simonyan",
        "A Vinyals",
        "N Graves",
        "A Kalchbrenner",
        "K Senior",
        "Kavukcuoglu"
      ],
      "year": "2016",
      "venue": "Wavenet: A generative model for raw audio",
      "arxiv": "arXiv:1609.0349912"
    },
    {
      "citation_id": "176",
      "title": "Targeted speech adversarial example generation with generative adversarial network",
      "authors": [
        "D Wang",
        "L Dong",
        "R Wang",
        "D Yan",
        "J Wang"
      ],
      "year": "2020",
      "venue": "IEEE Access",
      "doi": "10.1109/ACCESS.2020.3006130"
    },
    {
      "citation_id": "177",
      "title": "Adversarial machine learning and speech emotion recognition: Utilizing generative adversarial networks for robustness",
      "authors": [
        "S Latif",
        "R Rana",
        "J Qadir"
      ],
      "year": "2018",
      "venue": "Adversarial machine learning and speech emotion recognition: Utilizing generative adversarial networks for robustness"
    },
    {
      "citation_id": "178",
      "title": "Robust federated learning against adversarial attacks for speech emotion recognition",
      "authors": [
        "Y Chang",
        "S Laridi",
        "Z Ren",
        "G Palmer",
        "B Schuller",
        "M Fisichella"
      ],
      "year": "2022",
      "venue": "Robust federated learning against adversarial attacks for speech emotion recognition"
    },
    {
      "citation_id": "179",
      "title": "Deep facial expression recognition: A survey",
      "authors": [
        "S Li",
        "W Deng"
      ],
      "year": "2020",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "180",
      "title": "A survey of ai-based facial emotion recognition: Features, ml & dl techniques, age-wise datasets and future directions",
      "authors": [
        "C Dalvi",
        "M Rathod",
        "S Patil",
        "S Gite",
        "K Kotecha"
      ],
      "year": "2021",
      "venue": "Ieee Access"
    },
    {
      "citation_id": "181",
      "title": "A discriminatively deep fusion approach with improved conditional gan (im-cgan) for facial expression recognition",
      "authors": [
        "Z Sun",
        "H Zhang",
        "J Bai",
        "M Liu",
        "Z Hu"
      ],
      "year": "2023",
      "venue": "Pattern Recognition",
      "doi": "10.1016/J.PATCOG.2022.109157"
    },
    {
      "citation_id": "182",
      "title": "Very deep convolutional networks for large-scale image recognition",
      "authors": [
        "K Simonyan",
        "A Zisserman"
      ],
      "year": "2015",
      "venue": "International Conference on Learning Representations"
    },
    {
      "citation_id": "183",
      "title": "Emotion classification with data augmentation using generative adversarial networks",
      "authors": [
        "X Zhu",
        "Y Liu",
        "J Li",
        "T Wan",
        "Z Qin"
      ],
      "year": "2018",
      "venue": "Advances in Knowledge Discovery and Data Mining: 22nd Pacific-Asia Conference"
    },
    {
      "citation_id": "184",
      "title": "Comp-gan: Compositional generative adversarial network in synthesizing and recognizing facial expression",
      "authors": [
        "W Wang",
        "Q Sun",
        "Y Fu",
        "T Chen",
        "C Cao",
        "Z Zheng",
        "G Xu",
        "H Qiu",
        "Y Jiang",
        "X Xue"
      ],
      "year": "2019",
      "venue": "MM 2019 -Proceedings of the 27th ACM International Conference on Multimedia",
      "doi": "10.1145/3343031.3351032"
    },
    {
      "citation_id": "185",
      "title": "Facial expression emotion recognition based on transfer learning and generative model",
      "authors": [
        "T Kusunose",
        "X Kang",
        "K Kiuchi",
        "R Nishimura",
        "M Sasayama",
        "K Matsumoto"
      ],
      "year": "2022",
      "venue": "ICSAI 2022 -8th International Conference on Systems and Informatics",
      "doi": "10.1109/ICSAI57119.2022.10005478"
    },
    {
      "citation_id": "186",
      "title": "Analyzing and improving the image quality of stylegan",
      "authors": [
        "T Karras",
        "S Laine",
        "M Aittala",
        "J Hellsten",
        "J Lehtinen",
        "T Aila"
      ],
      "year": "2020",
      "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "187",
      "title": "Ee-gan:facial expression recognition method based on generative adversarial network and network integration",
      "authors": [
        "D Yang",
        "S Huang",
        "S Wang",
        "P Zhai",
        "Y Li",
        "L Zhang"
      ],
      "year": "2022",
      "venue": "Journal of Computer Applications",
      "doi": "10.11772/j.issn.1001-9081.2021040807"
    },
    {
      "citation_id": "188",
      "title": "The facial expression data enhancement method induced by improved stargan v2, Symmetry",
      "authors": [
        "B Han",
        "M Hu"
      ],
      "year": "2023",
      "venue": "The facial expression data enhancement method induced by improved stargan v2, Symmetry",
      "doi": "10.3390/SYM15040956"
    },
    {
      "citation_id": "189",
      "title": "Going deeper with convolutions",
      "authors": [
        "C Szegedy",
        "W Liu",
        "Y Jia",
        "P Sermanet",
        "S Reed",
        "D Anguelov",
        "D Erhan",
        "V Vanhoucke",
        "A Rabinovich"
      ],
      "year": "2015",
      "venue": "Going deeper with convolutions"
    },
    {
      "citation_id": "190",
      "title": "Squeeze-and-excitation networks",
      "authors": [
        "J Hu",
        "L Shen",
        "G Sun"
      ],
      "year": "2018",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "191",
      "title": "Saanet: Siamese actionunits attention network for improving dynamic facial expression recognition",
      "authors": [
        "D Liu",
        "X Ouyang",
        "S Xu",
        "P Zhou",
        "K He",
        "S Wen"
      ],
      "year": "2020",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "192",
      "title": "Laun improved stargan for facial emotion recognition",
      "authors": [
        "X Wang",
        "J Gong",
        "M Hu",
        "Y Gu",
        "F Ren"
      ],
      "year": "2020",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "193",
      "title": "Et-cyclegan: Generating thermal images from images in the visible spectrum for facial emotion recognition",
      "authors": [
        "G Pons",
        "A Ali",
        "P Cesar"
      ],
      "year": "2020",
      "venue": "Companion publication of the 2020 international conference on multimodal interaction"
    },
    {
      "citation_id": "194",
      "title": "Facial action coding system, Environmental Psychology & Nonverbal Behavior",
      "authors": [
        "P Ekman",
        "W Friesen"
      ],
      "year": "1978",
      "venue": "Facial action coding system, Environmental Psychology & Nonverbal Behavior"
    },
    {
      "citation_id": "195",
      "title": "Deep-emotion: Facial expression recognition using attentional convolutional network",
      "authors": [
        "S Minaee",
        "M Minaei",
        "A Abdolrashidi"
      ],
      "year": "2021",
      "venue": "Sensors"
    },
    {
      "citation_id": "196",
      "title": "A brief review of facial emotion recognition based on visual information",
      "authors": [
        "B Ko"
      ],
      "year": "2018",
      "venue": "sensors"
    },
    {
      "citation_id": "197",
      "title": "Occlusion aware facial expression recognition using cnn with attention mechanism",
      "authors": [
        "Y Li",
        "J Zeng",
        "S Shan",
        "X Chen"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Image Processing"
    },
    {
      "citation_id": "198",
      "title": "Neural style transfer generative adversarial network (nst-gan) for facial expression recognition",
      "authors": [
        "F Khemakhem",
        "H Ltifi"
      ],
      "year": "2023",
      "venue": "International Journal of Multimedia Information Retrieval",
      "doi": "10.1007/s13735-023-00285-6"
    },
    {
      "citation_id": "199",
      "title": "Facial expression recognition by de-expression residue learning",
      "authors": [
        "H Yang",
        "U Ciftci",
        "L Yin"
      ],
      "year": "2018",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "200",
      "title": "An efficacious method for facial expression recognition: Gan erased facial feature network (ge2fn)",
      "authors": [
        "T Zhang",
        "K Tang"
      ],
      "year": "2021",
      "venue": "Proceedings of the 2021 13th International Conference on Machine Learning and Computing"
    },
    {
      "citation_id": "201",
      "title": "Facial expression recognition with two-branch disentangled generative adversarial network",
      "authors": [
        "S Xie",
        "H Hu",
        "Y Chen"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Circuits and Systems for Video Technology",
      "doi": "10.1109/TCSVT.2020.3024201"
    },
    {
      "citation_id": "202",
      "title": "Facial expression recognition using disentangled adversarial learning",
      "authors": [
        "K Ali",
        "C Hughes"
      ],
      "year": "2019",
      "venue": "Facial expression recognition using disentangled adversarial learning",
      "arxiv": "arXiv:1909.13135"
    },
    {
      "citation_id": "203",
      "title": "Facial expression recognition using expression generative adversarial network and attention cnn",
      "authors": [
        "G Tiwary",
        "S Chauhan",
        "K Goyal"
      ],
      "year": "2023",
      "venue": "International Journal of Intelligent Systems and Applications in Engineering"
    },
    {
      "citation_id": "204",
      "title": "Automatic expression recognition of face image sequence based on key-frame generation and differential emotion feature",
      "authors": [
        "Y Sima",
        "J Yi",
        "A Chen",
        "Z Jin"
      ],
      "year": "2021",
      "venue": "Applied Soft Computing"
    },
    {
      "citation_id": "205",
      "title": "Identity-adaptive facial expression recognition through expression regeneration using conditional generative adversarial networks",
      "authors": [
        "H Yang",
        "Z Zhang",
        "L Yin"
      ],
      "year": "2018",
      "venue": "2018 13th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2018)"
    },
    {
      "citation_id": "206",
      "title": "Improved facial expression recognition method based on gan, Scientific Programming 2021",
      "authors": [
        "J Wang"
      ],
      "year": "2021",
      "venue": "Improved facial expression recognition method based on gan, Scientific Programming 2021"
    },
    {
      "citation_id": "207",
      "title": "Identity preserving multi-pose facial expression recognition using fine tuned vgg on the latent space vector of generative adversarial network",
      "authors": [
        "R Abiram",
        "P Vincent",
        "P Vincent"
      ],
      "year": "2021",
      "venue": "Math. Biosci. Eng"
    },
    {
      "citation_id": "208",
      "title": "Facial expression recognition through person-wise regeneration of expressions using auxiliary classifier generative adversarial network (ac-gan) based model",
      "authors": [
        "V Dharanya",
        "A Raj",
        "V Gopi"
      ],
      "year": "2021",
      "venue": "Journal of Visual Communication and Image Representation"
    },
    {
      "citation_id": "209",
      "title": "Deep generative-contrastive networks for facial expression recognition",
      "authors": [
        "Y Kim",
        "B Yoo",
        "Y Kwak",
        "C Choi",
        "J Kim"
      ],
      "year": "2017",
      "venue": "Deep generative-contrastive networks for facial expression recognition",
      "arxiv": "arXiv:1703.07140"
    },
    {
      "citation_id": "210",
      "title": "Cross-vae: Towards disentangling expression from identity for human faces",
      "authors": [
        "H Wu",
        "J Jia",
        "L Xie",
        "G Qi",
        "Y Shi",
        "Q Tian"
      ],
      "year": "2020",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing -Proceedings",
      "doi": "10.1109/ICASSP40776.2020.9053608"
    },
    {
      "citation_id": "211",
      "title": "Improving facial emotion recognition using residual autoencoder coupled affinity based overlapping reduction",
      "authors": [
        "S Chatterjee",
        "A Das",
        "J Nayak",
        "D Pelusi"
      ],
      "year": "2022",
      "venue": "Mathematics"
    },
    {
      "citation_id": "212",
      "title": "Majority biased facial emotion recognition using residual variational autoencoders",
      "authors": [
        "S Chatterjee",
        "S Maity",
        "K Ghosh",
        "A Das",
        "S Banerjee"
      ],
      "year": "2024",
      "venue": "Multimedia Tools and Applications"
    },
    {
      "citation_id": "213",
      "title": "Enhancing emotion recognition with pre-trained masked autoencoders and sequential learning",
      "authors": [
        "W Zhou",
        "J Lu",
        "C Ling",
        "W Wang",
        "S Liu"
      ],
      "year": "2024",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "214",
      "title": "Unsupervised domain adaptation for facial expression recognition using generative adversarial networks, Computational intelligence and neuroscience",
      "authors": [
        "X Wang",
        "X Wang",
        "Y Ni"
      ],
      "year": "2018",
      "venue": "Unsupervised domain adaptation for facial expression recognition using generative adversarial networks, Computational intelligence and neuroscience"
    },
    {
      "citation_id": "215",
      "title": "Unsupervised domain adaptation with generative adversarial networks for facial emotion recognition",
      "authors": [
        "Y Fan",
        "J Lam",
        "V Li"
      ],
      "year": "2018",
      "venue": "2018 IEEE International Conference on Big Data (Big Data)"
    },
    {
      "citation_id": "216",
      "title": "Facial expression recognition in the wild: A cycle-consistent adversarial attention transfer approach",
      "authors": [
        "F Zhang",
        "T Zhang",
        "Q Mao",
        "L Duan",
        "C Xu"
      ],
      "year": "2018",
      "venue": "Proceedings of the 26th ACM international conference on Multimedia"
    },
    {
      "citation_id": "217",
      "title": "Learning associative representation for facial expression recognition",
      "authors": [
        "Y Du",
        "D Yang",
        "P Zhai",
        "M Li",
        "L Zhang"
      ],
      "year": "2021",
      "venue": "2021 IEEE International Conference on Image Processing"
    },
    {
      "citation_id": "218",
      "title": "Emotion recognition using generative adversarial networks",
      "authors": [
        "Z Peng",
        "J Li",
        "Z Sun"
      ],
      "year": "2020",
      "venue": "2020 International Conference on Computer Engineering and Intelligent Control (ICCEIC)"
    },
    {
      "citation_id": "219",
      "title": "Conditional image synthesis with auxiliary classifier gans",
      "authors": [
        "A Odena",
        "C Olah",
        "J Shlens"
      ],
      "year": "2017",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "220",
      "title": "Wgan-based robust occluded facial expression recognition",
      "authors": [
        "Y Lu",
        "S Wang",
        "W Zhao",
        "Y Zhao"
      ],
      "year": "2019",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "221",
      "title": "Expression conditional gan for facial expression-to-expression translation",
      "authors": [
        "H Tang",
        "W Wang",
        "S Wu",
        "X Chen",
        "D Xu",
        "N Sebe",
        "Y Yan"
      ],
      "year": "2019",
      "venue": "IEEE international conference on image processing (ICIP)"
    },
    {
      "citation_id": "222",
      "title": "Mask vision transformer for facial expression recognition in the wild",
      "authors": [
        "H Li",
        "M Sui",
        "F Zhao",
        "Z Zha",
        "F Wu"
      ],
      "year": "2021",
      "venue": "Mask vision transformer for facial expression recognition in the wild"
    },
    {
      "citation_id": "223",
      "title": "Gan based three-stage-training algorithm for multi-view facial expression recognition",
      "authors": [
        "Z Han",
        "H Huang"
      ],
      "year": "2021",
      "venue": "Neural Processing Letters",
      "doi": "10.1007/s11063-021-10591-x"
    },
    {
      "citation_id": "224",
      "title": "Geometry guided pose-invariant facial expression recognition",
      "authors": [
        "F Zhang",
        "T Zhang",
        "Q Mao",
        "C Xu"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Image Processing"
    },
    {
      "citation_id": "225",
      "title": "Emotion-preserving representation learning via generative adversarial network for multi-view facial expression recognition",
      "authors": [
        "Y.-H Lai",
        "S.-H Lai"
      ],
      "year": "2018",
      "venue": "2018 13th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2018)"
    },
    {
      "citation_id": "226",
      "title": "Multi-pose facial expression recognition based on generative adversarial network",
      "authors": [
        "D Li",
        "Z Li",
        "R Luo",
        "J Deng",
        "S Sun"
      ],
      "year": "2019",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "227",
      "title": "A multi-view face expression recognition method based on densenet and gan",
      "authors": [
        "J Dong",
        "Y Zhang",
        "L Fan"
      ],
      "year": "2023",
      "venue": "Electronics",
      "doi": "10.3390/ELECTRONICS12112527"
    },
    {
      "citation_id": "228",
      "title": "Intensity enhancement via gan for multimodal face expression recognition",
      "authors": [
        "H Yang",
        "K Zhu",
        "D Huang",
        "H Li",
        "Y Wang",
        "L Chen"
      ],
      "year": "2021",
      "venue": "Neurocomputing",
      "doi": "10.1016/J.NEUCOM.2021.05.022"
    },
    {
      "citation_id": "229",
      "title": "Feature super-resolution based facial expression recognition for multi-scale low-resolution images",
      "authors": [
        "F Nan",
        "W Jing",
        "F Tian",
        "J Zhang",
        "K Chao",
        "Z Hong",
        "Q Zheng"
      ],
      "year": "2022",
      "venue": "Knowledge-Based Systems",
      "doi": "10.1016/J.KNOSYS.2021.107678"
    },
    {
      "citation_id": "230",
      "title": "Diffusion models for underdisplay camera emotion recognition",
      "authors": [
        "Z Wang",
        "K Zhang",
        "R Sankaranarayana"
      ],
      "year": "2024",
      "venue": "Diffusion models for underdisplay camera emotion recognition",
      "arxiv": "arXiv:2402.00250"
    },
    {
      "citation_id": "231",
      "title": "Real-life dynamic facial expression recognition: a review",
      "authors": [
        "S Saleem",
        "S Zeebaree",
        "M Abdulrazzaq"
      ],
      "year": "2021",
      "venue": "Journal of Physics: Conference Series"
    },
    {
      "citation_id": "232",
      "title": "Facial expression recognition from image sequences using twofold random forest classifier",
      "authors": [
        "X Pu",
        "K Fan",
        "X Chen",
        "L Ji",
        "Z Zhou"
      ],
      "year": "2015",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "233",
      "title": "A review of dynamic datasets for facial expression research",
      "authors": [
        "E Krumhuber",
        "L Skora",
        "D Küster",
        "L Fou"
      ],
      "year": "2017",
      "venue": "Emotion Review"
    },
    {
      "citation_id": "234",
      "title": "Mimamo net: Integrating micro-and macro-motion for video emotion recognition",
      "authors": [
        "D Deng",
        "Z Chen",
        "Y Zhou",
        "B Shi"
      ],
      "year": "2020",
      "venue": "Proceedings of the AAAI conference on artificial intelligence"
    },
    {
      "citation_id": "235",
      "title": "Masked autoencoder for facial video representation learning",
      "authors": [
        "Z Cai",
        "S Ghosh",
        "K Stefanov",
        "A Dhall",
        "J Cai",
        "H Rezatofighi",
        "R Haffari",
        "M Hayat"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "236",
      "title": "Improved deep generative adversarial network with illuminant invariant local binary pattern features for facial expression recognition",
      "authors": [
        "P Gavade",
        "V Bhat",
        "J Pujari"
      ],
      "year": "2023",
      "venue": "Computer Methods in Biomechanics and Biomedical Engineering: Imaging & Visualization",
      "doi": "10.1080/21681163.2022.2103450"
    },
    {
      "citation_id": "237",
      "title": "Learning inter-class optical flow difference using generative adversarial networks for facial expression recognition",
      "authors": [
        "W Guo",
        "X Zhao",
        "S Zhang",
        "X Pan"
      ],
      "year": "2023",
      "venue": "Multimedia Tools and Applications",
      "doi": "10.1007/s11042-022-13360-7"
    },
    {
      "citation_id": "238",
      "title": "Evaluation of the spatio-temporal features and gan for microexpression recognition system",
      "authors": [
        "S.-T Liong",
        "Y Gan",
        "D Zheng",
        "S.-M Li",
        "H.-X Xu",
        "H.-Z Zhang",
        "R.-K Lyu",
        "K.-H Liu"
      ],
      "year": "2020",
      "venue": "Journal of Signal Processing Systems"
    },
    {
      "citation_id": "239",
      "title": "Dffcn: Dual flow fusion convolutional network for micro expression recognition",
      "authors": [
        "J Chen",
        "Y Fu",
        "Y Jin",
        "T Liu"
      ],
      "year": "2021",
      "venue": "Neural Information Processing: 28th International Conference, ICONIP 2021"
    },
    {
      "citation_id": "240",
      "title": "Real time face expression recognition along with balanced fer2013 dataset using cyclegan",
      "authors": [
        "F Mazen",
        "A Nashat",
        "R Seoud"
      ],
      "year": "2021",
      "venue": "International Journal of Advanced Computer Science and Applications"
    },
    {
      "citation_id": "241",
      "title": "A survey of state-of-the-art approaches for emotion recognition in text",
      "authors": [
        "N Alswaidan",
        "M Menai"
      ],
      "year": "2020",
      "venue": "Knowledge and Information Systems"
    },
    {
      "citation_id": "242",
      "title": "Understanding emotions in text using deep learning and big data",
      "authors": [
        "A Chatterjee",
        "U Gupta",
        "M Chinnakotla",
        "R Srikanth",
        "M Galley"
      ],
      "year": "2019",
      "venue": "Computers in Human Behavior"
    },
    {
      "citation_id": "243",
      "title": "Current state of text sentiment analysis from opinion to emotion mining",
      "authors": [
        "A Yadollahi",
        "A Shahraki",
        "O Zaiane"
      ],
      "year": "2017",
      "venue": "ACM Computing Surveys (CSUR)"
    },
    {
      "citation_id": "244",
      "title": "Sentiment analysis: Detecting valence, emotions, and other affectual states from text",
      "authors": [
        "S Mohammad"
      ],
      "year": "2016",
      "venue": "Emotion measurement"
    },
    {
      "citation_id": "245",
      "title": "Deep learning for sentiment analysis: A survey",
      "authors": [
        "L Zhang",
        "S Wang",
        "B Liu"
      ],
      "year": "2018",
      "venue": "Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery"
    },
    {
      "citation_id": "246",
      "title": "A review on sentiment analysis and emotion detection from text",
      "authors": [
        "P Nandwani",
        "R Verma"
      ],
      "year": "2021",
      "venue": "Social Network Analysis and Mining"
    },
    {
      "citation_id": "247",
      "title": "Sentiment analysis and opinion mining",
      "authors": [
        "B Liu"
      ],
      "year": "2022",
      "venue": "Sentiment analysis and opinion mining"
    },
    {
      "citation_id": "248",
      "title": "A survey on deep learning for textual emotion analysis in social networks",
      "authors": [
        "S Peng",
        "L Cao",
        "Y Zhou",
        "Z Ouyang",
        "A Yang",
        "X Li",
        "W Jia",
        "S Yu"
      ],
      "year": "2022",
      "venue": "Digital Communications and Networks"
    },
    {
      "citation_id": "249",
      "title": "Emotional chatting machine: Emotional conversation generation with internal and external memory",
      "authors": [
        "H Zhou",
        "M Huang",
        "T Zhang",
        "X Zhu",
        "B Liu"
      ],
      "year": "2018",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "250",
      "title": "Exploring text-generating large language models (llms) for emotion recognition in affective intelligent agents",
      "authors": [
        "A Pico",
        "E Vivancos",
        "A García-Fornes",
        "V Botti"
      ],
      "year": "2024",
      "venue": "ICAART"
    },
    {
      "citation_id": "251",
      "title": "Cosmic: Commonsense knowledge for emotion identification in conversations",
      "authors": [
        "D Ghosal",
        "N Majumder",
        "A Gelbukh",
        "R Mihalcea",
        "S Poria"
      ],
      "year": "2020",
      "venue": "Cosmic: Commonsense knowledge for emotion identification in conversations",
      "arxiv": "arXiv:2010.02795"
    },
    {
      "citation_id": "252",
      "title": "Commonsense transformers for automatic knowledge graph construction",
      "authors": [
        "A Bosselut",
        "H Rashkin",
        "M Sap",
        "C Malaviya",
        "A Celikyilmaz",
        "Y Choi"
      ],
      "year": "2019",
      "venue": "Commonsense transformers for automatic knowledge graph construction",
      "arxiv": "arXiv:1906.05317"
    },
    {
      "citation_id": "253",
      "title": "Emotion recognition in conversation with multistep prompting using large language model",
      "authors": [
        "K Hama",
        "A Otsuka",
        "R Ishii"
      ],
      "year": "2024",
      "venue": "International Conference on Human-Computer Interaction"
    },
    {
      "citation_id": "254",
      "title": "Instructerc: Reforming emotion recognition in conversation with a retrieval multi-task llms framework",
      "authors": [
        "S Lei",
        "G Dong",
        "X Wang",
        "K Wang",
        "S Wang"
      ],
      "year": "2023",
      "venue": "Instructerc: Reforming emotion recognition in conversation with a retrieval multi-task llms framework",
      "arxiv": "arXiv:2309.11911"
    },
    {
      "citation_id": "255",
      "title": "Ckerc: Joint large language models with commonsense knowledge for emotion recognition in conversation",
      "authors": [
        "Y Fu"
      ],
      "year": "2024",
      "venue": "Ckerc: Joint large language models with commonsense knowledge for emotion recognition in conversation",
      "arxiv": "arXiv:2403.07260"
    },
    {
      "citation_id": "256",
      "title": "A review of emotion recognition using physiological signals",
      "authors": [
        "L Shu",
        "J Xie",
        "M Yang",
        "Z Li",
        "Z Li",
        "D Liao",
        "X Xu",
        "X Yang"
      ],
      "year": "2018",
      "venue": "Sensors"
    },
    {
      "citation_id": "257",
      "title": "Emotions recognition using eeg signals: A survey",
      "authors": [
        "S Alarcao",
        "M Fonseca"
      ],
      "year": "2017",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "258",
      "title": "Eeg-based emotion recognition using regularized graph neural networks",
      "authors": [
        "P Zhong",
        "D Wang",
        "C Miao"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "259",
      "title": "Human emotion recognition with electroencephalographic multidimensional features by hybrid deep neural networks",
      "authors": [
        "Y Li",
        "J Huang",
        "H Zhou",
        "N Zhong"
      ],
      "year": "2017",
      "venue": "Applied Sciences"
    },
    {
      "citation_id": "260",
      "title": "Electrocardiogram-based emotion recognition systems and their applications in healthcare-a review",
      "authors": [
        "M Hasnul",
        "N Aziz",
        "S Alelyani",
        "M Mohana",
        "A Aziz"
      ],
      "year": "2021",
      "venue": "Sensors"
    },
    {
      "citation_id": "261",
      "title": "Self-supervised ecg representation learning for emotion recognition",
      "authors": [
        "P Sarkar",
        "A Etemad"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "262",
      "title": "Emotion recognition by heart rate variability",
      "authors": [
        "H Ferdinando",
        "L Ye",
        "T Seppänen",
        "E Alasaarela"
      ],
      "year": "2014",
      "venue": "Australian Journal of Basic and Applied Science"
    },
    {
      "citation_id": "263",
      "title": "Feature extraction and selection for emotion recognition from electrodermal activity",
      "authors": [
        "J Shukla",
        "M Barreda-Angeles",
        "J Oliver",
        "G Nandi",
        "D Puig"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "264",
      "title": "A review on semi-supervised learning for eeg-based emotion recognition",
      "authors": [
        "S Qiu",
        "Y Chen",
        "Y Yang",
        "P Wang",
        "Z Wang",
        "H Zhao",
        "Y Kang",
        "R Nie"
      ],
      "year": "2023",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "265",
      "title": "Eeg data augmentation for emotion recognition using a conditional wasserstein gan",
      "authors": [
        "Y Luo",
        "B.-L Lu"
      ],
      "year": "2018",
      "venue": "2018 40th annual international conference of the IEEE engineering in medicine and biology society (EMBC)"
    },
    {
      "citation_id": "266",
      "title": "Data augmentation for eeg-based emotion recognition using generative adversarial networks",
      "authors": [
        "G Bao",
        "B Yan",
        "L Tong",
        "J Shu",
        "L Wang",
        "K Yang",
        "Y Zeng"
      ],
      "year": "2021",
      "venue": "Frontiers in Computational Neuroscience",
      "doi": "10.3389/FNCOM.2021.723843/BIBTEX"
    },
    {
      "citation_id": "267",
      "title": "Gan-based data augmentation for improving the classification of eeg signals",
      "authors": [
        "S Bhat",
        "E Hortal"
      ],
      "year": "2021",
      "venue": "Proceedings of the 14th PErvasive Technologies Related to Assistive Environments Conference"
    },
    {
      "citation_id": "268",
      "title": "Ganser: A self-supervised data augmentation framework for eeg-based emotion recognition",
      "authors": [
        "Z Zhang",
        "S -H. Zhong",
        "Y Liu"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "269",
      "title": "Beyond mimicking under-represented emotions: Deep data augmentation with emotional subspace constraints for eeg-based emotion recognition",
      "authors": [
        "Z Zhang",
        "S Zhong",
        "Y Liu"
      ],
      "year": "2024",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "270",
      "title": "Biosignal data augmentation based on generative adversarial networks",
      "authors": [
        "S Haradal",
        "H Hayashi",
        "S Uchida"
      ],
      "year": "2018",
      "venue": "2018 40th annual international conference of the IEEE engineering in medicine and biology society (EMBC)"
    },
    {
      "citation_id": "271",
      "title": "Emotion recognition based on eeg using generative adversarial nets and convolutional neural network, Computational and mathematical methods in medicine",
      "authors": [
        "B Pan",
        "W Zheng"
      ],
      "year": "2021",
      "venue": "Emotion recognition based on eeg using generative adversarial nets and convolutional neural network, Computational and mathematical methods in medicine",
      "doi": "10.1155/2021/2520394"
    },
    {
      "citation_id": "272",
      "title": "Data augmentation for enhancing eeg-based emotion recognition with deep generative models",
      "authors": [
        "Y Luo",
        "L Zhu",
        "Z Wan",
        "B Lu"
      ],
      "year": "2020",
      "venue": "Journal of Neural Engineering",
      "doi": "10.1088/1741-2552/abb580https://iopscience.iop.org/article/10.1088/1741-2552/abb580/meta"
    },
    {
      "citation_id": "273",
      "title": "Eeg feature extraction and data augmentation in emotion recognition",
      "authors": [
        "M Kalashami",
        "M Pedram",
        "H Sadr"
      ],
      "year": "2022",
      "venue": "Computational intelligence and neuroscience"
    },
    {
      "citation_id": "274",
      "title": "Enhancing eeg signal-based emotion recognition with synthetic data: Diffusion modeel approach",
      "authors": [
        "G Siddhad",
        "M Iwamura",
        "P Roy"
      ],
      "year": "2024",
      "venue": "Enhancing eeg signal-based emotion recognition with synthetic data: Diffusion modeel approach",
      "arxiv": "arXiv:2401.16878"
    },
    {
      "citation_id": "275",
      "title": "Eeg synthetic data generation using probabilistic diffusion models",
      "authors": [
        "G Tosato",
        "C Dalbagno",
        "F Fumagalli"
      ],
      "year": "2023",
      "venue": "Eeg synthetic data generation using probabilistic diffusion models",
      "arxiv": "arXiv:2303.06068"
    },
    {
      "citation_id": "276",
      "title": "A weighted co-training framework for emotion recognition based on eeg data generation using frequency-spatial diffusion transformer",
      "authors": [
        "Y Yi",
        "Y Xu",
        "B Yang",
        "Y Tian"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "277",
      "title": "Unsupervised feature learning for eeg-based emotion recognition",
      "authors": [
        "Z Lan",
        "O Sourina",
        "L Wang",
        "R Scherer",
        "G Müller-Putz"
      ],
      "year": "2017",
      "venue": "2017 International Conference on Cyberworlds (CW)"
    },
    {
      "citation_id": "278",
      "title": "Subject independent emotion recognition using eeg signals employing attention driven neural networks",
      "authors": [
        "A Rajpoot",
        "M Panicker"
      ],
      "year": "2022",
      "venue": "Biomedical Signal Processing and Control"
    },
    {
      "citation_id": "279",
      "title": "Eeg-based emotion recognition using deep learning network with principal component based covariate shift adaptation",
      "authors": [
        "S Jirayucharoensak",
        "S Pan-Ngum",
        "P Israsena"
      ],
      "year": "2014",
      "venue": "The Scientific World Journal"
    },
    {
      "citation_id": "280",
      "title": "Variational autoencoder based latent factor decoding of multichannel eeg for emotion recognition",
      "authors": [
        "X Li",
        "Z Zhao",
        "D Song",
        "Y Zhang",
        "C Niu",
        "J Zhang",
        "J Huo",
        "J Li"
      ],
      "year": "2019",
      "venue": "2019 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)"
    },
    {
      "citation_id": "281",
      "title": "Eeg2vec: Learning affective eeg representations via variational autoencoders",
      "authors": [
        "D Bethge",
        "P Hallgarten",
        "T Grosse-Puppendahl",
        "M Kari",
        "L Chuang",
        "O Özdenizci",
        "A Schmidt"
      ],
      "year": "2022",
      "venue": "2022 IEEE International Conference on Systems, Man, and Cybernetics"
    },
    {
      "citation_id": "282",
      "title": "Eeg-based emotion classification using a deep neural network and sparse autoencoder",
      "authors": [
        "J Liu",
        "G Wu",
        "Y Luo",
        "S Qiu",
        "S Yang",
        "W Li",
        "Y Bi"
      ],
      "year": "2020",
      "venue": "Frontiers in Systems Neuroscience"
    },
    {
      "citation_id": "283",
      "title": "Interpretable emotion recognition using eeg signals",
      "authors": [
        "C Qing",
        "R Qiao",
        "X Xu",
        "Y Cheng"
      ],
      "year": "2019",
      "venue": "Ieee Access"
    },
    {
      "citation_id": "284",
      "title": "Latent factor decoding of multi-channel eeg for emotion recognition through autoencoder-like neural networks",
      "authors": [
        "X Li",
        "Z Zhao"
      ],
      "year": "2020",
      "venue": "Frontiers in neuroscience"
    },
    {
      "citation_id": "285",
      "title": "A domain generative graph network for eeg-based emotion recognition",
      "authors": [
        "Y Gu",
        "X Zhong",
        "C Qu",
        "C Liu",
        "B Chen"
      ],
      "year": "2023",
      "venue": "IEEE Journal of Biomedical and Health Informatics",
      "doi": "10.1109/JBHI.2023.3242090"
    },
    {
      "citation_id": "286",
      "title": "Deep recurrent semi-supervised eeg representation learning for emotion recognition",
      "authors": [
        "G Zhang",
        "A Etemad"
      ],
      "year": "2021",
      "venue": "2021 9th International Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "287",
      "title": "Unsupervised domain adaptation techniques based on auto-encoder for non-stationary eeg-based emotion recognition",
      "authors": [
        "X Chai",
        "Q Wang",
        "Y Zhao",
        "X Liu",
        "O Bai",
        "Y Li"
      ],
      "year": "2016",
      "venue": "Computers in biology and medicine"
    },
    {
      "citation_id": "288",
      "title": "Multi-modal domain adaptation variational autoencoder for eeg-based emotion recognition",
      "authors": [
        "Y Wang",
        "S Qiu",
        "D Li",
        "C Du",
        "B.-L Lu",
        "H He"
      ],
      "year": "2022",
      "venue": "IEEE/CAA Journal of Automatica Sinica"
    },
    {
      "citation_id": "289",
      "title": "Generator-based domain adaptation method with knowledge free for cross-subject eeg emotion recognition",
      "authors": [
        "D Huang",
        "S Zhou",
        "D Jiang"
      ],
      "year": "2022",
      "venue": "Cognitive Computation"
    },
    {
      "citation_id": "290",
      "title": "End-toend multimodal emotion recognition using deep neural networks",
      "authors": [
        "P Tzirakis",
        "G Trigeorgis",
        "M Nicolaou",
        "B Schuller",
        "S Zafeiriou"
      ],
      "year": "2017",
      "venue": "IEEE Journal of selected topics in signal processing"
    },
    {
      "citation_id": "291",
      "title": "Emotion recognition using multi-modal data and machine learning techniques: A tutorial and review",
      "authors": [
        "J Zhang",
        "Z Yin",
        "P Chen",
        "S Nichele"
      ],
      "year": "2020",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "292",
      "title": "A connection between correlation and contingency",
      "authors": [
        "H Hirschfeld"
      ],
      "year": "1935",
      "venue": "Mathematical Proceedings of the Cambridge Philosophical Society"
    },
    {
      "citation_id": "293",
      "title": "Das statistische problem der korrelation als variations-und eigenwertproblem und sein zusammenhang mit der ausgleichsrechnung",
      "authors": [
        "H Gebelein"
      ],
      "year": "1941",
      "venue": "ZAMM-Journal of Applied Mathematics and Mechanics/Zeitschrift für Angewandte Mathematik und Mechanik"
    },
    {
      "citation_id": "294",
      "title": "On measures of dependence",
      "authors": [
        "A Rényi"
      ],
      "year": "1959",
      "venue": "Acta mathematica hungarica"
    },
    {
      "citation_id": "295",
      "title": "A gan-based data augmentation method for multimodal emotion recognition",
      "authors": [
        "Y Luo",
        "L.-Z Zhu",
        "B.-L Lu"
      ],
      "year": "2019",
      "venue": "Advances in Neural Networks-ISNN 2019: 16th International Symposium on Neural Networks, ISNN 2019, Moscow, Russia"
    },
    {
      "citation_id": "296",
      "title": "Began: Boundary equilibrium generative adversarial networks",
      "authors": [
        "D Berthelot",
        "T Schumm",
        "L Metz"
      ],
      "year": "2017",
      "venue": "Began: Boundary equilibrium generative adversarial networks",
      "arxiv": "arXiv:1703.10717"
    },
    {
      "citation_id": "297",
      "title": "Simplifying multimodal emotion recognition with single eye movement modality",
      "authors": [
        "X Yan",
        "L.-M Zhao",
        "B.-L Lu"
      ],
      "year": "2021",
      "venue": "Proceedings of the 29th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "298",
      "title": "Generating fmrienriched acoustic vectors using a cross-modality adversarial network for emotion recognition",
      "authors": [
        "G.-Y Chao",
        "C.-M Chang",
        "J.-L Li",
        "Y.-T Wu",
        "C.-C Lee"
      ],
      "year": "2018",
      "venue": "Proceedings of the 20th ACM International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "299",
      "title": "Identifying gender differences in multimodal emotion recognition using bimodal deep autoencoder",
      "authors": [
        "X Yan",
        "W.-L Zheng",
        "W Liu",
        "B.-L Lu"
      ],
      "year": "2017",
      "venue": "Neural Information Processing: 24th International Conference"
    },
    {
      "citation_id": "300",
      "title": "Theory of fuzzy integrals and its applications",
      "authors": [
        "M Sugeno"
      ],
      "year": "1974",
      "venue": "Theory of fuzzy integrals and its applications"
    },
    {
      "citation_id": "301",
      "title": "Multimodal emotion recognition from eye image, eye movement and eeg using deep neural networks",
      "authors": [
        "J.-J Guo",
        "R Zhou",
        "L.-M Zhao",
        "B.-L Lu"
      ],
      "year": "2019",
      "venue": "2019 41st annual international conference of the IEEE engineering in medicine and biology society (EMBC)"
    },
    {
      "citation_id": "302",
      "title": "Expression-eeg based collaborative multimodal emotion recognition using deep autoencoder",
      "authors": [
        "H Zhang"
      ],
      "year": "2020",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "303",
      "title": "An autoencoder-based feature level fusion for speech emotion recognition",
      "authors": [
        "P Shixin",
        "C Kai",
        "T Tian",
        "C Jingying"
      ],
      "year": "2022",
      "venue": "An autoencoder-based feature level fusion for speech emotion recognition"
    },
    {
      "citation_id": "304",
      "title": "Multi-modal fusion for continuous emotion recognition by using auto-encoders",
      "authors": [
        "S Hamieh",
        "V Heiries",
        "H Al Osman",
        "C Godin"
      ],
      "year": "2021",
      "venue": "Proceedings of the 2nd on Multimodal Sentiment Analysis Challenge"
    },
    {
      "citation_id": "305",
      "title": "Deep auto-encoders with sequential learning for multimodal dimensional emotion recognition",
      "authors": [
        "D Nguyen",
        "D Nguyen",
        "R Zeng",
        "T Nguyen",
        "S Tran",
        "T Nguyen",
        "S Sridharan",
        "C Fookes"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "306",
      "title": "An end-to-end learning approach for multimodal emotion recognition: Extracting common and private information",
      "authors": [
        "F Ma",
        "W Zhang",
        "Y Li",
        "S.-L Huang",
        "L Zhang"
      ],
      "year": "2019",
      "venue": "2019 IEEE International Conference on Multimedia and Expo (ICME)"
    },
    {
      "citation_id": "307",
      "title": "Multi-channel weight-sharing autoencoder based on cascade multi-head attention for multimodal emotion recognition",
      "authors": [
        "J Zheng",
        "S Zhang",
        "Z Wang",
        "X Wang",
        "Z Zeng"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "308",
      "title": "Incomplete multimodality-diffused emotion recognition",
      "authors": [
        "Y Wang",
        "Y Li",
        "Z Cui"
      ],
      "year": "2024",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "309",
      "title": "Semi-supervised bayesian deep multi-modal emotion recognition",
      "authors": [
        "C Du",
        "C Du",
        "J Li",
        "W -L. Zheng",
        "B -L. Lu",
        "H He"
      ],
      "year": "2017",
      "venue": "Semi-supervised bayesian deep multi-modal emotion recognition",
      "arxiv": "arXiv:1704.07548"
    },
    {
      "citation_id": "310",
      "title": "Semi-supervised multimodal emotion recognition with improved wasserstein gans",
      "authors": [
        "J Liang",
        "S Chen",
        "Q Jin"
      ],
      "year": "2019",
      "venue": "Asia-Pacific Signal and Information Processing Association Annual Summit and Conference"
    },
    {
      "citation_id": "311",
      "title": "Improving the improved training of wasserstein gans: A consistency term and its dual effect",
      "authors": [
        "X Wei",
        "B Gong",
        "Z Liu",
        "W Lu",
        "L Wang"
      ],
      "year": "2018",
      "venue": "Improving the improved training of wasserstein gans: A consistency term and its dual effect",
      "arxiv": "arXiv:1803.01541"
    },
    {
      "citation_id": "312",
      "title": "Multimodal autoencoder: A deep learning approach to filling in missing sensor data and enabling better mood prediction",
      "authors": [
        "N Jaques",
        "S Taylor",
        "A Sano",
        "R Picard"
      ],
      "year": "2017",
      "venue": "Seventh International Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "313",
      "title": "Multimodal emotion recognition with deep learning: advancements, challenges, and future directions",
      "authors": [
        "A Geetha",
        "T Mala",
        "D Priyanka",
        "E Uma"
      ],
      "year": "2024",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "314",
      "title": "Contrastive learning based modality-invariant feature acquisition for robust multimodal emotion recognition with missing modalities",
      "authors": [
        "R Liu",
        "H Zuo",
        "Z Lian",
        "B Schuller",
        "H Li"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "315",
      "title": "Diffusion models beat gans on image synthesis, Advances in neural information processing systems",
      "authors": [
        "P Dhariwal",
        "A Nichol"
      ],
      "year": "2021",
      "venue": "Diffusion models beat gans on image synthesis, Advances in neural information processing systems"
    },
    {
      "citation_id": "316",
      "title": "Diffused heads: Diffusion models beat gans on talking-face generation",
      "authors": [
        "M Stypułkowski",
        "K Vougioukas",
        "S He",
        "M Zięba",
        "S Petridis",
        "M Pantic"
      ],
      "year": "2024",
      "venue": "Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision"
    },
    {
      "citation_id": "317",
      "title": "A survey on federated learning, Knowledge-Based Systems",
      "authors": [
        "C Zhang",
        "Y Xie",
        "H Bai",
        "B Yu",
        "W Li",
        "Y Gao"
      ],
      "year": "2021",
      "venue": "A survey on federated learning, Knowledge-Based Systems"
    },
    {
      "citation_id": "318",
      "title": "Emotionally intelligent chatbots: A systematic literature review",
      "authors": [
        "G Bilquise",
        "S Ibrahim",
        "K Shaalan"
      ],
      "year": "2022",
      "venue": "Human Behavior and Emerging Technologies"
    },
    {
      "citation_id": "319",
      "title": "Federated learning for speech emotion recognition applications",
      "authors": [
        "S Latif",
        "S Khalifa",
        "R Rana",
        "R Jurdak"
      ],
      "year": "2020",
      "venue": "2020 19th ACM/IEEE international conference on information processing in sensor networks (IPSN)"
    },
    {
      "citation_id": "320",
      "title": "Privacy-enhanced federated learning against attribute inference attack for speech emotion recognition",
      "authors": [
        "H Zhao",
        "H Chen",
        "Y Xiao",
        "Z Zhang"
      ],
      "year": "2023",
      "venue": "ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "321",
      "title": "Is virtual reality emotionally arousing? investigating five emotion inducing virtual park scenarios",
      "authors": [
        "A Felnhofer",
        "O Kothgassner",
        "M Schmidt",
        "A.-K Heinzle",
        "L Beutl",
        "H Hlavacs",
        "I Kryspin-Exner"
      ],
      "year": "2015",
      "venue": "International journal of human-computer studies"
    },
    {
      "citation_id": "322",
      "title": "Empathic aurea: Exploring the effects of an augmented reality cue for emotional sharing across three face-toface tasks",
      "authors": [
        "A Valente",
        "D Lopes",
        "N Nunes",
        "A Esteves"
      ],
      "year": "2022",
      "venue": "2022 IEEE conference on virtual reality and 3D user interfaces (VR)"
    },
    {
      "citation_id": "323",
      "title": "Musical element-based regularization for generating symbolic music with emotion",
      "authors": [
        "S Ji",
        "X Yang"
      ],
      "year": "2024",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "324",
      "title": "Customer service with ai-powered human-robot collaboration (hrc): a literature review",
      "authors": [
        "D Leocádio",
        "L Guedes",
        "J Oliveira",
        "J Reis",
        "N Melão"
      ],
      "year": "2024",
      "venue": "Procedia Computer Science"
    },
    {
      "citation_id": "325",
      "title": "Visual writing prompts: Character-grounded story generation with curated image sequences",
      "authors": [
        "X Hong",
        "A Sayeed",
        "K Mehra",
        "V Demberg",
        "B Schiele"
      ],
      "year": "2023",
      "venue": "Transactions of the Association for Computational Linguistics"
    }
  ]
}