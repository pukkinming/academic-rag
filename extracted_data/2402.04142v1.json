{
  "paper_id": "2402.04142v1",
  "title": "Human Emotions Analysis And Recognition Using Eeg Signals In Response To 360°Videos",
  "published": "2024-02-06T16:48:58Z",
  "authors": [
    "Haseeb ur Rahman Abbasi",
    "Zeeshan Rashid",
    "Muhammad Majid",
    "Syed Muhammad Anwar"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Emotion recognition (ER) technology is an integral part for developing innovative applications such as drowsiness detection and health monitoring that plays a pivotal role in contemporary society. This study delves into ER using electroencephalography (EEG), within immersive virtual reality (VR) environments. There are four main stages in our proposed methodology including data acquisition, pre-processing, feature extraction, and emotion classification. Acknowledging the limitations of existing 2D datasets, we introduce a groundbreaking 3D VR dataset to elevate the precision of emotion elicitation. Leveraging the Interaxon Muse headband for EEG recording and Oculus Quest 2 for VR stimuli, we meticulously recorded data from 40 participants, prioritizing subjects without reported mental illnesses. Pre-processing entails rigorous cleaning, uniform truncation, and the application of a Savitzky-Golay filter to the EEG data. Feature extraction encompasses a comprehensive analysis of metrics such as power spectral density, correlation, rational and divisional asymmetry, and power spectrum. To ensure the robustness of our model, we employed a 10-fold cross-validation, revealing an average validation accuracy of 85.54%, with a noteworthy maximum accuracy of 90.20% in the best fold. Subsequently, the trained model demonstrated a commendable test accuracy of 82.03%, promising favorable outcomes.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "Emotion recognition, a technology with diverse applications, is currently employed in driving drowsiness detection, workload evaluation, and health monitoring, significantly impacting society. It involves both emotion elicitation and classification. Emotion recognition not only portrays behavioral and mental states but also strengthens humancomputer interaction. Positive emotions indicate a healthy state, contrasting with negative emotions which could even be linked to conditions such as depression and an elevated suicide risk  [1] . Recent studies favor physiological signals like electroencephalography (EEG) over visible signs (like speech, and facial expressions), considering them more accurate indicators of genuine emotions due to their connection to the central nervous system. There has been a significant interest in EEG based analysis, due to its potential to offer a straightforward, cost-effective, portable, and user-friendly solution for emotion identification  [2] . Therefore, EEG finds extensive application in diverse biomedical contexts, playing a crucial role in diverse tasks such as stress assessment  [3] , the detection of depression disorders  [4] , and the exploration of schizophrenia  [5]  to name a few.\n\nVirtual reality (VR) is a technology that uses computergenerated simulated environments, that give users the illusion of real physical exposure. VR is widely used in different fields like education, medicine, entertainment, defense, marketing, real estate, and many more  [6] . Environments that are difficult to realize can be simulated using VR. The 2021 launch of Horizon Worlds by Meta Platforms has ignited debates on the societal implications of the Metaverse, described as \"the layer between you and reality.\" Meta envisions seamlessly integrating avatars and holograms in a 3D virtual shared world for work and social interactions  [7] . Although VR technology is making progress rapidly, it has not yet reached the peak of its development, and its limitations cannot be reliably predicted for now.\n\nAt present, most emotion recognition systems are datadependent. Many datasets have been proposed in the literature for research. Prominent publicly accessible datasets for emotion recognition, such as DEAP  [8] , DREAMER  [9] , and ASCERTAIN  [10] , incorporate EEG signals in conjunction with other physiological data. These datasets used videos, music, and images to elicit different emotions. The main limitation of these datasets is that all these use stimuli that are 2D, non-immersive, and hence they lack the feel of presence for users when they interact with them. So, a 3D virtual environment is required for more effective emotion elicitation.\n\nStudies have shown that emotions elicited using immersive virtual environments are better than those induced using nonimmersive methods  [11] . Aelee Kim et al  [12]  showed that VR gives a more immersive experience and greater emotional response to horror films. The effectiveness of VR as emotion elicitation stimuli has been proven  [13] [14] [15] [16] . In  [17] , a VRbased EEG dataset named as VREED, was presented and used to recognize different emotions. The dataset is currently in the embargo phase and is not yet available publicly. In  [18] , emotion recognition based on VR was conducted to classify four classes of emotions. Another study  [19]  was conducted for emotion recognition in response to VR using data from frontal EEG electrodes.\n\nDue to limited research in emotion recognition in response to VR and the unavailability of a physiological signals dataset, this paper acquires a new dataset for emotion analysis using EEG signals in response to VR environments. In the near future, interactions with VR technology is expected to increase manifold, as leading IT industries have already started developing VR-based applications. So, our work is Section II explains the proposed methodology for this study. Section III presents results and discussion and a conclusion is drawn in section IV.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Our Proposed Methodology",
      "text": "An overflow workflow for the proposed strategy for human emotion analysis and recognition in response to a virtual reality environment (360°videos) is shown in Figure  1 . The four main steps are: 1)data acquisition, 2) pre-processing, 3) feature extraction, and 4) emotion classification.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "A. Data Acquisition",
      "text": "1) Participants and Apparatus: Data were recorded from a total of 40 subjects (24 males and 16 females). The age range was between 18 to 35 years. Subjects did not report any mental illness before recording. For EEG recording, we employed the Interaxon Muse headband, a versatile and userfriendly EEG recording system with four channels located at AF7, AF8, TP9, and TP10 positions. The Muse headband records EEG data at 256 Hz and connects to a smartphone via Bluetooth for data transmission. To present the stimuli to subjects, a head-mounted display (HMD) from Meta named Oculus Quest 2 with six degrees of freedom with realistic precision, 1832 × 1920 resolution per eye, and with 90 Hz refresh rate was used.\n\n2) Stimuli Selection: Sixteen videos, four from each quadrant were chosen from a publicly available immersive VR video database, each accompanied by valence and arousal ratings obtained through the Self-Assessment Manikin (SAM) scale  [19] . The videos whose ratings were farthest from the origin were selected. From the first quadrant of the valence arousal scale, we selected video numbers 50, 62, 69, and 52. Video numbers 33, 32, 27, and 22 were selected from the second quadrant. For the third quadrant, we selected 15, 3, 1, and 14, and for the fourth quadrant 20, 65, 68, and 21 were selected from  [19] . Each quadrant represents happy, angry, sad, and relaxed emotions respectively. These selected videos were then organized into four sessions, each comprising of four videos. The design of each session was aimed to maintain a total duration of approximately 15 minutes, considering user fatigue, as studies suggest discomfort after prolonged viewing periods  [19] .\n\n3) Experimental Procedure: For EEG recordings, participants were taken to temperature-regulated room with consistent lighting conditions. In this setting, an overview of the experimental process was presented, and participants were invited to sign the consent form. Furthermore, individuals were requested to complete a demographic questionnaire to supply details about their age, gender, and any pertinent information related to mental health. Participants were informed that they had the freedom to quit the experiment at any time of their choosing. The experiments for this study are designed following the Helsinki Declaration and the study was approved by the Board of Advanced Studies Research and Technological Development at the University of Engineering and Technology, Taxila.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "B. Data Pre-Processing",
      "text": "The recorded data was transferred to a computer for preprocessing. Initially, missing data points from raw EEG channels were generated using window-based averaging to ensure data cleanliness. Although, the recorded data had very few of these points. Subsequently, the cleaned data files were uniformly truncated to match the duration of each video. Following this, a Savitzky-Golay filter with a third-order polynomial and a window size of 11 was applied to smooth out potential outliers in the data. Furthermore, five frequency bands: delta (0-4 Hz), theta (4-7 Hz), alpha (8-12 Hz), beta (12-30 Hz), and gamma (30-50 Hz) were obtained from raw EEG channels data.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "C. Features Extraction",
      "text": "To analyze the recorded EEG data, distinct frequency domain feature groups were derived from each channel. These included power spectral density (PSD), correlation (C), divisional asymmetry (DASM), rational asymmetry (RASM), and power spectrum (PS). In particular, PSD characterizes the power distribution across specific frequency ranges. Features from this group included the mean and variance of the PSD for each channel. This feature group comprised 8 features. Correlation, a statistical measure reflecting the degree of variation between two values, was computed for asymmetric channels of the left and right hemispheres-specifically, (TP9, TP10) and (AF7, AF8). Two features were computed for this feature group. DASM represented the variance in absolute power between asymmetric channels of the brain hemispheres, while RASM denoted the ratio of absolute power between left and right hemisphere channels. Four features were extracted from these feature groups. The power spectrum involved the average absolute power across four scalp electrodes in the five frequency bands of the EEG signal. 20 Features were extracted from this group, comprising a total feature vector length of 34 features.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "D. Classification",
      "text": "For classification, we used a machine learning algorithm named Support Vector Machine (SVM). In SVM, data items are positioned in an n-dimensional space. The classification of these data points involves the identification of a hyperplane, strategically separating the classes for optimal distinction. SVM employs an iterative training algorithm to pinpoint the most advantageous hyperplane, working towards the minimization of an error function. Using SVM, we classified four classes of emotions, based on videos selected from four quadrants. We applied different kernel functions like radial basis function (RBF), Gaussian, linear, and polynomial to compare results.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Iii. Results And Discussion",
      "text": "To train the machine learning model, we split the feature vector into 80 percent training and 20 percent testing sets. 10-fold cross-validation was applied to the training set. We then tested the test set on the model trained with the best fold (with maximum accuracy). SVM was trained using RBF,   I  compares the results of different kernels for four class classifications. The 2nd-degree polynomial kernel outperforms all other kernels. Fig.  2 . shows the confusion matrix for all four kernels used for 4-class classification on the test set. In evaluating the performance of the four kernel functions (RBF, Linear, Gaussian, and Polynomial) on a four-class classification problem (Happy, Angry, Sad, Relaxed), distinct strengths and weaknesses emerge. The Polynomial kernel exhibits the highest average precision 0.82, recall 0.81, and F1 score of 0.82 across all classes, showcasing its superior performance. On the other hand, the Linear kernel demonstrates balanced precision across classes. The RBF and Gaussian kernels perform similarly, excelling in distinguishing instances in the \"Happy\" and \"Relaxed\" classes but facing challenges in accurately classifying instances in the \"Angry\" and \"Sad\" categories.\n\nTable  II  presents the performance comparison of our proposed method with recent studies conducted for the classification of emotions in response to VR content. Studies are selected for comparison as they used VR 360-degree videos for emotion elicitation stimuli. The comparison was made based on the number of participants, the number of EEG channels used, and the type of classification. A study conducted in  [15] , that used VR stimuli to elicit the target emotions. EEG and ECG signals were recorded for 60 participants. Features were extracted from the recorded signals using principal component analysis (PCA). Extracted features were then trained using SVM to recognize the desired emotions. Leave-One-Subject-Out (LOSO) cross-validation method was used. The model achieved an accuracy of 75.0% for arousal and 71.0% accuracy for the valence dimension. In  [17] , a VRbased EEG dataset named DER-VREED was presented. 25 participants (15 males and 10 females) were engaged in the experiment. 60 3D videos were used each 4 sec long, and 20 videos for each positive, negative, and neutral emotion. A 64-channel wireless EEG device was used to collect the signals. The dataset is currently in the embargo phase and is not yet available publicly. Another VR-based EEG dataset presented in  [18]  that targets four classes of emotions namely happy, scared, calm, and bored. 32 individuals participated and watched 39 3D VR videos.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Iv. Conclusion",
      "text": "In this paper, we used VR 360-degree videos as emotion elicitation stimuli for collecting an EEG dataset, for human emotions analysis and recognition. From raw EEG signals, we extracted five feature groups named correlation, power spectral density, power spectrum, rational asymmetry, and divisional asymmetry. These features were then used to train SVM with different kernels to classify four classes of emotions. Results confirmed that the polynomial kernel outperformed other kernels with a maximum average accuracy of 85.54% on 10-fold cross-validation and 82.03% accuracy on testing. This paper not only bridges a gap in VR-based emotion datasets but also establishes a foundation for the integration of emotion recognition into future VR technologies. In the future, we intend to extend our work to multimodal studies and use other physiological signals as well along with EEG recordings.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The proposed method for emotion recognition in response to 360° videos",
      "page": 2
    },
    {
      "caption": "Figure 2: Confusion matrix on the test set for (a) RBF, (b)",
      "page": 3
    },
    {
      "caption": "Figure 2: shows the confusion matrix for all four kernels",
      "page": 3
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Kernel\nFunction": "RBF",
          "Average\naccuracy\nacross 10 folds": "70.49%",
          "Maximum\nAccuracy on\nthe best\nfold": "76.92%",
          "Test Accuracy\non Best Fold": "64.84%"
        },
        {
          "Kernel\nFunction": "Linear",
          "Average\naccuracy\nacross 10 folds": "72.82%",
          "Maximum\nAccuracy on\nthe best\nfold": "82.69%",
          "Test Accuracy\non Best Fold": "75%"
        },
        {
          "Kernel\nFunction": "Gaussian",
          "Average\naccuracy\nacross 10 folds": "69.52%",
          "Maximum\nAccuracy on\nthe best\nfold": "75%",
          "Test Accuracy\non Best Fold": "64.84%"
        },
        {
          "Kernel\nFunction": "Polynomial",
          "Average\naccuracy\nacross 10 folds": "85.54%",
          "Maximum\nAccuracy on\nthe best\nfold": "90.20%",
          "Test Accuracy\non Best Fold": "82.03%"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Method": "DER-\nVREEG\n[17]",
          "Subjects": "32",
          "EEG\nchannels": "4",
          "Accuracy": "85.01%",
          "Classifier": "SVM",
          "Type": "4\nclass"
        },
        {
          "Method": "VREED\n[18]",
          "Subjects": "25",
          "EEG\nchannels": "60",
          "Accuracy": "71.35%",
          "Classifier": "SVM",
          "Type": "3\nclass"
        },
        {
          "Method": "[15]",
          "Subjects": "60",
          "EEG\nchannels": "10-20",
          "Accuracy": "75%",
          "Classifier": "SVM",
          "Type": "Based\non\nValance\nand\nArousal"
        },
        {
          "Method": "Proposed",
          "Subjects": "40",
          "EEG\nchannels": "4",
          "Accuracy": "85.54%",
          "Classifier": "SVM",
          "Type": "4\nclass"
        }
      ],
      "page": 4
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Recent survey on emotion recognition using physiological signals",
      "authors": [
        "E Joy",
        "R Joseph",
        "M Lakshmi",
        "W Joseph",
        "M Rajeswari"
      ],
      "year": "2021",
      "venue": "th International Conference on Advanced Computing and Communication Systems (ICACCS)"
    },
    {
      "citation_id": "2",
      "title": "Emotions Recognition Using EEG Signals: A Survey",
      "authors": [
        "S Alarcao",
        "M Fonseca"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "3",
      "title": "EEG Based Stress Level Detection During Gameplay",
      "authors": [
        "P Samal",
        "R Singla"
      ],
      "year": "2021",
      "venue": "2021 2nd Global Conference for Advancement in Technology (GCAT)"
    },
    {
      "citation_id": "4",
      "title": "Non-invasive EEG signal processing framework for real-time depression analysis",
      "authors": [
        "S Mantri",
        "D Patil",
        "P Agrawal",
        "V Wadhai"
      ],
      "year": "2015",
      "venue": "SAI Intelligent Systems Conference (IntelliSys)"
    },
    {
      "citation_id": "5",
      "title": "Abnormal Dynamics of EEG Oscillations in Schizophrenia Patients on Multiple Time Scales",
      "authors": [
        "J Sun",
        "Y Tang",
        "K Lim",
        "J Wang",
        "S Tong",
        "H Li",
        "B He"
      ],
      "year": "2014",
      "venue": "IEEE Transactions on Biomedical Engineering"
    },
    {
      "citation_id": "6",
      "title": "Prospects and modern technologies in the development of VR/AR",
      "authors": [
        "A Ptukhin",
        "K Serkov",
        "A Khrushkov",
        "E Bozhko"
      ],
      "year": "2018",
      "venue": "Ural Symposium on Biomedical Engineering, Radioelectronics and Information Technology (USBEREIT)"
    },
    {
      "citation_id": "7",
      "title": "Metaverse beyond the hype: Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy",
      "authors": [
        "Y Dwivedi",
        "L Hughes",
        "A Baabdullah",
        "S Ribeiro-Navarrete",
        "M Giannakis",
        "M Al-Debei",
        "D Dennehy",
        "B Metri",
        "D Buhalis",
        "C Cheung",
        "K Conboy"
      ],
      "year": "2022",
      "venue": "Metaverse beyond the hype: Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy"
    },
    {
      "citation_id": "8",
      "title": "DEAP: A Database for Emotion Analysis; Using Physiological Signals",
      "authors": [
        "S Koelstra",
        "C Muhl",
        "M Soleymani",
        "J Lee",
        "A Yazdani",
        "T Ebrahimi",
        "T Pun",
        "A Nijholt",
        "I Patras"
      ],
      "year": "2012",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "9",
      "title": "DREAMER: A Database for Emotion Recognition Through EEG and ECG Signals from Wireless Lowcost Off-the-Shelf Devices",
      "authors": [
        "S Katsigiannis",
        "N Ramzan"
      ],
      "year": "2018",
      "venue": "IEEE Journal of Biomedical and Health Informatics"
    },
    {
      "citation_id": "10",
      "title": "ASCERTAIN: Emotion and Personality Recognition Using Commercial Sensors",
      "authors": [
        "R Subramanian",
        "J Wache",
        "M Abadi",
        "R Vieriu",
        "S Winkler",
        "N Sebe"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "11",
      "title": "Design and Evaluation of Affective Virtual Reality System Based on Multimodal Physiological Signals and Self-Assessment Manikin",
      "authors": [
        "D Liao"
      ],
      "year": "2020",
      "venue": "IEEE Journal of Electromagnetics, RF, and Microwaves in Medicine and Biology"
    },
    {
      "citation_id": "12",
      "title": "The effect of immersion on emotional responses to film viewing in a virtual environment",
      "authors": [
        "A Kim",
        "M Chang",
        "Y Choi",
        "S Jeon",
        "K Lee"
      ],
      "year": "2018",
      "venue": "2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)"
    },
    {
      "citation_id": "13",
      "title": "Induction and Profiling of Strong Multi-Componential Emotions in Virtual Reality",
      "authors": [
        "B Meuleman",
        "D Rudrauf"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "14",
      "title": "Design and evaluation of affective virtual reality system based on multimodal physiological signals and self-assessment manikin",
      "authors": [
        "D Liao",
        "L Shu",
        "G Liang",
        "Y Li",
        "Y Zhang",
        "W Zhang",
        "X Xu"
      ],
      "year": "2019",
      "venue": "RF and Microwaves in Medicine and Biology"
    },
    {
      "citation_id": "15",
      "title": "Affective computing in virtual reality: emotion recognition from brain and heartbeat dynamics using wearable sensors",
      "authors": [
        "J Marín-Morales",
        "J Higuera-Trujillo",
        "A Greco",
        "J Guixeres",
        "C Llinares",
        "E Scilingo",
        "M Alcañiz",
        "G Valenza"
      ],
      "year": "2018",
      "venue": "Scientific reports"
    },
    {
      "citation_id": "16",
      "title": "EEG-based emotion recognition in an immersive virtual reality environment: From local activity to brain network features",
      "authors": [
        "M Yu",
        "S Xiao",
        "M Hua",
        "H Wang",
        "X Chen",
        "F Tian",
        "Y Li"
      ],
      "year": "2022",
      "venue": "Biomedical Signal Processing and Control"
    },
    {
      "citation_id": "17",
      "title": "A Dataset for Emotion Recognition Using Virtual Reality and EEG (DER-VREEG): Emotional State Classification Using Low-Cost Wearable VR-EEG Headsets",
      "authors": [
        "N Suhaimi",
        "J Mountstephens",
        "J Teo"
      ],
      "year": "2022",
      "venue": "Big Data and Cognitive Computing"
    },
    {
      "citation_id": "18",
      "title": "Emotion Recognition Using Frontal EEG in VR Affective Scenes",
      "authors": [
        "T Xu",
        "R Yin",
        "L Shu",
        "X Xu"
      ],
      "year": "2019",
      "venue": "2019 IEEE MTT-S International Microwave Biomedical Conference (IMBioC)"
    },
    {
      "citation_id": "19",
      "title": "A public database of immersive VR videos with corresponding ratings of arousal, valence, and correlations between head movements and self report measures",
      "authors": [
        "B Li",
        "J Bailenson",
        "A Pines",
        "W Greenleaf",
        "L Williams"
      ],
      "year": "2017",
      "venue": "Frontiers in psychology"
    }
  ]
}