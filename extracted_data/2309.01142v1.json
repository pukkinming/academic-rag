{
  "paper_id": "2309.01142v1",
  "title": "Msm-Vc: High-Fidelity Source Style Transfer For Non-Parallel Voice Conversion By Multi-Scale Style Modeling",
  "published": "2023-09-03T11:33:27Z",
  "authors": [
    "Zhichao Wang",
    "Xinsheng Wang",
    "Qicong Xie",
    "Tao Li",
    "Lei Xie",
    "Qiao Tian",
    "Yuping Wang"
  ],
  "keywords": [
    "voice conversion",
    "style modeling",
    "multi-scale"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "In addition to conveying the linguistic content from source speech to converted speech, maintaining the speaking style of source speech also plays an important role in the voice conversion (VC) task, which is essential in many scenarios with highly expressive source speech, such as dubbing and data augmentation. Previous work generally took explicit prosodic features or fixed-length style embedding extracted from source speech to model the speaking style of source speech, which is insufficient to achieve comprehensive style modeling and target speaker timbre preservation. Inspired by the style's multi-scale nature of human speech, a multi-scale style modeling method for the VC task, referred to as MSM-VC, is proposed in this paper. MSM-VC models the speaking style of source speech from different levels, i.e., global, local, and frame levels. To effectively convey the speaking style and meanwhile prevent timbre leakage from source speech to converted speech, each level's style is modeled by specific representation. Specifically, prosodic features, pre-trained ASR model's bottleneck features, and features extracted by a model trained with a self-supervised strategy are adopted to model the frame, local, and global-level styles, respectively. Besides, to balance the performance of source style modeling and target speaker timbre preservation, an explicit constraint module consisting of a pre-trained speech emotion recognition model and a speaker classifier is introduced to MSM-VC. This explicit constraint module also makes it possible to simulate the style transfer inference process during the training to improve the disentanglement ability and alleviate the mismatch between training and inference. Experiments performed on the highly expressive speech corpus demonstrate that MSM-VC is superior to the state-of-the-art VC methods for modeling source speech style while maintaining good speech quality and speaker similarity. Furthermore, ablation analysis indicates the indispensable of every style level's modeling and the effectiveness of each module.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "I. Introduction",
      "text": "V OICE conversion (VC) aims to modify speech from a source speaker to sound like that of a target speaker while maintaining the linguistic content and speaking style. Traditional VC methods  [1] -  [3]  primarily rely on statistical parametric approaches to learn the conversion function between the source and target parallel utterance. Due to the high cost of collecting parallel data, many recent VC approaches  [4] -  [11]  using non-parallel data have been proposed. Despite recent progress, most voice conversion methods focus on preserving the linguistic content and do not explicitly consider the speaking style of source speaker. In many scenarios, such as dubbing and data augmentation, it is essential to preserve the source speech's speaking style, including emotion, pitch, loudness, and duration. In this paper, we focus on accurately delivering the source speech style in the converted speech while preserving the linguistic content.\n\nOne popular approach for style modeling in the VC task is to extract the style embedding of the source speech  [12] -  [17] , which captures the style information at a global level. Commonly employed strategies to obtain the style embedding include the use of a reference encoder  [18] , a global style token (GST)  [19] , and a variational autoencoder (VAE)  [20] . For instance, in  [13]  and  [15] , GST is adopted to learn a highdimensional representation that encodes source speech style. Du et al.  [17] ,  [21]  introduce a speech emotion recognition model (SER) trained on an emotion speech corpus to extract a hidden representation to represent source speech style. However, the global level style is too coarse to describe the various aspects of style. Hence, the conversion result may inevitably have a speaking style not so consistent with the source speech. In addition to modeling the global level style information, some efforts also have been conducted from a fine-grained level  [22] -  [25] . A straightforward way to represent the finegrained style of source speech is to extract explicit prosodic features  [22] , such as fundamental frequency (f0) and energy. However, handcrafted acoustic features have difficulties to perfectly describe style. Some studies  [23] ,  [25]  attempt to model frame-level style representations from the mel spectrogram along with explicit prosodic features, which have demonstrated superiority in the VC task compared to using explicit prosodic features alone. The most recent work  [24]  tries to describe the style at the phoneme level by leveraging the transcription of the source speech.\n\nWhile the above progress has been made in modeling the source speech style, it is insufficient to accurately represent the richness of style information found in human speech at just one or two levels. In general, human speech has a multi-scale nature  [26]  and can be seen as a combination of multi-scale acoustic factors. The style of speech has rich and detailed variations that manifest at different scales. Specifically, we can categorize an utterance based on its speaking style, e.g., reading style, storytelling style, and poetry style, which is based on the style from the global level. From the local level perspective, each speech unit within an utterance, such as syllable or phoneme, has its own characteristics, such as tone, stress, speed, and pause. In addition to the style reflected from the global and local levels, the style can also naturally be reflected at the frame level when speech is represented by frame-level acoustic features. Many previous efforts  [27] -  [32]  in TTS task have proved the effectiveness of multi-scale style modeling. But most methods primarily focus on modeling style predefined within the corpus. In VC task, modeling arbitrary style without predefined style categories is needed due to the inherent diversity of source speech in practice. Another challenge in VC is the issue of speaker leakage caused by the entanglement of style and speaker timbre  [12] ,  [14] , i.e., the speaker's timbre of the source speech is also passed to the converted speech with style representation, consequently impacting the speaker similarity.\n\nWith the aim to convey the speaking style of source speech while maintaining the target speaker's identity, this paper proposes a new VC model called MSM-VC. Inspired by the multi-scale nature of human speech, MSM-VC employs a multi-scale style modeling approach that captures style at different levels, i.e., global, local, and frame levels. Considering the unique character of the style reflected from each level and preventing speaker timbre leakage from source speech, each level's style is modeled by a specific representation. Specifically, the self-supervised learning (SSL) features extracted by vq-wav2vec  [33]  and bottleneck (BN) features from ASR encoder are used to perform global and local-level style modeling, respectively. As for frame-level style modeling, the prosodic features, including logarithmic domain fundamental frequency (lf0), the short-term average amplitude (energy), and the voice/unvoice flag (VUV), are used. Besides, an explicit constraint module consisting of a speaker classifier  [9]  and a pre-trained speech emotion recognition model (SER)  [34]  is introduced to ensure the retention of the source speech style and target speaker timbre. And inspired by the training process of CycleGAN  [35] , we further employ this explicit constraint module to simulate the style transfer inference process during training to improve the style and speaker disentanglement ability further and alleviate the mismatch between the training and inference process. Experimental results demonstrate that the proposed approach performs superior to the previous stateof-the-art systems on source style modeling while maintaining high speech quality and speaker similarity. Additionally, ablation analysis highlights the importance of each style level, indicating the good design of the proposed model.\n\nOur preliminary work has been presented in  [14] , in which only global-level and frame-level style modeling was considered. In this paper, we improved the model's style modeling ability with the proposed multi-scale style modeling module and explicit constraint module. To sum up, the main contributions of this work are as follows:\n\n• We propose a novel multi-scale framework for source style modeling in voice conversion. The multi-scale style modeling module is designed to model source speech's style from different levels, i.e., global, local, and frame levels, with a specific feature for each level. • We introduce an explicit constraint model to ensure the retention of the source speech style and target speaker's timbre and meanwhile eliminate the mismatch between training and inference. The rest of this paper is organized as follows. Section II reviews related work on style modeling. Section III presents the proposed multi-scale source style modeling method for VC. Section IV describes the experimental details. Section V presents the experimental results. Section VI discusses the performance and limitations of the proposed method and also the possible future research direction. Finally, Section VII concludes the paper. Examples of synthesized speech can be found on the project page  1  .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Ii. Related Work",
      "text": "Regarding the different levels of style modeling, this section will review related works on speaking style modeling. Besides, we will also introduce the literature on speaker and style disentanglement.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "A. Speaking Style Modeling",
      "text": "Using the style category label to explicitly control the speaking style of synthetic speech is an intuitive way  [36] ,  [37] . But it is limited to predefined style categories in the manually labeled corpus. In contrast, obtaining the style representation from reference speech makes it possible to eliminate the dependence on the explicit label  [17] -  [20] . Skerry-Ryan et al.  [18]  propose the reference encoder to extract style embedding with fixed length from reference speech. To make the learned style embedding prominent, global style token (GST)  [19]  model, which extends the reference encoder by adding a style token layer, and variational autoencoder (VAE)  [20]  model are proposed subsequently. Due to the flexibility of these reference speech embedding-based methods, many further efforts are then conducted based on them  [12] ,  [13] ,  [15] ,  [38] -  [45] . For instance, the emotion classifier are used to improve the interpretability of style representation learned by GST  [42] . And to enhance the style control ability, Raitio et al.  [39]  adopt a reference encoder to extract globallevel prosodic features, including pitch, energy, and spectral tilt. Some recent work  [46] -  [50]  tries to model fine-grained style representations. The phoneme-level and word-level style representations are two intuitive local-level representations. Fastspeech2  [46]  adopts a variance predictor to predict the phoneme-level duration, pitch, and energy to represent style in speech. In  [50] , word-level style variation (WSV) is proposed to describe the word-level style, in which WSV is extracted from reference speech during training or text with BERT  [51]  during inference. In addition to word and phoneme levels, finer-grained style representations could be obtained from the spectrogram, resulting in frame-level style features  [4] ,  [16] ,  [22] ,  [23] ,  [25] . In  [4] ,  [22] , explicit prosodic features in the frame level are used to represent the style information. al.  [25]  and Lian et al.  [23]  utilize an implicit style extractor to extract frame-level style from mel spectrogram to enhance the ability of style modeling. Compared with the above-mentioned style modeling methods based on a single coarse or fine-grained level, the most recent methods that consider different levels show superiority in style modeling  [14] ,  [27] -  [32] . For instance, a multi-scale reference encoder is introduced in  [28]  to extract the globallevel and local-level features from reference speech. In  [27] , the authors use phoneme-level emotion strength representations and global-level emotion categories to achieve finegrained emotional speech synthesis. In the multi-speaker and multi-style TTS task  [29] , the phoneme-level features, e.g., pitch, duration, and energy, and global style tags are used to model the speaking style.\n\nDespite the superiority of the multi-scale modeling methods on style modeling, few related efforts of multi-scale style modeling have been conducted on the VC task. Meanwhile, in the absence of ground-truth transcription  [52] , accurate pronunciation unit boundaries are unavailable, making modeling local-level style challenging. Besides, most previous methods mainly focus on modeling styles defined in the corpus. However, in practice, the VC system generally has to face arbitrary source speech with a style and speaker that never appears in the training stage, making an effective speaker-style disentanglement method rather than the predefined categorybased style modeling method necessary.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "B. Style And Speaker Disentanglement",
      "text": "The speaking style and speaker timbre are highly entangled. It is therefore crucial to squeeze out the source speaker's timbre information while modeling the speaking style. Adversarial training  [12] ,  [14] ,  [53] ,  [54]  is a popular method to squeeze out speaker-related information from style representations, which usually utilizes an auxiliary speaker classifier to predict speaker identity. To suppress the information of the source speaker's identity, this speaker classifier is optimized with adversarial training to make the obtained style embedding speaker-indistinguishable. Besides, constraining the relationship between speaker embedding and style embedding is another popular strategy. For instance, mutual information  [21] ,  [55]  and Frobenius norm  [38]  have been adopted to reduce the correlation between speaker representation and style representation. Qian et al.  [4] , Lian et al.  [23] , and Gan et al.  [24]  set the small size bottleneck of style representation to squeeze the speaker information out of the style path. Instead of obtaining the style embedding from the spectrogram, some recent work tries to utilize speaker-irrelevant but style-related features as reference features. For instance, in  [31] , the features extracted from a pre-trained ASR model are used to present the speaking style. Lei et al.  [56]  model speaking style on the perturbed waveform in which the speaker identity has been changed.\n\nDue to the need to face unlabeled and even unseen styles, disentanglement methods based on limited style categories are difficult to apply in our task. Besides, style modeling methods in VC are usually designed in an unsupervised manner without explicit supervision, which makes it difficult to balance the style consistency and speaker similarity of the converted speech. For instance, if the speaking style of the source speech is too different from the speaking style of the target speaker, the speaker similarity will easily be affected. In the training stage, speaking style, linguistic content, and speaker identity all come from the same speech but different speeches during style transfer inference. This mismatch between the training and inference brings insufficient disentanglement and potential performance degradation.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Iii. Methodology",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "A. Overview",
      "text": "The proposed MSM-VC is built with a typical encoderdecoder architecture, as shown in Fig.  1 . This framework consists of three main components: a multi-scale style modeling module, a conversion module, and an explicit constraint module. The multi-scale style modeling module extracts comprehensive style representations from three levels, i.e., global,  local, and frame levels. To obtain the global-level and locallevel representations, SSL and BN features extracted from source speech are adopted, respectively. Prosodic features, including lf0, VUV, and energy, are used to represent the frame-level style. The conversion module, which consists of a conformer encoder  [57]  and an auto-regressive decoder  [58] , takes speaker id, ASR-based content representation BN, and style representations from different levels as input and outputs the mel spectrogram with target speaker timbre and source speaker's speaking style. Besides, to effectively optimize the proposed MSM-VC, an explicit constraint module consisting of a speaker classifier and a pre-trained SER model is introduced in our framework during training. Finally, a modified LPCnet  [59]  is adopted to reconstruct waveform from mel spectrogram. Note that ASR, vq-wav2vec, SER, and LPCnet are pre-trained models and will not be optimized further during the training of the proposed model.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "B. Global-Level Style Modeling",
      "text": "The global-level style indicates overall speaker style, intensity, and diversity in utterance. The global-level style representation is generally extracted from the mel spectrogram or BN of reference speech using a neural style extractor, e.g., GST, VAE, and reference encoder. However, the mel spectrogram is far from ideal for style modeling due to the redundant information, such as speaker timbre. In contrast, while limited irrelevant acoustic information remained in the BN, the damage to the style information makes it hard to obtain a comprehensive style from BN. Inspired by the characteristic of discrete self-supervised learning feature (SSL) extracted by the self-supervised model vq-wav2vec  [33] ,  [60] , which contains less speaker information than mel spectrogram and richer style information than BN, SSL is adopted for the global-level style modeling. This conclusion will be verified in Section V-D.\n\nAs shown in Fig.  2 , the global-level style modeling module consists of three parts, i.e., pre-trained vq-wav2vec model, vq-wav2vec indices look-up table, and global-level reference encoder. The SSL features are first extracted by the pretrained vq-wav2vec model with the reference speech as input, resulting in features with the dimensionality of D SSL and sequence length of T . Here, the SSL feature value indicates the VQ codebook's index. Following Huang et al.  [60] , the SSL features are then separated into different groups along the dimension axis to look up different embedding tables, which is helpful for the convergence of training. The resulting embeddings are used as input to the global-level reference encoder  [18]  to obtain the final global-level style embedding. It is essential to note that the SSL feature still contains speaker information which may be conveyed to the converted spectrogram (See Section. V-D). In order to prevent the speaker timbre of the source speaker from leaking to the target speech, we set a small bottleneck  [4] ,  [23]  in the global-level style embedding with the dimension D glo . Finally, the obtained global-level style embedding is repeated T times along the time dimension and concatenated with the conformer output.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "C. Local-Level Style Modeling",
      "text": "While global-level style information can convey the overall speaking style, the local style expression, e.g., tone, stress, speed, and pause, is also crucial for speaking style. Therefore, it is important to model the local style. Generally, the local style expression is reflected in the speech units, e.g., phonemes or syllables. It is natural to model local style from the phoneme level or syllable level. Unfortunately, the lack of ground-truth transcription in practice makes accurate speech pronunciation unit boundaries inaccessible. To face this challenge, pseudospeech units are obtained with fixed-length speech segments. While SSL features show superiority in reducing redundant information and maintaining style information, it is not fit to work as the local style modeling feature due to the mispronunciation issue  [61] . The discrete process may lead the SSL features of vq-wav2vec to discard some linguistic content. In contrast, the training object of ASR model makes BN extracted from a pre-trained ASR model contain the integrity of the pronunciation information and the consistency within the speech pronunciation unit. Therefore, instead of SSL, here, BN is used for the local-level style modeling.\n\nAs shown in Fig.  2 , we use BN extracted by a pretrained ASR model as local-level modeling's input with the dimension of D BN and the length of T . A modified reference encoder  [23]  is adopted to extract frame-level features. The modified reference encoder consists of six 2D convolutional layers and a GRU layer. The output of the GRU is taken as the frame-level feature, which is then downsampled along the time axis with a fixed ratio γ to obtain local-level features. Specifically, taking γ frames as a pronunciation unit, we divide the sequence into several segments, and the average of the frames within each segment represents the current pronunciation unit. Then the local-level feature with sequence length of T /γ is broadcast-concatenated to the conformer output. As the common duration of consonant-vowel syllables ranges from 150ms to 200ms  [62] , we use γ as 16 in practice. To be specific, the duration of each speech pronunciation unit is 200ms with a frameshift of 12.5ms. This pseudo speech unit feature can get rid of the dependency on the transcriptions, making it convenient in the VC task. Meanwhile, the characteristics of BN also ensure that speaker timbre leakage will not happen.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "D. Frame-Level Style Modeling",
      "text": "When speech is represented as frame-level acoustic features, e.g., spectrogram, the style naturally varies with the frame. Therefore, in addition to global level and local level, finer grain, i.e., frame-level style, should also be considered. To this end, source speech's explicit acoustic features, including pitch and energy, are adopted. To be specific, lf0 and shortterm average amplitude are extracted from source speech to present the pitch and energy, respectively. Besides, VUV, which indicates the frame's voicing, is also used in framelevel style modeling. In practice, lf0 and energy of each utterance are normalized to [0, 1] by utterance-level minmax normalization, which is helpful to prevent naturalness and speaker similarity degradation caused by the unseen style and unseen speaker during the inference process. Normalized energy and lf0 are used to indicate the trend of pitch and energy in the source speech. These features are embedded by linear layers respectively to work as frame-level style embeddings.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "E. Explicit Constraint Module",
      "text": "Achieving high style modeling performance and speaker similarity is an essential goal of the VC task. Since specific representations mentioned above still contain speaker-related information (See Section. V-D), only using them is insufficient to achieve information coupling between speaker and style. Meanwhile, style modeling methods in VC are usually designed in an unsupervised manner without explicit supervision, which makes it difficult to balance the speaking style and timbre, in which the former should be consistent with the source speech while the latter should same as the target speaker. If the speaking style of the source speech is far from that of the target speaker, the speaker similarity will easily be affected. Thus style matching to source speech and speaker similarity to the target speaker should be simultaneously considered to explicitly guide the disentanglement process and balance the source style modeling and target speaker timbre preservation. In this paper, an explicit constraint module consisting of a pre-trained SER model and a speaker classifier is introduced to achieve this end.\n\n1) Style matching to source speech: An intuitive way to constrain the style category is to use a style classification objective function. However, since the speaking styles are distributed in a continuous space, discrete style labels are too coarse to capture finer-grained variation between styles and cannot cover all possible styles. In contrast, representations directly extracted from speech via learnable deep neural networks are considered more suitable as style descriptor  [34] ,  [41] ,  [63] . This representation can capture stylerelated attributes from a specific utterance, even if the speaking style cannot be accurately represented by manually defined labels. Therefore, the style matching loss here is suitable for measuring the style consistency between style representations of source speech and converted speech. In practice, pre-trained on a style classification task, an SER model is used as a style descriptor to obtain the style representations and calculate the style matching loss.\n\n2) Speaker similarity to target speaker: Since the target speaker's recordings are contained in the training dataset, a speaker classifier is commonly introduced to ensure the speaker similarity to the target speaker, as in Kameoka et al.  [9] . In practice, the speaker classifier takes the predicted mel spectrogram as input and outputs the probability of this spectrogram belonging to the target speaker identity.\n\nGenerally, in the training phase, all conditional information, including speaking style, content, and speaker identity, comes from the same utterance and is used to reconstruct the original utterance. However, in the style transfer scenario, i.e., the inference stage, the content and speaking style are from the source speech, while the speaker identity is from the target speaker, resulting in inconsistency between these two phases. This inconsistency could make it hard to measure the disentanglement ability during training and result in limited performance for the VC task. To solve this problem, inspired by the training process of CycleGAN  [35] , applying the explicit constraint module allows us to divide the training process into two different modes, i.e., reconstruction mode, and simulation mode. In the reconstruction mode, the whole model is trained using paired data with style, content, and speaker id of the same utterance. In contrast, to perform style transfer simulation, style and content are extracted from the same utterance, while the speaker id is randomly assigned. Without ground-truth utterance in simulation mode, the explicit constraint module plays a core role in the balance of style modeling and speaker timbre preservation.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "F. Objective Functions",
      "text": "To effectively optimize the proposed model, style matching loss, speaker classification loss, and mel reconstruction loss are introduced to ensure style consistency with the source speech, speaker timbre similarity with the target speaker, and the reconstruction quality of the mel spectrogram, respectively.\n\n1) Style matching loss: The matching loss measured by the SER model ensures that the converted speech has the same style as the source speech. As shown in Fig.  3 , to constrain the style from different levels, style-related features extracted from different layers of the SER model are obtained. Following  [34] , 2D convolution extracts a variable-length hidden representation h low from the mel spectrogram Y , and the GRU extracts the fixed-length vector h middle from the temporal information of h low . Besides, we take the hidden representation calculated by the second FC layer as h high . Low, middle and high stand for the abstraction degree of the hidden representation of the style. Finally, with the features extracted from different layers, the style matching loss between the source mel spectrogram Y and the predicted mel spectrogram Ŷ can be defined as\n\nwhere h and ĥ are extracted from the SER model using Y and Ŷ as input, respectively. 2) Speaker classification loss: The architecture of the speaker classifier is the same as the SER model. Same as the practice in  [9] , it takes the predicted mel spectrogram as input to predict the current speaker identity. The corresponding speaker classification loss is defined as:\n\nwhere s represents the given target speaker label, Ŷ is the predicted mel spectrogram, and P (s| Ŷ ) represents the probability of being identified as speaker s under the condition of inputting Ŷ .\n\n3) Mel reconstruction loss: The mel reconstruction loss, working as a basic objective function for the speech synthesis, is to make the model create reasonable target speech based on style, speaker, and content. L2 distance between predicted mel spectrogram Y and ground-truth mel spectrogram Ŷ is adopted as the mel reconstruction loss, which is defined as:",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "4) Overall Objective Function:",
      "text": "The overall objective function is described as follows:\n\nwhere the value of α is 0 or 1 to indicate the simulation mode and the reconstruction mode of model training, respectively. In particular, due to the lack of ground-truth mel spectrogram in the simulation mode, the model is only optimized with L style and L speaker due to the lack of ground-truth mel spectrogram. Note that, due to the high similarity between h low and mel spectrogram  [34] , the low-level style feature h low contains rich speaker-related information. Therefore, the h low -based style loss L style low is neither considered in the simulation mode to avoid the effect on the speaker similarity.\n\nIn practice, we first train the model in reconstruction mode (α = 1) until the model is converged. Then, we finetune the trained model in both reconstruction and simulation modes. Since only training exists in the simulation mode, where no ground-truth mel spectrogram is available for mel reconstruction loss, the model attends to fit the other training objectives and ignores the mel reconstruction ability which has been learned in the first stage. Thus, following the similar process of CycleGAN  [35] , to ensure the model's mel reconstruction ability, the reconstruction mode is introduced in the finetune stage and used alternately with the simulation mode. Besides, only the decoder is updated in this stage to further prevent the forgetting of learned reconstruction ability, since intuitively more training parameters are more likely to lead to overfitting  [64] ,  [65] .",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Iv. Experimental Setup",
      "text": "To evaluate the performance of MSM-VC on the VC task, experiments are conducted on a Chinese multi-speaker speech corpus. In this section, the databases for the voice conversion model and also for the pre-trained models will be introduced. Besides, implementation details, compared methods, and the evaluation method will also be introduced.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "A. Corpus",
      "text": "An internal multi-speaker speech corpus licensed from Databaker 2  is adopted to evaluate the proposed method. This corpus is a standard Mandarin reading corpus recorded by 57 professional voice actors, including 30 females and 27 males. Each speaker performs 500 utterances, resulting in a total duration of 42h. One female speaker labeled with s1 in this corpus is used as the target speaker. A test set contains a series of highly expressive speech in different scenarios, including emotions, movies, novels, daily conversation, and variety shows. Twenty sentences are randomly selected from 6 kinds of emotions and the other four speaking styles, respectively, resulting in 200 utterances for the test. In the experiments, the ASR model is trained with 10k hours of speech from Wenetspeech  [66] . The SER model is trained with the open-source emotional data ESD  [63] , which is recorded by 20 Chinese and English speakers by performing five kinds of emotions, i.e., Angry, Happy, Neutral, Sad, and Surprise. The vq-wav2vec is an open-source pre-trained model trained on the 960h Librispeech dataset  [67] .",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "B. Implement Details",
      "text": "All speech utterances are downsampled to 16kHz and represented by 80-dim mel spectrogram which is computed with 50ms frame length and 12.5ms frame shift. The ASR system is a TDNN-F model implemented by Kaidi toolkits  [68] . We use the 256-dim bottleneck features (D BN = 256) as the linguistic representation, which is extracted from the last fully-connected layer before softmax. The officially released vq-wav2vec model 3  is used to extract 2-dim SSL features (D SSL = 2). Pyworld toolkit 4  is adopted to extract F0. Note that lf0, VUV, and energy used in this paper are all 1-dim features (D LF 0 = D V U V = D Energy = 1). Modified LPCnet  [59]  based on official implementation 5  is adopted to reconstruct waveform from mel spectrogram. We use groundtruth mel spectrogram to train the modified LPCnet on the multi-speaker corpus and finetune it with data of the target speaker s 1 .\n\nThe conformer encoder consists of one conformer block, which contains eight heads of multi-head attention module, convolution module with 31 kernel size, a feed-forward module with one expansion factor and the settings of other parts remain the same as the official setting  6  . The architecture and hyperparameters of the global-reference encoder keep the origin configuration  [18] . To be specific, it consists of 6 convolution layers and a GRU layer. Each convolution layer is composed of 3×3 filters with 2×2 stride, SAME padding, and ReLU activation. The number of filters in each layer is 32, 32, 64, 64, 128, and 128, respectively. Batch normalization is applied to each layer. The output of convolution layers is fed into the GRU with four units (D glo = 4). Different from the global-level reference encoder, the frame-level reference encoder adopts convolution layers composed of 3 × 3 filters with 1 × 2 stride and the outputs of the GRU at every timestep form the frame-level representation (D seg = 4). The decoder is an auto-regressive module  [58]  which consists of prenet, decoder RNN, and postnet. In the reconstruction training stage, the conversion model is trained for 240 epochs with batch size of 32. Adam optimizer is used to optimize the model with learning rate decay, which starts from 1 × 10 -3 and decays every 20 epochs with decay rate of 0.7. In the simulation training stage, the conversion model is trained for 70 epochs, in which process the learning rate starts from 1 × 10 -6 and decays every 20 epochs with decay rate of 0.5.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "C. Compared Methods",
      "text": "To evaluate the performance of the proposed method MSM-VC on the VC task, three recent state-of-the-art systems designed for VC are compared in the experiments. These compared methods represent three typical VC approaches, i.e., global-level reference embedding based method, framelevel representation based method, and a hybrid strategy. Note that all systems use the same vocoder LPCNET to reconstruct waveform from the mel spectrogram. Details of these compared methods are introduced as followings.\n\nGST-VC  [13]  is a typical global reference representationbased VC method. In this model, GST  [19]  extracts global style information from the source speech's mel spectrogram. As for the linguistic content of source speech, a pre-trained ASR model is used to obtain the phoneme sequence. Then, the converted speech is produced conditioned on the phoneme sequence, global style information, and target speaker identity information.\n\nREF-VC  [23]  is a frame-level style representation-based method. In this model, the authors utilize a modified reference encoder to learn frame-level style representation from the mel spectrogram in an unsupervised manner. The speaking style of source speech is conveyed to the converted result by this learned frame-level style representation together with f0.\n\nHybrid-VC  [14]  is the preliminary work of the current method, in which explicit prosodic features (lf0 and energy) together with global-level style representation are used to model the speaking style. Unlike the current work, this preliminary method simply describes style from two levels and lacks explicit supervision for style and speaker.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "D. Evaluation Metrics",
      "text": "With the input of source speech, the goal of the VC task is to obtain the converted speech that shares the same linguistic content and speaking style as the source speech but with the timbre of the target speaker. Therefore, there are two aspects that should be considered in the evaluation: 1) the style similarity between the source speech and converted speech; 2) the speaker similarity between the target speaker and that of converted speech. Besides, as a kind of speech synthesis task, the quality of the produced speech should also be evaluated. To evaluate the converted speech from the above three aspects, both objective and subjective evaluation methods are conducted in the experiment.\n\n1) Objective metrics: Considering both lf0 and energy are style-related acoustic features, Pearson correlation coefficients of lf0 and energy are calculated between the source speech and convert speech to objectively reflect the style similarity. Higher Pearson correlation coefficients of lf0 or energy indicate better style similarity. As for the evaluation of speaker similarity, a pre-trained speaker verification system  [69]  trained on CN-Celeb  [70]  is introduced. Cosine similarity between the SV model-based speaker embeddings of converted speech and speech from the target speaker shows speaker timbre similarity between them. Higher cosine similarity means better similarity between speaker timbres of converted speech and the target speaker.\n\n2) Subjective metrics: In addition to the objective evaluation, a human perceptual rating experiment is performed to evaluate the converted speech in terms of style similarity (between converted speech and source speech), speech quality, and speaker similarity (between converted speech and target speaker speech). To facilitate the comparison between different models in terms of style modeling, a comparative mean opinion score (CMOS) test is also performed in the experiment. In the test, given the reference speech, listeners are asked to rate whether the first sample is better or worse than the second one in terms of style similarity, using a seven-point scale comprised of +3 (much better), +2 (better), +1 (slightly better), 0 (same), -1 (slightly worse), -2 (worse), -3 (much worse). Besides, A/B preference is also presented, which can be simultaneously obtained during the CMOS test, to provide another perspective into the comparison. Different from the",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "V. Experimental Results",
      "text": "Experimental results, including the comparison with other methods and ablation studies, will be presented in this section. Besides, the rationality of the feature choosing for differentlevel style modeling is also presented. We also investigate the model's behavior in two training modes and the model size of different systems. We highly recommend readers listen to the converted samples from https://kerwinchao.github.io/ VCStyleModeling.github.io.\n\nA. Subjective Evaluations 1) Source style modeling performance: Table  I  presents the comparison of the style modeling performances between the proposed and compared methods, in which subjective evaluation with CMOS is reported. In this subjective rating test, participants have to rate two compared samples, in which one is obtained by the proposed method and another one is from a compared method. A positive value means that the proposed method is better than the compared one and vice versa. Besides, the A/B preference test is also presented to give further evaluation between the compared methods and the proposed method.\n\nAs shown in this table, all CMOS values are larger than 0, which means that compared with all of these listed methods, the proposed method shows superiority in style modeling. And the scores of A/B preference also show that MSM-VC significantly outperforms the compared methods (p-value smaller than 0.01). These results demonstrate the effectiveness of the proposed method on style preservation. When we pay attention to the specific CMOS and preference scores compared with different methods, it can be found that the largest performance gap exists between the proposed method and GST-VC, indicating the inferiority of GST-VC in style modeling. This poor performance demonstrates that only modeling the style from a global coarse-grained is insufficient for style preserving in the VC task. In contrast, modeling the style from a fine-grained level, e.g., the frame-level-based method REF-VC, results in better performance. However, no matter the coarse-grained style modeling method or fine-grained style modeling method, this single-level modeling method is inferior to Hybrid-VC and the proposed method, which indicates the importance of modeling style from different levels.\n\n2) Speech quality and speaker similarity: In addition to style modeling ability, speech quality and speaker similarity are also important aspects to evaluate the performance of a VC model. The results of MOS tests in terms of speech quality and speaker similarity for different models are shown in Table  II . Compared with REF-VC and Hybrid-VC, which are obviously superior to GST-VC in terms of style modeling, the proposed method achieves better MOS scores both in speech quality and speaker similarity. Moreover, significant tests confirm that MSM-VC outperforms REF-VC (p-value smaller than 0.05) but no significant difference is shown between the MOS results of MSM-VC and Hybrid-VC.\n\nCompared with the proposed method, GST-VC gets better MOS values in speech quality and speaker similarity. This good performance is attributable to the phoneme-based content modeling method and global-level modeling strategy. To be specific, using the phoneme sequence as the content input can filter out all information that is unrelated to the content from the source speech, thus preventing the effect from the source speech to the final results. Besides, the global-level style embedding only indicates an overall style, which could bring very limited noise information from the source speech to the converted speech. However, this good performance in the speech quality and speaker similarity is at the expense of style modeling because of the limited information taken from the source speech. Furthermore, the significance test result between GST-VC and MSM-VC shows that the differences are not significant (p-value greater than 0.05) in the speech quality and speaker similarity. It demonstrates that MSM-VC can maintain high speech quality and speaker similarity while achieving modeling source style.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "B. Objective Evaluations",
      "text": "The objective comparison among different models is shown in Table V-B, in which a higher Pearson correlation coefficient indicates the speaking style of source speech is better reflected in the converted speech. As can be seen from this table, while the energy Pearson coefficient of MSM-VC is slightly lower than Hybrid-VC, the proposed system achieves overall better scores than other compared methods, which is consistent with the subjective evaluation results. The objective comparison of different models on the speaker similarity also presents similar results to that obtained in the subjective evaluation test. To be specific, GST-VC gets the highest speaker similarity, which means that GST-VC has the best performance in achieving converted speech with the target speaker's timbre. The proposed MSM-VC ranks next to GST-VC and shows better performance than REF-VC and Hybrid-VC. As discussed in the subjective evaluation and also the worst performance of GST-VC in the subjective style modeling evaluation, the proposed method presents the most balanced performance in the style modeling and speaker similarity, demonstrating the superiority of MSM-VC in source style modeling while achieving high speaker similarity.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "C. Component Analysis",
      "text": "In this section, ablation studies will be conducted to validate the effectiveness of each component of MSM-VC, i.e., the multi-scale style modeling module, explicit constraint module, and the simulation strategy for the training of MSM-VC.\n\n1) Effectiveness of different style level: As we argue that human speech's multi-scale nature makes it is necessary to model the speaking style from different levels, in this section, we would like to analyze the effectiveness of style modeling from each level on the VC task. To be specific, several variants of MSM-VC are evaluated by dropping one of the style modeling levels. As shown in Table  IV   As can be seen from this table, all CMOS values are positive, indicating that without modeling the style from any level will decrease the performance of style modeling. And the scores of A/B preference also show similar results. Among the three levels for style modeling, the frame-level modeling module shows the most important role in style modeling, by dropping which the CMOS value is larger than 1. This obvious effect is attributed to those frame-level style representations, i.e., lf0, VUV, and energy, which are able to represent finegrained style from different perspectives directly. The globallevel modeling module and local-level modeling module also play important roles in the final style modeling, which can be demonstrated by the large CMOS values that are larger than 0.4. All these results show the necessity to model the speaking style from different levels and also indicate the good design of the proposed multi-scale modeling method.\n\n2) Effectiveness of explicit constraint module: An ablation study to analyze the effectiveness of the explicit constraint module and simulation training stage driven by explicit constraints is also conducted. The results are shown in Table  V , in which the performances of two MSM-VC variants obtained by dropping speaker classifier and SER, referred to as w/o Speaker Classifier and w/o SER respectively, are presented. Besides, the performance of MSM-VC trained without the simulation stage, named w/o Simulation, is also compared.\n\nAs shown in this table, the speaker classifier and SER show obvious effects on speaker similarity and style consistency, respectively. To be specific, the dropping of the speaker classifier brings a 3.5% relative decrease compared with MSM-VC in speaker cosine similarity. After removing the SER, the pearson coefficients of lf0 and energy decrease by 5.5% and 6.2% compared with the proposed method. As for the speech quality, a tiny negative impact exists when the speaker classifier or SER module is utilized. However, this negative impact is very limited, indicating the effectiveness of the speaker classifier and SER in modeling the speaker timbre and style while maintaining the quality of synthesized speech. Furthermore, as shown in Table  V , the simulation training stage also shows a positive effect on speaker similarity and style modeling. The model trained without the simulation stage presents a 1.5% speaker similarity drop. The pearson coefficients of lf0 and energy also show 4.3% and 6.4% relative decrease. While a slight performance decrease appears when the simulation training stage is adopted in terms of speech quality, similar to the effect of using speaker classifier and SER, this effect is quite tiny. The reason behind this speech quality decrease caused by the explicit constraint is straightforward, which is caused by the lack of reconstruction constraints for performing these explicit losses. However, the obvious improvements to the speaker modeling and style modeling but a slight decrease in the speech quality demonstrates the effectiveness of the explicit constraint module in the VC task.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "D. Analysis Of Feature Choosing For Style Modeling",
      "text": "Using the appropriate feature for style modeling of a specific level is important. In MSM-VC, SSL and BN features are adopted for the global and local-level style modeling. Here, we would like to show the information richness of these features in terms of style and speaker information. In addition to the BN and SSL features, mel spectrogram, which is the commonly used acoustic feature, is also compared. Due to the lack of direct indicators of information richness, these features are compared in several prediction tasks, alternatively. Specifically, each kind of feature is used to train models to predict f0, energy, and speaker, respectively. The predicted accuracy of f0 and energy are evaluated by the MSE between the predicted results and ground truth. A lower MSE of f0 and energy means that more style information is contained in the feature. The speaker classification performance is evaluated by the classification accuracy, and higher speaker accuracy indicates more contained speaker information. We use the multi-speaker corpus mentioned in Section IV for the model training and randomly select 50 sentences from each speaker as the test set. The model's structure is the same as the SER.\n\nAs shown in Table  VI , the mel spectrogram achieves the best performance in speaker classification and prosodic feature prediction, which is as expected due to the least acoustic information loss compared with BN and SSL features. This high speaker-related correlation makes mel spectrogram a nonideal styling modeling feature because of the speaker leakage issue caused by the speaker information. In contrast, BN and SSL show less speaker information. The characteristic of SSL has richer style and speaker information than BN but less speaker information than mel spectrogram making SSL suitable for global style modeling, in which the obtaining of the global style embedding could effectively reduce the speaker-related information. As for the modeling of locallevel style modeling, more information could be kept in the final style embedding, thus making the feature with rich speaker information unsuitable. Therefore, the BN feature, which contains the least speaker information, can prevent the speaker leakage issue when it is taken for the local-level style modeling. While less style information is contained in BN and SSL compared with mel spectrogram, modeling the style from different levels can effectively bridge this gap. Furthermore, considering that the vq-wav2vec model is trained on English data, we also tested the style richness of SSL extracted from English data, referred to as SSL(EN). Fifty-seven speakers from VCTK  [71]  are selected for training and 50 sentences from each speaker are used as the test set. As shown in Table  VI , SSL(EN) extracted from English speech has a similar style richness to that extracted from Chinese speech, which indicates the language independence of style information carried by the SSL feature.",
      "page_start": 10,
      "page_end": 11
    },
    {
      "section_name": "Vi. Discussion",
      "text": "In this paper, a multi-scale style modeling method for the VC task, named MSM-VC, is proposed to preserve the speaking style of source speech in converted speech. The multi-scale modeling module is designed to model the source speech's style from different levels, i.e., global, local, and frame levels. Besides, an explicit constraint module and simulation training strategy are proposed to directly guide the training of MSM-VC towards the aim of the VC task, i.e., preserving the speaking style of source speech while maintaining the target speaker's timbre. In this section, more details of MSM-VC and its limitations will be discussed as follows.\n\n1) Visualization of reconstruction and simulation modes: To explore the behavior of the simulation mode, the finetune stage's loss curves are visualized in different aspects, including mel reconstruction (L recons ), style matching (L style ), and speaker classification (L speaker ). As shown in Fig.  4 , the two training modes show different behaviors. Specifically, from the beginning of the finetune stage, in terms of L style and L speaker , the loss value of the simulation mode is significantly higher than that of the reconstruction mode. This high style matching loss and speaker classification loss obviously drop with the application of the simulation mode. Besides, in terms of the reconstruction loss, the simulation mode does not bring negative effects to the reconstruction process, indicating the effectiveness of the simulation mode in alleviating the mismatch between training and inference and improving the disentanglement ability.\n\n2) Model size investigation of different systems: To investigate the impact of model size on VC performance, the trainable parameters amount among comparison, ablation, and proposed models is shown in Table . VII. The number of MSM-VC's trainable parameters has no significant increase compared to other comparison systems. Compared with ablation systems, the differences in the trainable parameter amount come from the multi-scale style extraction of the source speech and the generation of constraints for the training process, which bring the enhancement of style modeling and target speaker timber maintenance. Besides, we also investigated the performance of the base model Base-VC, which is a variant  Simply expanding the number of model parameters is unlikely to provide additional style information.\n\n3) Limitations: While experiments have demonstrated the good performance of the proposed model on source style modeling in most scenes, we have to point out that some limitations still exist. Specifically, in some extreme cases, such as speech with high emotional intensity, crying, laughing, shouting, and murmuring, the intelligibility and quality of the converted speech will be greatly affected. The high-quality data of this kind of speech is very difficult to collect, and it is hard to cover all categories simultaneously. Improving the stability and generalization of semantic representation is an interesting topic that needs to be paid more attention to in practice. Furthermore, MSM-VC requires several pre-trained models, e.g., ASR, SER, and vq-wav2vec, which would be disadvantageous in a practical scenario. As presented in Table  VII , we tested our model's real-time performance on a single NVIDIA RTX 2080 GPU, achieving a rate of 0.088, which is slightly slower than the comparison methods. While our model is capable of transferring source style to target speakers, it is still non-real-time, limiting its potential applications. Developing a streaming model to accomplish this task would be valuable.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Vii. Conclusion",
      "text": "This paper proposes a multi-style modeling approach for the voice conversion task based on a recognition-synthesis framework, which can convey not only the linguistic content but also the speaking style of source speech to the converted speech. In order to obtain comprehensive speaker-irrelevant style representations, the multi-level style modeling module obtains frame-level, local-level, and global-level styles from specific representations. Besides, to directly guide the source style modeling and target speaker timbre preservation of the proposed model, an explicit constraint module consisting of a speaker classifier and a pre-trained speech emotion model is introduced. This explicit constraint module also makes it possible to simulate the style transfer inference process during the training to encourage the speaker and style disentanglement and prevent the mismatch between training and inference. Experimental results demonstrate that the proposed approach achieves superior performance in conveying source speech style while maintaining target speaker timbre and good speech quality.",
      "page_start": 11,
      "page_end": 11
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The architecture of the proposed MSM-VC model. Note that local-level style representation is concatenated with BN, and meanwhile, global-level",
      "page": 3
    },
    {
      "caption": "Figure 1: This framework",
      "page": 3
    },
    {
      "caption": "Figure 2: The architecture of the multi-scale style modeling module. Please note that in the figure, we assume γ = 3, we average each group of three vectors",
      "page": 4
    },
    {
      "caption": "Figure 2: , the global-level style modeling module",
      "page": 4
    },
    {
      "caption": "Figure 2: , we use BN extracted by a pre-",
      "page": 4
    },
    {
      "caption": "Figure 3: , to constrain the",
      "page": 5
    },
    {
      "caption": "Figure 3: The network architecture of the SER model. The features of the three",
      "page": 6
    },
    {
      "caption": "Figure 4: Visualization of loss curves for reconstruction and simulation modes in finetune stage.",
      "page": 11
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Style CMOS": "",
          "Preference (%)": "Compared\nNeutral MSM-VC\nMethod"
        },
        {
          "Style CMOS": "1.393\n0.507\n0.461",
          "Preference (%)": "5.9\n11.6\n82.5\n20.0\n26.2\n53.8\n21.6\n28.4\n50.0"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "MOS (↑)": "p\nSpeech Quality"
        },
        {
          "MOS (↑)": "3.60±0.094\n0.331\n3.40±0.106\n0.014\n3.53±0.117\n0.579\n3.54±0.083\n-"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Cosine\nSimilarity\n(↑, 0.881)": "",
          "Pearson Coefficient (↑)": "Lf0",
          "Speech\nQuality(↑)": ""
        },
        {
          "Cosine\nSimilarity\n(↑, 0.881)": "0.794\n0.827\n0.810\n0.823",
          "Pearson Coefficient (↑)": "0.748\n0.715\n0.724\n0.757",
          "Speech\nQuality(↑)": "3.56±0.081\n3.58±0.089\n3.56±0.072\n3.54±0.083"
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Pearson Coefficient (↑)": "Lf0",
          "Cosine Similarity\n(↑, 0.881)": ""
        },
        {
          "Pearson Coefficient (↑)": "0.633\n0.738\n0.742\n0.757",
          "Cosine Similarity\n(↑, 0.881)": "0.828\n0.791\n0.811\n0.823"
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Style CMOS": "",
          "Preference (%)": "Compared\nNeutral MSM-VC\nMethod"
        },
        {
          "Style CMOS": "0.40\n0.44\n1.06",
          "Preference (%)": "17.3\n37.4\n45.3\n17.4\n32.0\n50.6\n9.4\n12.0\n78.6"
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "MSE (↓)": "Lf0",
          "Speaker Accuracy (↑)": ""
        },
        {
          "MSE (↓)": "1.25\n2.14\n1.70\n1.64",
          "Speaker Accuracy (↑)": "0.93\n0.48\n0.73\n-"
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Trainable Parameters (M)": "4.44\n3.92\n4.24",
          "Real-time Factor": "0.087\n0.083\n0.088"
        },
        {
          "Trainable Parameters (M)": "4.14\n3.97\n3.85\n4.14\n3.75\n4.14\n4.14",
          "Real-time Factor": "0.088\n0.087\n0.085\n0.088\n0.088\n0.088\n0.088"
        }
      ],
      "page": 11
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Cosine\nSimilarity\n(↑, 0.881)": "",
          "Pearson\nCoefficient (↑)": "Lf0",
          "Speech\nQuality(↑)": ""
        },
        {
          "Cosine\nSimilarity\n(↑, 0.881)": "0.830\n0.833\n0.823",
          "Pearson\nCoefficient (↑)": "0.596\n0.603\n0.757",
          "Speech\nQuality(↑)": "3.57±0.091\n3.58±0.109\n3.54±0.083"
        }
      ],
      "page": 11
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Voice conversion based on maximum-likelihood estimation of spectral parameter trajectory",
      "authors": [
        "T Toda",
        "A Black",
        "K Tokuda"
      ],
      "year": "2007",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "2",
      "title": "Voice conversion using dynamic frequency warping with amplitude scaling, for parallel or nonparallel corpora",
      "authors": [
        "E Godoy",
        "O Rosec",
        "T Chonavel"
      ],
      "year": "2012",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "3",
      "title": "Exemplar-based sparse representation with residual compensation for voice conversion",
      "authors": [
        "Z Wu",
        "T Virtanen",
        "E Chng",
        "H Li"
      ],
      "year": "2014",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "4",
      "title": "AutoVC: Zero-shot voice style transfer with only autoencoder loss",
      "authors": [
        "K Qian",
        "Y Zhang",
        "S Chang",
        "X Yang",
        "M Hasegawa-Johnson"
      ],
      "year": "2019",
      "venue": "International Conference on Machine Learning (ICML)"
    },
    {
      "citation_id": "5",
      "title": "Adversarially learning disentangled speech representations for robust multi-factor voice conversion",
      "authors": [
        "J Wang",
        "J Li",
        "X Zhao",
        "Z Wu",
        "S Kang",
        "H Meng"
      ],
      "year": "2021",
      "venue": "International Speech Communication Association (Interspeech)"
    },
    {
      "citation_id": "6",
      "title": "Voice conversion from non-parallel corpora using variational auto-encoder",
      "authors": [
        "C.-C Hsu",
        "H.-T Hwang",
        "Y.-C Wu",
        "Y Tsao",
        "H.-M Wang"
      ],
      "year": "2016",
      "venue": "Asia-Pacific Signal and Information Processing Association (APSIPA)"
    },
    {
      "citation_id": "7",
      "title": "One-shot voice conversion by separating speaker and content representations with instance normalization",
      "authors": [
        "J.-C Chou",
        "C.-C Yeh",
        "H.-Y Lee"
      ],
      "year": "2019",
      "venue": "International Speech Communication Association (Interspeech)"
    },
    {
      "citation_id": "8",
      "title": "Voice impersonation using generative adversarial networks",
      "authors": [
        "Y Gao",
        "R Singh",
        "B Raj"
      ],
      "year": "2018",
      "venue": "International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "9",
      "title": "StarGAN-VC: Nonparallel many-to-many voice conversion using star generative adversarial networks",
      "authors": [
        "H Kameoka",
        "T Kaneko",
        "K Tanaka",
        "N Hojo"
      ],
      "year": "2018",
      "venue": "Spoken Language Technology Workshop (SLT)"
    },
    {
      "citation_id": "10",
      "title": "Phonetic posteriorgrams for many-to-one voice conversion without parallel data training",
      "authors": [
        "L Sun",
        "K Li",
        "H Wang",
        "S Kang",
        "H Meng"
      ],
      "year": "2016",
      "venue": "International Conference on Multimedia and Expo"
    },
    {
      "citation_id": "11",
      "title": "Oneshot voice conversion for style transfer based on speaker adaptation",
      "authors": [
        "Z Wang",
        "Q Xie",
        "T Li",
        "H Du",
        "L Xie",
        "P Zhu",
        "M Bi"
      ],
      "year": "2022",
      "venue": "International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "12",
      "title": "Voice conversion by cascading automatic speech recognition and text-to-speech synthesis with prosody transfer",
      "authors": [
        "J.-X Zhang",
        "L.-J Liu",
        "Y.-N Chen",
        "Y.-J Hu",
        "Y Jiang",
        "Z.-H Ling",
        "L.-R Dai"
      ],
      "year": "2020",
      "venue": "Joint Workshop for the Blizzard Challenge and Voice Conversion Challenge"
    },
    {
      "citation_id": "13",
      "title": "Transferring source style in non-parallel voice conversion",
      "authors": [
        "S Liu",
        "Y Cao",
        "S Kang",
        "N Hu",
        "X Liu",
        "D Su",
        "D Yu",
        "H Meng"
      ],
      "year": "2020",
      "venue": "International Speech Communication Association (Interspeech)"
    },
    {
      "citation_id": "14",
      "title": "Enriching source style transfer in recognition-synthesis based non-parallel voice conversion",
      "authors": [
        "Z Wang",
        "X Zhou",
        "F Yang",
        "T Li",
        "H Du",
        "L Xie",
        "W Gan",
        "H Chen",
        "H Li"
      ],
      "year": "2021",
      "venue": "International Speech Communication Association (Interspeech)"
    },
    {
      "citation_id": "15",
      "title": "On prosody modeling for ASR+TTS based voice conversion",
      "authors": [
        "W.-C Huang",
        "T Hayashi",
        "X Li",
        "S Watanabe",
        "T Toda"
      ],
      "year": "2021",
      "venue": "Automatic Speech Recognition and Understanding Workshop"
    },
    {
      "citation_id": "16",
      "title": "Identity conversion for emotional speakers: A study for disentanglement of emotion style and speaker identity",
      "authors": [
        "Z Du",
        "B Sisman",
        "K Zhou",
        "H Li"
      ],
      "year": "2021",
      "venue": "ArXiv"
    },
    {
      "citation_id": "17",
      "title": "Expressive voice conversion: A joint framework for speaker identity and emotional style transfer",
      "authors": [
        "D Zongyang",
        "S Berrak",
        "Z Kun",
        "L Haizhou"
      ],
      "year": "2021",
      "venue": "ArXiv"
    },
    {
      "citation_id": "18",
      "title": "Towards end-to-end prosody transfer for expressive speech synthesis with tacotron",
      "authors": [
        "R Skerry-Ryan",
        "E Battenberg",
        "Y Xiao",
        "Y Wang",
        "D Stanton",
        "J Shor",
        "R Weiss",
        "R Clark",
        "R Saurous"
      ],
      "year": "2018",
      "venue": "International Conference on Machine Learning (ICML)"
    },
    {
      "citation_id": "19",
      "title": "Style tokens: Unsupervised style modeling, control and transfer in end-to-end speech synthesis",
      "authors": [
        "Y Wang",
        "D Stanton",
        "Y Zhang",
        "R.-S Ryan",
        "E Battenberg",
        "J Shor",
        "Y Xiao",
        "Y Jia",
        "F Ren",
        "R Saurous"
      ],
      "year": "2018",
      "venue": "International Conference on Machine Learning (ICML)"
    },
    {
      "citation_id": "20",
      "title": "Learning latent representations for style control and transfer in end-to-end speech synthesis",
      "authors": [
        "Y.-J Zhang",
        "S Pan",
        "L He",
        "Z Ling"
      ],
      "year": "2019",
      "venue": "International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "21",
      "title": "Disentanglement of emotional style and speaker identity for expressive voice conversion",
      "authors": [
        "Z Du",
        "B Sisman",
        "K Zhou",
        "H Li"
      ],
      "year": "2022",
      "venue": "International Speech Communication Association (Interspeech)"
    },
    {
      "citation_id": "22",
      "title": "Casia voice conversion system for the voice conversion challenge 2020",
      "authors": [
        "L Zheng",
        "J Tao",
        "Z Wen",
        "R Zhong"
      ],
      "year": "2020",
      "venue": "Joint Workshop for the Blizzard Challenge and Voice Conversion Challenge"
    },
    {
      "citation_id": "23",
      "title": "Towards fine-grained prosody control for voice conversion",
      "authors": [
        "Z Lian",
        "J Tao",
        "Z Wen",
        "B Liu",
        "Y Zheng",
        "R Zhong"
      ],
      "year": "2021",
      "venue": "International Symposium on Chinese Spoken Language Processing"
    },
    {
      "citation_id": "24",
      "title": "IQDUBBING: Prosody modeling based on discrete self-supervised speech representation for expressive voice conversion",
      "authors": [
        "W Gan",
        "B Wen",
        "Y Yan",
        "H Chen",
        "Z Wang",
        "H Du",
        "L Xie",
        "K Guo",
        "H Li"
      ],
      "year": "2022",
      "venue": "ArXiv"
    },
    {
      "citation_id": "25",
      "title": "Unsupervised speech decomposition via triple information bottleneck",
      "authors": [
        "K Qian",
        "Y Zhang",
        "S Chang",
        "M Hasegawa-Johnson",
        "D Cox"
      ],
      "year": "2020",
      "venue": "International Conference on Machine Learning (ICML)"
    },
    {
      "citation_id": "26",
      "title": "On derived domains in sentence phonology",
      "authors": [
        "E Selkirk"
      ],
      "year": "1986",
      "venue": "Phonology"
    },
    {
      "citation_id": "27",
      "title": "Fine-grained emotion strength transfer, control and prediction for emotional speech synthesis",
      "authors": [
        "Y Lei",
        "S Yang",
        "L Xie"
      ],
      "year": "2021",
      "venue": "Spoken Language Technology Workshop (SLT)"
    },
    {
      "citation_id": "28",
      "title": "Towards multi-scale style control for expressive speech synthesis",
      "authors": [
        "X Li",
        "C Song",
        "J Li",
        "Z Wu",
        "J Jia",
        "H Meng"
      ],
      "year": "2021",
      "venue": "International Speech Communication Association (Interspeech)"
    },
    {
      "citation_id": "29",
      "title": "Multispeaker multi-style text-to-speech synthesis with single-speaker singlestyle training data scenarios",
      "authors": [
        "Q Xie",
        "T Li",
        "X Wang",
        "Z Wang",
        "L Xie",
        "G Yu",
        "G Wan"
      ],
      "year": "2021",
      "venue": "ArXiv"
    },
    {
      "citation_id": "30",
      "title": "AdaSpeech: Adaptive text to speech for custom voice",
      "authors": [
        "M Chen",
        "X Tan",
        "B Li",
        "Y Liu",
        "T Qin",
        "T.-Y Liu"
      ],
      "venue": "International Conference on Learning Representations (ICLR)"
    },
    {
      "citation_id": "31",
      "title": "Towards high-fidelity singing voice conversion with acoustic reference and contrastive predictive coding",
      "authors": [
        "C Wang",
        "Z Li",
        "B Tang",
        "X Yin",
        "Y Wan",
        "Y Yu",
        "Z Ma"
      ],
      "year": "2022",
      "venue": "International Speech Communication Association (Interspeech)"
    },
    {
      "citation_id": "32",
      "title": "Learning hierarchical representations for expressive speaking style in end-to-end speech synthesis",
      "authors": [
        "X An",
        "Y Wang",
        "S Yang",
        "Z Ma",
        "L Xie"
      ],
      "year": "2019",
      "venue": "Automatic Speech Recognition and Understanding Workshop"
    },
    {
      "citation_id": "33",
      "title": "VQ-wav2vec: Self-supervised learning of discrete speech representations",
      "authors": [
        "A Baevski",
        "S Schneider",
        "M Auli"
      ],
      "year": "2019",
      "venue": "International Conference on Learning Representations (ICLR)"
    },
    {
      "citation_id": "34",
      "title": "Expressive TTS training with frame and style reconstruction loss",
      "authors": [
        "R Liu",
        "B Sisman",
        "G Gao",
        "H Li"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "35",
      "title": "CycleGAN-VC: Non-parallel voice conversion using cycle-consistent adversarial networks",
      "authors": [
        "T Kaneko",
        "H Kameoka"
      ],
      "year": "2018",
      "venue": "European Signal Processing Conference"
    },
    {
      "citation_id": "36",
      "title": "Emotional end-to-end neural speech synthesizer",
      "authors": [
        "Y Lee",
        "A Rabiee",
        "S.-Y Lee"
      ],
      "year": "2017",
      "venue": "Neural Information Processing Systems (NIPS)"
    },
    {
      "citation_id": "37",
      "title": "Adapting and controlling DNN-based speech synthesis using input codes",
      "authors": [
        "H.-T Luong",
        "S Takaki",
        "G Henter",
        "J Yamagishi"
      ],
      "year": "2017",
      "venue": "International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "38",
      "title": "Cross-speaker emotion disentangling and transfer for end-to-end speech synthesis",
      "authors": [
        "T Li",
        "X Wang",
        "Q Xie",
        "Z Wang",
        "L Xie"
      ],
      "year": "2022",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "39",
      "title": "Controllable neural textto-speech synthesis using intuitive prosodic features",
      "authors": [
        "T Raitio",
        "R Rasipuram",
        "D Castellani"
      ],
      "year": "2020",
      "venue": "International Speech Communication Association (Interspeech)"
    },
    {
      "citation_id": "40",
      "title": "Predicting expressive speaking style from text in end-to-end speech synthesis",
      "authors": [
        "D Stanton",
        "Y Wang",
        "R Skerry-Ryan"
      ],
      "year": "2018",
      "venue": "Spoken Language Technology Workshop (SLT)"
    },
    {
      "citation_id": "41",
      "title": "Emotion controllable speech synthesis using emotion-unlabeled dataset with the assistance of cross-domain speech emotion recognition",
      "authors": [
        "X Cai",
        "D Dai",
        "Z Wu",
        "X Li",
        "J Li",
        "H Meng"
      ],
      "year": "2021",
      "venue": "International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "42",
      "title": "End-to-end emotional speech synthesis using style tokens and semi-supervised training",
      "authors": [
        "P Wu",
        "Z Ling",
        "L Liu",
        "Y Jiang",
        "H Wu",
        "L Dai"
      ],
      "year": "2019",
      "venue": "Asia-Pacific Signal and Information Processing Association (APSIPA)"
    },
    {
      "citation_id": "43",
      "title": "Hierarchical generative modeling for controllable speech synthesis",
      "authors": [
        "W.-N Hsu",
        "Y Zhang",
        "R Weiss",
        "H Zen",
        "Y Wu",
        "Y Wang",
        "Y Cao",
        "Y Jia",
        "Z Chen",
        "J Shen",
        "P Nguyen",
        "R Pang"
      ],
      "year": "2019",
      "venue": "International Conference on Learning Representations (ICLR)"
    },
    {
      "citation_id": "44",
      "title": "Improving emotional speech synthesis by using sus-constrained VAE and text encoder aggregation",
      "authors": [
        "F Yang",
        "J Luan",
        "Y Wang"
      ],
      "year": "2022",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "45",
      "title": "EmoCat: Language-agnostic emotional voice conversion",
      "authors": [
        "B Schnell",
        "G Huybrechts",
        "B Perz",
        "T Drugman",
        "J Lorenzo-Trueba"
      ],
      "year": "2021",
      "venue": "11th ISCA Speech Synthesis Workshop"
    },
    {
      "citation_id": "46",
      "title": "FastSpeech 2: Fast and high-quality end-to-end text to speech",
      "authors": [
        "Y Ren",
        "C Hu",
        "X Tan",
        "T Qin",
        "S Zhao",
        "Z Zhao",
        "T.-Y Liu"
      ],
      "year": "2020",
      "venue": "International Conference on Learning Representations (ICLR)"
    },
    {
      "citation_id": "47",
      "title": "Fine-grained robust prosody transfer for single-speaker neural text-to-speech",
      "authors": [
        "V Klimkov",
        "S Ronanki",
        "J Rohnke",
        "T Drugman"
      ],
      "year": "2019",
      "venue": "International Speech Communication Association (Interspeech)"
    },
    {
      "citation_id": "48",
      "title": "Fine-grained style modelling and transfer in textto-speech synthesis via content-style disentanglement",
      "authors": [
        "D Tan",
        "T Lee"
      ],
      "year": "2021",
      "venue": "International Speech Communication Association (Interspeech)"
    },
    {
      "citation_id": "49",
      "title": "Fine-grained style modeling, transfer and prediction in text-to-speech synthesis via phone-level content-style disentanglement",
      "authors": [
        "T Daxin",
        "L Tan"
      ],
      "year": "2021",
      "venue": "International Speech Communication Association"
    },
    {
      "citation_id": "50",
      "title": "Extracting and predicting word-level style variations for speech synthesis",
      "authors": [
        "Y.-J Zhang",
        "Z.-H Ling"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "51",
      "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "J Devlin",
        "M.-W Chang",
        "K Lee",
        "K Toutanova"
      ],
      "year": "2019",
      "venue": "BERT: Pre-training of deep bidirectional transformers for language understanding"
    },
    {
      "citation_id": "52",
      "title": "Unsupervised Speech Recognition",
      "authors": [
        "A Baevski",
        "W.-N Hsu",
        "A Conneau",
        "M Auli"
      ],
      "year": "2021",
      "venue": "Neural Information Processing Systems (NIPS)"
    },
    {
      "citation_id": "53",
      "title": "Styler: Style factor modeling with rapidity and robustness via speech decomposition for expressive and controllable neural text to speech",
      "authors": [
        "K Lee",
        "K Park",
        "D Kim"
      ],
      "year": "2021",
      "venue": "International Speech Communication Association"
    },
    {
      "citation_id": "54",
      "title": "PPG-based singing voice conversion with adversarial representation learning",
      "authors": [
        "Z Li",
        "B Tang",
        "X Yin",
        "Y Wan",
        "L Xu",
        "C Shen",
        "Z Ma"
      ],
      "year": "2021",
      "venue": "International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "55",
      "title": "VQMIVC: Vector quantization and mutual information-based unsupervised speech representation disentanglement for one-shot voice conversion",
      "authors": [
        "D Wang",
        "L Deng",
        "Y Yeung",
        "X Chen",
        "X Liu",
        "H Meng"
      ],
      "year": "2021",
      "venue": "International Speech Communication Association (Interspeech)"
    },
    {
      "citation_id": "56",
      "title": "Cross-speaker emotion transfer through information perturbation in emotional speech synthesis",
      "authors": [
        "Y Lei",
        "S Yang",
        "X Zhu",
        "L Xie",
        "D Su"
      ],
      "year": "2022",
      "venue": "IEEE Signal Processing Letters"
    },
    {
      "citation_id": "57",
      "title": "Conformer: Convolutionaugmented transformer for speech recognition",
      "authors": [
        "A Gulati",
        "J Qin",
        "C.-C Chiu",
        "N Parmar",
        "Y Zhang",
        "J Yu",
        "W Han",
        "S Wang",
        "Z Zhang",
        "Y Wu",
        "R Pang"
      ],
      "year": "2020",
      "venue": "International Speech Communication Association (Interspeech)"
    },
    {
      "citation_id": "58",
      "title": "Natural TTS synthesis by conditioning wavenet on mel spectrogram predictions",
      "authors": [
        "J Shen",
        "R Pang",
        "R Weiss",
        "M Schuster",
        "N Jaitly",
        "Z Yang",
        "Z Chen",
        "Y Zhang",
        "Y Wang",
        "R Skerrv-Ryan"
      ],
      "year": "2018",
      "venue": "International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "59",
      "title": "LPCNet: Improving neural speech synthesis through linear prediction",
      "authors": [
        "J.-M Valin",
        "J Skoglund"
      ],
      "year": "2019",
      "venue": "International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "60",
      "title": "Any-to-one sequence-tosequence voice conversion using self-supervised discrete speech representations",
      "authors": [
        "W.-C Huang",
        "Y.-C Wu",
        "T Hayashi"
      ],
      "year": "2021",
      "venue": "International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "61",
      "title": "A comparison of discrete and soft speech units for improved voice conversion",
      "authors": [
        "B Van Niekerk",
        "M.-A Carbonneau",
        "J Zaïdi",
        "M Baas",
        "H Seuté",
        "H Kamper"
      ],
      "year": "2022",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "62",
      "title": "Representation of speech in human auditory cortex: is it special?",
      "authors": [
        "M Steinschneider",
        "K Nourski",
        "Y Fishman"
      ],
      "year": "2013",
      "venue": "Hearing research"
    },
    {
      "citation_id": "63",
      "title": "Seen and unseen emotional style transfer for voice conversion with a new emotional speech dataset",
      "authors": [
        "K Zhou",
        "B Sisman",
        "R Liu",
        "H Li"
      ],
      "year": "2021",
      "venue": "International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "64",
      "title": "BOFFIN TTS: Few-shot speaker adaptation by bayesian optimization",
      "authors": [
        "H Moss",
        "V Aggarwal",
        "N Prateek",
        "J González",
        "R Barra-Chicote"
      ],
      "year": "2020",
      "venue": "ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "65",
      "title": "GC-TTS: Few-shot speaker adaptation with geometric constraints",
      "authors": [
        "J.-H Kim",
        "S.-H Lee",
        "J.-H Lee",
        "H.-G Jung",
        "S.-W Lee"
      ],
      "year": "2021",
      "venue": "International Conference on Systems, Man, and Cybernetics"
    },
    {
      "citation_id": "66",
      "title": "Wenetspeech: A 10000+ hours multi-domain mandarin corpus for speech recognition",
      "authors": [
        "B Zhang",
        "H Lv",
        "P Guo",
        "Q Shao",
        "C Yang",
        "L Xie",
        "X Xu",
        "H Bu",
        "X Chen",
        "C Zeng"
      ],
      "year": "2022",
      "venue": "International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "67",
      "title": "LibriSpeech: An ASR corpus based on public domain audio books",
      "authors": [
        "V Panayotov",
        "G Chen",
        "D Povey",
        "S Khudanpur"
      ],
      "year": "2015",
      "venue": "International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "68",
      "title": "The Kaldi speech recognition toolkit",
      "authors": [
        "D Povey",
        "A Ghoshal",
        "G Boulianne",
        "L Burget",
        "O Glembek",
        "N Goel",
        "M Hannemann",
        "P Motlicek",
        "Y Qian",
        "P Schwarz"
      ],
      "year": "2011",
      "venue": "Workshop on automatic speech recognition and understanding"
    },
    {
      "citation_id": "69",
      "title": "ECAPA-TDNN: Emphasized channel attention, propagation and aggregation in TDNN based speaker verification",
      "authors": [
        "B Desplanques",
        "J Thienpondt",
        "K Demuynck"
      ],
      "year": "2020",
      "venue": "International Speech Communication Association (Interspeech)"
    },
    {
      "citation_id": "70",
      "title": "CN-Celeb: A challenging chinese speaker recognition dataset",
      "authors": [
        "Y Fan",
        "J Kang",
        "L Li",
        "K Li",
        "H Chen",
        "S Cheng",
        "P Zhang",
        "Z Zhou",
        "Y Cai",
        "D Wang"
      ],
      "year": "2020",
      "venue": "International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "71",
      "title": "CSTR VCTK Corpus: English multi-speaker corpus for CSTR voice cloning toolkit",
      "authors": [
        "C Veaux",
        "J Yamagishi",
        "K Macdonald"
      ],
      "year": "2016",
      "venue": "University of Edinburgh. The Centre for Speech Technology Research (CSTR)"
    }
  ]
}