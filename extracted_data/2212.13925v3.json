{
  "paper_id": "2212.13925v3",
  "title": "Quality At The Tail Of Machine Learning Inference",
  "published": "2022-12-25T14:49:37Z",
  "authors": [
    "Zhengxin Yang",
    "Wanling Gao",
    "Chunjie Luo",
    "Lei Wang",
    "Fei Tang",
    "Xu Wen",
    "Jianfeng Zhan"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Machine learning inference should be subject to stringent inference time constraints while ensuring high inference quality, especially in safety-critical (e.g., autonomous driving) and mission-critical (e.g., emotion recognition) contexts. Neglecting either aspect can lead to severe consequences, such as loss of life and property damage. Many studies lack a comprehensive consideration of these metrics, leading to incomplete or misleading evaluations. The study unveils a counterintuitive revelation: deep learning inference quality exhibits fluctuations due to inference time. To depict this phenomenon, the authors coin a new term, \"tail quality,\" providing a more comprehensive evaluation, and overcoming conventional metric limitations. Moreover, the research proposes an initial evaluation framework to analyze factors affecting quality fluctuations, facilitating the prediction of the potential distribution of inference quality. The effectiveness of the evaluation framework is validated through experiments conducted on deep learning models for three different tasks across four systems.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Deep learning is catalyzing rapid advancements across a wide range of domains, encompassing areas such as natural language processing  (Vaswani et al., 2017; Devlin et al., 2019; Zheng et al., 2023)  and computer vision  (Goodfellow et al., 2014; He et al., 2016; Carion et al., 2020; Dosovitskiy et al., 2023) . This surge is paving the way for an escalating deployment of deep learning models and systems in cutting-edge real-world applications such as autonomous driving  (Chen et al., 2015; Xu et al., 2017; Li et al., 2020)  and smart healthcare  (Rajpurkar et al., 2018; McKinney et al., 2020) . In the face of this expansion across diverse fields, the execution of benchmarking and evaluation is essential to ensure the proper and effective development of these models and systems. Furthermore, within real-world applications, it is paramount to consider the metrics of inference quality and inference time carefully and comprehensively when establishing benchmarking and evaluation methodologies. These metrics reflect the capabilities of deep learning and significantly influence the user experience.\n\nUnfortunately, many current studies tend to focus on evaluating only one aspect while neglecting the other and fail to consider the evaluation in real-world applications, especially critical 1  tasks. One significant reason for this issue is that it is commonly believed that once the neural network model is trained and its parameters are fixed, the inference quality of deep learning on the same data will never change regardless of variations in the deployment environment. However, as this paper is about to reveal, a counterintuitive phenomenon in the practical deployment and application of deep learning is captured, in which fluctuations in inference quality can occur due to variations in inference time, even when the inference inputs remain unchanged. Specifically, for safety-critical tasks like autonomic driving, ensuring highquality inference while adhering to stringent inference time requirements is of utmost importance. For example, given that humans typically need about 390 to 600 milliseconds to respond to hazards  (Wolfe et al., 2020; 2021) , the capability of object detection systems in autonomous driving must surpass human levels  (Turay & Vladimirova, 2022)  within mere tens to hundreds of milliseconds, to reduce the likelihood of accidents. For a specific critical task object detection model, even if the inference time exceeds the specific time constraint by just a few tens of milliseconds, it may have already traveled several meters and caused severe consequences such as loss of life and property damage.\n\nTo better illustrate this phenomenon, the authors propose a new term \"tail quality.\" Specifically, tail quality @x%=t identifies inference quality when the inference time threshold is set as x% tail latency t (e.g. 99.9% tail latency = 10ms (milliseconds) as the threshold). For a hard real-time system, it's essential to ensure that the tail quality @100%=t remains high even when t is minimal, guaranteeing the reliainclude autonomous driving (safety-critical), business negotiation (business-critical), and navigational system for a spacecraft (mission-critical)  (Gao et al., 2022) . bility of the system. If the acceptable threshold is 10ms, this means the tail quality @100%=10ms must be sufficiently high.  Figure  1  provides a more intuitive depiction of the phenomenon. As observed, compared to the original inference quality without time constraints, where tail quality @100%=t is 81.512% accuracy, the worst-case tail quality @99%=472ms dropped by approximately 1.7 percentage points. More seriously, the worst-case tail quality @90%=392ms plummeted by around 13 percentage points.\n\nAs the threshold becomes stricter, moving from 400ms to 300ms, the model's inference quality exhibited significant fluctuations and declined dramatically, eventually reaching an accuracy of 0%.\n\nHowever, such threshold settings are stringent in safetycritical tasks; for instance, tasks like autonomous driving need to achieve high quality when inference times are within a few tens of milliseconds, e.g., the tail quality @100%=10ms should be higher than 99.9% accuracy, or irreversible consequences may occur. This highlights the severity of the \"tail quality\" phenomenon. \"Tail quality\" could have profound implications in real-world scenarios, particularly in contexts like driving, where decisions are constantly made based on rapidly changing traffic conditions. Considering the enormous number of vehicles on the roads daily, if control were to be handed over to a deep learning model, even if the probability of encountering a \"tail quality\" event (i.e., inference failure) is low, within such a vast population, any decision-making error arising from such an occurrence could lead to loss of life or property, which is unacceptable.\n\nMany studies have predominantly used individual quality metrics such as accuracy  (He et al., 2016; Dosovitskiy et al., 2023) , average precision  (Everingham et al., 2010; Lin et al., 2014; Carion et al., 2020) , or individual inference time metrics like tail latency  (Dean & Barroso, 2013; Reddi et al., 2020; Gao et al., 2019b; a)  to characterize the performance of deep learning. What sets apart the concept of \"tail quality\" is that it allows for a more intuitive depiction of the impact of inference time on inference quality and thus provides insights into the extent of potential consequences caused by inference failures. To the best of our knowledge, this paper is the first to present this discovery.\n\nDue to the intricacies of benchmarking in computer science, deep learning software and hardware systems, along with the models that serve as their workloads, are entangled and mutually influential  (Reddi et al., 2020; Zhan, 2022) . Conducting a systematic analysis of the causes behind fluctuations in deep learning inference quality has been challenging. Additionally, with the aim of promoting extensive research in this direction, this research proposes an initial evaluation framework to analyze various factors that affect the fluctuation of inference quality. It systematically analyzes the effects of the entire deployment and application environment of deep learning by disassembling it into several components, such as software and hardware systems, models, and data. Furthermore, statistical methods intuitively depict the approximate distribution of inference time and inference quality under the influence of different components, enabling the prediction of the \"tail quality\" phenomenon before deploying deep learning applications.\n\nExperiments are conducted on frequently used deep learning models for three different tasks across four systems and two deep learning frameworks. On the one hand, through experiments, it has been further validated that the superiority of adopting the proposed novel term \"tail quality\" can provide a more intuitive characterization of the severe consequences of instability in deep learning models and systems in realworld applications compared to relying solely on inference quality or inference time as evaluation metrics. On the other hand, the experiments confirmed the effectiveness of this evaluation framework in predicting \"tail quality.\" During the testing phase, the evaluation framework achieved an average squared root of Jensen-Shannon Divergence (rJSD) value of 0.051 for the predicted probability distribution of inference times. In predicting the worst-case tail quality @99%=t, @95%=t, and @90%=t, it exhibited an average discrepancy level of -0.07, indicating its ability to reasonably forecast the worst-case tail quality. In terms of the framework performance, MLPerf Inference  (Reddi et al., 2020)   As the first work in the novel research direction, the authors urge caution in dealing with the discovered phenomenon of \"tail quality\" and call for establishing innovative methodologies and tools based on the proposed evaluation framework to tackle the challenge above.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Tail Quality And Evaluation Framework",
      "text": "In order to proactively anticipate and prevent the occurrence of \"tail quality,\" this section will begin by providing its definition. This definition will serve as the foundation for the systematic construction of tail quality evaluation framework, enabling a scientifically grounded approach. Subsequently, an evaluation framework for \"tail quality\" will be introduced. This framework enables the prediction of potential future instances of tail quality in deep learning models with relatively low computational costs as shown in Section 3. Additionally, it facilitates the analysis of potential factors that influence the emergence of tail quality. The calculation method of inference quality depends on the specific quality evaluation metric used, such as accuracy, AP, and F-score. Thus, the calculation process can be abstracted by using the metric calculation function q, thus Q can be represented as Q = q({M (x i )} n i=1 , {y i } n i=1 ). It is evident that when calculating inference quality, the inference result of each instance contributes to the overall inference quality Q accordingly. The specific contribution of each instance x i can be abstracted into a contribution function c i . Therefore, the quality Q can be further abstracted as\n\n). In a specific task, assume the maximum allowable inference time is denoted as θ, which serves as the inference time threshold. The inference time taken by the model M on a particular instance x i is denoted as t i (x i , M ). Thus, the following indicator function 1 i can be used to determine the effectiveness of the inference result:\n\nAdditionally, the instability of deep learning systems leads to constant fluctuations in model inference time. Thus, to better investigate the impact of inference time on inference quality, it can be assumed that the inference time t i (x i , M ) of model M on instance x i follows a certain conditional probability distribution\n\nwhere T , X , and M are three random variables representing time, instance, and model, respectively. t i (x i , M ) is replaced by t to make the expression more concise. Note that the specific model is already determined during the deep learning inference phase.\n\nUltimately, under the constraint of inference time, the overall inference quality Q can be represented as:\n\nBased on the formulas provided above, it becomes evident that once deterministic inference quality algorithm is now reliant on the inference time of each instance, introducing considerable uncertainty in the calculation inference quality due to considering inference time constraints.\n\nNote that the above analysis applies when the input is a batch; the probability distribution and indicator function corresponding to each instance in the batch can be simply replaced by the ones corresponding to the entire batch.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Establishment Of Evaluation Framework",
      "text": "This section introduces a novel evaluation framework which models the entire evaluation process from the system, model, data, and other potential components to the inference time and then to the inference quality, based on the definition of \"tail quality\" from the previous subsection. It allows researchers to adjust the modeling formulas and parameters to guide the establishment of evaluation processes with varying levels of granularity and precision. This provides the evaluation framework with high flexibility and scalability.\n\nThe variation of each component can potentially impact the probability distribution of the deep learning inference time.\n\nTherefore, the probability distribution model that describes and analyzes the various influencing factors that impact the inference time of deep learning can be represented as the conditional probability distribution of inference time\n\n, where C j denotes the influencing components such as model M. In a practical application of the framework, components that are not considered will not be listed in the formula. Therefore, to ensure the objectivity of the analysis, the evaluation environment for deep learning inference should remain unchanged except for the listed components.\n\nBuilding on this inference time distribution model, the inference quality represented by Formula 2 can be rewritten to create the model that describes and analyzes the impact of inference time on inference quality, as shown below:\n\nTherefore, the construction of the evaluation framework mainly consists of two parts: (1) establishing a probability distribution model for inference time,\n\nthrough which the relationship between individual components and inference time can be determined; (2) constructing a \"tail quality\" calculation model to establish the mapping relationship between inference quality and inference time through this model, as Equation 3 represents.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Inference Time Probability Model",
      "text": "In this section, a heuristic algorithm is proposed to estimate the probability distribution of inference times, which can be seen as a basic Monte Carlo simulation. As the probability distribution governing the inference time of each instance is unknown, this part proposes a heuristic algorithm to estimate the probability distribution of inference time, in order to construct the inference time probability model.\n\nAlgorithm 1 shows details of the proposed algorithm. Initially, the deep learning model M conducted r rounds of inference for all instances x i in the dataset D where |D| = n, recording the corresponding inference times, and then, by using the method Fit(), the probability density functions (PDF) for each instance are fitted based on the initial recorded r rounds of inference times (line 2-8). Researchers can opt for an appropriate function-fitting method to model the probability distribution of inference times for each instance. In this study, the kernel density estimation (KDE) method is chosen to employ. According to the definition of KDE, the fitted PDF of the instance x i is as follows:\n\nwhere t i,j is the jth round inference time on the instance x i , K is the kernel function which is chosen as a Gaussian distribution in the paper, and h is a smoothing parameter.\n\nThe algorithm then proceeds to perform deep learning inference on the entire dataset repeatedly until it deems that the PDF for all instances has been sufficiently well-fitted (lines 12-36). Firstly, during each inference, the inference times on all instances are recorded (line 15). Then, on the entire dataset, for every s rounds of inference, the probability distribution models for the inference times of all instances are re-fitted based on the newly recorded s inference times as well as all the previously recorded inference times  (line 16, 17) . The newly fitted model F i,l+1 is then compared with the previously fitted models. for all j ← 1 . . . r do 5:\n\nfor all i ← 1 . . . n do 14:\n\n17:\n\n18:\n\nfor all k ← l + 1w . . . l do 20:\n\nWith an increase in the number of inference rounds, the accumulation of more recorded inference times yields additional information about the population of inference times. This enables a more accurate estimation and prediction of probability distribution. Therefore, when newly recorded inference times cease to contribute information significantly to the estimation of the probability distribution, indicating that the fitting of the distribution hardly changes, it is considered that a reasonably good estimation of the overall distribution has been achieved.\n\nBased on these considerations, Algorithm 1 employs a sliding window to store the previous w fitted distributions. If the differences between these fitted distributions within the sliding window are sufficiently small, i.e., they are less than the specified tolerance threshold δ, it is inferred that a sufficiently good estimation of the statistical population can be obtained based on the available inference time records (line 20-25).\n\nThe algorithm employs the Check() method to compare differences among different fitting results (line 22). This method can be specified by researchers as long as it ensures the convergence of the algorithm is not compromised. In this paper, the Check() method employs Jensen-Shannon (JS) divergence, which is a symmetrized and smoothed version of the Kullback-Leibler (KL) divergence, to quantify the similarity between any two probability distributions. The definition of the JS divergence is:\n\nwhere P and Q are the probability distributions to be compared, they correspond to F i,k and F i,l+1 respectively in the context of Algorithm 1, and M = 1 2 (P + Q) represents the mixture distribution of P and Q. D(P ∥ Q) denotes the calculation of the KL divergence between P and Q.\n\nUltimately, after finishing the estimation of the inference time distribution for all instances, it can be inferred that the fitted probability density functions f i n i=1 of each instance and the minimum required number of inference rounds to obtain these density functions. The computation cost of this algorithm mainly depends on the number of iterations needed to fit PDFs of inference times for all samples. Consequently, the total number of inferences should be the product of the dataset size and the number of iterations. Through experiments in Section 3, it can be observed that the algorithm's fitting convergence rate is quite satisfactory and, in most cases, it outperforms the inference count required by MLPerf Inference  (Reddi et al., 2020) .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Tail Quality Calculation Model",
      "text": "As evident from the previous sections, deriving an analysis model for inference quality directly from the inference time analysis model through Formula 3 is quite challenging. Given the variety of evaluation metrics, it is not feasible to construct quality calculation functions for each metric and to estimate the contribution functions of all instances under specific quality indicators.\n\nFortunately, the process of estimating the inference time distribution model involves a comprehensive exploration of all potential deep learning inference conditions that may occur. Hence, given all the recorded inference times, researchers can easily calculate the inference quality for each round of inference at a specific threshold θ by utilizing Algorithm 2, which is derived from the Equations 3.\n\nIn Algorithm 2, the inference result effectiveness of each instance is tagged by comparing its inference time with the threshold θ (line 4-8). Deep learning inference results that surpass the inference time threshold will be marked as invalid and considered as erroneous outcomes in the overall statistical evaluation of inference quality. After each round of tagging is completed, the inference quality can be recalculated by using method Evaluate() based on the effectiveness of all instances in the dataset (line 10). Implementing the Evaluate() method depends on a specific inference quality metric. For instance, in the case of accuracy evaluation metric, all invalid instances (i.e., instances where v j = F alse) can be considered as errors when assessing correctness against the ground truth. This means they are not included in the count of correct samples, while other valid instances are counted according to the original calculation method.\n\nAlgorithm 2 Calculation of Inference Quality 1: q i ← Evaluate({x j , ŷj , v j } n j=1 ) 12: end for Finally, with the calculated qualities through Algorithm 2, researchers can directly estimate the probability distribution of the inference quality under the specified inference time threshold θ, which makes the inference quality analysis model more concrete. However, in practical applications, estimating this distribution model is not always necessary. The emphasis should lie on particular attributes crucial for real-world applications, such as the worst-case tail quality @99%=t with specific designated inference time constraint.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Experimental Analysis And Results",
      "text": "In this section, the authors first instantiate the proposed evaluation framework and validate its effectiveness through experiments. Subsequently, the instantiated evaluation framework is employed to analyze various factors, such as systems and data, that impact inference time and inference quality separately.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Instantiation Of The Evaluation Framework",
      "text": "The instantiation of the evaluation framework essentially involves defining the analysis models for inference time and inference quality and selecting appropriate parameters for the estimation Algorithms 1 and 2. For the sake of experiments simplicity, this study considers four influencing factors: hardware systems S, deep learning frameworks F, models M, and data X . Thus, the instantiation can be represented as P (T | X , M, S, F).\n\nTo define the search space for influencing components, four servers, A, B, C and D are selected with different types of graphics processing units (GPU) as subjects for hardware systems S investigation in experiments. Among them, Server A is equipped with 4 identical GeForce RTX 2080 Ti GPUs, facilitating distributed inference for large language models (LLM). Server B, C, and D is equipped with TITAN V, Tesla P100 and V100, respectively. For deep learning frameworks F, experiments are conducted under the two most popular frameworks, PyTorch  (Paszke et al., 2019)  and TensorFlow  (Abadi et al., 2016) . Noting that the framework versions are consistent across all servers, and the CUDA version is 11.7 for all except Server C with 11.6.\n\nIn order to ensure comprehensiveness while maintaining simplicity in experiments, for modelsM and dataX inference components, three widely used models were selected across the Computer Vision (CV) and Natural Language Processing (NLP) domains. These models include Detection Transformer (DETR)  (Carion et al., 2020)  for object detection, Vision Transformer (ViT)  (Dosovitskiy et al., 2023)  for image classification, and the large language model Vicuna  (Zheng et al., 2023)  for dialogue systems (Chatbot). Corresponding to the selected models, three datasets used to evaluate the model inference quality, namely COCO  (Lin et al., 2014 ) val2017, ImageNet (Russakovsky et al., 2015)  val2012, and MMLU  (Hendrycks et al., 2020)  dev, were chosen for conducting the experiments. Table  1  contains specific information about the data and models. Another part of the framework instantiation involves select-ing hyperparameters for the estimation Algorithm 1 of the inference time probability model. Specifically, the initial number r of inference rounds is set to 30, the size of sliding window w for storing fitted analysis models is set to 5, and the step size s for re-fitting is set to 5. The tolerance δ for similarity differences between the fitted models is set to 0.2. It's important to note that the evaluation metric for tolerance difference is the square root of Jensen-Shannon Divergence (rJSD). JSD ranges from 0 to 1, where lower values indicate more remarkable similarity between two fitted results, with 0 representing identical.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Validation Of Effectiveness",
      "text": "To validate the effectiveness of the evaluation framework, the validation process is divided into two stages: the training and testing stages.\n\nIn training stage, inference time probability models are estimated on different servers and deep learning frameworks for each model and its corresponding dataset. A probability density distribution (PDF) is fitted for each instance based on its inference times. The fitted PDF should stabilize as the algorithm iterates, indicating that the rJSD between the refitted PDF and the existing w PDFs, which were fitted using a smaller amount of inference time data, should gradually approach 0. Therefore, rJSD serves as a metric to assess the quality of the fitting results for each instance. Furthermore, the mean rJSD across all instances in the dataset is employed as an indicator of the overall quality of the algorithm in fitting the entire dataset.\n\nIn testing phase, to assess the generalization performance of PDF of inference times corresponding to each fitted instance by the algorithm, all deep learning models undergo an additional 30 rounds of inference on their respective datasets. This process generated new inference time data points, which were then used to validate the generalization performance of the probability distributions that are fitted during the training phase. A better generalization performance indicates that the fitting results encompass sufficient information about the population, capturing a broader range of scenarios that deep learning models might encounter during repeated inferences on the same instance. Specifically, during the testing phase, probability distribution are fitted to all the inference time data obtained for each instance in the dataset. Subsequently, the fitted results obtained for each sample during the training phase are individually compared with the corresponding testing phase results, and the rJSD is calculated for each comparison. The overall generalization performance of the estimated inference time probability model across the whole dataset can be represented by the average rJSD computed for all instances.\n\nThe validation of the tail quality calculation model can be conducted by comparing the statistical metrics of the predicted tail quality by the calculation model with the difference in tail quality observed during the testing phase.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Fitting Quality And Generalization Performance",
      "text": "Table  2  presents the fitting quality of inference time probability models obtained from training phase for all deep learning models across various servers and frameworks, as well as the generalization performance of the probability models in testing phase. It is evident that all the rJSD values are below 0.05. This indicates that the evaluation framework can effectively fit the probability distribution of inference times for each instance when estimating the inference time probability models for the influencing components.\n\nFurthermore, each probability model demonstrates strong generalization on new data, indicating that the model comprehensively captures various scenarios during the deep learning inference process. One reason for some test rJSD values being larger than their corresponding training phase values is that the testing phase involves fewer data points for fitting the probability density function. Consequently, the fitting process during the testing phase lacks a significant amount of crucial information compared to the training phase, leading to discrepancies in the fitting results between the two phases.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Characterization Of Tail Quality",
      "text": "By contrasting the differences in statistical indicators of these inference quality measurements, it is possible to preliminarily determine whether the tail quality calculation model possesses certain statistical characteristics. The differences can be represented as ∆ = ab, where a and b is the worst-case values of training and testing stage, respectively. Experiments employ tail latencies from 3 distinct percentiles -specifically, the 99th, 95th, and 90th percentiles of all inference times -as thresholds for computing tail quality. Subsequently, the worst-case tail quality values are determined to observe the ability of prediction for tail quality in testing stage of the calculation model. As illustrated in the Table  3 , differences among all models on Server A are almost all below 0. This suggests that the tail quality calculation model has achieved favorable estimations through the inference time probability model, and the value of the worst-case tail quality has been accurately predicted.\n\nFurthermore, by examining the statistics of different tail quality in the Table  3 , it is evident that the calculation models effectively capture the tail quality phenomena at various inference time thresholds. As the thresholds become stricter, the worst-case tail quality gradually decreases. This further validates the significance of tail quality, and the efficacy of this evaluation framework in predicting tail quality.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Efficiency Of The Evaluation Framework",
      "text": "Due to the adoption of the heuristic algorithm for estimating the evaluation model, the computational resource expenditure in constructing this model depends on the number of iterations the algorithm takes to converge and the size of the dataset. Table  provides  an account of the total inference count required for estimating the analysis model across all experiments. This total inference count pertains to the fitting of PDF for inference times across all instances, rather than referring to inference rounds conducted on the entire dataset. MLPerf Inference  (Reddi et al., 2020)  stipulates that a total of 270,336 inferences should be conducted to statistically capture the 99th percentile tail latency. As evident from the Table  4 , the inference counts in all experiments are below this value except the model DETR-DC5. The evaluation framework's average number of inference iterations across all models, compared to MLPerf, resulted in approximately 37.74% reduction in computational workload. Consequently, the computational resource expenditure of this evaluation framework is manageable in practical applications.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Analysis Of Influencing Components",
      "text": "This section employs the instantiated framework to conduct a comprehensive analysis of the individual components that impact tail quality. For the sake of conciseness and focused attention, this section primarily delves into the specific anal- ysis of experiments related to the DETR-DC5 model in object detection tasks.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Effect Of The Input Data",
      "text": "Impact of different instance sizes on inference time: Due to the characteristics of the DETR model and the nature of the object detection task, instances of the dataset isn't uniformly cropped to the same size before being fed into the model for inference. Therefore, it is reasonable to assume that the inference time of the model may be influenced by the size of the input data, indicating a potential correlation between the two. To validate this hypothesis, experiments conducted linear regression about the DETR model, examining the relationship between the number of pixels of input images and the time taken for inference on different deep learning frameworks and servers. As depicted in Figure  2  (a) and (b), across all servers and regardless of whether Tensor-Flow or PyTorch is chosen as the deep learning framework, the model's inference time exhibits a positive linear correlation with the image size.\n\nThis confirms the hypothesis that as the image size increases, the computational workload of the deep learning system during inference also increases, subsequently leading to longer inference times. Therefore, if the aim is to reduce model inference time, exploring solutions from the perspective of compressing image sizes could be a viable approach. A similar phenomenon is observed in dialogue systems as well.\n\nFor chatbots like Vicuna, the inputs consist of sentences of varying lengths, also known as prompts, where the length is determined by the number of tokens after sentence segmentation. In human-machine dialog scenarios, the input length during each model inference is highly likely to be different. Therefore, as depicted in Figure  2  (c), the length of input sentences shows a positive linear correlation with the inference time of Vicuna.\n\nImpact of same instance sizes on inference time: Interestingly, as evidenced by the scatter plot in Figure  2  (a) and (b), even when the image pixel count remains consistent and inferences are conducted on the same system, DETR model still exhibits significant fluctuations in inference time for different input samples. To analyze the underlying reasons for these discrepancies, experiments compared the frequency distributions of inference times for different images of the same size. As shown in Figure  3 , even though images #4159, #3961, and #17 share the same dimensions, their frequency distributions for inference times still exhibit notable differences. One possible reason for this phenomenon might be linked to the sparsity of images. Furthermore, an insightful observation is that when categorizing images of the same size based on their width and height, it becomes apparent that images with longer heights tend to have relatively longer inference times. Unfortunately, due to space constraints, the detailed analysis data is not presented here. For a more comprehensive analysis, a finer-grained segmentation of the conditional random variables within the evaluation framework would be necessary.\n\nSensitivity of inference time analysis model to variations in input data: It's evident that input data indeed impact the model's inference time, though the differences in the effects generated by many instances are not excessively significant. As inference time is positively correlated with input instance size, its distribution of statistical population should also exhibit a positive correlation with input size. Figure  4  highlights that the evaluation framework's fitting of inference time distribution across different input data sizes nearly aligns with a positive correlation relationship. Furthermore, as observed in Figure  3 , the evaluation framework also performs well in fitting models with very small distinctions in inference time frequency distribution. Notably, the differences in the fitted curves corresponding to three images of the same size are clearly discernible. Effect of the Hardware and Software Systems\n\nIn addition to the analysis of input data, the instantiated evaluation framework also takes into account the influence of deep learning frameworks F and systems S on the inference results. As the search space for F and S is significantly smaller than that of input data, this aspect of the analysis is relatively straightforward. As granularity increases for the division of frameworks and systems, the complexity of analysis will increase along with the expansion of the search space. However, it's worth noting that these considerations are beyond the scope of this work. As evident from Figure  2  in the experiments with the DETR model, regardless of the framework used, Server C (Tesla V100) consistently exhibits shorter inference times compared to the other two servers. Conversely, Server C (GeForce RTX 2080 Ti) notably performs worse than the other two servers, and its rate of increase in inference time with respect to increasing image size (slope of the regression line) is slower than that of the other servers. This can also be observed from the fitting results of the inference time distribution for image #4159 across different servers, as depicted in Figure  3 . Also, the inference speed of PyTorch is notably higher than that of TensorFlow. Furthermore, due to Vicuna's extensive parameter count of 13 billion, it necessitates the utilization of 4 GPUs for distributed model inference on Server A. As depicted in the Figure  2 , the inference time on Server A remains considerably longer than that on Server C. This can be attributed to a combination of factors, including GPU performance differences and the impact of data communication speed among GPUs during distributed inference.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Related Work",
      "text": "4.1 Optimization of Inference Quality and Time Domain generalization aims to address the issue of significant inference quality degradation that may occur in well-trained deep learning models when facing unseen domains  (Wang et al., 2021; Zhou et al., 2023) . In contrast, our research primarily focuses on the phenomenon called tail quality, where the inference quality of deep learning models may fluctuate and experience a substantial decline when processing the same data.\n\nEfficient neural network inference primarily focuses on increasing the computational efficiency of models under resource constraints, employing solutions like model quantization and model pruning, which often entail alteration of the original model  (Gong et al., 2014; Jacob et al., 2018; Guo et al., 2019; Gholami et al., 2021) . However, our work primarily concerns identifying inefficient inference processes and analyzing the underlying causes while keeping the model intact.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Benchmarking Of Deep Learning Inference",
      "text": "Evaluation of deep learning models primarily focuses on specific inference quality metrics. For instance, Average Precision (AP) is the most popular metric that is used in various benchmark challenges for object detection, such as Pascal VOC  (Everingham et al., 2010)  and MS COCO  (Lin et al., 2014) . Accuracy and F-score are utilized to evaluate most state-of-the-art classification models, such as large language models in the field of natural language processing  (Devlin et al., 2019; Zheng et al., 2023)  and image classification models in autonomous driving, emotion recognition and healthcare  (Turay & Vladimirova, 2022; Yurtsever et al., 2020; Zhang & Tan, 2021; Rajpurkar et al., 2018; McKinney et al., 2020) . Using only inference quality metrics can only reflect the optimal predictive ability of models on a specific test dataset. Although, in addition to quality metrics, many studies also employ other system-related metrics to evaluate the processing speed of models  (Turay & Vladimirova, 2022) , such as floating-point operations per second (FLOPS)  (Tan et al., 2020; Dosovitskiy et al., 2023)  and frames per second (FPS)  (Liu et al., 2016) , the impact of changes in deep learning systems on inference quality and time are not taken into account. These inference time metrics, such as FLOPS and FPS, can only reflect the average inference efficiency of the model on specific deep learning software and hardware systems. When processing samples, they cannot illustrate how poorly models perform on different software and hardware systems.\n\nBenchmarking of deep learning systems tends to prioritize inference time, throughput, and other system-related metrics.\n\nMLPerf Inference  (Reddi et al., 2020)  and AIBench  (Gao et al., 2019b; a) , for example, utilizes tail latency as its evaluation metric, while DAWNBench  (Coleman et al., 2017)  adopts average inference latency. Although tail latency can reflect the stability and reliability of deep learning systems  (Dean & Barroso, 2013; Reddi et al., 2020) , helping identify issues for performance optimization, it cannot directly reflect the impact of variability and performance issues of systems on inference quality, as well as the potential adverse consequences may rise. Indeed, these benchmark suites also incorporate inference quality as part of the evaluation procedure. Still, it is primarily used to set specific inference quality targets to ensure that the workloads meet the conditions imposed as benchmarks and is sufficient to assist in measuring the inference time consumed by different systems.",
      "page_start": 10,
      "page_end": 11
    },
    {
      "section_name": "Conclusion",
      "text": "The paper unveils a counterintuitive phenomenon that there are fluctuations in machine learning inference quality. Authors coin a new term, \"tail quality,\" to characterize this phenomenon, overcoming existing evaluation methodologies limitations. Due to the potential severe consequences of tail quality, such as loss of life or property damage, effective prediction and comprehensive analysis of tail quality are crucial. This paper proposes a flexible and scalable evaluation framework, which can make reasonably accurate predictions of tail quality with lower computational costs than the state-of-the-practice like MLPerf Inference. In conclusion, the authors aim to draw attention to \"tail quality\" phenomenon and call for exploration of evaluation methods based on the proposed evaluation framework in this paper.",
      "page_start": 10,
      "page_end": 10
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Quality Fluctuations of an Image Classification Model",
      "page": 2
    },
    {
      "caption": "Figure 1: provides a more intuitive depiction of the phe-",
      "page": 2
    },
    {
      "caption": "Figure 2: (c), the length",
      "page": 8
    },
    {
      "caption": "Figure 3: , even though images",
      "page": 8
    },
    {
      "caption": "Figure 2: The relationship between the size of the instances of the dataset (COCO and MMLU) and the corresponding average inference",
      "page": 9
    },
    {
      "caption": "Figure 4: highlights that the evaluation framework’s fitting",
      "page": 9
    },
    {
      "caption": "Figure 3: , the evaluation frame-",
      "page": 9
    },
    {
      "caption": "Figure 3: Fitting of probability density functions for inference",
      "page": 9
    },
    {
      "caption": "Figure 4: The Jensen-Shannon Divergence (JSD) between the",
      "page": 9
    },
    {
      "caption": "Figure 2: in the experiments with the",
      "page": 10
    },
    {
      "caption": "Figure 3: Also, the inference speed of PyTorch",
      "page": 10
    }
  ],
  "tables": [
    {
      "caption": "Table 1: contains performanceoftheprobabilitydistributionsthatarefitted",
      "data": [
        {
          "CV": "CV",
          "Object\nDetection": "Image\nClassification",
          "DETR\n(≈ 60M)": "ViT\n(≈ 306M)",
          "COCO\n(5K)": "ImageNet\n(50K)",
          "mAP": "Acc"
        },
        {
          "CV": "NLP",
          "Object\nDetection": "Dialogue\nSystem",
          "DETR\n(≈ 60M)": "Vicuna\n(≈ 13B)",
          "COCO\n(5K)": "MMLU\n(1531)",
          "mAP": "Acc"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table 4: , the inference counts in all experiments",
      "data": [
        {
          "DETR\n(DC5)": "",
          "1": "",
          "PyTorch": "TensorFlow",
          "0.000": "0.010",
          "0.004": "0.204",
          "0.002": "0.025",
          "0.036": "0.155",
          "0.007": "0.190",
          "N/A": "N/A"
        },
        {
          "DETR\n(DC5)": "DETR",
          "1": "2",
          "PyTorch": "PyTorch",
          "0.000": "0.015",
          "0.004": "0.004",
          "0.002": "0.001",
          "0.036": "0.011",
          "0.007": "0.207",
          "N/A": "N/A"
        },
        {
          "DETR\n(DC5)": "",
          "1": "",
          "PyTorch": "TensorFlow",
          "0.000": "0.006",
          "0.004": "0.214",
          "0.002": "0.018",
          "0.036": "0.154",
          "0.007": "0.011",
          "N/A": "N/A"
        },
        {
          "DETR\n(DC5)": "ViT",
          "1": "256",
          "PyTorch": "PyTorch",
          "0.000": "0.001",
          "0.004": "0.011",
          "0.002": "0.001",
          "0.036": "0.013",
          "0.007": "0.015",
          "N/A": "N/A"
        },
        {
          "DETR\n(DC5)": "",
          "1": "",
          "PyTorch": "TensorFlow",
          "0.000": "0.009",
          "0.004": "0.195",
          "0.002": "0.006",
          "0.036": "0.127",
          "0.007": "0.206",
          "N/A": "N/A"
        },
        {
          "DETR\n(DC5)": "Vicuna",
          "1": "1",
          "PyTorch": "PyTorch",
          "0.000": "N/A",
          "0.004": "0.111",
          "0.002": "N/A",
          "0.036": "N/A",
          "0.007": "N/A",
          "N/A": "0.007"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "DETR\n(DC5)": "",
          "PyTorch": "TensorFlow",
          "43.93": "44.11",
          "44.29": "44.18",
          "-0.36": "-0.07",
          "41.72": "41.90",
          "42.35": "42.20",
          "-0.62": "-0.30",
          "39.47": "39.21",
          "40.03": "39.45",
          "-0.56": "-0.24",
          "44.90": "44.80"
        },
        {
          "DETR\n(DC5)": "DETR",
          "PyTorch": "PyTorch",
          "43.93": "43.03",
          "44.29": "43.04",
          "-0.36": "-0.02",
          "41.72": "41.10",
          "42.35": "41.12",
          "-0.62": "-0.03",
          "39.47": "39.00",
          "40.03": "39.16",
          "-0.56": "-0.16",
          "44.90": "43.50"
        },
        {
          "DETR\n(DC5)": "",
          "PyTorch": "TensorFlow",
          "43.93": "42.57",
          "44.29": "42.79",
          "-0.36": "-0.22",
          "41.72": "40.16",
          "42.35": "40.32",
          "-0.62": "-0.16",
          "39.47": "37.16",
          "40.03": "37.69",
          "-0.56": "-0.54",
          "44.90": "43.40"
        },
        {
          "DETR\n(DC5)": "ViT",
          "PyTorch": "PyTorch",
          "43.93": "78.53",
          "44.29": "76.72",
          "-0.36": "1.81",
          "41.72": "65.36",
          "42.35": "68.64",
          "-0.62": "-3.29",
          "39.47": "48.00",
          "40.03": "53.65",
          "-0.56": "-5.65",
          "44.90": "81.51"
        },
        {
          "DETR\n(DC5)": "",
          "PyTorch": "TensorFlow",
          "43.93": "79.29",
          "44.29": "79.73",
          "-0.36": "-0.44",
          "41.72": "70.18",
          "42.35": "72.31",
          "-0.62": "-2.13",
          "39.47": "63.76",
          "40.03": "65.62",
          "-0.56": "-1.87",
          "44.90": "81.51"
        },
        {
          "DETR\n(DC5)": "LLM",
          "PyTorch": "PyTorch",
          "43.93": "51.99",
          "44.29": "51.99",
          "-0.36": "0.00",
          "41.72": "50.23",
          "42.35": "50.16",
          "-0.62": "0.07",
          "39.47": "48.47",
          "40.03": "48.47",
          "-0.56": "0.00",
          "44.90": "52.71"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "DETR (DC5)\nPyTorch / TensorFlow": "DETR\nPyTorch / TensorFlow",
          "350,000": "175,000",
          "350000": "175000",
          "N/A": "N/A",
          "262,742": "262,742"
        },
        {
          "DETR (DC5)\nPyTorch / TensorFlow": "ViT\nPyTorch / TensorFlow",
          "350,000": "13,720",
          "350000": "13720",
          "N/A": "N/A",
          "262,742": "262,742"
        },
        {
          "DETR (DC5)\nPyTorch / TensorFlow": "LLM\nPyTorch / TensorFlow",
          "350,000": "N/A",
          "350000": "N/A",
          "N/A": "107,170",
          "262,742": "262,742"
        },
        {
          "DETR (DC5)\nPyTorch / TensorFlow": "Average Inference Count",
          "350,000": "179,573\n(68.35%)",
          "350000": "179,573\n(68.35%)",
          "N/A": "107,170\n(40.79%)",
          "262,742": "262,742\n(100%)"
        }
      ],
      "page": 8
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "TensorFlow: A system for large-scale machine learning",
      "authors": [
        "M Abadi",
        "P Barham",
        "J Chen",
        "Z Chen",
        "A Davis",
        "J Dean",
        "M Devin",
        "S Ghemawat",
        "G Irving",
        "M Isard",
        "M Kudlur",
        "J Levenberg",
        "R Monga",
        "S Moore",
        "D Murray",
        "B Steiner",
        "P Tucker",
        "V Vasudevan",
        "P Warden",
        "M Wicke",
        "Y Yu",
        "X Zheng"
      ],
      "year": "2016",
      "venue": "Proceedings of the 12th USENIX Conference on Operating Systems Design and Implementation"
    },
    {
      "citation_id": "2",
      "title": "End-to-End Object Detection with Transformers",
      "authors": [
        "N Carion",
        "F Massa",
        "G Synnaeve",
        "N Usunier",
        "A Kirillov",
        "S Zagoruyko"
      ],
      "year": "2020",
      "venue": "Computer Vision -ECCV 2020",
      "doi": "10.1007/978-3-030-58452-813"
    },
    {
      "citation_id": "3",
      "title": "Deep-Driving: Learning Affordance for Direct Perception in Autonomous Driving",
      "authors": [
        "C Chen",
        "A Seff",
        "A Kornhauser"
      ],
      "year": "2015",
      "venue": "2015 IEEE International Conference on Computer Vision (ICCV)",
      "doi": "10.1109/ICCV.2015.312"
    },
    {
      "citation_id": "4",
      "title": "DAWNBench: An End-to-End Deep Learning Benchmark and Competition",
      "authors": [
        "C Coleman",
        "D Narayanan",
        "D Kang",
        "T Zhao",
        "J Zhang",
        "L Nardi",
        "P Bailis",
        "K Olukotun",
        "C Ré",
        "M Zaharia"
      ],
      "year": "2017",
      "venue": "Workshop on ML Systems at Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "5",
      "title": "The tail at scale",
      "authors": [
        "J Dean",
        "L Barroso"
      ],
      "year": "2013",
      "venue": "Communications of the ACM",
      "doi": "10.1145/2408776.2408794"
    },
    {
      "citation_id": "6",
      "title": "Pre-training of Deep Bidirectional Transformers for Language Understanding",
      "authors": [
        "J Devlin",
        "M.-W Chang",
        "K Lee",
        "K Toutanova",
        "Bert"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
      "doi": "10.18653/v1/N19-1423"
    },
    {
      "citation_id": "7",
      "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
      "authors": [
        "A Dosovitskiy",
        "L Beyer",
        "A Kolesnikov",
        "D Weissenborn",
        "X Zhai",
        "T Unterthiner",
        "M Dehghani",
        "M Minderer",
        "G Heigold",
        "S Gelly",
        "J Uszkoreit",
        "N Houlsby"
      ],
      "year": "2023",
      "venue": "International Conference on Learning Representations"
    },
    {
      "citation_id": "8",
      "title": "The Pascal Visual Object Classes (VOC) Challenge",
      "authors": [
        "M Everingham",
        "L Van Gool",
        "C Williams",
        "J Winn",
        "A Zisserman"
      ],
      "year": "2010",
      "venue": "International Journal of Computer Vision",
      "doi": "10.1007/s11263-009-0275-4"
    },
    {
      "citation_id": "9",
      "title": "Towards Scalable and Comprehensive Datacenter AI Benchmarking",
      "authors": [
        "W Gao",
        "C Luo",
        "L Wang",
        "X Xiong",
        "J Chen",
        "T Hao",
        "Z Jiang",
        "F Fan",
        "M Du",
        "Y Huang",
        "F Zhang",
        "X Wen",
        "C Zheng",
        "X He",
        "J Dai",
        "H Ye",
        "Z Cao",
        "Z Jia",
        "K Zhan",
        "H Tang",
        "D Zheng",
        "B Xie",
        "W Li",
        "X Wang",
        "J Zhan",
        "Aibench"
      ],
      "year": "2019",
      "venue": "Towards Scalable and Comprehensive Datacenter AI Benchmarking",
      "doi": "10.1007/978-3-030-32813-91"
    },
    {
      "citation_id": "10",
      "title": "An Industry Standard Internet Service AI Benchmark Suite, October",
      "authors": [
        "W Gao",
        "F Tang",
        "L Wang",
        "J Zhan",
        "C Lan",
        "C Luo",
        "Y Huang",
        "C Zheng",
        "J Dai",
        "Z Cao",
        "D Zheng",
        "H Tang",
        "K Zhan",
        "B Wang",
        "D Kong",
        "T Wu",
        "M Yu",
        "C Tan",
        "H Li",
        "X Tian",
        "Y Li",
        "J Shao",
        "Z Wang",
        "X Wang",
        "H Ye",
        "Aibench"
      ],
      "year": "2019",
      "venue": "An Industry Standard Internet Service AI Benchmark Suite, October"
    },
    {
      "citation_id": "11",
      "title": "High fusion computers: The IoTs, edges, data centers, and humans-in-the-loop as a computer",
      "authors": [
        "W Gao",
        "L Wang",
        "M Chen",
        "J Xiong",
        "C Luo",
        "W Zhang",
        "Y Huang",
        "W Li",
        "G Kang",
        "C Zheng",
        "B Xie",
        "S Dai",
        "Q He",
        "H Ye",
        "Y Bao",
        "J Zhan"
      ],
      "year": "2022",
      "venue": "BenchCouncil Transactions on Benchmarks, Standards and Evaluations",
      "doi": "10.1016/j.tbench.2022.100075"
    },
    {
      "citation_id": "12",
      "title": "A Survey of Quantization Methods for Efficient Neural Network Inference",
      "authors": [
        "A Gholami",
        "S Kim",
        "Z Dong",
        "Z Yao",
        "M Mahoney",
        "K Keutzer"
      ],
      "year": "2021",
      "venue": "A Survey of Quantization Methods for Efficient Neural Network Inference"
    },
    {
      "citation_id": "13",
      "title": "Compressing Deep Convolutional Networks using Vector Quantization",
      "authors": [
        "Y Gong",
        "L Liu",
        "M Yang",
        "L Bourdev"
      ],
      "year": "2014",
      "venue": "Compressing Deep Convolutional Networks using Vector Quantization"
    },
    {
      "citation_id": "14",
      "title": "Generative Adversarial Nets",
      "authors": [
        "I Goodfellow",
        "J Pouget-Abadie",
        "M Mirza",
        "B Xu",
        "D Warde-Farley",
        "S Ozair",
        "A Courville",
        "Y Bengio"
      ],
      "year": "2014",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "15",
      "title": "An Empirical Study Towards Characterizing Deep Learning Development and Deployment Across Different Frameworks and Platforms",
      "authors": [
        "Q Guo",
        "S Chen",
        "X Xie",
        "L Ma",
        "Q Hu",
        "H Liu",
        "Y Liu",
        "J Zhao",
        "X Li"
      ],
      "year": "2019",
      "venue": "2019 34th IEEE/ACM International Conference on Automated Software Engineering (ASE)",
      "doi": "10.1109/ASE.2019.00080"
    },
    {
      "citation_id": "16",
      "title": "Deep Residual Learning for Image Recognition",
      "authors": [
        "K He",
        "X Zhang",
        "S Ren",
        "J Sun"
      ],
      "year": "2016",
      "venue": "2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
      "doi": "10.1109/CVPR.2016.90"
    },
    {
      "citation_id": "17",
      "title": "Measuring Massive Multitask Language Understanding",
      "authors": [
        "D Hendrycks",
        "C Burns",
        "S Basart",
        "A Zou",
        "M Mazeika",
        "D Song",
        "J Steinhardt"
      ],
      "year": "2020",
      "venue": "International Conference on Learning Representations"
    },
    {
      "citation_id": "18",
      "title": "Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference",
      "authors": [
        "B Jacob",
        "S Kligys",
        "B Chen",
        "M Zhu",
        "M Tang",
        "A Howard",
        "H Adam",
        "D Kalenichenko"
      ],
      "year": "2018",
      "venue": "2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition",
      "doi": "10.1109/CVPR.2018.00286"
    },
    {
      "citation_id": "19",
      "title": "Deep Learning Approaches on Pedestrian Detection in Hazy Weather",
      "authors": [
        "G Li",
        "Y Yang",
        "X Qu"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Industrial Electronics",
      "doi": "10.1109/TIE.2019.2945295"
    },
    {
      "citation_id": "20",
      "title": "Common Objects in Context",
      "authors": [
        "T.-Y Lin",
        "M Maire",
        "S Belongie",
        "J Hays",
        "P Perona",
        "D Ramanan",
        "P Dollár",
        "C Zitnick",
        "Coco Microsoft"
      ],
      "year": "2014",
      "venue": "Computer Vision -ECCV 2014",
      "doi": "10.1007/978-3-319-10602-148"
    },
    {
      "citation_id": "21",
      "title": "Single Shot MultiBox Detector",
      "authors": [
        "W Liu",
        "D Anguelov",
        "D Erhan",
        "C Szegedy",
        "S Reed",
        "C.-Y Fu",
        "A Berg",
        "Ssd"
      ],
      "year": "2016",
      "venue": "Computer Vision -ECCV 2016",
      "doi": "10.1007/978-3-319-46448-02"
    },
    {
      "citation_id": "22",
      "title": "Addendum: International evaluation of an AI system for breast cancer screening",
      "authors": [
        "S Mckinney",
        "M Sieniek",
        "V Godbole",
        "J Godwin",
        "N Antropova",
        "H Ashrafian",
        "T Back",
        "M Chesus",
        "G Corrado",
        "A Darzi",
        "M Etemadi",
        "F Garcia-Vicente",
        "F Gilbert",
        "M Halling-Brown",
        "D Hassabis",
        "S Jansen",
        "A Karthikesalingam",
        "C Kelly",
        "D King",
        "J Ledsam",
        "D Melnick",
        "H Mostofi",
        "L Peng",
        "J Reicher",
        "B Romera-Paredes",
        "R Sidebottom",
        "M Suleyman",
        "D Tse",
        "K Young",
        "J De Fauw",
        "S Shetty"
      ],
      "year": "2020",
      "venue": "E19-E19",
      "doi": "10.1038/s41586-020-2679-9"
    },
    {
      "citation_id": "23",
      "title": "An Imperative Style, High-Performance Deep Learning Library, December",
      "authors": [
        "A Paszke",
        "S Gross",
        "F Massa",
        "A Lerer",
        "J Bradbury",
        "G Chanan",
        "T Killeen",
        "Z Lin",
        "N Gimelshein",
        "L Antiga",
        "A Desmaison",
        "A Köpf",
        "E Yang",
        "Z Devito",
        "M Raison",
        "A Tejani",
        "S Chilamkurthy",
        "B Steiner",
        "L Fang",
        "J Bai",
        "S Chintala",
        "Pytorch"
      ],
      "year": "2019",
      "venue": "An Imperative Style, High-Performance Deep Learning Library, December"
    },
    {
      "citation_id": "24",
      "title": "Deep learning for chest radiograph diagnosis: A retrospective comparison of the CheXNeXt algorithm to practicing radiologists",
      "authors": [
        "P Rajpurkar",
        "J Irvin",
        "R Ball",
        "K Zhu",
        "B Yang",
        "H Mehta",
        "T Duan",
        "D Ding",
        "A Bagul",
        "C Langlotz",
        "B Patel",
        "K Yeom",
        "K Shpanskaya",
        "F Blankenberg",
        "J Seekins",
        "T Amrhein",
        "D Mong",
        "S Halabi",
        "E Zucker",
        "A Ng",
        "M Lungren"
      ],
      "year": "2018",
      "venue": "PLOS Medicine",
      "doi": "https://journals.plos.org/plosmedicine/article?id=10.1371/journal.pmed.1002686"
    },
    {
      "citation_id": "25",
      "title": "MLPerf Inference Benchmark",
      "authors": [
        "V Reddi",
        "C Cheng",
        "D Kanter",
        "P Mattson",
        "G Schmuelling",
        "C.-J Wu",
        "B Anderson",
        "M Breughe",
        "M Charlebois",
        "W Chou",
        "R Chukka",
        "C Coleman",
        "S Davis",
        "P Deng",
        "G Diamos",
        "J Duke",
        "D Fick",
        "J Gardner",
        "I Hubara",
        "S Idgunji",
        "T Jablin",
        "J Jiao",
        "T John",
        "P Kanwar",
        "D Lee",
        "J Liao",
        "A Lokhmotov",
        "F Massa",
        "P Meng",
        "P Micikevicius",
        "C Osborne",
        "G Pekhimenko",
        "A Rajan",
        "D Sequeira",
        "A Sirasao",
        "F Sun",
        "H Tang",
        "M Thomson",
        "F Wei",
        "E Wu",
        "L Xu",
        "K Yamada",
        "B Yu",
        "G Yuan",
        "A Zhong",
        "P Zhang",
        "Y Zhou"
      ],
      "year": "2020",
      "venue": "2020 ACM/IEEE 47th Annual International Symposium on Computer Architecture (ISCA)",
      "doi": "10.1109/ISCA45697.2020.00045"
    },
    {
      "citation_id": "26",
      "title": "ImageNet Large Scale Visual Recognition Challenge",
      "authors": [
        "O Russakovsky",
        "J Deng",
        "H Su",
        "J Krause",
        "S Satheesh",
        "S Ma",
        "Z Huang",
        "A Karpathy",
        "A Khosla",
        "M Bernstein",
        "A Berg",
        "L Fei-Fei"
      ],
      "year": "2015",
      "venue": "International Journal of Computer Vision",
      "doi": "10.1007/s11263-015-0816-y"
    },
    {
      "citation_id": "27",
      "title": "I. Software Engineering",
      "authors": [
        "Sommerville"
      ],
      "year": "2011",
      "venue": "I. Software Engineering"
    },
    {
      "citation_id": "28",
      "title": "EfficientDet: Scalable and Efficient Object Detection",
      "authors": [
        "M Tan",
        "R Pang",
        "Q Le"
      ],
      "year": "2020",
      "venue": "2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)",
      "doi": "10.1109/CVPR42600.2020.01079"
    },
    {
      "citation_id": "29",
      "title": "Toward Performing Image Classification and Object Detection With Convolutional Neural Networks in Autonomous Driving Systems: A Survey",
      "authors": [
        "T Turay",
        "T Vladimirova"
      ],
      "year": "2022",
      "venue": "IEEE Access",
      "doi": "10.1109/ACCESS.2022.3147495"
    },
    {
      "citation_id": "30",
      "title": "Attention is All you Need",
      "authors": [
        "A Vaswani",
        "N Shazeer",
        "N Parmar",
        "J Uszkoreit",
        "L Jones",
        "A Gomez",
        "Ł Kaiser",
        "I Polosukhin"
      ],
      "year": "2017",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "31",
      "title": "Generalizing to Unseen Domains: A Survey on Domain Generalization",
      "authors": [
        "J Wang",
        "C Lan",
        "C Liu",
        "Y Ouyang",
        "T Qin"
      ],
      "venue": "Twenty-Ninth International Joint Conference on Artificial Intelligence",
      "doi": "10.24963/ijcai.2021/628"
    },
    {
      "citation_id": "32",
      "title": "Rapid holistic perception and evasion of road hazards",
      "authors": [
        "B Wolfe",
        "B Seppelt",
        "B Mehler",
        "B Reimer",
        "R Rosenholtz"
      ],
      "year": "2020",
      "venue": "Journal of Experimental Psychology. General",
      "doi": "10.1037/xge0000665"
    },
    {
      "citation_id": "33",
      "title": "Effects of temporal and spatiotemporal cues on detection of dynamic road hazards",
      "authors": [
        "B Wolfe",
        "A Kosovicheva",
        "S Stent",
        "R Rosenholtz"
      ],
      "year": "2021",
      "venue": "Cognitive Research: Principles and Implications",
      "doi": "10.1186/s41235-021-00348-4"
    },
    {
      "citation_id": "34",
      "title": "End-to-End Learning of Driving Models from Large-Scale Video Datasets",
      "authors": [
        "H Xu",
        "Y Gao",
        "F Yu",
        "T Darrell"
      ],
      "year": "2017",
      "venue": "2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
      "doi": "10.1109/CVPR.2017.376"
    },
    {
      "citation_id": "35",
      "title": "A Survey of Autonomous Driving: Common Practices and Emerging Technologies",
      "authors": [
        "E Yurtsever",
        "J Lambert",
        "A Carballo",
        "K Takeda"
      ],
      "year": "2020",
      "venue": "IEEE Access",
      "doi": "10.1109/ACCESS.2020.2983149"
    },
    {
      "citation_id": "36",
      "title": "A BenchCouncil view on benchmarking emerging and future computing. BenchCouncil Transactions on Benchmarks, Standards and Evaluations",
      "authors": [
        "J Zhan"
      ],
      "year": "2022",
      "venue": "A BenchCouncil view on benchmarking emerging and future computing. BenchCouncil Transactions on Benchmarks, Standards and Evaluations",
      "doi": "10.1016/j.tbench.2022.100064"
    },
    {
      "citation_id": "37",
      "title": "Deep Emotion Recognition using Facial, Speech and Textual Cues: A Survey",
      "authors": [
        "T Zhang",
        "Z Tan"
      ],
      "year": "2021",
      "venue": "Deep Emotion Recognition using Facial, Speech and Textual Cues: A Survey"
    },
    {
      "citation_id": "38",
      "title": "Judging LLM-as-a-judge with MT-Bench and Chatbot Arena",
      "authors": [
        "L Zheng",
        "W.-L Chiang",
        "Y Sheng",
        "S Zhuang",
        "Z Wu",
        "Y Zhuang",
        "Z Lin",
        "Z Li",
        "D Li",
        "E Xing",
        "H Zhang",
        "J Gonzalez",
        "I Stoica"
      ],
      "year": "2023",
      "venue": "Judging LLM-as-a-judge with MT-Bench and Chatbot Arena"
    },
    {
      "citation_id": "39",
      "title": "Domain Generalization: A Survey",
      "authors": [
        "K Zhou",
        "Z Liu",
        "Y Qiao",
        "T Xiang",
        "C Loy"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
      "doi": "10.1109/TPAMI.2022.3195549"
    }
  ]
}