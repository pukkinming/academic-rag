{
  "paper_id": "2008.06682v1",
  "title": "Jointly Fine-Tuning \"Bert-Like\" Self Supervised Models To Improve Multimodal Speech Emotion Recognition",
  "published": "2020-08-15T08:54:48Z",
  "authors": [
    "Shamane Siriwardhana",
    "Andrew Reis",
    "Rivindu Weerasekera",
    "Suranga Nanayakkara"
  ],
  "keywords": [
    "speech emotion recognition",
    "self supervised learning",
    "Transformers",
    "BERT",
    "multimodal deep learning"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Multimodal emotion recognition from speech is an important area in affective computing. Fusing multiple data modalities and learning representations with limited amounts of labeled data is a challenging task. In this paper, we explore the use of modality specific\"BERT-like\" pretrained Self Supervised Learning (SSL) architectures to represent both speech and text modalities for the task of multimodal speech emotion recognition. By conducting experiments on three publicly available datasets (IEMOCAP, CMU-MOSEI, and CMU-MOSI), we show that jointly fine-tuning \"BERT-like\" SSL architectures achieve state-of-the-art (SOTA) results. We also evaluate two methods of fusing speech and text modalities and show that a simple fusion mechanism can outperform more complex ones when using SSL models that have similar architectural properties to BERT.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Emotion recognition plays a significant role in many intelligent interfaces  [1] . Even with the recent advances in Deep Learning (DL), this is still a challenging task. The main reason being that most publicly available annotated datasets in this domain are small in scale, which makes DL models prone to over-fitting. Another important feature of emotion recognition is the inherent multi-modality in the way we express emotions  [2] . Emotional information can be captured by studying many modalities, including facial expressions, body postures, and EEG  [3] . Of these, arguably, speech is the most accessible. In addition to accessibility, speech signals contain many other emotional cues  [4] . Although speech signals contain substantial amounts of information, it can be unrewarding to drop the linguistic component that coexists with it, especially given that the text component can be easily transcribed in real world applications with the considerable successes in the domain of speech-to-text with several commercial-scale APIs being available  [5] .\n\nIn multimodal emotion recognition, representation learning and fusion of modalities can be identified as a major research area  [6, 7, 8] . Recent work has explored the use of deep representations in contrast to low level representations  [9]  such as MFCC, COVAREP  [10]  or GloVe embeddings  [11] . Such deep representation techniques can mainly be categorised into two main categories: 1) transfer learning techniques that use pretrained networks to extract features  [12, 13, 14]  or fine-tune models  [15] ; and 2) unsupervised embeddings learning techniques which include variational auto-encoders (VAE)  [16]  and adversarial auto-encoders (AE)  [17] . It is also important to highlight that performance usually degrades in transfer learning techniques due to the mismatch of source and target tasks. Recent work  [18]  explains the problems related to learn disen-tangled representations from VAE when no inductive bias on the model or the dataset exists. In terms of fusing multiple modalities, recent work has explored architectures like attention  [2, 7] , graph neural networks  [19]  and transformers  [8] . Multimodal fusion mechanisms, especially those that fuse deep representations, usually result in architecturally complex models  [20] .\n\nIn representation learning, a class of techniques known as SSL has achieved SOTA performance in many areas of Natual Language Processing (NLP)  [21, 22] , Computer Vision  [23, 24]  (CV) and speech recognition  [25, 26, 27] . SSL enables us to use a large unlabelled dataset to train models that can be later used to extract representations and fine-tune for specific problems that may have limited amounts of training data. Prior works  [21, 23]  have highlighted the effectiveness of fine-tuning pre-trained SSL models for specific tasks in contrast to using them only as frozen feature extractors. A significant transition happened in the field of NLP with the introduction of SSL models like the Deep Bidirectional Transformers  [21]  (BERT) and its successors  [22] . By adding a single task-specific layer to a pre-trained SSL model like BERT, one can solve multiple downstream tasks. BERT-like models also consist of favourable architectural features such as the CLS token, which can be used as a representation for the entire sequence. Another important factor is the extensive availability of pre-trained models in the open-source community, which leads to both cost and time savings since these models tend to be very computationally expensive to train from scratch.\n\nEven though several SSL models have been introduced for speech recognition related tasks like speech-to-text  [26, 27]  and speech emotion recognition  [25] , prior work has not looked at combining multiple separate SSL models, each specializing in one modality. This may be due to the architectural complexity of such models brought on by the fact that usually, these SSL networks have a large number of parameters. Combining multiple drastically different high dimensional representations is also not a simple task and may increase the parameter count even further. If, however, the modality specific SSL architectures share similar properties, then we may be able to use simpler fusion mechanisms to extract information for the desired task. Our work was heavily inspired by recent work  [27, 28] , which explored the effectiveness of self supervised pre-training with discretized speech representations for the task of Automatic Speech Recognition (ASR).\n\nFor the first time in the literature, we jointly finetuned modality-specific \"BERT-like\" SSL models that represent speech  [28, 27]  and text  [24]  on the task of multimodal emotion recognition. We further evaluate how simple fusion methods,which add minimal additional trainable parameters, performed when compared with more complex fusion mechanisms such as Co-Attentional  [24]  fusion. We also conducted a series of ablation studies to explore which factors affect the performance of these models. Please refer the Pytorch implementation.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Pre-Trained Ssl Models",
      "text": "We summarise the three pretrained SSL models that were used to process speech and text signals in the proposed framework in the following sections. We use pretrained models available in the Fairseq toolkit  [29] .",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Vq-Wav2Vec",
      "text": "VQ-Wav2Vec  [27]  is an extension of Wav2Vec  [26] , which focuses on moving continuous speech representations into the discrete domain. Wav2Vec  [26]  learns representations from speech signals based on Contrastive Predictive Coding  [30]  (CPC). The major difference in VQ-Wav2Vec  [27]  from Wav2Vec  [26]  is the application of Vector Quantization  [31]  methods to generate discretized speech representations. In our experiments, we used a pretrained VQ-Wav2Vec  [27]  model that trained on Librispeech-960  [32]  to represent speech signals as a sequence of tokens, similar to the tokenization step for a sentence in NLP.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Speech-Bert",
      "text": "The term Speech-BERT is used in our work to define a BERTlike Transformer architecture trained on a set of discretized speech tokens, where the speech signal was discretized and tokenized by a pretrained VQ-Wav2Vec as mentioned in the above section. We were heavily motivated by recent work  [28] , which illustrated the effectiveness of BERT-like models in the domain of ASR. We used a pretrained Speech-BERT, which was trained on the discretized Librispeech-960  [32]  dataset with the pretext task of mask token prediction. The Speech-BERT model architecture is similar to BERT-base  [21]  that consists of 12 layers and an embedding dimension of 768. During our experiments, we fine-tune the Speech-BERT model for the task of multimodal emotion recognition.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Roberta Model",
      "text": "RoBERTa  [22]  is an extension of the BERT  [21]  model which does not use the next sentence prediction task  [22]  during training. The RoBERTa  [22]  architecture consists of 24 layers and an embedding dimension of 1024. Similar to Speech-BERT, we fine-tune the RoBERTA  [22]  model for the task of multimodal emotion recognition.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Methodology",
      "text": "We explore the use of Speech-BERT and RoBERTa SSL models for the task of multimodal speech emotion recognition. As the first step, we evaluate two possible fusion mechanisms to combine the two SSL models. The performance of the final proposed model was then compared with published SOTA results on IEMOCAP  [33] , CMU-MOSEI  [34] , and CMU-MOSI  [35]  datasets. Finally, we conduct an extensive set of ablation studies with the IEMOCAP dataset  [33]  to understand the behaviour of our proposed framework under different settings. We investigate the effects of fine-tuning and frozen states for Speech-BERT, RoBERTa, as well as the effects of different fusion mechanisms.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Model Pipeline",
      "text": "Figure  1  gives an overview of the proposed framework. Both the speech signal and text transcripts are simultaneously fed into the model via two different pipelines. The speech signal gets discretized by a pre-trained VQ-Wav2Vec  [27]   text transcript is tokenized with the GPT-2 tokenizer  [36] . Once speech and text modalities are tokenized, we send them through pre-trained Speech-BERT and RoBERTa models, where the outputs have embedding sizes 768 and 1024 and maximum sequence lengths of 2048 and 512 respectively. The next step is fusing these embeddings prior to the prediction head, which consists of a single fully connected layer. In this paper explore two possible mechanisms, which we will discuss in Section 3.2. Finally, we fine-tune the entire framework, including both Speech-BERT and Roberta SSL models (the components inside the blue dotted box in Figure  1 ).",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Fusion Of Ssl Model Outputs",
      "text": "The fusion mechanism plays an essential role in any multimodal speech emotion recognition framework. In this work, we analyse how the following different fusion mechanisms affect performance.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Shallow Fusion",
      "text": "The success of BERT in sentence classification tasks  [21]  highlights the effective use of the CLS token as a representation of the entire sentence. CLS stands for the classification and it is the first token of every input sequence to the BERT. Motivated by recent work in the domain of NLP  [21, 22] , we concatenate the two CLS tokens computed respectively from speech-BERT and RoBERTa models as described in Figure  1  . Finally we send the concatenated embedding through a classification head that includes a fully connected layer that outputs logits followed by a softmax function. Due to the simplicity of the fusion, we describe this mechanism as Shallow-Fusion. We also use Shallow-Fusion as the standard fusion mechanism in the ablation studies since it achieved superior performances in our experiments.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Co-Attentional Fusion",
      "text": "In order to provide an opportunity for an embedding level interaction between the two modalities, we propose to use a Co-Attentional layer  [24] . Co-Attention is a variant of Self-Attention  [37]  that has been used in visual-linguistic Trans-   [24] . In contrast to Self-Attention, Co-Attention is computed by interchanging Key-Value vector pairs from one modality with the Query vector from another modality. Since the CLS token from each modality already aggregates the sequential information  [21] , we calculate the Query vector from each mortality's CLS token and let it attend the entire sequence of other modality embeddings. After the Co-Attentional layer, we concatenate the modified CLS tokens from each modality and send these through a prediction head. Figure  2  gives a detailed illustration of the Co-Attentional layer and the fusion mechanism.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Text To Speech Co-Attention Speech To Text Co-Attention",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Add",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Multi-Head Attention",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Experimental Setup",
      "text": "We implemented our model using Pytorch and the Fairseq  [29]  toolkit. All models were trained under distributed settings, using two Tesla v100 32GB GPUs with an effective batch-size of 16. The Adam optimiser was used in the optimization with warm-up updates and the polynomial decay learning-rate scheduler. The initial learning rate and dropout were set to 1e -5 and 0.1.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Performance Comparison With Sota",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Iemocap Experiments",
      "text": "The IEMOCAP  [33]  dataset contains conversation data of 10 male and female actors. Similar to prior work  [38, 2] , we selected the most commonly used four emotion categories of Happy (& Excitement), Sad, Anger, and Neutral. We followed the experimental procedure and evaluation metrics of previous studies  [8, 39] . Table  4  provides a comparison of model performance with other SOTA models on Binary Accuracy (BA) and the F1-score  [39, 8] . Table  5  illustrates the performance comparison w.r.t 4-class unweighted accuracy metric following the recent work done by Li et al  [2] .  [34]  is the current largest dataset for multimodal emotion recognition that consists of 22, 000 examples created by extracting review videos from YouTube. Each example is annotated with an integer score between -3 to +3. CMU-MOSI  [35]  also has similar properties to CMU-MOSEI  [34]  but only with 2000 examples. To compare our model with both datasets, we follow the latest prior work that has used these datasets  [8, 39] . For both, we used 7-class accuracy, Mean-Average-Error (MAE), 2-class accuracy (binary), and F1-score. Table  2  and Table  3  describe the evaluation results on CMU-MOSEI  [34]  and CMU-MOSI  [35]  datasets respectively.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Cmu-Mosei And Cmu-Mosi Experiments",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Cmu-Mosei",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Ablation Studies",
      "text": "We conducted four ablation studies using the IEMOCAP  [33]  dataset to understand the behaviour of the proposed framework. We use Binary-Accuracy and F1 score for each emotion as the evaluation metric. In order to have a fair setting in our ablation experiments, we use the first three sessions of the IEMO-CAP  [33]  dataset as training, the fourth session as validation and fifth session as the test set. Table  1  illustrates the results of our four ablation studies discussed in the following sections.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Comparison Between Two Fusion Mechanisms In Fine-Tune State",
      "text": "In the first ablation study, Table  1  (5.1), we compare the performance of each fusion mechanism when fine-tuning Speech-Bert and Roberta for the downstream task of multimodal-emotion recognition. Shallow-Fusion shows a slight improvement with respect to binary accuracy and F1-scores for each emotion over Co-Attentional fusion. This illustrates how a simple fusion mechanism followed by a classification head work remarkably well with finetuned \"Bert-Like\"\" pretrained SSL models even in a multimodal setting.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Comparison Between Unimodal Inputs",
      "text": "Table  1  (5.2) illustrates the performance comparison between uni-modal inputs. In this experiment, we use the CLS token of Speech-BERT and Roberta as the sequence representation for speech and text. As the results suggest, text-only performs better than speech-only. A possible reason might be the availability of high emotional clues in the linguistic structure. However, we can still see a clear improvement when comparing text-only results with our best performing multimodal model, which highlights the importance of multi-modality.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Comparison Between Fine-Tuning And Frozen Ssl Architectures",
      "text": "Table  1  (5.3) looks at the effect of finetuning vs having frozen Speech-BERT and RoBERTa models. Unsurprisingly, finetuning the two SSL models with Shallow-Fusion for the downstream task leads to better performance.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Comparison Between The Two Fusion Mechanisms When Keeping Ssl Models Frozen",
      "text": "Finally, we compare how the two fusion mechanism behave when we keep Speech-BERT and RoBERTa in a frozen state -using them only as feature extractors. Table  1  (5.4) shows when the SSL networks are frozen, Co-Attentional fusion performs better. We highlight that the increased number of interactions prior to the prediction layers enables Co-Attentional fusion to adapt better than Shallow-Fusion. Co-Attentioal fusion   requires a much larger number of new trainable parameters. While the Co-Attentional layer adds nearly 6 million new parameters, Shallow-Fusion only adds close to fourteen thousand parameters.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Conclusion",
      "text": "In this work, we use two pretrained \"BERT-like\" architectures to solve the downstream task of multimodal emotion recognition. As per our knowledge, this is the first time that two SSL algorithms that represent speech and texts are fine-tuned for the task of multimodal speech emotion recognition. By conducting several experiments, we show how a simple fusion mechanism (Shallow-Fusion) makes the overall framework simple and straightforward and improve on more complex fusion mechanisms. We also highlight the importance of introducing BERTlike models to process speech signals, which can easily be used to improve the performance of multimodal tasks like emotion recognition. Having structurally similar \"BERT-like\" architectures to represent both speech and text allows us to fuse modalities in a straightforward way and quickly adapt standard practices in the NLP domain.\n\nIn future work, we hope to visualize and further explore the behavior of SSL models for the task of multimodal emotion recognition. Exploring the use of BERT-like models to represent speech could enable advances in NLP to be easily used in the domain of speech.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: gives an overview of the proposed framework. Both the",
      "page": 2
    },
    {
      "caption": "Figure 1: Overview of the proposed framework with Shallow-",
      "page": 2
    },
    {
      "caption": "Figure 1: . Finally we send",
      "page": 2
    },
    {
      "caption": "Figure 2: Co-Attentional layer and fusion mechanism",
      "page": 3
    },
    {
      "caption": "Figure 2: gives a detailed illustration of the Co-Attentional layer",
      "page": 3
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Ablation Studies\nHappy\nSad\nAngry\nNeutral": "Algorithm                                        Metric\nAcc(h)\nF1(h) \nAcc(h)\nF1(h) \nAcc(h)\nF1(h) \nAcc(h)\nF1(h)"
        },
        {
          "Ablation Studies\nHappy\nSad\nAngry\nNeutral": "(5.1) Shallow-Fusion VS Co-Attentional Fusion  (SSL models in get finetuned on the task)"
        },
        {
          "Ablation Studies\nHappy\nSad\nAngry\nNeutral": "Co-Attentional-Fusion\n83.64\n83.07\n90.12\n90.13\n92.56\n93.01\n81.06\n79.97\nShallow-Fusion (Best Performaing Model)\n84.13\n83.75\n90.894\n90.7\n93.56\n93.62\n81.55\n81.05"
        },
        {
          "Ablation Studies\nHappy\nSad\nAngry\nNeutral": "(5.2)  Uni-Modal Comparison"
        },
        {
          "Ablation Studies\nHappy\nSad\nAngry\nNeutral": "Speech - Speech-BERT only\n71.15\n66.87\n85.657\n85.01\n87.67\n87.73\n71.78\n68.9\nText -  Roberta Only\n81.02\n81.62\n87.67\n87.1\n91.02\n91.21\n78.72\n78.85"
        },
        {
          "Ablation Studies\nHappy\nSad\nAngry\nNeutral": "(5.3)  Fine-Tune VS Frozen (using Shallow-Fusion)"
        },
        {
          "Ablation Studies\nHappy\nSad\nAngry\nNeutral": "Frozen SSL\n69.22\n62.98\n85.57\n83.92\n89.68\n89.5\n75.34\n72.95\nFine-Tune SSL (same as best performing model)\n84.13\n83.75\n90.894\n90.7\n93.56\n93.62\n81.55\n81.05"
        },
        {
          "Ablation Studies\nHappy\nSad\nAngry\nNeutral": "Ablation Studies\nNeutral\n(5.4) Shallow-Fusion VS Co-Attentional Fusion (SSL models in frozen state)"
        },
        {
          "Ablation Studies\nHappy\nSad\nAngry\nNeutral": "Algorithm                                        Metric\nAcc(h)\nF1(h) \nAcc(h)\nF1(h) \nAcc(h)\nF1(h) \nAcc(h)\nF1(h) \n(5.1) Shallow-Fusion VS Co-Attentional Fusion  (SSL models in get finetuned on the task)\nShallow-Fusion (frozen SSL)\n75.34\n72.95"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Task": "Algorithm                                        Metric",
          "Happy": "Acc(h)\nF1(h)",
          "Sad": "Acc(h)\nF1(h)",
          "Angry": "Acc(h)\nF1(h)",
          "Neutral": "Acc(h)\nF1(h)"
        },
        {
          "Task": "MulT (Tsai et al., 2019)",
          "Happy": "84.4\n81.9",
          "Sad": "77.7\n74.1",
          "Angry": "73.9\n70.2",
          "Neutral": "62.5\n59.7"
        },
        {
          "Task": "ICCN (Sun et al - 2019)",
          "Happy": "87.41\n84.72",
          "Sad": "86.26\n85.93",
          "Angry": "88.62\n88.02",
          "Neutral": "69.73\n68.47"
        },
        {
          "Task": "Shallow-Fusion of SSL models  (Ours)",
          "Happy": "89.71\n88.34",
          "Sad": "89.48\n89.2",
          "Angry": "93.82\n93.9",
          "Neutral": "80.93\n81.01"
        }
      ],
      "page": 4
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "SSL models in get finetuned on the task) Co-Attentional-Fusion",
      "venue": "SSL models in get finetuned on the task) Co-Attentional-Fusion"
    },
    {
      "citation_id": "2",
      "title": "",
      "authors": [
        "Fine-Tune Vs Frozen"
      ],
      "venue": ""
    },
    {
      "citation_id": "3",
      "title": "",
      "authors": [
        "Ssl Frozen"
      ],
      "venue": ""
    },
    {
      "citation_id": "4",
      "title": "SSL models in frozen state) Co-Attentional-Fusion",
      "venue": "SSL models in frozen state) Co-Attentional-Fusion"
    },
    {
      "citation_id": "5",
      "title": "Shallow-Fusion (frozen SSL)",
      "venue": "Shallow-Fusion (frozen SSL)"
    },
    {
      "citation_id": "6",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "7",
      "title": "Affective computing",
      "authors": [
        "R Picard"
      ],
      "year": "2000",
      "venue": "Affective computing"
    },
    {
      "citation_id": "8",
      "title": "Attentive to individual: A multimodal emotion recognition network with personalized attention profile",
      "authors": [
        "J.-L Li",
        "C.-C Lee"
      ],
      "year": "2019",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "9",
      "title": "Multimodal approaches for emotion recognition: a survey",
      "authors": [
        "N Sebe",
        "I Cohen",
        "T Gevers",
        "T Huang"
      ],
      "year": "2005",
      "venue": "Internet Imaging VI"
    },
    {
      "citation_id": "10",
      "title": "Emotion recognition from human speech using temporal information and deep learning",
      "authors": [
        "J Kim",
        "R Saurous"
      ],
      "year": "2018",
      "venue": "Interspeech"
    },
    {
      "citation_id": "11",
      "title": "Asroil: a comprehensive survey for automatic speech recognition of indian languages",
      "authors": [
        "A Singh",
        "V Kadyan",
        "M Kumar",
        "N Bassan"
      ],
      "year": "2019",
      "venue": "Artificial Intelligence Review"
    },
    {
      "citation_id": "12",
      "title": "Learning factorized multimodal representations",
      "authors": [
        "Y.-H Tsai",
        "P Liang",
        "A Zadeh",
        "L.-P Morency",
        "R Salakhutdinov"
      ],
      "year": "2018",
      "venue": "Learning factorized multimodal representations",
      "arxiv": "arXiv:1806.06176"
    },
    {
      "citation_id": "13",
      "title": "Multi-attention recurrent network for human communication comprehension",
      "authors": [
        "A Zadeh",
        "P Liang",
        "S Poria",
        "P Vij",
        "E Cambria",
        "L.-P Morency"
      ],
      "year": "2018",
      "venue": "Thirty-Second AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "14",
      "title": "Multimodal transformer for unaligned multimodal language sequences",
      "authors": [
        "Y.-H Tsai",
        "S Bai",
        "P Liang",
        "J Kolter",
        "L.-P Morency",
        "R Salakhutdinov"
      ],
      "year": "2019",
      "venue": "Multimodal transformer for unaligned multimodal language sequences",
      "arxiv": "arXiv:1906.00295"
    },
    {
      "citation_id": "15",
      "title": "Databases, features and classifiers for speech emotion recognition: a review",
      "authors": [
        "M Swain",
        "A Routray",
        "P Kabisatpathy"
      ],
      "year": "2018",
      "venue": "International Journal of Speech Technology"
    },
    {
      "citation_id": "16",
      "title": "Covarepa collaborative voice analysis repository for speech technologies",
      "authors": [
        "G Degottex",
        "J Kane",
        "T Drugman",
        "T Raitio",
        "S Scherer"
      ],
      "year": "2014",
      "venue": "2014 ieee international conference on acoustics, speech and signal processing"
    },
    {
      "citation_id": "17",
      "title": "Glove: Global vectors for word representation",
      "authors": [
        "J Pennington",
        "R Socher",
        "C Manning"
      ],
      "year": "2014",
      "venue": "Proceedings of the 2014 conference on empirical methods in natural language processing"
    },
    {
      "citation_id": "18",
      "title": "Transfer learning for speech emotion recognition",
      "authors": [
        "Z Han",
        "H Zhao",
        "R Wang"
      ],
      "year": "2019",
      "venue": "2019 IEEE 5th Intl Conference on Big Data Security on Cloud (BigDataSecurity), IEEE Intl Conference on High Performance and Smart Computing,(HPSC) and IEEE Intl Conference on Intelligent Data and Security"
    },
    {
      "citation_id": "19",
      "title": "Improving emotion classification through variational inference of latent variables",
      "authors": [
        "S Parthasarathy",
        "V Rozgic",
        "M Sun",
        "C Wang"
      ],
      "year": "2019",
      "venue": "ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "20",
      "title": "A review of generalizable transfer learning in automatic emotion recognition",
      "authors": [
        "K Feng",
        "T Chaspari"
      ],
      "year": "2020",
      "venue": "Frontiers in Computer Science"
    },
    {
      "citation_id": "21",
      "title": "Speech sentiment analysis via pre-trained features from end-to-end asr models",
      "authors": [
        "Z Lu",
        "L Cao",
        "Y Zhang",
        "C.-C Chiu",
        "J Fan"
      ],
      "year": "2019",
      "venue": "Speech sentiment analysis via pre-trained features from end-to-end asr models",
      "arxiv": "arXiv:1911.09762"
    },
    {
      "citation_id": "22",
      "title": "Variational autoencoders for learning latent representations of speech emotion: A preliminary study",
      "authors": [
        "S Latif",
        "R Rana",
        "J Qadir",
        "J Epps"
      ],
      "year": "2017",
      "venue": "Variational autoencoders for learning latent representations of speech emotion: A preliminary study",
      "arxiv": "arXiv:1712.08708"
    },
    {
      "citation_id": "23",
      "title": "Adversarial auto-encoders for speech based emotion recognition",
      "authors": [
        "S Sahu",
        "R Gupta",
        "G Sivaraman",
        "W Abdalmageed",
        "C Espy-Wilson"
      ],
      "year": "2018",
      "venue": "Adversarial auto-encoders for speech based emotion recognition",
      "arxiv": "arXiv:1806.02146"
    },
    {
      "citation_id": "24",
      "title": "Challenging common assumptions in the unsupervised learning of disentangled representations",
      "authors": [
        "F Locatello",
        "S Bauer",
        "M Lucic",
        "G Rätsch",
        "S Gelly",
        "B Schölkopf",
        "O Bachem"
      ],
      "year": "2018",
      "venue": "Challenging common assumptions in the unsupervised learning of disentangled representations",
      "arxiv": "arXiv:1811.12359"
    },
    {
      "citation_id": "25",
      "title": "Dialoguegcn: A graph convolutional neural network for emotion recognition in conversation",
      "authors": [
        "D Ghosal",
        "N Majumder",
        "S Poria",
        "N Chhaya",
        "A Gelbukh"
      ],
      "year": "2019",
      "venue": "Dialoguegcn: A graph convolutional neural network for emotion recognition in conversation",
      "arxiv": "arXiv:1908.11540"
    },
    {
      "citation_id": "26",
      "title": "Multimodal intelligence: Representation learning, information fusion, and applications",
      "authors": [
        "C Zhang",
        "Z Yang",
        "X He",
        "L Deng"
      ],
      "year": "2019",
      "venue": "Multimodal intelligence: Representation learning, information fusion, and applications",
      "arxiv": "arXiv:1911.03977"
    },
    {
      "citation_id": "27",
      "title": "Bert: Pretraining of deep bidirectional transformers for language understanding",
      "authors": [
        "J Devlin",
        "M.-W Chang",
        "K Lee",
        "K Toutanova"
      ],
      "year": "2018",
      "venue": "Bert: Pretraining of deep bidirectional transformers for language understanding",
      "arxiv": "arXiv:1810.04805"
    },
    {
      "citation_id": "28",
      "title": "Roberta: A robustly optimized bert pretraining approach",
      "authors": [
        "Y Liu",
        "M Ott",
        "N Goyal",
        "J Du",
        "M Joshi",
        "D Chen",
        "O Levy",
        "M Lewis",
        "L Zettlemoyer",
        "V Stoyanov"
      ],
      "year": "2019",
      "venue": "Roberta: A robustly optimized bert pretraining approach",
      "arxiv": "arXiv:1907.11692"
    },
    {
      "citation_id": "29",
      "title": "Revisiting self-supervised visual representation learning",
      "authors": [
        "A Kolesnikov",
        "X Zhai",
        "L Beyer"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "30",
      "title": "Vilbert: Pretraining taskagnostic visiolinguistic representations for vision-and-language tasks",
      "authors": [
        "J Lu",
        "D Batra",
        "D Parikh",
        "S Lee"
      ],
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "31",
      "title": "Learning problem-agnostic speech representations from multiple self-supervised tasks",
      "authors": [
        "S Pascual",
        "M Ravanelli",
        "J Serrà",
        "A Bonafonte",
        "Y Bengio"
      ],
      "year": "2019",
      "venue": "Learning problem-agnostic speech representations from multiple self-supervised tasks",
      "arxiv": "arXiv:1904.03416"
    },
    {
      "citation_id": "32",
      "title": "wav2vec: Unsupervised pre-training for speech recognition",
      "authors": [
        "S Schneider",
        "A Baevski",
        "R Collobert",
        "M Auli"
      ],
      "year": "2019",
      "venue": "wav2vec: Unsupervised pre-training for speech recognition",
      "arxiv": "arXiv:1904.05862"
    },
    {
      "citation_id": "33",
      "title": "vq-wav2vec: Selfsupervised learning of discrete speech representations",
      "authors": [
        "A Baevski",
        "S Schneider",
        "M Auli"
      ],
      "year": "2019",
      "venue": "vq-wav2vec: Selfsupervised learning of discrete speech representations",
      "arxiv": "arXiv:1910.05453"
    },
    {
      "citation_id": "34",
      "title": "Effectiveness of selfsupervised pre-training for speech recognition",
      "authors": [
        "A Baevski",
        "M Auli",
        "A Mohamed"
      ],
      "year": "2019",
      "venue": "Effectiveness of selfsupervised pre-training for speech recognition",
      "arxiv": "arXiv:1911.03912"
    },
    {
      "citation_id": "35",
      "title": "fairseq: A fast, extensible toolkit for sequence modeling",
      "authors": [
        "M Ott",
        "S Edunov",
        "A Baevski",
        "A Fan",
        "S Gross",
        "N Ng",
        "D Grangier",
        "M Auli"
      ],
      "year": "2019",
      "venue": "fairseq: A fast, extensible toolkit for sequence modeling",
      "arxiv": "arXiv:1904.01038"
    },
    {
      "citation_id": "36",
      "title": "Representation learning with contrastive predictive coding",
      "authors": [
        "A Oord",
        "Y Li",
        "O Vinyals"
      ],
      "year": "2018",
      "venue": "Representation learning with contrastive predictive coding",
      "arxiv": "arXiv:1807.03748"
    },
    {
      "citation_id": "37",
      "title": "Neural discrete representation learning",
      "authors": [
        "A Van Den Oord",
        "O Vinyals"
      ],
      "year": "2017",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "38",
      "title": "Librispeech: an asr corpus based on public domain audio books",
      "authors": [
        "V Panayotov",
        "G Chen",
        "D Povey",
        "S Khudanpur"
      ],
      "year": "2015",
      "venue": "2015 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "39",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "40",
      "title": "Multimodal language analysis in the wild: Cmu-mosei dataset and interpretable dynamic fusion graph",
      "authors": [
        "A Zadeh",
        "P Liang",
        "S Poria",
        "E Cambria",
        "L.-P Morency"
      ],
      "year": "2018",
      "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "41",
      "title": "Mosi: multimodal corpus of sentiment intensity and subjectivity analysis in online opinion videos",
      "authors": [
        "A Zadeh",
        "R Zellers",
        "E Pincus",
        "L.-P Morency"
      ],
      "year": "2016",
      "venue": "Mosi: multimodal corpus of sentiment intensity and subjectivity analysis in online opinion videos",
      "arxiv": "arXiv:1606.06259"
    },
    {
      "citation_id": "42",
      "title": "Language models are unsupervised multitask learners",
      "authors": [
        "A Radford",
        "J Wu",
        "R Child",
        "D Luan",
        "D Amodei",
        "I Sutskever"
      ],
      "year": "2019",
      "venue": "OpenAI Blog"
    },
    {
      "citation_id": "43",
      "title": "Attention is all you need",
      "authors": [
        "A Vaswani",
        "N Shazeer",
        "N Parmar",
        "J Uszkoreit",
        "L Jones",
        "A Gomez",
        "Ł Kaiser",
        "I Polosukhin"
      ],
      "year": "2017",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "44",
      "title": "Context-dependent sentiment analysis in usergenerated videos",
      "authors": [
        "S Poria",
        "E Cambria",
        "D Hazarika",
        "N Majumder",
        "A Zadeh",
        "L.-P Morency"
      ],
      "year": "2017",
      "venue": "Proceedings of the 55th annual meeting of the association for computational linguistics"
    },
    {
      "citation_id": "45",
      "title": "Learning relationships between text, audio, and video via deep canonical correlation for multimodal language analysis",
      "authors": [
        "Z Sun",
        "P Sarma",
        "W Sethares",
        "Y Liang"
      ],
      "year": "2019",
      "venue": "Learning relationships between text, audio, and video via deep canonical correlation for multimodal language analysis",
      "arxiv": "arXiv:1911.05544"
    }
  ]
}