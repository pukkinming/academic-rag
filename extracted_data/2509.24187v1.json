{
  "paper_id": "2509.24187v1",
  "title": "Reasoning Beyond Majority Vote: An Explainable Speechlm Framework For Speech Emotion Recognition",
  "published": "2025-09-29T02:06:27Z",
  "authors": [
    "Bo-Hao Su",
    "Hui-Ying Shih",
    "Jinchuan Tian",
    "Jiatong Shi",
    "Chi-Chun Lee",
    "Carlos Busso",
    "Shinji Watanabe"
  ],
  "keywords": [
    "Speech emotion recognition",
    "speech emotion reasoning",
    "speech large language model"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Speech Emotion Recognition (SER) is typically trained and evaluated on majority-voted labels, which simplifies benchmarking but masks subjectivity and provides little transparency into why predictions are made. This neglects valid minority annotations and limits interpretability. We propose an explainable Speech Language Model (SpeechLM) framework that frames SER as a generative reasoning task. Given an utterance, the model first produces a transcript, then outputs both an emotion label and a concise natural-language rationale grounded in lexical and acoustic cues. Rationales are generated by a reasoning-capable teacher LLM and used as intermediate supervision, combined with majority labels during fine-tuning. Unlike prior work primarily focused on boosting classification accuracy, we aim to enhance explainability while preserving competitive performance. To this end, we complement majority-label metrics with annotator-aware scoring that credits matches with any annotator label. On MSP-Podcast v1.12, our model maintains improvements over zero-shot SpeechLM baselines, and produces rationales that human evaluators find plausible and well grounded. This demonstrates that incorporating rationale supervision offers a practical path toward interpretable SER without sacrificing predictive quality.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Speech Emotion Recognition (SER) has traditionally framed affect inference as a single-label classification problem trained and evaluated on majority-voted annotations. While this convention simplifies modeling and benchmarking, it implicitly assumes a unique \"gold\" label per utterance and disregards the inherent variability in how emotions are perceived. In practice, emotional perception is subjective and context dependent, where individual factors, such as cultural background, lived experience, and conversational role, systematically shape interpretation  [1, 2, 3, 4] . Even within a single dataset, annotators may disagree substantially, reflecting genuine ambiguity in speech signals rather than noise. Consequently, reducing diverse judgments to a single consensus label discards valuable information about emotional nuance and fails to capture the range of human perspectives. As a result, purely label-only supervision offers little insight into why a model selected a category, limiting interpretability, trustworthiness, and downstream utility in sensitive applications.\n\nThese limitations are most acute at evaluation time. Majorityonly metrics penalize predictions that agree with well-supported but non-majority annotations, thereby underestimating performance on inherently ambiguous utterances and masking where models align with valid minority judgments  [5] . To address this, SER research has increasingly embraced soft-label formulations, where the full annotator distribution or uncertainty-aware targets are preserved in training and evaluation  [6, 7, 8] . Such methods not only yield higher accuracy on ambiguous cases but also align more faithfully with human variability. Beyond scores, a model requires understanding how systems map linguistic content and acoustic cues (prosody, pitch, energy, timing) to emotions so we can target weaknesses (e.g., lexical shortcuts vs. prosodic evidence). While recent SpeechLLM studies have begun entering the SER domain  [9, 10, 11, 12] , and some report headline accuracies in technical reports  [13] , the majority still treat these models primarily as classifiers. This paradigm under-utilizes their generative capabilities, leaving untapped the potential to combine reasoning, explanation, and label uncertainty into a more holistic framework for emotion recognition.\n\nIn this work, we introduce an explainable SpeechLM framework that outputs a natural-language rationale alongside the emotion label. A reasoning-capable teacher LLM synthesizes concise explanations grounded in both lexical content and acoustic cues simultaneously. These rationales serve as intermediate supervision signals for a SpeechLM backbone. Given an utterance, the model first emits an ASR transcript to preserve transcription skill, then an emotion decision followed by a brief evidence-based rationale, all within a single coherent response. This structure encourages the model to provide both the emotion prediction and its rationale, improving interpretability while leaving majority-label training unchanged. To our knowledge, this is the first SER study to pair qualitative/quantitative rationale analysis with annotator-aware evaluation on a large in-the-wild corpus.\n\nEmpirically, our approach improves interpretability while maintaining competitive accuracy against traditional SER classifiers. Compared with zero-shot explainable CoT baselines, our model consistently performs better. On the MSP-Podcast v1.12  [14]  test set, the model achieves higher Macro-F1 under conventional majority-label metrics, and the advantages persist when scoring on the union set of all annotators' labels. Qualitative analyses show that the generated rationales frequently highlight salient acoustic-linguistic cues and are broadly accepted by humans. Moreover, external evaluation with LLMs-as-judges confirms that our rationales are consistently preferred over those from strong zero-shot baselines. In summary, our contributions are threefold: (i) an explainable SER framework that jointly produces labels and rationales, (ii) an evaluation protocol combining majority-label and annotatoraware Macro-F1 while keeping training scheme unchanged with rationales augmented, and (iii) empirical evidence showing that rationale-augmented SpeechLMs better align with both human and LLM-based judgments. arXiv:2509.24187v1 [eess.AS] 29 Sep 2025",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Methodology",
      "text": "We propose an explainable SpeechLM that produces a naturallanguage rationale with the final emotion decision at the same time. The framework comprises two components: (i) reasoning generation by a multimodal teacher LLM, and (ii) supervised fine-tuning of a backbone SpeechLM using these rationales as intermediate supervision. We detail the reasoning module below.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Reasoning Generation",
      "text": "Recent advances in reasoning-capable LLMs have made strong open models widely available. To bridge the gap between speech inputs and categorical emotion outputs, we leverage these models to generate human-readable rationales for each input-output pair in speech emotion recognition (SER). The prompt template used for rationale synthesis is provided in the Prompt Design box, where the $CON-TENT and $EMOTION mean the transcript and ground-truth emotion labels of the training utterances respectively.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Prompt For Reasoning Generation",
      "text": "Given the Content: {$CONTENT} Emotion: {$EMOTION} Provide a detailed reasoning that explains how the emotion and its intensity relate to the spoken content and the audio's characteristics, such as tone, pitch, speaking style, clarity, and other acoustic properties. ## Here are some rules you should follow: -DO NOT use the word {$EMOTION} in your reasoning.\n\n-DO NOT use open-ended questions or follow-up invitations in the reasoning. For example, \"What do you think about this?\", \"What do you think?\" or \"If you want to talk more about this or have other thoughts, feel free to share.\" -DO NOT use \"Well, you know\" or \"Well\" in the reasoning.\n\nConcretely, we adopt Qwen2.5-Omni as the reasoning agent due to its strong multimodal understanding and explicit reasoning capabilities. For each utterance, we provide the audio signal (encoded into embeddings by audio encoder) concatenated with prompt token embeddings consisting of its transcript and the reference emotion label to generate rationales for training data. Note that unlike prior work, we do not discretize acoustic cues into fixed levels. This strategy enables diverse natural-language descriptions and reflects real-world settings where the target distribution and class granularity are unknown. All rationales are generated exclusively on the training split, ensuring no label information leaks into validation or test evaluation.\n\nTo preserve explainability, we impose the following constraints: (1) the rationale must not state or paraphrase the gold label; (2) it must integrate both content and acoustic considerations rather than relying on transcript alone; and (3) it must avoid trailing open-ended prompts or meta-commentary (e.g., \"What do you think?\"). We apply simple post-processing to strip disallowed phrases and discard generations that violate length or evidence-coverage requirements. The resulting rationales (r) and corresponding emotion labels (e) serve as intermediate supervision for our backbone model in the subsequent fine-tuning stage.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Supervised Fine-Tuning (Sft)",
      "text": "Given the curated triples {(xi, ei, ri)} N i=1 consisting of speech x, gold emotion e and rationale r from previous stage, we fine-tune the backbone SpeechLM (Qwen2Audio) with Low-Rank Adaptation (LoRA)  [15] . We adapt only the query, key, value and output projection (Q/K/V/O) weights in self-attention blocks and the languagemodel head, keeping the remaining parameters frozen. To mitigate catastrophic forgetting and overfitting, we employ a small learning rate with warmup and cosine decay, and weight decay on LoRA parameters.\n\nIn the target formatting, each training instance is rendered as a single autoregressive target string to preserve ASR ability with reference transcript ai while supervising the final decision and reasoning:\n\nwhere ei is a single emotion token from the closed set. Therefore, let xi be the input audio, and yi = (yi,1, . . . , yi,t) the tokens, where t stands for time steps. The SFT objective is:\n\nHere θ denotes the LoRA-augmented model parameters, and B represents the batch size. Only LoRA adapters are updated. The complete dataset composition and fine-tuning configurations are described in Section 3.1.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Experiment",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Experiment Setup",
      "text": "We construct the training set by pooling CREMA-D  [17] , IEMO-CAP  [18] , MELD (train)  [19] , RAVDESS  [20] , and MSP-Podcast v1.12 (train), yielding approximately 132 hours of SFT data with paired speech-label-rationale triples. All emotion categories present in the training corpora are already covered within the eight categories defined in MSP-Podcast, so no additional label mapping is required.\n\nFor evaluation, we use the official MSP-Podcast v1.12 Test 1 split due to its scale and diversity. All experiments were run with LLaMA-Factory  [21]  in sft mode on Qwen2-Audio-7B-Instruct using LoRA (rank = 8, α = 16). We fine-tune for 2 epoch (decided by early stopping) with learning rate 1×10 -5 , cosine scheduler with 10% warmup, bf16 precision, per-device batch size = 1, and gradient accumulation = 8. We cap the training set at max samples= 15, 000 and use a maximum sequence length of 2, 048 tokens. Data are formatted with the qwen2 audio template so that only the assistant segment (containing transcript ai, emotion ri, and the rationale ri in Eq. (  1 )) contributes to the token-level cross-entropy loss in Eq.  (2) .\n\nFor evaluation, we compute the Macro-F1 score for the overall performance on the eight-category emotion prediction task excluding others in the MSP-Podcast. In this work, to further capture alignment with minority judgments, we include both majority emotion (MV-labels) and annotator-aware Macro-F1 (All-labels) in our final reports. In annotator-aware Macro-F1, counting a prediction as correct if and only if ŷi ∈ Ai, where Ai is the union set of all annotators' labels (i.e., it matches any annotator's label for that item). We also report results on the strict-majority subset-items with one label receiving >50% of votes (distinct from plurality).\n\nGiven outputs comprising an open-form rationale and a closedform decision. Open-form means that we do not limit the output space of the prediction to a subset of emotional classes. In the openform evaluation, we simply adopt the prompt \"What emotion do you think the audio expresses?\" with audio as input. Otherwise, we call it closed-form. The inputs of the closed-form are the audio and prompt: Then, we employ GPT-OSS-20B as a parser to extract the predicted emotion. We subsequently prompt ChatGPT-5 to cluster labels into nine categories-eight target emotions and other, which includes prediction failures and any label outside the target set. We provide all detailed prompts for extraction and grouping on our demo page 1 to enhance the reproducibility of our experiments.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Baseline Comparisons",
      "text": "In this work, we include open-source baseline SpeechLMs as our baseline models, including Qwen2-Audio-Instruct-7B (Q2A)  [22] , Qwen2.5-Omni (Q2.5O)  [23] , SALMONN  [24]  and Baichuan-Omni-1.5  [25] . Besides, we list the most recent result  [16]  from the MSP-Podcast Corpus as traditional classification baseline models utilizing self-supervised representation (WavLM, Wav2vec 2.0, and HuBERT) as referenced SFT point. For ease of reference, Table  1  includes a model-index (id) column that we use to cross-reference systems in the results discussion.\n\nTable  1  summarizes results on the MSP-Podcast test set across open-form and closed-form prediction. Model 4, 5, 7, and 8 represent strong zero-shot baselines. Surprisingly, closed-form Macro-F1 is inconsistent across models and generally lower than open-form, likely because emotions are subtle and benefit from fine-grained wording: open-form lets the LLM leverage broad lexical knowledge, whereas closed-form's fixed label tokens restrict precision and accentuate label-prior and tokenization errors.\n\nWe additionally report the effect of Chain-of-Thought (CoT) prompting. As expected, CoT improves most systems by encouraging more structured reasoning, though the overall accuracy remains modest. For instance, Q2A+CoT achieves 27.8% Macro-F1 (MVlabels, open-form), while Q2.5O+CoT improves to 26.0%, indicating that CoT is a useful but limited tool in the zero-shot setting. Interestingly, Q2A+CoT slightly outperforms Q2.5O+CoT in SER. This result highlights the trade-off between specialization and generalization. Q2A, being an audio-specialized model, is more sensitive to fine-grained prosodic cues such as pitch, energy, and rhythm, which are critical for distinguishing emotional states. Despite the versatility of multimodal generalist models, this finding suggests that task-  specialized audio models retain a marginal advantage in domains where paralinguistic features are paramount.\n\nIn contrast, our proposed SFT-based model (Model 11) substantially outperforms all zero-shot baselines (including CoT-based models), reaching 36.1% Macro-F1 on MV-labels (open-form) and 30.6% in the closed-form setting. Importantly, these gains are not merely a byproduct of fine-tuning: traditional SFT models trained directly for SER (Models 1-3) achieve only 23-30% on closed-form metrics, highlighting that simply applying supervised training does not guarantee strong performance. Instead, the improvement of Model 10 stems from incorporating rationale supervision during fine-tuning, which enhances both predictive accuracy and interpretability.\n\nWe emphasize that the comparison is not entirely symmetric and our model is fine-tuned while baselines are zero-shot. However, by including conventional SFT-trained systems (Models 1-3) in the table, we provide a fairer reference point. Against this stronger baseline, our approach still achieves clear improvements, demonstrating that rationale-guided supervision provides additional benefits beyond standard fine-tuning.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Ablation On Rationales",
      "text": "To further assess the contribution of rationale supervision, we compare our proposed model trained with reasoning (Model 11) against its counterpart trained without reasoning (Model 10). Crucially, as described in Section 2.1, adding rationale supervision (Model 11) yields the strongest results, surpassing both Model 10 and all baseline systems. The gains are consistent across open-form and closed-form evaluation and persist under annotator-aware Macro-F1, confirming that rationales not only enhance interpretability but also serve as effective auxiliary training signals. This ablation again supports that our performance specifically arise from integrating rationale generation into training.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Table 2:",
      "text": "Open-form reasoning demonstrations on sample utterances. Each cell shows label and the model's rationale, where the yellow, green and blue color indicate the rationale corresponding to content, prediction and acoustic information, respectively.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Id",
      "text": "MV-labels Ours 0340 0148 Surprise Surprise -When someone says \"I didn't know that she has pink hair. How did I miss that?\" it sounds like they're a bit surprised. I think the emotion is surprise. The tone might be a bit raised, a bit more excited than usual. And the pitch could be a bit higher too. It's like they're realizing something new and a bit shocked. 3737 0192 Angry Angry -The speaker is saying \"fight with me, like, oh, again, again, again, you forgot our anniversary, if -\". I think the emotion is angry. The tone seems to be a bit frustrated or annoyed. The pitch might be a bit higher, indicating some level of irritation. The speaking style is a bit repetitive, which could show that the speaker is trying to emphasize the importance of the anniversary. The clarity of the speech is good, so it's easy to understand the words. Overall, it seems like the speaker is a bit upset about the anniversary being forgotten. 2991 1041 Neutral Sad -The content \"no possibility of saving anything\" suggests a sense of hopelessness or despair. I think the emotion is sad. The tone of the speaker might be low and slow, indicating a lack of energy or motivation. The pitch could be lower than normal, reflecting a feeling of being overwhelmed or defeated. The speaking style might be hesitant or even shaky, as if the speaker is struggling to find the words to express their feelings. The clarity of the speech could be affected by the speaker's emotional state, possibly making it a bit muddled. All these factors contribute to the overall sad emotion.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Subjective Test",
      "text": "To assess rationale quality, we design two human studies (Test I and Test II), where ten raters participated in each study. All human raters were volunteer lab members, currently pursuing Master's or Ph.D. degrees. Raters listened to the audio, read the model's predicted label and rationale, and provided judgments in the end. Items were randomized and raters were blind to correctness and system identity.\n\nTest I -Accept/Reject: To assess plausibility of the label-rationale pair, we sampled 32 utterances: 16 where the predicted label matches the label of the reference, and 16 where it does not (balanced by emotion). Raters marked the pair as Accept or Reject. We report overall Accept rate.\n\nTest II -Plausibility: To assess the reasoning plausibility, we selected 32 utterances where the model's predicted label disagreed with the reference label. For each utterance, we formed two candidates including Model-predicted and Goldreferenced (rationale generated by our prompting procedure using the reference label as guidance). Raters marked the preference one, and we report the Gold-vs-Predicted preference rate. All results are summarized in Figure  1 . In Test I, the overall average acceptance rate reached 69.1% (95% confidence intervals(CIs): [58.9%, 79.2%]), with 72.5% for the match condition and 65.6% for the non-match condition. These results indicate that the reasoning generated by our model is perceived as plausible by human raters, even when the predicted emotion does not align with the ground truth.\n\nIn Test II, when directly compared against gold-reference samples, our generated reasoning was still preferred in 39.7% of cases (95% CIs: [30.2%, 49.2%]). This finding suggests that providing explicit reasoning alongside emotion predictions can sometimes outweigh strict alignment with majority-vote labels. In other words, human raters may value the plausibility and interpretability of the reasoning process itself, highlighting the importance of considering minority labels as correct under our proposed explainable modeling framework.\n\nTable  2  presents open-form reasoning examples from our model. In the third example of Table  2 , although the majority-vote label is neutral (3 neutral and 2 sad among all annotations), after reviewing the reasoning and predictions, 9 raters judged our model's output as more plausible and better aligned with the audio. The rationales explicitly cite linguistic content and acoustic/prosodic cues, providing evidence for the predicted label and improving interpretability. Additional examples and side-by-side comparisons are available in our demo page.   [26, 27, 28] . In this evaluation process, we conduct win-rate comparison by using gemini-2.0-flash API between the reasoning generated through our proposed model, Q2A+CoT, and Q2.5O+CoT. To fairly compare these samples, we select samples that deliver the same final prediction from comparison models, and then we randomly sample 50 items from each emotion category. To avoid LLMs preferring our model, we do not provide the model information of the input reason by simply mentioning model A and B. The overall results are presented in Fig  2 , where our model's reasoning was preferred in 90.0% (95% CIs: [87.5%, 92.4%]) of pairs vs. Q2A+CoT and 79.0% (95% CIs: [75.7%, 82.2%]) vs. Q2.5O+CoT. These results confirm that rationale supervision not only improves predictive accuracy but also produces explanations that are consistently judged as more convincing and informative by a strong commercial LLM evaluator. Detailed win-rate prompt could be also found from our demo page.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Conclusions",
      "text": "We proposed an explainable SpeechLM that produces transcripts, rationales, and emotion labels in a unified response. Trained with majority labels and teacher-synthesized rationales, the model achieves competitive accuracy while improving interpretability. On MSP-Podcast v1.12, it outperforms zero-shot baselines under both majority-label and annotator-aware Macro-F1. Human studies and LLM-as-judge evaluations further confirm the plausibility and preference of its explanations. These findings show that rationale supervision offers a practical path to transparent SER without sacrificing accuracy. Future work includes zero-shot extensions, conversational corpora, and explicit uncertainty modeling.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Subjective evaluation results (10 raters).",
      "page": 3
    },
    {
      "caption": "Figure 1: In Test I, the overall aver-",
      "page": 4
    },
    {
      "caption": "Figure 2: Win-rate comparison by gemini-2.0-flash",
      "page": 4
    },
    {
      "caption": "Figure 2: , where our model’s reasoning was preferred in 90.0% (95%",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "1\nWavLM [16]\n2\nWav2vec 2.0 [16]\n3\nHuBERT [16]": "4\nSALMONN\n5\nQ2A\n6\nQ2A+CoT\n7\nBaichuan-Omni-1.5\n8\nQ2.5O\n9\nQ2.5O+CoT",
          "-\n-\n-": "20.11%\n40.24%\n44.79%\n40.38%\n42.27%\n52.74%",
          "29.70%\n23.80%\n28.50%": "12.32%\n22.96%\n26.39%\n21.52%\n23.31%\n28.70%"
        }
      ],
      "page": 3
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Emotion and rationality: A critical review and interpretation of empirical evidence",
      "authors": [
        "Tuan Michel",
        "Pham"
      ],
      "year": "2007",
      "venue": "Emotion and rationality: A critical review and interpretation of empirical evidence"
    },
    {
      "citation_id": "3",
      "title": "Cultural differences in emotions: A context for interpreting emotional experiences",
      "authors": [
        "Batja Mesquita",
        "Robert Walker"
      ],
      "year": "2003",
      "venue": "Behaviour research and therapy"
    },
    {
      "citation_id": "4",
      "title": "When the majority is wrong: Modeling annotator disagreement for subjective tasks",
      "authors": [
        "Eve Fleisig",
        "Rediet Abebe",
        "Dan Klein"
      ],
      "year": "2023",
      "venue": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "5",
      "title": "Unveiling the multi-annotation process: Examining the influence of annotation quantity and instance difficulty on model performance",
      "authors": [
        "Pritam Kadasi",
        "Mayank Singh"
      ],
      "year": "2023",
      "venue": "Findings of the Association for Computational Linguistics: EMNLP 2023"
    },
    {
      "citation_id": "6",
      "title": "Analysis of emotion annotation strength improves generalization in speech emotion recognition models",
      "authors": [
        "Joao Palotti",
        "Gagan Narula",
        "Lekan Raheem",
        "Herbert Bay"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "7",
      "title": "Modelling variability in human annotator simulation",
      "authors": [
        "Wen Wu",
        "Wenlin Chen",
        "Chao Zhang",
        "Phil Woodland"
      ],
      "year": "2024",
      "venue": "Findings of the Association for Computational Linguistics ACL 2024"
    },
    {
      "citation_id": "8",
      "title": "Embracing ambiguity and subjectivity using the all-inclusive aggregation rule for evaluating multi-label speech emotion recognition systems",
      "authors": [
        "Huang-Cheng Chou",
        "Haibin Wu",
        "Lucas Goncalves",
        "Seong-Gyun Leem",
        "Ali Salman",
        "Carlos Busso",
        "Hung-Yi Lee",
        "Chi-Chun Lee"
      ],
      "year": "2024",
      "venue": "2024 IEEE Spoken Language Technology Workshop (SLT)"
    },
    {
      "citation_id": "9",
      "title": "Aer-llm: Ambiguity-aware emotion recognition leveraging large language models",
      "authors": [
        "Xin Hong",
        "Yuan Gong",
        "Vidhyasaharan Sethu",
        "Ting Dang"
      ],
      "year": "2025",
      "venue": "ICASSP 2025 -2025 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "10",
      "title": "Aa-sllm: An acoustically augmented speech large language model for speech emotion recognition",
      "authors": [
        "Jialong Mai",
        "Xiaofen Xing",
        "Weidong Chen",
        "Yuanbo Fang",
        "Xiangmin Xu"
      ],
      "year": "2025",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "11",
      "title": "Blsp-emo: Towards empathetic large speech-language models",
      "authors": [
        "Chen Wang",
        "Minpeng Liao",
        "Zhongqiang Huang",
        "Junhong Wu",
        "Chengqing Zong",
        "Jiajun Zhang"
      ],
      "year": "2024",
      "venue": "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "12",
      "title": "Exploring in-context learning of textless speech language model for speech classification tasks",
      "authors": [
        "Kai-Wei Chang",
        "Ming-Hao Hsu",
        "Shan-Wen Li",
        "Hung-Yi Lee"
      ],
      "venue": "Proc. Interspeech, 2024"
    },
    {
      "citation_id": "13",
      "title": "Selm: Enhancing speech emotion recognition for out-of-domain scenarios",
      "authors": [
        "Hazim Bukhari",
        "Soham Deshmukh",
        "Hira Dhamyal",
        "Bhiksha Raj",
        "Rita Singh"
      ],
      "year": "2024",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "14",
      "title": "Audio flamingo 3: Advancing audio intelligence with fully open large audio language models",
      "authors": [
        "Arushi Goel",
        "Sreyan Ghosh",
        "Jaehyeon Kim",
        "Sonal Kumar",
        "Zhifeng Kong",
        "Sang-Gil Lee",
        "Chao-Han Huck Yang",
        "Ramani Duraiswami",
        "Dinesh Manocha",
        "Rafael Valle"
      ],
      "year": "2025",
      "venue": "Audio flamingo 3: Advancing audio intelligence with fully open large audio language models",
      "arxiv": "arXiv:2507.08128"
    },
    {
      "citation_id": "15",
      "title": "Building naturalistic emotionally balanced speech corpus by retrieving emotional speech from existing podcast recordings",
      "authors": [
        "Reza Lotfian",
        "Carlos Busso"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "16",
      "title": "Lora: Low-rank adaptation of large language models",
      "authors": [
        "J Edward",
        "Yelong Hu",
        "Phillip Shen",
        "Zeyuan Wallis",
        "Yuanzhi Allen-Zhu",
        "Shean Li",
        "Lu Wang",
        "Weizhu Wang",
        "Chen"
      ],
      "year": "2022",
      "venue": "ICLR"
    },
    {
      "citation_id": "17",
      "title": "The msp-podcast corpus",
      "authors": [
        "Carlos Busso",
        "Reza Lotfian",
        "Kusha Sridhar",
        "Ali Salman",
        "Wei-Cheng Lin",
        "Lucas Goncalves",
        "Srinivas Parthasarathy",
        "Abinay Reddy Naini",
        "Seong-Gyun Leem",
        "Luz Martinez-Lucas",
        "Huang-Cheng Chou",
        "Pravin Mote"
      ],
      "year": "2025",
      "venue": "The msp-podcast corpus",
      "arxiv": "arXiv:2509.09791"
    },
    {
      "citation_id": "18",
      "title": "Crema-d: Crowd-sourced emotional multimodal actors dataset",
      "authors": [
        "Houwei Cao",
        "David Cooper",
        "Ruben Michael K Keutmann",
        "Ani Gur",
        "Ragini Nenkova",
        "Verma"
      ],
      "year": "2014",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "19",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "Carlos Busso",
        "Murtaza Bulut",
        "Chi-Chun Lee",
        "Abe Kazemzadeh",
        "Emily Mower",
        "Samuel Kim",
        "Jeannette Chang",
        "Sungbok Lee",
        "Shrikanth S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "20",
      "title": "Meld: A multimodal multi-party dataset for emotion recognition in conversations",
      "authors": [
        "Soujanya Poria",
        "Devamanyu Hazarika",
        "Navonil Majumder"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "21",
      "title": "The ryerson audiovisual database of emotional speech and song (ravdess): A dynamic, multimodal set of facial and vocal expressions in north american english",
      "authors": [
        "R Steven",
        "Frank Livingstone",
        "Russo"
      ],
      "year": "2018",
      "venue": "PloS one"
    },
    {
      "citation_id": "22",
      "title": "Llamafactory: Unified efficient fine-tuning of 100+ language models",
      "authors": [
        "Yaowei Zheng",
        "Richong Zhang",
        "Junhao Zhang",
        "Yanhan Ye",
        "Zheyan Luo",
        "Zhangchi Feng",
        "Yongqiang Ma"
      ],
      "year": "2024",
      "venue": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "23",
      "title": "Qwen2-audio technical report",
      "authors": [
        "Yunfei Chu",
        "Jin Xu",
        "Qian Yang",
        "Haojie Wei",
        "Xipin Wei",
        "Zhifang Guo",
        "Yichong Leng",
        "Yuanjun Lv",
        "Jinzheng He",
        "Junyang Lin"
      ],
      "year": "2024",
      "venue": "Qwen2-audio technical report",
      "arxiv": "arXiv:2407.10759"
    },
    {
      "citation_id": "24",
      "title": "Qwen2. 5-omni technical report",
      "authors": [
        "Jin Xu",
        "Zhifang Guo",
        "Jinzheng He",
        "Hangrui Hu",
        "Ting He",
        "Shuai Bai",
        "Keqin Chen",
        "Jialin Wang",
        "Yang Fan",
        "Kai Dang"
      ],
      "year": "2025",
      "venue": "Qwen2. 5-omni technical report",
      "arxiv": "arXiv:2503.20215"
    },
    {
      "citation_id": "25",
      "title": "Salmonn: Towards generic hearing abilities for large language models",
      "authors": [
        "Changli Tang",
        "Wenyi Yu",
        "Guangzhi Sun",
        "Xianzhao Chen",
        "Tian Tan",
        "Wei Li",
        "Lu Lu",
        "Zejun Ma",
        "Chao Zhang"
      ],
      "year": "2023",
      "venue": "Salmonn: Towards generic hearing abilities for large language models",
      "arxiv": "arXiv:2310.13289"
    },
    {
      "citation_id": "26",
      "title": "Baichuan-omni-1.5 technical report",
      "authors": [
        "Yadong Li",
        "Jun Liu",
        "Tao Zhang",
        "Song Chen",
        "Tianpeng Li",
        "Zehuan Li",
        "Lijun Liu",
        "Lingfeng Ming",
        "Guosheng Dong",
        "Da Pan"
      ],
      "year": "2025",
      "venue": "Baichuan-omni-1.5 technical report",
      "arxiv": "arXiv:2501.15368"
    },
    {
      "citation_id": "27",
      "title": "Can llm be a personalized judge?",
      "authors": [
        "Yijiang Dong",
        "Tiancheng Hu",
        "Nigel Collier"
      ],
      "year": "2024",
      "venue": "Findings of the Association for Computational Linguistics: EMNLP 2024"
    },
    {
      "citation_id": "28",
      "title": "Judging llm-as-a-judge with mt-bench and chatbot arena",
      "authors": [
        "Lianmin Zheng",
        "Wei-Lin Chiang",
        "Ying Sheng",
        "Siyuan Zhuang",
        "Zhanghao Wu",
        "Yonghao Zhuang",
        "Zi Lin",
        "Zhuohan Li",
        "Dacheng Li",
        "Eric Xing"
      ],
      "year": "2023",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "29",
      "title": "Audio large language models can be descriptive speech quality evaluators",
      "authors": [
        "Chen Chen",
        "Yuchen Hu",
        "Siyin Wang",
        "Helin Wang",
        "Zhehuai Chen",
        "Chao Zhang",
        "Yang Chao-Han Huck",
        "Eng Siong"
      ],
      "year": "2025",
      "venue": "Audio large language models can be descriptive speech quality evaluators",
      "arxiv": "arXiv:2501.17202"
    }
  ]
}