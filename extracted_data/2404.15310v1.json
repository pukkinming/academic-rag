{
  "paper_id": "2404.15310v1",
  "title": "Automated Assessment Of Encouragement And Warmth In Classrooms Leveraging Multimodal Emotional Features And Chatgpt",
  "published": "2024-04-01T16:58:09Z",
  "authors": [
    "Ruikun Hou",
    "Tim Fütterer",
    "Babette Bühler",
    "Efe Bozkir",
    "Peter Gerjets",
    "Ulrich Trautwein",
    "Enkelejda Kasneci"
  ],
  "keywords": [
    "Classroom observation",
    "AI in Education",
    "Teaching effectiveness",
    "Multimodal machine learning",
    "ChatGPT zero-shot annotation"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Classroom observation protocols standardize the assessment of teaching effectiveness and facilitate comprehension of classroom interactions. Whereas these protocols offer teachers specific feedback on their teaching practices, the manual coding by human raters is resourceintensive and often unreliable. This has sparked interest in developing AI-driven, cost-effective methods for automating such holistic coding. Our work explores a multimodal approach to automatically estimating encouragement and warmth in classrooms, a key component of the Global Teaching Insights (GTI) study's observation protocol. To this end, we employed facial and speech emotion recognition with sentiment analysis to extract interpretable features from video, audio, and transcript data. The prediction task involved both classification and regression methods. Additionally, in light of recent large language models' remarkable text annotation capabilities, we evaluated ChatGPT's zero-shot performance on this scoring task based on transcripts. We demonstrated our approach on the GTI dataset, comprising 367 16-minute video segments from 92 authentic lesson recordings. The inferences of GPT-4 and the best-trained model yielded correlations of r = .341 and r = .441 with human ratings, respectively. Combining estimates from both models through averaging, an ensemble approach achieved a correlation of r = .513, comparable to human inter-rater reliability. Our model explanation analysis indicated that text sentiment features were the primary contributors to the trained model's decisions. Moreover, GPT-4 could deliver logical and concrete reasoning as potential teacher guidelines. Our findings provide insights into using advanced, multimodal techniques for automated classroom observation, aiming to foster teacher training through frequent and valuable feedback.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Introduction",
      "text": "A comprehensive understanding of classroom interactions is crucial to deciphering the quality of teaching, providing hence the opportunity to foster an educational environment where learning thrives  [25] . This understanding paves the way for interventions like real-time feedback, enriching the teaching and learning processes and empowering researchers to dissect teaching scenarios with heightened reliability and efficiency. These insights are particularly pivotal when assessing facets of teaching effectiveness, such as student support, where elements like classroom encouragement and warmth are not mere niceties but essential catalysts for effective teaching  [20] . The ability to delve deeper into these components promises to enhance educational practices and refine teacher professional development programs, steering them toward fostering these nurturing classroom atmospheres. Traditionally, the task of capturing the nuances of teaching dynamics has involved human observers, employing structured classroom observation protocols like CLASS (Classroom Assessment Scoring System  [21] ). For this task, human observers watch lesson recordings and assign scores based on teaching effectiveness measures defined in the protocols. Whereas a human rating approach is valuable, it is fraught with multiple challenges  [8] . Human-based observations are inherently subjective, often leading to higher-inference holisticlevel assessments with low inter-rater agreement. Moreover, the manual nature of these assessments makes them resource-intensive in terms of time and cost.\n\nAgainst this backdrop, there is growing interest in developing AI-driven approaches to automatically coding classroom observation protocols. Prior research addressed the task by either employing multimodal feature extraction together with supervised classifiers  [13, 22]  or relying on advanced large language models (LLMs)  [31, 32] . In line with existing studies, our goal is to make an initial contribution to automatic evaluation approaches that reflect the eye of a highly trained human evaluator but overcome the limitations of human subjectivity and resource constraints. We focus on a specific aspect of teaching effectiveness, namely Encouragement and Warmth (EW), a significant component in the observation protocol of the Global Teaching Insights (GTI) study  [18] . The component involves the provision of encouragement for students throughout their work, including positive comments and compliments, along with moments of shared warmth such as smiling and laughter  [4] . This corresponds to the Positive Climate (PC) dimension in CLASS.\n\nTo assess classroom EW, we investigate the use of both multimodal models tailored to domain-specific data and LLMs' generative capabilities, aiming to harness each method's unique strengths. We first propose a supervised-learning approach based on multimodal representations of emotion and then apply the Shapley additive explanations (SHAP)  [16]  technique to examine the contributions of these explicit features. Additionally, we explore whether recent LLMs like ChatGPT, relying on their zero-shot annotation capabilities, can effectively score EW based on classroom discourse and reasonably interpret their decisions. Lastly, we evaluate the predictive performance of an ensemble approach that combines estimates from supervised models and ChatGPT.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Related Work",
      "text": "Recently, the success of machine learning has triggered a growing trend towards AI applications in classroom settings, such as analysis of student behavior  [2, 5]  and engagement  [11, 27] , classroom discourse  [12, 14] , as well as teacher perception  [28] . Specifically, a few recent studies have targeted the holistic analysis of automated teaching effectiveness coding within classroom observation protocols. They can be categorized into two strands: multimodal supervised methods  [13, 22]  and LLM-based methods  [31, 32] .\n\nThe first multimodal machine-learning system was proposed by James et al.  [13] , which employed visual, conversational, and acoustic features to identify whether the classroom climate is positive following the CLASS protocol. Their trained binary classifier yielded a F 1-score of 0.77. Furthermore, Ramakrishnan et al.  [22]  presented a multimodal architecture to achieve a more fine-grained scoring of both PC and Negative Climate (NC) dimensions in CLASS. In line with the 7-point coding scale of CLASS, the prediction task was formulated as a 7-class classification problem. They utilized an ensemble model integrating visual and auditory pathways, achieving correlations between predictions and human ratings of r = .55 (PC) and r = .63 (NC).\n\nIn addition to multimodal approaches, recent research investigated using LLMs for transcript-based classroom observation scoring. Wang and Demszky  [31]  pioneered the employment of ChatGPT's zero-shot capabilities to score classroom transcripts across various dimensions of teaching effectiveness. They prompted GPT-3.5 with an entire transcript segment and a description of the respective dimension requiring rating. Based on 100 authentic transcript segments, ChatGPT estimates resulted in a weak correlation of r = .04 with human-coded scores regarding CLASS PC. Instead of using complete transcripts as prompts, Whitehill and LoCasale-Crouch  [32]  introduced an LLM-based approach focusing on utterance-level analysis. They leveraged zero-shot prompting with an LLM to distinguish individual teacher utterances and further trained a linear regressor on aggregated session-level features to assess the CLASS Instructional Support domain. Their best-performing model, employing features concatenated from the LLM and a Bag-of-Words method, reached a correlation of r = .46. Although this utterance-level method approached human inter-rater reliability, it lacked in grasping the semantic context in dialogues.\n\nWe propose leveraging both multimodal models and LLMs to predict classroom observation scores. Our work stands for a further contribution to this relatively unexplored domain, extending prior studies in the following aspects:\n\n(1) We focus on extracting interpretative features that explicitly constitute EWrelated behavioral indicators, as opposed to utilizing low-level auditory features  [13, 22] . This enables us to (2) apply explainability frameworks to understand which behaviors contribute to model predictions, which is central in practical applications, such as teacher training. (3) Additionally, we explore regression methods that account for the ordering attribute of data labels compared to standard classification. Moreover, GPT-3.5 resulted in subpar performance for CLASS PC scoring  [31] . As the recent GPT-4 model has demonstrated improved text understanding and generation capabilities as well as reduced hallucination  [1] , (4) we evaluate if GPT-4 surpasses its predecessor in achieving adequate zeroshot performance for this particular scoring task. (5) Furthermore, we investigate the potential of an ensemble method to boost predictive accuracy by leveraging the strengths of supervised models and ChatGPT's zero-shot approaches.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Gti Dataset",
      "text": "The dataset employed in this work stems from GTI  [18] , a large-scale classroom video study aiming to achieve a profound understanding of teaching and learning worldwide. Across eight participating countries, the study centers on a shared pedagogical topic in mathematics (quadratic equations) and emphasizes objective evidence on classroom practice by directly observing authentic lesson videos and instructional materials. A video observation protocol was developed to ensure consistent rating processes within the study. At a high level, the protocol consists of six domains: Classroom Management, Social-Emotional Support, Discourse, Quality of Subject Matter, Student Cognitive Engagement, and Assessment of and Responses to Student Understanding. Each domain comprises multiple components measuring the quality of distinct teaching constructs at higher inference levels. Human raters observed instructional videos divided into 16-minute segments and assigned each segment a score on a 4-point scale for each component, guided by associated behavioral examples. The raters were required to attend dedicated training lessons and engage in several quality control checks, thus yielding heavy workloads to guarantee rating reliability. We focus on the EW component from the Social-Emotional Support domain, which captures behavioral characteristics comparable to the CLASS PC dimension. Particularly, Encouragement refers to using positive verbal and nonverbal cues to inspire students to begin or persist in tasks, such as reassurance for students' mistakes, positive comments, and compliments on their work, while Warmth is represented by, e.g., smiling, laughter, joking, and playfulness  [4] . The score scale from one to four aligns with the occurrence frequency of these behaviors from no evidence to frequent instances throughout one segment.  We used the GTI data collected in Germany, involving 100 video-recorded math lessons, 50 recruited teachers, and over 1,140 students. Due to data protection regulations, we were only allowed to access 92 of the recordings. Each lesson lasted from 40 to 90 minutes. The lessons were videotaped simultaneously by two cameras at 25 FPS (frames per second): One tracked the teacher's movements (Fig.  1a ), while the other was stationary and positioned to capture as many students as possible (Fig.  1b ). We utilized the recordings from the latter camera, where the frontal faces of most participants were visible. In addition, GTI supplied lesson transcripts created by human transcribers, which we employed as the text modality in this work. In transcripts, timestamps and speakers were annotated following every conversation turn, where speakers were anonymized by their IDs, such as \"L\" for the teacher and \"S01\" for a student. The rating process involved 14 raters in Germany, with each lesson being annotated by two randomly assigned raters. Following the GTI protocol, we preprocessed the data by splitting lesson recordings and transcripts into 16-minute segments. If the last segment of one recording spanned less than eight minutes, it was merged with the preceding segment. This resulted in 367 segments, serving as the dataset on which we built and evaluated our automated estimation methods.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Methodology",
      "text": "In this section, we elaborate on our proposed approach to automated estimation of EW scores (Fig.  2",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Multimodal Supervised Models",
      "text": "We aimed to build a machine-learning approach to mimic a human's rating process to the greatest extent possible. Thus, we focused on the GTI rater training materials (e.g., coding rubrics and guidelines), which played an essential role in helping raters comprehend rating specifications. As indicated in the training materials, raters were required to pay attention solely to the dedicated behaviors listed in the EW definition (see Sect. 3). Hence, we extracted interpretable features by employing off-the-shelf techniques to represent the EW-associated behaviors. Notably, these behavioral cues are typically linked to the affective states of teachers and students. Therefore, we implemented facial emotion recognition to identify \"smiling\" in videos, applied speech emotion recognition (SER) to detect \"laughter\" in audio, and carried out text sentiment analysis to distinguish \"positive comments\" in classroom discourse.\n\nFacial Emotion Recognition We adopted a deep neural network architecture, EmoNet  [30] , which performs multi-task predictions of facial emotion in a single pass. Beyond the recognition of discrete emotion categories, EmoNet simultaneously estimates continuous valence (positive or negative) and arousal (excited or calm) values defined in the circumplex model of affect  [23] , enabling a more comprehensive depiction of human emotions. We utilized an EmoNet model pretrained on AffectNet  [17] , a dataset containing a vast amount of facial images in the wild annotated with discrete and continuous emotion labels. To apply the model to our classroom dataset, we first down-sampled video segments from 25 to 2 FPS (i.e., 1920 frames for a 16-minute segment), which considered the minimal variation between consecutive frames and reduced computational resources. We then employed RetinaFace  [9] , known for its competitive performance in crowded environments, to detect faces in each frame. Afterward, each face crop was input into EmoNet, which predicted valence and arousal values ranging from -1 to 1, along with a probability distribution over five emotions (neutral, happy, sad, surprise, fear). For each frame, we aggregated the predictions by averaging the valence and arousal values as well as the estimation scores over five discrete labels across all detected faces, yielding a 7D feature vector.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Speech Emotion Recognition",
      "text": "Recent work  [19]  tackled SER by employing transfer learning based on embeddings derived from pre-trained deep models, showing superior performance over conventional methods that relied on low-level acoustic features. Given that the spoken language in our dataset is German, we utilized XLSR proposed by Facebook AI  [7]  to extract cross-lingual deep embeddings from raw audio signals. XLSR is pre-trained in over 50 languages and built on top of wav2vec 2.0  [3] , a transformer-based model trained on unlabeled data in a self-supervised manner for speech recognition. This approach enhances performance for low-resource languages. In particular, we used a publicly released XLSR model fine-tuned in German, mapping a speech signal to a 1024D latent embedding. We applied the model as a feature extractor to EmoDB  [6] , a database consisting of 535 audio instances, each representing a German utterance categorized into seven discrete emotion labels (anger, boredom, disgust, fear, happiness, sadness, neutral). Based on the resulting features, we trained a two-layer Multi-Layer Perceptron (MLP) classifier on the randomly selected 80% of the EmoDB data and used the remainder for testing. The accuracy on the test set achieved 0.95. In our classroom setup, we divided the 16-minute audio segments into 192 5-second windows without overlap due to the observation that over 95% of the EmoDB instances lasted under 5 seconds, with an average duration of 2.8 seconds. We then applied the XLSR model and the trained MLP classifier to infer emotions expressed in each audio window, yielding a 7D feature vector as a probability distribution over the seven emotion labels.\n\nSentiment Analysis TextBlob 4  is a well-established toolkit supporting multiple natural language processing functionalities, such as tokenization and translation. It conducts lexicon-based sentiment analysis utilizing a predefined polarity dictionary. Prior research employed the toolkit for sentiment analysis of student feedback  [24] . In this work, we applied TextBlob-de 5  , a German language extension, to the transcript segments that were manually annotated regarding turn-taking between speakers. The tool assessed the sentiment of each teacher or student utterance by assigning a polarity score ranging from -1 to 1. In line with  [24] , we categorized utterances according to their polarity scores, labeling those above/equal to/below 0 as positive/neutral/negative, respectively. We generated a 4D feature vector for each transcript segment by counting positive, neutral, and negative utterances and computing a cumulative polarity score across all utterances. Since each lesson recording's final segment typically did not span 16 minutes, we normalized the feature vector by the respective segment duration.\n\nPrediction It is noteworthy that both facial and speech emotion features were temporal, whereas text sentiment features were generated in a segment-wise way. We aggregated the visual and auditory features for each segment by computing the mean along the temporal dimension. We then concatenated the segment-wise features from all three modalities into a single 18D vector. Afterward, we formulated the EW estimation problem as both classification and regression tasks. To compare the two approaches fairly, we trained an identical set of models: Random Forest (RF), Support Vector Machine (SVM), and MLP with two layers. As each segment was double-rated, we calculated the average of the two human ratings as the ground truth. For classification, we rounded the ground truth in the training set to the nearest integer, within the range of one to four. To guarantee generalization, all models were evaluated through stratified, lesson-independent 5-fold cross-validation, such that segments from the same lesson were always grouped into the same fold, and each fold maintained the original score distribution. The evaluation results were averaged across all test folds. Before model fitting, we standardized the features using the mean and the standard deviation computed from the training data. Moreover, we carried out grid-search hyperparameter tuning to identify the best-performing configuration for each model.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Chatgpt Zero-Shot Annotation",
      "text": "Recent advances in LLMs introduce vast opportunities for their application in the education sector to enhance teaching and learning  [15, 26] . LLMs are typically trained on extensive text corpora and thus equipped with broad knowledge, enabling quick adaptation to new tasks without the need for retraining. Such zero-shot capability has shown notable effectiveness across various text annotation problems  [10] . In the GTI study, transcripts play a significant role in rating processes (e.g., raters in many countries employed shorthand and highlighting tools directly on transcripts). Therefore, we explore the potential of ChatGPT to assess the component without requiring training, relying exclusively on classroom transcripts. In particular, we prompted ChatGPT with a transcript segment to rate, along with EW's definition, behavioral examples, and coding rubrics, as depicted in Fig.  3 . Besides, we requested ChatGPT to reason its decision on the score assignment, as suggested by  [31] . For this task, we employed two GPT models via the OpenAI API to compare their performance, namely gpt-3.5-turbo-1106 and gpt-4-1106-vision-preview. To consistently compare with those trained models, we evaluated ChatGPT's zero-shot performance on the same five test folds and averaged the results.\n\nConsider the following math classroom transcript in German delimited by triple backticks. The teacher is labeled by a capital \"L\", while students are anonymized with IDs starting with a capital \"S\". ```{Transcript}``B ased on the transcript, your task is to rate Encouragement and Warmth, one of the components used to evaluate classroom interactions. Specifically, Encouragement is represented by the teacher's and students' using positive verbal cues that may inspire or motivate students to begin or keep trying to accomplish a task, e.g., reassuring students when errors are made, complimenting students' work, and making positive comments. Warmth indicates there are moments of shared warmth between the teacher and students and among students, e.g., smiling, laughter, joking, and playfulness.\n\nPlease rate this component on a scale of 1-4 (low-high), representing no/occasional/some/frequent occurrences of the afore-defined behaviors, respectively. Format your answer as: Rating: <the score on a 1-4 integer scale here> Reasoning: <the reason for your decision here> Fig.  3 . Prompt for ChatGPT.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "Ensemble Model",
      "text": "For text feature extraction, the sentiment analysis was performed at the level of individual utterances. ChatGPT analyzed an entire transcript, thus leveraging contextual information for a comprehensive understanding of classroom discourse  [32] . Considering that each method might provide distinct and potentially complementary insights, we constructed an ensemble model that integrates the trained model with the ChatGPT zero-shot approach. The ensemble model computes the weighted average of estimates from both base models. Weights are allocated according to their evaluation performance on the training set in each fold. The unweighted averaging was also tested for comparison. Notes. For humans, r corresponds to the average across raters. For automatic models, r is the average over 5 folds between estimates and mean human ratings. Standard error estimates are shown in parentheses, with the best model for each approach in bold.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "Results",
      "text": "",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Model Performance",
      "text": "Similar to a recent study  [32] , we treat automatic estimation approaches as individual raters and explore the extent of their consistency with human raters in terms of Pearson correlation coefficient r. For this purpose, we first computed human inter-rater reliability (IRR) in a leave-one-rater-out fashion. In our context, there were 14 human raters conducting double ratings. We computed r for each rater by comparing their ratings with those assigned by other raters. The coefficients obtained were then averaged across all raters. Meanwhile, we report standard error estimates (standard deviation over all raters divided by √ 14). For model evaluation, we calculated the average of r between model predictions and human ratings over five test folds, as well as the corresponding standard error estimates. The results are summarized in Table  1 . Except for GPT-3.5, the correlations for each model in each fold are statistically significant (p < .05).\n\nFor multimodal supervised approaches, the best-trained model is the MLP regressor (r = .441). Overall, it seems advantageous to formulate the problem as a regression task. This is evident from the fact that classifiers achieve the highest Pearson r of .392, trailing behind all regressors in performance. Another interesting result is that the two-layer MLP outperforms the other two conventional models in both classification and regression tasks, underscoring the efficacy of contemporary neural networks. It is noteworthy that adding more layers did not enhance the performance, potentially attributed to the risk of overfitting inherent in deeper models when confronted with a limited dataset size. When turning to ChatGPT zero-shot results, a clear difference between the two model gen- erations is identified. The estimates of GPT-3.5 show no significant correlation with human-rated scores. In contrast, GPT-4 has remarkably enhanced the capability to understand text, particularly achieving superior zero-shot performance over its predecessor to score EW in classroom discourse. It attains a Pearson r of .341 without requiring any training. Finally, the ensemble model, combining the best-performing MLP regressor with GPT-4, boosts the performance significantly. Using a simple averaging method raises the Pearson r to .499. When the averaging process considers the respective reliability of both base models, the correlation reaches a peak of .513, identical to that of the human IRR.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Model Explanation",
      "text": "Supervised Models To explore which explicit features from which modality are influential in model decisions, we further applied Shapley additive explanation (SHAP)  [16]  to our MLP regressor. A SHAP value quantifies the impact of a feature on an individual prediction. In the cross-validation setting, we gathered SHAP values from each test fold and then created a summary plot involving the entire dataset. As illustrated in Fig.  4 , the group of verbal and auditory features appears to be more informative than those derived from videos in enabling the regressor to predict EW scores. Specifically, the feature describing the number of positive utterances within a transcript segment contributes the most, followed by the overall polarity and three speech emotion features. By analyzing the relationship between feature values and SHAP values illustrated in the plot, it is apparent that high values (shown in red) in the two most important features are associated with positive SHAP values, suggesting that more positive sentiment cues contribute to increasing the predicted EW score. Additionally, the plot re-veals the negative impact of detected anger and disgust as well as the positive impact of detected happiness in the audio on the prediction of higher EW scores.\n\nChatGPT GPT-4's efficacy is evident not only in its agreement with human raters but also in its capability to provide logical reasoning. For example, GPT-4 assigned a score of 4 to a transcript consistently with the human ratings and explained its decision by identifying relevant evidence from the discourse: \"[...] For instance, the teacher praises S15's work as 'sieht schön aus perfekt'('looks beautiful perfect') and encourages S04 by validating their thinking process. The teacher's tone is patient and nurturing, especially visible in exchanges like 'keine Panik'('no panic') [...] The teacher often uses humor, as seen in statements like 'bevor hier einer weint'('before someone cries here') [...]\"\n\nThis aligns with the human rating procedure that measures the frequency of dedicated behaviors. Conversely, we observed that GPT-3.5 typically presented broad-level reasoning lacking concrete examples, which limited its explainability.",
      "page_start": 10,
      "page_end": 11
    },
    {
      "section_name": "Discussion",
      "text": "Human raters achieve moderate agreement (r = .513) for EW coding in our dataset. Prior work  [22]  reported Pearson r of .38 and .42 for CLASS PC in two datasets they utilized. This indicates that coding teaching effectiveness requires holistic analysis and higher-level inference. Compared to human subjectivity, automatic tools benefit from providing more objective insights at scale  [8, 14] . To this end, we explore a novel approach to automated EW assessment. Our methods achieve correlations of .441 (the best-trained model) and .341 (GPT-4 zero-shot annotation) between estimates and human ratings. Combining both methods yields the highest predictive accuracy (r = .513) on par with human IRR. The findings from supervised models show that a set of multimodal, explicit, and low-dimensional features can effectively capture EW-relevant signals in a 16-minute lesson segment. Unlike classifiers that treat categories independently, regressors account for the ordering attribute inherent in the data labels. For example, mistaking a true score of one as four incurs a higher penalty than confusing it with two. This may explain the observed superior performance of regressors. Another reason is that the ground-truth labels were rounded to integers used as classification categories, leading to a loss of information. Additionally, the SHAP analysis highlights the importance of text features, which can be interpreted by the EW coding rubrics where verbal cues constitute a large proportion of the associated behavioral indicators. Meanwhile, auditory features contribute more than visual features, which aligns with prior research findings  [22] . Besides the competitive accuracy achieved by supervised models, GPT-4's ability to deliver persuasive reasoning showcases its potential as an easily accessible tool for teachers to obtain valuable feedback on classroom climate. Further, the efficacy of the ensemble approach suggests that the two base models may capture complementary information. Integrating GPT-4 zero-shot annotations with a specialized and shallow model does not require resource-intensive fine-tuning of LLMs yet enhances the final performance, providing insights into strategies for using LLMs both effectively and efficiently when addressing similar tasks.\n\nDue to the use of distinct datasets and protocols, we note that the results are not comparable to prior studies  [13, 22, 31, 32]  which focused on CLASS dimensions. GTI EW exhibits similar behavioral indicators to CLASS PC, but their coding rubrics differ in aspects such as score scale (4-vs. 7-point). Our exploration contributes to a broader understanding of similar constructs across different protocols. Further, we explore methods for German speech and text processing, diverging from the common use of English data in existing research, which enriches the discussion on developing educational technologies in multilingual contexts. Moreover, transcripts not only provide more informative features for predictive models but also serve as a privacy-conscious modality compared to video and audio recordings. As classroom recordings are sensitive data involving minors, ethical considerations are of utmost importance. We emphasize that automated observation tools intend to streamline post hoc analysis and reduce the need for manual coding instead of being used for real-time classroom monitoring.\n\nOur work is subject to several limitations. First, our analysis utilized manual transcripts. We could implement advanced speech recognition techniques, toward a fully automatic system. In classroom environments, students' faces far away from the camera lead to limited spatial resolution, reducing the performance of emotion recognition models. Solutions could be applying super-resolution methods to improve face image quality. Due to the potential disagreement between distinct model explanation methods  [29] , it is valuable to validate various explainers on our EW scoring task. Another approach to feature importance analysis would be an ablation study to compare the performance using unimodality. Furthermore, one future direction is to adapt the proposed methods for the automated coding of additional GTI aspects such as classroom discourse to achieve a more comprehensive measurement of teaching effectiveness. As GPT-4 is a multimodal LLM capable of image understanding, another future study involves exploring whether the inclusion of classroom frames together with transcripts could enhance estimation accuracy. To further improve model generalizability, conducting cross-dataset evaluation, particularly employing data from diverse countries or cultures, would be part of future research.",
      "page_start": 11,
      "page_end": 12
    },
    {
      "section_name": "Conclusions",
      "text": "We explore a machine-learning approach that harnesses multimodal supervised models and LLMs' zero-shot capabilities to automatically assess Encouragement and Warmth in classrooms. The results indicate that our approach achieves rating performance on par with human inter-rater reliability. We further show that verbal features contribute the most to model predictions, and GPT-4 provides specific evidence for its scoring decisions, outperforming GPT-3.5. Such AI-driven methods have the potential to replicate and augment human observational capabilities in the future, enabling frequent and valuable feedback for educational researchers regarding several teaching effectiveness facets.",
      "page_start": 12,
      "page_end": 12
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Classroom frame from two cameras, with people erased for privacy.",
      "page": 4
    },
    {
      "caption": "Figure 1: a), while the other was stationary and positioned to capture as many",
      "page": 5
    },
    {
      "caption": "Figure 1: b). We utilized the recordings from the latter cam-",
      "page": 5
    },
    {
      "caption": "Figure 2: ), including supervised learning methods based on multi-",
      "page": 5
    },
    {
      "caption": "Figure 2: Pipeline for multimodal estimation of EW scores.",
      "page": 5
    },
    {
      "caption": "Figure 3: Besides, we requested ChatGPT to reason its de-",
      "page": 8
    },
    {
      "caption": "Figure 3: Prompt for ChatGPT.",
      "page": 8
    },
    {
      "caption": "Figure 4: SHAP summary plot for MLP regressor. Points depict SHAP values per",
      "page": 10
    },
    {
      "caption": "Figure 4: , the group of verbal and auditory features",
      "page": 10
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Automated Assessment of Encouragement and": "Warmth in Classrooms Leveraging Multimodal"
        },
        {
          "Automated Assessment of Encouragement and": "Emotional Features and ChatGPT"
        },
        {
          "Automated Assessment of Encouragement and": "Ruikun Hou1,3, Tim Fütterer1, Babette Bühler1, Efe Bozkir1,3, Peter Gerjets2,"
        },
        {
          "Automated Assessment of Encouragement and": "Ulrich Trautwein1, and Enkelejda Kasneci3"
        },
        {
          "Automated Assessment of Encouragement and": "1 University of Tübingen, Geschwister-Scholl-Platz, 72074 Tübingen, Germany"
        },
        {
          "Automated Assessment of Encouragement and": "{ruikun.hou,tim.fuetterer,babette.buehler,efe.bozkir,"
        },
        {
          "Automated Assessment of Encouragement and": "ulrich.trautwein}@uni-tuebingen.de"
        },
        {
          "Automated Assessment of Encouragement and": "2 Leibniz-Instiut für Wissensmedien, Schleichstrasse 6, 72076 Tübingen, Germany"
        },
        {
          "Automated Assessment of Encouragement and": "p.gerjets@iwm-tuebingen.de"
        },
        {
          "Automated Assessment of Encouragement and": "3 Technical University of Munich, Arcisstrsse 21, 80333 Munich, Germany"
        },
        {
          "Automated Assessment of Encouragement and": "{ruikun.hou,efe.bozkir,enkelejda.kasneci}@tum.de"
        },
        {
          "Automated Assessment of Encouragement and": "Abstract. Classroom observation protocols standardize the assessment"
        },
        {
          "Automated Assessment of Encouragement and": "of\nteaching effectiveness and facilitate comprehension of classroom in-"
        },
        {
          "Automated Assessment of Encouragement and": "teractions. Whereas\nthese protocols offer\nteachers\nspecific feedback on"
        },
        {
          "Automated Assessment of Encouragement and": "their teaching practices, the manual coding by human raters is resource-"
        },
        {
          "Automated Assessment of Encouragement and": "intensive and often unreliable. This has\nsparked interest\nin developing"
        },
        {
          "Automated Assessment of Encouragement and": "AI-driven,\ncost-effective methods\nfor automating such holistic\ncoding."
        },
        {
          "Automated Assessment of Encouragement and": "Our work explores a multimodal approach to automatically estimating"
        },
        {
          "Automated Assessment of Encouragement and": "encouragement and warmth in classrooms, a key component of the Global"
        },
        {
          "Automated Assessment of Encouragement and": "Teaching Insights\n(GTI)\nstudy’s observation protocol. To this end, we"
        },
        {
          "Automated Assessment of Encouragement and": "employed facial and speech emotion recognition with sentiment analysis"
        },
        {
          "Automated Assessment of Encouragement and": "to extract interpretable features from video, audio, and transcript data."
        },
        {
          "Automated Assessment of Encouragement and": "The prediction task involved both classification and regression methods."
        },
        {
          "Automated Assessment of Encouragement and": "Additionally, in light of recent large language models’ remarkable text an-"
        },
        {
          "Automated Assessment of Encouragement and": "notation capabilities, we evaluated ChatGPT’s zero-shot performance on"
        },
        {
          "Automated Assessment of Encouragement and": "this scoring task based on transcripts. We demonstrated our approach on"
        },
        {
          "Automated Assessment of Encouragement and": "the GTI dataset, comprising 367 16-minute video segments from 92 au-"
        },
        {
          "Automated Assessment of Encouragement and": "thentic lesson recordings. The inferences of GPT-4 and the best-trained"
        },
        {
          "Automated Assessment of Encouragement and": "model yielded correlations of r = .341 and r = .441 with human ratings,"
        },
        {
          "Automated Assessment of Encouragement and": "respectively. Combining estimates from both models through averaging,"
        },
        {
          "Automated Assessment of Encouragement and": "an ensemble approach achieved a correlation of r = .513, comparable"
        },
        {
          "Automated Assessment of Encouragement and": "to human inter-rater\nreliability. Our model\nexplanation analysis\nindi-"
        },
        {
          "Automated Assessment of Encouragement and": "cated that text sentiment features were the primary contributors to the"
        },
        {
          "Automated Assessment of Encouragement and": "trained model’s decisions. Moreover, GPT-4 could deliver\nlogical and"
        },
        {
          "Automated Assessment of Encouragement and": "concrete reasoning as potential teacher guidelines. Our findings provide"
        },
        {
          "Automated Assessment of Encouragement and": "insights into using advanced, multimodal techniques for automated class-"
        },
        {
          "Automated Assessment of Encouragement and": "room observation, aiming to foster teacher training through frequent and"
        },
        {
          "Automated Assessment of Encouragement and": "valuable feedback."
        },
        {
          "Automated Assessment of Encouragement and": "in Education · Teaching effec-\nKeywords: Classroom observation · AI"
        },
        {
          "Automated Assessment of Encouragement and": "tiveness · Multimodal machine learning · ChatGPT zero-shot annotation."
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "2\nR. Hou et al.": "1\nIntroduction"
        },
        {
          "2\nR. Hou et al.": "A comprehensive understanding of classroom interactions is crucial to decipher-"
        },
        {
          "2\nR. Hou et al.": "ing the quality of teaching, providing hence the opportunity to foster an educa-"
        },
        {
          "2\nR. Hou et al.": "tional environment where learning thrives [25]. This understanding paves the way"
        },
        {
          "2\nR. Hou et al.": "for interventions like real-time feedback, enriching the teaching and learning pro-"
        },
        {
          "2\nR. Hou et al.": "cesses and empowering researchers to dissect teaching scenarios with heightened"
        },
        {
          "2\nR. Hou et al.": "reliability and efficiency. These insights are particularly pivotal when assessing"
        },
        {
          "2\nR. Hou et al.": "facets of\nteaching effectiveness,\nsuch as\nstudent\nsupport, where\nelements\nlike"
        },
        {
          "2\nR. Hou et al.": "classroom encouragement and warmth are not mere niceties but essential cat-"
        },
        {
          "2\nR. Hou et al.": "alysts for effective teaching [20]. The ability to delve deeper into these compo-"
        },
        {
          "2\nR. Hou et al.": "nents promises to enhance educational practices and refine teacher professional"
        },
        {
          "2\nR. Hou et al.": "development programs,\nsteering them toward fostering these nurturing class-"
        },
        {
          "2\nR. Hou et al.": "room atmospheres. Traditionally, the task of capturing the nuances of teaching"
        },
        {
          "2\nR. Hou et al.": "dynamics has involved human observers, employing structured classroom obser-"
        },
        {
          "2\nR. Hou et al.": "vation protocols like CLASS (Classroom Assessment Scoring System [21]). For"
        },
        {
          "2\nR. Hou et al.": "this task, human observers watch lesson recordings and assign scores based on"
        },
        {
          "2\nR. Hou et al.": "teaching effectiveness measures defined in the protocols. Whereas a human rat-"
        },
        {
          "2\nR. Hou et al.": "ing approach is valuable,\nit is fraught with multiple challenges [8]. Human-based"
        },
        {
          "2\nR. Hou et al.": "observations are inherently subjective, often leading to higher-inference holistic-"
        },
        {
          "2\nR. Hou et al.": "level assessments with low inter-rater agreement. Moreover, the manual nature"
        },
        {
          "2\nR. Hou et al.": "of these assessments makes them resource-intensive in terms of time and cost."
        },
        {
          "2\nR. Hou et al.": "Against this backdrop, there is growing interest in developing AI-driven ap-"
        },
        {
          "2\nR. Hou et al.": "proaches to automatically coding classroom observation protocols. Prior research"
        },
        {
          "2\nR. Hou et al.": "addressed the task by either employing multimodal\nfeature extraction together"
        },
        {
          "2\nR. Hou et al.": "with supervised classifiers\n[13,22] or\nrelying on advanced large language mod-"
        },
        {
          "2\nR. Hou et al.": "els (LLMs) [31,32].\nIn line with existing studies, our goal\nis to make an initial"
        },
        {
          "2\nR. Hou et al.": "contribution to automatic evaluation approaches that reflect the eye of a highly"
        },
        {
          "2\nR. Hou et al.": "trained human evaluator but overcome\nthe\nlimitations of human subjectivity"
        },
        {
          "2\nR. Hou et al.": "and resource constraints. We focus on a specific aspect of teaching effectiveness,"
        },
        {
          "2\nR. Hou et al.": "namely Encouragement and Warmth (EW), a significant component in the ob-"
        },
        {
          "2\nR. Hou et al.": "servation protocol of the Global Teaching Insights (GTI) study [18]. The com-"
        },
        {
          "2\nR. Hou et al.": "ponent\ninvolves\nthe provision of encouragement\nfor\nstudents\nthroughout\ntheir"
        },
        {
          "2\nR. Hou et al.": "work,\nincluding positive\ncomments and compliments, along with moments of"
        },
        {
          "2\nR. Hou et al.": "shared warmth such as smiling and laughter [4]. This corresponds to the Posi-"
        },
        {
          "2\nR. Hou et al.": "tive Climate (PC) dimension in CLASS."
        },
        {
          "2\nR. Hou et al.": "To assess classroom EW, we investigate the use of both multimodal models"
        },
        {
          "2\nR. Hou et al.": "tailored to domain-specific data and LLMs’ generative capabilities, aiming to"
        },
        {
          "2\nR. Hou et al.": "harness each method’s unique strengths. We first propose a supervised-learning"
        },
        {
          "2\nR. Hou et al.": "approach based on multimodal\nrepresentations of emotion and then apply the"
        },
        {
          "2\nR. Hou et al.": "Shapley additive explanations (SHAP) [16] technique to examine the contribu-"
        },
        {
          "2\nR. Hou et al.": "tions of\nthese explicit\nfeatures. Additionally, we explore whether\nrecent LLMs"
        },
        {
          "2\nR. Hou et al.": "like ChatGPT, relying on their zero-shot annotation capabilities, can effectively"
        },
        {
          "2\nR. Hou et al.": "score EW based on classroom discourse and reasonably interpret their decisions."
        },
        {
          "2\nR. Hou et al.": "Lastly, we evaluate the predictive performance of an ensemble approach that"
        },
        {
          "2\nR. Hou et al.": "combines estimates from supervised models and ChatGPT."
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Automated Assessment of Encouragement and Warmth in Classrooms\n3": "2\nRelated Work"
        },
        {
          "Automated Assessment of Encouragement and Warmth in Classrooms\n3": "Recently, the success of machine learning has triggered a growing trend towards"
        },
        {
          "Automated Assessment of Encouragement and Warmth in Classrooms\n3": "AI applications in classroom settings, such as analysis of student behavior [2,5]"
        },
        {
          "Automated Assessment of Encouragement and Warmth in Classrooms\n3": "and engagement [11,27], classroom discourse [12,14], as well as teacher percep-"
        },
        {
          "Automated Assessment of Encouragement and Warmth in Classrooms\n3": "tion [28]. Specifically, a few recent\nstudies have\ntargeted the holistic analysis"
        },
        {
          "Automated Assessment of Encouragement and Warmth in Classrooms\n3": "of automated teaching effectiveness coding within classroom observation proto-"
        },
        {
          "Automated Assessment of Encouragement and Warmth in Classrooms\n3": "cols. They can be categorized into two strands: multimodal supervised methods"
        },
        {
          "Automated Assessment of Encouragement and Warmth in Classrooms\n3": "[13,22] and LLM-based methods [31,32]."
        },
        {
          "Automated Assessment of Encouragement and Warmth in Classrooms\n3": "The first multimodal machine-learning system was proposed by James et al."
        },
        {
          "Automated Assessment of Encouragement and Warmth in Classrooms\n3": "[13], which employed visual,\nconversational, and acoustic\nfeatures\nto identify"
        },
        {
          "Automated Assessment of Encouragement and Warmth in Classrooms\n3": "whether the classroom climate is positive following the CLASS protocol. Their"
        },
        {
          "Automated Assessment of Encouragement and Warmth in Classrooms\n3": "trained binary classifier yielded a F 1-score of 0.77. Furthermore, Ramakrishnan"
        },
        {
          "Automated Assessment of Encouragement and Warmth in Classrooms\n3": "et al.\n[22] presented a multimodal architecture to achieve a more fine-grained"
        },
        {
          "Automated Assessment of Encouragement and Warmth in Classrooms\n3": "scoring of both PC and Negative Climate (NC) dimensions in CLASS.\nIn line"
        },
        {
          "Automated Assessment of Encouragement and Warmth in Classrooms\n3": "with the 7-point coding scale of CLASS, the prediction task was formulated as"
        },
        {
          "Automated Assessment of Encouragement and Warmth in Classrooms\n3": "a 7-class\nclassification problem. They utilized an ensemble model\nintegrating"
        },
        {
          "Automated Assessment of Encouragement and Warmth in Classrooms\n3": "visual and auditory pathways, achieving correlations between predictions and"
        },
        {
          "Automated Assessment of Encouragement and Warmth in Classrooms\n3": "human ratings of r = .55 (PC) and r = .63 (NC)."
        },
        {
          "Automated Assessment of Encouragement and Warmth in Classrooms\n3": "In addition to multimodal\napproaches,\nrecent\nresearch investigated using"
        },
        {
          "Automated Assessment of Encouragement and Warmth in Classrooms\n3": "LLMs\nfor\ntranscript-based classroom observation scoring. Wang and Demszky"
        },
        {
          "Automated Assessment of Encouragement and Warmth in Classrooms\n3": "[31] pioneered the\nemployment\nof ChatGPT’s\nzero-shot\ncapabilities\nto\nscore"
        },
        {
          "Automated Assessment of Encouragement and Warmth in Classrooms\n3": "classroom transcripts across various dimensions of teaching effectiveness. They"
        },
        {
          "Automated Assessment of Encouragement and Warmth in Classrooms\n3": "prompted GPT-3.5 with an entire transcript segment and a description of the re-"
        },
        {
          "Automated Assessment of Encouragement and Warmth in Classrooms\n3": "spective dimension requiring rating. Based on 100 authentic transcript segments,"
        },
        {
          "Automated Assessment of Encouragement and Warmth in Classrooms\n3": "ChatGPT estimates resulted in a weak correlation of r = .04 with human-coded"
        },
        {
          "Automated Assessment of Encouragement and Warmth in Classrooms\n3": "scores regarding CLASS PC. Instead of using complete transcripts as prompts,"
        },
        {
          "Automated Assessment of Encouragement and Warmth in Classrooms\n3": "Whitehill and LoCasale-Crouch [32]\nintroduced an LLM-based approach focus-"
        },
        {
          "Automated Assessment of Encouragement and Warmth in Classrooms\n3": "ing on utterance-level analysis. They leveraged zero-shot prompting with an"
        },
        {
          "Automated Assessment of Encouragement and Warmth in Classrooms\n3": "LLM to distinguish individual\nteacher utterances and further\ntrained a linear"
        },
        {
          "Automated Assessment of Encouragement and Warmth in Classrooms\n3": "regressor on aggregated session-level\nfeatures to assess the CLASS Instructional"
        },
        {
          "Automated Assessment of Encouragement and Warmth in Classrooms\n3": "Support domain. Their best-performing model, employing features concatenated"
        },
        {
          "Automated Assessment of Encouragement and Warmth in Classrooms\n3": "from the LLM and a Bag-of-Words method,\nreached a correlation of r = .46."
        },
        {
          "Automated Assessment of Encouragement and Warmth in Classrooms\n3": "Although this utterance-level method approached human inter-rater reliability,"
        },
        {
          "Automated Assessment of Encouragement and Warmth in Classrooms\n3": "it lacked in grasping the semantic context in dialogues."
        },
        {
          "Automated Assessment of Encouragement and Warmth in Classrooms\n3": "We propose leveraging both multimodal models and LLMs to predict class-"
        },
        {
          "Automated Assessment of Encouragement and Warmth in Classrooms\n3": "room observation scores. Our work stands\nfor a further\ncontribution to this"
        },
        {
          "Automated Assessment of Encouragement and Warmth in Classrooms\n3": "relatively unexplored domain, extending prior studies in the following aspects:"
        },
        {
          "Automated Assessment of Encouragement and Warmth in Classrooms\n3": "(1) We focus on extracting interpretative features that explicitly constitute EW-"
        },
        {
          "Automated Assessment of Encouragement and Warmth in Classrooms\n3": "related behavioral\nindicators, as opposed to utilizing low-level auditory features"
        },
        {
          "Automated Assessment of Encouragement and Warmth in Classrooms\n3": "[13,22]. This enables us\nto (2) apply explainability frameworks\nto understand"
        },
        {
          "Automated Assessment of Encouragement and Warmth in Classrooms\n3": "which behaviors contribute to model predictions, which is central\nin practical"
        },
        {
          "Automated Assessment of Encouragement and Warmth in Classrooms\n3": "applications,\nsuch as\nteacher\ntraining.\n(3) Additionally, we\nexplore\nregression"
        },
        {
          "Automated Assessment of Encouragement and Warmth in Classrooms\n3": "methods\nthat account\nfor\nthe ordering attribute of data labels\ncompared to"
        },
        {
          "Automated Assessment of Encouragement and Warmth in Classrooms\n3": "standard classification. Moreover, GPT-3.5 resulted in subpar performance for"
        },
        {
          "Automated Assessment of Encouragement and Warmth in Classrooms\n3": "CLASS PC scoring [31]. As the recent GPT-4 model has demonstrated improved"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "4\nR. Hou et al.": "text understanding and generation capabilities as well as reduced hallucination"
        },
        {
          "4\nR. Hou et al.": "[1], (4) we evaluate if GPT-4 surpasses its predecessor in achieving adequate zero-"
        },
        {
          "4\nR. Hou et al.": "shot performance for this particular scoring task. (5) Furthermore, we investigate"
        },
        {
          "4\nR. Hou et al.": "the potential of an ensemble method to boost predictive accuracy by leveraging"
        },
        {
          "4\nR. Hou et al.": "the strengths of supervised models and ChatGPT’s zero-shot approaches."
        },
        {
          "4\nR. Hou et al.": "3\nGTI Dataset"
        },
        {
          "4\nR. Hou et al.": "The dataset\nemployed in this work stems\nfrom GTI\n[18], a large-scale\nclass-"
        },
        {
          "4\nR. Hou et al.": "room video study aiming to achieve a profound understanding of teaching and"
        },
        {
          "4\nR. Hou et al.": "learning worldwide. Across eight participating countries,\nthe study centers on"
        },
        {
          "4\nR. Hou et al.": "a shared pedagogical\ntopic in mathematics\n(quadratic equations) and empha-"
        },
        {
          "4\nR. Hou et al.": "sizes objective evidence on classroom practice by directly observing authentic"
        },
        {
          "4\nR. Hou et al.": "lesson videos and instructional materials. A video observation protocol was de-"
        },
        {
          "4\nR. Hou et al.": "veloped to ensure consistent rating processes within the study. At a high level,"
        },
        {
          "4\nR. Hou et al.": "the protocol consists of six domains: Classroom Management, Social-Emotional"
        },
        {
          "4\nR. Hou et al.": "Support, Discourse, Quality of Subject Matter, Student Cognitive Engagement,"
        },
        {
          "4\nR. Hou et al.": "and Assessment of and Responses to Student Understanding. Each domain com-"
        },
        {
          "4\nR. Hou et al.": "prises multiple components measuring the quality of distinct teaching constructs"
        },
        {
          "4\nR. Hou et al.": "at higher\ninference levels. Human raters observed instructional videos divided"
        },
        {
          "4\nR. Hou et al.": "into 16-minute segments and assigned each segment a score on a 4-point scale"
        },
        {
          "4\nR. Hou et al.": "for each component, guided by associated behavioral examples. The raters were"
        },
        {
          "4\nR. Hou et al.": "required to attend dedicated training lessons and engage in several quality con-"
        },
        {
          "4\nR. Hou et al.": "trol\nchecks,\nthus yielding heavy workloads\nto guarantee\nrating reliability. We"
        },
        {
          "4\nR. Hou et al.": "focus on the EW component from the Social-Emotional Support domain, which"
        },
        {
          "4\nR. Hou et al.": "captures behavioral\ncharacteristics\ncomparable\nto the CLASS PC dimension."
        },
        {
          "4\nR. Hou et al.": "Particularly, Encouragement refers to using positive verbal and nonverbal cues"
        },
        {
          "4\nR. Hou et al.": "to inspire students to begin or persist in tasks, such as reassurance for students’"
        },
        {
          "4\nR. Hou et al.": "mistakes, positive comments, and compliments on their work, while Warmth is"
        },
        {
          "4\nR. Hou et al.": "represented by, e.g., smiling, laughter, joking, and playfulness [4]. The score scale"
        },
        {
          "4\nR. Hou et al.": "from one to four aligns with the occurrence frequency of these behaviors from"
        },
        {
          "4\nR. Hou et al.": "no evidence to frequent instances throughout one segment."
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Automated Assessment of Encouragement and Warmth in Classrooms\n5": "lasted from 40 to 90 minutes. The lessons were videotaped simultaneously by two"
        },
        {
          "Automated Assessment of Encouragement and Warmth in Classrooms\n5": "cameras at 25 FPS (frames per second): One tracked the teacher’s movements"
        },
        {
          "Automated Assessment of Encouragement and Warmth in Classrooms\n5": "(Fig. 1a), while\nthe other was\nstationary and positioned to capture as many"
        },
        {
          "Automated Assessment of Encouragement and Warmth in Classrooms\n5": "students as possible (Fig. 1b). We utilized the recordings from the latter cam-"
        },
        {
          "Automated Assessment of Encouragement and Warmth in Classrooms\n5": "era, where the frontal\nfaces of most participants were visible.\nIn addition, GTI"
        },
        {
          "Automated Assessment of Encouragement and Warmth in Classrooms\n5": "supplied lesson transcripts created by human transcribers, which we employed"
        },
        {
          "Automated Assessment of Encouragement and Warmth in Classrooms\n5": "as the text modality in this work. In transcripts, timestamps and speakers were"
        },
        {
          "Automated Assessment of Encouragement and Warmth in Classrooms\n5": "annotated following every conversation turn, where speakers were anonymized"
        },
        {
          "Automated Assessment of Encouragement and Warmth in Classrooms\n5": "by their\nIDs, such as \"L\" for\nthe teacher and \"S01\" for a student. The rating"
        },
        {
          "Automated Assessment of Encouragement and Warmth in Classrooms\n5": "process involved 14 raters in Germany, with each lesson being annotated by two"
        },
        {
          "Automated Assessment of Encouragement and Warmth in Classrooms\n5": "randomly assigned raters. Following the GTI protocol, we preprocessed the data"
        },
        {
          "Automated Assessment of Encouragement and Warmth in Classrooms\n5": "by splitting lesson recordings and transcripts into 16-minute segments. If the last"
        },
        {
          "Automated Assessment of Encouragement and Warmth in Classrooms\n5": "segment of one recording spanned less than eight minutes,\nit was merged with"
        },
        {
          "Automated Assessment of Encouragement and Warmth in Classrooms\n5": "the preceding segment. This resulted in 367 segments, serving as the dataset on"
        },
        {
          "Automated Assessment of Encouragement and Warmth in Classrooms\n5": "which we built and evaluated our automated estimation methods."
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "6\nR. Hou et al.": "materials\n(e.g.,\ncoding rubrics and guidelines), which played an essential\nrole"
        },
        {
          "6\nR. Hou et al.": "in helping raters comprehend rating specifications. As indicated in the training"
        },
        {
          "6\nR. Hou et al.": "materials, raters were required to pay attention solely to the dedicated behav-"
        },
        {
          "6\nR. Hou et al.": "iors listed in the EW definition (see Sect. 3). Hence, we extracted interpretable"
        },
        {
          "6\nR. Hou et al.": "features by employing off-the-shelf techniques to represent the EW-associated be-"
        },
        {
          "6\nR. Hou et al.": "haviors. Notably, these behavioral cues are typically linked to the affective states"
        },
        {
          "6\nR. Hou et al.": "of teachers and students. Therefore, we implemented facial emotion recognition"
        },
        {
          "6\nR. Hou et al.": "to identify \"smiling\" in videos, applied speech emotion recognition (SER) to de-"
        },
        {
          "6\nR. Hou et al.": "tect \"laughter\" in audio, and carried out text sentiment analysis to distinguish"
        },
        {
          "6\nR. Hou et al.": "\"positive comments\" in classroom discourse."
        },
        {
          "6\nR. Hou et al.": "Facial Emotion Recognition We adopted a deep neural network architecture,"
        },
        {
          "6\nR. Hou et al.": "EmoNet [30], which performs multi-task predictions of facial emotion in a single"
        },
        {
          "6\nR. Hou et al.": "pass. Beyond the recognition of discrete emotion categories, EmoNet simultane-"
        },
        {
          "6\nR. Hou et al.": "ously estimates continuous valence (positive or negative) and arousal\n(excited"
        },
        {
          "6\nR. Hou et al.": "or calm) values defined in the circumplex model of affect [23], enabling a more"
        },
        {
          "6\nR. Hou et al.": "comprehensive depiction of human emotions. We utilized an EmoNet model pre-"
        },
        {
          "6\nR. Hou et al.": "trained on AffectNet [17], a dataset containing a vast amount of facial\nimages in"
        },
        {
          "6\nR. Hou et al.": "the wild annotated with discrete and continuous emotion labels. To apply the"
        },
        {
          "6\nR. Hou et al.": "model to our classroom dataset, we first down-sampled video segments from 25"
        },
        {
          "6\nR. Hou et al.": "to 2 FPS (i.e., 1920 frames for a 16-minute segment), which considered the mini-"
        },
        {
          "6\nR. Hou et al.": "mal variation between consecutive frames and reduced computational resources."
        },
        {
          "6\nR. Hou et al.": "We\nthen employed RetinaFace\n[9], known for\nits\ncompetitive performance\nin"
        },
        {
          "6\nR. Hou et al.": "crowded environments, to detect faces in each frame. Afterward, each face crop"
        },
        {
          "6\nR. Hou et al.": "was input into EmoNet, which predicted valence and arousal values ranging from"
        },
        {
          "6\nR. Hou et al.": "-1 to 1, along with a probability distribution over five emotions (neutral, happy,"
        },
        {
          "6\nR. Hou et al.": "sad, surprise,\nfear). For each frame, we aggregated the predictions by averaging"
        },
        {
          "6\nR. Hou et al.": "the valence and arousal values as well as the estimation scores over five discrete"
        },
        {
          "6\nR. Hou et al.": "labels across all detected faces, yielding a 7D feature vector."
        },
        {
          "6\nR. Hou et al.": "Speech Emotion Recognition Recent work [19] tackled SER by employing"
        },
        {
          "6\nR. Hou et al.": "transfer\nlearning based on embeddings derived from pre-trained deep models,"
        },
        {
          "6\nR. Hou et al.": "showing superior performance over conventional methods that relied on low-level"
        },
        {
          "6\nR. Hou et al.": "acoustic features. Given that the spoken language in our dataset is German, we"
        },
        {
          "6\nR. Hou et al.": "utilized XLSR proposed by Facebook AI\n[7]\nto extract cross-lingual deep em-"
        },
        {
          "6\nR. Hou et al.": "beddings from raw audio signals. XLSR is pre-trained in over 50 languages and"
        },
        {
          "6\nR. Hou et al.": "built on top of wav2vec 2.0 [3], a transformer-based model trained on unlabeled"
        },
        {
          "6\nR. Hou et al.": "data in a self-supervised manner for speech recognition. This approach enhances"
        },
        {
          "6\nR. Hou et al.": "performance\nfor\nlow-resource\nlanguages.\nIn particular, we used a publicly re-"
        },
        {
          "6\nR. Hou et al.": "leased XLSR model fine-tuned in German, mapping a speech signal to a 1024D"
        },
        {
          "6\nR. Hou et al.": "latent embedding. We applied the model as a feature extractor to EmoDB [6],"
        },
        {
          "6\nR. Hou et al.": "a database consisting of 535 audio instances, each representing a German ut-"
        },
        {
          "6\nR. Hou et al.": "terance categorized into seven discrete emotion labels (anger, boredom, disgust,"
        },
        {
          "6\nR. Hou et al.": "fear, happiness,\nsadness, neutral). Based on the resulting features, we trained"
        },
        {
          "6\nR. Hou et al.": "a two-layer Multi-Layer Perceptron (MLP) classifier on the randomly selected"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Automated Assessment of Encouragement and Warmth in Classrooms\n7": "80% of the EmoDB data and used the remainder for testing. The accuracy on"
        },
        {
          "Automated Assessment of Encouragement and Warmth in Classrooms\n7": "the test set achieved 0.95. In our classroom setup, we divided the 16-minute au-"
        },
        {
          "Automated Assessment of Encouragement and Warmth in Classrooms\n7": "dio segments into 192 5-second windows without overlap due to the observation"
        },
        {
          "Automated Assessment of Encouragement and Warmth in Classrooms\n7": "that over 95% of the EmoDB instances lasted under 5 seconds, with an average"
        },
        {
          "Automated Assessment of Encouragement and Warmth in Classrooms\n7": "duration of 2.8 seconds. We then applied the XLSR model and the trained MLP"
        },
        {
          "Automated Assessment of Encouragement and Warmth in Classrooms\n7": "classifier to infer emotions expressed in each audio window, yielding a 7D feature"
        },
        {
          "Automated Assessment of Encouragement and Warmth in Classrooms\n7": "vector as a probability distribution over the seven emotion labels."
        },
        {
          "Automated Assessment of Encouragement and Warmth in Classrooms\n7": "Sentiment Analysis TextBlob4 is a well-established toolkit supporting multi-"
        },
        {
          "Automated Assessment of Encouragement and Warmth in Classrooms\n7": "ple natural language processing functionalities, such as tokenization and transla-"
        },
        {
          "Automated Assessment of Encouragement and Warmth in Classrooms\n7": "tion. It conducts lexicon-based sentiment analysis utilizing a predefined polarity"
        },
        {
          "Automated Assessment of Encouragement and Warmth in Classrooms\n7": "dictionary. Prior\nresearch employed the\ntoolkit\nfor\nsentiment analysis of\nstu-"
        },
        {
          "Automated Assessment of Encouragement and Warmth in Classrooms\n7": "dent feedback [24].\nIn this work, we applied TextBlob-de5, a German language"
        },
        {
          "Automated Assessment of Encouragement and Warmth in Classrooms\n7": "extension,\nto the transcript segments that were manually annotated regarding"
        },
        {
          "Automated Assessment of Encouragement and Warmth in Classrooms\n7": "turn-taking between speakers. The tool assessed the sentiment of each teacher or"
        },
        {
          "Automated Assessment of Encouragement and Warmth in Classrooms\n7": "student utterance by assigning a polarity score ranging from -1 to 1. In line with"
        },
        {
          "Automated Assessment of Encouragement and Warmth in Classrooms\n7": "[24], we categorized utterances according to their polarity scores,\nlabeling those"
        },
        {
          "Automated Assessment of Encouragement and Warmth in Classrooms\n7": "above/equal to/below 0 as positive/neutral/negative, respectively. We generated"
        },
        {
          "Automated Assessment of Encouragement and Warmth in Classrooms\n7": "a 4D feature vector\nfor each transcript\nsegment by counting positive, neutral,"
        },
        {
          "Automated Assessment of Encouragement and Warmth in Classrooms\n7": "and negative utterances and computing a cumulative polarity score across all"
        },
        {
          "Automated Assessment of Encouragement and Warmth in Classrooms\n7": "utterances. Since each lesson recording’s final segment typically did not span 16"
        },
        {
          "Automated Assessment of Encouragement and Warmth in Classrooms\n7": "minutes, we normalized the feature vector by the respective segment duration."
        },
        {
          "Automated Assessment of Encouragement and Warmth in Classrooms\n7": "Prediction It is noteworthy that both facial and speech emotion features were"
        },
        {
          "Automated Assessment of Encouragement and Warmth in Classrooms\n7": "temporal, whereas text sentiment features were generated in a segment-wise way."
        },
        {
          "Automated Assessment of Encouragement and Warmth in Classrooms\n7": "We aggregated the visual and auditory features for each segment by computing"
        },
        {
          "Automated Assessment of Encouragement and Warmth in Classrooms\n7": "the mean along the temporal dimension. We then concatenated the segment-wise"
        },
        {
          "Automated Assessment of Encouragement and Warmth in Classrooms\n7": "features from all three modalities into a single 18D vector. Afterward, we formu-"
        },
        {
          "Automated Assessment of Encouragement and Warmth in Classrooms\n7": "lated the EW estimation problem as both classification and regression tasks. To"
        },
        {
          "Automated Assessment of Encouragement and Warmth in Classrooms\n7": "compare the two approaches fairly, we trained an identical set of models: Random"
        },
        {
          "Automated Assessment of Encouragement and Warmth in Classrooms\n7": "Forest (RF), Support Vector Machine (SVM), and MLP with two layers. As each"
        },
        {
          "Automated Assessment of Encouragement and Warmth in Classrooms\n7": "segment was double-rated, we calculated the average of the two human ratings as"
        },
        {
          "Automated Assessment of Encouragement and Warmth in Classrooms\n7": "the ground truth. For classification, we rounded the ground truth in the training"
        },
        {
          "Automated Assessment of Encouragement and Warmth in Classrooms\n7": "set to the nearest integer, within the range of one to four. To guarantee general-"
        },
        {
          "Automated Assessment of Encouragement and Warmth in Classrooms\n7": "ization, all models were evaluated through stratified,\nlesson-independent 5-fold"
        },
        {
          "Automated Assessment of Encouragement and Warmth in Classrooms\n7": "cross-validation, such that segments from the same lesson were always grouped"
        },
        {
          "Automated Assessment of Encouragement and Warmth in Classrooms\n7": "into the same fold, and each fold maintained the original score distribution. The"
        },
        {
          "Automated Assessment of Encouragement and Warmth in Classrooms\n7": "evaluation results were averaged across all\ntest\nfolds. Before model fitting, we"
        },
        {
          "Automated Assessment of Encouragement and Warmth in Classrooms\n7": "standardized the features using the mean and the standard deviation computed"
        },
        {
          "Automated Assessment of Encouragement and Warmth in Classrooms\n7": "from the\ntraining data. Moreover, we\ncarried out grid-search hyperparameter"
        },
        {
          "Automated Assessment of Encouragement and Warmth in Classrooms\n7": "tuning to identify the best-performing configuration for each model."
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "8\nR. Hou et al.": "4.2\nChatGPT Zero-Shot Annotation"
        },
        {
          "8\nR. Hou et al.": "Recent advances in LLMs introduce vast opportunities for their application in"
        },
        {
          "8\nR. Hou et al.": "the education sector\nto enhance teaching and learning [15,26]. LLMs are typi-"
        },
        {
          "8\nR. Hou et al.": "cally trained on extensive text corpora and thus equipped with broad knowledge,"
        },
        {
          "8\nR. Hou et al.": "enabling quick adaptation to new tasks without\nthe need for\nretraining. Such"
        },
        {
          "8\nR. Hou et al.": "zero-shot capability has shown notable effectiveness across various text annota-"
        },
        {
          "8\nR. Hou et al.": "tion problems [10]. In the GTI study, transcripts play a significant role in rating"
        },
        {
          "8\nR. Hou et al.": "processes\n(e.g.,\nraters\nin many countries employed shorthand and highlighting"
        },
        {
          "8\nR. Hou et al.": "tools directly on transcripts). Therefore, we explore the potential of ChatGPT"
        },
        {
          "8\nR. Hou et al.": "to assess the EW component without requiring training, relying exclusively on"
        },
        {
          "8\nR. Hou et al.": "classroom transcripts.\nIn particular, we prompted ChatGPT with a transcript"
        },
        {
          "8\nR. Hou et al.": "segment\nto rate, along with EW’s definition, behavioral examples, and coding"
        },
        {
          "8\nR. Hou et al.": "rubrics, as depicted in Fig. 3. Besides, we requested ChatGPT to reason its de-"
        },
        {
          "8\nR. Hou et al.": "cision on the score assignment, as suggested by [31]. For this task, we employed"
        },
        {
          "8\nR. Hou et al.": "two GPT models via the OpenAI API\nto compare their performance, namely"
        },
        {
          "8\nR. Hou et al.": "gpt-3.5-turbo-1106 and gpt-4-1106-vision-preview. To consistently compare with"
        },
        {
          "8\nR. Hou et al.": "those trained models, we evaluated ChatGPT’s zero-shot performance on the"
        },
        {
          "8\nR. Hou et al.": "same five test folds and averaged the results."
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "those trained models, we evaluated ChatGPT’s zero-shot performance on the": "same five test folds and averaged the results."
        },
        {
          "those trained models, we evaluated ChatGPT’s zero-shot performance on the": ""
        },
        {
          "those trained models, we evaluated ChatGPT’s zero-shot performance on the": ""
        },
        {
          "those trained models, we evaluated ChatGPT’s zero-shot performance on the": "```{Transcript}```"
        },
        {
          "those trained models, we evaluated ChatGPT’s zero-shot performance on the": ""
        },
        {
          "those trained models, we evaluated ChatGPT’s zero-shot performance on the": ""
        },
        {
          "those trained models, we evaluated ChatGPT’s zero-shot performance on the": ""
        },
        {
          "those trained models, we evaluated ChatGPT’s zero-shot performance on the": ""
        },
        {
          "those trained models, we evaluated ChatGPT’s zero-shot performance on the": ""
        },
        {
          "those trained models, we evaluated ChatGPT’s zero-shot performance on the": "among students, e.g., smiling,"
        },
        {
          "those trained models, we evaluated ChatGPT’s zero-shot performance on the": ""
        },
        {
          "those trained models, we evaluated ChatGPT’s zero-shot performance on the": ""
        },
        {
          "those trained models, we evaluated ChatGPT’s zero-shot performance on the": "Format your answer as:"
        },
        {
          "those trained models, we evaluated ChatGPT’s zero-shot performance on the": ""
        },
        {
          "those trained models, we evaluated ChatGPT’s zero-shot performance on the": ""
        }
      ],
      "page": 8
    },
    {
      "caption": "Table 1: Results of GTI Encouragement and Warmth estimation.",
      "data": [
        {
          "Automated Assessment of Encouragement and Warmth in Classrooms": "Table 1. Results of GTI Encouragement and Warmth estimation.",
          "9": ""
        },
        {
          "Automated Assessment of Encouragement and Warmth in Classrooms": "Approach\nModel\nPearson r",
          "9": ""
        },
        {
          "Automated Assessment of Encouragement and Warmth in Classrooms": "Inter-Rater Reliability\nHuman Raters\n0.513 (0.028)",
          "9": ""
        },
        {
          "Automated Assessment of Encouragement and Warmth in Classrooms": "RF\n0.391 (0.041)",
          "9": ""
        },
        {
          "Automated Assessment of Encouragement and Warmth in Classrooms": "Multimodal Classifier\nSVM\n0.375 (0.058)",
          "9": ""
        },
        {
          "Automated Assessment of Encouragement and Warmth in Classrooms": "MLP\n0.392 (0.040)",
          "9": ""
        },
        {
          "Automated Assessment of Encouragement and Warmth in Classrooms": "RF\n0.429 (0.051)",
          "9": ""
        },
        {
          "Automated Assessment of Encouragement and Warmth in Classrooms": "Multimodal Regressor\nSVM\n0.433 (0.041)",
          "9": ""
        },
        {
          "Automated Assessment of Encouragement and Warmth in Classrooms": "MLP\n0.441 (0.039)",
          "9": ""
        },
        {
          "Automated Assessment of Encouragement and Warmth in Classrooms": "GPT-3.5\n0.027 (0.071)",
          "9": ""
        },
        {
          "Automated Assessment of Encouragement and Warmth in Classrooms": "ChatGPT Zero-Shot",
          "9": ""
        },
        {
          "Automated Assessment of Encouragement and Warmth in Classrooms": "GPT-4\n0.341 (0.037)",
          "9": ""
        },
        {
          "Automated Assessment of Encouragement and Warmth in Classrooms": "0.499 (0.033)\nMLP Reg.+GPT-4 (Unweighted)",
          "9": ""
        },
        {
          "Automated Assessment of Encouragement and Warmth in Classrooms": "Ensemble",
          "9": ""
        },
        {
          "Automated Assessment of Encouragement and Warmth in Classrooms": "MLP Reg.+GPT-4 (Weighted)\n0.513 (0.036)",
          "9": ""
        },
        {
          "Automated Assessment of Encouragement and Warmth in Classrooms": "Notes. For humans, r corresponds to the average across raters. For automatic models, r",
          "9": ""
        },
        {
          "Automated Assessment of Encouragement and Warmth in Classrooms": "is the average over 5 folds between estimates and mean human ratings. Standard error",
          "9": ""
        },
        {
          "Automated Assessment of Encouragement and Warmth in Classrooms": "estimates are shown in parentheses, with the best model\nfor each approach in bold.",
          "9": ""
        }
      ],
      "page": 9
    },
    {
      "caption": "Table 1: Results of GTI Encouragement and Warmth estimation.",
      "data": [
        {
          "5\nResults": "5.1\nModel Performance"
        },
        {
          "5\nResults": "Similar to a recent study [32], we treat automatic estimation approaches as in-"
        },
        {
          "5\nResults": "dividual raters and explore the extent of their consistency with human raters in"
        },
        {
          "5\nResults": "terms of Pearson correlation coefficient r. For\nthis purpose, we first computed"
        },
        {
          "5\nResults": "human inter-rater reliability (IRR) in a leave-one-rater-out fashion. In our con-"
        },
        {
          "5\nResults": "text, there were 14 human raters conducting double ratings. We computed r for"
        },
        {
          "5\nResults": "each rater by comparing their ratings with those assigned by other raters. The"
        },
        {
          "5\nResults": "coefficients obtained were then averaged across all raters. Meanwhile, we report"
        },
        {
          "5\nResults": "√"
        },
        {
          "5\nResults": "standard error estimates\n(standard deviation over all\nraters divided by\n14)."
        },
        {
          "5\nResults": "For model evaluation, we calculated the average of r between model predictions"
        },
        {
          "5\nResults": "and human ratings over five test\nfolds, as well as\nthe corresponding standard"
        },
        {
          "5\nResults": "error estimates. The results are summarized in Table 1. Except for GPT-3.5, the"
        },
        {
          "5\nResults": "correlations for each model\nin each fold are statistically significant (p < .05)."
        },
        {
          "5\nResults": "For multimodal supervised approaches,\nthe best-trained model\nis the MLP"
        },
        {
          "5\nResults": "regressor (r = .441). Overall,\nit seems advantageous to formulate the problem as"
        },
        {
          "5\nResults": "a regression task. This is evident from the fact that classifiers achieve the highest"
        },
        {
          "5\nResults": "Pearson r of\n.392, trailing behind all regressors in performance. Another inter-"
        },
        {
          "5\nResults": "esting result is that the two-layer MLP outperforms the other two conventional"
        },
        {
          "5\nResults": "models\nin both classification and regression tasks, underscoring the efficacy of"
        },
        {
          "5\nResults": "contemporary neural networks. It is noteworthy that adding more layers did not"
        },
        {
          "5\nResults": "enhance the performance, potentially attributed to the risk of overfitting inher-"
        },
        {
          "5\nResults": "ent in deeper models when confronted with a limited dataset size. When turning"
        },
        {
          "5\nResults": "to ChatGPT zero-shot\nresults, a clear difference between the two model gen-"
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "R. Hou et al.": "TextNumPosUtt"
        },
        {
          "R. Hou et al.": "TextSumPolarity"
        },
        {
          "R. Hou et al.": "AudioAnger"
        },
        {
          "R. Hou et al.": "AudioHappiness"
        },
        {
          "R. Hou et al.": "AudioDisgust"
        },
        {
          "R. Hou et al.": "TextNumNeutUtt"
        },
        {
          "R. Hou et al.": "VideoNeutral"
        },
        {
          "R. Hou et al.": "TextNumNegUtt"
        },
        {
          "R. Hou et al.": "AudioSadness"
        },
        {
          "R. Hou et al.": "VideoFear"
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "VideoFear": "Low"
        },
        {
          "VideoFear": "0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00"
        },
        {
          "VideoFear": "SHAP value (impact on model output)"
        },
        {
          "VideoFear": "Fig. 4. SHAP summary plot for MLP regressor. Points depict SHAP values per"
        },
        {
          "VideoFear": "feature per data sample. Features are ranked by their importance (sum of SHAP"
        },
        {
          "VideoFear": "value magnitudes over all samples). The 10 most influential\nfeatures are shown."
        },
        {
          "VideoFear": "erations is identified. The estimates of GPT-3.5 show no significant correlation"
        },
        {
          "VideoFear": "with human-rated scores. In contrast, GPT-4 has remarkably enhanced the capa-"
        },
        {
          "VideoFear": "bility to understand text, particularly achieving superior zero-shot performance"
        },
        {
          "VideoFear": "over its predecessor to score EW in classroom discourse. It attains a Pearson r"
        },
        {
          "VideoFear": "of\n.341 without requiring any training. Finally, the ensemble model, combining"
        },
        {
          "VideoFear": "the best-performing MLP regressor with GPT-4, boosts the performance signifi-"
        },
        {
          "VideoFear": "cantly. Using a simple averaging method raises the Pearson r to .499. When the"
        },
        {
          "VideoFear": "averaging process considers\nthe respective reliability of both base models,\nthe"
        },
        {
          "VideoFear": "correlation reaches a peak of\n.513,\nidentical to that of the human IRR."
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Automated Assessment of Encouragement and Warmth in Classrooms": "veals the negative impact of detected anger and disgust as well as the positive",
          "11": ""
        },
        {
          "Automated Assessment of Encouragement and Warmth in Classrooms": "impact of detected happiness in the audio on the prediction of higher EW scores.",
          "11": ""
        },
        {
          "Automated Assessment of Encouragement and Warmth in Classrooms": "ChatGPT GPT-4’s efficacy is evident not only in its agreement with human",
          "11": ""
        },
        {
          "Automated Assessment of Encouragement and Warmth in Classrooms": "raters but also in its capability to provide logical reasoning. For example, GPT-4",
          "11": ""
        },
        {
          "Automated Assessment of Encouragement and Warmth in Classrooms": "assigned a score of 4 to a transcript consistently with the human ratings and",
          "11": ""
        },
        {
          "Automated Assessment of Encouragement and Warmth in Classrooms": "explained its decision by identifying relevant evidence from the discourse:",
          "11": ""
        }
      ],
      "page": 11
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "ChatGPT GPT-4’s efficacy is evident not only in its agreement with human": "raters but also in its capability to provide logical reasoning. For example, GPT-4"
        },
        {
          "ChatGPT GPT-4’s efficacy is evident not only in its agreement with human": "assigned a score of 4 to a transcript consistently with the human ratings and"
        },
        {
          "ChatGPT GPT-4’s efficacy is evident not only in its agreement with human": "explained its decision by identifying relevant evidence from the discourse:"
        },
        {
          "ChatGPT GPT-4’s efficacy is evident not only in its agreement with human": "\"[...] For instance, the teacher praises S15’s work as ‘sieht schön aus perfekt’(‘looks beau-"
        },
        {
          "ChatGPT GPT-4’s efficacy is evident not only in its agreement with human": "tiful perfect’) and encourages S04 by validating their thinking process. The teacher’s tone"
        },
        {
          "ChatGPT GPT-4’s efficacy is evident not only in its agreement with human": "is patient and nurturing, especially visible in exchanges like ‘keine Panik’(‘no panic’) [...]"
        },
        {
          "ChatGPT GPT-4’s efficacy is evident not only in its agreement with human": "The teacher often uses humor, as seen in statements like ‘bevor hier einer weint’(‘before"
        },
        {
          "ChatGPT GPT-4’s efficacy is evident not only in its agreement with human": "someone cries here’) [...]\""
        }
      ],
      "page": 11
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "12\nR. Hou et al.": "of LLMs yet enhances the final performance, providing insights into strategies"
        },
        {
          "12\nR. Hou et al.": "for using LLMs both effectively and efficiently when addressing similar tasks."
        },
        {
          "12\nR. Hou et al.": "Due to the use of distinct datasets and protocols, we note that\nthe results"
        },
        {
          "12\nR. Hou et al.": "are not comparable to prior\nstudies\n[13,22,31,32] which focused on CLASS di-"
        },
        {
          "12\nR. Hou et al.": "mensions. GTI EW exhibits\nsimilar behavioral\nindicators\nto CLASS PC, but"
        },
        {
          "12\nR. Hou et al.": "their coding rubrics differ\nin aspects\nsuch as\nscore scale (4- vs. 7-point). Our"
        },
        {
          "12\nR. Hou et al.": "exploration contributes to a broader understanding of similar constructs across"
        },
        {
          "12\nR. Hou et al.": "different protocols. Further, we explore methods\nfor German speech and text"
        },
        {
          "12\nR. Hou et al.": "processing, diverging from the common use of English data in existing research,"
        },
        {
          "12\nR. Hou et al.": "which enriches the discussion on developing educational technologies in multilin-"
        },
        {
          "12\nR. Hou et al.": "gual contexts. Moreover, transcripts not only provide more informative features"
        },
        {
          "12\nR. Hou et al.": "for predictive models but also serve as a privacy-conscious modality compared to"
        },
        {
          "12\nR. Hou et al.": "video and audio recordings. As classroom recordings are sensitive data involving"
        },
        {
          "12\nR. Hou et al.": "minors, ethical considerations are of utmost importance. We emphasize that au-"
        },
        {
          "12\nR. Hou et al.": "tomated observation tools intend to streamline post hoc analysis and reduce the"
        },
        {
          "12\nR. Hou et al.": "need for manual coding instead of being used for real-time classroom monitoring."
        },
        {
          "12\nR. Hou et al.": "Our work is subject to several\nlimitations. First, our analysis utilized manual"
        },
        {
          "12\nR. Hou et al.": "transcripts. We could implement advanced speech recognition techniques, toward"
        },
        {
          "12\nR. Hou et al.": "a fully automatic system.\nIn classroom environments,\nstudents’\nfaces\nfar away"
        },
        {
          "12\nR. Hou et al.": "from the camera lead to limited spatial resolution, reducing the performance of"
        },
        {
          "12\nR. Hou et al.": "emotion recognition models. Solutions could be applying super-resolution meth-"
        },
        {
          "12\nR. Hou et al.": "ods to improve face image quality. Due to the potential disagreement between"
        },
        {
          "12\nR. Hou et al.": "distinct model explanation methods\n[29],\nit\nis valuable to validate various ex-"
        },
        {
          "12\nR. Hou et al.": "plainers on our EW scoring task. Another approach to feature importance anal-"
        },
        {
          "12\nR. Hou et al.": "ysis would be an ablation study to compare the performance using unimodality."
        },
        {
          "12\nR. Hou et al.": "Furthermore, one future direction is to adapt the proposed methods for the auto-"
        },
        {
          "12\nR. Hou et al.": "mated coding of additional GTI aspects such as classroom discourse to achieve"
        },
        {
          "12\nR. Hou et al.": "a more\ncomprehensive measurement of\nteaching effectiveness. As GPT-4 is a"
        },
        {
          "12\nR. Hou et al.": "multimodal LLM capable of image understanding, another future study involves"
        },
        {
          "12\nR. Hou et al.": "exploring whether\nthe inclusion of classroom frames\ntogether with transcripts"
        },
        {
          "12\nR. Hou et al.": "could enhance estimation accuracy. To further\nimprove model generalizability,"
        },
        {
          "12\nR. Hou et al.": "conducting cross-dataset evaluation, particularly employing data from diverse"
        },
        {
          "12\nR. Hou et al.": "countries or cultures, would be part of\nfuture research."
        }
      ],
      "page": 12
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Automated Assessment of Encouragement and Warmth in Classrooms\n13": "References"
        },
        {
          "Automated Assessment of Encouragement and Warmth in Classrooms\n13": "1. Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F.L., Almeida,"
        },
        {
          "Automated Assessment of Encouragement and Warmth in Classrooms\n13": "D., Altenschmidt, J., Altman, S., Anadkat, S., et al.: Gpt-4 technical report. arXiv"
        },
        {
          "Automated Assessment of Encouragement and Warmth in Classrooms\n13": "preprint arXiv:2303.08774 (2023)"
        },
        {
          "Automated Assessment of Encouragement and Warmth in Classrooms\n13": "2. Ahuja, K., Kim, D., Xhakaj, F., Varga, V., Xie, A., Zhang, S., Townsend, J.E.,"
        },
        {
          "Automated Assessment of Encouragement and Warmth in Classrooms\n13": "Harrison, C., Ogan, A., Agarwal, Y.: Edusense: Practical\nclassroom sensing at"
        },
        {
          "Automated Assessment of Encouragement and Warmth in Classrooms\n13": "scale. Proceedings of\nthe ACM on Interactive, Mobile, Wearable and Ubiquitous"
        },
        {
          "Automated Assessment of Encouragement and Warmth in Classrooms\n13": "Technologies 3(3), 1–26 (2019)"
        },
        {
          "Automated Assessment of Encouragement and Warmth in Classrooms\n13": "3. Baevski, A., Zhou, Y., Mohamed, A., Auli, M.: wav2vec 2.0: A framework for"
        },
        {
          "Automated Assessment of Encouragement and Warmth in Classrooms\n13": "self-supervised learning of speech representations. Advances in neural\ninformation"
        },
        {
          "Automated Assessment of Encouragement and Warmth in Classrooms\n13": "processing systems 33, 12449–12460 (2020)"
        },
        {
          "Automated Assessment of Encouragement and Warmth in Classrooms\n13": "4. Bell, C., Qi, Y., Witherspoon, M., Barragan, M., Howell, H.: Annex a: Talis video"
        },
        {
          "Automated Assessment of Encouragement and Warmth in Classrooms\n13": "training notes: Holistic domain ratings and components.\nIn: OECD (ed.) Global"
        },
        {
          "Automated Assessment of Encouragement and Warmth in Classrooms\n13": "Teaching Insights: Technical Report. OECD Publishing (2018)"
        },
        {
          "Automated Assessment of Encouragement and Warmth in Classrooms\n13": "5. Bühler, B., Hou, R., Bozkir, E., Goldberg, P., Gerjets, P., Trautwein, U., Kas-"
        },
        {
          "Automated Assessment of Encouragement and Warmth in Classrooms\n13": "neci, E.: Automated hand-raising detection in classroom videos: A view-invariant"
        },
        {
          "Automated Assessment of Encouragement and Warmth in Classrooms\n13": "and occlusion-robust machine learning approach.\nIn:\nInternational Conference on"
        },
        {
          "Automated Assessment of Encouragement and Warmth in Classrooms\n13": "Artificial Intelligence in Education. pp. 102–113 (2023)"
        },
        {
          "Automated Assessment of Encouragement and Warmth in Classrooms\n13": "6. Burkhardt, F., Paeschke, A., Rolfes, M., Sendlmeier, W.F., Weiss, B., et al.: A"
        },
        {
          "Automated Assessment of Encouragement and Warmth in Classrooms\n13": "database of german emotional speech. In: Interspeech. vol. 5, pp. 1517–1520 (2005)"
        },
        {
          "Automated Assessment of Encouragement and Warmth in Classrooms\n13": "7. Conneau, A., Baevski, A., Collobert, R., Mohamed, A., Auli, M.: Unsuper-"
        },
        {
          "Automated Assessment of Encouragement and Warmth in Classrooms\n13": "vised cross-lingual\nrepresentation learning for\nspeech recognition. arXiv preprint"
        },
        {
          "Automated Assessment of Encouragement and Warmth in Classrooms\n13": "arXiv:2006.13979 (2020)"
        },
        {
          "Automated Assessment of Encouragement and Warmth in Classrooms\n13": "8. Demszky, D., Liu, J., Hill, H.C., Jurafsky, D., Piech, C.: Can automated feedback"
        },
        {
          "Automated Assessment of Encouragement and Warmth in Classrooms\n13": "improve teachers’ uptake of student ideas? evidence from a randomized controlled"
        },
        {
          "Automated Assessment of Encouragement and Warmth in Classrooms\n13": "trial\nin a large-scale online\ncourse. Educational Evaluation and Policy Analysis"
        },
        {
          "Automated Assessment of Encouragement and Warmth in Classrooms\n13": "(2023)"
        },
        {
          "Automated Assessment of Encouragement and Warmth in Classrooms\n13": "9. Deng, J., Guo, J., Ververas, E., Kotsia,\nI., Zafeiriou, S.: Retinaface: Single-shot"
        },
        {
          "Automated Assessment of Encouragement and Warmth in Classrooms\n13": "multi-level\nface localisation in the wild.\nIn: The IEEE Conference on Computer"
        },
        {
          "Automated Assessment of Encouragement and Warmth in Classrooms\n13": "Vision and Pattern Recognition (CVPR). pp. 5203–5212 (2020)"
        },
        {
          "Automated Assessment of Encouragement and Warmth in Classrooms\n13": "10. Gilardi, F., Alizadeh, M., Kubli, M.: Chatgpt outperforms crowd-workers for text-"
        },
        {
          "Automated Assessment of Encouragement and Warmth in Classrooms\n13": "annotation tasks. arXiv preprint arXiv:2303.15056 (2023)"
        },
        {
          "Automated Assessment of Encouragement and Warmth in Classrooms\n13": "11. Goldberg, P., Sümer, Ö., Stürmer, K., Wagner, W., Göllner, R., Gerjets, P., Kas-"
        },
        {
          "Automated Assessment of Encouragement and Warmth in Classrooms\n13": "neci, E., Trautwein, U.: Attentive or not? toward a machine learning approach to"
        },
        {
          "Automated Assessment of Encouragement and Warmth in Classrooms\n13": "assessing students’ visible engagement in classroom instruction. Educational Psy-"
        },
        {
          "Automated Assessment of Encouragement and Warmth in Classrooms\n13": "chology Review 33, 27–49 (2021)"
        },
        {
          "Automated Assessment of Encouragement and Warmth in Classrooms\n13": "12. Hunkins, N., Kelly, S., D’Mello, S.:\n“beautiful work, you’re rock stars!”: Teacher"
        },
        {
          "Automated Assessment of Encouragement and Warmth in Classrooms\n13": "analytics\nto uncover discourse that\nsupports or undermines\nstudent motivation,"
        },
        {
          "Automated Assessment of Encouragement and Warmth in Classrooms\n13": "identity,\nand belonging\nin classrooms.\nIn: LAK22:\n12th International Learning"
        },
        {
          "Automated Assessment of Encouragement and Warmth in Classrooms\n13": "Analytics and Knowledge Conference. pp. 230–238 (2022)"
        },
        {
          "Automated Assessment of Encouragement and Warmth in Classrooms\n13": "13.\nJames, A., Kashyap, M., Chua, Y.H.V., Maszczyk, T., Núñez, A.M., Bull, R.,"
        },
        {
          "Automated Assessment of Encouragement and Warmth in Classrooms\n13": "Dauwels, J.: Inferring the climate in classrooms from audio and video recordings:"
        },
        {
          "Automated Assessment of Encouragement and Warmth in Classrooms\n13": "a machine learning approach. In: IEEE International Conference on Teaching, As-"
        },
        {
          "Automated Assessment of Encouragement and Warmth in Classrooms\n13": "sessment, and Learning for Engineering. pp. 983–988 (2018)"
        },
        {
          "Automated Assessment of Encouragement and Warmth in Classrooms\n13": "14.\nJensen, E., L. Pugh, S., K. D’Mello, S.: A deep transfer\nlearning approach to"
        },
        {
          "Automated Assessment of Encouragement and Warmth in Classrooms\n13": "modeling teacher discourse in the classroom. In: LAK21: 11th international learning"
        },
        {
          "Automated Assessment of Encouragement and Warmth in Classrooms\n13": "analytics and knowledge conference. pp. 302–312 (2021)"
        },
        {
          "Automated Assessment of Encouragement and Warmth in Classrooms\n13": "15. Kasneci, E., Seßler, K., Küchemann, S., Bannert, M., Dementieva, D., Fischer, F.,"
        },
        {
          "Automated Assessment of Encouragement and Warmth in Classrooms\n13": "Gasser, U., Groh, G., Günnemann, S., Hüllermeier, E., et al.: Chatgpt for good?"
        }
      ],
      "page": 13
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "14\nR. Hou et al.": "on opportunities and challenges of\nlarge language models for education. Learning"
        },
        {
          "14\nR. Hou et al.": "and individual differences 103, 102274 (2023)"
        },
        {
          "14\nR. Hou et al.": "16. Lundberg, S.M., Lee, S.I.: A unified approach to interpreting model predictions."
        },
        {
          "14\nR. Hou et al.": "Advances in neural\ninformation processing systems 30 (2017)"
        },
        {
          "14\nR. Hou et al.": "17. Mollahosseini, A., Hasani, B., Mahoor, M.H.: Affectnet: A database for facial ex-"
        },
        {
          "14\nR. Hou et al.": "pression, valence, and arousal computing in the wild. IEEE Transactions on Affec-"
        },
        {
          "14\nR. Hou et al.": "tive Computing 10(1), 18–31 (2017)"
        },
        {
          "14\nR. Hou et al.": "18. OECD: Global Teaching InSights: A Video Study of Teaching. OECD Publishing,"
        },
        {
          "14\nR. Hou et al.": "Paris (2020)"
        },
        {
          "14\nR. Hou et al.": "19. Pepino, L., Riera, P., Ferrer, L.: Emotion Recognition from Speech Using wav2vec"
        },
        {
          "14\nR. Hou et al.": "2.0 Embeddings. In: Proc. Interspeech. pp. 3400–3404 (2021)"
        },
        {
          "14\nR. Hou et al.": "20. Pianta, R.C., Hamre, B.K.: Conceptualization, measurement, and improvement of"
        },
        {
          "14\nR. Hou et al.": "classroom processes: Standardized observation can leverage capacity. Educational"
        },
        {
          "14\nR. Hou et al.": "researcher 38(2), 109–119 (2009)"
        },
        {
          "14\nR. Hou et al.": "21. Pianta, R.C., La Paro, K.M., Hamre, B.K.: Classroom Assessment Scoring Sys-"
        },
        {
          "14\nR. Hou et al.": "tem™: Manual K-3. Paul H Brookes Publishing (2008)"
        },
        {
          "14\nR. Hou et al.": "22. Ramakrishnan, A., Zylich, B., Ottmar, E., LoCasale-Crouch, J., Whitehill, J.: To-"
        },
        {
          "14\nR. Hou et al.": "ward automated classroom observation: Multimodal machine learning to estimate"
        },
        {
          "14\nR. Hou et al.": "class positive climate and negative climate. IEEE Transactions on Affective Com-"
        },
        {
          "14\nR. Hou et al.": "puting (2021)"
        },
        {
          "14\nR. Hou et al.": "23. Russell, J.A.: A circumplex model of affect. Journal of personality and social psy-"
        },
        {
          "14\nR. Hou et al.": "1161 (1980)\nchology 39(6),"
        },
        {
          "14\nR. Hou et al.": "24. Sadriu, S., Nuci, K.P., Imran, A.S., Uddin, I., Sajjad, M.: An automated approach"
        },
        {
          "14\nR. Hou et al.": "for analysing students feedback using sentiment analysis techniques.\nIn: Mediter-"
        },
        {
          "14\nR. Hou et al.": "ranean Conference on Pattern Recognition and Artificial Intelligence (2021)"
        },
        {
          "14\nR. Hou et al.": "25. Seidel, T., Shavelson, R.J.: Teaching effectiveness research in the past decade: The"
        },
        {
          "14\nR. Hou et al.": "role of theory and research design in disentangling meta-analysis results. Review"
        },
        {
          "14\nR. Hou et al.": "of educational research 77(4), 454–499 (2007)"
        },
        {
          "14\nR. Hou et al.": "26. Seßler, K., Xiang, T., Bogenrieder, L., Kasneci, E.: Peer: Empowering writing with"
        },
        {
          "14\nR. Hou et al.": "large language models. In: European Conference on Technology Enhanced Learn-"
        },
        {
          "14\nR. Hou et al.": "ing. pp. 755–761 (2023)"
        },
        {
          "14\nR. Hou et al.": "27. Sümer, Ö., Goldberg, P., D’Mello, S., Gerjets, P., Trautwein, U., Kasneci, E.: Multi-"
        },
        {
          "14\nR. Hou et al.": "modal engagement analysis from facial videos in the classroom. IEEE Transactions"
        },
        {
          "14\nR. Hou et al.": "on Affective Computing (2021)"
        },
        {
          "14\nR. Hou et al.": "28. Sümer, Ö., Goldberg, P., Sturmer, K., Seidel, T., Gerjets, P., Trautwein, U., Kas-"
        },
        {
          "14\nR. Hou et al.": "neci, E.: Teachers’ perception in the classroom. In: Proceedings of the IEEE Con-"
        },
        {
          "14\nR. Hou et al.": "ference on Computer Vision and Pattern Recognition Workshops (2018)"
        },
        {
          "14\nR. Hou et al.": "29. Swamy, V., Radmehr, B., Krco, N., Marras, M., Käser, T.: Evaluating the\nex-"
        },
        {
          "14\nR. Hou et al.": "plainers: black-box explainable machine learning for student success prediction in"
        },
        {
          "14\nR. Hou et al.": "moocs. arXiv preprint arXiv:2207.00551 (2022)"
        },
        {
          "14\nR. Hou et al.": "30. Toisoul, A., Kossaifi, J., Bulat, A., Tzimiropoulos, G., Pantic, M.: Estimation of"
        },
        {
          "14\nR. Hou et al.": "continuous valence and arousal\nlevels from faces in naturalistic conditions. Nature"
        },
        {
          "14\nR. Hou et al.": "Machine Intelligence (2021)"
        },
        {
          "14\nR. Hou et al.": "31. Wang, R.E., Demszky, D.:\nIs chatgpt a good teacher coach? measuring zero-shot"
        },
        {
          "14\nR. Hou et al.": "performance for scoring and providing actionable insights on classroom instruction."
        },
        {
          "14\nR. Hou et al.": "arXiv preprint arXiv:2306.03090 (2023)"
        },
        {
          "14\nR. Hou et al.": "32. Whitehill, J., LoCasale-Crouch, J.: Automated evaluation of\nclassroom instruc-"
        },
        {
          "14\nR. Hou et al.": "tional support with llms and bows: Connecting global predictions to specific feed-"
        },
        {
          "14\nR. Hou et al.": "back. arXiv preprint arXiv:2310.01132 (2023)"
        }
      ],
      "page": 14
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Gpt-4 technical report",
      "authors": [
        "J Achiam",
        "S Adler",
        "S Agarwal",
        "L Ahmad",
        "I Akkaya",
        "F Aleman",
        "D Almeida",
        "J Altenschmidt",
        "S Altman",
        "S Anadkat"
      ],
      "year": "2023",
      "venue": "Gpt-4 technical report",
      "arxiv": "arXiv:2303.08774"
    },
    {
      "citation_id": "2",
      "title": "Edusense: Practical classroom sensing at scale",
      "authors": [
        "K Ahuja",
        "D Kim",
        "F Xhakaj",
        "V Varga",
        "A Xie",
        "S Zhang",
        "J Townsend",
        "C Harrison",
        "A Ogan",
        "Y Agarwal"
      ],
      "year": "2019",
      "venue": "Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies"
    },
    {
      "citation_id": "3",
      "title": "wav2vec 2.0: A framework for self-supervised learning of speech representations",
      "authors": [
        "A Baevski",
        "Y Zhou",
        "A Mohamed",
        "M Auli"
      ],
      "year": "2020",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "4",
      "title": "Annex a: Talis video training notes: Holistic domain ratings and components",
      "authors": [
        "C Bell",
        "Y Qi",
        "M Witherspoon",
        "M Barragan",
        "H Howell"
      ],
      "year": "2018",
      "venue": "Global Teaching Insights"
    },
    {
      "citation_id": "5",
      "title": "Automated hand-raising detection in classroom videos: A view-invariant and occlusion-robust machine learning approach",
      "authors": [
        "B Bühler",
        "R Hou",
        "E Bozkir",
        "P Goldberg",
        "P Gerjets",
        "U Trautwein",
        "E Kasneci"
      ],
      "year": "2023",
      "venue": "International Conference on Artificial Intelligence in Education"
    },
    {
      "citation_id": "6",
      "title": "A database of german emotional speech",
      "authors": [
        "F Burkhardt",
        "A Paeschke",
        "M Rolfes",
        "W Sendlmeier",
        "B Weiss"
      ],
      "year": "2005",
      "venue": "Interspeech"
    },
    {
      "citation_id": "7",
      "title": "Unsupervised cross-lingual representation learning for speech recognition",
      "authors": [
        "A Conneau",
        "A Baevski",
        "R Collobert",
        "A Mohamed",
        "M Auli"
      ],
      "year": "2020",
      "venue": "Unsupervised cross-lingual representation learning for speech recognition",
      "arxiv": "arXiv:2006.13979"
    },
    {
      "citation_id": "8",
      "title": "Can automated feedback improve teachers' uptake of student ideas? evidence from a randomized controlled trial in a large-scale online course",
      "authors": [
        "D Demszky",
        "J Liu",
        "H Hill",
        "D Jurafsky",
        "C Piech"
      ],
      "year": "2023",
      "venue": "Educational Evaluation and Policy Analysis"
    },
    {
      "citation_id": "9",
      "title": "Retinaface: Single-shot multi-level face localisation in the wild",
      "authors": [
        "J Deng",
        "J Guo",
        "E Ververas",
        "I Kotsia",
        "S Zafeiriou"
      ],
      "year": "2020",
      "venue": "The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "10",
      "title": "Chatgpt outperforms crowd-workers for textannotation tasks",
      "authors": [
        "F Gilardi",
        "M Alizadeh",
        "M Kubli"
      ],
      "year": "2023",
      "venue": "Chatgpt outperforms crowd-workers for textannotation tasks",
      "arxiv": "arXiv:2303.15056"
    },
    {
      "citation_id": "11",
      "title": "Attentive or not? toward a machine learning approach to assessing students' visible engagement in classroom instruction",
      "authors": [
        "P Goldberg",
        "Ö Sümer",
        "K Stürmer",
        "W Wagner",
        "R Göllner",
        "P Gerjets",
        "E Kasneci",
        "U Trautwein"
      ],
      "year": "2021",
      "venue": "Educational Psychology Review"
    },
    {
      "citation_id": "12",
      "title": "beautiful work, you're rock stars!\": Teacher analytics to uncover discourse that supports or undermines student motivation, identity, and belonging in classrooms",
      "authors": [
        "N Hunkins",
        "S Kelly",
        "S D'mello"
      ],
      "year": "2022",
      "venue": "12th International Learning Analytics and Knowledge Conference"
    },
    {
      "citation_id": "13",
      "title": "Inferring the climate in classrooms from audio and video recordings: a machine learning approach",
      "authors": [
        "A James",
        "M Kashyap",
        "Y Chua",
        "T Maszczyk",
        "A Núñez",
        "R Bull",
        "J Dauwels"
      ],
      "year": "2018",
      "venue": "IEEE International Conference on Teaching, Assessment, and Learning for Engineering"
    },
    {
      "citation_id": "14",
      "title": "A deep transfer learning approach to modeling teacher discourse in the classroom",
      "authors": [
        "E Jensen",
        "L Pugh",
        "K D'mello"
      ],
      "year": "2021",
      "venue": "LAK21: 11th international learning analytics and knowledge conference"
    },
    {
      "citation_id": "15",
      "title": "Chatgpt for good? R. Hou et al. on opportunities and challenges of large language models for education",
      "authors": [
        "E Kasneci",
        "K Seßler",
        "S Küchemann",
        "M Bannert",
        "D Dementieva",
        "F Fischer",
        "U Gasser",
        "G Groh",
        "S Günnemann",
        "E Hüllermeier"
      ],
      "year": "2023",
      "venue": "Learning and individual differences"
    },
    {
      "citation_id": "16",
      "title": "A unified approach to interpreting model predictions",
      "authors": [
        "S Lundberg",
        "S Lee"
      ],
      "year": "2017",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "17",
      "title": "Affectnet: A database for facial expression, valence, and arousal computing in the wild",
      "authors": [
        "A Mollahosseini",
        "B Hasani",
        "M Mahoor"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "18",
      "title": "Global Teaching InSights: A Video Study of Teaching",
      "year": "2020",
      "venue": "Global Teaching InSights: A Video Study of Teaching"
    },
    {
      "citation_id": "19",
      "title": "Emotion Recognition from Speech Using wav2vec 2.0 Embeddings",
      "authors": [
        "L Pepino",
        "P Riera",
        "L Ferrer"
      ],
      "year": "2021",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "20",
      "title": "Conceptualization, measurement, and improvement of classroom processes: Standardized observation can leverage capacity",
      "authors": [
        "R Pianta",
        "B Hamre"
      ],
      "year": "2009",
      "venue": "Educational researcher"
    },
    {
      "citation_id": "21",
      "title": "Classroom Assessment Scoring Sys-tem™: Manual K-3",
      "authors": [
        "R Pianta",
        "K La Paro",
        "B Hamre"
      ],
      "year": "2008",
      "venue": "Classroom Assessment Scoring Sys-tem™: Manual K-3"
    },
    {
      "citation_id": "22",
      "title": "Toward automated classroom observation: Multimodal machine learning to estimate class positive climate and negative climate",
      "authors": [
        "A Ramakrishnan",
        "B Zylich",
        "E Ottmar",
        "J Locasale-Crouch",
        "J Whitehill"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "23",
      "title": "A circumplex model of affect",
      "authors": [
        "J Russell"
      ],
      "year": "1980",
      "venue": "Journal of personality and social psychology"
    },
    {
      "citation_id": "24",
      "title": "An automated approach for analysing students feedback using sentiment analysis techniques",
      "authors": [
        "S Sadriu",
        "K Nuci",
        "A Imran",
        "I Uddin",
        "M Sajjad"
      ],
      "year": "2021",
      "venue": "Mediterranean Conference on Pattern Recognition and Artificial Intelligence"
    },
    {
      "citation_id": "25",
      "title": "Teaching effectiveness research in the past decade: The role of theory and research design in disentangling meta-analysis results",
      "authors": [
        "T Seidel",
        "R Shavelson"
      ],
      "year": "2007",
      "venue": "Review of educational research"
    },
    {
      "citation_id": "26",
      "title": "Peer: Empowering writing with large language models",
      "authors": [
        "K Seßler",
        "T Xiang",
        "L Bogenrieder",
        "E Kasneci"
      ],
      "year": "2023",
      "venue": "European Conference on Technology Enhanced Learning"
    },
    {
      "citation_id": "27",
      "title": "Multimodal engagement analysis from facial videos in the classroom",
      "authors": [
        "Ö Sümer",
        "P Goldberg",
        "S D'mello",
        "P Gerjets",
        "U Trautwein",
        "E Kasneci"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "28",
      "title": "Teachers' perception in the classroom",
      "authors": [
        "Ö Sümer",
        "P Goldberg",
        "K Sturmer",
        "T Seidel",
        "P Gerjets",
        "U Trautwein",
        "E Kasneci"
      ],
      "year": "2018",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops"
    },
    {
      "citation_id": "29",
      "title": "Evaluating the explainers: black-box explainable machine learning for student success prediction in moocs",
      "authors": [
        "V Swamy",
        "B Radmehr",
        "N Krco",
        "M Marras",
        "T Käser"
      ],
      "year": "2022",
      "venue": "Evaluating the explainers: black-box explainable machine learning for student success prediction in moocs",
      "arxiv": "arXiv:2207.00551"
    },
    {
      "citation_id": "30",
      "title": "Estimation of continuous valence and arousal levels from faces in naturalistic conditions",
      "authors": [
        "A Toisoul",
        "J Kossaifi",
        "A Bulat",
        "G Tzimiropoulos",
        "M Pantic"
      ],
      "year": "2021",
      "venue": "Nature Machine Intelligence"
    },
    {
      "citation_id": "31",
      "title": "Is chatgpt a good teacher coach? measuring zero-shot performance for scoring and providing actionable insights on classroom instruction",
      "authors": [
        "R Wang",
        "D Demszky"
      ],
      "year": "2023",
      "venue": "Is chatgpt a good teacher coach? measuring zero-shot performance for scoring and providing actionable insights on classroom instruction",
      "arxiv": "arXiv:2306.03090"
    },
    {
      "citation_id": "32",
      "title": "Automated evaluation of classroom instructional support with llms and bows: Connecting global predictions to specific feedback",
      "authors": [
        "J Whitehill",
        "J Locasale-Crouch"
      ],
      "year": "2023",
      "venue": "Automated evaluation of classroom instructional support with llms and bows: Connecting global predictions to specific feedback",
      "arxiv": "arXiv:2310.01132"
    }
  ]
}