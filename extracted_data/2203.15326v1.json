{
  "paper_id": "2203.15326v1",
  "title": "Speech Emotion Recognition With Co-Attention Based Multi-Level Acoustic Information",
  "published": "2022-03-29T08:17:28Z",
  "authors": [
    "Heqing Zou",
    "Yuke Si",
    "Chen Chen",
    "Deepu Rajan",
    "Eng Siong Chng"
  ],
  "keywords": [
    "Speech emotion recognition",
    "Multimodal fusion",
    "Multi-level acoustic information",
    "Co-attention mechanism"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Speech Emotion Recognition (SER) aims to help the machine to understand human's subjective emotion from only audio information. However, extracting and utilizing comprehensive in-depth audio information is still a challenging task. In this paper, we propose an end-to-end speech emotion recognition system using multi-level acoustic information with a newly designed co-attention module. We firstly extract multi-level acoustic information, including MFCC, spectrogram, and the embedded high-level acoustic information with CNN, BiL-STM and wav2vec2, respectively. Then these extracted features are treated as multimodal inputs and fused by the proposed co-attention mechanism. Experiments are carried on the IEMOCAP dataset, and our model achieves competitive performance with two different speaker-independent crossvalidation strategies. Our code is available on GitHub.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Automatic recognition of emotions finds several applications such as human-computer interaction  [1]  and surveillance  [2] . Some researchers propose to combine acoustic information with textual information and learn high-level context information to help make the final emotion prediction  [3] . However, the corresponding transcriptions are not always available for most emotion recognition applications. Besides, the generated text with a current automatic speech recognition (ASR) system could also introduce word recognition errors and interfere with the emotion recognition task. Emotion perception from only audio signals is much easier to implement compared with multimodal emotion recognition with additional textual and visual signals because single audio data is easier to be obtained. Transforming the speech emotion recognition (SER) problem into a multi-level fusion problem by integrating multiple acoustic information is a potentially effective method to utilize the complete audio information.\n\nThe vast majority of SER problems involve extracting key audio features like Mel-frequency Cepstral Coefficient (MFCC), Constant-Q Transform (CQT) or constructing the corresponding spectrogram image to treat the problem as an image classification problem  [4] . Both MFCC and spectrogram reflect more information of a speech signal in the frequency domain. MFCC can be regarded as a low-level feature based on human knowledge. Spectrogram can be further processed to obtain high-level information through a deep neural network. These methods are intuitive and simple but usually ignore time-domain information of the speech signal.\n\nVarious encoders with different architecture details are designed for different acoustic signals, e.g., CNN for spectrogram and CNN/LSTM for MFCC. The acoustic information is mined using a series of CNNs with different kernel sizes in  [3] . Some methods propose to introduce a combination of networks to extract acoustic information, e.g.,  [5]  combine LSTM and Gated Multi-features Unit (GMU) to extract both static and dynamic speech signals. In  [6] , Gao et al propose a domain-adversarial auto-encoder to extract discriminative representations with pre-trained spectrogram information. Extracting features from different sources requires the corresponding source-specific neural networks.\n\nDifferent types of attention mechanisms have been proposed for processing the extracted features, like the commonly used self-attention  [5, 7]  and cross-modal attention  [8] . For the models with more complex input combinations, new attention mechanisms are introduced.  [9]  fuses two modalities and then combines the result with another modality using the proposed attentive modality-hop mechanism. In  [10] , a hierarchical attention-based temporal convolutional network is designed to fuse the inter-channel and intra-channel features for spectrogram images.\n\nIn this paper, we introduce three different encoders for multiple levels of acoustic information: CNN for spectrogram, BiLSTM for MFCC and the transformer-based acoustic extracting network wav2vec2  [11]  for raw audio signals.\n\nWith the designed co-attention module, we optimize to get the final wav2vec2 embedding (W2E) after weighting each frame by utilizing the effective information extracted from MFCC and spectrogram features. We concatenate all three extracted features and make the final emotion prediction with this finally fused information. The proposed model surpasses current competitive models on the widely used IEMOCAP dataset with the leave-one-speaker-out and leave-one-sessionout cross-validation strategy.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Proposed Method",
      "text": "In this section, we describe our co-attention-based SER system by integrating multiple acoustic information. Fig.  1  shows the overall structure of our proposed method. As illustrated, after splitting the raw audio utterance into several segments, three levels of acoustic information (MFCC, spectrogram and W2E) of a segment are introduced to the respective feature encoder networks and fused with the proposed co-attention method for the final emotion recognition.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Model Overview",
      "text": "We denote the MFCC, spectrogram and wav2vec2, which are obtained from the same audio segment, as x m ∈ R Tm×Dm , x s ∈ R Ts×Ds and x w ∈ R Tw×1 , respectively. The extracted MFCC features x m and spectrogram features x s are concatenated and transformed with linear layers to get the weights for different frames of wav2vec outputs x w . After multiplication with these generated weights, we get the final W2E vector from the raw wav2vec outputs. The final obtained W2E x w are concatenated with the previous MFCC features x m and spectrogram features x s for the final emotion recognition task. The generated weights of wav2vec frames from MFCC and spectrogram features and the final feature combination are denoted as x coatt and x , respectively. The target of the data is denoted by y and the final prediction is denoted as ŷ.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Learning With Multi-Level Acoustic Information",
      "text": "Here we define the multi-level acoustic information as the combination of the human knowledge based low-level MFCC, deep learning based high-level spectrogram and W2E, thus to cover characteristics of the speech signal in both frequency and time domain. MFCC sequence is processed by a bidirectional LSTM with a dropout of 0.5 and flattened. The flattened vector is input to a linear layer with ReLU as an activation function with a dropout of 0.1 to obtain\n\nwhere\n\nThe spectrogram image is first reshaped for the pretrained AlexNet. A similar operation as for MFCC features is conducted on the AlexNet extracted features to obtain\n\nwhere\n\nRaw audio segments are sent directly to the corresponding wav2vec2 processor and wav2vec2 model to get the target raw wav2vec2 outputs as\n\nwhere x w ∈ R T w ×D w .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Co-Attention-Based Fusion",
      "text": "Considering that all three acoustic information sources play a similar role in the final emotion prediction, we use the correlation among them to guide the feature adaptation. Generally, the last frame or the average of the wav2vec2 output is used to represent the wav2vec2 features. It is obvious that we lose some effective information among the sequence dimension.\n\nTable  1 .\n\nPerformance comparison with 5-fold leaveone-session-out  [12, 7, 13]  and 10-fold leave-one-speaker-out  [14, 15, 16, 17, 18, 5]  cross-validation strategy on IEMOCAP.\n\nModel WA UA CNN-ELM+STC attention  [12]  61.32 60.43 Audio 25  [7]  60.64±1.96 61.32±2.26 IS09 -classification  [13]  68.1 63.8 Ours 69.80 71.05 RNN(prop.)-ELM  [14]  62.85 63.89 3D ACRNN  [15]  -64.74±5.44 BLSTM-CTC-CA  [16]  69.0 67.0 CNN GRU-SeqCap  [17]  72.73 59.71 CNN TF Att.pooling  [18]  71.75 68.06 HNSD  [5]  70.5 72.5 Ours 71.64 72.70\n\nHere, we introduce a kind of co-attention module to combine different frames of W2E with frame weights generated by the features of MFCC and spectrogram features. Firstly, we create a 1-dimension matrix from MFCC features x m and spectrogram features x s with a transformation layer given by\n\nwhere x att ∈ R 1×T w . The wav2vec2 outputs are multiplied with the previous generated weights to get the final weighted wav2vec2 features as\n\nwhere x w ∈ R D w . The final MFCC, spectrogram features and the weighted W2Es are concatenated and the speech emotion prediction is written as",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Objective",
      "text": "We use the commonly used cross-entropy loss for emotion classification and our objective is",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Experiment",
      "text": "Our proposed method is validated on the Interactive Emotional Dyadic Motion Capture (IEMOCAP)  [19]  dataset. In this section, we firstly introduce the dataset processing and audio sources used. Then we describe our experimental setup and the used validation strategy.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Datasets",
      "text": "IEMOCAP is a widely used emotion recognition dataset, recorded from ten different actors with audio, video, transcriptions and motion-capture information. Following others' work  [12, 7, 5] , we merge \"happy\" and \"excited\" into the category of \"happy\" and we consider the 5531 acoustic utterances from 4 emotions, angry, sad, happy and neutral.\n\nIn order to more accurately evaluate the performance of the model, we test our model with the 5-fold leave-one-sessionout and the 10-fold leave-one-speaker-out cross-validation strategy to generate the speaker-independent results. Also, we use the commonly used weighted accuracy (WA) and the unweighted accuracy (UA) as the evaluation metrics.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Experimental Setup",
      "text": "The used raw audio signals are sampled at 16 kHz. We spilt each audio utterance into several segments with a length of 3 seconds. When a segment is less than 3 seconds, a padding operation with 0 will be applied to this segment to keep the same length. The final prediction result of an audio utterance will be decided by all split segments from this utterance.\n\nTo make full use of different levels of speech information, we use three kinds of acoustic information in this SER task, MFCC, spectrogram and W2E. MFCC is a 40-dimension HTK-style Mel frequencies feature that taking into account the human auditory characteristics. It is extracted from the raw audio segments with librosa library  [20] . Spectrogram and the W2E are the deep features of audio signals. For spectrogram, a series of 40-ms Hamming windows with a hop length of 10 ms is applied and here we treat each windowed block as a frame. Each frame is transformed into a frequency domain with the Discrete Fourier Transform (DFT) of length 800. The first 200 DFT points are used as input spectrogram features. We finally get a spectrogram image with a size of 300*200 for each audio segment. Like the multimodal emotion recognition method  [21] , W2E are obtained from the pre-trained transformer-based wav2vec2 network. It is the reflection of the deep feature of speech in the time domain.\n\nThis SER system is implemented in PyTorch. The optimizer for the model is AdamW with a learning rate of 1e-5.\n\nThe training batch size is 64 and we set the early stopping setting as 8 epochs. Our code will be available on Github 1  .",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Results And Analysis",
      "text": "In this section, we present the model performance and design an ablation study to evaluate the influence of different inputs and used modules. We also visualize the extracted features of our model with t-distributed stochastic neighbour embedding (t-SNE) and the final normalized confusion matrix.   1 , our proposed method could achieve the best performance of 69.80% and 71.05% in terms of UA and WA for the leave-one-session-out validation strategy. And for the leave-one-speaker-out validation strategy, this method could also achieve the highest UA with a value of 72.70%. At the same time, its performance in WA is also competitive with a very similar result of 71.64% compared with UA on this unbalanced IEMOCAP dataset.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Ablation Study",
      "text": "Our proposed method utilizes the multiple levels of acoustic information, which contains the time domain and frequency domain. Table  2  shows the ablation study of model performance with different combinations of acoustic information. The first three rows are the emotion recognition results with only one level of acoustic information: MFCC, spectrogram and W2E. W2E provides better performance than the others for the final emotion recognition. The next three rows summarize the results from the combination of different features with W2E. The last four rows present the results of different combination features with the weighted W2E information after co-attention. The combination of multiple acoustic information and the proposed co-attention module are observed to contribute a lot to improve the whole model's performance.\n\nThe ablation study also shows the effectiveness of the proposed co-attention mechanism. From the last two rows of Table 2, the co-attention mechanism further optimizes the fused data and performs better than the direct concatenating operation with 4.42% and 4.89% improvement on WA and UA, respectively. As shown in Fig.  2 , the t-SNE visualization of the weighted W2E and final combined features after co-attention present a much more clear classification boundary when compared with the results of the unweighted W2E and final combined features without co-attention. From Fig.  3 , we also observe that the final classification results of the model with coattention are much better than the model without co-attention from the final normalized confusion matrix.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Conclusion",
      "text": "This paper proposes a co-attention-based SER system utilizing multi-level acoustic information. By designing different encoders, this model could get feature-specific information from the raw audio signals and enables complementary acoustic information for the SER problem. Also, this method introduces a co-attention based fusion method for getting weighted wav2vec2 embeddings and combining the final features. The experiments on the IEMOCAP dataset show that our proposed method achieves competitive performance with different speaker-independent cross-validation methods. In the future, we would like to combine the knowledge from different languages or datasets to improve the final performance.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The overall architecture of our proposed method.",
      "page": 2
    },
    {
      "caption": "Figure 3: , we also ob-",
      "page": 4
    },
    {
      "caption": "Figure 2: The t-SNE visualization of feature distribution. (a)",
      "page": 4
    },
    {
      "caption": "Figure 3: The normalized confusion matrix for the ﬁnal",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table 2: shows the ablation study of model perfor-",
      "data": [
        {
          "Model": "MFCC\nSpectrogram\nW2E",
          "WA\nUA": "57.60\n58.09\n62.13\n62.25\n64.03\n65.67"
        },
        {
          "Model": "MFCC+W2E (w/o co-att)\nSpectrogram+W2E (w/o co-att)\nMFCC+Spectrogram+W2E (w/o co-att)",
          "WA\nUA": "64.62\n65.93\n66.20\n67.22\n67.22\n67.81"
        },
        {
          "Model": "W2E (w/ co-att)\nMFCC+W2E (w/ co-att)\nSpectrogram+W2E (w/ co-att)\nMFCC+Spectrogram+W2E (w/ co-att)",
          "WA\nUA": "67.55\n68.65\n69.11\n70.30\n70.05\n71.30\n71.64\n72.70"
        }
      ],
      "page": 4
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Emotion recognition in human-computer interaction",
      "authors": [
        "R Cowie",
        "E Douglas-Cowie",
        "N Tsapatsoulis",
        "G Votsis",
        "S Kollias",
        "W Fellenz",
        "J Taylor"
      ],
      "year": "2001",
      "venue": "IEEE Signal processing magazine"
    },
    {
      "citation_id": "3",
      "title": "Fear-type emotion recognition for future audio-based surveillance systems",
      "authors": [
        "C Clavel",
        "I Vasilescu",
        "L Devillers",
        "G Richard",
        "T Ehrette"
      ],
      "year": "2008",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "4",
      "title": "Efficient speech emotion recognition using multi-scale cnn and attention",
      "authors": [
        "Z Peng",
        "Y Lu",
        "S Pan",
        "Y Liu"
      ],
      "year": "2021",
      "venue": "2021 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "5",
      "title": "Multi-timescale convolution for emotion recognition from speech audio signals",
      "authors": [
        "E Guizzo",
        "T Weyde",
        "J Leveson"
      ],
      "year": "2020",
      "venue": "2020 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "6",
      "title": "Hierarchical network based on the fusion of static and dynamic features for speech emotion recognition",
      "authors": [
        "Q Cao",
        "M Hou",
        "B Chen",
        "Z Zhang",
        "G Lu"
      ],
      "year": "2021",
      "venue": "2021 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "7",
      "title": "Domainadversarial autoencoder with attention based feature level fusion for speech emotion recognition",
      "authors": [
        "Y Gao",
        "J Liu",
        "L Wang",
        "J Dang"
      ],
      "year": "2021",
      "venue": "2021 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "8",
      "title": "Emotion recognition by fusing time synchronous and time asynchronous representations",
      "authors": [
        "W Wu",
        "C Zhang",
        "P Woodland"
      ],
      "year": "2021",
      "venue": "2021 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "9",
      "title": "Multimodal crossand self-attention network for speech emotion recognition",
      "authors": [
        "L Sun",
        "B Liu",
        "J Tao",
        "Z Lian"
      ],
      "year": "2021",
      "venue": "2021 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "10",
      "title": "Attentive modality hopping mechanism for speech emotion recognition",
      "authors": [
        "S Yoon",
        "S Dey",
        "H Lee",
        "K Jung"
      ],
      "year": "2020",
      "venue": "2020 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "11",
      "title": "Hierarchical attention-based temporal convolutional networks for eeg-based emotion recognition",
      "authors": [
        "C Li",
        "B Chen",
        "Z Zhao",
        "N Cummins",
        "B Schuller"
      ],
      "year": "2021",
      "venue": "2021 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "12",
      "title": "wav2vec 2.0: A framework for self-supervised learning of speech representations",
      "authors": [
        "A Baevski",
        "H Zhou",
        "A Mohamed",
        "M Auli"
      ],
      "year": "2020",
      "venue": "wav2vec 2.0: A framework for self-supervised learning of speech representations",
      "arxiv": "arXiv:2006.11477"
    },
    {
      "citation_id": "13",
      "title": "Representation learning with spectro-temporal-channel attention for speech emotion recognition",
      "authors": [
        "L Guo",
        "L Wang",
        "C Xu",
        "J Dang",
        "E Chng",
        "H Li"
      ],
      "year": "2021",
      "venue": "2021 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "14",
      "title": "Selfattention for speech emotion recognition",
      "authors": [
        "L Tarantino",
        "P Garner",
        "A Lazaridis"
      ],
      "year": "2019",
      "venue": "Interspeech"
    },
    {
      "citation_id": "15",
      "title": "High-level feature representation using recurrent neural network for speech emotion recognition",
      "authors": [
        "J Lee",
        "I Tashev"
      ],
      "year": "2015",
      "venue": "High-level feature representation using recurrent neural network for speech emotion recognition"
    },
    {
      "citation_id": "16",
      "title": "3-d convolutional recurrent neural networks with attention model for speech emotion recognition",
      "authors": [
        "M Chen",
        "X He",
        "J Yang",
        "H Zhang"
      ],
      "year": "2018",
      "venue": "IEEE Signal Processing Letters"
    },
    {
      "citation_id": "17",
      "title": "Attention-enhanced connectionist temporal classification for discrete speech emotion recognition",
      "authors": [
        "Z Zhao",
        "Z Bao",
        "Z Zhang",
        "N Cummins",
        "H Wang",
        "B Schuller"
      ],
      "year": "2019",
      "venue": "Attention-enhanced connectionist temporal classification for discrete speech emotion recognition"
    },
    {
      "citation_id": "18",
      "title": "Speech emotion recognition using capsule networks",
      "authors": [
        "X Wu",
        "S Liu",
        "Y Cao",
        "X Li",
        "J Yu",
        "D Dai",
        "X Ma",
        "S Hu",
        "Z Wu",
        "X Liu"
      ],
      "year": "2019",
      "venue": "2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "19",
      "title": "An attention pooling based representation learning method for speech emotion recognition",
      "authors": [
        "P Li",
        "Y Song",
        "I Mcloughlin",
        "W Guo",
        "L.-R Dai"
      ],
      "year": "2018",
      "venue": "An attention pooling based representation learning method for speech emotion recognition"
    },
    {
      "citation_id": "20",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "21",
      "title": "librosa: Audio and music signal analysis in python",
      "authors": [
        "B Mcfee",
        "C Raffel",
        "D Liang",
        "D Ellis",
        "M Mcvicar",
        "E Battenberg",
        "O Nieto"
      ],
      "year": "2015",
      "venue": "Proceedings of the 14th python in science conference"
    },
    {
      "citation_id": "22",
      "title": "Speech emotion recognition with multi-task learning",
      "authors": [
        "X Cai",
        "J Yuan",
        "R Zheng",
        "L Huang",
        "K Church"
      ],
      "year": "2021",
      "venue": "Speech emotion recognition with multi-task learning"
    }
  ]
}