{
  "paper_id": "2310.14614v1",
  "title": "Efficient Cross-Task Prompt Tuning For Few-Shot Conversational Emotion Recognition",
  "published": "2023-10-23T06:46:03Z",
  "authors": [
    "Yige Xu",
    "Zhiwei Zeng",
    "Zhiqi Shen"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Emotion Recognition in Conversation (ERC) has been widely studied due to its importance in developing emotion-aware empathetic machines. The rise of pre-trained language models (PLMs) has further pushed the limit of ERC performance. However, most recent works on ERC using PLMs are heavily data-driven and require fine-tuning the entire PLMs. To improve both sample and computational efficiency, we propose a derivative-free optimization method called Cross-Task Prompt Tuning (CTPT) for few-shot conversational emotion recognition. Unlike existing methods that learn independent knowledge from individual tasks, CTPT leverages sharable cross-task knowledge by exploiting external knowledge from other source tasks to improve learning performance under the few-shot setting. Moreover, CTPT only needs to optimize a vector under the low intrinsic dimensionality without gradient, which is highly training-efficient compared with existing approaches. Experiments on five different contextual conversation datasets demonstrate that our CTPT method has superior results on both few-shot scenarios and zero-shot transfers.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Emotion Recognition in Conversation (ERC) detects emotion categories (e.g., netural, happiness, sadness) of each utterance in a given textual conversation. As pre-trained language models (PLMs)  (Devlin et al., 2019; Liu et al., 2019)  have brought a huge breakthrough to natural language processing (NLP), PLMs are also increasingly employed by ERC models as encoders to improve recognition performance  (Zhong et al., 2019; Kim and Vossen, 2021; Chudasama et al., 2022) . However, the performance gain from PLMs is often achieved at the exorbitant cost of expensive training and fine-tuning processes. PLM-based ERC models tend to suffer from poor sample efficiency and computational efficiency as they often involve a large number of training examples and millions of trainable parameters, which potentially prevents current PLM-based models from achieving their best performance in low-resource scenarios.\n\nFew-shot learning techniques  (Motiian et al., 2017; Wang et al., 2021)  hold the promise to improve both sample and computation efficiency for deploying PLMs in new scenarios where data can be limited. Recently, prompt tuning  (Li and Liang, 2021; Lester et al., 2021) , which trains a set of discrete or continuous prompt embeddings conditioned on a frozen PLM, has shown promising results in few-shot learning settings  (Gao et al., 2021; Gu et al., 2022; Guo et al., 2022) . The prompt can be regarded as a way to retrieve the knowledge already memorized in the PLM. The effectiveness of prompts lies in their capability to adapt to new tasks while preserving the knowledge embedded in PLMs, without causing overfitting issues that can arise from full-model fine-tuning  (Liu et al., 2021) .\n\nHowever, most recent works on ERC are largescale data-driven that focus on the full dataset setting  (Lee and Choi, 2021; Song et al., 2022a) .  Guibon et al. (2022)  firstly explore the few-shot ERC task, but their setting is not strictly few-shot, which may lead to a variety of examples for each label. For example, their training set contains more than k examples for each label under the k-shot setting.\n\nTo this end, we strictly define the ERC task under the few-shot setting and propose a Cross-Task Prompt Tuning (CTPT) solution. Existing prompt tuning methods independently learn task-specific knowledge from each task, yet such knowledge is often very limited in the few-shot setting. Our proposed CTPT leverages cross-task knowledge by exploiting external knowledge from other source tasks to improve learning performance under the few-shot setting. The cross-task knowledge from other source tasks can be divided into two parts: external task-specific knowledge and emotional knowledge. For external task-specific knowledge, we utilize a multi-head attention module  (Vaswani et al., 2017)  to learn knowledge from source tasks. For emotional knowledge, we combine the same emotion within different textual categories from different tasks and then reformulate the verbalizer that decodes the output to the label distribution.\n\nOne limitation of prompt tuning is that it involves backpropagating the loss through all the Transformer layers of a PLM for every batch even though we freeze the PLM, which can lead to computational inefficiency. To further improve the computational efficiency of PLM-based ERC models, we optimize a vector with intrinsic dimensionality  (Li et al., 2018)  instead of the whole continuous prompt, which reduces the number of parameters from hundreds of thousands to about 1,000. Following  Sun et al. (2022) , we use a Covariance Matrix Adaptation Evolution Strategy (CMA-ES)  (Hansen and Ostermeier, 2001; Hansen et al., 2003)  to optimize the parameters, which is derivative-free. With the derivative-free optimization, we separate our approach from the PLM and do not require backpropagation for parameter learning.\n\nCompared with single-task prompt tuning, our proposed CTPT method can utilize external knowledge from other tasks to boost the performance of the target task. Experiments under the fewshot scenarios and the zero-shot transfer show that CTPT can obtain a better result. In addition to this, our proposed CTPT is derivative-free, which does not need backpropagation. Compared with derivative-based backpropagation, the experiment result shows that CTPT can obtain comparable results without derivative information.\n\nThe main contributions of this paper are summarized as follows:\n\n(1) To the best of our knowledge, we are the first to strictly define and tackle the few-shot setting for the ERC task. We propose a Cross-Task Prompt Tuning (CTPT) method that can efficiently learn and utilize cross-task knowledge.\n\n(2) To improve the training efficiency, we use the derivative-free optimization algorithm to optimize the parameter. It skips the backpropagation stage and does not require gradient information.\n\n(3) Our proposed CTPT only needs to optimize about 1,000 parameters, which is much more training-efficient than any other existing PLMbased ERC method.\n\n(4) Our proposed CTPT is trained under the few-shot setting, which is sample-efficient. CTPT can also obtain a better experimental result on zero-shot transfer, which can be deployed in new scenarios with limited training examples.\n\n2 Related Works",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Emotion Recognition In Conversation",
      "text": "Early studies on ERC mainly utilized audio-based features  (Lee and Narayanan, 2005)  or lexiconbased features  (Devillers and Vidrascu, 2006) . Recently, there are a series of deep learning approaches focused on emotion recognition in conversational videos or multi-turn Tweets  (Hazarika et al., 2018; Zahiri and Choi, 2018; Zhong et al., 2019; Ishiwatari et al., 2020) . In recent years, PLM has been increasingly applied in ERC models  (Lee and Choi, 2021; Shen et al., 2021; Song et al., 2022a) . A commonality among these prior approaches is their shared approach on the integration of various forms of external knowledge to enhance emotion detection, including knowledge from knowledge base  (Zhong et al., 2019) , knowledge from commonsense  (Ghosal et al., 2020; Yi et al., 2022) , knowledge from multi-modal  (Li et al., 2022) , and inherent knowledge within PLM  (Kim and Vossen, 2021) . Unlike existing methods that focus on enriching task-specific knowledge only, we also explore sharable cross-task knowledge from other source tasks.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Prompt Tuning",
      "text": "Despite the success of  GPT-3 (Brown et al., 2020)  with 175 billion parameters, it has become increasingly difficult and expensive to utilize such big language models. One possible solution to leverage large pre-trained models is parameter-efficient tuning methods, such as prompt-tuning  (Lester et al., 2021; Li and Liang, 2021) . In prompt tuning, downstream tasks are reformulated as a language modelling task with the help of a textual prompt. For example, a classification task that aims to predict the emotion category of a given sentence can be reformulated as: \"I felt so [MASK], [X]\". Here [X] is the given sentence, [MASK] is the mask token that PLM needs to predict, and \"I felt so [MASK]\" is the template of prompting. The aforementioned prompt consists of discrete tokens, which are also known as a hard prompt. There is another prompt named soft prompt  (Qin and Eisner, 2021) , which consists of continuous embeddings. Recently, prompt tuning has been proven successful in both few-shot scenarios  (Gu et al., 2022; Vu et al., 2022)  and zero-shot transfer  (Guo et al., 2022) .\n\nAlthough prompt tuning has brought success in many NLP domains such as text classification  (Gao et al., 2021) , question answering  (Yang et al., 2022) , and commonsense reasoning  (Liu et al., 2022) ,  Yi et al. (2022)  first practising prompt tuning on the ERC task that utilizes learnable continuous prompt to model the relationship between contextual information and commonsense knowledge. In this paper, we utilize learnable prompts to model the relationship between emotional categories among different tasks under the few-shot setting.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Derivative-Free Optimization",
      "text": "Different from many neural networks that require gradient information for backpropagation, derivative-free optimization (DFO) algorithms aim to obtain optimal solutions without derivative information. Most DFO algorithms  (Hansen et al., 2003; Shahriari et al., 2016)  are under the sampling-andupdating structure, which firstly samples a solution x and then optimize the parameters via the function values f (x). In recent years, DFO algorithms have been applied to many downstream areas, such as automatic machine learning  (Snoek et al., 2012) , and reinforcement learning  (Salimans et al., 2017) . More recently,  Sun et al. (2022)  proposed a DFO method to optimize continuous prompts without gradient information. In this paper, we further extend the DFO method to optimize not only the continuous prompt but also the parameters for crosstask learning.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Methodology",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Problem Definition And Notations",
      "text": "In this section, we will briefly define the emotion recognition in conversation (ERC) task and the ERC task under the few-shot setting.\n\nThe full dataset setting contains the conversation set\n\nMore specifically, the input of the task is a conversational content\n\nThe ground-truth is also an emotional category set\n\nThe target of the task is to predict the emotional category for each utterance to maximum match the ground-truth emotion category. In the i-th conversation,\n\n] indicates the j-th utterance, x i j,k indicates the k-th token in the j-th utterance, |x i j | indicates the sequence length of the j-th utterance, and |x i | is the number of utterances in the i-th conversation.\n\nDataset under the few-shot setting is a subset that under the full dataset setting. The new dataset under the k-shot setting is marked as {(x, y)|x ∈ X , y ∈ Ŷ} k , where X and Ŷ indicate the input sequence as well as the emotion category for the new training set. Correspondingly, the predicted emotion category is Ê. Here k-shot indicates that there are k training examples for each emotion category. We randomly select the new dataset and keep it the same in the following experiments. Similar to the full dataset setting, the aims under the few-shot setting is to predict the emotion category êi .\n\nUnder the few-shot setting, the training set as well as the development set, are sampled randomly from the vanilla dataset of the full dataset setting, while the testing set keeps unchanged. At the beginning of the training stage, we first randomly select some training examples under the following rules: for each given emotion category (e.g., netural, happiness, sadness, etc.), we randomly select k utterances from the vanilla training set. In other words, we keep the textual conversational content but retain only one emotion category for one conversation. Therefore, for each training example, the input content remains the conversation content\n\nand the ground-truth becomes ŷi = y i j that j is randomly selected before the training stage.\n\nAll experiments are conducted on few-shot settings (k = 16). For each dataset, we sample the subset of the training set and the development set and keep the testing set unchanged. For a fair comparison, all baselines and CTPT are trained by the same training set.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Overview Of The Model",
      "text": "In this section, we will briefly introduce the overview of the whole model. The input of CTPT is a textual sequence that contains the conversational context, and the output of CTPT is an emotion label. CTPT can be mainly divided into three parts: taskspecific prompt tuning (TSPT), cross-task prompt learning (CTPL), and cross-task prompt observation (CTPO). The overall architecture is shown in  The first part of CTPT is TSPT. In this part, we learn task-specific knowledge from different source tasks 1 , which is harnessed later to the target task 2 . Then, we have CTPL that employs an attention module to learn the external task-specific knowledge learned by TSPT from source tasks and the emotional knowledge from commonsense. Lastly, we have CTPO that utilizes a gate-like mechanism to summarize pertinent cross-task knowledge learned by CTPL. In summary, we have \"TSPT + CTPL + CTPO = CTPT\".\n\nWe concatenate the prompt with summarized cross-task knowledge pi as well as the input sequence x, and then pass it into the PLM. After we obtain the logits of the [MASK] token from PLM, we first decode the logits to word distribution, then map the word distribution to the emotion label, which is the output of the whole model.\n\nIn the derivative-free optimization, learnable parameters are contained a vector z with intrinsic dimensionality. The parameters in the neural network are computed by a linear projection from the vector z. We use the cross-entropy function to compute the loss between logits and the ground-truth label and then use Covariance Matrix Adaptation Evolution Strategy (CMA-ES)  (Hansen and Ostermeier, 2001; Hansen et al., 2003)  to optimize z.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Task-Specific Prompt Tuning",
      "text": "To address the computational efficiency problem, prompt tuning  (Li and Liang, 2021; Lester et al., 2021)  is a promising solution. Before CTPT, we use prompt tuning methods to learn knowledge for 1 Source tasks indicate tasks exclude the target task i. 2 Target task indicates the task i for evaluation. the target task, named task-specific prompt tuning (TSPT). Similar to the existing prompt tuning methods, we use soft prompt  (Qin and Eisner, 2021)  as the template, which can be formulated as:\n\n(1)\n\nwhere concat[•; •] is the concatenation, PLM(•) indicates a pre-trained language model, P (•) is the pattern projective function that converts the input sequence x into a phrased sequence with a [MASK] token. Here v(•) is the verbalizer injective function that decodes the label by the predicted distribution of the [MASK] token, which can be formulated as:\n\nwhere h = PLM(P (x)) is the hidden states outputed from a PLM, V i is the verbalizer set for target task i, and g(•) is a function trasnsforming the probability of v to the probability of the label. Here different task has different verbalizer. The soft prompt p ∈ R n×d in Eq (2) is a learnable matrix, and the objective function is:\n\nwhere L is the cross-entropy loss function, Ŷ and Ê are defined in Section 3.1, and n is the number of prompt tokens. The soft prompt can be regarded as a task-specific embedding that contains latent knowledge from the specific task.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Cross-Task Prompt Learning",
      "text": "With TSPT, we obtain a task-specific prompt p i t for the i-th task, which contains the task-specific knowledge of the target task. The independently learned task-specific knowledge is usually limited under the few-shot setting. One promising solution to address this problem is to introduce abundant sharable knowledge from other source tasks. The sharable knowledge includes external task-specific knowledge from source tasks and prior emotional knowledge learned from commonsense.\n\nExternal Task-Specific Knowledge Since taskspecific knowledge is often very limited in the fewshot setting, we introduce external task-specific knowledge from other source tasks. As aforementioned, the external task-specific knowledge is stored in the prompt learned by TSPT. Therefore, we modify the Equation (  2 ) for the i-th task as follows:\n\nwhere p i c indicates the cross-task prompt for the i-th task, and f (•) indicates the combination of task-specific prompt and cross-task prompt.\n\nInspired by the success of the attention mechanism, we utilize a multi-head attention module to decide what kind of knowledge should be collected from the source tasks. In multi-head attention, the query term is the task-specific prompt from the target task. The key term, as well as the value term, are the task-specific prompt from each source task. The whole module is formulated as:\n\nwhere MHA(•, •) indicates the multi-head attention, d indicates the dimension of hidden state. Here Q, K, V are:\n\nIn this module, W Q , W K , and W V are learnable parameters projected by a learnable vector z (More details about optimization are shown in Section 3.6).\n\nEmotional Knowledge In order to facilitate the ERC task, we also introduce emotional knowledge collected from commonsense in addition to external task-specific knowledge. Across different ERC datasets, varying labels might be used to denote identical emotional states. For example, DailyDialog uses \"happiness\" while MELD uses \"joy\" to represent the state of being happy. Given this understanding, we can learn cross-task emotional knowledge from disparate tasks encompassing the same emotional state, notwithstanding the divergence in emotion labels, by modifying the verbalizer. Therefore, Eq (3) can be modified as:\n\nwhere h is a mapping function that maps v from different task-specific verbalizers to a union verbalizer, and n is the number of tasks. With the new verbalizer, the model can learn knowledge from source tasks under the same emotion.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Cross-Task Prompt Observation",
      "text": "In cross-task prompt learning, we obtain a prompt p i c that contains the external task-specific knowledge collected from other source tasks and emotional knowledge collected from commonsense. Similarly to us,  Asai et al. (2022)  also utilizes an input-attention module to combine multiple source prompts. However,  Asai et al. (2022)  only considers how to learn prompts from source tasks. We empirically notice that part of the learned knowledge is beneficial to the target task while the other part is useless. To address this problem, we propose an extra stage: cross-task prompt observation.\n\nIn the cross-task prompt observation stage, more knowledge from the source task will be observed if it is helpful to improve the validation performance of the target task, while less in contrast. Formulatedly, we optimize a vector g as a gatelike controller via the derivative-free optimization mentioned in Section 3.6. Thus, the final prompt becomes:\n\nwhere ⊗ indicates the token-level element-wise multiple, I is an all one vector, and g i are learnable parameters for i-th task learned by the following objective function:",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Derivative-Free Optimization",
      "text": "According to  Li et al. (2018) , the intrinsic dimensionality is the minimum number of parameters needed to obtain comparable results. Sun et al. (  2022 ) also shows the efficiency of derivative-free optimization for intrinsic dimensionality vector in prompt tuning. To improve the computational efficiency, instead of the derivative-based backpropagation, we utilize a Covariance Matrix Adaptation Evolution Strategy (CMA-ES)  (Hansen and Ostermeier, 2001; Hansen et al., 2003)  to optimize a vector with intrinsic dimensionality. In each optimization step, the optimizer will first sample some solutions of the learnable vector z. Then we can calculate the loss of each solution that is used by the optimizer to suggest a new z.\n\nTo adapt our proposed CTPT, we modify the optimization step as follows:\n\nTask-Specific Prompt Tuning In this step, we follow  Sun et al. (2022)  to compute the taskspecific prompt p t by a learnable vector z:\n\nwhere A is randomly initialized and fixed, and p 0 is the initialized prompts from most widely-used tokens.\n\nCross-Task Prompt Learning In this step, we optimize a vector z ′ instead of the parameters in the multi-head attention module. Similar to the optimization of TSPT, we firstly project z ′ to the parameters space and then separate the parameter space by:\n\nwhere W = A ′ z ′ that A ′ is also randomly initialized and fixed. Then, we reshape the parameters and add a randomly initialized and fixed term:\n\nCross-Task Prompt Observation In this step, we optimize the vector z ′′ in the same way as the vector z being optimized in TSPT:\n\nwhere A ′′ and g 0 are fixed.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Experiments",
      "text": "",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Datasets",
      "text": "We conduct experiments on five widely-used public datasets to show the efficiency of CTPT, including: EC  (Chatterjee et al., 2019) , DailyDialog  (Li et al., 2017) , MELD  (Poria et al., 2019) ,  EmoryNLP (Zahiri and Choi, 2018) , and IEMO-CAP  (Busso et al., 2008) . Detailed statistics are shown in Table  1 . Though some of the datasets are multi-modality, we only utilize the textual information as the input for a fair comparison with baselines.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Baselines",
      "text": "For a comprehensive performance evaluation, we select the following four baselines for comparison:\n\nKET  (Zhong et al., 2019)  The KET is a knowledge-enriched transformer model specifically designed for Emotion Recognition in Conversation (ERC). It employs a knowledge base to infuse external knowledge, which is a representative baseline model before the PLM decade.  (Lee and Choi, 2021)  The TUCORE-GCN model is a turn-context aware graph convolutional network designed for ERC. It incorporates both a PLM encoder and a graph convolutional network, making it an exemplary representation of PLM-based baseline models.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Tucore-Gcn",
      "text": "EmotionFlow  (Song et al., 2022b)  The Emo-tionFlow is a PLM-based model with an additional CRF layer to capture the emotion transition probability among different utterances.\n\nSPCL  (Song et al., 2022a)  The SPCL is a PLMbased model using supervised prototypical contrastive learning loss, focusing primarily on imbalanced classification problems. It has achieved state-of-the-art results on MELD and EmoryNLP.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Implementation Details",
      "text": "In this paper, we use a soft prompt extended to the input and freeze the PLM. We use CMA-ES  (Hansen and Ostermeier, 2001; Hansen et al., 2003)  algorithm to optimize the parameters. We choose T5  (Raffel et al., 2020)  as our backbone model. All few-shot settings share the same training and development set with k = 16 following the settings of few-shot prompt tuning  (Gao et al., 2021; Sun et al., 2022) . Following  Sun et al. (2022)  and  Lester et al. (2021) , we use the soft prompt template and only train the continuous prompts extended to the input texts while freezing the PLM parameters. We utilize a Covariance Matrix Adaptation Evolution Strategy (CMA-ES)  (Hansen and Ostermeier, 2001; Hansen et al., 2003)  to optimize the parameters without gradient information.\n\nWe use the soft template extended to the input sequence with a length of 50. The last token of the template is set as <unk> so that the model can better predict the masked token. Since the task is reformulated as a generalization task with the textto-text format, we choose T5  (Raffel et al., 2020)  as our backbone model.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Evaluation Metric",
      "text": "For EC and DailyDialog, due to the imbalance distribution of categories (more than 80% examples are neutral emotion), we use micro-averaged F 1 score excluding neutral category following  Chatterjee et al. (2019) . For the rest three datasets, following  Majumder et al. (2019) , we use weighted macro-F 1 score. The overall evaluation setting is the same as  Zhong et al. (2019) .",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Results And Analysis",
      "text": "",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Main Results",
      "text": "We compare the performance of CTPT against the baselines aforementioned in Section 4.2. We first re-implement the baseline models and achieve the similar performance reported in the original paper. Then we modify the preprocessing code to let all baselines be trained under the same few-shot setting. The result under the few-shot setting is shown in Table  2 .\n\nCompared with the baseline models, taskspecific prompt tuning (TSPT) outperforms finetuning PLMs on most tasks under the few-shot setting. Meanwhile, benefiting from the cross-task knowledge, our proposed cross-task prompt tuning (CTPT) obtains an improvement compared with TSPT. Specifically, in addition to EC that TSPT has already obtained a high performance under the few-shot setting, CTPT brings significant improvement compared with TSPT, which demonstrates the effectiveness of utilizing crosstask knowledge. Since DailyDialog and MELD share the same emotion labels, CTPT obtains the most gain on these two datasets, which shows that our cross-task prompt tuning can learn emotional knowledge from the labels.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Model Analysis",
      "text": "To gain deeper insights into CTPT from diverse angles, we undertake analytical experiments in a few-shot setting, where k = 16, in this section.   Analysis in Source Data To explore the impact of external source data, we remove some external source tasks for MELD and IEMOCAP. For fair comparisons, we sample all the possible combinations of external source tasks 3  and report the average score. As shown in Figure  2 , both MELD and IEMOCAP perform better when increasing the number of external source tasks. However, though different combinations bring different improvements, the average score improvement is likely linear.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Analysis In Training Stage",
      "text": "",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Analysis In Pipeline Component",
      "text": "In this section, we examine the influence of various components within our CTPT framework. Initially, we carried out ablation experiments to investigate the effect of integrating emotional knowledge into CTPL. As depicted in Table  4 , the incorporation of emotional knowledge enhances the performance of the downstream task, highlighting its importance. Further, to understand the contributions of CTPL and CTPO, we performed ablation experiments by omitting these components. Comparing the result of \"TSPT + CTPL\" with \"CTPT\" in Table  2 , we can conclude that CTPO is important to CTPT since the validation performance will be significantly degraded without CTPO. Though the experiment result shows that CTPL is negative to the validation performance in some tasks like EC and IEMOCAP, adding CTPO will be positive. In summary, while CTPL's cross-task knowledge might not consistently enhance validation performance, due to the potential inclusion of redundant information, its combination with CTPO proves advantageous.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Analysis In Different Backbones",
      "text": "As highlighted in Section 4.3, we selected T5, one of the most emblematic text-to-text generative language models, as our primary backbone model. To further validate the generalizability of CTPT, we conducted additional experiments using an encoder-only backbone language model, RoBERTa.\n\nAs evidenced in Table  5 , when employing RoBERTa as the backbone model, CTPT achieves results that surpass the TSPT baseline, comparable to those obtained with T5. This underscores the robust generalizability of our proposed CTPT across various backbone language models. It also suggests the potential for consistent enhancement in downstream task performance with the adoption of even more advanced backbone models.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Zero-Shot Transfer",
      "text": "In real-world scenarios, annotated training examples are not always available. Therefore, it is worthwhile exploring the zero-shot generalization ability of CTPT. In this subsection, we conduct experiments under the zero-shot setting that train the prompt by source task and evaluate the target task while excluding the external task-specific prompt from the target task.\n\nAs shown in Table  6 , CTPT performs surprisingly under the zero-shot transfer. It outperforms baseline methods under the few-shot setting in EC, DailyDialog and IEMOCAP. Compared with the few-shot result of TSPT, CTPT zero-shot obtains better performance in DailyDialog and a sightly degradation in MELD and IEMOCAP.\n\nDue to the similarity among the first three tasks (EC, DailyDialog, and MELD), the prompt trained by these three tasks can be easily transferred with each other, which almost achieves the result of TSPT under 16-shot. Meanwhile, since the conversations in EmoryNLP contain fine-grained and more complex emotions, prompts learned from other tasks can hardly be transferred to EmoryNLP. Specifically, the results of zero-shot transfer from the first three tasks to the rest two tasks are poor, and vice versa. In other words, the more similarity the two tasks have, the better zero-shot transfer performance they obtain. In summary, the experiment result shows that CTPT has a good generalization ability in zero-shot transfer.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Conclusion",
      "text": "In this paper, we strictly define the task of the few-shot setting for ERC and propose a cross-task prompt tuning (CTPT) method to tackle this problem utilizing the cross-task knowledge. CTPT learns from external task-specific knowledge from other tasks and emotional knowledge from commonsense and then summarizes the learned crosstask knowledge to improve the validation performance. We use a derivative-free optimization method to optimize the parameters without gradient information, which skips the backpropagation stage. Experiments on ERC benchmarks show that CTPT can outperform baseline models in the fewshot setting and obtain a surprising result in the zero-shot transfer. In summary, CTPT is trainingefficient that includes: (1) sample-efficiency: it is trained by few-shot training examples, and (2) computational-efficiency: it tunes only about 1,000 parameters with derivative-free algorithms that skip the backpropagation.",
      "page_start": 13,
      "page_end": 13
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Overall architecture of our proposed CTPT model.",
      "page": 4
    },
    {
      "caption": "Figure 1: The first part of CTPT is TSPT. In this part, we",
      "page": 4
    },
    {
      "caption": "Figure 2: Impacts of removing external source tasks for",
      "page": 7
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Nanyang Technological University, Singapore": "yige002@e.ntu.edu.sg, {zhiwei.zeng,zqshen}@ntu.edu.sg"
        },
        {
          "Nanyang Technological University, Singapore": "and computational efficiency as they often involve"
        },
        {
          "Nanyang Technological University, Singapore": "a large number of training examples and millions"
        },
        {
          "Nanyang Technological University, Singapore": ""
        },
        {
          "Nanyang Technological University, Singapore": "of trainable parameters, which potentially prevents"
        },
        {
          "Nanyang Technological University, Singapore": ""
        },
        {
          "Nanyang Technological University, Singapore": "current PLM-based models from achieving their"
        },
        {
          "Nanyang Technological University, Singapore": ""
        },
        {
          "Nanyang Technological University, Singapore": "best performance in low-resource scenarios."
        },
        {
          "Nanyang Technological University, Singapore": ""
        },
        {
          "Nanyang Technological University, Singapore": "Few-shot\nlearning techniques\n(Motiian et al.,"
        },
        {
          "Nanyang Technological University, Singapore": ""
        },
        {
          "Nanyang Technological University, Singapore": "2017; Wang et al., 2021) hold the promise to im-"
        },
        {
          "Nanyang Technological University, Singapore": "prove both sample and computation efficiency for"
        },
        {
          "Nanyang Technological University, Singapore": ""
        },
        {
          "Nanyang Technological University, Singapore": "deploying PLMs in new scenarios where data can"
        },
        {
          "Nanyang Technological University, Singapore": ""
        },
        {
          "Nanyang Technological University, Singapore": "be limited. Recently, prompt tuning (Li and Liang,"
        },
        {
          "Nanyang Technological University, Singapore": ""
        },
        {
          "Nanyang Technological University, Singapore": "2021; Lester et al., 2021), which trains a set of"
        },
        {
          "Nanyang Technological University, Singapore": ""
        },
        {
          "Nanyang Technological University, Singapore": "discrete or continuous prompt embeddings condi-"
        },
        {
          "Nanyang Technological University, Singapore": ""
        },
        {
          "Nanyang Technological University, Singapore": "tioned on a frozen PLM, has shown promising re-"
        },
        {
          "Nanyang Technological University, Singapore": ""
        },
        {
          "Nanyang Technological University, Singapore": "sults in few-shot learning settings (Gao et al., 2021;"
        },
        {
          "Nanyang Technological University, Singapore": "Gu et al., 2022; Guo et al., 2022).\nThe prompt"
        },
        {
          "Nanyang Technological University, Singapore": "can be regarded as a way to retrieve the knowledge"
        },
        {
          "Nanyang Technological University, Singapore": ""
        },
        {
          "Nanyang Technological University, Singapore": "already memorized in the PLM. The effectiveness"
        },
        {
          "Nanyang Technological University, Singapore": ""
        },
        {
          "Nanyang Technological University, Singapore": "of prompts lies in their capability to adapt to new"
        },
        {
          "Nanyang Technological University, Singapore": ""
        },
        {
          "Nanyang Technological University, Singapore": "tasks while preserving the knowledge embedded in"
        },
        {
          "Nanyang Technological University, Singapore": ""
        },
        {
          "Nanyang Technological University, Singapore": "PLMs, without causing overfitting issues that can"
        },
        {
          "Nanyang Technological University, Singapore": ""
        },
        {
          "Nanyang Technological University, Singapore": "arise from full-model fine-tuning (Liu et al., 2021)."
        },
        {
          "Nanyang Technological University, Singapore": ""
        },
        {
          "Nanyang Technological University, Singapore": "However, most recent works on ERC are large-"
        },
        {
          "Nanyang Technological University, Singapore": ""
        },
        {
          "Nanyang Technological University, Singapore": "scale data-driven that focus on the full dataset set-"
        },
        {
          "Nanyang Technological University, Singapore": ""
        },
        {
          "Nanyang Technological University, Singapore": "ting (Lee and Choi, 2021; Song et al., 2022a). Gui-"
        },
        {
          "Nanyang Technological University, Singapore": ""
        },
        {
          "Nanyang Technological University, Singapore": "bon et al. (2022) firstly explore the few-shot ERC"
        },
        {
          "Nanyang Technological University, Singapore": "task, but their setting is not strictly few-shot, which"
        },
        {
          "Nanyang Technological University, Singapore": ""
        },
        {
          "Nanyang Technological University, Singapore": "may lead to a variety of examples for each label."
        },
        {
          "Nanyang Technological University, Singapore": "For example, their training set contains more than"
        },
        {
          "Nanyang Technological University, Singapore": "k examples for each label under the k-shot setting."
        },
        {
          "Nanyang Technological University, Singapore": "To this end, we strictly define the ERC task un-"
        },
        {
          "Nanyang Technological University, Singapore": "der the few-shot setting and propose a Cross-Task"
        },
        {
          "Nanyang Technological University, Singapore": "Prompt Tuning (CTPT) solution. Existing prompt"
        },
        {
          "Nanyang Technological University, Singapore": "tuning methods independently learn task-specific"
        },
        {
          "Nanyang Technological University, Singapore": "knowledge from each task, yet such knowledge"
        },
        {
          "Nanyang Technological University, Singapore": "is often very limited in the few-shot setting. Our"
        },
        {
          "Nanyang Technological University, Singapore": "proposed CTPT leverages cross-task knowledge by"
        },
        {
          "Nanyang Technological University, Singapore": "exploiting external knowledge from other source"
        },
        {
          "Nanyang Technological University, Singapore": "tasks to improve learning performance under the"
        },
        {
          "Nanyang Technological University, Singapore": "few-shot setting. The cross-task knowledge from"
        },
        {
          "Nanyang Technological University, Singapore": "other source tasks can be divided into two parts:"
        },
        {
          "Nanyang Technological University, Singapore": "external\ntask-specific knowledge and emotional"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "knowledge. For external task-specific knowledge,": "we utilize a multi-head attention module (Vaswani",
          "shot setting, which is sample-efficient. CTPT can": "also obtain a better experimental result on zero-shot"
        },
        {
          "knowledge. For external task-specific knowledge,": "et al., 2017) to learn knowledge from source tasks.",
          "shot setting, which is sample-efficient. CTPT can": "transfer, which can be deployed in new scenarios"
        },
        {
          "knowledge. For external task-specific knowledge,": "For emotional knowledge, we combine the same",
          "shot setting, which is sample-efficient. CTPT can": "with limited training examples."
        },
        {
          "knowledge. For external task-specific knowledge,": "emotion within different\ntextual categories from",
          "shot setting, which is sample-efficient. CTPT can": ""
        },
        {
          "knowledge. For external task-specific knowledge,": "",
          "shot setting, which is sample-efficient. CTPT can": "2\nRelated Works"
        },
        {
          "knowledge. For external task-specific knowledge,": "different tasks and then reformulate the verbalizer",
          "shot setting, which is sample-efficient. CTPT can": ""
        },
        {
          "knowledge. For external task-specific knowledge,": "that decodes the output to the label distribution.",
          "shot setting, which is sample-efficient. CTPT can": ""
        },
        {
          "knowledge. For external task-specific knowledge,": "",
          "shot setting, which is sample-efficient. CTPT can": "2.1\nEmotion Recognition in Conversation"
        },
        {
          "knowledge. For external task-specific knowledge,": "One limitation of prompt\ntuning is\nthat\nit\nin-",
          "shot setting, which is sample-efficient. CTPT can": ""
        },
        {
          "knowledge. For external task-specific knowledge,": "",
          "shot setting, which is sample-efficient. CTPT can": "Early studies on ERC mainly utilized audio-based"
        },
        {
          "knowledge. For external task-specific knowledge,": "volves backpropagating the loss through all\nthe",
          "shot setting, which is sample-efficient. CTPT can": ""
        },
        {
          "knowledge. For external task-specific knowledge,": "",
          "shot setting, which is sample-efficient. CTPT can": "features (Lee and Narayanan, 2005) or\nlexicon-"
        },
        {
          "knowledge. For external task-specific knowledge,": "Transformer layers of a PLM for every batch even",
          "shot setting, which is sample-efficient. CTPT can": ""
        },
        {
          "knowledge. For external task-specific knowledge,": "",
          "shot setting, which is sample-efficient. CTPT can": "based features (Devillers and Vidrascu, 2006). Re-"
        },
        {
          "knowledge. For external task-specific knowledge,": "though we freeze the PLM, which can lead to com-",
          "shot setting, which is sample-efficient. CTPT can": ""
        },
        {
          "knowledge. For external task-specific knowledge,": "",
          "shot setting, which is sample-efficient. CTPT can": "cently,\nthere\nare\na\nseries of deep learning ap-"
        },
        {
          "knowledge. For external task-specific knowledge,": "putational inefficiency. To further improve the com-",
          "shot setting, which is sample-efficient. CTPT can": ""
        },
        {
          "knowledge. For external task-specific knowledge,": "",
          "shot setting, which is sample-efficient. CTPT can": "proaches focused on emotion recognition in con-"
        },
        {
          "knowledge. For external task-specific knowledge,": "putational efficiency of PLM-based ERC models,",
          "shot setting, which is sample-efficient. CTPT can": ""
        },
        {
          "knowledge. For external task-specific knowledge,": "",
          "shot setting, which is sample-efficient. CTPT can": "versational videos or multi-turn Tweets (Hazarika"
        },
        {
          "knowledge. For external task-specific knowledge,": "we optimize a vector with intrinsic dimensional-",
          "shot setting, which is sample-efficient. CTPT can": ""
        },
        {
          "knowledge. For external task-specific knowledge,": "",
          "shot setting, which is sample-efficient. CTPT can": "et al., 2018; Zahiri and Choi, 2018; Zhong et al.,"
        },
        {
          "knowledge. For external task-specific knowledge,": "ity (Li et al., 2018) instead of the whole continuous",
          "shot setting, which is sample-efficient. CTPT can": ""
        },
        {
          "knowledge. For external task-specific knowledge,": "",
          "shot setting, which is sample-efficient. CTPT can": "2019;\nIshiwatari et al., 2020).\nIn recent years,"
        },
        {
          "knowledge. For external task-specific knowledge,": "prompt, which reduces the number of parameters",
          "shot setting, which is sample-efficient. CTPT can": ""
        },
        {
          "knowledge. For external task-specific knowledge,": "",
          "shot setting, which is sample-efficient. CTPT can": "PLM has been increasingly applied in ERC mod-"
        },
        {
          "knowledge. For external task-specific knowledge,": "from hundreds of thousands to about 1,000. Follow-",
          "shot setting, which is sample-efficient. CTPT can": ""
        },
        {
          "knowledge. For external task-specific knowledge,": "",
          "shot setting, which is sample-efficient. CTPT can": "els (Lee and Choi, 2021; Shen et al., 2021; Song"
        },
        {
          "knowledge. For external task-specific knowledge,": "ing Sun et al. (2022), we use a Covariance Matrix",
          "shot setting, which is sample-efficient. CTPT can": ""
        },
        {
          "knowledge. For external task-specific knowledge,": "",
          "shot setting, which is sample-efficient. CTPT can": "et al., 2022a). A commonality among these prior"
        },
        {
          "knowledge. For external task-specific knowledge,": "Adaptation Evolution Strategy (CMA-ES) (Hansen",
          "shot setting, which is sample-efficient. CTPT can": ""
        },
        {
          "knowledge. For external task-specific knowledge,": "",
          "shot setting, which is sample-efficient. CTPT can": "approaches is their shared approach on the inte-"
        },
        {
          "knowledge. For external task-specific knowledge,": "and Ostermeier, 2001; Hansen et al., 2003) to opti-",
          "shot setting, which is sample-efficient. CTPT can": ""
        },
        {
          "knowledge. For external task-specific knowledge,": "",
          "shot setting, which is sample-efficient. CTPT can": "gration of various forms of external knowledge to"
        },
        {
          "knowledge. For external task-specific knowledge,": "mize the parameters, which is derivative-free. With",
          "shot setting, which is sample-efficient. CTPT can": ""
        },
        {
          "knowledge. For external task-specific knowledge,": "",
          "shot setting, which is sample-efficient. CTPT can": "enhance emotion detection, including knowledge"
        },
        {
          "knowledge. For external task-specific knowledge,": "the derivative-free optimization, we separate our",
          "shot setting, which is sample-efficient. CTPT can": ""
        },
        {
          "knowledge. For external task-specific knowledge,": "",
          "shot setting, which is sample-efficient. CTPT can": "from knowledge base (Zhong et al., 2019), knowl-"
        },
        {
          "knowledge. For external task-specific knowledge,": "approach from the PLM and do not require back-",
          "shot setting, which is sample-efficient. CTPT can": ""
        },
        {
          "knowledge. For external task-specific knowledge,": "",
          "shot setting, which is sample-efficient. CTPT can": "edge from commonsense (Ghosal et al., 2020; Yi"
        },
        {
          "knowledge. For external task-specific knowledge,": "propagation for parameter learning.",
          "shot setting, which is sample-efficient. CTPT can": ""
        },
        {
          "knowledge. For external task-specific knowledge,": "",
          "shot setting, which is sample-efficient. CTPT can": "et al., 2022), knowledge from multi-modal (Li et al.,"
        },
        {
          "knowledge. For external task-specific knowledge,": "Compared with single-task prompt tuning, our",
          "shot setting, which is sample-efficient. CTPT can": ""
        },
        {
          "knowledge. For external task-specific knowledge,": "",
          "shot setting, which is sample-efficient. CTPT can": "2022), and inherent knowledge within PLM (Kim"
        },
        {
          "knowledge. For external task-specific knowledge,": "proposed CTPT method can utilize external knowl-",
          "shot setting, which is sample-efficient. CTPT can": ""
        },
        {
          "knowledge. For external task-specific knowledge,": "",
          "shot setting, which is sample-efficient. CTPT can": "and Vossen, 2021). Unlike existing methods that fo-"
        },
        {
          "knowledge. For external task-specific knowledge,": "edge from other\ntasks to boost\nthe performance",
          "shot setting, which is sample-efficient. CTPT can": ""
        },
        {
          "knowledge. For external task-specific knowledge,": "",
          "shot setting, which is sample-efficient. CTPT can": "cus on enriching task-specific knowledge only, we"
        },
        {
          "knowledge. For external task-specific knowledge,": "of\nthe target\ntask.\nExperiments under\nthe few-",
          "shot setting, which is sample-efficient. CTPT can": ""
        },
        {
          "knowledge. For external task-specific knowledge,": "",
          "shot setting, which is sample-efficient. CTPT can": "also explore sharable cross-task knowledge from"
        },
        {
          "knowledge. For external task-specific knowledge,": "shot scenarios and the zero-shot transfer show that",
          "shot setting, which is sample-efficient. CTPT can": ""
        },
        {
          "knowledge. For external task-specific knowledge,": "",
          "shot setting, which is sample-efficient. CTPT can": "other source tasks."
        },
        {
          "knowledge. For external task-specific knowledge,": "CTPT can obtain a better\nresult.\nIn addition to",
          "shot setting, which is sample-efficient. CTPT can": ""
        },
        {
          "knowledge. For external task-specific knowledge,": "this, our proposed CTPT is derivative-free, which",
          "shot setting, which is sample-efficient. CTPT can": ""
        },
        {
          "knowledge. For external task-specific knowledge,": "",
          "shot setting, which is sample-efficient. CTPT can": "2.2\nPrompt Tuning"
        },
        {
          "knowledge. For external task-specific knowledge,": "does not need backpropagation. Compared with",
          "shot setting, which is sample-efficient. CTPT can": ""
        },
        {
          "knowledge. For external task-specific knowledge,": "",
          "shot setting, which is sample-efficient. CTPT can": "Despite the success of GPT-3 (Brown et al., 2020)"
        },
        {
          "knowledge. For external task-specific knowledge,": "derivative-based backpropagation, the experiment",
          "shot setting, which is sample-efficient. CTPT can": ""
        },
        {
          "knowledge. For external task-specific knowledge,": "",
          "shot setting, which is sample-efficient. CTPT can": "with 175 billion parameters, it has become increas-"
        },
        {
          "knowledge. For external task-specific knowledge,": "result shows that CTPT can obtain comparable re-",
          "shot setting, which is sample-efficient. CTPT can": ""
        },
        {
          "knowledge. For external task-specific knowledge,": "",
          "shot setting, which is sample-efficient. CTPT can": "ingly difficult and expensive to utilize such big lan-"
        },
        {
          "knowledge. For external task-specific knowledge,": "sults without derivative information.",
          "shot setting, which is sample-efficient. CTPT can": ""
        },
        {
          "knowledge. For external task-specific knowledge,": "",
          "shot setting, which is sample-efficient. CTPT can": "guage models. One possible solution to leverage"
        },
        {
          "knowledge. For external task-specific knowledge,": "The main contributions of this paper are summa-",
          "shot setting, which is sample-efficient. CTPT can": ""
        },
        {
          "knowledge. For external task-specific knowledge,": "",
          "shot setting, which is sample-efficient. CTPT can": "large pre-trained models is parameter-efficient tun-"
        },
        {
          "knowledge. For external task-specific knowledge,": "rized as follows:",
          "shot setting, which is sample-efficient. CTPT can": ""
        },
        {
          "knowledge. For external task-specific knowledge,": "",
          "shot setting, which is sample-efficient. CTPT can": "ing methods, such as prompt-tuning (Lester et al.,"
        },
        {
          "knowledge. For external task-specific knowledge,": "(1) To the best of our knowledge, we are the first",
          "shot setting, which is sample-efficient. CTPT can": ""
        },
        {
          "knowledge. For external task-specific knowledge,": "",
          "shot setting, which is sample-efficient. CTPT can": "2021; Li and Liang, 2021). In prompt tuning, down-"
        },
        {
          "knowledge. For external task-specific knowledge,": "to strictly define and tackle the few-shot setting for",
          "shot setting, which is sample-efficient. CTPT can": ""
        },
        {
          "knowledge. For external task-specific knowledge,": "",
          "shot setting, which is sample-efficient. CTPT can": "stream tasks are reformulated as a language mod-"
        },
        {
          "knowledge. For external task-specific knowledge,": "the ERC task. We propose a Cross-Task Prompt",
          "shot setting, which is sample-efficient. CTPT can": ""
        },
        {
          "knowledge. For external task-specific knowledge,": "",
          "shot setting, which is sample-efficient. CTPT can": "elling task with the help of a textual prompt. For ex-"
        },
        {
          "knowledge. For external task-specific knowledge,": "Tuning (CTPT) method that can efficiently learn",
          "shot setting, which is sample-efficient. CTPT can": ""
        },
        {
          "knowledge. For external task-specific knowledge,": "",
          "shot setting, which is sample-efficient. CTPT can": "ample, a classification task that aims to predict the"
        },
        {
          "knowledge. For external task-specific knowledge,": "and utilize cross-task knowledge.",
          "shot setting, which is sample-efficient. CTPT can": ""
        },
        {
          "knowledge. For external task-specific knowledge,": "",
          "shot setting, which is sample-efficient. CTPT can": "emotion category of a given sentence can be refor-"
        },
        {
          "knowledge. For external task-specific knowledge,": "(2) To improve the training efficiency, we use the",
          "shot setting, which is sample-efficient. CTPT can": ""
        },
        {
          "knowledge. For external task-specific knowledge,": "",
          "shot setting, which is sample-efficient. CTPT can": "mulated as: “I felt so [MASK], [X]”. Here [X] is the"
        },
        {
          "knowledge. For external task-specific knowledge,": "derivative-free optimization algorithm to optimize",
          "shot setting, which is sample-efficient. CTPT can": ""
        },
        {
          "knowledge. For external task-specific knowledge,": "",
          "shot setting, which is sample-efficient. CTPT can": "given sentence, [MASK] is the mask token that PLM"
        },
        {
          "knowledge. For external task-specific knowledge,": "the parameter. It skips the backpropagation stage",
          "shot setting, which is sample-efficient. CTPT can": ""
        },
        {
          "knowledge. For external task-specific knowledge,": "",
          "shot setting, which is sample-efficient. CTPT can": "needs to predict, and “I felt so [MASK]” is the tem-"
        },
        {
          "knowledge. For external task-specific knowledge,": "and does not require gradient information.",
          "shot setting, which is sample-efficient. CTPT can": ""
        },
        {
          "knowledge. For external task-specific knowledge,": "",
          "shot setting, which is sample-efficient. CTPT can": "plate of prompting. The aforementioned prompt"
        },
        {
          "knowledge. For external task-specific knowledge,": "(3) Our proposed CTPT only needs\nto opti-",
          "shot setting, which is sample-efficient. CTPT can": "consists of discrete tokens, which are also known"
        },
        {
          "knowledge. For external task-specific knowledge,": "mize about 1,000 parameters, which is much more",
          "shot setting, which is sample-efficient. CTPT can": "as a hard prompt. There is another prompt named"
        },
        {
          "knowledge. For external task-specific knowledge,": "training-efficient\nthan any other\nexisting PLM-",
          "shot setting, which is sample-efficient. CTPT can": "soft prompt (Qin and Eisner, 2021), which consists"
        },
        {
          "knowledge. For external task-specific knowledge,": "based ERC method.",
          "shot setting, which is sample-efficient. CTPT can": "of continuous embeddings. Recently, prompt tun-"
        },
        {
          "knowledge. For external task-specific knowledge,": "(4) Our proposed CTPT is trained under the few-",
          "shot setting, which is sample-efficient. CTPT can": "ing has been proven successful\nin both few-shot"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "scenarios (Gu et al., 2022; Vu et al., 2022) and": "zero-shot transfer (Guo et al., 2022).",
          "category for each utterance to maximum match the": "ground-truth emotion category. In the i-th conver-"
        },
        {
          "scenarios (Gu et al., 2022; Vu et al., 2022) and": "",
          "category for each utterance to maximum match the": ", xi\nsation, xi"
        },
        {
          "scenarios (Gu et al., 2022; Vu et al., 2022) and": "Although prompt tuning has brought success in",
          "category for each utterance to maximum match the": "indicates the\nj = [xi\nj,1, xi\nj,2, · · ·"
        },
        {
          "scenarios (Gu et al., 2022; Vu et al., 2022) and": "",
          "category for each utterance to maximum match the": "j,|xi\nj |]"
        },
        {
          "scenarios (Gu et al., 2022; Vu et al., 2022) and": "many NLP domains such as text classification (Gao",
          "category for each utterance to maximum match the": ""
        },
        {
          "scenarios (Gu et al., 2022; Vu et al., 2022) and": "",
          "category for each utterance to maximum match the": "j-th utterance, xi"
        },
        {
          "scenarios (Gu et al., 2022; Vu et al., 2022) and": "",
          "category for each utterance to maximum match the": "j,k indicates the k-th token in the"
        },
        {
          "scenarios (Gu et al., 2022; Vu et al., 2022) and": "et al., 2021), question answering (Yang et al., 2022),",
          "category for each utterance to maximum match the": ""
        },
        {
          "scenarios (Gu et al., 2022; Vu et al., 2022) and": "",
          "category for each utterance to maximum match the": "|xi"
        },
        {
          "scenarios (Gu et al., 2022; Vu et al., 2022) and": "",
          "category for each utterance to maximum match the": "indicates the sequence length\nj|"
        },
        {
          "scenarios (Gu et al., 2022; Vu et al., 2022) and": "and commonsense reasoning (Liu et al., 2022), Yi",
          "category for each utterance to maximum match the": ""
        },
        {
          "scenarios (Gu et al., 2022; Vu et al., 2022) and": "",
          "category for each utterance to maximum match the": "of\nis the number of\nthe j-th utterance, and |xi|"
        },
        {
          "scenarios (Gu et al., 2022; Vu et al., 2022) and": "et al. (2022) first practising prompt tuning on the",
          "category for each utterance to maximum match the": ""
        },
        {
          "scenarios (Gu et al., 2022; Vu et al., 2022) and": "",
          "category for each utterance to maximum match the": "utterances in the i-th conversation."
        },
        {
          "scenarios (Gu et al., 2022; Vu et al., 2022) and": "ERC task that utilizes learnable continuous prompt",
          "category for each utterance to maximum match the": ""
        },
        {
          "scenarios (Gu et al., 2022; Vu et al., 2022) and": "",
          "category for each utterance to maximum match the": "Dataset under\nthe few-shot setting is a subset"
        },
        {
          "scenarios (Gu et al., 2022; Vu et al., 2022) and": "to model\nthe relationship between contextual\nin-",
          "category for each utterance to maximum match the": ""
        },
        {
          "scenarios (Gu et al., 2022; Vu et al., 2022) and": "",
          "category for each utterance to maximum match the": "that under the full dataset setting. The new dataset"
        },
        {
          "scenarios (Gu et al., 2022; Vu et al., 2022) and": "formation and commonsense knowledge.\nIn this",
          "category for each utterance to maximum match the": ""
        },
        {
          "scenarios (Gu et al., 2022; Vu et al., 2022) and": "",
          "category for each utterance to maximum match the": "under the k-shot setting is marked as {(x, y)|x ∈"
        },
        {
          "scenarios (Gu et al., 2022; Vu et al., 2022) and": "paper, we utilize learnable prompts to model\nthe",
          "category for each utterance to maximum match the": ""
        },
        {
          "scenarios (Gu et al., 2022; Vu et al., 2022) and": "",
          "category for each utterance to maximum match the": "ˆ\nˆ\nˆ\nˆ"
        },
        {
          "scenarios (Gu et al., 2022; Vu et al., 2022) and": "",
          "category for each utterance to maximum match the": "X , y ∈\nX and\nY indicate the input\nY}k, where"
        },
        {
          "scenarios (Gu et al., 2022; Vu et al., 2022) and": "relationship between emotional categories among",
          "category for each utterance to maximum match the": ""
        },
        {
          "scenarios (Gu et al., 2022; Vu et al., 2022) and": "",
          "category for each utterance to maximum match the": "sequence as well as the emotion category for the"
        },
        {
          "scenarios (Gu et al., 2022; Vu et al., 2022) and": "different tasks under the few-shot setting.",
          "category for each utterance to maximum match the": ""
        },
        {
          "scenarios (Gu et al., 2022; Vu et al., 2022) and": "",
          "category for each utterance to maximum match the": "new training set. Correspondingly,\nthe predicted"
        },
        {
          "scenarios (Gu et al., 2022; Vu et al., 2022) and": "",
          "category for each utterance to maximum match the": "emotion category is ˆE. Here k-shot indicates that"
        },
        {
          "scenarios (Gu et al., 2022; Vu et al., 2022) and": "2.3\nDerivative-Free Optimization",
          "category for each utterance to maximum match the": ""
        },
        {
          "scenarios (Gu et al., 2022; Vu et al., 2022) and": "",
          "category for each utterance to maximum match the": "there are k training examples for each emotion cate-"
        },
        {
          "scenarios (Gu et al., 2022; Vu et al., 2022) and": "Different\nfrom many\nneural\nnetworks\nthat\nre-",
          "category for each utterance to maximum match the": ""
        },
        {
          "scenarios (Gu et al., 2022; Vu et al., 2022) and": "",
          "category for each utterance to maximum match the": "gory. We randomly select the new dataset and keep"
        },
        {
          "scenarios (Gu et al., 2022; Vu et al., 2022) and": "quire gradient\ninformation for backpropagation,",
          "category for each utterance to maximum match the": ""
        },
        {
          "scenarios (Gu et al., 2022; Vu et al., 2022) and": "",
          "category for each utterance to maximum match the": "it the same in the following experiments. Similar to"
        },
        {
          "scenarios (Gu et al., 2022; Vu et al., 2022) and": "derivative-free optimization (DFO) algorithms aim",
          "category for each utterance to maximum match the": ""
        },
        {
          "scenarios (Gu et al., 2022; Vu et al., 2022) and": "",
          "category for each utterance to maximum match the": "the full dataset setting, the aims under the few-shot"
        },
        {
          "scenarios (Gu et al., 2022; Vu et al., 2022) and": "to obtain optimal solutions without derivative infor-",
          "category for each utterance to maximum match the": ""
        },
        {
          "scenarios (Gu et al., 2022; Vu et al., 2022) and": "",
          "category for each utterance to maximum match the": "setting is to predict the emotion category ˆei."
        },
        {
          "scenarios (Gu et al., 2022; Vu et al., 2022) and": "mation. Most DFO algorithms (Hansen et al., 2003;",
          "category for each utterance to maximum match the": ""
        },
        {
          "scenarios (Gu et al., 2022; Vu et al., 2022) and": "",
          "category for each utterance to maximum match the": "Under the few-shot setting,\nthe training set as"
        },
        {
          "scenarios (Gu et al., 2022; Vu et al., 2022) and": "Shahriari et al., 2016) are under the sampling-and-",
          "category for each utterance to maximum match the": ""
        },
        {
          "scenarios (Gu et al., 2022; Vu et al., 2022) and": "",
          "category for each utterance to maximum match the": "well as the development set, are sampled randomly"
        },
        {
          "scenarios (Gu et al., 2022; Vu et al., 2022) and": "updating structure, which firstly samples a solution",
          "category for each utterance to maximum match the": ""
        },
        {
          "scenarios (Gu et al., 2022; Vu et al., 2022) and": "",
          "category for each utterance to maximum match the": "from the vanilla dataset of the full dataset setting,"
        },
        {
          "scenarios (Gu et al., 2022; Vu et al., 2022) and": "x and then optimize the parameters via the function",
          "category for each utterance to maximum match the": ""
        },
        {
          "scenarios (Gu et al., 2022; Vu et al., 2022) and": "",
          "category for each utterance to maximum match the": "while the testing set keeps unchanged. At the be-"
        },
        {
          "scenarios (Gu et al., 2022; Vu et al., 2022) and": "values f (x). In recent years, DFO algorithms have",
          "category for each utterance to maximum match the": ""
        },
        {
          "scenarios (Gu et al., 2022; Vu et al., 2022) and": "",
          "category for each utterance to maximum match the": "ginning of\nthe training stage, we first\nrandomly"
        },
        {
          "scenarios (Gu et al., 2022; Vu et al., 2022) and": "been applied to many downstream areas, such as",
          "category for each utterance to maximum match the": ""
        },
        {
          "scenarios (Gu et al., 2022; Vu et al., 2022) and": "",
          "category for each utterance to maximum match the": "select some training examples under the following"
        },
        {
          "scenarios (Gu et al., 2022; Vu et al., 2022) and": "automatic machine learning (Snoek et al., 2012),",
          "category for each utterance to maximum match the": ""
        },
        {
          "scenarios (Gu et al., 2022; Vu et al., 2022) and": "",
          "category for each utterance to maximum match the": "rules: for each given emotion category (e.g., netu-"
        },
        {
          "scenarios (Gu et al., 2022; Vu et al., 2022) and": "and reinforcement learning (Salimans et al., 2017).",
          "category for each utterance to maximum match the": ""
        },
        {
          "scenarios (Gu et al., 2022; Vu et al., 2022) and": "",
          "category for each utterance to maximum match the": "ral, happiness, sadness, etc.), we randomly select"
        },
        {
          "scenarios (Gu et al., 2022; Vu et al., 2022) and": "More recently, Sun et al. (2022) proposed a DFO",
          "category for each utterance to maximum match the": ""
        },
        {
          "scenarios (Gu et al., 2022; Vu et al., 2022) and": "",
          "category for each utterance to maximum match the": "k utterances from the vanilla training set. In other"
        },
        {
          "scenarios (Gu et al., 2022; Vu et al., 2022) and": "method to optimize continuous prompts without",
          "category for each utterance to maximum match the": ""
        },
        {
          "scenarios (Gu et al., 2022; Vu et al., 2022) and": "",
          "category for each utterance to maximum match the": "words, we keep the textual conversational content"
        },
        {
          "scenarios (Gu et al., 2022; Vu et al., 2022) and": "gradient information. In this paper, we further ex-",
          "category for each utterance to maximum match the": ""
        },
        {
          "scenarios (Gu et al., 2022; Vu et al., 2022) and": "",
          "category for each utterance to maximum match the": "but retain only one emotion category for one con-"
        },
        {
          "scenarios (Gu et al., 2022; Vu et al., 2022) and": "tend the DFO method to optimize not only the con-",
          "category for each utterance to maximum match the": ""
        },
        {
          "scenarios (Gu et al., 2022; Vu et al., 2022) and": "",
          "category for each utterance to maximum match the": "versation. Therefore,\nfor each training example,"
        },
        {
          "scenarios (Gu et al., 2022; Vu et al., 2022) and": "tinuous prompt but also the parameters for cross-",
          "category for each utterance to maximum match the": ""
        },
        {
          "scenarios (Gu et al., 2022; Vu et al., 2022) and": "",
          "category for each utterance to maximum match the": "the input content remains the conversation content"
        },
        {
          "scenarios (Gu et al., 2022; Vu et al., 2022) and": "task learning.",
          "category for each utterance to maximum match the": ""
        },
        {
          "scenarios (Gu et al., 2022; Vu et al., 2022) and": "",
          "category for each utterance to maximum match the": ", xi"
        },
        {
          "scenarios (Gu et al., 2022; Vu et al., 2022) and": "",
          "category for each utterance to maximum match the": "1, xi\n2, · · ·\n|xi|], and the ground-truth be-"
        },
        {
          "scenarios (Gu et al., 2022; Vu et al., 2022) and": "3\nMethodology",
          "category for each utterance to maximum match the": "that j is randomly selected before\ncomes ˆyi = yi"
        },
        {
          "scenarios (Gu et al., 2022; Vu et al., 2022) and": "",
          "category for each utterance to maximum match the": "j"
        },
        {
          "scenarios (Gu et al., 2022; Vu et al., 2022) and": "",
          "category for each utterance to maximum match the": "the training stage."
        },
        {
          "scenarios (Gu et al., 2022; Vu et al., 2022) and": "3.1\nProblem Definition and Notations",
          "category for each utterance to maximum match the": ""
        },
        {
          "scenarios (Gu et al., 2022; Vu et al., 2022) and": "",
          "category for each utterance to maximum match the": "All experiments are conducted on few-shot set-"
        },
        {
          "scenarios (Gu et al., 2022; Vu et al., 2022) and": "In this section, we will briefly define the emotion",
          "category for each utterance to maximum match the": "tings (k = 16). For each dataset, we sample the"
        },
        {
          "scenarios (Gu et al., 2022; Vu et al., 2022) and": "recognition in conversation (ERC)\ntask and the",
          "category for each utterance to maximum match the": "subset of the training set and the development set"
        },
        {
          "scenarios (Gu et al., 2022; Vu et al., 2022) and": "ERC task under the few-shot setting.",
          "category for each utterance to maximum match the": "and keep the testing set unchanged. For a fair com-"
        },
        {
          "scenarios (Gu et al., 2022; Vu et al., 2022) and": "The full dataset setting contains the conversa-",
          "category for each utterance to maximum match the": "parison, all baselines and CTPT are trained by the"
        },
        {
          "scenarios (Gu et al., 2022; Vu et al., 2022) and": "tion set X\n= {x1, x2, · · ·\n, xn} with n differ-",
          "category for each utterance to maximum match the": "same training set."
        },
        {
          "scenarios (Gu et al., 2022; Vu et al., 2022) and": "ent\nconversations\nas well\nas\nthe\nemotion cate-",
          "category for each utterance to maximum match the": ""
        },
        {
          "scenarios (Gu et al., 2022; Vu et al., 2022) and": "",
          "category for each utterance to maximum match the": "3.2\nOverview of the Model"
        },
        {
          "scenarios (Gu et al., 2022; Vu et al., 2022) and": "The target\nis\ngory set Y = {y1, y2, · · ·\n, yn}.",
          "category for each utterance to maximum match the": ""
        },
        {
          "scenarios (Gu et al., 2022; Vu et al., 2022) and": "to predict the corresponding emotion category set",
          "category for each utterance to maximum match the": ""
        },
        {
          "scenarios (Gu et al., 2022; Vu et al., 2022) and": "",
          "category for each utterance to maximum match the": "In\nthis\nsection, we will\nbriefly\nintroduce\nthe"
        },
        {
          "scenarios (Gu et al., 2022; Vu et al., 2022) and": "E = {e1, e2, · · ·\n, en}.",
          "category for each utterance to maximum match the": "overview of the whole model. The input of CTPT is"
        },
        {
          "scenarios (Gu et al., 2022; Vu et al., 2022) and": "More specifically, the input of the task is a con-",
          "category for each utterance to maximum match the": ""
        },
        {
          "scenarios (Gu et al., 2022; Vu et al., 2022) and": "",
          "category for each utterance to maximum match the": "a textual sequence that contains the conversational"
        },
        {
          "scenarios (Gu et al., 2022; Vu et al., 2022) and": ", xi",
          "category for each utterance to maximum match the": ""
        },
        {
          "scenarios (Gu et al., 2022; Vu et al., 2022) and": "1, xi\n2, · · ·\n|xi|]. The",
          "category for each utterance to maximum match the": "context, and the output of CTPT is an emotion label."
        },
        {
          "scenarios (Gu et al., 2022; Vu et al., 2022) and": "output of\nthe task is an emotional category set",
          "category for each utterance to maximum match the": ""
        },
        {
          "scenarios (Gu et al., 2022; Vu et al., 2022) and": "",
          "category for each utterance to maximum match the": "CTPT can be mainly divided into three parts:\ntask-"
        },
        {
          "scenarios (Gu et al., 2022; Vu et al., 2022) and": ", ei",
          "category for each utterance to maximum match the": ""
        },
        {
          "scenarios (Gu et al., 2022; Vu et al., 2022) and": "1, ei\n2, · · ·\n|xi|]. The ground-truth is also",
          "category for each utterance to maximum match the": "specific prompt tuning (TSPT), cross-task prompt"
        },
        {
          "scenarios (Gu et al., 2022; Vu et al., 2022) and": ", yi",
          "category for each utterance to maximum match the": ""
        },
        {
          "scenarios (Gu et al., 2022; Vu et al., 2022) and": "1, yi\n2, · · ·\n|xi|].",
          "category for each utterance to maximum match the": "learning (CTPL), and cross-task prompt observa-"
        },
        {
          "scenarios (Gu et al., 2022; Vu et al., 2022) and": "The target of the task is to predict\nthe emotional",
          "category for each utterance to maximum match the": "tion (CTPO). The overall architecture is shown in"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Figure 1: Overall architecture of our proposed CTPT model.": "Figure 1."
        },
        {
          "Figure 1: Overall architecture of our proposed CTPT model.": "The first part of CTPT is TSPT. In this part, we"
        },
        {
          "Figure 1: Overall architecture of our proposed CTPT model.": "learn task-specific knowledge from different source"
        },
        {
          "Figure 1: Overall architecture of our proposed CTPT model.": "tasks 1, which is harnessed later to the target task 2."
        },
        {
          "Figure 1: Overall architecture of our proposed CTPT model.": "Then, we have CTPL that employs an attention"
        },
        {
          "Figure 1: Overall architecture of our proposed CTPT model.": ""
        },
        {
          "Figure 1: Overall architecture of our proposed CTPT model.": "module to learn the external\ntask-specific knowl-"
        },
        {
          "Figure 1: Overall architecture of our proposed CTPT model.": "edge learned by TSPT from source tasks and the"
        },
        {
          "Figure 1: Overall architecture of our proposed CTPT model.": "emotional knowledge from commonsense. Lastly,"
        },
        {
          "Figure 1: Overall architecture of our proposed CTPT model.": ""
        },
        {
          "Figure 1: Overall architecture of our proposed CTPT model.": "we have CTPO that utilizes a gate-like mecha-"
        },
        {
          "Figure 1: Overall architecture of our proposed CTPT model.": ""
        },
        {
          "Figure 1: Overall architecture of our proposed CTPT model.": "nism to summarize pertinent cross-task knowledge"
        },
        {
          "Figure 1: Overall architecture of our proposed CTPT model.": ""
        },
        {
          "Figure 1: Overall architecture of our proposed CTPT model.": "learned by CTPL. In summary, we have “TSPT +"
        },
        {
          "Figure 1: Overall architecture of our proposed CTPT model.": ""
        },
        {
          "Figure 1: Overall architecture of our proposed CTPT model.": "CTPL + CTPO = CTPT”."
        },
        {
          "Figure 1: Overall architecture of our proposed CTPT model.": ""
        },
        {
          "Figure 1: Overall architecture of our proposed CTPT model.": "We concatenate the prompt with summarized"
        },
        {
          "Figure 1: Overall architecture of our proposed CTPT model.": ""
        },
        {
          "Figure 1: Overall architecture of our proposed CTPT model.": "cross-task knowledge ˆpi as well as the input se-"
        },
        {
          "Figure 1: Overall architecture of our proposed CTPT model.": ""
        },
        {
          "Figure 1: Overall architecture of our proposed CTPT model.": "quence x, and then pass it into the PLM. After we"
        },
        {
          "Figure 1: Overall architecture of our proposed CTPT model.": ""
        },
        {
          "Figure 1: Overall architecture of our proposed CTPT model.": "obtain the logits of the [MASK] token from PLM,"
        },
        {
          "Figure 1: Overall architecture of our proposed CTPT model.": ""
        },
        {
          "Figure 1: Overall architecture of our proposed CTPT model.": "we first decode the logits to word distribution, then"
        },
        {
          "Figure 1: Overall architecture of our proposed CTPT model.": "map the word distribution to the emotion label,"
        },
        {
          "Figure 1: Overall architecture of our proposed CTPT model.": ""
        },
        {
          "Figure 1: Overall architecture of our proposed CTPT model.": "which is the output of the whole model."
        },
        {
          "Figure 1: Overall architecture of our proposed CTPT model.": ""
        },
        {
          "Figure 1: Overall architecture of our proposed CTPT model.": "In the derivative-free optimization, learnable pa-"
        },
        {
          "Figure 1: Overall architecture of our proposed CTPT model.": ""
        },
        {
          "Figure 1: Overall architecture of our proposed CTPT model.": "rameters are contained a vector z with intrinsic di-"
        },
        {
          "Figure 1: Overall architecture of our proposed CTPT model.": ""
        },
        {
          "Figure 1: Overall architecture of our proposed CTPT model.": "mensionality. The parameters in the neural network"
        },
        {
          "Figure 1: Overall architecture of our proposed CTPT model.": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "knowledge of the target\ntask. The independently": "learned task-specific knowledge is usually limited",
          "collected from commonsense in addition to exter-": "nal task-specific knowledge. Across different ERC"
        },
        {
          "knowledge of the target\ntask. The independently": "under the few-shot setting. One promising solution",
          "collected from commonsense in addition to exter-": "datasets, varying labels might be used to denote"
        },
        {
          "knowledge of the target\ntask. The independently": "to address this problem is to introduce abundant",
          "collected from commonsense in addition to exter-": "identical emotional states. For example, DailyDi-"
        },
        {
          "knowledge of the target\ntask. The independently": "sharable knowledge from other source tasks. The",
          "collected from commonsense in addition to exter-": "alog uses “happiness” while MELD uses “joy” to"
        },
        {
          "knowledge of the target\ntask. The independently": "sharable knowledge includes external task-specific",
          "collected from commonsense in addition to exter-": "represent the state of being happy. Given this under-"
        },
        {
          "knowledge of the target\ntask. The independently": "knowledge from source tasks and prior emotional",
          "collected from commonsense in addition to exter-": "standing, we can learn cross-task emotional knowl-"
        },
        {
          "knowledge of the target\ntask. The independently": "knowledge learned from commonsense.",
          "collected from commonsense in addition to exter-": "edge from disparate tasks encompassing the same"
        },
        {
          "knowledge of the target\ntask. The independently": "",
          "collected from commonsense in addition to exter-": "emotional state, notwithstanding the divergence in"
        },
        {
          "knowledge of the target\ntask. The independently": "External Task-Specific Knowledge\nSince task-",
          "collected from commonsense in addition to exter-": ""
        },
        {
          "knowledge of the target\ntask. The independently": "",
          "collected from commonsense in addition to exter-": "emotion labels, by modifying the verbalizer. There-"
        },
        {
          "knowledge of the target\ntask. The independently": "specific knowledge is often very limited in the few-",
          "collected from commonsense in addition to exter-": ""
        },
        {
          "knowledge of the target\ntask. The independently": "",
          "collected from commonsense in addition to exter-": "fore, Eq (3) can be modified as:"
        },
        {
          "knowledge of the target\ntask. The independently": "shot setting, we introduce external\ntask-specific",
          "collected from commonsense in addition to exter-": ""
        },
        {
          "knowledge of the target\ntask. The independently": "",
          "collected from commonsense in addition to exter-": "ˆ"
        },
        {
          "knowledge of the target\ntask. The independently": "knowledge from other\nsource tasks.\nAs afore-",
          "collected from commonsense in addition to exter-": ", n},\n(8)\nV ={v|∀v ∈ Vi, i = 1, 2, · · ·"
        },
        {
          "knowledge of the target\ntask. The independently": "mentioned,\nthe external\ntask-specific knowledge",
          "collected from commonsense in addition to exter-": ""
        },
        {
          "knowledge of the target\ntask. The independently": "",
          "collected from commonsense in addition to exter-": "h : ˆV → V,"
        },
        {
          "knowledge of the target\ntask. The independently": "is stored in the prompt\nlearned by TSPT. There-",
          "collected from commonsense in addition to exter-": ""
        },
        {
          "knowledge of the target\ntask. The independently": "",
          "collected from commonsense in addition to exter-": "(cid:16)\n(cid:17)"
        },
        {
          "knowledge of the target\ntask. The independently": "fore, we modify the Equation (2) for the i-th task",
          "collected from commonsense in addition to exter-": "v(h) =g\np([MASK] = v|h)|v ∈ V\n,"
        },
        {
          "knowledge of the target\ntask. The independently": "as follows:",
          "collected from commonsense in addition to exter-": ""
        },
        {
          "knowledge of the target\ntask. The independently": "",
          "collected from commonsense in addition to exter-": "where h is a mapping function that maps v from"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table 1: Though some of the datasets",
      "data": [
        {
          "3.6\nDerivative-Free Optimization": "According to Li et al. (2018), the intrinsic dimen-",
          "4\nExperiments": ""
        },
        {
          "3.6\nDerivative-Free Optimization": "",
          "4\nExperiments": "4.1\nDatasets"
        },
        {
          "3.6\nDerivative-Free Optimization": "sionality is the minimum number of parameters",
          "4\nExperiments": ""
        },
        {
          "3.6\nDerivative-Free Optimization": "",
          "4\nExperiments": "We conduct experiments on five widely-used pub-"
        },
        {
          "3.6\nDerivative-Free Optimization": "needed to obtain comparable results.\nSun et al.",
          "4\nExperiments": ""
        },
        {
          "3.6\nDerivative-Free Optimization": "",
          "4\nExperiments": "lic datasets to show the efficiency of CTPT,\nin-"
        },
        {
          "3.6\nDerivative-Free Optimization": "(2022) also shows the efficiency of derivative-free",
          "4\nExperiments": ""
        },
        {
          "3.6\nDerivative-Free Optimization": "",
          "4\nExperiments": "cluding: EC (Chatterjee et al., 2019), DailyDia-"
        },
        {
          "3.6\nDerivative-Free Optimization": "optimization for intrinsic dimensionality vector in",
          "4\nExperiments": ""
        },
        {
          "3.6\nDerivative-Free Optimization": "",
          "4\nExperiments": "log (Li et al., 2017), MELD (Poria et al., 2019),"
        },
        {
          "3.6\nDerivative-Free Optimization": "prompt tuning. To improve the computational effi-",
          "4\nExperiments": ""
        },
        {
          "3.6\nDerivative-Free Optimization": "",
          "4\nExperiments": "EmoryNLP (Zahiri and Choi, 2018), and IEMO-"
        },
        {
          "3.6\nDerivative-Free Optimization": "ciency, instead of the derivative-based backpropa-",
          "4\nExperiments": ""
        },
        {
          "3.6\nDerivative-Free Optimization": "",
          "4\nExperiments": "CAP (Busso et al., 2008). Detailed statistics are"
        },
        {
          "3.6\nDerivative-Free Optimization": "gation, we utilize a Covariance Matrix Adaptation",
          "4\nExperiments": ""
        },
        {
          "3.6\nDerivative-Free Optimization": "",
          "4\nExperiments": "shown in Table 1. Though some of the datasets"
        },
        {
          "3.6\nDerivative-Free Optimization": "Evolution Strategy (CMA-ES) (Hansen and Oster-",
          "4\nExperiments": ""
        },
        {
          "3.6\nDerivative-Free Optimization": "",
          "4\nExperiments": "are multi-modality, we only utilize the textual in-"
        },
        {
          "3.6\nDerivative-Free Optimization": "meier, 2001; Hansen et al., 2003)\nto optimize a",
          "4\nExperiments": ""
        },
        {
          "3.6\nDerivative-Free Optimization": "",
          "4\nExperiments": "formation as the input for a fair comparison with"
        },
        {
          "3.6\nDerivative-Free Optimization": "vector with intrinsic dimensionality. In each opti-",
          "4\nExperiments": ""
        },
        {
          "3.6\nDerivative-Free Optimization": "",
          "4\nExperiments": "baselines."
        },
        {
          "3.6\nDerivative-Free Optimization": "mization step, the optimizer will first sample some",
          "4\nExperiments": ""
        },
        {
          "3.6\nDerivative-Free Optimization": "solutions of the learnable vector z. Then we can",
          "4\nExperiments": ""
        },
        {
          "3.6\nDerivative-Free Optimization": "",
          "4\nExperiments": "4.2\nBaselines"
        },
        {
          "3.6\nDerivative-Free Optimization": "calculate the loss of each solution that is used by",
          "4\nExperiments": ""
        },
        {
          "3.6\nDerivative-Free Optimization": "",
          "4\nExperiments": "For a comprehensive performance evaluation, we"
        },
        {
          "3.6\nDerivative-Free Optimization": "the optimizer to suggest a new z.",
          "4\nExperiments": ""
        },
        {
          "3.6\nDerivative-Free Optimization": "",
          "4\nExperiments": "select the following four baselines for comparison:"
        },
        {
          "3.6\nDerivative-Free Optimization": "To adapt our proposed CTPT, we modify the",
          "4\nExperiments": ""
        },
        {
          "3.6\nDerivative-Free Optimization": "optimization step as follows:",
          "4\nExperiments": ""
        },
        {
          "3.6\nDerivative-Free Optimization": "",
          "4\nExperiments": "KET\n(Zhong\net\nal.,\n2019) The KET is\na"
        },
        {
          "3.6\nDerivative-Free Optimization": "Task-Specific Prompt Tuning\nIn this step, we",
          "4\nExperiments": "knowledge-enriched transformer model specifically"
        },
        {
          "3.6\nDerivative-Free Optimization": "follow Sun et\nal.\n(2022)\nto compute\nthe\ntask-",
          "4\nExperiments": "designed for Emotion Recognition in Conversation"
        },
        {
          "3.6\nDerivative-Free Optimization": "",
          "4\nExperiments": "(ERC). It employs a knowledge base to infuse exter-"
        },
        {
          "3.6\nDerivative-Free Optimization": "specific prompt pt by a learnable vector z:",
          "4\nExperiments": ""
        },
        {
          "3.6\nDerivative-Free Optimization": "",
          "4\nExperiments": "nal knowledge, which is a representative baseline"
        },
        {
          "3.6\nDerivative-Free Optimization": "(11)\npt = Az + p0,",
          "4\nExperiments": "model before the PLM decade."
        },
        {
          "3.6\nDerivative-Free Optimization": "",
          "4\nExperiments": "TUCORE-GCN\n(Lee\nand Choi, 2021) The"
        },
        {
          "3.6\nDerivative-Free Optimization": "where A is randomly initialized and fixed, and p0",
          "4\nExperiments": ""
        },
        {
          "3.6\nDerivative-Free Optimization": "is the initialized prompts from most widely-used",
          "4\nExperiments": "TUCORE-GCN model\nis\na\nturn-context\naware"
        },
        {
          "3.6\nDerivative-Free Optimization": "tokens.",
          "4\nExperiments": "graph convolutional network designed for ERC."
        },
        {
          "3.6\nDerivative-Free Optimization": "",
          "4\nExperiments": "It\nincorporates both a PLM encoder and a graph"
        },
        {
          "3.6\nDerivative-Free Optimization": "Cross-Task Prompt Learning\nIn this step, we",
          "4\nExperiments": ""
        },
        {
          "3.6\nDerivative-Free Optimization": "",
          "4\nExperiments": "convolutional network, making it an exemplary rep-"
        },
        {
          "3.6\nDerivative-Free Optimization": "optimize a vector z′\ninstead of the parameters in",
          "4\nExperiments": ""
        },
        {
          "3.6\nDerivative-Free Optimization": "",
          "4\nExperiments": "resentation of PLM-based baseline models."
        },
        {
          "3.6\nDerivative-Free Optimization": "the multi-head attention module.\nSimilar\nto the",
          "4\nExperiments": ""
        },
        {
          "3.6\nDerivative-Free Optimization": "optimization of TSPT, we firstly project z′\nto the",
          "4\nExperiments": "EmotionFlow\n(Song et al., 2022b) The Emo-"
        },
        {
          "3.6\nDerivative-Free Optimization": "parameters space and then separate the parameter",
          "4\nExperiments": "tionFlow is a PLM-based model with an additional"
        },
        {
          "3.6\nDerivative-Free Optimization": "space by:",
          "4\nExperiments": "CRF layer to capture the emotion transition proba-"
        },
        {
          "3.6\nDerivative-Free Optimization": "",
          "4\nExperiments": "bility among different utterances."
        },
        {
          "3.6\nDerivative-Free Optimization": "W = concat[ ˆW Q, ˆW K, ˆW V ],\n(12)",
          "4\nExperiments": ""
        },
        {
          "3.6\nDerivative-Free Optimization": "",
          "4\nExperiments": "SPCL\n(Song et al., 2022a) The SPCL is a PLM-"
        },
        {
          "3.6\nDerivative-Free Optimization": "where W = A′z′\nthat A′\nis also randomly initial-",
          "4\nExperiments": "based model using supervised prototypical con-"
        },
        {
          "3.6\nDerivative-Free Optimization": "ized and fixed. Then, we reshape the parameters",
          "4\nExperiments": "trastive learning loss,\nfocusing primarily on im-"
        },
        {
          "3.6\nDerivative-Free Optimization": "and add a randomly initialized and fixed term:",
          "4\nExperiments": "balanced classification problems.\nIt has achieved"
        },
        {
          "3.6\nDerivative-Free Optimization": "",
          "4\nExperiments": "state-of-the-art results on MELD and EmoryNLP."
        },
        {
          "3.6\nDerivative-Free Optimization": "W Q = ˆW Q + W Q\n,\n(13)",
          "4\nExperiments": ""
        },
        {
          "3.6\nDerivative-Free Optimization": "0",
          "4\nExperiments": ""
        },
        {
          "3.6\nDerivative-Free Optimization": "",
          "4\nExperiments": "4.3\nImplementation Details"
        },
        {
          "3.6\nDerivative-Free Optimization": "W K = ˆW K + W K\n,",
          "4\nExperiments": ""
        },
        {
          "3.6\nDerivative-Free Optimization": "0",
          "4\nExperiments": ""
        },
        {
          "3.6\nDerivative-Free Optimization": "",
          "4\nExperiments": "In this paper, we use a soft prompt extended to"
        },
        {
          "3.6\nDerivative-Free Optimization": "W V = ˆW V + W V\n.",
          "4\nExperiments": ""
        },
        {
          "3.6\nDerivative-Free Optimization": "0",
          "4\nExperiments": ""
        },
        {
          "3.6\nDerivative-Free Optimization": "",
          "4\nExperiments": "the\ninput\nand freeze\nthe PLM. We use CMA-"
        },
        {
          "3.6\nDerivative-Free Optimization": "",
          "4\nExperiments": "ES (Hansen and Ostermeier, 2001; Hansen et al.,"
        },
        {
          "3.6\nDerivative-Free Optimization": "Cross-Task Prompt Observation\nIn this step,",
          "4\nExperiments": ""
        },
        {
          "3.6\nDerivative-Free Optimization": "",
          "4\nExperiments": "2003) algorithm to optimize the parameters. We"
        },
        {
          "3.6\nDerivative-Free Optimization": "we optimize the vector z′′\nin the same way as the",
          "4\nExperiments": ""
        },
        {
          "3.6\nDerivative-Free Optimization": "",
          "4\nExperiments": "choose T5 (Raffel et al., 2020) as our backbone"
        },
        {
          "3.6\nDerivative-Free Optimization": "vector z being optimized in TSPT:",
          "4\nExperiments": ""
        },
        {
          "3.6\nDerivative-Free Optimization": "",
          "4\nExperiments": "model. All few-shot settings share the same train-"
        },
        {
          "3.6\nDerivative-Free Optimization": "",
          "4\nExperiments": "ing and development set with k = 16 following"
        },
        {
          "3.6\nDerivative-Free Optimization": "(14)\ng = A′′z′′ + g0,",
          "4\nExperiments": ""
        },
        {
          "3.6\nDerivative-Free Optimization": "",
          "4\nExperiments": "the settings of few-shot prompt tuning (Gao et al.,"
        },
        {
          "3.6\nDerivative-Free Optimization": "where A′′ and g0 are fixed.",
          "4\nExperiments": "2021; Sun et al., 2022)."
        }
      ],
      "page": 6
    },
    {
      "caption": "Table 2: , CTPT optimized by DFO algorithms",
      "data": [
        {
          "Dataset": "EC (Chatterjee et al., 2019)",
          "Domain": "Tweet",
          "# Emotions": "4",
          "# Conv.": "30,160/2,755/5,509",
          "# Utter.": "90,480/8,265/16,527"
        },
        {
          "Dataset": "DailyDialog (Li et al., 2017)",
          "Domain": "Daily Chat",
          "# Emotions": "7",
          "# Conv.": "11,118/1,000/1,000",
          "# Utter.": "87,170/8,069/7,740"
        },
        {
          "Dataset": "MELD (Poria et al., 2019)",
          "Domain": "TV Show Scripts",
          "# Emotions": "7",
          "# Conv.": "1,038/114/280",
          "# Utter.": "9,989/1,109/2,610"
        },
        {
          "Dataset": "EmoryNLP (Zahiri and Choi, 2018)",
          "Domain": "TV Show Scripts",
          "# Emotions": "7",
          "# Conv.": "659/89/79",
          "# Utter.": "7,551/954/984"
        },
        {
          "Dataset": "IEMOCAP (Busso et al., 2008)",
          "Domain": "Daily Chat",
          "# Emotions": "6",
          "# Conv.": "100/20/31",
          "# Utter.": "4,758/1,000/1,622"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table 2: , CTPT optimized by DFO algorithms",
      "data": [
        {
          "DailyDialog (Li et al., 2017)\nDaily Chat": "MELD (Poria et al., 2019)\nTV Show Scripts",
          "7\n11,118/1,000/1,000\n87,170/8,069/7,740": "7\n1,038/114/280\n9,989/1,109/2,610"
        },
        {
          "DailyDialog (Li et al., 2017)\nDaily Chat": "EmoryNLP (Zahiri and Choi, 2018)\nTV Show Scripts",
          "7\n11,118/1,000/1,000\n87,170/8,069/7,740": "7\n659/89/79\n7,551/954/984"
        },
        {
          "DailyDialog (Li et al., 2017)\nDaily Chat": "IEMOCAP (Busso et al., 2008)\nDaily Chat",
          "7\n11,118/1,000/1,000\n87,170/8,069/7,740": "6\n100/20/31\n4,758/1,000/1,622"
        },
        {
          "DailyDialog (Li et al., 2017)\nDaily Chat": "Table 1: Statistics of five ERC datasets. a/b/c indicates the number of examples in the training set, development set,",
          "7\n11,118/1,000/1,000\n87,170/8,069/7,740": ""
        },
        {
          "DailyDialog (Li et al., 2017)\nDaily Chat": "and testing set, respectively.",
          "7\n11,118/1,000/1,000\n87,170/8,069/7,740": ""
        },
        {
          "DailyDialog (Li et al., 2017)\nDaily Chat": "Following Sun et al.\n(2022) and Lester et al.",
          "7\n11,118/1,000/1,000\n87,170/8,069/7,740": "0.32"
        },
        {
          "DailyDialog (Li et al., 2017)\nDaily Chat": "(2021), we use the soft prompt template and only",
          "7\n11,118/1,000/1,000\n87,170/8,069/7,740": ""
        },
        {
          "DailyDialog (Li et al., 2017)\nDaily Chat": "train the continuous prompts extended to the in-",
          "7\n11,118/1,000/1,000\n87,170/8,069/7,740": "0.3"
        },
        {
          "DailyDialog (Li et al., 2017)\nDaily Chat": "put texts while freezing the PLM parameters. We",
          "7\n11,118/1,000/1,000\n87,170/8,069/7,740": "macro-F1"
        },
        {
          "DailyDialog (Li et al., 2017)\nDaily Chat": "utilize a Covariance Matrix Adaptation Evolution",
          "7\n11,118/1,000/1,000\n87,170/8,069/7,740": "0.28"
        },
        {
          "DailyDialog (Li et al., 2017)\nDaily Chat": "Strategy (CMA-ES) (Hansen and Ostermeier, 2001;",
          "7\n11,118/1,000/1,000\n87,170/8,069/7,740": ""
        },
        {
          "DailyDialog (Li et al., 2017)\nDaily Chat": "",
          "7\n11,118/1,000/1,000\n87,170/8,069/7,740": "MELD"
        },
        {
          "DailyDialog (Li et al., 2017)\nDaily Chat": "",
          "7\n11,118/1,000/1,000\n87,170/8,069/7,740": "0.26"
        },
        {
          "DailyDialog (Li et al., 2017)\nDaily Chat": "Hansen et al., 2003)\nto optimize the parameters",
          "7\n11,118/1,000/1,000\n87,170/8,069/7,740": "IEMOCAP"
        },
        {
          "DailyDialog (Li et al., 2017)\nDaily Chat": "without gradient information.",
          "7\n11,118/1,000/1,000\n87,170/8,069/7,740": ""
        },
        {
          "DailyDialog (Li et al., 2017)\nDaily Chat": "",
          "7\n11,118/1,000/1,000\n87,170/8,069/7,740": "0\n1\n2\n3\n4"
        },
        {
          "DailyDialog (Li et al., 2017)\nDaily Chat": "We use the soft template extended to the input",
          "7\n11,118/1,000/1,000\n87,170/8,069/7,740": "Number of External Source Tasks"
        },
        {
          "DailyDialog (Li et al., 2017)\nDaily Chat": "sequence with a length of 50. The last\ntoken of",
          "7\n11,118/1,000/1,000\n87,170/8,069/7,740": ""
        },
        {
          "DailyDialog (Li et al., 2017)\nDaily Chat": "",
          "7\n11,118/1,000/1,000\n87,170/8,069/7,740": "Figure 2: Impacts of removing external source tasks for"
        },
        {
          "DailyDialog (Li et al., 2017)\nDaily Chat": "the template is set as <unk> so that the model can",
          "7\n11,118/1,000/1,000\n87,170/8,069/7,740": ""
        },
        {
          "DailyDialog (Li et al., 2017)\nDaily Chat": "",
          "7\n11,118/1,000/1,000\n87,170/8,069/7,740": "MELD and IEMOCAP."
        },
        {
          "DailyDialog (Li et al., 2017)\nDaily Chat": "better predict the masked token. Since the task is",
          "7\n11,118/1,000/1,000\n87,170/8,069/7,740": ""
        },
        {
          "DailyDialog (Li et al., 2017)\nDaily Chat": "reformulated as a generalization task with the text-",
          "7\n11,118/1,000/1,000\n87,170/8,069/7,740": ""
        },
        {
          "DailyDialog (Li et al., 2017)\nDaily Chat": "to-text format, we choose T5 (Raffel et al., 2020)",
          "7\n11,118/1,000/1,000\n87,170/8,069/7,740": "with TSPT. Specifically,\nin addition to EC that"
        },
        {
          "DailyDialog (Li et al., 2017)\nDaily Chat": "as our backbone model.",
          "7\n11,118/1,000/1,000\n87,170/8,069/7,740": "TSPT has already obtained a high performance"
        },
        {
          "DailyDialog (Li et al., 2017)\nDaily Chat": "",
          "7\n11,118/1,000/1,000\n87,170/8,069/7,740": "under\nthe few-shot setting, CTPT brings signif-"
        },
        {
          "DailyDialog (Li et al., 2017)\nDaily Chat": "4.4\nEvaluation Metric",
          "7\n11,118/1,000/1,000\n87,170/8,069/7,740": ""
        },
        {
          "DailyDialog (Li et al., 2017)\nDaily Chat": "",
          "7\n11,118/1,000/1,000\n87,170/8,069/7,740": "icant\nimprovement compared with TSPT, which"
        },
        {
          "DailyDialog (Li et al., 2017)\nDaily Chat": "For EC and DailyDialog, due to the imbalance dis-",
          "7\n11,118/1,000/1,000\n87,170/8,069/7,740": "demonstrates the effectiveness of utilizing cross-"
        },
        {
          "DailyDialog (Li et al., 2017)\nDaily Chat": "tribution of categories (more than 80% examples",
          "7\n11,118/1,000/1,000\n87,170/8,069/7,740": "task knowledge.\nSince DailyDialog and MELD"
        },
        {
          "DailyDialog (Li et al., 2017)\nDaily Chat": "are neutral emotion), we use micro-averaged F1",
          "7\n11,118/1,000/1,000\n87,170/8,069/7,740": "share the same emotion labels, CTPT obtains the"
        },
        {
          "DailyDialog (Li et al., 2017)\nDaily Chat": "score excluding neutral category following Chat-",
          "7\n11,118/1,000/1,000\n87,170/8,069/7,740": "most gain on these two datasets, which shows that"
        },
        {
          "DailyDialog (Li et al., 2017)\nDaily Chat": "terjee et al.\n(2019).\nFor\nthe rest\nthree datasets,",
          "7\n11,118/1,000/1,000\n87,170/8,069/7,740": "our cross-task prompt tuning can learn emotional"
        },
        {
          "DailyDialog (Li et al., 2017)\nDaily Chat": "following Majumder et al. (2019), we use weighted",
          "7\n11,118/1,000/1,000\n87,170/8,069/7,740": "knowledge from the labels."
        },
        {
          "DailyDialog (Li et al., 2017)\nDaily Chat": "macro-F1 score. The overall evaluation setting is",
          "7\n11,118/1,000/1,000\n87,170/8,069/7,740": ""
        },
        {
          "DailyDialog (Li et al., 2017)\nDaily Chat": "",
          "7\n11,118/1,000/1,000\n87,170/8,069/7,740": "5.2\nModel Analysis"
        },
        {
          "DailyDialog (Li et al., 2017)\nDaily Chat": "the same as Zhong et al. (2019).",
          "7\n11,118/1,000/1,000\n87,170/8,069/7,740": ""
        },
        {
          "DailyDialog (Li et al., 2017)\nDaily Chat": "",
          "7\n11,118/1,000/1,000\n87,170/8,069/7,740": "To gain deeper insights into CTPT from diverse"
        },
        {
          "DailyDialog (Li et al., 2017)\nDaily Chat": "5\nResults and Analysis",
          "7\n11,118/1,000/1,000\n87,170/8,069/7,740": ""
        },
        {
          "DailyDialog (Li et al., 2017)\nDaily Chat": "",
          "7\n11,118/1,000/1,000\n87,170/8,069/7,740": "angles, we undertake analytical experiments in a"
        },
        {
          "DailyDialog (Li et al., 2017)\nDaily Chat": "",
          "7\n11,118/1,000/1,000\n87,170/8,069/7,740": "few-shot setting, where k = 16, in this section."
        },
        {
          "DailyDialog (Li et al., 2017)\nDaily Chat": "5.1\nMain Results",
          "7\n11,118/1,000/1,000\n87,170/8,069/7,740": ""
        },
        {
          "DailyDialog (Li et al., 2017)\nDaily Chat": "We compare the performance of CTPT against the",
          "7\n11,118/1,000/1,000\n87,170/8,069/7,740": "Analysis in Training Stage\nSince the training"
        },
        {
          "DailyDialog (Li et al., 2017)\nDaily Chat": "baselines aforementioned in Section 4.2. We first",
          "7\n11,118/1,000/1,000\n87,170/8,069/7,740": "stage of CTPT is different from other approaches,"
        },
        {
          "DailyDialog (Li et al., 2017)\nDaily Chat": "re-implement the baseline models and achieve the",
          "7\n11,118/1,000/1,000\n87,170/8,069/7,740": "it is worthwhile exploring the training stage."
        },
        {
          "DailyDialog (Li et al., 2017)\nDaily Chat": "similar performance reported in the original paper.",
          "7\n11,118/1,000/1,000\n87,170/8,069/7,740": "First,\nto prevent\nthe validation\nperformance"
        },
        {
          "DailyDialog (Li et al., 2017)\nDaily Chat": "Then we modify the preprocessing code to let all",
          "7\n11,118/1,000/1,000\n87,170/8,069/7,740": "degradation brought by DFO algorithms, we train"
        },
        {
          "DailyDialog (Li et al., 2017)\nDaily Chat": "baselines be trained under the same few-shot set-",
          "7\n11,118/1,000/1,000\n87,170/8,069/7,740": "CTPT with backpropagation methods. As shown"
        },
        {
          "DailyDialog (Li et al., 2017)\nDaily Chat": "ting. The result under the few-shot setting is shown",
          "7\n11,118/1,000/1,000\n87,170/8,069/7,740": "in Table 2, CTPT optimized by DFO algorithms"
        },
        {
          "DailyDialog (Li et al., 2017)\nDaily Chat": "in Table 2.",
          "7\n11,118/1,000/1,000\n87,170/8,069/7,740": "(CTPT w/o. BP) has a comparable result with that"
        },
        {
          "DailyDialog (Li et al., 2017)\nDaily Chat": "Compared with\nthe\nbaseline models,\ntask-",
          "7\n11,118/1,000/1,000\n87,170/8,069/7,740": "optimized by backpropagation methods (CTPT w."
        },
        {
          "DailyDialog (Li et al., 2017)\nDaily Chat": "specific prompt\ntuning (TSPT) outperforms fine-",
          "7\n11,118/1,000/1,000\n87,170/8,069/7,740": "BP). In some tasks such as MELD, DFO algorithms"
        },
        {
          "DailyDialog (Li et al., 2017)\nDaily Chat": "tuning PLMs on most\ntasks under\nthe few-shot",
          "7\n11,118/1,000/1,000\n87,170/8,069/7,740": "perform better than backpropagation methods. The"
        },
        {
          "DailyDialog (Li et al., 2017)\nDaily Chat": "setting. Meanwhile, benefiting from the cross-task",
          "7\n11,118/1,000/1,000\n87,170/8,069/7,740": "experiment result shows that CTPT obtains compa-"
        },
        {
          "DailyDialog (Li et al., 2017)\nDaily Chat": "knowledge, our proposed cross-task prompt\ntun-",
          "7\n11,118/1,000/1,000\n87,170/8,069/7,740": "rable results without derivative information, which"
        },
        {
          "DailyDialog (Li et al., 2017)\nDaily Chat": "ing (CTPT) obtains an improvement compared",
          "7\n11,118/1,000/1,000\n87,170/8,069/7,740": "can be deployed in non-GPU devices."
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Model": "KET (Zhong et al., 2019)",
          "EC": "0.1296",
          "DailyDialog": "0.0909",
          "MELD": "0.0897",
          "EmoryNLP": "0.1312",
          "IEMOCAP": "0.1646"
        },
        {
          "Model": "TUCORE-GCN (Lee and Choi, 2021)",
          "EC": "0.1918",
          "DailyDialog": "0.2029",
          "MELD": "0.2596",
          "EmoryNLP": "0.1311",
          "IEMOCAP": "0.1527"
        },
        {
          "Model": "EmotionFlow (Song et al., 2022b)",
          "EC": "0.4084",
          "DailyDialog": "0.3749",
          "MELD": "0.2934",
          "EmoryNLP": "0.1465",
          "IEMOCAP": "0.1699"
        },
        {
          "Model": "SPCL (Song et al., 2022a)",
          "EC": "0.4269",
          "DailyDialog": "0.3699",
          "MELD": "0.2941",
          "EmoryNLP": "0.1499",
          "IEMOCAP": "0.1873"
        },
        {
          "Model": "TSPT",
          "EC": "0.6274",
          "DailyDialog": "0.4996",
          "MELD": "0.2521",
          "EmoryNLP": "0.1613",
          "IEMOCAP": "0.2877"
        },
        {
          "Model": "TSPT + CTPL",
          "EC": "0.6226",
          "DailyDialog": "0.5193",
          "MELD": "0.2732",
          "EmoryNLP": "0.1724",
          "IEMOCAP": "0.2829"
        },
        {
          "Model": "CTPT (w/o. BP)",
          "EC": "0.6394",
          "DailyDialog": "0.5571",
          "MELD": "0.3212",
          "EmoryNLP": "0.1902",
          "IEMOCAP": "0.3124"
        },
        {
          "Model": "CTPT (w. BP)",
          "EC": "0.6405",
          "DailyDialog": "0.5588",
          "MELD": "0.3128",
          "EmoryNLP": "0.2057",
          "IEMOCAP": "0.3182"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "prompt tuning, “CTPT” indicates cross-task prompt tuning. The result of EC and DailyDialog are micro-averaged": "F1, and the result of other datasets are weighted macro-F1. We bolded the best result and underline the second best."
        },
        {
          "prompt tuning, “CTPT” indicates cross-task prompt tuning. The result of EC and DailyDialog are micro-averaged": "Training\nGPU Memory"
        },
        {
          "prompt tuning, “CTPT” indicates cross-task prompt tuning. The result of EC and DailyDialog are micro-averaged": "Micro-F1"
        },
        {
          "prompt tuning, “CTPT” indicates cross-task prompt tuning. The result of EC and DailyDialog are micro-averaged": "Time\nUsage"
        },
        {
          "prompt tuning, “CTPT” indicates cross-task prompt tuning. The result of EC and DailyDialog are micro-averaged": ""
        },
        {
          "prompt tuning, “CTPT” indicates cross-task prompt tuning. The result of EC and DailyDialog are micro-averaged": "KET\n0.0909\n4.5 mins\n1.2 GB"
        },
        {
          "prompt tuning, “CTPT” indicates cross-task prompt tuning. The result of EC and DailyDialog are micro-averaged": "EmotionFlow\n0.3749\n7.5 mins\n8.7 GB"
        },
        {
          "prompt tuning, “CTPT” indicates cross-task prompt tuning. The result of EC and DailyDialog are micro-averaged": ""
        },
        {
          "prompt tuning, “CTPT” indicates cross-task prompt tuning. The result of EC and DailyDialog are micro-averaged": "SPCL\n0.3699\n7 mins\n7.2 GB"
        },
        {
          "prompt tuning, “CTPT” indicates cross-task prompt tuning. The result of EC and DailyDialog are micro-averaged": ""
        },
        {
          "prompt tuning, “CTPT” indicates cross-task prompt tuning. The result of EC and DailyDialog are micro-averaged": "CTPT\n0.5571\n6 mins\n2.8 GB"
        },
        {
          "prompt tuning, “CTPT” indicates cross-task prompt tuning. The result of EC and DailyDialog are micro-averaged": ""
        },
        {
          "prompt tuning, “CTPT” indicates cross-task prompt tuning. The result of EC and DailyDialog are micro-averaged": "Table 3: Comparison of resources requirements on Dai-"
        },
        {
          "prompt tuning, “CTPT” indicates cross-task prompt tuning. The result of EC and DailyDialog are micro-averaged": "lyDialog."
        },
        {
          "prompt tuning, “CTPT” indicates cross-task prompt tuning. The result of EC and DailyDialog are micro-averaged": ""
        },
        {
          "prompt tuning, “CTPT” indicates cross-task prompt tuning. The result of EC and DailyDialog are micro-averaged": ""
        },
        {
          "prompt tuning, “CTPT” indicates cross-task prompt tuning. The result of EC and DailyDialog are micro-averaged": "DailyDialog\nIEMOCAP"
        },
        {
          "prompt tuning, “CTPT” indicates cross-task prompt tuning. The result of EC and DailyDialog are micro-averaged": ""
        },
        {
          "prompt tuning, “CTPT” indicates cross-task prompt tuning. The result of EC and DailyDialog are micro-averaged": "TSPT\n0.4996\n0.2877"
        },
        {
          "prompt tuning, “CTPT” indicates cross-task prompt tuning. The result of EC and DailyDialog are micro-averaged": "CTPT"
        },
        {
          "prompt tuning, “CTPT” indicates cross-task prompt tuning. The result of EC and DailyDialog are micro-averaged": ""
        },
        {
          "prompt tuning, “CTPT” indicates cross-task prompt tuning. The result of EC and DailyDialog are micro-averaged": "w/o. EK\n0.5481\n0.3076"
        },
        {
          "prompt tuning, “CTPT” indicates cross-task prompt tuning. The result of EC and DailyDialog are micro-averaged": "w. EK\n0.5571\n0.3124"
        },
        {
          "prompt tuning, “CTPT” indicates cross-task prompt tuning. The result of EC and DailyDialog are micro-averaged": ""
        },
        {
          "prompt tuning, “CTPT” indicates cross-task prompt tuning. The result of EC and DailyDialog are micro-averaged": "Table 4: Ablations of emotional knowledge. Here “EK”"
        },
        {
          "prompt tuning, “CTPT” indicates cross-task prompt tuning. The result of EC and DailyDialog are micro-averaged": ""
        },
        {
          "prompt tuning, “CTPT” indicates cross-task prompt tuning. The result of EC and DailyDialog are micro-averaged": "indicates “Emotional Knowledge”."
        },
        {
          "prompt tuning, “CTPT” indicates cross-task prompt tuning. The result of EC and DailyDialog are micro-averaged": ""
        },
        {
          "prompt tuning, “CTPT” indicates cross-task prompt tuning. The result of EC and DailyDialog are micro-averaged": ""
        },
        {
          "prompt tuning, “CTPT” indicates cross-task prompt tuning. The result of EC and DailyDialog are micro-averaged": ""
        },
        {
          "prompt tuning, “CTPT” indicates cross-task prompt tuning. The result of EC and DailyDialog are micro-averaged": "Second,\nto explore the training efficiency of"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Second,\nto explore the training efficiency of": ""
        },
        {
          "Second,\nto explore the training efficiency of": "CTPT, we compare CTPT and other baseline mod-"
        },
        {
          "Second,\nto explore the training efficiency of": ""
        },
        {
          "Second,\nto explore the training efficiency of": "els in terms of training resource requirements. Sim-"
        },
        {
          "Second,\nto explore the training efficiency of": ""
        },
        {
          "Second,\nto explore the training efficiency of": "ply, we compare the two main factors:\ntraining"
        },
        {
          "Second,\nto explore the training efficiency of": ""
        },
        {
          "Second,\nto explore the training efficiency of": "time and GPU memory usage. All the methods are"
        },
        {
          "Second,\nto explore the training efficiency of": ""
        },
        {
          "Second,\nto explore the training efficiency of": "implemented with PyTorch and experimented on a"
        },
        {
          "Second,\nto explore the training efficiency of": ""
        },
        {
          "Second,\nto explore the training efficiency of": "single Tesla V-100 GPU. We keep the batch size as"
        },
        {
          "Second,\nto explore the training efficiency of": ""
        },
        {
          "Second,\nto explore the training efficiency of": "one for a fair comparison. The experiment results"
        },
        {
          "Second,\nto explore the training efficiency of": ""
        },
        {
          "Second,\nto explore the training efficiency of": "show that CTPT is training efficiency compared"
        },
        {
          "Second,\nto explore the training efficiency of": ""
        },
        {
          "Second,\nto explore the training efficiency of": "to EmotionFlow and SPCL. As shown in Table 3,"
        },
        {
          "Second,\nto explore the training efficiency of": ""
        },
        {
          "Second,\nto explore the training efficiency of": "CTPT requires less training time a less GPU mem-"
        },
        {
          "Second,\nto explore the training efficiency of": ""
        },
        {
          "Second,\nto explore the training efficiency of": "ory than EmotionFlow and SPCL while offering a"
        },
        {
          "Second,\nto explore the training efficiency of": ""
        },
        {
          "Second,\nto explore the training efficiency of": "better validation performance."
        }
      ],
      "page": 8
    },
    {
      "caption": "Table 5: , when employing",
      "data": [
        {
          "Target Task": ""
        },
        {
          "Target Task": "Source Task"
        },
        {
          "Target Task": "EC"
        },
        {
          "Target Task": "DailyDialog"
        },
        {
          "Target Task": "MELD"
        },
        {
          "Target Task": "EmoryNLP"
        },
        {
          "Target Task": "IEMOCAP"
        }
      ],
      "page": 9
    },
    {
      "caption": "Table 5: , when employing",
      "data": [
        {
          "0.5276\nDailyDialog": "MELD\n0.4579",
          "0.2400\n0.0308\n0.2204": "0.4834\n0.0245\n0.2313"
        },
        {
          "0.5276\nDailyDialog": "EmoryNLP\n0.3642",
          "0.2400\n0.0308\n0.2204": "0.2658\n0.1804\n0.1315"
        },
        {
          "0.5276\nDailyDialog": "IEMOCAP\n0.3870",
          "0.2400\n0.0308\n0.2204": "0.0599\n0.2192\n0.1104"
        },
        {
          "0.5276\nDailyDialog": "Table 6: Performance of zero-shot",
          "0.2400\n0.0308\n0.2204": "transfers. The task-specific prompt of the target\ntask is excluded during the"
        },
        {
          "0.5276\nDailyDialog": "training stage. We bolded the best zero-shot transfer result for each target task.",
          "0.2400\n0.0308\n0.2204": ""
        },
        {
          "0.5276\nDailyDialog": "as our primary backbone model. To further validate",
          "0.2400\n0.0308\n0.2204": "the two tasks have, the better zero-shot transfer per-"
        },
        {
          "0.5276\nDailyDialog": "the generalizability of CTPT, we conducted addi-",
          "0.2400\n0.0308\n0.2204": "formance they obtain. In summary, the experiment"
        },
        {
          "0.5276\nDailyDialog": "tional experiments using an encoder-only backbone",
          "0.2400\n0.0308\n0.2204": "result shows that CTPT has a good generalization"
        },
        {
          "0.5276\nDailyDialog": "language model, RoBERTa.",
          "0.2400\n0.0308\n0.2204": "ability in zero-shot transfer."
        },
        {
          "0.5276\nDailyDialog": "As\nevidenced\nin Table\n5, when\nemploying",
          "0.2400\n0.0308\n0.2204": ""
        },
        {
          "0.5276\nDailyDialog": "RoBERTa as the backbone model, CTPT achieves",
          "0.2400\n0.0308\n0.2204": "6\nConclusion"
        },
        {
          "0.5276\nDailyDialog": "results that surpass the TSPT baseline, compara-",
          "0.2400\n0.0308\n0.2204": ""
        },
        {
          "0.5276\nDailyDialog": "",
          "0.2400\n0.0308\n0.2204": "In this paper, we strictly define the task of\nthe"
        },
        {
          "0.5276\nDailyDialog": "ble to those obtained with T5. This underscores",
          "0.2400\n0.0308\n0.2204": ""
        },
        {
          "0.5276\nDailyDialog": "",
          "0.2400\n0.0308\n0.2204": "few-shot setting for ERC and propose a cross-task"
        },
        {
          "0.5276\nDailyDialog": "the robust generalizability of our proposed CTPT",
          "0.2400\n0.0308\n0.2204": ""
        },
        {
          "0.5276\nDailyDialog": "",
          "0.2400\n0.0308\n0.2204": "prompt tuning (CTPT) method to tackle this prob-"
        },
        {
          "0.5276\nDailyDialog": "across various backbone language models. It also",
          "0.2400\n0.0308\n0.2204": ""
        },
        {
          "0.5276\nDailyDialog": "",
          "0.2400\n0.0308\n0.2204": "lem utilizing the cross-task knowledge.\nCTPT"
        },
        {
          "0.5276\nDailyDialog": "suggests the potential for consistent enhancement",
          "0.2400\n0.0308\n0.2204": ""
        },
        {
          "0.5276\nDailyDialog": "",
          "0.2400\n0.0308\n0.2204": "learns from external task-specific knowledge from"
        },
        {
          "0.5276\nDailyDialog": "in downstream task performance with the adoption",
          "0.2400\n0.0308\n0.2204": ""
        },
        {
          "0.5276\nDailyDialog": "",
          "0.2400\n0.0308\n0.2204": "other tasks and emotional knowledge from com-"
        },
        {
          "0.5276\nDailyDialog": "of even more advanced backbone models.",
          "0.2400\n0.0308\n0.2204": ""
        },
        {
          "0.5276\nDailyDialog": "",
          "0.2400\n0.0308\n0.2204": "monsense and then summarizes the learned cross-"
        },
        {
          "0.5276\nDailyDialog": "",
          "0.2400\n0.0308\n0.2204": "task knowledge to improve the validation perfor-"
        },
        {
          "0.5276\nDailyDialog": "5.3\nZero-Shot Transfer",
          "0.2400\n0.0308\n0.2204": ""
        },
        {
          "0.5276\nDailyDialog": "",
          "0.2400\n0.0308\n0.2204": "mance.\nWe use\na derivative-free optimization"
        },
        {
          "0.5276\nDailyDialog": "In real-world scenarios, annotated training exam-",
          "0.2400\n0.0308\n0.2204": ""
        },
        {
          "0.5276\nDailyDialog": "",
          "0.2400\n0.0308\n0.2204": "method to optimize the parameters without gradi-"
        },
        {
          "0.5276\nDailyDialog": "ples are not always available. Therefore, it is worth-",
          "0.2400\n0.0308\n0.2204": ""
        },
        {
          "0.5276\nDailyDialog": "",
          "0.2400\n0.0308\n0.2204": "ent information, which skips the backpropagation"
        },
        {
          "0.5276\nDailyDialog": "while exploring the zero-shot generalization ability",
          "0.2400\n0.0308\n0.2204": ""
        },
        {
          "0.5276\nDailyDialog": "",
          "0.2400\n0.0308\n0.2204": "stage. Experiments on ERC benchmarks show that"
        },
        {
          "0.5276\nDailyDialog": "of CTPT.\nIn this subsection, we conduct exper-",
          "0.2400\n0.0308\n0.2204": ""
        },
        {
          "0.5276\nDailyDialog": "",
          "0.2400\n0.0308\n0.2204": "CTPT can outperform baseline models in the few-"
        },
        {
          "0.5276\nDailyDialog": "iments under\nthe zero-shot setting that\ntrain the",
          "0.2400\n0.0308\n0.2204": ""
        },
        {
          "0.5276\nDailyDialog": "",
          "0.2400\n0.0308\n0.2204": "shot setting and obtain a surprising result\nin the"
        },
        {
          "0.5276\nDailyDialog": "prompt by source task and evaluate the target task",
          "0.2400\n0.0308\n0.2204": ""
        },
        {
          "0.5276\nDailyDialog": "",
          "0.2400\n0.0308\n0.2204": "zero-shot transfer. In summary, CTPT is training-"
        },
        {
          "0.5276\nDailyDialog": "while excluding the external task-specific prompt",
          "0.2400\n0.0308\n0.2204": ""
        },
        {
          "0.5276\nDailyDialog": "",
          "0.2400\n0.0308\n0.2204": "efficient\nthat\nincludes:\n(1) sample-efficiency:\nit"
        },
        {
          "0.5276\nDailyDialog": "from the target task.",
          "0.2400\n0.0308\n0.2204": ""
        },
        {
          "0.5276\nDailyDialog": "",
          "0.2400\n0.0308\n0.2204": "is trained by few-shot\ntraining examples, and (2)"
        },
        {
          "0.5276\nDailyDialog": "As shown in Table 6, CTPT performs surpris-",
          "0.2400\n0.0308\n0.2204": ""
        },
        {
          "0.5276\nDailyDialog": "",
          "0.2400\n0.0308\n0.2204": "computational-efficiency:\nit tunes only about 1,000"
        },
        {
          "0.5276\nDailyDialog": "ingly under the zero-shot transfer. It outperforms",
          "0.2400\n0.0308\n0.2204": ""
        },
        {
          "0.5276\nDailyDialog": "",
          "0.2400\n0.0308\n0.2204": "parameters with derivative-free algorithms that skip"
        },
        {
          "0.5276\nDailyDialog": "baseline methods under the few-shot setting in EC,",
          "0.2400\n0.0308\n0.2204": ""
        },
        {
          "0.5276\nDailyDialog": "",
          "0.2400\n0.0308\n0.2204": "the backpropagation."
        },
        {
          "0.5276\nDailyDialog": "DailyDialog and IEMOCAP. Compared with the",
          "0.2400\n0.0308\n0.2204": ""
        },
        {
          "0.5276\nDailyDialog": "few-shot result of TSPT, CTPT zero-shot obtains",
          "0.2400\n0.0308\n0.2204": ""
        },
        {
          "0.5276\nDailyDialog": "",
          "0.2400\n0.0308\n0.2204": "Acknowledgements"
        },
        {
          "0.5276\nDailyDialog": "better performance in DailyDialog and a sightly",
          "0.2400\n0.0308\n0.2204": ""
        },
        {
          "0.5276\nDailyDialog": "degradation in MELD and IEMOCAP.",
          "0.2400\n0.0308\n0.2204": "This research is supported,\nin part, by the Joint"
        },
        {
          "0.5276\nDailyDialog": "Due to the similarity among the first three tasks",
          "0.2400\n0.0308\n0.2204": "NTU-WeBank Research Centre on Fintech (Award"
        },
        {
          "0.5276\nDailyDialog": "(EC, DailyDialog, and MELD), the prompt trained",
          "0.2400\n0.0308\n0.2204": "No. NWJ-2020-007), Nanyang Technological Uni-"
        },
        {
          "0.5276\nDailyDialog": "by these three tasks can be easily transferred with",
          "0.2400\n0.0308\n0.2204": "versity, Singapore.\nThis\nresearch is\nalso sup-"
        },
        {
          "0.5276\nDailyDialog": "each other, which almost achieves the result of",
          "0.2400\n0.0308\n0.2204": "ported, in part, by the National Research Founda-"
        },
        {
          "0.5276\nDailyDialog": "TSPT under 16-shot. Meanwhile, since the con-",
          "0.2400\n0.0308\n0.2204": "tion, Prime Minister’s Office, Singapore under its"
        },
        {
          "0.5276\nDailyDialog": "versations in EmoryNLP contain fine-grained and",
          "0.2400\n0.0308\n0.2204": "NRF Investigatorship Programme (NRFI Award"
        },
        {
          "0.5276\nDailyDialog": "more complex emotions, prompts\nlearned from",
          "0.2400\n0.0308\n0.2204": "No.\nNRF-NRFI05-2019-0002).\nAny opinions,"
        },
        {
          "0.5276\nDailyDialog": "other tasks can hardly be transferred to EmoryNLP.",
          "0.2400\n0.0308\n0.2204": "findings and conclusions or recommendations ex-"
        },
        {
          "0.5276\nDailyDialog": "Specifically, the results of zero-shot transfer from",
          "0.2400\n0.0308\n0.2204": "pressed in this material are those of\nthe authors"
        },
        {
          "0.5276\nDailyDialog": "the first three tasks to the rest two tasks are poor,",
          "0.2400\n0.0308\n0.2204": "and do not reflect the views of National Research"
        },
        {
          "0.5276\nDailyDialog": "and vice versa. In other words, the more similarity",
          "0.2400\n0.0308\n0.2204": "Foundation, Singapore."
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Limitations": "",
          "2020. Language models are few-shot learners.\nIn Ad-": "vances in Neural Information Processing Systems 33:"
        },
        {
          "Limitations": "Though our proposed CTPT works well in source-",
          "2020. Language models are few-shot learners.\nIn Ad-": "Annual Conference on Neural Information Process-"
        },
        {
          "Limitations": "",
          "2020. Language models are few-shot learners.\nIn Ad-": "ing Systems 2020, NeurIPS 2020, December 6-12,"
        },
        {
          "Limitations": "limited scenarios, it has two main limitations:",
          "2020. Language models are few-shot learners.\nIn Ad-": ""
        },
        {
          "Limitations": "",
          "2020. Language models are few-shot learners.\nIn Ad-": "2020, virtual."
        },
        {
          "Limitations": "• The DFO algorithm we\nuse\nis\nunder\nthe",
          "2020. Language models are few-shot learners.\nIn Ad-": ""
        },
        {
          "Limitations": "",
          "2020. Language models are few-shot learners.\nIn Ad-": "Carlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe"
        },
        {
          "Limitations": "sampling-and-updating structure so we need",
          "2020. Language models are few-shot learners.\nIn Ad-": ""
        },
        {
          "Limitations": "",
          "2020. Language models are few-shot learners.\nIn Ad-": "Kazemzadeh, Emily Mower, Samuel Kim,\nJean-"
        },
        {
          "Limitations": "to compute\nthe\nlogits of\nall\nsampled can-",
          "2020. Language models are few-shot learners.\nIn Ad-": ""
        },
        {
          "Limitations": "",
          "2020. Language models are few-shot learners.\nIn Ad-": "nette N. Chang, Sungbok Lee,\nand Shrikanth S."
        },
        {
          "Limitations": "didate solutions to select\nthe most optimal",
          "2020. Language models are few-shot learners.\nIn Ad-": "Narayanan. 2008.\nIEMOCAP: interactive emotional"
        },
        {
          "Limitations": "",
          "2020. Language models are few-shot learners.\nIn Ad-": "dyadic motion capture database. Lang. Resour. Eval-"
        },
        {
          "Limitations": "one. Meanwhile,\nthe convergence speed of",
          "2020. Language models are few-shot learners.\nIn Ad-": ""
        },
        {
          "Limitations": "",
          "2020. Language models are few-shot learners.\nIn Ad-": "uation, 42(4):335–359."
        },
        {
          "Limitations": "DFO algorithms is slower than backpropaga-",
          "2020. Language models are few-shot learners.\nIn Ad-": ""
        },
        {
          "Limitations": "tion. Therefore, CTPT requires more forward",
          "2020. Language models are few-shot learners.\nIn Ad-": ""
        },
        {
          "Limitations": "",
          "2020. Language models are few-shot learners.\nIn Ad-": "Ankush Chatterjee, Umang Gupta, Manoj Kumar Chin-"
        },
        {
          "Limitations": "passes than derivative-based methods due to",
          "2020. Language models are few-shot learners.\nIn Ad-": "nakotla, Radhakrishnan Srikanth, Michel Galley, and"
        },
        {
          "Limitations": "the aforementioned limitations.",
          "2020. Language models are few-shot learners.\nIn Ad-": "Puneet Agrawal. 2019. Understanding emotions in"
        },
        {
          "Limitations": "",
          "2020. Language models are few-shot learners.\nIn Ad-": "text using deep learning and big data. Comput. Hum."
        },
        {
          "Limitations": "",
          "2020. Language models are few-shot learners.\nIn Ad-": "Behav., 93:309–317."
        },
        {
          "Limitations": "•\nIn this paper, we use T5 as our backbone",
          "2020. Language models are few-shot learners.\nIn Ad-": ""
        },
        {
          "Limitations": "model. However, many large language models",
          "2020. Language models are few-shot learners.\nIn Ad-": ""
        },
        {
          "Limitations": "",
          "2020. Language models are few-shot learners.\nIn Ad-": "Vishal Chudasama, Purbayan Kar, Ashish Gudmalwar,"
        },
        {
          "Limitations": "have been proven successful in other scenar-",
          "2020. Language models are few-shot learners.\nIn Ad-": ""
        },
        {
          "Limitations": "",
          "2020. Language models are few-shot learners.\nIn Ad-": "Nirmesh Shah, Pankaj Wasnik, and Naoyuki Onoe."
        },
        {
          "Limitations": "ios. It is worthwhile to explore how to utilize",
          "2020. Language models are few-shot learners.\nIn Ad-": "2022. M2FNet: Multi-modal\nfusion network for"
        },
        {
          "Limitations": "a larger language model under source-limited",
          "2020. Language models are few-shot learners.\nIn Ad-": "emotion recognition in conversation.\nIn IEEE/CVF"
        },
        {
          "Limitations": "",
          "2020. Language models are few-shot learners.\nIn Ad-": "Conference on Computer Vision and Pattern Recog-"
        },
        {
          "Limitations": "scenarios in future.",
          "2020. Language models are few-shot learners.\nIn Ad-": ""
        },
        {
          "Limitations": "",
          "2020. Language models are few-shot learners.\nIn Ad-": "nition Workshops, CVPR Workshops 2022, New Or-"
        },
        {
          "Limitations": "",
          "2020. Language models are few-shot learners.\nIn Ad-": "leans, LA, USA, June 19-20, 2022, pages 4651–4660."
        },
        {
          "Limitations": "Ethics Statement",
          "2020. Language models are few-shot learners.\nIn Ad-": ""
        },
        {
          "Limitations": "",
          "2020. Language models are few-shot learners.\nIn Ad-": "IEEE."
        },
        {
          "Limitations": "In this paper, we do not involve extra ethical con-",
          "2020. Language models are few-shot learners.\nIn Ad-": ""
        },
        {
          "Limitations": "",
          "2020. Language models are few-shot learners.\nIn Ad-": "Laurence Devillers and Laurence Vidrascu. 2006. Real-"
        },
        {
          "Limitations": "siderations:",
          "2020. Language models are few-shot learners.\nIn Ad-": ""
        },
        {
          "Limitations": "",
          "2020. Language models are few-shot learners.\nIn Ad-": "life emotions detection with lexical and paralinguistic"
        },
        {
          "Limitations": "",
          "2020. Language models are few-shot learners.\nIn Ad-": "cues on human-human call center dialogs.\nIn INTER-"
        },
        {
          "Limitations": "•\nIn this paper, we do not release any new data.",
          "2020. Language models are few-shot learners.\nIn Ad-": "SPEECH 2006 - ICSLP, Ninth International Confer-"
        },
        {
          "Limitations": "All datasets we used are either public datasets",
          "2020. Language models are few-shot learners.\nIn Ad-": "ence on Spoken Language Processing, Pittsburgh, PA,"
        },
        {
          "Limitations": "",
          "2020. Language models are few-shot learners.\nIn Ad-": "USA, September 17-21, 2006. ISCA."
        },
        {
          "Limitations": "or licensed for academic usage.",
          "2020. Language models are few-shot learners.\nIn Ad-": ""
        },
        {
          "Limitations": "",
          "2020. Language models are few-shot learners.\nIn Ad-": "Jacob Devlin, Ming-Wei Chang, Kenton Lee,\nand"
        },
        {
          "Limitations": "•\nIn this paper, the source codes of baselines and",
          "2020. Language models are few-shot learners.\nIn Ad-": ""
        },
        {
          "Limitations": "",
          "2020. Language models are few-shot learners.\nIn Ad-": "Kristina Toutanova. 2019.\nBERT: pre-training of"
        },
        {
          "Limitations": "other artefacts are open-sourced or licensed",
          "2020. Language models are few-shot learners.\nIn Ad-": ""
        },
        {
          "Limitations": "",
          "2020. Language models are few-shot learners.\nIn Ad-": "deep bidirectional transformers for language under-"
        },
        {
          "Limitations": "for academic usage.",
          "2020. Language models are few-shot learners.\nIn Ad-": "standing.\nIn Proceedings of the 2019 Conference of"
        },
        {
          "Limitations": "",
          "2020. Language models are few-shot learners.\nIn Ad-": "the North American Chapter of the Association for"
        },
        {
          "Limitations": "• Our paper does not use demographic or iden-",
          "2020. Language models are few-shot learners.\nIn Ad-": "Computational Linguistics: Human Language Tech-"
        },
        {
          "Limitations": "tity characteristics information, and it does",
          "2020. Language models are few-shot learners.\nIn Ad-": "nologies, NAACL-HLT 2019, Minneapolis, MN, USA,"
        },
        {
          "Limitations": "",
          "2020. Language models are few-shot learners.\nIn Ad-": "June 2-7, 2019, Volume 1 (Long and Short Papers),"
        },
        {
          "Limitations": "not harm anyone.",
          "2020. Language models are few-shot learners.\nIn Ad-": ""
        },
        {
          "Limitations": "",
          "2020. Language models are few-shot learners.\nIn Ad-": "pages 4171–4186. Association for Computational"
        },
        {
          "Limitations": "",
          "2020. Language models are few-shot learners.\nIn Ad-": "Linguistics."
        },
        {
          "Limitations": "",
          "2020. Language models are few-shot learners.\nIn Ad-": "Tianyu Gao, Adam Fisch,\nand Danqi Chen. 2021."
        },
        {
          "Limitations": "References",
          "2020. Language models are few-shot learners.\nIn Ad-": ""
        },
        {
          "Limitations": "",
          "2020. Language models are few-shot learners.\nIn Ad-": "Making pre-trained language models better few-shot"
        },
        {
          "Limitations": "Akari Asai, Mohammadreza Salehi, Matthew E Peters,",
          "2020. Language models are few-shot learners.\nIn Ad-": "learners.\nIn Proceedings of the 59th Annual Meeting"
        },
        {
          "Limitations": "and Hannaneh Hajishirzi. 2022.\nAttentional mix-",
          "2020. Language models are few-shot learners.\nIn Ad-": "of the Association for Computational Linguistics and"
        },
        {
          "Limitations": "tures of soft prompt\ntuning for parameter-efficient",
          "2020. Language models are few-shot learners.\nIn Ad-": "the 11th International Joint Conference on Natural"
        },
        {
          "Limitations": "arXiv\npreprint\nmulti-task\nknowledge\nsharing.",
          "2020. Language models are few-shot learners.\nIn Ad-": "Language Processing, ACL/IJCNLP 2021, (Volume 1:"
        },
        {
          "Limitations": "arXiv:2205.11961.",
          "2020. Language models are few-shot learners.\nIn Ad-": "Long Papers), Virtual Event, August 1-6, 2021, pages"
        },
        {
          "Limitations": "",
          "2020. Language models are few-shot learners.\nIn Ad-": "3816–3830. Association for Computational Linguis-"
        },
        {
          "Limitations": "Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie",
          "2020. Language models are few-shot learners.\nIn Ad-": "tics."
        },
        {
          "Limitations": "Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind",
          "2020. Language models are few-shot learners.\nIn Ad-": ""
        },
        {
          "Limitations": "Neelakantan, Pranav Shyam, Girish Sastry, Amanda",
          "2020. Language models are few-shot learners.\nIn Ad-": "Deepanway Ghosal, Navonil Majumder, Alexander F."
        },
        {
          "Limitations": "Askell,\nSandhini Agarwal,\nAriel Herbert-Voss,",
          "2020. Language models are few-shot learners.\nIn Ad-": "Gelbukh, Rada Mihalcea, and Soujanya Poria. 2020."
        },
        {
          "Limitations": "Gretchen Krueger, Tom Henighan, Rewon Child,",
          "2020. Language models are few-shot learners.\nIn Ad-": "COSMIC:\ncommonsense knowledge\nfor\nemotion"
        },
        {
          "Limitations": "Aditya Ramesh, Daniel M. Ziegler,\nJeffrey Wu,",
          "2020. Language models are few-shot learners.\nIn Ad-": "the\nidentification in conversations.\nIn Findings of"
        },
        {
          "Limitations": "Clemens Winter, Christopher Hesse, Mark Chen, Eric",
          "2020. Language models are few-shot learners.\nIn Ad-": "Association for Computational Linguistics: EMNLP"
        },
        {
          "Limitations": "Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess,",
          "2020. Language models are few-shot learners.\nIn Ad-": "2020, Online Event, 16-20 November 2020, volume"
        },
        {
          "Limitations": "Jack Clark, Christopher Berner, Sam McCandlish,",
          "2020. Language models are few-shot learners.\nIn Ad-": "EMNLP 2020 of Findings of ACL, pages 2470–2481."
        },
        {
          "Limitations": "Alec Radford,\nIlya Sutskever, and Dario Amodei.",
          "2020. Language models are few-shot learners.\nIn Ad-": "Association for Computational Linguistics."
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Yuxian Gu, Xu Han, Zhiyuan Liu, and Minlie Huang.": "2022. PPT: pre-trained prompt tuning for few-shot",
          "Chul Min Lee and Shrikanth S. Narayanan. 2005. To-": "IEEE\nward detecting emotions in spoken dialogs."
        },
        {
          "Yuxian Gu, Xu Han, Zhiyuan Liu, and Minlie Huang.": "the 60th Annual Meet-\nlearning.\nIn Proceedings of",
          "Chul Min Lee and Shrikanth S. Narayanan. 2005. To-": "Trans. Speech Audio Process., 13(2):293–303."
        },
        {
          "Yuxian Gu, Xu Han, Zhiyuan Liu, and Minlie Huang.": "ing of the Association for Computational Linguistics",
          "Chul Min Lee and Shrikanth S. Narayanan. 2005. To-": ""
        },
        {
          "Yuxian Gu, Xu Han, Zhiyuan Liu, and Minlie Huang.": "(Volume 1: Long Papers), ACL 2022, Dublin, Ireland,",
          "Chul Min Lee and Shrikanth S. Narayanan. 2005. To-": "Brian Lester, Rami Al-Rfou, and Noah Constant. 2021."
        },
        {
          "Yuxian Gu, Xu Han, Zhiyuan Liu, and Minlie Huang.": "May 22-27, 2022, pages 8410–8423. Association for",
          "Chul Min Lee and Shrikanth S. Narayanan. 2005. To-": "The power of scale for parameter-efficient prompt"
        },
        {
          "Yuxian Gu, Xu Han, Zhiyuan Liu, and Minlie Huang.": "Computational Linguistics.",
          "Chul Min Lee and Shrikanth S. Narayanan. 2005. To-": "the 2021 Conference on\ntuning.\nIn Proceedings of"
        },
        {
          "Yuxian Gu, Xu Han, Zhiyuan Liu, and Minlie Huang.": "",
          "Chul Min Lee and Shrikanth S. Narayanan. 2005. To-": "Empirical Methods in Natural Language Processing,"
        },
        {
          "Yuxian Gu, Xu Han, Zhiyuan Liu, and Minlie Huang.": "Gaël Guibon, Matthieu Labeau, Luce Lefeuvre, and",
          "Chul Min Lee and Shrikanth S. Narayanan. 2005. To-": "EMNLP 2021, Virtual Event / Punta Cana, Domini-"
        },
        {
          "Yuxian Gu, Xu Han, Zhiyuan Liu, and Minlie Huang.": "Chloé Clavel. 2022. Few-shot emotion recognition",
          "Chul Min Lee and Shrikanth S. Narayanan. 2005. To-": "can Republic, 7-11 November, 2021, pages 3045–"
        },
        {
          "Yuxian Gu, Xu Han, Zhiyuan Liu, and Minlie Huang.": "in conversation with sequential prototypical networks.",
          "Chul Min Lee and Shrikanth S. Narayanan. 2005. To-": "3059. Association for Computational Linguistics."
        },
        {
          "Yuxian Gu, Xu Han, Zhiyuan Liu, and Minlie Huang.": "Softw. Impacts, 12:100237.",
          "Chul Min Lee and Shrikanth S. Narayanan. 2005. To-": ""
        },
        {
          "Yuxian Gu, Xu Han, Zhiyuan Liu, and Minlie Huang.": "",
          "Chul Min Lee and Shrikanth S. Narayanan. 2005. To-": "Chunyuan Li, Heerad Farkhoor, Rosanne Liu, and Jason"
        },
        {
          "Yuxian Gu, Xu Han, Zhiyuan Liu, and Minlie Huang.": "",
          "Chul Min Lee and Shrikanth S. Narayanan. 2005. To-": "Yosinski. 2018. Measuring the intrinsic dimension"
        },
        {
          "Yuxian Gu, Xu Han, Zhiyuan Liu, and Minlie Huang.": "Xu Guo, Boyang Li, and Han Yu. 2022.\nImproving",
          "Chul Min Lee and Shrikanth S. Narayanan. 2005. To-": ""
        },
        {
          "Yuxian Gu, Xu Han, Zhiyuan Liu, and Minlie Huang.": "",
          "Chul Min Lee and Shrikanth S. Narayanan. 2005. To-": "of objective landscapes.\nIn 6th International Con-"
        },
        {
          "Yuxian Gu, Xu Han, Zhiyuan Liu, and Minlie Huang.": "the sample efficiency of prompt tuning with domain",
          "Chul Min Lee and Shrikanth S. Narayanan. 2005. To-": ""
        },
        {
          "Yuxian Gu, Xu Han, Zhiyuan Liu, and Minlie Huang.": "",
          "Chul Min Lee and Shrikanth S. Narayanan. 2005. To-": "ference on Learning Representations,\nICLR 2018,"
        },
        {
          "Yuxian Gu, Xu Han, Zhiyuan Liu, and Minlie Huang.": "adaptation.\nIn Findings of the Association for Com-",
          "Chul Min Lee and Shrikanth S. Narayanan. 2005. To-": ""
        },
        {
          "Yuxian Gu, Xu Han, Zhiyuan Liu, and Minlie Huang.": "",
          "Chul Min Lee and Shrikanth S. Narayanan. 2005. To-": "Vancouver, BC, Canada, April 30 - May 3, 2018,"
        },
        {
          "Yuxian Gu, Xu Han, Zhiyuan Liu, and Minlie Huang.": "putational Linguistics: EMNLP 2022, Abu Dhabi,",
          "Chul Min Lee and Shrikanth S. Narayanan. 2005. To-": ""
        },
        {
          "Yuxian Gu, Xu Han, Zhiyuan Liu, and Minlie Huang.": "",
          "Chul Min Lee and Shrikanth S. Narayanan. 2005. To-": "Conference Track Proceedings. OpenReview.net."
        },
        {
          "Yuxian Gu, Xu Han, Zhiyuan Liu, and Minlie Huang.": "United Arab Emirates, December 7-11, 2022, pages",
          "Chul Min Lee and Shrikanth S. Narayanan. 2005. To-": ""
        },
        {
          "Yuxian Gu, Xu Han, Zhiyuan Liu, and Minlie Huang.": "3523–3537. Association for Computational Linguis-",
          "Chul Min Lee and Shrikanth S. Narayanan. 2005. To-": ""
        },
        {
          "Yuxian Gu, Xu Han, Zhiyuan Liu, and Minlie Huang.": "",
          "Chul Min Lee and Shrikanth S. Narayanan. 2005. To-": "Xiang Lisa Li and Percy Liang. 2021. Prefix-Tuning:"
        },
        {
          "Yuxian Gu, Xu Han, Zhiyuan Liu, and Minlie Huang.": "tics.",
          "Chul Min Lee and Shrikanth S. Narayanan. 2005. To-": ""
        },
        {
          "Yuxian Gu, Xu Han, Zhiyuan Liu, and Minlie Huang.": "",
          "Chul Min Lee and Shrikanth S. Narayanan. 2005. To-": "Optimizing continuous prompts for generation.\nIn"
        },
        {
          "Yuxian Gu, Xu Han, Zhiyuan Liu, and Minlie Huang.": "",
          "Chul Min Lee and Shrikanth S. Narayanan. 2005. To-": "Proceedings of the 59th Annual Meeting of the Asso-"
        },
        {
          "Yuxian Gu, Xu Han, Zhiyuan Liu, and Minlie Huang.": "Nikolaus Hansen,\nSibylle D. Müller,\nand\nPetros",
          "Chul Min Lee and Shrikanth S. Narayanan. 2005. To-": ""
        },
        {
          "Yuxian Gu, Xu Han, Zhiyuan Liu, and Minlie Huang.": "",
          "Chul Min Lee and Shrikanth S. Narayanan. 2005. To-": "ciation for Computational Linguistics and the 11th"
        },
        {
          "Yuxian Gu, Xu Han, Zhiyuan Liu, and Minlie Huang.": "Koumoutsakos. 2003. Reducing the time complexity",
          "Chul Min Lee and Shrikanth S. Narayanan. 2005. To-": ""
        },
        {
          "Yuxian Gu, Xu Han, Zhiyuan Liu, and Minlie Huang.": "",
          "Chul Min Lee and Shrikanth S. Narayanan. 2005. To-": "International Joint Conference on Natural Language"
        },
        {
          "Yuxian Gu, Xu Han, Zhiyuan Liu, and Minlie Huang.": "of the derandomized evolution strategy with covari-",
          "Chul Min Lee and Shrikanth S. Narayanan. 2005. To-": ""
        },
        {
          "Yuxian Gu, Xu Han, Zhiyuan Liu, and Minlie Huang.": "",
          "Chul Min Lee and Shrikanth S. Narayanan. 2005. To-": "Processing, ACL/IJCNLP 2021,\n(Volume 1: Long"
        },
        {
          "Yuxian Gu, Xu Han, Zhiyuan Liu, and Minlie Huang.": "ance matrix adaptation (CMA-ES). Evol. Comput.,",
          "Chul Min Lee and Shrikanth S. Narayanan. 2005. To-": ""
        },
        {
          "Yuxian Gu, Xu Han, Zhiyuan Liu, and Minlie Huang.": "",
          "Chul Min Lee and Shrikanth S. Narayanan. 2005. To-": "Papers), Virtual Event, August 1-6, 2021, pages 4582–"
        },
        {
          "Yuxian Gu, Xu Han, Zhiyuan Liu, and Minlie Huang.": "11(1):1–18.",
          "Chul Min Lee and Shrikanth S. Narayanan. 2005. To-": ""
        },
        {
          "Yuxian Gu, Xu Han, Zhiyuan Liu, and Minlie Huang.": "",
          "Chul Min Lee and Shrikanth S. Narayanan. 2005. To-": "4597. Association for Computational Linguistics."
        },
        {
          "Yuxian Gu, Xu Han, Zhiyuan Liu, and Minlie Huang.": "Nikolaus Hansen and Andreas Ostermeier. 2001. Com-",
          "Chul Min Lee and Shrikanth S. Narayanan. 2005. To-": ""
        },
        {
          "Yuxian Gu, Xu Han, Zhiyuan Liu, and Minlie Huang.": "",
          "Chul Min Lee and Shrikanth S. Narayanan. 2005. To-": "Yanran Li, Hui Su, Xiaoyu Shen, Wenjie Li, Ziqiang"
        },
        {
          "Yuxian Gu, Xu Han, Zhiyuan Liu, and Minlie Huang.": "pletely derandomized self-adaptation in evolution",
          "Chul Min Lee and Shrikanth S. Narayanan. 2005. To-": ""
        },
        {
          "Yuxian Gu, Xu Han, Zhiyuan Liu, and Minlie Huang.": "",
          "Chul Min Lee and Shrikanth S. Narayanan. 2005. To-": "Cao, and Shuzi Niu. 2017. Dailydialog: A manually"
        },
        {
          "Yuxian Gu, Xu Han, Zhiyuan Liu, and Minlie Huang.": "strategies. Evol. Comput., 9(2):159–195.",
          "Chul Min Lee and Shrikanth S. Narayanan. 2005. To-": ""
        },
        {
          "Yuxian Gu, Xu Han, Zhiyuan Liu, and Minlie Huang.": "",
          "Chul Min Lee and Shrikanth S. Narayanan. 2005. To-": "labelled multi-turn dialogue dataset.\nIn Proceedings"
        },
        {
          "Yuxian Gu, Xu Han, Zhiyuan Liu, and Minlie Huang.": "",
          "Chul Min Lee and Shrikanth S. Narayanan. 2005. To-": "of the Eighth International Joint Conference on Natu-"
        },
        {
          "Yuxian Gu, Xu Han, Zhiyuan Liu, and Minlie Huang.": "Devamanyu Hazarika, Soujanya Poria, Amir Zadeh,",
          "Chul Min Lee and Shrikanth S. Narayanan. 2005. To-": ""
        },
        {
          "Yuxian Gu, Xu Han, Zhiyuan Liu, and Minlie Huang.": "",
          "Chul Min Lee and Shrikanth S. Narayanan. 2005. To-": "ral Language Processing, IJCNLP 2017, Taipei, Tai-"
        },
        {
          "Yuxian Gu, Xu Han, Zhiyuan Liu, and Minlie Huang.": "Erik Cambria, Louis-Philippe Morency, and Roger",
          "Chul Min Lee and Shrikanth S. Narayanan. 2005. To-": ""
        },
        {
          "Yuxian Gu, Xu Han, Zhiyuan Liu, and Minlie Huang.": "",
          "Chul Min Lee and Shrikanth S. Narayanan. 2005. To-": "wan, November 27 - December 1, 2017 - Volume 1:"
        },
        {
          "Yuxian Gu, Xu Han, Zhiyuan Liu, and Minlie Huang.": "Zimmermann. 2018.\nConversational memory net-",
          "Chul Min Lee and Shrikanth S. Narayanan. 2005. To-": ""
        },
        {
          "Yuxian Gu, Xu Han, Zhiyuan Liu, and Minlie Huang.": "",
          "Chul Min Lee and Shrikanth S. Narayanan. 2005. To-": "Long Papers, pages 986–995. Asian Federation of"
        },
        {
          "Yuxian Gu, Xu Han, Zhiyuan Liu, and Minlie Huang.": "work for emotion recognition in dyadic dialogue",
          "Chul Min Lee and Shrikanth S. Narayanan. 2005. To-": ""
        },
        {
          "Yuxian Gu, Xu Han, Zhiyuan Liu, and Minlie Huang.": "",
          "Chul Min Lee and Shrikanth S. Narayanan. 2005. To-": "Natural Language Processing."
        },
        {
          "Yuxian Gu, Xu Han, Zhiyuan Liu, and Minlie Huang.": "videos.\nIn Proceedings of the 2018 Conference of the",
          "Chul Min Lee and Shrikanth S. Narayanan. 2005. To-": ""
        },
        {
          "Yuxian Gu, Xu Han, Zhiyuan Liu, and Minlie Huang.": "North American Chapter of the Association for Com-",
          "Chul Min Lee and Shrikanth S. Narayanan. 2005. To-": ""
        },
        {
          "Yuxian Gu, Xu Han, Zhiyuan Liu, and Minlie Huang.": "",
          "Chul Min Lee and Shrikanth S. Narayanan. 2005. To-": "Zaijing Li, Fengxiao Tang, Ming Zhao, and Yusen Zhu."
        },
        {
          "Yuxian Gu, Xu Han, Zhiyuan Liu, and Minlie Huang.": "putational Linguistics: Human Language Technolo-",
          "Chul Min Lee and Shrikanth S. Narayanan. 2005. To-": ""
        },
        {
          "Yuxian Gu, Xu Han, Zhiyuan Liu, and Minlie Huang.": "",
          "Chul Min Lee and Shrikanth S. Narayanan. 2005. To-": "2022. EmoCaps: Emotion capsule based model for"
        },
        {
          "Yuxian Gu, Xu Han, Zhiyuan Liu, and Minlie Huang.": "gies, NAACL-HLT 2018, New Orleans, Louisiana,",
          "Chul Min Lee and Shrikanth S. Narayanan. 2005. To-": ""
        },
        {
          "Yuxian Gu, Xu Han, Zhiyuan Liu, and Minlie Huang.": "",
          "Chul Min Lee and Shrikanth S. Narayanan. 2005. To-": "conversational emotion recognition.\nIn Findings of"
        },
        {
          "Yuxian Gu, Xu Han, Zhiyuan Liu, and Minlie Huang.": "USA, June 1-6, 2018, Volume 1 (Long Papers), pages",
          "Chul Min Lee and Shrikanth S. Narayanan. 2005. To-": ""
        },
        {
          "Yuxian Gu, Xu Han, Zhiyuan Liu, and Minlie Huang.": "",
          "Chul Min Lee and Shrikanth S. Narayanan. 2005. To-": "the Association for Computational Linguistics: ACL"
        },
        {
          "Yuxian Gu, Xu Han, Zhiyuan Liu, and Minlie Huang.": "2122–2132. Association for Computational Linguis-",
          "Chul Min Lee and Shrikanth S. Narayanan. 2005. To-": ""
        },
        {
          "Yuxian Gu, Xu Han, Zhiyuan Liu, and Minlie Huang.": "",
          "Chul Min Lee and Shrikanth S. Narayanan. 2005. To-": "2022, Dublin, Ireland, May 22-27, 2022, pages 1610–"
        },
        {
          "Yuxian Gu, Xu Han, Zhiyuan Liu, and Minlie Huang.": "tics.",
          "Chul Min Lee and Shrikanth S. Narayanan. 2005. To-": ""
        },
        {
          "Yuxian Gu, Xu Han, Zhiyuan Liu, and Minlie Huang.": "",
          "Chul Min Lee and Shrikanth S. Narayanan. 2005. To-": "1618. Association for Computational Linguistics."
        },
        {
          "Yuxian Gu, Xu Han, Zhiyuan Liu, and Minlie Huang.": "Taichi Ishiwatari, Yuki Yasuda, Taro Miyazaki, and Jun",
          "Chul Min Lee and Shrikanth S. Narayanan. 2005. To-": ""
        },
        {
          "Yuxian Gu, Xu Han, Zhiyuan Liu, and Minlie Huang.": "",
          "Chul Min Lee and Shrikanth S. Narayanan. 2005. To-": "Jiacheng Liu, Alisa Liu, Ximing Lu, Sean Welleck, Pe-"
        },
        {
          "Yuxian Gu, Xu Han, Zhiyuan Liu, and Minlie Huang.": "Goto. 2020. Relation-aware graph attention networks",
          "Chul Min Lee and Shrikanth S. Narayanan. 2005. To-": ""
        },
        {
          "Yuxian Gu, Xu Han, Zhiyuan Liu, and Minlie Huang.": "",
          "Chul Min Lee and Shrikanth S. Narayanan. 2005. To-": "ter West, Ronan Le Bras, Yejin Choi, and Hannaneh"
        },
        {
          "Yuxian Gu, Xu Han, Zhiyuan Liu, and Minlie Huang.": "with relational position encodings for emotion recog-",
          "Chul Min Lee and Shrikanth S. Narayanan. 2005. To-": ""
        },
        {
          "Yuxian Gu, Xu Han, Zhiyuan Liu, and Minlie Huang.": "",
          "Chul Min Lee and Shrikanth S. Narayanan. 2005. To-": "Hajishirzi. 2022. Generated knowledge prompting"
        },
        {
          "Yuxian Gu, Xu Han, Zhiyuan Liu, and Minlie Huang.": "nition in conversations.\nIn Proceedings of the 2020",
          "Chul Min Lee and Shrikanth S. Narayanan. 2005. To-": ""
        },
        {
          "Yuxian Gu, Xu Han, Zhiyuan Liu, and Minlie Huang.": "",
          "Chul Min Lee and Shrikanth S. Narayanan. 2005. To-": "for commonsense reasoning.\nIn Proceedings of the"
        },
        {
          "Yuxian Gu, Xu Han, Zhiyuan Liu, and Minlie Huang.": "Conference on Empirical Methods in Natural Lan-",
          "Chul Min Lee and Shrikanth S. Narayanan. 2005. To-": ""
        },
        {
          "Yuxian Gu, Xu Han, Zhiyuan Liu, and Minlie Huang.": "",
          "Chul Min Lee and Shrikanth S. Narayanan. 2005. To-": "60th Annual Meeting of the Association for Compu-"
        },
        {
          "Yuxian Gu, Xu Han, Zhiyuan Liu, and Minlie Huang.": "guage Processing, EMNLP 2020, Online, November",
          "Chul Min Lee and Shrikanth S. Narayanan. 2005. To-": ""
        },
        {
          "Yuxian Gu, Xu Han, Zhiyuan Liu, and Minlie Huang.": "",
          "Chul Min Lee and Shrikanth S. Narayanan. 2005. To-": "tational Linguistics (Volume 1: Long Papers), ACL"
        },
        {
          "Yuxian Gu, Xu Han, Zhiyuan Liu, and Minlie Huang.": "16-20, 2020, pages 7360–7370. Association for Com-",
          "Chul Min Lee and Shrikanth S. Narayanan. 2005. To-": ""
        },
        {
          "Yuxian Gu, Xu Han, Zhiyuan Liu, and Minlie Huang.": "",
          "Chul Min Lee and Shrikanth S. Narayanan. 2005. To-": "2022, Dublin, Ireland, May 22-27, 2022, pages 3154–"
        },
        {
          "Yuxian Gu, Xu Han, Zhiyuan Liu, and Minlie Huang.": "putational Linguistics.",
          "Chul Min Lee and Shrikanth S. Narayanan. 2005. To-": ""
        },
        {
          "Yuxian Gu, Xu Han, Zhiyuan Liu, and Minlie Huang.": "",
          "Chul Min Lee and Shrikanth S. Narayanan. 2005. To-": "3169. Association for Computational Linguistics."
        },
        {
          "Yuxian Gu, Xu Han, Zhiyuan Liu, and Minlie Huang.": "Taewoon Kim and Piek Vossen. 2021.\nEmoBERTa:",
          "Chul Min Lee and Shrikanth S. Narayanan. 2005. To-": "Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang,"
        },
        {
          "Yuxian Gu, Xu Han, Zhiyuan Liu, and Minlie Huang.": "Speaker-aware emotion recognition in conversation",
          "Chul Min Lee and Shrikanth S. Narayanan. 2005. To-": "Hiroaki Hayashi, and Graham Neubig. 2021.\nPre-"
        },
        {
          "Yuxian Gu, Xu Han, Zhiyuan Liu, and Minlie Huang.": "with roberta. arXiv preprint arXiv:2108.12009.",
          "Chul Min Lee and Shrikanth S. Narayanan. 2005. To-": "train, prompt, and predict: A systematic survey of"
        },
        {
          "Yuxian Gu, Xu Han, Zhiyuan Liu, and Minlie Huang.": "",
          "Chul Min Lee and Shrikanth S. Narayanan. 2005. To-": "prompting methods in natural language processing."
        },
        {
          "Yuxian Gu, Xu Han, Zhiyuan Liu, and Minlie Huang.": "Bongseok Lee and Yong Suk Choi. 2021. Graph based",
          "Chul Min Lee and Shrikanth S. Narayanan. 2005. To-": "arXiv preprint arXiv:2107.13586."
        },
        {
          "Yuxian Gu, Xu Han, Zhiyuan Liu, and Minlie Huang.": "network with contextualized representations of turns",
          "Chul Min Lee and Shrikanth S. Narayanan. 2005. To-": ""
        },
        {
          "Yuxian Gu, Xu Han, Zhiyuan Liu, and Minlie Huang.": "in dialogue.\nIn Proceedings of the 2021 Conference",
          "Chul Min Lee and Shrikanth S. Narayanan. 2005. To-": "Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-"
        },
        {
          "Yuxian Gu, Xu Han, Zhiyuan Liu, and Minlie Huang.": "on Empirical Methods in Natural Language Process-",
          "Chul Min Lee and Shrikanth S. Narayanan. 2005. To-": "dar\nJoshi, Danqi Chen, Omer Levy, Mike Lewis,"
        },
        {
          "Yuxian Gu, Xu Han, Zhiyuan Liu, and Minlie Huang.": "ing, EMNLP 2021, Virtual Event / Punta Cana, Do-",
          "Chul Min Lee and Shrikanth S. Narayanan. 2005. To-": "Luke Zettlemoyer,\nand Veselin\nStoyanov.\n2019."
        },
        {
          "Yuxian Gu, Xu Han, Zhiyuan Liu, and Minlie Huang.": "minican Republic, 7-11 November, 2021, pages 443–",
          "Chul Min Lee and Shrikanth S. Narayanan. 2005. To-": "RoBERTa: A robustly optimized bert pretraining ap-"
        },
        {
          "Yuxian Gu, Xu Han, Zhiyuan Liu, and Minlie Huang.": "455. Association for Computational Linguistics.",
          "Chul Min Lee and Shrikanth S. Narayanan. 2005. To-": "proach. arXiv preprint arXiv:1907.11692."
        }
      ],
      "page": 11
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Navonil Majumder, Soujanya Poria, Devamanyu Haz-": "arika, Rada Mihalcea, Alexander F. Gelbukh, and",
          "Jasper Snoek, Hugo Larochelle, and Ryan P. Adams.": "2012.\nPractical bayesian optimization of machine"
        },
        {
          "Navonil Majumder, Soujanya Poria, Devamanyu Haz-": "Erik Cambria. 2019. DialogueRNN: An attentive",
          "Jasper Snoek, Hugo Larochelle, and Ryan P. Adams.": "learning algorithms.\nIn Advances in Neural Informa-"
        },
        {
          "Navonil Majumder, Soujanya Poria, Devamanyu Haz-": "RNN for emotion detection in conversations.\nIn The",
          "Jasper Snoek, Hugo Larochelle, and Ryan P. Adams.": "tion Processing Systems 25: 26th Annual Conference"
        },
        {
          "Navonil Majumder, Soujanya Poria, Devamanyu Haz-": "Thirty-Third AAAI Conference on Artificial Intelli-",
          "Jasper Snoek, Hugo Larochelle, and Ryan P. Adams.": "on Neural Information Processing Systems 2012. Pro-"
        },
        {
          "Navonil Majumder, Soujanya Poria, Devamanyu Haz-": "gence, AAAI 2019, The Thirty-First Innovative Ap-",
          "Jasper Snoek, Hugo Larochelle, and Ryan P. Adams.": "ceedings of a meeting held December 3-6, 2012, Lake"
        },
        {
          "Navonil Majumder, Soujanya Poria, Devamanyu Haz-": "plications of Artificial Intelligence Conference, IAAI",
          "Jasper Snoek, Hugo Larochelle, and Ryan P. Adams.": "Tahoe, Nevada, United States, pages 2960–2968."
        },
        {
          "Navonil Majumder, Soujanya Poria, Devamanyu Haz-": "2019, The Ninth AAAI Symposium on Educational",
          "Jasper Snoek, Hugo Larochelle, and Ryan P. Adams.": ""
        },
        {
          "Navonil Majumder, Soujanya Poria, Devamanyu Haz-": "Advances in Artificial Intelligence, EAAI 2019, Hon-",
          "Jasper Snoek, Hugo Larochelle, and Ryan P. Adams.": "Xiaohui Song, Longtao Huang, Hui Xue, and Songlin"
        },
        {
          "Navonil Majumder, Soujanya Poria, Devamanyu Haz-": "olulu, Hawaii, USA, January 27 - February 1, 2019,",
          "Jasper Snoek, Hugo Larochelle, and Ryan P. Adams.": "Hu. 2022a. Supervised prototypical contrastive learn-"
        },
        {
          "Navonil Majumder, Soujanya Poria, Devamanyu Haz-": "pages 6818–6825. AAAI Press.",
          "Jasper Snoek, Hugo Larochelle, and Ryan P. Adams.": "ing for emotion recognition in conversation.\nIn Pro-"
        },
        {
          "Navonil Majumder, Soujanya Poria, Devamanyu Haz-": "",
          "Jasper Snoek, Hugo Larochelle, and Ryan P. Adams.": "ceedings of the 2022 Conference on Empirical Meth-"
        },
        {
          "Navonil Majumder, Soujanya Poria, Devamanyu Haz-": "Saeid Motiian, Quinn Jones, Seyed Mehdi Iranmanesh,",
          "Jasper Snoek, Hugo Larochelle, and Ryan P. Adams.": "ods in Natural Language Processing, pages 5197–"
        },
        {
          "Navonil Majumder, Soujanya Poria, Devamanyu Haz-": "and Gianfranco Doretto. 2017. Few-shot adversarial",
          "Jasper Snoek, Hugo Larochelle, and Ryan P. Adams.": "5206, Abu Dhabi, United Arab Emirates. Association"
        },
        {
          "Navonil Majumder, Soujanya Poria, Devamanyu Haz-": "domain adaptation.\nIn Advances in Neural Informa-",
          "Jasper Snoek, Hugo Larochelle, and Ryan P. Adams.": "for Computational Linguistics."
        },
        {
          "Navonil Majumder, Soujanya Poria, Devamanyu Haz-": "tion Processing Systems 30: Annual Conference on",
          "Jasper Snoek, Hugo Larochelle, and Ryan P. Adams.": ""
        },
        {
          "Navonil Majumder, Soujanya Poria, Devamanyu Haz-": "Neural\nInformation Processing Systems 2017, De-",
          "Jasper Snoek, Hugo Larochelle, and Ryan P. Adams.": "Xiaohui Song, Liangjun Zang, Rong Zhang, Songlin"
        },
        {
          "Navonil Majumder, Soujanya Poria, Devamanyu Haz-": "cember 4-9, 2017, Long Beach, CA, USA, pages",
          "Jasper Snoek, Hugo Larochelle, and Ryan P. Adams.": "Hu, and Longtao Huang. 2022b. Emotionflow: Cap-"
        },
        {
          "Navonil Majumder, Soujanya Poria, Devamanyu Haz-": "6670–6680.",
          "Jasper Snoek, Hugo Larochelle, and Ryan P. Adams.": "ture the dialogue level emotion transitions.\nIn IEEE"
        },
        {
          "Navonil Majumder, Soujanya Poria, Devamanyu Haz-": "",
          "Jasper Snoek, Hugo Larochelle, and Ryan P. Adams.": "International Conference on Acoustics, Speech and"
        },
        {
          "Navonil Majumder, Soujanya Poria, Devamanyu Haz-": "Soujanya Poria, Devamanyu Hazarika, Navonil Ma-",
          "Jasper Snoek, Hugo Larochelle, and Ryan P. Adams.": "Signal Processing, ICASSP 2022, Virtual and Singa-"
        },
        {
          "Navonil Majumder, Soujanya Poria, Devamanyu Haz-": "jumder, Gautam Naik, Erik Cambria, and Rada Mi-",
          "Jasper Snoek, Hugo Larochelle, and Ryan P. Adams.": "pore, 23-27 May 2022, pages 8542–8546. IEEE."
        },
        {
          "Navonil Majumder, Soujanya Poria, Devamanyu Haz-": "halcea. 2019. MELD: A multimodal multi-party",
          "Jasper Snoek, Hugo Larochelle, and Ryan P. Adams.": ""
        },
        {
          "Navonil Majumder, Soujanya Poria, Devamanyu Haz-": "dataset for emotion recognition in conversations.\nIn",
          "Jasper Snoek, Hugo Larochelle, and Ryan P. Adams.": "Tianxiang Sun, Yunfan Shao, Hong Qian, Xuanjing"
        },
        {
          "Navonil Majumder, Soujanya Poria, Devamanyu Haz-": "Proceedings of the 57th Conference of the Associa-",
          "Jasper Snoek, Hugo Larochelle, and Ryan P. Adams.": "Huang, and Xipeng Qiu. 2022. Black-box tuning"
        },
        {
          "Navonil Majumder, Soujanya Poria, Devamanyu Haz-": "tion for Computational Linguistics, ACL 2019, Flo-",
          "Jasper Snoek, Hugo Larochelle, and Ryan P. Adams.": "for\nlanguage-model-as-a-service.\nIn International"
        },
        {
          "Navonil Majumder, Soujanya Poria, Devamanyu Haz-": "rence, Italy, July 28- August 2, 2019, Volume 1: Long",
          "Jasper Snoek, Hugo Larochelle, and Ryan P. Adams.": "Conference on Machine Learning, ICML 2022, 17-23"
        },
        {
          "Navonil Majumder, Soujanya Poria, Devamanyu Haz-": "Papers, pages 527–536. Association for Computa-",
          "Jasper Snoek, Hugo Larochelle, and Ryan P. Adams.": "July 2022, Baltimore, Maryland, USA, volume 162 of"
        },
        {
          "Navonil Majumder, Soujanya Poria, Devamanyu Haz-": "tional Linguistics.",
          "Jasper Snoek, Hugo Larochelle, and Ryan P. Adams.": "Proceedings of Machine Learning Research, pages"
        },
        {
          "Navonil Majumder, Soujanya Poria, Devamanyu Haz-": "",
          "Jasper Snoek, Hugo Larochelle, and Ryan P. Adams.": "20841–20855. PMLR."
        },
        {
          "Navonil Majumder, Soujanya Poria, Devamanyu Haz-": "Guanghui Qin and Jason Eisner. 2021. Learning how",
          "Jasper Snoek, Hugo Larochelle, and Ryan P. Adams.": ""
        },
        {
          "Navonil Majumder, Soujanya Poria, Devamanyu Haz-": "to ask: Querying lms with mixtures of soft prompts.",
          "Jasper Snoek, Hugo Larochelle, and Ryan P. Adams.": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob"
        },
        {
          "Navonil Majumder, Soujanya Poria, Devamanyu Haz-": "In Proceedings of the 2021 Conference of the North",
          "Jasper Snoek, Hugo Larochelle, and Ryan P. Adams.": "Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz"
        },
        {
          "Navonil Majumder, Soujanya Poria, Devamanyu Haz-": "American Chapter of the Association for Computa-",
          "Jasper Snoek, Hugo Larochelle, and Ryan P. Adams.": "Kaiser, and Illia Polosukhin. 2017. Attention is all"
        },
        {
          "Navonil Majumder, Soujanya Poria, Devamanyu Haz-": "tional Linguistics: Human Language Technologies,",
          "Jasper Snoek, Hugo Larochelle, and Ryan P. Adams.": "you need.\nIn Advances in Neural Information Pro-"
        },
        {
          "Navonil Majumder, Soujanya Poria, Devamanyu Haz-": "NAACL-HLT 2021, Online, June 6-11, 2021, pages",
          "Jasper Snoek, Hugo Larochelle, and Ryan P. Adams.": "cessing Systems 30: Annual Conference on Neural"
        },
        {
          "Navonil Majumder, Soujanya Poria, Devamanyu Haz-": "5203–5212. Association for Computational Linguis-",
          "Jasper Snoek, Hugo Larochelle, and Ryan P. Adams.": "Information Processing Systems 2017, December 4-9,"
        },
        {
          "Navonil Majumder, Soujanya Poria, Devamanyu Haz-": "tics.",
          "Jasper Snoek, Hugo Larochelle, and Ryan P. Adams.": "2017, Long Beach, CA, USA, pages 5998–6008."
        },
        {
          "Navonil Majumder, Soujanya Poria, Devamanyu Haz-": "Colin Raffel, Noam Shazeer, Adam Roberts, Katherine",
          "Jasper Snoek, Hugo Larochelle, and Ryan P. Adams.": "Tu Vu, Brian Lester, Noah Constant, Rami Al-Rfou’,"
        },
        {
          "Navonil Majumder, Soujanya Poria, Devamanyu Haz-": "Lee, Sharan Narang, Michael Matena, Yanqi Zhou,",
          "Jasper Snoek, Hugo Larochelle, and Ryan P. Adams.": "and Daniel Cer. 2022.\nSPoT: Better frozen model"
        },
        {
          "Navonil Majumder, Soujanya Poria, Devamanyu Haz-": "Wei Li, and Peter J. Liu. 2020. Exploring the limits",
          "Jasper Snoek, Hugo Larochelle, and Ryan P. Adams.": "adaptation through soft prompt transfer.\nIn Proceed-"
        },
        {
          "Navonil Majumder, Soujanya Poria, Devamanyu Haz-": "of transfer learning with a unified text-to-text trans-",
          "Jasper Snoek, Hugo Larochelle, and Ryan P. Adams.": "ings of the 60th Annual Meeting of the Association"
        },
        {
          "Navonil Majumder, Soujanya Poria, Devamanyu Haz-": "former. J. Mach. Learn. Res., 21:140:1–140:67.",
          "Jasper Snoek, Hugo Larochelle, and Ryan P. Adams.": "for Computational Linguistics (Volume 1: Long Pa-"
        },
        {
          "Navonil Majumder, Soujanya Poria, Devamanyu Haz-": "",
          "Jasper Snoek, Hugo Larochelle, and Ryan P. Adams.": "pers), pages 5039–5059, Dublin, Ireland. Association"
        },
        {
          "Navonil Majumder, Soujanya Poria, Devamanyu Haz-": "Tim Salimans, Jonathan Ho, Xi Chen, Szymon Sidor,",
          "Jasper Snoek, Hugo Larochelle, and Ryan P. Adams.": "for Computational Linguistics."
        },
        {
          "Navonil Majumder, Soujanya Poria, Devamanyu Haz-": "and Ilya Sutskever. 2017. Evolution strategies as a",
          "Jasper Snoek, Hugo Larochelle, and Ryan P. Adams.": ""
        },
        {
          "Navonil Majumder, Soujanya Poria, Devamanyu Haz-": "scalable alternative to reinforcement learning. arXiv",
          "Jasper Snoek, Hugo Larochelle, and Ryan P. Adams.": "Yaqing Wang, Quanming Yao, James T. Kwok, and Li-"
        },
        {
          "Navonil Majumder, Soujanya Poria, Devamanyu Haz-": "preprint arXiv:1703.03864.",
          "Jasper Snoek, Hugo Larochelle, and Ryan P. Adams.": "onel M. Ni. 2021. Generalizing from a few examples:"
        },
        {
          "Navonil Majumder, Soujanya Poria, Devamanyu Haz-": "",
          "Jasper Snoek, Hugo Larochelle, and Ryan P. Adams.": "A survey on few-shot learning. ACM Comput. Surv.,"
        },
        {
          "Navonil Majumder, Soujanya Poria, Devamanyu Haz-": "Bobak Shahriari, Kevin Swersky, Ziyu Wang, Ryan P.",
          "Jasper Snoek, Hugo Larochelle, and Ryan P. Adams.": ""
        },
        {
          "Navonil Majumder, Soujanya Poria, Devamanyu Haz-": "",
          "Jasper Snoek, Hugo Larochelle, and Ryan P. Adams.": "53(3):63:1–63:34."
        },
        {
          "Navonil Majumder, Soujanya Poria, Devamanyu Haz-": "Adams, and Nando de Freitas. 2016.\nTaking the",
          "Jasper Snoek, Hugo Larochelle, and Ryan P. Adams.": ""
        },
        {
          "Navonil Majumder, Soujanya Poria, Devamanyu Haz-": "human out of the loop: A review of bayesian opti-",
          "Jasper Snoek, Hugo Larochelle, and Ryan P. Adams.": "Antoine Yang, Antoine Miech, Josef Sivic, Ivan Laptev,"
        },
        {
          "Navonil Majumder, Soujanya Poria, Devamanyu Haz-": "mization. Proc. IEEE, 104(1):148–175.",
          "Jasper Snoek, Hugo Larochelle, and Ryan P. Adams.": "and Cordelia Schmid. 2022. Zero-shot video ques-"
        },
        {
          "Navonil Majumder, Soujanya Poria, Devamanyu Haz-": "",
          "Jasper Snoek, Hugo Larochelle, and Ryan P. Adams.": "tion answering via frozen bidirectional language mod-"
        },
        {
          "Navonil Majumder, Soujanya Poria, Devamanyu Haz-": "Weizhou Shen,\nJunqing Chen, Xiaojun Quan,\nand",
          "Jasper Snoek, Hugo Larochelle, and Ryan P. Adams.": ""
        },
        {
          "Navonil Majumder, Soujanya Poria, Devamanyu Haz-": "",
          "Jasper Snoek, Hugo Larochelle, and Ryan P. Adams.": "els. arXiv preprint arXiv:2206.08155."
        },
        {
          "Navonil Majumder, Soujanya Poria, Devamanyu Haz-": "Zhixian Xie. 2021. Dialogxl: All-in-one xlnet for",
          "Jasper Snoek, Hugo Larochelle, and Ryan P. Adams.": ""
        },
        {
          "Navonil Majumder, Soujanya Poria, Devamanyu Haz-": "multi-party conversation emotion recognition.\nIn",
          "Jasper Snoek, Hugo Larochelle, and Ryan P. Adams.": "Jingjie Yi, Deqing Yang,\nSiyu Yuan, Caiyan Cao,"
        },
        {
          "Navonil Majumder, Soujanya Poria, Devamanyu Haz-": "Thirty-Fifth AAAI Conference on Artificial\nIntelli-",
          "Jasper Snoek, Hugo Larochelle, and Ryan P. Adams.": "Zhiyao Zhang, and Yanghua Xiao. 2022. Contex-"
        },
        {
          "Navonil Majumder, Soujanya Poria, Devamanyu Haz-": "gence, AAAI 2021, Thirty-Third Conference on In-",
          "Jasper Snoek, Hugo Larochelle, and Ryan P. Adams.": "tual information and commonsense based prompt for"
        },
        {
          "Navonil Majumder, Soujanya Poria, Devamanyu Haz-": "novative Applications of Artificial Intelligence, IAAI",
          "Jasper Snoek, Hugo Larochelle, and Ryan P. Adams.": "emotion recognition in conversation. arXiv preprint"
        },
        {
          "Navonil Majumder, Soujanya Poria, Devamanyu Haz-": "2021, The Eleventh Symposium on Educational Ad-",
          "Jasper Snoek, Hugo Larochelle, and Ryan P. Adams.": "arXiv:2207.13254."
        },
        {
          "Navonil Majumder, Soujanya Poria, Devamanyu Haz-": "vances\nin Artificial\nIntelligence, EAAI 2021, Vir-",
          "Jasper Snoek, Hugo Larochelle, and Ryan P. Adams.": ""
        },
        {
          "Navonil Majumder, Soujanya Poria, Devamanyu Haz-": "tual Event, February 2-9, 2021, pages 13789–13797.",
          "Jasper Snoek, Hugo Larochelle, and Ryan P. Adams.": "Sayyed M. Zahiri and Jinho D. Choi. 2018. Emotion de-"
        },
        {
          "Navonil Majumder, Soujanya Poria, Devamanyu Haz-": "AAAI Press.",
          "Jasper Snoek, Hugo Larochelle, and Ryan P. Adams.": "tection on TV show transcripts with sequence-based"
        }
      ],
      "page": 12
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "convolutional neural networks.\nIn The Workshops of": "the The Thirty-Second AAAI Conference on Artificial"
        },
        {
          "convolutional neural networks.\nIn The Workshops of": "Intelligence, New Orleans, Louisiana, USA, Febru-"
        },
        {
          "convolutional neural networks.\nIn The Workshops of": "ary 2-7, 2018, volume WS-18 of AAAI Technical"
        },
        {
          "convolutional neural networks.\nIn The Workshops of": "Report, pages 44–52. AAAI Press."
        },
        {
          "convolutional neural networks.\nIn The Workshops of": "Peixiang Zhong, Di Wang, and Chunyan Miao. 2019."
        },
        {
          "convolutional neural networks.\nIn The Workshops of": "Knowledge-enriched transformer for emotion detec-"
        },
        {
          "convolutional neural networks.\nIn The Workshops of": "tion in textual conversations.\nIn Proceedings of the"
        },
        {
          "convolutional neural networks.\nIn The Workshops of": "2019 Conference on Empirical Methods\nin Natu-"
        },
        {
          "convolutional neural networks.\nIn The Workshops of": "ral Language Processing and the 9th International"
        },
        {
          "convolutional neural networks.\nIn The Workshops of": "Joint Conference on Natural Language Processing,"
        },
        {
          "convolutional neural networks.\nIn The Workshops of": "EMNLP-IJCNLP 2019, Hong Kong, China, Novem-"
        },
        {
          "convolutional neural networks.\nIn The Workshops of": "ber 3-7, 2019, pages 165–176. Association for Com-"
        },
        {
          "convolutional neural networks.\nIn The Workshops of": "putational Linguistics."
        }
      ],
      "page": 13
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Attentional mixtures of soft prompt tuning for parameter-efficient multi-task knowledge sharing",
      "authors": [
        "Akari Asai",
        "Mohammadreza Salehi",
        "Matthew Peters",
        "Hannaneh Hajishirzi"
      ],
      "year": "2022",
      "venue": "Attentional mixtures of soft prompt tuning for parameter-efficient multi-task knowledge sharing",
      "arxiv": "arXiv:2205.11961"
    },
    {
      "citation_id": "2",
      "title": "Language models are few-shot learners",
      "authors": [
        "B Tom",
        "Benjamin Brown",
        "Nick Mann",
        "Melanie Ryder",
        "Jared Subbiah",
        "Prafulla Kaplan",
        "Arvind Dhariwal",
        "Pranav Neelakantan",
        "Girish Shyam",
        "Amanda Sastry",
        "Sandhini Askell",
        "Ariel Agarwal",
        "Gretchen Herbert-Voss",
        "Tom Krueger",
        "Rewon Henighan",
        "Aditya Child",
        "Daniel Ramesh",
        "Jeffrey Ziegler",
        "Clemens Wu",
        "Christopher Winter",
        "Mark Hesse",
        "Eric Chen",
        "Mateusz Sigler",
        "Scott Litwin",
        "Benjamin Gray",
        "Jack Chess",
        "Christopher Clark",
        "Sam Berner",
        "Alec Mccandlish",
        "Ilya Radford",
        "Dario Sutskever",
        "Amodei"
      ],
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems"
    },
    {
      "citation_id": "3",
      "title": "IEMOCAP: interactive emotional dyadic motion capture database",
      "authors": [
        "Carlos Busso",
        "Murtaza Bulut",
        "Chi-Chun Lee",
        "Abe Kazemzadeh",
        "Emily Mower",
        "Samuel Kim",
        "Jeannette Chang",
        "Sungbok Lee",
        "Shrikanth Narayanan"
      ],
      "year": "2008",
      "venue": "Lang. Resour. Evaluation",
      "doi": "10.1007/s10579-008-9076-6"
    },
    {
      "citation_id": "4",
      "title": "Understanding emotions in text using deep learning and big data",
      "authors": [
        "Ankush Chatterjee",
        "Umang Gupta",
        "Manoj Kumar Chinnakotla",
        "Radhakrishnan Srikanth",
        "Michel Galley",
        "Puneet Agrawal"
      ],
      "year": "2019",
      "venue": "Comput. Hum. Behav",
      "doi": "10.1016/j.chb.2018.12.029"
    },
    {
      "citation_id": "5",
      "title": "Pankaj Wasnik, and Naoyuki Onoe. 2022. M2FNet: Multi-modal fusion network for emotion recognition in conversation",
      "authors": [
        "Purbayan Vishal Chudasama",
        "Ashish Kar",
        "Nirmesh Gudmalwar",
        "Shah"
      ],
      "year": "2022",
      "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, CVPR Workshops 2022",
      "doi": "10.1109/CVPRW56347.2022.00511"
    },
    {
      "citation_id": "6",
      "title": "Reallife emotions detection with lexical and paralinguistic cues on human-human call center dialogs",
      "authors": [
        "Laurence Devillers",
        "Laurence Vidrascu"
      ],
      "year": "2006",
      "venue": "INTER-SPEECH 2006 -ICSLP, Ninth International Conference on Spoken Language Processing"
    },
    {
      "citation_id": "7",
      "title": "BERT: pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "Jacob Devlin",
        "Ming-Wei Chang",
        "Kenton Lee",
        "Kristina Toutanova"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019",
      "doi": "10.18653/v1/n19-1423"
    },
    {
      "citation_id": "8",
      "title": "Making pre-trained language models better few-shot learners",
      "authors": [
        "Tianyu Gao",
        "Adam Fisch",
        "Danqi Chen"
      ],
      "year": "2021",
      "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021",
      "doi": "10.18653/v1/2021.acl-long.295"
    },
    {
      "citation_id": "9",
      "title": "COSMIC: commonsense knowledge for emotion identification in conversations",
      "authors": [
        "Deepanway Ghosal",
        "Navonil Majumder",
        "Alexander Gelbukh",
        "Rada Mihalcea",
        "Soujanya Poria"
      ],
      "year": "2020",
      "venue": "Findings of the Association for Computational Linguistics: EMNLP 2020, Online Event",
      "doi": "10.18653/v1/2020.findings-emnlp.224"
    },
    {
      "citation_id": "10",
      "title": "PPT: pre-trained prompt tuning for few-shot learning",
      "authors": [
        "Yuxian Gu",
        "Xu Han",
        "Zhiyuan Liu",
        "Minlie Huang"
      ],
      "year": "2022",
      "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics",
      "doi": "10.18653/v1/2022.acl-long.576"
    },
    {
      "citation_id": "11",
      "title": "Few-shot emotion recognition in conversation with sequential prototypical networks",
      "authors": [
        "Matthieu Gaël Guibon",
        "Luce Labeau",
        "Chloé Lefeuvre",
        "Clavel"
      ],
      "year": "2022",
      "venue": "Softw. Impacts",
      "doi": "10.1016/j.simpa.2022.100237"
    },
    {
      "citation_id": "12",
      "title": "Improving the sample efficiency of prompt tuning with domain adaptation",
      "authors": [
        "Boyang Xu Guo",
        "Han Li",
        "Yu"
      ],
      "year": "2022",
      "venue": "Findings of the Association for Computational Linguistics: EMNLP 2022"
    },
    {
      "citation_id": "13",
      "title": "Reducing the time complexity of the derandomized evolution strategy with covariance matrix adaptation (CMA-ES)",
      "authors": [
        "Nikolaus Hansen",
        "Sibylle Müller",
        "Petros Koumoutsakos"
      ],
      "year": "2003",
      "venue": "Evol. Comput",
      "doi": "10.1162/106365603321828970"
    },
    {
      "citation_id": "14",
      "title": "Completely derandomized self-adaptation in evolution strategies",
      "authors": [
        "Nikolaus Hansen",
        "Andreas Ostermeier"
      ],
      "year": "2001",
      "venue": "Evol. Comput",
      "doi": "10.1162/106365601750190398"
    },
    {
      "citation_id": "15",
      "title": "Conversational memory network for emotion recognition in dyadic dialogue videos",
      "authors": [
        "Devamanyu Hazarika",
        "Soujanya Poria",
        "Amir Zadeh",
        "Erik Cambria",
        "Louis-Philippe Morency",
        "Roger Zimmermann"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2018",
      "doi": "10.18653/v1/n18-1193"
    },
    {
      "citation_id": "16",
      "title": "Relation-aware graph attention networks with relational position encodings for emotion recognition in conversations",
      "authors": [
        "Taichi Ishiwatari",
        "Yuki Yasuda",
        "Taro Miyazaki",
        "Jun Goto"
      ],
      "year": "2020",
      "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing",
      "doi": "10.18653/v1/2020.emnlp-main.597"
    },
    {
      "citation_id": "17",
      "title": "EmoBERTa: Speaker-aware emotion recognition in conversation with roberta",
      "authors": [
        "Taewoon Kim",
        "Piek Vossen"
      ],
      "year": "2021",
      "venue": "EmoBERTa: Speaker-aware emotion recognition in conversation with roberta",
      "arxiv": "arXiv:2108.12009"
    },
    {
      "citation_id": "18",
      "title": "Graph based network with contextualized representations of turns in dialogue",
      "authors": [
        "Bongseok Lee",
        "Yong Choi"
      ],
      "year": "2005",
      "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana",
      "doi": "10.1109/TSA.2004.838534"
    },
    {
      "citation_id": "19",
      "title": "The power of scale for parameter-efficient prompt tuning",
      "authors": [
        "Brian Lester",
        "Rami Al-Rfou",
        "Noah Constant"
      ],
      "year": "2021",
      "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana",
      "doi": "10.18653/v1/2021.emnlp-main.243"
    },
    {
      "citation_id": "20",
      "title": "Measuring the intrinsic dimension of objective landscapes",
      "authors": [
        "Chunyuan Li",
        "Heerad Farkhoor",
        "Rosanne Liu",
        "Jason Yosinski"
      ],
      "year": "2018",
      "venue": "6th International Conference on Learning Representations, ICLR 2018"
    },
    {
      "citation_id": "21",
      "title": "Prefix-Tuning: Optimizing continuous prompts for generation",
      "authors": [
        "Lisa Xiang",
        "Percy Li",
        "Liang"
      ],
      "year": "2021",
      "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021",
      "doi": "10.18653/v1/2021.acl-long.353"
    },
    {
      "citation_id": "22",
      "title": "Dailydialog: A manually labelled multi-turn dialogue dataset",
      "authors": [
        "Yanran Li",
        "Hui Su",
        "Xiaoyu Shen",
        "Wenjie Li",
        "Ziqiang Cao",
        "Shuzi Niu"
      ],
      "year": "2017",
      "venue": "Proceedings of the Eighth International Joint Conference on Natural Language Processing"
    },
    {
      "citation_id": "23",
      "title": "EmoCaps: Emotion capsule based model for conversational emotion recognition",
      "authors": [
        "Zaijing Li",
        "Fengxiao Tang",
        "Ming Zhao",
        "Yusen Zhu"
      ],
      "year": "2022",
      "venue": "Findings of the Association for Computational Linguistics: ACL 2022",
      "doi": "10.18653/v1/2022.findings-acl.126"
    },
    {
      "citation_id": "24",
      "title": "Generated knowledge prompting for commonsense reasoning",
      "authors": [
        "Jiacheng Liu",
        "Alisa Liu",
        "Ximing Lu",
        "Sean Welleck",
        "Peter West",
        "Le Ronan",
        "Yejin Bras",
        "Hannaneh Choi",
        "Hajishirzi"
      ],
      "year": "2022",
      "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics",
      "doi": "10.18653/v1/2022.acl-long.225"
    },
    {
      "citation_id": "25",
      "title": "Pretrain, prompt, and predict: A systematic survey of prompting methods in natural language processing",
      "authors": [
        "Pengfei Liu",
        "Weizhe Yuan",
        "Jinlan Fu",
        "Zhengbao Jiang",
        "Hiroaki Hayashi",
        "Graham Neubig"
      ],
      "year": "2021",
      "venue": "Pretrain, prompt, and predict: A systematic survey of prompting methods in natural language processing",
      "arxiv": "arXiv:2107.13586"
    },
    {
      "citation_id": "26",
      "title": "RoBERTa: A robustly optimized bert pretraining approach",
      "authors": [
        "Yinhan Liu",
        "Myle Ott",
        "Naman Goyal",
        "Jingfei Du",
        "Mandar Joshi",
        "Danqi Chen",
        "Omer Levy",
        "Mike Lewis",
        "Luke Zettlemoyer",
        "Veselin Stoyanov"
      ],
      "year": "2019",
      "venue": "RoBERTa: A robustly optimized bert pretraining approach",
      "arxiv": "arXiv:1907.11692"
    },
    {
      "citation_id": "27",
      "title": "DialogueRNN: An attentive RNN for emotion detection in conversations",
      "authors": [
        "Navonil Majumder",
        "Soujanya Poria",
        "Devamanyu Hazarika",
        "Rada Mihalcea",
        "Alexander Gelbukh",
        "Erik Cambria"
      ],
      "year": "2019",
      "venue": "The Thirty-Third AAAI Conference on Artificial Intelligence, AAAI 2019, The Thirty-First Innovative Applications of Artificial Intelligence Conference, IAAI 2019, The Ninth AAAI Symposium on Educational Advances in Artificial Intelligence",
      "doi": "10.1609/aaai.v33i01.33016818"
    },
    {
      "citation_id": "28",
      "title": "Few-shot adversarial domain adaptation",
      "authors": [
        "Saeid Motiian",
        "Quinn Jones",
        "Seyed Mehdi Iranmanesh",
        "Gianfranco Doretto"
      ],
      "year": "2017",
      "venue": "Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems"
    },
    {
      "citation_id": "29",
      "title": "MELD: A multimodal multi-party dataset for emotion recognition in conversations",
      "authors": [
        "Soujanya Poria",
        "Devamanyu Hazarika",
        "Navonil Majumder",
        "Gautam Naik",
        "Erik Cambria",
        "Rada Mihalcea"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019",
      "doi": "10.18653/v1/p19-1050"
    },
    {
      "citation_id": "30",
      "title": "Learning how to ask: Querying lms with mixtures of soft prompts",
      "authors": [
        "Guanghui Qin",
        "Jason Eisner"
      ],
      "year": "2021",
      "venue": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021",
      "doi": "10.18653/v1/2021.naacl-main.410"
    },
    {
      "citation_id": "31",
      "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
      "authors": [
        "Colin Raffel",
        "Noam Shazeer",
        "Adam Roberts",
        "Katherine Lee",
        "Sharan Narang",
        "Michael Matena",
        "Yanqi Zhou",
        "Wei Li",
        "Peter Liu"
      ],
      "year": "2020",
      "venue": "J. Mach. Learn. Res"
    },
    {
      "citation_id": "32",
      "title": "Evolution strategies as a scalable alternative to reinforcement learning",
      "authors": [
        "Tim Salimans",
        "Jonathan Ho",
        "Xi Chen",
        "Szymon Sidor",
        "Ilya Sutskever"
      ],
      "year": "2017",
      "venue": "Evolution strategies as a scalable alternative to reinforcement learning",
      "arxiv": "arXiv:1703.03864"
    },
    {
      "citation_id": "33",
      "title": "Taking the human out of the loop: A review of bayesian optimization",
      "authors": [
        "Bobak Shahriari",
        "Kevin Swersky",
        "Ziyu Wang",
        "Ryan Adams",
        "Nando De Freitas"
      ],
      "year": "2016",
      "venue": "Proc. IEEE",
      "doi": "10.1109/JPROC.2015.2494218"
    },
    {
      "citation_id": "34",
      "title": "Dialogxl: All-in-one xlnet for multi-party conversation emotion recognition",
      "authors": [
        "Weizhou Shen",
        "Junqing Chen",
        "Xiaojun Quan",
        "Zhixian Xie"
      ],
      "year": "2021",
      "venue": "Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Thirty-Third Conference on Innovative Applications of Artificial Intelligence, IAAI 2021, The Eleventh Symposium on Educational Advances in Artificial Intelligence, EAAI 2021, Virtual Event"
    },
    {
      "citation_id": "35",
      "title": "Practical bayesian optimization of machine learning algorithms",
      "authors": [
        "Jasper Snoek",
        "Hugo Larochelle",
        "Ryan Adams"
      ],
      "year": "2012",
      "venue": "Advances in Neural Information Processing Systems 25: 26th Annual Conference on Neural Information Processing Systems 2012. Proceedings of a meeting held"
    },
    {
      "citation_id": "36",
      "title": "2022a. Supervised prototypical contrastive learning for emotion recognition in conversation",
      "authors": [
        "Xiaohui Song",
        "Longtao Huang",
        "Hui Xue",
        "Songlin Hu"
      ],
      "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "37",
      "title": "2022b. Emotionflow: Capture the dialogue level emotion transitions",
      "authors": [
        "Xiaohui Song",
        "Liangjun Zang",
        "Rong Zhang",
        "Songlin Hu",
        "Longtao Huang"
      ],
      "year": "2022",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP 2022, Virtual and Singapore",
      "doi": "10.1109/ICASSP43922.2022.9746464"
    },
    {
      "citation_id": "38",
      "title": "Xuanjing Huang, and Xipeng Qiu. 2022. Black-box tuning for language-model-as-a-service",
      "authors": [
        "Tianxiang Sun",
        "Yunfan Shao",
        "Hong Qian"
      ],
      "year": "2022",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "39",
      "title": "Attention is all you need",
      "authors": [
        "Ashish Vaswani",
        "Noam Shazeer",
        "Niki Parmar",
        "Jakob Uszkoreit",
        "Llion Jones",
        "Aidan Gomez",
        "Lukasz Kaiser",
        "Illia Polosukhin"
      ],
      "year": "2017",
      "venue": "Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems"
    },
    {
      "citation_id": "40",
      "title": "SPoT: Better frozen model adaptation through soft prompt transfer",
      "authors": [
        "Tu Vu",
        "Brian Lester",
        "Noah Constant",
        "Rami Al-Rfou",
        "Daniel Cer"
      ],
      "year": "2022",
      "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics",
      "doi": "10.18653/v1/2022.acl-long.346"
    },
    {
      "citation_id": "41",
      "title": "Generalizing from a few examples: A survey on few-shot learning",
      "authors": [
        "Yaqing Wang",
        "Quanming Yao",
        "James Kwok",
        "Lionel Ni"
      ],
      "year": "2021",
      "venue": "ACM Comput. Surv",
      "doi": "10.1145/3386252"
    },
    {
      "citation_id": "42",
      "title": "Zero-shot video question answering via frozen bidirectional language models",
      "authors": [
        "Antoine Yang",
        "Antoine Miech",
        "Josef Sivic",
        "Ivan Laptev",
        "Cordelia Schmid"
      ],
      "year": "2022",
      "venue": "Zero-shot video question answering via frozen bidirectional language models",
      "arxiv": "arXiv:2206.08155"
    },
    {
      "citation_id": "43",
      "title": "Contextual information and commonsense based prompt for emotion recognition in conversation",
      "authors": [
        "Jingjie Yi",
        "Deqing Yang",
        "Siyu Yuan",
        "Caiyan Cao",
        "Zhiyao Zhang",
        "Yanghua Xiao"
      ],
      "year": "2022",
      "venue": "Contextual information and commonsense based prompt for emotion recognition in conversation",
      "arxiv": "arXiv:2207.13254"
    },
    {
      "citation_id": "44",
      "title": "Emotion detection on TV show transcripts with sequence-based convolutional neural networks",
      "authors": [
        "M Sayyed",
        "Jinho Zahiri",
        "Choi"
      ],
      "year": "2018",
      "venue": "The Workshops of the The Thirty-Second AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "45",
      "title": "Knowledge-enriched transformer for emotion detection in textual conversations",
      "authors": [
        "Peixiang Zhong",
        "Di Wang",
        "Chunyan Miao"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019",
      "doi": "10.18653/v1/D19-1016"
    }
  ]
}