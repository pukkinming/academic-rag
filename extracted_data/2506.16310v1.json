{
  "paper_id": "2506.16310v1",
  "title": "Optimizing Multilingual Text-To-Speech With Accents And Emotions",
  "published": "2025-06-19T13:35:05Z",
  "authors": [
    "Pranav Pawar",
    "Akshansh Dwivedi",
    "Jenish Boricha",
    "Himanshu Gohil",
    "Aditya Dubey"
  ],
  "keywords": [
    "Text-to-Speech",
    "Transliteration",
    "Speaker Embedding",
    "Neural Vocoder",
    "Phoneme Alignment"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "State-of-the-art text-to-speech (TTS) systems realize high naturalness in monolingual environments; however, synthesizing speech with correct multilingual accents (especially for Indic languages) and context-relevant emotions still poses difficulty owing to cultural nuance discrepancies in current frameworks. This paper introduces a new TTS architecture integrating accent along with preserving transliteration with multi-scale emotion modelling, in particular tuned for Hindi and Indian English accent. Our approach extends the Parler-TTS model by integrating a language-specific phoneme alignment hybrid encoder-decoder architecture, and culture-sensitive emotion embedding layers trained on native speaker corpora, as well as incorporating a dynamic accent code switching with residual vector quantization. Quantitative tests demonstrate 23.7% improvement in accent accuracy (Word Error Rate reduction from 15.4% to 11.8%) and 85.3% emotion recognition accuracy from native listeners, surpassing METTS and VECL-TTS baselines. The novelty of the system is that it can mix code in real time-generating statements such as \"Namaste, let's talk about <Hindi phrase>\" with uninterrupted accent shifts while preserving emotional consistency. Subjective evaluation with 200 users reported a mean opinion score (MOS) of 4.2/5 for cultural correctness, much better than existing multilingual systems (p < 0.01). This research makes cross-lingual synthesis more feasible by showcasing scalable accent-emotion disentanglement, with direct application in South Asian EdTech and accessibility software.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Introduction",
      "text": "The advancement of TTS technology was revolutionized through the work done in a 2019 Google paper on a neural network-based TTS system voice multi-synthesis. This paper transformed speech synthesis and provided the foundation of a speaker encoder, a sequence-to-sequence synthesizer, and a WaveNet vocoder  [1] . This innovative framework set the standard for utilizing underused audio for shot voices and established TTS technology's new epoch. Since then, synthesis technology has sharpened the adaptability, flexibility, and authenticity of voicing and speaking styles in TTS systems. Tremendous progress keeps occurring.\n\nThe practice of transliteration that revolutionized the 9th century onto the modern world is fundamental to TTS systems for differing languages. It stems from Al-Kindi's invention of frequency analysis during the Arabic Crusade. He advanced cryptography by combining languages, which modern society practices in multilingual TTS systems  [2] . Now Takumi, the TTS system utilizes the precise adjustments needed for pronunciation aimed to be preserved from the original language while hybrids and crypto linguistics are used to adjust for other scripts used for speaking.\n\nThe synthesis of speech modeling, particularly accent, is an area of cross-lingual speech synthesis that has received little attention in the past but is significant  [3] . While recognizing the importance of language accent emerged in the XIX century, accentuation of different languages in TTS systems became popular only recently. Our work focuses not just on accent reproduction, but on real-time shifts of speech styles, as in our example, Indian English and Hindi can be pronounced simultaneously  [4] . This feature is a breakthrough in fulfilling contextual and culturally appropriate requirements for speech synthesis.\n\nAnother important change in TTS technology comes from personalization, which would not have been possible without the new wave of AI enabled customization. Users can define their needs regarding language, tone and accent. For example, a user may ask, 'a calm, authoritative voice with a slight Indian accent' and the system will deliver the speech based on the parameters provided. This is a great step forward to cater to the needs of a wider range of people.\n\nIntoned speech is the most difficult nuance in TTS, as the artificial emotional voice generated in old TTS systems sounded robotic and emotionally handicapped. However, with the application of deep learning and concepts from neuroscience, the system can effortlessly mimic the broad spectrum of human emotions. The improvement enables a large scope of human emotions to be effectively articulated, including but not limited to, reverberating sadness, excited delight, ideal for marketing an audiobook or programming a virtual assistant.\n\nThe disruption happens when all these factors, such as accurate transcription, subtone accent recognition, user-centered customization  [5] , and complex emotion reproduction, are placed and processed in one bracing system. Whereas these solutions take the back seat, our system is the first one that learns and adjusts to different peoples' linguistic and sociocultural speech frameworks  [6] . It can produce fluent speech in different languages and dialects in a single utterance and is perfect for international companies or for use in multi-ethnic regions. For instance, the system can create a marketing advertisement in Hindi with a localized accent, followed by American English spoken in a business accent without any loss of emotion.\n\nOur system's possible uses are broad and diverse. It can improve virtual assistant applications by adding human-like voices for the blind, change how languages are taught by synthesizing speech with different accents, emotions, and even transform the entertainment industry by quickly producing rich multilingual voice overs. Moreover, our system serves as the base for even more sophisticated TTS system developments and sets a stage for real AI powered conversationalist which can communicate coherently with any person, irrespective of their nationality and culture.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Related Works",
      "text": "The development of TTS technology revolutionized when the system needed to handle accents and emotional intonation within multilingual contexts. Jia et al.'s milestone work  [1]  introduced a neural network-based TTS system with multivoice synthesis which marked the beginning of unprecedented field development. The system implemented zero-shot voice adaption through speaker encoders alongside sequence-to-sequence synthesizers and WaveNet vocoders when minimum reference audio was available. The field of speech synthesis has experienced significant advancement because of these new developments which aim to produce more authentic imitations of different voices and speech styles. The process of generating various multilingual accents and emotional tones through synthetic speech remains problematic especially for Indian languages.\n\nAccent modeling is a key element in TTS systems that significantly impacts user experience and application effectiveness. In the past, there was neglect towards accent in traditional speech synthesis models, which caused the generated voice to sound unrealistic and out of context. Latest research puts heavy emphasis on accent in language education and communication and stresses that emphasizing accent differences improves user experiences vastly. For example, Zhang et al.  [6]  showed that accented TTS voices could be used to support second language learning by introducing learners to native-like pronunciations. Nonetheless, current models tend not to consider the long-term effects of accented TTS on learners' speaking proficiency and effectiveness Liu et al.  [7] .\n\nBuilding on these foundations, researchers have developed increasingly sophisticated approaches to cross-lingual accent adaptation. Recent research by Zhu et al.  [8]  in their METTS (Multilingual Emotional Text-to-Speech) system solves the foreign accent issue by separating language-agnostic and language-specific emotion representations. Their multiscale emotion modeling strategy reduces undesirable accents in cross-lingual synthesis with improved naturalness and speaker similarity scores over baselines. Likewise, Gudmalwar et al.  [9]  introduce VECL-TTS that combines speaker and emotion embeddings to improve cross-lingual synthesis of Indian languages, showing improved emotion similarity and pronunciation quality through content and style consistency losses.\n\nEmotional Expressiveness and Multi-Scale Modeling is another critical feature of TTS technology that has gained prominence in recent years. Early efforts at emotional TTS were unable to generate natural-sounding expressions because of the complexity of modeling human emotions. With the development of deep learning methods, researchers have been able to capture the dynamics of emotional expression in speech more effectively. Tang et al.  [10]  present ED-TTS, a multi-scale emotional speech synthesis model that utilizes Speech Emotion Diarization (SED) and Speech Emotion Recognition (SER) to condition on emotions at utterance and frame levels. Their method, founded on a denoising diffusion probabilistic model (DDPM), captures higher emotion similarity and naturalness by adding crossdomain SED to forecast fine-grained emotion labels. This is an indicator of the significance of multi-scale emotion modeling in recording subtle prosodic variations.\n\nExpanding the emotional capabilities of TTS systems, Jayapratha et al.  [11]  extend emotional TTS further with the incorporation of speech emotion recognition, multi-speaker distinction, and voice cloning in their EIMVT system. Their approach maintains emotional tone and speaker identity across languages, with an 85% recognition accuracy for emotion and high-fidelity voice cloning, as testified by Mean Opinion Scores (MOS) of 4.5/5 for voice similarity. This integration of multiple technologies demonstrates how modern TTS systems are evolving to capture the full range of human speech characteristics.\n\nThe multilingualism of TTS offers both opportunities and challenges to researchers and developers. Although certain models have taken it a step forward in producing accented voices for second language learners, they fail to recognize long-term effects on learners' proficiency and efficiency. The transferability of emotional expressions across languages is also underexplored. One gap is particularly striking regarding how cultural difference affects speech synthesis; most TTS systems use a one-size-fits-all strategy that is not necessarily relatable to people from various linguistic backgrounds. Addressing this cultural diversity need, Gudmalwar et al.  [9]  bridge this need by their focus on crosslingual synthesis for Indian languages (Telugu, Marathi, Hindi) and English, which have over 600 million speakers. Their VECL-TTS system utilizes multilingual speaker and emotion embeddings to preserve voice identity and emotional style in all languages and outperforms state-of-the-art techniques with an 8.83% boost. This indicates the requirement for culturally oriented TTS systems sensitive to regional linguistic and emotional styles.\n\nThe recent research developments focus on enhancing TTS outputs through advanced modeling methods to improve their naturalness and control. The natural language guidance systems operate by specifying target speaker characteristics which include tone and style to deliver individualized experiences. OpenVoice enables users to generate accurate voice clones through its advanced models that operate with minimal data input requirements and maintain high sound quality. The research on deep models like SpeechT5 has demonstrated their effectiveness in improving spoken language processing across multiple languages. The studies conducted by Kothari et al.  [12]  show how user satisfaction with Indian English TTS systems depends on accent authenticity thus highlighting the current trend toward more controllable systems. The study reveals that accent-specific training datasets must exist to help models perform better across diverse linguistic situations. The study conducted by Liu et al.  [13]  explored the potential of transferring emotional features through different languages in TTS systems thus proving its practicality for maintaining naturalness while achieving cross-lingual emotional transfer. Despite these advances, multilingual TTS requires research to fill some existing gaps. Context-dependent transliteration requires further examination; presently existing transliteration models tend to mostly ignore context. Longitudinal investigations of the impact of accented TTS on language learners' speaking proficiency over several months would also benefit research. Cultural sensitivity also requires further investigation; future work should place more emphasis on developing culturally sensitive TTS systems and increasing the level of emotional variation depending on region. Lastly, future research should investigate using Large Language Models (LLMs) to factor emotional expressiveness, as Yang et al.  [14]  propose, where emotions can be varied according to context and contextual models such as Wang et al.  [15]  on emotional recognition accuracy. Addressing these gaps could help the next generation of TTS systems with enhancing cultural relevance, emotional richness, and linguistic flexibility, ultimately closing the 'gaps' in communication in a globalized world.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Methodology",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Dataset Description",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Dataset Acquisition",
      "text": "The project utilized the \"hindi_speech_male_5hr\" dataset of the Hugging Face Hub  [16] . The dataset is Hindi speech samples with their respective transcriptions, offering a basis for training a multilingual TTS model with Hindi language and male voice emphasis. We utilized the \"indian_accent_english\" dataset for the Indian accent and the \"english_emotions\" dataset that we extracted from the expresso dataset offered by Parler TTS developers  [17] .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Dataset Exploration",
      "text": "Initial dataset exploration yielded its composition and structure: The dataset has been split into training splits and consists of several parquet files (train-00000-of00009.parquet through train-00008-of00009.parquet) holding the audio samples and their transcriptions. Every sample in the dataset consists of two important aspects: A transcription field with the Hindi text content and an audio field holding the related speech audio as a numpy array and its sampling rate.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Data Handling",
      "text": "The audio samples were retrieved as numpy arrays, and thus were easily manipulable and feature-extractible. Sampleby-sample checking was carried out for the integrity of the audio-text pairs. This included printing the transcription and playing the relating audio for checking the consistency between the spoken material and the text given.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Transliteration",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Dataset Cleaning",
      "text": "For the fine-tuning approach of Parler TTS which is our reference model, dataset cleaning is critical to the process of optimizing the quality of training data. It involves a number of key steps: first, the stripping off of special characters and normalization of the audio array to maintain uniformity; second, the standardization of audio file formats to ensure consistency across the dataset. All audio files are resampled to a standard 44.1kHz sampling rate to ensure compatibility with Parler's audio compression DAC (parler-tts/dac_44khZ_8kbps). In addition, only certain voice datasets are extracted to ensure consistency in training. The audio and its associated text are also checked for alignment correctness. The cleaned data is then prepared in paired audio-text samples for smooth model ingestion. Feature Tagging: During the feature tagging process for fine-tuning Parler-TTS on the Indian accent data set, we employ the dataspeech library to tag several speech features. Some of these features are speaking rate (computed as phonemes per utterance length), signal-to-noise ratio (SNR), reverberation, and speech monotony.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Feature Tagging",
      "text": "These features help to identify key aspects of speech, which are essential for fine-tuning the model. For Multilingual training Hindi language was manually tagged and also emotions; \"whisper\", \"enunciation\", \"sad\", \"default\", \"laughing\", \"confused\", \"happy\", and \"emphasis\", were also tagged. Annotation Mapping: Mapping the annotated features to text bins is the subsequent step, maintaining consistency with the bins of the datasets on which Parler-TTS v0.1 model 14 was first trained. By using the existing v01_bin_edges.json file, we don't need to manually recompute the bins, which is a huge time saver.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Annotation Mapping",
      "text": "This script executes fairly quickly, and the resulting dataset, which has been enriched with feature tags such as \"quite noisy\" or \"very fast,\" is subsequently pushed to the HuggingFace hub under an assigned handle (e.g., En1gma02/indian_accent_english_tagged). This makes the dataset available for subsequent training and model testing. Natural Language Description Generation: After tagging the Indian Accent dataset with text bins, the next step is to create natural language prompts based on these features. For example: \"In a very expressive voice, Akshansh speaks slowly with some background noise and echo.\" This process requires more computational resources, typically running on GPUs.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Natural Language Description Generation",
      "text": "Utilizing the Gemma 2B model  [18]  to generate prompts, the script is executed within roughly 15 minutes on a publicly available T4 GPU in Google Colab. Important flags such as -speaker_name \"Akshansh\" and -is_single_speaker facilitate ensuring the accuracy of prompts within a monospeaker environment.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Data Tokenization And Encoding",
      "text": "In this step, the text descriptions are encoded and tokenized to get them ready for training. The text encoder projects each description into a sequence of hidden-state representations so that the model can process the text data. For our model, the text encoder is frozen and initialized completely from the Flan-T5  [19]  model. This ensures that the text is always represented in a manner consistent with the pretrained language model's capabilities, without additional training on the encoder itself.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Content Encoder",
      "text": "This module is responsible for extracting content representation from the input. It usually employs a feedforward Transformer structure with several layers (e.g., 4 feedforward Transformer blocks  [20] ). The hidden size is normally 256, with 2 attention heads. A variance adaptor is added for predicting duration, pitch, and other prosodic features.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Style Encoder",
      "text": "This module processes the style prompt to retrieve style information. It typically uses a pretrained language model such as RoBERTa or BERT  [21] . The [CLS] token representation is often used as the style embedding. To enhance style control, a multi-stage training procedure is often adopted: a. Pre-training on a large text corpus b. Fine-tuning on style-related tasks (e.g., natural language inference) c. Cross-modal representation learning between style prompts and speech A learnable embedding table is used to represent speaker identities.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Acoustic Model",
      "text": "The acoustic model generates the output speech features. Two main approaches are observed: a. Continuous acoustic modeling: Directly predicting mel-spectrograms using Transformer-based or diffusionbased models.  [22]  b. Discrete acoustic modeling: Using vector quantization (VQ) to transform mel spectrograms or waveforms into discrete tokens, followed by a discrete diffusion model or autoregressive model to generate these tokens.  [23]  For discrete modeling, a VQ-VAE is first trained to quantize the acoustic features.  [24]  The acoustic model subsequently predicts these discrete tokens, typically with a Transformer-based architecture with discrete diffusion. The model is most commonly trained with a mix of reconstruction losses, adversarial losses for enhancing audio quality, and mutual information minimization to separate style from content and speaker information. In inference, the style prompt is encoded and applied to condition the acoustic model to produce speech with the required style features. The architecture supports flexible control of speech style through natural language prompts while preserving high-quality speech synthesis.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Fine-Tuning For Indian Accent",
      "text": "The training process to adapt our Indian Accent to an English-speaking corpus consisted of a number of steps which used a learning rate of 1 -4 with the AdamW optimizer. Gradient clipping was used with a max norm of 1.0 to avoid exploding gradients. The model was trained over 100,000 steps at a batch size of 32. A linear learning rate 16 scheduler was used, from the initial learning rate to decay to zero over training. The loss function summed multiple components together, such as melspectrogram reconstruction loss, duration prediction loss, and pitch prediction loss. These were weighted and summed together to create the total loss that was used for backpropagation. The training cycle involved frequent monitoring on a held-out validation set every 1,000 steps to keep an eye on performance and avoid overfitting. Periodic checkpoints were saved, and the best model was chosen based on minimum validation loss. A special collate function was also developed to deal with variable length input sequences in each batch. This opened up the possibility of having a model ready on which we could train our Indic Languages.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Fine-Tuning For Hindi Speech Generation",
      "text": "The training procedure for multilingual speech synthesis was based on an existing model that was first trained on Indian speaker accents as discussed earlier. This established a robust foundation for subsequent improvement of the TTS system in a multilingual Indian scenario  [25] . The following training used a batch size of 32, which is the amount of samples processed before the model's internal parameters are updated. A learning rate of 5 × 10 -5 was utilized to manage the size of model weight updates during training. Training was performed for 2 epochs, enabling several passes over the full dataset to sharpen predictions. Optimization was performed using the Adam optimizer; an algorithm that is specifically developed for training deep neural networks. Cross-entropy loss was utilized as the loss function, a function which is widely used in classification tasks. These parameters and features were important to set up in configuring the model and fine-tuning its performance in the training process, augmenting the early Indian accent groundwork to enhance the TTS system's capacity for processing varied Indian language pronunciations and intonations.  [26]  Figure  3 : Training process for the emotion-based model, illustrating the flow from the base model, addition of accents, custom speaker training, and resulting final model",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Emotion-Based Model Fine-Tuning",
      "text": "The emotion-based speech generation model was also trained to record and generate speech with diverse emotional tones. The training utilized a pre-trained base model (\"parler-tts-mini-v1\") that came with a feature extractor specifically tailored for high-quality audio outputs (\"dac_44khZ_8kbps\"). The model was trained using a dataset consisting of labeled emotional speech examples (\"processed_english_emotions\") so that the model could learn the mappings between text prompts and emotional speech features. The training employed a batch size of 1 and gradient accumulation steps of 18, ensuring memory-efficient utilization and stable training. The learning rate was 8 × 10-5, and Adam was used as the weight update optimizer with a 50-step warmup to stabilize the training. The model was optimized over 10 epochs with a constant learning rate scheduler with warmup. Cross-entropy loss function was employed to reduce the difference between predicted and real speech emotions. The ultimate model assessment had a loss of 3.27, which indicates its ability to produce emotionally rich speech outputs.  [27]  4 Results\n\nThe model's performance was stringently tested using a mix of objective and subjective measures, highlighting its flexibility to Indian accents and emotional expressions. Objective tests showed a 94% accuracy in controlling gender and 68% in accent control, with the model exhibiting high correlations between descriptions and synthesized speech on different attributes. For audio quality, the model surpassed the Audiobox system in critical metrics such as Perceptual Evaluation of Speech Quality (PESQ), Short-Time Objective Intelligibility (STOI)  [28] , and Scale-invariant signalto-distortion ratio (SISDR) and was very close to ground truth quality. Subjective testing also verified the model's superiority in naturalness and relevance to descriptions (REL), which was superior even to the ground truth because of inherent label noise and audio artifacts in the original recordings. The performance of the model was especially impressive in the synthesis of Hindi-accented English speech with lower WER than that of English-accented Hindi, demonstrating its skill in cross-lingual synthesis and capability to convey emotional expressions  [29]  and variation in accents with ease.  The heatmap depicted in Figure  5 , visualizes key performance metrics-Accent Authenticity, Emotion Recognition (%), MOS Score, Naturalness Score, and Word Error Rate (%) for different combinations of language and emotion. More intense shades have higher scores or better performance  [30] . As an example, Hindi-Excited scores consistently high across nearly all metrics and English-Neutral has lower MOS and Naturalness values. It shows the fluctuations in model performance based on the language-emotion combination and the areas that can be improved based on certain combinations. results show that the model performs better than the baseline for all emotions with scores closer to SOTA. Significantly, the performance discrepancy is strongest on \"Sad\" emotion synthesis in which both SOTA and model exhibit greater emotional fidelity than that of the baseline. It further compares the performance of the model, together with a baseline model, and an existing state-of-the-art (SOTA) system on three objective evaluation scores: Mel Cepstral Distortion (MCD), Perceptual Evaluation of Speech Quality (PESQ)  [32] , and Short-Time Objective Intelligibility (STOI). MCD (better lower) is lowest for the model with best spectral accuracy. PESQ and STOI (better higher) are considerably higher for the model than the baseline, indicating better speech quality and intelligibility.\n\nThe SOTA system has the highest scores across the board, but the model is competitive, especially in PESQ and STOI. Spectrograms shown in Figure  5 . reveal frequency distribution in time for four emotion-language pairings: Hindi Neutral, Hindi-Excited, Indian-English-Neutral, and Indian-English-Formal. Each of these spectrograms represents distinctive energy distribution patterns between frequencies. For example, Hindi-Excited has more acute high-frequency parts showing greater emotional intensity. From these representations, one can discern the emotional context and accent differentiation picked up by the model on synthesized speech.  The WER is lower in the case of the Hindi-English configuration, implying higher accuracy in synthesizing speech with a Hindi accent  [33] . The English-Hindi configuration has a slightly greater WER, implying scope for improvement in synthesizing speech with an English accent in Hindi. The results point to the difficulties of adhering to linguistic authenticity in cross-lingual TTS systems.",
      "page_start": 6,
      "page_end": 10
    },
    {
      "section_name": "Conclusion",
      "text": "The research successfully proposes a state-of-the-art TTS system that helps resolve major issues in multilingual accents and emotion-based TTS systems, specifically for Indic languages. The system combines transliteration, accent modeling, and emotive expression techniques that fills significant gaps in linguistic and cultural nuances inclusion. It adjusts dynamically among accents, like-Indian English and Hindi language, in a single speech synthesis, which is extremely useful for multicultural use cases. It also provides prompt-based description personalization that allows forcustomized speech outputs depending on individual's language, tone, and style choices.\n\nOne of the unique aspects of the system is that it can capture a varied spectrum of human emotions through deep learning methods, that provides depth and realism to use cases such as virtual assistants and even audiobook narrations. The model design promotes efficiency, scalability, and flexibility, making it applicable across various industries, ranging from Ed-Tech to accessible content creation and even localized virtual assistants. This model provides for more inclusive and culturally aware solutions, thereby expanding the range of applications for TTS technology.\n\nFuture research could be extended to provide multilingual support to more different Indic languages and regional dialects. More diverse methods to capture regional dialect features and cross-lingual transfer learning could be applied to enhance TTS quality for diverse languages. Incorporating multimodal nuances and context-based emotion modeling could further enhance emotional expression by making it more robust. Increasing the amount of training data, training the system with high-power GPUs and using more extensible models such as Parler TTS Large v1 can further enhance performance. Cloud deployment could also be used to offer scalable and accessible TTS services.",
      "page_start": 10,
      "page_end": 10
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Illustration of transliteration examples across English-Hindi. The columns represent source and target words.",
      "page": 4
    },
    {
      "caption": "Figure 2: Overview of the training phase of text-to-speech synthesis pipeline.",
      "page": 5
    },
    {
      "caption": "Figure 3: Training process for the emotion-based model, illustrating the flow from the base model, addition of accents,",
      "page": 6
    },
    {
      "caption": "Figure 4: Results and Evaluation Metrics for Indian Accent",
      "page": 7
    },
    {
      "caption": "Figure 4: plots the audio features Mean Spectral Centroid, Standard Deviation of MFCCs, Mean Zero-Crossing Rate,",
      "page": 7
    },
    {
      "caption": "Figure 5: Relevance of our model in Emotions and Accents",
      "page": 8
    },
    {
      "caption": "Figure 5: , visualizes key performance metrics—Accent Authenticity, Emotion Recognition",
      "page": 8
    },
    {
      "caption": "Figure 6: Objective and Subjective Evaluation for Metric for Emotion Against other models",
      "page": 8
    },
    {
      "caption": "Figure 6: illustrates the Mean Opinion Scores (MOS) for synthesized speech for three emotional categories: Happy, Sad,",
      "page": 8
    },
    {
      "caption": "Figure 5: reveal frequency distribution in time for four emotion-language pairings: Hindi",
      "page": 9
    },
    {
      "caption": "Figure 7: Spectrogram of Decibel & Energy Fluctuation in Accent of generated speech",
      "page": 9
    },
    {
      "caption": "Figure 7: plots audio features like Mean Spectral Centroid, Standard Deviation of MFCCs [34], Mean Zero-Crossing",
      "page": 9
    },
    {
      "caption": "Figure 8: Cross-Lingual Comparison of our Model",
      "page": 10
    },
    {
      "caption": "Figure 8: plots the Word Error Rate (WER) for cross-lingual speech synthesis in Hindi-English and English-Hindi",
      "page": 10
    }
  ],
  "tables": [],
  "citations": [
    {
      "citation_id": "1",
      "title": "Transfer learning from speaker verification to multispeaker text-to-speech synthesis",
      "authors": [
        "Ye Jia",
        "Yu Zhang",
        "Ron Weiss",
        "Quan Wang",
        "Jonathan Shen",
        "Fei Ren",
        "Zhifeng Chen",
        "Patrick Nguyen",
        "Ruoming Pang",
        "Ignacio Moreno",
        "Yonghui Wu"
      ],
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "2",
      "title": "Multilingual text-to-speech synthesis for turkic languages using transliteration",
      "authors": [
        "Rustem Yeshpanov",
        "Saida Mussakhojayeva",
        "Yerbolat Khassanov"
      ],
      "year": "2023",
      "venue": "Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH",
      "arxiv": "arXiv:2305.15749"
    },
    {
      "citation_id": "3",
      "title": "Accented text-to-speech synthesis with limited data",
      "authors": [
        "Xuehao Zhou",
        "Mingyang Zhang",
        "Yi Zhou",
        "Zhizheng Wu",
        "Haizhou Li"
      ],
      "year": "2024",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "4",
      "title": "Multi-scale accent modeling with disentangling for multi-speaker multi-accent tts synthesis",
      "authors": [
        "Xuehao Zhou",
        "Mingyang Zhang",
        "Yi Zhou",
        "Zhizheng Wu",
        "Haizhou Li"
      ],
      "year": "2024",
      "venue": "Multi-scale accent modeling with disentangling for multi-speaker multi-accent tts synthesis",
      "arxiv": "arXiv:2406.10844"
    },
    {
      "citation_id": "5",
      "title": "Text to speech synthesis: A systematic review, deep learning based architecture and future research direction",
      "authors": [
        "Fahima Khanam",
        "Farha Akhter Munmun",
        "Nadia Ritu",
        "Aloke Saha",
        "Muhammad Firoz"
      ],
      "year": "2022",
      "venue": "Journal of Advances in Information Technology"
    },
    {
      "citation_id": "6",
      "title": "Learning to speak fluently in a foreign language: Multilingual speech synthesis and cross-language voice cloning",
      "authors": [
        "Yu Zhang",
        "Ron Weiss",
        "Heiga Zen",
        "Yonghui Wu",
        "Zhifeng Chen",
        "R Skerry-Ryan",
        "Ye Jia",
        "Andrew Rosenberg",
        "Bhuvana Ramabhadran"
      ],
      "year": "2019",
      "venue": "Learning to speak fluently in a foreign language: Multilingual speech synthesis and cross-language voice cloning",
      "arxiv": "arXiv:1907.04448"
    },
    {
      "citation_id": "7",
      "title": "Explicit intensity control for accented text-to-speech",
      "authors": [
        "Rui Liu",
        "Haolin Zuo",
        "De Hu",
        "Guanglai Gao",
        "Haizhou Li"
      ],
      "year": "2022",
      "venue": "Explicit intensity control for accented text-to-speech",
      "arxiv": "arXiv:2210.15364"
    },
    {
      "citation_id": "8",
      "title": "Metts: Multilingual emotional text-to-speech by crossspeaker and cross-lingual emotion transfer",
      "authors": [
        "Xinfa Zhu",
        "Yi Lei",
        "Tao Li",
        "Yongmao Zhang",
        "Hongbin Zhou",
        "Heng Lu",
        "Lei Xie"
      ],
      "year": "2024",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "9",
      "title": "Vecl-tts: voice identity and emotional style controllable cross-lingual text-to-speech",
      "authors": [
        "Ashishkumar Gudmalwar",
        "Nirmesh Shah",
        "Sai Akarsh",
        "Pankaj Wasnik",
        "Rajiv Ratn Shah"
      ],
      "year": "2024",
      "venue": "Vecl-tts: voice identity and emotional style controllable cross-lingual text-to-speech",
      "arxiv": "arXiv:2406.08076"
    },
    {
      "citation_id": "10",
      "title": "Ed-tts: Multi-scale emotion modeling using cross-domain emotion diarization for emotional speech synthesis",
      "authors": [
        "Haobin Tang",
        "Xulong Zhang",
        "Ning Cheng",
        "Jing Xiao",
        "Jianzong Wang"
      ],
      "year": "2024",
      "venue": "ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "11",
      "title": "Advanced emotion and multi-speaker recognition with multilingual voicecloning in cross-cultural communication",
      "authors": [
        "N Jayapratha",
        "M Vijaysurya",
        "G Lingeshwaran",
        "Vema Naga Karish",
        "Shivaprasanna Gupta"
      ],
      "year": "2024",
      "venue": "International Journal of Innovative Science and Research Technology (IJISRT)"
    },
    {
      "citation_id": "12",
      "title": "Exploring cross-linguistic speech perception in hindi, english, and romance-language through temporal dynamics of neural activity",
      "authors": [
        "Yuga Kothari"
      ],
      "year": "2024",
      "venue": "Exploring cross-linguistic speech perception in hindi, english, and romance-language through temporal dynamics of neural activity"
    },
    {
      "citation_id": "13",
      "title": "Diclet-tts: Diffusion model based crosslingual emotion transfer for text-to-speech-a study between english and mandarin",
      "authors": [
        "Tao Li",
        "Chenxu Hu",
        "Jian Cong",
        "Xinfa Zhu",
        "Jingbei Li",
        "Qiao Tian",
        "Yuping Wang",
        "Lei Xie"
      ],
      "year": "2023",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "14",
      "title": "Discrete diffusion model for text-to-sound generation",
      "authors": [
        "Dongchao Yang",
        "Jianwei Yu",
        "Helin Wang",
        "Wen Wang",
        "Chao Weng",
        "Yuexian Zou",
        "Dong Yu",
        "Diffsound"
      ],
      "year": "2023",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "15",
      "title": "Sparktts: An efficient llm-based text-to-speech model with single-stream decoupled speech tokens",
      "authors": [
        "Xinsheng Wang",
        "Mingqi Jiang",
        "Ziyang Ma",
        "Ziyu Zhang",
        "Songxiang Liu",
        "Linqin Li",
        "Zheng Liang"
      ],
      "year": "2025",
      "venue": "Sparktts: An efficient llm-based text-to-speech model with single-stream decoupled speech tokens",
      "arxiv": "arXiv:2503.01710"
    },
    {
      "citation_id": "16",
      "title": "Indicspeech: Text-to-speech corpus for indian languages",
      "authors": [
        "Nimisha Srivastava",
        "Rudrabha Mukhopadhyay",
        "C Jawahar"
      ],
      "year": "2020",
      "venue": "Proceedings of the Twelfth Language Resources and Evaluation Conference"
    },
    {
      "citation_id": "17",
      "title": "Natural language guidance of high-fidelity text-to-speech with synthetic annotations",
      "authors": [
        "Dan Lyth",
        "Simon King"
      ],
      "year": "2024",
      "venue": "Natural language guidance of high-fidelity text-to-speech with synthetic annotations",
      "arxiv": "arXiv:2402.01912"
    },
    {
      "citation_id": "18",
      "title": "Gemma 2: Improving open language models at a practical size",
      "authors": [
        "Gemma Team",
        "Morgane Riviere",
        "Shreya Pathak",
        "Giuseppe Sessa",
        "Cassidy Hardin",
        "Surya Bhupatiraju",
        "Léonard Hussenot"
      ],
      "year": "2024",
      "venue": "Gemma 2: Improving open language models at a practical size",
      "arxiv": "arXiv:2408.00118"
    },
    {
      "citation_id": "19",
      "title": "Scaling instruction-finetuned language models",
      "authors": [
        "Chung Hyung Won",
        "Le Hou",
        "Shayne Longpre",
        "Barret Zoph",
        "Yi Tay",
        "William Fedus",
        "Yunxuan Li"
      ],
      "year": "2024",
      "venue": "Journal of Machine Learning Research"
    },
    {
      "citation_id": "20",
      "title": "Attention is all you need",
      "authors": [
        "Ashish Vaswani",
        "Noam Shazeer",
        "Niki Parmar",
        "Jakob Uszkoreit",
        "Llion Jones",
        "Aidan Gomez",
        "Łukasz Kaiser",
        "Illia Polosukhin"
      ],
      "year": "2017",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "21",
      "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "Jacob Devlin",
        "Ming-Wei Chang",
        "Kenton Lee",
        "Kristina Toutanova"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 conference of the North American chapter of the association for computational linguistics: human language technologies"
    },
    {
      "citation_id": "22",
      "title": "Boosting fast and high-quality speech synthesis with linear diffusion",
      "authors": [
        "Haogeng Liu",
        "Tao Wang",
        "Jie Cao"
      ],
      "year": "2023",
      "venue": "Boosting fast and high-quality speech synthesis with linear diffusion",
      "arxiv": "arXiv:2306.05708"
    },
    {
      "citation_id": "23",
      "title": "Emovoice: Llm-based emotional textto-speech model with freestyle text prompting",
      "authors": [
        "Guanrou Yang",
        "Chen Yang",
        "Qian Chen",
        "Ziyang Ma",
        "Wenxi Chen",
        "Wen Wang",
        "Tianrui Wang"
      ],
      "year": "2025",
      "venue": "Emovoice: Llm-based emotional textto-speech model with freestyle text prompting",
      "arxiv": "arXiv:2504.12867"
    },
    {
      "citation_id": "24",
      "title": "Vqalattent: a transparent speech generation pipeline based on transformer-learned vq-vae latent space",
      "authors": [
        "Armani Rodriguez",
        "Silvija Kokalj-Filipovic"
      ],
      "year": "2024",
      "venue": "Vqalattent: a transparent speech generation pipeline based on transformer-learned vq-vae latent space",
      "arxiv": "arXiv:2411.14642"
    },
    {
      "citation_id": "25",
      "title": "Indicvoices-r: Unlocking a massive multilingual multispeaker speech corpus for scaling indian tts",
      "authors": [
        "Ashwin Sankar",
        "Srija Anand",
        "Praveen Varadhan",
        "Sherry Thomas",
        "Mehak Singal",
        "Shridhar Kumar",
        "Deovrat Mehendale",
        "Aditi Krishana",
        "Giri Raju",
        "Mitesh Khapra"
      ],
      "year": "2024",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "26",
      "title": "Detection of ai synthesized hindi speech",
      "authors": [
        "Karan Bhatia",
        "Ansh Agrawal",
        "Priyanka Singh",
        "Arun Singh"
      ],
      "year": "2022",
      "venue": "Detection of ai synthesized hindi speech",
      "arxiv": "arXiv:2203.03706"
    },
    {
      "citation_id": "27",
      "title": "emotion2vec: Self-supervised pre-training for speech emotion representation",
      "authors": [
        "Ziyang Ma",
        "Zhisheng Zheng",
        "Jiaxin Ye",
        "Jinchao Li",
        "Zhifu Gao",
        "Shiliang Zhang",
        "Xie Chen"
      ],
      "year": "2023",
      "venue": "emotion2vec: Self-supervised pre-training for speech emotion representation",
      "arxiv": "arXiv:2312.15185"
    },
    {
      "citation_id": "28",
      "title": "Objective quality and intelligibility prediction for users of assistive listening devices: Advantages and limitations of existing tools",
      "authors": [
        "H Tiago",
        "Vijay Falk",
        "Joao Parsa",
        "Kathryn Santos",
        "Oldooz Arehart",
        "Rainer Hazrati",
        "James Huber",
        "Susan Kates",
        "Scollie"
      ],
      "year": "2015",
      "venue": "IEEE signal processing magazine"
    },
    {
      "citation_id": "29",
      "title": "Emotion identification for evaluation of synthesized emotional speech",
      "authors": [
        "Stefan Steidl",
        "Tim Polzehl",
        "H Bunnell",
        "Ying Dou",
        "Prasanna Kumar Muthukumar",
        "Daniel Perry",
        "Kishore Prahallad",
        "Callie Vaughn",
        "Alan Black",
        "Florian Metze"
      ],
      "year": "2012",
      "venue": "Proc. speech prosody"
    },
    {
      "citation_id": "30",
      "title": "Stuck in the mos pit: A critical analysis of mos test methodology in tts evaluation",
      "authors": [
        "Ambika Kirkland",
        "Shivam Mehta",
        "Harm Lameris",
        "Gustav Henter",
        "Eva Székely",
        "Joakim Gustafson"
      ],
      "venue": "12th Speech Synthesis Workshop (SSW)"
    },
    {
      "citation_id": "31",
      "title": "The limits of the mean opinion score for speech synthesis evaluation",
      "authors": [
        "Le Sébastien",
        "Simon Maguer",
        "Naomi King",
        "Harte"
      ],
      "year": "2024",
      "venue": "Computer Speech & Language"
    },
    {
      "citation_id": "32",
      "title": "Perceptual evaluation of speech quality (pesq)-a new method for speech quality assessment of telephone networks and codecs",
      "authors": [
        "Antony Rix",
        "John Beerends",
        "Michael Hollier",
        "Andries Hekstra"
      ],
      "year": "2001",
      "venue": "2001 IEEE international conference on acoustics, speech, and signal processing. Proceedings"
    },
    {
      "citation_id": "33",
      "title": "Identification and classification of tts intelligibility errors using asr: A method for automatic evaluation of speech intelligibility",
      "authors": [
        "Erik Henriksson"
      ],
      "year": "2023",
      "venue": "Identification and classification of tts intelligibility errors using asr: A method for automatic evaluation of speech intelligibility"
    },
    {
      "citation_id": "34",
      "title": "Towards interpretable speech biomarkers: exploring mfccs",
      "authors": [
        "B Tracey",
        "D Volfson",
        "J Glass"
      ],
      "year": "2023",
      "venue": "Sci Rep"
    }
  ]
}