{
  "paper_id": "2109.09366v1",
  "title": "Few-Shot Emotion Recognition In Conversation With Sequential Prototypical Networks",
  "published": "2021-09-20T08:33:38Z",
  "authors": [
    "Gaël Guibon",
    "Matthieu Labeau",
    "Hélène Flamein",
    "Luce Lefeuvre",
    "Chloé Clavel"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Several recent studies on dyadic humanhuman interactions have been done on conversations without specific business objectives. However, many companies might benefit from studies dedicated to more precise environments such as after sales services or customer satisfaction surveys. In this work, we place ourselves in the scope of a live chat customer service in which we want to detect emotions and their evolution in the conversation flow. This context leads to multiple challenges that range from exploiting restricted, small and mostly unlabeled datasets to finding and adapting methods for such context. We tackle these challenges by using Few-Shot Learning while making the hypothesis it can serve conversational emotion classification for different languages and sparse labels. We contribute by proposing a variation of Prototypical Networks for sequence labeling in conversation that we name ProtoSeq. We test this method on two datasets with different languages: daily conversations in English and customer service chat conversations in French. When applied to emotion classification in conversations, our method proved to be competitive even when compared to other ones. The code for Proto-Seq is available at https://github.com/ gguibon/ProtoSeq.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "There has been a recent surge in research focusing on analyzing dyadic human to human interactions. Many of these studies  (Poria et al., 2017; Zadeh et al., 2018a,b; Majumder et al., 2019)  focus on emotion recognition in conversations (ERC) taking into account multiple data modalities. Moreover, most of the progress made in ERC has been done without factoring in constraints corresponding to specific but prominent industrial applications, like customer service. This is partly due to studies focusing on using artificial datasets  (Li et al., 2017; Busso et al., 2008)  made of mock-up conversations to facilitate result replication and comparison. A few existing studies address customer service applications  (Mundra et al., 2017; Yom-Tov et al., 2018; Maslowski et al., 2017)  and show the difficulties to deal with such in-the-wild and domain-specific data.\n\nIn this work, we focus on data from a live chat support in which we want to detect emotions and their evolution in the conversational flow. This setting corresponds to a human dyadic conversation, albeit with a specific business-related objective. We make the hypothesis that the emotion flows of the visitor and the operator will bring information on the quality of the service and help operators better assist customers. This hypothesis is close to relevant studies on the importance of emotions and empathy in dyadic call center conversations  (Alam, 2017; Alam et al., 2018) . This specific setting leads to multiple challenges: indeed, it is difficult and costly to label this kind of data -and even then, these exchanges are very sparse in emotions, most of the labels associated with utterances being neutral. To maximize data efficiency, we use Few-Shot Learning (FSL), and adapt a popular approach to our highly unbalanced data. By setting up this approach in an episodic fashion  (Ravi and Larochelle, 2016) , we join studies on ERC and studies on FSL to tackle this industrial use-case.\n\nWe contribute by proposing a variant to Prototypical Networks  (Snell et al., 2017)  dedicated to ERC on data produced by company services, framing it as a sequence labeling task. We modify the original model by allowing it to consider the whole conversational context when making predictions, through a sequential context encoder and the use of Conditional Random Fields (CRF) on top of the model. We test our method on two datasets, in two different languages. The first one, made of daily conversations in English, allows us to compare ourselves to previous methods, while the second one, made of private data from a live chat customer ser-vice, allows us to conduct a performance analysis in our target setting. We also present the latter dataset, along with its annotation process.\n\nThis paper is organized as follows. First, we sum up the related work on textual ERC and FSL in conversations (Section 2). Then we present the datasets along with the emotional annotation scheme and the annotation campaign set up for the customer service live chats dataset (Section 3). We continue by thoroughly presenting the Sequential Prototypical Networks (Section 4) before looking at the achieved results on both datasets (Section 5). Finally, we present the limitations of such a system (Section 6) and conclude (Section 7).",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Related Work",
      "text": "Emotion Recognition in Conversations In recent years, the widening scope of emotion detection tasks led to the rise of another sub-topic: detecting emotions in conversations. This research topic, commonly referred to as ERC, gained popularity when  Poria et al. (2017)  first applied recurrent neural networks (RNN) (Jordan, 1997) to multi-modal emotion recognition in conversations. This led to many improvements  (Zadeh et al., 2018a,b; Hazarika et al., 2018; Majumder et al., 2019) . Among those,  Majumder et al. (2019)  used 3 Gated Recurrent Units (GRU)  (Cho et al., 2014)  units, one for each context representation target (speaker, utterance, emotion). Studies on ERC applied to text followed, mainly built on an artificial conversation dataset named DailyDialog  (Li et al., 2017) .  (Zhong et al., 2019)  incorporated a knowledge base into the network using context-aware attention and hierarchical self-attention using Transformers  (Vaswani et al., 2017) .  Ghosal et al. (2019)  uses graph neural networks to deal with context propagation limitations. These approaches in ERC consider the conversational context surrounding the current utterance; on the other hand, some recent studies consider it as a sequence and tackled ERC through a sequence labeling task  (Wang et al., 2020) . We follow this last approach and consider the ERC task as a sequence labeling task. However, these supervised approaches are difficult to use, as it is hard to find a sufficient amount of conversations labeled with emotions. Hence, in this paper, we approach ERC as a few-shot learning problem.\n\nFew-Shot Learning FSL  (Miller et al., 2000; Fei-Fei et al., 2006; Lake, 2015)  is suitable to tackle this data limitation. It aims at generaliz-ing faster, leading to a lower dependency on data quantity. It is mainly set up through episodic composition  (Ravi and Larochelle, 2016)  which recreates the few-shot learning setting by working with small training episodes. Several learning methods are based on metric-learning: Siamese Networks, which share some weights, are used to learn a metric between examples  (Koch et al., 2015) . Matching Networks  (Vinyals et al., 2017)  use the training examples to find the weighted nearest neighbors  (Vinyals et al., 2017) . Prototypical Networks  Snell et al. (2017)  consider averaged class representations from the training examples and a cosine distance to compare the elements to these class representations. Relation networks replace the Euclidean by the deep neural network which aims at training a distance metric  (Sung et al., 2018) . In this work, we consider approaches based on Prototypical Networks. As  Al-Shedivat et al. (2021)  recently showed it, such approaches are the most efficient when working with a low amount of training samples. Many variants have been proposed, on different tasks and topics such as relation classification in text  (Gao et al., 2019; Hui et al., 2020; Ren et al., 2020) , sentiment classification in Amazon comments  (Bao et al., 2020) , named entity recognition  (Fritzler et al., 2019; Hou et al., 2020; Perl et al., 2020; Safranchik et al., 2020) , or even speech classification in conversation  (Koluguri et al., 2020) . This surge of interest on applying few-shot learning to these topics can be attributed to specific datasets, such as Few-Rel  (Han et al., 2018)  for relation classification. While ERC is mainly considered in a fully supervised learning setting, we intend to view it as a few-shot learning sequence labeling class. In this paper, we propose the first few-shot learning approach on ERC using sequence labeling through adapting Prototypical Networks. We compare our method to the original Prototypical Networks  (Snell et al., 2017)  and to a variant dedicated to named entity recognition  (Fritzler et al., 2019)  that is easily applicable to our task.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Data",
      "text": "To be able to both study the behavior of our model in its targeted industrial use-case, and allow performance comparison with baselines, we will work with two very different corpora: our proprietary live chat customer service dataset, and DailyDialog  (Li et al., 2017) . In both datasets, messages are labeled with emotions while considering the context of the conversation. However, they vary considerably in their topics and lexical fields: ordinary matters for DailyDialog and railway related customer service for the live chats. They also vary in the assumptions they make about the speakers : while the topics discussed in DailyDialog imply a sense of proximity, the live chat customer service involve complete strangers with pre-existing emotional states (e.g. the visitor is already stressed due to a refund issue). Both datasets' statistics can be found in Table  1 .",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Dailydialog",
      "text": "DailyDialog is a dyadic conversation dataset in English whose purpose is to represent casual, everyday interactions between people, in order to facilitate training and sharing of dialog systems. The exchanges in DailyDialog are artificial conversations which are neither dedicated to a specific topic nor task-oriented: they mainly deal with relationships, everyday life, and work. Each utterance corresponds to a speaker turn, and is labeled with one of 7 labels: the 6 basic emotions (anger, disgust, fear, joy, sadness, and surprise) and \"no_emotion\" denoting the absence of one. The \"no_emotion\" label represents 80% of the corpus, leading to a very unbalanced dataset with an average length of 8 messages per conversation and a maximum of 35 messages. For this dataset, the inter annotator agreement achieved 78.9%. We choose DailyDialog for comparison and reproducibility purposes, as it is often used for ERC. In this work, we use the train/val/test splits provided by  (Zhong et al., 2019) .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Live Chat Customer Service",
      "text": "Our primary objective is to detect emotions in conversations from a customer service live chat involving a visitor (i.e. the customer looking for help) and an operator (i.e. an employee being there to assist the visitor and better satisfy him). The corpus is written in French and is made of 5,000 conversations from which we annotate a subset of 1,500 conversations, leading to a total of 20,754 messages. The average message length is higher than DailyDialog, with 15.14 messages per conversation. We do not have a way to identify real speaker turns. Indeed, a speaker turn is not necessarily the sequence of contiguous segments corresponding to a same speaker because there could be a time delay between two messages of a same speaker, indicat-ing that the speaker is changing the topic. Because all our messages have a very short time difference we prefer not to automatically infer speaker turns and consider the message as the unit of analysis. This means the conversation context is a sequence of messages instead of a sequence of speaker turns which could have contained one or more messages artificially glued together. Two annotators were involved in the process, which unrolled as follows: first, each message is labeled with an emotion. Once all the messages in a conversation have been assigned an emotion label, the conversation is labeled with a visitor satisfaction score (ranging from -3 to 3), and the status of the customer request (\"solved\", \"test_required\", \"out of scope\", or \"aborted\"). After a preliminary study of the corpus, we identify 10 emotion labels as relevant in this corpus: neutral, surprise, amusement, satisfaction 1  , relief, fear-anxiety-stress, sadness, disappointment, anger, and frustration. Compared to  (Chowdhury et al., 2016) , we consider the satisfaction at the conversation level and we are more precise with not only positive, neutral, and negative levels, but also with 4 additional intermediate levels (from -3 to +3 included). We have also a higher number of emotions, with 10 emotions instead of 4, with more precise emotions such as relief for instance. In our customer service interface, some alerts are automatically prompted for specific actions such as \"user x left the chat\" or \"operator sent a link\". We call these \"alerts\", and they are labeled as \"no_emotion\". The \"neutral\" label means that the emotional content of the message, written by a human, has been considered as neutral by the annotator. Figure  1  illustrates the distribution of emotion labels in the Live Chat Customer Service dataset. We can see that the neutral label is the most frequent by a large margin. The Cohen's κ scores obtained on the 3 label types correspond to substantial agreement at the message level and moderate agreement at the conversation level  (Landis and Koch, 1977) . κ-score is given for 3 label types: 1) the emotions at the message level (κ = 0.65); 2) the visitor's satisfaction at the conversation level (κ = 0.45); and 3) the request's status at the conversation level (κ = 0.46). Similarly to DailyDialog, the \"neutral\" label represents 81.5% of the corpus, resulting in another very unbalanced dataset in terms of emotions, as rendered obvious by Figure  1 . Excluding this label gives a slightly more balanced label set, as the satisfaction represents 44.9% of the other emotions, and the \"frustration\" 20.8%. To tackle our hypothesis that the conversational emotion flow can define the overall visitor satisfaction, we calculate the Pearson correlation between the emotions at the message level and the global satisfaction of the visitor at the conversation level. These scores show the more extreme the emotion, the greater the correlation with the satisfaction score is 2 .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Dataset",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Methodology",
      "text": "Formally, our dataset D is comprised of conversations (C 1 , C 2 , . . . , C |D| ), which are in turn made of utterances:\n\nTo each of these utterances is associated an emotion label, giving a sequence of labels by conversation:\n\nFinally, an utterance is a sequence of words, u j = (w j 1 , w j 2 , . . . , w j |u j | ) .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Episodic Learning",
      "text": "We use the episodic approach  (Ravi and Larochelle, 2016) , which simulates a context where only a few examples per class are available during training and the model must adapt during testing. This approach perfectly fits into our need for FSL. The episodic composition is defined by setting the number of classes (ways) N C , the number of examples per class N S (shots) and the number of elements to label N Q (queries). In our experiments with Dai-lyDialog, the task is 5-shot 7-way 10-query, and when using our customer service chats, the number of classes changes, making it a 5-shot 11-way 10-query. In the context of sequential ERC, this means that for each episode we train the model on 5 conversations (i.e. sequences to label) per emotion and apply it to 10 conversations per emotion. We identify a sequence as belonging to the target class set if at least one message is labeled with the target class in the sequence. This means that the number of example messages in each support set S k of class k can vary (with a minimum of N S elements), while the number of sequences is fixed.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Protoseq: Prototypical Networks For Emotion Sequence Labeling",
      "text": "In order to apply FSL to ERC, we choose to base our model on Prototypical Networks  (Snell et al., 2017) , which create prototypes from the average of the embeddings of the words forming the utterance. Our proposed model, ProtoSeq, builds on this by factoring in conversational context and performing sequence labeling, thus allowing the use of both input and output dependencies when applying FSL to ERC. ProtoSeq is divided into four main components, applied at each consecutive level of granularity of the data. Utterance Encoder: Similar to the encoder of the Prototypical Networks, our utterance encoder f u reduces the utterance u i to only one vector:\n\nThe architecture of our encoder is based on the Convolutional Neural Network (CNN) described by  (Kim, 2014)  , which makes tokens through different convolution filters and merges the representation through max-over-time pooling.\n\nContext Encoder: After applying a non-linear activation (ReLU), we use a Bi-directional LSTM layer (BiLSTM)  (Huang et al., 2015) , to integrate contextual information from the conversation, thus following the trend initiated by  (Poria et al., 2017)  in ERC to use a recurrent context encoder. We obtain contextual utterance representations v j :\n\nAs we work in a few-shot learning setting, we try not to over-complexify our model, hence we do not add a transformer-based global context encoder  (Wang et al., 2020)  on top of the BiLSTM.\n\nPrototypes Creation: We feed the output of the context encoder to a multi-layer perceptron made of 2 fully connected layers with dropout and ReLU.\n\nThe resulting representations are then used to create prototypes c: for the class k,\n\nwhere N C is the number of classes, and MLP refers to Multi-Layer Perceptron.\n\nSequence Prediction: We compute the euclidean distance from the contextual representation of the utterance to each class prototype. The predicted label ŷj to each utterance u j is the class corresponding to its closest prototype:\n\nWe allow our model to consider dependencies between the labels, we add a final CRF layer on top of label prediction, the emission scores being the euclidean distances for each utterance. Overall, our model is a variation of the traditional BiLSM-CRF model, based on prototypical networks.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Experimental Protocol",
      "text": "We follow the setting used by  (Bao et al., 2020)  by considering a training epoch as a set of 100 random episodes from the training set, and applying a validation step made of 100 random episodes from the validation set after each epoch. We test our model using 1,000 random episodes from the test set. The maximum number of epochs is set to 1,000, but when the F1-micro score does not improve for 100 consecutive epochs, we stop the training and reload the best model's weights. We use the Adam (Kingma and Ba, 2017;  Loshchilov and Hutter, 2019)  optimizer to train the model while maximizing the log-likelihood loss of the correct emotion sequences in the query set\n\nDuring inference, we apply the Viterbi algorithm to output the best-scoring sequence of labels. We do not cut on either utterance or conversation length. To obtain an initial token representation, we use pre-trained FastText  (Bojanowski et al., 2017)  embeddings from Wiki News 3  for English (Daily-Dialog), and from Common Crawls 4  for French (customer service live chats). Both sets of embeddings are of dimension 300 and both datasets are tokenized with NLTK 5  . We choose our hyperparameters using a very targeted grid search for the learning rate (set to 1e3 for all the experiments) and manual tuning for the other parameters. In the following, we experiment with several variants of our model, each having dedicated hyper-parameters.\n\n• ProtoSeq: We use hyper-parameters from  Kim (2014)  for the CNN: 50 filters with windows 3 different sizes (3, 4 and 5). We use one BiLSTM layer with 150 hidden units in order to fit to the 300 dimensions of the inputs considering the two directions.\n\n• ProtoSeq-CNN: A lighter version of our model, without the BiLSTM context-encoder.\n\nThe CNN configuration follows the same parameters from  Kim (2014) .\n\n• ProtoSeq-Tr: A ProtoSeq with a 2-layers Transformer-based utterance encoder with 4 attention heads and a hidden size of 300. The global dropout is set to 0.2 while the position encoder dropout is set to 0.1.\n\n• ProtoSeq-AVG: A ProtoSeq where the utterance encoder is just an average of the token representations. However, it should be noted that the averaging process excludes the padding elements in the utterances.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Results",
      "text": "Tables  3  and 4  show the performance of the model using the micro F1-score. We use the protocol usually followed by the literature and do not take into account the majority class \"no_emotion\" as it represents 80% of the DailyDialog corpus. This allows performance comparison with related work on ERC through supervised learning. We do the same for the Live Chat Customer Service corpus by ignoring the \"neutral\" label.\n\nComparison to supervised learning DailyDialog is used to compare our FSL approach with recent supervised learning results on ERC. As expected, our best FSL model, ProtoSeq, yields lesser performance than supervised approaches. The latter presuppose the availability of a sufficiently large amount of annotated data and their performance thus represents the upper bound of the expected results. More precisely, we focus on the difference between ProtoSeq with a state-of-the-art supervised model, CESTa  (Wang et al., 2020) , which is computation-heavy. Indeed, CESTa is a contextualized emotion sequence tagging model which considers the fusion of a combination of a transformer and BiLSTM as the global context encoder with a recurrent individual context encoder before feeding a CRF layer. CESTa achieves 63% in micro F1-score in a fully supervised learning approach. ProtoSeq, much lighter, achieves a 31% micro F1 score, demonstrating the potential of FSL for sequence labeling when available data is scarce, especially when many supervised approaches obtained F1-scores around 50%. While using the Live Chat Customer Service dataset, we only change the initial embeddings from English to French, and apply the two best models according to 3: CESTa and KET  (Zhong et al., 2019) . The CESTa implementation yielded inconclusive results 6  , this is why we present the KET results on our specific corpus in Table  4 . KET relies on ConceptNet  (Speer et al., 2017) , a multilingual knowledge base. Thus, we only switch from GloVe embeddings  (Pennington et al., 2014)  to French FastText ones in order to ensure comparison with our ProtoSeq model. As expected, performance is lower on the Live Chat Customer Service corpus.\n\nFew-shot learning baselines We consider two baselines. We apply the original Prototypical Networks  (Snell et al., 2017) , only retrieving the labels using the euclidean distance to class prototypes. We also apply the WarmProto-CRF (Frit-Model F1 (weighted) MCC F1 (micro)\n\nSupervised Learning cLSTM 0.4990 CNN  (Kim, 2014)  0.4934 CNN+cLSTM  (Poria et al., 2017)  0.5184 BERT BASE  (Devlin et al., 2019)  0.5312 DialogueRNN  (Majumder et al., 2019)  0.5164 KET  (Zhong et al., 2019)  0.5337 CESTa  (Wang et al., 2020)  0.6312",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Few-Shot Learning",
      "text": "Proto  (Snell et al., 2017)  0.2377 ±0.0136 0.3448 ±0.0105 0.2141 ±0.0141 WarmProto-CRF  (Fritzler et al., 2019)    zler et al., 2019) which is a variant of Prototypical Networks designed for sequence labeling by integrating CRFs. We implement it without including the bias they created for the O label in the BIO sequence labeling task. This method uses a BiLSTM utterance encoder to further compute the prototypes with the euclidean distance.\n\nFew-shot learning on DailyDialog Table  3  shows FSL results in the bottom section. All these models are trained in an episodic fashion, with the same episode constitution (5-shot 7-way 10-query). We can see the micro F1-score is really low with only 16.43%. By considering a ProtoSeq only using an utterance encoder based on CNN (ProtoSeq-CNN) or an utterance encoder based on a 2-layers 4-heads Transformer (ProtoSeq-Tr) we can see the score improve. The addition of the BiL-STM context encoder really enables the model to capture more information: these variants show the importance of integrating a context encoder in the model.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Few-Shot Learning On Customer Service Live Chats",
      "text": "We also apply this approach on the Customer Service Live Chats, further motivated by the high annotation cost and the fact that supervised approaches on clean data such as DailyDialog did not achieve an acceptable score for this use case (starting from 70 % in micro F1 score). Besides, new conversations with evolving contents (e.g., due to the evolution of company services) are created everyday. As a consequence, it would render the ideally annotated training corpus obsolete at some point. This FSL prediction leads to lesser scores, but with the same hierarchy among variants. Proto-Seq, using a BiLSTM context encoder, yields again best scores. The higher number of classes (with 11 classes including 9 emotions versus 7 classes including 6 emotions) may explain the overall lower numbers we observe here, compared to those we obtain on DailyDialog.\n\nArtificial versus Real Data DailyDialog is an artificial corpus which follows standard, idealized conversations. We can see that ERC performance is quite sensible to the conversation length, which seems to confirm conclusions drawn in recent literature  (Wang et al., 2020) . Customer Service Live Chats being real use-case data, their length varies a lot, ranging from 2 to 85 messages (where conversations from DailyDialog go from 2 to 35 messages). However, ERC also seems to be impacted by the utterance textual content, as our data contains a lot of spelling mistakes, shortcuts, or slurs. More importantly, the visitor may often use several small messages rather than only one to transmit information; this flow may be interrupted by a message from the operator, making it impossible to detect the whole set of messages as an utterance. This is specific to online instant conversations where speakers do not necessarily wait for the complete message to be written or sent by the addressee. By contrast, DailyDialog is made of clean and perfect exchanges, where one waits for the other to send the answer. Here is an example with the following clean conversation subset from DailyDialog.\n\nA: Does your family have a record of your ancestors? B: Sure. My mom has been working on our family tree for years. This conversation would often be represented as follows in real data from instant chat:\n\nOperator: Did you make the simulation using the promo code? Visitor: I did it 5 minutes ago Operator: Ok, you have to wait 30min Visitor: but as said before, I didn't finished the \"simulation\" because I had to pay a 10C ticket even th Visitor: ....even though the right one is 11.5 C Operator: And the code will be available again Moreover, specific lexical fields, relevant to the customer service being provided, can also make the task more difficult for the model.\n\nQuantifying the impact of the CRF layer Our model benefits from the addition of a final CRF layer to compute the best possible output sequence. This allows the model to generalize faster and to achieve a higher score despite the few examples. However, the prediction stability lowers, as the standard deviation across episodes shows in Table  5 . The downgrade in performance while omitting the CRF layer may be due to the label dependency it emphasizes. Indeed, without the CRF, label dependency can only be inferred from the BiLSTM context encoder. The CRF layer accentuates in-episode label dependency by allowing the prediction to be further adapted to the conversation context for each query conversation.  Emotion Predictions Tables  6  and 7  show additional information from ProtoSeq's performance on each label. These tables present averaged scores from all episodes' query sets. We can see the predictions differ a lot depending on the target label. When applied to DailyDialog, the model has no difficulty in detecting the absence of emotion. This is to be expected as this label mainly represents the conversations. However, the prediction scores for emotion labels are imbalanced, with recall scores higher than precision on both datasets. On DailyDialog, the anger and the sadness labels really hinders the overall prediction. How-ever, on the Customer Service Live Chats, Table  7  shows really poor prediction for the disappointment (translated from the French \"déception\" label) and fear labels. Actually, in this dataset the precision seems to be the main issue with only the frustration and satisfaction labels being somewhat correctly labeled. Given the model and the task, the detailed results obtained on both datasets show that performance score may benefit from the usage of macro F1-score along with the micro F1-score. Indeed, be it DailyDialog or Customer Service Live Chats, the multi-class prediction of sequence tagging is really sparse, and thus leads to imbalanced prediction, even while using an episodic strategy.\n\nMoreover, the gap between results on DailyDialog and the ones on the Customer Service Live Chats confirms the necessity for the ERC-related studies to focus on real conversation datasets whenever it is possible.  Table  7 : Additional results on customer service live chats with our ProtoSeq prediction. We define the \"fear\" label as \"fear/anxiety/stress\". \"no emotion\" is only used for automatic chat prompts.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Limitations",
      "text": "While the ProtoSeq model seems to be suitable for FSL in ERC, it still has inherent limitations related to its architecture. ProtoSeq uses a CRF as its final layer, leading to a sequence labeling optimizer that does not take the order into account. While this yields better performance, it does not guarantee that the order information retrieved from the context encoder is wisely used, especially since we use the euclidean distances to class prototypes as emission scores for the sequence labeling. An ordered-prediction approach may allow the model to better assist operators in real-time during their decision process.\n\nAnother limitation of our model is that it may be difficult to adapt to changes in the context in which customer service is provided. Indeed, the type of service or the plaftorm used may lead to lexical field changes or very different emotional states for the incoming visitors.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Conclusion",
      "text": "In this paper, we presented the first study on emotion recognition in conversations using few-shot learning. We proposed a variant of Prototypical Networks taking into account the emotion recognition as a sequence labeling task while allowing fast convergence. When compared to other prototypical networks for sequence labeling in few-shot, our model obtained higher scores on both Daily-Dialog and Customer Service Live chats. Through this work, we showed that few-shot learning is possible for this task even though it is still difficult to achieve the same performance as supervised learning approaches. This study also shows the challenges that remain when tackling in-the-wild data collected in the context of a real application.\n\nFuture work will be dedicated to the improvement of the current few-shot ERC approach by adding unlabeled elements in the support set and by investigating the addition of external business knowledge to such an approach.  This Figure shows the correlation scores are higher when the emotion is extreme within a given polarity. For instance, anger is greatly correlated to a negative satisfaction score (vsent -3) than fear or disappointment, while \"Satisfaction\" is more correlated to a positive overall satisfaction score (vsent +3) than \"Amusement\" or \"Relief\" are to intermediate satisfaction scores (vsent_1 or vsent_2).",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "A Correlation Scores",
      "text": "",
      "page_start": 12,
      "page_end": 12
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: illustrates the distribution of",
      "page": 3
    },
    {
      "caption": "Figure 1: Excluding this label",
      "page": 4
    },
    {
      "caption": "Figure 1: Emotion Distribution in Live Chat Customer",
      "page": 4
    },
    {
      "caption": "Figure 2: ProtoSeq Global View",
      "page": 5
    },
    {
      "caption": "Figure 3: Pearson correlation scores between the vis-",
      "page": 13
    },
    {
      "caption": "Figure 3: presents the Pearson correlation scores",
      "page": 13
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "{gael.guibon,matthieu.labeau,chloe.clavel}@telecom-paris.fr": "{ext.gael.guibon,helene.ﬂamein,luce.lefeuvre}@sncf.fr"
        },
        {
          "{gael.guibon,matthieu.labeau,chloe.clavel}@telecom-paris.fr": "to facilitate result replication and comparison. A"
        },
        {
          "{gael.guibon,matthieu.labeau,chloe.clavel}@telecom-paris.fr": "few existing studies address customer service appli-"
        },
        {
          "{gael.guibon,matthieu.labeau,chloe.clavel}@telecom-paris.fr": ""
        },
        {
          "{gael.guibon,matthieu.labeau,chloe.clavel}@telecom-paris.fr": "cations (Mundra et al., 2017; Yom-Tov et al., 2018;"
        },
        {
          "{gael.guibon,matthieu.labeau,chloe.clavel}@telecom-paris.fr": ""
        },
        {
          "{gael.guibon,matthieu.labeau,chloe.clavel}@telecom-paris.fr": "Maslowski et al., 2017) and show the difﬁculties"
        },
        {
          "{gael.guibon,matthieu.labeau,chloe.clavel}@telecom-paris.fr": ""
        },
        {
          "{gael.guibon,matthieu.labeau,chloe.clavel}@telecom-paris.fr": "to deal with such in-the-wild and domain-speciﬁc"
        },
        {
          "{gael.guibon,matthieu.labeau,chloe.clavel}@telecom-paris.fr": "data."
        },
        {
          "{gael.guibon,matthieu.labeau,chloe.clavel}@telecom-paris.fr": ""
        },
        {
          "{gael.guibon,matthieu.labeau,chloe.clavel}@telecom-paris.fr": "In this work, we focus on data from a live chat"
        },
        {
          "{gael.guibon,matthieu.labeau,chloe.clavel}@telecom-paris.fr": ""
        },
        {
          "{gael.guibon,matthieu.labeau,chloe.clavel}@telecom-paris.fr": "support in which we want to detect emotions and"
        },
        {
          "{gael.guibon,matthieu.labeau,chloe.clavel}@telecom-paris.fr": ""
        },
        {
          "{gael.guibon,matthieu.labeau,chloe.clavel}@telecom-paris.fr": "their evolution in the conversational ﬂow. This set-"
        },
        {
          "{gael.guibon,matthieu.labeau,chloe.clavel}@telecom-paris.fr": ""
        },
        {
          "{gael.guibon,matthieu.labeau,chloe.clavel}@telecom-paris.fr": "ting corresponds to a human dyadic conversation,"
        },
        {
          "{gael.guibon,matthieu.labeau,chloe.clavel}@telecom-paris.fr": ""
        },
        {
          "{gael.guibon,matthieu.labeau,chloe.clavel}@telecom-paris.fr": "albeit with a speciﬁc business-related objective. We"
        },
        {
          "{gael.guibon,matthieu.labeau,chloe.clavel}@telecom-paris.fr": "make the hypothesis that the emotion ﬂows of the"
        },
        {
          "{gael.guibon,matthieu.labeau,chloe.clavel}@telecom-paris.fr": "visitor and the operator will bring information on"
        },
        {
          "{gael.guibon,matthieu.labeau,chloe.clavel}@telecom-paris.fr": ""
        },
        {
          "{gael.guibon,matthieu.labeau,chloe.clavel}@telecom-paris.fr": "the quality of the service and help operators better"
        },
        {
          "{gael.guibon,matthieu.labeau,chloe.clavel}@telecom-paris.fr": ""
        },
        {
          "{gael.guibon,matthieu.labeau,chloe.clavel}@telecom-paris.fr": "assist customers. This hypothesis is close to rel-"
        },
        {
          "{gael.guibon,matthieu.labeau,chloe.clavel}@telecom-paris.fr": ""
        },
        {
          "{gael.guibon,matthieu.labeau,chloe.clavel}@telecom-paris.fr": "evant studies on the importance of emotions and"
        },
        {
          "{gael.guibon,matthieu.labeau,chloe.clavel}@telecom-paris.fr": ""
        },
        {
          "{gael.guibon,matthieu.labeau,chloe.clavel}@telecom-paris.fr": "empathy in dyadic call center conversations (Alam,"
        },
        {
          "{gael.guibon,matthieu.labeau,chloe.clavel}@telecom-paris.fr": ""
        },
        {
          "{gael.guibon,matthieu.labeau,chloe.clavel}@telecom-paris.fr": "2017; Alam et al., 2018). This speciﬁc setting leads"
        },
        {
          "{gael.guibon,matthieu.labeau,chloe.clavel}@telecom-paris.fr": ""
        },
        {
          "{gael.guibon,matthieu.labeau,chloe.clavel}@telecom-paris.fr": "to multiple challenges:\nindeed,\nit\nis difﬁcult and"
        },
        {
          "{gael.guibon,matthieu.labeau,chloe.clavel}@telecom-paris.fr": "costly to label this kind of data — and even then,"
        },
        {
          "{gael.guibon,matthieu.labeau,chloe.clavel}@telecom-paris.fr": ""
        },
        {
          "{gael.guibon,matthieu.labeau,chloe.clavel}@telecom-paris.fr": "these exchanges are very sparse in emotions, most"
        },
        {
          "{gael.guibon,matthieu.labeau,chloe.clavel}@telecom-paris.fr": ""
        },
        {
          "{gael.guibon,matthieu.labeau,chloe.clavel}@telecom-paris.fr": "of the labels associated with utterances being neu-"
        },
        {
          "{gael.guibon,matthieu.labeau,chloe.clavel}@telecom-paris.fr": ""
        },
        {
          "{gael.guibon,matthieu.labeau,chloe.clavel}@telecom-paris.fr": "tral. To maximize data efﬁciency, we use Few-Shot"
        },
        {
          "{gael.guibon,matthieu.labeau,chloe.clavel}@telecom-paris.fr": ""
        },
        {
          "{gael.guibon,matthieu.labeau,chloe.clavel}@telecom-paris.fr": "Learning (FSL), and adapt a popular approach to"
        },
        {
          "{gael.guibon,matthieu.labeau,chloe.clavel}@telecom-paris.fr": ""
        },
        {
          "{gael.guibon,matthieu.labeau,chloe.clavel}@telecom-paris.fr": "our highly unbalanced data. By setting up this ap-"
        },
        {
          "{gael.guibon,matthieu.labeau,chloe.clavel}@telecom-paris.fr": ""
        },
        {
          "{gael.guibon,matthieu.labeau,chloe.clavel}@telecom-paris.fr": "proach in an episodic fashion (Ravi and Larochelle,"
        },
        {
          "{gael.guibon,matthieu.labeau,chloe.clavel}@telecom-paris.fr": "2016), we join studies on ERC and studies on FSL"
        },
        {
          "{gael.guibon,matthieu.labeau,chloe.clavel}@telecom-paris.fr": "to tackle this industrial use-case."
        },
        {
          "{gael.guibon,matthieu.labeau,chloe.clavel}@telecom-paris.fr": ""
        },
        {
          "{gael.guibon,matthieu.labeau,chloe.clavel}@telecom-paris.fr": "We contribute by proposing a variant\nto Proto-"
        },
        {
          "{gael.guibon,matthieu.labeau,chloe.clavel}@telecom-paris.fr": "typical Networks (Snell et al., 2017) dedicated to"
        },
        {
          "{gael.guibon,matthieu.labeau,chloe.clavel}@telecom-paris.fr": "ERC on data produced by company services, fram-"
        },
        {
          "{gael.guibon,matthieu.labeau,chloe.clavel}@telecom-paris.fr": "ing it as a sequence labeling task. We modify the"
        },
        {
          "{gael.guibon,matthieu.labeau,chloe.clavel}@telecom-paris.fr": "original model by allowing it to consider the whole"
        },
        {
          "{gael.guibon,matthieu.labeau,chloe.clavel}@telecom-paris.fr": "conversational context when making predictions,"
        },
        {
          "{gael.guibon,matthieu.labeau,chloe.clavel}@telecom-paris.fr": "through a sequential context encoder and the use"
        },
        {
          "{gael.guibon,matthieu.labeau,chloe.clavel}@telecom-paris.fr": "of Conditional Random Fields (CRF) on top of the"
        },
        {
          "{gael.guibon,matthieu.labeau,chloe.clavel}@telecom-paris.fr": "model. We test our method on two datasets, in two"
        },
        {
          "{gael.guibon,matthieu.labeau,chloe.clavel}@telecom-paris.fr": "different\nlanguages. The ﬁrst one, made of daily"
        },
        {
          "{gael.guibon,matthieu.labeau,chloe.clavel}@telecom-paris.fr": "conversations in English, allows us to compare our-"
        },
        {
          "{gael.guibon,matthieu.labeau,chloe.clavel}@telecom-paris.fr": "selves to previous methods, while the second one,"
        },
        {
          "{gael.guibon,matthieu.labeau,chloe.clavel}@telecom-paris.fr": "made of private data from a live chat customer ser-"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Seq is available at https://github.com/": "gguibon/ProtoSeq.",
          "proach in an episodic fashion (Ravi and Larochelle,": "2016), we join studies on ERC and studies on FSL"
        },
        {
          "Seq is available at https://github.com/": "",
          "proach in an episodic fashion (Ravi and Larochelle,": "to tackle this industrial use-case."
        },
        {
          "Seq is available at https://github.com/": "1\nIntroduction",
          "proach in an episodic fashion (Ravi and Larochelle,": ""
        },
        {
          "Seq is available at https://github.com/": "",
          "proach in an episodic fashion (Ravi and Larochelle,": "We contribute by proposing a variant\nto Proto-"
        },
        {
          "Seq is available at https://github.com/": "There has been a recent surge in research focusing",
          "proach in an episodic fashion (Ravi and Larochelle,": "typical Networks (Snell et al., 2017) dedicated to"
        },
        {
          "Seq is available at https://github.com/": "on analyzing dyadic human to human interactions.",
          "proach in an episodic fashion (Ravi and Larochelle,": "ERC on data produced by company services, fram-"
        },
        {
          "Seq is available at https://github.com/": "Many of these studies (Poria et al., 2017; Zadeh",
          "proach in an episodic fashion (Ravi and Larochelle,": "ing it as a sequence labeling task. We modify the"
        },
        {
          "Seq is available at https://github.com/": "et al., 2018a,b; Majumder et al., 2019) focus on",
          "proach in an episodic fashion (Ravi and Larochelle,": "original model by allowing it to consider the whole"
        },
        {
          "Seq is available at https://github.com/": "emotion recognition in conversations (ERC) taking",
          "proach in an episodic fashion (Ravi and Larochelle,": "conversational context when making predictions,"
        },
        {
          "Seq is available at https://github.com/": "into account multiple data modalities. Moreover,",
          "proach in an episodic fashion (Ravi and Larochelle,": "through a sequential context encoder and the use"
        },
        {
          "Seq is available at https://github.com/": "most of the progress made in ERC has been done",
          "proach in an episodic fashion (Ravi and Larochelle,": "of Conditional Random Fields (CRF) on top of the"
        },
        {
          "Seq is available at https://github.com/": "without factoring in constraints corresponding to",
          "proach in an episodic fashion (Ravi and Larochelle,": "model. We test our method on two datasets, in two"
        },
        {
          "Seq is available at https://github.com/": "speciﬁc but prominent industrial applications, like",
          "proach in an episodic fashion (Ravi and Larochelle,": "different\nlanguages. The ﬁrst one, made of daily"
        },
        {
          "Seq is available at https://github.com/": "customer service. This is partly due to studies fo-",
          "proach in an episodic fashion (Ravi and Larochelle,": "conversations in English, allows us to compare our-"
        },
        {
          "Seq is available at https://github.com/": "cusing on using artiﬁcial datasets (Li et al., 2017;",
          "proach in an episodic fashion (Ravi and Larochelle,": "selves to previous methods, while the second one,"
        },
        {
          "Seq is available at https://github.com/": "Busso et al., 2008) made of mock-up conversations",
          "proach in an episodic fashion (Ravi and Larochelle,": "made of private data from a live chat customer ser-"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "vice, allows us to conduct a performance analysis": "in our\ntarget setting. We also present\nthe latter",
          "ing faster, leading to a lower dependency on data": "quantity. It is mainly set up through episodic com-"
        },
        {
          "vice, allows us to conduct a performance analysis": "dataset, along with its annotation process.",
          "ing faster, leading to a lower dependency on data": "position (Ravi and Larochelle, 2016) which recre-"
        },
        {
          "vice, allows us to conduct a performance analysis": "This paper is organized as follows. First, we sum",
          "ing faster, leading to a lower dependency on data": "ates the few-shot learning setting by working with"
        },
        {
          "vice, allows us to conduct a performance analysis": "up the related work on textual ERC and FSL in con-",
          "ing faster, leading to a lower dependency on data": "small training episodes. Several learning methods"
        },
        {
          "vice, allows us to conduct a performance analysis": "versations (Section 2). Then we present the datasets",
          "ing faster, leading to a lower dependency on data": "are based on metric-learning: Siamese Networks,"
        },
        {
          "vice, allows us to conduct a performance analysis": "along with the emotional annotation scheme and",
          "ing faster, leading to a lower dependency on data": "which share some weights, are used to learn a met-"
        },
        {
          "vice, allows us to conduct a performance analysis": "the annotation campaign set up for the customer",
          "ing faster, leading to a lower dependency on data": "ric between examples (Koch et al., 2015). Match-"
        },
        {
          "vice, allows us to conduct a performance analysis": "service live chats dataset (Section 3). We continue",
          "ing faster, leading to a lower dependency on data": "ing Networks (Vinyals et al., 2017) use the train-"
        },
        {
          "vice, allows us to conduct a performance analysis": "by thoroughly presenting the Sequential Prototyp-",
          "ing faster, leading to a lower dependency on data": "ing examples to ﬁnd the weighted nearest neigh-"
        },
        {
          "vice, allows us to conduct a performance analysis": "ical Networks (Section 4) before looking at\nthe",
          "ing faster, leading to a lower dependency on data": "bors (Vinyals et al., 2017). Prototypical Networks"
        },
        {
          "vice, allows us to conduct a performance analysis": "achieved results on both datasets (Section 5). Fi-",
          "ing faster, leading to a lower dependency on data": "Snell et al. (2017) consider averaged class repre-"
        },
        {
          "vice, allows us to conduct a performance analysis": "nally, we present the limitations of such a system",
          "ing faster, leading to a lower dependency on data": "sentations from the training examples and a cosine"
        },
        {
          "vice, allows us to conduct a performance analysis": "(Section 6) and conclude (Section 7).",
          "ing faster, leading to a lower dependency on data": "distance to compare the elements to these class"
        },
        {
          "vice, allows us to conduct a performance analysis": "",
          "ing faster, leading to a lower dependency on data": "representations. Relation networks replace the Eu-"
        },
        {
          "vice, allows us to conduct a performance analysis": "2\nRelated Work",
          "ing faster, leading to a lower dependency on data": ""
        },
        {
          "vice, allows us to conduct a performance analysis": "",
          "ing faster, leading to a lower dependency on data": "clidean by the deep neural network which aims at"
        },
        {
          "vice, allows us to conduct a performance analysis": "",
          "ing faster, leading to a lower dependency on data": "training a distance metric (Sung et al., 2018)."
        },
        {
          "vice, allows us to conduct a performance analysis": "Emotion Recognition in Conversations\nIn re-",
          "ing faster, leading to a lower dependency on data": ""
        },
        {
          "vice, allows us to conduct a performance analysis": "",
          "ing faster, leading to a lower dependency on data": "In this work, we consider approaches based on Pro-"
        },
        {
          "vice, allows us to conduct a performance analysis": "cent years, the widening scope of emotion detection",
          "ing faster, leading to a lower dependency on data": ""
        },
        {
          "vice, allows us to conduct a performance analysis": "",
          "ing faster, leading to a lower dependency on data": "totypical Networks. As Al-Shedivat et al. (2021)"
        },
        {
          "vice, allows us to conduct a performance analysis": "tasks led to the rise of another sub-topic: detect-",
          "ing faster, leading to a lower dependency on data": ""
        },
        {
          "vice, allows us to conduct a performance analysis": "",
          "ing faster, leading to a lower dependency on data": "recently showed it, such approaches are the most"
        },
        {
          "vice, allows us to conduct a performance analysis": "ing emotions in conversations. This research topic,",
          "ing faster, leading to a lower dependency on data": ""
        },
        {
          "vice, allows us to conduct a performance analysis": "",
          "ing faster, leading to a lower dependency on data": "efﬁcient when working with a\nlow amount of"
        },
        {
          "vice, allows us to conduct a performance analysis": "commonly referred to as ERC, gained popularity",
          "ing faster, leading to a lower dependency on data": ""
        },
        {
          "vice, allows us to conduct a performance analysis": "",
          "ing faster, leading to a lower dependency on data": "training samples. Many variants have been pro-"
        },
        {
          "vice, allows us to conduct a performance analysis": "when Poria et al. (2017) ﬁrst applied recurrent neu-",
          "ing faster, leading to a lower dependency on data": ""
        },
        {
          "vice, allows us to conduct a performance analysis": "",
          "ing faster, leading to a lower dependency on data": "posed, on different\ntasks and topics such as rela-"
        },
        {
          "vice, allows us to conduct a performance analysis": "ral networks (RNN) (Jordan, 1997) to multi-modal",
          "ing faster, leading to a lower dependency on data": ""
        },
        {
          "vice, allows us to conduct a performance analysis": "",
          "ing faster, leading to a lower dependency on data": "tion classiﬁcation in text\n(Gao et al., 2019; Hui"
        },
        {
          "vice, allows us to conduct a performance analysis": "emotion recognition in conversations. This led to",
          "ing faster, leading to a lower dependency on data": ""
        },
        {
          "vice, allows us to conduct a performance analysis": "",
          "ing faster, leading to a lower dependency on data": "et al., 2020; Ren et al., 2020), sentiment classiﬁ-"
        },
        {
          "vice, allows us to conduct a performance analysis": "many improvements (Zadeh et al., 2018a,b; Haz-",
          "ing faster, leading to a lower dependency on data": ""
        },
        {
          "vice, allows us to conduct a performance analysis": "",
          "ing faster, leading to a lower dependency on data": "cation in Amazon comments (Bao et al., 2020),"
        },
        {
          "vice, allows us to conduct a performance analysis": "arika et al., 2018; Majumder et al., 2019). Among",
          "ing faster, leading to a lower dependency on data": ""
        },
        {
          "vice, allows us to conduct a performance analysis": "",
          "ing faster, leading to a lower dependency on data": "named entity recognition (Fritzler et al., 2019; Hou"
        },
        {
          "vice, allows us to conduct a performance analysis": "those, Majumder et al. (2019) used 3 Gated Recur-",
          "ing faster, leading to a lower dependency on data": ""
        },
        {
          "vice, allows us to conduct a performance analysis": "",
          "ing faster, leading to a lower dependency on data": "et al., 2020; Perl et al., 2020; Safranchik et al.,"
        },
        {
          "vice, allows us to conduct a performance analysis": "rent Units (GRU) (Cho et al., 2014) units, one for",
          "ing faster, leading to a lower dependency on data": ""
        },
        {
          "vice, allows us to conduct a performance analysis": "",
          "ing faster, leading to a lower dependency on data": "2020), or even speech classiﬁcation in conversation"
        },
        {
          "vice, allows us to conduct a performance analysis": "each context representation target (speaker, utter-",
          "ing faster, leading to a lower dependency on data": ""
        },
        {
          "vice, allows us to conduct a performance analysis": "",
          "ing faster, leading to a lower dependency on data": "(Koluguri et al., 2020).\nThis surge of interest on"
        },
        {
          "vice, allows us to conduct a performance analysis": "ance, emotion).\nStudies on ERC applied to text",
          "ing faster, leading to a lower dependency on data": ""
        },
        {
          "vice, allows us to conduct a performance analysis": "",
          "ing faster, leading to a lower dependency on data": "applying few-shot learning to these topics can be at-"
        },
        {
          "vice, allows us to conduct a performance analysis": "followed, mainly built on an artiﬁcial conversa-",
          "ing faster, leading to a lower dependency on data": ""
        },
        {
          "vice, allows us to conduct a performance analysis": "",
          "ing faster, leading to a lower dependency on data": "tributed to speciﬁc datasets, such as Few-Rel (Han"
        },
        {
          "vice, allows us to conduct a performance analysis": "tion dataset named DailyDialog (Li et al., 2017).",
          "ing faster, leading to a lower dependency on data": ""
        },
        {
          "vice, allows us to conduct a performance analysis": "",
          "ing faster, leading to a lower dependency on data": "et al., 2018) for relation classiﬁcation. While ERC"
        },
        {
          "vice, allows us to conduct a performance analysis": "(Zhong et al., 2019)\nincorporated a knowledge",
          "ing faster, leading to a lower dependency on data": ""
        },
        {
          "vice, allows us to conduct a performance analysis": "",
          "ing faster, leading to a lower dependency on data": "is mainly considered in a fully supervised learning"
        },
        {
          "vice, allows us to conduct a performance analysis": "base into the network using context-aware attention",
          "ing faster, leading to a lower dependency on data": ""
        },
        {
          "vice, allows us to conduct a performance analysis": "",
          "ing faster, leading to a lower dependency on data": "setting, we intend to view it as a few-shot learning"
        },
        {
          "vice, allows us to conduct a performance analysis": "and hierarchical self-attention using Transformers",
          "ing faster, leading to a lower dependency on data": ""
        },
        {
          "vice, allows us to conduct a performance analysis": "",
          "ing faster, leading to a lower dependency on data": "sequence labeling class. In this paper, we propose"
        },
        {
          "vice, allows us to conduct a performance analysis": "(Vaswani et al., 2017). Ghosal et al. (2019) uses",
          "ing faster, leading to a lower dependency on data": ""
        },
        {
          "vice, allows us to conduct a performance analysis": "",
          "ing faster, leading to a lower dependency on data": "the ﬁrst few-shot learning approach on ERC using"
        },
        {
          "vice, allows us to conduct a performance analysis": "graph neural networks to deal with context propaga-",
          "ing faster, leading to a lower dependency on data": ""
        },
        {
          "vice, allows us to conduct a performance analysis": "",
          "ing faster, leading to a lower dependency on data": "sequence labeling through adapting Prototypical"
        },
        {
          "vice, allows us to conduct a performance analysis": "tion limitations. These approaches in ERC consider",
          "ing faster, leading to a lower dependency on data": ""
        },
        {
          "vice, allows us to conduct a performance analysis": "",
          "ing faster, leading to a lower dependency on data": "Networks. We compare our method to the origi-"
        },
        {
          "vice, allows us to conduct a performance analysis": "the conversational context surrounding the current",
          "ing faster, leading to a lower dependency on data": ""
        },
        {
          "vice, allows us to conduct a performance analysis": "",
          "ing faster, leading to a lower dependency on data": "nal Prototypical Networks (Snell et al., 2017) and"
        },
        {
          "vice, allows us to conduct a performance analysis": "utterance; on the other hand, some recent studies",
          "ing faster, leading to a lower dependency on data": ""
        },
        {
          "vice, allows us to conduct a performance analysis": "",
          "ing faster, leading to a lower dependency on data": "to a variant dedicated to named entity recognition"
        },
        {
          "vice, allows us to conduct a performance analysis": "consider it as a sequence and tackled ERC through",
          "ing faster, leading to a lower dependency on data": ""
        },
        {
          "vice, allows us to conduct a performance analysis": "",
          "ing faster, leading to a lower dependency on data": "(Fritzler et al., 2019) that is easily applicable to our"
        },
        {
          "vice, allows us to conduct a performance analysis": "a sequence labeling task (Wang et al., 2020). We",
          "ing faster, leading to a lower dependency on data": ""
        },
        {
          "vice, allows us to conduct a performance analysis": "",
          "ing faster, leading to a lower dependency on data": "task."
        },
        {
          "vice, allows us to conduct a performance analysis": "follow this last approach and consider the ERC task",
          "ing faster, leading to a lower dependency on data": ""
        },
        {
          "vice, allows us to conduct a performance analysis": "as a sequence labeling task. However, these super-",
          "ing faster, leading to a lower dependency on data": ""
        },
        {
          "vice, allows us to conduct a performance analysis": "",
          "ing faster, leading to a lower dependency on data": "3\nData"
        },
        {
          "vice, allows us to conduct a performance analysis": "vised approaches are difﬁcult to use, as it is hard",
          "ing faster, leading to a lower dependency on data": ""
        },
        {
          "vice, allows us to conduct a performance analysis": "to ﬁnd a sufﬁcient amount of conversations labeled",
          "ing faster, leading to a lower dependency on data": ""
        },
        {
          "vice, allows us to conduct a performance analysis": "",
          "ing faster, leading to a lower dependency on data": "To be able to both study the behavior of our model"
        },
        {
          "vice, allows us to conduct a performance analysis": "with emotions. Hence, in this paper, we approach",
          "ing faster, leading to a lower dependency on data": ""
        },
        {
          "vice, allows us to conduct a performance analysis": "",
          "ing faster, leading to a lower dependency on data": "in its targeted industrial use-case, and allow perfor-"
        },
        {
          "vice, allows us to conduct a performance analysis": "ERC as a few-shot learning problem.",
          "ing faster, leading to a lower dependency on data": ""
        },
        {
          "vice, allows us to conduct a performance analysis": "",
          "ing faster, leading to a lower dependency on data": "mance comparison with baselines, we will work"
        },
        {
          "vice, allows us to conduct a performance analysis": "Few-Shot Learning\nFSL (Miller et al., 2000;",
          "ing faster, leading to a lower dependency on data": "with two very different corpora: our proprietary"
        },
        {
          "vice, allows us to conduct a performance analysis": "Fei-Fei et al., 2006; Lake, 2015)\nis\nsuitable to",
          "ing faster, leading to a lower dependency on data": "live chat customer service dataset, and DailyDia-"
        },
        {
          "vice, allows us to conduct a performance analysis": "tackle this data limitation.\nIt aims at generaliz-",
          "ing faster, leading to a lower dependency on data": "log (Li et al., 2017).\nIn both datasets, messages"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table 1: Statistics for both datasets DailyDialog (DD)",
      "data": [
        {
          "are labeled with emotions while considering the": "context of the conversation. However,\nthey vary",
          "ing that the speaker is changing the topic. Because": "all our messages have a very short time difference"
        },
        {
          "are labeled with emotions while considering the": "considerably in their topics and lexical ﬁelds: ordi-",
          "ing that the speaker is changing the topic. Because": "we prefer not to automatically infer speaker turns"
        },
        {
          "are labeled with emotions while considering the": "nary matters for DailyDialog and railway related",
          "ing that the speaker is changing the topic. Because": "and consider the message as the unit of analysis."
        },
        {
          "are labeled with emotions while considering the": "customer service for the live chats. They also vary",
          "ing that the speaker is changing the topic. Because": "This means the conversation context is a sequence"
        },
        {
          "are labeled with emotions while considering the": "in the assumptions they make about the speakers :",
          "ing that the speaker is changing the topic. Because": "of messages instead of a sequence of speaker turns"
        },
        {
          "are labeled with emotions while considering the": "while the topics discussed in DailyDialog imply a",
          "ing that the speaker is changing the topic. Because": "which could have contained one or more messages"
        },
        {
          "are labeled with emotions while considering the": "sense of proximity, the live chat customer service",
          "ing that the speaker is changing the topic. Because": "artiﬁcially glued together."
        },
        {
          "are labeled with emotions while considering the": "involve complete strangers with pre-existing emo-",
          "ing that the speaker is changing the topic. Because": ""
        },
        {
          "are labeled with emotions while considering the": "",
          "ing that the speaker is changing the topic. Because": "Dataset\nDD\nChat"
        },
        {
          "are labeled with emotions while considering the": "tional states (e.g.\nthe visitor is already stressed due",
          "ing that the speaker is changing the topic. Because": ""
        },
        {
          "are labeled with emotions while considering the": "to a refund issue). Both datasets’ statistics can be",
          "ing that the speaker is changing the topic. Because": "Language\nEnglish\nFrench"
        },
        {
          "are labeled with emotions while considering the": "found in Table 1.",
          "ing that the speaker is changing the topic. Because": "Type\nArtiﬁcial\nCustomer Service"
        },
        {
          "are labeled with emotions while considering the": "",
          "ing that the speaker is changing the topic. Because": "Max Msg/Conv\n35\n84"
        },
        {
          "are labeled with emotions while considering the": "3.1\nDailyDialog",
          "ing that the speaker is changing the topic. Because": ""
        },
        {
          "are labeled with emotions while considering the": "",
          "ing that the speaker is changing the topic. Because": "Avg Msg/Conv\n8\n13"
        },
        {
          "are labeled with emotions while considering the": "DailyDialog is a dyadic conversation dataset\nin",
          "ing that the speaker is changing the topic. Because": "Labels\n7\n11"
        },
        {
          "are labeled with emotions while considering the": "English whose purpose is to represent casual, ev-",
          "ing that the speaker is changing the topic. Because": "Labels for eval\n6\n9"
        },
        {
          "are labeled with emotions while considering the": "eryday interactions between people,\nin order\nto",
          "ing that the speaker is changing the topic. Because": "Nb. Conv.\n13,118\n1,500"
        },
        {
          "are labeled with emotions while considering the": "facilitate training and sharing of dialog systems.",
          "ing that the speaker is changing the topic. Because": ""
        },
        {
          "are labeled with emotions while considering the": "",
          "ing that the speaker is changing the topic. Because": "Table 1: Statistics for both datasets DailyDialog (DD)"
        },
        {
          "are labeled with emotions while considering the": "The exchanges in DailyDialog are artiﬁcial conver-",
          "ing that the speaker is changing the topic. Because": ""
        },
        {
          "are labeled with emotions while considering the": "",
          "ing that the speaker is changing the topic. Because": "and Live Chat Customer Service (chat)."
        },
        {
          "are labeled with emotions while considering the": "sations which are neither dedicated to a speciﬁc",
          "ing that the speaker is changing the topic. Because": ""
        },
        {
          "are labeled with emotions while considering the": "topic nor task-oriented:\nthey mainly deal with rela-",
          "ing that the speaker is changing the topic. Because": ""
        },
        {
          "are labeled with emotions while considering the": "",
          "ing that the speaker is changing the topic. Because": "Two annotators were involved in the process, which"
        },
        {
          "are labeled with emotions while considering the": "tionships, everyday life, and work. Each utterance",
          "ing that the speaker is changing the topic. Because": ""
        },
        {
          "are labeled with emotions while considering the": "",
          "ing that the speaker is changing the topic. Because": "unrolled as follows: ﬁrst, each message is labeled"
        },
        {
          "are labeled with emotions while considering the": "corresponds to a speaker turn, and is labeled with",
          "ing that the speaker is changing the topic. Because": ""
        },
        {
          "are labeled with emotions while considering the": "",
          "ing that the speaker is changing the topic. Because": "with an emotion. Once all the messages in a con-"
        },
        {
          "are labeled with emotions while considering the": "one of 7 labels:\nthe 6 basic emotions (anger, disgust,",
          "ing that the speaker is changing the topic. Because": ""
        },
        {
          "are labeled with emotions while considering the": "",
          "ing that the speaker is changing the topic. Because": "versation have been assigned an emotion label, the"
        },
        {
          "are labeled with emotions while considering the": "fear, joy, sadness, and surprise) and \"no_emotion\"",
          "ing that the speaker is changing the topic. Because": ""
        },
        {
          "are labeled with emotions while considering the": "",
          "ing that the speaker is changing the topic. Because": "conversation is labeled with a visitor satisfaction"
        },
        {
          "are labeled with emotions while considering the": "denoting the absence of one. The \"no_emotion\"",
          "ing that the speaker is changing the topic. Because": ""
        },
        {
          "are labeled with emotions while considering the": "",
          "ing that the speaker is changing the topic. Because": "score (ranging from -3 to 3), and the status of the"
        },
        {
          "are labeled with emotions while considering the": "label represents 80% of the corpus,\nleading to a",
          "ing that the speaker is changing the topic. Because": ""
        },
        {
          "are labeled with emotions while considering the": "",
          "ing that the speaker is changing the topic. Because": "customer request (\"solved\", \"test_required\", \"out"
        },
        {
          "are labeled with emotions while considering the": "very unbalanced dataset with an average length of",
          "ing that the speaker is changing the topic. Because": ""
        },
        {
          "are labeled with emotions while considering the": "",
          "ing that the speaker is changing the topic. Because": "of scope\", or \"aborted\"). After a preliminary study"
        },
        {
          "are labeled with emotions while considering the": "8 messages per conversation and a maximum of",
          "ing that the speaker is changing the topic. Because": ""
        },
        {
          "are labeled with emotions while considering the": "",
          "ing that the speaker is changing the topic. Because": "of the corpus, we identify 10 emotion labels as rel-"
        },
        {
          "are labeled with emotions while considering the": "35 messages. For this dataset, the inter annotator",
          "ing that the speaker is changing the topic. Because": ""
        },
        {
          "are labeled with emotions while considering the": "",
          "ing that the speaker is changing the topic. Because": "evant in this corpus: neutral, surprise, amusement,"
        },
        {
          "are labeled with emotions while considering the": "agreement achieved 78.9%. We choose DailyDia-",
          "ing that the speaker is changing the topic. Because": ""
        },
        {
          "are labeled with emotions while considering the": "",
          "ing that the speaker is changing the topic. Because": "satisfaction1,\nrelief,\nfear-anxiety-stress,\nsadness,"
        },
        {
          "are labeled with emotions while considering the": "log for comparison and reproducibility purposes,",
          "ing that the speaker is changing the topic. Because": ""
        },
        {
          "are labeled with emotions while considering the": "",
          "ing that the speaker is changing the topic. Because": "disappointment, anger, and frustration. Compared"
        },
        {
          "are labeled with emotions while considering the": "as it\nis often used for ERC. In this work, we use",
          "ing that the speaker is changing the topic. Because": ""
        },
        {
          "are labeled with emotions while considering the": "",
          "ing that the speaker is changing the topic. Because": "to (Chowdhury et al., 2016), we consider the satis-"
        },
        {
          "are labeled with emotions while considering the": "the train/val/test splits provided by (Zhong et al.,",
          "ing that the speaker is changing the topic. Because": ""
        },
        {
          "are labeled with emotions while considering the": "",
          "ing that the speaker is changing the topic. Because": "faction at the conversation level and we are more"
        },
        {
          "are labeled with emotions while considering the": "2019).",
          "ing that the speaker is changing the topic. Because": ""
        },
        {
          "are labeled with emotions while considering the": "",
          "ing that the speaker is changing the topic. Because": "precise with not only positive, neutral, and negative"
        },
        {
          "are labeled with emotions while considering the": "3.2\nLive chat customer service",
          "ing that the speaker is changing the topic. Because": "levels, but also with 4 additional intermediate lev-"
        },
        {
          "are labeled with emotions while considering the": "",
          "ing that the speaker is changing the topic. Because": "els (from -3 to +3 included). We have also a higher"
        },
        {
          "are labeled with emotions while considering the": "Our primary objective is to detect emotions in con-",
          "ing that the speaker is changing the topic. Because": ""
        },
        {
          "are labeled with emotions while considering the": "",
          "ing that the speaker is changing the topic. Because": "number of emotions, with 10 emotions instead of"
        },
        {
          "are labeled with emotions while considering the": "versations from a customer service live chat involv-",
          "ing that the speaker is changing the topic. Because": ""
        },
        {
          "are labeled with emotions while considering the": "",
          "ing that the speaker is changing the topic. Because": "4, with more precise emotions such as relief for"
        },
        {
          "are labeled with emotions while considering the": "ing a visitor (i.e.\nthe customer looking for help)",
          "ing that the speaker is changing the topic. Because": ""
        },
        {
          "are labeled with emotions while considering the": "",
          "ing that the speaker is changing the topic. Because": "instance. In our customer service interface, some"
        },
        {
          "are labeled with emotions while considering the": "and an operator (i.e.\nan employee being there to",
          "ing that the speaker is changing the topic. Because": ""
        },
        {
          "are labeled with emotions while considering the": "",
          "ing that the speaker is changing the topic. Because": "alerts are automatically prompted for speciﬁc ac-"
        },
        {
          "are labeled with emotions while considering the": "assist the visitor and better satisfy him). The corpus",
          "ing that the speaker is changing the topic. Because": ""
        },
        {
          "are labeled with emotions while considering the": "",
          "ing that the speaker is changing the topic. Because": "tions such as “user x left the chat” or “operator sent"
        },
        {
          "are labeled with emotions while considering the": "is written in French and is made of 5,000 conver-",
          "ing that the speaker is changing the topic. Because": ""
        },
        {
          "are labeled with emotions while considering the": "",
          "ing that the speaker is changing the topic. Because": "a link”. We call these “alerts”, and they are labeled"
        },
        {
          "are labeled with emotions while considering the": "sations from which we annotate a subset of 1,500",
          "ing that the speaker is changing the topic. Because": ""
        },
        {
          "are labeled with emotions while considering the": "",
          "ing that the speaker is changing the topic. Because": "as “no_emotion”. The “neutral” label means that"
        },
        {
          "are labeled with emotions while considering the": "conversations,\nleading to a total of 20,754 mes-",
          "ing that the speaker is changing the topic. Because": ""
        },
        {
          "are labeled with emotions while considering the": "",
          "ing that the speaker is changing the topic. Because": "the emotional content of the message, written by"
        },
        {
          "are labeled with emotions while considering the": "sages. The average message length is higher than",
          "ing that the speaker is changing the topic. Because": ""
        },
        {
          "are labeled with emotions while considering the": "",
          "ing that the speaker is changing the topic. Because": "a human, has been considered as neutral by the"
        },
        {
          "are labeled with emotions while considering the": "DailyDialog, with 15.14 messages per conversa-",
          "ing that the speaker is changing the topic. Because": ""
        },
        {
          "are labeled with emotions while considering the": "",
          "ing that the speaker is changing the topic. Because": "annotator. Figure 1 illustrates the distribution of"
        },
        {
          "are labeled with emotions while considering the": "tion. We do not have a way to identify real speaker",
          "ing that the speaker is changing the topic. Because": ""
        },
        {
          "are labeled with emotions while considering the": "",
          "ing that the speaker is changing the topic. Because": "emotion labels in the Live Chat Customer Service"
        },
        {
          "are labeled with emotions while considering the": "turns. Indeed, a speaker turn is not necessarily the",
          "ing that the speaker is changing the topic. Because": ""
        },
        {
          "are labeled with emotions while considering the": "",
          "ing that the speaker is changing the topic. Because": "1It is interesting to notice here that in the current applica-"
        },
        {
          "are labeled with emotions while considering the": "sequence of contiguous segments corresponding to",
          "ing that the speaker is changing the topic. Because": ""
        },
        {
          "are labeled with emotions while considering the": "",
          "ing that the speaker is changing the topic. Because": "tion setting, \"joy\" label has been replaced by \"satisfaction\","
        },
        {
          "are labeled with emotions while considering the": "a same speaker because there could be a time delay",
          "ing that the speaker is changing the topic. Because": ""
        },
        {
          "are labeled with emotions while considering the": "",
          "ing that the speaker is changing the topic. Because": "because it is more suited to the customer relationship context"
        },
        {
          "are labeled with emotions while considering the": "between two messages of a same speaker, indicat-",
          "ing that the speaker is changing the topic. Because": "(Danesi and Clavel, 2010)."
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "dataset. We can see that\nthe neutral\nlabel\nis the": "most frequent by a large margin.",
          "Emotion\nκ-score": ""
        },
        {
          "dataset. We can see that\nthe neutral\nlabel\nis the": "",
          "Emotion\nκ-score": "Amusement\n0.1115"
        },
        {
          "dataset. We can see that\nthe neutral\nlabel\nis the": "The Cohen’s κ scores obtained on the 3 label",
          "Emotion\nκ-score": "Anger\n0.1608"
        },
        {
          "dataset. We can see that\nthe neutral\nlabel\nis the": "types correspond to substantial agreement at\nthe",
          "Emotion\nκ-score": "Disappointment\n0.1609"
        },
        {
          "dataset. We can see that\nthe neutral\nlabel\nis the": "message level and moderate agreement at the con-",
          "Emotion\nκ-score": "Frustration\n0.1193"
        },
        {
          "dataset. We can see that\nthe neutral\nlabel\nis the": "versation level (Landis and Koch, 1977). κ-score is",
          "Emotion\nκ-score": "Neutral\n0.3187"
        },
        {
          "dataset. We can see that\nthe neutral\nlabel\nis the": "given for 3 label types: 1) the emotions at the mes-",
          "Emotion\nκ-score": "Fear\n0.1111"
        },
        {
          "dataset. We can see that\nthe neutral\nlabel\nis the": "sage level (κ = 0.65); 2) the visitor’s satisfaction",
          "Emotion\nκ-score": "Satisfaction\n0.2068"
        },
        {
          "dataset. We can see that\nthe neutral\nlabel\nis the": "at the conversation level (κ = 0.45); and 3) the re-",
          "Emotion\nκ-score": "Relief\n0.1429"
        },
        {
          "dataset. We can see that\nthe neutral\nlabel\nis the": "quest’s status at the conversation level (κ = 0.46).",
          "Emotion\nκ-score": "Surprise\n0.1885"
        },
        {
          "dataset. We can see that\nthe neutral\nlabel\nis the": "Similarly to DailyDialog,\nthe \"neutral\" label rep-",
          "Emotion\nκ-score": "Sadness\n0.2860"
        },
        {
          "dataset. We can see that\nthe neutral\nlabel\nis the": "resents 81.5% of the corpus, resulting in another",
          "Emotion\nκ-score": "Global\n0.6499"
        },
        {
          "dataset. We can see that\nthe neutral\nlabel\nis the": "very unbalanced dataset\nin terms of emotions, as",
          "Emotion\nκ-score": "Global w/o Neutral and no_emotion\n0.3885"
        },
        {
          "dataset. We can see that\nthe neutral\nlabel\nis the": "rendered obvious by Figure 1. Excluding this label",
          "Emotion\nκ-score": ""
        },
        {
          "dataset. We can see that\nthe neutral\nlabel\nis the": "gives a slightly more balanced label set, as the sat-",
          "Emotion\nκ-score": "Table 2: By-category agreement scores for emotions in"
        },
        {
          "dataset. We can see that\nthe neutral\nlabel\nis the": "",
          "Emotion\nκ-score": "Live Chat Customer Service"
        },
        {
          "dataset. We can see that\nthe neutral\nlabel\nis the": "isfaction represents 44.9% of the other emotions,",
          "Emotion\nκ-score": ""
        },
        {
          "dataset. We can see that\nthe neutral\nlabel\nis the": "and the \"frustration\" 20.8%.",
          "Emotion\nκ-score": ""
        },
        {
          "dataset. We can see that\nthe neutral\nlabel\nis the": "",
          "Emotion\nκ-score": "4.1\nEpisodic learning"
        },
        {
          "dataset. We can see that\nthe neutral\nlabel\nis the": "",
          "Emotion\nκ-score": "We use the episodic approach (Ravi and Larochelle,"
        },
        {
          "dataset. We can see that\nthe neutral\nlabel\nis the": "",
          "Emotion\nκ-score": "2016), which simulates a context where only a few"
        },
        {
          "dataset. We can see that\nthe neutral\nlabel\nis the": "",
          "Emotion\nκ-score": "examples per class are available during training and"
        },
        {
          "dataset. We can see that\nthe neutral\nlabel\nis the": "",
          "Emotion\nκ-score": "the model must adapt during testing. This approach"
        },
        {
          "dataset. We can see that\nthe neutral\nlabel\nis the": "",
          "Emotion\nκ-score": "perfectly ﬁts into our need for FSL. The episodic"
        },
        {
          "dataset. We can see that\nthe neutral\nlabel\nis the": "",
          "Emotion\nκ-score": "composition is deﬁned by setting the number of"
        },
        {
          "dataset. We can see that\nthe neutral\nlabel\nis the": "",
          "Emotion\nκ-score": "the number of examples per\nclasses (ways) NC,"
        },
        {
          "dataset. We can see that\nthe neutral\nlabel\nis the": "",
          "Emotion\nκ-score": "class NS (shots) and the number of elements to"
        },
        {
          "dataset. We can see that\nthe neutral\nlabel\nis the": "",
          "Emotion\nκ-score": "In our experiments with Dai-\nlabel NQ (queries)."
        },
        {
          "dataset. We can see that\nthe neutral\nlabel\nis the": "Figure 1: Emotion Distribution in Live Chat Customer",
          "Emotion\nκ-score": "lyDialog,\nthe task is 5-shot 7-way 10-query, and"
        },
        {
          "dataset. We can see that\nthe neutral\nlabel\nis the": "Service",
          "Emotion\nκ-score": ""
        },
        {
          "dataset. We can see that\nthe neutral\nlabel\nis the": "",
          "Emotion\nκ-score": "when using our customer service chats,\nthe num-"
        },
        {
          "dataset. We can see that\nthe neutral\nlabel\nis the": "",
          "Emotion\nκ-score": "ber of classes changes, making it a 5-shot 11-way"
        },
        {
          "dataset. We can see that\nthe neutral\nlabel\nis the": "",
          "Emotion\nκ-score": "10-query.\nIn the context of sequential ERC,\nthis"
        },
        {
          "dataset. We can see that\nthe neutral\nlabel\nis the": "To tackle our hypothesis that\nthe conversational",
          "Emotion\nκ-score": ""
        },
        {
          "dataset. We can see that\nthe neutral\nlabel\nis the": "",
          "Emotion\nκ-score": "means that for each episode we train the model on"
        },
        {
          "dataset. We can see that\nthe neutral\nlabel\nis the": "emotion ﬂow can deﬁne the overall visitor satis-",
          "Emotion\nκ-score": ""
        },
        {
          "dataset. We can see that\nthe neutral\nlabel\nis the": "",
          "Emotion\nκ-score": "5 conversations (i.e. sequences to label) per emo-"
        },
        {
          "dataset. We can see that\nthe neutral\nlabel\nis the": "faction, we calculate the Pearson correlation be-",
          "Emotion\nκ-score": ""
        },
        {
          "dataset. We can see that\nthe neutral\nlabel\nis the": "",
          "Emotion\nκ-score": "tion and apply it to 10 conversations per emotion."
        },
        {
          "dataset. We can see that\nthe neutral\nlabel\nis the": "tween the emotions at\nthe message level and the",
          "Emotion\nκ-score": ""
        },
        {
          "dataset. We can see that\nthe neutral\nlabel\nis the": "",
          "Emotion\nκ-score": "We identify a sequence as belonging to the target"
        },
        {
          "dataset. We can see that\nthe neutral\nlabel\nis the": "global satisfaction of\nthe visitor at\nthe conversa-",
          "Emotion\nκ-score": ""
        },
        {
          "dataset. We can see that\nthe neutral\nlabel\nis the": "",
          "Emotion\nκ-score": "class set if at least one message is labeled with the"
        },
        {
          "dataset. We can see that\nthe neutral\nlabel\nis the": "tion level. These scores show the more extreme",
          "Emotion\nκ-score": ""
        },
        {
          "dataset. We can see that\nthe neutral\nlabel\nis the": "",
          "Emotion\nκ-score": "target class in the sequence. This means that\nthe"
        },
        {
          "dataset. We can see that\nthe neutral\nlabel\nis the": "the emotion,\nthe greater the correlation with the",
          "Emotion\nκ-score": ""
        },
        {
          "dataset. We can see that\nthe neutral\nlabel\nis the": "",
          "Emotion\nκ-score": "number of example messages in each support set"
        },
        {
          "dataset. We can see that\nthe neutral\nlabel\nis the": "satisfaction score is2.",
          "Emotion\nκ-score": ""
        },
        {
          "dataset. We can see that\nthe neutral\nlabel\nis the": "",
          "Emotion\nκ-score": "Sk of class k can vary (with a minimum of NS"
        },
        {
          "dataset. We can see that\nthe neutral\nlabel\nis the": "",
          "Emotion\nκ-score": "elements), while the number of sequences is ﬁxed."
        },
        {
          "dataset. We can see that\nthe neutral\nlabel\nis the": "4\nMethodology",
          "Emotion\nκ-score": ""
        },
        {
          "dataset. We can see that\nthe neutral\nlabel\nis the": "",
          "Emotion\nκ-score": "4.2\nProtoSeq: Prototypical Networks for"
        },
        {
          "dataset. We can see that\nthe neutral\nlabel\nis the": "Formally, our dataset D is comprised of conversa-",
          "Emotion\nκ-score": ""
        },
        {
          "dataset. We can see that\nthe neutral\nlabel\nis the": "",
          "Emotion\nκ-score": "Emotion Sequence Labeling"
        },
        {
          "dataset. We can see that\nthe neutral\nlabel\nis the": "tions (C1, C2, . . . , C|D|), which are in turn made",
          "Emotion\nκ-score": ""
        },
        {
          "dataset. We can see that\nthe neutral\nlabel\nis the": "",
          "Emotion\nκ-score": "In order to apply FSL to ERC, we choose to base"
        },
        {
          "dataset. We can see that\nthe neutral\nlabel\nis the": "of utterances: Ci = (u1, u2, . . . , u|Ci|). To each",
          "Emotion\nκ-score": ""
        },
        {
          "dataset. We can see that\nthe neutral\nlabel\nis the": "",
          "Emotion\nκ-score": "our model on Prototypical Networks (Snell et al.,"
        },
        {
          "dataset. We can see that\nthe neutral\nlabel\nis the": "of\nthese utterances\nis associated an emotion la-",
          "Emotion\nκ-score": ""
        },
        {
          "dataset. We can see that\nthe neutral\nlabel\nis the": "",
          "Emotion\nκ-score": "2017), which create prototypes from the average"
        },
        {
          "dataset. We can see that\nthe neutral\nlabel\nis the": "bel, giving a sequence of labels by conversation:",
          "Emotion\nκ-score": ""
        },
        {
          "dataset. We can see that\nthe neutral\nlabel\nis the": "",
          "Emotion\nκ-score": "of the embeddings of the words forming the utter-"
        },
        {
          "dataset. We can see that\nthe neutral\nlabel\nis the": "Yi = (y1, y2, . . . , y|Ci|). Finally, an utterance is a",
          "Emotion\nκ-score": ""
        },
        {
          "dataset. We can see that\nthe neutral\nlabel\nis the": "",
          "Emotion\nκ-score": "ance. Our proposed model, ProtoSeq, builds on"
        },
        {
          "dataset. We can see that\nthe neutral\nlabel\nis the": "sequence of words, uj = (wj\n1, wj\n2, . . . , wj",
          "Emotion\nκ-score": ""
        },
        {
          "dataset. We can see that\nthe neutral\nlabel\nis the": "|uj |) .",
          "Emotion\nκ-score": "this by factoring in conversational context and per-"
        },
        {
          "dataset. We can see that\nthe neutral\nlabel\nis the": "",
          "Emotion\nκ-score": "forming sequence labeling, thus allowing the use of"
        },
        {
          "dataset. We can see that\nthe neutral\nlabel\nis the": "2See appendix A",
          "Emotion\nκ-score": "both input and output dependencies when applying"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "FSL to ERC. ProtoSeq is divided into four main": "components, applied at each consecutive level of",
          "contextual information from the conversation, thus": "following the trend initiated by (Poria et al., 2017)"
        },
        {
          "FSL to ERC. ProtoSeq is divided into four main": "granularity of the data.",
          "contextual information from the conversation, thus": "in ERC to use a recurrent context encoder. We"
        },
        {
          "FSL to ERC. ProtoSeq is divided into four main": "",
          "contextual information from the conversation, thus": "obtain contextual utterance representations vj:"
        },
        {
          "FSL to ERC. ProtoSeq is divided into four main": "",
          "contextual information from the conversation, thus": "vj = fc([uk]j−1\nk=1, uj, [uk]|Ci|\nk=j+1)"
        },
        {
          "FSL to ERC. ProtoSeq is divided into four main": "",
          "contextual information from the conversation, thus": "As we work in a few-shot learning setting, we try"
        },
        {
          "FSL to ERC. ProtoSeq is divided into four main": "",
          "contextual information from the conversation, thus": "not\nto over-complexify our model, hence we do"
        },
        {
          "FSL to ERC. ProtoSeq is divided into four main": "",
          "contextual information from the conversation, thus": "not add a transformer-based global context encoder"
        },
        {
          "FSL to ERC. ProtoSeq is divided into four main": "",
          "contextual information from the conversation, thus": "(Wang et al., 2020) on top of the BiLSTM."
        },
        {
          "FSL to ERC. ProtoSeq is divided into four main": "",
          "contextual information from the conversation, thus": "Prototypes Creation:\nWe feed the output of the"
        },
        {
          "FSL to ERC. ProtoSeq is divided into four main": "",
          "contextual information from the conversation, thus": "context encoder to a multi-layer perceptron made"
        },
        {
          "FSL to ERC. ProtoSeq is divided into four main": "",
          "contextual information from the conversation, thus": "of 2 fully connected layers with dropout and ReLU."
        },
        {
          "FSL to ERC. ProtoSeq is divided into four main": "",
          "contextual information from the conversation, thus": "The resulting representations are then used to create"
        },
        {
          "FSL to ERC. ProtoSeq is divided into four main": "",
          "contextual information from the conversation, thus": "prototypes c: for the class k,"
        },
        {
          "FSL to ERC. ProtoSeq is divided into four main": "",
          "contextual information from the conversation, thus": "(cid:88)"
        },
        {
          "FSL to ERC. ProtoSeq is divided into four main": "",
          "contextual information from the conversation, thus": "1 N\nM LP (vj)\nck ←"
        },
        {
          "FSL to ERC. ProtoSeq is divided into four main": "",
          "contextual information from the conversation, thus": "C"
        },
        {
          "FSL to ERC. ProtoSeq is divided into four main": "",
          "contextual information from the conversation, thus": "(uj ,yj ) with yj =k"
        },
        {
          "FSL to ERC. ProtoSeq is divided into four main": "",
          "contextual information from the conversation, thus": "where NC is the number of classes, and MLP refers"
        },
        {
          "FSL to ERC. ProtoSeq is divided into four main": "",
          "contextual information from the conversation, thus": "to Multi-Layer Perceptron."
        },
        {
          "FSL to ERC. ProtoSeq is divided into four main": "",
          "contextual information from the conversation, thus": "Sequence\nPrediction:\nWe\ncompute\nthe\neu-"
        },
        {
          "FSL to ERC. ProtoSeq is divided into four main": "",
          "contextual information from the conversation, thus": "clidean distance from the contextual representation"
        },
        {
          "FSL to ERC. ProtoSeq is divided into four main": "",
          "contextual information from the conversation, thus": "of the utterance to each class prototype. The pre-"
        },
        {
          "FSL to ERC. ProtoSeq is divided into four main": "",
          "contextual information from the conversation, thus": "is the class\ndicted label ˆyj\nto each utterance uj"
        },
        {
          "FSL to ERC. ProtoSeq is divided into four main": "",
          "contextual information from the conversation, thus": "corresponding to its closest prototype:"
        },
        {
          "FSL to ERC. ProtoSeq is divided into four main": "",
          "contextual information from the conversation, thus": "yj ← arg min\nd(M LP (vj), ck)"
        },
        {
          "FSL to ERC. ProtoSeq is divided into four main": "",
          "contextual information from the conversation, thus": "k∈C"
        },
        {
          "FSL to ERC. ProtoSeq is divided into four main": "",
          "contextual information from the conversation, thus": "We allow our model\nto consider dependencies"
        },
        {
          "FSL to ERC. ProtoSeq is divided into four main": "",
          "contextual information from the conversation, thus": "between the labels, we add a ﬁnal CRF layer on top"
        },
        {
          "FSL to ERC. ProtoSeq is divided into four main": "",
          "contextual information from the conversation, thus": "of label prediction, the emission scores being the"
        },
        {
          "FSL to ERC. ProtoSeq is divided into four main": "",
          "contextual information from the conversation, thus": "euclidean distances for each utterance. Overall, our"
        },
        {
          "FSL to ERC. ProtoSeq is divided into four main": "",
          "contextual information from the conversation, thus": "model is a variation of the traditional BiLSM-CRF"
        },
        {
          "FSL to ERC. ProtoSeq is divided into four main": "Figure 2: ProtoSeq Global View",
          "contextual information from the conversation, thus": "model, based on prototypical networks."
        },
        {
          "FSL to ERC. ProtoSeq is divided into four main": "",
          "contextual information from the conversation, thus": "4.3\nExperimental protocol"
        },
        {
          "FSL to ERC. ProtoSeq is divided into four main": "Utterance Encoder:\nSimilar\nto the encoder of",
          "contextual information from the conversation, thus": ""
        },
        {
          "FSL to ERC. ProtoSeq is divided into four main": "",
          "contextual information from the conversation, thus": "We follow the setting used by (Bao et al., 2020)"
        },
        {
          "FSL to ERC. ProtoSeq is divided into four main": "the Prototypical Networks, our utterance encoder",
          "contextual information from the conversation, thus": ""
        },
        {
          "FSL to ERC. ProtoSeq is divided into four main": "",
          "contextual information from the conversation, thus": "by considering a training epoch as a set of 100"
        },
        {
          "FSL to ERC. ProtoSeq is divided into four main": "to only one vector:\nfu reduces the utterance ui",
          "contextual information from the conversation, thus": ""
        },
        {
          "FSL to ERC. ProtoSeq is divided into four main": "",
          "contextual information from the conversation, thus": "random episodes from the training set, and apply-"
        },
        {
          "FSL to ERC. ProtoSeq is divided into four main": "",
          "contextual information from the conversation, thus": "ing a validation step made of 100 random episodes"
        },
        {
          "FSL to ERC. ProtoSeq is divided into four main": "uj = fu(wj\n1, wj\n2, . . . , wj",
          "contextual information from the conversation, thus": ""
        },
        {
          "FSL to ERC. ProtoSeq is divided into four main": "|uj |)",
          "contextual information from the conversation, thus": ""
        },
        {
          "FSL to ERC. ProtoSeq is divided into four main": "",
          "contextual information from the conversation, thus": "from the validation set after each epoch. We test"
        },
        {
          "FSL to ERC. ProtoSeq is divided into four main": "",
          "contextual information from the conversation, thus": "our model using 1,000 random episodes from the"
        },
        {
          "FSL to ERC. ProtoSeq is divided into four main": "The architecture of our encoder is based on the",
          "contextual information from the conversation, thus": ""
        },
        {
          "FSL to ERC. ProtoSeq is divided into four main": "",
          "contextual information from the conversation, thus": "test set. The maximum number of epochs is set to"
        },
        {
          "FSL to ERC. ProtoSeq is divided into four main": "Convolutional Neural Network (CNN) described by",
          "contextual information from the conversation, thus": ""
        },
        {
          "FSL to ERC. ProtoSeq is divided into four main": "",
          "contextual information from the conversation, thus": "1,000, but when the F1-micro score does not\nim-"
        },
        {
          "FSL to ERC. ProtoSeq is divided into four main": "(Kim, 2014) , which makes tokens through different",
          "contextual information from the conversation, thus": ""
        },
        {
          "FSL to ERC. ProtoSeq is divided into four main": "",
          "contextual information from the conversation, thus": "prove for 100 consecutive epochs, we stop the train-"
        },
        {
          "FSL to ERC. ProtoSeq is divided into four main": "convolution ﬁlters and merges the representation",
          "contextual information from the conversation, thus": ""
        },
        {
          "FSL to ERC. ProtoSeq is divided into four main": "",
          "contextual information from the conversation, thus": "ing and reload the best model’s weights. We use"
        },
        {
          "FSL to ERC. ProtoSeq is divided into four main": "through max-over-time pooling.",
          "contextual information from the conversation, thus": ""
        },
        {
          "FSL to ERC. ProtoSeq is divided into four main": "",
          "contextual information from the conversation, thus": "the Adam (Kingma and Ba, 2017; Loshchilov and"
        },
        {
          "FSL to ERC. ProtoSeq is divided into four main": "Context Encoder:\nAfter applying a non-linear",
          "contextual information from the conversation, thus": "Hutter, 2019) optimizer to train the model while"
        },
        {
          "FSL to ERC. ProtoSeq is divided into four main": "activation (ReLU), we use a Bi-directional LSTM",
          "contextual information from the conversation, thus": "maximizing the log-likelihood loss of the correct"
        },
        {
          "FSL to ERC. ProtoSeq is divided into four main": "layer (BiLSTM) (Huang et al., 2015), to integrate",
          "contextual information from the conversation, thus": "emotion sequences in the query set Qk"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table 4: KET relies on ConceptNet (Speer et al.,",
      "data": [
        {
          "pervised model, CESTa (Wang et al., 2020), which": ""
        },
        {
          "pervised model, CESTa (Wang et al., 2020), which": "is computation-heavy. Indeed, CESTa is a contex-"
        },
        {
          "pervised model, CESTa (Wang et al., 2020), which": ""
        },
        {
          "pervised model, CESTa (Wang et al., 2020), which": "tualized emotion sequence tagging model which"
        },
        {
          "pervised model, CESTa (Wang et al., 2020), which": "considers the fusion of a combination of a trans-"
        },
        {
          "pervised model, CESTa (Wang et al., 2020), which": "former and BiLSTM as the global context encoder"
        },
        {
          "pervised model, CESTa (Wang et al., 2020), which": "with a recurrent individual context encoder before"
        },
        {
          "pervised model, CESTa (Wang et al., 2020), which": "feeding a CRF layer. CESTa achieves 63% in micro"
        },
        {
          "pervised model, CESTa (Wang et al., 2020), which": "F1-score in a fully supervised learning approach."
        },
        {
          "pervised model, CESTa (Wang et al., 2020), which": "ProtoSeq, much lighter, achieves a 31% micro F1"
        },
        {
          "pervised model, CESTa (Wang et al., 2020), which": "score, demonstrating the potential of FSL for se-"
        },
        {
          "pervised model, CESTa (Wang et al., 2020), which": ""
        },
        {
          "pervised model, CESTa (Wang et al., 2020), which": "quence labeling when available data is scarce, espe-"
        },
        {
          "pervised model, CESTa (Wang et al., 2020), which": ""
        },
        {
          "pervised model, CESTa (Wang et al., 2020), which": "cially when many supervised approaches obtained"
        },
        {
          "pervised model, CESTa (Wang et al., 2020), which": ""
        },
        {
          "pervised model, CESTa (Wang et al., 2020), which": "F1-scores around 50%. While using the Live Chat"
        },
        {
          "pervised model, CESTa (Wang et al., 2020), which": ""
        },
        {
          "pervised model, CESTa (Wang et al., 2020), which": "Customer Service dataset, we only change the ini-"
        },
        {
          "pervised model, CESTa (Wang et al., 2020), which": "tial embeddings from English to French, and apply"
        },
        {
          "pervised model, CESTa (Wang et al., 2020), which": ""
        },
        {
          "pervised model, CESTa (Wang et al., 2020), which": "the two best models according to 3: CESTa and"
        },
        {
          "pervised model, CESTa (Wang et al., 2020), which": ""
        },
        {
          "pervised model, CESTa (Wang et al., 2020), which": "KET (Zhong et al., 2019). The CESTa implemen-"
        },
        {
          "pervised model, CESTa (Wang et al., 2020), which": ""
        },
        {
          "pervised model, CESTa (Wang et al., 2020), which": "tation yielded inconclusive results6, this is why we"
        },
        {
          "pervised model, CESTa (Wang et al., 2020), which": ""
        },
        {
          "pervised model, CESTa (Wang et al., 2020), which": "present the KET results on our speciﬁc corpus in"
        },
        {
          "pervised model, CESTa (Wang et al., 2020), which": ""
        },
        {
          "pervised model, CESTa (Wang et al., 2020), which": "Table 4. KET relies on ConceptNet (Speer et al.,"
        },
        {
          "pervised model, CESTa (Wang et al., 2020), which": ""
        },
        {
          "pervised model, CESTa (Wang et al., 2020), which": "2017), a multilingual knowledge base. Thus, we"
        },
        {
          "pervised model, CESTa (Wang et al., 2020), which": ""
        },
        {
          "pervised model, CESTa (Wang et al., 2020), which": "only switch from GloVe embeddings (Pennington"
        },
        {
          "pervised model, CESTa (Wang et al., 2020), which": ""
        },
        {
          "pervised model, CESTa (Wang et al., 2020), which": "et al., 2014) to French FastText ones in order to"
        },
        {
          "pervised model, CESTa (Wang et al., 2020), which": ""
        },
        {
          "pervised model, CESTa (Wang et al., 2020), which": "ensure comparison with our ProtoSeq model. As"
        },
        {
          "pervised model, CESTa (Wang et al., 2020), which": ""
        },
        {
          "pervised model, CESTa (Wang et al., 2020), which": "expected, performance is lower on the Live Chat"
        },
        {
          "pervised model, CESTa (Wang et al., 2020), which": "Customer Service corpus."
        }
      ],
      "page": 6
    },
    {
      "caption": "Table 4: KET relies on ConceptNet (Speer et al.,",
      "data": [
        {
          "into account the majority class \"no_emotion\" as it": ""
        },
        {
          "into account the majority class \"no_emotion\" as it": "represents 80% of the DailyDialog corpus.\nThis"
        },
        {
          "into account the majority class \"no_emotion\" as it": ""
        },
        {
          "into account the majority class \"no_emotion\" as it": ""
        },
        {
          "into account the majority class \"no_emotion\" as it": "allows performance comparison with related work"
        },
        {
          "into account the majority class \"no_emotion\" as it": ""
        },
        {
          "into account the majority class \"no_emotion\" as it": "on ERC through supervised learning. We do the"
        },
        {
          "into account the majority class \"no_emotion\" as it": "same for the Live Chat Customer Service corpus"
        },
        {
          "into account the majority class \"no_emotion\" as it": "by ignoring the \"neutral\" label."
        },
        {
          "into account the majority class \"no_emotion\" as it": ""
        },
        {
          "into account the majority class \"no_emotion\" as it": "Comparison to supervised learning\nDailyDia-"
        },
        {
          "into account the majority class \"no_emotion\" as it": ""
        },
        {
          "into account the majority class \"no_emotion\" as it": "log is used to compare our FSL approach with"
        },
        {
          "into account the majority class \"no_emotion\" as it": ""
        },
        {
          "into account the majority class \"no_emotion\" as it": "recent supervised learning results on ERC. As ex-"
        },
        {
          "into account the majority class \"no_emotion\" as it": ""
        },
        {
          "into account the majority class \"no_emotion\" as it": "pected, our best FSL model, ProtoSeq, yields lesser"
        },
        {
          "into account the majority class \"no_emotion\" as it": ""
        },
        {
          "into account the majority class \"no_emotion\" as it": "performance than supervised approaches. The lat-"
        },
        {
          "into account the majority class \"no_emotion\" as it": ""
        },
        {
          "into account the majority class \"no_emotion\" as it": "ter presuppose the availability of a sufﬁciently large"
        },
        {
          "into account the majority class \"no_emotion\" as it": ""
        },
        {
          "into account the majority class \"no_emotion\" as it": "amount of annotated data and their performance"
        },
        {
          "into account the majority class \"no_emotion\" as it": ""
        },
        {
          "into account the majority class \"no_emotion\" as it": "thus represents the upper bound of the expected"
        },
        {
          "into account the majority class \"no_emotion\" as it": ""
        },
        {
          "into account the majority class \"no_emotion\" as it": "results. More precisely, we focus on the differ-"
        },
        {
          "into account the majority class \"no_emotion\" as it": ""
        },
        {
          "into account the majority class \"no_emotion\" as it": "ence between ProtoSeq with a state-of-the-art su-"
        },
        {
          "into account the majority class \"no_emotion\" as it": ""
        },
        {
          "into account the majority class \"no_emotion\" as it": "pervised model, CESTa (Wang et al., 2020), which"
        },
        {
          "into account the majority class \"no_emotion\" as it": ""
        },
        {
          "into account the majority class \"no_emotion\" as it": "is computation-heavy. Indeed, CESTa is a contex-"
        },
        {
          "into account the majority class \"no_emotion\" as it": ""
        },
        {
          "into account the majority class \"no_emotion\" as it": "tualized emotion sequence tagging model which"
        },
        {
          "into account the majority class \"no_emotion\" as it": "considers the fusion of a combination of a trans-"
        },
        {
          "into account the majority class \"no_emotion\" as it": "former and BiLSTM as the global context encoder"
        },
        {
          "into account the majority class \"no_emotion\" as it": "with a recurrent individual context encoder before"
        },
        {
          "into account the majority class \"no_emotion\" as it": "feeding a CRF layer. CESTa achieves 63% in micro"
        },
        {
          "into account the majority class \"no_emotion\" as it": "F1-score in a fully supervised learning approach."
        },
        {
          "into account the majority class \"no_emotion\" as it": "ProtoSeq, much lighter, achieves a 31% micro F1"
        },
        {
          "into account the majority class \"no_emotion\" as it": "score, demonstrating the potential of FSL for se-"
        },
        {
          "into account the majority class \"no_emotion\" as it": ""
        },
        {
          "into account the majority class \"no_emotion\" as it": "quence labeling when available data is scarce, espe-"
        },
        {
          "into account the majority class \"no_emotion\" as it": ""
        },
        {
          "into account the majority class \"no_emotion\" as it": "cially when many supervised approaches obtained"
        },
        {
          "into account the majority class \"no_emotion\" as it": ""
        },
        {
          "into account the majority class \"no_emotion\" as it": "F1-scores around 50%. While using the Live Chat"
        },
        {
          "into account the majority class \"no_emotion\" as it": ""
        },
        {
          "into account the majority class \"no_emotion\" as it": "Customer Service dataset, we only change the ini-"
        },
        {
          "into account the majority class \"no_emotion\" as it": "tial embeddings from English to French, and apply"
        },
        {
          "into account the majority class \"no_emotion\" as it": ""
        },
        {
          "into account the majority class \"no_emotion\" as it": "the two best models according to 3: CESTa and"
        },
        {
          "into account the majority class \"no_emotion\" as it": ""
        },
        {
          "into account the majority class \"no_emotion\" as it": "KET (Zhong et al., 2019). The CESTa implemen-"
        },
        {
          "into account the majority class \"no_emotion\" as it": ""
        },
        {
          "into account the majority class \"no_emotion\" as it": "tation yielded inconclusive results6, this is why we"
        },
        {
          "into account the majority class \"no_emotion\" as it": ""
        },
        {
          "into account the majority class \"no_emotion\" as it": "present the KET results on our speciﬁc corpus in"
        },
        {
          "into account the majority class \"no_emotion\" as it": ""
        },
        {
          "into account the majority class \"no_emotion\" as it": "Table 4. KET relies on ConceptNet (Speer et al.,"
        },
        {
          "into account the majority class \"no_emotion\" as it": ""
        },
        {
          "into account the majority class \"no_emotion\" as it": "2017), a multilingual knowledge base. Thus, we"
        },
        {
          "into account the majority class \"no_emotion\" as it": ""
        },
        {
          "into account the majority class \"no_emotion\" as it": "only switch from GloVe embeddings (Pennington"
        },
        {
          "into account the majority class \"no_emotion\" as it": ""
        },
        {
          "into account the majority class \"no_emotion\" as it": "et al., 2014) to French FastText ones in order to"
        },
        {
          "into account the majority class \"no_emotion\" as it": ""
        },
        {
          "into account the majority class \"no_emotion\" as it": "ensure comparison with our ProtoSeq model. As"
        },
        {
          "into account the majority class \"no_emotion\" as it": ""
        },
        {
          "into account the majority class \"no_emotion\" as it": "expected, performance is lower on the Live Chat"
        },
        {
          "into account the majority class \"no_emotion\" as it": "Customer Service corpus."
        },
        {
          "into account the majority class \"no_emotion\" as it": ""
        },
        {
          "into account the majority class \"no_emotion\" as it": "Few-shot\nlearning baselines\nWe consider\ntwo"
        },
        {
          "into account the majority class \"no_emotion\" as it": ""
        },
        {
          "into account the majority class \"no_emotion\" as it": "baselines. We apply the original Prototypical Net-"
        },
        {
          "into account the majority class \"no_emotion\" as it": ""
        },
        {
          "into account the majority class \"no_emotion\" as it": "works (Snell et al., 2017), only retrieving the la-"
        },
        {
          "into account the majority class \"no_emotion\" as it": ""
        },
        {
          "into account the majority class \"no_emotion\" as it": "bels using the euclidean distance to class proto-"
        },
        {
          "into account the majority class \"no_emotion\" as it": ""
        },
        {
          "into account the majority class \"no_emotion\" as it": "types. We also apply the WarmProto-CRF (Frit-"
        },
        {
          "into account the majority class \"no_emotion\" as it": ""
        },
        {
          "into account the majority class \"no_emotion\" as it": ""
        },
        {
          "into account the majority class \"no_emotion\" as it": "6We present\nin our code an implementation of CESTa"
        },
        {
          "into account the majority class \"no_emotion\" as it": "following the paper’s descriptions. On our dataset,\nit only"
        },
        {
          "into account the majority class \"no_emotion\" as it": "labeled the two majority classes ’no_emotion’ and ’neutral’,"
        },
        {
          "into account the majority class \"no_emotion\" as it": "leading to a null F1 (micro)."
        }
      ],
      "page": 6
    },
    {
      "caption": "Table 4: Few-shot learning results on Customer Service Live Chats (seq size = 18): 11-way 5-shot 10-query",
      "data": [
        {
          "KET (Zhong et al., 2019)": "CESTa (Wang et al., 2020)",
          "0.5337": "0.6312"
        },
        {
          "KET (Zhong et al., 2019)": "",
          "0.5337": ""
        },
        {
          "KET (Zhong et al., 2019)": "Proto (Snell et al., 2017)",
          "0.5337": "0.2141 ±0.0141"
        },
        {
          "KET (Zhong et al., 2019)": "WarmProto-CRF (Fritzler et al., 2019)",
          "0.5337": "0.2607 ±0.0381"
        },
        {
          "KET (Zhong et al., 2019)": "ProtoSeq-AVG",
          "0.5337": "0.1643 ±0.0258"
        },
        {
          "KET (Zhong et al., 2019)": "ProtoSeq-Tr",
          "0.5337": "0.2557 ±0.0317"
        },
        {
          "KET (Zhong et al., 2019)": "ProtoSeq-CNN",
          "0.5337": "0.2560 ±0.0275"
        },
        {
          "KET (Zhong et al., 2019)": "ProtoSeq",
          "0.5337": "0.3181 ±0.0276"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table 4: Few-shot learning results on Customer Service Live Chats (seq size = 18): 11-way 5-shot 10-query",
      "data": [
        {
          "Model": "",
          "F1 (weighted)": "Supervised Learning",
          "MCC": "",
          "F1 (micro)": ""
        },
        {
          "Model": "cLSTM",
          "F1 (weighted)": "",
          "MCC": "",
          "F1 (micro)": "0.4990"
        },
        {
          "Model": "CNN (Kim, 2014)",
          "F1 (weighted)": "",
          "MCC": "",
          "F1 (micro)": "0.4934"
        },
        {
          "Model": "CNN+cLSTM (Poria et al., 2017)",
          "F1 (weighted)": "",
          "MCC": "",
          "F1 (micro)": "0.5184"
        },
        {
          "Model": "BERT BASE (Devlin et al., 2019)",
          "F1 (weighted)": "",
          "MCC": "",
          "F1 (micro)": "0.5312"
        },
        {
          "Model": "DialogueRNN (Majumder et al., 2019)",
          "F1 (weighted)": "",
          "MCC": "",
          "F1 (micro)": "0.5164"
        },
        {
          "Model": "KET (Zhong et al., 2019)",
          "F1 (weighted)": "",
          "MCC": "",
          "F1 (micro)": "0.5337"
        },
        {
          "Model": "CESTa (Wang et al., 2020)",
          "F1 (weighted)": "",
          "MCC": "",
          "F1 (micro)": "0.6312"
        },
        {
          "Model": "",
          "F1 (weighted)": "Few-Shot Learning",
          "MCC": "",
          "F1 (micro)": ""
        },
        {
          "Model": "Proto (Snell et al., 2017)",
          "F1 (weighted)": "0.2377 ±0.0136",
          "MCC": "0.3448 ±0.0105",
          "F1 (micro)": "0.2141 ±0.0141"
        },
        {
          "Model": "WarmProto-CRF (Fritzler et al., 2019)",
          "F1 (weighted)": "0.2384 ±0.0383",
          "MCC": "0.3403 ±0.0365",
          "F1 (micro)": "0.2607 ±0.0381"
        },
        {
          "Model": "ProtoSeq-AVG",
          "F1 (weighted)": "0.1312 ±0.0201",
          "MCC": "0.2622 ±0.0225",
          "F1 (micro)": "0.1643 ±0.0258"
        },
        {
          "Model": "ProtoSeq-Tr",
          "F1 (weighted)": "0.1694 ±0.0293",
          "MCC": "0.3329 ±0.0241",
          "F1 (micro)": "0.2557 ±0.0317"
        },
        {
          "Model": "ProtoSeq-CNN",
          "F1 (weighted)": "0.2244 ±0.0359",
          "MCC": "0.3494 ±0.0182",
          "F1 (micro)": "0.2560 ±0.0275"
        },
        {
          "Model": "ProtoSeq",
          "F1 (weighted)": "0.3522 ±0.0302",
          "MCC": "0.3922 ±0.0233",
          "F1 (micro)": "0.3181 ±0.0276"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table 4: Few-shot learning results on Customer Service Live Chats (seq size = 18): 11-way 5-shot 10-query",
      "data": [
        {
          "Supervised Learning": ""
        },
        {
          "Supervised Learning": "Few-Shot Learning"
        },
        {
          "Supervised Learning": "0.1749 ±0.0481"
        },
        {
          "Supervised Learning": "0.1556 ±0.0522"
        },
        {
          "Supervised Learning": "0.1297 ±0.0246"
        },
        {
          "Supervised Learning": "0.1774 ±0.0285"
        },
        {
          "Supervised Learning": "0.1197 ±0.0198"
        },
        {
          "Supervised Learning": "0.3022 ±0.0256"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table 4: Few-shot learning results on Customer Service Live Chats (seq size = 18): 11-way 5-shot 10-query",
      "data": [
        {
          "Table 4:\nFew-shot\nlearning results on Customer Service Live Chats (seq size = 18):": "(padding & trim). MCC = multi-class Matthews Correlation Coefﬁcient (MCC). ± = test episodes variance.",
          "11-way 5-shot 10-query": ""
        },
        {
          "Table 4:\nFew-shot\nlearning results on Customer Service Live Chats (seq size = 18):": "zler et al., 2019) which is a variant of Prototypical",
          "11-way 5-shot 10-query": "query). We can see the micro F1-score is really"
        },
        {
          "Table 4:\nFew-shot\nlearning results on Customer Service Live Chats (seq size = 18):": "Networks designed for sequence labeling by inte-",
          "11-way 5-shot 10-query": "low with only 16.43%. By considering a ProtoSeq"
        },
        {
          "Table 4:\nFew-shot\nlearning results on Customer Service Live Chats (seq size = 18):": "grating CRFs. We implement it without including",
          "11-way 5-shot 10-query": "only using an utterance encoder based on CNN"
        },
        {
          "Table 4:\nFew-shot\nlearning results on Customer Service Live Chats (seq size = 18):": "the bias they created for the O label in the BIO se-",
          "11-way 5-shot 10-query": "(ProtoSeq-CNN) or an utterance encoder based on"
        },
        {
          "Table 4:\nFew-shot\nlearning results on Customer Service Live Chats (seq size = 18):": "quence labeling task. This method uses a BiLSTM",
          "11-way 5-shot 10-query": "a 2-layers 4-heads Transformer (ProtoSeq-Tr) we"
        },
        {
          "Table 4:\nFew-shot\nlearning results on Customer Service Live Chats (seq size = 18):": "utterance encoder to further compute the prototypes",
          "11-way 5-shot 10-query": "can see the score improve. The addition of the BiL-"
        },
        {
          "Table 4:\nFew-shot\nlearning results on Customer Service Live Chats (seq size = 18):": "with the euclidean distance.",
          "11-way 5-shot 10-query": "STM context encoder really enables the model to"
        },
        {
          "Table 4:\nFew-shot\nlearning results on Customer Service Live Chats (seq size = 18):": "",
          "11-way 5-shot 10-query": "capture more information:\nthese variants show the"
        },
        {
          "Table 4:\nFew-shot\nlearning results on Customer Service Live Chats (seq size = 18):": "",
          "11-way 5-shot 10-query": "importance of integrating a context encoder in the"
        },
        {
          "Table 4:\nFew-shot\nlearning results on Customer Service Live Chats (seq size = 18):": "Few-shot\nlearning\non DailyDialog\nTable\n3",
          "11-way 5-shot 10-query": ""
        },
        {
          "Table 4:\nFew-shot\nlearning results on Customer Service Live Chats (seq size = 18):": "",
          "11-way 5-shot 10-query": "model."
        }
      ],
      "page": 7
    },
    {
      "caption": "Table 5: isquitesensibletotheconversationlength,which Thedowngradeinperformancewhileomittingthe",
      "data": [
        {
          "everyday. As a consequence,\nit would render the": "ideally annotated training corpus obsolete at some",
          "Operator: And the code will be avail-": "able again"
        },
        {
          "everyday. As a consequence,\nit would render the": "point. This FSL prediction leads to lesser scores,",
          "Operator: And the code will be avail-": ""
        },
        {
          "everyday. As a consequence,\nit would render the": "but with the same hierarchy among variants. Proto-",
          "Operator: And the code will be avail-": ""
        },
        {
          "everyday. As a consequence,\nit would render the": "",
          "Operator: And the code will be avail-": "Moreover, speciﬁc lexical ﬁelds,\nrelevant\nto the"
        },
        {
          "everyday. As a consequence,\nit would render the": "Seq, using a BiLSTM context encoder, yields again",
          "Operator: And the code will be avail-": ""
        },
        {
          "everyday. As a consequence,\nit would render the": "",
          "Operator: And the code will be avail-": "customer service being provided, can also make"
        },
        {
          "everyday. As a consequence,\nit would render the": "the best scores. The higher number of classes (with",
          "Operator: And the code will be avail-": ""
        },
        {
          "everyday. As a consequence,\nit would render the": "",
          "Operator: And the code will be avail-": "the task more difﬁcult for the model."
        },
        {
          "everyday. As a consequence,\nit would render the": "11 classes including 9 emotions versus 7 classes in-",
          "Operator: And the code will be avail-": ""
        },
        {
          "everyday. As a consequence,\nit would render the": "cluding 6 emotions) may explain the overall lower",
          "Operator: And the code will be avail-": "Quantifying the impact of the CRF layer\nOur"
        },
        {
          "everyday. As a consequence,\nit would render the": "numbers we observe here, compared to those we",
          "Operator: And the code will be avail-": "model beneﬁts from the addition of a ﬁnal CRF"
        },
        {
          "everyday. As a consequence,\nit would render the": "obtain on DailyDialog.",
          "Operator: And the code will be avail-": "layer to compute the best possible output sequence."
        },
        {
          "everyday. As a consequence,\nit would render the": "",
          "Operator: And the code will be avail-": "This allows the model\nto generalize faster and to"
        },
        {
          "everyday. As a consequence,\nit would render the": "Artiﬁcial versus Real Data\nDailyDialog is an",
          "Operator: And the code will be avail-": ""
        },
        {
          "everyday. As a consequence,\nit would render the": "",
          "Operator: And the code will be avail-": "achieve a higher score despite the few examples."
        },
        {
          "everyday. As a consequence,\nit would render the": "artiﬁcial corpus which follows standard, idealized",
          "Operator: And the code will be avail-": ""
        },
        {
          "everyday. As a consequence,\nit would render the": "",
          "Operator: And the code will be avail-": "However, the prediction stability lowers, as the stan-"
        },
        {
          "everyday. As a consequence,\nit would render the": "conversations. We can see that ERC performance",
          "Operator: And the code will be avail-": ""
        },
        {
          "everyday. As a consequence,\nit would render the": "",
          "Operator: And the code will be avail-": "dard deviation across episodes shows in Table 5."
        },
        {
          "everyday. As a consequence,\nit would render the": "is quite sensible to the conversation length, which",
          "Operator: And the code will be avail-": ""
        },
        {
          "everyday. As a consequence,\nit would render the": "",
          "Operator: And the code will be avail-": "The downgrade in performance while omitting the"
        },
        {
          "everyday. As a consequence,\nit would render the": "seems to conﬁrm conclusions drawn in recent liter-",
          "Operator: And the code will be avail-": ""
        },
        {
          "everyday. As a consequence,\nit would render the": "",
          "Operator: And the code will be avail-": "CRF layer may be due to the label dependency it"
        },
        {
          "everyday. As a consequence,\nit would render the": "ature (Wang et al., 2020). Customer Service Live",
          "Operator: And the code will be avail-": ""
        },
        {
          "everyday. As a consequence,\nit would render the": "",
          "Operator: And the code will be avail-": "emphasizes. Indeed, without the CRF, label depen-"
        },
        {
          "everyday. As a consequence,\nit would render the": "Chats being real use-case data, their length varies a",
          "Operator: And the code will be avail-": ""
        },
        {
          "everyday. As a consequence,\nit would render the": "",
          "Operator: And the code will be avail-": "dency can only be inferred from the BiLSTM con-"
        },
        {
          "everyday. As a consequence,\nit would render the": "lot, ranging from 2 to 85 messages (where conversa-",
          "Operator: And the code will be avail-": ""
        },
        {
          "everyday. As a consequence,\nit would render the": "",
          "Operator: And the code will be avail-": "text encoder. The CRF layer accentuates in-episode"
        },
        {
          "everyday. As a consequence,\nit would render the": "tions from DailyDialog go from 2 to 35 messages).",
          "Operator: And the code will be avail-": ""
        },
        {
          "everyday. As a consequence,\nit would render the": "",
          "Operator: And the code will be avail-": "label dependency by allowing the prediction to be"
        },
        {
          "everyday. As a consequence,\nit would render the": "However, ERC also seems to be impacted by the",
          "Operator: And the code will be avail-": ""
        },
        {
          "everyday. As a consequence,\nit would render the": "",
          "Operator: And the code will be avail-": "further adapted to the conversation context for each"
        },
        {
          "everyday. As a consequence,\nit would render the": "utterance textual content, as our data contains a",
          "Operator: And the code will be avail-": ""
        },
        {
          "everyday. As a consequence,\nit would render the": "",
          "Operator: And the code will be avail-": "query conversation."
        },
        {
          "everyday. As a consequence,\nit would render the": "lot of spelling mistakes, shortcuts, or slurs. More",
          "Operator: And the code will be avail-": ""
        }
      ],
      "page": 8
    },
    {
      "caption": "Table 5: isquitesensibletotheconversationlength,which Thedowngradeinperformancewhileomittingthe",
      "data": [
        {
          "Few-shot\nlearning on Customer Service Live": "Chats\nWe also apply this approach on the Cus-",
          "Operator: Did you make the simulation": "using the promo code?"
        },
        {
          "Few-shot\nlearning on Customer Service Live": "tomer Service Live Chats, further motivated by the",
          "Operator: Did you make the simulation": "Visitor: I did it 5 minutes ago"
        },
        {
          "Few-shot\nlearning on Customer Service Live": "high annotation cost and the fact\nthat supervised",
          "Operator: Did you make the simulation": "Operator: Ok, you have to wait 30min"
        },
        {
          "Few-shot\nlearning on Customer Service Live": "approaches on clean data such as DailyDialog did",
          "Operator: Did you make the simulation": "Visitor:\nbut\nas\nsaid\nbefore,\nI\ndidn’t"
        },
        {
          "Few-shot\nlearning on Customer Service Live": "not achieve an acceptable score for this use case",
          "Operator: Did you make the simulation": "ﬁnished the \"simulation\" because I had"
        },
        {
          "Few-shot\nlearning on Customer Service Live": "(starting from 70 % in micro F1 score). Besides,",
          "Operator: Did you make the simulation": "to pay a 10C ticket even th"
        },
        {
          "Few-shot\nlearning on Customer Service Live": "new conversations with evolving contents (e.g., due",
          "Operator: Did you make the simulation": "Visitor:\n....even though the right one is"
        },
        {
          "Few-shot\nlearning on Customer Service Live": "to the evolution of company services) are created",
          "Operator: Did you make the simulation": "11.5 C"
        },
        {
          "Few-shot\nlearning on Customer Service Live": "everyday. As a consequence,\nit would render the",
          "Operator: Did you make the simulation": "Operator: And the code will be avail-"
        },
        {
          "Few-shot\nlearning on Customer Service Live": "ideally annotated training corpus obsolete at some",
          "Operator: Did you make the simulation": "able again"
        },
        {
          "Few-shot\nlearning on Customer Service Live": "point. This FSL prediction leads to lesser scores,",
          "Operator: Did you make the simulation": ""
        },
        {
          "Few-shot\nlearning on Customer Service Live": "but with the same hierarchy among variants. Proto-",
          "Operator: Did you make the simulation": ""
        },
        {
          "Few-shot\nlearning on Customer Service Live": "",
          "Operator: Did you make the simulation": "Moreover, speciﬁc lexical ﬁelds,\nrelevant\nto the"
        },
        {
          "Few-shot\nlearning on Customer Service Live": "Seq, using a BiLSTM context encoder, yields again",
          "Operator: Did you make the simulation": ""
        },
        {
          "Few-shot\nlearning on Customer Service Live": "",
          "Operator: Did you make the simulation": "customer service being provided, can also make"
        },
        {
          "Few-shot\nlearning on Customer Service Live": "the best scores. The higher number of classes (with",
          "Operator: Did you make the simulation": ""
        },
        {
          "Few-shot\nlearning on Customer Service Live": "",
          "Operator: Did you make the simulation": "the task more difﬁcult for the model."
        },
        {
          "Few-shot\nlearning on Customer Service Live": "11 classes including 9 emotions versus 7 classes in-",
          "Operator: Did you make the simulation": ""
        },
        {
          "Few-shot\nlearning on Customer Service Live": "cluding 6 emotions) may explain the overall lower",
          "Operator: Did you make the simulation": "Quantifying the impact of the CRF layer\nOur"
        },
        {
          "Few-shot\nlearning on Customer Service Live": "numbers we observe here, compared to those we",
          "Operator: Did you make the simulation": "model beneﬁts from the addition of a ﬁnal CRF"
        },
        {
          "Few-shot\nlearning on Customer Service Live": "obtain on DailyDialog.",
          "Operator: Did you make the simulation": "layer to compute the best possible output sequence."
        },
        {
          "Few-shot\nlearning on Customer Service Live": "",
          "Operator: Did you make the simulation": "This allows the model\nto generalize faster and to"
        },
        {
          "Few-shot\nlearning on Customer Service Live": "Artiﬁcial versus Real Data\nDailyDialog is an",
          "Operator: Did you make the simulation": ""
        },
        {
          "Few-shot\nlearning on Customer Service Live": "",
          "Operator: Did you make the simulation": "achieve a higher score despite the few examples."
        },
        {
          "Few-shot\nlearning on Customer Service Live": "artiﬁcial corpus which follows standard, idealized",
          "Operator: Did you make the simulation": ""
        },
        {
          "Few-shot\nlearning on Customer Service Live": "",
          "Operator: Did you make the simulation": "However, the prediction stability lowers, as the stan-"
        },
        {
          "Few-shot\nlearning on Customer Service Live": "conversations. We can see that ERC performance",
          "Operator: Did you make the simulation": ""
        },
        {
          "Few-shot\nlearning on Customer Service Live": "",
          "Operator: Did you make the simulation": "dard deviation across episodes shows in Table 5."
        },
        {
          "Few-shot\nlearning on Customer Service Live": "is quite sensible to the conversation length, which",
          "Operator: Did you make the simulation": ""
        },
        {
          "Few-shot\nlearning on Customer Service Live": "",
          "Operator: Did you make the simulation": "The downgrade in performance while omitting the"
        },
        {
          "Few-shot\nlearning on Customer Service Live": "seems to conﬁrm conclusions drawn in recent liter-",
          "Operator: Did you make the simulation": ""
        },
        {
          "Few-shot\nlearning on Customer Service Live": "",
          "Operator: Did you make the simulation": "CRF layer may be due to the label dependency it"
        },
        {
          "Few-shot\nlearning on Customer Service Live": "ature (Wang et al., 2020). Customer Service Live",
          "Operator: Did you make the simulation": ""
        },
        {
          "Few-shot\nlearning on Customer Service Live": "",
          "Operator: Did you make the simulation": "emphasizes. Indeed, without the CRF, label depen-"
        },
        {
          "Few-shot\nlearning on Customer Service Live": "Chats being real use-case data, their length varies a",
          "Operator: Did you make the simulation": ""
        },
        {
          "Few-shot\nlearning on Customer Service Live": "",
          "Operator: Did you make the simulation": "dency can only be inferred from the BiLSTM con-"
        },
        {
          "Few-shot\nlearning on Customer Service Live": "lot, ranging from 2 to 85 messages (where conversa-",
          "Operator: Did you make the simulation": ""
        },
        {
          "Few-shot\nlearning on Customer Service Live": "",
          "Operator: Did you make the simulation": "text encoder. The CRF layer accentuates in-episode"
        },
        {
          "Few-shot\nlearning on Customer Service Live": "tions from DailyDialog go from 2 to 35 messages).",
          "Operator: Did you make the simulation": ""
        },
        {
          "Few-shot\nlearning on Customer Service Live": "",
          "Operator: Did you make the simulation": "label dependency by allowing the prediction to be"
        },
        {
          "Few-shot\nlearning on Customer Service Live": "However, ERC also seems to be impacted by the",
          "Operator: Did you make the simulation": ""
        },
        {
          "Few-shot\nlearning on Customer Service Live": "",
          "Operator: Did you make the simulation": "further adapted to the conversation context for each"
        },
        {
          "Few-shot\nlearning on Customer Service Live": "utterance textual content, as our data contains a",
          "Operator: Did you make the simulation": ""
        },
        {
          "Few-shot\nlearning on Customer Service Live": "",
          "Operator: Did you make the simulation": "query conversation."
        },
        {
          "Few-shot\nlearning on Customer Service Live": "lot of spelling mistakes, shortcuts, or slurs. More",
          "Operator: Did you make the simulation": ""
        },
        {
          "Few-shot\nlearning on Customer Service Live": "importantly, the visitor may often use several small",
          "Operator: Did you make the simulation": "Model\nDailyDialog\nCSChats"
        },
        {
          "Few-shot\nlearning on Customer Service Live": "messages rather than only one to transmit informa-",
          "Operator: Did you make the simulation": ""
        },
        {
          "Few-shot\nlearning on Customer Service Live": "",
          "Operator: Did you make the simulation": "ProtoSeq-noCRF\n0.2156 ±0.0105\n0.1351 ±0.0144"
        },
        {
          "Few-shot\nlearning on Customer Service Live": "tion;\nthis ﬂow may be interrupted by a message",
          "Operator: Did you make the simulation": "ProtoSeq\n0.3181 ±0.0276\n0.2668 ±0.0270"
        },
        {
          "Few-shot\nlearning on Customer Service Live": "from the operator, making it impossible to detect",
          "Operator: Did you make the simulation": ""
        },
        {
          "Few-shot\nlearning on Customer Service Live": "",
          "Operator: Did you make the simulation": "Table 5: Micro F1-scores without and with the ﬁnal"
        },
        {
          "Few-shot\nlearning on Customer Service Live": "the whole set of messages as an utterance. This",
          "Operator: Did you make the simulation": ""
        },
        {
          "Few-shot\nlearning on Customer Service Live": "",
          "Operator: Did you make the simulation": "CRF layer. CSChats = Customer service live chat"
        },
        {
          "Few-shot\nlearning on Customer Service Live": "is speciﬁc to online instant conversations where",
          "Operator: Did you make the simulation": ""
        },
        {
          "Few-shot\nlearning on Customer Service Live": "speakers do not necessarily wait for the complete",
          "Operator: Did you make the simulation": ""
        },
        {
          "Few-shot\nlearning on Customer Service Live": "message to be written or sent by the addressee. By",
          "Operator: Did you make the simulation": "Emotion Predictions\nTables 6 and 7 show addi-"
        },
        {
          "Few-shot\nlearning on Customer Service Live": "contrast, DailyDialog is made of clean and perfect",
          "Operator: Did you make the simulation": "tional\ninformation from ProtoSeq’s performance"
        },
        {
          "Few-shot\nlearning on Customer Service Live": "exchanges, where one waits for the other to send",
          "Operator: Did you make the simulation": "on each label. These tables present averaged scores"
        },
        {
          "Few-shot\nlearning on Customer Service Live": "the answer. Here is an example with the following",
          "Operator: Did you make the simulation": "from all episodes’ query sets. We can see the pre-"
        },
        {
          "Few-shot\nlearning on Customer Service Live": "clean conversation subset from DailyDialog.",
          "Operator: Did you make the simulation": "dictions differ a lot depending on the target label."
        },
        {
          "Few-shot\nlearning on Customer Service Live": "",
          "Operator: Did you make the simulation": "When applied to DailyDialog,\nthe model has no"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table 6: Additional results on DailyDialog with our",
      "data": [
        {
          "studies to focus on real conversation datasets when-": ""
        },
        {
          "studies to focus on real conversation datasets when-": "ever it is possible."
        },
        {
          "studies to focus on real conversation datasets when-": ""
        },
        {
          "studies to focus on real conversation datasets when-": ""
        },
        {
          "studies to focus on real conversation datasets when-": ""
        },
        {
          "studies to focus on real conversation datasets when-": "no emotion"
        },
        {
          "studies to focus on real conversation datasets when-": "anger"
        },
        {
          "studies to focus on real conversation datasets when-": "disgust"
        },
        {
          "studies to focus on real conversation datasets when-": ""
        },
        {
          "studies to focus on real conversation datasets when-": "fear"
        },
        {
          "studies to focus on real conversation datasets when-": "happiness"
        },
        {
          "studies to focus on real conversation datasets when-": "sadness"
        },
        {
          "studies to focus on real conversation datasets when-": "surprise"
        },
        {
          "studies to focus on real conversation datasets when-": ""
        }
      ],
      "page": 9
    },
    {
      "caption": "Table 6: Additional results on DailyDialog with our",
      "data": [
        {
          "ProtoSeq prediction.": ""
        },
        {
          "ProtoSeq prediction.": ""
        },
        {
          "ProtoSeq prediction.": ""
        },
        {
          "ProtoSeq prediction.": ""
        },
        {
          "ProtoSeq prediction.": "no emotion"
        },
        {
          "ProtoSeq prediction.": "surprise"
        },
        {
          "ProtoSeq prediction.": ""
        },
        {
          "ProtoSeq prediction.": "amusement"
        },
        {
          "ProtoSeq prediction.": ""
        },
        {
          "ProtoSeq prediction.": "satisfaction"
        },
        {
          "ProtoSeq prediction.": "relief"
        },
        {
          "ProtoSeq prediction.": "neutral"
        },
        {
          "ProtoSeq prediction.": ""
        },
        {
          "ProtoSeq prediction.": "fear*"
        },
        {
          "ProtoSeq prediction.": ""
        },
        {
          "ProtoSeq prediction.": "sadness"
        },
        {
          "ProtoSeq prediction.": "disappointment"
        },
        {
          "ProtoSeq prediction.": "anger"
        },
        {
          "ProtoSeq prediction.": ""
        },
        {
          "ProtoSeq prediction.": "frustration"
        },
        {
          "ProtoSeq prediction.": ""
        }
      ],
      "page": 9
    },
    {
      "caption": "Table 6: Additional results on DailyDialog with our",
      "data": [
        {
          "ever, on the Customer Service Live Chats, Table 7": "shows really poor prediction for the disappointment",
          "optimizer that does not take the order into account.": "While this yields better performance,\nit does not"
        },
        {
          "ever, on the Customer Service Live Chats, Table 7": "(translated from the French \"déception\" label) and",
          "optimizer that does not take the order into account.": "guarantee that the order information retrieved from"
        },
        {
          "ever, on the Customer Service Live Chats, Table 7": "fear labels. Actually, in this dataset the precision",
          "optimizer that does not take the order into account.": "the context encoder is wisely used, especially since"
        },
        {
          "ever, on the Customer Service Live Chats, Table 7": "seems to be the main issue with only the frustration",
          "optimizer that does not take the order into account.": "we use the euclidean distances to class prototypes"
        },
        {
          "ever, on the Customer Service Live Chats, Table 7": "and satisfaction labels being somewhat correctly",
          "optimizer that does not take the order into account.": "as emission scores for the sequence labeling. An"
        },
        {
          "ever, on the Customer Service Live Chats, Table 7": "labeled. Given the model and the task, the detailed",
          "optimizer that does not take the order into account.": "ordered-prediction approach may allow the model"
        },
        {
          "ever, on the Customer Service Live Chats, Table 7": "results obtained on both datasets show that perfor-",
          "optimizer that does not take the order into account.": "to better assist operators in real-time during their"
        },
        {
          "ever, on the Customer Service Live Chats, Table 7": "mance score may beneﬁt from the usage of macro",
          "optimizer that does not take the order into account.": "decision process."
        },
        {
          "ever, on the Customer Service Live Chats, Table 7": "F1-score along with the micro F1-score. Indeed, be",
          "optimizer that does not take the order into account.": "Another limitation of our model is that it may be"
        },
        {
          "ever, on the Customer Service Live Chats, Table 7": "it DailyDialog or Customer Service Live Chats, the",
          "optimizer that does not take the order into account.": "difﬁcult to adapt to changes in the context in which"
        },
        {
          "ever, on the Customer Service Live Chats, Table 7": "multi-class prediction of sequence tagging is really",
          "optimizer that does not take the order into account.": "customer service is provided.\nIndeed, the type of"
        },
        {
          "ever, on the Customer Service Live Chats, Table 7": "sparse, and thus leads to imbalanced prediction,",
          "optimizer that does not take the order into account.": "service or the plaftorm used may lead to lexical"
        },
        {
          "ever, on the Customer Service Live Chats, Table 7": "even while using an episodic strategy.",
          "optimizer that does not take the order into account.": "ﬁeld changes or very different emotional states for"
        },
        {
          "ever, on the Customer Service Live Chats, Table 7": "Moreover, the gap between results on DailyDi-",
          "optimizer that does not take the order into account.": "the incoming visitors."
        },
        {
          "ever, on the Customer Service Live Chats, Table 7": "alog and the ones on the Customer Service Live",
          "optimizer that does not take the order into account.": ""
        },
        {
          "ever, on the Customer Service Live Chats, Table 7": "Chats conﬁrms the necessity for the ERC-related",
          "optimizer that does not take the order into account.": "7\nConclusion"
        },
        {
          "ever, on the Customer Service Live Chats, Table 7": "studies to focus on real conversation datasets when-",
          "optimizer that does not take the order into account.": ""
        },
        {
          "ever, on the Customer Service Live Chats, Table 7": "",
          "optimizer that does not take the order into account.": "In this paper, we presented the ﬁrst study on emo-"
        },
        {
          "ever, on the Customer Service Live Chats, Table 7": "ever it is possible.",
          "optimizer that does not take the order into account.": ""
        },
        {
          "ever, on the Customer Service Live Chats, Table 7": "",
          "optimizer that does not take the order into account.": "tion recognition in conversations using few-shot"
        },
        {
          "ever, on the Customer Service Live Chats, Table 7": "precision\nrecall\nf1-score",
          "optimizer that does not take the order into account.": "learning. We proposed a variant of Prototypical"
        },
        {
          "ever, on the Customer Service Live Chats, Table 7": "",
          "optimizer that does not take the order into account.": "Networks taking into account\nthe emotion recog-"
        },
        {
          "ever, on the Customer Service Live Chats, Table 7": "no emotion\n0.98\n0.91\n0.94",
          "optimizer that does not take the order into account.": ""
        },
        {
          "ever, on the Customer Service Live Chats, Table 7": "anger\n0.24\n0.38\n0.30",
          "optimizer that does not take the order into account.": "nition as a sequence labeling task while allowing"
        },
        {
          "ever, on the Customer Service Live Chats, Table 7": "disgust\n0.12\n0.29\n0.17",
          "optimizer that does not take the order into account.": ""
        },
        {
          "ever, on the Customer Service Live Chats, Table 7": "",
          "optimizer that does not take the order into account.": "fast convergence. When compared to other proto-"
        },
        {
          "ever, on the Customer Service Live Chats, Table 7": "fear\n0.58\n0.55\n0.57",
          "optimizer that does not take the order into account.": ""
        },
        {
          "ever, on the Customer Service Live Chats, Table 7": "happiness\n0.39\n0.63\n0.49",
          "optimizer that does not take the order into account.": "typical networks for sequence labeling in few-shot,"
        },
        {
          "ever, on the Customer Service Live Chats, Table 7": "sadness\n0.07\n0.21\n0.11",
          "optimizer that does not take the order into account.": "our model obtained higher scores on both Daily-"
        },
        {
          "ever, on the Customer Service Live Chats, Table 7": "surprise\n0.17\n0.37\n0.24",
          "optimizer that does not take the order into account.": ""
        },
        {
          "ever, on the Customer Service Live Chats, Table 7": "",
          "optimizer that does not take the order into account.": "Dialog and Customer Service Live chats. Through"
        },
        {
          "ever, on the Customer Service Live Chats, Table 7": "",
          "optimizer that does not take the order into account.": "this work, we showed that\nfew-shot\nlearning is"
        },
        {
          "ever, on the Customer Service Live Chats, Table 7": "Table 6: Additional\nresults on DailyDialog with our",
          "optimizer that does not take the order into account.": ""
        },
        {
          "ever, on the Customer Service Live Chats, Table 7": "",
          "optimizer that does not take the order into account.": "possible for this task even though it\nis still difﬁ-"
        },
        {
          "ever, on the Customer Service Live Chats, Table 7": "ProtoSeq prediction.",
          "optimizer that does not take the order into account.": ""
        },
        {
          "ever, on the Customer Service Live Chats, Table 7": "",
          "optimizer that does not take the order into account.": "cult to achieve the same performance as supervised"
        },
        {
          "ever, on the Customer Service Live Chats, Table 7": "",
          "optimizer that does not take the order into account.": "learning approaches. This study also shows the"
        },
        {
          "ever, on the Customer Service Live Chats, Table 7": "precision\nrecall\nf1-score",
          "optimizer that does not take the order into account.": "challenges that remain when tackling in-the-wild"
        },
        {
          "ever, on the Customer Service Live Chats, Table 7": "",
          "optimizer that does not take the order into account.": "data collected in the context of a real application."
        },
        {
          "ever, on the Customer Service Live Chats, Table 7": "no emotion\n1.00\n1.00\n1.00",
          "optimizer that does not take the order into account.": ""
        },
        {
          "ever, on the Customer Service Live Chats, Table 7": "surprise\n0.06\n0.10\n0.07",
          "optimizer that does not take the order into account.": ""
        },
        {
          "ever, on the Customer Service Live Chats, Table 7": "",
          "optimizer that does not take the order into account.": "Future work will be dedicated to the improve-"
        },
        {
          "ever, on the Customer Service Live Chats, Table 7": "amusement\n0.12\n0.54\n0.20",
          "optimizer that does not take the order into account.": ""
        },
        {
          "ever, on the Customer Service Live Chats, Table 7": "",
          "optimizer that does not take the order into account.": "ment of\nthe current\nfew-shot ERC approach by"
        },
        {
          "ever, on the Customer Service Live Chats, Table 7": "satisfaction\n0.47\n0.60\n0.53",
          "optimizer that does not take the order into account.": ""
        },
        {
          "ever, on the Customer Service Live Chats, Table 7": "relief\n0.21\n0.23\n0.22",
          "optimizer that does not take the order into account.": "adding unlabeled elements in the support set and"
        },
        {
          "ever, on the Customer Service Live Chats, Table 7": "neutral\n0.92\n0.79\n0.85",
          "optimizer that does not take the order into account.": ""
        },
        {
          "ever, on the Customer Service Live Chats, Table 7": "",
          "optimizer that does not take the order into account.": "by investigating the addition of external business"
        },
        {
          "ever, on the Customer Service Live Chats, Table 7": "fear*\n0.02\n0.01\n0.01",
          "optimizer that does not take the order into account.": ""
        },
        {
          "ever, on the Customer Service Live Chats, Table 7": "",
          "optimizer that does not take the order into account.": "knowledge to such an approach."
        },
        {
          "ever, on the Customer Service Live Chats, Table 7": "sadness\n0.08\n0.18\n0.11",
          "optimizer that does not take the order into account.": ""
        },
        {
          "ever, on the Customer Service Live Chats, Table 7": "disappointment\n0.03\n0.07\n0.04",
          "optimizer that does not take the order into account.": ""
        },
        {
          "ever, on the Customer Service Live Chats, Table 7": "anger\n0.02\n0.40\n0.03",
          "optimizer that does not take the order into account.": ""
        },
        {
          "ever, on the Customer Service Live Chats, Table 7": "",
          "optimizer that does not take the order into account.": "Acknowledgements"
        },
        {
          "ever, on the Customer Service Live Chats, Table 7": "frustration\n0.45\n0.43\n0.44",
          "optimizer that does not take the order into account.": ""
        },
        {
          "ever, on the Customer Service Live Chats, Table 7": "",
          "optimizer that does not take the order into account.": "This project has received funding from SNCF, the"
        },
        {
          "ever, on the Customer Service Live Chats, Table 7": "Table 7: Additional\nresults on customer\nservice live",
          "optimizer that does not take the order into account.": ""
        },
        {
          "ever, on the Customer Service Live Chats, Table 7": "",
          "optimizer that does not take the order into account.": "French National Research Agency’s grant ANR-17-"
        },
        {
          "ever, on the Customer Service Live Chats, Table 7": "chats with our ProtoSeq prediction.\nWe deﬁne\nthe",
          "optimizer that does not take the order into account.": ""
        },
        {
          "ever, on the Customer Service Live Chats, Table 7": "",
          "optimizer that does not take the order into account.": "MAOI and the DSAIDIS chair at Télécom-Paris."
        },
        {
          "ever, on the Customer Service Live Chats, Table 7": "\"fear\" label as \"fear/anxiety/stress\".\n\"no emotion\" is",
          "optimizer that does not take the order into account.": ""
        },
        {
          "ever, on the Customer Service Live Chats, Table 7": "only used for automatic chat prompts.",
          "optimizer that does not take the order into account.": ""
        },
        {
          "ever, on the Customer Service Live Chats, Table 7": "",
          "optimizer that does not take the order into account.": "References"
        },
        {
          "ever, on the Customer Service Live Chats, Table 7": "6\nLimitations",
          "optimizer that does not take the order into account.": ""
        },
        {
          "ever, on the Customer Service Live Chats, Table 7": "",
          "optimizer that does not take the order into account.": "Maruan Al-Shedivat, Liam Li, Eric Xing, and Ameet"
        },
        {
          "ever, on the Customer Service Live Chats, Table 7": "",
          "optimizer that does not take the order into account.": "Talwalkar.\n2021.\nOn\ndata\nefﬁciency\nof meta-"
        },
        {
          "ever, on the Customer Service Live Chats, Table 7": "While the ProtoSeq model seems to be suitable",
          "optimizer that does not take the order into account.": ""
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Firoj Alam. 2017. Computational Models for Analyz-": "ing Affective Behavior and Personality from Speech",
          "Tianyu Gao, Xu Han, Zhiyuan Liu, and Maosong Sun.": "2019. Hybrid attention-based prototypical networks"
        },
        {
          "Firoj Alam. 2017. Computational Models for Analyz-": "and Text. Ph.D. thesis, DIT - University of Trento.",
          "Tianyu Gao, Xu Han, Zhiyuan Liu, and Maosong Sun.": "for noisy few-shot\nrelation classiﬁcation.\nIn Pro-"
        },
        {
          "Firoj Alam. 2017. Computational Models for Analyz-": "",
          "Tianyu Gao, Xu Han, Zhiyuan Liu, and Maosong Sun.": "ceedings of\nthe AAAI Conference on Artiﬁcial Intel-"
        },
        {
          "Firoj Alam. 2017. Computational Models for Analyz-": "Firoj Alam, Morena Danieli, and Giuseppe Riccardi.",
          "Tianyu Gao, Xu Han, Zhiyuan Liu, and Maosong Sun.": "ligence, volume 33, pages 6407–6414."
        },
        {
          "Firoj Alam. 2017. Computational Models for Analyz-": "2018.\nAnnotating and modeling empathy in spo-",
          "Tianyu Gao, Xu Han, Zhiyuan Liu, and Maosong Sun.": ""
        },
        {
          "Firoj Alam. 2017. Computational Models for Analyz-": "",
          "Tianyu Gao, Xu Han, Zhiyuan Liu, and Maosong Sun.": "Deepanway Ghosal, Navonil Majumder, Soujanya Po-"
        },
        {
          "Firoj Alam. 2017. Computational Models for Analyz-": "ken conversations. Computer Speech & Language,",
          "Tianyu Gao, Xu Han, Zhiyuan Liu, and Maosong Sun.": ""
        },
        {
          "Firoj Alam. 2017. Computational Models for Analyz-": "",
          "Tianyu Gao, Xu Han, Zhiyuan Liu, and Maosong Sun.": "ria, Niyati Chhaya, and Alexander Gelbukh. 2019."
        },
        {
          "Firoj Alam. 2017. Computational Models for Analyz-": "50:40–61.",
          "Tianyu Gao, Xu Han, Zhiyuan Liu, and Maosong Sun.": ""
        },
        {
          "Firoj Alam. 2017. Computational Models for Analyz-": "",
          "Tianyu Gao, Xu Han, Zhiyuan Liu, and Maosong Sun.": "DialogueGCN: A graph convolutional neural net-"
        },
        {
          "Firoj Alam. 2017. Computational Models for Analyz-": "",
          "Tianyu Gao, Xu Han, Zhiyuan Liu, and Maosong Sun.": "work for emotion recognition in conversation.\nIn"
        },
        {
          "Firoj Alam. 2017. Computational Models for Analyz-": "Yujia Bao, Menghua Wu, Shiyu Chang,\nand Regina",
          "Tianyu Gao, Xu Han, Zhiyuan Liu, and Maosong Sun.": ""
        },
        {
          "Firoj Alam. 2017. Computational Models for Analyz-": "",
          "Tianyu Gao, Xu Han, Zhiyuan Liu, and Maosong Sun.": "Proceedings of\nthe 2019 Conference on Empirical"
        },
        {
          "Firoj Alam. 2017. Computational Models for Analyz-": "Barzilay. 2020. Few-shot text classiﬁcation with dis-",
          "Tianyu Gao, Xu Han, Zhiyuan Liu, and Maosong Sun.": ""
        },
        {
          "Firoj Alam. 2017. Computational Models for Analyz-": "",
          "Tianyu Gao, Xu Han, Zhiyuan Liu, and Maosong Sun.": "Methods\nin Natural Language Processing and the"
        },
        {
          "Firoj Alam. 2017. Computational Models for Analyz-": "tributional signatures.\nIn International Conference",
          "Tianyu Gao, Xu Han, Zhiyuan Liu, and Maosong Sun.": ""
        },
        {
          "Firoj Alam. 2017. Computational Models for Analyz-": "",
          "Tianyu Gao, Xu Han, Zhiyuan Liu, and Maosong Sun.": "9th International Joint Conference on Natural Lan-"
        },
        {
          "Firoj Alam. 2017. Computational Models for Analyz-": "on Learning Representations.",
          "Tianyu Gao, Xu Han, Zhiyuan Liu, and Maosong Sun.": ""
        },
        {
          "Firoj Alam. 2017. Computational Models for Analyz-": "",
          "Tianyu Gao, Xu Han, Zhiyuan Liu, and Maosong Sun.": "guage Processing\n(EMNLP-IJCNLP),\npages\n154–"
        },
        {
          "Firoj Alam. 2017. Computational Models for Analyz-": "",
          "Tianyu Gao, Xu Han, Zhiyuan Liu, and Maosong Sun.": "164, Hong Kong, China. Association for Computa-"
        },
        {
          "Firoj Alam. 2017. Computational Models for Analyz-": "Piotr Bojanowski, Edouard Grave, Armand Joulin, and",
          "Tianyu Gao, Xu Han, Zhiyuan Liu, and Maosong Sun.": ""
        },
        {
          "Firoj Alam. 2017. Computational Models for Analyz-": "",
          "Tianyu Gao, Xu Han, Zhiyuan Liu, and Maosong Sun.": "tional Linguistics."
        },
        {
          "Firoj Alam. 2017. Computational Models for Analyz-": "Tomas Mikolov. 2017. Enriching word vectors with",
          "Tianyu Gao, Xu Han, Zhiyuan Liu, and Maosong Sun.": ""
        },
        {
          "Firoj Alam. 2017. Computational Models for Analyz-": "subword information.",
          "Tianyu Gao, Xu Han, Zhiyuan Liu, and Maosong Sun.": ""
        },
        {
          "Firoj Alam. 2017. Computational Models for Analyz-": "",
          "Tianyu Gao, Xu Han, Zhiyuan Liu, and Maosong Sun.": "Xu Han, Hao Zhu, Pengfei Yu, Ziyun Wang, Yuan Yao,"
        },
        {
          "Firoj Alam. 2017. Computational Models for Analyz-": "",
          "Tianyu Gao, Xu Han, Zhiyuan Liu, and Maosong Sun.": "Zhiyuan Liu,\nand Maosong Sun. 2018.\nFewRel:"
        },
        {
          "Firoj Alam. 2017. Computational Models for Analyz-": "Carlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe",
          "Tianyu Gao, Xu Han, Zhiyuan Liu, and Maosong Sun.": ""
        },
        {
          "Firoj Alam. 2017. Computational Models for Analyz-": "",
          "Tianyu Gao, Xu Han, Zhiyuan Liu, and Maosong Sun.": "A Large-Scale Supervised Few-Shot Relation Clas-"
        },
        {
          "Firoj Alam. 2017. Computational Models for Analyz-": "Kazemzadeh,\nEmily Mower,\nSamuel Kim,\nJean-",
          "Tianyu Gao, Xu Han, Zhiyuan Liu, and Maosong Sun.": ""
        },
        {
          "Firoj Alam. 2017. Computational Models for Analyz-": "",
          "Tianyu Gao, Xu Han, Zhiyuan Liu, and Maosong Sun.": "siﬁcation Dataset with State-of-the-Art Evaluation."
        },
        {
          "Firoj Alam. 2017. Computational Models for Analyz-": "nette N Chang,\nSungbok Lee,\nand\nShrikanth\nS",
          "Tianyu Gao, Xu Han, Zhiyuan Liu, and Maosong Sun.": ""
        },
        {
          "Firoj Alam. 2017. Computational Models for Analyz-": "",
          "Tianyu Gao, Xu Han, Zhiyuan Liu, and Maosong Sun.": "arXiv:1810.10147 [cs, stat]. ArXiv: 1810.10147."
        },
        {
          "Firoj Alam. 2017. Computational Models for Analyz-": "Narayanan. 2008.\nIemocap:\nInteractive emotional",
          "Tianyu Gao, Xu Han, Zhiyuan Liu, and Maosong Sun.": ""
        },
        {
          "Firoj Alam. 2017. Computational Models for Analyz-": "Language\nre-\ndyadic motion\ncapture\ndatabase.",
          "Tianyu Gao, Xu Han, Zhiyuan Liu, and Maosong Sun.": ""
        },
        {
          "Firoj Alam. 2017. Computational Models for Analyz-": "",
          "Tianyu Gao, Xu Han, Zhiyuan Liu, and Maosong Sun.": "Devamanyu Hazarika, Soujanya Poria, Amir Zadeh,"
        },
        {
          "Firoj Alam. 2017. Computational Models for Analyz-": "sources and evaluation, 42(4):335–359.",
          "Tianyu Gao, Xu Han, Zhiyuan Liu, and Maosong Sun.": ""
        },
        {
          "Firoj Alam. 2017. Computational Models for Analyz-": "",
          "Tianyu Gao, Xu Han, Zhiyuan Liu, and Maosong Sun.": "Erik Cambria, Louis-Philippe Morency, and Roger"
        },
        {
          "Firoj Alam. 2017. Computational Models for Analyz-": "",
          "Tianyu Gao, Xu Han, Zhiyuan Liu, and Maosong Sun.": "Zimmermann. 2018.\nConversational memory net-"
        },
        {
          "Firoj Alam. 2017. Computational Models for Analyz-": "Kyunghyun Cho, Bart Van Merriënboer, Caglar Gul-",
          "Tianyu Gao, Xu Han, Zhiyuan Liu, and Maosong Sun.": ""
        },
        {
          "Firoj Alam. 2017. Computational Models for Analyz-": "",
          "Tianyu Gao, Xu Han, Zhiyuan Liu, and Maosong Sun.": "work for\nemotion recognition in dyadic dialogue"
        },
        {
          "Firoj Alam. 2017. Computational Models for Analyz-": "cehre, Dzmitry Bahdanau, Fethi Bougares, Holger",
          "Tianyu Gao, Xu Han, Zhiyuan Liu, and Maosong Sun.": ""
        },
        {
          "Firoj Alam. 2017. Computational Models for Analyz-": "",
          "Tianyu Gao, Xu Han, Zhiyuan Liu, and Maosong Sun.": "the conference. Associa-\nvideos.\nIn Proceedings of"
        },
        {
          "Firoj Alam. 2017. Computational Models for Analyz-": "Schwenk,\nand Yoshua Bengio.\n2014.\nLearning",
          "Tianyu Gao, Xu Han, Zhiyuan Liu, and Maosong Sun.": ""
        },
        {
          "Firoj Alam. 2017. Computational Models for Analyz-": "",
          "Tianyu Gao, Xu Han, Zhiyuan Liu, and Maosong Sun.": "tion for Computational Linguistics. North American"
        },
        {
          "Firoj Alam. 2017. Computational Models for Analyz-": "phrase\nrepresentations\nusing\nrnn\nencoder-decoder",
          "Tianyu Gao, Xu Han, Zhiyuan Liu, and Maosong Sun.": ""
        },
        {
          "Firoj Alam. 2017. Computational Models for Analyz-": "",
          "Tianyu Gao, Xu Han, Zhiyuan Liu, and Maosong Sun.": "Chapter. Meeting, volume 2018, page 2122. NIH"
        },
        {
          "Firoj Alam. 2017. Computational Models for Analyz-": "arXiv preprint\nfor\nstatistical machine translation.",
          "Tianyu Gao, Xu Han, Zhiyuan Liu, and Maosong Sun.": ""
        },
        {
          "Firoj Alam. 2017. Computational Models for Analyz-": "",
          "Tianyu Gao, Xu Han, Zhiyuan Liu, and Maosong Sun.": "Public Access."
        },
        {
          "Firoj Alam. 2017. Computational Models for Analyz-": "arXiv:1406.1078.",
          "Tianyu Gao, Xu Han, Zhiyuan Liu, and Maosong Sun.": ""
        },
        {
          "Firoj Alam. 2017. Computational Models for Analyz-": "",
          "Tianyu Gao, Xu Han, Zhiyuan Liu, and Maosong Sun.": "Yutai Hou, Wanxiang Che, Yongkui Lai, Zhihan Zhou,"
        },
        {
          "Firoj Alam. 2017. Computational Models for Analyz-": "Shammur Absar Chowdhury,\nEvgeny A Stepanov,",
          "Tianyu Gao, Xu Han, Zhiyuan Liu, and Maosong Sun.": "Yijia Liu, Han Liu,\nand Ting Liu.\n2020.\nFew-"
        },
        {
          "Firoj Alam. 2017. Computational Models for Analyz-": "Giuseppe Riccardi, et al. 2016. Predicting user satis-",
          "Tianyu Gao, Xu Han, Zhiyuan Liu, and Maosong Sun.": "shot slot\ntagging with collapsed dependency trans-"
        },
        {
          "Firoj Alam. 2017. Computational Models for Analyz-": "faction from turn-taking in spoken conversations.\nIn",
          "Tianyu Gao, Xu Han, Zhiyuan Liu, and Maosong Sun.": "fer and label-enhanced task-adaptive projection net-"
        },
        {
          "Firoj Alam. 2017. Computational Models for Analyz-": "Interspeech, pages 2910–2914.",
          "Tianyu Gao, Xu Han, Zhiyuan Liu, and Maosong Sun.": "work. arXiv preprint arXiv:2006.05702."
        },
        {
          "Firoj Alam. 2017. Computational Models for Analyz-": "Charlotte Danesi and Chloé Clavel. 2010.\nImpact of",
          "Tianyu Gao, Xu Han, Zhiyuan Liu, and Maosong Sun.": "Zhiheng Huang, Wei Xu, and Kai Yu. 2015. Bidirec-"
        },
        {
          "Firoj Alam. 2017. Computational Models for Analyz-": "spontaneous speech features on business concept de-",
          "Tianyu Gao, Xu Han, Zhiyuan Liu, and Maosong Sun.": "tional lstm-crf models for sequence tagging."
        },
        {
          "Firoj Alam. 2017. Computational Models for Analyz-": "tection: a study of call-centre data.\nIn Proceedings",
          "Tianyu Gao, Xu Han, Zhiyuan Liu, and Maosong Sun.": ""
        },
        {
          "Firoj Alam. 2017. Computational Models for Analyz-": "",
          "Tianyu Gao, Xu Han, Zhiyuan Liu, and Maosong Sun.": "Bei Hui, Liang Liu,\nJia Chen, Xue Zhou, and Yuhui"
        },
        {
          "Firoj Alam. 2017. Computational Models for Analyz-": "of\nthe 2010 international workshop on Searching",
          "Tianyu Gao, Xu Han, Zhiyuan Liu, and Maosong Sun.": ""
        },
        {
          "Firoj Alam. 2017. Computational Models for Analyz-": "",
          "Tianyu Gao, Xu Han, Zhiyuan Liu, and Maosong Sun.": "Nian. 2020. Few-shot relation classiﬁcation by con-"
        },
        {
          "Firoj Alam. 2017. Computational Models for Analyz-": "spontaneous conversational speech, pages 11–14.",
          "Tianyu Gao, Xu Han, Zhiyuan Liu, and Maosong Sun.": ""
        },
        {
          "Firoj Alam. 2017. Computational Models for Analyz-": "",
          "Tianyu Gao, Xu Han, Zhiyuan Liu, and Maosong Sun.": "text attention-based prototypical networks with bert."
        },
        {
          "Firoj Alam. 2017. Computational Models for Analyz-": "",
          "Tianyu Gao, Xu Han, Zhiyuan Liu, and Maosong Sun.": "EURASIP Journal on Wireless Communications and"
        },
        {
          "Firoj Alam. 2017. Computational Models for Analyz-": "Jacob Devlin, Ming-Wei Chang, Kenton Lee,\nand",
          "Tianyu Gao, Xu Han, Zhiyuan Liu, and Maosong Sun.": ""
        },
        {
          "Firoj Alam. 2017. Computational Models for Analyz-": "",
          "Tianyu Gao, Xu Han, Zhiyuan Liu, and Maosong Sun.": "Networking, 2020:1–17."
        },
        {
          "Firoj Alam. 2017. Computational Models for Analyz-": "Kristina Toutanova. 2019.\nBERT: Pre-training of",
          "Tianyu Gao, Xu Han, Zhiyuan Liu, and Maosong Sun.": ""
        },
        {
          "Firoj Alam. 2017. Computational Models for Analyz-": "deep bidirectional\ntransformers for language under-",
          "Tianyu Gao, Xu Han, Zhiyuan Liu, and Maosong Sun.": ""
        },
        {
          "Firoj Alam. 2017. Computational Models for Analyz-": "",
          "Tianyu Gao, Xu Han, Zhiyuan Liu, and Maosong Sun.": "Michael\nI Jordan. 1997.\nSerial order: A parallel dis-"
        },
        {
          "Firoj Alam. 2017. Computational Models for Analyz-": "the 2019 Conference\nstanding.\nIn Proceedings of",
          "Tianyu Gao, Xu Han, Zhiyuan Liu, and Maosong Sun.": ""
        },
        {
          "Firoj Alam. 2017. Computational Models for Analyz-": "",
          "Tianyu Gao, Xu Han, Zhiyuan Liu, and Maosong Sun.": "tributed processing approach.\nIn Advances in psy-"
        },
        {
          "Firoj Alam. 2017. Computational Models for Analyz-": "of\nthe North American Chapter of\nthe Association",
          "Tianyu Gao, Xu Han, Zhiyuan Liu, and Maosong Sun.": ""
        },
        {
          "Firoj Alam. 2017. Computational Models for Analyz-": "",
          "Tianyu Gao, Xu Han, Zhiyuan Liu, and Maosong Sun.": "chology, volume 121, pages 471–495. Elsevier."
        },
        {
          "Firoj Alam. 2017. Computational Models for Analyz-": "for Computational Linguistics: Human Language",
          "Tianyu Gao, Xu Han, Zhiyuan Liu, and Maosong Sun.": ""
        },
        {
          "Firoj Alam. 2017. Computational Models for Analyz-": "Technologies, Volume 1 (Long and Short Papers),",
          "Tianyu Gao, Xu Han, Zhiyuan Liu, and Maosong Sun.": ""
        },
        {
          "Firoj Alam. 2017. Computational Models for Analyz-": "",
          "Tianyu Gao, Xu Han, Zhiyuan Liu, and Maosong Sun.": "Yoon\nKim.\n2014.\nConvolutional\nneural\nnet-"
        },
        {
          "Firoj Alam. 2017. Computational Models for Analyz-": "pages 4171–4186, Minneapolis, Minnesota. Associ-",
          "Tianyu Gao, Xu Han, Zhiyuan Liu, and Maosong Sun.": ""
        },
        {
          "Firoj Alam. 2017. Computational Models for Analyz-": "",
          "Tianyu Gao, Xu Han, Zhiyuan Liu, and Maosong Sun.": "arXiv preprint\nworks\nfor\nsentence\nclassiﬁcation."
        },
        {
          "Firoj Alam. 2017. Computational Models for Analyz-": "ation for Computational Linguistics.",
          "Tianyu Gao, Xu Han, Zhiyuan Liu, and Maosong Sun.": ""
        },
        {
          "Firoj Alam. 2017. Computational Models for Analyz-": "",
          "Tianyu Gao, Xu Han, Zhiyuan Liu, and Maosong Sun.": "arXiv:1408.5882."
        },
        {
          "Firoj Alam. 2017. Computational Models for Analyz-": "Li Fei-Fei, Rob Fergus, and Pietro Perona. 2006. One-",
          "Tianyu Gao, Xu Han, Zhiyuan Liu, and Maosong Sun.": ""
        },
        {
          "Firoj Alam. 2017. Computational Models for Analyz-": "",
          "Tianyu Gao, Xu Han, Zhiyuan Liu, and Maosong Sun.": "Diederik P. Kingma and Jimmy Ba. 2017. Adam: A"
        },
        {
          "Firoj Alam. 2017. Computational Models for Analyz-": "IEEE transac-\nshot\nlearning of object categories.",
          "Tianyu Gao, Xu Han, Zhiyuan Liu, and Maosong Sun.": ""
        },
        {
          "Firoj Alam. 2017. Computational Models for Analyz-": "",
          "Tianyu Gao, Xu Han, Zhiyuan Liu, and Maosong Sun.": "method for stochastic optimization."
        },
        {
          "Firoj Alam. 2017. Computational Models for Analyz-": "tions on pattern analysis and machine intelligence,",
          "Tianyu Gao, Xu Han, Zhiyuan Liu, and Maosong Sun.": ""
        },
        {
          "Firoj Alam. 2017. Computational Models for Analyz-": "28(4):594–611.",
          "Tianyu Gao, Xu Han, Zhiyuan Liu, and Maosong Sun.": ""
        },
        {
          "Firoj Alam. 2017. Computational Models for Analyz-": "",
          "Tianyu Gao, Xu Han, Zhiyuan Liu, and Maosong Sun.": "Gregory Koch, Richard Zemel, and Ruslan Salakhutdi-"
        },
        {
          "Firoj Alam. 2017. Computational Models for Analyz-": "",
          "Tianyu Gao, Xu Han, Zhiyuan Liu, and Maosong Sun.": "nov. 2015.\nSiamese Neural Networks for One-shot"
        },
        {
          "Firoj Alam. 2017. Computational Models for Analyz-": "Alexander Fritzler, Varvara Logacheva,\nand Maksim",
          "Tianyu Gao, Xu Han, Zhiyuan Liu, and Maosong Sun.": "Image Recognition.\nICML, page 8."
        },
        {
          "Firoj Alam. 2017. Computational Models for Analyz-": "Kretov. 2019.\nFew-shot classiﬁcation in named en-",
          "Tianyu Gao, Xu Han, Zhiyuan Liu, and Maosong Sun.": ""
        },
        {
          "Firoj Alam. 2017. Computational Models for Analyz-": "the 34th\ntity recognition task.\nIn Proceedings of",
          "Tianyu Gao, Xu Han, Zhiyuan Liu, and Maosong Sun.": "Nithin Rao Koluguri, Manoj Kumar, So Hyun Kim,"
        },
        {
          "Firoj Alam. 2017. Computational Models for Analyz-": "ACM/SIGAPP Symposium on Applied Computing,",
          "Tianyu Gao, Xu Han, Zhiyuan Liu, and Maosong Sun.": "Catherine Lord,\nand Shrikanth Narayanan.\n2020."
        },
        {
          "Firoj Alam. 2017. Computational Models for Analyz-": "pages 993–1000.",
          "Tianyu Gao, Xu Han, Zhiyuan Liu, and Maosong Sun.": "Meta-learning for\nrobust\nchild-adult\nclassiﬁcation"
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "from speech.\nIn ICASSP 2020-2020 IEEE Interna-": "tional Conference on Acoustics, Speech and Signal",
          "Sachin Ravi and Hugo Larochelle. 2016. Optimization": "as a model for few-shot learning. OpenReview."
        },
        {
          "from speech.\nIn ICASSP 2020-2020 IEEE Interna-": "Processing (ICASSP), pages 8094–8098. IEEE.",
          "Sachin Ravi and Hugo Larochelle. 2016. Optimization": ""
        },
        {
          "from speech.\nIn ICASSP 2020-2020 IEEE Interna-": "",
          "Sachin Ravi and Hugo Larochelle. 2016. Optimization": "Haopeng Ren, Yi Cai, Xiaofeng Chen, Guohua Wang,"
        },
        {
          "from speech.\nIn ICASSP 2020-2020 IEEE Interna-": "Brenden\nLake.\n2015.\nLakeEtAl2015Science-",
          "Sachin Ravi and Hugo Larochelle. 2016. Optimization": "and Qing Li. 2020.\nA two-phase prototypical net-"
        },
        {
          "from speech.\nIn ICASSP 2020-2020 IEEE Interna-": "startOfFewShot.pdf. Sciences Mag.",
          "Sachin Ravi and Hugo Larochelle. 2016. Optimization": "work model for incremental few-shot relation classi-"
        },
        {
          "from speech.\nIn ICASSP 2020-2020 IEEE Interna-": "",
          "Sachin Ravi and Hugo Larochelle. 2016. Optimization": "the 28th International\nﬁcation.\nIn Proceedings of"
        },
        {
          "from speech.\nIn ICASSP 2020-2020 IEEE Interna-": "J Richard Landis and Gary G Koch. 1977. An appli-",
          "Sachin Ravi and Hugo Larochelle. 2016. Optimization": "Conference\non Computational\nLinguistics,\npages"
        },
        {
          "from speech.\nIn ICASSP 2020-2020 IEEE Interna-": "cation of hierarchical kappa-type statistics in the as-",
          "Sachin Ravi and Hugo Larochelle. 2016. Optimization": "1618–1629."
        },
        {
          "from speech.\nIn ICASSP 2020-2020 IEEE Interna-": "sessment of majority agreement among multiple ob-",
          "Sachin Ravi and Hugo Larochelle. 2016. Optimization": ""
        },
        {
          "from speech.\nIn ICASSP 2020-2020 IEEE Interna-": "servers. Biometrics, pages 363–374.",
          "Sachin Ravi and Hugo Larochelle. 2016. Optimization": "Esteban Safranchik, Shiying Luo, and Stephen Bach."
        },
        {
          "from speech.\nIn ICASSP 2020-2020 IEEE Interna-": "",
          "Sachin Ravi and Hugo Larochelle. 2016. Optimization": "2020. Weakly supervised sequence tagging from"
        },
        {
          "from speech.\nIn ICASSP 2020-2020 IEEE Interna-": "Yanran Li, Hui Su, Xiaoyu Shen, Wenjie Li, Ziqiang",
          "Sachin Ravi and Hugo Larochelle. 2016. Optimization": "noisy rules.\nIn Proceedings of the AAAI Conference"
        },
        {
          "from speech.\nIn ICASSP 2020-2020 IEEE Interna-": "Cao,\nand\nShuzi Niu.\n2017.\nDailyDialog:\nA",
          "Sachin Ravi and Hugo Larochelle. 2016. Optimization": "on Artiﬁcial\nIntelligence, volume 34, pages 5570–"
        },
        {
          "from speech.\nIn ICASSP 2020-2020 IEEE Interna-": "Manually\nLabelled Multi-turn Dialogue Dataset.",
          "Sachin Ravi and Hugo Larochelle. 2016. Optimization": "5578."
        },
        {
          "from speech.\nIn ICASSP 2020-2020 IEEE Interna-": "arXiv:1710.03957 [cs]. ArXiv: 1710.03957.",
          "Sachin Ravi and Hugo Larochelle. 2016. Optimization": ""
        },
        {
          "from speech.\nIn ICASSP 2020-2020 IEEE Interna-": "",
          "Sachin Ravi and Hugo Larochelle. 2016. Optimization": "Jake Snell, Kevin Swersky, and Richard Zemel. 2017."
        },
        {
          "from speech.\nIn ICASSP 2020-2020 IEEE Interna-": "Ilya Loshchilov and Frank Hutter. 2019.\nDecoupled",
          "Sachin Ravi and Hugo Larochelle. 2016. Optimization": "Prototypical networks for few-shot\nlearning.\nIn Ad-"
        },
        {
          "from speech.\nIn ICASSP 2020-2020 IEEE Interna-": "weight decay regularization.",
          "Sachin Ravi and Hugo Larochelle. 2016. Optimization": "vances\nin\nneural\ninformation\nprocessing\nsystems,"
        },
        {
          "from speech.\nIn ICASSP 2020-2020 IEEE Interna-": "",
          "Sachin Ravi and Hugo Larochelle. 2016. Optimization": "pages 4077–4087."
        },
        {
          "from speech.\nIn ICASSP 2020-2020 IEEE Interna-": "Navonil Majumder, Soujanya Poria, Devamanyu Haz-",
          "Sachin Ravi and Hugo Larochelle. 2016. Optimization": ""
        },
        {
          "from speech.\nIn ICASSP 2020-2020 IEEE Interna-": "arika, Rada Mihalcea, Alexander Gelbukh, and Erik",
          "Sachin Ravi and Hugo Larochelle. 2016. Optimization": "Robyn Speer, Joshua Chin, and Catherine Havasi. 2017."
        },
        {
          "from speech.\nIn ICASSP 2020-2020 IEEE Interna-": "Cambria. 2019. DialogueRNN: An Attentive RNN",
          "Sachin Ravi and Hugo Larochelle. 2016. Optimization": "Conceptnet 5.5: An open multilingual graph of gen-"
        },
        {
          "from speech.\nIn ICASSP 2020-2020 IEEE Interna-": "Proceed-\nfor Emotion Detection in Conversations.",
          "Sachin Ravi and Hugo Larochelle. 2016. Optimization": "eral knowledge.\nIn Thirty-ﬁrst AAAI conference on"
        },
        {
          "from speech.\nIn ICASSP 2020-2020 IEEE Interna-": "ings of\nthe AAAI Conference on Artiﬁcial\nIntelli-",
          "Sachin Ravi and Hugo Larochelle. 2016. Optimization": "artiﬁcial intelligence."
        },
        {
          "from speech.\nIn ICASSP 2020-2020 IEEE Interna-": "gence, 33:6818–6825.",
          "Sachin Ravi and Hugo Larochelle. 2016. Optimization": ""
        },
        {
          "from speech.\nIn ICASSP 2020-2020 IEEE Interna-": "",
          "Sachin Ravi and Hugo Larochelle. 2016. Optimization": "Flood Sung, Yongxin Yang, Li Zhang, Tao Xiang,"
        },
        {
          "from speech.\nIn ICASSP 2020-2020 IEEE Interna-": "Irina Maslowski, Delphine Lagarde, and Chloé Clavel.",
          "Sachin Ravi and Hugo Larochelle. 2016. Optimization": "Philip HS Torr, and Timothy M Hospedales. 2018."
        },
        {
          "from speech.\nIn ICASSP 2020-2020 IEEE Interna-": "2017.\nIn-the-wild chatbot\ncorpus:\nfrom opinion",
          "Sachin Ravi and Hugo Larochelle. 2016. Optimization": "Learning to compare: Relation network for few-shot"
        },
        {
          "from speech.\nIn ICASSP 2020-2020 IEEE Interna-": "analysis\nto interaction problem detection.\nIn IC-",
          "Sachin Ravi and Hugo Larochelle. 2016. Optimization": "the IEEE Conference\nlearning.\nIn Proceedings of"
        },
        {
          "from speech.\nIn ICASSP 2020-2020 IEEE Interna-": "NLSSP 2017, pages 115–120.",
          "Sachin Ravi and Hugo Larochelle. 2016. Optimization": "on Computer Vision and Pattern Recognition, pages"
        },
        {
          "from speech.\nIn ICASSP 2020-2020 IEEE Interna-": "",
          "Sachin Ravi and Hugo Larochelle. 2016. Optimization": "1199–1208."
        },
        {
          "from speech.\nIn ICASSP 2020-2020 IEEE Interna-": "Erik G Miller, Nicholas E Matsakis, and Paul A Viola.",
          "Sachin Ravi and Hugo Larochelle. 2016. Optimization": ""
        },
        {
          "from speech.\nIn ICASSP 2020-2020 IEEE Interna-": "2000.\nLearning from one example through shared",
          "Sachin Ravi and Hugo Larochelle. 2016. Optimization": "Ashish Vaswani, Noam Shazeer, Niki Parmar,\nJakob"
        },
        {
          "from speech.\nIn ICASSP 2020-2020 IEEE Interna-": "densities on transforms.\nIn Proceedings IEEE Con-",
          "Sachin Ravi and Hugo Larochelle. 2016. Optimization": "Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz"
        },
        {
          "from speech.\nIn ICASSP 2020-2020 IEEE Interna-": "ference on Computer Vision and Pattern Recogni-",
          "Sachin Ravi and Hugo Larochelle. 2016. Optimization": "Kaiser, and Illia Polosukhin. 2017. Attention is all"
        },
        {
          "from speech.\nIn ICASSP 2020-2020 IEEE Interna-": "tion. CVPR 2000 (Cat. No. PR00662), volume 1,",
          "Sachin Ravi and Hugo Larochelle. 2016. Optimization": "information pro-\nyou need.\nIn Advances in neural"
        },
        {
          "from speech.\nIn ICASSP 2020-2020 IEEE Interna-": "pages 464–471. IEEE.",
          "Sachin Ravi and Hugo Larochelle. 2016. Optimization": "cessing systems, pages 5998–6008."
        },
        {
          "from speech.\nIn ICASSP 2020-2020 IEEE Interna-": "Shreshtha Mundra,\nAnirban\nSen, Manjira\nSinha,",
          "Sachin Ravi and Hugo Larochelle. 2016. Optimization": "Oriol\nVinyals,\nCharles\nBlundell,\nTimothy\nLilli-"
        },
        {
          "from speech.\nIn ICASSP 2020-2020 IEEE Interna-": "Sandya Mannarswamy,\nSandipan Dandapat,\nand",
          "Sachin Ravi and Hugo Larochelle. 2016. Optimization": "crap,\nKoray Kavukcuoglu,\nand Daan Wierstra."
        },
        {
          "from speech.\nIn ICASSP 2020-2020 IEEE Interna-": "Shourya Roy. 2017. Fine-grained emotion detection",
          "Sachin Ravi and Hugo Larochelle. 2016. Optimization": "2017. Matching Networks for One Shot Learning."
        },
        {
          "from speech.\nIn ICASSP 2020-2020 IEEE Interna-": "in contact center chat utterances.\nIn Paciﬁc-Asia",
          "Sachin Ravi and Hugo Larochelle. 2016. Optimization": "arXiv:1606.04080 [cs, stat]. ArXiv: 1606.04080."
        },
        {
          "from speech.\nIn ICASSP 2020-2020 IEEE Interna-": "Conference on Knowledge Discovery and Data Min-",
          "Sachin Ravi and Hugo Larochelle. 2016. Optimization": ""
        },
        {
          "from speech.\nIn ICASSP 2020-2020 IEEE Interna-": "ing, pages 337–349. Springer.",
          "Sachin Ravi and Hugo Larochelle. 2016. Optimization": "Yan Wang, Jiayu Zhang, Jun Ma, Shaojun Wang, and"
        },
        {
          "from speech.\nIn ICASSP 2020-2020 IEEE Interna-": "",
          "Sachin Ravi and Hugo Larochelle. 2016. Optimization": "Jing Xiao. 2020.\nContextualized emotion recogni-"
        },
        {
          "from speech.\nIn ICASSP 2020-2020 IEEE Interna-": "Jeffrey Pennington, Richard Socher, and Christopher D.",
          "Sachin Ravi and Hugo Larochelle. 2016. Optimization": "tion in conversation as\nsequence tagging.\nIn Pro-"
        },
        {
          "from speech.\nIn ICASSP 2020-2020 IEEE Interna-": "Manning. 2014. Glove: Global vectors for word rep-",
          "Sachin Ravi and Hugo Larochelle. 2016. Optimization": "ceedings of\nthe 21th Annual Meeting of\nthe Special"
        },
        {
          "from speech.\nIn ICASSP 2020-2020 IEEE Interna-": "resentation.\nIn Empirical Methods in Natural Lan-",
          "Sachin Ravi and Hugo Larochelle. 2016. Optimization": "Interest Group on Discourse and Dialogue, pages"
        },
        {
          "from speech.\nIn ICASSP 2020-2020 IEEE Interna-": "guage Processing (EMNLP), pages 1532–1543.",
          "Sachin Ravi and Hugo Larochelle. 2016. Optimization": "186–195, 1st virtual meeting. Association for Com-"
        },
        {
          "from speech.\nIn ICASSP 2020-2020 IEEE Interna-": "",
          "Sachin Ravi and Hugo Larochelle. 2016. Optimization": "putational Linguistics."
        },
        {
          "from speech.\nIn ICASSP 2020-2020 IEEE Interna-": "Tal Perl, Sriram Chaudhury,\nand Raja Giryes. 2020.",
          "Sachin Ravi and Hugo Larochelle. 2016. Optimization": ""
        },
        {
          "from speech.\nIn ICASSP 2020-2020 IEEE Interna-": "Low resource sequence tagging using sentence re-",
          "Sachin Ravi and Hugo Larochelle. 2016. Optimization": "Galit B Yom-Tov,\nShelly Ashtar,\nDaniel Altman,"
        },
        {
          "from speech.\nIn ICASSP 2020-2020 IEEE Interna-": "the 58th Annual\nconstruction.\nIn Proceedings of",
          "Sachin Ravi and Hugo Larochelle. 2016. Optimization": "Michael Natapov, Neta Barkay, Monika Westphal,"
        },
        {
          "from speech.\nIn ICASSP 2020-2020 IEEE Interna-": "Meeting of\nthe Association for Computational Lin-",
          "Sachin Ravi and Hugo Larochelle. 2016. Optimization": "and Anat Rafaeli. 2018. Customer sentiment in web-"
        },
        {
          "from speech.\nIn ICASSP 2020-2020 IEEE Interna-": "guistics, pages 2692–2698, Online. Association for",
          "Sachin Ravi and Hugo Larochelle. 2016. Optimization": "based service interactions: Automated analyses and"
        },
        {
          "from speech.\nIn ICASSP 2020-2020 IEEE Interna-": "Computational Linguistics.",
          "Sachin Ravi and Hugo Larochelle. 2016. Optimization": "new insights.\nIn Companion Proceedings of the The"
        },
        {
          "from speech.\nIn ICASSP 2020-2020 IEEE Interna-": "",
          "Sachin Ravi and Hugo Larochelle. 2016. Optimization": "Web Conference 2018, pages 1689–1697."
        },
        {
          "from speech.\nIn ICASSP 2020-2020 IEEE Interna-": "Soujanya Poria, Erik Cambria, Devamanyu Hazarika,",
          "Sachin Ravi and Hugo Larochelle. 2016. Optimization": ""
        },
        {
          "from speech.\nIn ICASSP 2020-2020 IEEE Interna-": "Navonil Majumder, Amir Zadeh, and Louis-Philippe",
          "Sachin Ravi and Hugo Larochelle. 2016. Optimization": "Amir\nZadeh,\nPaul\nPu\nLiang,\nNavonil Mazumder,"
        },
        {
          "from speech.\nIn ICASSP 2020-2020 IEEE Interna-": "Morency. 2017. Context-dependent sentiment anal-",
          "Sachin Ravi and Hugo Larochelle. 2016. Optimization": "Soujanya Poria, Erik Cambria, and Louis-Philippe"
        },
        {
          "from speech.\nIn ICASSP 2020-2020 IEEE Interna-": "ysis in user-generated videos.\nIn Proceedings of the",
          "Sachin Ravi and Hugo Larochelle. 2016. Optimization": "Morency.\n2018a.\nMemory\nfusion\nnetwork\nfor"
        },
        {
          "from speech.\nIn ICASSP 2020-2020 IEEE Interna-": "55th annual meeting of\nthe association for compu-",
          "Sachin Ravi and Hugo Larochelle. 2016. Optimization": "multi-view sequential\nlearning.\nIn Proceedings of"
        },
        {
          "from speech.\nIn ICASSP 2020-2020 IEEE Interna-": "tational\nlinguistics (volume 1: Long papers), pages",
          "Sachin Ravi and Hugo Larochelle. 2016. Optimization": "the AAAI Conference on Artiﬁcial Intelligence, vol-"
        },
        {
          "from speech.\nIn ICASSP 2020-2020 IEEE Interna-": "873–883.",
          "Sachin Ravi and Hugo Larochelle. 2016. Optimization": "ume 32."
        }
      ],
      "page": 11
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Amir Zadeh,\nPaul Pu Liang,\nSoujanya Poria,\nPra-": "teek Vij, Erik Cambria, and Louis-Philippe Morency."
        },
        {
          "Amir Zadeh,\nPaul Pu Liang,\nSoujanya Poria,\nPra-": "2018b. Multi-attention recurrent network for human"
        },
        {
          "Amir Zadeh,\nPaul Pu Liang,\nSoujanya Poria,\nPra-": "communication comprehension.\nIn Proceedings of"
        },
        {
          "Amir Zadeh,\nPaul Pu Liang,\nSoujanya Poria,\nPra-": "the AAAI Conference on Artiﬁcial Intelligence, vol-"
        },
        {
          "Amir Zadeh,\nPaul Pu Liang,\nSoujanya Poria,\nPra-": "ume 32."
        },
        {
          "Amir Zadeh,\nPaul Pu Liang,\nSoujanya Poria,\nPra-": "Peixiang Zhong, Di Wang, and Chunyan Miao. 2019."
        },
        {
          "Amir Zadeh,\nPaul Pu Liang,\nSoujanya Poria,\nPra-": "Knowledge-Enriched Transformer\nfor Emotion De-"
        },
        {
          "Amir Zadeh,\nPaul Pu Liang,\nSoujanya Poria,\nPra-": "tection in Textual Conversations. arXiv:1909.10681"
        },
        {
          "Amir Zadeh,\nPaul Pu Liang,\nSoujanya Poria,\nPra-": "[cs]. ArXiv: 1909.10681."
        }
      ],
      "page": 12
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Figure 3: Pearson correlation scores between the vis-": "satisfaction\nscore\nin\nthe\nconversation"
        },
        {
          "Figure 3: Pearson correlation scores between the vis-": "speciﬁc emotions\nin the"
        },
        {
          "Figure 3: Pearson correlation scores between the vis-": ""
        },
        {
          "Figure 3: Pearson correlation scores between the vis-": "Figure 3 presents the Pearson correlation scores"
        },
        {
          "Figure 3: Pearson correlation scores between the vis-": "between visitor’s emotions and satisfaction for the"
        },
        {
          "Figure 3: Pearson correlation scores between the vis-": "Customer Service Live Chats. While emotions are"
        },
        {
          "Figure 3: Pearson correlation scores between the vis-": "labeled for each utterance in conversation, satisfac-"
        },
        {
          "Figure 3: Pearson correlation scores between the vis-": "label for the whole conversation."
        },
        {
          "Figure 3: Pearson correlation scores between the vis-": "This Figure shows the correlation scores are higher"
        },
        {
          "Figure 3: Pearson correlation scores between the vis-": "when the emotion is extreme within a given polar-"
        },
        {
          "Figure 3: Pearson correlation scores between the vis-": "ity. For instance, anger is greatly correlated to a"
        },
        {
          "Figure 3: Pearson correlation scores between the vis-": "negative satisfaction score (vsent -3) than fear or"
        },
        {
          "Figure 3: Pearson correlation scores between the vis-": "disappointment, while “Satisfaction” is more corre-"
        },
        {
          "Figure 3: Pearson correlation scores between the vis-": "lated to a positive overall satisfaction score (vsent"
        },
        {
          "Figure 3: Pearson correlation scores between the vis-": ""
        },
        {
          "Figure 3: Pearson correlation scores between the vis-": ""
        }
      ],
      "page": 13
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "On data efficiency of metalearning",
      "authors": [
        "Maruan Al-Shedivat",
        "Liam Li"
      ],
      "year": "2021",
      "venue": "Proceedings of The 24th International Conference on Artificial Intelligence and Statistics"
    },
    {
      "citation_id": "2",
      "title": "Computational Models for Analyzing Affective Behavior and Personality from Speech and Text",
      "authors": [
        "Firoj Alam"
      ],
      "year": "2017",
      "venue": "Computational Models for Analyzing Affective Behavior and Personality from Speech and Text"
    },
    {
      "citation_id": "3",
      "title": "Annotating and modeling empathy in spoken conversations",
      "authors": [
        "Firoj Alam",
        "Morena Danieli",
        "Giuseppe Riccardi"
      ],
      "year": "2018",
      "venue": "Computer Speech & Language"
    },
    {
      "citation_id": "4",
      "title": "Few-shot text classification with distributional signatures",
      "authors": [
        "Yujia Bao",
        "Menghua Wu",
        "Shiyu Chang",
        "Regina Barzilay"
      ],
      "year": "2020",
      "venue": "International Conference on Learning Representations"
    },
    {
      "citation_id": "5",
      "title": "Enriching word vectors with subword information",
      "authors": [
        "Piotr Bojanowski",
        "Edouard Grave",
        "Armand Joulin",
        "Tomas Mikolov"
      ],
      "year": "2017",
      "venue": "Enriching word vectors with subword information"
    },
    {
      "citation_id": "6",
      "title": "Iemocap: Interactive emotional dyadic motion capture database. Language resources and evaluation",
      "authors": [
        "Carlos Busso",
        "Murtaza Bulut",
        "Chi-Chun Lee",
        "Abe Kazemzadeh",
        "Emily Mower",
        "Samuel Kim",
        "Jeannette Chang",
        "Sungbok Lee",
        "Shrikanth S Narayanan"
      ],
      "year": "2008",
      "venue": "Iemocap: Interactive emotional dyadic motion capture database. Language resources and evaluation"
    },
    {
      "citation_id": "7",
      "title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation",
      "authors": [
        "Kyunghyun Cho",
        "Bart Van Merriënboer",
        "Caglar Gulcehre",
        "Dzmitry Bahdanau",
        "Fethi Bougares",
        "Holger Schwenk",
        "Yoshua Bengio"
      ],
      "year": "2014",
      "venue": "Learning phrase representations using rnn encoder-decoder for statistical machine translation",
      "arxiv": "arXiv:1406.1078"
    },
    {
      "citation_id": "8",
      "title": "Predicting user satisfaction from turn-taking in spoken conversations",
      "authors": [
        "Evgeny Shammur Absar Chowdhury",
        "Giuseppe Stepanov",
        "Riccardi"
      ],
      "year": "2016",
      "venue": "Interspeech"
    },
    {
      "citation_id": "9",
      "title": "Impact of spontaneous speech features on business concept detection: a study of call-centre data",
      "authors": [
        "Charlotte Danesi",
        "Chloé Clavel"
      ],
      "year": "2010",
      "venue": "Proceedings of the 2010 international workshop on Searching spontaneous conversational speech"
    },
    {
      "citation_id": "10",
      "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "Jacob Devlin",
        "Ming-Wei Chang",
        "Kenton Lee",
        "Kristina Toutanova"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
      "doi": "10.18653/v1/N19-1423"
    },
    {
      "citation_id": "11",
      "title": "Oneshot learning of object categories. IEEE transactions on pattern analysis and machine intelligence",
      "authors": [
        "Li Fei-Fei",
        "Rob Fergus",
        "Pietro Perona"
      ],
      "year": "2006",
      "venue": "Oneshot learning of object categories. IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "12",
      "title": "Few-shot classification in named entity recognition task",
      "authors": [
        "Alexander Fritzler",
        "Varvara Logacheva",
        "Maksim Kretov"
      ],
      "year": "2019",
      "venue": "Proceedings of the 34th ACM/SIGAPP Symposium on Applied Computing"
    },
    {
      "citation_id": "13",
      "title": "Hybrid attention-based prototypical networks for noisy few-shot relation classification",
      "authors": [
        "Tianyu Gao",
        "Xu Han",
        "Zhiyuan Liu",
        "Maosong Sun"
      ],
      "year": "2019",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "14",
      "title": "DialogueGCN: A graph convolutional neural network for emotion recognition in conversation",
      "authors": [
        "Deepanway Ghosal",
        "Navonil Majumder",
        "Soujanya Poria",
        "Niyati Chhaya",
        "Alexander Gelbukh"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
      "doi": "10.18653/v1/D19-1015"
    },
    {
      "citation_id": "15",
      "title": "FewRel: A Large-Scale Supervised Few-Shot Relation Classification Dataset with State-of-the-Art Evaluation",
      "authors": [
        "Xu Han",
        "Hao Zhu",
        "Pengfei Yu",
        "Ziyun Wang",
        "Yuan Yao",
        "Zhiyuan Liu",
        "Maosong Sun"
      ],
      "year": "2018",
      "venue": "FewRel: A Large-Scale Supervised Few-Shot Relation Classification Dataset with State-of-the-Art Evaluation",
      "arxiv": "arXiv:1810.10147"
    },
    {
      "citation_id": "16",
      "title": "Conversational memory network for emotion recognition in dyadic dialogue videos",
      "authors": [
        "Devamanyu Hazarika",
        "Soujanya Poria",
        "Amir Zadeh",
        "Erik Cambria",
        "Louis-Philippe Morency",
        "Roger Zimmermann"
      ],
      "year": "2018",
      "venue": "Proceedings of the conference"
    },
    {
      "citation_id": "17",
      "title": "Fewshot slot tagging with collapsed dependency transfer and label-enhanced task-adaptive projection network",
      "authors": [
        "Yutai Hou",
        "Wanxiang Che",
        "Yongkui Lai",
        "Zhihan Zhou",
        "Yijia Liu",
        "Han Liu",
        "Ting Liu"
      ],
      "year": "2020",
      "venue": "Fewshot slot tagging with collapsed dependency transfer and label-enhanced task-adaptive projection network",
      "arxiv": "arXiv:2006.05702"
    },
    {
      "citation_id": "18",
      "title": "Bidirectional lstm-crf models for sequence tagging",
      "authors": [
        "Zhiheng Huang",
        "Wei Xu",
        "Kai Yu"
      ],
      "year": "2015",
      "venue": "Bidirectional lstm-crf models for sequence tagging"
    },
    {
      "citation_id": "19",
      "title": "Few-shot relation classification by context attention-based prototypical networks with bert",
      "authors": [
        "Bei Hui",
        "Liang Liu",
        "Jia Chen",
        "Xue Zhou",
        "Yuhui Nian"
      ],
      "year": "2020",
      "venue": "EURASIP Journal on Wireless Communications and Networking"
    },
    {
      "citation_id": "20",
      "title": "Serial order: A parallel distributed processing approach",
      "authors": [
        "Jordan Michael"
      ],
      "year": "1997",
      "venue": "Advances in psychology"
    },
    {
      "citation_id": "21",
      "title": "Convolutional neural networks for sentence classification",
      "authors": [
        "Yoon Kim"
      ],
      "year": "2014",
      "venue": "Convolutional neural networks for sentence classification",
      "arxiv": "arXiv:1408.5882"
    },
    {
      "citation_id": "22",
      "title": "Adam: A method for stochastic optimization",
      "authors": [
        "P Diederik",
        "Jimmy Kingma",
        "Ba"
      ],
      "year": "2017",
      "venue": "Adam: A method for stochastic optimization"
    },
    {
      "citation_id": "23",
      "title": "Siamese Neural Networks for One-shot Image Recognition",
      "authors": [
        "Gregory Koch",
        "Richard Zemel",
        "Ruslan Salakhutdi"
      ],
      "year": "2015",
      "venue": "ICML"
    },
    {
      "citation_id": "24",
      "title": "",
      "authors": [
        "Nithin Rao Koluguri",
        "Manoj Kumar",
        "Hyun Kim",
        "Catherine Lord",
        "Shrikanth Narayanan"
      ],
      "year": "2020",
      "venue": ""
    },
    {
      "citation_id": "25",
      "title": "Meta-learning for robust child-adult classification from speech",
      "year": "2015",
      "venue": "ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "26",
      "title": "An application of hierarchical kappa-type statistics in the assessment of majority agreement among multiple observers",
      "authors": [
        "Richard Landis",
        "Gary Koch"
      ],
      "year": "1977",
      "venue": "Biometrics"
    },
    {
      "citation_id": "27",
      "title": "DailyDialog: A Manually Labelled Multi-turn Dialogue Dataset",
      "authors": [
        "Yanran Li",
        "Hui Su",
        "Xiaoyu Shen",
        "Wenjie Li",
        "Ziqiang Cao",
        "Shuzi Niu"
      ],
      "year": "2017",
      "venue": "DailyDialog: A Manually Labelled Multi-turn Dialogue Dataset",
      "arxiv": "arXiv:1710.03957[cs].ArXiv:1710.03957"
    },
    {
      "citation_id": "28",
      "title": "Decoupled weight decay regularization",
      "authors": [
        "Ilya Loshchilov",
        "Frank Hutter"
      ],
      "year": "2019",
      "venue": "Decoupled weight decay regularization"
    },
    {
      "citation_id": "29",
      "title": "DialogueRNN: An Attentive RNN for Emotion Detection in Conversations. Proceedings of the AAAI Conference on Artificial Intelligence",
      "authors": [
        "Navonil Majumder",
        "Soujanya Poria",
        "Devamanyu Hazarika",
        "Rada Mihalcea",
        "Alexander Gelbukh",
        "Erik Cambria"
      ],
      "year": "2019",
      "venue": "DialogueRNN: An Attentive RNN for Emotion Detection in Conversations. Proceedings of the AAAI Conference on Artificial Intelligence",
      "doi": "10.1609/aaai.v33i01.33016818"
    },
    {
      "citation_id": "30",
      "title": "In-the-wild chatbot corpus: from opinion analysis to interaction problem detection",
      "authors": [
        "Irina Maslowski",
        "Delphine Lagarde",
        "Chloé Clavel"
      ],
      "year": "2017",
      "venue": "IC-NLSSP 2017"
    },
    {
      "citation_id": "31",
      "title": "Learning from one example through shared densities on transforms",
      "authors": [
        "Erik Miller",
        "Nicholas Matsakis",
        "Paul Viola"
      ],
      "year": "2000",
      "venue": "Proceedings IEEE Conference on Computer Vision and Pattern Recognition. CVPR 2000"
    },
    {
      "citation_id": "32",
      "title": "Fine-grained emotion detection in contact center chat utterances",
      "authors": [
        "Shreshtha Mundra",
        "Anirban Sen",
        "Manjira Sinha",
        "Sandya Mannarswamy",
        "Sandipan Dandapat",
        "Shourya Roy"
      ],
      "year": "2017",
      "venue": "Pacific-Asia Conference on Knowledge Discovery and Data Mining"
    },
    {
      "citation_id": "33",
      "title": "Glove: Global vectors for word representation",
      "authors": [
        "Jeffrey Pennington",
        "Richard Socher",
        "Christopher Manning"
      ],
      "year": "2014",
      "venue": "Empirical Methods in Natural Language Processing (EMNLP)"
    },
    {
      "citation_id": "34",
      "title": "Low resource sequence tagging using sentence reconstruction",
      "authors": [
        "Tal Perl",
        "Sriram Chaudhury",
        "Raja Giryes"
      ],
      "year": "2020",
      "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
      "doi": "10.18653/v1/2020.acl-main.239"
    },
    {
      "citation_id": "35",
      "title": "Context-dependent sentiment analysis in user-generated videos",
      "authors": [
        "Soujanya Poria",
        "Erik Cambria",
        "Devamanyu Hazarika",
        "Navonil Majumder",
        "Amir Zadeh",
        "Louis-Philippe Morency"
      ],
      "year": "2017",
      "venue": "Proceedings of the 55th annual meeting of the association for computational linguistics"
    },
    {
      "citation_id": "36",
      "title": "Optimization as a model for few-shot learning",
      "authors": [
        "Sachin Ravi",
        "Hugo Larochelle"
      ],
      "year": "2016",
      "venue": "Optimization as a model for few-shot learning"
    },
    {
      "citation_id": "37",
      "title": "A two-phase prototypical network model for incremental few-shot relation classification",
      "authors": [
        "Haopeng Ren",
        "Yi Cai",
        "Xiaofeng Chen",
        "Guohua Wang",
        "Qing Li"
      ],
      "year": "2020",
      "venue": "Proceedings of the 28th International Conference on Computational Linguistics"
    },
    {
      "citation_id": "38",
      "title": "Weakly supervised sequence tagging from noisy rules",
      "authors": [
        "Esteban Safranchik",
        "Shiying Luo",
        "Stephen Bach"
      ],
      "year": "2020",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "39",
      "title": "Prototypical networks for few-shot learning",
      "authors": [
        "Jake Snell",
        "Kevin Swersky",
        "Richard Zemel"
      ],
      "year": "2017",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "40",
      "title": "Conceptnet 5.5: An open multilingual graph of general knowledge",
      "authors": [
        "Robyn Speer",
        "Joshua Chin",
        "Catherine Havasi"
      ],
      "year": "2017",
      "venue": "Thirty-first AAAI conference on artificial intelligence"
    },
    {
      "citation_id": "41",
      "title": "Learning to compare: Relation network for few-shot learning",
      "authors": [
        "Flood Sung",
        "Yongxin Yang",
        "Li Zhang",
        "Tao Xiang",
        "Timothy Philip Hs Torr",
        "Hospedales"
      ],
      "year": "2018",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "42",
      "title": "Attention is all you need",
      "authors": [
        "Ashish Vaswani",
        "Noam Shazeer",
        "Niki Parmar",
        "Jakob Uszkoreit",
        "Llion Jones",
        "Aidan Gomez",
        "Łukasz Kaiser",
        "Illia Polosukhin"
      ],
      "year": "2017",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "43",
      "title": "Matching Networks for One Shot Learning",
      "authors": [
        "Oriol Vinyals",
        "Charles Blundell",
        "Timothy Lillicrap",
        "Koray Kavukcuoglu",
        "Daan Wierstra"
      ],
      "year": "2017",
      "venue": "Matching Networks for One Shot Learning",
      "arxiv": "arXiv:1606.04080"
    },
    {
      "citation_id": "44",
      "title": "Contextualized emotion recognition in conversation as sequence tagging",
      "authors": [
        "Yan Wang",
        "Jiayu Zhang",
        "Jun Ma",
        "Shaojun Wang",
        "Jing Xiao"
      ],
      "year": "2020",
      "venue": "Proceedings of the 21th Annual Meeting of the Special Interest Group on Discourse and Dialogue"
    },
    {
      "citation_id": "45",
      "title": "Customer sentiment in webbased service interactions: Automated analyses and new insights",
      "authors": [
        "Shelly Galit B Yom-Tov",
        "Daniel Ashtar",
        "Michael Altman",
        "Neta Natapov",
        "Monika Barkay",
        "Anat Westphal",
        "Rafaeli"
      ],
      "year": "2018",
      "venue": "Companion Proceedings of the The Web Conference 2018"
    },
    {
      "citation_id": "46",
      "title": "Soujanya Poria, Erik Cambria, and Louis-Philippe Morency. 2018a. Memory fusion network for multi-view sequential learning",
      "authors": [
        "Amir Zadeh",
        "Paul Liang",
        "Navonil Mazumder"
      ],
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "47",
      "title": "Multi-attention recurrent network for human communication comprehension",
      "authors": [
        "Amir Zadeh",
        "Paul Liang",
        "Soujanya Poria"
      ],
      "year": "2018",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "48",
      "title": "Knowledge-Enriched Transformer for Emotion Detection in Textual Conversations",
      "authors": [
        "Peixiang Zhong",
        "Di Wang",
        "Chunyan Miao"
      ],
      "year": "2019",
      "venue": "Knowledge-Enriched Transformer for Emotion Detection in Textual Conversations",
      "arxiv": "arXiv:1909.10681[cs].ArXiv:1909.10681"
    }
  ]
}