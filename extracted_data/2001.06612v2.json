{
  "paper_id": "2001.06612v2",
  "title": "Deep Metric Structured Learning For Facial Expression Recognition",
  "published": "2020-01-18T06:23:18Z",
  "authors": [
    "Pedro D. Marrero Fernandez",
    "Tsang Ing Ren",
    "Tsang Ing Jyh",
    "Fidel A. Guerrero Peña",
    "Alexandre Cunha"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "We propose a deep metric learning model to create embedded sub-spaces with a well defined structure. A new loss function that imposes Gaussian structures on the output space is introduced to create these sub-spaces thus shaping the distribution of the data. Having a mixture of Gaussians solution space is advantageous given its simplified and well established structure. It allows fast discovering of classes within classes and the identification of mean representatives at the centroids of individual classes. We also propose a new semi-supervised method to create sub-classes. We illustrate our methods on the facial expression recognition problem and validate results on the FER+, AffectNet, Extended Cohn-Kanade (CK+), BU-3DFE, and JAFFE datasets. We experimentally demonstrate that the learned embedding can be successfully used for various applications including expression retrieval and emotion recognition.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Classical distance metrics like Euclidean distance and cosine similarity are limited and do not always perform well when computing distances between images or their parts. Recently, end-to-end methods  [25, 1, 28, 31]  have shown much progress in learning an intrinsic distance metric. They Figure  1 : Classes within classes. The figure depicts some faces in the FER+ dataset classified by our method as having a surprise expression. Our method further separates these faces into other sub-classes, as shown in the three examples above. Each row contains the top eight images identified to be the closest ones to the centroid of their respective subclass, and each represented by its own Gaussian. One could tentatively visually describe the top row as faces with strong eye and mouth expressions of surprise, the middle row with mostly mildly surprised eyes, and the bottom row faces with strong surprise expressed with wide open eyes and mouth, and hands on face. Observe the face similarities in each sub-class. train a network to discriminatively learn embeddings so that similar images are close to each other and images from different classes are far away in the feature space. These methods are shown to outperform others adopting manually crafted features such as SIFT and binary descriptors  [8, 27] .\n\nFeedforward networks trained by supervised learning can be seen as performing representation learning, where the last layer of the network is typically a linear classifier, e.g. a softmax regression classifier.\n\nRepresentation learning is of great interest as a tool to enable semi-supervised and unsupervised learning. It is often the case that datasets are comprised of vast training data but with relatively little labeled training data. Training with supervised learning techniques on a reduced labeled subset generally results in severe overfitting. Semi-supervised learning is an alternative to resolve the overfitting problem by learning from the vast unlabeled data. Specifically, it is possible to learn good representations for the unlabeled data and use them to solve the supervised learning task.\n\nThe adoption of a particular cost function in learning methods imposes constraints on the solution space, whose shape can take any form satisfying the underlying properties induced by the loss function. For example, in the case of triplet loss  [25] , the optimization of the cost function leads to the creation of a solution space where every object has the nearest neighbors within the same class. Unfortunately, it does not generate a much desired probability distribution function, which is achieved by our formulation.\n\nIn theory, we would like to have the solution manifold to be a continuous function representing the true original information, because, as in the case of the facial expression recognition problem, face expressions are points in the continuous facial action space resulting from the smooth activation of facial muscles  [9] . The transition from one expression to another is represented as the trajectory between the embedded vectors on the manifold surface.\n\nThe objective of this work is to offer a formulation for the creation of separable sub-spaces each with a defined structure and with a fixed data distribution. We propose a new loss function that imposes Gaussian structures in the creation of these sub-spaces. In addition, we also propose a new semi-supervised method to create sub-classes within each facial expression class, as exemplified in Figure  1 .",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work",
      "text": "Siamese networks applied to signature verification showed the ability of neural networks to learn compact embedding  [5] . OASIS  [6]  and local distance learning  [10]  learn fine-grained image similarity ranking models using hand-crafted features that are not based on deep-learning. Recent methods such as  [25, 1, 28, 31]  approaches the problem of learning a distance metric by discriminatively training a neural network. Features generated by those approaches are shown to outperform manually crafted features  [1] , such as SIFT and various binary descriptors  [8, 27] .\n\nDistance Metric Learning (DML) can be broadly divided into contrastive loss based methods, triplet networks, and approaches that go beyond triplets such as quadruplets, or even batch-wise loss. Contrastive embedding is trained on paired data, and it tries to minimize the distance between pairs of examples with the same class label while penalizing examples with different class labels that are closer than a margin α  [11] . Triplet embedding is trained on triplets of data with anchor points, a positive that belongs to the same class, and a negative that belongs to a different class  [33, 14] . Triplet networks use a loss over triplets to push the anchor and positive closer, while penalizing triplets where the distance between the anchor and negative is less than the distance between the anchor and positive, plus a margin α. Contrastive embedding has been used for learning visual similarity for products  [4] , while triplet networks have been used for face verification, person re-identification, patch matching, for learning similarity between images and for fine-grained visual categorization  [25, 26, 31, 7, 1] .\n\nSeveral works are based on triplet-based loss functions for learning image representations. However, the majority of them use category label-based triplets  [36, 32, 23] . Some existing works such as  [6, 31]  have focused on learning finegrained representations. In addition,  [36]  used a similarity measure computing several existing feature representations to generate ground truth annotations for the triplets, while  [31]  used text image relevance, based on Google image search to annotate the triplets. Unlike those approaches, we use human raters to annotate the triplets. None of those works focus on facial expressions, only recently  [30]  proposed a system of facial expression recognition based on triplet loss.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Methodology",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Structured Gaussian Manifold Loss",
      "text": "Let S = {x i |x i ∈ R D } be a collection of i.i.d. samples\n\nx i to be classified into c classes, and let w j represent the j-th class, for j = 1, . . . , c. The computed class function l(x) = arg max p(w|f Θ (x)) returns the class w j of sample x -maximum a posteriori probability estimate -for the neural net function f Θ : R D → R d drawn independently according to probability p(x|w j ) for input x. Suppose we separate S in an embedded space such that each set C j = {x|x ∈ S, l(x) = w j } contains the samples belonging to class w j . Our goal is to find a Gaussian representation for each C j which would allow a clear separation of S in a reduced space, d D. We assume that p(f Θ (x)|w j ) has a known parametric form, and it is therefore determined uniquely by the value of a parameter vector θ j . For example, we might have p(f Θ (x)|w j ) ∼ N (µ j , Σ j ), where θ j = (µ j , Σ j ), for N (., .) the normal distribution with mean µ j and variance Σ j . To show the dependence of p(f Θ (x)|w j ) on θ j explicitly, we write p(f Θ (x)|w j ) as p(f Θ (x)|w j , θ j ). Our problem is to use the information provided by the training sam-ples to obtain a good transformation function f Θ (x j ) that generates embedded spaces with a known distribution associated with each category. Then the a posteriori probability P (w j |f Θ (x)) can be computed from p(f Θ (x)|w j ) by the Bayes' formula:\n\nWe use the normal density function for p(x|w j , θ j ). The objective is to generate embedded sub-spaces with defined structure. Thus, using the Gaussian structures:\n\nwhere X = (f Θ (x) -µ j ). For the case Σ j = σ 2 I, where I is the identity matrix:\n\nIn a supervised problem, we know the a posteriori probability P (w j |x) for the input set. From this, we can define our structured loss function as the mean square error between the a posteriori probability of the input set and the a posteriori probability estimated for the embedded space:\n\nWe applied the steps described in Algorithm 1 to train the system. The batch size is given by n × c where c is the number of classes, and n is the sample size. In this work, we use n = 30, thus for eight classes the batch size is 240, which was used for the estimation of the parameters in Equation  4 .\n\nWe define the accuracy of the model as the ability of the parameter vector θ to represent the test dataset in the embedded space.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Deep Gaussian Mixture Sub-Space",
      "text": "The same facial expression may possess a different set of global features. For example, ethnicity can determine specific color and shape, while age provides physiological differences of facial characteristics; moreover, gender, weight, and other features can determine different facial characteristics, while having the same expression. Our proposal can group and extract these characteristics automatically. We propose to represent each facial expression class as a Gaussians Mixture. These Gaussian parameters are obtained in an unsupervised way as part of the learning processes. We start from a representation space given by Algorithm 1.\n\nAlgorithm 1 Structured Gaussian Manifold Learning. f Θ : Neural Network; S: dataset; C j are the subset of the elements of class w j ; N : number of updates;\n\n{Sample(x i , w i )} ∼ S, get current batch.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "4:",
      "text": "z ← f Θ (x i ), representation.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "5:",
      "text": "θ j ← {µ j , σj} where σ is a parameters (σ = 0.5 in this work) and µ j is the mean of the elements of the class w j :\n\nwhere |.| denotes set cardinality.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "6:",
      "text": "Evaluation of the Loss function. For the explanation of the loss representation see equation 4:\n\nΘ t+1 = Θ t -∇L, backward and optimization steps.\n\nfor all class w j do 10:",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "2(H(Ω)+H(G))",
      "text": ", for complete details see  [21] . For the retrieval task, we use the Recall@K  [15]  measure. Each test image (query) first retrieves K Nearest Neighbour (KNN) from the test set and receives score 1 if an image of the same class is retrieved among the KNN, and 0 otherwise. Recall@K averages those score over all the images. Moreover, we also evaluate accuracy, i.e. the fraction of results that are the same class as queried image, averaged over all queries. While the classification task is evaluated using KNN.\n\nFor the training process, we use the Adam method  [16]  with a learning rate of 0.0001 and batch size of 256 (samples of size 32 to estimate the parameters in each iteration). In the TripletLoss case, we used 128 triplets in each batch. The neural networks were initialized with the same weights in all cases.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Result",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Representation And Recover",
      "text": "The groups used for the evaluation of the measures are obtained using K-means, whereas K equals the number of classes (8 in the case of the FER+  [2] , AffectNet  [24] , CK+  [19]  datasets, and 7 for JAFFE  [20]  and BU-3DFE  [34]  datasets).\n\nThe results obtained for the clustering task show that the proposed method presents good group quality (see table  1 ) in similar domains. As can be observed, the results are degraded for different domains. In general, we observe that the TripletLoss is most robust to the change of domains on all models. However, the best result is achieved using the proposed method for the RestNet18  [12]  model in FER+, CK+, and BU-3DFE.\n\nFigure  2  shows a 2D t-SNE  [29]  visualization of the learned SGMLoss embedding space using the FER+ training set. The amount of overlap between the two categories in this figure roughly demonstrates the extent of the visual similarity between them. For example, happy and neutral have some objects overlap, indicating that these cases could be confused easily, and both of them have a very low over-",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Metric",
      "text": "Recall@K SGMLoss Recall@K TripletLoss Acc@K SGMLoss Acc@K TripletLoss lap with fear indicating that they are visually very distinct from fear. Also, the spread of a category in this figure indicates the visual diversity within that category. For example, happiness category maps to some distinct regions indicating that there are some visually distinct modes within this category.\n\nFigure  3  shows the results obtained in the recovery task (Recall@K and Acc@K measures) for K = {1, 2, 4, 8, 16, 32}. TripletLoss obtains better recovery results for all K but to the detriment of accuracy. Our method manages to increase its recovery value while preserving quality. It means that most neighbors are of the same class. Figure  4  shows the top-5 retrieved images for some of the queries on CelebA dataset  [18] . The overall results of the proposed SGMLoss embedding are clearly better than the results of TripletLoss embedding.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Method",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Classification",
      "text": "The proposed SGMLoss method can be used for FER by combining it with the KNN classifier. Figure  5  shows the average F1-score of the SGMLoss and TripletLoss on the FER+ validation set as a function of the number of neighbors used. F1-score is maximized for K=11. Table  2  compares the classification performance of the SGMLoss embedding (using 11 neighbors) with Triplet-Loss and CNN models. In general, our method obtains the best classification results for all architectures. ResNet18 CNN model does not obtains a significant higher accuracy. Moreover, our results surpass the accuracy 84.99 presented in  [2] .\n\nThe Facial Expression dataset constitute a great challenge due to the subjectivity of the emotions  [22] . The labeling process requires the effort of a group of specialists to make the annotations. FER+ and AffectNet datasets contains many problems in the labels. In  [2]  an effort was made to improve the quality of the labels of the FER+ (dataset used in our experiments) by re-tagging the dataset using crowd sourcing. Figure  6  shows some mislabeled images Method Arch.\n\nAcc. Prec.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Rec. F1",
      "text": "CNN FMPNet  [35]  79.535 66.697 68.582 67.627 CVGG13  [3]  84.316 75.151 67.425 71.079 AlexNet  [17]  86.038 77.658 68.657 72.881 ResNet18  [12]  87.695 85.956 69.659 76.954 PreActResNet18  [13]  82.372 76.915 65.238 70.597 TripletLoss FMPNet  [35]  82.563 79.554 62.406 69.944 CVGG13  [3]  85.974 82.034 68.112 74.428 AlexNet  [17]  86.038 80.598 67.895 73.703 ResNet18  [12]  87.121 78.543 68.378 73.109 PreActResNet18  [13]     retrieved by our method. The scale, position, and context could influence the decision of a non-expert tagger such as those in crowd sourcing.\n\nExperimental results show the quality of the embedded representation obtained by SGMLoss in the classification problems. Our representation improves the representation obtained by TripletLoss, which is the method most used in the identification and representation problems.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Clustering",
      "text": "For the training process, we use the Adam method  [16]  with a learning rate of 0.0001, a batch size of 640 and 500 epoch. The maximum level of subdivision used is L=5 (this value  guarantee that the batch for a subclass in this level to be 128). The ResNet18 architecture is selected to train the FER+ dataset. The objective of this experiment is to visually analyse the clustering obtained by this approach.\n\nThe results shown in Figure  7  present 64-dimensional embedded space using the Barnes-Hut t-SNE visualization scheme  [29]  using the Deep Gaussian Mixture Sub-space model for the FER+ dataset. The method created five Gaussian sub-spaces for the unsupervised case for each class.\n\nFor the clustering task, all embedded vectors are calculated and EM method is applied creating 40 groups. For each group, the medoid is calculated. The medoid is the object in the group closest to the centroid (mean to the sam- Figure  8  shows the Top-16 images obtained for the happiness category. The first group (Figure  8   The presented method is a powerful tool for tasks such as photo album summarization. In this task, we are interested in summarizing the diverse expression content present in a given photo album using a fixed number of images. Figure  9  shows 5 of the 40 groups obtained on AffectNet dataset. The obtained groups show great similarity in terms of FER. These results demonstrate the generalization capacity of the proposed method and its applicability to problems of FER clustering.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Conclusions",
      "text": "We introduced two new metric learning representation models in this work, namely Deep Gaussian Mixture Subspace Learning and Structured Gaussian Manifold Learning. In the first model, we build a Gaussian representation of expressions leading to a robust classification and grouping of facial expressions. We illustrate through many examples, the high quality of the vectors obtained in recovery tasks, thus demonstrating the effectiveness of the proposed representation. In the second case, we provide a semi-supervised method for grouping facial expressions. We were able to obtain embedded subgroups sharing the same facial expression group. These subgroups emerged due to shared specific characteristics other than the general appearance. For example, individuals with glasses expressing a happy appearance.",
      "page_start": 7,
      "page_end": 7
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Classes within classes. The ﬁgure depicts some",
      "page": 1
    },
    {
      "caption": "Figure 1: 2. Related Work",
      "page": 2
    },
    {
      "caption": "Figure 2: shows a 2D t-SNE [29] visualization of the",
      "page": 4
    },
    {
      "caption": "Figure 2: Barnes-Hut t-SNE visualization [29] of the SGM-",
      "page": 4
    },
    {
      "caption": "Figure 3: Recall@K and Acc@K measures for the test split",
      "page": 4
    },
    {
      "caption": "Figure 3: shows the results obtained in the recov-",
      "page": 4
    },
    {
      "caption": "Figure 4: shows the top-5 retrieved images for some of the",
      "page": 4
    },
    {
      "caption": "Figure 4: Top-5 images retrieved using SGMLoss (left) and TripletLoss (right) embeddings. The overall results of the",
      "page": 5
    },
    {
      "caption": "Figure 5: shows the",
      "page": 5
    },
    {
      "caption": "Figure 6: shows some mislabeled images",
      "page": 5
    },
    {
      "caption": "Figure 5: Classiﬁcation performance of the SGMLoss and",
      "page": 6
    },
    {
      "caption": "Figure 6: Examples of mislabeled images on the FER+",
      "page": 6
    },
    {
      "caption": "Figure 7: present 64-dimensional",
      "page": 6
    },
    {
      "caption": "Figure 7: Barnes-Hut t-SNE visualization [29] of the Deep",
      "page": 7
    },
    {
      "caption": "Figure 8: shows the Top-16 images obtained for the hap-",
      "page": 7
    },
    {
      "caption": "Figure 8: (a) ) shows an",
      "page": 7
    },
    {
      "caption": "Figure 8: (b)) represents an ex-",
      "page": 7
    },
    {
      "caption": "Figure 8: (d) ) shows",
      "page": 7
    },
    {
      "caption": "Figure 9: shows 5 of the 40 groups obtained on AffectNet dataset.",
      "page": 7
    },
    {
      "caption": "Figure 8: Top-16 images of the clustering obtained from the",
      "page": 7
    },
    {
      "caption": "Figure 9: Top-16 images of the 5 clustering obtained from",
      "page": 8
    }
  ],
  "tables": [
    {
      "caption": "Table 2: Classification results of the CNN, TripletLoss and SGMLoss models trained on FER+. SGMLoss: Structured",
      "data": [
        {
          "TripletLoss": "SGMLoss"
        }
      ],
      "page": 6
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Learning local feature descriptors with triplets and shallow convolutional neural networks",
      "authors": [
        "Vassileios Balntas",
        "Edgar Riba",
        "Daniel Ponsa",
        "Krystian Mikolajczyk"
      ],
      "year": "2016",
      "venue": "In BMVC"
    },
    {
      "citation_id": "2",
      "title": "Training deep networks for facial expression recognition with crowd-sourced label distribution",
      "authors": [
        "Emad Barsoum",
        "Cha Zhang",
        "Cristian Canton Ferrer",
        "Zhengyou Zhang"
      ],
      "year": "2016",
      "venue": "ACM International Conference on Multimodal Interaction (ICMI)"
    },
    {
      "citation_id": "3",
      "title": "Training deep networks for facial expression recognition with crowd-sourced label distribution",
      "authors": [
        "Emad Barsoum",
        "Cha Zhang",
        "Cristian Canton Ferrer",
        "Zhengyou Zhang"
      ],
      "year": "2016",
      "venue": "ACM International Conference on Multimodal Interaction (ICMI)"
    },
    {
      "citation_id": "4",
      "title": "Learning visual similarity for product design with convolutional neural networks",
      "authors": [
        "Sean Bell",
        "Kavita Bala"
      ],
      "year": "2015",
      "venue": "ACM Transactions on Graphics (TOG)"
    },
    {
      "citation_id": "5",
      "title": "Signature verification using a\" siamese\" time delay neural network",
      "authors": [
        "Jane Bromley",
        "Isabelle Guyon",
        "Yann Lecun",
        "Eduard Säckinger",
        "Roopak Shah"
      ],
      "year": "1994",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "6",
      "title": "Large scale online learning of image similarity through ranking",
      "authors": [
        "Gal Chechik",
        "Varun Sharma",
        "Uri Shalit",
        "Samy Bengio"
      ],
      "year": "2010",
      "venue": "Journal of Machine Learning Research"
    },
    {
      "citation_id": "7",
      "title": "Fine-grained categorization and dataset bootstrapping using deep metric learning with humans in the loop",
      "authors": [
        "Yin Cui",
        "Feng Zhou",
        "Yuanqing Lin",
        "Serge Belongie"
      ],
      "year": "2016",
      "venue": "Proceed-ings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "8",
      "title": "Discriminative unsupervised feature learning with exemplar convolutional neural networks",
      "authors": [
        "Alexey Dosovitskiy",
        "Philipp Fischer",
        "Jost Tobias Springenberg",
        "Martin Riedmiller",
        "Thomas Brox"
      ],
      "year": "2016",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "9",
      "title": "Facial action coding system. A Human Face",
      "authors": [
        "Paul Ekman",
        "Wallace Friesen",
        "Joseph Hager"
      ],
      "year": "2002",
      "venue": "Facial action coding system. A Human Face"
    },
    {
      "citation_id": "10",
      "title": "Image retrieval and classification using local distance functions",
      "authors": [
        "Andrea Frome",
        "Yoram Singer",
        "Jitendra Malik"
      ],
      "year": "2007",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "11",
      "title": "Dimensionality reduction by learning an invariant mapping",
      "authors": [
        "Raia Hadsell",
        "Sumit Chopra",
        "Yann Lecun"
      ],
      "year": "2006",
      "venue": "Computer vision and pattern recognition"
    },
    {
      "citation_id": "12",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "Kaiming He",
        "Xiangyu Zhang",
        "Shaoqing Ren",
        "Jian Sun"
      ],
      "year": "2015",
      "venue": "Deep residual learning for image recognition"
    },
    {
      "citation_id": "13",
      "title": "Identity mappings in deep residual networks",
      "authors": [
        "Kaiming He",
        "Xiangyu Zhang",
        "Shaoqing Ren",
        "Jian Sun"
      ],
      "year": "2016",
      "venue": "European conference on computer vision"
    },
    {
      "citation_id": "14",
      "title": "Deep metric learning using triplet network",
      "authors": [
        "Elad Hoffer",
        "Nir Ailon"
      ],
      "year": "2015",
      "venue": "International Workshop on Similarity-Based Pattern Recognition"
    },
    {
      "citation_id": "15",
      "title": "Product quantization for nearest neighbor search",
      "authors": [
        "Herve Jegou",
        "Matthijs Douze",
        "Cordelia Schmid"
      ],
      "year": "2011",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "16",
      "title": "Adam: A method for stochastic optimization",
      "authors": [
        "P Diederik",
        "Jimmy Kingma",
        "Ba"
      ],
      "year": "2014",
      "venue": "Adam: A method for stochastic optimization"
    },
    {
      "citation_id": "17",
      "title": "One weird trick for parallelizing convolutional neural networks",
      "authors": [
        "Alex Krizhevsky"
      ],
      "year": "2014",
      "venue": "One weird trick for parallelizing convolutional neural networks"
    },
    {
      "citation_id": "18",
      "title": "Deep learning face attributes in the wild",
      "authors": [
        "Ziwei Liu",
        "Ping Luo",
        "Xiaogang Wang",
        "Xiaoou Tang"
      ],
      "year": "2015",
      "venue": "Proceedings of the IEEE international conference on computer vision"
    },
    {
      "citation_id": "19",
      "title": "The Extended Cohn-Kanade Dataset (CK+): A complete dataset for action unit and emotion-specified expression",
      "authors": [
        "Patrick Lucey",
        "Jeffrey Cohn",
        "Takeo Kanade",
        "Jason Saragih",
        "Zara Ambadar",
        "Iain Matthews"
      ],
      "year": "2010",
      "venue": "Computer Vision and Pattern Recognition Workshops (CVPRW), 2010 IEEE Computer Society Conference on"
    },
    {
      "citation_id": "20",
      "title": "Coding facial expressions with gabor wavelets",
      "authors": [
        "Michael Lyons",
        "Shigeru Akamatsu",
        "Miyuki Kamachi",
        "Jiro Gyoba"
      ],
      "year": "1998",
      "venue": "Automatic Face and Gesture Recognition"
    },
    {
      "citation_id": "21",
      "title": "Introduction to information retrieval",
      "authors": [
        "Prabhakar Christopher D Manning",
        "Hinrich Raghavan",
        "Schütze"
      ],
      "year": "2008",
      "venue": "Introduction to information retrieval"
    },
    {
      "citation_id": "22",
      "title": "Evaluating the research in automatic emotion recognition",
      "authors": [
        "Pedro Marrero-Fernández",
        "Arquímedes Montoya-Padrón",
        "Antoni Jaume-I-Capó",
        "Jose Maria",
        "Buades Rubio"
      ],
      "year": "2014",
      "venue": "IETE Technical Review (Institution of Electronics and Telecommunication Engineers, India)"
    },
    {
      "citation_id": "23",
      "title": "Deep metric learning via lifted structured feature embedding",
      "authors": [
        "Hyun Oh Song",
        "Yu Xiang",
        "Stefanie Jegelka",
        "Silvio Savarese"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "24",
      "title": "Affective Computing",
      "authors": [
        "R Picard"
      ],
      "year": "2000",
      "venue": "Affective Computing"
    },
    {
      "citation_id": "25",
      "title": "Facenet: A unified embedding for face recognition and clustering",
      "authors": [
        "Florian Schroff",
        "Dmitry Kalenichenko",
        "James Philbin"
      ],
      "year": "2015",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "26",
      "title": "Embedding deep metric for person re-identification: A study against large variations",
      "authors": [
        "Hailin Shi",
        "Yang Yang",
        "Xiangyu Zhu",
        "Shengcai Liao",
        "Zhen Lei",
        "Weishi Zheng",
        "Stan Li"
      ],
      "year": "2016",
      "venue": "European Conference on Computer Vision"
    },
    {
      "citation_id": "27",
      "title": "Discriminative learning of deep convolutional feature point descriptors",
      "authors": [
        "Edgar Simo-Serra",
        "Eduard Trulls",
        "Luis Ferraz",
        "Iasonas Kokkinos",
        "Pascal Fua",
        "Francesc Moreno-Noguer"
      ],
      "year": "2015",
      "venue": "Computer Vision (ICCV), 2015 IEEE International Conference on"
    },
    {
      "citation_id": "28",
      "title": "Deep metric learning via lifted structured feature embedding",
      "authors": [
        "Hyun Oh Song",
        "Yu Xiang",
        "Stefanie Jegelka",
        "Silvio Savarese"
      ],
      "year": "2016",
      "venue": "Computer Vision and Pattern Recognition (CVPR), 2016 IEEE Conference on"
    },
    {
      "citation_id": "29",
      "title": "Accelerating t-sne using treebased algorithms",
      "authors": [
        "Laurens Van Der Maaten"
      ],
      "year": "2014",
      "venue": "Journal of machine learning research"
    },
    {
      "citation_id": "30",
      "title": "A compact embedding for facial expression similarity",
      "authors": [
        "Raviteja Vemulapalli",
        "Aseem Agarwala"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "31",
      "title": "Learning fine-grained image similarity with deep ranking",
      "authors": [
        "Jiang Wang",
        "Thomas Leung",
        "Chuck Rosenberg",
        "Jinbin Wang",
        "James Philbin",
        "Bo Chen",
        "Ying Wu"
      ],
      "year": "2014",
      "venue": "Learning fine-grained image similarity with deep ranking",
      "arxiv": "arXiv:1404.4661"
    },
    {
      "citation_id": "32",
      "title": "Deep metric learning with angular loss",
      "authors": [
        "Jian Wang",
        "Feng Zhou",
        "Shilei Wen",
        "Xiao Liu",
        "Yuanqing Lin"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE International Conference on Computer Vision"
    },
    {
      "citation_id": "33",
      "title": "Distance metric learning for large margin nearest neighbor classification",
      "authors": [
        "Q Kilian",
        "Lawrence Weinberger",
        "Saul"
      ],
      "year": "2009",
      "venue": "Journal of Machine Learning Research"
    },
    {
      "citation_id": "34",
      "title": "A 3D facial expression database for facial behavior research",
      "authors": [
        "Lijun Yin",
        "Xiaozhou Wei",
        "Yi Sun",
        "Jun Wang",
        "Matthew Rosato"
      ],
      "year": "2006",
      "venue": "FGR 2006. 7th international conference on"
    },
    {
      "citation_id": "35",
      "title": "Image based static facial expression recognition with multiple deep network learning",
      "authors": [
        "Zhiding Yu",
        "Cha Zhang"
      ],
      "year": "2015",
      "venue": "Proceedings of the 2015 ACM on International Conference on Multimodal Interaction, ICMI '15"
    },
    {
      "citation_id": "36",
      "title": "Fast training of triplet-based deep binary embedding networks",
      "authors": [
        "Bohan Zhuang",
        "Guosheng Lin",
        "Chunhua Shen",
        "Ian Reid"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    }
  ]
}