{
  "paper_id": "2307.12343v1",
  "title": "Self-Supervised Learning For Audio-Based Emotion Recognition",
  "published": "2023-07-23T14:40:50Z",
  "authors": [
    "Peranut Nimitsurachat",
    "Peter Washington"
  ],
  "keywords": [
    "emotion classification",
    "emotion recognition",
    "selfsupervised learning",
    "transfer learning"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Emotion recognition models using audio input data can enable the development of interactive systems with applications in mental healthcare, marketing, gaming, and social media analysis. While the field of affective computing using audio data is rich, a major barrier to achieve consistently high-performance models is the paucity of available training labels. Self-supervised learning (SSL) is a family of methods which can learn despite a scarcity of supervised labels by predicting properties of the data itself. To understand the utility of self-supervised learning for audio-based emotion recognition, we have applied self-supervised learning pre-training to the classification of emotions from the CMU Multimodal Opinion Sentiment and Emotion Intensity (CMU-MOSEI)'s acoustic modality. Unlike prior papers that have experimented with raw acoustic data, our technique has been applied to encoded acoustic data with 74 parameters of distinctive audio features at discrete timesteps. Our model is first pretrained to uncover the randomly-masked timestamps of the acoustic data. The pre-trained model is then fine-tuned using a small sample of annotated data. The performance of the final model is then evaluated via several evaluation metrics, including overall mean absolute error (MAE), mean absolute error (MAE) per emotion, overall 4-class accuracy, and 4-class accuracy per emotion. These metrics are compared against a baseline deep learning model with an identical backbone architecture. We find that self-supervised learning consistently improves the performance of the model across all metrics, especially when the number of annotated data points in the finetuning step is small. Furthermore, we quantify the behaviors of the self-supervised model and its convergence to baseline model as the amount of annotated data increases. This work shows the utility of self-supervised learning for affective computing, demonstrating that self-supervised learning is most useful when the number of training examples is small, and that the effect is most pronounced for emotions which are easier to classify such as happy, sad and anger. This work further demonstrates that self-supervised learning works when applied to embedded feature representations rather than the traditional approach of pre-training on the raw input space.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "E MOTION classification has became increasingly used in a multitude of academic disciplines including digital psychiatry  [1] -  [4] , autonomous vehicles  [5] ,  [6] , and media analytics  [7] ,  [8] . By having sufficient labeled training data, researchers can improve the performance of emotion classifiers. A disproportionate amount of prior work in emotion recognition with video data places a stronger emphasis on the visual modality and often neglect the acoustic modality. Thus, deep learning models that classify emotions from audio input can potentially improve the performance of existing emotion classifiers. A major challenge of training a performant emotion classifier for both the visual and acoustic modalities is the paucity of annotated data. Deep learning models are usually trained using a supervised-learning paradigm in which the model learns to map input-such as speech, facial movement, or facial expression-to the output or corresponding emotions. In order to attain high performance, the model usually requires large annotated training set; this type of dataset, however, tends to be scarce. This lack of sufficient annotated data, therefore, makes training deep learning model to classify emotions accurately challenging. One solution is to manually annotate the unlabeled datasets with corresponding emotions; however, this task is very demanding and requires well-trained annotators.\n\nIn this paper, we propose a self-supervision technique which can boost the accuracy of a deep learning model by taking advantage of additional pre-training steps on unannotated embedded audio data. We conduct our experiment on the CMU-MOSEI dataset  [9] -one of the largest multimodal datasets to date. We apply a similar methodology to the pre-training of large language models  [10] , where we pre-train our model on acoustic data before fine-tuning on the small number of annotated emotion data. Pre-trained on the unlabeled data, we find that the model can learn useful internal representations of the embedded acoustic data which, in turn, can increase the model's accuracy in the fine-tuning stage even with a small number of annotated data. This work further demonstrates that self-supervised learning works when applied to the embedding space rather than the input space. Self-supervised learning has been applied mostly in natural language processing and computer vision. For instance, the Bidirectional Encoder Representations from Transformers (BERT)  [10]  model pre-trains deep bi-directional representations on a large corpus through masked language modeling and next sentence prediction. The masked language modeling involves simply masking some percentage of the input tokens at random and predicting those masked tokens  [10] . The pre-trained model is then transferred to a fine-tuning step where it is fine-tuned using annotated data. This pre-training technique helps improving the overall performance of the model especially when the number of labels is too small for deep learning.\n\nRecent works have proposed to use similar self-supervised techniques on audio in raw format to perform several tasks such as speech recognition or emotion classification. In the case of emotion classification, Audio-only Self-Supervision (Odd)  [11]  jumbles 25 % of the clips; two windows of a length of 15 % of the selected clips are randomly selected and swapped. The encoder is then tasked to identify the elements in the input batch that have been swapped. During this pretraining step, the model will learn useful representation of the audio which can be beneficial in classifying discrete emotions in the fine-tuning step. Odd model explores this pretraining technique on several datasets, including CREMA-D  [12] , Ravdess  [13] , and IEMOCAP  [14] . All of these datasets store audio in raw format. Large language models like GPT-3 and GPT-4  [15]  also use similar strategy to predict the following word in the unlabeled text during the pre-training step.\n\nSelf-supervised techniques have also been applied to other models such as CPC (Contrast Predictive Coding)  [16]  and APC (Autoregressive Predictive Coding)  [17] . In the case of CPC (Contrast Predictive Coding), the authors propose a universal unsupervised learning approach to extract useful representations from high-dimensional data such as audio. This framework pre-trains the model by making predictions of the next token given the previous tokens. Specifically, it tries to learn representations that separate the target future frame from randomly sampled negative frames. This pre-trained model is then fine-tuned and evaluated on phone prediction performance. CPC (Contrast Predictive Coding)  [16]  is trained on the publicly available LibriSpeech dataset  [18]  and takes in audio inputs as 16KHz PCM audio waveform. Similary to CPC (Contrast Predictive Coding)  [16] , APC (Autoregressive Predictive Coding)  [17]  is also trying to predict the target future frames within the audio input. However, unlike CPC (Contrast Predictive Coding)  [16]  that focuses on separating target frame from randomly sampled negative frame, APC is only allowed to ignore information that is common across dataset. This APC framework  [17]  takes in audio data in the form of an 80-dimensional log Mel spectogram.\n\nAnother relevant work is Problem Agnostic Speech Encoder (PASE)  [19] . This method pre-trains the model by predicting the seven extracted features-Waveform, Log power spectrum (LPS), Mel-frequency cepstral coefficients (MFCC), Prosody, Local info max (LIM), Global info max (GIM), Sequence predicting coding (SPC)-of raw audio. This pre-trained model can be used for any downstream tasks by adding the linear Fig.  3 . An overview of our experiment on how to pre-train and fine-tune the model with a variety of number of annotated datapoints and evaluate the performance of models in comparison to the baseline classifier on top of frozen pre-trained layer. Finally, SeCoSt framework  [20]  also applied a teacher-student self-supervised approach to audio by using the predictions of current iteration as labels for the next one. Similarly to previous works, the input features are in form of logmel features.\n\nAs discussed above, most, if not all, of the previous works on self-supervised on audio apply this technique on the raw audio input. Our proposed method, however, will apply the masking technique to the encoded input data. Furthermore, we also monitor the performance of this technique for each of the six emotions in addition to the overall performance across all emotions. We are unaware of prior literature that characterizes this discrepancy.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Iii. Methods",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "A. Dataset And Features",
      "text": "The experiment is conducted on CMU Multimodal Opinion Sentiment and Emotion Intensity (CMU-MOSEI)  [9]  which is considered to be one of the largest datasets of multimodal sentiment analysis and emotion recognition to date. This dataset contains more than 23,500 sentences utterance videos from 1,000 speakers on YouTube and has three modalities, including acoustic and visual. The dataset is also annotated with labels that indicate the level of six Ekman emotions  [21]  including happiness, sadness, anger, surprise, disgust ,and fear. The labels are the array of six numbers; each one represents each of the intensity of each emotion x on a scale of 0-3: [0: no evidence of x, 1: weakly x, 2: x, 3: highly x]  [22] . These annotations are assigned by 3 crowdsourced judges from Amazon Mechanical Turk platform who received prior training and received 98% or higher  [22]  approval rate to assure high quality annotations. The distribution of emotions in this dataset is shown in Fig.  1 .\n\nHere, we work with the emotion labels and the acoustic modality of CMU-MOSEI. It is important to note that the acoustic data in CMU-MOSEI is not provided in a raw format; instead, each timestep of the input consists of 74 parameters that are extracted COVAREP software  [23] . Therefore, we provide one of the first explorations of self-supervised learning on datasets which are encoded with an established feature embedding.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "B. Model Details",
      "text": "We pre-train a deep learning model to uncover the randomly masked timesteps within the audio clips. As mentioned in the previous section, each timestep of the audio input consists of 74 parameters extracted from COVAREP software  [23] .\n\nApproximately 10% of the audio input or 30 consecutive timesteps are randomly selected from every input; this is done by randomly selecting the valid starting timestep such that the following 30 timesteps are within the entire duration of input. As shown in Fig.  2 , all 74 parameters of the selected timesteps are masked or replaced with -30 which is completely outside of the standardized range of each parameter.\n\nOur deep learning model, consisting of two layers of a 256unit GRU followed by one 74-unit dense layer, is trained to predict the features of the original audio clip from the generated clip with masked features. By pre-training the model  in this way, we encourage the pre-trained model to learn useful representations of the parameters of the audio clip that can be transferred to the fine-tuning step where the pre-trained model is trained with a small number of inputs annotated with an emotion label.\n\nThe pre-trained model is then transferred to the fine-tuning step. One 6-unit dense layer is added on top of the pre-trained model so that the new model can predict the emotion label, which is an array of 6 floating-point numbers, from the audio input. This 6-unit dense layer is then fine-tuned with a small fraction of audio input and corresponding emotion labels while all layers of pre-trained model are frozen. The performance of the final model is evaluated on the unseen validation set and compared with that of baseline model with a identical architecture.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "C. Experiment",
      "text": "We explore the performance of our deep learning model pre-trained with self-supervised learning in comparison to the baseline model with a identical architecture. The performance\n\nGiven the rounding function in Equation 1, the 4-class overall accuracy is defined as following. The metrics in Equation 2 consist of six columns that represent the predicted and true values of six emotions of n samples.\n\nThe 4-class accuracy for each emotion is defined similarly. However, instead of considering all six columns of six emotions, only one column that corresponds to the emotion of interest is considered.\n\nThe overall mean absolute error (MAE) and mean absolute error (MAE) for each emotion use the standard definition of mean absolute error (MAE) on all six columns and one column of interest respectively.\n\nAs shown in Fig.  3 , the entire dataset is split into a training set (80%) and validation set (20%). The training set is used to train the baseline model and self-supervised models; the baseline model has the same architecture (two layers of 256unit GRU and one 74-unit dense layer, one layer of 74-unit dense, and one layer of 6-unit dense) as the final pre-trained model shown in Fig.  2  but has no pre-training. These two models are evaluated on the evaluation set using the four metrics mentioned above. The entire unlabeled training set is used to pre-train the model. We mask all 74 parameters of 30 randomly-selected consecutive timesteps and train a deep neural network model with two layers of a 256-unit GRUs and one 6-unit dense layer to uncover these masked timesteps. By predicting the masked timesteps of original audio input, the pre-trained model learns useful representation that can be transferred to the downstream task. It is imperative to note that we do not use the emotion labels of audio input at all in the pre-training step. This pre-trained model is then fine-tuned to emotion labels. As shown in Fig.  3 , we fine-tune our pretrained model with different numbers of annotated data points in order to monitor the benefit of our pre-training technique when annotated data are scarce. In this experiment, we monitor the performance of the pre-trained and baseline models at 20, 35, 50, ..., 200 and 400, 600,..., 1200 data points.\n\nA sample of data points are randomly selected from the training dataset along with their corresponding emotion labels; these annotated data points are then used to fine-tune the pretrained model and train the baseline model for 30 epochs. The baseline and pre-trained models are then evaluated on the unseen evaluation set according to four performance metrics. We perform these three steps-randomly sampling, fine-tuning, and evaluating-three times per each number of data points in order to monitor the mean and variance of evaluation metrics.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Iv. Results",
      "text": "Fig.  6  shows the 4-class overall accuracy along with standard deviation of baseline and pre-trained model at each number of labeled data; the standard deviation in the figure is calculated from the accuracy of three iterations at each number of labeled data. The 4-class overall accuracy of the pre-trained model is around 85-87 % when there are a few number of labeled data (0-200). The accuracy gradually increases as the number of labeled increases. The accuracy of baseline model, on the other hand, starts at around 81-82 % and increases rapidly as the number of labeled data approach 200. According to Fig.  6 , it is clear that the pre-trained model outperforms the baseline around 4-5 % at small number of labeled data; the 4class overall accuracy of both models starts to converge as the number of labeled data increase. It is also interesting to note that, for both baseline and pre-trained models, the standard deviations are relatively high with a few number of labeled data and becomes smaller once the number of labeled data increase.\n\nSimilarly, the overall mean absolute error (MAE) of the pretrained model is around 0.26 at 20 labeled data and continues to shrink as the number of labeled data increases. The mean absolute error of the baseline model, however, is around 0.29-0.30 at 20 labeled data and shrinks as the number of labeled data increase. As shown in Fig.  7 , the performance of pretrained model is better than the baseline especially at the small number of labeled data. Apart from higher accuracy and lower mean abosolute error, the standard deviations of these two metrics of pre-trained model are also lower than the baseline model.\n\nAs mentioned in previous section, we also monitor mean absolute error and 4-class accuracy for each emotion (happy, sad, anger, surprise, disgust, and fear). According to Fig.  4 , the pre-trained model outperforms the baseline in accuracy of happy, sad, anger, and disgust by large margin. The largest gap between the accuracy of pre-trained and baseline models can be seen in happy and anger in which the pre-trained model outperforms by approximately 20% and 10% respectively. Our pre-trained technique, however, does not improve the accuracy of emotions like surprise and fear. Fig.  4  shows that the baseline model performs slightly better than the pretrained model. This can stem from the fact that these two emotions are relatively uncommon in this dataset according to the distribution in Fig 1 . Thus, the pre-trained model might not be able to benefit from the large amount of unlabeled data in the pre-training step. Furthermore, the accuracy of these two emotions are relatively high around 90%; this can potentially make it difficult to improve beyond this threshold.\n\nThe mean absolute error for each emotion in Fig.  5  also shows similar results. Firstly, we can see the large gap in the mean absolute error of baseline and pre-trained models in most emotions when the number of labels is small; this large gap then starts to shrink as the number of labels increase. The pre-trained model also outperforms the baseline with a large margin in traditionally \"easier\" emotions (e.g., happy, sad, and anger). The benefit of our pre-training technique, however, becomes marginal when coping with more nuanced expressions (e.g., surprise and fear). As discussed in the case of 4-class accuracy, the pre-trained model might not be able to benefit from the the large amount of unlabeled data in the pretraining step when classifying more nuanced expressions due to the scarcity of these emotions in the dataset. Furthermore, the mean absolute error of these nuance expressions are low at approximately 0.12-0.16 compared to the mean absolute error of traditionally \"easier\" emotions which can be as high as 0.4-0.5. Therefore, it might be difficult for our pre-trained model to improve beyond that.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "V. Discussion And Conclusion",
      "text": "Consistent with prior self-supervised learning literature, we find that the benefits of self-supervised learning are observed when there are few supervised labels to learn from. Selfsupervised learning provides marginal benefit when the number of labels is medium-to-large. This can be seen in both of 4class accuracy and mean absolute error; the pre-trained model outperforms the baseline when the number of labels is small (0-200) and gradually converges to the baseline as the number of labels increase. Interestingly, we observe that the benefits of self-supervised learning are most pronounced for traditionally \"easier\" emotions (e.g., happy, sad, and anger) and more difficult for more nuanced expressions (e.g., surprise and fear). We are unaware of prior literature which characterizes this discrepancy. Furthermore, related works on self-supervised learning are usually applying this technique to the raw audio data. This work, on the other hand, applies self-supervised technique to the encoded audio data from COVAREP software  [23] .\n\nAlthough our pre-trained model can significantly outperform the baseline model in both evaluation metrics, there are some notable limitations to this work. Firstly, the baseline and pre-trained models are trained and evaluated for three iterations at each number of labels due to limited computing resources. We can obtain more stable results regarding the benefit of our pre-training technique by increasing the number of iterations at each number of labels. Furthermore, this work only applies self-supervised technique to audio modality. This could be another limitation since this technique can also work with other available modalities within CMU-MOSEI  [9]  or even cross-modality classification as discussed above.\n\nTherefore, it would be interesting to investigate this pretraining technique on different encoded modalities within CMU-MOSEI  [9]  or datasets. Furthermore, we can also use the model pre-trained on audio modality to guide emotion recognition as well as any similar tasks on other available modalities (e.g., visual). We also want to explore alternative architectures of deep neural network (DNNs) that can potentially lead to better performance of our pre-trained model. This pre-training technique can also be applied to the raw audio data in which our results can be compared to the state-of-art methods from other related works.\n\nOn the other hand, the presented SSL methodology can be applied to datasets collected from video and audio data streams. While video-based emotion classifiers often use only visuals to make predictions, the addition of audio can bolster discriminative performance. These improved models can accelerate a variety of applications of affective computing. For example, SuperpowerGlass  [24] -  [31]  is a wearable application that uses real-time computer vision to deliver real-time social cues to children with Autism Spectrum Disorder (ASD). The device uses the outward facing camera to read facial expressions of a conversation partner, and these expressions are classified into discrete Ekman emotions  [21] . The device can provide children with real-time social cues reflecting the emotional expressions evoked by their conversation partners. Guess What?, another digital therapeutic for children with ASD, also curates videos enriched for emotion expression  [32] -  [36] . While these data have traditionally been used for purely visual prediction tasks  [37] ,  [38] , the addition of the audio modality combined with self-supervised learning can bolster performance to enable more personalized healthcare experiences. In general, self-supervised learning has the potential to learn the baseline temporal dynamics of data collected from a highly specialized domain distribution such as in ASD diagnostics. These 'personalized' pre-trained models can be fine-tuned to traditionally complex target tasks like affect recognition.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Vi. Biography Section",
      "text": "",
      "page_start": 6,
      "page_end": 6
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Distribution of emotions in CMU-MOSEI",
      "page": 1
    },
    {
      "caption": "Figure 2: An overview of our proposed methodology for pre-training the deep learning model before transferring pre-trained model to the fine-tuning step",
      "page": 2
    },
    {
      "caption": "Figure 3: An overview of our experiment on how to pre-train and fine-tune the model with a variety of number of annotated datapoints and evaluate the",
      "page": 3
    },
    {
      "caption": "Figure 1: Here, we work with the emotion labels and the acoustic",
      "page": 3
    },
    {
      "caption": "Figure 2: , all 74 parameters of the selected timesteps",
      "page": 3
    },
    {
      "caption": "Figure 4: The 4-class accuracy for each of six emotions (Happy, Sad, Anger, Surprise, Disgust, Fear) along with the standard deviation calculated from three",
      "page": 4
    },
    {
      "caption": "Figure 5: A plot showing the mean absolute error (MAE) for each of six emotions (Happy, Sad, Anger, Surprise, Disgust, Fear) along with the standard",
      "page": 4
    },
    {
      "caption": "Figure 6: The 4-class overall accuracy across all six emotions (Happy, Sad,",
      "page": 5
    },
    {
      "caption": "Figure 7: The MAE across all six emotions (Happy, Sad, Anger, Surprise,",
      "page": 5
    },
    {
      "caption": "Figure 3: , the entire dataset is split into a training",
      "page": 5
    },
    {
      "caption": "Figure 6: , it is clear that the pre-trained model outperforms the",
      "page": 6
    },
    {
      "caption": "Figure 7: , the performance of pre-",
      "page": 6
    },
    {
      "caption": "Figure 1: Thus, the pre-trained model might not",
      "page": 6
    }
  ],
  "tables": [],
  "citations": [
    {
      "citation_id": "1",
      "title": "The growing field of digital psychiatry: current evidence and the future of apps, social media, chatbots, and virtual reality",
      "authors": [
        "J Torous",
        "S Bucci",
        "I Bell",
        "L Kessing",
        "M Faurholt-Jepsen",
        "P Whelan",
        "A Carvalho",
        "M Keshavan",
        "J Linardon",
        "J Firth"
      ],
      "year": "2021",
      "venue": "World Psychiatry"
    },
    {
      "citation_id": "2",
      "title": "Data-driven diagnostics and the potential of mobile artificial intelligence for digital therapeutic phenotyping in computational psychiatry",
      "authors": [
        "P Washington",
        "N Park",
        "P Srivastava",
        "C Voss",
        "A Kline",
        "M Varma",
        "Q Tariq",
        "H Kalantarian",
        "J Schwartz",
        "R Patnaik"
      ],
      "year": "2020",
      "venue": "Biological Psychiatry: Cognitive Neuroscience and Neuroimaging"
    },
    {
      "citation_id": "3",
      "title": "A review of and roadmap for data science and machine learning for the neuropsychiatric phenotype of autism",
      "authors": [
        "P Washington",
        "D Wall"
      ],
      "year": "2023",
      "venue": "A review of and roadmap for data science and machine learning for the neuropsychiatric phenotype of autism",
      "arxiv": "arXiv:2303.03577"
    },
    {
      "citation_id": "4",
      "title": "Automatic emotion recognition in clinical scenario: a systematic review of methods",
      "authors": [
        "L Pepa",
        "L Spalazzi",
        "M Capecci",
        "M Ceravolo"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "5",
      "title": "Emotion recognition for semi-autonomous vehicles framework",
      "authors": [
        "J Izquierdo-Reyes",
        "R Ramirez-Mendoza",
        "M Bustamante-Bello",
        "J Pons-Rovira",
        "J Gonzalez-Vargas"
      ],
      "year": "2018",
      "venue": "International Journal on Interactive Design and Manufacturing (IJIDeM)"
    },
    {
      "citation_id": "6",
      "title": "Automatic emotion recognition for the calibration of autonomous driving functions",
      "authors": [
        "J Sini",
        "A Marceddu",
        "M Violante"
      ],
      "year": "2020",
      "venue": "Electronics"
    },
    {
      "citation_id": "7",
      "title": "Emotion recognition and affective computing on vocal social media",
      "authors": [
        "W Dai",
        "D Han",
        "Y Dai",
        "D Xu"
      ],
      "year": "2015",
      "venue": "Information & Management"
    },
    {
      "citation_id": "8",
      "title": "Video analytics for customer emotion and satisfaction at contact centers",
      "authors": [
        "K Seng",
        "L.-M Ang"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Human-Machine Systems"
    },
    {
      "citation_id": "9",
      "title": "Multi-attention recurrent network for human communication comprehension",
      "authors": [
        "A Zadeh",
        "P Liang",
        "S Poria",
        "P Vij",
        "E Cambria",
        "L.-P Morency"
      ],
      "year": "2018",
      "venue": "Thirty-Second AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "10",
      "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "J Devlin",
        "M.-W Chang",
        "K Lee",
        "K Toutanova"
      ],
      "year": "2019",
      "venue": "ArXiv"
    },
    {
      "citation_id": "11",
      "title": "Does visual self-supervision improve learning of speech representations for emotion recognition?",
      "authors": [
        "A Shukla",
        "S Petridis",
        "M Pantic"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "12",
      "title": "Crema-d: Crowd-sourced emotional multimodal actors dataset",
      "authors": [
        "H Cao",
        "D Cooper",
        "M Keutmann",
        "R Gur",
        "A Nenkova",
        "R Verma"
      ],
      "year": "2014",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "13",
      "title": "The ryerson audio-visual database of emotional speech and song (RAVDESS): A dynamic, multimodal set of facial and vocal expressions in north american english",
      "authors": [
        "S Livingstone",
        "F Russo"
      ],
      "year": "2018",
      "venue": "PLOS ONE",
      "doi": "10.1371/journal.pone.0196391"
    },
    {
      "citation_id": "14",
      "title": "Iemocap: interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "E Kazemzadeh",
        "E Provost",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language Resources and Evaluation"
    },
    {
      "citation_id": "15",
      "title": "Language models are few-shot learners",
      "authors": [
        "T Brown",
        "B Mann",
        "N Ryder",
        "M Subbiah",
        "J Kaplan",
        "P Dhariwal",
        "A Neelakantan",
        "P Shyam",
        "G Sastry",
        "A Askell",
        "S Agarwal",
        "A Herbert-Voss",
        "G Krueger",
        "T Henighan",
        "R Child",
        "A Ramesh",
        "D Ziegler",
        "J Wu",
        "C Winter",
        "C Hesse",
        "M Chen",
        "E Sigler",
        "M Litwin",
        "S Gray",
        "B Chess",
        "J Clark",
        "C Berner",
        "S Mccandlish",
        "A Radford",
        "I Sutskever",
        "D Amodei"
      ],
      "year": "2020",
      "venue": "Language models are few-shot learners"
    },
    {
      "citation_id": "16",
      "title": "Representation learning with contrastive predictive coding",
      "authors": [
        "A Van Den Oord",
        "Y Li",
        "O Vinyals"
      ],
      "year": "2018",
      "venue": "Representation learning with contrastive predictive coding"
    },
    {
      "citation_id": "17",
      "title": "An unsupervised autoregressive model for speech representation learning",
      "authors": [
        "Y.-A Chung",
        "W.-N Hsu",
        "H Tang",
        "J Glass"
      ],
      "year": "2019",
      "venue": "An unsupervised autoregressive model for speech representation learning"
    },
    {
      "citation_id": "18",
      "title": "The kaldi speech recognition toolkit",
      "authors": [
        "D Povey",
        "A Ghoshal",
        "G Boulianne",
        "L Burget",
        "O Glembek",
        "N Goel",
        "M Hannemann",
        "P Motlíček",
        "Y Qian",
        "P Schwarz",
        "J Silovský",
        "G Stemmer",
        "K Vesel"
      ],
      "year": "2011",
      "venue": "IEEE"
    },
    {
      "citation_id": "19",
      "title": "Learning problem-agnostic speech representations from multiple selfsupervised tasks",
      "authors": [
        "S Pascual",
        "M Ravanelli",
        "J Serrà",
        "A Bonafonte",
        "Y Bengio"
      ],
      "year": "2019",
      "venue": "Learning problem-agnostic speech representations from multiple selfsupervised tasks"
    },
    {
      "citation_id": "20",
      "title": "Secost: Sequential co-supervision for weakly labeled audio event detection",
      "authors": [
        "A Kumar",
        "V Ithapu"
      ],
      "year": "1910",
      "venue": "CoRR"
    },
    {
      "citation_id": "21",
      "title": "Facial signs of emotional experience",
      "authors": [
        "F Ekman",
        "S Ancoli"
      ],
      "year": "1980",
      "venue": "Journal of Personality and Social Psychology"
    },
    {
      "citation_id": "22",
      "title": "Multimodal language analysis in the wild: CMU-MOSEI dataset and interpretable dynamic fusion graph",
      "authors": [
        "A Zadeh",
        "P Liang",
        "S Poria",
        "E Cambria",
        "L.-P Morency"
      ],
      "year": "2018",
      "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "23",
      "title": "Covarep -a collaborative voice analysis repository for speech technologies",
      "authors": [
        "G Degottex",
        "J Kane",
        "T Drugman",
        "T Raitio",
        "S Scherer"
      ],
      "year": "2014",
      "venue": "2014 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "24",
      "title": "Superpowerglass: A wearable aid for the at-home therapy of children with autism",
      "authors": [
        "P Washington",
        "C Voss",
        "A Kline",
        "N Haber",
        "J Daniels",
        "A Fazel",
        "T De",
        "C Feinstein",
        "T Winograd",
        "D Wall"
      ],
      "year": "2017",
      "venue": "Proc. ACM Interact. Mob. Wearable Ubiquitous Technol",
      "doi": "10.1145/3130977"
    },
    {
      "citation_id": "25",
      "title": "Superpower glass",
      "authors": [
        "A Kline",
        "C Voss",
        "P Washington",
        "N Haber",
        "H Schwartz",
        "Q Tariq",
        "T Winograd",
        "C Feinstein",
        "D Wall"
      ],
      "year": "2019",
      "venue": "GetMobile: Mobile Computing and Communications"
    },
    {
      "citation_id": "26",
      "title": "Superpower glass: delivering unobtrusive real-time social cues in wearable systems",
      "authors": [
        "C Voss",
        "P Washington",
        "N Haber",
        "A Kline",
        "J Daniels",
        "A Fazel",
        "T De",
        "B Mccarthy",
        "C Feinstein",
        "T Winograd"
      ],
      "year": "2016",
      "venue": "Proceedings of the 2016 ACM International Joint Conference on Pervasive and Ubiquitous Computing: Adjunct"
    },
    {
      "citation_id": "27",
      "title": "Effect of wearable digital intervention for improving socialization in children with autism spectrum disorder: a randomized clinical trial",
      "authors": [
        "C Voss",
        "J Schwartz",
        "J Daniels",
        "A Kline",
        "N Haber",
        "P Washington",
        "Q Tariq",
        "T Robinson",
        "M Desai",
        "J Phillips"
      ],
      "year": "2019",
      "venue": "JAMA pediatrics"
    },
    {
      "citation_id": "28",
      "title": "Exploratory study examining the at-home feasibility of a wearable tool for socialaffective learning in children with autism",
      "authors": [
        "J Daniels",
        "J Schwartz",
        "C Voss",
        "N Haber",
        "A Fazel",
        "A Kline",
        "P Washington",
        "C Feinstein",
        "T Winograd",
        "D Wall"
      ],
      "year": "2018",
      "venue": "NPJ digital medicine"
    },
    {
      "citation_id": "29",
      "title": "superpower glass: An augmented reality intervention for improving social deficits in children with autism spectrum disorder",
      "authors": [
        "A Kline",
        "M Ning",
        "A Husic",
        "P Washington",
        "C Voss",
        "K Dunlap",
        "Y Penev",
        "E Leblanc",
        "N Haber",
        "D Wall"
      ],
      "year": "2020",
      "venue": "INSAR 2020 Virtual Meeting. INSAR"
    },
    {
      "citation_id": "30",
      "title": "A wearable social interaction aid for children with autism",
      "authors": [
        "P Washington",
        "C Voss",
        "N Haber",
        "S Tanaka",
        "J Daniels",
        "C Feinstein",
        "T Winograd",
        "D Wall"
      ],
      "year": "2016",
      "venue": "Proceedings of the 2016 CHI Conference Extended Abstracts on Human Factors in Computing Systems"
    },
    {
      "citation_id": "31",
      "title": "Feasibility testing of a wearable behavioral aid for social learning in children with autism",
      "authors": [
        "J Daniels",
        "N Haber",
        "C Voss",
        "J Schwartz",
        "S Tamura",
        "A Fazel",
        "A Kline",
        "P Washington",
        "J Phillips",
        "T Winograd"
      ],
      "year": "2018",
      "venue": "Applied clinical informatics"
    },
    {
      "citation_id": "32",
      "title": "Labeling images with facial emotion and the potential for pediatric healthcare",
      "authors": [
        "H Kalantarian",
        "K Jedoui",
        "P Washington",
        "Q Tariq",
        "K Dunlap",
        "J Schwartz",
        "D Wall"
      ],
      "year": "2019",
      "venue": "Artificial intelligence in medicine",
      "doi": "10.1016/j.artmed.2019.06.004"
    },
    {
      "citation_id": "33",
      "title": "Guess what? towards understanding autism from structured video using facial affect",
      "authors": [
        "H Kalantarian",
        "P Washington",
        "J Schwartz",
        "J Daniels",
        "N Haber",
        "D Wall"
      ],
      "year": "2019",
      "venue": "Journal of healthcare informatics research"
    },
    {
      "citation_id": "34",
      "title": "A gamified mobile system for crowdsourcing video for autism research",
      "authors": [
        "H Kalantarian",
        "P Washington",
        "J Schwartz",
        "J Daniels",
        "N Haber",
        "D Wall"
      ],
      "year": "2018",
      "venue": "2018 IEEE international conference on healthcare informatics (ICHI)"
    },
    {
      "citation_id": "35",
      "title": "The performance of emotion classifiers for children with parent-reported autism: quantitative feasibility study",
      "authors": [
        "H Kalantarian",
        "K Jedoui",
        "K Dunlap",
        "J Schwartz",
        "P Washington",
        "A Husic",
        "Q Tariq",
        "M Ning",
        "A Kline",
        "D Wall"
      ],
      "year": "2020",
      "venue": "JMIR mental health"
    },
    {
      "citation_id": "36",
      "title": "A mobile game for automatic emotion-labeling of images",
      "authors": [
        "H Kalantarian",
        "K Jedoui",
        "P Washington",
        "D Wall"
      ],
      "year": "2018",
      "venue": "IEEE transactions on games"
    },
    {
      "citation_id": "37",
      "title": "Improved digital therapy for developmental pediatrics using domain-specific artificial intelligence: Machine learning study",
      "authors": [
        "P Washington",
        "H Kalantarian",
        "J Kent",
        "A Husic",
        "A Kline",
        "E Leblanc",
        "C Hou",
        "O Mutlu",
        "K Dunlap",
        "Y Penev"
      ],
      "year": "2022",
      "venue": "JMIR Pediatrics and Parenting"
    },
    {
      "citation_id": "38",
      "title": "Leveraging video data from a digital smartphone autism therapy to train an emotion detection classifier",
      "authors": [
        "C Hou",
        "H Kalantarian",
        "P Washington",
        "K Dunlap",
        "D Wall"
      ],
      "year": "2021",
      "venue": "medRxiv"
    }
  ]
}