{
  "paper_id": "2103.05597v1",
  "title": "A Discriminative Vectorial Framework For Multi-Modal Feature Representation",
  "published": "2021-03-09T18:18:06Z",
  "authors": [
    "Lei Gao",
    "Ling Guan"
  ],
  "keywords": [
    "knowledge discovery",
    "multi-modal feature representation",
    "multi-modal hashing",
    "discriminative correlation maximization",
    "image analysis and recognition",
    "cross-modal analysis",
    "audio emotion recognition"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Due to the rapid advancements of sensory and computing technology, multi-modal data sources that represent the same pattern or phenomenon have attracted growing attention. As a result, finding means to explore useful information from these multi-modal data sources has quickly become a necessity. In this paper, a discriminative vectorial framework is proposed for multi-modal feature representation in knowledge discovery by employing multi-modal hashing (MH) and discriminative correlation maximization (DCM) analysis. Specifically, the proposed framework is capable of minimizing the semantic similarity among different modalities by MH and exacting intrinsic discriminative representations across multiple data sources by DCM analysis jointly, enabling a novel vectorial framework of multimodal feature representation. Moreover, the proposed feature representation strategy is analyzed and further optimized based on canonical and non-canonical cases, respectively. Consequently, the generated feature representation leads to effective utilization of the input data sources of high quality, producing improved, sometimes quite impressive, results in various applications. The effectiveness and generality of the proposed framework are demonstrated by utilizing classical features and deep neural network (DNN) based features with applications to image and multimedia analysis and recognition tasks, including data visualization, face recognition, object recognition; cross-modal (textimage) recognition and audio emotion recognition. Experimental results show that the proposed solutions are superior to state-ofthe-art statistical machine learning (SML) and DNN algorithms.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "K NOWLEDGE discovery is a process of exploring useful information in large volumes of data. It plays a key role in image analysis and synthesis, optimization, highperformance computing and many other tasks  [1] [2] [3] [4] [97] [98] . Nowadays, as sensory and computing technology has rapidly developed, the same pattern or phenomenon can be described from different types of acquisition techniques or even heterogeneous sensors. As a result, multi-modal information analysis has grown at an extremely rapid pace  [5] [6] [114] [115] , clearly demonstrating its potential to improve system performance by utilizing the content across multiple data sources.\n\nNevertheless, as benefits often come at a cost, new challenges in multi-modal information/data analysis also await us. As a central component in knowledge discovery, feature representation transforms the original features into more L. Gao and L. Guan are with the Department of Electrical and Computer Engineering, Ryerson University, Toronto, ON M5B 2K3, Canada (email:iegaolei@gmail.com; lguan@ee.ryerson.ca).\n\neffective representations with improved performance  [4] . In essence, feature representation aims to explore appropriate representations embedded in the data sets for different applications. In recent years, deep neural networks (DNNs) have shown remarkable performance in image computation, classification and other visual information processing tasks  [7] [8] , particularly for feature representation learning in knowledge discovery  [74] . Therefore, the development on feature representation has drawn enormous attention in both academia and industry sectors.\n\nAs one of the important descriptors in multi-modal information analysis, multi-modal hashing (MH) has attracted broad interest and the criterion of semantic similarity is widely applied to multimedia retrieval on data sets of different scales, leading to many successes  [113] . Generally, the elementwise 'sign' function is utilized for evaluation of the semantic similarity. Since outputs of the 'sign' function are two different directions (+1 or -1), essentially, semantic similarity provides a direction criterion to guide the accomplishment of multimedia retrieval among various data sources  [88] . While the existing algorithms have generated promising performance, only the semantic similarity amongst modalities is employed; without considering the intrinsic discriminative representations across modalities. It leads to an even bigger challenge: jointly exploring semantic similarity and intrinsic discriminative representations from different modalities in information fusion.\n\nIn this paper, a discriminative vectorial framework is proposed for multi-modal feature representation. By this framework, both multi-modal hashing (MH) and discriminative correlation maximization (DCM) analysis are exploited to generate a discriminative vectorial vehicle of multi-modal feature representation, providing an effective solution to the aforementioned challenges. Not only is the minimum of the semantic similarity among modalities achieved by MH learning, but the intrinsic discriminative representation is also explored by maximizing the within-class correlation and minimizing the between-class correlation, leading to a high-quality feature representation. Thus, the generated feature representation results in better utilization of the input multi-modal data sources, achieving improved performance. Frameworks of the existing multi-modal hashing method and the proposed strategy are depicted graphically in Figure . 1.\n\nMore importantly, the proposed framework is analyzed and further optimized according to canonical and non-canonical cases respectively, leading to a new way of guiding discriminative projection selection. The generated feature representation is evaluated by comparison with state-of-the-art statistical machine learning (SML) and DNN algorithms. Note, due to In the following, a review of related work is given in Section II. The proposed discriminative vectorial framework is formulated in Section III. In Section IV, feature extraction is introduced. Experimental results and analysis are presented in Section V. Conclusions are summarized in Section VI.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Ii. Related Work",
      "text": "Currently, as a powerful analysis tool in knowledge discovery, hashing analysis has attracted significant interest in academic and industrial sectors  [9, [89] [90] [91] . The purpose of hashing is to transform the original input data to a Hamming space with binary hash codes, where the data dimensionality is largely reduced and the query speed can be dramatically improved  [10] . Benefiting from low dimensionality and fast query speed, hashing has been applied to visual analysis and recognition, image processing and many other tasks  [9] [10] .\n\nHence, the main target of hashing is to accomplish the task of similarity preservation between original input data and the transformed data in a hashing space  [11] . To represent the similarity between original and hash space better, the semantic information is adopted widely in hashing analysis with application to various tasks, such as visual classification, large scale data search, etc  [12] [13] [14] [15] . Lu et al.  [12]  proposed a latent semantic minimal hashing method to produce the appropriate semantic-preserving binary codes according to query images. Zhu et al.  [13]  introduced a semantic-assisted visual hashing method to exploit the semantics between auxiliary texts and images for retrieval. Zhou et al.  [14]  presented a kernel-based semantic hashing model with application in gait retrieval, which can handle the inherent shortcomings of facebased and appearance-based methods. A new discrete semantic transfer hashing algorithm was investigated for image retrieval in  [15] . Based on this algorithm, we can extract rich auxiliary contextual information to improve the semantics of discrete codes.\n\nAlthough there are extensive investigations on hashing analysis, most of them merely focus on a single-view data set, thereby limiting the applications of this technique. Recently, with the development of multi-modal information analysis, greater effort has been committed to multi-modal hashing. Zheng et al.  [16]  presented a fast discrete collaborative multimodal hashing method with applications in large scale multimedia retrieval. Hu et al.  [17]  studied a collective reconstructive embeddings strategy for multi-modal hashing. Wei et al.  [18]  presented a heterogeneous translated hashing strategy which can enhance the multi-view search speed as well as improve similarity accuracies within heterogeneous data. Recently, a deep cross-modal hashing model was introduced to multimedia retrieval applications  [19] . As an end-to-end framework, it can accomplish feature extraction and hashing transform simultaneously.\n\nIn general, multi-modal hashing is divided into two types: multi-source hashing and cross-modal hashing  [20] . The goal of multi-source hashing is to study a better transform strategy by utilizing multi-view data sets rather than a single-view data set  [21] [22] . Comparatively, cross-modal hashing attracts much more attentions and has found wider applications since only one view is required for a query point  [23] . Furthermore, according to the information used in learning, multi-modal hashing is classified into unsupervised multi-modal hashing and supervised multi-modal hashing. As extra supervised information is introduced to supervised multi-modal hashing, it usually achieves better performance than unsupervised techniques. Although most of the existing supervised hashing methods have achieved promising performance in many applications, only the semantic similarity from multiple data sources is utilized, without exploiting intrinsic discriminative representations across different modalities. Therefore, state-ofthe-art is still far from satisfactory.\n\nOn the other hand, in the recent past, the correlation analysis based methods have drawn significant attention towards multi-modal information analysis. As a typical representation of correlation analysis-based methods, canonical correlation analysis (CCA) has been applied to multi-modal information fusion, image classification and more  [24] . Nevertheless, as an unsupervised method, CCA neither represents the similarity between the samples in the same class, nor effectively evaluates the dissimilarity between the samples in different classes. To tackle the aforementioned problem, the withinclass correlation and between-class correlation are utilized jointly to extract more discriminative representations  [25] . However, there exists an challenge as how to explore semantic similarity across modalities to gain better generalization ability for feature representation on the small scale data sets.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Iii. The Proposed Methods",
      "text": "In this section, the outline of multi-modal hashing and discriminative correlation maximization analysis is briefly presented, and then used to formulate the proposed methods.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "A. The Multi-Modal Hashing Method",
      "text": "Suppose we have two sets of variables x = [x 1 , ..., x N ] T ∈ R N ×m and y = [y 1 , ..., y N ] T ∈ R N ×p as the inputs, where m and p equal the number of dimensions of x and y, and N is the number of training samples. In accordance to the aforementioned categories of multi-modal hashing, supervised multi-modal hashing is the focus of study in this paper.\n\nIt is known that semantic labels for training samples in supervised multi-modal hashing are accessible. Semantic labels are expressed in the form of vectors\n\nc }, where c is the number of all classes. Then, l i,k = 1 denotes the ith sample being in the kth class. Otherwise, l i,k = 0. Suppose we use a matrix U ∈ R N ×c to consist of semantic labels of training samples with U i,k = l i,k , where U i,k denotes an element in the ith row and kth column. Then, the matrix of semantic similarity for multi-modal hashing is formulated in equation (  1 )\n\nThe purpose of multi-modal hashing is to learn two hashing functions f () and g() across two data sources f (x i ) : R m → {-1, 1} L and g(y i ) : R p → {-1, 1} L with L being the length of the binary hash codes. While many functions are able to be utilized to define f (x) and g(y), the functions in equation (  2 ) are adopted as\n\nwhere sgn() is the element-wise sign function.\n\nSince the sgn function owns the following property\n\nit yields the following relation\n\nA linear element-wise operation is performed on S to obtain the relation in equation (  5 )\n\nwhere\n\nThus, the solution to the multi-modal hashing is to seek W x and W y with the minimum semantic similarity among modalities, which is formulated as follows  [16] :\n\nwhere ||.|| denotes the norm operation.\n\nIn equation (  6 ), since the 'sign' function is utilized for multi-modal hashing, multi-modal feature representation is achieved by a direction ({-1, 1}) criterion.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "B. The Discriminative Correlation Maximization",
      "text": "be the two variables sets. The mean vector value of x and y is given in equation (  7 )\n\nTherefore,\n\nT are two sets of zero-mean variables, which satisfy the relation in equations (  8 ) and (  9 )\n\nn d is the number of samples in the dth class and it owns the relation in (  10 )\n\nThen, the within-class correlation is\n\nwhere\n\nwhere H n d ×n d is in the form of n d × n d and all the elements in H n d ×n d are unit values. Therefore, the discriminative multimodal correlation function is written as follows\n\nSubstituting C w x ′ y ′ and C b x ′ y ′ into (12) leads to the following relation\n\n) In equation  (13) , it is observed that matrix A plays a central role in extracting discriminative representations. Then, the discriminative correlation maximization is achieved by finding two projected matrices W x ′ and W y ′ in (  14 )\n\nFigure . 3 The representation of the proposed solution.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "C. Discriminative Vectorial Multi-Modal Feature Representation",
      "text": "From the above subsections, the semantic similarity amongst different modalities is capable of providing a 'direction' criterion for effective multi-modal feature representation, but unable to extract discriminative representations effectively.\n\nOn the other side, although the discriminative correlation maximization is capable of exploring more discriminative representations, it only adopts the criterion of 'distance' (ie. within-class correlation and between-class correlation in equation (  12 )), ignoring the direction (semantic similarity) information across different data sources. As a result, it leads to unsatisfied performance, as depicted in Figure  2 . In To address this problem properly, a discriminative vectorial multi-modal feature representation is formulated by integrating the 'direction' and 'distance'. As the matrix A plays an important role in discriminative representations extraction and satisfies the symmetric property, equation (  14 ) is further written as below arg max\n\nTo integrate the multi-modal hashing and discriminative multimodal correlation maximization effectively, in this paper, two mapping functions f () and g() are defined as f\n\nThe objective function of the proposed strategy is written as follows min\n\nThus, the discriminative and semantic representations among different modalities are optimized jointly to enable a novel discriminative vectorial representation of multi-modal features with both 'distance' (discriminative) and 'direction' (semantic) components, and thus parallel to the definition of 'vector' in Physics. This vectorial representation is illustrated in Figure . \n3.\n\nIn the following, the proposed strategy is analyzed and further optimized under the canonical and non-canonical cases, respectively.\n\n1) Canonical Correlation Maximization: Under the canonical condition, two variables sets x ′ and y ′ satisfy equation  (17)    [76] :\n\nwhere I L is an identity matrix and its size is in the form of L × L. Then, according to the spectral relaxation algorithms in  [26] , equation (  16 ) is expressed in (  18 )\n\ns.t.\n\nThen, equation (  18 ) is further formulated in  (20)  min\n\nThe derivation of equation (  20 ) is given in (20.a). Then, equation (  20 ) is rewritten as follows max\n\nmin\n\nwhere tr() is the trace operation of a given matrix. As LN 2 and L 2 tr{[(S ′ ) T S ′ ]} are const, equation (20.a) is turned into the expression in equation  (20) .\n\nWith application of Lagrange multiplier to equation  (21) , it yields\n\nTo find the maximum of (  22 ), it satisfies the following relation\n\nand\n\nThus, the solution to equation (  22 ) is converted to (25)\n\nwhere\n\nThen, equation (  25 ) is addressed by the generalized eigenvalue (GEV) problem. Based on the aforementioned description, the proposed multi-modal hashing with discriminative canonical correlation maximization (MH-DCCM) method is summarized in Algorithm 1.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Algorithm 1 The Mh-Dccm Method",
      "text": "Require: * Calculate the zero-mean sets x ′ and y ′ from the original data sets; * Construct A and S ′ with the semantic label information; Ensure:\n\n* Calculate the matrices\n\n* Find the solutions to  (25) . return W x ′ and W y ′ .",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "2) Non-Canonical Correlation Maximization:",
      "text": "Although we obtain a close-form solution to equation (  25 ) by GEV, it results in the following practical problem. For the canonical correlation analysis, the 'canonical' condition drives the solution to select projected dimensions with low variance, as shown in Figure  4 . Nevertheless, it is acknowledged that there are obvious variances in different projected dimensions and the dimensions with larger-variance potentially contain more significant information  [27, 77] .",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Figure. 4 Directions Selection By Canonical Correlation Analysis",
      "text": "Moreover, it is known that the non-canonical projection vectors are able to obtain better recognition accuracy than that of canonical ones  [28] . As a result, a non-canonical solution is considered in this subsection. In this paper, to find the projected matrices W x ′ and W y ′ to minimize equation (  16 ), an iterative algorithm is presented as follows:\n\nInitially, suppose we have t -1 pairs of projected vectors w 1 x ′ , w 2 x ′ , ..., w t-1 x ′ and w 1 y ′ , w 2 y ′ , ..., w t-1 y ′ . The next step is to obtain the tth vectors w t x ′ and w t y ′ , which satisfy the normalized relation,\n\nDefine a matrix R t in equation (  31 )\n\nThen, equation (  16 ) is converted into (32)\n\n(32) Again, the method of spectral relaxation is performed on equation  (32) , leading to (33)\n\nEquation (  33 ) is converted into (34)\n\nThe derivation of equation (  34 ) is given in (34.a). Equivalently, the solution to equation (  34 ) is expressed in equation (  35 )\n\nDefine the objective function of non-canonical correlation analysis D (t) x ′ y ′ as follows\n\nThen, equation (  35 ) is rewritten as\n\nIn addition, since D (t) x ′ y ′ satisfies the following relation\n\nand the previous vectors w t-1 x ′ and w t-1 y ′ are const for the tth iteration, the solutions (w t x ′ and w t y ′ ) to equation  (37)  are also achieved by the GEV method.\n\nIn this paper, set R 0 = S ′ and the initial D (0) x ′ y ′ is given as follows\n\nThus, according to the aforementioned description, an iterative strategy is proposed and applied to the non-canonical correlation maximization method. Moreover, as both w t x and w t y are achieved by the GEV method, the proposed non-canonical method possesses a closed-form solution. Hence, it is not necessary to force hyper-parameters or stopping conditions on this non-canonical method. This is an important characteristic, especially in large scale problems.\n\nBased on the above analysis and discussion, the proposed multi-modal hashing with discriminative non-canonical correlation maximization (MH-DNCCM) method is summarized in Algorithm 2.\n\nAlgorithm 2 The MH-DNCCM Method Require:\n\n* Calculate the zero-mean sets x ′ and y ′ from the original data sets; * Construct A and S ′ with the semantic label information; Ensure:\n\n* Compute D (0) x ′ y ′ according to A, S ′ , x ′ and y ′ ; For t = 1 → Q ( Q = m + p) do * Calculate the generalized eigenvalue problem in equation  (35) . * Obtain the eigenvector w t x ′ and w t y ′ associated with the largest eigenvalue. * Construct D (t) x ′ y ′ in equation  (36) . * Substitute D (t) x ′ y ′ to equation  (37) .\n\nIV. FEATURE EXTRACTION In this section, features extracted for evaluating the performance of the proposed MH-DCCM and MH-DNCCM are presented. Note, since multi-modal hashing is able to handle both multi-source and cross-modal feature representation, features generated from a single data modality but different algorithms are also utilized to verify the effectiveness of the proposed solutions. To demonstrate the generic nature of the feature representation methods, we conduct experiments based on features generated by different means, including classic features and DNN based features.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "A. Feature Extraction For Data Visualization",
      "text": "In this work, we conducted experiments on the University of California Irvine (UCI) iris data set to visually compare the performance of the proposed solutions. Specifically, a twoclass sub-data set with species of 'setosa' and 'versicolor' by two features, 'sepal length' and 'sepal width'. (a)The sepal length. (b)The sepal width.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "B. Feature Extraction For Face Recognition",
      "text": "In this paper, two kinds of classical features are used in fusion for face recognition  [29] [30] [31] : (c) The histogram of oriented gradients (HOG). (d) Gabor features (6 orientations and 4 scales).",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "C. Feature Extraction For Object Recognition",
      "text": "Recently, a great number of DNN based algorithms have been proposed for object recognition. To further demonstrate\n\nSince the vectors w t x ′ and w t y ′ have been normalized to be const individually and R t is only associated with vectors w 1 x ′ , w 2 x ′ , ..., w t-1 x ′ and w 1 y ′ , w 2 y ′ , ..., w t-1 y ′ , equation (34.a) is turned into the expression in equation  (34) .\n\nthe power of information fusion via the proposed strategy, a relatively simple DNN model, VGG-19  [75] , is employed for DNN based feature extraction and compared with state-of-theart DNN based algorithms. In this paper, two fully connected layers fc7 and fc8 of the VGG-19 model are utilized as DNN based features.\n\n(e) The DNN feature from fc7 layer. (f) The DNN feature from fc8 layer.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "D. Feature Extraction For Text-Image Recognition",
      "text": "Text-image recognition has drawn notable attention within machine learning and data mining communities  [33] [34] [35] . In this paper, we extract two kinds of features for text-image recognition: the bag-of-visual SIFT (BOV-SIFT) vector from images  [36]  and the deep learning-based feature Deep Latent Dirichlet Allocation (DLDA) from texts  [37] . (g) The BOV-SIFT feature. (h) The DLDA feature.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "E. Feature Extraction For Audio Emotion Recognition",
      "text": "Audio is usually considered as one of the most natural and passive types of factors in emotion recognition. Therefore, two audio features, Prosodic and MFCC, are used in this study for human audio emotion recognition  [78] [79] . (i) The Prosodic feature  [80] . (j) The MFCC feature  [80] .",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "V. Experimental Results And Analysis",
      "text": "To demonstrate the effectiveness of MH-DCCM and MH-DNCCM for multi-modal feature representation, we conduct data visualization on the UCI iris data set, face recognition on the ORL face database, object recognition on the Caltech 256 database, cross-modal recognition on the Wiki database and audio emotion recognition on the eNTERFACE (eNT) emotion database, respectively. Note that, during the experiments, the related multi-modal feature fusion experiments are conducted with the same features for fair comparison. The recognition accuracy, which is calculated as the ratio of the number of correctly classified samples over the total number of testing samples, is utilized to evaluate the performance of different algorithms. Moreover, in this work, we conducted experiments with different code length, and the optimal results are reported.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "A. The Uci Iris Data",
      "text": "In this subsection, we construct a two-class sub-data set with species of 'setosa' (50 samples) and 'versicolor' (50 samples) by two features, 'sepal length' and 'sepal width'. Then, the distributions of the two features with the chosen 100 samples from the two species are plotted in Figure . 5. The horizontal coordinate is the index of samples and the vertical coordinate denotes the values of the two features.  In summary, by minimizing the semantic similarity among different features and exacting intrinsic discriminative representations across multiple data features with discriminative correlation maximization analysis jointly, MH-DCCM is able to generate more discriminant representations. According to the analysis in 'Non-Canonical Correlation Maximization', since more significant information is potentially stored in non-canonical projections  [28] , the non-canonical projection vectors are capable of obtaining better recognition accuracy than that of canonical ones.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "B. The Orl Database",
      "text": "The ORL consists of images from 40 persons, with each person providing 10 different samples. Each sample is normalized with size 64 × 64. In this database, some samples were obtained under different conditions, such as illumination, facial expression, and facial detail  [99] [100] [101] [102] . Some samples from the ORL database are given in Figure.      II . For comparison, the performance of state-of-the-art algorithms  [22, 25, [38] [39] [40] [41] [42] [43] [44] [45] [46] [47] [48] [70] [71] [72]  is also presented in  95.50% DSR  [41]  94.50% CRC  [42]  88.50% BULDP  [70]  88.80% PIM  [71]  89.50% L1LS  [43]  92.50% DALM  [44]  90.00% SOLDE-TR  [45]  95.03% GDLMPP  [46]  94.50% CNN  [47]  95.00% PCANet  [48]  96.50% CS-SRC  [72]  96.00% DCCA  [25]  97.00% LCCA  [40]  95.50% CCA  [39]  94.50% Serial Fusion  [38]  77.50%",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "C. The Caltech 256 Database",
      "text": "The Caltech 256 database contains 257 classes, including one background clutter class. Since Caltech 256 database represents a varying set of illumination, movements, backgrounds, etc., it is a challenging object recognition database  [103] [104] [105] [106] . The classes are hand-picked to represent a wide variety of natural and artificial objects in various settings. Several sample images from the Caltech 256 database are given in Figure . 8. In addition, for fair comparison, the experimental settings are similar to the other studies  [49] [50] [51] [52] [53] [54] [55] [56] [57] [58] [59] [60] [61] [62] [63] . Specifically, 30 images are chosen from each class to construct the training subset. Then, the performance of two fully connected layers fc7 and fc8 is tabulated in TABLE III.    64.06% CMFA-SR  [50]  71.44% DGFLP  [51]  69.17% DCS  [52]  69.86% SROSR  [53]  75.60% BMDDL  [54]  59.30% LLKc  [55]  72.09% AL-ALL  [56]  74.20% LMCCA  [40]  76.52% ISC-LG  [57]  50.62% BLF-FV  [58]  51.42% FSDH  [96]  52.50% OCB-FV  [59]  53.15% LLC-SVM  [60]  70.70% SWSS-VGG  [61]  73.56% Hybrid1365-VGG  [87]  76.04% ResNet152  [62]  78.00% SSAH-Q  [95]  78.60% ResFeats152 + PCA-SVM  [63]  79.50% DMCCA  [25]  80.32%",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "D. The Wiki Database",
      "text": "The Wiki database is captured from various featured Wikipedia articles. In total, 2,866 documents are stored in image-text pairs and associated with supervised semantic labels from 10 classes  [107] [108] [109] . In this paper, all documents are further divided into a training subset with 2173 documents and a testing subset with 693 documents, respectively. The performance of BOV-SIFT feature and DLDA feature is tested as a benchmark, as shown in   55.40% SkeletonNet  [68]  56.31% SkeletonNet+pool  [68]  57.67% SSAH  [92]  59.40% CCA  [39]  62.77% LCCA  [40]  63.49% DCCA  [25]  64.79% SePH  [93]  62.59% SCM-Seq  [22]  63.92% MSC  [64]  61.48% MVLS  [65]  62.70% RE-DNN  [66]  63.95% DJSRH  [94]  65.83% L 2,1 CCA  [67]  65.99% SPIB  [69]  66.50%",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "E. The Ent Database",
      "text": "To further validate the effectiveness and verify the generic nature of the proposed methods for feature representation, we extend performance evaluation to audio emotion recognition on the eNT database. This database includes video samples from 43 subjects, expressing six principal emotions  [110] [111] [112] . The audio channel is recorded at a sampling rate of 48000 Hz. During the experiment, 456 audio samples from the eNT database are chosen. Then, the chosen 456 samples are further classified into the training subset and the testing subset, including 360 and 96 samples. Afterwards, the performance of Prosodic and MFCC features in audio channel is investigated and results are reported in TABLE VII.  63.54% FDA  [81]  53.13% LCCA  [40]  64.58% DCCA  [25]  67.71% SCM-Seq  [22]  71.88% DCNN  [82]  72.80% DCNN-DTPM  [82]  76.56% Alexnet (Fine-tuned)  [83]  78.08% TSFFCNN  [84]  73.27% RF  [85]  47.11% RBF-NN  [86]  75.89%\n\nAccording to aforementioned description, the experimental results clearly validated that the generated feature representation from multiple data sources is the winner in all the cases. Since the semantic similarity and intrinsic discriminant representations across different modalities are exploited jointly, it leads to improved performance compared with state-of-the-art. Moreover, since the non-canonical projection vectors potentially contain more important information than the canonical ones, MH-DNCCM always achieves better performance than that of MH-DCCM on the evaluated data sets.",
      "page_start": 9,
      "page_end": 10
    },
    {
      "section_name": "Vi. Conclusions",
      "text": "In this paper, a discriminative vectorial framework is proposed to generate a high quality feature representation from multi-modal features. The presented strategy is mathematically analyzed and further optimized according to canonical and non-canonical cases to implement feature coding for multimodal feature representation. The experimental results clearly demonstrate that the proposed solutions outperform state-ofthe-art on the data sets evaluated.",
      "page_start": 10,
      "page_end": 10
    }
  ],
  "figures": [
    {
      "caption": "Figure 2: In Figure",
      "page": 4
    },
    {
      "caption": "Figure 4: Nevertheless, it is acknowledged that there",
      "page": 5
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "IEEE\nLei Gao, Member, IEEE, and Ling Guan, Fellow,": "Abstract—Due to the rapid advancements of sensory and com-\neffective\nrepresentations with improved performance [4].\nIn"
        },
        {
          "IEEE\nLei Gao, Member, IEEE, and Ling Guan, Fellow,": "puting technology, multi-modal data sources\nthat represent\nthe"
        },
        {
          "IEEE\nLei Gao, Member, IEEE, and Ling Guan, Fellow,": "essence,\nfeature\nrepresentation\naims\nto\nexplore\nappropriate"
        },
        {
          "IEEE\nLei Gao, Member, IEEE, and Ling Guan, Fellow,": "same pattern or phenomenon have attracted growing attention."
        },
        {
          "IEEE\nLei Gao, Member, IEEE, and Ling Guan, Fellow,": "representations\nembedded in the data\nsets\nfor different\nap-"
        },
        {
          "IEEE\nLei Gao, Member, IEEE, and Ling Guan, Fellow,": "As a result, ﬁnding means\nto explore useful\ninformation from"
        },
        {
          "IEEE\nLei Gao, Member, IEEE, and Ling Guan, Fellow,": "plications.\nIn\nrecent\nyears,\ndeep\nneural\nnetworks\n(DNNs)"
        },
        {
          "IEEE\nLei Gao, Member, IEEE, and Ling Guan, Fellow,": "these multi-modal data sources has quickly become a necessity."
        },
        {
          "IEEE\nLei Gao, Member, IEEE, and Ling Guan, Fellow,": "have\nshown remarkable performance in image\ncomputation,\nIn this paper, a discriminative vectorial\nframework is proposed"
        },
        {
          "IEEE\nLei Gao, Member, IEEE, and Ling Guan, Fellow,": "for multi-modal feature representation in knowledge discovery by\nclassiﬁcation and other visual information processing tasks [7-"
        },
        {
          "IEEE\nLei Gao, Member, IEEE, and Ling Guan, Fellow,": "employing multi-modal hashing (MH) and discriminative corre-"
        },
        {
          "IEEE\nLei Gao, Member, IEEE, and Ling Guan, Fellow,": "8], particularly for\nfeature representation learning in knowl-"
        },
        {
          "IEEE\nLei Gao, Member, IEEE, and Ling Guan, Fellow,": "lation maximization (DCM) analysis. Speciﬁcally,\nthe proposed"
        },
        {
          "IEEE\nLei Gao, Member, IEEE, and Ling Guan, Fellow,": "edge discovery [74]. Therefore,\nthe development on feature"
        },
        {
          "IEEE\nLei Gao, Member, IEEE, and Ling Guan, Fellow,": "framework\nis\ncapable\nof minimizing\nthe\nsemantic\nsimilarity"
        },
        {
          "IEEE\nLei Gao, Member, IEEE, and Ling Guan, Fellow,": "representation has drawn enormous attention in both academia"
        },
        {
          "IEEE\nLei Gao, Member, IEEE, and Ling Guan, Fellow,": "among different modalities by MH and exacting intrinsic dis-"
        },
        {
          "IEEE\nLei Gao, Member, IEEE, and Ling Guan, Fellow,": "and industry sectors.\ncriminative representations across multiple data sources by DCM"
        },
        {
          "IEEE\nLei Gao, Member, IEEE, and Ling Guan, Fellow,": "analysis\njointly, enabling a novel vectorial\nframework of multi-\nAs\none\nof\nthe\nimportant\ndescriptors\nin multi-modal\nin-"
        },
        {
          "IEEE\nLei Gao, Member, IEEE, and Ling Guan, Fellow,": "modal\nfeature\nrepresentation. Moreover,\nthe proposed feature"
        },
        {
          "IEEE\nLei Gao, Member, IEEE, and Ling Guan, Fellow,": "formation analysis, multi-modal hashing (MH) has attracted"
        },
        {
          "IEEE\nLei Gao, Member, IEEE, and Ling Guan, Fellow,": "representation strategy is analyzed and further optimized based"
        },
        {
          "IEEE\nLei Gao, Member, IEEE, and Ling Guan, Fellow,": "broad interest and the criterion of semantic similarity is widely"
        },
        {
          "IEEE\nLei Gao, Member, IEEE, and Ling Guan, Fellow,": "on canonical and non-canonical cases, respectively. Consequently,"
        },
        {
          "IEEE\nLei Gao, Member, IEEE, and Ling Guan, Fellow,": "applied to multimedia retrieval on data sets of different scales,"
        },
        {
          "IEEE\nLei Gao, Member, IEEE, and Ling Guan, Fellow,": "the generated feature representation leads to effective utilization"
        },
        {
          "IEEE\nLei Gao, Member, IEEE, and Ling Guan, Fellow,": "leading\nto many\nsuccesses\n[113]. Generally,\nthe\nelement-\nof\nthe input data sources of high quality, producing improved,"
        },
        {
          "IEEE\nLei Gao, Member, IEEE, and Ling Guan, Fellow,": "sometimes quite impressive, results in various applications. The\nwise ‘sign’\nfunction is utilized for evaluation of\nthe semantic"
        },
        {
          "IEEE\nLei Gao, Member, IEEE, and Ling Guan, Fellow,": "effectiveness\nand\ngenerality\nof\nthe\nproposed\nframework\nare"
        },
        {
          "IEEE\nLei Gao, Member, IEEE, and Ling Guan, Fellow,": "similarity. Since outputs of\nthe\n‘sign’\nfunction are\ntwo dif-"
        },
        {
          "IEEE\nLei Gao, Member, IEEE, and Ling Guan, Fellow,": "demonstrated\nby\nutilizing\nclassical\nfeatures\nand\ndeep\nneural"
        },
        {
          "IEEE\nLei Gao, Member, IEEE, and Ling Guan, Fellow,": "ferent directions\n(+1 or\n-1),\nessentially,\nsemantic\nsimilarity"
        },
        {
          "IEEE\nLei Gao, Member, IEEE, and Ling Guan, Fellow,": "network (DNN) based features with applications\nto image and"
        },
        {
          "IEEE\nLei Gao, Member, IEEE, and Ling Guan, Fellow,": "provides a direction criterion to guide the accomplishment of"
        },
        {
          "IEEE\nLei Gao, Member, IEEE, and Ling Guan, Fellow,": "multimedia analysis and recognition tasks,\nincluding data visu-"
        },
        {
          "IEEE\nLei Gao, Member, IEEE, and Ling Guan, Fellow,": "multimedia retrieval among various data sources [88]. While\nalization,\nface recognition, object recognition; cross-modal\n(text-"
        },
        {
          "IEEE\nLei Gao, Member, IEEE, and Ling Guan, Fellow,": "image) recognition and audio emotion recognition. Experimental\nthe existing algorithms have generated promising performance,"
        },
        {
          "IEEE\nLei Gao, Member, IEEE, and Ling Guan, Fellow,": "results show that the proposed solutions are superior to state-of-"
        },
        {
          "IEEE\nLei Gao, Member, IEEE, and Ling Guan, Fellow,": "only the semantic similarity amongst modalities is employed;"
        },
        {
          "IEEE\nLei Gao, Member, IEEE, and Ling Guan, Fellow,": "the-art statistical machine learning (SML) and DNN algorithms."
        },
        {
          "IEEE\nLei Gao, Member, IEEE, and Ling Guan, Fellow,": "without considering the intrinsic discriminative representations"
        },
        {
          "IEEE\nLei Gao, Member, IEEE, and Ling Guan, Fellow,": "across modalities. It\nleads to an even bigger challenge:\njointly"
        },
        {
          "IEEE\nLei Gao, Member, IEEE, and Ling Guan, Fellow,": "Index Terms—knowledge discovery, multi-modal feature repre-"
        },
        {
          "IEEE\nLei Gao, Member, IEEE, and Ling Guan, Fellow,": "exploring semantic similarity and intrinsic discriminative rep-"
        },
        {
          "IEEE\nLei Gao, Member, IEEE, and Ling Guan, Fellow,": "sentation, multi-modal hashing, discriminative correlation max-"
        },
        {
          "IEEE\nLei Gao, Member, IEEE, and Ling Guan, Fellow,": "resentations from different modalities in information fusion."
        },
        {
          "IEEE\nLei Gao, Member, IEEE, and Ling Guan, Fellow,": "imization,\nimage analysis and recognition, cross-modal analysis,"
        },
        {
          "IEEE\nLei Gao, Member, IEEE, and Ling Guan, Fellow,": "In this paper, a discriminative vectorial\nframework is pro-"
        },
        {
          "IEEE\nLei Gao, Member, IEEE, and Ling Guan, Fellow,": "audio emotion recognition."
        },
        {
          "IEEE\nLei Gao, Member, IEEE, and Ling Guan, Fellow,": "posed for multi-modal\nfeature representation. By this\nframe-"
        },
        {
          "IEEE\nLei Gao, Member, IEEE, and Ling Guan, Fellow,": "work,\nboth multi-modal\nhashing\n(MH)\nand\ndiscriminative"
        },
        {
          "IEEE\nLei Gao, Member, IEEE, and Ling Guan, Fellow,": "I.\nINTRODUCTION"
        },
        {
          "IEEE\nLei Gao, Member, IEEE, and Ling Guan, Fellow,": "correlation maximization\n(DCM)\nanalysis\nare\nexploited\nto"
        },
        {
          "IEEE\nLei Gao, Member, IEEE, and Ling Guan, Fellow,": "generate\na\ndiscriminative\nvectorial\nvehicle\nof multi-modal"
        },
        {
          "IEEE\nLei Gao, Member, IEEE, and Ling Guan, Fellow,": "feature representation, providing an effective solution to the\nK NOWLEDGE discovery is a process of exploring useful"
        },
        {
          "IEEE\nLei Gao, Member, IEEE, and Ling Guan, Fellow,": "role\nin\nimage\nanalysis\nand\nsynthesis,\noptimization,\nhigh-\naforementioned challenges. Not only is the minimum of the se-"
        },
        {
          "IEEE\nLei Gao, Member, IEEE, and Ling Guan, Fellow,": "performance computing and many other\ntasks\n[1-4, 97-98].\nmantic similarity among modalities achieved by MH learning,"
        },
        {
          "IEEE\nLei Gao, Member, IEEE, and Ling Guan, Fellow,": "Nowadays, as sensory and computing technology has rapidly\nbut\nthe intrinsic discriminative representation is also explored"
        },
        {
          "IEEE\nLei Gao, Member, IEEE, and Ling Guan, Fellow,": "developed,\nthe same pattern or phenomenon can be described\nby maximizing the within-class\ncorrelation and minimizing"
        },
        {
          "IEEE\nLei Gao, Member, IEEE, and Ling Guan, Fellow,": "from different\ntypes of acquisition techniques or even hetero-\nthe between-class correlation, leading to a high-quality feature"
        },
        {
          "IEEE\nLei Gao, Member, IEEE, and Ling Guan, Fellow,": "geneous sensors. As a result, multi-modal information analysis\nrepresentation. Thus,\nthe generated feature representation re-"
        },
        {
          "IEEE\nLei Gao, Member, IEEE, and Ling Guan, Fellow,": "has grown at an extremely rapid pace [5-6, 114-115], clearly\nsults in better utilization of the input multi-modal data sources,"
        },
        {
          "IEEE\nLei Gao, Member, IEEE, and Ling Guan, Fellow,": "demonstrating its potential\nto improve system performance by\nachieving improved performance. Frameworks of the existing"
        },
        {
          "IEEE\nLei Gao, Member, IEEE, and Ling Guan, Fellow,": "utilizing the content across multiple data sources.\nmulti-modal hashing method and the proposed strategy are"
        },
        {
          "IEEE\nLei Gao, Member, IEEE, and Ling Guan, Fellow,": "Nevertheless, as beneﬁts often come at\na\ncost, new chal-\ndepicted graphically in Figure. 1."
        },
        {
          "IEEE\nLei Gao, Member, IEEE, and Ling Guan, Fellow,": "lenges\nin multi-modal\ninformation/data\nanalysis\nalso\nawait\nMore importantly, the proposed framework is analyzed and"
        },
        {
          "IEEE\nLei Gao, Member, IEEE, and Ling Guan, Fellow,": "us. As\na\ncentral\ncomponent\nin\nknowledge\ndiscovery,\nfea-\nfurther optimized according to canonical\nand non-canonical"
        },
        {
          "IEEE\nLei Gao, Member, IEEE, and Ling Guan, Fellow,": "ture representation transforms the original\nfeatures into more\ncases respectively, leading to a new way of guiding discrimina-"
        },
        {
          "IEEE\nLei Gao, Member, IEEE, and Ling Guan, Fellow,": "tive projection selection. The generated feature representation"
        },
        {
          "IEEE\nLei Gao, Member, IEEE, and Ling Guan, Fellow,": "L. Gao\nand L. Guan\nare with\nthe Department\nof Electrical\nand Com-"
        },
        {
          "IEEE\nLei Gao, Member, IEEE, and Ling Guan, Fellow,": "is\nevaluated\nby\ncomparison with\nstate-of-the-art\nstatistical"
        },
        {
          "IEEE\nLei Gao, Member, IEEE, and Ling Guan, Fellow,": "puter\nEngineering, Ryerson University,\nToronto, ON M5B 2K3, Canada"
        },
        {
          "IEEE\nLei Gao, Member, IEEE, and Ling Guan, Fellow,": "(email:iegaolei@gmail.com;\nlguan@ee.ryerson.ca).\nmachine learning (SML) and DNN algorithms. Note, due to"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Figure. 1 (a) The framework of\nthe existing multi-modal hashing. (b) The framework of\nthe proposed strategy.": "the generic nature of\nthe\nfeature\nrepresentation framework,\nAlthough there are extensive investigations on hashing anal-"
        },
        {
          "Figure. 1 (a) The framework of\nthe existing multi-modal hashing. (b) The framework of\nthe proposed strategy.": "the proposed methods are able to handle input\nfeatures gen-\nysis, most of\nthem merely focus on a\nsingle-view data\nset,"
        },
        {
          "Figure. 1 (a) The framework of\nthe existing multi-modal hashing. (b) The framework of\nthe proposed strategy.": "erated by both the classic features and DNN based features.\nthereby limiting the applications of\nthis\ntechnique. Recently,"
        },
        {
          "Figure. 1 (a) The framework of\nthe existing multi-modal hashing. (b) The framework of\nthe proposed strategy.": "The effectiveness of the proposed framework is demonstrated\nwith\nthe\ndevelopment of multi-modal\ninformation analysis,"
        },
        {
          "Figure. 1 (a) The framework of\nthe existing multi-modal hashing. (b) The framework of\nthe proposed strategy.": "through applications\nto data visualization,\nface\nrecognition,\ngreater\neffort has\nbeen\ncommitted to multi-modal hashing."
        },
        {
          "Figure. 1 (a) The framework of\nthe existing multi-modal hashing. (b) The framework of\nthe proposed strategy.": "object\nrecognition; cross-modal\n(text-image)\nrecognition and\nZheng et al. [16] presented a fast discrete collaborative multi-"
        },
        {
          "Figure. 1 (a) The framework of\nthe existing multi-modal hashing. (b) The framework of\nthe proposed strategy.": "audio emotion recognition.\nmodal hashing method with applications in large scale mul-"
        },
        {
          "Figure. 1 (a) The framework of\nthe existing multi-modal hashing. (b) The framework of\nthe proposed strategy.": "In\nthe\nfollowing,\na\nreview of\nrelated work\nis\ngiven\nin\ntimedia\nretrieval. Hu et\nal.\n[17]\nstudied a\ncollective\nrecon-"
        },
        {
          "Figure. 1 (a) The framework of\nthe existing multi-modal hashing. (b) The framework of\nthe proposed strategy.": "Section II. The proposed discriminative vectorial\nframework\nstructive embeddings strategy for multi-modal hashing. Wei et"
        },
        {
          "Figure. 1 (a) The framework of\nthe existing multi-modal hashing. (b) The framework of\nthe proposed strategy.": "is formulated in Section III.\nIn Section IV,\nfeature extraction\nal.\n[18] presented a heterogeneous translated hashing strategy"
        },
        {
          "Figure. 1 (a) The framework of\nthe existing multi-modal hashing. (b) The framework of\nthe proposed strategy.": "is introduced. Experimental results and analysis are presented\nwhich\ncan\nenhance\nthe multi-view search\nspeed\nas well"
        },
        {
          "Figure. 1 (a) The framework of\nthe existing multi-modal hashing. (b) The framework of\nthe proposed strategy.": "in Section V. Conclusions are summarized in Section VI.\nas\nimprove similarity accuracies within heterogeneous data."
        },
        {
          "Figure. 1 (a) The framework of\nthe existing multi-modal hashing. (b) The framework of\nthe proposed strategy.": "Recently, a deep cross-modal hashing model was\nintroduced"
        },
        {
          "Figure. 1 (a) The framework of\nthe existing multi-modal hashing. (b) The framework of\nthe proposed strategy.": "to multimedia\nretrieval\napplications\n[19]. As\nan end-to-end"
        },
        {
          "Figure. 1 (a) The framework of\nthe existing multi-modal hashing. (b) The framework of\nthe proposed strategy.": "II. RELATED WORK"
        },
        {
          "Figure. 1 (a) The framework of\nthe existing multi-modal hashing. (b) The framework of\nthe proposed strategy.": "framework,\nit can accomplish feature extraction and hashing"
        },
        {
          "Figure. 1 (a) The framework of\nthe existing multi-modal hashing. (b) The framework of\nthe proposed strategy.": "Currently,\nas\na powerful\nanalysis\ntool\nin knowledge dis-\ntransform simultaneously."
        },
        {
          "Figure. 1 (a) The framework of\nthe existing multi-modal hashing. (b) The framework of\nthe proposed strategy.": "covery, hashing analysis has\nattracted signiﬁcant\ninterest\nin\nIn general, multi-modal hashing is divided into two types:"
        },
        {
          "Figure. 1 (a) The framework of\nthe existing multi-modal hashing. (b) The framework of\nthe proposed strategy.": "academic\nand industrial\nsectors\n[9, 89-91]. The purpose of\nmulti-source hashing and cross-modal hashing [20]. The goal"
        },
        {
          "Figure. 1 (a) The framework of\nthe existing multi-modal hashing. (b) The framework of\nthe proposed strategy.": "hashing is to transform the original\ninput data to a Hamming\nof multi-source hashing is to study a better transform strategy"
        },
        {
          "Figure. 1 (a) The framework of\nthe existing multi-modal hashing. (b) The framework of\nthe proposed strategy.": "space with binary hash codes, where the data dimensionality\nby utilizing multi-view data sets rather than a single-view data"
        },
        {
          "Figure. 1 (a) The framework of\nthe existing multi-modal hashing. (b) The framework of\nthe proposed strategy.": "is\nlargely reduced and the query speed can be dramatically\nset [21-22]. Comparatively, cross-modal hashing attracts much"
        },
        {
          "Figure. 1 (a) The framework of\nthe existing multi-modal hashing. (b) The framework of\nthe proposed strategy.": "improved [10]. Beneﬁting from low dimensionality and fast\nmore attentions and has\nfound wider applications since only"
        },
        {
          "Figure. 1 (a) The framework of\nthe existing multi-modal hashing. (b) The framework of\nthe proposed strategy.": "query speed, hashing has been applied to visual analysis and\none\nview is\nrequired\nfor\na\nquery\npoint\n[23]. Furthermore,"
        },
        {
          "Figure. 1 (a) The framework of\nthe existing multi-modal hashing. (b) The framework of\nthe proposed strategy.": "recognition,\nimage processing and many other\ntasks [9-10].\naccording to the\ninformation used in learning, multi-modal"
        },
        {
          "Figure. 1 (a) The framework of\nthe existing multi-modal hashing. (b) The framework of\nthe proposed strategy.": "Hence,\nthe main\ntarget\nof\nhashing\nis\nto\naccomplish\nthe\nhashing is\nclassiﬁed into unsupervised multi-modal hashing"
        },
        {
          "Figure. 1 (a) The framework of\nthe existing multi-modal hashing. (b) The framework of\nthe proposed strategy.": "task of similarity preservation between original\ninput data and\nand\nsupervised multi-modal\nhashing. As\nextra\nsupervised"
        },
        {
          "Figure. 1 (a) The framework of\nthe existing multi-modal hashing. (b) The framework of\nthe proposed strategy.": "the\ntransformed data\nin a hashing space\n[11]. To represent\ninformation\nis\nintroduced\nto\nsupervised multi-modal\nhash-"
        },
        {
          "Figure. 1 (a) The framework of\nthe existing multi-modal hashing. (b) The framework of\nthe proposed strategy.": "the\nsimilarity\nbetween\noriginal\nand\nhash\nspace\nbetter,\nthe\ning,\nit usually achieves better performance than unsupervised"
        },
        {
          "Figure. 1 (a) The framework of\nthe existing multi-modal hashing. (b) The framework of\nthe proposed strategy.": "semantic\ninformation is\nadopted widely in hashing analysis\ntechniques. Although most of\nthe\nexisting supervised hash-"
        },
        {
          "Figure. 1 (a) The framework of\nthe existing multi-modal hashing. (b) The framework of\nthe proposed strategy.": "with application to various tasks, such as visual classiﬁcation,\ning methods have achieved promising performance in many"
        },
        {
          "Figure. 1 (a) The framework of\nthe existing multi-modal hashing. (b) The framework of\nthe proposed strategy.": "large scale data search, etc [12-15]. Lu et al.\n[12] proposed\napplications, only the semantic similarity from multiple data"
        },
        {
          "Figure. 1 (a) The framework of\nthe existing multi-modal hashing. (b) The framework of\nthe proposed strategy.": "a latent semantic minimal hashing method to produce the ap-\nsources is utilized, without exploiting intrinsic discriminative"
        },
        {
          "Figure. 1 (a) The framework of\nthe existing multi-modal hashing. (b) The framework of\nthe proposed strategy.": "propriate semantic-preserving binary codes according to query\nrepresentations across different modalities. Therefore, state-of-"
        },
        {
          "Figure. 1 (a) The framework of\nthe existing multi-modal hashing. (b) The framework of\nthe proposed strategy.": "images. Zhu et al.\n[13]\nintroduced a semantic-assisted visual\nthe-art\nis still\nfar\nfrom satisfactory."
        },
        {
          "Figure. 1 (a) The framework of\nthe existing multi-modal hashing. (b) The framework of\nthe proposed strategy.": "hashing method to exploit\nthe\nsemantics between auxiliary\nOn the other hand,\nin the recent past,\nthe correlation anal-"
        },
        {
          "Figure. 1 (a) The framework of\nthe existing multi-modal hashing. (b) The framework of\nthe proposed strategy.": "texts\nand images\nfor\nretrieval. Zhou et\nal.\n[14] presented a\nysis based methods have drawn signiﬁcant attention towards"
        },
        {
          "Figure. 1 (a) The framework of\nthe existing multi-modal hashing. (b) The framework of\nthe proposed strategy.": "kernel-based semantic hashing model with application in gait\nmulti-modal\ninformation analysis. As a typical\nrepresentation"
        },
        {
          "Figure. 1 (a) The framework of\nthe existing multi-modal hashing. (b) The framework of\nthe proposed strategy.": "retrieval, which can handle the inherent shortcomings of face-\nof\ncorrelation analysis-based methods,\ncanonical\ncorrelation"
        },
        {
          "Figure. 1 (a) The framework of\nthe existing multi-modal hashing. (b) The framework of\nthe proposed strategy.": "based and appearance-based methods. A new discrete semantic\nanalysis (CCA) has been applied to multi-modal\ninformation"
        },
        {
          "Figure. 1 (a) The framework of\nthe existing multi-modal hashing. (b) The framework of\nthe proposed strategy.": "transfer hashing algorithm was investigated for image retrieval\nfusion,\nimage classiﬁcation and more\n[24]. Nevertheless, as"
        },
        {
          "Figure. 1 (a) The framework of\nthe existing multi-modal hashing. (b) The framework of\nthe proposed strategy.": "in [15]. Based on this algorithm, we can extract rich auxiliary\nan unsupervised method, CCA neither\nrepresents the similar-"
        },
        {
          "Figure. 1 (a) The framework of\nthe existing multi-modal hashing. (b) The framework of\nthe proposed strategy.": "contextual\ninformation to improve the\nsemantics of discrete\nity\nbetween the\nsamples\nin\nthe\nsame\nclass,\nnor\neffectively"
        },
        {
          "Figure. 1 (a) The framework of\nthe existing multi-modal hashing. (b) The framework of\nthe proposed strategy.": "codes."
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "3": "where ||.|| denotes the norm operation."
        },
        {
          "3": "In\nequation (6),\nsince\nthe\n‘sign’\nfunction is\nutilized\nfor"
        },
        {
          "3": "multi-modal\nhashing, multi-modal\nfeature\nrepresentation\nis"
        },
        {
          "3": "achieved by a direction ({–1, 1}) criterion."
        },
        {
          "3": ""
        },
        {
          "3": ""
        },
        {
          "3": ""
        },
        {
          "3": "B. The Discriminative Correlation Maximization"
        },
        {
          "3": "Let x = [x1, ..., xN ]T ∈ RN ×m and y = [y1, ..., yN ]T ∈"
        },
        {
          "3": "R N ×p be the two variables sets. The mean vector value of x"
        },
        {
          "3": ""
        },
        {
          "3": "and y is given in equation (7)"
        },
        {
          "3": ""
        },
        {
          "3": ""
        },
        {
          "3": "N\nN"
        },
        {
          "3": "1 N\n1 N\n(7)\nxi, yM =\nyi.\nxM ="
        },
        {
          "3": ""
        },
        {
          "3": "X\nX"
        },
        {
          "3": ""
        },
        {
          "3": ""
        },
        {
          "3": "x′\ny′\nTherefore,\nand\n=\n=\n[x1 − xM , ..., xN − xM ]T"
        },
        {
          "3": ""
        },
        {
          "3": "[y1 − yM , ..., yN − yM ]T are two sets of zero-mean variables,"
        },
        {
          "3": ""
        },
        {
          "3": "which satisfy the relation in equations (8) and (9)"
        },
        {
          "3": ""
        },
        {
          "3": ""
        },
        {
          "3": "(8)\nx′T · 1 = 0 ∈ Rm,"
        },
        {
          "3": ""
        },
        {
          "3": ""
        },
        {
          "3": "(9)\ny′T · 1 = 0 ∈ Rp."
        },
        {
          "3": ""
        },
        {
          "3": ""
        },
        {
          "3": "(d)\n(d)\nare the ith sample in the dth class.\nand y′\nSuppose x′"
        },
        {
          "3": "i\ni"
        },
        {
          "3": "nd is the number of samples in the dth class and it owns the"
        },
        {
          "3": "relation in (10)"
        },
        {
          "3": ""
        },
        {
          "3": "c"
        },
        {
          "3": "(10)\nnd = N."
        },
        {
          "3": ""
        },
        {
          "3": "X"
        },
        {
          "3": ""
        },
        {
          "3": ""
        },
        {
          "3": "Then,\nthe within-class\ncorrelation\nand\nis Cwx′ y′ = x′T Ay′"
        },
        {
          "3": "[25], where\nbetween-class correlation is Cbx′ y′ = −x′T Ay′"
        },
        {
          "3": ""
        },
        {
          "3": "0"
        },
        {
          "3": ". . .\nHn1×n1"
        },
        {
          "3": "N×N\n.\n."
        },
        {
          "3": ".\n.\nA =\n∈ R\n,\n(11)"
        },
        {
          "3": " \n \n \n \n.\n.\nHnd×nd"
        },
        {
          "3": "0"
        },
        {
          "3": "Hnc×nc"
        },
        {
          "3": ""
        },
        {
          "3": "the elements\nis in the form of nd × nd and all\nwhere Hnd×nd"
        },
        {
          "3": ""
        },
        {
          "3": ""
        },
        {
          "3": "in Hnd×nd are unit values. Therefore, the discriminative multi-"
        },
        {
          "3": "modal correlation function is written as follows"
        },
        {
          "3": ""
        },
        {
          "3": "∼"
        },
        {
          "3": ""
        },
        {
          "3": "(12)\nCx′y′ = Cwx′ y′ − Cbx′ y′ ."
        },
        {
          "3": ""
        },
        {
          "3": "into (12) leads to the following\nSubstituting Cwx′ y′ and Cbx′ y′"
        },
        {
          "3": ""
        },
        {
          "3": "relation"
        },
        {
          "3": ""
        },
        {
          "3": "∼"
        },
        {
          "3": "− (−x′T Ay′\n) = 2x′T Ay′.\nCx′y′ = Cwx′ y′ − Cbx′ y′ = x′T Ay′"
        },
        {
          "3": ""
        },
        {
          "3": "(13)"
        },
        {
          "3": ""
        },
        {
          "3": "In equation (13),\nit\nis observed that matrix A plays a central"
        },
        {
          "3": "role\nin\nextracting\ndiscriminative\nrepresentations. Then,\nthe"
        },
        {
          "3": "discriminative correlation maximization is achieved by ﬁnding"
        },
        {
          "3": ""
        },
        {
          "3": "in (14)\nand Wy′\ntwo projected matrices Wx′"
        },
        {
          "3": ""
        },
        {
          "3": "∼"
        },
        {
          "3": "(14)\narg max\nWx′ T\nCx′y′ Wy′."
        },
        {
          "3": "Wx′ ,Wy′"
        },
        {
          "3": ""
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Figure. 3 The representation of\nthe proposed solution.": "C. Discriminative Vectorial Multi-modal Feature Representa-\nsgn(A1/2x′Wx′ ) and g(y′) = sgn(A1/2y′Wy′ ). The objective"
        },
        {
          "Figure. 3 The representation of\nthe proposed solution.": "tion\nfunction of\nthe proposed strategy is written as follows"
        },
        {
          "Figure. 3 The representation of\nthe proposed solution.": "2"
        },
        {
          "Figure. 3 The representation of\nthe proposed solution.": "′\nT\nFrom\nthe\nabove\nsubsections,\nthe\nsemantic\nsimilarity"
        },
        {
          "Figure. 3 The representation of\nthe proposed solution.": ".\n(16)\nmin\n− LS\nsgn(A1/2x′Wx′ )sgn(A1/2y′Wy′ )"
        },
        {
          "Figure. 3 The representation of\nthe proposed solution.": "amongst different modalities is capable of providing a ‘direc-"
        },
        {
          "Figure. 3 The representation of\nthe proposed solution.": "Wx′ ,Wy′ (cid:13)"
        },
        {
          "Figure. 3 The representation of\nthe proposed solution.": "(cid:13)(cid:13)\n(cid:13)(cid:13)(cid:13)\ntion’ criterion for effective multi-modal feature representation,"
        },
        {
          "Figure. 3 The representation of\nthe proposed solution.": "Thus,\nthe discriminative and semantic representations among"
        },
        {
          "Figure. 3 The representation of\nthe proposed solution.": "but unable to extract discriminative representations effectively."
        },
        {
          "Figure. 3 The representation of\nthe proposed solution.": "different modalities\nare optimized jointly to enable\na novel"
        },
        {
          "Figure. 3 The representation of\nthe proposed solution.": "On the other\nside, although the discriminative correlation"
        },
        {
          "Figure. 3 The representation of\nthe proposed solution.": "discriminative vectorial representation of multi-modal features"
        },
        {
          "Figure. 3 The representation of\nthe proposed solution.": "maximization\nis\ncapable\nof\nexploring more\ndiscriminative"
        },
        {
          "Figure. 3 The representation of\nthe proposed solution.": "with both ‘distance’ (discriminative) and ‘direction’ (semantic)"
        },
        {
          "Figure. 3 The representation of\nthe proposed solution.": "representations,\nit\nonly\nadopts\nthe\ncriterion\nof\n‘distance’"
        },
        {
          "Figure. 3 The representation of\nthe proposed solution.": "components, and thus parallel\nto the deﬁnition of\n‘vector’\nin"
        },
        {
          "Figure. 3 The representation of\nthe proposed solution.": "(ie. within-class correlation and between-class correlation in"
        },
        {
          "Figure. 3 The representation of\nthe proposed solution.": "Physics. This vectorial\nrepresentation is illustrated in Figure."
        },
        {
          "Figure. 3 The representation of\nthe proposed solution.": "equation\n(12)),\nignoring\nthe\ndirection\n(semantic\nsimilarity)"
        },
        {
          "Figure. 3 The representation of\nthe proposed solution.": "3."
        },
        {
          "Figure. 3 The representation of\nthe proposed solution.": "information across different data sources. As a result,\nit\nleads"
        },
        {
          "Figure. 3 The representation of\nthe proposed solution.": "In\nthe\nfollowing,\nthe\nproposed\nstrategy\nis\nanalyzed\nand"
        },
        {
          "Figure. 3 The representation of\nthe proposed solution.": "to unsatisﬁed performance, as depicted in Figure 2. In Figure"
        },
        {
          "Figure. 3 The representation of\nthe proposed solution.": "further optimized under the canonical and non-canonical cases,"
        },
        {
          "Figure. 3 The representation of\nthe proposed solution.": "2,\ndue\nto\na\nsmaller\n‘distance’,\none\nsample\nof\nclass\n1\nis"
        },
        {
          "Figure. 3 The representation of\nthe proposed solution.": "respectively."
        },
        {
          "Figure. 3 The representation of\nthe proposed solution.": "misclassiﬁed into class 3 while another sample from class 3"
        },
        {
          "Figure. 3 The representation of\nthe proposed solution.": "1) Canonical Correlation Maximization: Under the canon-\nis misclassiﬁed into class 1."
        },
        {
          "Figure. 3 The representation of\nthe proposed solution.": "ical\ncondition,\ntwo variables\nsets x′\nand y′\nsatisfy equation"
        },
        {
          "Figure. 3 The representation of\nthe proposed solution.": "(17) [76]:"
        },
        {
          "Figure. 3 The representation of\nthe proposed solution.": ")*\"++&’"
        },
        {
          "Figure. 3 The representation of\nthe proposed solution.": ")*\"++&,\n)*\"++&(\nWx′ T x′T Ax′Wx′ = N IL,"
        },
        {
          "Figure. 3 The representation of\nthe proposed solution.": "(17)"
        },
        {
          "Figure. 3 The representation of\nthe proposed solution.": "Wy′ T y′T Ay′Wy′ = N IL,"
        },
        {
          "Figure. 3 The representation of\nthe proposed solution.": ":;!&)%-#!%-01<&"
        },
        {
          "Figure. 3 The representation of\nthe proposed solution.": "size is\nin the form of\nwhere IL is an identity matrix and its\n3\"=1-#$7!&0%&7-+#\"1/!"
        },
        {
          "Figure. 3 The representation of\nthe proposed solution.": "!\"#$%!&’\n)*\"++&’\nrelaxation algorithms\nL × L. Then, according to the spectral"
        },
        {
          "Figure. 3 The representation of\nthe proposed solution.": "2-+/%-3-1\"#-4!&5$*#-6307\"*&"
        },
        {
          "Figure. 3 The representation of\nthe proposed solution.": ")*\"++-.-/\"#-01\n)*\"++&("
        },
        {
          "Figure. 3 The representation of\nthe proposed solution.": ")0%%!*\"#-01&5\"8-3-9\"#-01\nin [26], equation (16) is expressed in (18)\n !\"#$%!&(\n)*\"++&,"
        },
        {
          "Figure. 3 The representation of\nthe proposed solution.": "Figure. 2 The representation by discriminative multi-modal\n2"
        },
        {
          "Figure. 3 The representation of\nthe proposed solution.": "′\nT"
        },
        {
          "Figure. 3 The representation of\nthe proposed solution.": "(A1/2x′W ′"
        },
        {
          "Figure. 3 The representation of\nthe proposed solution.": ",\n(18)\nmin\n− LS\ncorrelation maximization\nx)(A1/2y′W ′\ny)"
        },
        {
          "Figure. 3 The representation of\nthe proposed solution.": "W ′\nx,W ′"
        },
        {
          "Figure. 3 The representation of\nthe proposed solution.": "(cid:13)(cid:13)(cid:13)\ny (cid:13)"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "With application of Lagrange multiplier": "yields",
          "to equation (21),\nit": "",
          "2) Non-Canonical Correlation Maximization: Although we": "obtain\na\nclose-form solution\nto\nequation\n(25)\nby GEV,\nit"
        },
        {
          "With application of Lagrange multiplier": "′",
          "to equation (21),\nit": "",
          "2) Non-Canonical Correlation Maximization: Although we": "results\nin the following practical problem. For\nthe canonical"
        },
        {
          "With application of Lagrange multiplier": "J(Wx′ , Wy′) = tr{2[(Wx′ T x′T A1/2)S",
          "to equation (21),\nit": "(A1/2y′Wy′ )]}−",
          "2) Non-Canonical Correlation Maximization: Although we": ""
        },
        {
          "With application of Lagrange multiplier": "",
          "to equation (21),\nit": "",
          "2) Non-Canonical Correlation Maximization: Although we": "correlation analysis,\nthe ‘canonical’ condition drives\nthe so-"
        },
        {
          "With application of Lagrange multiplier": "λ2\n{[(A1/2x′Wx′ )T A1/2x′Wx′) + (A1/2y′Wy′ )T A1/2y′Wy′ ]",
          "to equation (21),\nit": "",
          "2) Non-Canonical Correlation Maximization: Although we": ""
        },
        {
          "With application of Lagrange multiplier": "",
          "to equation (21),\nit": "",
          "2) Non-Canonical Correlation Maximization: Although we": "lution to select projected dimensions with low variance,\nas"
        },
        {
          "With application of Lagrange multiplier": "−2N IL}.",
          "to equation (21),\nit": "",
          "2) Non-Canonical Correlation Maximization: Although we": "shown in Figure 4. Nevertheless,\nit\nis acknowledged that\nthere"
        },
        {
          "With application of Lagrange multiplier": "",
          "to equation (21),\nit": "(22)",
          "2) Non-Canonical Correlation Maximization: Although we": ""
        },
        {
          "With application of Lagrange multiplier": "",
          "to equation (21),\nit": "",
          "2) Non-Canonical Correlation Maximization: Although we": "are obvious variances\nin different projected dimensions and"
        },
        {
          "With application of Lagrange multiplier": "To ﬁnd the maximum of (22), it satisﬁes the following relation",
          "to equation (21),\nit": "",
          "2) Non-Canonical Correlation Maximization: Although we": ""
        },
        {
          "With application of Lagrange multiplier": "",
          "to equation (21),\nit": "",
          "2) Non-Canonical Correlation Maximization: Although we": "the dimensions with larger-variance potentially contain more"
        },
        {
          "With application of Lagrange multiplier": "∂J(Wx′, Wy′ )",
          "to equation (21),\nit": "",
          "2) Non-Canonical Correlation Maximization: Although we": "signiﬁcant\ninformation [27, 77]."
        },
        {
          "With application of Lagrange multiplier": "= 0,",
          "to equation (21),\nit": "(23)",
          "2) Non-Canonical Correlation Maximization: Although we": ""
        },
        {
          "With application of Lagrange multiplier": "∂Wx′",
          "to equation (21),\nit": "",
          "2) Non-Canonical Correlation Maximization: Although we": ""
        },
        {
          "With application of Lagrange multiplier": "and",
          "to equation (21),\nit": "",
          "2) Non-Canonical Correlation Maximization: Although we": ""
        },
        {
          "With application of Lagrange multiplier": "∂J(Wx′, Wy′ )",
          "to equation (21),\nit": "",
          "2) Non-Canonical Correlation Maximization: Although we": ""
        },
        {
          "With application of Lagrange multiplier": "= 0.",
          "to equation (21),\nit": "(24)",
          "2) Non-Canonical Correlation Maximization: Although we": "Directions selection"
        },
        {
          "With application of Lagrange multiplier": "∂Wy′",
          "to equation (21),\nit": "",
          "2) Non-Canonical Correlation Maximization: Although we": ""
        },
        {
          "With application of Lagrange multiplier": "",
          "to equation (21),\nit": "",
          "2) Non-Canonical Correlation Maximization: Although we": "Feature space 1"
        },
        {
          "With application of Lagrange multiplier": "Thus,\nthe solution to equation (22) is converted to (25)",
          "to equation (21),\nit": "",
          "2) Non-Canonical Correlation Maximization: Although we": "Correlation\nDirections"
        },
        {
          "With application of Lagrange multiplier": "",
          "to equation (21),\nit": "",
          "2) Non-Canonical Correlation Maximization: Although we": "abandon"
        },
        {
          "With application of Lagrange multiplier": "0\nRx′y′\nRx′x′",
          "to equation (21),\nit": "0",
          "2) Non-Canonical Correlation Maximization: Although we": ""
        },
        {
          "With application of Lagrange multiplier": "W = λ",
          "to equation (21),\nit": "W,\n(25)",
          "2) Non-Canonical Correlation Maximization: Although we": "Variance"
        },
        {
          "With application of Lagrange multiplier": "0\n0\nRy′x′\n(cid:20)\n(cid:21)\n(cid:20)",
          "to equation (21),\nit": "Ry′y′\n(cid:21)",
          "2) Non-Canonical Correlation Maximization: Although we": ""
        },
        {
          "With application of Lagrange multiplier": "",
          "to equation (21),\nit": "",
          "2) Non-Canonical Correlation Maximization: Although we": "Canonical correlation analysis"
        },
        {
          "With application of Lagrange multiplier": "",
          "to equation (21),\nit": "",
          "2) Non-Canonical Correlation Maximization: Although we": "Canonical correlation space"
        },
        {
          "With application of Lagrange multiplier": "where",
          "to equation (21),\nit": "",
          "2) Non-Canonical Correlation Maximization: Although we": "Feature space 2"
        },
        {
          "With application of Lagrange multiplier": "Rx′x′ = x′T Ax′,",
          "to equation (21),\nit": "(26)",
          "2) Non-Canonical Correlation Maximization: Although we": "Figure. 4 Directions selection by canonical correlation"
        },
        {
          "With application of Lagrange multiplier": "",
          "to equation (21),\nit": "",
          "2) Non-Canonical Correlation Maximization: Although we": "analysis"
        },
        {
          "With application of Lagrange multiplier": "Ry′y′ = y′T Ay′,",
          "to equation (21),\nit": "(27)",
          "2) Non-Canonical Correlation Maximization: Although we": ""
        },
        {
          "With application of Lagrange multiplier": "",
          "to equation (21),\nit": "",
          "2) Non-Canonical Correlation Maximization: Although we": "Moreover,\nit\nis\nknown\nthat\nthe\nnon-canonical projection"
        },
        {
          "With application of Lagrange multiplier": "′",
          "to equation (21),\nit": "",
          "2) Non-Canonical Correlation Maximization: Although we": ""
        },
        {
          "With application of Lagrange multiplier": "Rx′y′ = x′T A1/2S",
          "to equation (21),\nit": "A1/2y′,\n(28)",
          "2) Non-Canonical Correlation Maximization: Although we": "vectors are able to obtain better recognition accuracy than that"
        },
        {
          "With application of Lagrange multiplier": "",
          "to equation (21),\nit": "",
          "2) Non-Canonical Correlation Maximization: Although we": "of canonical ones [28]. As a result, a non-canonical solution"
        },
        {
          "With application of Lagrange multiplier": "Ry′x′ = Rx′y′ T ,",
          "to equation (21),\nit": "(29)",
          "2) Non-Canonical Correlation Maximization: Although we": ""
        },
        {
          "With application of Lagrange multiplier": "",
          "to equation (21),\nit": "",
          "2) Non-Canonical Correlation Maximization: Although we": "is\nconsidered in\nthis\nsubsection.\nIn\nthis\npaper,\nto ﬁnd\nthe"
        },
        {
          "With application of Lagrange multiplier": "",
          "to equation (21),\nit": "",
          "2) Non-Canonical Correlation Maximization: Although we": "to minimize equation (16),\nand Wy′\nprojected matrices Wx′"
        },
        {
          "With application of Lagrange multiplier": "Wx′",
          "to equation (21),\nit": "",
          "2) Non-Canonical Correlation Maximization: Although we": ""
        },
        {
          "With application of Lagrange multiplier": ".\nW =",
          "to equation (21),\nit": "(30)",
          "2) Non-Canonical Correlation Maximization: Although we": "an iterative algorithm is presented as follows:"
        },
        {
          "With application of Lagrange multiplier": "Wy′\n(cid:20)\n(cid:21)",
          "to equation (21),\nit": "",
          "2) Non-Canonical Correlation Maximization: Although we": ""
        },
        {
          "With application of Lagrange multiplier": "",
          "to equation (21),\nit": "",
          "2) Non-Canonical Correlation Maximization: Although we": "Initially, suppose we have t − 1 pairs of projected vectors"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "in Algorithm 1.": "",
          "Deﬁne a matrix Rt": "t−1",
          "in equation (31)": ""
        },
        {
          "in Algorithm 1.": "",
          "Deﬁne a matrix Rt": "",
          "in equation (31)": ""
        },
        {
          "in Algorithm 1.": "Algorithm 1 The MH-DCCM Method",
          "Deﬁne a matrix Rt": "",
          "in equation (31)": ""
        },
        {
          "in Algorithm 1.": "",
          "Deﬁne a matrix Rt": "sgn(A1/2x′wk",
          "in equation (31)": ""
        },
        {
          "in Algorithm 1.": "Require:",
          "Deﬁne a matrix Rt": "k=1\nX",
          "in equation (31)": ""
        },
        {
          "in Algorithm 1.": "",
          "Deﬁne a matrix Rt": "",
          "in equation (31)": "is converted into (32)"
        },
        {
          "in Algorithm 1.": "data sets;",
          "Deﬁne a matrix Rt": "",
          "in equation (31)": ""
        },
        {
          "in Algorithm 1.": "",
          "Deﬁne a matrix Rt": "",
          "in equation (31)": ""
        },
        {
          "in Algorithm 1.": "* Construct A and S",
          "Deﬁne a matrix Rt": "sgn(A1/2x′wt",
          "in equation (31)": "x′)sgn(A1/2y′wt"
        },
        {
          "in Algorithm 1.": "",
          "Deﬁne a matrix Rt": "",
          "in equation (31)": ""
        },
        {
          "in Algorithm 1.": "",
          "Deﬁne a matrix Rt": "",
          "in equation (31)": ""
        },
        {
          "in Algorithm 1.": "Ensure:",
          "Deﬁne a matrix Rt": "",
          "in equation (31)": ""
        },
        {
          "in Algorithm 1.": "",
          "Deﬁne a matrix Rt": "",
          "in equation (31)": ""
        },
        {
          "in Algorithm 1.": "",
          "Deﬁne a matrix Rt": "",
          "in equation (31)": ""
        },
        {
          "in Algorithm 1.": "",
          "Deﬁne a matrix Rt": "the method\nof\nspectral",
          "in equation (31)": "relaxation"
        },
        {
          "in Algorithm 1.": "* Find the solutions to (25).",
          "Deﬁne a matrix Rt": "",
          "in equation (31)": ""
        },
        {
          "in Algorithm 1.": "",
          "Deﬁne a matrix Rt": "leading to (33)",
          "in equation (31)": ""
        },
        {
          "in Algorithm 1.": "return Wx′",
          "Deﬁne a matrix Rt": "",
          "in equation (31)": ""
        },
        {
          "in Algorithm 1.": "",
          "Deﬁne a matrix Rt": "",
          "in equation (31)": ""
        },
        {
          "in Algorithm 1.": "",
          "Deﬁne a matrix Rt": "",
          "in equation (31)": "T"
        },
        {
          "in Algorithm 1.": "",
          "Deﬁne a matrix Rt": "(A1/2x′wt",
          "in equation (31)": "y′)\nx′)(A1/2y′wt"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "6": "Algorithm 2 The MH-DNCCM Method"
        },
        {
          "6": ""
        },
        {
          "6": "Require:"
        },
        {
          "6": ""
        },
        {
          "6": "* Calculate the zero-mean sets x′ and y′\nfrom the original"
        },
        {
          "6": ""
        },
        {
          "6": ""
        },
        {
          "6": "data sets;"
        },
        {
          "6": "′"
        },
        {
          "6": "* Construct A and S\nwith the semantic label\ninformation;"
        },
        {
          "6": ""
        },
        {
          "6": "Ensure:"
        },
        {
          "6": "′"
        },
        {
          "6": "* Compute D(0)\naccording to A, S\n, x′ and y′;\nx′y′"
        },
        {
          "6": "For t = 1 → Q ( Q = m + p) do"
        },
        {
          "6": "* Calculate the generalized eigenvalue problem in equation"
        },
        {
          "6": "(35)."
        },
        {
          "6": ""
        },
        {
          "6": ""
        },
        {
          "6": "* Obtain the eigenvector wt\ny′ associated with the\nx′ and wt"
        },
        {
          "6": "largest eigenvalue."
        },
        {
          "6": ""
        },
        {
          "6": "in equation (36).\n* Construct D(t)\nx′y′"
        },
        {
          "6": ""
        },
        {
          "6": "to equation (37).\n* Substitute D(t)\nx′y′"
        },
        {
          "6": "End for"
        },
        {
          "6": "x′],\nx′, ...wQ\nx′, w2\nreturn Wx′ = [w1"
        },
        {
          "6": ""
        },
        {
          "6": "y′].\ny′, ...wQ\ny′ , w2\nWy′ = [w1"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "coordinate stands for the projection of the original features in": ""
        },
        {
          "coordinate stands for the projection of the original features in": "the semantic similarity space, DCM space, MH–DCCM space"
        },
        {
          "coordinate stands for the projection of the original features in": ""
        },
        {
          "coordinate stands for the projection of the original features in": "respectively."
        },
        {
          "coordinate stands for the projection of the original features in": ""
        },
        {
          "coordinate stands for the projection of the original features in": ""
        },
        {
          "coordinate stands for the projection of the original features in": ""
        },
        {
          "coordinate stands for the projection of the original features in": ""
        },
        {
          "coordinate stands for the projection of the original features in": ""
        },
        {
          "coordinate stands for the projection of the original features in": ""
        },
        {
          "coordinate stands for the projection of the original features in": ""
        },
        {
          "coordinate stands for the projection of the original features in": ""
        },
        {
          "coordinate stands for the projection of the original features in": ""
        },
        {
          "coordinate stands for the projection of the original features in": ""
        },
        {
          "coordinate stands for the projection of the original features in": ""
        },
        {
          "coordinate stands for the projection of the original features in": ""
        },
        {
          "coordinate stands for the projection of the original features in": "20\n25\n30"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "7": "10\nSetosa in Sepal Length"
        },
        {
          "7": "Setosa in Sepal Width\n9"
        },
        {
          "7": "Versicolor in Sepal Length\nVersicolor in Sepal Width\n8"
        },
        {
          "7": "7"
        },
        {
          "7": "6"
        },
        {
          "7": "5"
        },
        {
          "7": "4"
        },
        {
          "7": "3"
        },
        {
          "7": "2"
        },
        {
          "7": "1"
        },
        {
          "7": "0"
        },
        {
          "7": "0\n5\n10\n15\n20\n25\n30\n35\n40\n45\n50"
        },
        {
          "7": "Figure. 5. The original distribution of\nthe two features"
        },
        {
          "7": ""
        },
        {
          "7": "Next, we conducted experiments by the semantic similarity,"
        },
        {
          "7": ""
        },
        {
          "7": "the discriminative correlation maximization (DCM),\nthe pro-"
        },
        {
          "7": ""
        },
        {
          "7": "posed MH-DCCM and MH-DNCCM with the distributions"
        },
        {
          "7": ""
        },
        {
          "7": "plotted in Figure. 6 (a),\n(b),\n(c)\nand (d). Among them,\nthe"
        },
        {
          "7": ""
        },
        {
          "7": "horizontal coordinate is the index of samples and the vertical"
        },
        {
          "7": ""
        },
        {
          "7": "coordinate stands for the projection of the original features in"
        },
        {
          "7": ""
        },
        {
          "7": "the semantic similarity space, DCM space, MH–DCCM space"
        },
        {
          "7": ""
        },
        {
          "7": "and MH–DNCCM space,\nrespectively."
        },
        {
          "7": ""
        },
        {
          "7": "0.16"
        },
        {
          "7": "Setosa in Sepal Length"
        },
        {
          "7": "Setosa in Sepal Width"
        },
        {
          "7": "Versicolor in Sepal Length\n0.14"
        },
        {
          "7": "Versicolor in Sepal Width"
        },
        {
          "7": "0.12"
        },
        {
          "7": "0.1"
        },
        {
          "7": "0.08"
        },
        {
          "7": "0.06"
        },
        {
          "7": ""
        },
        {
          "7": "0.04"
        },
        {
          "7": "0\n5\n10\n15\n20\n25\n30\n35\n40\n45\n50"
        },
        {
          "7": ""
        },
        {
          "7": "Figure. 6(a). The visualization of semantic similarity"
        },
        {
          "7": "1.5"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "discriminant representations.": "(3) Beneﬁting from strengths of\nthe semantic similarity and"
        },
        {
          "discriminant representations.": ""
        },
        {
          "discriminant representations.": "discriminative\ncorrelation maximization, MH-DCCM brings"
        },
        {
          "discriminant representations.": ""
        },
        {
          "discriminant representations.": "out more discriminant representations than semantic similarity"
        },
        {
          "discriminant representations.": ""
        },
        {
          "discriminant representations.": "alone."
        },
        {
          "discriminant representations.": ""
        },
        {
          "discriminant representations.": "(4) Compared with the MH-DCCM solution, MH-NDCCM"
        },
        {
          "discriminant representations.": ""
        },
        {
          "discriminant representations.": "explores more discriminant\ninformation."
        },
        {
          "discriminant representations.": ""
        },
        {
          "discriminant representations.": ""
        },
        {
          "discriminant representations.": "In summary, by minimizing the semantic similarity among"
        },
        {
          "discriminant representations.": ""
        },
        {
          "discriminant representations.": "different\nfeatures and exacting intrinsic discriminative repre-"
        },
        {
          "discriminant representations.": ""
        },
        {
          "discriminant representations.": "sentations\nacross multiple data\nfeatures with\ndiscriminative"
        },
        {
          "discriminant representations.": ""
        },
        {
          "discriminant representations.": "correlation maximization analysis jointly, MH-DCCM is able"
        },
        {
          "discriminant representations.": ""
        },
        {
          "discriminant representations.": "to generate more discriminant\nrepresentations. According to"
        },
        {
          "discriminant representations.": ""
        },
        {
          "discriminant representations.": "the\nanalysis\nin\n‘Non-Canonical Correlation Maximization’,"
        },
        {
          "discriminant representations.": ""
        },
        {
          "discriminant representations.": "since more\nsigniﬁcant\ninformation\nis\npotentially\nstored\nin"
        },
        {
          "discriminant representations.": ""
        },
        {
          "discriminant representations.": "non-canonical projections\n[28],\nthe non-canonical projection"
        },
        {
          "discriminant representations.": ""
        },
        {
          "discriminant representations.": "vectors are capable of obtaining better\nrecognition accuracy"
        },
        {
          "discriminant representations.": ""
        },
        {
          "discriminant representations.": "than that of canonical ones."
        },
        {
          "discriminant representations.": ""
        },
        {
          "discriminant representations.": ""
        },
        {
          "discriminant representations.": "B. The ORL Database"
        },
        {
          "discriminant representations.": "The ORL\nconsists\nof\nimages\nfrom 40\npersons, with"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "8": "Next, we conducted experiments with MH-DCCM & MH-"
        },
        {
          "8": "DNCCM methods and the results are given in Table II. For"
        },
        {
          "8": "comparison,\nthe\nperformance\nof\nstate-of-the-art\nalgorithms"
        },
        {
          "8": "[22, 25, 38-48, 70-72]\nis\nalso presented in TABLE II. The"
        },
        {
          "8": "comparison\nclearly\nshows\nthe\nsuperiority\nof\nthe\nproposed"
        },
        {
          "8": "algorithms."
        },
        {
          "8": "TABLE II"
        },
        {
          "8": "THE PERFORMANCE WITH DIFFERENT ALGORITHMS ON"
        },
        {
          "8": ""
        },
        {
          "8": "THE ORL DATABASE"
        },
        {
          "8": ""
        },
        {
          "8": ""
        },
        {
          "8": "Methods\nAccuracy"
        },
        {
          "8": ""
        },
        {
          "8": "MH-DNCCM\n98.00%"
        },
        {
          "8": ""
        },
        {
          "8": "MH-DCCM\n97.50%"
        },
        {
          "8": ""
        },
        {
          "8": "SCM-Seq [22]\n95.50%"
        },
        {
          "8": ""
        },
        {
          "8": "DSR [41]\n94.50%"
        },
        {
          "8": ""
        },
        {
          "8": "CRC [42]\n88.50%"
        },
        {
          "8": "BULDP [70]\n88.80%"
        },
        {
          "8": ""
        },
        {
          "8": "PIM [71]\n89.50%"
        },
        {
          "8": ""
        },
        {
          "8": "L1LS [43]\n92.50%"
        },
        {
          "8": ""
        },
        {
          "8": "DALM [44]\n90.00%"
        },
        {
          "8": ""
        },
        {
          "8": "SOLDE-TR [45]\n95.03%"
        },
        {
          "8": ""
        },
        {
          "8": "GDLMPP [46]\n94.50%"
        },
        {
          "8": ""
        },
        {
          "8": "CNN [47]\n95.00%"
        },
        {
          "8": ""
        },
        {
          "8": "PCANet\n[48]\n96.50%"
        },
        {
          "8": ""
        },
        {
          "8": "CS-SRC [72]\n96.00%"
        },
        {
          "8": ""
        },
        {
          "8": "DCCA [25]\n97.00%"
        },
        {
          "8": ""
        },
        {
          "8": "LCCA [40]\n95.50%"
        },
        {
          "8": "CCA [39]\n94.50%"
        },
        {
          "8": "Serial Fusion [38]\n77.50%"
        },
        {
          "8": ""
        },
        {
          "8": ""
        },
        {
          "8": "C. The Caltech 256 Database"
        },
        {
          "8": ""
        },
        {
          "8": "The Caltech 256 database contains 257 classes,\nincluding"
        },
        {
          "8": "one background clutter class. Since Caltech 256 database rep-"
        },
        {
          "8": "resents a varying set of illumination, movements, backgrounds,"
        },
        {
          "8": "etc.,\nit\nis a challenging object recognition database [103-106]."
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Feature": "",
          "Accuracy": ""
        },
        {
          "Feature": "BOV-SIFT",
          "Accuracy": "17.46%"
        },
        {
          "Feature": "",
          "Accuracy": ""
        },
        {
          "Feature": "DLDA",
          "Accuracy": "66.23%"
        },
        {
          "Feature": "TABLE VI",
          "Accuracy": ""
        },
        {
          "Feature": "THE PERFORMANCE WITH DIFFERENT ALGORITHMS ON",
          "Accuracy": ""
        },
        {
          "Feature": "",
          "Accuracy": ""
        },
        {
          "Feature": "THE WIKI DATABASE",
          "Accuracy": ""
        },
        {
          "Feature": "",
          "Accuracy": ""
        },
        {
          "Feature": "",
          "Accuracy": ""
        },
        {
          "Feature": "Methods",
          "Accuracy": "Accuracy"
        },
        {
          "Feature": "",
          "Accuracy": ""
        },
        {
          "Feature": "MH-DNCCM",
          "Accuracy": "69.26%"
        },
        {
          "Feature": "MH-DCCM",
          "Accuracy": "68.11%"
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "TABLE IV. Again, MH-DNCCM outperforms state-of-the-art": "methods in object recognition, including SML and DNN based"
        },
        {
          "TABLE IV. Again, MH-DNCCM outperforms state-of-the-art": ""
        },
        {
          "TABLE IV. Again, MH-DNCCM outperforms state-of-the-art": "algorithms, such as ResNet\n[62–63]."
        },
        {
          "TABLE IV. Again, MH-DNCCM outperforms state-of-the-art": ""
        },
        {
          "TABLE IV. Again, MH-DNCCM outperforms state-of-the-art": "TABLE IV"
        },
        {
          "TABLE IV. Again, MH-DNCCM outperforms state-of-the-art": ""
        },
        {
          "TABLE IV. Again, MH-DNCCM outperforms state-of-the-art": ""
        },
        {
          "TABLE IV. Again, MH-DNCCM outperforms state-of-the-art": "CALTECH 256 DATABASE"
        },
        {
          "TABLE IV. Again, MH-DNCCM outperforms state-of-the-art": ""
        },
        {
          "TABLE IV. Again, MH-DNCCM outperforms state-of-the-art": ""
        },
        {
          "TABLE IV. Again, MH-DNCCM outperforms state-of-the-art": "Methods"
        },
        {
          "TABLE IV. Again, MH-DNCCM outperforms state-of-the-art": ""
        },
        {
          "TABLE IV. Again, MH-DNCCM outperforms state-of-the-art": "MH-DNCCM"
        },
        {
          "TABLE IV. Again, MH-DNCCM outperforms state-of-the-art": ""
        },
        {
          "TABLE IV. Again, MH-DNCCM outperforms state-of-the-art": "MH-DCCM"
        },
        {
          "TABLE IV. Again, MH-DNCCM outperforms state-of-the-art": ""
        },
        {
          "TABLE IV. Again, MH-DNCCM outperforms state-of-the-art": "HMML [49]"
        },
        {
          "TABLE IV. Again, MH-DNCCM outperforms state-of-the-art": ""
        },
        {
          "TABLE IV. Again, MH-DNCCM outperforms state-of-the-art": "CMFA-SR [50]"
        },
        {
          "TABLE IV. Again, MH-DNCCM outperforms state-of-the-art": ""
        },
        {
          "TABLE IV. Again, MH-DNCCM outperforms state-of-the-art": "DGFLP [51]"
        },
        {
          "TABLE IV. Again, MH-DNCCM outperforms state-of-the-art": ""
        },
        {
          "TABLE IV. Again, MH-DNCCM outperforms state-of-the-art": "DCS [52]"
        },
        {
          "TABLE IV. Again, MH-DNCCM outperforms state-of-the-art": ""
        },
        {
          "TABLE IV. Again, MH-DNCCM outperforms state-of-the-art": "SROSR [53]"
        },
        {
          "TABLE IV. Again, MH-DNCCM outperforms state-of-the-art": ""
        },
        {
          "TABLE IV. Again, MH-DNCCM outperforms state-of-the-art": "BMDDL [54]"
        },
        {
          "TABLE IV. Again, MH-DNCCM outperforms state-of-the-art": ""
        },
        {
          "TABLE IV. Again, MH-DNCCM outperforms state-of-the-art": "LLKc [55]"
        },
        {
          "TABLE IV. Again, MH-DNCCM outperforms state-of-the-art": ""
        },
        {
          "TABLE IV. Again, MH-DNCCM outperforms state-of-the-art": "AL-ALL [56]"
        },
        {
          "TABLE IV. Again, MH-DNCCM outperforms state-of-the-art": ""
        },
        {
          "TABLE IV. Again, MH-DNCCM outperforms state-of-the-art": "LMCCA [40]"
        },
        {
          "TABLE IV. Again, MH-DNCCM outperforms state-of-the-art": ""
        },
        {
          "TABLE IV. Again, MH-DNCCM outperforms state-of-the-art": "ISC-LG [57]"
        },
        {
          "TABLE IV. Again, MH-DNCCM outperforms state-of-the-art": "BLF-FV [58]"
        },
        {
          "TABLE IV. Again, MH-DNCCM outperforms state-of-the-art": "FSDH [96]"
        },
        {
          "TABLE IV. Again, MH-DNCCM outperforms state-of-the-art": "OCB-FV [59]"
        },
        {
          "TABLE IV. Again, MH-DNCCM outperforms state-of-the-art": "LLC-SVM [60]"
        },
        {
          "TABLE IV. Again, MH-DNCCM outperforms state-of-the-art": ""
        },
        {
          "TABLE IV. Again, MH-DNCCM outperforms state-of-the-art": "SWSS-VGG [61]"
        },
        {
          "TABLE IV. Again, MH-DNCCM outperforms state-of-the-art": ""
        },
        {
          "TABLE IV. Again, MH-DNCCM outperforms state-of-the-art": "Hybrid1365-VGG [87]"
        },
        {
          "TABLE IV. Again, MH-DNCCM outperforms state-of-the-art": ""
        },
        {
          "TABLE IV. Again, MH-DNCCM outperforms state-of-the-art": "ResNet152 [62]"
        },
        {
          "TABLE IV. Again, MH-DNCCM outperforms state-of-the-art": ""
        },
        {
          "TABLE IV. Again, MH-DNCCM outperforms state-of-the-art": "SSAH-Q [95]"
        },
        {
          "TABLE IV. Again, MH-DNCCM outperforms state-of-the-art": ""
        },
        {
          "TABLE IV. Again, MH-DNCCM outperforms state-of-the-art": "ResFeats152 + PCA-SVM [63]"
        },
        {
          "TABLE IV. Again, MH-DNCCM outperforms state-of-the-art": ""
        },
        {
          "TABLE IV. Again, MH-DNCCM outperforms state-of-the-art": "DMCCA [25]"
        },
        {
          "TABLE IV. Again, MH-DNCCM outperforms state-of-the-art": ""
        },
        {
          "TABLE IV. Again, MH-DNCCM outperforms state-of-the-art": ""
        },
        {
          "TABLE IV. Again, MH-DNCCM outperforms state-of-the-art": ""
        },
        {
          "TABLE IV. Again, MH-DNCCM outperforms state-of-the-art": ""
        },
        {
          "TABLE IV. Again, MH-DNCCM outperforms state-of-the-art": "D. The Wiki Database"
        },
        {
          "TABLE IV. Again, MH-DNCCM outperforms state-of-the-art": ""
        },
        {
          "TABLE IV. Again, MH-DNCCM outperforms state-of-the-art": "The Wiki\ndatabase\nis\ncaptured"
        },
        {
          "TABLE IV. Again, MH-DNCCM outperforms state-of-the-art": ""
        },
        {
          "TABLE IV. Again, MH-DNCCM outperforms state-of-the-art": "Wikipedia\narticles.\nIn total,"
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "10": "[7] Y. LeCun, Y. Bengio, and G. Hinton. “Deep learning.” nature, vol. 521,"
        },
        {
          "10": "pp. 436–444, 2015."
        },
        {
          "10": ""
        },
        {
          "10": "[8]\nJ. Han, D. Zhang, G. Cheng, N. Liu, and D. Xu. “Advanced deep-learning"
        },
        {
          "10": "techniques\nfor\nsalient\nand category-speciﬁc object detection:\na survey.”"
        },
        {
          "10": "IEEE Signal Processing Magazine, vol. 35, no. 1, pp. 84–100, 2018."
        },
        {
          "10": "[9]\nJ. Wang, T. Zhang, N. Sebe, and H.T. Shen. “A survey on learning to"
        },
        {
          "10": "hash.” IEEE transactions on pattern analysis and machine intelligence,"
        },
        {
          "10": "vol. 40, no. 4, pp. 769–790, 2017."
        },
        {
          "10": "[10] M. Norouzi, A. Punjani, and D.J. Fleet. “Fast search in hamming space"
        },
        {
          "10": "with multi-index hashing.” 2012 IEEE Conference on Computer Vision"
        },
        {
          "10": ""
        },
        {
          "10": "and Pattern Recognition, pp. 3108–3115, 2012."
        },
        {
          "10": "[11] V.E. Liong, J. Lu, Y.P. Tan, and J. Zhou. “Deep video hashing.” IEEE"
        },
        {
          "10": "Transactions on Multimedia, vol. 19, no. 6, pp. 1209–1219, 2016."
        },
        {
          "10": "[12] X. Lu, X. Zheng,\nand X. Li.\n“Latent\nsemantic minimal hashing\nfor"
        },
        {
          "10": ""
        },
        {
          "10": "image retrieval.” IEEE Transactions on Image Processing, vol. 26, no. 1,"
        },
        {
          "10": ""
        },
        {
          "10": "pp. 355–368, 2016."
        },
        {
          "10": "[13]\nL. Zhu,\nJ. Shen, L. Xie, and Z. Cheng. “Unsupervised visual hashing"
        },
        {
          "10": "with semantic assistant for content-based image retrieval.” IEEE Transac-"
        },
        {
          "10": ""
        },
        {
          "10": "tions on Knowledge and Data Engineering, vol. 29, no. 2, pp. 472–486,"
        },
        {
          "10": ""
        },
        {
          "10": "2016."
        },
        {
          "10": "[14] Y. Zhou, Y. Huang, Q. Hu,\nand L. Wang.\n“Kernel-Based\nSemantic"
        },
        {
          "10": "Hashing for Gait Retrieval.” IEEE Transactions on Circuits and Systems"
        },
        {
          "10": ""
        },
        {
          "10": "for Video Technology, vol. 28, no. 10, pp. 2742–2752, 2017."
        },
        {
          "10": ""
        },
        {
          "10": "[15]\nL. Zhu, Z. Huang, Z. Li, L. Xie, and H.T. Shen. “Exploring auxiliary"
        },
        {
          "10": "context: discrete semantic transfer hashing for scalable image retrieval.”"
        },
        {
          "10": "IEEE transactions on neural networks and learning systems, vol. 29, no."
        },
        {
          "10": "11, pp. 5264–5276, 2018."
        },
        {
          "10": "[16] C. Zheng, L. Zhu, X. Lu, J. Li, Z. Cheng, H. Zhang. “Fast Discrete Col-"
        },
        {
          "10": "laborative Multi-modal Hashing for Large-scale Multimedia Retrieval.”"
        },
        {
          "10": "IEEE Transactions on Knowledge and Data Engineering (Early Access),"
        },
        {
          "10": ""
        },
        {
          "10": "2019."
        },
        {
          "10": ""
        },
        {
          "10": "[17] M. Hu, Y. Yang, F. Shen, N. Xie, R. Hong, and H.T. Shen. “Collective"
        },
        {
          "10": "IEEE Transac-\nReconstructive Embeddings\nfor Cross-Modal Hashing.”"
        },
        {
          "10": "tions on Image Processing, vol. 28, no. 6, pp. 2770–2784, 2018."
        },
        {
          "10": ""
        },
        {
          "10": "[18] Y. Wei, Y.\nSong, Y. Zhen, B. Liu,\nand Q. Yang.\n“Heterogeneous"
        },
        {
          "10": ""
        },
        {
          "10": "translated\nhashing: A scalable\nsolution\ntowards multi-modal\nsimilarity"
        },
        {
          "10": "search.” ACM Transactions on Knowledge Discovery from Data (TKDD),"
        },
        {
          "10": "vol. 10, no. 4, pp. 36–63, 2016."
        },
        {
          "10": "[19] Q.Y. Jiang, and W.J. Li. “Deep cross-modal hashing.” IEEE conference"
        },
        {
          "10": ""
        },
        {
          "10": "on computer vision and pattern recognition, pp. 3232–3240. 2017."
        },
        {
          "10": ""
        },
        {
          "10": "[20]\nJ. Song, Y. Yang, Z. Huang, H.T. Shen, and R. Hong. “Multiple feature"
        },
        {
          "10": "hashing for real-time large scale near-duplicate video retrieval.” 19th ACM"
        },
        {
          "10": "international conference on Multimedia, pp. 423–432, 2011."
        },
        {
          "10": ""
        },
        {
          "10": "[21] D. Zhang, F. Wang, and L. Si. “Composite hashing with multiple infor-"
        },
        {
          "10": "mation sources.” 34th international ACM SIGIR conference on Research"
        },
        {
          "10": "and development\nin Information Retrieval, pp. 225–234, 2011."
        },
        {
          "10": ""
        },
        {
          "10": "[22] D. Zhang, and W.J. Li. “Large-scale supervised multimodal hashing with"
        },
        {
          "10": ""
        },
        {
          "10": "semantic correlation maximization.” Twenty-Eighth AAAI Conference on"
        },
        {
          "10": "Artiﬁcial\nIntelligence, pp. 2177–2183, 2014."
        },
        {
          "10": "[23] Y. Wang, X. Lin, L. Wu, W. Zhang, and Q. Zhang. “Lbmch: Learning"
        },
        {
          "10": ""
        },
        {
          "10": "38th\ninternational\nACM\nbridging mapping\nfor\ncross-modal\nhashing.”"
        },
        {
          "10": ""
        },
        {
          "10": "SIGIR conference on research and development\nin information retrieval,"
        },
        {
          "10": "pp. 999–1002, 2015."
        },
        {
          "10": "[24] N. M. Correa, T. Adali, Y. Li, and V. D. Calhoun. “Canonical correlation"
        },
        {
          "10": "IEEE signal processing\nanalysis\nfor data\nfusion and group inferences.”"
        },
        {
          "10": "magazine, vol. 27, no. 4, pp. 39–50, 2010."
        },
        {
          "10": "[25]\nL. Gao, L. Qi, E. Chen, and L. Guan. “Discriminative multiple canonical"
        },
        {
          "10": "correlation analysis for information fusion.” IEEE Transactions on Image"
        },
        {
          "10": ""
        },
        {
          "10": "Processing, vol. 27, no. 4, pp. 1951–1965, 2018."
        },
        {
          "10": ""
        },
        {
          "10": "[26] A. Torralba, R. Fergus,\nand Y. Weiss. “Small\ncodes\nand large\nimage"
        },
        {
          "10": ""
        },
        {
          "10": "2008 IEEE conference\non computer\nvision\ndatabases\nfor\nrecognition,”"
        },
        {
          "10": ""
        },
        {
          "10": "and pattern recognition, pp. 1–8, 2008."
        },
        {
          "10": ""
        },
        {
          "10": "in neural\n[27] W. Kong,\nand W.J. Li. “Isotropic hashing.” 2012 Advances"
        },
        {
          "10": ""
        },
        {
          "10": "information processing systems, pp. 1646–1654, 2012."
        },
        {
          "10": ""
        },
        {
          "10": "[28]\nJ. Wang,\nS. Kumar,\nand\nS.F. Chang.\n“Semi-supervised\nhashing\nfor"
        },
        {
          "10": ""
        },
        {
          "10": "large-scale search.” IEEE Transactions on Pattern Analysis and Machine"
        },
        {
          "10": ""
        },
        {
          "10": "Intelligence, vol. 34, no. 12, pp. 2393–2406, 2012."
        },
        {
          "10": ""
        },
        {
          "10": "[29] D. Huang, C. Zhu, Y. Wang, and L. Chen “HSOG: a novel\nlocal\nimage"
        },
        {
          "10": "descriptor based on histograms of the second-order gradients.” IEEE TIP,"
        },
        {
          "10": "vol. 23, no. 11, pp.4680-4695, 2014."
        },
        {
          "10": "[30] A. Satpathy, X. Jiang, and H.L. Eng. “LBP-based edge-texture features"
        },
        {
          "10": "for object\nrecognition.” IEEE TIP, vol. 23, no. 5, pp. 1953–1964, 2014."
        },
        {
          "10": "[31] M. Yang, and L. Zhang. “Gabor\nfeature based sparse representation for"
        },
        {
          "10": "face recognition with gabor occlusion dictionary.” ECCV, pp. 448–461,"
        },
        {
          "10": "2010."
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "11": "[56] K. Wang, D. Y. Zhang, Y. Li, R. M. Zhang, and L. Lin. “Cost–effective"
        },
        {
          "11": "IEEE Transactions\non\nactive\nlearning\nfor\ndeep\nimage\nclassiﬁcation.”"
        },
        {
          "11": "Circuits and Systems\nfor Video Technology, vol. 27, no. 12, pp. 2591–"
        },
        {
          "11": "2600, 2017."
        },
        {
          "11": "[57] C. J. Zhang, J. Cheng, C. Li, and Q. Tian. “Image-speciﬁc classiﬁcation"
        },
        {
          "11": "IEEE transactions\non\nneural\nwith\nlocal\nand\nglobal\ndiscriminations.”"
        },
        {
          "11": "networks and learning systems. vol. 29, no. 9, 4479–4486, 2018."
        },
        {
          "11": "[58] C. J. Zhang, J. Sang, G. B. Zhu, and Q. Tian. “Bundled local\nfeatures"
        },
        {
          "11": "for image representation.” IEEE Transactions on Circuits and Systems for"
        },
        {
          "11": "Video Technology, vol. 28, no. 8, 1719–1726, 2018."
        },
        {
          "11": "[59] C. Zhang, G. Zhu, C. Liang, Y. Zhang, Q. Huang, and Q. Tian. “Image"
        },
        {
          "11": "class prediction by joint object, context, and background modeling.” IEEE"
        },
        {
          "11": "Transactions on Circuits and Systems for Video Technology, vol. 28, no."
        },
        {
          "11": "2, pp. 428–438, 2018."
        },
        {
          "11": "[60] W. Luo,\nJ. Li,\nJ. Yang, W. Xu,\nand J. Zhang.\n“Convolutional\nsparse"
        },
        {
          "11": "IEEE transactions\non\nneural\nautoencoders\nfor\nimage\nclassiﬁcation.”"
        },
        {
          "11": "networks and learning systems, vol. 29, no. 7, 3289–3294, 2018."
        },
        {
          "11": "[61] C.\nJ. zhang,\nJ. Cheng, and Q. Tian. “Structured weak semantic space"
        },
        {
          "11": "IEEE transactions\non\nneural\nconstruction\nfor\nvisual\ncategorization.”"
        },
        {
          "11": ""
        },
        {
          "11": "networks and learning systems, vol. 29, no. 8, pp. 3442–3451, 2018."
        },
        {
          "11": ""
        },
        {
          "11": "[62] K. M. He, X. Zhang, S. Ren, and J. Sun. “Deep residual\nlearning for"
        },
        {
          "11": ""
        },
        {
          "11": "the IEEE conference on computer\nimage recognition.” In Proceedings of"
        },
        {
          "11": ""
        },
        {
          "11": "vision and pattern recognition, pp. 770–778. 2016"
        },
        {
          "11": ""
        },
        {
          "11": "[63] A. Mahmood, M. Bennamoun, S. An and F. Sohel. “Resfeats: Residual"
        },
        {
          "11": ""
        },
        {
          "11": "network based features for image classiﬁcation.” 2017 IEEE International"
        },
        {
          "11": ""
        },
        {
          "11": "Conference on In Image Processing (ICIP), pp. 1597–1601, 2017."
        },
        {
          "11": ""
        },
        {
          "11": "[64] R. He, M. Zhang, L. Wang, Y. Ji, and Q. Yin. “Cross-modal\nsubspace"
        },
        {
          "11": ""
        },
        {
          "11": "learning via pairwise constraints.” IEEE Transactions on Image Process-"
        },
        {
          "11": ""
        },
        {
          "11": "ing, vol. 24, no. 12, pp. 5543–5556, 2015."
        },
        {
          "11": ""
        },
        {
          "11": "[65] C. Zhang,\nJ. Cheng, and Q. Tian. “Multiview label\nsharing for visual"
        },
        {
          "11": ""
        },
        {
          "11": "representations\nand classiﬁcations.”\nIEEE Transactions on Multimedia,"
        },
        {
          "11": ""
        },
        {
          "11": "vol. 20, no. 4, pp. 903–913, 2017."
        },
        {
          "11": ""
        },
        {
          "11": "[66] C. Wang, H. Yang,\nand C. Meinel.\n“A deep semantic\nframework for"
        },
        {
          "11": ""
        },
        {
          "11": "multimodal\nrepresentation learning.” Multimedia Tools and Applications,"
        },
        {
          "11": ""
        },
        {
          "11": "vol. 75, no. 15, pp. 9255–9276, 2016."
        },
        {
          "11": ""
        },
        {
          "11": "[67] M. Xu, Z. Zhu, X. Zhang, Y. Zhao, and X. Li. “Canonical Correlation"
        },
        {
          "11": ""
        },
        {
          "11": "IEEE\nAnalysis With L2,1-Norm for Multiview Data Representation.”"
        },
        {
          "11": ""
        },
        {
          "11": "transactions on cybernetics\n(Early Access), 2019."
        },
        {
          "11": ""
        },
        {
          "11": "[68]\nS. Yang, L. Li, S. Wang, W. Zhang, Q. Huang, and Q. Tian. “Skeleton-"
        },
        {
          "11": ""
        },
        {
          "11": "Net: A Hybrid Network with a Skeleton-Embedding Process\nfor Multi-"
        },
        {
          "11": ""
        },
        {
          "11": "view Image Representation Learning.” IEEE Transactions on Multimedia"
        },
        {
          "11": ""
        },
        {
          "11": "(Early Access), 2019."
        },
        {
          "11": ""
        },
        {
          "11": "[69] X. Yan, Y. Ye, Y. Mao, and H. Yu. “Shared-private information bottle-"
        },
        {
          "11": ""
        },
        {
          "11": "neck method for cross-modal clustering.” IEEE Access, vol. 7, pp. 36045–"
        },
        {
          "11": ""
        },
        {
          "11": "36056, 2019."
        },
        {
          "11": ""
        },
        {
          "11": "[70] X. Ning, W. Li, B. Tang, and H. He. “BULDP: biomimetic uncorrelated"
        },
        {
          "11": ""
        },
        {
          "11": "locality discriminant projection for feature extraction in face recognition.”"
        },
        {
          "11": ""
        },
        {
          "11": "IEEE Transactions on Image Processing, vol. 27, no. 5, pp. 2575–2586,"
        },
        {
          "11": ""
        },
        {
          "11": "2018."
        },
        {
          "11": ""
        },
        {
          "11": "[71] B. Xu, and Q. Liu. “Iterative projection based sparse reconstruction for"
        },
        {
          "11": ""
        },
        {
          "11": "face recognition.” Neurocomputing, vol. 284, pp. 99–106, 2018."
        },
        {
          "11": ""
        },
        {
          "11": "[72] M. Banitalebi-Dehkordi, A. Banitalebi-Dehkordi,\nJ. Abouei, and K.N."
        },
        {
          "11": ""
        },
        {
          "11": "Plataniotis.\n“Face\nrecognition\nusing\na\nnew compressive\nsensing-based"
        },
        {
          "11": ""
        },
        {
          "11": "feature extraction method.” Multimedia Tools and Applications, vol. 77,"
        },
        {
          "11": ""
        },
        {
          "11": "no. 11, pp. 14007–14027, 2018."
        },
        {
          "11": ""
        },
        {
          "11": "[73] V.E. Liong, J. Lu, Y.P. Tan, and J. Zhou. “Deep coupled metric learning"
        },
        {
          "11": ""
        },
        {
          "11": "for cross-modal matching.”\nIEEE Transactions on Multimedia, vol. 19,"
        },
        {
          "11": "no. 6, pp. 1234–1244, 2017."
        },
        {
          "11": ""
        },
        {
          "11": "[74] Y. Bengio, A. Courville,\nand P. Vincent.\n“Representation\nlearning: A"
        },
        {
          "11": ""
        },
        {
          "11": "review and new perspectives.” IEEE transactions on pattern analysis and"
        },
        {
          "11": "machine intelligence, vol. 35, no. 8, pp. 1798–1828, 2013."
        },
        {
          "11": "[75] K. Simonyan, A. Zisserman.\n“Very\ndeep\nconvolutional\nnetworks\nfor"
        },
        {
          "11": "large-scale image recognition.” 2015 International Conference on Learn-"
        },
        {
          "11": "ing Representations, pp. 1–14, 2015."
        },
        {
          "11": "[76]\nL. Gao, and L. Guan. “Information fusion via multimodal hashing with"
        },
        {
          "11": "2019\nInternational\ndiscriminant\ncanonical\ncorrelation maximization.”"
        },
        {
          "11": "Conference\non\nImage Analysis\nand Recognition,\npp.\n81–93, Springer,"
        },
        {
          "11": "2019."
        },
        {
          "11": "[77]\nL. Gao,\nand L. Guan.\n“Information Fusion via Multimodal Hashing"
        },
        {
          "11": "With Discriminant Correlation Maximization.” 2019 IEEE International"
        },
        {
          "11": "Conference on Image Processing (ICIP), pp. 2224–2228, 2019."
        },
        {
          "11": "[78] N. Sebe,\nI. Cohen, T. Gevers,\nand T.S. Huang.\n“Emotion recognition"
        },
        {
          "11": "based on joint visual and audio cues.” 18th International Conference on"
        },
        {
          "11": "Pattern Recognition (ICPR’06), vol. 1, pp. 1136–1139, 2006."
        },
        {
          "11": "[79]\nE. Ayadi, M.S. Kamel,\nand\nF. Karray.\n“Survey\non\nspeech\nemotion"
        },
        {
          "11": "Pattern\nrecognition:\nFeatures,\nclassiﬁcation\nschemes,\nand\ndatabases.”"
        },
        {
          "11": "Recognition, vol. 44, no. 3, pp. 572–587, 2011."
        }
      ],
      "page": 11
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "12": "[103]\nI. Ghalyan, and F.\nJasim. “Estimation of ergodicity limits of bag-of-"
        },
        {
          "12": "words modeling for guaranteed stochastic convergence.” Pattern Recog-"
        },
        {
          "12": "nition (Early Access), 2020."
        },
        {
          "12": "[104]\nL. Zhou, P. Cui, X. Jia, S. Yang, and Q. Tian. “Learning to Select Base"
        },
        {
          "12": "In Proceedings of\nthe\nIEEE/CVF\nClasses\nfor Few-shot Classiﬁcation.”"
        },
        {
          "12": "Conference on Computer Vision and Pattern Recognition, pp. 4624–4633,"
        },
        {
          "12": "2020."
        },
        {
          "12": "[105] N. Han, J. Wu, X. Fang, J. Wen, S. Zhan, S. Xie, and X. Li. “Trans-"
        },
        {
          "12": "IEEE Transactions\non Neural\nferable Linear Discriminant Analysis.”"
        },
        {
          "12": "Networks and Learning Systems (Early Access), 2020."
        },
        {
          "12": "[106]\nP. Angelov, and E. Soares. “Towards explainable deep neural networks"
        },
        {
          "12": ""
        },
        {
          "12": "(xDNN).” Neural Networks, vol. 130, pp. 185–194, 2020."
        },
        {
          "12": ""
        },
        {
          "12": "[107] M. Xu, Z. Zhu, X. Zhang, Y. Zhao, and X. Li. “Canonical Correlation"
        },
        {
          "12": ""
        },
        {
          "12": "IEEE\nAnalysis with L2,1-Norm for Multiview Data Representation.”"
        },
        {
          "12": ""
        },
        {
          "12": "transactions on cybernetics\n(Early Access), 2020."
        },
        {
          "12": ""
        },
        {
          "12": "[108]\nS. Yang, L. Li, S. Wang, W. Zhang, Q. Huang, and Q. Tian. “Skeleton-"
        },
        {
          "12": ""
        },
        {
          "12": "Net: A Hybrid Network with a Skeleton-Embedding Process\nfor Multi-"
        },
        {
          "12": ""
        },
        {
          "12": "view Image Representation Learning.” IEEE Transactions on Multimedia"
        },
        {
          "12": ""
        },
        {
          "12": "(Early Access), 2020."
        },
        {
          "12": ""
        },
        {
          "12": "[109] H.K. Azad, and A. Deepak. “A new approach for query expansion using"
        },
        {
          "12": ""
        },
        {
          "12": "Wikipedia\nand WordNet.” Information sciences, vol. 492, pp. 147–163,"
        },
        {
          "12": ""
        },
        {
          "12": "2019."
        },
        {
          "12": ""
        },
        {
          "12": "[110] M. Wu, W. Su, L. Chen, W. Pedrycz,\nand K. Hirota.\n“Two-stage"
        },
        {
          "12": ""
        },
        {
          "12": "Fuzzy Fusion based-Convolution Neural Network for Dynamic Emotion"
        },
        {
          "12": ""
        },
        {
          "12": "Recognition.” IEEE Transactions on Affective Computing (Early Access),"
        },
        {
          "12": ""
        },
        {
          "12": "2020."
        },
        {
          "12": ""
        },
        {
          "12": "[111] M. Hao, W. Cao, Z. Liu, M. Wu, and P. Xiao. “Visual-audio emotion"
        },
        {
          "12": ""
        },
        {
          "12": "recognition\nbased\non multi-task\nand\nensemble\nlearning with multiple"
        },
        {
          "12": ""
        },
        {
          "12": "features.” Neurocomputing (Early Access), 2020."
        },
        {
          "12": ""
        },
        {
          "12": "[112] X. Wang, X. Chen,\nand C. Cao.\n“Human\nemotion\nrecognition\nby"
        },
        {
          "12": ""
        },
        {
          "12": "optimally fusing facial expression and speech feature.” Signal Processing:"
        },
        {
          "12": ""
        },
        {
          "12": "Image Communication (Early Access), 2020."
        },
        {
          "12": "[113] K. Li, G. Qi, J. Ye, and K.A. Hua. “Linear subspace ranking hashing"
        },
        {
          "12": "IEEE transactions\non\npattern\nanalysis\nand\nfor\ncross-modal\nretrieval.”"
        },
        {
          "12": "machine intelligence, vol. 39, no. 9, pp. 1825–1838, 2016."
        },
        {
          "12": "[114]\nL. Nie, X.\nSong,\nand T.S. Chua.\n“Learning\nfrom multiple\nsocial"
        },
        {
          "12": "networks.” Synthesis Lectures on Information Concepts, Retrieval, and"
        },
        {
          "12": "Services, vol. 8, no. 2, pp. 1–118, 2016."
        },
        {
          "12": "[115]\nL. Nie, M. Liu, and X. Song. “Multimodal\nlearning toward micro-video"
        },
        {
          "12": "Synthesis\nLectures\non\nImage, Video,\nand Multimedia\nunderstanding.”"
        },
        {
          "12": "Processing, vol. 9, no. 4, pp. 1–186, 2019."
        },
        {
          "12": ""
        },
        {
          "12": ""
        },
        {
          "12": ""
        },
        {
          "12": ""
        },
        {
          "12": ""
        },
        {
          "12": ""
        },
        {
          "12": ""
        },
        {
          "12": ""
        },
        {
          "12": ""
        },
        {
          "12": ""
        },
        {
          "12": ""
        },
        {
          "12": ""
        },
        {
          "12": ""
        },
        {
          "12": ""
        },
        {
          "12": ""
        },
        {
          "12": ""
        },
        {
          "12": ""
        },
        {
          "12": ""
        },
        {
          "12": ""
        },
        {
          "12": ""
        },
        {
          "12": ""
        },
        {
          "12": ""
        },
        {
          "12": ""
        },
        {
          "12": ""
        },
        {
          "12": ""
        },
        {
          "12": ""
        },
        {
          "12": ""
        },
        {
          "12": ""
        },
        {
          "12": ""
        },
        {
          "12": ""
        },
        {
          "12": ""
        },
        {
          "12": ""
        },
        {
          "12": ""
        },
        {
          "12": ""
        },
        {
          "12": ""
        }
      ],
      "page": 12
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "A survey of temporal knowledge discovery paradigms and methods",
      "authors": [
        "J Roddick",
        "M Spiliopoulou"
      ],
      "year": "2002",
      "venue": "IEEE Transactions on Knowledge and data engineering"
    },
    {
      "citation_id": "2",
      "title": "Guest editorial: special section on music data mining",
      "authors": [
        "T Li",
        "M Ogihara",
        "G Tzanetakis"
      ],
      "year": "2014",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "3",
      "title": "Knowledge discovery in data streams with the orthogonal series-based generalized regression neural networks",
      "authors": [
        "P Duda",
        "M Jaworski",
        "L Rutkowski"
      ],
      "year": "2018",
      "venue": "Information Sciences"
    },
    {
      "citation_id": "4",
      "title": "Knowledge Discovery in Databases: An Overview",
      "authors": [
        "W Frawley",
        "G Shapiro",
        "C Matheus"
      ],
      "year": "1992",
      "venue": "AI magazine"
    },
    {
      "citation_id": "5",
      "title": "Multimodal information fusion for selected multimedia applications",
      "authors": [
        "L Guan",
        "Y Wang",
        "R Zhang",
        "Y Tie",
        "A Bulzacki",
        "M Ibrahim"
      ],
      "year": "2010",
      "venue": "International Journal of Multimedia Intelligence and Security"
    },
    {
      "citation_id": "6",
      "title": "Infrared and visible image fusion methods and applications: a survey",
      "authors": [
        "J Ma",
        "Y Ma",
        "C Li"
      ],
      "year": "2019",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "7",
      "title": "Deep learning",
      "authors": [
        "Y Lecun",
        "Y Bengio",
        "G Hinton"
      ],
      "year": "2015",
      "venue": "nature"
    },
    {
      "citation_id": "8",
      "title": "Advanced deep-learning techniques for salient and category-specific object detection: a survey",
      "authors": [
        "J Han",
        "D Zhang",
        "G Cheng",
        "N Liu",
        "D Xu"
      ],
      "year": "2018",
      "venue": "IEEE Signal Processing Magazine"
    },
    {
      "citation_id": "9",
      "title": "A survey on learning to hash",
      "authors": [
        "J Wang",
        "T Zhang",
        "N Sebe",
        "H Shen"
      ],
      "year": "2017",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "10",
      "title": "Fast search in hamming space with multi-index hashing",
      "authors": [
        "M Norouzi",
        "A Punjani",
        "D Fleet"
      ],
      "year": "2012",
      "venue": "2012 IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "11",
      "title": "Deep video hashing",
      "authors": [
        "V Liong",
        "J Lu",
        "Y Tan",
        "J Zhou"
      ],
      "year": "2016",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "12",
      "title": "Latent semantic minimal hashing for image retrieval",
      "authors": [
        "X Lu",
        "X Zheng",
        "X Li"
      ],
      "year": "2016",
      "venue": "IEEE Transactions on Image Processing"
    },
    {
      "citation_id": "13",
      "title": "Unsupervised visual hashing with semantic assistant for content-based image retrieval",
      "authors": [
        "L Zhu",
        "J Shen",
        "L Xie",
        "Z Cheng"
      ],
      "year": "2016",
      "venue": "IEEE Transactions on Knowledge and Data Engineering"
    },
    {
      "citation_id": "14",
      "title": "Kernel-Based Semantic Hashing for Gait Retrieval",
      "authors": [
        "Y Zhou",
        "Y Huang",
        "Q Hu",
        "L Wang"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Circuits and Systems for Video Technology"
    },
    {
      "citation_id": "15",
      "title": "Exploring auxiliary context: discrete semantic transfer hashing for scalable image retrieval",
      "authors": [
        "L Zhu",
        "Z Huang",
        "Z Li",
        "L Xie",
        "H Shen"
      ],
      "year": "2018",
      "venue": "IEEE transactions on neural networks and learning systems"
    },
    {
      "citation_id": "16",
      "title": "Fast Discrete Collaborative Multi-modal Hashing for Large-scale Multimedia Retrieval",
      "authors": [
        "C Zheng",
        "L Zhu",
        "X Lu",
        "J Li",
        "Z Cheng",
        "H Zhang"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Knowledge and Data Engineering (Early Access)"
    },
    {
      "citation_id": "17",
      "title": "Collective Reconstructive Embeddings for Cross-Modal Hashing",
      "authors": [
        "M Hu",
        "Y Yang",
        "F Shen",
        "N Xie",
        "R Hong",
        "H Shen"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Image Processing"
    },
    {
      "citation_id": "18",
      "title": "Heterogeneous translated hashing: A scalable solution towards multi-modal similarity search",
      "authors": [
        "Y Wei",
        "Y Song",
        "Y Zhen",
        "B Liu",
        "Q Yang"
      ],
      "year": "2016",
      "venue": "ACM Transactions on Knowledge Discovery from Data (TKDD)"
    },
    {
      "citation_id": "19",
      "title": "Deep cross-modal hashing",
      "authors": [
        "Q Jiang",
        "W Li"
      ],
      "year": "2017",
      "venue": "Deep cross-modal hashing"
    },
    {
      "citation_id": "20",
      "title": "Multiple feature hashing for real-time large scale near-duplicate video retrieval",
      "authors": [
        "J Song",
        "Y Yang",
        "Z Huang",
        "H Shen",
        "R Hong"
      ],
      "year": "2011",
      "venue": "19th ACM international conference on Multimedia"
    },
    {
      "citation_id": "21",
      "title": "Composite hashing with multiple information sources",
      "authors": [
        "D Zhang",
        "F Wang",
        "L Si"
      ],
      "year": "2011",
      "venue": "34th international ACM SIGIR conference on Research and development in Information Retrieval"
    },
    {
      "citation_id": "22",
      "title": "Large-scale supervised multimodal hashing with semantic correlation maximization",
      "authors": [
        "D Zhang",
        "W Li"
      ],
      "year": "2014",
      "venue": "Twenty-Eighth AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "23",
      "title": "Lbmch: Learning bridging mapping for cross-modal hashing",
      "authors": [
        "Y Wang",
        "X Lin",
        "L Wu",
        "W Zhang",
        "Q Zhang"
      ],
      "year": "2015",
      "venue": "38th international ACM SIGIR conference on research and development in information retrieval"
    },
    {
      "citation_id": "24",
      "title": "Canonical correlation analysis for data fusion and group inferences",
      "authors": [
        "N Correa",
        "T Adali",
        "Y Li",
        "V Calhoun"
      ],
      "year": "2010",
      "venue": "IEEE signal processing magazine"
    },
    {
      "citation_id": "25",
      "title": "Discriminative multiple canonical correlation analysis for information fusion",
      "authors": [
        "L Gao",
        "L Qi",
        "E Chen",
        "L Guan"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Image Processing"
    },
    {
      "citation_id": "26",
      "title": "Small codes and large image databases for recognition",
      "authors": [
        "A Torralba",
        "R Fergus",
        "Y Weiss"
      ],
      "year": "2008",
      "venue": "Small codes and large image databases for recognition"
    },
    {
      "citation_id": "27",
      "title": "Isotropic hashing",
      "authors": [
        "W Kong",
        "W Li"
      ],
      "year": "2012",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "28",
      "title": "Semi-supervised hashing for large-scale search",
      "authors": [
        "J Wang",
        "S Kumar",
        "S Chang"
      ],
      "year": "2012",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "29",
      "title": "HSOG: a novel local image descriptor based on histograms of the second-order gradients",
      "authors": [
        "D Huang",
        "C Zhu",
        "Y Wang",
        "L Chen"
      ],
      "year": "2014",
      "venue": "IEEE TIP"
    },
    {
      "citation_id": "30",
      "title": "LBP-based edge-texture features for object recognition",
      "authors": [
        "A Satpathy",
        "X Jiang",
        "H Eng"
      ],
      "year": "2014",
      "venue": "IEEE TIP"
    },
    {
      "citation_id": "31",
      "title": "Gabor feature based sparse representation for face recognition with gabor occlusion dictionary",
      "authors": [
        "M Yang",
        "L Zhang"
      ],
      "year": "2010",
      "venue": "ECCV"
    },
    {
      "citation_id": "32",
      "title": "Very Deep Convolutional Networks for Large-Scale Image Recognition",
      "authors": [
        "K Simonyan",
        "A Zisserman"
      ],
      "year": "2015",
      "venue": "2015 International Conference on Learning Representations"
    },
    {
      "citation_id": "33",
      "title": "Joint image-text hashing for fast large-scale cross-media retrieval using self-supervised deep learning",
      "authors": [
        "G Wu",
        "J Han",
        "Z Lin",
        "G Ding",
        "B Zhang",
        "Q Ni"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Industrial Electronics"
    },
    {
      "citation_id": "34",
      "title": "Boosting image captioning with attributes",
      "authors": [
        "T Yao",
        "Y Pan",
        "Y Li",
        "Z Qiu",
        "T Mei"
      ],
      "year": "2017",
      "venue": "IEEE International Conference on Computer Vision"
    },
    {
      "citation_id": "35",
      "title": "VQA: Visual question answering",
      "authors": [
        "S Antol",
        "A Agrawal",
        "J Lu",
        "M Mitchell",
        "D Batra",
        "C Zitnick",
        "D Parikh"
      ],
      "year": "2015",
      "venue": "VQA: Visual question answering"
    },
    {
      "citation_id": "36",
      "title": "Contextual bag-of-words for visual categorization",
      "authors": [
        "T Li",
        "T Mei",
        "I Kweon",
        "X Hua"
      ],
      "year": "2010",
      "venue": "IEEE Transactions on Circuits and Systems for Video Technology"
    },
    {
      "citation_id": "37",
      "title": "Deep LDA: A new way to topic model",
      "authors": [
        "M Bhat",
        "M Kundroo",
        "T Tarray",
        "B Agarwal"
      ],
      "year": "2020",
      "venue": "Journal of Information and Optimization Sciences"
    },
    {
      "citation_id": "38",
      "title": "Feature fusion: parallel strategy vs. serial strategy",
      "authors": [
        "J Yang",
        "J Yang",
        "D Zhang",
        "J Lu"
      ],
      "year": "2003",
      "venue": "Pattern Recognition"
    },
    {
      "citation_id": "39",
      "title": "A new method of feature fusion and its application in image recognition",
      "authors": [
        "Q Sun",
        "S Zeng",
        "Y Liu",
        "P Heng",
        "D Xia"
      ],
      "year": "2005",
      "venue": "Pattern Recognition"
    },
    {
      "citation_id": "40",
      "title": "The labeled multiple canonical correlation analysis for information fusion",
      "authors": [
        "L Gao",
        "R Zhang",
        "L Qi",
        "E Chen",
        "L Guan"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "41",
      "title": "A New Discriminative Sparse Representation Method for Robust Face Recognition via l 1 Regularization",
      "authors": [
        "Y Xu",
        "Z Zhong",
        "J Yang",
        "J You",
        "David Zhang"
      ],
      "year": "2017",
      "venue": "IEEE transactions on neural networks and learning systems"
    },
    {
      "citation_id": "42",
      "title": "Sparse representation or collaborative representation: Which helps face recognition?",
      "authors": [
        "L Zhang",
        "M Yang",
        "X Feng"
      ],
      "year": "2011",
      "venue": "2011 IEEE international conference on Computer Vision (ICCV)"
    },
    {
      "citation_id": "43",
      "title": "1 l s: Simple MATLAB Solver for l1-regularized Least Squares Problems",
      "venue": "1 l s: Simple MATLAB Solver for l1-regularized Least Squares Problems"
    },
    {
      "citation_id": "44",
      "title": "Fast ℓ 1 -Minimization Algorithms for Robust Face Recognition",
      "authors": [
        "A Yang",
        "Z Zhou",
        "A Balasubramanian",
        "S Sastry",
        "Y Ma"
      ],
      "year": "2013",
      "venue": "IEEE Transactions on Image Processing"
    },
    {
      "citation_id": "45",
      "title": "Stable and orthogonal local discriminant embedding using trace ratio criterion for dimension-ality reduction",
      "authors": [
        "X Yang",
        "G Liu",
        "Q Yu",
        "R Wang"
      ],
      "year": "2018",
      "venue": "Multimedia Tools and Applications"
    },
    {
      "citation_id": "46",
      "title": "Generalized Discriminant Local Median Preserving Projections (GDLMPP) for Face Recognition",
      "authors": [
        "Ming-Hua Wan",
        "Zhi-Hui Lai"
      ],
      "year": "2018",
      "venue": "Neural Processing Letters"
    },
    {
      "citation_id": "47",
      "title": "ImageNet classification with deep convolutional neural network",
      "authors": [
        "A Krizhevsky",
        "I Sutskever",
        "G Hinton"
      ],
      "year": "2012",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "48",
      "title": "PCANet: A simple deep learning baseline for image classification",
      "authors": [
        "T Chan",
        "K Jia",
        "S Gao",
        "J Lu",
        "Z Zeng",
        "Y Ma"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on Image Processing"
    },
    {
      "citation_id": "49",
      "title": "Hierarchical learning of multi-task sparse metrics for large scale image classification",
      "authors": [
        "Y Zheng",
        "J Fan",
        "J Zhang",
        "X Gao"
      ],
      "year": "2017",
      "venue": "Pattern Recognition"
    },
    {
      "citation_id": "50",
      "title": "A sparse representation model using the complete marginal fisher analysis framework and its applications to visual recognition",
      "authors": [
        "A Puthenputhussery",
        "Q Liu",
        "C Liu"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "51",
      "title": "Dynamic graph fusion label propagation for semi-supervised multi-modality classification",
      "authors": [
        "G Lin",
        "K Liao",
        "B Sun",
        "Y Chen",
        "F Zhao"
      ],
      "year": "2017",
      "venue": "Pattern Recognition"
    },
    {
      "citation_id": "52",
      "title": "Deep co-space: Sample mining across feature transformation for semisupervised learning",
      "authors": [
        "Z Chen",
        "K Wang",
        "X Wang",
        "P Peng",
        "E Izquierdo",
        "L Lin"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Circuits and Systems for Video Technology"
    },
    {
      "citation_id": "53",
      "title": "Sparse representation-based open set recognition",
      "authors": [
        "H Zhang",
        "V Patel"
      ],
      "year": "2017",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "54",
      "title": "Bilevel model-based discriminative dictionary learning for recognition",
      "authors": [
        "P Zhou",
        "C Zhang",
        "Z Lin"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Image Processing"
    },
    {
      "citation_id": "55",
      "title": "A novel locally linear KNN method with applications to visual recognition",
      "authors": [
        "Q Liu",
        "C Liu"
      ],
      "year": "2017",
      "venue": "IEEE transactions on neural networks and learning systems"
    },
    {
      "citation_id": "56",
      "title": "Cost-effective active learning for deep image classification",
      "authors": [
        "K Wang",
        "D Zhang",
        "Y Li",
        "R Zhang",
        "L Lin"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Circuits and Systems for Video Technology"
    },
    {
      "citation_id": "57",
      "title": "Image-specific classification with local and global discriminations",
      "authors": [
        "C Zhang",
        "J Cheng",
        "C Li",
        "Q Tian"
      ],
      "year": "2018",
      "venue": "IEEE transactions on neural networks and learning systems"
    },
    {
      "citation_id": "58",
      "title": "Bundled local features for image representation",
      "authors": [
        "C Zhang",
        "J Sang",
        "G Zhu",
        "Q Tian"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Circuits and Systems for Video Technology"
    },
    {
      "citation_id": "59",
      "title": "Image class prediction by joint object, context, and background modeling",
      "authors": [
        "C Zhang",
        "G Zhu",
        "C Liang",
        "Y Zhang",
        "Q Huang",
        "Q Tian"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Circuits and Systems for Video Technology"
    },
    {
      "citation_id": "60",
      "title": "Convolutional sparse autoencoders for image classification",
      "authors": [
        "W Luo",
        "J Li",
        "J Yang",
        "W Xu",
        "J Zhang"
      ],
      "year": "2018",
      "venue": "IEEE transactions on neural networks and learning systems"
    },
    {
      "citation_id": "61",
      "title": "Structured weak semantic space construction for visual categorization",
      "authors": [
        "C Zhang",
        "J Cheng",
        "Q Tian"
      ],
      "year": "2018",
      "venue": "IEEE transactions on neural networks and learning systems"
    },
    {
      "citation_id": "62",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "K He",
        "X Zhang",
        "S Ren",
        "J Sun"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "63",
      "title": "Resfeats: Residual network based features for image classification",
      "authors": [
        "A Mahmood",
        "M Bennamoun",
        "S An",
        "F Sohel"
      ],
      "year": "2017",
      "venue": "2017 IEEE International Conference on In Image Processing"
    },
    {
      "citation_id": "64",
      "title": "Cross-modal subspace learning via pairwise constraints",
      "authors": [
        "R He",
        "M Zhang",
        "L Wang",
        "Y Ji",
        "Q Yin"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on Image Processing"
    },
    {
      "citation_id": "65",
      "title": "Multiview label sharing for visual representations and classifications",
      "authors": [
        "C Zhang",
        "J Cheng",
        "Q Tian"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "66",
      "title": "A deep semantic framework for multimodal representation learning",
      "authors": [
        "C Wang",
        "H Yang",
        "C Meinel"
      ],
      "year": "2016",
      "venue": "Multimedia Tools and Applications"
    },
    {
      "citation_id": "67",
      "title": "Canonical Correlation Analysis With L2,1-Norm for Multiview Data Representation",
      "authors": [
        "M Xu",
        "Z Zhu",
        "X Zhang",
        "Y Zhao",
        "X Li"
      ],
      "year": "2019",
      "venue": "IEEE transactions on cybernetics (Early Access)"
    },
    {
      "citation_id": "68",
      "title": "Skeleton-Net: A Hybrid Network with a Skeleton-Embedding Process for Multiview Image Representation Learning",
      "authors": [
        "S Yang",
        "L Li",
        "S Wang",
        "W Zhang",
        "Q Huang",
        "Q Tian"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Multimedia (Early Access)"
    },
    {
      "citation_id": "69",
      "title": "Shared-private information bottleneck method for cross-modal clustering",
      "authors": [
        "X Yan",
        "Y Ye",
        "Y Mao",
        "H Yu"
      ],
      "year": "2019",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "70",
      "title": "BULDP: biomimetic uncorrelated locality discriminant projection for feature extraction in face recognition",
      "authors": [
        "X Ning",
        "W Li",
        "B Tang",
        "H He"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Image Processing"
    },
    {
      "citation_id": "71",
      "title": "Iterative projection based sparse reconstruction for face recognition",
      "authors": [
        "B Xu",
        "Q Liu"
      ],
      "year": "2018",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "72",
      "title": "Face recognition using a new compressive sensing-based feature extraction method",
      "authors": [
        "M Banitalebi-Dehkordi",
        "A Banitalebi-Dehkordi",
        "J Abouei",
        "K Plataniotis"
      ],
      "year": "2018",
      "venue": "Multimedia Tools and Applications"
    },
    {
      "citation_id": "73",
      "title": "Deep coupled metric learning for cross-modal matching",
      "authors": [
        "V Liong",
        "J Lu",
        "Y Tan",
        "J Zhou"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "74",
      "title": "Representation learning: A review and new perspectives",
      "authors": [
        "Y Bengio",
        "A Courville",
        "P Vincent"
      ],
      "year": "2013",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "75",
      "title": "Very deep convolutional networks for large-scale image recognition",
      "authors": [
        "K Simonyan",
        "A Zisserman"
      ],
      "year": "2015",
      "venue": "2015 International Conference on Learning Representations"
    },
    {
      "citation_id": "76",
      "title": "Information fusion via multimodal hashing with discriminant canonical correlation maximization",
      "authors": [
        "L Gao",
        "L Guan"
      ],
      "year": "2019",
      "venue": "2019 International Conference on Image Analysis and Recognition"
    },
    {
      "citation_id": "77",
      "title": "Information Fusion via Multimodal Hashing With Discriminant Correlation Maximization",
      "authors": [
        "L Gao",
        "L Guan"
      ],
      "year": "2019",
      "venue": "2019 IEEE International Conference on Image Processing"
    },
    {
      "citation_id": "78",
      "title": "Emotion recognition based on joint visual and audio cues",
      "authors": [
        "N Sebe",
        "I Cohen",
        "T Gevers",
        "T Huang"
      ],
      "year": "2006",
      "venue": "18th International Conference on Pattern Recognition (ICPR'06)"
    },
    {
      "citation_id": "79",
      "title": "Survey on speech emotion recognition: Features, classification schemes, and databases",
      "authors": [
        "E Ayadi",
        "M Kamel",
        "F Karray"
      ],
      "year": "2011",
      "venue": "Pattern Recognition"
    },
    {
      "citation_id": "80",
      "title": "Recognizing human emotional state from audiovisual signals",
      "authors": [
        "Y Wang",
        "L Guan"
      ],
      "year": "2008",
      "venue": "IEEE transactions on multimedia"
    },
    {
      "citation_id": "81",
      "title": "Optimal fault classification using fisher discriminant analysis in the parity space for applications to npps",
      "authors": [
        "S Cho",
        "J Jiang"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Nuclear Science"
    },
    {
      "citation_id": "82",
      "title": "Speech Emotion Recognition Using Deep Convolutional Neural Network and Discriminant Temporal Pyramid Matching",
      "authors": [
        "S Zhang",
        "S Zhang",
        "T Huang",
        "W Gao"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "83",
      "title": "Learning affective features with a hybrid deep model for audio-visual emotion recognition",
      "authors": [
        "S Zhang",
        "S Zhang",
        "T Huang",
        "W Gao",
        "Q Tian"
      ],
      "year": "2018",
      "venue": "IEEE TCSVT"
    },
    {
      "citation_id": "84",
      "title": "Two-stage Fuzzy Fusion based-Convolution Neural Network for Dynamic Emotion Recognition",
      "authors": [
        "M Wu",
        "W Su",
        "L Chen",
        "W Pedrycz",
        "K Hirota"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing (Early Access)"
    },
    {
      "citation_id": "85",
      "title": "Audio-visual emotion recognition in video clips",
      "authors": [
        "F Noroozi",
        "M Marjanovic",
        "A Njegus",
        "S Escalera",
        "G Anbarjafari"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "86",
      "title": "A new approach of audio emotion recognition",
      "authors": [
        "C Ooi",
        "K Seng",
        "L Ang",
        "L Chew"
      ],
      "year": "2014",
      "venue": "A new approach of audio emotion recognition"
    },
    {
      "citation_id": "87",
      "title": "Places: A 10 million image database for scene recognition",
      "authors": [
        "B Zhou",
        "A Lapedriza",
        "A Khosla",
        "A Oliva",
        "A Torralba"
      ],
      "year": "2018",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "88",
      "title": "An approach for measuring semantic similarity between words using multiple information sources",
      "authors": [
        "Y Li",
        "Z Bandar",
        "D Mclean"
      ],
      "year": "2003",
      "venue": "IEEE Transactions on knowledge and data engineering"
    },
    {
      "citation_id": "89",
      "title": "Learning deep binary descriptor with multi-quantization",
      "authors": [
        "Y Duan",
        "J Lu",
        "Z Wang",
        "J Feng",
        "J Zhou"
      ],
      "year": "2017",
      "venue": "Learning deep binary descriptor with multi-quantization"
    },
    {
      "citation_id": "90",
      "title": "Deep binaries: Encoding semantic-rich cues for efficient textual-visual cross retrieval",
      "authors": [
        "Y Shen",
        "L Liu",
        "L Shao",
        "J Song"
      ],
      "year": "2017",
      "venue": "IEEE International Conference on Computer Vision"
    },
    {
      "citation_id": "91",
      "title": "Context-aware local binary feature learning for face recognition",
      "authors": [
        "Y Duan",
        "J Lu",
        "J Feng",
        "J Zhou"
      ],
      "year": "2018",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "92",
      "title": "Self-supervised adversarial hashing networks for cross-modal retrieval",
      "authors": [
        "C Li",
        "C Deng",
        "N Li",
        "W Liu",
        "X Gao",
        "D Tao"
      ],
      "year": "2018",
      "venue": "Self-supervised adversarial hashing networks for cross-modal retrieval"
    },
    {
      "citation_id": "93",
      "title": "Cross-view retrieval via probability-based semantics-preserving hashing",
      "authors": [
        "Z Lin",
        "G Ding",
        "J Han",
        "J Wang"
      ],
      "year": "2017",
      "venue": "IEEE transactions on cybernetics"
    },
    {
      "citation_id": "94",
      "title": "Deep joint-semantics reconstructing hashing for large-scale unsupervised cross-modal retrieval",
      "authors": [
        "S Su",
        "Z Zhong",
        "C Zhang"
      ],
      "year": "2019",
      "venue": "IEEE International Conference on Computer Vision"
    },
    {
      "citation_id": "95",
      "title": "Scalable supervised asymmetric hashing with semantic and latent factor embedding",
      "authors": [
        "Z Zhang",
        "Z Lai",
        "Z Huang",
        "W Wong",
        "G Xie",
        "L Liu",
        "L Shao"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Image Processing"
    },
    {
      "citation_id": "96",
      "title": "Fast supervised discrete hashing",
      "authors": [
        "J Gui",
        "T Liu",
        "Z Sun",
        "D Tao",
        "T Tan"
      ],
      "year": "2018",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "97",
      "title": "Data mining based on clustering and association rule analysis for knowledge discovery in multiobjective topology optimization",
      "authors": [
        "Y Sato",
        "K Izui",
        "T Yamada",
        "S Nishiwaki"
      ],
      "year": "2019",
      "venue": "Expert Systems with Applications"
    },
    {
      "citation_id": "98",
      "title": "Accelerated knowledge discovery from omics data by optimal experimental design",
      "authors": [
        "X Wang",
        "N Rai",
        "B Pereira",
        "A Eetemadi",
        "I Tagkopoulos"
      ],
      "year": "2020",
      "venue": "Nature Communications"
    },
    {
      "citation_id": "99",
      "title": "Constrained Discriminative Projection Learning for Image Classification",
      "authors": [
        "M Meng",
        "M Lan",
        "J Yu",
        "J Wu",
        "D Tao"
      ],
      "year": "2019",
      "venue": "¡± IEEE Transactions on Image Processing"
    },
    {
      "citation_id": "100",
      "title": "Local distances preserving based manifold learning",
      "authors": [
        "R Hajizadeh",
        "A Aghagolzadeh",
        "M Ezoji"
      ],
      "year": "2020",
      "venue": "Expert Systems with Applications (Early Access)"
    },
    {
      "citation_id": "101",
      "title": "Unconstrained and Constrained Face Recognition using Dense Local Descriptor with Ensemble Framework",
      "authors": [
        "D Kumar",
        "J Garain",
        "D Kisku",
        "J Sing",
        "P Gupta"
      ],
      "year": "2020",
      "venue": "Neurocomputing (Early Access)"
    },
    {
      "citation_id": "102",
      "title": "Sparsity adaptive matching pursuit for face recognition",
      "authors": [
        "Y Wang",
        "Y Peng",
        "S Liu",
        "J Li",
        "X Wang"
      ],
      "year": "2020",
      "venue": "Journal of Visual Communication and Image Representation (Early Access)"
    },
    {
      "citation_id": "103",
      "title": "Estimation of ergodicity limits of bag-ofwords modeling for guaranteed stochastic convergence",
      "authors": [
        "I Ghalyan",
        "F Jasim"
      ],
      "year": "2020",
      "venue": "Pattern Recognition (Early Access)"
    },
    {
      "citation_id": "104",
      "title": "Learning to Select Base Classes for Few-shot Classification",
      "authors": [
        "L Zhou",
        "P Cui",
        "X Jia",
        "S Yang",
        "Q Tian"
      ],
      "year": "2020",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "105",
      "title": "Transferable Linear Discriminant Analysis",
      "authors": [
        "N Han",
        "J Wu",
        "X Fang",
        "J Wen",
        "S Zhan",
        "S Xie",
        "X Li"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Neural Networks and Learning Systems (Early Access)"
    },
    {
      "citation_id": "106",
      "title": "Towards explainable deep neural networks (xDNN)",
      "authors": [
        "P Angelov",
        "E Soares"
      ],
      "year": "2020",
      "venue": "Neural Networks"
    },
    {
      "citation_id": "107",
      "title": "Canonical Correlation Analysis with L2,1-Norm for Multiview Data Representation",
      "authors": [
        "M Xu",
        "Z Zhu",
        "X Zhang",
        "Y Zhao",
        "X Li"
      ],
      "year": "2020",
      "venue": "IEEE transactions on cybernetics (Early Access)"
    },
    {
      "citation_id": "108",
      "title": "Skeleton-Net: A Hybrid Network with a Skeleton-Embedding Process for Multiview Image Representation Learning",
      "authors": [
        "S Yang",
        "L Li",
        "S Wang",
        "W Zhang",
        "Q Huang",
        "Q Tian"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Multimedia (Early Access)"
    },
    {
      "citation_id": "109",
      "title": "A new approach for query expansion using Wikipedia and WordNet",
      "authors": [
        "H Azad",
        "A Deepak"
      ],
      "year": "2019",
      "venue": "Information sciences"
    },
    {
      "citation_id": "110",
      "title": "Two-stage Fuzzy Fusion based-Convolution Neural Network for Dynamic Emotion Recognition",
      "authors": [
        "M Wu",
        "W Su",
        "L Chen",
        "W Pedrycz",
        "K Hirota"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing (Early Access)"
    },
    {
      "citation_id": "111",
      "title": "Visual-audio emotion recognition based on multi-task and ensemble learning with multiple features",
      "authors": [
        "M Hao",
        "W Cao",
        "Z Liu",
        "M Wu",
        "P Xiao"
      ],
      "year": "2020",
      "venue": "Neurocomputing (Early Access)"
    },
    {
      "citation_id": "112",
      "title": "Human emotion recognition by optimally fusing facial expression and speech feature",
      "authors": [
        "X Wang",
        "X Chen",
        "C Cao"
      ],
      "year": "2020",
      "venue": "Signal Processing: Image Communication (Early Access)"
    },
    {
      "citation_id": "113",
      "title": "Linear subspace ranking hashing for cross-modal retrieval",
      "authors": [
        "K Li",
        "G Qi",
        "J Ye",
        "K Hua"
      ],
      "year": "2016",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "114",
      "title": "Learning from multiple social networks",
      "authors": [
        "L Nie",
        "X Song",
        "T Chua"
      ],
      "year": "2016",
      "venue": "Synthesis Lectures on Information Concepts, Retrieval, and Services"
    },
    {
      "citation_id": "115",
      "title": "Multimodal learning toward micro-video understanding",
      "authors": [
        "L Nie",
        "M Liu",
        "X Song"
      ],
      "year": "2019",
      "venue": "Synthesis Lectures on Image, Video, and Multimedia Processing"
    }
  ]
}