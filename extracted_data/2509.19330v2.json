{
  "paper_id": "2509.19330v2",
  "title": "Libemer: A Novel Benchmark And Algorithms Library For Eeg-Based Multimodal Emotion Recognition",
  "published": "2025-09-14T03:50:07Z",
  "authors": [
    "Zejun Liu",
    "Yunshan Chen",
    "Chengxi Xie",
    "Yugui Xie",
    "Huan Liu"
  ],
  "keywords": [
    "multimodal learning",
    "emotion recognition",
    "benchmark",
    "electroencephalography(EEG)",
    "open-source library"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "EEG-based multimodal emotion recognition(EMER) has gained significant attention and witnessed notable advancements, the inherent complexity of human neural systems has motivated substantial efforts toward multimodal approaches. However, this field currently suffers from three critical limitations: (i) the absence of open-source implementations. (ii) the lack of standardized and transparent benchmarks for fair performance analysis. (iii) in-depth discussion regarding main challenges and promising research directions is a notable scarcity. To address these challenges, we introduce LibEMER, a unified evaluation framework that provides fully reproducible Py-Torch implementations of curated deep learning methods alongside standardized protocols for data preprocessing, model realization, and experimental setups. This framework enables unbiased performance assessment on three widely-used public datasets across two learning tasks. The open-source library is publicly accessible at: LibEMER",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "EEG-based multimodal emotion recognition (EMER) represents a critical research domain within affective computing, focusing on the development of computational models for precise identification of human emotional states. As electroencephalography (EEG) provides direct measurements of cortical neural activity, it has received continuous attention. Furthermore, given the inherent complexity of human neurophysiological systems, multimodal approaches integrating EEG with complementary physiological signals have increasingly emerged as a key research focus in recent years These supplementary modalities encompass: electrocardiography(ECG), blood volume pressure(BVP), respiration(RSP), electrooculography(EOG), electromyography(EMG), galvanic skin response(GSR), eye movements and facial expressions.\n\nIn the field of EER, TorchEEGEMO  [1]  provides a deep learning toolkit, GNN4EEG  [2]  focuses on evaluating SOTA GNN-based methods. However, neither establishes a sufficiently rigorous framework for benchmarking, thus failing to address the prevailing lack of standardized benchmark. While LibEER  [3]  presents a more comprehensive and rigorous benchmark with deeper analysis, its applicability is confined to unimodal EEG-based approaches, rendering it unsuitable for multimodal scenarios that integrate EEG with other physiological signals. In contrast, our work establishes a rigorous benchmark for multimodal scenarios.\n\n# corresponding author.\n\nAlthough substantial progress has been made in developing deep learning models for EMER, some systematic issues that hinder development of relevant research remain unsolved. The most core challenge is the absence of comprehensive standardized benchmarks, which fosters significant discrepancies in preprocessing pipelines and experimental configurations, thereby compromising transparency and comparability across studies. The problem is compounded by insufficience of details in publications and open-source codebases, which collectively obstruct verification and replication efforts. Even when methods are open-sourced, they are often implemented on different platforms like PyTorch  [4]  and TensorFlow  [5] , and their training processes also have various detailed discrepancies, making a fair and unified evaluation difficult.Furthermore, while existing surveys  [6, 7]  have summarized the achivements, they generally fall short of providing a critical evaluation and insights to inspire promising progress.\n\nTo address these challenges, we introduce LibEMER, a benchmark and open-source algorithm library based on PyTorch that establishes unified standards for model implementation, data processing, experimental configurations, and evaluation metrics. This toolkit provides fully reproducible workflows and supports three widelyadopted public datasets, SEED  [8] , SEEDV  [9] , and DEAP  [10] , across two critical learning paradigms: subject-dependent and subject-independent recognition tasks. Comprehensive evaluation analyses are conducted on ten state-of-the-art methods across multiple datasets under standardized benchmark.\n\nOverall, our work makes the following key contributions:\n\n• We introduce an open-source multimodal deep learning library for emotion recognition based on the fusion of EEG and other physiological signals. Our library implements a standardized, end-to-end workflow, containing every stage from data loading to training and evaluation. It facilitates efficient adoption, flexible configuration, and streamlined experimentation for researchers.\n\n• We establish a fair, transparent, and standardlized benchmark for multimodal EEG-based emotion recognition. This benchmark provides meticulously defined protocols for data preprocessing, experimental tasks, and evaluation metrics, thereby ensuring an unbiased and consistent assessment of diverse methods.\n\n• We conduct extensive experiments on the implemented methods, strictly adhering to the proposed benchmark, and provide a thorough analysis of the results. We systematically report the performance of various models across different tasks and datasets, offering valuable insights and actionable recommendations that can guide and inspire future research in the field.   [11] , DCCA AM  [12]  ,BDDAE  [13] , CFDA-CSF  [14] .\n\nCNN: CNNs use convolutional operations to extract local patterns. In EMER, they typically process multi-channel signals as 2D representations, such as topological electrode maps. G2G  [15]  and CRNN  [16]  are included.\n\nRNN: RNNs utilize recurrent connections to model temporal dependencies in sequential data. In EMER, they are primarily applied to capture signal dynamics over time, though some also process channels sequentially to extract spatial features. BimodalLSTM  [13]  is a representative algorithm. CRNN also cotains RNN architecture.\n\nTransformer: Transformer employs a self-attention mechanism to capture long-range dependencies. For EMER, signals are segmented into patches (by time, channel, or region) to learn their contextual interrelations. CMCM  [17]  and MCAF  [18]  are incorporated.\n\nGNN: GNNs are designed for graph-structured data, making them ideal for modeling the non-Euclidean nature in brain connectivity. In this approach, EEG channels are treated as nodes, and their relationships are captured by an adjacency matrix that can be either predefined from spatial proximity or learned adaptively. HetEmo-tionNet  [19]  is incorporated.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Datasets",
      "text": "Through systematic evaluation of usage frequency, sample scale, data collection and modality coverage we select SEED, SEEDV, and DEAP as our primary evaluation corpora. Table2 shows the basic information.\n\nSEED  [8] : SEED contains EEG recordings from 15 adults (8 male, 7 female), though eye movement data exists only for 12 subjects. Each participant completed three experimental sessions, viewing 15 video stimuli per session corresponding to three emotional states (positive, neutral, negative). It delivers raw data, PSD features, and DE features of EEG data. For eye movement, it provides excel files of eye tracking information and extracted features.\n\nSEEDV  [9] : The SEEDV dataset Comprises EEG and eye movement data from 16 participants (6 male, 10 female), this dataset captures three experimental sessions per subject. Each session presents 15 validated video stimuli eliciting 5 emotional states (happy, sad, neutral, disgust, fear). EEG data contains both raw data and DE featrues. concurrently, eye movement data encompasses information in tabular format and extracted features.\n\nDEAP  [10] : The DEAP dataset encompasses multimodal physiological signals from 32 participants viewing 40 standardized oneminute music video stimuli, with self-reported assessments across four dimensions (arousal, valence, dominance, and liking) following each stimulus presentation; It provides both the original signals sampled at 512 Hz and a preprocessed 40-channel downsampled version (128 Hz) integrating 32 EEG channels alongside 8 PPS channels.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Data Preprocessing",
      "text": "A significant challenge in multimodal emotion recognition lies in the absence of unified and standardized preprocessing protocols. To address this gap, we establish a preprocessing framework tailored to EEG characteristics and dataset modality properties. For EEG processing: baseline removal is first performed, followed by 0.3-50 Hz band-pass filtering. Artifact contamination is subsequently mitigated through principal component analysis (PCA). We then extract differential entropy (DE) features  [20]  across five canonical frequency bands(δ, θ, α, β, γ)  [21]  and apply linear dynamic system (LDS) to smooth the features  [22] .\n\nFor the SEED and SEEDV datasets, eye movement is utilized as a supplementary modality. From this data, a total of 33 features  [23]  are extracted using a 4s non-overlapping window. These features included the Differential Entropy (DE) of pupil diameters and dispersion on the X and Y axes, fixation duration, blink duration, and additional statistical features such as blink frequency and saccade frequency. To ensure temporal alignment between EEG and eyemovement, the DE features from the EEG signals were similarly extracted using a 4s non-overlapping window.\n\nFor the DEAP dataset, since previous research lacks a standardized approach for extraction of peripheral physiological signals, with methods varying between the use of raw signals and various statistical features. In this work, we adopt a unified methodology, extracting EEG DE features and peripheral physiological signal data with a one-second non-overlapping window. For methodologies that originally specified the use of raw signals, we maintained this ap-proach. However, for methods that utilized statistical features or had not been previously tested on the DEAP dataset, a feature set was established through a voting method  [11]    [12] [13] . This process involved computing the maximum value, minimum value, mean, standard deviation, variance, and sum of squares for each of the 8 channels, yielding a 48-dimensional feature in total.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Data Splitting",
      "text": "The strategy for dataset spllitting critically impacts the performance of model. We observe that most studies directly split data into train and test sets and report peak performance on test set without setting up validation set for model selection. Although no data leakage occurs, the approach can raise risk of implicit model tuning towards test set, which may lead to overestimated performance and fail to reflect the true generalization ability of model.\n\nTo address this issue, we adopt a strict cross-validation strategy. Data will be splitted into train, validation and test in a 3:1:1 ratio, with no overlap between them. Model selection is based on optimal performance on validation set. Final reported performance is then derived from evaluation of the selected model on test set.The splitting strategy yields a more accurate assessment of the model's true generalization and performance across tasks.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Evaluation And Metrics",
      "text": "We evaluate the model using two primary emotion recognition tasks shown in Figure1: subject-dependent and subject-independent evaluation. In the Subject-Dependent (SD) task, the training and test sets are derived from different trials of the same subject in the same session. This task primarily assesses the model's performance in scenarios requiring personalization and customization. In the Subject-Independent (SI) task, the training and test data are partitioned based on different subjects, meaning that a subject's data will appear exclusively in either the training or the test set. This task evaluates the model's adaptability to unseen individuals, which is crucial for its viability in large-scale deployment scenarios.\n\nFor performance evaluation, we select the F1-score as our primary metric. Given that the F1-score balances precision and recall, it provides a robust assessment even in the presence of data imbalance. Following our validation protocol, the model checkpoint achieving the highest F1-score on the validation set is chosen for final evaluation. We then report its F1-score on the test set. Furthermore, we also report accuracy, as it is a key metric for classification tasks.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Experiments",
      "text": "We first reproduce the model by strictly following the settings from the source paper. We consider the reproduction successful if its performance is within 10 % of the originally reported results and incorporate it into our algorithm library. These validated models are then used for further experiments on our defined benchmark, followed by a complete evaluation and analysis of the results.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Model Reproduction",
      "text": "As Table3 shows, the performance of reproduced methods, when compared to their originally reported values, exhibited a decline across most tasks. However, these performance discrepancies were consistently within a 10% margin. Conversely, a minority of algorithms showed marginal performance gains on specific tasks. These variations can be primarily attributed to several pervasive challenges. A principal factor is the insufficiently detailed description of model architectures  [13, 18] , data preprocessing pipelines, and experimental settings  [16]  within the source publications. Even in cases where code was provided  [12, 15, 19] , significant performance deviations arose from unavoidable disparities in hardware environments, software platforms, and library versions.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Result And Analysis",
      "text": "Table4 shows the results from evaluating the models under a unified framework with standardized preprocessing and a three-way split. Notably, HetEmotionNet is tested only on DEAP due to its specific architecture. We derive the following key analyses.\n\nObservation 1: Physiological signals provide an objective reflection of responses to external stimuli,aligning more closely with objective, discrete labels than with subjective ratings. Objective data with clear boundaries serve as a higher-quality benchmark for robust evaluation. Models generally show poor performance on the binary classification tasks of the DEAP dataset, whereas they perform better overall on the three-class classification tasks of the SEED dataset. We attribute this to label evaluation and modality characteristics. SEED pre-screens the target emotions for its stimuli to obtain discrete and objective labels. In contrast, the DEAP dataset uses participants' subjective ratings on the two continuous dimensions of arousal and valence as labels. Although peripheral physiological signals can objectively reflect physiological states under different stimuli, they are not controlled by the central nervous system and are less effective for recognizing subjective cognition. Simultaneously, learning complex thought patterns unrelated to emotional stimuli from EEG data is also more challenging. Therefore, objective and discrete labels are clean and suitable for classification tasks. Table  4 . The mean and standard deviations of accuracy (%) and F1-score (%) for chosen methods following the proposed benchmark on subject-dependent and subject-independent tasks. The best model is highlighted in bold. The second best is underlined.  Observation 2: The strategic design of learning tasks and fusion strategies tailored to modal heterogeneity is more critical for performance enhancement than the choice of model architecture alone.\n\nTo evaluate this, we ranked the models that completed all experiments on a scale from n to 1 based on two key metrics for each dataset. The final score for each model is the aggregate of its rankings across all metrics. Figure2 shows scores on two tasks. In subject-dependent tasks, DCCA AM demonstrated the best overall performance, whereas CFDA-CSF achieved the highest score in subject-independent tasks. Concurrently, DNN-based methods presented more robust performance across both task categories, with three such models securing the top rank in 11 out of 16 metrics. The success is mainly attributed to their meticulously designed learning tasks and strategies for cross-modal representation, which facilitate the alignment of modal distributions to improve cross-modal correlation and fusion reliability. Within the Transformer-based models, CMCM significantly outperformed MCAF. This disparity arises because CMCM addresses modal heterogeneity through a credibilityfusion strategy, exploiting a cross-attention mechanism to model sequence consistency. In contrast, MCAF primarily focuses on channel selection and enhancing representation. CNN-based methods showed relatively weaker performance. However, models incorporating RNN architectures performed well on certain tasks, suggest-ing that their inherent sequence learning capabilities hold considerable potential. Based on sophisticated cross-modal strategy , architectures capable of modeling temporal or spatial properties may further achieve more effective representation learning.\n\nObservation 3: The sparsity constrains the representational capacity of deep learning models, hindering their ability to learn complex cross-modal emotion representations.\n\nA significant performance discrepancy was observed under the unified benchmark, with all models performing substantially below their originally reported results. This is further underscored by the large standard deviations across all tasks, which in some cases exceeded the mean, indicating high performance instability. These findings highlight that class and inter-subject variability remain formidable challenges. Deep learning methods rely on large-scale, high-quality data to adequately learn complex patterns. However, the limited size of mainstream dataset with the absence of a validation set in many experimental settings, intensifies the risk of overfitting and leads to inflated performance claims. Our rigorous setup reveals that existing models hard to effectively learn complex cross-modal emotion representations from such sparse data. Therefore, the establishment of large-scale, high-quality datasets with a greater diversity of stimuli and subjects is imperative for advancing the development of EMER.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Conclusion",
      "text": "In this paper, we proposed a unified benchmark and algorithm library for EEG-based multimodal emotion recognition named LibE-MER. LibEMER includes 10 models and standardize end-to-end workflow to support unbiased evaluation across three most popular public datasets on two primary tasks. We put forward some key findings and analyses and identify current research challenges and directions through fair experiments to inspire research in related fields. We believe that LibEMER will accelerate the standardization and transparency process and facilitate the development of new algorithms. In the future, we plan to continue to improve the library and expand the scope of the benchmark.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Two primary tasks for emotion recognition",
      "page": 3
    },
    {
      "caption": "Figure 2: Scores of model performance on two tasks based on Accu-",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "⋆ MIGU Video Co., Ltd., Shanghai, China": "Although\nsubstantial\nprogress\nhas\nbeen made\nin\ndeveloping"
        },
        {
          "⋆ MIGU Video Co., Ltd., Shanghai, China": "deep learning models for EMER, some systematic issues that hin-"
        },
        {
          "⋆ MIGU Video Co., Ltd., Shanghai, China": ""
        },
        {
          "⋆ MIGU Video Co., Ltd., Shanghai, China": "der development of\nrelevant\nresearch remain unsolved.\nThe most"
        },
        {
          "⋆ MIGU Video Co., Ltd., Shanghai, China": ""
        },
        {
          "⋆ MIGU Video Co., Ltd., Shanghai, China": "core challenge is the absence of comprehensive standardized bench-"
        },
        {
          "⋆ MIGU Video Co., Ltd., Shanghai, China": ""
        },
        {
          "⋆ MIGU Video Co., Ltd., Shanghai, China": "marks, which\nfosters\nsignificant\ndiscrepancies\nin\npreprocessing"
        },
        {
          "⋆ MIGU Video Co., Ltd., Shanghai, China": ""
        },
        {
          "⋆ MIGU Video Co., Ltd., Shanghai, China": "pipelines and experimental configurations,\nthereby compromising"
        },
        {
          "⋆ MIGU Video Co., Ltd., Shanghai, China": ""
        },
        {
          "⋆ MIGU Video Co., Ltd., Shanghai, China": "transparency and comparability across studies. The problem is com-"
        },
        {
          "⋆ MIGU Video Co., Ltd., Shanghai, China": ""
        },
        {
          "⋆ MIGU Video Co., Ltd., Shanghai, China": "pounded by insufficience of details in publications and open-source"
        },
        {
          "⋆ MIGU Video Co., Ltd., Shanghai, China": ""
        },
        {
          "⋆ MIGU Video Co., Ltd., Shanghai, China": "codebases, which collectively obstruct verification and replication"
        },
        {
          "⋆ MIGU Video Co., Ltd., Shanghai, China": ""
        },
        {
          "⋆ MIGU Video Co., Ltd., Shanghai, China": "efforts. Even when methods are open-sourced, they are often imple-"
        },
        {
          "⋆ MIGU Video Co., Ltd., Shanghai, China": ""
        },
        {
          "⋆ MIGU Video Co., Ltd., Shanghai, China": "mented on different platforms like PyTorch [4] and TensorFlow [5],"
        },
        {
          "⋆ MIGU Video Co., Ltd., Shanghai, China": ""
        },
        {
          "⋆ MIGU Video Co., Ltd., Shanghai, China": "and their training processes also have various detailed discrepancies,"
        },
        {
          "⋆ MIGU Video Co., Ltd., Shanghai, China": ""
        },
        {
          "⋆ MIGU Video Co., Ltd., Shanghai, China": "making a\nfair\nand unified evaluation difficult.Furthermore, while"
        },
        {
          "⋆ MIGU Video Co., Ltd., Shanghai, China": ""
        },
        {
          "⋆ MIGU Video Co., Ltd., Shanghai, China": "existing surveys [6, 7] have summarized the achivements,\nthey gen-"
        },
        {
          "⋆ MIGU Video Co., Ltd., Shanghai, China": ""
        },
        {
          "⋆ MIGU Video Co., Ltd., Shanghai, China": "erally fall\nshort of providing a critical evaluation and insights\nto"
        },
        {
          "⋆ MIGU Video Co., Ltd., Shanghai, China": ""
        },
        {
          "⋆ MIGU Video Co., Ltd., Shanghai, China": "inspire promising progress."
        },
        {
          "⋆ MIGU Video Co., Ltd., Shanghai, China": ""
        },
        {
          "⋆ MIGU Video Co., Ltd., Shanghai, China": "To address these challenges, we introduce LibEMER, a bench-"
        },
        {
          "⋆ MIGU Video Co., Ltd., Shanghai, China": "mark and open-source algorithm library based on PyTorch that estab-"
        },
        {
          "⋆ MIGU Video Co., Ltd., Shanghai, China": "lishes unified standards for model\nimplementation, data processing,"
        },
        {
          "⋆ MIGU Video Co., Ltd., Shanghai, China": "experimental configurations,\nand evaluation metrics.\nThis\ntoolkit"
        },
        {
          "⋆ MIGU Video Co., Ltd., Shanghai, China": "provides\nfully reproducible workflows and supports\nthree widely-"
        },
        {
          "⋆ MIGU Video Co., Ltd., Shanghai, China": "adopted public datasets, SEED [8], SEEDV [9], and DEAP [10],"
        },
        {
          "⋆ MIGU Video Co., Ltd., Shanghai, China": "across\ntwo\ncritical\nlearning\nparadigms:\nsubject-dependent\nand"
        },
        {
          "⋆ MIGU Video Co., Ltd., Shanghai, China": "subject-independent\nrecognition tasks.\nComprehensive evaluation"
        },
        {
          "⋆ MIGU Video Co., Ltd., Shanghai, China": "analyses are conducted on ten state-of-the-art methods across multi-"
        },
        {
          "⋆ MIGU Video Co., Ltd., Shanghai, China": "ple datasets under standardized benchmark."
        },
        {
          "⋆ MIGU Video Co., Ltd., Shanghai, China": ""
        },
        {
          "⋆ MIGU Video Co., Ltd., Shanghai, China": "Overall, our work makes the following key contributions:"
        },
        {
          "⋆ MIGU Video Co., Ltd., Shanghai, China": ""
        },
        {
          "⋆ MIGU Video Co., Ltd., Shanghai, China": "• We introduce an open-source multimodal deep learning li-"
        },
        {
          "⋆ MIGU Video Co., Ltd., Shanghai, China": "brary for emotion recognition based on the fusion of EEG and"
        },
        {
          "⋆ MIGU Video Co., Ltd., Shanghai, China": "other physiological signals. Our\nlibrary implements a stan-"
        },
        {
          "⋆ MIGU Video Co., Ltd., Shanghai, China": "dardized, end-to-end workflow, containing every stage from"
        },
        {
          "⋆ MIGU Video Co., Ltd., Shanghai, China": "data loading to training and evaluation.\nIt facilitates efficient"
        },
        {
          "⋆ MIGU Video Co., Ltd., Shanghai, China": "adoption, flexible configuration, and streamlined experimen-"
        },
        {
          "⋆ MIGU Video Co., Ltd., Shanghai, China": "tation for researchers."
        },
        {
          "⋆ MIGU Video Co., Ltd., Shanghai, China": ""
        },
        {
          "⋆ MIGU Video Co., Ltd., Shanghai, China": "• We\nestablish a\nfair,\ntransparent,\nand standardlized bench-"
        },
        {
          "⋆ MIGU Video Co., Ltd., Shanghai, China": ""
        },
        {
          "⋆ MIGU Video Co., Ltd., Shanghai, China": "mark for multimodal EEG-based emotion recognition. This"
        },
        {
          "⋆ MIGU Video Co., Ltd., Shanghai, China": ""
        },
        {
          "⋆ MIGU Video Co., Ltd., Shanghai, China": "benchmark provides meticulously defined protocols for data"
        },
        {
          "⋆ MIGU Video Co., Ltd., Shanghai, China": ""
        },
        {
          "⋆ MIGU Video Co., Ltd., Shanghai, China": "preprocessing,\nexperimental\ntasks,\nand evaluation metrics,"
        },
        {
          "⋆ MIGU Video Co., Ltd., Shanghai, China": ""
        },
        {
          "⋆ MIGU Video Co., Ltd., Shanghai, China": "thereby ensuring an unbiased and consistent assessment of"
        },
        {
          "⋆ MIGU Video Co., Ltd., Shanghai, China": ""
        },
        {
          "⋆ MIGU Video Co., Ltd., Shanghai, China": "diverse methods."
        },
        {
          "⋆ MIGU Video Co., Ltd., Shanghai, China": ""
        },
        {
          "⋆ MIGU Video Co., Ltd., Shanghai, China": "• We conduct extensive experiments on the implemented meth-"
        },
        {
          "⋆ MIGU Video Co., Ltd., Shanghai, China": ""
        },
        {
          "⋆ MIGU Video Co., Ltd., Shanghai, China": "ods, strictly adhering to the proposed benchmark, and provide"
        },
        {
          "⋆ MIGU Video Co., Ltd., Shanghai, China": ""
        },
        {
          "⋆ MIGU Video Co., Ltd., Shanghai, China": "a thorough analysis of the results. We systematically report"
        },
        {
          "⋆ MIGU Video Co., Ltd., Shanghai, China": ""
        },
        {
          "⋆ MIGU Video Co., Ltd., Shanghai, China": "the performance of various models across different tasks and"
        },
        {
          "⋆ MIGU Video Co., Ltd., Shanghai, China": ""
        },
        {
          "⋆ MIGU Video Co., Ltd., Shanghai, China": "datasets, offering valuable insights and actionable recommen-"
        },
        {
          "⋆ MIGU Video Co., Ltd., Shanghai, China": "dations that can guide and inspire future research in the field."
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Table 1. key modules and information of proposed benchmark": "",
          "Table 2. The basic information of datasets in LibEMER.": "Dataset\nSession\nSubject\nTrial\nModality\nLabels"
        },
        {
          "Table 1. key modules and information of proposed benchmark": "Module\nKey Information",
          "Table 2. The basic information of datasets in LibEMER.": ""
        },
        {
          "Table 1. key modules and information of proposed benchmark": "",
          "Table 2. The basic information of datasets in LibEMER.": "positive/neutral/"
        },
        {
          "Table 1. key modules and information of proposed benchmark": "Dataset\nSEED/SEEDV/DEAP",
          "Table 2. The basic information of datasets in LibEMER.": "SEED\n3\n12\n15\nEEG/Eye movement"
        },
        {
          "Table 1. key modules and information of proposed benchmark": "",
          "Table 2. The basic information of datasets in LibEMER.": "negative"
        },
        {
          "Table 1. key modules and information of proposed benchmark": "EEG\nBaseline Removal/ Bandpass filter/Artifacts",
          "Table 2. The basic information of datasets in LibEMER.": "happy/sad/fear/"
        },
        {
          "Table 1. key modules and information of proposed benchmark": "",
          "Table 2. The basic information of datasets in LibEMER.": "SEEDV\n3\n16\n15\nEEG/Eye movement"
        },
        {
          "Table 1. key modules and information of proposed benchmark": "Removal/DE Features/Segment",
          "Table 2. The basic information of datasets in LibEMER.": "disgust/neutral"
        },
        {
          "Table 1. key modules and information of proposed benchmark": "",
          "Table 2. The basic information of datasets in LibEMER.": "EEG/EOG/EMG/"
        },
        {
          "Table 1. key modules and information of proposed benchmark": "Eye movement\nExtracted Features /Segment",
          "Table 2. The basic information of datasets in LibEMER.": ""
        },
        {
          "Table 1. key modules and information of proposed benchmark": "",
          "Table 2. The basic information of datasets in LibEMER.": "DEAP\n1\n32\n40\nGSR/RSP/\narousal/valence"
        },
        {
          "Table 1. key modules and information of proposed benchmark": "PPS\nSegment/ Statical features",
          "Table 2. The basic information of datasets in LibEMER.": "TEMP/BVP"
        },
        {
          "Table 1. key modules and information of proposed benchmark": "Data Split\nTrain : Val : Test = 3 : 1 : 1",
          "Table 2. The basic information of datasets in LibEMER.": ""
        },
        {
          "Table 1. key modules and information of proposed benchmark": "Tasks\nSubject-dependent/Subject-independent",
          "Table 2. The basic information of datasets in LibEMER.": ""
        },
        {
          "Table 1. key modules and information of proposed benchmark": "Metrics\nAccuracy/F1-Score",
          "Table 2. The basic information of datasets in LibEMER.": "DEAP as our primary evaluation corpora. Table2 shows the basic"
        },
        {
          "Table 1. key modules and information of proposed benchmark": "",
          "Table 2. The basic information of datasets in LibEMER.": "information."
        },
        {
          "Table 1. key modules and information of proposed benchmark": "",
          "Table 2. The basic information of datasets in LibEMER.": "SEED [8]: SEED contains EEG recordings from 15 adults (8"
        },
        {
          "Table 1. key modules and information of proposed benchmark": "2. BENCHMARK BUILDING",
          "Table 2. The basic information of datasets in LibEMER.": "male, 7 female),\nthough eye movement data exists only for 12 sub-"
        },
        {
          "Table 1. key modules and information of proposed benchmark": "",
          "Table 2. The basic information of datasets in LibEMER.": "jects. Each participant completed three experimental sessions, view-"
        },
        {
          "Table 1. key modules and information of proposed benchmark": "In EEG-based multimodal emotion recognition, no open,\ntranspar-",
          "Table 2. The basic information of datasets in LibEMER.": "ing 15 video stimuli per session corresponding to three emotional"
        },
        {
          "Table 1. key modules and information of proposed benchmark": "ent and well-organized benchmark exists.\nIn this section, we de-",
          "Table 2. The basic information of datasets in LibEMER.": "states (positive, neutral, negative).\nIt delivers raw data, PSD fea-"
        },
        {
          "Table 1. key modules and information of proposed benchmark": "lineate methods selection, datasets, data loading and preprocessing,",
          "Table 2. The basic information of datasets in LibEMER.": "tures, and DE features of EEG data. For eye movement,\nit provides"
        },
        {
          "Table 1. key modules and information of proposed benchmark": "data splitting, and evaluation metrics. Table1 shows the key infor-",
          "Table 2. The basic information of datasets in LibEMER.": "excel files of eye tracking information and extracted features."
        },
        {
          "Table 1. key modules and information of proposed benchmark": "mation of proposed benchmark.",
          "Table 2. The basic information of datasets in LibEMER.": "SEEDV [9]:\nThe SEEDV dataset Comprises EEG and eye"
        },
        {
          "Table 1. key modules and information of proposed benchmark": "",
          "Table 2. The basic information of datasets in LibEMER.": "movement\ndata\nfrom 16\nparticipants\n(6 male,\n10\nfemale),\nthis"
        },
        {
          "Table 1. key modules and information of proposed benchmark": "",
          "Table 2. The basic information of datasets in LibEMER.": "dataset captures three experimental sessions per subject. Each ses-"
        },
        {
          "Table 1. key modules and information of proposed benchmark": "2.1. Methods Selection",
          "Table 2. The basic information of datasets in LibEMER.": ""
        },
        {
          "Table 1. key modules and information of proposed benchmark": "",
          "Table 2. The basic information of datasets in LibEMER.": "sion presents 15 validated video stimuli eliciting 5 emotional states"
        },
        {
          "Table 1. key modules and information of proposed benchmark": "This\nstudy\nadopts\na\nsystematic methodology\ncuration\napproach",
          "Table 2. The basic information of datasets in LibEMER.": "(happy, sad, neutral, disgust, fear). EEG data contains both raw data"
        },
        {
          "Table 1. key modules and information of proposed benchmark": "based on rigorous review of recent publications in leading domain-",
          "Table 2. The basic information of datasets in LibEMER.": "and DE featrues.\nconcurrently,\neye movement data encompasses"
        },
        {
          "Table 1. key modules and information of proposed benchmark": "specific venues\n(such as TAFFC, ACM MM,\nICONIP). Baseline",
          "Table 2. The basic information of datasets in LibEMER.": "information in tabular format and extracted features."
        },
        {
          "Table 1. key modules and information of proposed benchmark": "methods were extracted and frequency-ranked to identify highly-",
          "Table 2. The basic information of datasets in LibEMER.": "DEAP [10]: The DEAP dataset encompasses multimodal phys-"
        },
        {
          "Table 1. key modules and information of proposed benchmark": "cited recurrent\ntechniques,\nsupplemented by SOTA methods pub-",
          "Table 2. The basic information of datasets in LibEMER.": "iological signals from 32 participants viewing 40 standardized one-"
        },
        {
          "Table 1. key modules and information of proposed benchmark": "lished since 2023. From the resulting ranked list, 21 representative",
          "Table 2. The basic information of datasets in LibEMER.": "minute music video stimuli, with self-reported assessments across"
        },
        {
          "Table 1. key modules and information of proposed benchmark": "methods were selected and categorized by architectural paradigm:",
          "Table 2. The basic information of datasets in LibEMER.": "four dimensions (arousal, valence, dominance, and liking) following"
        },
        {
          "Table 1. key modules and information of proposed benchmark": "DNN, CNN, RNN, Transformer, and GNN. Each method underwent",
          "Table 2. The basic information of datasets in LibEMER.": "each stimulus presentation; It provides both the original signals sam-"
        },
        {
          "Table 1. key modules and information of proposed benchmark": "rigorous\nreimplementation with 10 % replication error\ntolerance.",
          "Table 2. The basic information of datasets in LibEMER.": "pled at 512 Hz and a preprocessed 40-channel downsampled version"
        },
        {
          "Table 1. key modules and information of proposed benchmark": "The final curated collection of ten methods ensures minimum repre-",
          "Table 2. The basic information of datasets in LibEMER.": "(128 Hz) integrating 32 EEG channels alongside 8 PPS channels."
        },
        {
          "Table 1. key modules and information of proposed benchmark": "sentation per architectural category.",
          "Table 2. The basic information of datasets in LibEMER.": ""
        },
        {
          "Table 1. key modules and information of proposed benchmark": "DNN: DNNs are forward fully-connected networks that\nlearn",
          "Table 2. The basic information of datasets in LibEMER.": ""
        },
        {
          "Table 1. key modules and information of proposed benchmark": "",
          "Table 2. The basic information of datasets in LibEMER.": "2.3. Data Preprocessing"
        },
        {
          "Table 1. key modules and information of proposed benchmark": "deep emotional\nrepresentations by mapping physiological\nsignals",
          "Table 2. The basic information of datasets in LibEMER.": ""
        },
        {
          "Table 1. key modules and information of proposed benchmark": "into a high-dimensional\nspace.\nThe library contains DCCA [11],",
          "Table 2. The basic information of datasets in LibEMER.": "A significant challenge in multimodal emotion recognition lies in the"
        },
        {
          "Table 1. key modules and information of proposed benchmark": "DCCA AM [12] ,BDDAE [13], CFDA-CSF [14].",
          "Table 2. The basic information of datasets in LibEMER.": "absence of unified and standardized preprocessing protocols. To ad-"
        },
        {
          "Table 1. key modules and information of proposed benchmark": "CNN: CNNs use convolutional operations to extract\nlocal pat-",
          "Table 2. The basic information of datasets in LibEMER.": "dress this gap, we establish a preprocessing framework tailored to"
        },
        {
          "Table 1. key modules and information of proposed benchmark": "terns.\nIn EMER, they typically process multi-channel signals as 2D",
          "Table 2. The basic information of datasets in LibEMER.": "EEG characteristics and dataset modality properties. For EEG pro-"
        },
        {
          "Table 1. key modules and information of proposed benchmark": "representations, such as topological electrode maps. G2G [15] and",
          "Table 2. The basic information of datasets in LibEMER.": "cessing: baseline removal is first performed, followed by 0.3-50 Hz"
        },
        {
          "Table 1. key modules and information of proposed benchmark": "CRNN [16] are included.",
          "Table 2. The basic information of datasets in LibEMER.": "band-pass filtering. Artifact contamination is subsequently mitigated"
        },
        {
          "Table 1. key modules and information of proposed benchmark": "RNN: RNNs utilize recurrent connections\nto model\ntemporal",
          "Table 2. The basic information of datasets in LibEMER.": "through principal component analysis (PCA). We then extract dif-"
        },
        {
          "Table 1. key modules and information of proposed benchmark": "dependencies in sequential data.\nIn EMER,\nthey are primarily ap-",
          "Table 2. The basic information of datasets in LibEMER.": "ferential entropy (DE) features [20] across five canonical frequency"
        },
        {
          "Table 1. key modules and information of proposed benchmark": "plied to capture signal dynamics over time, though some also process",
          "Table 2. The basic information of datasets in LibEMER.": "bands(δ, θ, α, β, γ) [21] and apply linear dynamic system (LDS) to"
        },
        {
          "Table 1. key modules and information of proposed benchmark": "channels sequentially to extract spatial features. BimodalLSTM [13]",
          "Table 2. The basic information of datasets in LibEMER.": "smooth the features [22]."
        },
        {
          "Table 1. key modules and information of proposed benchmark": "is a representative algorithm. CRNN also cotains RNN architecture.",
          "Table 2. The basic information of datasets in LibEMER.": "For the SEED and SEEDV datasets, eye movement is utilized as"
        },
        {
          "Table 1. key modules and information of proposed benchmark": "Transformer: Transformer employs a self-attention mechanism",
          "Table 2. The basic information of datasets in LibEMER.": "a supplementary modality. From this data, a total of 33 features [23]"
        },
        {
          "Table 1. key modules and information of proposed benchmark": "to capture long-range dependencies.\nFor EMER, signals are seg-",
          "Table 2. The basic information of datasets in LibEMER.": "are extracted using a 4s non-overlapping window.\nThese features"
        },
        {
          "Table 1. key modules and information of proposed benchmark": "mented into patches (by time, channel, or region) to learn their con-",
          "Table 2. The basic information of datasets in LibEMER.": "included the Differential Entropy (DE) of pupil diameters and dis-"
        },
        {
          "Table 1. key modules and information of proposed benchmark": "textual interrelations. CMCM [17] and MCAF [18] are incorporated.",
          "Table 2. The basic information of datasets in LibEMER.": "persion on the X and Y axes, fixation duration, blink duration, and"
        },
        {
          "Table 1. key modules and information of proposed benchmark": "GNN: GNNs are designed for graph-structured data, making",
          "Table 2. The basic information of datasets in LibEMER.": "additional statistical\nfeatures such as blink frequency and saccade"
        },
        {
          "Table 1. key modules and information of proposed benchmark": "them ideal for modeling the non-Euclidean nature in brain connec-",
          "Table 2. The basic information of datasets in LibEMER.": "frequency.\nTo ensure temporal alignment between EEG and eye-"
        },
        {
          "Table 1. key modules and information of proposed benchmark": "tivity. In this approach, EEG channels are treated as nodes, and their",
          "Table 2. The basic information of datasets in LibEMER.": "movement, the DE features from the EEG signals were similarly ex-"
        },
        {
          "Table 1. key modules and information of proposed benchmark": "relationships are captured by an adjacency matrix that can be either",
          "Table 2. The basic information of datasets in LibEMER.": "tracted using a 4s non-overlapping window."
        },
        {
          "Table 1. key modules and information of proposed benchmark": "predefined from spatial proximity or\nlearned adaptively. HetEmo-",
          "Table 2. The basic information of datasets in LibEMER.": "For the DEAP dataset, since previous research lacks a standard-"
        },
        {
          "Table 1. key modules and information of proposed benchmark": "tionNet [19] is incorporated.",
          "Table 2. The basic information of datasets in LibEMER.": "ized approach for extraction of peripheral physiological signals, with"
        },
        {
          "Table 1. key modules and information of proposed benchmark": "",
          "Table 2. The basic information of datasets in LibEMER.": "methods varying between the use of raw signals and various statis-"
        },
        {
          "Table 1. key modules and information of proposed benchmark": "",
          "Table 2. The basic information of datasets in LibEMER.": "tical\nfeatures.\nIn this work, we adopt a unified methodology, ex-"
        },
        {
          "Table 1. key modules and information of proposed benchmark": "2.2. Datasets",
          "Table 2. The basic information of datasets in LibEMER.": ""
        },
        {
          "Table 1. key modules and information of proposed benchmark": "",
          "Table 2. The basic information of datasets in LibEMER.": "tracting EEG DE features and peripheral physiological signal data"
        },
        {
          "Table 1. key modules and information of proposed benchmark": "Through systematic evaluation of usage frequency,\nsample scale,",
          "Table 2. The basic information of datasets in LibEMER.": "with a one-second non-overlapping window. For methodologies that"
        },
        {
          "Table 1. key modules and information of proposed benchmark": "data collection and modality coverage we select SEED, SEEDV, and",
          "Table 2. The basic information of datasets in LibEMER.": "originally specified the use of\nraw signals, we maintained this ap-"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "proach. However, for methods that utilized statistical features or had": ""
        },
        {
          "proach. However, for methods that utilized statistical features or had": "not been previously tested on the DEAP dataset, a feature set was"
        },
        {
          "proach. However, for methods that utilized statistical features or had": ""
        },
        {
          "proach. However, for methods that utilized statistical features or had": "established through a voting method [11] [12] [13]. This process in-"
        },
        {
          "proach. However, for methods that utilized statistical features or had": ""
        },
        {
          "proach. However, for methods that utilized statistical features or had": "volved computing the maximum value, minimum value, mean, stan-"
        },
        {
          "proach. However, for methods that utilized statistical features or had": ""
        },
        {
          "proach. However, for methods that utilized statistical features or had": "dard deviation, variance, and sum of squares for each of the 8 chan-"
        },
        {
          "proach. However, for methods that utilized statistical features or had": ""
        },
        {
          "proach. However, for methods that utilized statistical features or had": "nels, yielding a 48-dimensional feature in total."
        },
        {
          "proach. However, for methods that utilized statistical features or had": ""
        },
        {
          "proach. However, for methods that utilized statistical features or had": ""
        },
        {
          "proach. However, for methods that utilized statistical features or had": "2.4. Data Splitting"
        },
        {
          "proach. However, for methods that utilized statistical features or had": ""
        },
        {
          "proach. However, for methods that utilized statistical features or had": "The strategy for dataset spllitting critically impacts the performance"
        },
        {
          "proach. However, for methods that utilized statistical features or had": ""
        },
        {
          "proach. However, for methods that utilized statistical features or had": "of model. We observe that most studies directly split data into train"
        },
        {
          "proach. However, for methods that utilized statistical features or had": ""
        },
        {
          "proach. However, for methods that utilized statistical features or had": "and test sets and report peak performance on test set without setting"
        },
        {
          "proach. However, for methods that utilized statistical features or had": ""
        },
        {
          "proach. However, for methods that utilized statistical features or had": "up validation set for model selection. Although no data leakage oc-"
        },
        {
          "proach. However, for methods that utilized statistical features or had": ""
        },
        {
          "proach. However, for methods that utilized statistical features or had": "curs,\nthe approach can raise risk of\nimplicit model\ntuning towards"
        },
        {
          "proach. However, for methods that utilized statistical features or had": ""
        },
        {
          "proach. However, for methods that utilized statistical features or had": "test set, which may lead to overestimated performance and fail\nto"
        },
        {
          "proach. However, for methods that utilized statistical features or had": ""
        },
        {
          "proach. However, for methods that utilized statistical features or had": "reflect the true generalization ability of model."
        },
        {
          "proach. However, for methods that utilized statistical features or had": ""
        },
        {
          "proach. However, for methods that utilized statistical features or had": ""
        },
        {
          "proach. However, for methods that utilized statistical features or had": "To address this issue, we adopt a strict cross-validation strategy."
        },
        {
          "proach. However, for methods that utilized statistical features or had": ""
        },
        {
          "proach. However, for methods that utilized statistical features or had": "Data will be splitted into train, validation and test\nin a 3:1:1 ratio,"
        },
        {
          "proach. However, for methods that utilized statistical features or had": ""
        },
        {
          "proach. However, for methods that utilized statistical features or had": ""
        },
        {
          "proach. However, for methods that utilized statistical features or had": "with no overlap between them. Model selection is based on optimal"
        },
        {
          "proach. However, for methods that utilized statistical features or had": "performance on validation set.\nFinal\nreported performance is then"
        },
        {
          "proach. However, for methods that utilized statistical features or had": "derived from evaluation of the selected model on test set.The split-"
        },
        {
          "proach. However, for methods that utilized statistical features or had": "ting strategy yields a more accurate assessment of the model’s true"
        },
        {
          "proach. However, for methods that utilized statistical features or had": "generalization and performance across tasks."
        },
        {
          "proach. However, for methods that utilized statistical features or had": ""
        },
        {
          "proach. However, for methods that utilized statistical features or had": "Train\nTrain\nVal\nTest\nVal\nTest"
        },
        {
          "proach. However, for methods that utilized statistical features or had": ""
        },
        {
          "proach. However, for methods that utilized statistical features or had": "Trial 1\nTrial 2\nTrial n-1\nTrial n\n···\nSub\nSub 1\nSub 2\nSub n-1\nSub n"
        },
        {
          "proach. However, for methods that utilized statistical features or had": "···"
        },
        {
          "proach. However, for methods that utilized statistical features or had": ""
        },
        {
          "proach. However, for methods that utilized statistical features or had": "(a) subject-dependent\n(b) subject-independent"
        },
        {
          "proach. However, for methods that utilized statistical features or had": ""
        },
        {
          "proach. However, for methods that utilized statistical features or had": ""
        },
        {
          "proach. However, for methods that utilized statistical features or had": ""
        },
        {
          "proach. However, for methods that utilized statistical features or had": "Fig. 1. Two primary tasks for emotion recognition"
        },
        {
          "proach. However, for methods that utilized statistical features or had": ""
        },
        {
          "proach. However, for methods that utilized statistical features or had": ""
        },
        {
          "proach. However, for methods that utilized statistical features or had": ""
        },
        {
          "proach. However, for methods that utilized statistical features or had": "2.5. Evaluation and Metrics"
        },
        {
          "proach. However, for methods that utilized statistical features or had": ""
        },
        {
          "proach. However, for methods that utilized statistical features or had": ""
        },
        {
          "proach. However, for methods that utilized statistical features or had": "We evaluate the model using two primary emotion recognition tasks"
        },
        {
          "proach. However, for methods that utilized statistical features or had": "shown in Figure1: subject-dependent and subject-independent eval-"
        },
        {
          "proach. However, for methods that utilized statistical features or had": "(SD)\nuation.\nIn the Subject-Dependent\ntask,\nthe training and test"
        },
        {
          "proach. However, for methods that utilized statistical features or had": "sets are derived from different trials of the same subject in the same"
        },
        {
          "proach. However, for methods that utilized statistical features or had": ""
        },
        {
          "proach. However, for methods that utilized statistical features or had": "session. This task primarily assesses the model’s performance in sce-"
        },
        {
          "proach. However, for methods that utilized statistical features or had": ""
        },
        {
          "proach. However, for methods that utilized statistical features or had": "narios requiring personalization and customization.\nIn the Subject-"
        },
        {
          "proach. However, for methods that utilized statistical features or had": ""
        },
        {
          "proach. However, for methods that utilized statistical features or had": "Independent\n(SI)\ntask,\nthe training and test data are partitioned"
        },
        {
          "proach. However, for methods that utilized statistical features or had": ""
        },
        {
          "proach. However, for methods that utilized statistical features or had": "based on different subjects, meaning that a subject’s data will appear"
        },
        {
          "proach. However, for methods that utilized statistical features or had": "exclusively in either the training or the test set. This task evaluates"
        },
        {
          "proach. However, for methods that utilized statistical features or had": "the model’s adaptability to unseen individuals, which is crucial for"
        },
        {
          "proach. However, for methods that utilized statistical features or had": "its viability in large-scale deployment scenarios."
        },
        {
          "proach. However, for methods that utilized statistical features or had": "For performance evaluation, we select\nthe F1-score as our pri-"
        },
        {
          "proach. However, for methods that utilized statistical features or had": "mary metric. Given that the F1-score balances precision and recall, it"
        },
        {
          "proach. However, for methods that utilized statistical features or had": "provides a robust assessment even in the presence of data imbalance."
        },
        {
          "proach. However, for methods that utilized statistical features or had": ""
        },
        {
          "proach. However, for methods that utilized statistical features or had": "Following our validation protocol,\nthe model checkpoint achieving"
        },
        {
          "proach. However, for methods that utilized statistical features or had": ""
        },
        {
          "proach. However, for methods that utilized statistical features or had": "the highest F1-score on the validation set\nis chosen for final evalu-"
        },
        {
          "proach. However, for methods that utilized statistical features or had": ""
        },
        {
          "proach. However, for methods that utilized statistical features or had": "ation. We then report\nits F1-score on the test set. Furthermore, we"
        },
        {
          "proach. However, for methods that utilized statistical features or had": ""
        },
        {
          "proach. However, for methods that utilized statistical features or had": "also report accuracy, as it is a key metric for classification tasks."
        },
        {
          "proach. However, for methods that utilized statistical features or had": ""
        },
        {
          "proach. However, for methods that utilized statistical features or had": ""
        },
        {
          "proach. However, for methods that utilized statistical features or had": "3. EXPERIMENTS"
        },
        {
          "proach. However, for methods that utilized statistical features or had": ""
        },
        {
          "proach. However, for methods that utilized statistical features or had": "We first reproduce the model by strictly following the settings from"
        },
        {
          "proach. However, for methods that utilized statistical features or had": "the source paper. We consider the reproduction successful if its per-"
        },
        {
          "proach. However, for methods that utilized statistical features or had": "formance is within 10 % of the originally reported results and incor-"
        },
        {
          "proach. However, for methods that utilized statistical features or had": "porate it into our algorithm library. These validated models are then"
        },
        {
          "proach. However, for methods that utilized statistical features or had": "used for further experiments on our defined benchmark, followed by"
        },
        {
          "proach. However, for methods that utilized statistical features or had": "a complete evaluation and analysis of the results."
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Table 4. The mean and standard deviations of accuracy (%) and F1-score (%) for chosen methods following the proposed benchmark on": "subject-dependent and subject-independent tasks. The best model is highlighted in bold. The second best is underlined."
        },
        {
          "Table 4. The mean and standard deviations of accuracy (%) and F1-score (%) for chosen methods following the proposed benchmark on": ""
        },
        {
          "Table 4. The mean and standard deviations of accuracy (%) and F1-score (%) for chosen methods following the proposed benchmark on": "Method"
        },
        {
          "Table 4. The mean and standard deviations of accuracy (%) and F1-score (%) for chosen methods following the proposed benchmark on": ""
        },
        {
          "Table 4. The mean and standard deviations of accuracy (%) and F1-score (%) for chosen methods following the proposed benchmark on": ""
        },
        {
          "Table 4. The mean and standard deviations of accuracy (%) and F1-score (%) for chosen methods following the proposed benchmark on": "CFDA-CSF"
        },
        {
          "Table 4. The mean and standard deviations of accuracy (%) and F1-score (%) for chosen methods following the proposed benchmark on": ""
        },
        {
          "Table 4. The mean and standard deviations of accuracy (%) and F1-score (%) for chosen methods following the proposed benchmark on": ""
        },
        {
          "Table 4. The mean and standard deviations of accuracy (%) and F1-score (%) for chosen methods following the proposed benchmark on": "BDDAE"
        },
        {
          "Table 4. The mean and standard deviations of accuracy (%) and F1-score (%) for chosen methods following the proposed benchmark on": ""
        },
        {
          "Table 4. The mean and standard deviations of accuracy (%) and F1-score (%) for chosen methods following the proposed benchmark on": ""
        },
        {
          "Table 4. The mean and standard deviations of accuracy (%) and F1-score (%) for chosen methods following the proposed benchmark on": "CMCM"
        },
        {
          "Table 4. The mean and standard deviations of accuracy (%) and F1-score (%) for chosen methods following the proposed benchmark on": ""
        },
        {
          "Table 4. The mean and standard deviations of accuracy (%) and F1-score (%) for chosen methods following the proposed benchmark on": ""
        },
        {
          "Table 4. The mean and standard deviations of accuracy (%) and F1-score (%) for chosen methods following the proposed benchmark on": "DCCA"
        },
        {
          "Table 4. The mean and standard deviations of accuracy (%) and F1-score (%) for chosen methods following the proposed benchmark on": ""
        },
        {
          "Table 4. The mean and standard deviations of accuracy (%) and F1-score (%) for chosen methods following the proposed benchmark on": ""
        },
        {
          "Table 4. The mean and standard deviations of accuracy (%) and F1-score (%) for chosen methods following the proposed benchmark on": "DCCA AM"
        },
        {
          "Table 4. The mean and standard deviations of accuracy (%) and F1-score (%) for chosen methods following the proposed benchmark on": ""
        },
        {
          "Table 4. The mean and standard deviations of accuracy (%) and F1-score (%) for chosen methods following the proposed benchmark on": ""
        },
        {
          "Table 4. The mean and standard deviations of accuracy (%) and F1-score (%) for chosen methods following the proposed benchmark on": "CRNN"
        },
        {
          "Table 4. The mean and standard deviations of accuracy (%) and F1-score (%) for chosen methods following the proposed benchmark on": ""
        },
        {
          "Table 4. The mean and standard deviations of accuracy (%) and F1-score (%) for chosen methods following the proposed benchmark on": ""
        },
        {
          "Table 4. The mean and standard deviations of accuracy (%) and F1-score (%) for chosen methods following the proposed benchmark on": "BimodalLSTM"
        },
        {
          "Table 4. The mean and standard deviations of accuracy (%) and F1-score (%) for chosen methods following the proposed benchmark on": ""
        },
        {
          "Table 4. The mean and standard deviations of accuracy (%) and F1-score (%) for chosen methods following the proposed benchmark on": ""
        },
        {
          "Table 4. The mean and standard deviations of accuracy (%) and F1-score (%) for chosen methods following the proposed benchmark on": "MCAF"
        },
        {
          "Table 4. The mean and standard deviations of accuracy (%) and F1-score (%) for chosen methods following the proposed benchmark on": ""
        },
        {
          "Table 4. The mean and standard deviations of accuracy (%) and F1-score (%) for chosen methods following the proposed benchmark on": ""
        },
        {
          "Table 4. The mean and standard deviations of accuracy (%) and F1-score (%) for chosen methods following the proposed benchmark on": "G2G"
        },
        {
          "Table 4. The mean and standard deviations of accuracy (%) and F1-score (%) for chosen methods following the proposed benchmark on": ""
        },
        {
          "Table 4. The mean and standard deviations of accuracy (%) and F1-score (%) for chosen methods following the proposed benchmark on": ""
        },
        {
          "Table 4. The mean and standard deviations of accuracy (%) and F1-score (%) for chosen methods following the proposed benchmark on": "HetEmotionNet"
        },
        {
          "Table 4. The mean and standard deviations of accuracy (%) and F1-score (%) for chosen methods following the proposed benchmark on": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "HetEmotionNet\n-\n-\n-\n-": "(6.87)\n(8.53)\n(11.45)",
          "-\n-\n-\n-": "(8.67)\n(4.38)\n(6.22)\n(7.49)\n(8.86)"
        },
        {
          "HetEmotionNet\n-\n-\n-\n-": "60\n60\n56",
          "-\n-\n-\n-": "ing that\ntheir\ninherent sequence learning capabilities hold consid-"
        },
        {
          "HetEmotionNet\n-\n-\n-\n-": "53",
          "-\n-\n-\n-": ""
        },
        {
          "HetEmotionNet\n-\n-\n-\n-": "50\n50\n48\n48\n47",
          "-\n-\n-\n-": ""
        },
        {
          "HetEmotionNet\n-\n-\n-\n-": "46\n45\n43\n43\n41\n41",
          "-\n-\n-\n-": "erable potential. Based on sophisticated cross-modal strategy , ar-"
        },
        {
          "HetEmotionNet\n-\n-\n-\n-": "40\n40\n38",
          "-\n-\n-\n-": ""
        },
        {
          "HetEmotionNet\n-\n-\n-\n-": "36\n33",
          "-\n-\n-\n-": "chitectures capable of modeling temporal or spatial properties may"
        },
        {
          "HetEmotionNet\n-\n-\n-\n-": "29\n30\n30\n28",
          "-\n-\n-\n-": ""
        },
        {
          "HetEmotionNet\n-\n-\n-\n-": "23\n22\n20\n20",
          "-\n-\n-\n-": "further achieve more effective representation learning."
        },
        {
          "HetEmotionNet\n-\n-\n-\n-": "10\n10",
          "-\n-\n-\n-": "Observation 3: The sparsity constrains the representational ca-"
        },
        {
          "HetEmotionNet\n-\n-\n-\n-": "0\n0",
          "-\n-\n-\n-": ""
        },
        {
          "HetEmotionNet\n-\n-\n-\n-": "F\nF\nF\nF\nE\nE\nN\nN\nA\nA\nG\nG\nM\nM\nM\nM\nM\nM\nS\nS\nA\nA\n2\n2\nA\nA\nT\nT\nN\nN\nC\nC\nA\nA\nC\nC\nC\nC\nC\nC\nG\nG\nS\nS\nD\nD\n_\n_\nC\nC\nR\nR\n-\n-\nM\nM\nL\nL\nD\nD\nA\nA\nD\nD\nC\nC\nM\nM\nl\nl\nA\nA\nC\nC",
          "-\n-\n-\n-": "pacity of deep learning models, hindering their ability to learn"
        },
        {
          "HetEmotionNet\n-\n-\n-\n-": "a\na\nC\nC\nB\nB\nD\nD\nd\nd\nC\nC\nF\nF\no\no",
          "-\n-\n-\n-": ""
        },
        {
          "HetEmotionNet\n-\n-\n-\n-": "D\nD\nC\nC\nm\nm\ni\ni",
          "-\n-\n-\n-": ""
        },
        {
          "HetEmotionNet\n-\n-\n-\n-": "B\nB",
          "-\n-\n-\n-": "complex cross-modal emotion representations."
        },
        {
          "HetEmotionNet\n-\n-\n-\n-": "",
          "-\n-\n-\n-": "A significant performance discrepancy was observed under the"
        },
        {
          "HetEmotionNet\n-\n-\n-\n-": "(a) subject-dependent\n(b) subject-independent",
          "-\n-\n-\n-": ""
        },
        {
          "HetEmotionNet\n-\n-\n-\n-": "",
          "-\n-\n-\n-": "unified benchmark, with all models performing substantially below"
        },
        {
          "HetEmotionNet\n-\n-\n-\n-": "",
          "-\n-\n-\n-": "their originally reported results. This is further underscored by the"
        },
        {
          "HetEmotionNet\n-\n-\n-\n-": "Fig. 2. Scores of model performance on two tasks based on Accu-",
          "-\n-\n-\n-": ""
        },
        {
          "HetEmotionNet\n-\n-\n-\n-": "",
          "-\n-\n-\n-": "large standard deviations across all\ntasks, which in some cases ex-"
        },
        {
          "HetEmotionNet\n-\n-\n-\n-": "racy and F1-score",
          "-\n-\n-\n-": ""
        },
        {
          "HetEmotionNet\n-\n-\n-\n-": "",
          "-\n-\n-\n-": "ceeded the mean,\nindicating high performance instability.\nThese"
        },
        {
          "HetEmotionNet\n-\n-\n-\n-": "",
          "-\n-\n-\n-": "findings\nhighlight\nthat\nclass\nand\ninter-subject\nvariability\nremain"
        },
        {
          "HetEmotionNet\n-\n-\n-\n-": "Observation 2: The strategic design of\nlearning tasks and fu-",
          "-\n-\n-\n-": "formidable challenges. Deep learning methods rely on large-scale,"
        },
        {
          "HetEmotionNet\n-\n-\n-\n-": "sion strategies tailored to modal heterogeneity is more critical",
          "-\n-\n-\n-": "high-quality data to adequately learn complex patterns. However,"
        },
        {
          "HetEmotionNet\n-\n-\n-\n-": "for performance enhancement\nthan the choice of model archi-",
          "-\n-\n-\n-": "the limited size of mainstream dataset with the absence of a val-"
        },
        {
          "HetEmotionNet\n-\n-\n-\n-": "tecture alone.",
          "-\n-\n-\n-": "idation set\nin many experimental\nsettings,\nintensifies\nthe\nrisk of"
        },
        {
          "HetEmotionNet\n-\n-\n-\n-": "",
          "-\n-\n-\n-": "overfitting and leads to inflated performance claims. Our\nrigorous"
        },
        {
          "HetEmotionNet\n-\n-\n-\n-": "To evaluate this, we ranked the models that completed all ex-",
          "-\n-\n-\n-": ""
        },
        {
          "HetEmotionNet\n-\n-\n-\n-": "",
          "-\n-\n-\n-": "setup reveals that existing models hard to effectively learn complex"
        },
        {
          "HetEmotionNet\n-\n-\n-\n-": "periments on a scale from n to 1 based on two key metrics for each",
          "-\n-\n-\n-": ""
        },
        {
          "HetEmotionNet\n-\n-\n-\n-": "",
          "-\n-\n-\n-": "cross-modal emotion representations from such sparse data. There-"
        },
        {
          "HetEmotionNet\n-\n-\n-\n-": "dataset. The final score for each model\nis the aggregate of its rank-",
          "-\n-\n-\n-": ""
        },
        {
          "HetEmotionNet\n-\n-\n-\n-": "",
          "-\n-\n-\n-": "fore,\nthe establishment of\nlarge-scale, high-quality datasets with a"
        },
        {
          "HetEmotionNet\n-\n-\n-\n-": "ings across all metrics.\nFigure2 shows\nscores on two tasks.\nIn",
          "-\n-\n-\n-": ""
        },
        {
          "HetEmotionNet\n-\n-\n-\n-": "",
          "-\n-\n-\n-": "greater diversity of stimuli and subjects is imperative for advancing"
        },
        {
          "HetEmotionNet\n-\n-\n-\n-": "subject-dependent\ntasks, DCCA AM demonstrated the best over-",
          "-\n-\n-\n-": ""
        },
        {
          "HetEmotionNet\n-\n-\n-\n-": "",
          "-\n-\n-\n-": "the development of EMER."
        },
        {
          "HetEmotionNet\n-\n-\n-\n-": "all performance, whereas CFDA-CSF achieved the highest score in",
          "-\n-\n-\n-": ""
        },
        {
          "HetEmotionNet\n-\n-\n-\n-": "subject-independent\ntasks. Concurrently, DNN-based methods pre-",
          "-\n-\n-\n-": ""
        },
        {
          "HetEmotionNet\n-\n-\n-\n-": "sented more robust performance across both task categories, with",
          "-\n-\n-\n-": "4. CONCLUSION"
        },
        {
          "HetEmotionNet\n-\n-\n-\n-": "three such models securing the top rank in 11 out of 16 metrics. The",
          "-\n-\n-\n-": ""
        },
        {
          "HetEmotionNet\n-\n-\n-\n-": "success is mainly attributed to their meticulously designed learning",
          "-\n-\n-\n-": "In this paper, we proposed a unified benchmark and algorithm li-"
        },
        {
          "HetEmotionNet\n-\n-\n-\n-": "tasks and strategies for cross-modal representation, which facilitate",
          "-\n-\n-\n-": "brary for EEG-based multimodal emotion recognition named LibE-"
        },
        {
          "HetEmotionNet\n-\n-\n-\n-": "the alignment of modal distributions to improve cross-modal corre-",
          "-\n-\n-\n-": "MER. LibEMER includes 10 models and standardize end-to-end"
        },
        {
          "HetEmotionNet\n-\n-\n-\n-": "lation and fusion reliability. Within the Transformer-based models,",
          "-\n-\n-\n-": "workflow to support unbiased evaluation across\nthree most popu-"
        },
        {
          "HetEmotionNet\n-\n-\n-\n-": "CMCM significantly outperformed MCAF. This disparity arises be-",
          "-\n-\n-\n-": "lar public datasets on two primary tasks. We put\nforward some"
        },
        {
          "HetEmotionNet\n-\n-\n-\n-": "cause CMCM addresses modal heterogeneity through a credibility-",
          "-\n-\n-\n-": "key findings and analyses and identify current\nresearch challenges"
        },
        {
          "HetEmotionNet\n-\n-\n-\n-": "fusion strategy, exploiting a cross-attention mechanism to model se-",
          "-\n-\n-\n-": "and directions through fair experiments to inspire research in related"
        },
        {
          "HetEmotionNet\n-\n-\n-\n-": "quence consistency.\nIn contrast, MCAF primarily focuses on chan-",
          "-\n-\n-\n-": "fields. We believe that LibEMER will accelerate the standardization"
        },
        {
          "HetEmotionNet\n-\n-\n-\n-": "nel\nselection and enhancing representation.\nCNN-based methods",
          "-\n-\n-\n-": "and transparency process and facilitate the development of new al-"
        },
        {
          "HetEmotionNet\n-\n-\n-\n-": "showed relatively weaker performance. However, models incorpo-",
          "-\n-\n-\n-": "gorithms.\nIn the future, we plan to continue to improve the library"
        },
        {
          "HetEmotionNet\n-\n-\n-\n-": "rating RNN architectures performed well on certain tasks, suggest-",
          "-\n-\n-\n-": "and expand the scope of the benchmark."
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "5. ACKNOWLEDGEMENT": "",
          "[11]\nJie-Lin Qiu, Wei Liu, and Bao-Liang Lu,\n“Multi-view emo-": "tion recognition using deep canonical\ncorrelation analysis,”"
        },
        {
          "5. ACKNOWLEDGEMENT": "This work was supported by the National Natural Science Founda-",
          "[11]\nJie-Lin Qiu, Wei Liu, and Bao-Liang Lu,\n“Multi-view emo-": "in International conference on neural\ninformation processing."
        },
        {
          "5. ACKNOWLEDGEMENT": "tion of China (62202367), Project of China Knowledge Centre for",
          "[11]\nJie-Lin Qiu, Wei Liu, and Bao-Liang Lu,\n“Multi-view emo-": "Springer, 2018, pp. 221–231."
        },
        {
          "5. ACKNOWLEDGEMENT": "Engineering Science and Technology.",
          "[11]\nJie-Lin Qiu, Wei Liu, and Bao-Liang Lu,\n“Multi-view emo-": ""
        },
        {
          "5. ACKNOWLEDGEMENT": "",
          "[11]\nJie-Lin Qiu, Wei Liu, and Bao-Liang Lu,\n“Multi-view emo-": "[12] Wei Liu,\nJie-Lin Qiu, Wei-Long Zheng, and Bao-Liang Lu,"
        },
        {
          "5. ACKNOWLEDGEMENT": "",
          "[11]\nJie-Lin Qiu, Wei Liu, and Bao-Liang Lu,\n“Multi-view emo-": "“Comparing recognition performance and robustness of mul-"
        },
        {
          "5. ACKNOWLEDGEMENT": "",
          "[11]\nJie-Lin Qiu, Wei Liu, and Bao-Liang Lu,\n“Multi-view emo-": "timodal deep learning models for multimodal emotion recog-"
        },
        {
          "5. ACKNOWLEDGEMENT": "6. REFERENCES",
          "[11]\nJie-Lin Qiu, Wei Liu, and Bao-Liang Lu,\n“Multi-view emo-": ""
        },
        {
          "5. ACKNOWLEDGEMENT": "",
          "[11]\nJie-Lin Qiu, Wei Liu, and Bao-Liang Lu,\n“Multi-view emo-": "IEEE Transactions on Cognitive and Developmental\nnition,”"
        },
        {
          "5. ACKNOWLEDGEMENT": "",
          "[11]\nJie-Lin Qiu, Wei Liu, and Bao-Liang Lu,\n“Multi-view emo-": "Systems, vol. 14, no. 2, pp. 715–729, 2021."
        },
        {
          "5. ACKNOWLEDGEMENT": "[1] Zhi Zhang, Sheng-hua Zhong, and Yan Liu, “Torcheegemo: A",
          "[11]\nJie-Lin Qiu, Wei Liu, and Bao-Liang Lu,\n“Multi-view emo-": ""
        },
        {
          "5. ACKNOWLEDGEMENT": "deep learning toolbox towards eeg-based emotion recognition,”",
          "[11]\nJie-Lin Qiu, Wei Liu, and Bao-Liang Lu,\n“Multi-view emo-": "[13] Hao Tang, Wei Liu, Wei-Long Zheng,\nand Bao-Liang Lu,"
        },
        {
          "5. ACKNOWLEDGEMENT": "Expert Systems with Applications, vol. 249, pp. 123550, 2024.",
          "[11]\nJie-Lin Qiu, Wei Liu, and Bao-Liang Lu,\n“Multi-view emo-": "“Multimodal\nemotion\nrecognition\nusing\ndeep\nneural\nnet-"
        },
        {
          "5. ACKNOWLEDGEMENT": "",
          "[11]\nJie-Lin Qiu, Wei Liu, and Bao-Liang Lu,\n“Multi-view emo-": "Information\nworks,”\nin International Conference on Neural"
        },
        {
          "5. ACKNOWLEDGEMENT": "[2] Kaiyuan Zhang, Ziyi Ye, Qingyao Ai, Xiaohui Xie, and Yiqun",
          "[11]\nJie-Lin Qiu, Wei Liu, and Bao-Liang Lu,\n“Multi-view emo-": ""
        },
        {
          "5. ACKNOWLEDGEMENT": "",
          "[11]\nJie-Lin Qiu, Wei Liu, and Bao-Liang Lu,\n“Multi-view emo-": "Processing. Springer, 2017, pp. 811–819."
        },
        {
          "5. ACKNOWLEDGEMENT": "Liu,\n“Gnn4eeg:\nA benchmark\nand\ntoolkit\nfor\nelectroen-",
          "[11]\nJie-Lin Qiu, Wei Liu, and Bao-Liang Lu,\n“Multi-view emo-": ""
        },
        {
          "5. ACKNOWLEDGEMENT": "cephalography classification with graph neural network,”\nin",
          "[11]\nJie-Lin Qiu, Wei Liu, and Bao-Liang Lu,\n“Multi-view emo-": "[14] Magdiel\nJim´enez-Guarneros\nand\nGibran\nFuentes-Pineda,"
        },
        {
          "5. ACKNOWLEDGEMENT": "Companion of\nthe 2024 on ACM International Joint Confer-",
          "[11]\nJie-Lin Qiu, Wei Liu, and Bao-Liang Lu,\n“Multi-view emo-": "“Cfda-csf:\nA multi-modal\ndomain\nadaptation method\nfor"
        },
        {
          "5. ACKNOWLEDGEMENT": "ence on Pervasive and Ubiquitous Computing, 2024, pp. 612–",
          "[11]\nJie-Lin Qiu, Wei Liu, and Bao-Liang Lu,\n“Multi-view emo-": "cross-subject emotion recognition,” IEEE Transactions on Af-"
        },
        {
          "5. ACKNOWLEDGEMENT": "617.",
          "[11]\nJie-Lin Qiu, Wei Liu, and Bao-Liang Lu,\n“Multi-view emo-": "fective Computing, vol. 15, no. 3, pp. 1502–1513, 2024."
        },
        {
          "5. ACKNOWLEDGEMENT": "",
          "[11]\nJie-Lin Qiu, Wei Liu, and Bao-Liang Lu,\n“Multi-view emo-": "[15] Ming Jin and Jinpeng Li,\n“Graph to grid: Learning deep rep-"
        },
        {
          "5. ACKNOWLEDGEMENT": "[3] Huan Liu, Shusen Yang, Yuzhe Zhang, Mengze Wang, Fanyu",
          "[11]\nJie-Lin Qiu, Wei Liu, and Bao-Liang Lu,\n“Multi-view emo-": ""
        },
        {
          "5. ACKNOWLEDGEMENT": "",
          "[11]\nJie-Lin Qiu, Wei Liu, and Bao-Liang Lu,\n“Multi-view emo-": "resentations for multimodal emotion recognition,” in Proceed-"
        },
        {
          "5. ACKNOWLEDGEMENT": "Gong, Chengxi Xie, Guanjian Liu, Zejun Liu, Yong-Jin Liu,",
          "[11]\nJie-Lin Qiu, Wei Liu, and Bao-Liang Lu,\n“Multi-view emo-": ""
        },
        {
          "5. ACKNOWLEDGEMENT": "",
          "[11]\nJie-Lin Qiu, Wei Liu, and Bao-Liang Lu,\n“Multi-view emo-": "ings of the 31st ACM International Conference on Multimedia,"
        },
        {
          "5. ACKNOWLEDGEMENT": "Bao-Liang Lu,\net\nal.,\n“Libeer:\nA comprehensive\nbench-",
          "[11]\nJie-Lin Qiu, Wei Liu, and Bao-Liang Lu,\n“Multi-view emo-": ""
        },
        {
          "5. ACKNOWLEDGEMENT": "",
          "[11]\nJie-Lin Qiu, Wei Liu, and Bao-Liang Lu,\n“Multi-view emo-": "2023, pp. 5985–5993."
        },
        {
          "5. ACKNOWLEDGEMENT": "mark and algorithm library for\neeg-based emotion recogni-",
          "[11]\nJie-Lin Qiu, Wei Liu, and Bao-Liang Lu,\n“Multi-view emo-": ""
        },
        {
          "5. ACKNOWLEDGEMENT": "tion,” IEEE Transactions on Affective Computing, 2025.",
          "[11]\nJie-Lin Qiu, Wei Liu, and Bao-Liang Lu,\n“Multi-view emo-": "[16]\nJinxiang Liao, Qinghua Zhong, Yongsheng Zhu, and Dongli"
        },
        {
          "5. ACKNOWLEDGEMENT": "",
          "[11]\nJie-Lin Qiu, Wei Liu, and Bao-Liang Lu,\n“Multi-view emo-": "Cai,\n“Multimodal physiological\nsignal emotion recognition"
        },
        {
          "5. ACKNOWLEDGEMENT": "[4] Adam Paszke,\nSam Gross, Francisco Massa, Adam Lerer,",
          "[11]\nJie-Lin Qiu, Wei Liu, and Bao-Liang Lu,\n“Multi-view emo-": ""
        },
        {
          "5. ACKNOWLEDGEMENT": "",
          "[11]\nJie-Lin Qiu, Wei Liu, and Bao-Liang Lu,\n“Multi-view emo-": "based on convolutional recurrent neural network,” in IOP con-"
        },
        {
          "5. ACKNOWLEDGEMENT": "James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin,",
          "[11]\nJie-Lin Qiu, Wei Liu, and Bao-Liang Lu,\n“Multi-view emo-": ""
        },
        {
          "5. ACKNOWLEDGEMENT": "",
          "[11]\nJie-Lin Qiu, Wei Liu, and Bao-Liang Lu,\n“Multi-view emo-": "ference series: materials science and engineering.\nIOP Pub-"
        },
        {
          "5. ACKNOWLEDGEMENT": "Natalia Gimelshein, Luca Antiga, et al.,\n“Pytorch: An imper-",
          "[11]\nJie-Lin Qiu, Wei Liu, and Bao-Liang Lu,\n“Multi-view emo-": ""
        },
        {
          "5. ACKNOWLEDGEMENT": "",
          "[11]\nJie-Lin Qiu, Wei Liu, and Bao-Liang Lu,\n“Multi-view emo-": "lishing, 2020, vol. 782, p. 032005."
        },
        {
          "5. ACKNOWLEDGEMENT": "ative style, high-performance deep learning library,” Advances",
          "[11]\nJie-Lin Qiu, Wei Liu, and Bao-Liang Lu,\n“Multi-view emo-": ""
        },
        {
          "5. ACKNOWLEDGEMENT": "in neural information processing systems, vol. 32, 2019.",
          "[11]\nJie-Lin Qiu, Wei Liu, and Bao-Liang Lu,\n“Multi-view emo-": "[17] Yuzhe Zhang, Huan Liu, Di Wang, Dalin Zhang, Tianyu"
        },
        {
          "5. ACKNOWLEDGEMENT": "",
          "[11]\nJie-Lin Qiu, Wei Liu, and Bao-Liang Lu,\n“Multi-view emo-": "Lou, Qinghua Zheng, and Chai Quek,\n“Cross-modal credibil-"
        },
        {
          "5. ACKNOWLEDGEMENT": "[5] Mart´ın Abadi, Paul Barham,\nJianmin Chen, Zhifeng Chen,",
          "[11]\nJie-Lin Qiu, Wei Liu, and Bao-Liang Lu,\n“Multi-view emo-": ""
        },
        {
          "5. ACKNOWLEDGEMENT": "",
          "[11]\nJie-Lin Qiu, Wei Liu, and Bao-Liang Lu,\n“Multi-view emo-": "ity modelling for eeg-based multimodal emotion recognition,”"
        },
        {
          "5. ACKNOWLEDGEMENT": "Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat,",
          "[11]\nJie-Lin Qiu, Wei Liu, and Bao-Liang Lu,\n“Multi-view emo-": ""
        },
        {
          "5. ACKNOWLEDGEMENT": "",
          "[11]\nJie-Lin Qiu, Wei Liu, and Bao-Liang Lu,\n“Multi-view emo-": "Journal of Neural Engineering, vol. 21, no. 2, pp. 026040,"
        },
        {
          "5. ACKNOWLEDGEMENT": "Geoffrey Irving, Michael Isard, et al.,\n“{TensorFlow}: a sys-",
          "[11]\nJie-Lin Qiu, Wei Liu, and Bao-Liang Lu,\n“Multi-view emo-": ""
        },
        {
          "5. ACKNOWLEDGEMENT": "",
          "[11]\nJie-Lin Qiu, Wei Liu, and Bao-Liang Lu,\n“Multi-view emo-": "2024."
        },
        {
          "5. ACKNOWLEDGEMENT": "tem for {Large-Scale} machine learning,”\nin 12th USENIX",
          "[11]\nJie-Lin Qiu, Wei Liu, and Bao-Liang Lu,\n“Multi-view emo-": ""
        },
        {
          "5. ACKNOWLEDGEMENT": "symposium on operating systems design and implementation",
          "[11]\nJie-Lin Qiu, Wei Liu, and Bao-Liang Lu,\n“Multi-view emo-": "[18]\nJialai Yin, Minchao Wu, Yan Yang, Ping Li, Fan Li, Wen"
        },
        {
          "5. ACKNOWLEDGEMENT": "(OSDI 16), 2016, pp. 265–283.",
          "[11]\nJie-Lin Qiu, Wei Liu, and Bao-Liang Lu,\n“Multi-view emo-": "Liang,\nand Zhao Lv,\n“Research\non multimodal\nemotion"
        },
        {
          "5. ACKNOWLEDGEMENT": "",
          "[11]\nJie-Lin Qiu, Wei Liu, and Bao-Liang Lu,\n“Multi-view emo-": "recognition based on fusion of electroencephalogram and elec-"
        },
        {
          "5. ACKNOWLEDGEMENT": "[6] Huan Liu, Tianyu Lou, Yuzhe Zhang, Yixiao Wu, Yang Xiao,",
          "[11]\nJie-Lin Qiu, Wei Liu, and Bao-Liang Lu,\n“Multi-view emo-": ""
        },
        {
          "5. ACKNOWLEDGEMENT": "",
          "[11]\nJie-Lin Qiu, Wei Liu, and Bao-Liang Lu,\n“Multi-view emo-": "IEEE Transactions on Instrumentation and\ntrooculography,”"
        },
        {
          "5. ACKNOWLEDGEMENT": "Christian S Jensen, and Dalin Zhang,\n“Eeg-based multimodal",
          "[11]\nJie-Lin Qiu, Wei Liu, and Bao-Liang Lu,\n“Multi-view emo-": ""
        },
        {
          "5. ACKNOWLEDGEMENT": "",
          "[11]\nJie-Lin Qiu, Wei Liu, and Bao-Liang Lu,\n“Multi-view emo-": "Measurement, vol. 73, pp. 1–12, 2024."
        },
        {
          "5. ACKNOWLEDGEMENT": "IEEE\nemotion recognition: A machine learning perspective,”",
          "[11]\nJie-Lin Qiu, Wei Liu, and Bao-Liang Lu,\n“Multi-view emo-": ""
        },
        {
          "5. ACKNOWLEDGEMENT": "",
          "[11]\nJie-Lin Qiu, Wei Liu, and Bao-Liang Lu,\n“Multi-view emo-": "[19] Ziyu Jia, Youfang Lin, Jing Wang, Zhiyang Feng, Xiangheng"
        },
        {
          "5. ACKNOWLEDGEMENT": "Transactions on Instrumentation and Measurement, vol. 73,",
          "[11]\nJie-Lin Qiu, Wei Liu, and Bao-Liang Lu,\n“Multi-view emo-": ""
        },
        {
          "5. ACKNOWLEDGEMENT": "",
          "[11]\nJie-Lin Qiu, Wei Liu, and Bao-Liang Lu,\n“Multi-view emo-": "Xie, and Caijie Chen,\n“Hetemotionnet:\ntwo-stream heteroge-"
        },
        {
          "5. ACKNOWLEDGEMENT": "pp. 1–29, 2024.",
          "[11]\nJie-Lin Qiu, Wei Liu, and Bao-Liang Lu,\n“Multi-view emo-": ""
        },
        {
          "5. ACKNOWLEDGEMENT": "",
          "[11]\nJie-Lin Qiu, Wei Liu, and Bao-Liang Lu,\n“Multi-view emo-": "neous graph recurrent neural network for multi-modal emotion"
        },
        {
          "5. ACKNOWLEDGEMENT": "[7] Rajasekhar Pillalamarri and Udhayakumar Shanmugam,\n“A",
          "[11]\nJie-Lin Qiu, Wei Liu, and Bao-Liang Lu,\n“Multi-view emo-": ""
        },
        {
          "5. ACKNOWLEDGEMENT": "",
          "[11]\nJie-Lin Qiu, Wei Liu, and Bao-Liang Lu,\n“Multi-view emo-": "the 29th ACM international\nrecognition,”\nin Proceedings of"
        },
        {
          "5. ACKNOWLEDGEMENT": "review on eeg-based multimodal\nlearning for emotion recog-",
          "[11]\nJie-Lin Qiu, Wei Liu, and Bao-Liang Lu,\n“Multi-view emo-": ""
        },
        {
          "5. ACKNOWLEDGEMENT": "",
          "[11]\nJie-Lin Qiu, Wei Liu, and Bao-Liang Lu,\n“Multi-view emo-": "conference on multimedia, 2021, pp. 1047–1056."
        },
        {
          "5. ACKNOWLEDGEMENT": "nition,” Artificial Intelligence Review, vol. 58, no. 5, pp. 131,",
          "[11]\nJie-Lin Qiu, Wei Liu, and Bao-Liang Lu,\n“Multi-view emo-": ""
        },
        {
          "5. ACKNOWLEDGEMENT": "",
          "[11]\nJie-Lin Qiu, Wei Liu, and Bao-Liang Lu,\n“Multi-view emo-": "[20] Ruo-Nan Duan, Jia-Yi Zhu, and Bao-Liang Lu,\n“Differential"
        },
        {
          "5. ACKNOWLEDGEMENT": "2025.",
          "[11]\nJie-Lin Qiu, Wei Liu, and Bao-Liang Lu,\n“Multi-view emo-": ""
        },
        {
          "5. ACKNOWLEDGEMENT": "",
          "[11]\nJie-Lin Qiu, Wei Liu, and Bao-Liang Lu,\n“Multi-view emo-": "entropy feature for eeg-based emotion classification,”\nin 2013"
        },
        {
          "5. ACKNOWLEDGEMENT": "[8] Wei-Long Zheng and Bao-Liang Lu,\n“Investigating critical",
          "[11]\nJie-Lin Qiu, Wei Liu, and Bao-Liang Lu,\n“Multi-view emo-": "6th international IEEE/EMBS conference on neural engineer-"
        },
        {
          "5. ACKNOWLEDGEMENT": "frequency bands and channels\nfor eeg-based emotion recog-",
          "[11]\nJie-Lin Qiu, Wei Liu, and Bao-Liang Lu,\n“Multi-view emo-": "ing (NER). IEEE, 2013, pp. 81–84."
        },
        {
          "5. ACKNOWLEDGEMENT": "IEEE Transactions on\nnition with deep neural networks,”",
          "[11]\nJie-Lin Qiu, Wei Liu, and Bao-Liang Lu,\n“Multi-view emo-": ""
        },
        {
          "5. ACKNOWLEDGEMENT": "",
          "[11]\nJie-Lin Qiu, Wei Liu, and Bao-Liang Lu,\n“Multi-view emo-": "[21]\nSaeid Sanei and Jonathon A Chambers, EEG signal process-"
        },
        {
          "5. ACKNOWLEDGEMENT": "autonomous mental development, vol. 7, no. 3, pp. 162–175,",
          "[11]\nJie-Lin Qiu, Wei Liu, and Bao-Liang Lu,\n“Multi-view emo-": ""
        },
        {
          "5. ACKNOWLEDGEMENT": "",
          "[11]\nJie-Lin Qiu, Wei Liu, and Bao-Liang Lu,\n“Multi-view emo-": "ing, John Wiley & Sons, 2013."
        },
        {
          "5. ACKNOWLEDGEMENT": "2015.",
          "[11]\nJie-Lin Qiu, Wei Liu, and Bao-Liang Lu,\n“Multi-view emo-": ""
        },
        {
          "5. ACKNOWLEDGEMENT": "",
          "[11]\nJie-Lin Qiu, Wei Liu, and Bao-Liang Lu,\n“Multi-view emo-": "[22] Ruo-Nan Duan, Xiao-Wei Wang, and Bao-Liang Lu,\n“Eeg-"
        },
        {
          "5. ACKNOWLEDGEMENT": "[9] Tian-Hao Li, Wei Liu, Wei-Long Zheng, and Bao-Liang Lu,",
          "[11]\nJie-Lin Qiu, Wei Liu, and Bao-Liang Lu,\n“Multi-view emo-": ""
        },
        {
          "5. ACKNOWLEDGEMENT": "",
          "[11]\nJie-Lin Qiu, Wei Liu, and Bao-Liang Lu,\n“Multi-view emo-": "based emotion recognition in listening music by using support"
        },
        {
          "5. ACKNOWLEDGEMENT": "“Classification of five emotions from eeg and eye movement",
          "[11]\nJie-Lin Qiu, Wei Liu, and Bao-Liang Lu,\n“Multi-view emo-": ""
        },
        {
          "5. ACKNOWLEDGEMENT": "",
          "[11]\nJie-Lin Qiu, Wei Liu, and Bao-Liang Lu,\n“Multi-view emo-": "vector machine and linear dynamic system,”\nin International"
        },
        {
          "5. ACKNOWLEDGEMENT": "signals: Discrimination ability and stability over\ntime,”\nin",
          "[11]\nJie-Lin Qiu, Wei Liu, and Bao-Liang Lu,\n“Multi-view emo-": ""
        },
        {
          "5. ACKNOWLEDGEMENT": "",
          "[11]\nJie-Lin Qiu, Wei Liu, and Bao-Liang Lu,\n“Multi-view emo-": "conference on neural\ninformation processing. Springer, 2012,"
        },
        {
          "5. ACKNOWLEDGEMENT": "2019 9th International IEEE/EMBS Conference on Neural En-",
          "[11]\nJie-Lin Qiu, Wei Liu, and Bao-Liang Lu,\n“Multi-view emo-": ""
        },
        {
          "5. ACKNOWLEDGEMENT": "",
          "[11]\nJie-Lin Qiu, Wei Liu, and Bao-Liang Lu,\n“Multi-view emo-": "pp. 468–475."
        },
        {
          "5. ACKNOWLEDGEMENT": "gineering (NER). IEEE, 2019, pp. 607–610.",
          "[11]\nJie-Lin Qiu, Wei Liu, and Bao-Liang Lu,\n“Multi-view emo-": ""
        },
        {
          "5. ACKNOWLEDGEMENT": "",
          "[11]\nJie-Lin Qiu, Wei Liu, and Bao-Liang Lu,\n“Multi-view emo-": "[23] Yifei Lu, Wei-Long Zheng, Binbin Li,\nand Bao-Liang Lu,"
        },
        {
          "5. ACKNOWLEDGEMENT": "[10]\nSander Koelstra,\nChristian Muhl, Mohammad\nSoleymani,",
          "[11]\nJie-Lin Qiu, Wei Liu, and Bao-Liang Lu,\n“Multi-view emo-": ""
        },
        {
          "5. ACKNOWLEDGEMENT": "",
          "[11]\nJie-Lin Qiu, Wei Liu, and Bao-Liang Lu,\n“Multi-view emo-": "“Combining\neye movements\nand\neeg\nto\nenhance\nemotion"
        },
        {
          "5. ACKNOWLEDGEMENT": "Jong-Seok Lee, Ashkan Yazdani, Touradj Ebrahimi, Thierry",
          "[11]\nJie-Lin Qiu, Wei Liu, and Bao-Liang Lu,\n“Multi-view emo-": ""
        },
        {
          "5. ACKNOWLEDGEMENT": "",
          "[11]\nJie-Lin Qiu, Wei Liu, and Bao-Liang Lu,\n“Multi-view emo-": "recognition.,” in IJCAI. Buenos Aires, 2015, vol. 15, pp. 1170–"
        },
        {
          "5. ACKNOWLEDGEMENT": "Pun, Anton Nijholt, and Ioannis Patras, “Deap: A database for",
          "[11]\nJie-Lin Qiu, Wei Liu, and Bao-Liang Lu,\n“Multi-view emo-": ""
        },
        {
          "5. ACKNOWLEDGEMENT": "",
          "[11]\nJie-Lin Qiu, Wei Liu, and Bao-Liang Lu,\n“Multi-view emo-": "1176."
        },
        {
          "5. ACKNOWLEDGEMENT": "emotion analysis; using physiological signals,” IEEE transac-",
          "[11]\nJie-Lin Qiu, Wei Liu, and Bao-Liang Lu,\n“Multi-view emo-": ""
        },
        {
          "5. ACKNOWLEDGEMENT": "tions on affective computing, vol. 3, no. 1, pp. 18–31, 2011.",
          "[11]\nJie-Lin Qiu, Wei Liu, and Bao-Liang Lu,\n“Multi-view emo-": ""
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Torcheegemo: A deep learning toolbox towards eeg-based emotion recognition",
      "authors": [
        "Zhi Zhang",
        "Sheng-Hua Zhong",
        "Yan Liu"
      ],
      "year": "2024",
      "venue": "Expert Systems with Applications"
    },
    {
      "citation_id": "3",
      "title": "Gnn4eeg: A benchmark and toolkit for electroencephalography classification with graph neural network",
      "authors": [
        "Kaiyuan Zhang",
        "Ziyi Ye",
        "Qingyao Ai",
        "Xiaohui Xie",
        "Yiqun Liu"
      ],
      "year": "2024",
      "venue": "Companion of the 2024 on ACM International Joint Conference on Pervasive and Ubiquitous Computing"
    },
    {
      "citation_id": "4",
      "title": "Libeer: A comprehensive benchmark and algorithm library for eeg-based emotion recognition",
      "authors": [
        "Huan Liu",
        "Shusen Yang",
        "Yuzhe Zhang",
        "Mengze Wang",
        "Fanyu Gong",
        "Chengxi Xie",
        "Guanjian Liu",
        "Zejun Liu",
        "Yong-Jin Liu",
        "Bao-Liang Lu"
      ],
      "year": "2025",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "5",
      "title": "Pytorch: An imperative style, high-performance deep learning library",
      "authors": [
        "Adam Paszke",
        "Sam Gross",
        "Francisco Massa",
        "Adam Lerer",
        "James Bradbury",
        "Gregory Chanan",
        "Trevor Killeen",
        "Zeming Lin",
        "Natalia Gimelshein",
        "Luca Antiga"
      ],
      "year": "2019",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "6",
      "title": "{TensorFlow}: a system for {Large-Scale} machine learning",
      "authors": [
        "Martín Abadi",
        "Paul Barham",
        "Jianmin Chen",
        "Zhifeng Chen",
        "Andy Davis",
        "Jeffrey Dean",
        "Matthieu Devin",
        "Sanjay Ghemawat",
        "Geoffrey Irving",
        "Michael Isard"
      ],
      "year": "2016",
      "venue": "12th USENIX symposium on operating systems design and implementation"
    },
    {
      "citation_id": "7",
      "title": "Eeg-based multimodal emotion recognition: A machine learning perspective",
      "authors": [
        "Huan Liu",
        "Tianyu Lou",
        "Yuzhe Zhang",
        "Yixiao Wu",
        "Yang Xiao",
        "Christian Jensen",
        "Dalin Zhang"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Instrumentation and Measurement"
    },
    {
      "citation_id": "8",
      "title": "A review on eeg-based multimodal learning for emotion recognition",
      "authors": [
        "Rajasekhar Pillalamarri",
        "Udhayakumar Shanmugam"
      ],
      "year": "2025",
      "venue": "Artificial Intelligence Review"
    },
    {
      "citation_id": "9",
      "title": "Investigating critical frequency bands and channels for eeg-based emotion recognition with deep neural networks",
      "authors": [
        "Wei-Long Zheng",
        "Bao-Liang Lu"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on autonomous mental development"
    },
    {
      "citation_id": "10",
      "title": "Classification of five emotions from eeg and eye movement signals: Discrimination ability and stability over time",
      "authors": [
        "Tian-Hao Li",
        "Wei Liu",
        "Wei-Long Zheng",
        "Bao-Liang Lu"
      ],
      "year": "2019",
      "venue": "2019 9th International IEEE/EMBS Conference on Neural Engineering (NER)"
    },
    {
      "citation_id": "11",
      "title": "Deap: A database for emotion analysis; using physiological signals",
      "authors": [
        "Sander Koelstra",
        "Christian Muhl",
        "Mohammad Soleymani",
        "Jong-Seok Lee",
        "Ashkan Yazdani",
        "Touradj Ebrahimi",
        "Anton Thierry Pun",
        "Ioannis Nijholt",
        "Patras"
      ],
      "year": "2011",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "12",
      "title": "Multi-view emotion recognition using deep canonical correlation analysis",
      "authors": [
        "Jie-Lin Qiu",
        "Wei Liu",
        "Bao-Liang Lu"
      ],
      "year": "2018",
      "venue": "International conference on neural information processing"
    },
    {
      "citation_id": "13",
      "title": "Comparing recognition performance and robustness of multimodal deep learning models for multimodal emotion recognition",
      "authors": [
        "Wei Liu",
        "Jie-Lin Qiu",
        "Wei-Long Zheng",
        "Bao-Liang Lu"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Cognitive and Developmental Systems"
    },
    {
      "citation_id": "14",
      "title": "Multimodal emotion recognition using deep neural networks",
      "authors": [
        "Hao Tang",
        "Wei Liu",
        "Wei-Long Zheng",
        "Bao-Liang Lu"
      ],
      "year": "2017",
      "venue": "International Conference on Neural Information Processing"
    },
    {
      "citation_id": "15",
      "title": "Cfda-csf: A multi-modal domain adaptation method for cross-subject emotion recognition",
      "authors": [
        "Magdiel Jiménez",
        "Gibran Fuentes-Pineda"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "16",
      "title": "Graph to grid: Learning deep representations for multimodal emotion recognition",
      "authors": [
        "Ming Jin",
        "Jinpeng Li"
      ],
      "year": "2023",
      "venue": "Proceedings of the 31st ACM International Conference on Multimedia"
    },
    {
      "citation_id": "17",
      "title": "Multimodal physiological signal emotion recognition based on convolutional recurrent neural network,\" in IOP conference series: materials science and engineering",
      "authors": [
        "Jinxiang Liao",
        "Qinghua Zhong",
        "Yongsheng Zhu",
        "Dongli Cai"
      ],
      "year": "2020",
      "venue": "Multimodal physiological signal emotion recognition based on convolutional recurrent neural network,\" in IOP conference series: materials science and engineering"
    },
    {
      "citation_id": "18",
      "title": "Cross-modal credibility modelling for eeg-based multimodal emotion recognition",
      "authors": [
        "Yuzhe Zhang",
        "Huan Liu",
        "Di Wang",
        "Dalin Zhang",
        "Tianyu Lou",
        "Qinghua Zheng",
        "Chai Quek"
      ],
      "year": "2024",
      "venue": "Journal of Neural Engineering"
    },
    {
      "citation_id": "19",
      "title": "Research on multimodal emotion recognition based on fusion of electroencephalogram and electrooculography",
      "authors": [
        "Jialai Yin",
        "Minchao Wu",
        "Yan Yang",
        "Ping Li",
        "Fan Li",
        "Wen Liang",
        "Zhao Lv"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Instrumentation and Measurement"
    },
    {
      "citation_id": "20",
      "title": "Hetemotionnet: two-stream heterogeneous graph recurrent neural network for multi-modal emotion recognition",
      "authors": [
        "Ziyu Jia",
        "Youfang Lin",
        "Jing Wang",
        "Zhiyang Feng",
        "Xiangheng Xie",
        "Caijie Chen"
      ],
      "year": "2021",
      "venue": "Proceedings of the 29th ACM international conference on multimedia"
    },
    {
      "citation_id": "21",
      "title": "Differential entropy feature for eeg-based emotion classification",
      "authors": [
        "Jia-Yi Ruo-Nan Duan",
        "Bao-Liang Zhu",
        "Lu"
      ],
      "year": "2013",
      "venue": "2013 6th international IEEE/EMBS conference on neural engineering (NER)"
    },
    {
      "citation_id": "22",
      "title": "EEG signal processing",
      "authors": [
        "Saeid Sanei",
        "Jonathon Chambers"
      ],
      "year": "2013",
      "venue": "EEG signal processing"
    },
    {
      "citation_id": "23",
      "title": "Eegbased emotion recognition in listening music by using support vector machine and linear dynamic system",
      "authors": [
        "Xiao-Wei Ruo-Nan Duan",
        "Bao-Liang Wang",
        "Lu"
      ],
      "year": "2012",
      "venue": "International conference on neural information processing"
    },
    {
      "citation_id": "24",
      "title": "Combining eye movements and eeg to enhance emotion recognition",
      "authors": [
        "Yifei Lu",
        "Wei-Long Zheng",
        "Binbin Li",
        "Bao-Liang Lu"
      ],
      "year": "2015",
      "venue": "IJCAI. Buenos Aires"
    }
  ]
}