{
  "paper_id": "2508.01318v1",
  "title": "Affectgpt-R1: Leveraging Reinforcement Learning For Open-Vocabulary Emotion Recognition",
  "published": "2025-08-02T11:16:47Z",
  "authors": [
    "Zheng Lian"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Open-Vocabulary Multimodal Emotion Recognition (OV-MER) aims to predict emotions without being constrained by predefined label spaces, enabling finegrained and human-like emotion understanding. Unlike traditional discriminative methods, OV-MER leverages generative models, such as large language models (LLMs) with extensive vocabularies, to capture the full spectrum of emotions. Previous approaches (like AffectGPT) primarily rely on token-level loss for training. However, this objective does not align with the emotion wheel (EW)-based evaluation metrics used in OV-MER. Unfortunately, EW-based metrics cannot be directly optimized via gradient backpropagation. In this paper, we propose AffectGPT-R1, a reinforcement learning framework that directly optimizes performance on EW-based metrics. Specifically, we treat these metrics as the reward function and employ Group Relative Policy Optimization (GRPO) to maximize rewards. Experimental results demonstrate that AffectGPT-R1 achieves significant improvements on OV-MER. We hope this work advances the field of multimodal emotion recognition. Our code will be publicly available at: https://github.com/zeroQiaoba/AffectGPT.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Multimodal emotion recognition (MER) is a crucial research area in artificial intelligence, playing a vital role in advancing emotion-intelligent human-machine interaction  [1] . Recently, open-vocabulary MER (OV-MER) has emerged as a promising research direction, aiming to shift emotion recognition from constrained emotion categories to the full spectrum of emotions, thereby enabling finer-grained and more nuanced emotion understanding  [2] .\n\nTo accommodate this paradigm shift, OV-MER transitions from discriminative to generative models, leveraging the extensive vocabulary of large language models (LLMs) to expand the scope of emotion recognition. Simultaneously, it introduces emotion wheel (EW) based metrics to account for the semantic relationships between distinct emotion words during evaluation. In model optimization, previous works such as  AffectGPT [3]  employ token-level loss to align predicted and ground-truth labels. However, this approach suffers from a critical misalignment: token-level loss exhibits limited correlation with EW-based metrics. For instance, a model may achieve low token-level loss while performing poorly on EW-based metrics. Unfortunately, since EW-based metrics cannot be directly optimized via gradient backpropagation, this introduces significant challenges for model training.\n\nTo address these limitations, we propose AffectGPT-R1, which uses the EW-based metrics as the reward function and employs reinforcement learning for model optimization. This enables the model to be directly optimized on EW-based metrics. We emphasize that this work differs from R1-Omni  [4] , another RL-based MER framework. Specifically, R1-Omni focuses on basic emotion recognition, but AffectGPT-R1 shifts the task to OV-MER. Due to this task difference, we adopt different reward functions and adjust the primary objective from demonstrating the necessity of reasoning processes to highlighting the importance of direct optimization on EW-based metrics. Furthermore, our preliminary experiments reveal that the limited training data used in R1-Omni for cold-start training is insufficient. In contrast, AffectGPT-R1 incorporates a larger dataset for cold-start training. Additionally, R1-Omni only opensources its inference process, but we provide both training and inference code to facilitate reproducibility in future research. To the best of our knowledge, this is the first work to apply reinforcement learning to OV-MER.\n\n2 Method: AffectGPT-R1\n\nOur training pipeline comprises two key stages: cold start initialization and reinforcement learning. For the cold start phase, we leverage the large-scale emotion description dataset MER-Caption+  [3]  to establish initial emotion understanding capabilities. The reinforcement learning stage employs the MER2025-OV test set  [5]  for policy update. For a fair comparison, we evaluate all methods on the OV-MERD+ dataset  [3] , the same benchmark used in AffectGPT  [3] . Table  1  provides detailed statistical information about these datasets across different stages.\n\nTable  1 : Dataset statistics. This table summarizes the datasets used in each stage. Here, CS and RL denote cold start and reinforcement learning, respectively. Since 200 samples overlap between MER2025-OV and OV-MERD+, we exclude these overlapping samples during the RL phase. In OV-MERD+, the symbol ✔ ✗ indicates that only a subset of samples includes emotion descriptions.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Stage",
      "text": "Dataset #samples Emotion Description Emotion Lables",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Cold Start",
      "text": "We employ MER-Caption+  [3] , a large-scale emotion description dataset, for cold-start training.\n\nCompared to R1-Omni  [4] , we use a larger dataset, as our preliminary experiments demonstrate that a small dataset is insufficient for the model to fully grasp emotional nuances. The primary reason is that, unlike other tasks, human emotions are highly complex and rely on multimodal subtle cues, such as facial expressions, tone of voice, and body gestures  [6] . Current open-source models lack the capability to recognize all these cues and accurately interpret human emotions  [7] . Therefore, we use more datasets centered on human emotions and subtle cues for cold-start training. Figure  1  provides an example showing the input and output of the cold-start process. MER-Caption+ contains both emotion descriptions and associated emotion labels (see Table  1 ), which can be naturally converted to the <think></think><answer></answer> format in Figure  1 .\n\nUser Message: Please recognize all possible emotional states of the character. Output the thinking process in <think> </think> and final answer in <answer> </answer> tags.\n\nOutput: <think>In the video, the screen shows a male character in an indoor setting. At the beginning of the video, his eyes are wide open and his mouth is also open, indicating a surprised facial expression. In the following scenes, he looks around, seemingly explaining or narrating something to the people around him. Overall, his emotions are not positive or optimistic. In the audio, the character speaks with a stutter, which usually expresses feelings of nervousness, anxiety, or unease. Combined with the text content, the character seems to be unhappy and angry due to the prejudice of the people around him. The subtitle in the text says, \"Why are you all looking at me like that? So, as long as it's a woman, does she have to have a relationship with me?\" This sentence expresses the male character's dissatisfaction and anger towards the people around him. Based on the surprised and negative facial expression of the male character in the video clues, as well as the stuttering speech in the audio clues, we can infer that the male character is expressing a feeling of dissatisfaction and anger in this sentence. He may feel troubled by the prejudice of the people around him and is unhappy with this unfair treatment.</think><answer>surprise, nervous, dissatisfied</answer> During cold-start training, we adopt the same architecture as AffectGPT  [3] , as this framework effectively integrates multimodal information and demonstrates superior performance in emotion understanding. Compared to the original AffectGPT, our key modification is the output format, which we redesign as <think></think><answer></answer>, as shown in Figure  2 .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Reinforcement Learning",
      "text": "We first define the reward functions and then apply GRPO  [8]  to maximize rewards. For this phase, we utilize the MER2025-OV dataset, which includes emotion labels but does not provide emotion descriptions (see Table  1 ). Since our reward model focuses on evaluating response format and label accuracy, rather than the correctness of emotion descriptions, this dataset is well-suited for this phase.\n\nReward Function During training, we employ two distinct reward functions: format reward to ensure proper output structure and accuracy reward to evaluate prediction correctness. Specifically, to ensure structured outputs, we define a binary format reward that evaluates whether the model's response adheres to the required <think></think><answer></answer> format (see Figure  1 ): R format (o|v, q) = 1, o strictly follows the required format 0, otherwise\n\nwhere v represents the video containing multimodal data (visual, acoustic, and lexical information), q denotes the user message, and o is the model's output. For the accuracy reward, we employ EW-based metrics for score calculation, following the official evaluation protocol in OV-MER  [2] :\n\nwhere a is the answer extracted from the model's output (i.e., the content between the <answer> and </answer> tags), and y represents the ground-truth open-vocabulary labels. The final reward score is a weighted combination of these two rewards, controlled by the trade-off hyperparameter β: R(o, y|v, q) = R accuracy (o, y|v, q) + βR format (o|v, q).\n\nGroup Relative Policy Optimization In this section, we employ GRPO  [8]  to maximize rewards. Unlike PPO  [9]  that relies on a critic model, GRPO eliminates this component and instead uses group scores for advantage estimation, significantly reducing memory overhead while improving training efficiency. Specifically, for each q, we sample a group of outputs from the policy model:\n\nNext, we calculate their rewards using the above reward functions: r i = R(o i , y|v, q), i ∈ [1, G]. The advantage score A i is then calculated by group normalization:\n\nDuring training, we introduce a KL loss between the output logits of the reference model and the policy model. The final loss function is defined as follows:\n\nwhere π θ is the policy model, π ref is the reference model, and ϵ controls the clipping threshold. This formula is derived from DeepSeekMath  [8] .",
      "page_start": 3,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: User Message: Please recognize all possible emotional states of the character. Output the thinking process in <think> </think> and final answer",
      "page": 2
    },
    {
      "caption": "Figure 1: Input and output of cold-start training.",
      "page": 2
    },
    {
      "caption": "Figure 2: Cold-start training framework.",
      "page": 3
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Abstract": "Open-Vocabulary Multimodal Emotion Recognition (OV-MER) aims to predict"
        },
        {
          "Abstract": "emotions without being constrained by predefined label spaces, enabling fine-"
        },
        {
          "Abstract": "grained and human-like emotion understanding. Unlike traditional discriminative"
        },
        {
          "Abstract": "methods, OV-MER leverages generative models, such as large language models"
        },
        {
          "Abstract": "(LLMs) with extensive vocabularies,\nto capture the full spectrum of emotions."
        },
        {
          "Abstract": "Previous approaches (like AffectGPT) primarily rely on token-level loss for train-"
        },
        {
          "Abstract": "ing. However, this objective does not align with the emotion wheel (EW)-based"
        },
        {
          "Abstract": "evaluation metrics used in OV-MER. Unfortunately, EW-based metrics cannot"
        },
        {
          "Abstract": "be directly optimized via gradient backpropagation.\nIn this paper, we propose"
        },
        {
          "Abstract": "AffectGPT-R1, a reinforcement learning framework that directly optimizes per-"
        },
        {
          "Abstract": "formance on EW-based metrics.\nSpecifically, we treat\nthese metrics as the re-"
        },
        {
          "Abstract": "ward function and employ Group Relative Policy Optimization (GRPO) to max-"
        },
        {
          "Abstract": "imize rewards. Experimental\nresults demonstrate that AffectGPT-R1 achieves"
        },
        {
          "Abstract": "significant\nimprovements on OV-MER. We hope this work advances the field"
        },
        {
          "Abstract": "of multimodal emotion recognition.\nOur code will be publicly available at:"
        },
        {
          "Abstract": "https://github.com/zeroQiaoba/AffectGPT."
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "https://github.com/zeroQiaoba/AffectGPT.": "1\nIntroduction"
        },
        {
          "https://github.com/zeroQiaoba/AffectGPT.": "Multimodal emotion recognition (MER) is a crucial research area in artificial intelligence, playing a"
        },
        {
          "https://github.com/zeroQiaoba/AffectGPT.": "vital role in advancing emotion-intelligent human-machine interaction [1]. Recently, open-vocabulary"
        },
        {
          "https://github.com/zeroQiaoba/AffectGPT.": "MER (OV-MER) has emerged as a promising research direction, aiming to shift emotion recognition"
        },
        {
          "https://github.com/zeroQiaoba/AffectGPT.": "from constrained emotion categories to the full spectrum of emotions, thereby enabling finer-grained"
        },
        {
          "https://github.com/zeroQiaoba/AffectGPT.": "and more nuanced emotion understanding [2]."
        },
        {
          "https://github.com/zeroQiaoba/AffectGPT.": "To accommodate this paradigm shift, OV-MER transitions from discriminative to generative models,"
        },
        {
          "https://github.com/zeroQiaoba/AffectGPT.": "leveraging the extensive vocabulary of large language models (LLMs) to expand the scope of emotion"
        },
        {
          "https://github.com/zeroQiaoba/AffectGPT.": "recognition. Simultaneously,\nit\nintroduces emotion wheel (EW) based metrics to account for the"
        },
        {
          "https://github.com/zeroQiaoba/AffectGPT.": "semantic relationships between distinct emotion words during evaluation.\nIn model optimization,"
        },
        {
          "https://github.com/zeroQiaoba/AffectGPT.": "previous works such as AffectGPT [3] employ token-level loss to align predicted and ground-truth"
        },
        {
          "https://github.com/zeroQiaoba/AffectGPT.": "labels. However, this approach suffers from a critical misalignment:\ntoken-level loss exhibits limited"
        },
        {
          "https://github.com/zeroQiaoba/AffectGPT.": "correlation with EW-based metrics. For instance, a model may achieve low token-level loss while"
        },
        {
          "https://github.com/zeroQiaoba/AffectGPT.": "performing poorly on EW-based metrics. Unfortunately, since EW-based metrics cannot be directly"
        },
        {
          "https://github.com/zeroQiaoba/AffectGPT.": "optimized via gradient backpropagation, this introduces significant challenges for model training."
        },
        {
          "https://github.com/zeroQiaoba/AffectGPT.": "To address these limitations, we propose AffectGPT-R1, which uses the EW-based metrics as the"
        },
        {
          "https://github.com/zeroQiaoba/AffectGPT.": "reward function and employs reinforcement\nlearning for model optimization.\nThis enables the"
        },
        {
          "https://github.com/zeroQiaoba/AffectGPT.": "model\nto be directly optimized on EW-based metrics. We emphasize that\nthis work differs from"
        },
        {
          "https://github.com/zeroQiaoba/AffectGPT.": "R1-Omni [4], another RL-based MER framework. Specifically, R1-Omni focuses on basic emotion"
        },
        {
          "https://github.com/zeroQiaoba/AffectGPT.": "recognition, but AffectGPT-R1 shifts the task to OV-MER. Due to this task difference, we adopt"
        },
        {
          "https://github.com/zeroQiaoba/AffectGPT.": "different reward functions and adjust\nthe primary objective from demonstrating the necessity of"
        },
        {
          "https://github.com/zeroQiaoba/AffectGPT.": "Preprint. Under review."
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "reasoning processes to highlighting the importance of direct optimization on EW-based metrics.": "Furthermore, our preliminary experiments reveal\nthat\nthe limited training data used in R1-Omni"
        },
        {
          "reasoning processes to highlighting the importance of direct optimization on EW-based metrics.": "for cold-start\ntraining is insufficient.\nIn contrast, AffectGPT-R1 incorporates a larger dataset for"
        },
        {
          "reasoning processes to highlighting the importance of direct optimization on EW-based metrics.": "cold-start training. Additionally, R1-Omni only opensources its inference process, but we provide"
        },
        {
          "reasoning processes to highlighting the importance of direct optimization on EW-based metrics.": "both training and inference code to facilitate reproducibility in future research. To the best of our"
        },
        {
          "reasoning processes to highlighting the importance of direct optimization on EW-based metrics.": "knowledge, this is the first work to apply reinforcement learning to OV-MER."
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "User Message: Please recognize all possible emotional states of the character. Output the thinking process in <think> </think> and final answer": "in <answer> </answer> tags."
        },
        {
          "User Message: Please recognize all possible emotional states of the character. Output the thinking process in <think> </think> and final answer": "Output: <think>In the video, the screen shows a male character in an indoor setting. At the beginning of the video, his eyes are wide open and"
        },
        {
          "User Message: Please recognize all possible emotional states of the character. Output the thinking process in <think> </think> and final answer": "his mouth is also open,\nindicating a surprised facial expression."
        },
        {
          "User Message: Please recognize all possible emotional states of the character. Output the thinking process in <think> </think> and final answer": "something to the people around him. Overall, his emotions are not positive or optimistic. In the audio, the character speaks with a stutter, which"
        },
        {
          "User Message: Please recognize all possible emotional states of the character. Output the thinking process in <think> </think> and final answer": "usually expresses feelings of nervousness, anxiety, or unease. Combined with the text content, the character seems to be unhappy and angry due"
        },
        {
          "User Message: Please recognize all possible emotional states of the character. Output the thinking process in <think> </think> and final answer": "to the prejudice of the people around him. The subtitle in the text says, \"Why are you all"
        },
        {
          "User Message: Please recognize all possible emotional states of the character. Output the thinking process in <think> </think> and final answer": "does she have to have a relationship with me?\" This sentence expresses the male character's dissatisfaction and anger towards the people around"
        },
        {
          "User Message: Please recognize all possible emotional states of the character. Output the thinking process in <think> </think> and final answer": "him. Based on the surprised and negative facial expression of the male character in the video clues, as well as the stuttering speech in the audio"
        },
        {
          "User Message: Please recognize all possible emotional states of the character. Output the thinking process in <think> </think> and final answer": "clues, we can infer\nthat\nthe male character"
        },
        {
          "User Message: Please recognize all possible emotional states of the character. Output the thinking process in <think> </think> and final answer": "prejudice of the people around him and is unhappy with this unfair treatment.</think><answer>surprise, nervous, dissatisfied</answer>"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Audio\nVideo\nPrompt": "Figure 2: Cold-start training framework."
        },
        {
          "Audio\nVideo\nPrompt": "2.2\nReinforcement Learning"
        },
        {
          "Audio\nVideo\nPrompt": "We first define the reward functions and then apply GRPO [8] to maximize rewards. For this phase,"
        },
        {
          "Audio\nVideo\nPrompt": "we utilize the MER2025-OV dataset, which includes emotion labels but does not provide emotion"
        },
        {
          "Audio\nVideo\nPrompt": "descriptions (see Table 1). Since our reward model focuses on evaluating response format and label"
        },
        {
          "Audio\nVideo\nPrompt": "accuracy, rather than the correctness of emotion descriptions, this dataset is well-suited for this phase."
        },
        {
          "Audio\nVideo\nPrompt": "Reward Function\nDuring training, we employ two distinct reward functions: format reward to"
        },
        {
          "Audio\nVideo\nPrompt": "ensure proper output structure and accuracy reward to evaluate prediction correctness. Specifically,"
        },
        {
          "Audio\nVideo\nPrompt": "to ensure structured outputs, we define a binary format reward that evaluates whether the model’s"
        },
        {
          "Audio\nVideo\nPrompt": "response adheres to the required <think></think><answer></answer> format (see Figure 1):"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table 2: , AffectGPT-R1 achieves above 3% performance",
      "data": [
        {
          "3\nExperiments": "In this section, we report the performance of different models on OV-MERD+, the same benchmark"
        },
        {
          "3\nExperiments": "used in AffectGPT [3]. As shown in Table 2, AffectGPT-R1 achieves above 3% performance"
        },
        {
          "3\nExperiments": "improvement over AffectGPT, demonstrating the advantages of our reinforcement learning approach."
        },
        {
          "3\nExperiments": "Further analysis indicates that while cold-start\ntraining enables the model\nto generate outputs in"
        },
        {
          "3\nExperiments": "the target <think></think><answer></answer> format,\nit does not consistently produce optimal"
        },
        {
          "3\nExperiments": "responses. By applying reinforcement\nlearning, we significantly enhance the model’s sampling"
        },
        {
          "3\nExperiments": "quality, leading to remarkable performance gains on OV-MER."
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 2: , AffectGPT-R1 achieves above 3% performance",
      "data": [
        {
          "References": "[1] Rosalind W Picard. Affective computing. MIT press, 2000."
        },
        {
          "References": "[2] Zheng Lian, Haiyang Sun, Licai Sun, Haoyu Chen, Lan Chen, Hao Gu, Zhuofan Wen, Shun"
        },
        {
          "References": ""
        },
        {
          "References": ""
        },
        {
          "References": "[3] Zheng Lian, Haoyu Chen, Lan Chen, Haiyang Sun, Licai Sun, Yong Ren, Zebang Cheng, Bin"
        },
        {
          "References": ""
        },
        {
          "References": ""
        },
        {
          "References": ""
        },
        {
          "References": "[4]"
        },
        {
          "References": ""
        },
        {
          "References": "[5] Zheng Lian, Rui Liu, Kele Xu, Bin Liu, Xuefei Liu, Yazhou Zhang, Xin Liu, Yong Li, Zebang"
        },
        {
          "References": ""
        },
        {
          "References": ""
        },
        {
          "References": "[6]"
        },
        {
          "References": ""
        },
        {
          "References": "[7] Zheng Lian, Licai Sun, Haiyang Sun, Kang Chen, Zhuofan Wen, Hao Gu, Bin Liu, and Jian-"
        },
        {
          "References": ""
        },
        {
          "References": ""
        },
        {
          "References": "[8] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang,"
        },
        {
          "References": ""
        },
        {
          "References": ""
        },
        {
          "References": "[9]"
        },
        {
          "References": ""
        }
      ],
      "page": 4
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Affective computing",
      "authors": [
        "Rosalind Picard"
      ],
      "year": "2000",
      "venue": "Affective computing"
    },
    {
      "citation_id": "2",
      "title": "Ov-mer: Towards open-vocabulary multimodal emotion recognition",
      "authors": [
        "Zheng Lian",
        "Haiyang Sun",
        "Licai Sun",
        "Haoyu Chen",
        "Lan Chen",
        "Hao Gu",
        "Zhuofan Wen",
        "Shun Chen",
        "Zhang Siyuan",
        "Hailiang Yao"
      ],
      "year": "2025",
      "venue": "Forty-second International Conference on Machine Learning"
    },
    {
      "citation_id": "3",
      "title": "Affectgpt: A new dataset, model, and benchmark for emotion understanding with multimodal large language models",
      "authors": [
        "Zheng Lian",
        "Haoyu Chen",
        "Lan Chen",
        "Haiyang Sun",
        "Licai Sun",
        "Yong Ren",
        "Zebang Cheng",
        "Bin Liu",
        "Rui Liu",
        "Xiaojiang Peng"
      ],
      "year": "2025",
      "venue": "Forty-second International Conference on Machine Learning"
    },
    {
      "citation_id": "4",
      "title": "R1-omni: Explainable omni-multimodal emotion recognition with reinforcement learning",
      "authors": [
        "Jiaxing Zhao",
        "Xihan Wei",
        "Liefeng Bo"
      ],
      "year": "2025",
      "venue": "R1-omni: Explainable omni-multimodal emotion recognition with reinforcement learning",
      "arxiv": "arXiv:2503.05379"
    },
    {
      "citation_id": "5",
      "title": "When affective computing meets large language models",
      "authors": [
        "Zheng Lian",
        "Rui Liu",
        "Kele Xu",
        "Bin Liu",
        "Xuefei Liu",
        "Yazhou Zhang",
        "Xin Liu",
        "Yong Li",
        "Zebang Cheng",
        "Haolin Zuo"
      ],
      "year": "2025",
      "venue": "When affective computing meets large language models",
      "arxiv": "arXiv:2504.19423"
    },
    {
      "citation_id": "6",
      "title": "Emotion. Annual Review of Psychology",
      "authors": [
        "T John",
        "Wendi Cacioppo",
        "Gardner"
      ],
      "year": "1999",
      "venue": "Emotion. Annual Review of Psychology"
    },
    {
      "citation_id": "7",
      "title": "Gpt-4v with emotion: A zero-shot benchmark for generalized emotion recognition",
      "authors": [
        "Zheng Lian",
        "Licai Sun",
        "Haiyang Sun",
        "Kang Chen",
        "Zhuofan Wen",
        "Hao Gu",
        "Bin Liu",
        "Jianhua Tao"
      ],
      "year": "2024",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "8",
      "title": "Pushing the limits of mathematical reasoning in open language models",
      "authors": [
        "Zhihong Shao",
        "Peiyi Wang",
        "Qihao Zhu",
        "Runxin Xu",
        "Junxiao Song",
        "Xiao Bi",
        "Haowei Zhang",
        "Mingchuan Zhang",
        "Y Li",
        "Yang Wu"
      ],
      "year": "2024",
      "venue": "Pushing the limits of mathematical reasoning in open language models",
      "arxiv": "arXiv:2402.03300"
    },
    {
      "citation_id": "9",
      "title": "Proximal policy optimization algorithms",
      "authors": [
        "John Schulman",
        "Filip Wolski",
        "Prafulla Dhariwal",
        "Alec Radford",
        "Oleg Klimov"
      ],
      "year": "2017",
      "venue": "Proximal policy optimization algorithms",
      "arxiv": "arXiv:1707.06347"
    }
  ]
}