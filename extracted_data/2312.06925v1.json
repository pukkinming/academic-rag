{
  "paper_id": "2312.06925v1",
  "title": "Facial Emotion Recognition In Vr Games",
  "published": "2023-12-12T01:40:14Z",
  "authors": [
    "Fatemeh Dehghani",
    "Loutfouz Zaman"
  ],
  "keywords": [
    "Players",
    "Emotions",
    "Virtual Reality",
    "Games",
    "Facial Expressions"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Emotion detection is a crucial component of Games User Research (GUR), as it allows game developers to gain insights into players' emotional experiences and tailor their games accordingly. However, detecting emotions in Virtual Reality (VR) games is challenging due to the Head-Mounted Display (HMD) that covers the top part of the player's face, namely, their eyes and eyebrows, which provide crucial information for recognizing the impression. To tackle this we used a Convolutional Neural Network (CNN) to train a model to predict emotions in full-face images where the eyes and eyebrows are covered. We used the FER2013 dataset, which we modified to cover eyes and eyebrows in images. The model in these images can accurately recognize seven different emotions which are anger, happiness, disgust, fear, impartiality, sadness and surprise. We assessed the model's performance by testing it on two VR games and using it to detect players' emotions. We collected self-reported emotion data from the players after the gameplay sessions. We analyzed the data collected from our experiment to understand which emotions players experience during the gameplay. We found that our approach has the potential to enhance gameplay analysis by enabling the detection of players' emotions in VR games, which can help game developers create more engaging and immersive game experiences.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "Virtual Reality (VR) refers to simulated environments in which physical properties of the real world can be emulated. VR is used in diverse domains, such as medicine, social sciences, psychology, and entertainment. One of the most used applications of VR is in video games, which is one of the world's most popular and prevailing entertainment tools, offering game players an innovative gaming experience.\n\nGame analytics involves utilizing analytics for game development and research purposes  [1] . In game analytics data mining methods are used to extract insights from game-related data and for pattern detection, with a particular focus on player behavior  [2] . The process of examining data goes through data preparation, modeling, and evaluation, which helps game developers and game researchers to improve games to the extent that game players catch the experience that designers planned for  [3] . To achieve this, game developers have designed many evaluation methods to identify problems that a player might face while playing and then resolve these problems using player data. The level of player engagement is considered a primary key in evaluating the success of games. In order to achieve this goal, game developers look for toolkits and methods to understand why players give up on the specific level of computer games and never return to finish them  [4] . One of the ways to figure out the reason for this is analyzing players' impressions. It has been observed that players show different reactions, and experience both negative and positive emotions (e.g., sadness, fear, excitement, bored) while facing challenges  [5] . Understanding players' emotions is very crucial for game developers as it helps them to\n\n• understand the level of player engagement,\n\n• ensure positive emotional states,\n\n• introduce the right level of dynamism to the game  [6] . If the player's expression suggests stress, the pertinent game difficulty factors may be lowered until the player is at ease  [7] .",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Ii. Related Work",
      "text": "Burns and Tulip  [6]  developed a method to measure gamer involvement, specifically \"flow,\" using facial expressions. They recorded players' facial expressions while gaming with a webcam and used machine learning to identify features associated with flow, achieving 78% accuracy on their dataset. Liu et al.  [8]  conducted research on the use of real-time anxiety-based feedback to implement Dynamic Difficulty Adjustment (DDA) in games. They proposed tracking players' physiological reactions, particularly their skin conductance levels, to measure fear and adjust the game difficulty accordingly to maintain a state of flow. A study with 15 individuals playing a modified version of Tetris found that DDA improved player engagement and positive affective experiences compared to playing without DDA. Khezri et al.  [9]  proposed a method to improve emotion recognition by combining multiple emotional modalities. They used recorded signals and multiple classification units to independently identify emotions, resulting in a significant performance boost for their system. Yang et al.  [10]  introduced a method to classify emotions according to skin conductance and electroencephalography signals.\n\nThe classification was performed on images. The method establishes a connection between experiential information 979-8-3503-2277-4/23/$31.00 ©2023 IEEE and the viewer's expected emotion experience, resulting in a positive experience. Jang et al.  [11]  checked the accuracy of using changes in physiological signals in response to emotions. Six emotions were measured. Jang et al. found skin conductance, blood volume signals and heart rate more dependable compared to baseline evaluations. Ouellet  [12]  proposed a new technique for recognizing emotions in real-time gaming using a Convolutional Neural Network (CNN) to analyze facial expressions of players. Players' faces were detected with the Viola-Jones detector in OpenCV from a continuous video stream captured from the webcam. The proposed method was compared to other emotion recognition methods and showed better accuracy and speed, with a 94.4% accuracy rate. Hickson et al.  [13]  described a unique method for identifying and categorizing facial expressions related to five different emotions by utilizing eye movement and gaze attributes. Long Short-Term Memory (LSTM) networks are combined with CNNs to achieve 74% accuracy in recognizing face expressions and 70% accuracy in detecting facial action units. Suzuki et al.  [14]  developed a technique to use integrated photo reflecting sensors in an HMD to identify and map face emotions to avatars. Their method recognized seven fundamental facial emotions with an accuracy of 82% and mapped these expressions to avatars with an accuracy of 86%.\n\nWe avoid the complexity, expenses, and possible constraints associated with eye-tracking and sensor gear by concentrating entirely on facial emotions collected through full-face photos. Our technique provides generalizability across multiple VR systems and gaming conditions, allowing us to accommodate instances where eye-tracking may not be provided or maintained continuously by players.\n\nCNNs demonstrated success in image processing since their inception in the late 1990s  [15] . The architecture of a CNN makes it particularly effective for handling static images  [16] . In recent years, larger datasets and increased processing power have made CNNs a more practical approach for feature extraction and image categorization  [17] .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Iii. Proposed Method",
      "text": "In this work, we aim to assess the emotions of players engaged in VR gaming while wearing HMDs. These displays tend to occlude the upper facial regions, such as the eyes and eyebrows, known for providing crucial cues in emotion recognition. We first created our required dataset: face images with covered eyes and eyebrows. Next, we trained the existing facial emotion recognition model on the original dataset, FER, and then on our dataset, to see if the model can estimate human emotions based on the mouth and chin. We then performed an evaluation. Next, in order to see the real application of this work, we asked five participants to play two VR games, and recorded their faces while wearing the VR headset. We also recorded their gameplay synchronously. After each game, participants filled out a form where they were asked which emotions they experienced while playing the games. We then converted the videos to images and fed them to the model. We compared the feelings declared by participants in each game and the emotions predicted by the model to see how accurately the model classifies emotions.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "A. Vggnet",
      "text": "VGGNet is an architecture of convolutional neural networks used in pattern recognition and image processing  [18] . Four convolutional blocks are responsible for extracting the features of high level, and the task of categorizing emotions is performed by the fully connected layers  [19] .",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "B. Dataset",
      "text": "The FER2013 is a publicly accessible dataset of facial emotion recognition (FER), which contains a vast collection of 35,887 face crops. The dataset has train, test and validation sets. For each sample, simple expression labels are offered. Each image has a dimension of 48 × 48 pixels and is grayscale. FER2013 provides seven emotional expressions: happy, fearful, disgusted, sad, angry, neutral, and surprised. Moreover, we recorded ten 20-minute long videos, two per participant (one for each game), while participants were playing, then converted them to images and made annotations to make them ready to feed the model. For this purpose, we got the pixels of images and saved them as a CSV file, and then fed them to the model, which was trained on the covered eyes dataset (Fig.  1 ), to see which emotions the model can accurately detect.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Iv. Training And Testing Of The Model",
      "text": "To create our desired dataset containing a covered-eye human face, we used the OpenCV library to get the coordinates of eyes and eyebrows in the images. Then, we rendered a black rectangle that covered these areas. We applied this function on 35,000 images. Eyes and eyebrows were well covered in the images. Then, since the model's checkpoint was unavailable, we first trained the model on the original dataset over 300 epochs. Next, we trained the model on our desired dataset. The model used in this work achieved a 73.28% accuracy of a single-network on FER2013. Additional training data was not utilized. When tested on the covered eyes dataset, the model's accuracy was 68.59%.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "V. Real World Validation",
      "text": "While our model showed good results in theory, we also wanted to put it into practice and see if it is actually usable as a tool for emotion detection in Games User Research (GUR). For this validation, we designed a small user study where we followed a traditional Human-Computer Interaction (HCI) procedure described below. For this study, we picked two PSVR games: Astro Bot Rescue Mission and Paranormal Activity. The reason they were chosen is because we hypothesized that together these two games would be able to capture most emotions our model is capable of detecting. Namely, we hypothesized that playing Astro Bot Rescue Mission would result in detecting positive emotions, while Paranormal Activity would result in negative ones.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "A. Participants",
      "text": "We recruited five participants (three males and two females) to play two games: Astro Bot Rescue Mission and Paranormal Activity. The participants were between the ages of 21 and 27 (Mdn = 24) and were recruited from the graduate student population in computer science at our university. We gathered participants' demographic information through a Google form. Participants expressed whether they played VR games before. While three had played VR games before, two participants did not. Also, they were asked to rate their expertise as video game players. One considered themselves a casual player, three consider themselves core players, and one was a hardcore player. Moreover, they were asked about their familiarity with the games. Two were new to Astro Bot Rescue Mission, and three were new to Paranormal Activity.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "B. Apparatus",
      "text": "We used two camera devices to record players' faces, one of which was Apple iPhone 14 Pro Max and the other Logitech BCC950 ConferenceCam. The gameplay was captured using 4K Ultra HD HDMI Capture card. The ConferenceCam footage was synced with the output of the capture card and recorded using Open Broadcaster Software (OBS) during the gameplay for later analysis. Both cameras were placed at about waist height to the participants and facing upwards to capture the facial expression under the VR helmet. A PC was used to record the output from the ConferenceCam and the capture card. The iPhone footage was used for feeding into the model because it produced video quality that was more acceptable for this purpose, unlike that of the Logitech camera. Both games were hosted on Sony PlayStation 4 with PSVR. The players used PlayStation 4 controllers for playing the games. Fig  2  shows the environment and equipment we used for collecting the data.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "C. Procedure",
      "text": "After filling out the demographic questionnaire, we instructed the participants to play one of the two games first. We counterbalanced the order in which the games were presented to the participants. The participants played the game from the beginning. In Astro Bot Rescue Mission participants started from World 1-1 and played until the 20 minute time limit expired. In Paranormal Activity participants played the game from the very beginning and had to perform all the steps to enter the house and pass through the first door. When stuck they received verbal instructions from the 1st author of this work. After the participants played the game for 20 minutes, they were requested to complete an in-game survey, where on a 5-point Likert scale they rated how much anger, happiness, disgust, fear, neutrality, happiness, sadness, and surprise they experienced, with 1 being not experiencing the corresponding emotion at all, and 5 being experiencing it very much. The procedure was then repeated for the other game. The whole study took about 1 hour.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Vi. Results",
      "text": "In total, we recorded 5 videos for each game (10 in total). We converted the videos to images. We then obtained their pixels and saved them in two separate CSV files to detect experienced emotions in each game. We put an empty column in the files for the model to predict corresponding labels, indicating detected emotions in images. After that, we compared the detected feelings by the model and the declared emotions by each player. Furthermore, we reviewed the recorded videos alongside the forms filled out by participants after each game for validation.\n\nIn terms of emotion detection, participants reported experiencing three emotions: happy, neutral, and surprise, while playing Astro Bot Rescue Mission. The model successfully detected happy and neutral emotions during this game. When playing Paranormal Activity, participants reported experiencing four emotions: anger, fear, surprise, and neutral, and the model was able to detect fear, neutral, and surprise emotions accurately. In Astro Bot Rescue Mission, participants felt happy mostly when all rescued bots appeared and danced before the end of each level or sometimes when the bot waved its hands. In Paranormal Activity, most participants felt fear 2-3 times: when they walked by an opened window and it suddenly closed, when they went upstairs, and the girl (NPC character) suddenly came downstairs, when they saw the lady with an axe in her hand on the second floor in the house, which was the scariest scene for all of them. Some of them were surprised when they saw the TV screen showing the girl (NPC character) scared of seeing something or when objects moved. The neutral feeling was the most prevailing emotion in both games. Overall, our experimental findings indicate that the CNN can accurately categorize emotions using solely the facial features extracted from areas of the face other than the eyes and eyebrows, even when the facial information in those regions is absent. This suggests that the model can effectively detect emotions in different game scenarios. Some of the pictures from the video footage show participants' faces, the game scene, and the detected emotions. See Fig.  3 .",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Vii. Limitations And Future Work",
      "text": "The obvious limitation of the current work is that we did not capture players' expressions when they moved their heads or turned around while playing. The model could only detect emotions when players' faces were facing the camera straight. This limitation can be addressed by attaching, e.g., a selfie stick to the VR helmet, so that the camera is always facing the player's face as they turn their head. Alternatively, the model can be fine-tuned in the future to detect emotions from multiple views. One aspect for further consideration is ensuring precise synchronization between the recorded emotions and the specific times they occurred, in order to enhance confidence in the accuracy and validation of our model. Furthermore, an emotion-based difficulty adjustment solution for VR games can be developed based on this work, which potentially could improve player experience.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: ), to see which emotions the model can accurately detect.",
      "page": 2
    },
    {
      "caption": "Figure 1: Samples of all the expressions with eyes and eyebrows blacked",
      "page": 2
    },
    {
      "caption": "Figure 2: shows the environment and equipment we used for collecting",
      "page": 3
    },
    {
      "caption": "Figure 2: Environment and equipment used for collecting the data.",
      "page": 3
    },
    {
      "caption": "Figure 3: Fig. 3: Some of the pictures from the video footage show participants’",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Fatemeh.Dehghani@ontariotechu.ca": "Abstract—Emotion detection is a crucial component of Games",
          "loutfouz.zaman@ontariotechu.ca": "have designed many evaluation methods to identify problems"
        },
        {
          "Fatemeh.Dehghani@ontariotechu.ca": "User Research\n(GUR),\nas\nit\nallows\ngame\ndevelopers\nto\ngain",
          "loutfouz.zaman@ontariotechu.ca": ""
        },
        {
          "Fatemeh.Dehghani@ontariotechu.ca": "",
          "loutfouz.zaman@ontariotechu.ca": "that a player might\nface while playing and then resolve these"
        },
        {
          "Fatemeh.Dehghani@ontariotechu.ca": "insights\ninto\nplayers’\nemotional\nexperiences\nand\ntailor\ntheir",
          "loutfouz.zaman@ontariotechu.ca": ""
        },
        {
          "Fatemeh.Dehghani@ontariotechu.ca": "",
          "loutfouz.zaman@ontariotechu.ca": "problems using player data. The level of player engagement\nis"
        },
        {
          "Fatemeh.Dehghani@ontariotechu.ca": "games\naccordingly. However,\ndetecting\nemotions\nin\nVirtual",
          "loutfouz.zaman@ontariotechu.ca": ""
        },
        {
          "Fatemeh.Dehghani@ontariotechu.ca": "",
          "loutfouz.zaman@ontariotechu.ca": "considered a primary key in evaluating the success of games."
        },
        {
          "Fatemeh.Dehghani@ontariotechu.ca": "Reality\n(VR)\ngames\nis\nchallenging due\nto\nthe Head-Mounted",
          "loutfouz.zaman@ontariotechu.ca": ""
        },
        {
          "Fatemeh.Dehghani@ontariotechu.ca": "Display\n(HMD)\nthat\ncovers\nthe\ntop\npart\nof\nthe\nplayer’s",
          "loutfouz.zaman@ontariotechu.ca": "In order to achieve this goal, game developers look for toolkits"
        },
        {
          "Fatemeh.Dehghani@ontariotechu.ca": "face, namely,\ntheir\neyes\nand eyebrows, which provide\ncrucial",
          "loutfouz.zaman@ontariotechu.ca": "and methods to understand why players give up on the specific"
        },
        {
          "Fatemeh.Dehghani@ontariotechu.ca": "information for\nrecognizing\nthe\nimpression. To\ntackle\nthis we",
          "loutfouz.zaman@ontariotechu.ca": ""
        },
        {
          "Fatemeh.Dehghani@ontariotechu.ca": "",
          "loutfouz.zaman@ontariotechu.ca": "level of computer games and never\nreturn to finish them [4]."
        },
        {
          "Fatemeh.Dehghani@ontariotechu.ca": "used a Convolutional Neural Network (CNN) to train a model to",
          "loutfouz.zaman@ontariotechu.ca": ""
        },
        {
          "Fatemeh.Dehghani@ontariotechu.ca": "",
          "loutfouz.zaman@ontariotechu.ca": "One of\nthe ways to figure out\nthe reason for\nthis is analyzing"
        },
        {
          "Fatemeh.Dehghani@ontariotechu.ca": "predict emotions in full-face images where the eyes and eyebrows",
          "loutfouz.zaman@ontariotechu.ca": ""
        },
        {
          "Fatemeh.Dehghani@ontariotechu.ca": "",
          "loutfouz.zaman@ontariotechu.ca": "players’\nimpressions.\nIt has been observed that players show"
        },
        {
          "Fatemeh.Dehghani@ontariotechu.ca": "are covered. We used the FER2013 dataset, which we modified",
          "loutfouz.zaman@ontariotechu.ca": ""
        },
        {
          "Fatemeh.Dehghani@ontariotechu.ca": "to cover eyes and eyebrows in images. The model\nin these images",
          "loutfouz.zaman@ontariotechu.ca": "different\nreactions, and experience both negative and positive"
        },
        {
          "Fatemeh.Dehghani@ontariotechu.ca": "can\naccurately\nrecognize\nseven\ndifferent\nemotions which\nare",
          "loutfouz.zaman@ontariotechu.ca": "emotions (e.g., sadness,\nfear, excitement, bored) while facing"
        },
        {
          "Fatemeh.Dehghani@ontariotechu.ca": "anger, happiness, disgust, fear, impartiality, sadness and surprise.",
          "loutfouz.zaman@ontariotechu.ca": ""
        },
        {
          "Fatemeh.Dehghani@ontariotechu.ca": "",
          "loutfouz.zaman@ontariotechu.ca": "challenges [5]. Understanding players’ emotions is very crucial"
        },
        {
          "Fatemeh.Dehghani@ontariotechu.ca": "",
          "loutfouz.zaman@ontariotechu.ca": "for game developers as it helps them to"
        },
        {
          "Fatemeh.Dehghani@ontariotechu.ca": "We\nassessed the model’s performance by\ntesting\nit\non two",
          "loutfouz.zaman@ontariotechu.ca": ""
        },
        {
          "Fatemeh.Dehghani@ontariotechu.ca": "",
          "loutfouz.zaman@ontariotechu.ca": "•\nunderstand the level of player engagement,"
        },
        {
          "Fatemeh.Dehghani@ontariotechu.ca": "VR games and using it\nto detect players’ emotions. We collected",
          "loutfouz.zaman@ontariotechu.ca": ""
        },
        {
          "Fatemeh.Dehghani@ontariotechu.ca": "self-reported emotion data from the players after the gameplay",
          "loutfouz.zaman@ontariotechu.ca": "•\nensure positive emotional states,"
        },
        {
          "Fatemeh.Dehghani@ontariotechu.ca": "sessions. We analyzed the data collected from our\nexperiment",
          "loutfouz.zaman@ontariotechu.ca": "•\nintroduce the right\nlevel of dynamism to the game [6]."
        },
        {
          "Fatemeh.Dehghani@ontariotechu.ca": "to\nunderstand which\nemotions\nplayers\nexperience\nduring\nthe",
          "loutfouz.zaman@ontariotechu.ca": ""
        },
        {
          "Fatemeh.Dehghani@ontariotechu.ca": "",
          "loutfouz.zaman@ontariotechu.ca": "If\nthe player’s expression suggests stress,\nthe pertinent game"
        },
        {
          "Fatemeh.Dehghani@ontariotechu.ca": "gameplay. We\nfound\nthat\nour\napproach\nhas\nthe\npotential\nto",
          "loutfouz.zaman@ontariotechu.ca": ""
        },
        {
          "Fatemeh.Dehghani@ontariotechu.ca": "",
          "loutfouz.zaman@ontariotechu.ca": "difficulty factors may be lowered until\nthe player\nis at ease"
        },
        {
          "Fatemeh.Dehghani@ontariotechu.ca": "enhance gameplay analysis by enabling the detection of players’",
          "loutfouz.zaman@ontariotechu.ca": ""
        },
        {
          "Fatemeh.Dehghani@ontariotechu.ca": "emotions\nin VR games, which can help game developers create",
          "loutfouz.zaman@ontariotechu.ca": "[7]."
        },
        {
          "Fatemeh.Dehghani@ontariotechu.ca": "more engaging and immersive game experiences.",
          "loutfouz.zaman@ontariotechu.ca": ""
        },
        {
          "Fatemeh.Dehghani@ontariotechu.ca": "",
          "loutfouz.zaman@ontariotechu.ca": "II. RELATED WORK"
        },
        {
          "Fatemeh.Dehghani@ontariotechu.ca": "Index Terms—Players, Emotions, Virtual Reality, Games, Fa-",
          "loutfouz.zaman@ontariotechu.ca": ""
        },
        {
          "Fatemeh.Dehghani@ontariotechu.ca": "cial Expressions.",
          "loutfouz.zaman@ontariotechu.ca": "Burns and Tulip [6] developed a method to measure gamer"
        },
        {
          "Fatemeh.Dehghani@ontariotechu.ca": "",
          "loutfouz.zaman@ontariotechu.ca": "involvement,\nspecifically\n”flow,”\nusing\nfacial\nexpressions."
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "and\nthe\nviewer’s\nexpected\nemotion\nexperience,\nresulting\nin": "a positive experience.\nJang et al.\n[11] checked the accuracy",
          "the games. We then converted the videos\nto images and fed": "them to the model. We\ncompared the\nfeelings declared by"
        },
        {
          "and\nthe\nviewer’s\nexpected\nemotion\nexperience,\nresulting\nin": "of\nusing\nchanges\nin\nphysiological\nsignals\nin\nresponse\nto",
          "the games. We then converted the videos\nto images and fed": "participants\nin\neach\ngame\nand\nthe\nemotions\npredicted\nby"
        },
        {
          "and\nthe\nviewer’s\nexpected\nemotion\nexperience,\nresulting\nin": "emotions. Six\nemotions were measured.\nJang\net\nal.\nfound",
          "the games. We then converted the videos\nto images and fed": "the model to see how accurately the model classifies emotions."
        },
        {
          "and\nthe\nviewer’s\nexpected\nemotion\nexperience,\nresulting\nin": "skin conductance, blood volume signals and heart\nrate more",
          "the games. We then converted the videos\nto images and fed": ""
        },
        {
          "and\nthe\nviewer’s\nexpected\nemotion\nexperience,\nresulting\nin": "dependable\ncompared\nto\nbaseline\nevaluations. Ouellet\n[12]",
          "the games. We then converted the videos\nto images and fed": ""
        },
        {
          "and\nthe\nviewer’s\nexpected\nemotion\nexperience,\nresulting\nin": "",
          "the games. We then converted the videos\nto images and fed": "A. VGGNet"
        },
        {
          "and\nthe\nviewer’s\nexpected\nemotion\nexperience,\nresulting\nin": "proposed\na\nnew\ntechnique\nfor\nrecognizing\nemotions\nin",
          "the games. We then converted the videos\nto images and fed": ""
        },
        {
          "and\nthe\nviewer’s\nexpected\nemotion\nexperience,\nresulting\nin": "",
          "the games. We then converted the videos\nto images and fed": "VGGNet is an architecture of convolutional neural networks"
        },
        {
          "and\nthe\nviewer’s\nexpected\nemotion\nexperience,\nresulting\nin": "real-time\ngaming\nusing\na\nConvolutional Neural Network",
          "the games. We then converted the videos\nto images and fed": ""
        },
        {
          "and\nthe\nviewer’s\nexpected\nemotion\nexperience,\nresulting\nin": "",
          "the games. We then converted the videos\nto images and fed": "used in pattern recognition and image processing [18]. Four"
        },
        {
          "and\nthe\nviewer’s\nexpected\nemotion\nexperience,\nresulting\nin": "(CNN) to analyze facial expressions of players. Players’ faces",
          "the games. We then converted the videos\nto images and fed": ""
        },
        {
          "and\nthe\nviewer’s\nexpected\nemotion\nexperience,\nresulting\nin": "",
          "the games. We then converted the videos\nto images and fed": "convolutional blocks\nare\nresponsible\nfor\nextracting the\nfea-"
        },
        {
          "and\nthe\nviewer’s\nexpected\nemotion\nexperience,\nresulting\nin": "were detected with the Viola-Jones detector\nin OpenCV from",
          "the games. We then converted the videos\nto images and fed": ""
        },
        {
          "and\nthe\nviewer’s\nexpected\nemotion\nexperience,\nresulting\nin": "",
          "the games. We then converted the videos\nto images and fed": "tures of high level, and the task of categorizing emotions\nis"
        },
        {
          "and\nthe\nviewer’s\nexpected\nemotion\nexperience,\nresulting\nin": "a\ncontinuous video stream captured from the webcam. The",
          "the games. We then converted the videos\nto images and fed": ""
        },
        {
          "and\nthe\nviewer’s\nexpected\nemotion\nexperience,\nresulting\nin": "",
          "the games. We then converted the videos\nto images and fed": "performed by the fully connected layers [19]."
        },
        {
          "and\nthe\nviewer’s\nexpected\nemotion\nexperience,\nresulting\nin": "proposed method was compared to other emotion recognition",
          "the games. We then converted the videos\nto images and fed": ""
        },
        {
          "and\nthe\nviewer’s\nexpected\nemotion\nexperience,\nresulting\nin": "methods and showed better accuracy and speed, with a 94.4%",
          "the games. We then converted the videos\nto images and fed": "B. Dataset"
        },
        {
          "and\nthe\nviewer’s\nexpected\nemotion\nexperience,\nresulting\nin": "accuracy rate. Hickson et al.\n[13] described a unique method",
          "the games. We then converted the videos\nto images and fed": ""
        },
        {
          "and\nthe\nviewer’s\nexpected\nemotion\nexperience,\nresulting\nin": "",
          "the games. We then converted the videos\nto images and fed": "The\nFER2013\nis\na\npublicly\naccessible\ndataset\nof\nfacial"
        },
        {
          "and\nthe\nviewer’s\nexpected\nemotion\nexperience,\nresulting\nin": "for\nidentifying and categorizing facial expressions\nrelated to",
          "the games. We then converted the videos\nto images and fed": ""
        },
        {
          "and\nthe\nviewer’s\nexpected\nemotion\nexperience,\nresulting\nin": "",
          "the games. We then converted the videos\nto images and fed": "emotion recognition (FER), which contains a vast collection"
        },
        {
          "and\nthe\nviewer’s\nexpected\nemotion\nexperience,\nresulting\nin": "five different emotions by utilizing eye movement and gaze",
          "the games. We then converted the videos\nto images and fed": ""
        },
        {
          "and\nthe\nviewer’s\nexpected\nemotion\nexperience,\nresulting\nin": "",
          "the games. We then converted the videos\nto images and fed": "of 35,887 face crops. The dataset has train,\ntest and validation"
        },
        {
          "and\nthe\nviewer’s\nexpected\nemotion\nexperience,\nresulting\nin": "attributes. Long Short-Term Memory (LSTM) networks\nare",
          "the games. We then converted the videos\nto images and fed": ""
        },
        {
          "and\nthe\nviewer’s\nexpected\nemotion\nexperience,\nresulting\nin": "",
          "the games. We then converted the videos\nto images and fed": "sets. For\neach sample,\nsimple\nexpression labels\nare offered."
        },
        {
          "and\nthe\nviewer’s\nexpected\nemotion\nexperience,\nresulting\nin": "combined with CNNs to achieve 74% accuracy in recognizing",
          "the games. We then converted the videos\nto images and fed": ""
        },
        {
          "and\nthe\nviewer’s\nexpected\nemotion\nexperience,\nresulting\nin": "",
          "the games. We then converted the videos\nto images and fed": "Each image has a dimension of 48 × 48 pixels and is grayscale."
        },
        {
          "and\nthe\nviewer’s\nexpected\nemotion\nexperience,\nresulting\nin": "face expressions and 70% accuracy in detecting facial action",
          "the games. We then converted the videos\nto images and fed": ""
        },
        {
          "and\nthe\nviewer’s\nexpected\nemotion\nexperience,\nresulting\nin": "",
          "the games. We then converted the videos\nto images and fed": "FER2013 provides seven emotional expressions: happy,\nfear-"
        },
        {
          "and\nthe\nviewer’s\nexpected\nemotion\nexperience,\nresulting\nin": "units.\nSuzuki\net\nal.\n[14]\ndeveloped\na\ntechnique\nto\nuse",
          "the games. We then converted the videos\nto images and fed": ""
        },
        {
          "and\nthe\nviewer’s\nexpected\nemotion\nexperience,\nresulting\nin": "",
          "the games. We then converted the videos\nto images and fed": "ful, disgusted,\nsad,\nangry, neutral,\nand surprised. Moreover,"
        },
        {
          "and\nthe\nviewer’s\nexpected\nemotion\nexperience,\nresulting\nin": "integrated photo reflecting sensors in an HMD to identify and",
          "the games. We then converted the videos\nto images and fed": ""
        },
        {
          "and\nthe\nviewer’s\nexpected\nemotion\nexperience,\nresulting\nin": "",
          "the games. We then converted the videos\nto images and fed": "we recorded ten 20-minute long videos,\ntwo per participant"
        },
        {
          "and\nthe\nviewer’s\nexpected\nemotion\nexperience,\nresulting\nin": "map face emotions to avatars. Their method recognized seven",
          "the games. We then converted the videos\nto images and fed": ""
        },
        {
          "and\nthe\nviewer’s\nexpected\nemotion\nexperience,\nresulting\nin": "",
          "the games. We then converted the videos\nto images and fed": "(one\nfor\neach game), while participants were playing,\nthen"
        },
        {
          "and\nthe\nviewer’s\nexpected\nemotion\nexperience,\nresulting\nin": "fundamental\nfacial\nemotions with an accuracy of 82% and",
          "the games. We then converted the videos\nto images and fed": ""
        },
        {
          "and\nthe\nviewer’s\nexpected\nemotion\nexperience,\nresulting\nin": "",
          "the games. We then converted the videos\nto images and fed": "converted them to images and made annotations to make them"
        },
        {
          "and\nthe\nviewer’s\nexpected\nemotion\nexperience,\nresulting\nin": "mapped these expressions to avatars with an accuracy of 86%.",
          "the games. We then converted the videos\nto images and fed": ""
        },
        {
          "and\nthe\nviewer’s\nexpected\nemotion\nexperience,\nresulting\nin": "",
          "the games. We then converted the videos\nto images and fed": "ready to feed the model. For\nthis purpose, we got\nthe pixels"
        },
        {
          "and\nthe\nviewer’s\nexpected\nemotion\nexperience,\nresulting\nin": "",
          "the games. We then converted the videos\nto images and fed": "of images and saved them as a CSV file, and then fed them to"
        },
        {
          "and\nthe\nviewer’s\nexpected\nemotion\nexperience,\nresulting\nin": "We avoid the complexity, expenses, and possible constraints",
          "the games. We then converted the videos\nto images and fed": ""
        },
        {
          "and\nthe\nviewer’s\nexpected\nemotion\nexperience,\nresulting\nin": "",
          "the games. We then converted the videos\nto images and fed": "the model, which was trained on the covered eyes dataset (Fig."
        },
        {
          "and\nthe\nviewer’s\nexpected\nemotion\nexperience,\nresulting\nin": "associated with eye-tracking and sensor gear by concentrating",
          "the games. We then converted the videos\nto images and fed": ""
        },
        {
          "and\nthe\nviewer’s\nexpected\nemotion\nexperience,\nresulting\nin": "",
          "the games. We then converted the videos\nto images and fed": "1),\nto see which emotions the model can accurately detect."
        },
        {
          "and\nthe\nviewer’s\nexpected\nemotion\nexperience,\nresulting\nin": "entirely on facial emotions collected through full-face photos.",
          "the games. We then converted the videos\nto images and fed": ""
        },
        {
          "and\nthe\nviewer’s\nexpected\nemotion\nexperience,\nresulting\nin": "Our\ntechnique provides generalizability across multiple VR",
          "the games. We then converted the videos\nto images and fed": ""
        },
        {
          "and\nthe\nviewer’s\nexpected\nemotion\nexperience,\nresulting\nin": "systems and gaming conditions, allowing us to accommodate",
          "the games. We then converted the videos\nto images and fed": ""
        },
        {
          "and\nthe\nviewer’s\nexpected\nemotion\nexperience,\nresulting\nin": "instances\nwhere\neye-tracking\nmay\nnot\nbe\nprovided\nor",
          "the games. We then converted the videos\nto images and fed": ""
        },
        {
          "and\nthe\nviewer’s\nexpected\nemotion\nexperience,\nresulting\nin": "maintained continuously by players.",
          "the games. We then converted the videos\nto images and fed": ""
        },
        {
          "and\nthe\nviewer’s\nexpected\nemotion\nexperience,\nresulting\nin": "CNNs\ndemonstrated\nsuccess\nin\nimage\nprocessing\nsince",
          "the games. We then converted the videos\nto images and fed": ""
        },
        {
          "and\nthe\nviewer’s\nexpected\nemotion\nexperience,\nresulting\nin": "their\ninception in the late 1990s\n[15]. The architecture of a",
          "the games. We then converted the videos\nto images and fed": ""
        },
        {
          "and\nthe\nviewer’s\nexpected\nemotion\nexperience,\nresulting\nin": "CNN makes it particularly effective for handling static images",
          "the games. We then converted the videos\nto images and fed": ""
        },
        {
          "and\nthe\nviewer’s\nexpected\nemotion\nexperience,\nresulting\nin": "[16].\nIn recent years,\nlarger datasets and increased processing",
          "the games. We then converted the videos\nto images and fed": ""
        },
        {
          "and\nthe\nviewer’s\nexpected\nemotion\nexperience,\nresulting\nin": "power have made CNNs a more practical approach for feature",
          "the games. We then converted the videos\nto images and fed": ""
        },
        {
          "and\nthe\nviewer’s\nexpected\nemotion\nexperience,\nresulting\nin": "extraction and image categorization [17].",
          "the games. We then converted the videos\nto images and fed": ""
        },
        {
          "and\nthe\nviewer’s\nexpected\nemotion\nexperience,\nresulting\nin": "",
          "the games. We then converted the videos\nto images and fed": "Fig. 1: Samples of all the expressions with eyes and eyebrows blacked"
        },
        {
          "and\nthe\nviewer’s\nexpected\nemotion\nexperience,\nresulting\nin": "III. PROPOSED METHOD",
          "the games. We then converted the videos\nto images and fed": "out\nfor\ntraining the model."
        },
        {
          "and\nthe\nviewer’s\nexpected\nemotion\nexperience,\nresulting\nin": "In\nthis work, we\naim to\nassess\nthe\nemotions\nof\nplayers",
          "the games. We then converted the videos\nto images and fed": ""
        },
        {
          "and\nthe\nviewer’s\nexpected\nemotion\nexperience,\nresulting\nin": "",
          "the games. We then converted the videos\nto images and fed": "IV. TRAINING AND TESTING OF THE MODEL"
        },
        {
          "and\nthe\nviewer’s\nexpected\nemotion\nexperience,\nresulting\nin": "engaged in VR gaming while wearing HMDs. These displays",
          "the games. We then converted the videos\nto images and fed": ""
        },
        {
          "and\nthe\nviewer’s\nexpected\nemotion\nexperience,\nresulting\nin": "tend\nto\nocclude\nthe\nupper\nfacial\nregions,\nsuch\nas\nthe\neyes",
          "the games. We then converted the videos\nto images and fed": "To\ncreate\nour\ndesired\ndataset\ncontaining\na\ncovered-eye"
        },
        {
          "and\nthe\nviewer’s\nexpected\nemotion\nexperience,\nresulting\nin": "and eyebrows, known for providing crucial cues\nin emotion",
          "the games. We then converted the videos\nto images and fed": "human\nface,\nwe\nused\nthe\nOpenCV\nlibrary\nto\nget\nthe"
        },
        {
          "and\nthe\nviewer’s\nexpected\nemotion\nexperience,\nresulting\nin": "recognition. We\nfirst\ncreated\nour\nrequired\ndataset:\nface",
          "the games. We then converted the videos\nto images and fed": "coordinates of\neyes\nand eyebrows\nin the\nimages. Then, we"
        },
        {
          "and\nthe\nviewer’s\nexpected\nemotion\nexperience,\nresulting\nin": "images with\ncovered\neyes\nand\neyebrows. Next, we\ntrained",
          "the games. We then converted the videos\nto images and fed": "rendered\na\nblack\nrectangle\nthat\ncovered\nthese\nareas. We"
        },
        {
          "and\nthe\nviewer’s\nexpected\nemotion\nexperience,\nresulting\nin": "the existing facial emotion recognition model on the original",
          "the games. We then converted the videos\nto images and fed": "applied this\nfunction on 35,000 images. Eyes and eyebrows"
        },
        {
          "and\nthe\nviewer’s\nexpected\nemotion\nexperience,\nresulting\nin": "dataset, FER, and then on our dataset,\nto see if the model can",
          "the games. We then converted the videos\nto images and fed": "were well\ncovered\nin\nthe\nimages. Then,\nsince\nthe model’s"
        },
        {
          "and\nthe\nviewer’s\nexpected\nemotion\nexperience,\nresulting\nin": "estimate human emotions based on the mouth and chin. We",
          "the games. We then converted the videos\nto images and fed": "checkpoint was\nunavailable, we first\ntrained\nthe model\non"
        },
        {
          "and\nthe\nviewer’s\nexpected\nemotion\nexperience,\nresulting\nin": "then performed an evaluation. Next,\nin order\nto see the real",
          "the games. We then converted the videos\nto images and fed": "the original dataset over 300 epochs. Next, we\ntrained the"
        },
        {
          "and\nthe\nviewer’s\nexpected\nemotion\nexperience,\nresulting\nin": "application of\nthis work, we\nasked five participants\nto play",
          "the games. We then converted the videos\nto images and fed": "model on our desired dataset. The model used in this work"
        },
        {
          "and\nthe\nviewer’s\nexpected\nemotion\nexperience,\nresulting\nin": "two VR games,\nand recorded their\nfaces while wearing the",
          "the games. We then converted the videos\nto images and fed": "achieved a 73.28% accuracy of a single-network on FER2013."
        },
        {
          "and\nthe\nviewer’s\nexpected\nemotion\nexperience,\nresulting\nin": "VR headset. We also recorded their gameplay synchronously.",
          "the games. We then converted the videos\nto images and fed": "Additional\ntraining data was not utilized. When tested on the"
        },
        {
          "and\nthe\nviewer’s\nexpected\nemotion\nexperience,\nresulting\nin": "After\neach game, participants filled out\na\nform where\nthey",
          "the games. We then converted the videos\nto images and fed": "covered eyes dataset,\nthe model’s accuracy was 68.59%."
        },
        {
          "and\nthe\nviewer’s\nexpected\nemotion\nexperience,\nresulting\nin": "were\nasked which emotions\nthey experienced while playing",
          "the games. We then converted the videos\nto images and fed": ""
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "V. REAL WORLD VALIDATION": "While our model\nshowed good results\nin theory, we also"
        },
        {
          "V. REAL WORLD VALIDATION": "wanted to put\nit\ninto practice and see if\nit\nis actually usable"
        },
        {
          "V. REAL WORLD VALIDATION": "as\na\ntool\nfor\nemotion\ndetection\nin Games User Research"
        },
        {
          "V. REAL WORLD VALIDATION": "(GUR). For\nthis validation, we designed a small user\nstudy"
        },
        {
          "V. REAL WORLD VALIDATION": "where we followed a traditional Human-Computer Interaction"
        },
        {
          "V. REAL WORLD VALIDATION": "(HCI) procedure described below. For\nthis\nstudy, we picked"
        },
        {
          "V. REAL WORLD VALIDATION": "two PSVR games: Astro Bot Rescue Mission and Paranormal"
        },
        {
          "V. REAL WORLD VALIDATION": "Activity. The reason they were chosen is because we hypothe-"
        },
        {
          "V. REAL WORLD VALIDATION": "sized that\ntogether\nthese two games would be able to capture"
        },
        {
          "V. REAL WORLD VALIDATION": "most emotions our model\nis capable of detecting. Namely, we"
        },
        {
          "V. REAL WORLD VALIDATION": ""
        },
        {
          "V. REAL WORLD VALIDATION": "hypothesized that playing Astro Bot Rescue Mission would re-"
        },
        {
          "V. REAL WORLD VALIDATION": "sult\nin detecting positive emotions, while Paranormal Activity"
        },
        {
          "V. REAL WORLD VALIDATION": "would result\nin negative ones."
        },
        {
          "V. REAL WORLD VALIDATION": ""
        },
        {
          "V. REAL WORLD VALIDATION": ""
        },
        {
          "V. REAL WORLD VALIDATION": "A. Participants"
        },
        {
          "V. REAL WORLD VALIDATION": ""
        },
        {
          "V. REAL WORLD VALIDATION": "We recruited five participants (three males and two females)"
        },
        {
          "V. REAL WORLD VALIDATION": ""
        },
        {
          "V. REAL WORLD VALIDATION": "to play two games: Astro Bot Rescue Mission and Paranormal"
        },
        {
          "V. REAL WORLD VALIDATION": ""
        },
        {
          "V. REAL WORLD VALIDATION": "Activity.\nThe\nparticipants were\nbetween\nthe\nages\nof\n21"
        },
        {
          "V. REAL WORLD VALIDATION": ""
        },
        {
          "V. REAL WORLD VALIDATION": "and\n27\n(Mdn = 24)\nand were\nrecruited\nfrom the\ngraduate"
        },
        {
          "V. REAL WORLD VALIDATION": ""
        },
        {
          "V. REAL WORLD VALIDATION": "student\npopulation\nin\ncomputer\nscience\nat\nour\nuniversity."
        },
        {
          "V. REAL WORLD VALIDATION": ""
        },
        {
          "V. REAL WORLD VALIDATION": "We gathered participants’ demographic\ninformation through"
        },
        {
          "V. REAL WORLD VALIDATION": ""
        },
        {
          "V. REAL WORLD VALIDATION": "a Google\nform. Participants\nexpressed whether\nthey played"
        },
        {
          "V. REAL WORLD VALIDATION": ""
        },
        {
          "V. REAL WORLD VALIDATION": "VR games before. While three had played VR games before,"
        },
        {
          "V. REAL WORLD VALIDATION": ""
        },
        {
          "V. REAL WORLD VALIDATION": "two participants did not. Also,\nthey were asked to rate their"
        },
        {
          "V. REAL WORLD VALIDATION": ""
        },
        {
          "V. REAL WORLD VALIDATION": "expertise as video game players. One considered themselves"
        },
        {
          "V. REAL WORLD VALIDATION": ""
        },
        {
          "V. REAL WORLD VALIDATION": "a casual player,\nthree consider\nthemselves core players, and"
        },
        {
          "V. REAL WORLD VALIDATION": ""
        },
        {
          "V. REAL WORLD VALIDATION": "one was a hardcore player. Moreover,\nthey were asked about"
        },
        {
          "V. REAL WORLD VALIDATION": "their\nfamiliarity with the games. Two were new to Astro Bot"
        },
        {
          "V. REAL WORLD VALIDATION": ""
        },
        {
          "V. REAL WORLD VALIDATION": "Rescue Mission, and three were new to Paranormal Activity."
        },
        {
          "V. REAL WORLD VALIDATION": ""
        },
        {
          "V. REAL WORLD VALIDATION": ""
        },
        {
          "V. REAL WORLD VALIDATION": "B. Apparatus"
        },
        {
          "V. REAL WORLD VALIDATION": "We used two camera devices to record players’\nfaces, one"
        },
        {
          "V. REAL WORLD VALIDATION": "of which was Apple iPhone 14 Pro Max and the other Log-"
        },
        {
          "V. REAL WORLD VALIDATION": "itech BCC950 ConferenceCam. The gameplay was\ncaptured"
        },
        {
          "V. REAL WORLD VALIDATION": "using 4K Ultra HD HDMI Capture card. The ConferenceCam"
        },
        {
          "V. REAL WORLD VALIDATION": "footage was\nsynced with the output of\nthe capture card and"
        },
        {
          "V. REAL WORLD VALIDATION": "recorded using Open Broadcaster Software (OBS) during the"
        },
        {
          "V. REAL WORLD VALIDATION": "gameplay for later analysis. Both cameras were placed at about"
        },
        {
          "V. REAL WORLD VALIDATION": "waist height\nto the participants and facing upwards to capture"
        },
        {
          "V. REAL WORLD VALIDATION": "the facial expression under\nthe VR helmet. A PC was used"
        },
        {
          "V. REAL WORLD VALIDATION": "to record the output from the ConferenceCam and the capture"
        },
        {
          "V. REAL WORLD VALIDATION": "card. The iPhone footage was used for feeding into the model"
        },
        {
          "V. REAL WORLD VALIDATION": "because it produced video quality that was more acceptable for"
        },
        {
          "V. REAL WORLD VALIDATION": "this purpose, unlike that of\nthe Logitech camera. Both games"
        },
        {
          "V. REAL WORLD VALIDATION": "were hosted on Sony PlayStation 4 with PSVR. The players"
        },
        {
          "V. REAL WORLD VALIDATION": "used PlayStation 4 controllers\nfor playing the games. Fig 2"
        },
        {
          "V. REAL WORLD VALIDATION": "shows the environment and equipment we used for collecting"
        },
        {
          "V. REAL WORLD VALIDATION": "the data."
        },
        {
          "V. REAL WORLD VALIDATION": ""
        },
        {
          "V. REAL WORLD VALIDATION": "C. Procedure"
        },
        {
          "V. REAL WORLD VALIDATION": ""
        },
        {
          "V. REAL WORLD VALIDATION": "After\nfilling\nout\nthe\ndemographic\nquestionnaire, we\nin-"
        },
        {
          "V. REAL WORLD VALIDATION": "structed the participants to play one of the two games first. We"
        },
        {
          "V. REAL WORLD VALIDATION": "counterbalanced the order in which the games were presented"
        },
        {
          "V. REAL WORLD VALIDATION": "to the participants. The participants played the game from the"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "hand on the second floor in the house, which was the scariest": "scene\nfor\nall of\nthem. Some of\nthem were\nsurprised when",
          "can be fine-tuned in the future to detect emotions from multiple": "views. One aspect for further consideration is ensuring precise"
        },
        {
          "hand on the second floor in the house, which was the scariest": "they\nsaw the TV screen\nshowing\nthe\ngirl\n(NPC character)",
          "can be fine-tuned in the future to detect emotions from multiple": "synchronization between the recorded emotions and the spe-"
        },
        {
          "hand on the second floor in the house, which was the scariest": "scared\nof\nseeing\nsomething\nor when\nobjects moved. The",
          "can be fine-tuned in the future to detect emotions from multiple": "cific times\nthey occurred,\nin order\nto enhance confidence in"
        },
        {
          "hand on the second floor in the house, which was the scariest": "neutral feeling was the most prevailing emotion in both games.",
          "can be fine-tuned in the future to detect emotions from multiple": "the\naccuracy and validation of our model. Furthermore,\nan"
        },
        {
          "hand on the second floor in the house, which was the scariest": "",
          "can be fine-tuned in the future to detect emotions from multiple": "emotion-based difficulty adjustment\nsolution for VR games"
        },
        {
          "hand on the second floor in the house, which was the scariest": "Overall, our\nexperimental findings\nindicate\nthat\nthe CNN",
          "can be fine-tuned in the future to detect emotions from multiple": "can be developed based on this work, which potentially could"
        },
        {
          "hand on the second floor in the house, which was the scariest": "can\naccurately\ncategorize\nemotions\nusing\nsolely\nthe\nfacial",
          "can be fine-tuned in the future to detect emotions from multiple": "improve player experience."
        },
        {
          "hand on the second floor in the house, which was the scariest": "features extracted from areas of\nthe face other\nthan the eyes",
          "can be fine-tuned in the future to detect emotions from multiple": ""
        },
        {
          "hand on the second floor in the house, which was the scariest": "",
          "can be fine-tuned in the future to detect emotions from multiple": "REFERENCES"
        },
        {
          "hand on the second floor in the house, which was the scariest": "and\neyebrows,\neven when\nthe\nfacial\ninformation\nin\nthose",
          "can be fine-tuned in the future to detect emotions from multiple": ""
        },
        {
          "hand on the second floor in the house, which was the scariest": "",
          "can be fine-tuned in the future to detect emotions from multiple": "[1] A. Drachen, M. Seif El-Nasr,\nand A. Canossa,\n“Game\nanalytics–the"
        },
        {
          "hand on the second floor in the house, which was the scariest": "regions is absent. This suggests that\nthe model can effectively",
          "can be fine-tuned in the future to detect emotions from multiple": ""
        },
        {
          "hand on the second floor in the house, which was the scariest": "",
          "can be fine-tuned in the future to detect emotions from multiple": "basics,” Game analytics: Maximizing the value of player data, pp. 13–"
        },
        {
          "hand on the second floor in the house, which was the scariest": "detect\nemotions\nin\ndifferent\ngame\nscenarios.\nSome\nof\nthe",
          "can be fine-tuned in the future to detect emotions from multiple": ""
        },
        {
          "hand on the second floor in the house, which was the scariest": "",
          "can be fine-tuned in the future to detect emotions from multiple": "40, 2013."
        },
        {
          "hand on the second floor in the house, which was the scariest": "pictures from the video footage show participants’\nfaces,\nthe",
          "can be fine-tuned in the future to detect emotions from multiple": "[2] G. Wallner, Data Analytics Applications in Gaming and Entertainment."
        },
        {
          "hand on the second floor in the house, which was the scariest": "",
          "can be fine-tuned in the future to detect emotions from multiple": "CRC Press, 2019."
        },
        {
          "hand on the second floor in the house, which was the scariest": "game scene, and the detected emotions. See Fig. 3.",
          "can be fine-tuned in the future to detect emotions from multiple": ""
        },
        {
          "hand on the second floor in the house, which was the scariest": "",
          "can be fine-tuned in the future to detect emotions from multiple": "[3] D. MacCormick and L. Zaman, “Echo: Analyzing gameplay sessions by"
        },
        {
          "hand on the second floor in the house, which was the scariest": "",
          "can be fine-tuned in the future to detect emotions from multiple": "the Annual\nreconstructing them from recorded data,” in Proceedings of"
        },
        {
          "hand on the second floor in the house, which was the scariest": "",
          "can be fine-tuned in the future to detect emotions from multiple": "Symposium on Computer-Human Interaction in Play, 2020, pp. 281–"
        },
        {
          "hand on the second floor in the house, which was the scariest": "",
          "can be fine-tuned in the future to detect emotions from multiple": "293."
        },
        {
          "hand on the second floor in the house, which was the scariest": "",
          "can be fine-tuned in the future to detect emotions from multiple": "[4] H. Schoenau-Fog et al., “The player engagement process-an exploration"
        },
        {
          "hand on the second floor in the house, which was the scariest": "",
          "can be fine-tuned in the future to detect emotions from multiple": "of continuation desire in digital games.” in Digra conference, 2011."
        },
        {
          "hand on the second floor in the house, which was the scariest": "",
          "can be fine-tuned in the future to detect emotions from multiple": "[5] G. N. Yannakakis\nand A. Paiva,\n“Emotion in games,” Handbook on"
        },
        {
          "hand on the second floor in the house, which was the scariest": "",
          "can be fine-tuned in the future to detect emotions from multiple": "affective computing, vol. 2014, pp. 459–471, 2014."
        },
        {
          "hand on the second floor in the house, which was the scariest": "",
          "can be fine-tuned in the future to detect emotions from multiple": "[6] A. Burns and J. Tulip, “Detecting flow in games using facial expres-"
        },
        {
          "hand on the second floor in the house, which was the scariest": "",
          "can be fine-tuned in the future to detect emotions from multiple": "Intelligence and\nsions,”\nin 2017 IEEE Conference on Computational"
        },
        {
          "hand on the second floor in the house, which was the scariest": "",
          "can be fine-tuned in the future to detect emotions from multiple": "Games (CIG).\nIEEE, 2017, pp. 45–52."
        },
        {
          "hand on the second floor in the house, which was the scariest": "",
          "can be fine-tuned in the future to detect emotions from multiple": "[7] M. T. Akbar, M. N.\nIlmi,\nI. V. Rumayar,\nJ. Moniaga, T.-K. Chen,"
        },
        {
          "hand on the second floor in the house, which was the scariest": "",
          "can be fine-tuned in the future to detect emotions from multiple": "and A. Chowanda, “Enhancing game experience with facial expression"
        },
        {
          "hand on the second floor in the house, which was the scariest": "",
          "can be fine-tuned in the future to detect emotions from multiple": "recognition as dynamic balancing,” Procedia Computer Science, vol."
        },
        {
          "hand on the second floor in the house, which was the scariest": "",
          "can be fine-tuned in the future to detect emotions from multiple": "157, pp. 388–395, 2019."
        },
        {
          "hand on the second floor in the house, which was the scariest": "",
          "can be fine-tuned in the future to detect emotions from multiple": "[8] C.\nLiu,\nP. Agrawal, N.\nSarkar,\nand\nS. Chen,\n“Dynamic\ndifficulty"
        },
        {
          "hand on the second floor in the house, which was the scariest": "",
          "can be fine-tuned in the future to detect emotions from multiple": "adjustment\nin computer games\nthrough real-time\nanxiety-based affec-"
        },
        {
          "hand on the second floor in the house, which was the scariest": "",
          "can be fine-tuned in the future to detect emotions from multiple": "tive feedback,” International Journal of Human–Computer Interaction,"
        },
        {
          "hand on the second floor in the house, which was the scariest": "",
          "can be fine-tuned in the future to detect emotions from multiple": "vol. 25, pp. 506 – 529, 2009."
        },
        {
          "hand on the second floor in the house, which was the scariest": "",
          "can be fine-tuned in the future to detect emotions from multiple": "[9] M. Khezri, M. Firoozabadi,\nand A. R. Sharafat,\n“Reliable\nemotion"
        },
        {
          "hand on the second floor in the house, which was the scariest": "",
          "can be fine-tuned in the future to detect emotions from multiple": "recognition system based on dynamic adaptive fusion of forehead biopo-"
        },
        {
          "hand on the second floor in the house, which was the scariest": "",
          "can be fine-tuned in the future to detect emotions from multiple": "tentials and physiological signals,” Computer methods and programs in"
        },
        {
          "hand on the second floor in the house, which was the scariest": "",
          "can be fine-tuned in the future to detect emotions from multiple": "biomedicine, vol. 122, no. 2, pp. 149–164, 2015."
        },
        {
          "hand on the second floor in the house, which was the scariest": "",
          "can be fine-tuned in the future to detect emotions from multiple": "[10] M. Yang, L. Lin, and S. Milekic, “Affective image classification based"
        },
        {
          "hand on the second floor in the house, which was the scariest": "",
          "can be fine-tuned in the future to detect emotions from multiple": "on user eye movement and eeg experience information,” Interacting with"
        },
        {
          "hand on the second floor in the house, which was the scariest": "",
          "can be fine-tuned in the future to detect emotions from multiple": "Computers, vol. 30, no. 5, pp. 417–432, 2018."
        },
        {
          "hand on the second floor in the house, which was the scariest": "",
          "can be fine-tuned in the future to detect emotions from multiple": "[11]\nE.-H.\nJang,\nS. Byun, M.-S.\nPark,\nand\nJ.-H.\nSohn,\n“Reliability\nof"
        },
        {
          "hand on the second floor in the house, which was the scariest": "",
          "can be fine-tuned in the future to detect emotions from multiple": "physiological\nresponses\ninduced\nby\nbasic\nemotions: A pilot\nstudy,”"
        },
        {
          "hand on the second floor in the house, which was the scariest": "",
          "can be fine-tuned in the future to detect emotions from multiple": "Journal of Physiological Anthropology, vol. 38, no. 1, pp. 1–12, 2019."
        },
        {
          "hand on the second floor in the house, which was the scariest": "",
          "can be fine-tuned in the future to detect emotions from multiple": "[12]\nS. Ouellet,\n“Real-time\nemotion\nrecognition\nfor\ngaming\nusing\ndeep"
        },
        {
          "hand on the second floor in the house, which was the scariest": "",
          "can be fine-tuned in the future to detect emotions from multiple": "convolutional network features,” ArXiv, vol. abs/1408.3750, 2014."
        },
        {
          "hand on the second floor in the house, which was the scariest": "",
          "can be fine-tuned in the future to detect emotions from multiple": "[13]\nS. Hickson, N. Dufour, A. Sud, V. Kwatra, and I. Essa, “Eyemotion:"
        },
        {
          "hand on the second floor in the house, which was the scariest": "",
          "can be fine-tuned in the future to detect emotions from multiple": "Classifying facial expressions in vr using eye-tracking cameras,” in 2019"
        },
        {
          "hand on the second floor in the house, which was the scariest": "",
          "can be fine-tuned in the future to detect emotions from multiple": "IEEE Winter Conference on Applications of Computer Vision (WACV),"
        },
        {
          "hand on the second floor in the house, which was the scariest": "",
          "can be fine-tuned in the future to detect emotions from multiple": "2019, pp. 1626–1635."
        },
        {
          "hand on the second floor in the house, which was the scariest": "",
          "can be fine-tuned in the future to detect emotions from multiple": "[14] K. Suzuki, F. Nakamura, J. Otsuka, K. Masai, Y.\nItoh, Y. Sugiura, and"
        },
        {
          "hand on the second floor in the house, which was the scariest": "",
          "can be fine-tuned in the future to detect emotions from multiple": "M. Sugimoto, “Recognition and mapping of facial expressions to avatar"
        },
        {
          "hand on the second floor in the house, which was the scariest": "",
          "can be fine-tuned in the future to detect emotions from multiple": "by embedded photo reflective sensors in head mounted display,” in 2017"
        },
        {
          "hand on the second floor in the house, which was the scariest": "",
          "can be fine-tuned in the future to detect emotions from multiple": "IEEE Virtual Reality (VR).\nIEEE, 2017, pp. 177–185."
        },
        {
          "hand on the second floor in the house, which was the scariest": "",
          "can be fine-tuned in the future to detect emotions from multiple": "[15] Y. LeCun, Y. Bengio et al., “Convolutional networks for images, speech,"
        },
        {
          "hand on the second floor in the house, which was the scariest": "",
          "can be fine-tuned in the future to detect emotions from multiple": "and time series,” The handbook of brain theory and neural networks,"
        },
        {
          "hand on the second floor in the house, which was the scariest": "",
          "can be fine-tuned in the future to detect emotions from multiple": "vol. 3361, no. 10, p. 1995, 1995."
        },
        {
          "hand on the second floor in the house, which was the scariest": "Fig. 3: Some of the pictures from the video footage show participants’",
          "can be fine-tuned in the future to detect emotions from multiple": ""
        },
        {
          "hand on the second floor in the house, which was the scariest": "",
          "can be fine-tuned in the future to detect emotions from multiple": "[16] K. S. Sri, N. N. Kumar,\nand V. D. Satish,\n“Facial\nemotion recogni-"
        },
        {
          "hand on the second floor in the house, which was the scariest": "faces,\nthe game scene, and the detected emotions.",
          "can be fine-tuned in the future to detect emotions from multiple": ""
        },
        {
          "hand on the second floor in the house, which was the scariest": "",
          "can be fine-tuned in the future to detect emotions from multiple": "tion using dcnn algorithm,”\nin 2022 7th International Conference on"
        },
        {
          "hand on the second floor in the house, which was the scariest": "",
          "can be fine-tuned in the future to detect emotions from multiple": "Communication and Electronics Systems\n(ICCES).\nIEEE, 2022, pp."
        },
        {
          "hand on the second floor in the house, which was the scariest": "",
          "can be fine-tuned in the future to detect emotions from multiple": "1336–1341."
        },
        {
          "hand on the second floor in the house, which was the scariest": "VII. LIMITATIONS AND FUTURE WORK",
          "can be fine-tuned in the future to detect emotions from multiple": ""
        },
        {
          "hand on the second floor in the house, which was the scariest": "",
          "can be fine-tuned in the future to detect emotions from multiple": "[17] A. Krizhevsky,\nI. Sutskever, and G. E. Hinton, “Imagenet classification"
        },
        {
          "hand on the second floor in the house, which was the scariest": "",
          "can be fine-tuned in the future to detect emotions from multiple": "with\ndeep\nconvolutional\nneural\nnetworks,” Commun. ACM,\nvol.\n60,"
        },
        {
          "hand on the second floor in the house, which was the scariest": "The obvious\nlimitation of\nthe current work is\nthat we did",
          "can be fine-tuned in the future to detect emotions from multiple": ""
        },
        {
          "hand on the second floor in the house, which was the scariest": "",
          "can be fine-tuned in the future to detect emotions from multiple": "no. 6, p. 84–90, may 2017. [Online]. Available: https://doi.org/10.1145/"
        },
        {
          "hand on the second floor in the house, which was the scariest": "not capture players’ expressions when they moved their heads",
          "can be fine-tuned in the future to detect emotions from multiple": ""
        },
        {
          "hand on the second floor in the house, which was the scariest": "",
          "can be fine-tuned in the future to detect emotions from multiple": "3065386"
        },
        {
          "hand on the second floor in the house, which was the scariest": "or\nturned around while playing. The model could only detect",
          "can be fine-tuned in the future to detect emotions from multiple": "[18] K. Simonyan and A. Zisserman, “Very deep convolutional networks for"
        },
        {
          "hand on the second floor in the house, which was the scariest": "",
          "can be fine-tuned in the future to detect emotions from multiple": "large-scale image recognition,” arXiv preprint arXiv:1409.1556, 2014."
        },
        {
          "hand on the second floor in the house, which was the scariest": "emotions when players’ faces were facing the camera straight.",
          "can be fine-tuned in the future to detect emotions from multiple": ""
        },
        {
          "hand on the second floor in the house, which was the scariest": "",
          "can be fine-tuned in the future to detect emotions from multiple": "[19] Y. Khaireddin and Z. Chen, “Facial emotion recognition: State of\nthe"
        },
        {
          "hand on the second floor in the house, which was the scariest": "This\nlimitation can be\naddressed by attaching,\ne.g.,\na\nselfie",
          "can be fine-tuned in the future to detect emotions from multiple": ""
        },
        {
          "hand on the second floor in the house, which was the scariest": "",
          "can be fine-tuned in the future to detect emotions from multiple": "art performance on fer2013,” arXiv preprint arXiv:2105.03588, 2021."
        },
        {
          "hand on the second floor in the house, which was the scariest": "stick to the VR helmet, so that the camera is always facing the",
          "can be fine-tuned in the future to detect emotions from multiple": ""
        }
      ],
      "page": 4
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Game analytics-the basics",
      "authors": [
        "A Drachen",
        "M Seif El-Nasr",
        "A Canossa"
      ],
      "year": "2013",
      "venue": "Game analytics: Maximizing the value of player data"
    },
    {
      "citation_id": "2",
      "title": "Data Analytics Applications in Gaming and Entertainment",
      "authors": [
        "G Wallner"
      ],
      "year": "2019",
      "venue": "Data Analytics Applications in Gaming and Entertainment"
    },
    {
      "citation_id": "3",
      "title": "Echo: Analyzing gameplay sessions by reconstructing them from recorded data",
      "authors": [
        "D Maccormick",
        "L Zaman"
      ],
      "year": "2020",
      "venue": "Proceedings of the Annual Symposium on Computer-Human Interaction in Play"
    },
    {
      "citation_id": "4",
      "title": "The player engagement process-an exploration of continuation desire in digital games",
      "authors": [
        "H Schoenau-Fog"
      ],
      "year": "2011",
      "venue": "Digra conference"
    },
    {
      "citation_id": "5",
      "title": "Emotion in games",
      "authors": [
        "G Yannakakis",
        "A Paiva"
      ],
      "year": "2014",
      "venue": "Handbook on affective computing"
    },
    {
      "citation_id": "6",
      "title": "Detecting flow in games using facial expressions",
      "authors": [
        "A Burns",
        "J Tulip"
      ],
      "year": "2017",
      "venue": "2017 IEEE Conference on Computational Intelligence and Games (CIG)"
    },
    {
      "citation_id": "7",
      "title": "Enhancing game experience with facial expression recognition as dynamic balancing",
      "authors": [
        "M Akbar",
        "M Ilmi",
        "I Rumayar",
        "J Moniaga",
        "T.-K Chen",
        "A Chowanda"
      ],
      "year": "2019",
      "venue": "Procedia Computer Science"
    },
    {
      "citation_id": "8",
      "title": "Dynamic difficulty adjustment in computer games through real-time anxiety-based affective feedback",
      "authors": [
        "C Liu",
        "P Agrawal",
        "N Sarkar",
        "S Chen"
      ],
      "year": "2009",
      "venue": "International Journal of Human-Computer Interaction"
    },
    {
      "citation_id": "9",
      "title": "Reliable emotion recognition system based on dynamic adaptive fusion of forehead biopotentials and physiological signals",
      "authors": [
        "M Khezri",
        "M Firoozabadi",
        "A Sharafat"
      ],
      "year": "2015",
      "venue": "Computer methods and programs in biomedicine"
    },
    {
      "citation_id": "10",
      "title": "Affective image classification based on user eye movement and eeg experience information",
      "authors": [
        "M Yang",
        "L Lin",
        "S Milekic"
      ],
      "year": "2018",
      "venue": "Interacting with Computers"
    },
    {
      "citation_id": "11",
      "title": "Reliability of physiological responses induced by basic emotions: A pilot study",
      "authors": [
        "E.-H Jang",
        "S Byun",
        "M.-S Park",
        "J.-H Sohn"
      ],
      "year": "2019",
      "venue": "Journal of Physiological Anthropology"
    },
    {
      "citation_id": "12",
      "title": "Real-time emotion recognition for gaming using deep convolutional network features",
      "authors": [
        "S Ouellet"
      ],
      "year": "2014",
      "venue": "ArXiv"
    },
    {
      "citation_id": "13",
      "title": "Eyemotion: Classifying facial expressions in vr using eye-tracking cameras",
      "authors": [
        "S Hickson",
        "N Dufour",
        "A Sud",
        "V Kwatra",
        "I Essa"
      ],
      "year": "2019",
      "venue": "2019 IEEE Winter Conference on Applications of Computer Vision (WACV)"
    },
    {
      "citation_id": "14",
      "title": "Recognition and mapping of facial expressions to avatar by embedded photo reflective sensors in head mounted display",
      "authors": [
        "K Suzuki",
        "F Nakamura",
        "J Otsuka",
        "K Masai",
        "Y Itoh",
        "Y Sugiura",
        "M Sugimoto"
      ],
      "year": "2017",
      "venue": "IEEE Virtual Reality"
    },
    {
      "citation_id": "15",
      "title": "Convolutional networks for images, speech, and time series",
      "authors": [
        "Y Lecun",
        "Y Bengio"
      ],
      "year": "1995",
      "venue": "The handbook of brain theory and neural networks"
    },
    {
      "citation_id": "16",
      "title": "Facial emotion recognition using dcnn algorithm",
      "authors": [
        "K Sri",
        "N Kumar",
        "V Satish"
      ],
      "year": "2022",
      "venue": "2022 7th International Conference on Communication and Electronics Systems (ICCES)"
    },
    {
      "citation_id": "17",
      "title": "Imagenet classification with deep convolutional neural networks",
      "authors": [
        "A Krizhevsky",
        "I Sutskever",
        "G Hinton"
      ],
      "year": "2017",
      "venue": "Commun. ACM",
      "doi": "10.1145/3065386"
    },
    {
      "citation_id": "18",
      "title": "Very deep convolutional networks for large-scale image recognition",
      "authors": [
        "K Simonyan",
        "A Zisserman"
      ],
      "year": "2014",
      "venue": "Very deep convolutional networks for large-scale image recognition",
      "arxiv": "arXiv:1409.1556"
    },
    {
      "citation_id": "19",
      "title": "Facial emotion recognition: State of the art performance on fer2013",
      "authors": [
        "Y Khaireddin",
        "Z Chen"
      ],
      "year": "2021",
      "venue": "Facial emotion recognition: State of the art performance on fer2013",
      "arxiv": "arXiv:2105.03588"
    }
  ]
}