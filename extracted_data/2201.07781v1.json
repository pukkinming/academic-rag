{
  "paper_id": "2201.07781v1",
  "title": "Towards A General Deep Feature Extractor For Facial Expression Recognition",
  "published": "2022-01-19T18:42:23Z",
  "authors": [
    "Liam Schoneveld",
    "Alice Othmani"
  ],
  "keywords": [
    "Facial Expression Recognition",
    "Visual emotion recognition",
    "Deep learning",
    "model generalisation",
    "Knowledge distillation"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "The human face conveys a significant amount of information. Through facial expressions, the face is able to communicate numerous sentiments without the need for verbalisation. Visual emotion recognition has been extensively studied. Recently several end-to-end trained deep neural networks have been proposed for this task. However, such models often lack generalisation ability across datasets. In this paper, we propose the Deep Facial Expression Vector ExtractoR (DeepFEVER), a new deep learning-based approach that learns a visual feature extractor general enough to be applied to any other facial emotion recognition task or dataset. DeepFEVER outperforms state-ofthe-art results on the AffectNet and Google Facial Expression Comparison datasets. DeepFEVER's extracted features also generalise extremely well to other datasets -even those unseen during training -namely, the Real-World Affective Faces (RAF) dataset.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "Emotional Intelligence in Human-Computer Interaction has attracted increasing attention from researchers in multidisciplinary fields including psychology, computer vision, neuroscience, artificial intelligence, and related disciplines. Humans also tend to interact with computers in a face-to-face manner. As such, human facial expressions present an important opportunity to better link humans and computers. Designing interfaces able to understand human expressions and emotions can thus improve Human-Computer Interaction (HCI) overall.\n\nRecently, deep neural network-based approaches have become more popular for facial expression recognition (FER). These approaches have significantly improved the performance over more traditional approaches to computer vision  [1] -  [3]  due to their capacity to automatically learn both low and high level descriptors from facial images without manual feature engineering.\n\nDespite considerable advancements in the field, Facial Expression Recognition (FER) has remained a challenging task. Most existing methods still lack generalisation ability across datasets acquired under different conditions. The majority of modern approaches in the emotion recognition literature either learn or fine-tune an end-to-end network that can only be used for a specific dataset, and/or use more general features as input to a more basic model.\n\nTo overcome this limitation, it is proposed in this paper to learn an independent feature extractor for images of faces, specialised for facial expressions, that could be employed for any downstream FER task or dataset. This approach achieves its generalisation ability by training on multiple labeled FER datasets, and by employing knowledge distillation (specifically, self-distillation), alongside additional unlabeled data for FER. The proposed visual facial expression embedding network is described in Section II. Section III describes the experimental setting and discusses the results obtained on three FER datasets. Finally, Section IV concludes the paper and discusses future work.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Ii. Proposed Approach",
      "text": "This paper proposes a general and independent deep facial expression feature extractor called Deep Facial Expression Vector ExtractoR (DeepFEVER). DeepFEVER is a convolutional neural network (CNN) trained using: (1) two labeled FER datasets, (2) an additional unlabeled dataset and (3) the technique known as knowledge distillation  [4] .\n\nNot only does DeepFEVER outperform state-of-the-art results on both the datasets it was trained on, it also generalises very well; DeepFEVER achieves highly competitive results on an additional FER dataset, unseen during training, without any fine-tuning.\n\nDeepFEVER is trained using knowledge distillation: a two step process whereby a teacher network is trained on the task of interest, and then a (typically smaller) student network is trained on predictions made by the teacher. Specifically in this work, we leverage the benefits of self-distillation, wherein the student network has the same size as (or at least, is not smaller than) the teacher network. Self-distillation is used in this work to improve the performance of the facial expression embedding network, as well as to provide more flexibility over the final model architecture.\n\nThe training procedure for DeepFEVER consists of two phases: the teacher model training phase and the student model training phase. In the teacher model training phase, a FaceNet  [6]  is retrained simultaneously on two different visual FER datasets (Section II-A). In the student model training phase, we again train on these two datasets (Section II-B). However, the starting point for the student network can be any CNN architecture (we opt for a DenseNet  [5] ). The student is also arXiv:2201.07781v1 [cs.CV] 19 Jan 2022 trained on an additional unlabeled dataset, whose training 'labels' are provided by the teacher network.\n\nThe output of DeepFEVER is a compact vector of dimension D face . This was set to D face = 256 as such generally performed better in experiments. During training this vector serves as a shared input for various output heads that each produce predictions for each of the training tasks. After training, this vector can be extracted from face images to provide features for any downstream FER task. More details about the teacher and the student networks follow.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "A. The Teacher Network",
      "text": "The proposed teacher network starts from a pre-trained FaceNet  [6] . As FaceNet is for person re-identification, the teacher network is further trained using two more datasets to learn specialised features for FER. These datasets are Affect-Net  [7]  and Google Facial Expression Comparison (FEC)  [8] . Further details about the datasets are provided in Section III-A.\n\nThe teacher model's architecture (Fig.  1 ) is almost identical to the model proposed in  [8] , the only difference is that an additional output head is added for the AffectNet loss. Thus, following  [8] , a pre-trained FaceNet 1  is taken up until the Inception 4e block. This is followed by a 1x1 convolution and a series of five untrained DenseNet  [5]  blocks. After this, another 1x1 convolution followed by global average pooling reduces this representation to a single D face dimensional vector. After pooling, two independent linear transformations serve as output heads. These heads take the D face -dimensional facial expression representation vector as input and make separate predictions for the AffectNet and FEC tasks. A 32dimensional embedding is used for the FEC triplets task, while an 8-dimensional output head produces class logits for AffectNet (which has 8 classes). The teacher network training procedure is detailed in Algorithm 1, while implementation details are provided in Section III-B.\n\nTo improve the regularisation effects of self-distillation through model ensembling, two teacher networks are trained, and their outputs are concatenated to serve as distillation targets (see Section II-B for details). The only difference between the two teachers are the random seeds used for initialisation, and penultimate layer dimensionalities: one teacher network was trained with D face = 256, the other with D face = 128.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "B. Student Network",
      "text": "The student network is a DenseNet201 pre-trained on Imagenet.  2  The student network training procedure is essentially the same as described in Algorithm 1, except that we additionally sample batches of unlabeled data from an internal dataset, which we refer to as PowderFaces (See Section III-A). The sampled batches of face crops from the Google FEC, AffectNet, and PowderFaces datasets are passed through the Update feature extractor and output heads' parameters simultaneously end two teacher networks. Each of the two teacher networks produces predictions for the Google FEC task (32-dimensional) and AffectNet class logits (8-dimensional). These four vectors (i.e., two vectors from two teacher networks) are individually L2-normalised. The four normalised vectors are then concatenated, producing one long vector of dimension 80. A knowledge distillation loss (we specifically use Relational Knowledge Distillation  [9]  for the loss function) is then calculated by comparing the output of a third output head in the student network to this 80-dimensional target vector. This knowledge distillation loss is then added to the standard AffectNet and Google FEC losses, which are calculated as per the teacher network training procedure. Implementation details for the student network are provided in Section III-B.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Iii. Experiments And Results",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "A. Datasets",
      "text": "Two datasets are used to train the teacher network:\n\n• AffectNet  [7] , which consists of around 440,000 inthe-wild face crop images, each of which is humanannotated into one of eight facial expression categories (Neutral, Happy, Sad, Surprise, Fear, Disgust, Anger and Contempt). • Google Facial Expression Comparison (FEC)  [8] ,\n\nwhich consists of around 700,000 triplets of unique face crop images. Annotations denoting the most similar pair of face expressions in each triplet are provided. The goal is to train a model that places the similar pair closer together in a learned embedding space. An additional, unlabeled dataset is used to train the student network via knowledge distillation: the PowderFaces dataset. This dataset was created by downloading approximately 20,000 short, publicly-available videos from various online sources such as YouTube. To increase the frequency of faces in the dataset, specific search terms and topics were used when searching for videos, such as 'podcast', 'interview', or 'monologue'. MTCNN face detection  [10]  was then applied to the extracted frames from those videos, producing approximately 1 million individual face crops.\n\nTo evaluate the generalisation ability of DeepFEVER, the Real-World Affective Faces Database (RAF)  [11]  is used. RAF consists of 12,271 training face images and 3,068 validation face images, each annotated into one of seven classes (Surprise, Fear, Disgust, Happiness, Sadness, Anger and Neutral).",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "B. Implementation Details",
      "text": "Implementation details of the teacher network: The entire network is retrained (i.e., none of the original FaceNet model's weights are frozen). The SGD optimizer is used with a learning rate (LR) of 0.005 and Nesterov momentum of 0.9. No learning rate scheduling is used -rather training was simply stopped after 13 epochs, based on validation performance  3  . A batch size of 64 for both the triplets and AffectNet datasets is used. To bring the AffectNet crossentropy loss' magnitude more in line with that of the triplet loss, it is multiplied by a factor of α = 0.1 during training. Dropout of 0.1 is applied to the D face -dimensional expression vector before passing it to the output heads.\n\nImplementation details of the student network: For the knowledge distillation loss, Relational Knowledge Distillation (RKD)  (Park et al. [2019] ) is used. The RKD distance-wise loss is multiplied by 25, and the angle-wise loss is multiplied by 50, to bring their magnitude up to a similar level as per the AffectNet and Google FEC loss components. The implementation details of the student network are exactly the same as per the teacher network (Section II-A), except that it was trained for longer (18 epochs), uses smaller batch sizes (36 Google FEC triplets, 16 AffectNet images and 16 PowderFaces images per iteration) and has a penultimate layer dimension of D face = 256 (like one of the two teacher networks), with dropout of 0.2 instead of 0.1\n\nImplementation details of the linear evaluation on RAF: To evaluate DeepFEVER on RAF, the D face -dimensional feature vectors are extracted using the student model for every face in the RAF dataset. A logistic regression model  [17]  is then trained (with regularisation parameter C set to 10000) on the features extracted from the training set. The accuracy of this model is then calculated by predicting the validation set labels, using only the extracted feature vectors as input.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "C. Performance Of Deepfever For Visual Fer",
      "text": "We evaluate DeepFEVER on the standard evaluation subsets of the two datasets (AffectNet and Google FEC) used in training, plus a third dataset, unseen during training (RAF):\n\n1) AffectNet: for AffectNet, which requires classifying faces into eight discrete facial expression classes, a logistic regression model is trained on the features extracted by the student network for the entire AffectNet training set.  4  For eight-class classification, DeepFEVER achieves state-of-the-art results on the AffectNet validation set, with an accuracy of 61.6% (Table  I ). Some papers report seven-class accuracy by excluding images with the contempt class from the training and validation sets. DeepFEVER also achieves state-of-the-art results on this seven-class task, with an accuracy of 65.4%. 2) Google FEC: following  [8] , models are evaluated using triplet accuracy on the Google FEC test set. As shown in Table  II , DeepFEVER outperforms state-of-the-art methods, with an accuracy of 86.5% . 3) RAF: A logistic regression trained on DeepFEVER face vectors outperforms all but one of the previous state-of-the-art results on RAF, including those which train directly on the RAF dataset (Table  III ).   [12]  59.6% Pyramid with Super Resolution  [15]  60.7% 63.8% PAENet  [16]  65",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Methods",
      "text": "Accuracy IPA2LT  [13]  86.8% RAN-ResNet18  [14]  86.9% PSN  [15]  89.0% DeepFEVER (Distilled student, no fine-tuning) 87.4%",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "D. Ablation Study",
      "text": "To experimentally verify the importance of the different components of our approach, an ablation study is performed (Tables  I  and II ). Training the student model architecture without the distillation loss component revealed the importance of distillation: without such, performance dropped substantially: to 58.8% on AffectNet (eight-class) and 85% on Google FEC.\n\nSimilarly, to determine the importance of the unlabeled PowderFaces dataset, the student model was trained with distillation, but without the additional distillation targets provided by using this unlabeled data. The results suggest that the additional unlabeled data is not so important: accuracy on AffectNet dropped only slightly to 61.1%, and performance on Google FEC reduced by only 0.1% to 86.4%. Other works successfully applying similar approaches to self-distillation used an unlabeled dataset on the order of 300 times larger than the labeled dataset  [18] . In our case the ratio of unlabeled data to labeled data is closer to one to one. We leave determining whether larger volumes of unlabeled data can further improve DeepFEVER's performance to future work.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Iv. Conclusion And Future Work",
      "text": "This research proposes a new approach to learning a deep facial expression feature extractor, with high generalisation ability across datasets. The proposed DeepFEVER network outperforms state-of-the-art approaches on the datasets it was trained on, and achieves highly competitive results the RAF dataset without any fine-tuning. In future work, we plan to explore scaling up the unlabeled dataset for self-distillation and quantisation of the learned facial embedding space.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: ) is almost identical",
      "page": 2
    },
    {
      "caption": "Figure 1: The facial expression recognition neural network architecture, before distillation (i.e. the teacher network). Faces are detected and cropped using",
      "page": 3
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Methods": "CNNs and BOVW + local SVM [12]\nPyramid with Super Resolution [15]\nPAENet\n[16]\nDeepFEVER (Teacher model)\nDeepFEVER (Student, no distillation)\nDeepFEVER (Distilled student, no PowderFaces)\nDeepFEVER (Distilled student)",
          "8-class": "59.6%\n60.7%\n61.3%\n58.8%\n61.1%\n61.6%",
          "7-class": "63.8%\n65.3%\n65.4%\n62.6%\n65.2%\n65.4%"
        }
      ],
      "page": 4
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Deep facial expression recognition: A survey",
      "authors": [
        "S Li",
        "W Deng"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "2",
      "title": "Deep-emotion: Facial expression recognition using attentional convolutional network",
      "authors": [
        "S Minaee",
        "A Abdolrashidi"
      ],
      "year": "2019",
      "venue": "Deep-emotion: Facial expression recognition using attentional convolutional network",
      "arxiv": "arXiv:1902.01019"
    },
    {
      "citation_id": "3",
      "title": "Multi-Facial Patches Aggregation Network For Facial Expression Recognition and Facial Regions Contributions to Emotion Display",
      "authors": [
        "A Hazourli",
        "A Djeghri",
        "H Salam",
        "A Othmani"
      ],
      "year": "2021",
      "venue": "Multimedia Tools and Applications",
      "arxiv": "arXiv:2002.09298"
    },
    {
      "citation_id": "4",
      "title": "Distilling the knowledge in a neural network",
      "authors": [
        "G Hinton",
        "O Vinyals",
        "J Dean"
      ],
      "year": "2014",
      "venue": "Deep Learning Workshop NIPS",
      "arxiv": "arXiv:1503.02531"
    },
    {
      "citation_id": "5",
      "title": "Densely connected convolutional networks",
      "authors": [
        "G Huang",
        "Z Liu",
        "L Van Der Maaten",
        "K Weinberger"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "6",
      "title": "Facenet: A unified embedding for face recognition and clustering",
      "authors": [
        "F Schroff",
        "D Kalenichenko",
        "J Philbin"
      ],
      "year": "2015",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "7",
      "title": "Affectnet: A database for facial expression, valence, and arousal computing in the wild",
      "authors": [
        "A Mollahosseini",
        "B Hasani",
        "M Mahoor"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "8",
      "title": "A compact embedding for facial expression similarity",
      "authors": [
        "R Vemulapalli",
        "A Agarwala"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "9",
      "title": "Relational knowledge distillation",
      "authors": [
        "W Park",
        "D Kim",
        "Y Lu",
        "M Cho"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "10",
      "title": "Joint face detection and alignment using multitask cascaded convolutional networks",
      "authors": [
        "K Zhang",
        "Z Zhang",
        "Z Li",
        "Y Qiao"
      ],
      "year": "2016",
      "venue": "IEEE Signal Processing Letters"
    },
    {
      "citation_id": "11",
      "title": "Reliable Crowdsourcing and Deep Locality-Preserving Learning for Unconstrained Facial Expression Recognition",
      "authors": [
        "S Li",
        "W Deng"
      ],
      "year": "2019",
      "venue": "IEEE Trans. Image Process"
    },
    {
      "citation_id": "12",
      "title": "Local Learning With Deep and Handcrafted Features for Facial Expression Recognition",
      "authors": [
        "M Georgescu",
        "R Ionescu",
        "M Popescu"
      ],
      "year": "2019",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "13",
      "title": "Facial expression recognition with inconsistently annotated datasets",
      "authors": [
        "J Zeng",
        "S Shan",
        "X Chen"
      ],
      "year": "2018",
      "venue": "Proc. Eur. Conf. Comput. Vis"
    },
    {
      "citation_id": "14",
      "title": "Region attention networks for pose and occlusion robust facial expression recognition",
      "authors": [
        "K Wang",
        "X Peng",
        "J Yang",
        "D Meng",
        "Y Qiao"
      ],
      "year": "2020",
      "venue": "IEEE Trans. Image Process"
    },
    {
      "citation_id": "15",
      "title": "Pyramid With Super Resolution for In-the-Wild Facial Expression Recognition",
      "authors": [
        "T Vo",
        "G Lee",
        "H Yang",
        "S Kim"
      ],
      "year": "2020",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "16",
      "title": "Increasingly Packing Multiple Facial-Informatics Modules in A Unified Deep-Learning Model via Lifelong Learning",
      "authors": [
        "S Hung",
        "J Lee",
        "T Wan",
        "C Chen",
        "Y Chan",
        "C Chen"
      ],
      "year": "2019",
      "venue": "Proceedings of the Intl. Conference on Multimedia Retrieval"
    },
    {
      "citation_id": "17",
      "title": "Scikit-learn: Machine Learning in Python",
      "authors": [
        "F Pedregosa",
        "G Varoquaux",
        "A Gramfort",
        "V Michel",
        "B Thirion",
        "O Grisel",
        "M Blondel",
        "P Prettenhofer",
        "R Weiss",
        "V Dubourg",
        "J Vanderplas",
        "A Passos",
        "D Cournapeau",
        "M Brucher",
        "M Perrot",
        "E Duchesnay"
      ],
      "year": "2011",
      "venue": "Journal of Machine Learning Research"
    },
    {
      "citation_id": "18",
      "title": "Self-training with noisy student improves imagenet classification",
      "authors": [
        "Q Xie",
        "M Luong",
        "E Hovy",
        "Q Le"
      ],
      "year": "2020",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    }
  ]
}