{
  "paper_id": "2310.03992v1",
  "title": "Layer-Adapted Implicit Distribution Alignment Networks For Cross-Corpus Speech Emotion Recognition",
  "published": "2023-10-06T03:34:48Z",
  "authors": [
    "Yan Zhao",
    "Yuan Zong",
    "Jincen Wang",
    "Hailun Lian",
    "Cheng Lu",
    "Li Zhao",
    "Wenming Zheng"
  ],
  "keywords": [
    "Cross-corpus speech emotion recognition",
    "speech emotion recognition",
    "transfer learning",
    "deep learning",
    "unsupervised domain adaptation"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "In this paper, we propose a new unsupervised domain adaptation (DA) method called layer-adapted implicit distribution alignment networks (LIDAN) to address the challenge of cross-corpus speech emotion recognition (SER). LIDAN extends our previous ICASSP work, deep implicit distribution alignment networks (DIDAN), whose key contribution lies in the introduction of a novel regularization term called implicit distribution alignment (IDA). This term allows DIDAN trained on source (training) speech samples to remain applicable to predicting emotion labels for target (testing) speech samples, regardless of corpus variance in cross-corpus SER. To further enhance this method, we extend IDA to layer-adapted IDA (LIDA), resulting in LIDAN. This layer-adpated extention consists of three modified IDA terms that consider emotion labels at different levels of granularity. These terms are strategically arranged within different fully connected layers in LIDAN, aligning with the increasing emotion-discriminative abilities with respect to the layer depth. This arrangement enables LIDAN to more effectively learn emotion-discriminative and corpus-invariant features for SER across various corpora compared to DIDAN. It is also worthy to mention that unlike most existing methods that rely on estimating statistical moments to describe pre-assumed explicit distributions, both IDA and LIDA take a different approach. They utilize an idea of target sample reconstruction to directly bridge the feature distribution gap without making assumptions about their distribution type. As a result, DIDAN and LIDAN can be viewed as implicit cross-corpus SER methods. To evaluate LIDAN, we conducted extensive cross-corpus SER experiments on EmoDB, eNTERFACE, and CASIA corpora. The experimental results demonstrate that LIDAN surpasses recent state-of-the-art explicit unsupervised DA methods in tackling cross-corpus SER tasks.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "T HE research of speech emotion recognition (SER) aims to equip computers with the ability to automatically understand the emotional states conveyed in speech signals, e.g., Happiness, Sadness, and Surprise  [1] ,  [2] . If the computers possessed this capability, their interactions with humans would become more natural. Consequently, SER has emerged as a prominent and intriguing topic within the fields of affective computing, speech signal processing, and human-computer interaction over the past few decades. One of the earliest works on SER can be traced back to the study of Williams and Stevens  [3] , in which they investigated whether acoustic parameters extracted from speech signals recorded under different emotional states exhibited distinct characteristics. Since then, significant efforts have been devoted to SER, leading to numerous effective SER methods  [4] -  [10] .\n\nIt is worth noting that most of these SER methods often assume that the feature distributions of the training and testing speech signals follow the same or similar distribution, which allows them to achieve promising performance. However, in practical application scenarios, this assumption may be easily violated. For example, the training and testing speech signals encountered by the SER system may be most likely recorded by quite different acoustic sensors or in different languages. This introduces a new task in SER known as crosscorpus SER  [11] . Formally speaking, the source (training) and target (testing) speech signals in cross-corpus SER tasks belong to different corpora, which is quite different from the conventional SER setting. Moreover, only the emotion labels of the training speech signals are given, while the testing ones are completely unavailable. Consequently, cross-corpus SER is obviously a more challenging task than the conventional one, and therefore the performance of these originally wellperforming SER methods may decrease sharply, which does not satisfy the requirements of practical applications.\n\nResearchers have been aware of this issue in SER and have tried to address it from different angles. For example, Schuller et al.  [11]  proposed three different feature normalization strategies, including speaker normalization (SN), corpus normalization (CN), and speaker-corpus normalization (SCN), to overcome the issue of corpus variance in cross-corpus SER tasks. On the other hand, some researchers found that crosscorpus SER tasks can be seen as a typical unsupervised domain adaptation (DA) problem  [12] ,  [13] . Therefore, ideas widely used in the research of unsupervised DA have been employed to develop subspace or deep learning-based DA methods to alleviate the feature distribution mismatch between the source and target speech signals. For example, Hassan et al.  [14]  proposed an importance-weighted support vector machine (IW-SVM) to address the cross-corpus SER problem. They applied the importance weighted transfer learning method, e.g., kernel mean matching (KMM)  [15] , to narrow down the maximum mean discrepancy (MMD)  [16]  distance between the weighted source and target speech feature sets. In the work of  [17] , Zhang et al. presented a novel unsupervised DA framework, called joint distribution adaptive regression (JDAR), for crosscorpus SER. JDAR aims to find a corpus-invariant common subspace to bridge samples from different speech emotion corpora by minmizing the first-order statistical moment, i.e., mean value, of the marginal and conditional feature distributions of source and target speech samples. Recently, Zhao et al.  [18]  proposed a novel deep unsupervised DA method called deep transductive transfer regression networks (DTTRN), which includes a major module based on multi-kernel MMD for adapting source and target speech feature distributions.\n\nFrom the aforementioned works, it is evident that these methods can be categorized as Explicit Methods for addressing the issue of cross-corpus SER. This categorization is attributed to the assumption underlying their design, which considers the distributions of the source and target speech feature sets as explicitly pre-defined, e.g., following a Gaussian distribution. Consequently, appropriate statistical moments such as the mean value can be used to characterize the feature distribution in finite or kernel-induced infinite subspaces. The aim is then to minimize the distance between these distributions and thereby mitigate the mismatch between the distributions of source and target speech corpora. However, it is important to note that this assumption of speech samples being distributed following the assumed feature distributions may not hold true in reality. As a result, the chosen statistical moments can not accurately capture the true feature distribution. For this reason, these explicit methods, which aims to minimize the distance between statistical moments, such as MMD, may fail to effectively bridge the gap in feature distribution between the source and target speech corpora.\n\nTo overcome the limitations of the explicit methods, researchers have attempted to tackle the issue of cross-corpus SER from a different perspective, which can be summarized as Implicit Methods. Their aim is to bridge the gap between the feature distributions of source and target speech signals without relying on specific statistical moments to describe the pre-assumed distribution. One noteworthy example is the work of Abdelwahab and Busso on adversarial multitask learning for cross-corpus SER  [19] . In this work, they introduce an adversarial multitask learning framework to acquire corpus-invariant common representations of emotional speech signals across different speech corpora, rather than first assuming the speech feature distributions and then calculating statistical moment distances. More specifically, to prevent the model from being aware of corpus variance in cross-corpus SER, an additional module called the domain (corpus) classifier is incorporated to alternatively train with the emotion classifier. Subsequent to the success of Abdelwahab and Busso's pioneering work, several variants of adversarial learning models have emerged in recent years for cross-corpus SER tasks. Representative examples of such approaches include adversarial discriminative domain generalization (ADDoG)  [20] , self-supervised adversarial dual discriminator (sADDi)  [21] , and adversarial domain generalized Transformer (ADoGT)  [22] . These models have showcased the potential of the adversarial learning framework in capturing the underlying latent representations shared across different speech corpora, surpassing the performance of explicit methods.\n\nInspired by the success of the adversarial learning-based methods, our previsous work also focused on the research of implicit cross-corpus SER methods and proposed a novel implicit deep unsupervised DA model called deep implicit distribution alignment networks (DIDAN), which was published in ICASSP 2023  [23] . The main contribution of the proposed DIDAN was the design of an implicit distribution alignment (IDA) module guided by the idea of target sample reconstruction, distinguishing it from adversarial learning methods. This module aimed to bridge the feature distribution gap between source and target speech samples without assuming specific feature distributions. In this paper, we extend our previous work, DIDAN  [23] , to a more effective implicit model called layer-adapted implicit distribution alignment networks (LI-DAN) for cross-corpus SER. Compared with DIDAN, we introduce a multi-layer version of IDA called layer-adapted IDA (LIDA) for LIDAN, which is comprised of three different modified IDA terms. These terms are deliberately organized across various fully connected layers in LIDAN, in accordance with the enhanced emotion-discriminative capabilities as the layer depth increases. With this enhancement, LIDAN achieves a more effective alleviation of feature distribution mismatch between different speech corpora while simultaneously learning emotion-discriminative and corpus-invariant features for cross-corpus SER. In summary, compared with our conference work, the additional contributions of this paper are three-fold: The remainder of this paper is organized as follows: Section II provides a review of our previous work on DIDAN for cross-corpus SER and subsequently presents the extension from DIDAN to LIDAN. In Section III, we conduct extensive experiments to evaluate the performance of the proposed LIDAN in tackling cross-corpus SER tasks. Finally, our work is concluded in Section IV.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Proposed Method",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "A. Preliminary",
      "text": "In this section, we will provide a detailed explanation of how to extend DIDAN to LIDAN and demonstrate how LIDAN can be utilized to address the problem of crosscorpus SER. To begin with, suppose we have a source speech emotion corpus consisting of N s samples, denoted as D s = {(X s i , y s i )} Ns i=1 , where X s i is the Mel-spectrum pseudo image of the i th speech sample and y s i ∈ R C×1 is its corresponding one-hot label vector generated based on the emotion ground truth, respectively. Herein, C represents the number of emotions involved in the task of cross-corpus SER. Similarly, the unlabeled speech samples from the target corpus can be denoted as D t = {X t i } Nt i=1 , with X t i representing the Mel-spectrum pseudo image of the i th target speech sample and N t representing the total number of target speech samples.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "B. Building Didan",
      "text": "As LIDAN is based on DIDAN, we will first provide a brief overview of DIDAN  [23] . To illustrate its fundamental idea and network structure, we have re-drawn the overall picture of DIDAN in Fig.  1 (a) . DIDAN comprises two main modules: the Deep Regression (DR) and the Implicit Distribution Alignment (IDA). The DR module is composed of convolutional and fully connected layers. Its purpose is to establish a connection between the source speech Melspectrograms and their corresponding emotion labels, enabling DIDAN to possess emotion-discriminative capabilities. In this module, we use f to represent the combination operations of convolution and full connection, which are responsible for feature learning. Additionally, g represents the full connection and softmax operations, which serve as the emotion classifier in DIDAN. The objective function for the DR module is denoted as L dr and is defined as follows:\n\nwhere J(•) denotes the cross-entropy loss function. It is clear to see that by feeding the source speech samples into the DR module and minimizing the aforementioned loss function, the DIDAN can DIDAN can progressively learn to differentiate between various emotional speech signals.\n\nThe second module is the IDA, which aims to enable DI-DAN to recognize emotions in target speech signals, regardless of the differences between the source and target corpora. Unlike MMD-based methods that focus on measuring and reducing the gap of pre-assumed feature distributions explicitly, the IDA module adopts a target sample reconstruction approach. This approach encourages each target speech feature learned in the last fully connected layer in f to resemble the corresponding source features. Consequently, the DIDAN model is also required to train by minimizing the following loss function:\n\nIn this equation,\n\nwith respect to the reconstruction coefficient matrix. By minimizing this norm, DIDAN learns a sparse w i , which means that only a few source samples are involved in reconstructing the i th target sample. Additionally, α is a trade-off parameter to control the spasity of the learned reconstruction coefficient matrix.\n\nFinally, by combining Eqs. (  1 ) and (  2 ), the total loss function for learning DIDAN is obtained. The corresponding optimization problem can be formulated as follows:\n\nwhere θ f and θ g represent the network parameters corresponding to the operations of f and g, respectively, and λ is the trade-off parameter that balances the losses corresponding to DR and IDA modules.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "C. From Didan To Lidan",
      "text": "To illustrate the concept of LIDAN and highlight its distinctions from DIDAN, we present the overall structure of LIDAN in Fig.  1 (b ). Similar to DIDAN, LIDAN comprises two key components: Deep Regression (DR) and Layer-Adapted IDA (LIDA). These components correspond to two well-crafted loss functions, denoted as L dr and L lida .\n\n1) Deep Regression: In the case of LIDAN, we rewrite the formulation of L dr in DR module as follows, which allows us to conveniently describe L lida subsequently:\n\nwhere f 1 , f 2 , and f 3 represent the operations performed by convolutional and full connected layers in LIDAN, respectively. From Fig.  1 , it can be observed that their combination,\n\n2) Layer-Adapted IDA: The primary enhancement in LI-DAN is the modification of the IDA module into the layeradapted multi-layer version, LIDA. This updated version takes advantage of the hierarchical structure inherent in deep neural networks to address differences in feature distribution across different speech corpora layer by layer. LIDA also incorporates the concept of joint distribution adaptation (JDA)  [24]  from domain adaptation. Specifically, JDA introduces conditional probability distributions to estimate the distribution gap, resulting in more effective distribution alignment compared to marginal distributions. However, it requires additional steps for accurately predicting labels of originally unlabeled target samples to describe their conditional probability distribution, heavily relying on the discriminative abilities of learned features in the corresponding layers.\n\nUnder these considerations, we introduce two conditional modifications of IDA alongside the original IDA to create the LIDA module for LIDAN. These IDA modifications are thoughtfully integrated into different fully connected layers to align with the hierarchical learning nature of deep neural networks, where features become increasingly discriminative from shallow to deep layers. Following this idea, the loss function of LIDA, L lida , is designed as follows:\n\nAs shown in Eq. (  5 ), L lida is comprised of three distinct modified IDA losses: L lidam , L lida (cc) , and L lida (f c) . These losses correspond to different fully connected layers within the network, ranging from shallow to deep, as shown in Fig.  1  (b ). The first component, L lidam , can be referred to as Marginal Distribution Guided IDA. It is formulated as:\n\nwhere\n\nand W m ∈ R Ns×Nt represents the target sample reconstruction coefficient matrix. It is important to note that L lidam is identical to the IDA loss used in DIDAN. This design is made based on the consideration that shallow layers in deep neural networks usually have less discriminative features. Therefore, we have shifted the IDA constraint from the deep layer to the shallow layer of LIDAN, as it does not require any emotion information from unlabeled target speech samples.\n\nThe remaining two components, L lida (cc) and L lida (f c) , are conditional modifications of IDA. They can be referred to as the Coarse-grained Emotion Label Aware Conditional Distribution Guided IDA and Fine-grained Emotion Label Aware Conditional Distribution Guided IDA, respectively. As illustrated in Fig.  1 (b ), these components correspond to the deeper fully connected layers in LIDAN. Their objective is to utilize coarse-grained and fine-grained emotion labels to guide the alignment of conditional feature distributions between the source and target speech samples in their respective layers.\n\nSpecifically, L lida (cc) corresponds to the first deeper layer. It is worth noting that although the discriminative ability of the features learned in this layer experiences a moderate increase compared to the previous layer, it is not yet sufficiently powerful. Therefore, it is beneficial to alleviate the difficulty of predicting the emotion labels of target samples, which is required for aligning conditional feature distributions. To achieve this goal, each speech sample from both the source and target corpora is assigned a coarse-grained emotion label by combining multiple original emotion labels into a single label. As a result, with the features learned in this layer, predicting the coarse-grained emotion labels of target speech samples becomes notably easier compared to predicting the original fine-grained labels. This approach provides a suitable means of aligning conditional feature distributions in this layer for the cross-corpus SER problem. Under this consideration, we modify the original IDA as follows to serve as L lida (cc) :\n\nIn Eq. (7), X\n\n)),\n\n))] represent the source and target speech samples with the i th\n\nis the corresponding target sample reconstruction coefficient matrix. Here, N s(i) and N t(i) , satisfy\n\nrepresenting the number of source and target speech samples belonging to the i th coarse-grained emotion label. Moreover, C cc refers to the number of coarse-grained emotion classes.\n\nL lida (f c) corresponds to the second deeper fully connected layer, which is positioned close to the prediction layer, in LIDAN. In comparison to the previous layers, this layer undoubtedly learns more distriminative features, enabling LI-DAN to effectively align fine-grained emotion label guided conditional feature distributions in this layer. Therefore, we modify L lida (f c) based on the original IDA as follows:\n\nwhere X\n\nand X\n\ndenote the learned features of source and target speech samples in this layer. They can be represented as X\n\nand\n\nrepresents the target sample reconstruction coefficient matrix associated with the source and target speech samples belonging to the i th (i = {1, • • • , C}) fine-grained emotion label (original emotion label).\n\n3) Optimization Problem: Similar to DIDAN, we combine the DR and LIDA losses in Eqs. (  4 ) and (  5 ) into the objective function of the proposed LIDAN. Accordingly, the corresponding optimization problem can be formulated as follows:\n\nwhere λ is the trade-off parameter to control the balance between DR and Layer-Adated losses, θ f1 , θ f2 , θ f3 , and θ g are the parameters associated with the operations performed by the layers, f 1 , f 2 , f 3 , and g, and y t i corresponds to the pseudo emotion label of i th target speech sample, respectively.\n\nIt is worth noting that the emotion information of target speech samples are unlabeled in the task of cross-corpus SER. Therefore, we use their pseudo emotion labels {y t i } to serve as the model parameters to learn in the optimization of the proposed LIDAN.\n\nf2 , θ\n\ng }, and iteration index k = 0, 2: for k in K Iter do 3:\n\nCalculate the totoal loss function\n\nlida according to target pseudo emotion labels\n\n} Ccc i=1 , and Eqs. (  4 ), (  6 ),  (7) , and (  8 ), and update model parameters:\n\nUpdate target sample reconstruction matrices:\n\n10:\n\n11:\n\nUpdate\n\n13: k = k + 1; 14: end for",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "D. Optimization Of Lidan",
      "text": "The optimization problem of LIDAN can be efficiently solved by using the alternated direction method, i.e., alternatively updating the model parameters until convergence. Specifically, we initialize pseudo emotion labels, y t i 's, for target speech samples and then repeat the following three steps:\n\n1) Fix the target pseudo emotion labels {y t i }, and update the network parameters {θ f1 , θ f2 , θ f3 , θ g } and target sample reconstruction coefficient matrices",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Iii. Experiments",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "A. Speech Emotion Corpora And Experiment Protocol",
      "text": "In this section, we conduct extensive cross-corpus SER experiments to evaluate the proposed LIDAN model. To this end, we employ three widely-used speech emotion corpora: EmoDB  [25] , eNTERFACE  [26] , and CASIA  [27] . EmoDB is a German speech emotion corpus that consists of 535 audio samples from 10 professional German actors/actresses (five females and five males). Each speech sample was recorded when the corresponding actor or actress spoke a sentence expressing one of seven emotional states: Anger, Boredom, Disgust, Fear, Happiness, Neutral, and Sadness. eNTERFACE is an English bimodal emotion database that contains 1,257 video clips representing six basic emotions: Anger, Disgust, Fear, Happiness, Sadness, and Surprise. For our experiments, we extract the audio data from eNTERFACE to design crosscorpus SER tasks. CASIA is a Chinese speech emotion corpus comprising four speakers, including two males and two females. It involves the collection of 1,200 speech samples, with each speaker required to speak 300 sentences in Chinese. The corpus covers six different emotions: Anger, Fear, Happiness, Neutral, Sadness, and Surprise.\n\nBy using two of these three speech emotion corpora to alternatively serve as the source and target ones, we establish six cross-corpus SER tasks denoted as B → E, E → B, B → C, C → B, E → C, and C → E, respectively. Herein, B, E, and C represent EmoDB, eNTERFACE, and CASIA corpora, respectively. The left and right corpora in the arrow correspond to the source and target corpora for their respective crosscorpus SER task. As these corpora lack completely consistent emotion labels, we only select speech samples with matching emotion labels for each task. To provide readers with a detailed understanding of these cross-corpus SER tasks, we present the statistical information of the data used in our experiments in Table  I . Regarding the performance metric, we choose the unweighted average recall (UAR)  [11] , which is defined as the average accuracy across all emotion classes. Specifically, UAR is calculated as UAR\n\n, where C represents the total number of emotion classes, and N p i and N g i are the numbers of samples predicted as and belonging to the i th emotion, respectively.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "B. Comparison Methods And Implimentation Details",
      "text": "To evaluate the effectiveness of our proposed LIDAN method in addressing the issue of cross-corpus SER, we conduct a comparative study involving recent state-of-theart unsupervised DA methods. Specifically, we compare our method with five Subspace Learning: transfer component analysis (TCA)  [28] , geodesic flow kernel (GFK)  [29] , subspace alignment (SA)  [30] , domain-adaptive subspace learning (DoSL)  [31] , and joint distribution adaptive regression (JDAR)  [17] . Additionally, we evaluate our LIDAN against six Deep Learning approaches: deep adaptation network (DAN)  [32] , joint adaptation network (JAN)  [33] , deep subdomain adaptation network (DSAN)  [34] , domain-adversarial neural network (DANN)  [35] , conditional domain adversarial network (CDAN)  [36] , and DIDAN (the conference version of LIDAN)  [23] .\n\nSince the subspace learning methods require the feature set to describe the speech signals before adaptation, we choose two widely-used speech feature sets in the experiments, i.e., the feature set provided by the INTERSPEECH 2009 Emotion Challenge (IS09)  [37]  and the extended Geneva minimalistic acoustic parameter set (eGeMAPS)  [38] . The IS09 feature set consists of 384 elements derived from 16 low-level descriptors (LLDs) such as fundamental frequency (F0), harmonics-tonoise ratio (HNR), and Mel-frequency cepstral coefficients (MFCC) and their one-order differences through 12 statistical functions, e.g., mean, maximal, and minimal values. The eGeMAPS feature set is comprised of 25 LLDs, e.g., pitch, shimmer, Alpha ratio, and rate of loudness peaks, and 10 functions yeilding 88 parameters eventually. These two feature sets can be conveniently extracted from the speech signals by using the openSMILE toolkit  [39] .\n\nWhile for the deep learning comparison methods, VGG-11  [40]  is chosen as the CNN backbone and the Melspectrograms of speech signals are used to serve as their input. Hence, we subsequently resize the images of Melspectrograms at a size of 224 × 224 pixels. It is also noted that since the label information is entirely unavailable in the tasks of cross-corpus SER, in the experiments we follow the tradition of unsupervised DA evaluation and report their best results by searching the trade-off parameters from a given interval. Linear support vector machine (SVM)  [41]  is chosen with the parameter C = 1 for all the subspace learning methods without classification ability including TCA, GFK, and SA. In addition, we also directly use the SVM and VGG-11 to respectively conduct all the cross-corpus SER experiments as the baselines. In summary, the trade-off parameters for all the unsupervised DA methods are set in the experiments as follows: 1) TCA, GFK, and SA: These three methods aim to seek a d-dimensional subspace to relieve the feature distribution mismatch between the source and target speech samples. Hence, in the experiments, d is fixed at the one selected from a parameter interval, [5 : 5 : d max ], where d max is the element number of the feature set used in the experiments.\n\n2) DoSL and JDAR: In these three methods, two trade-off parameters need to be set, i.e., λ and µ, respectively controlling the balance between the sparsity and feature distribution elimination terms and the original regression loss function. λ and µ are set by searching from  [5 : 5 : 200 ] throughout all the experiments.\n\n3) DAN, JAN, DSAN, DANN, and CDAN: One trade-off parameter λ exists in the objective function to control the balance between the oringinal loss function and the domain adaptation term. In the experiments, we search it from an elaborate parameter interval [0.0001 : 0.0001 : 0.001, 0.002 : 0.001 : 0.01, 0.02 : 0.01 : 0.1, 0.2 : 0.1 : 1, 2, 5, 10, 100].",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "4) Didan And Lidan:",
      "text": "Besides λ, DIDAN and LIDAN also introduce a new trade-off parameter α to control the sparsity of the target sample reconstruction coefficient matrix. In our experiments, the parameter ranges for both λ and α are set similar to other deep learning comparison methods. Specifically, we use the intervals [0.0001 : 0.0001 : 0.001, 0.002 : 0.001 : 0.01, 0.02 : 0.01 : 0.1, 0.2 : 0.1 : 1, 2, 5, 10, 100]. In addition, LIDAN also requires the combination of finegrained emotion labels into coarse-grained emotion labels to calculate the Coarse-grained Emotion Label Aware Conditional Distribution Guided IDA (L lida (cc) ). In our experiments, we divide the fine emotion labels originally provided by the speech corpora into two groups based on their valence, inspired by the distribution of various emotions on recently designed arousal-valence emotion wheels  [42] -  [44] . The two groups are referred to as Positive, which includes Surprise, Happiness, and Neutral, and Negative, which includes Anger, Disgust, Fear, and Sadness. These groups serve as the coarsegrained emotion labels in our experiments.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "C. Comparison With State-Of-The-Art Subspace Learning-Based Unsupervised Da Methods",
      "text": "We begin by comparing the performance of our LIDAN method with recent state-of-the-art subspace learning-based unsupervised DA methods in coping with the tasks of crosscorpus SER. The results are presented in Table  II . From the table, several interesting observations and conclusions can be drawn.\n\nFirstly, our proposed LIDAN method achieved the highest average UAR among all the methods, reaching 41.23%. LI-DAN also outperformed all the comparison methods in three out of the six cross-corpus SER tasks: E → B (47.04%), C → B (58.06%), and C → E (34.97%). While LIDAN did not achieve the best performance in the remaining three tasks, it still demonstrated high competitiveness against the best-performing methods. For example, in the task, B → E, LIDAN achieved a UAR of 34.44% compared to 36.33% obtained by JDAR + IS09. These observations demonstrate the effectiveness and overall superior performance of our proposed LIDAN in addressing the challenge of cross-corpus SER.\n\nSecondly, it is worth noting that subspace learning methods utilizing the eGeMAPS feature set to describe speech signals attained promisingly better results than our LIDAN in coping with the cross-corpus SER task, E → C. In this case, all the subspace learning methods, including the baseline SVM without any DA, outperformed LIDAN with an increase in UAR of at least 2.52%. This could be attributed to the fact that the hand-crafted speech feature set used in subspace learning methods, namely eGeMAPS, is more emotion-discriminative and corpus-invariant compared to the deep features learned directly from speech Mel-spectrum images using the VGG-11 backbone in LIDAN. Further comparison results of these DA methods in the other task involving the same speech corpora, i.e., C → E, confirm that these DA methods generally exhibit competitive performance against our LIDAN, thereby supporting our explanations and analysis.\n\nLast but not least, it is interesting to observe that all subspace learning methods utilizing the eGeMAPS feature In addition to the comparison with subspace learning methods, we also compared our LIDAN with recent state-of-the-art deep learning-based unsupervised DA methods in Table  III . It can be observed that LIDAN achieved the highest average UAR among all the deep learning methods for recognizing emotions from speech signals across different corpora, surpassing them in five out of six cross-corpus SER tasks. Even for the task of E → B, there is only a negligible difference in performance between LIDAN (47.04%) and the best-performing comparison method, DSAN (47.58%). These results further demonstrate the effectiveness and superior performance of LIDAN compared to recent state-of-the-art deep learning methods, consistent with the comparisons made with the aforementioned subspace learning methods.\n\nFurthermore, we also highlighted the alignment types used in all deep learning methods. It can be clearly observed from Table III that both the proposed LIDAN and its origin, DIDAN, achieved more promising performance than all the explicit methods that adopt MMD and its variants to measure the feature distribution gap between the source and target speech samples. Moreover, in contrast to current widely-used adversarial learning-based implicit methods, our LIDAN and DIDAN exhibit superior performance as well, resulting in a remarkable UAR increase. These comparisons validate the effectiveness and superiority of the proposed implicit feature distribution alignment idea, namely target sample reconstruc-tion, utilized in LIDAN and DIDAN in bridging the feature distribution gap across the speech emotion corpora.\n\nAdditionally, we conducted a comparison between LIDAN and DIDAN. Table  III  demonstrates that LIDAN achieved an average UAR of 41.23% across all six cross-corpus SER tasks, while DIDAN only achieved 40.07%. In particular, LIDAN outperformed DIDAN in four out of the six SER tasks, showcasing a satisfactory performance increase. Moreover, even in the remaining two tasks, the performance of LIDAN only slightly dropped when compared to DIDAN. Note that the major difference between LIDAN and DIDAN can be observed in Eq. (  3 ) and (  9 ), where LIDAN extended the original IDA utilized in DIDAN to a layer-adapted version, LIDA. These comparative findings strongly support the effectiveness and feasibility of extending IDA to LIDA as evidence of its enhanced capability in addressing the challenge of crosscorpus SER compared to the original IDA.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "E. Going Deeper Into Layer-Adapted Ida In Lidan",
      "text": "As mentioned previously, the main contribution of our LIDAN method lies in the introduction of LIDA, which builds upon the IDA utilized in DIDAN. LIDA aims to further enhance the robustness of DIDAN to corpus variance by incorporating emotion class aware conditional IDA terms alongside the original IDA. These terms are strategically incorporated into the fully connected layers, with a progressive integration from shallow to deep layers. In this subsection, we select three representative cross-corpus SER tasks, namely, B → E, E → B, and B → C, to conduct additional experiments. The objective is to address the three key questions regarding LIDA, allowing readers to gain a deeper understanding of this well-designed module in LIDAN.\n\n1) Are All the Modified Terms Necessary for Layer-Adapted IDA of LIDAN?: The LIDA module, as illustrated in Eq. (  5 ), consists of three modified IDA terms: L idam , L ida (cc) , and L ida (f c) . Among them, L idam follows a similar formulation to the original IDA, while L ida (cc) and L ida (f c) are newlyintroduced modifications that take conditional distributions into account. It is natural to question whether all three modified IDA terms are necessary for improving the performance of LIDAN in tackling the tasks of cross-corpus SER. To address this concern, we conducted ablation experiments on three representative cross-corpus SER tasks. Specifically, we   6 ),  (7) , and (8), three modified IDA terms: L idam , L ida (cc) , and L ida (f c) in Layer-Adapted IDA module make use of varying levels of emotion labels, including no, coarse-grained, and finegrained labels. Consequently, the arrangement of these terms in LIDAN follows a progression from shallow to deep fully connected layers, where the features learned by these layers exhibit increasingly discriminative abilities for emotions with respect to the layer depth. This raises the question of whether aligning the arrangement of these terms with the emotiondiscriminative abilities of layers, based on their requirement of emotion label types, is a suitable choice. To investigate this point, we conducted experiments in LIDAN by exploring all possible permutations for these three terms. The experimental results are presented in Table  V . From the table, it is evident that the current arrangement of these modified IDA terms in LIDAN achieved the highest UAR performance among all possible permutations. This demonstrates the effectiveness and suitability of our designed arrangement for the LIDA module in LIDAN.\n\n3) Is \"Sparsely\" Necessary for Target Sample Reconstruction in LIDAN?: Similar to IDA module in DIDAN, the LIDA module in LIDAN utilizes the approach of target sample reconstruction to address the feature distribution mismatch between the source and target speech signals. It is worth noting that the LIDA module incorporates a sparse regularization term, as shown in Eqs. (  6 ),  (7) , and  (8) , which encourages LIDAN to select a few source samples for reconstructing the target samples. This raises another question of whether the sparse regularization term is necessary for achieving target sample reconstruction. In order to explore this further, we conducted cross-corpus SER experiments by excluding the sparse regularization term from the total loss function of both DIDAN and LIDAN. The experimental results, shown in Table  VI , demonstrate that the inclusion of the sparse regularization term leads to a significant performance improvement for both models compared to their non-sparse versions. These findings confirm the effectiveness of incorporating the sparse regularization term to guide the learning of the target reconstruction coefficient matrix in DIDAN and LIDAN.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "F. Sensitivity Analysis Of Trade-Off Parameters In Lidan",
      "text": "The proposed LIDAN method involves two crucial tradeoff parameters: λ and α. The selection of these parameters directly impacts the performance of LIDAN. In our experiments, we utilized fixed values for these parameters, which prompted concerns regarding the sensitivity of LIDAN's performance to changes in their values. To investigate this, we conducted experiments on three previously used crosscorpus SER tasks: B → E, E → B, and B → C. In these experiments, we alternatively held one parameter constant while varying the other. The intervals for varied λ and α were set as [0.0001, 0.01, 0.1, 1, 10, 100]. The constant values for λ and α matched those used in the experiments described in Section III-D. Fig.  2  presents the experimental results. As depicted in the figure, it is clear that the performance shows less variation with respect to changes in λ and α for all three experiments. This suggests that our proposed LIDAN method is relatively insensitive to changes in its trade-off parameters.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Iv. Conclusion",
      "text": "In this paper, we focus on the research of cross-corpus SER from the perspective of implicit distribution alignment. We have proposed a novel deep learning-based unsupervised DA method called LIDAN by building upon our previous work at ICASSP, DIDAN. The key improvement of LIDAN compared to DIDAN lies in the design of an extended IDA called LIDA. This allows LIDAN to implicitly and effectively align feature distributions of source and target speech samples across multiple layers, considering the emotion-discriminative capability of each layer's features. As a result, LIDAN is more advantageous in learning both emotion-discriminative and corpus-invariant features for cross-corpus SER, outperforming DIDAN. We evaluated the performance of LIDAN through extensive cross-corpus SER experiments on the EmoDB, eN-TERFACE, and CASIA corpora. The experimental results demonstrate that LIDAN exceeds recent state-of-the-art methods based on unsupervised DA using subspace learning and deep learning techniques in tackling cross-corpus SER tasks. Notably, our work, including DIDAN and LIDAN, is the first to highlight the distinction between implicit and explicit approaches in cross-corpus SER methods.",
      "page_start": 9,
      "page_end": 9
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Overview Structures of (a) DIDAN [23] and Its Extended Version, (b) the proposed LIDAN, for Addressing the Issue of Cross-Corpus SER.",
      "page": 3
    },
    {
      "caption": "Figure 1: (a). DIDAN comprises two",
      "page": 3
    },
    {
      "caption": "Figure 1: (b). Similar to DIDAN, LIDAN comprises two key",
      "page": 4
    },
    {
      "caption": "Figure 1: , it can be observed that their combination,",
      "page": 4
    },
    {
      "caption": "Figure 1: (b), these components correspond to the",
      "page": 4
    },
    {
      "caption": "Figure 2: presents the experimental results. As",
      "page": 9
    },
    {
      "caption": "Figure 2: Results of trade-off parameter sensitivity analysis experiments for the proposed LIDAN method. (a) illustrates the outcomes obtained by varying the",
      "page": 10
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "B→E\nE→B": "B→C\nC→B",
          "EmoDB (Anger: 127, Fear: 69, Disgust: 46, Happiness: 71, Sadness: 62)\neNTERFACE (Anger: 211, Fear: 211, Disgust: 211, Happiness: 208, Sadness: 211)": "EmoDB (Anger: 127, Fear: 69, Neutral: 79, Happiness: 71, Sadness: 62)\nCASIA (Anger: 200, Fear: 200, Neutral: 200, Happiness: 200, Sadness: 200)",
          "375\n1052": "408\n1000"
        },
        {
          "B→E\nE→B": "E→C\nC→E",
          "EmoDB (Anger: 127, Fear: 69, Disgust: 46, Happiness: 71, Sadness: 62)\neNTERFACE (Anger: 211, Fear: 211, Disgust: 211, Happiness: 208, Sadness: 211)": "eNTERFACE (Anger: 211, Fear: 211, Happiness: 208, Sadness: 211, Surprise: 211)\nCASIA (Anger: 200, Fear: 200, Happiness: 200, Sadness: 200, Surprise: 200)",
          "375\n1052": "1052\n1000"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Subspace Learning\n(IS09 Feature Set)": "Subspace Learning\n(eGeMAPS Feature Set)",
          "SVM\nTCA\nGFK\nSA\nDoSL\nJDAR": "SVM\nTCA\nGFK\nSA\nDoSL\nJDAR",
          "-\nExplicit\nExplicit\nExplicit\nExplicit\nExplicit": "-\nExplicit\nExplicit\nExplicit\nExplicit\nExplicit",
          "28.93\n23.58\n29.60\n35.01\n26.10\n25.14\n30.52\n44.03\n33.40\n45.07\n31.10\n32.32\n32.11\n42.48\n33.10\n48.08\n32.80\n28.13\n33.50\n43.89\n35.80\n49.03\n32.60\n28.17\n36.12\n38.95\n34.40\n45.75\n30.40\n31.59\n36.33\n39.97\n31.10\n46.29\n32.40\n31.50": "26.65\n32.58\n33.50\n51.84\n36.40\n34.79\n42.90\n40.80\n28.55\n33.29\n49.59\n34.95\n29.42\n33.86\n36.20\n50.13\n38.50\n34.00\n32.11\n36.78\n34.80\n52.00\n37.00\n35.32\n30.52\n40.17\n34.40\n52.05\n38.30\n32.92\n31.00\n43.48\n38.60\n55.11\n38.30\n32.89",
          "28.06\n36.07\n36.17\n36.33\n36.20\n36.27": "35.96\n38.35\n37.02\n38.00\n38.06\n39.90"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Deep Learning": "",
          "VGG-11\nDAN\nJAN\nDSAN\nDANN\nCDAN": "DIDAN (Ours)\nLIDAN (Ours)",
          "-\nExplicit\nExplicit\nExplicit\nImplicit\nImplicit": "Implicit\nImplicit",
          "27.08\n34.83\n34.80\n51.31\n26.90\n26.02\n33.58\n43.50\n36.30\n56.72\n29.30\n32.17\n35.23\n47.29\n37.00\n57.51\n31.00\n32.21\n47.58\n31.82\n35.80\n56.50\n29.00\n31.25\n32.56\n46.06\n36.40\n57.67\n30.50\n33.77\n31.62\n46.12\n35.40\n57.60\n30.30\n33.49": "33.05\n47.11\n38.90\n56.22\n31.10\n34.06\n34.44\n39.00\n58.06\n33.88\n34.97\n47.04",
          "33.49\n38.60\n40.04\n38.66\n39.49\n39.09": "40.07\n41.23"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "1\n2\n2\n3\n3": "1",
          "3\n1\n3\n1\n2": "2",
          "2\n3\n1\n2\n1": "3",
          "32.55\n43.25\n37.40\n32.95\n42.86\n38.70\n32.26\n42.67\n36.80\n33.36\n42.60\n37.60\n31.84\n41.26\n36.80": "34.44\n47.07\n39.00"
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "✗\n✓\n✗\n✗\n✗\n✓\n✓": "✓",
          "✗\n✗\n✓\n✗\n✓\n✗\n✓": "✓",
          "✗\n✗\n✗\n✓\n✓\n✓\n✗": "✓",
          "33.58\n43.50\n36.30\n31.22\n42.69\n35.90\n31.35\n43.56\n35.60\n31.20\n41.18\n35.80\n32.16\n42.83\n36.30\n30.68\n41.00\n37.50\n31.86\n41.01\n36.80": "34.44\n47.07\n39.00"
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "VGG-11": "DIDAN w nonSparse IDA\nDIDAN w Sparse IDA",
          "33.58\n43.50\n36.30": "31.83\n41.69\n37.70\n33.05\n47.11\n38.90"
        },
        {
          "VGG-11": "LIDAN w nonSparse Layer-Adapted IDA\nLIDAN w Sparse Layer-Adapted IDA",
          "33.58\n43.50\n36.30": "32.48\n42.32\n34.50\n34.44\n47.07\n39.00"
        }
      ],
      "page": 9
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Survey on speech emotion recognition: Features, classification schemes, and databases",
      "authors": [
        "M Ayadi",
        "M Kamel",
        "F Karray"
      ],
      "year": "2011",
      "venue": "Pattern Recognition"
    },
    {
      "citation_id": "2",
      "title": "Speech emotion recognition: Emotional models, databases, features, preprocessing methods, supporting modalities, and classifiers",
      "authors": [
        "M Akc",
        "K Oguz"
      ],
      "year": "2020",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "3",
      "title": "Emotions and speech: Some acoustical correlates",
      "authors": [
        "C Williams",
        "K Stevens"
      ],
      "year": "1972",
      "venue": "The journal of the acoustical society of America"
    },
    {
      "citation_id": "4",
      "title": "Hidden markov model-based speech emotion recognition",
      "authors": [
        "B Schuller",
        "G Rigoll",
        "M Lang"
      ],
      "year": "2003",
      "venue": "2003 IEEE International Conference on Acoustics, Speech, and Signal Processing"
    },
    {
      "citation_id": "5",
      "title": "Automatic speech emotion recognition using modulation spectral features",
      "authors": [
        "S Wu",
        "T Falk",
        "W.-Y Chan"
      ],
      "year": "2011",
      "venue": "Speech communication"
    },
    {
      "citation_id": "6",
      "title": "Speech emotion recognition using cnn",
      "authors": [
        "Z Huang",
        "M Dong",
        "Q Mao",
        "Y Zhan"
      ],
      "year": "2014",
      "venue": "Proceedings of the 22nd ACM international conference on Multimedia"
    },
    {
      "citation_id": "7",
      "title": "Spontaneous speech emotion recognition using multiscale deep convolutional lstm",
      "authors": [
        "S Zhang",
        "X Zhao",
        "Q Tian"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "8",
      "title": "Speech emotion recognition with deep convolutional neural networks",
      "authors": [
        "D Issa",
        "M Demirci",
        "A Yazici"
      ],
      "year": "2020",
      "venue": "Biomedical Signal Processing and Control"
    },
    {
      "citation_id": "9",
      "title": "Domain invariant feature learning for speaker-independent speech emotion recognition",
      "authors": [
        "C Lu",
        "Y Zong",
        "W Zheng",
        "Y Li",
        "C Tang",
        "B Schuller"
      ],
      "year": "2022",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "10",
      "title": "Speech emotion recognition via an attentive time-frequency neural network",
      "authors": [
        "C Lu",
        "W Zheng",
        "H Lian",
        "Y Zong",
        "C Tang",
        "S Li",
        "Y Zhao"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Computational Social Systems"
    },
    {
      "citation_id": "11",
      "title": "Cross-corpus acoustic emotion recognition: Variances and strategies",
      "authors": [
        "B Schuller",
        "B Vlasenko",
        "F Eyben",
        "M Wöllmer",
        "A Stuhlsatz",
        "A Wendemuth",
        "G Rigoll"
      ],
      "year": "2010",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "12",
      "title": "Deep visual domain adaptation: A survey",
      "authors": [
        "M Wang",
        "W Deng"
      ],
      "year": "2018",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "13",
      "title": "A comprehensive survey on transfer learning",
      "authors": [
        "F Zhuang",
        "Z Qi",
        "K Duan",
        "D Xi",
        "Y Zhu",
        "H Zhu",
        "H Xiong",
        "Q He"
      ],
      "year": "2020",
      "venue": "Proceedings of the IEEE"
    },
    {
      "citation_id": "14",
      "title": "On acoustic emotion recognition: compensating for covariate shift",
      "authors": [
        "A Hassan",
        "R Damper",
        "M Niranjan"
      ],
      "year": "2013",
      "venue": "IEEE Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "15",
      "title": "Covariate shift by kernel mean matching",
      "authors": [
        "A Gretton",
        "A Smola",
        "J Huang",
        "M Schmittfull",
        "K Borgwardt",
        "B Schölkopf"
      ],
      "year": "2009",
      "venue": "Dataset shift in machine learning"
    },
    {
      "citation_id": "16",
      "title": "Integrating structured biological data by kernel maximum mean discrepancy",
      "authors": [
        "K Borgwardt",
        "A Gretton",
        "M Rasch",
        "H.-P Kriegel",
        "B Schölkopf",
        "A Smola"
      ],
      "year": "2006",
      "venue": "Bioinformatics"
    },
    {
      "citation_id": "17",
      "title": "Cross-corpus speech emotion recognition using joint distribution adaptive regression",
      "authors": [
        "J Zhang",
        "L Jiang",
        "Y Zong",
        "W Zheng",
        "L Zhao"
      ],
      "year": "2021",
      "venue": "ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "18",
      "title": "Deep transductive transfer regression network for cross-corpus speech emotion recognition",
      "authors": [
        "Y Zhao",
        "J Wang",
        "R Ye",
        "Y Zong",
        "W Zheng",
        "L Zhao"
      ],
      "year": "2022",
      "venue": "Proceedings of the INTERSPEECH"
    },
    {
      "citation_id": "19",
      "title": "Domain adversarial for acoustic emotion recognition",
      "authors": [
        "M Abdelwahab",
        "C Busso"
      ],
      "year": "2018",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "20",
      "title": "Improving cross-corpus speech emotion recognition with adversarial discriminative domain generalization (addog)",
      "authors": [
        "J Gideon",
        "M Mcinnis",
        "E Provost"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "21",
      "title": "Self supervised adversarial domain adaptation for cross-corpus and crosslanguage speech emotion recognition",
      "authors": [
        "S Latif",
        "R Rana",
        "S Khalifa",
        "R Jurdak",
        "B Schuller"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "22",
      "title": "Adversarial domain generalized transformer for cross-corpus speech emotion recognition",
      "authors": [
        "Y Gao",
        "L Wang",
        "J Liu",
        "J Dang",
        "S Okada"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "23",
      "title": "Deep implicit distribution alignment networks for cross-corpus speech emotion recognition",
      "authors": [
        "Y Zhao",
        "J Wang",
        "Y Zong",
        "W Zheng",
        "H Lian",
        "L Zhao"
      ],
      "year": "2023",
      "venue": "ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "24",
      "title": "Transfer feature learning with joint distribution adaptation",
      "authors": [
        "M Long",
        "J Wang",
        "G Ding",
        "J Sun",
        "P Yu"
      ],
      "year": "2013",
      "venue": "Proceedings of the IEEE international conference on computer vision"
    },
    {
      "citation_id": "25",
      "title": "A database of german emotional speech",
      "authors": [
        "F Burkhardt",
        "A Paeschke",
        "M Rolfes",
        "W Sendlmeier",
        "B Weiss"
      ],
      "year": "2005",
      "venue": "Interspeech"
    },
    {
      "citation_id": "26",
      "title": "The enterface'05 audiovisual emotion database",
      "authors": [
        "O Martin",
        "I Kotsia",
        "B Macq",
        "I Pitas"
      ],
      "year": "2006",
      "venue": "22nd International Conference on Data Engineering Workshops (ICDEW'06)"
    },
    {
      "citation_id": "27",
      "title": "Design of speech corpus for mandarin text to speech",
      "authors": [
        "J Zhang",
        "H Jia"
      ],
      "year": "2008",
      "venue": "The blizzard challenge 2008 workshop"
    },
    {
      "citation_id": "28",
      "title": "Domain adaptation via transfer component analysis",
      "authors": [
        "S Pan",
        "I Tsang",
        "J Kwok",
        "Q Yang"
      ],
      "year": "2010",
      "venue": "IEEE transactions on neural networks"
    },
    {
      "citation_id": "29",
      "title": "Geodesic flow kernel for unsupervised domain adaptation",
      "authors": [
        "B Gong",
        "Y Shi",
        "F Sha"
      ],
      "year": "2012",
      "venue": "Geodesic flow kernel for unsupervised domain adaptation"
    },
    {
      "citation_id": "30",
      "title": "Unsupervised visual domain adaptation using subspace alignment",
      "authors": [
        "B Fernando",
        "A Habrard",
        "M Sebban",
        "T Tuytelaars"
      ],
      "year": "2013",
      "venue": "Proceedings of the IEEE international conference on computer vision"
    },
    {
      "citation_id": "31",
      "title": "Unsupervised cross-corpus speech emotion recognition using domainadaptive subspace learning",
      "authors": [
        "N Liu",
        "Y Zong",
        "B Zhang",
        "L Liu",
        "J Chen",
        "G Zhao",
        "J Zhu"
      ],
      "year": "2018",
      "venue": "2018 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "32",
      "title": "Learning transferable features with deep adaptation networks",
      "authors": [
        "M Long",
        "Y Cao",
        "J Wang",
        "M Jordan"
      ],
      "year": "2015",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "33",
      "title": "Deep transfer learning with joint adaptation networks",
      "authors": [
        "M Long",
        "H Zhu",
        "J Wang",
        "M Jordan"
      ],
      "year": "2017",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "34",
      "title": "Deep subdomain adaptation network for image classification",
      "authors": [
        "Y Zhu",
        "F Zhuang",
        "J Wang",
        "G Ke",
        "J Chen",
        "J Bian",
        "H Xiong",
        "Q He"
      ],
      "year": "2020",
      "venue": "IEEE transactions on neural networks and learning systems"
    },
    {
      "citation_id": "35",
      "title": "Domain-adversarial neural networks",
      "authors": [
        "H Ajakan",
        "P Germain",
        "H Larochelle",
        "F Laviolette",
        "M Marchand"
      ],
      "year": "2014",
      "venue": "Domain-adversarial neural networks",
      "arxiv": "arXiv:1412.4446"
    },
    {
      "citation_id": "36",
      "title": "Conditional adversarial domain adaptation",
      "authors": [
        "M Long",
        "Z Cao",
        "J Wang",
        "M Jordan"
      ],
      "year": "2018",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "37",
      "title": "The interspeech 2009 emotion challenge",
      "authors": [
        "B Schuller",
        "S Steidl",
        "A Batliner"
      ],
      "year": "2009",
      "venue": "The interspeech 2009 emotion challenge"
    },
    {
      "citation_id": "38",
      "title": "The geneva minimalistic acoustic parameter set (gemaps) for voice research and affective computing",
      "authors": [
        "F Eyben",
        "K Scherer",
        "B Schuller",
        "J Sundberg",
        "E André",
        "C Busso",
        "L Devillers",
        "J Epps",
        "P Laukka",
        "S Narayanan"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "39",
      "title": "Opensmile: the munich versatile and fast open-source audio feature extractor",
      "authors": [
        "F Eyben",
        "M Wöllmer",
        "B Schuller"
      ],
      "year": "2010",
      "venue": "Proceedings of the 18th ACM international conference on Multimedia"
    },
    {
      "citation_id": "40",
      "title": "Very deep convolutional networks for large-scale image recognition",
      "authors": [
        "K Simonyan",
        "A Zisserman"
      ],
      "year": "2014",
      "venue": "Very deep convolutional networks for large-scale image recognition",
      "arxiv": "arXiv:1409.1556"
    },
    {
      "citation_id": "41",
      "title": "Libsvm: a library for support vector machines",
      "authors": [
        "C.-C Chang",
        "C.-J Lin"
      ],
      "year": "2011",
      "venue": "ACM transactions on intelligent systems and technology (TIST)"
    },
    {
      "citation_id": "42",
      "title": "Automatic speech discrete labels to dimensional emotional values conversion method",
      "authors": [
        "S Jing",
        "X Mao",
        "L Chen"
      ],
      "year": "2019",
      "venue": "IET Biometrics"
    },
    {
      "citation_id": "43",
      "title": "Estimation of continuous valence and arousal levels from faces in naturalistic conditions",
      "authors": [
        "A Toisoul",
        "J Kossaifi",
        "A Bulat",
        "G Tzimiropoulos",
        "M Pantic"
      ],
      "year": "2021",
      "venue": "Nature Machine Intelligence"
    },
    {
      "citation_id": "44",
      "title": "Hybrid curriculum learning for emotion recognition in conversation",
      "authors": [
        "L Yang",
        "Y Shen",
        "Y Mao",
        "L Cai"
      ],
      "year": "2022",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    }
  ]
}