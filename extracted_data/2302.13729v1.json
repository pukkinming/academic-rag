{
  "paper_id": "2302.13729v1",
  "title": "Dst: Deformable Speech Transformer For Emotion Recognition",
  "published": "2023-02-27T12:52:23Z",
  "authors": [
    "Weidong Chen",
    "Xiaofen Xing",
    "Xiangmin Xu",
    "Jianxin Pang",
    "Lan Du"
  ],
  "keywords": [
    "speech emotion recognition",
    "deformable network",
    "Transformer",
    "deformable attention mechanism"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Enabled by multi-head self-attention, Transformer has exhibited remarkable results in speech emotion recognition (SER). Compared to the original full attention mechanism, windowbased attention is more effective in learning fine-grained features while greatly reducing model redundancy. However, emotional cues are present in a multi-granularity manner such that the pre-defined fixed window can severely degrade the model flexibility. In addition, it is difficult to obtain the optimal window settings manually. In this paper, we propose a Deformable Speech Transformer, named DST, for SER task. DST determines the usage of window sizes conditioned on input speech via a light-weight decision network. Meanwhile, data-dependent offsets derived from acoustic features are utilized to adjust the positions of the attention windows, allowing DST to adaptively discover and attend to the valuable information embedded in the speech. Extensive experiments on IEMOCAP and MELD demonstrate the superiority of DST.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Emotion is one of the most essential characteristics that distinguishes humans from robots  [1]  and speech is the most basic tool for daily communication  [2] . Therefore, analyzing emotion states through speech signals is a continuing concern for the research community. Owing to the rapid development of deep learning, many advanced models have been proposed and delivered promising results in speech emotion recognition (SER). In particular, convolutional neural networks  [3, 4] , recurrent neural networks  [5, 6]  and their variants  [7, 8, 9]  have been widely studied and deployed for applications.\n\nTransformer  [10] , which is the recent white hope architecture, is making a splash in deep learning domain. Different from previous networks, Transformer adopts the full attention mechanism, which is depicted in Fig.  1 (a), to learn a global representation of input signal. Although the effectiveness of Transformer in SER has already been confirmed  [11, 12, 13] , there are several key points to be aware of when handling emotion analysis with Transformer: 1) Emotional cues are multi-grained in nature, which means that beyond the In contrast to prior works that have pre-set window sizes or fixed window positions, we propose to make them both flexible and deformable. DCN-like attention is applied in vision.\n\nglobal representation, the details in speech are also important. For example, the local characteristics, such as articulation and prolongation, are highly relevant to the emotion states.\n\n2) The full attention mechanism suffers from a lack of diversity and is thus inadequate to capture the multi-granularity features.\n\n3) The computation of the full attention is quite redundant. One mainstream approach to improve Transformer is employing the window-based attention mechanism  [11, 14] . As shown in Fig  1(b) , window-based attention restricts the attention scope to a fixed local window whose size is typically set to a small value to focus on the fine-grained features. However, the immutable window also severely decreases the flexibility of model. What is worse, it weakens the ability of global learning and thus it inevitably requires considerable manual tuning of window configuration to obtain peak performance.\n\nTo alleviate the above issues, this paper proposes a deformable framework, named DST, for speech emotion recognition. In DST, the window sizes are learned by a light-weight decision network based on the input speech, breaking the limitations of using the pre-set configuration. Also, the window positions can be shifted by learned offsets on a per-input basis. These qualities follow the natures of emotion and greatly improve the model flexibility. In addition, unlike deformable convolutional networks (DCNs)  [15]   utilized in vision that model intermittently (Fig.  1(c ))  [16, 17] , DST models continuous tokens (Fig.  1(d )), which is more in line with the continuous speech signal. Finally, we visualize different attention mechanisms for an intuitive understanding. The contributions of this work are summarized as follows:\n\n• We endow Transformer with deformability by employing flexible, data-dependent window sizes and offsets.\n\n• Extensive experiments on IEMOCAP  [18]  and MELD  [19]  datasets show that DST outperforms the state-ofthe-art approaches. Our codes are publicly available at https://github.com/HappyColor/DST.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Methodology",
      "text": "The proposed DST, as illustrated in Fig.  2 , is composed of multiple stacked DST blocks. Each DST block mainly consists of a deformable speech attention (DSA) module and a feed-forward network (FFN). Equipped with the DSA module, the system is able to adaptively determine the usage of window sizes and window positions depending on the input speech signal, which greatly improves the model flexibility and can learn the multi-granularity emotional cues effectively.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Revisiting Transformer",
      "text": "At the core of the standard Transformer is the multi-head selfattention module (MSA), which makes Transformer stand out from other deep neural networks. More details can be found in  [10] . Specifically, the MSA mechanism can be written as:\n\nwhere Q, K, V are query, key and value matrices, respectively; d Q is a scaling factor and h denotes the number of attention heads; W Q i , W K i , W V i and W o are to be learned parameters.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Deformable Speech Transformer",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Deformable Speech Attention",
      "text": "Deformable speech attention (DSA) is at the core of the DST. Different from previous attention mechanisms, DSA is able to change the window sizes and modify the window positions via a simple decision network. Let Q j i be the j-th token of Q i in the i-th attention head, where i ∈  [1, h] . The decision network first produces the window size s ij and offset o ij conditioned on\n\nwhere j ∈ [0, L -1] and L denotes the sequence length of the features; W D i is the parameter matrix; σ 1 and σ 2 are two nonlinear functions for restricting the range of the outputs. For example, the window size s ij should lie in the range (0, L). Therefore, we first apply the sigmoid function to limit the value of s to (0, 1) and then scale it by the maximum length L. Similarly, since the valuable information can be on either side of the current j-th token, we apply the tanh function to normalize ōij to the range (-1, 1) before scaling it by L.\n\nGiving the current position index j and the offset o ij , the anchor of the critical segment A ij can be obtained. Combining with the predicted window size s ij , the boundaries of the attention window for the j-th query in the i-th head, L ij and R ij , are also given. The calculations are as follows:\n\nFinally, each query token attends to its respective deformed attention windows through the proposed DSA mechanism. The DSA is formulated as follows: where\n\nto the R ij -th tokens of K i and V i matrices, respectively; DH j i denotes the j-th output token of the i-th attention head.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "End-To-End Training",
      "text": "For ease of reading, we will omit the subscripts of the notations. In practice, the outputs of the decision network, the window size s and offset o, are decimals, causing the attention boundaries, L and R, to be decimals as well. However, in Eq. 8, the indexing operations K[L : R] and V [L : R] require both L and R to be integers. One simple solution is rounding the L and R to integers L and R , where • and • round a number up and down, respectively. However, the rounding operations are non-differentiable, resulting in a decision network that cannot be optimized by the back propagation algorithm. To add the decision network to the computation graph in a differentiable way, we leverage the distances between the predicted boundaries (L and R) and the true boundaries ( L and R ), and the distances between the central tokens ( A and A ) and the anchor (A) to yield weights for the selected key features in DSA. In general, only if the predicted boundaries are close to the true boundaries, the L -th and the Rth tokens will be assigned large weights. The weights for two central tokens are against each other, and whichever side the anchor is close to has a larger weight. Noting that we expect anchor to be the center of the important segment, thus the weights for the central tokens should be larger than 1 to emphasize them. Overall, the weights are computed as below:\n\nwhere k ∈ [ L , R ] denotes the token index and w k i is the weight for the k-th token in the K i matrix. Eventually, s and o are correlated with the weights, and the process of weighting is differentiable. The decision network can be optimized with the entire model jointly in an end-to-end manner. Suppose the current index j is 3, the weighting process is shown in Fig.  3 .",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Experiments",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Datasets And Acoustic Features",
      "text": "IEMOCAP  [18]  contains five sessions, every of which has one male and one female speaker, respectively. We merge excitement into happiness category and select 5,531 utterances from happy, angry, sad and neutral classes. Experiments are conducted in leave-one-session-out cross-validation strategy. MELD  [19]  dataset contains 13,708 utterances with 7 emotion classes. As MELD has been officially split into training, validation and testing sets, we use the validation set for hyperparameter turning and report the scores on the testing set. To be consistent with previous works, weighted accuracy (WA), unweighted accuracy (UA) and weighted average F1 (WF1) are used to assess the model performance. Features. Pre-trained self-supervised WavLM  [20]  is adopted to extract the acoustic features. The max sequence lengths are set to 326 and 224 for IEMOCAP and MELD, respectively.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Training Details And Hyper-Parameters",
      "text": "The number of training epochs is set to 120. SGD  [21]  with a learning rate of 5e -4 on IEMOCAP and 1e -3 on MELD is applied to optimize the model. Cosine annealing warm restarts scheduler  [22]  is used to adjust the learning rate in the training phase. Learning rate of the decision network is multiplied by a factor of 0.1. The batch size is 32. The number of attention heads is 8. The number of DST blocks is 4.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Experimental Results And Analysis",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Comparison With Other Attention Mechanisms",
      "text": "Performance Analysis. To analyze the potency of DST, we implement other common attention mechanisms, namely, full  [10] , window-based and DCN-like  [16]  attentions, for comparison. The fixed window size of the window-based attention and the number of sampling points in the DCN-like attention are empirically set to 10% of input length. Average percentage of activated tokens for each query is also listed for comprehensive analysis. As shown in Table  1 , DST outperforms the counterparts on IEMOCAP and MELD by a considerable margin. In particular, the use of DCN-like attention causes a significant drop in performance, which means that modeling the continuous tokens is essential for speech signal. Most interestingly, we find that on IEMOCAP, each query attends to an average of 8.7% of all input tokens, while on MELD, this percentage increases to 12.7%. This ambiguity exposes the difficulty of manual tuning and recommends configurations that are automatically determined by model itself. Also, we find that DST can learn all potential emotional features, both fine and coarse, through its deformable capabilities. Furthermore, we discard the learned window size (-def orm. size) or reset the offset to zero (-def orm. offset), and the ablation results shown in the last two rows of Table  1  once again confirm the effectiveness of the proposed deformable design. Visualization Analysis. To further understand the proposed model, we consider an utterance sample and intuitively compare the attention weights in each attention mechanism by visualization. As illustrated in Fig  4 , voiced fragments are distributed in a small part of the entire speech sample. The full attention has difficulty in highlighting the key parts owing to the large amount of noise that deeply confuses the model. Although the window-based attention is able to learn the finegrained features, it is inevitably limited in performance when the duration and position of the key segments mismatch with the pre-defined window settings. Since speech is a continuous signal, the DCN-like attention fails to judge importance by the discrete tokens, leading to little difference in its assigned weights. Inspiringly, DST successfully focuses on the critical segments (\"ashamed somehow\" in text) and highlights them by means of the learned window sizes and offsets.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Comparison To Previous State-Of-The-Art",
      "text": "Table  2  gives the comparison among the proposed DST with some known approaches on IEMOCAP and MELD. All approaches here adopt acoustic features as input for a fair comparison. On IEMOCAP, DST outperforms the previous best results obtained by  [3, 6] . On MELD, DST substantially surpasses the other competitors by a considerable margin.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Conclusion",
      "text": "In this paper, a deformable speech Transformer, named DST, has been proposed for speech emotion recognition. DST can   [4]  2021 0.654 0.667 AR-GRU  [8]  2021 0.669 0.683 ISNet  [3]  2022 0.704 0.650 Co-attention  [6]  2022 0.698 0.711 DST (Ours) 2023 0.718 0.736",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Meld Method",
      "text": "Year WF1 CTNet  [12]  2021 0.382 DECN  [7]  2021 0.439 SpeechFormer  [11]",
      "page_start": 5,
      "page_end": 5
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: (a), to learn",
      "page": 1
    },
    {
      "caption": "Figure 1: Comparison between different attention mechanisms.",
      "page": 1
    },
    {
      "caption": "Figure 1: (b), window-based attention restricts the atten-",
      "page": 1
    },
    {
      "caption": "Figure 2: Overview structure of the proposed DST. The only difference between DST block and the vanilla Transformer is the",
      "page": 2
    },
    {
      "caption": "Figure 1: (c)) [16, 17],",
      "page": 2
    },
    {
      "caption": "Figure 1: (d)), which is more in",
      "page": 2
    },
    {
      "caption": "Figure 2: , is composed of",
      "page": 2
    },
    {
      "caption": "Figure 3: Differentiable weighting process for end-to-end train-",
      "page": 3
    },
    {
      "caption": "Figure 3: 3. EXPERIMENTS",
      "page": 3
    },
    {
      "caption": "Figure 4: Visualization of different attention mechanisms. [N]",
      "page": 4
    },
    {
      "caption": "Figure 4: , voiced fragments are dis-",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Method\nYear": "Audio-CNN [4]\n2021\nAR-GRU [8]\n2021\nISNet [3]\n2022\nCo-attention [6]\n2022",
          "WA\nUA": "0.654\n0.667\n0.669\n0.683\n0.704\n0.650\n0.698\n0.711"
        },
        {
          "Method\nYear": "DST (Ours)\n2023",
          "WA\nUA": "0.718\n0.736"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Method\nYear": "CTNet [12]\n2021\nDECN [7]\n2021\nSpeechFormer [11]\n2022\nMM-DFN [9]\n2022",
          "WF1": "0.382\n0.439\n0.419\n0.427"
        },
        {
          "Method\nYear": "DST (Ours)\n2023",
          "WF1": "0.488"
        }
      ],
      "page": 4
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "From human emotions to robot emotions",
      "authors": [
        "J.-M Fellous"
      ],
      "year": "2004",
      "venue": "Architectures for Modeling Emotion: Cross-Disciplinary Foundations"
    },
    {
      "citation_id": "3",
      "title": "Introduction. the perception of speech: from sound to meaning",
      "authors": [
        "B Moore",
        "L Tyler",
        "W Marslen-Wilson"
      ],
      "year": "2008",
      "venue": "Philosophical transactions of the Royal Society of London. Series B, Biological sciences"
    },
    {
      "citation_id": "4",
      "title": "Isnet: Individual standardization network for speech emotion recognition",
      "authors": [
        "W Fan",
        "X Xu",
        "B Cai",
        "X Xing"
      ],
      "year": "2022",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "5",
      "title": "Efficient speech emotion recognition using multi-scale cnn and attention",
      "authors": [
        "Z Peng",
        "Y Lu",
        "S Pan",
        "Y Liu"
      ],
      "year": "2021",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "6",
      "title": "High-level feature representation using recurrent neural network for speech emotion recognition",
      "authors": [
        "J Lee",
        "I Tashev"
      ],
      "year": "2015",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "7",
      "title": "Speech emotion recognition with co-attention based multi-level acoustic information",
      "authors": [
        "H Zou",
        "Y Si",
        "C Chen",
        "D Rajan",
        "E Chng"
      ],
      "year": "2022",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "8",
      "title": "Decn: Dialogical emotion correction network for conversational emotion recognition",
      "authors": [
        "Z Lian",
        "B Liu",
        "J Tao"
      ],
      "year": "2021",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "9",
      "title": "A novel attention-based gated recurrent unit and its efficacy in speech emotion recognition",
      "authors": [
        "S Rajamani",
        "K Rajamani",
        "A Mallol-Ragolta",
        "S Liu",
        "B Schuller"
      ],
      "year": "2021",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "10",
      "title": "Mm-dfn: Multimodal dynamic fusion network for emotion recognition in conversations",
      "authors": [
        "D Hu",
        "X Hou",
        "L Wei",
        "L Jiang",
        "Y Mo"
      ],
      "year": "2022",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "11",
      "title": "Attention is all you need",
      "authors": [
        "A Vaswani",
        "N Shazeer",
        "N Parmar",
        "J Uszkoreit",
        "L Jones"
      ],
      "year": "2017",
      "venue": "Proceedings of the 31st International Conference on Neural Information Processing Systems"
    },
    {
      "citation_id": "12",
      "title": "Speech-Former: A Hierarchical Efficient Framework Incorporating the Characteristics of Speech",
      "authors": [
        "W Chen",
        "X Xing",
        "X Xu",
        "J Pang",
        "L Du"
      ],
      "year": "2022",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "13",
      "title": "Ctnet: Conversational transformer network for emotion recognition",
      "authors": [
        "Z Lian",
        "B Liu",
        "J Tao"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "14",
      "title": "Key-sparse transformer for multimodal speech emotion recognition",
      "authors": [
        "W Chen",
        "X Xing",
        "X Xu",
        "J Yang",
        "J Pang"
      ],
      "year": "2022",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "15",
      "title": "Hts-at: A hierarchical token-semantic audio transformer for sound classification and detection",
      "authors": [
        "K Chen",
        "X Du",
        "B Zhu",
        "Z Ma",
        "T Berg-Kirkpatrick",
        "S Dubnov"
      ],
      "year": "2022",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "16",
      "title": "Deformable convolutional networks",
      "authors": [
        "J Dai",
        "H Qi",
        "Y Xiong",
        "Y Li",
        "G Zhang",
        "H Hu",
        "Y Wei"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE International Conference on Computer Vision"
    },
    {
      "citation_id": "17",
      "title": "Vision transformer with deformable attention",
      "authors": [
        "Z Xia",
        "X Pan",
        "S Song",
        "L Li",
        "G Huang"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "18",
      "title": "Deformable detr: Deformable transformers for end-toend object detection",
      "authors": [
        "X Zhu",
        "W Su",
        "L Lu",
        "B Li",
        "X Wang",
        "J Dai"
      ],
      "year": "2020",
      "venue": "Deformable detr: Deformable transformers for end-toend object detection",
      "arxiv": "arXiv:2010.04159"
    },
    {
      "citation_id": "19",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "A Kazemzadeh",
        "E Mower"
      ],
      "year": "2008",
      "venue": "Language Resources and Evaluation"
    },
    {
      "citation_id": "20",
      "title": "MELD: A multimodal multiparty dataset for emotion recognition in conversations",
      "authors": [
        "S Poria",
        "D Hazarika",
        "N Majumder",
        "G Naik",
        "E Cambria",
        "R Mihalcea"
      ],
      "year": "2019",
      "venue": "MELD: A multimodal multiparty dataset for emotion recognition in conversations",
      "arxiv": "arXiv:1810.02508"
    },
    {
      "citation_id": "21",
      "title": "Wavlm: Large-scale self-supervised pretraining for full stack speech processing",
      "authors": [
        "S Chen",
        "C Wang",
        "Z Chen",
        "Y Wu",
        "S Liu",
        "Z Chen",
        "J Li",
        "N Kanda",
        "T Yoshioka",
        "X Xiao",
        "J Wu",
        "L Zhou"
      ],
      "year": "2022",
      "venue": "IEEE Journal of Selected Topics in Signal Processing"
    },
    {
      "citation_id": "22",
      "title": "A stochastic approximation method",
      "authors": [
        "H Robbins",
        "S Monro"
      ],
      "year": "1951",
      "venue": "The annals of mathematical statistics"
    },
    {
      "citation_id": "23",
      "title": "Sgdr: Stochastic gradient descent with warm restarts",
      "authors": [
        "I Loshchilov",
        "F Hutter"
      ],
      "year": "2016",
      "venue": "Sgdr: Stochastic gradient descent with warm restarts",
      "arxiv": "arXiv:1608.03983"
    }
  ]
}