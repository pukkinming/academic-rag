{
  "paper_id": "2502.15367v1",
  "title": "Advancing User-Voice Interaction: Exploring Emotion-Aware Voice Assistants Through A Role-Swapping Approach",
  "published": "2025-02-21T10:33:44Z",
  "authors": [
    "Yong Ma",
    "Yuchong Zhang",
    "Di Fu",
    "Stephanie Zubicueta Portales",
    "Danica Kragic",
    "Morten Fjeld"
  ],
  "keywords": [
    "Emotion-Aware Voice Assistants",
    "Role-Swapping Approach",
    "Speech and Linguistic Analysis",
    "Speech Emotion Recognition (SER)"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "As voice assistants (VAs) become increasingly integrated into daily life, the need for emotion-aware systems that can recognize and respond appropriately to user emotions has grown. While significant progress has been made in speech emotion recognition (SER) and sentiment analysis, effectively addressing user emotions-particularly negative ones-remains a challenge. This study explores human emotional response strategies in VA interactions using a role-swapping approach, where participants regulate AI emotions rather than receiving pre-programmed responses. Through speech feature analysis and natural language processing (NLP), we examined acoustic and linguistic patterns across various emotional scenarios. Results show that participants favor neutral or positive emotional responses when engaging with negative emotional cues, highlighting a natural tendency toward emotional regulation and de-escalation. Key acoustic indicators such as root mean square (RMS), zero-crossing rate (ZCR), and jitter were identified as sensitive to emotional states, while sentiment polarity and lexical diversity (TTR) distinguished between positive and negative responses. These findings provide valuable insights for developing adaptive, context-aware VAs capable of delivering empathetic, culturally sensitive, and user-aligned responses. By understanding how humans naturally regulate emotions in AI interactions, this research contributes to the design of more intuitive and emotionally intelligent voice assistants, enhancing user trust and engagement in human-AI interactions.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Introduction",
      "text": "Voice assistants (VAs) have become an integral part of our daily life, offering convenience and efficiency in tasks ranging from information retrieval to smart arXiv:2502.15367v1 [cs.HC] 21 Feb 2025 home control. However, traditional VAs primarily focus on functionality, such as in-home systems like Alexa  [25, 11] , often overlooking the emotional nuances of human interaction. As voice user interface (VUI) technologies advance, the ability to recognize, understand, and appropriately respond to user emotions is becoming increasingly important. Emotion-aware VAs, capable of detecting and responding to users' emotional states, have the potential to create more empathetic and engaging user experiences  [22, 15] .\n\nRecent advancements in artificial intelligence (AI), natural language processing (NLP), and affective computing have enabled VAs to detect emotional cues from speech, including vocal patterns, tone, and contextual information. This capability allows VAs to dynamically adjust their responses, tailoring interactions to the user's emotional state and improving user satisfaction and trust  [20, 29] . However, while advancements in speech emotion recognition and speech emotion synthesis have enabled the recognition and understanding of user emotions  [34]  and the generation of emotionally expressive speech  [33] , effectively responding to user emotions-particularly negative ones-remains a significant challenge. One major obstacle lies in the variability of emotional expression across individuals and cultures. Negative emotions, such as anger or sadness, can be conveyed differently depending on cultural norms, making it difficult for systems to accurately detect and respond to these emotions  [26] . For instance, speech emotion recognition (SER) systems may struggle with accents, dialects, or background noise, leading to misinterpretations of emotional cues  [4] .\n\nBeyond detection, responding appropriately to negative emotions is equally complex. While empathetic reactions can be beneficial to users  [30, 12] , as seen in Alexa's empathetic responses  [5] , striking the right balance between empathy and functionality is crucial. Overly intrusive or artificial responses can feel misaligned, ultimately undermining user trust and satisfaction  [1, 3] . Moreover, emotional expression varies based on cultural, gender, and contextual differences  [9, 8] . As a result, purely empathetic emotional responses may not always be suitable or effective for all users. Developing adaptive, context-aware strategies for emotion-aware systems remains a critical area for future research.\n\nTo address this issue, an effective approach is to understand how people respond to different emotions in various contexts. By studying human emotional responding strategies, we can transfer these insights into voice assistants (VAs) to create more empathetic and contextually appropriate interactions. This approach aligns with the concept of role-swapping  [20, 13] , where human strategies for emotional responses are mimicked in AI systems. In this study, we employ a role-swapping approach, where the traditional roles of VAs and individuals are reversed. Participants are tasked with applying their own strategies to respond to various emotional contexts displayed by VAs. The primary objective of this study is to influence and regulate the emoji's emotions, guiding them toward more positive or neutral states. To achieve this, participants are encouraged to engage with the VAs using diverse emotional tones and responses, allowing for dynamic and adaptive interactions. To facilitate this, we designed a website (shown in Figure  1 ) where participants can join the study via a provided web link. On the Fig.  1 : The web page is designed to collect voice samples from participants. When an emoji is clicked, it turns yellow and plays an emotional context (e.g., a sad or happy scenario). Participants are then prompted to say something comforting or engage in a conversation with the emoji by clicking the \"Start Recording\" button. This interactive design allows users to respond naturally to the emotional context, providing valuable data for emotion-aware systems.\n\nwebsite, participants can input their emotional responses, which are automatically recorded. After voice recording, the participants' voices are analyzed for emotion detection and speech features. By engaging with emotionally expressive VAs, participants apply their own emotional strategies to interact in ways that aim to de-escalate or regulate emotional states. Through speech feature analysis, sentiment polarity assessments, and linguistic evaluations, our study reveals key insights into how users react to different emotional scenarios. The results show that participants favor neutral or positive emotional responses when interacting with negative emotions, indicating a natural tendency toward emotional regulation in human-AI interactions. By integrating these findings, we aim to advance the development of emotion-aware VAs, enabling more empathetic, natural, and effective interactions between humans and voice assistants.",
      "page_start": 1,
      "page_end": 3
    },
    {
      "section_name": "Related Work",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Emotion Recognition In Voice Assistants",
      "text": "Emotion recognition is a critical component of emotion-aware voice assistants (VAs). By analyzing speech features such as tone, pitch, and speech patterns  [6, 31, 21] , speech emotion recognition (SER) enables VAs to detect emotional states from speech signals. Recent advancements in AI technologies, particularly in Large Language Models (LLMs)  [23]  and multimodal approaches  [16]  to SER, have significantly enhanced the accuracy and robustness of emotion detection  [17, 14] . These technologies leverage machine learning and deep learning models to identify emotions such as happiness, sadness, and anger, fostering more emotionally intelligent and contextually aware interactions in VAs. Despite these advancements, challenges remain in handling variability in emotional expression due to factors such as cultural differences, accents, and background noise  [26, 4] . For example, SER systems may misinterpret emotional cues in diverse linguistic or cultural contexts, reducing their effectiveness in real-world applications. Addressing these limitations requires further research into adaptive and contextaware emotion recognition models that can better generalize across different user demographics and environmental conditions.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Empathetic Responses In Human-Computer Interaction",
      "text": "Empathy in AI refers to a conversational agent's ability to recognize, interpret, and respond appropriately to a user's emotional state  [32] . Empathetic responses play a crucial role in fostering engaging and emotionally resonant interactions  [30, 18] , enhancing user engagement and trust by adapting speech tone, word choices, and emotional expressiveness  [2] . studies have shown that users respond positively to empathetic reactions from AI systems, such as Alexa's ability to provide comforting and supportive responses  [5, 24] . However, designing appropriate responses to negative emotions remains a significant challenge. Although empathetic reactions can improve user satisfaction, overly intrusive or artificial responses may feel misaligned with user expectations, ultimately undermining trust  [1, 3] . This underscores the need for contextually appropriate and culturally sensitive emotional responses in emotion-aware AI systems.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Role-Swapping In Emotion Regulation",
      "text": "Role-swapping in emotion regulation is a promising approach for enhancing voice user interface design, particularly in the development of emotion-aware voice assistants (VAs). By reversing traditional roles, users take on the responsibility of regulating the emotions of AI systems, such as responding to negative emotional cues displayed by VAs. This approach allows researchers to explore alternative emotional responding and reacting strategies, shedding light on how humans manage AI-driven emotions. Studies have shown that individuals often adopt neutral emotional responses when interacting with emotionally expressive AI systems. However, gender and cultural differences can significantly influence these strategies  [20, 13] . Furthermore, research on interpersonal emotion regulation highlights how individuals adjust their strategies to support others-for example, using cognitive reappraisal to reinterpret stressful situations or employing expressive suppression to prevent conflict escalation  [35, 10] . These insights can be applied to AI systems, enabling them to mimic human emotional responding strategies and create more empathetic and adaptive interactions. Despite its potential, role-swapping in emotion regulation presents challenges, particularly in addressing cultural and contextual differences in emotional expression and ensuring that such systems remain adaptive and effective across diverse user demographics  [26, 9] . By leveraging role-swapping approaches, researchers can develop emotion-aware VAs capable of providing contextually appropriate and emotionally resonant responses, ultimately enhancing user satisfaction and trust in AI-driven interactions.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Research Gaps",
      "text": "While significant progress has been made in emotion recognition and empathetic response generation, research on how humans naturally respond to emotional cues from AI systems-particularly in role-swapping scenarios-remains limited. Most existing studies focus on how AI systems respond to human emotions, rather than how humans regulate the emotions of AI systems. Our study addresses this gap by introducing a role-swapping approach, where participants interact with emotion-aware voice assistants (VAs) to transform negative emotions into positive or neutral states. By analyzing participants' emotional responses, we aim to develop effective strategies for designing more adaptive and empathetic VAs. This approach not only deepens our understanding of human emotional responding strategies but also provides valuable insights for enhancing emotion-aware AI systems, leading to more natural and engaging human-AI interactions.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Experiment Setup",
      "text": "As shown in Figure  1 , we conducted an online user study to explore users' strategies for responding to different emotional contexts. By analyzing participants' voices and the content of their interactions with our one-dialog simulated VAs, we evaluated emotional responding differences, speech features, and language features using speech analysis and natural language processing (NLP) techniques. This approach allowed us to gain insights into how users naturally adapt their responses to emotional cues, providing valuable data for improving emotion-aware systems.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Emotional Scenarios",
      "text": "In our study, we designed five distinct emotional scenarios based on the five basic emotions -neutral, happy, sad, angry, and fear  [28, 27, 19] . These scenarios were crafted to reflect situations that users might encounter in their daily lives, ensuring they were relatable and easily identifiable by general users. After confirming the emotional states, we conducted brainstorming sessions to determine which scenarios would best represent these emotions in a way that felt authentic and meaningful. The final five emotional scenarios are as follows:\n\n-Neutral: I put on my shoes before leaving the house.\n\n-Happy: I am visiting my favorite country or city.\n\n-Sad: I see children suffering from disease, sickness, or war.\n\n-Angry: I get betrayed by a close friend or relative.\n\n-Fear: I am walking in the dark in the woods when I stumble upon a dead body. The blood seems fresh, and I hear a branch breaking from behind.\n\nBased on these emotional scenarios, we recorded voice clips for each scenario and integrated them into our designed website. Users visiting the website can listen to these emotional scenario recordings, allowing them to immerse themselves in the context and respond naturally. This interactive setup enables participants to engage with the emotional content and provide authentic responses, which are then recorded and analyzed for further study.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Apparatus, Participants And Experimental Procedure",
      "text": "Apparatus and Participants In this user study, we primarily utilized Open-Vokaturi 6 to analyze the emotional states of speech. Additionally, we employed two key speech analysis packages: Librosa 7 and OpenSMILE 8 , to examine users' speech behaviors in response to emotional scenarios. For analyzing the content of users' responses, we relied on four main NLP packages: WordCloud 9 , NLTK 10 , TextStatc, and SpaCy 11 . Furthermore, OpenAI Whisper 12 was used as a critical tool for converting speech into text, enabling detailed analysis of both linguistic and emotional features. Participants' voices were recorded using the built-in microphones of their own computers and uploaded to our web server when they conducted our user study on our website. All recorded voice signals were captured at a sample rate of 48 kHz, ensuring high-quality audio for subsequent analysis. Additionally, we recruited 60 participants through Prolific 13 , comprising 30 males and 30 females. To account for potential gender-based differences in voice perception, we conducted two separate user studies: one using a male emotional voice and the other using a female emotional voice for the five emotional scenarios. The male emotional voice had a mean age of 32.29 years (SD = 9.71), while the female emotional voice had a mean age of 27.17 years (SD = 5.02). This approach ensured a balanced and comprehensive evaluation of emotional responses across different gender contexts. After completing the user study, each participant was compensated with £4.50 through the Prolific website. Participants were assured of complete anonymity, as clearly stated in the website's privacy policy. This policy also detailed the types of data collected during the study, which included voice recordings and responses to demographic questions, such as age and gender. All measures were taken to ensure the confidentiality and privacy of the participants' information.\n\nExperimental Procedure As mentioned earlier, we recruited participants through the Prolific website, and they accessed our designed website (shown in Figure1) via a provided link. The landing page provided an overview of the study, along with detailed instructions. Participants could click on the smiley emoji, which would turn yellow and play an emotional scenario voice. They were then free to respond in any way they liked. When participants clicked the \"Start Recording\" button, their microphone would begin recording their voice. They were given 20 seconds to speak, during which they could engage in conversation with the smiley or attempt to comfort it, especially in cases of negative emotions such as anger, fear, or sadness. In this study, participants engaged with five different emotional scenarios, with either a male or female emotional voice randomly selected for each scenario. To ensure gender balance, 15 male and 15 female participants interacted with the male emotional voice scenarios, while the remaining 30 participants interacted with the female emotional voice scenarios. After completing all five emotional scenarios, participants finished the main user study and proceeded to fill out a relevant questionnaire.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Speech Signals Analysis",
      "text": "The speech data collected from participants were downloaded from the website server and systematically classified according to different emotional scenarios, based on the naming conventions of the audio files. To extract relevant speech features, we utilized advanced speech processing Python packages, such as Librosa and OpenSMILE  [7] . Additionally, to analyze the content of the speech, we transcribed the audio data into text using OpenAI Whisper model. Leveraging natural language processing (NLP) techniques, we then extracted a variety of text features, including lexical, semantic, and syntactic features. Furthermore, we employed Openvokatuti to identify and analyze participants' emotional states during their interactions, specifically determining which emotional responses they exhibited while communicating with our VAs. This comprehensive approach allowed us to gain deeper insights into both the acoustic and linguistic aspects of the participants' interactions.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Results",
      "text": "Our study involved 60 participants, resulting in 242 valid audio files across five emotional scenarios: happy, neutral, angry, sad, and fear. Through a combination of speech signal analysis and linguistic evaluation, we identified key patterns in participants' emotional responses and speech features. Speech Emotion Analysis In this study, we aimed to uncover the strategies users employ when responding to different emotional scenarios. To achieve this, we utilized OpenVokaturi as our emotion analysis tool to extract emotional states from the collected audio data. As shown in Figure  2 , we observed that in happy and neutral speech scenarios, participants primarily used neutral and happy emotional responses to communicate. However, in the case of negative emotional scenarios (e.g., anger, sadness, and fear), the majority of participants preferred to use neutral emotional responses when interacting with others. This suggests that users tend to adopt a balanced and non-confrontational approach when dealing with negative emotions, highlighting the importance of neutrality in emotional communication.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Results From Speech Signals",
      "text": "Speech Signal Analysis Speech analysis typically involves examining timedomain speech features and frequency-domain speech features  (Madan et al., 2014) . In this study, we primarily used Librosa and OpenSMILE to extract these features. We then employed T-tests to compare the differences between various emotional scenarios. As shown in Figure  3 , we found that only three features-root mean square (RMS), zero-crossing rate (ZCR), and speech jitter-exhibited significant differences across some emotional scenarios. For instance, in RMS, we observed differences between the happy speech scenario and the angry speech scenario, as well as between the happy speech scenario and the sad speech scenario. In ZCR, significant differences were found between the happy speech scenario and the neutral speech scenario, as well as between the happy speech scenario and the fear speech scenario. For speech jitter, differences were identified between the happy speech scenario and the angry speech scenario, the happy speech scenario and the fear speech scenario, and the happy speech scenario and the sad speech scenario. These findings suggest that RMS, ZCR, and jitter are sensitive to variations in emotional expression, particularly in distinguishing between positive and negative emotional states. This highlights their potential as key indicators for emotion-aware systems to detect and respond to different emotional contexts effectively.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Results From Speech Content",
      "text": "Result from Word Cloud In this study, we utilized the wordcloud package to analyze linguistic patterns across various emotional states. The resulting word clouds, as depicted in Figure  4 , provide a visual representation of how individuals linguistically express emotions in different scenarios. Each emotional state reveals unique linguistic tendencies, reflecting the underlying feelings and cognitive processes associated with anger, fear, happiness, neutrality, and sadness.\n\nIn the angry speech scenario, the most frequently used words include \"angry,\" \"think,\" \"betrayed,\" and \"frustration.\" These terms highlight a strong focus on personal reflection, conflict, and perceived injustice. The presence of words like \"understand\" and \"really\" suggests that individuals in angry states often attempt to justify their emotions or seek validation, particularly in confrontational or tense situations. This linguistic pattern underscores the cognitive effort  to rationalize or communicate feelings of anger. For the fear speech scenario, the word cloud features terms such as \"help,\" \"calm,\" \"police,\" \"scared,\" and \"run.\" These words reflect a mix of distress and an urgent need for safety or assistance. The emphasis on phrases like \"stay calm\" and \"know\" indicates that individuals experiencing fear also strive to manage their emotions and seek solutions during stressful or threatening situations. This linguistic behavior highlights the dual focus on expressing fear while attempting to regain control. In the happy speech scenario, the dominant words include \"excited,\" \"ha ha,\" \"fun,\" \"happy,\" and \"wait.\" These terms reflect expressions of joy, enthusiasm, and anticipation. The frequent use of words like \"see,\" \"hear,\" and \"trip\" suggests that happiness is often associated with experiences, events, and social interactions. This linguistic pattern emphasizes the outward and experiential nature of happiness, as individuals share their positive emotions and engage with the world around them. The neutral speech scenario is characterized by words such as \"routine,\" \"shoes,\" \"morning,\" and \"house.\" These terms indicate discussions about daily life, habits, and general observations. The frequent appearance of \"story\" and \"interesting\" suggests that even in neutral emotional states, people engage in conversations about experiences and narratives rather than expressing strong emotional reactions. This linguistic pattern reflects the mundane yet meaningful nature of everyday communication. Finally, in the sad speech scenario, key words such as \"sad,\" \"sorry,\" \"feel,\" \"trying,\" and \"children\" highlight expressions of emotional distress, regret, and concern for others. Words like \"understand,\" \"hear,\" and \"people\" indicate a search for empathy, connection, or support in difficult situations. This linguistic pattern underscores the introspective and re-lational aspects of sadness, as individuals seek to articulate their feelings and connect with others during challenging times. These word clouds illustrate the distinct linguistic patterns associated with different emotional scenarios, offering valuable insights into how individuals communicate their emotions. Understanding these patterns can play a crucial role in enhancing the development of emotion-aware VAs, enabling them to generate more contextually appropriate and empathetic responses tailored to users' emotional states. Result from Correlation Between NLP Features As it show in Figure  5 , it reveals relationships between key linguistic metrics, including polarity, word count, and Type-Token Ratio (TTR). Polarity, which quantifies sentiment on a scale from negative to positive, exhibits a weak positive correlation (0.12) with word count, suggesting that longer texts may slightly amplify sentiment intensity. TTR, a measure of lexical diversity, demonstrates a strong correlation with itself (1.00) and a moderate positive correlation with word count (0.16), indicating that longer texts tend to exhibit greater vocabulary variety. Notably, a significant negative correlation (-0.35) between polarity and another linguistic feature suggests an inverse relationship, potentially reflecting how emotions or linguistic structures influence sentiment expression. These findings highlight the intricate interplay among linguistic features, offering valuable insights for the future design of emotion-aware VAs that can better interpret and respond to user sentiment.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Result From Sentiment Polarity Across Different Emotion Responses",
      "text": "The Figure  7  illustrates the Sentiment Polarity Across Different Emotion Responses, providing a clear understanding of how sentiment varies across emotions. The bar chart reveals that \"Happy\" emotions exhibit the highest positive polarity ( 0.4), indicating a strong positive sentiment, whereas \"Angry\" emotions have the lowest polarity (close to -0.1), reflecting a more negative sentiment. As expected, \"Neutral\" emotions remain close to zero, representing a balanced sentiment with neither strong positivity nor negativity. This figure effectively highlights the distinct variations in sentiment polarity across emotional responses, demonstrating that happiness is strongly associated with positive sentiment, while anger is linked to negative sentiment. These insights contribute to the development of emotion-aware VAs systems, enabling AI models to better recognize and respond to different emotional tones.",
      "page_start": 10,
      "page_end": 13
    },
    {
      "section_name": "Discussion",
      "text": "Our findings provide key insights into how users respond to different emotional scenarios when interacting with emotion-aware voice assistants (VAs). By analyzing speech signals and linguistic content, we identified patterns in emotional responses, speech features, and sentiment polarity. These findings have several implications for the design of more effective and empathetic VAs, ensuring they provide more meaningful, natural, and human-like interactions.",
      "page_start": 13,
      "page_end": 14
    },
    {
      "section_name": "Emotional Response Patterns And User Strategies",
      "text": "A key observation from our study is that users tend to adopt neutral emotional responses when interacting with negative emotional stimuli, such as anger, sadness, and fear. This suggests that individuals prefer a balanced, non-confrontational approach when faced with distressing scenarios, likely as a means of de-escalation. Such behavior aligns with established theories in interpersonal emotion regulation, where individuals employ strategies like cognitive reappraisal and expressive suppression to manage emotional interactions. Interestingly, while neutrality was the dominant response to negative emotions, participants demonstrated a greater emotional range when engaging with positive emotions, such as happiness. This variability suggests that users are more comfortable expressing emotions openly in positive contexts but may exhibit emotional restraint in negative situations. From a design perspective, VAs should recognize and mirror these behavioral tendencies, allowing for more intuitive interactions that do not feel forced or artificial. For VAs, this indicates that responses should be carefully calibrated to align with user tendencies. Instead of mirroring negative emotions or responding with excessive artificial empathy, VAs may be more effective when employing neutral yet supportive responses. Implementing context-aware, adaptive response strategies can help ensure appropriate engagement without overwhelming users.",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "Speech Features As Indicators Of Emotion",
      "text": "Our analysis of speech signals, including root mean square (RMS), zero-crossing rate (ZCR), and speech jitter, highlighted significant variations between emotional states. Specifically, happy speech scenarios exhibited higher RMS and ZCR values compared to anger, fear, and sadness, indicating greater energy and pitch variation in positive emotional expressions. These findings underscore the potential of integrating advanced speech signal processing techniques into emotion-aware VAs. By continuously analyzing these speech features, VAs can enhance their real-time emotion recognition capabilities, allowing for more precise and adaptive responses to user emotions. Moreover, these insights could be leveraged to train VAs to detect the subtleties of human emotion, moving beyond basic categorical recognition (e.g., happy vs. sad) toward a more nuanced understanding of mixed and subtle emotional states.",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "Linguistic Indicators Of Emotion",
      "text": "The word cloud and NLP analysis revealed distinct linguistic patterns associated with each emotional state. Angry responses featured terms related to conflict and frustration, while fearful responses included words indicating distress and a need for safety. Happy responses, in contrast, were characterized by words denoting enthusiasm and excitement. The correlation between sentiment polarity and type-token ratio (TTR) further illustrated that positive emotions tend to exhibit higher lexical diversity, while negative emotions are often expressed with a limited vocabulary. This suggests that individuals experiencing negative emotions may struggle to articulate their thoughts, potentially due to the cognitive load associated with distressing emotions. Conversely, positive emotions may encourage more elaborate and expressive language use. Understanding these linguistic variations can inform VA design, allowing for more tailored language models that adjust to user sentiment dynamically. Furthermore, beyond basic sentiment analysis, the integration of context-aware linguistic processing can enhance VA interactions. For example, VAs could analyze shifts in user tone and word choice over time, detecting emotional trends and adapting responses accordingly. This capability would make interactions feel more personalized and engaging, reinforcing user trust and reliance on these systems.",
      "page_start": 14,
      "page_end": 15
    },
    {
      "section_name": "Implications For Emotion-Aware Voice Assistant Design",
      "text": "Our findings provide strong support for the integration of emotion-aware mechanisms in VAs. However, designing such systems requires careful consideration of cultural, gender, and contextual differences in emotional expression. Future implementations should prioritize:\n\n-Adaptive Response Strategies: VAs should dynamically adjust their level of empathy based on user emotional cues, ensuring responses are neither overly detached nor excessively emotional. -Multimodal Emotion Recognition: Combining speech, linguistic, and contextual data can enhance emotion detection accuracy, mitigating issues related to accents, dialects, and background noise. -User-Centric Customization: Allowing users to set personal preferences for VA responses can improve engagement and trust, ensuring that interactions align with individual comfort levels.",
      "page_start": 15,
      "page_end": 15
    },
    {
      "section_name": "Limitations And Future Work",
      "text": "While our study provides valuable insights into emotional response strategies in human-VA interactions, several limitations should be acknowledged. First, the study was conducted in an online environment, which may not fully replicate real-world interactions with VAs. The controlled nature of the experiment could limit the generalizability of the findings to more dynamic, everyday scenarios. Additionally, cultural and linguistic diversity was not explicitly controlled, which may influence the applicability of our results across different populations. Emotional expression varies significantly across cultures, and future research should explore these variations to ensure that emotion-aware VAs are culturally sensitive and inclusive. Another limitation is the reliance on single-dialogue interactions, which may not capture the complexity of users' emotional responses in prolonged or repeated engagements with VAs. Emotional dynamics often evolve over time, and future studies should investigate how users' emotional responses and strategies change during extended interactions. Furthermore, participants' first impressions of VA responses may have influenced their reactions, potentially limiting the scope of observed emotional dynamics. To address this, future research could incorporate iterative interactions, allowing participants to engage with VAs over multiple sessions.\n\nTo build on these findings, future work should focus on several key areas. First, cross-cultural studies are needed to better understand how emotional expression and regulation vary across different cultural contexts. This would help design VAs that are adaptable to diverse user needs and expectations. Second, long-term interaction studies could provide insights into how users' emotional responses and satisfaction evolve over time, particularly in scenarios where VAs are used regularly. Third, expanding the dataset to include more diverse user demographics and real-time conversational exchanges would improve the robustness and generalizability of emotion recognition models. By addressing these challenges, future emotion-aware VAs can move closer to creating more natural, engaging, and emotionally intelligent human-AI interactions. The ultimate goal is not only to enhance the technical capabilities of VAs but also to ensure that they contribute positively to users' emotional well-being. Emotion-aware VAs should feel genuinely helpful, intuitive, and human-like, fostering trust and reliance in human-AI interactions.",
      "page_start": 15,
      "page_end": 16
    },
    {
      "section_name": "Conclusion",
      "text": "This study explored human emotional response strategies in interactions with emotion-aware VAs using a role-swapping approach. Participants predominantly adopted neutral or positive emotional responses, particularly in negative scenarios, demonstrating a natural tendency toward emotional regulation. Speech feature analysis identified RMS, ZCR, and jitter as key indicators of emotional expression, while sentiment polarity and lexical diversity revealed distinct patterns across emotional states. These insights provide a foundation for developing adaptive, emotion-aware VAs that respond contextually and empathetically, improving user trust and engagement. Future research should focus on crosscultural emotional models, deep learning-driven response adaptation, and multimodal emotion recognition to further enhance the emotional intelligence of VAs. By addressing these challenges, we can create VAs that not only enhance technical capabilities but also contribute positively to users' emotional well-being.",
      "page_start": 16,
      "page_end": 16
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: ) where participants can join the study via a provided web link. On the",
      "page": 2
    },
    {
      "caption": "Figure 1: The web page is designed to collect voice samples from participants. When",
      "page": 3
    },
    {
      "caption": "Figure 1: , we conducted an online user study to explore users’ strate-",
      "page": 5
    },
    {
      "caption": "Figure 2: The Emotional Responding Distribution from Different Emotional Sce-",
      "page": 8
    },
    {
      "caption": "Figure 2: , we observed that in happy",
      "page": 8
    },
    {
      "caption": "Figure 3: , we found that only three",
      "page": 8
    },
    {
      "caption": "Figure 3: Comparison of Different Features Using T-Test from Five Basic Emotional",
      "page": 9
    },
    {
      "caption": "Figure 4: , provide a visual representation of how individ-",
      "page": 9
    },
    {
      "caption": "Figure 4: Word Cloud Representations for Different Emotional Scenarios: (a) Angry,",
      "page": 10
    },
    {
      "caption": "Figure 5: Correlation Heatmap Between NLP Features. The heatmap visualizes the",
      "page": 11
    },
    {
      "caption": "Figure 6: Polarity vs. Type-Token Ratio (TTR) Across Different Emotion Scenar-",
      "page": 12
    },
    {
      "caption": "Figure 6: , the relationship between polarity (sentiment positivity/negativity)",
      "page": 12
    },
    {
      "caption": "Figure 7: Sentiment Polarity Across Different Emotion Scenarios. The bar plot illus-",
      "page": 13
    },
    {
      "caption": "Figure 7: illustrates the Sentiment Polarity Across Different Emotion Re-",
      "page": 13
    }
  ],
  "tables": [],
  "citations": [
    {
      "citation_id": "1",
      "title": "The influence of an emotion regulation intervention on challenges in emotion regulation and cognitive strategies in patients with depression",
      "authors": [
        "M Atta",
        "M El-Gueneidy",
        "O Lachine"
      ],
      "year": "2024",
      "venue": "BMC Psychology"
    },
    {
      "citation_id": "2",
      "title": "Impact of adaptive multimodal empathic behavior on the user interaction",
      "authors": [
        "M Barange",
        "S Rasendrasoa",
        "M Bouabdelli",
        "J Saunier",
        "A Pauchet"
      ],
      "year": "2022",
      "venue": "Proceedings of the 22nd ACM International Conference on Intelligent Virtual Agents"
    },
    {
      "citation_id": "3",
      "title": "Emotion regulation and mental health: Recent findings, current challenges, and future directions",
      "authors": [
        "M Berking",
        "P Wupperman"
      ],
      "year": "2012",
      "venue": "Current Opinion in Psychiatry"
    },
    {
      "citation_id": "4",
      "title": "Approaches, applications, and challenges in physiological emotion recognition-a tutorial overview",
      "authors": [
        "Y Can",
        "B Mahesh",
        "E André"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE"
    },
    {
      "citation_id": "5",
      "title": "'alexa, i feel for you!' observers' empathetic reactions towards a conversational agent",
      "authors": [
        "A Carolus",
        "C Wienrich",
        "A Törke",
        "T Friedel",
        "C Schwietering",
        "M Sperzel"
      ],
      "year": "2021",
      "venue": "Frontiers in Computer Science"
    },
    {
      "citation_id": "6",
      "title": "Survey on speech emotion recognition: Features, classification schemes, and databases",
      "authors": [
        "M El Ayadi",
        "M Kamel",
        "F Karray"
      ],
      "year": "2011",
      "venue": "Pattern Recognition"
    },
    {
      "citation_id": "7",
      "title": "OpenSMILE: The munich versatile and fast open-source audio feature extractor",
      "authors": [
        "F Eyben",
        "M Wöllmer",
        "B Schuller"
      ],
      "year": "2010",
      "venue": "Proceedings of the 18th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "8",
      "title": "The relation between gender and emotions in different cultures",
      "authors": [
        "A Fischer",
        "A Manstead"
      ],
      "year": "2000",
      "venue": "Gender and Emotion: Social Psychological Perspectives"
    },
    {
      "citation_id": "9",
      "title": "Gender and culture differences in emotion",
      "authors": [
        "A Fischer",
        "P Rodriguez Mosquera",
        "A Van Vianen",
        "A Manstead"
      ],
      "year": "2004",
      "venue": "Emotion"
    },
    {
      "citation_id": "10",
      "title": "Emotion regulation: Current status and future prospects",
      "authors": [
        "J Gross"
      ],
      "year": "2015",
      "venue": "Psychological Inquiry"
    },
    {
      "citation_id": "11",
      "title": "Alexa, siri, cortana, and more: An introduction to voice assistants",
      "authors": [
        "M Hoy"
      ],
      "year": "2018",
      "venue": "Medical Reference Services Quarterly"
    },
    {
      "citation_id": "12",
      "title": "The acoustically emotion-aware conversational agent with speech emotion recognition and empathetic responses",
      "authors": [
        "J Hu",
        "Y Huang",
        "X Hu",
        "Y Xu"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "13",
      "title": "Relationship between emotional awareness and self-acceptance: The mediating role of emotion regulation strategies",
      "authors": [
        "Z Huang",
        "S Chen",
        "H Chen"
      ],
      "year": "2024",
      "venue": "Current Psychology"
    },
    {
      "citation_id": "14",
      "title": "Speech emotion recognition using deep learning techniques: A review",
      "authors": [
        "R Khalil",
        "E Jones",
        "M Babar",
        "T Jan",
        "M Zafar",
        "T Alhussain"
      ],
      "year": "2019",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "15",
      "title": "Emotion-aware chatbots: Understanding, reacting, and adapting to human emotions in text conversations",
      "authors": [
        "P Kossack",
        "H Unger"
      ],
      "year": "2023",
      "venue": "International Conference on Autonomous Systems"
    },
    {
      "citation_id": "16",
      "title": "Multimodal emotion recognition using feature fusion: An LLM-based approach",
      "authors": [
        "C Kumar",
        "N Gowtham",
        "M Zakariah",
        "A Almazyad"
      ],
      "year": "2024",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "17",
      "title": "A review on speech emotion recognition using deep learning and attention mechanism",
      "authors": [
        "E Lieskovská",
        "M Jakubec",
        "R Jarina",
        "M Chmulík"
      ],
      "year": "2021",
      "venue": "Electronics"
    },
    {
      "citation_id": "18",
      "title": "Artificial empathy in marketing interactions: Bridging the human-AI gap in affective and social customer experience",
      "authors": [
        "Y Liu-Thompkins",
        "S Okazaki",
        "H Li"
      ],
      "year": "2022",
      "venue": "Journal of the Academy of Marketing Science"
    },
    {
      "citation_id": "19",
      "title": "Emotion-Aware Voice Interfaces Based on Speech Signal Processing",
      "authors": [
        "Y Ma"
      ],
      "year": "2022",
      "venue": "Emotion-Aware Voice Interfaces Based on Speech Signal Processing"
    },
    {
      "citation_id": "20",
      "title": "How should voice assistants deal with users",
      "authors": [
        "Y Ma",
        "H Drewes",
        "A Butz"
      ],
      "year": "2022",
      "venue": "How should voice assistants deal with users",
      "arxiv": "arXiv:2204.02212"
    },
    {
      "citation_id": "21",
      "title": "Understanding dementia speech: Towards an adaptive voice assistant for enhanced communication",
      "authors": [
        "Y Ma",
        "O Nordberg",
        "Y Zhang",
        "A Rongve",
        "M Bachinski",
        "M Fjeld"
      ],
      "year": "2024",
      "venue": "Companion Proceedings of the 16th ACM SIGCHI Symposium on Engineering Interactive Computing Systems"
    },
    {
      "citation_id": "22",
      "title": "Emotion-aware voice assistants: Design, implementation, and preliminary insights",
      "authors": [
        "Y Ma",
        "Y Zhang",
        "M Bachinski",
        "M Fjeld"
      ],
      "year": "2023",
      "venue": "Proceedings of the Eleventh International Symposium of Chinese CHI"
    },
    {
      "citation_id": "23",
      "title": "Leveraging speech PTM, text LLM, and emotional TTS for speech emotion recognition",
      "authors": [
        "Z Ma",
        "W Wu",
        "Z Zheng",
        "Y Guo",
        "Q Chen",
        "S Zhang",
        "X Chen"
      ],
      "year": "2024",
      "venue": "ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "24",
      "title": "Empathic voice assistants: Enhancing consumer responses in voice commerce",
      "authors": [
        "A Mari",
        "A Mandelli",
        "R Algesheimer"
      ],
      "year": "2024",
      "venue": "Journal of Business Research"
    },
    {
      "citation_id": "25",
      "title": "Hey alexa... examine the variables influencing the use of artificial intelligent in-home voice assistants",
      "authors": [
        "G Mclean",
        "K Osei-Frimpong"
      ],
      "year": "2019",
      "venue": "Computers in Human Behavior"
    },
    {
      "citation_id": "26",
      "title": "Cultural variations in emotions: A review",
      "authors": [
        "B Mesquita",
        "N Frijda"
      ],
      "year": "1992",
      "venue": "Psychological Bulletin"
    },
    {
      "citation_id": "27",
      "title": "Are all \"basic emotions\" emotions? a problem for the (basic) emotions construct",
      "authors": [
        "A Ortony"
      ],
      "year": "2022",
      "venue": "Perspectives on Psychological Science"
    },
    {
      "citation_id": "28",
      "title": "What's basic about basic emotions?",
      "authors": [
        "A Ortony",
        "T Turner"
      ],
      "year": "1990",
      "venue": "Psychological Review"
    },
    {
      "citation_id": "29",
      "title": "Voice-based smart system for emotion recognition and regulation",
      "authors": [
        "M Parvathi",
        "V Pranathi",
        "M Varma",
        "B Satyanarayana"
      ],
      "year": "2025",
      "venue": "Cognitive Computing and Cyber Physical Systems"
    },
    {
      "citation_id": "30",
      "title": "Empathetic conversational systems: A review of current advances, gaps, and opportunities",
      "authors": [
        "A Raamkumar",
        "Y Yang"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "31",
      "title": "Analyzing the influence of different speech data corpora and speech features on speech emotion recognition: A review",
      "authors": [
        "T Rathi",
        "M Tripathy"
      ],
      "year": "2024",
      "venue": "Analyzing the influence of different speech data corpora and speech features on speech emotion recognition: A review"
    },
    {
      "citation_id": "32",
      "title": "The role of empathy for artificial intelligence accountability",
      "authors": [
        "R Srinivasan",
        "B González"
      ],
      "year": "2022",
      "venue": "Journal of Responsible Technology"
    },
    {
      "citation_id": "33",
      "title": "An overview of affective speech synthesis and conversion in the deep learning era",
      "authors": [
        "A Triantafyllopoulos",
        "B Schuller",
        "G İymen",
        "M Sezgin",
        "X He",
        "Z Yang",
        "P Tzirakis",
        "S Liu",
        "S Mertes",
        "E André"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE"
    },
    {
      "citation_id": "34",
      "title": "A comprehensive review of speech emotion recognition systems",
      "authors": [
        "T Wani",
        "T Gunawan",
        "S Qadri",
        "M Kartiwi",
        "E Ambikairajah"
      ],
      "year": "2021",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "35",
      "title": "Interpersonal emotion regulation",
      "authors": [
        "J Zaki",
        "W Williams"
      ],
      "year": "2013",
      "venue": "Emotion"
    }
  ]
}