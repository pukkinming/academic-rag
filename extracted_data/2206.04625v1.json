{
  "paper_id": "2206.04625v1",
  "title": "Attx: Attentive Cross-Connections For Fusion Of Wearable Signals In Emotion Recognition",
  "published": "2022-06-09T17:18:33Z",
  "authors": [
    "Anubhav Bhatti",
    "Behnam Behinaein",
    "Paul Hungler",
    "Ali Etemad"
  ],
  "keywords": [
    "Multimodal",
    "representation learning",
    "fusion",
    "wearable signals",
    "emotion recognition"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "We propose cross-modal attentive connections, a new dynamic and effective technique for multimodal representation learning from wearable data. Our solution can be integrated into any stage of the pipeline, i.e., after any convolutional layer or block, to create intermediate connections between individual streams responsible for processing each modality. Additionally, our method benefits from two properties. First, it can share information uni-directionally (from one modality to the other) or bi-directionally. Second, it can be integrated into multiple stages at the same time to further allow network gradients to be exchanged in several touch-points. We perform extensive experiments on three public multimodal wearable datasets, WE-SAD, SWELL-KW, and CASE, and demonstrate that our method can effectively regulate and share information between different modalities to learn better representations. Our experiments further demonstrate that once integrated into simple CNN-based multimodal solutions (2, 3, or 4 modalities), our method can result in superior or competitive performance to state-of-the-art and outperform a variety of baseline uni-modal and classical multimodal methods.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "Affective computing is an emerging field of study for developing advanced computing systems that recognize, model, interpret, and respond to human affective states (e.g., moods and emotions)  [1] . It can be defined as an interdisciplinary field that involves psychology, computer science, and biomedical engineering  [2] ,  [3] . In general, affective computing systems enhance the quality of human-machine interaction by automatically recognizing and responding to user's emotional states, thereby making the machine interface more usable and effective. Human emotions have physiological fingerprints and can be recognized by analyzing bio-signals that can be monitored using wearables. As a result, several studies have focused on machine learning and deep learning to analyze physiological signals towards classification or quantification of human emotional states  [4] -  [7] . Recent applications of affectsensitive systems include stress and anxiety detection  [6] ,  [8] , human monitoring systems  [9] ,  [10] , healthcare (primarily mental health)  [11] ,  [12] , education  [13] , adaptive gaming, marketing, etc.  [2] .\n\nThe literature on affective computing have utilized various modalities to automatically classify emotional states such as A. Bhatti, B. Behinaein, and A. Etemad are with the Department of Electrical and Computer Engineering and Ingenuity Research Labs, Queen's University, Kingston, Ontario, Canada. E-mails: {anubhav.bhatti, 9hbb, ali.etemad}@queensu.ca P. Hungler is with Ingenuity Research Labs, Queen's University, Kingston, Ontario, Canada. E-mails: paul.hungler@queensu.ca stress. These modalities include electrocardiagram (ECG)  [5] ,  [14] , electrodermal activity (EDA), also known as, galvanic skin response  [15] , voice  [16] , posture and gait  [17] , facial expressions  [18] ,  [19] , electrooculogram (EOG)  [20] , electroencephalogram (EEG)  [7] ,  [21] , respiration (RESP)  [4] , and blood volume pulse (BVP)  [22] . In such solutions, different deep learning models are often used to learn features from different modalities that are then fed to classifiers for predicting the emotion class.\n\nIn emotion recognition, different physiological modalities comprise different information that can be combined to boost classification performance. To enhance performance, the different modalities, their learned features, or the respective decisions made by individual machine learning models, are often fused. Traditionally, the different modalities are combined using two fusion techniques, i.e., feature-level fusion also referred to as early fusion, and decision-level fusion (or scorelevel fusion) also called late fusion. In feature-level fusion, high-level embeddings from different modalities, extracted by the respective feature extractors, are concatenated, and then fed to a classifier for discriminative tasks  [23] . In decision-level fusion, modalities are provided to their respective classifiers, and the decision of each classifier is combined to obtain a final decision. Combining multiple physiological signals for emotion recognition has been the focus of many recent studies  [4] ,  [24] -  [28] . Several studies also focus on using deep learning pipelines to extract high-level embeddings from the modalities and then fuse them to classify emotion classes  [22] ,  [23] ,  [29] -  [31] . These studies use handcrafted features or extract features automatically using deep learning and then use early or late fusion to fuse the features for emotion recognition.\n\nRecent studies have shown that besides using early or late fusion, the learned representations from different hidden layers of the deep learning networks can also be combined via information-sharing branches referred to as cross-connections  [32] ,  [33] . These cross-connections can help exploit the complementary information between the different modalities effectively, leading to better performance. However, the crossconnections are generally connected directly between the hidden layers to share the learned representations in these methods. As a result, these cross-connections are not capable of performing any additional processing on the shared information to enhance the information. Also, these works do not present exhaustive experiments to show the impact of adding these information-sharing cross-connections at different stages of the networks on the overall performance.\n\nIn this paper, we introduce a novel attentive cross-modal connection (AttX) comprising an attentive feedforward network to learn enhanced shared representations for multimodal affective computing. This allows the shared information to be weighted based on its importance before being shared between the available modalities. To control the flow of information, we define three types of AttX connections: Type I shares information from the first modality to the second; Type II shares information from the second modality to the first; and Type III simultaneously shares information between both modalities. In addition, we study the behaviour of overall model when AttX is integrated in different locations (referred to as stages) in our deep learning pipeline for emotion classification. A broad overview of our work is depicted in Figure  1 . We use three publicly available datasets to evaluate our proposed solution: WESAD  [28] , SWELL-KW  [8] , and CASE  [34] . We use the fusion of 'ECG and EDA', 'BVP and EDA', and 'ECG, EDA, RESP, and skin temperature (ST)' to show the generalizability of our proposed method for enhancing multimodal fusion for wearable-based emotion recognition. Various experiments demonstrate the effectiveness of our method as models equipped with AttX outperform or obtain competitive results to uni-modal or other multimodal solutions.\n\nOur contributions can be summarized as follows: (1) We propose attentive cross-modal connections for sharing intermediate information between wearable modalities. To enable uni-directional or bi-directional information sharing between the modalities, we introduce three AttX connection types. Our proposed method can be integrated into different stages of deep learning pipelines and can successfully enhance the overall learned representations.  (2)  We extensively test our solution on three popular and public multimodal datasets (WESAD, SWELL-KW, and CASE). We exploit leave-onesubject-out cross-validation and observe that simple backbones equipped with AttX outperform the state-of-the-art methods. We show the applicability of our proposed method on different pipelines comprising VGG and ResNet encoder blocks. We perform thorough experiments to show the impact of adding AttX connections when used in a single-AttX configuration (only integrated at one stage) and a multi-AttX configuration (integrated at multiple stages) on different networks.\n\n(3) We perform analyses on the outcome of using different AttX configurations in the network to provide insights on the impact of sharing representations from one modality to another. Additionally, we show that the optimum stage to share information with Type I and Type II connections is generally halfway through the network. In contrast, Type III connections are more effective in learning better representations in later stages of the network. Further, when used in the contexts of learning multimodal ECG-EDA or BVP-EDA, our analysis shows that the sharing information from EDA to ECG or BVP is more beneficial than the opposite or the two-way connection (Type III). Lastly, our analysis shows that while single-AttX configurations across connection types and stages generally perform better than the multi-AttX configurations, the best performance of a network is often observed when a multi-AttX configuration is used simultaneously in the middle and end of the pipeline.\n\nThis paper is an extension of our work presented in  [35] , a ACII 2021 Workshop paper, compared to which this work comprises the following additions: (i) We add experiments for a 3-class classification task (neutral vs. amusement vs. stress) in WESAD dataset. (ii) We also use two additional publicly available datasets in our experiments, SWELL-KW (stress classification) and CASE (arousal classification). (iii) We show the application of AttX connections on two pipelines with different encoders: VGG and ResNet. (iv) We explore the fusion of more than two modalities using our proposed cross-modal connections. (v) Further, we perform an in-depth analysis to gain insights into selecting the direction and the optimum location for sharing intermediate information between modality streams.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Background And Related Work",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "A. Uni-Modal Affective Computing",
      "text": "Recent works in the field of affective computing have studied the use of machine learning and deep learning to determine the emotional state of subjects from physiological data. Here we review some of the works that specifically use physiological signals such as ECG, EDA, BVP, RESP, and ST. Ferdinando et al.  [36]  proposed the use of statistical distribution of dominant frequencies, calculated through spectrogram analysis. They then used k-nearest neighbours (KNN) for the classification of emotions in arousal and valence space in a multi-class setting. Hwang et al.  [37]  proposed a deep learning framework to monitor stress using ECG. They introduced a method to design a deep learning architecture based on periodic patterns of raw ECG signals. In  [38] , Sarkar et al. used ECG to classify participants' level of cognitive load and expertise using a multi-task neural network in the context of dynamically adaptive simulation. Sarkar and Etemad  [5] ,  [39]  proposed a self-supervised method for classifying arousal and valence based on ECG signals. In this method, generalized representations of unlabelled ECG were learned by training a multi-task convolutional neural network (CNN) to recognize transformations applied to the input signals as pretext tasks. This was followed by transfer learning for downstream supervised classification. Behinaein et al.  [40]  proposed a transformer mechanism to detect stress using ECG signals in two publicly available datasets. In this study, the deep learning network comprises a convolutional subnetwork, a transformer encoder, and a fully connected subnetwork for stress classification.\n\nEDA has also been used for the classification of emotions, notably stress. Hsieh et al.  [41]  used EDA and extracted time, frequency, entropy, and wavelet domain features for detecting stress using XGBoost. In a work by Setz et al.  [42] , timedomain features of EDA signals were used to discriminate stress from cognitive load in a laboratory environment using standard machine learning algorithms, e.g., linear discriminant analysis (LDA), KNN, and support vector machines (SVM). Recently, Aqajari et al.  [43]  developed a tool for the analysis of EDA, which used deep learning along with statistical algorithms to extract features for stress detection.",
      "page_start": 1,
      "page_end": 3
    },
    {
      "section_name": "B. Multimodal Affective Computing",
      "text": "The use of multimodal approaches for classification of affective states using machine learning methods has also been a focus of a number of studies in recent times. Plarre et al.  [4]  proposed two models for continuous prediction of stress from physiological signals. Handcrafted features from ECG and RESP were used for detecting stress and other psychologically and physically demanding conditions. Stamos et al.  [24]  used handcrafted features from ECG and EEG such as power spectral density and heart rate variability (HRV) to classify emotions using SVM. Ross et al.  [25]  used ECG and EDA features along with feature-level fusion to perform multimodal classification of two levels of expertise, i.e., expert and novice, using several machine learning models. AlZoubi et al.  [26]  relied upon handcrafted features from ECG, electromyography (EMG), and EDA, and used KNN and linear Bayes classifier to determine boredom, confusion, and curiosity. The work done by Das et al.  [27]  relied on frequency-domain features of ECG and EDA for the classification of happy, sad, and neutral states. Their work showed that the multimodal features with an SVM classifier perform better than the uni-modal features. Schmidt et al.  [28]  experimented on different models to perform binary classification of stress vs. non-stress states and multi-class classification of stress, amusement, and neutral states. For classification, their method relied on handcrafted statistical features, HRV, and frequency domain features from ECG, EDA, EMG, RESP, and ST. Mohammadi et al.  [44]  use Kruskal-Wallis analysis on 65 features extracted from ECG, EDA, RESP, and ST to identify features that demonstrate significant difference between stress and relaxed states and then used KNN to distinguish these states.\n\nA work by Siddharth et al.  [29]  used a combination of manually extracted features and deep representations from ECG, EDA, and EEG. An extreme learning machine then used the concatenated features to classify arousal, valence, liking, and emotions. Bota et al.  [22]  performed a multimodal classification of emotions in the arousal and valence space using feature-level and decision-level fusion of physiological data (ECG, EDA, RESP, and BVP). Lin et al.  [23]  introduced a multimodal-multisensory sequential fusion model to detect three affect states. The proposed fusion model was trained on different modalities with different sampling rates in the same training batch. Li et al.  [45]  proposed a one-dimensional (1D) CNN to automatically extract features from ECG, EDA, EMG, RESP, and ST and perform binary classification of stress versus non-stress state and multi-class classification of stress, amused, and neutral state. Bacciu et al.  [46]  benchmarked recurrent neural networks on human state and activity recognition tasks. This work showed that echo-state network models' performance on human state and activity recognition is comparable to other recurrent models and can serve as an alternative for implementing predictive models on low-powered devices. Samyoun et al.  [47]  presented a solution to detect stress using physiological data from wrist sensors that emulate the standard chest sensors. In this work, the data from wrist sensors is translated into the data from chest sensors using a generative adversarial network, recurrent neural network, or multilayer perceptron based translation model. The translated data is then used for stress detection without requiring the users to wear any device on the chest.\n\nYang et al.  [30]  proposed a variational autoencoder to learn personality-invariant physiological signal representations for ECG, EDA, and EEG. Arousal levels were then classified using an SVM by exploiting these latent representations. Ross et al.  [31]  proposed a method to use multi-corpus wearable data to learn multimodal representations in an unsupervised framework using auto-encoders followed by supervised classifiers. Yin et al.  [48]  introduced an ensemble-based multiplefusion-layer classifier of stacked auto-encoders for emotion classification in arousal and valence space using multiple physiological signals. They used handcrafted features from EEG, electrooculography, EMG, ST, EDA, BVP, and RESP signals to classify emotions.\n\nIn the end, we observe that in most of the prior works on multimodal representation learning for affective computing, either feature-level or decision-level fusion strategies have been explored. More specifically, the notion of cross-modal connections for sharing information between the models responsible for learning each modality has not been studied in this area.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Iii. Method",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "A. Problem Statement",
      "text": "Assume we have two signal modalities, for instance recorded by a wearable device, which we denote by X 1 and X 2 . Here X 1 ∈ R m1 and X 2 ∈ R m2 , where m 1 and m 2 are the dimensionality of the modalities. Further, let's assume we have two separate encoders, F 1 and F 2 , which we use for learning representations from X 1 and X 2 , respectively. Each encoder F i consists of a number of individual convolutional blocks (also henceforth referred to as 'stages'), denoted by F j i , where j is the index for a given stage. Our goal in this work is to design cross-modal connections Φ capable of exchanging information between F 1 and F 2 . We assume Φ can be integrated between F j 1 and F k 2 , where for simplicity, j = k. Accordingly, the key research questions in our work are as follows:\n\n(1) How should Φ be designed such that the flow of information can be automatically regulated between F 1 and F 2 ?\n\n(2) How does Φ impact the overall performance in comparison to uni-modal learning, standard feature-level fusion, and scorelevel fusion? (3) What are the best configurations of Φ for integration into the multimodal F 1 -F 2 architecture? In other words, should Φ share information from F 1 to F 2 (given a specific set of modalities), vice-versa, or in both directions? Moreover, in which stages of the overall pipeline should Φ be integrated, i.e., what is the optimum set of j in a F j 1 , F j 2 setup? In the following subsection, we design Φ with the capability of automatically learning to regulate the sharing of information. Following that, we design and conduct a set of experiments to address research questions 2 and 3.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "B. Proposed Solution",
      "text": "We propose Φ as attentive cross-modal connections to tackle the above-mentioned problems, henceforth referred to as AttX for simplicity. Let's define the output of each encoder block F j i as Z j i . Accordingly, we define AttX as a feedforward network that takes Z j 1 and Z j 2 as its inputs and learns weighted intermediate representations Ẑj 1 and Ẑj 2 , which are then shared between the respective encoder blocks. An overview of AttX integrated into a deep learning pipeline is depicted in Figure  2 . First, we define AttX with the aim of sharing information from the first modality (X 1 ) to (X 2 ). Accordingly, the inputs to the AttX block at stage j, which is integrated between Z j 1 and Z j 2 , are concatenated to obtain an intermediate representation tensor, S j :\n\nwhere d represents the number of input modalities, in our case d = 2, and n and m represent the dimensions of Z j 1 and Z j 2 . In case the dimensions of Z j 1 and Z j 2 are not equal, preprocessing steps can be made to modify the dimensionality such that equal dimensions are achieved.\n\nSubsequently, the intermediate representation tensor S j is fed to an attention block comprising a feedforward network with a hidden layer and an activation layer to obtain a projection of the tensor S j  [49] ,  [50] . Thus,\n\nwhere the learned weight matrix W j ∈ R d×d , and U j is the projection of the intermediate representation tensor S j .\n\nTo obtain the attention weights θ j , a softmax function is applied to U j . The attention weights are computed according to\n\nwhere w j u ∈ R m represents the learned weight vector, and softmax() computes the softmax of the dot product of the transposed projection of U j and the learned weight vector w j u along the second axis. Here, the learnt attention weight tensor θ j ∈ R n×d×m . The transpose of the second and third dimensions of the projection U j is indicated by U j T (2,3) .\n\nTo obtain the weighted intermediate representations from F j 1 , we extract the attention weights θ j 1 , corresponding to F j 1 , from the learned attention weight tensor θ j . Given d = 2, as we assume only two modalities (e.g., ECG and EDA), the θ j 1 would be:\n\nFurther, the attention weight tensor for θ j 1 is multiplied by Z j 1 , i.e., the outputs of encoder blocks F j 1 , to obtain weighted intermediate representations Ẑj\n\nwhere denotes the element-wise multiplication. In Eq. 5, Ẑj\n\nindicates the output representations from the attention block weighted by its respective attention weight tensor θ j 1 .",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "The Weighted Intermediate Representations, Ẑj",
      "text": "1 , obtained by Eq. 5, is combined with the output representations Z j 2 to generate input, X j+1 2 , for the next encoder blocks in the pipeline, i.e., F j+1",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "2",
      "text": ". This input X j+1 2 is obtained according to:\n\nwhere ⊗ denotes the concatenation operation. Figure  3  (left) presents the final architecture of AttX which shares information from X 1 to X 2 , referred to as Type I. Similar to this approach, we also define another type of AttX in which information is shared from the second modality to the first modality (X 2 to X 1 ). To do so, the entire process remains the same until Eq. 4, which will be modified as:\n\nSimilarly, in order to provide the intermediate weighted representations for X j 2 , Eq. 5 is replaced with:\n\nFinally, instead of Eq. 6, the weighted intermediate representations, Ẑj 2 , are combined with the output representations Z j 1 to generate the input X j+1 1 , as follows:\n\nThe architecture of this type of AttX, which is capable of sharing information from X 2 to X 1 is illustrated in Figure  3  (middle), which we refer to as Type II.\n\nIn addition to Type I and Type II AttX connections, we also define a third type of attentive cross-modal connections (Type III), for sharing information simultaneously between both modalities (from X 1 to X 2 and from X 2 to X 1 ). In this type of AttX, Eqs. 4 and 7 are used together to extract the attention weights for both modalities, θ j 1 and θ j 2 . Next, Eqs.  5   . The architecture of Type III connections is presented in Figure  3  (right).\n\nBeyond Two Modalities. We expand our AttX connections for integration into pipelines with more than two modalities, i.e., d > 2. Accordingly, the intermediate representation tensor S j can be denoted as\n\nThus the attention weight tensor, θ j T (2,3) (where the transpose of the second and third dimensions is represented by T (2, 3)) in Eq. 3, includes attention weights for d modalities and is represented as:\n\n, where\n\nFurther, intermediate weighted representations Ẑj 1 , Ẑj 2 , . . . , Ẑj d can be computed from their respective attention weights θ j 1 , θ j 2 , . . . , θ j d using the Eq. 5 or 8. These computed intermediate weighted representations can be combined with the output representation of the other modalities as follows:\n\nwhere 1 ≤ i ≤ d and k = {1, ..., d} -{i}.\n\nHere, there can be 2 d -1 types of AttX connections for sharing information between d modalities. For instance, as shown already for d = 2, we can have three types of connections, i.e., Type I (from X 1 to X 2 ), Type II (from X 2 to X 1 ), and Type III (from X 1 to X 2 and from X 2 to X 1 ). For three or more modalities, i.e., d > 2, we follow a greedy approach to reduce the search space to obtain an optimum AttX type. In this approach, we first attain the best AttX type for two modalities and only explore the subset of that type in combination with other modalities. For example, for d = 3, there can be seven different combinations in which we can share information between the three modalities. However, assuming that the best connection is Type II for two modalities, then using the greedy approach, we reduce the search space to just two combinations, i.e., from",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "C. Integration",
      "text": "Our proposed AttX connections can be integrated into pipelines with different backbone architectures responsible for processing each individual modality (see Figure  2 ). In this paper, we explore two commonly used CNN architecture types as the backbones for multimodal learning. For bi-modal representation learning, we design a pipeline with two branches, one for each modality. We integrate an encoder in each branch followed by FC layers, where the encoders consist of a number of individual encoder blocks. We keep the number of blocks equal between the two branches to reduce the complexity when studying all the possible integration strategies of AttX in the pipeline. Empirically, we find that four encoder blocks in each branch yield strong results. We use VGG-and ResNet-like encoder blocks as two different architectures to thoroughly understand the effect of AttX connections on the performance and the learnt embeddings. We integrate AttX connections in various combinations, i.e., Type I, II, and III at stages 1, 2, and 3 (please see Figure  2 ).\n\nFor the VGG-like encoder blocks, we use two 1D CNNs with ReLU activation and a MaxPool layer of filter size 2 and stride 2. For ResNet-like encoders, each encoder block comprises a pair of residual blocks. These residual blocks have three consecutive 1D CNNs followed by a Batch Normalization layer and ReLU activation. Encoder blocks (F j 1 and F j 2 ) at different stages (i.e., j = 1, 2, 3, and 4) have a different number of filters, filter sizes, and strides; however, F j 1 and F j 2 are kept the same for simplicity as well as a fair comparison between the different modalities under consideration. The details of each encoders are presented in Table  I .\n\nThe outputs of the fourth encoder block (F 4 1 and F 4 2 ) are fed to two fully-connected layers before merging. As we will describe later, three popular public datasets (WESAD, SWELL-KW, and CASE) are used to evaluate our work (the details are described in Section IV-A). For WESAD and SWELL-KW, two fully connected layers after the fourth encoder have 512 and 256 units, while for CASE, these two layers have 256 and 64 units. Finally, we add a classifier with SoftMax activation to generate the output class.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Iv. Experiments",
      "text": "",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "A. Datasets",
      "text": "To evaluate our proposed AttX connections on different emotion recognition tasks under different data collection settings, we use three publicly available multimodal datasets, WEarable Stress and Affect Detection (WESAD)  [28] , SWELL Knowledge Work (SWELL-KW)  [8] , and Continuously Annotated Signals of Emotion (CASE)  [34] . The details of these datasets are provided in the sections below.\n\n1) WESAD: WESAD is a multimodal dataset for stress and affect detection using wearable devices  [28] . The dataset comprises physiological and motion data collected from two sensors worn on the wrist and chest from 15 participants. A RespiBAN Professional 1  sensor, worn on the chest, is used to collect ECG, EDA, EMG, ST, 3-axis accelerometer (ACC), and RESP, at a sampling rate of 700 Hz. Three-electrode ECG is recorded from the chest, while the EDA is recorded from the rectus abdominis. An Empatica E4 2  , worn on the nondominant hand of the participants, is used to collect EDA (4 Hz), BVP (64 Hz), ST (4 Hz), and ACC (32 HZ). For our work, we only consider ECG, EDA, RESP, and ST from the chest-based sensors. The dataset contains three different affect states, namely, neutral, stress, and amusement. In the study, a baseline of 20 minutes was recorded at the beginning to induce a neutral affective state, where participants read neutral reading material, e.g., magazines. To induce the amusement condition among the participants, the participants watched eleven funny videos followed by a short neutral video of five seconds. The amusement condition lasted for approximately 6 minutes and 30 seconds. The participants performed public speaking and a mental arithmetic task for 10 minutes to simulate stress conditions. A guided meditation session to bring participants back to a neutral affective state was introduced to transition from amusement to stress conditions or vice versa. Positive and Negative Affect Schedule  [51]  scheme was used to collect the ground truths labels for the affect states at the end of each trial.\n\n2) SWELL-KW: The SWELL Knowledge Work dataset allows the study of stress and user modelling in a typical office environment  [8] . The dataset is collected from 25 participants that performed typical knowledge work such as writing reports, making presentations, reading emails, and searching for information. During the experiment, the working condition of the participants is manipulated with stressors: email interruptions and time-pressure. Participants are allowed to work on multiple tasks under normal conditions for 45 minutes. For the time-pressure session, the duration to perform similar tasks is reduced to 30 minutes. In the interruption session, the participants are asked to respond to incoming emails to distract them from their usual tasks. At the start of each experiment session, an eight-minutes relaxation period is recorded in which the subject watches nature videos. Physiological modalities such as ECG, EDA, and others are recorded. ECG signals are collected using a TMSI Mobi 3  device with self-adhesive electrodes, while EDA signals are collected using a Mobi device with finger electrodes. Both ECG and EDA are recorded at a sampling frequency of 2048 Hz. Self-reported affect scores are recorded on a scale of 1 to 9 at the end of each scenario.\n\n3) CASE: The CASE dataset focuses on the real-time continuous annotation of emotions experienced by participants while watching videos  [34] . The dataset contains eight physiological signals and annotation data from 30 participants (15 male and 15 female). These eight physiological signals, ECG, EDA, BVP, EMG, RESP, and ST, were collected at 1000 Hz. The authors developed a joystick-based annotation interface to facilitate real-time annotations for simultaneous reporting of arousal and valence. The videos were selected to elicit amused, bored, relaxed, and scared emotional states. During the experiment, participants first watched a calming video at the session's start and at the end, i.e., the cooldown phase. A blue-screen video was shown when switching from one emotional video to another. The dataset provides arousal/valence ratings on nine levels.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "B. Data Pre-Processing",
      "text": "For this work, we consider the following physiological signals for classifying emotion states: ECG, EDA, BVP, RESP, and ST. We apply basic pre-processing steps to filter the raw data. Usually, ECG signals contain EMG noise, powerline noise, baseline wander, T-wave interference, and other artifacts. To remove these artifacts, we apply a Butterworth bandpass filter with a passband frequency of 5-15 Hz  [52] . The filtered ECG signals are normalized using user-specific zscore normalization. We remove high-frequency noise from raw EDA signals by applying a lowpass filter with a cutoff frequency of 3 Hz. We use a Butterworth filter with a passband frequency of 0.5 Hz and a stopband frequency of 8 Hz for filtering raw BVP. For the raw RESP, we use a bandpass filter with a passband frequency of 0.1-0.35 Hz  [28] . ST is filtered using a Butterworth bandpass filter with a passband frequency of 0.0001-10 Hz. Like ECG, filtered EDA, RESP, and ST signals are normalized using user-specific zscore normalization. Normalized EDA signals are decomposed into skin conductance level, also known as tonic level, and skin conductance response, also known as the phasic response, using a high pass filter with a cut-off frequency of 0.05 Hz.\n\nFor WESAD, the filtered signals (ECG, EDA, RESP, and ST) are re-sampled at 256 Hz. For binary stress vs. non-stress classification, neutral and amusement classes are combined to create the non-stress class as done in prior literature  [28] ,  [47] , while for 3-class classification, stress vs. amusement vs. neutral is used for evaluation purposes. For SWELL-KW, the initial relaxation period (8 minutes) is removed from each experiment session. Also, the noisy 1-minute segment from the end of the signals is dropped. The filtered signals (ECG and EDA) are re-sampled from 2048 Hz to 256 Hz. For binary stress classification, time-pressure and interruption sessions are combined to create the stress class and neutral as nonstress class  [11] . For CASE, filtered signals (ECG, EDA, and BVP) are re-sampled from 1000 Hz to 256 Hz. The CASE dataset recorded arousal ratings on a scale of 1-9. For binary arousal classification, low class is considered if the reported arousal value by the participant is less than 5, and high class is considered for values equal to or greater than 5.\n\nThe pre-processed physiological signals are then segmented using 10-second window with 60 percent overlap and stacked into an array to form individual data samples. It is to be noted that the window size and the overlap are selected empirically.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "C. Implementation And Training Details",
      "text": "We implement our pipeline using Tensorflow on an NVIDIA GeForce RTX 2080 Ti GPU. We use a standard Adam algorithm with a learning rate of 1e-3 for optimization for WESAD and SWELL-KW, while for CASE, AdaDelta with a decay rate of 0.95 and learning rate of 5e-3 was used as it resulted in better and more stable training. For WESAD, we use focal loss with values of alpha and gamma as 4.0 and 2.0  [53] , respectively, while for SWELL-KW and CASE, we use standard cross-entropy loss. The networks are trained with a batch size of 256 for 100 epochs for all the experiments.\n\nFor evaluating our method on different architectures (both VGG and ResNet encoder pipelines), we use accuracy and F1-score with macro-averaging. For testing our model, we use Leave-One-Subject-Out (LOSO) evaluation scheme. For tuning the hyperparameters, we use twenty percent of the training set as a validation set.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "V. Results And Analysis",
      "text": "In this section, we explore the performance of our solution across various configurations. We then evaluate the optimum type and location for our proposed cross-modal connections across different datasets and modality combinations. We then further analyze our method by visualizing the learned representations. We wrap up our results section by comparing our method with state-of-the-art solutions in the field.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "A. Performance",
      "text": "We present the performance of AttX for all the configurations (different types and stages) on the three datasets. Figure  4  column 1 shows the performance of different AttX configurations for WESAD, 3-class classification, with VGG and ResNet backbones. For Type I connection, the best performance from the VGG pipeline (accuracy of 78.60 and f1 of 72.43, hereafter mentioned in the same order) is obtained at stage 2. In contrast, the ResNet pipeline achieves the best performance at stages  1 & 3 (78.58, 68.20)",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "6Wdjh",
      "text": ")\n\n)\n\n) )YV6WDJHIRU6:(//.:\n\n)\n\n)\n\n)\n\n)\n\n)YV6WDJHIRU6:(//.:\n\n)\n\n) Type II, the best performance for VGG is achieved at stage 1 (65.  20, 62.25)",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "B. Analysis",
      "text": "To further explore the effectiveness of different AttX configurations and get a holistic sense of which Type of AttX connection works best at which stage(s), we merge the performance of all the models by taking the arithmetic mean of accuracy as well as f1 across all the datasets. To help us understand which stage works best with what type of AttX connection, we plot accuracy and f1-scores at each stage for Types I, II, and II in Figure  5 . Figure  5a  shows that for Type I, fusing the modalities at stage 2 benefits the model the most; interestingly, fusion at stage 3 also yields comparable performance. Following the same trend, shown in Figure  5b , the ideal stage to fuse the modalities for Type II is stage 2. When embeddings from both the modalities are shared, i.e., Type III connection, it seems better to perform the fusion later, i.e., stage 3 (Figure  5c ). Among the multiinstance configurations, we observe that for Types I and II, AttX connections at stages 2 & 3 perform better than the other multi-AttX configurations, while for Type I, AttX connections  at stages 1 & 2 perform better. It is interesting to note that while the average performance of single-connections (AttX applied only in one stage of the network) is better than that of multi-stage, the best performances are generally achieved with multi-stage connections. Figure  6  shows the AttX connection types and their accumulated performance for VGG and ResNet models across all the datasets. We observe that for the VGG encoder network, the Type II AttX connections outperform the other two types, i.e., I and III, by 2.72 % and 2.76 % in accuracy, and 3.2% and 2.56% in f1, respectively. For the ResNet encoder network, Type II AttX connections perform better than Type I and III by 0.9% and 1.3% in accuracy and 1.31% and 1% in f1, respectively. Accordingly, we can conclude that sharing information from EDA to ECG or BVP is more beneficial for the model, as opposed to the other direction or two-way connections.\n\nFigure  7  shows the performance of all AttX connections at different stages accumulated over all datasets. The plot helps us understand the impact of adding AttX connections at different stages of the network and gives us more sense in identifying the optimum stage(s) to fuse the physiological modalities. The figure shows that our networks (VGG and ResNet) perform better when the fusion of the modalities occurs at stage 2, closely followed by stage 3. This indicates that early attentive fusion of the modalities (stage) generally results in sub-optimal learning.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "C. Representation Visualization",
      "text": "We explore the learned multimodal embeddings from our models with and without AttX connections to better understand the impact of sharing the intermediate information between pipelines. To visualize the high dimensional embedding space in 2D space, we utilize Uniform Manifold Approximation and Projection (UMAP)  [54]  to perform the dimensionality reduction. Figure  8  shows a comparison between learned multimodal embeddings when only feature fusion is used (first row) and when Attx connections are introduced in the network (second row). We observe that when feature fusion is performed, the learned embeddings from each class lie close to each other. However, by adding the AttX connections in the same setting, the embeddings from each class become more separable, which is a desired outcome as it helps the classifier classify each class better.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "D. Comparison With The State-Of-The-Art",
      "text": "In this section, we compare the performance of our bestperforming configurations on three datasets, i.e., WESAD (binary and 3-class problems), SWELL-KW, and CASE, to existing state-of-the-art methods and a number of baseline models.\n\n1) WESAD: Several recent studies have focused on using WESAD dataset to evaluate their methods. The works presented in  [5] ,  [22] ,  [23] ,  [28] ,  [40] ,  [44] -  [47] , and  [55]  use WESAD for emotion classification. However, the evaluation criteria used in  [5] ,  [44] -  [46]  is not LOSO, which prevents a fair comparison.\n\nFor 3-class problem, several prior works  [23] ,  [28] ,  [55]  have used all the following modalities, ECG, EDA, EMG, RESP, ACC, and ST, which they collectively refer to as 'chest modalities'. When standard LOSO evaluation is used, Table  II  shows that our model with two modalities, i.e., ECG and EDA, can outperform  [28] ,  [47] ,  [56]  by achieving an accuracy of 81.57 and an f1 of 75.68. To compare our method with  [23] , we follow the same evaluation criteria used in this work and achieve an accuracy of 89.57 and an f1-score of 82.57. Further, when RESP and ST are fused as the third and the fourth modalities in our network, we achieve accuracy and f1 of 83.30 and 76.12, which is competitive to  [55] . We also compare our results with uni-modal baselines using ECG and EDA, and traditional fusion techniques such as feature fusion  we observe that by adding AttX connections, the performance of the model is increased by approximately 5% and 0.8%, respectively.\n\n2) SWELL-KW: The works presented in  [5] ,  [11] ,  [40] ,  [58] -  [61] , and  [62]  use SWELL-KW for classification of workplace stress. However, studies  [5] ,  [58] -  [61]  use a different cross-validation scheme that prevents them from evaluating their method on unseen data. So, to have a fair comparison, we compare our results with studies  [11] ,  [40]  as they use the LOSO validation scheme. Table  IV  presents the performance of our model in comparison to the uni-modal baselines, feature fusion, score fusion and state-of-the-art methods. The table shows that our best configuration of AttX connection with only ECG and EDA fusion achieves an accuracy of 65.20 and an f1 of 62.25, outperforming the state-of-the-art method  [11]  by approximately 6.3%. Also, we show that AttX connections perform better than the feature fusion and score fusion techniques by 4% and 4.8%, respectively.\n\n3) CASE: For the CASE dataset, we present the performance of our model using a combination of ECG, EDA, BVP, and ST in Table  V . We compare the performance of our model with the uni-model baselines, feature fusion, and score fusion techniques. No comparison to state-of-the-art is performed as seminal works on this dataset were not published at the time when our study was in progress. The original paper, which introduced and accompanied the dataset  [34]  also did not provide any baseline values. The table shows that using fusion techniques such as feature and score fusion, the performance of the model is increased compared to the uni-modal models of modalities ECG, EDA, and BVP. We observe that adding AttX connections further increases the performance. For instance,",
      "page_start": 10,
      "page_end": 11
    },
    {
      "section_name": "Vi. Conclusion",
      "text": "In this paper, we propose a novel attentive cross-modal connection, AttX, for learning representation from multimodal wearable data. These connections comprise an attentive feedforward network that can be integrated at any stage of the pipeline to create intermediate connections between individual streams for processing each modality. These intermediate cross-connections share weighted information (based on the importance) between the modality streams. To control the flow of information, we introduce different types of AttX connections that can share intermediate information in uni-and bi-directional formats. We perform extensive experiments to demonstrate the effectiveness of our proposed solution on three publicly available datasets using different backbones networks. The experiments show that sharing information from EDA to ECG or BVP enhances the model performance the most. Also, the optimum stage to fuse modalities is stage 2 (midfusion), closely followed by stage 3. Our experiments establish that the proposed method achieves superior or competitive performance by integrating AttX connections into simple CNN-based multimodal solutions compared to baseline unimodel, classical multimodal, and state-of-the-art methods.",
      "page_start": 11,
      "page_end": 12
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: An overview of our study is presented where attentive",
      "page": 1
    },
    {
      "caption": "Figure 1: We use three publicly available datasets to evaluate",
      "page": 2
    },
    {
      "caption": "Figure 2: A multimodal pipeline consisting of two CNNs, one for learning ECG and the other for learning EDA, is presented.",
      "page": 4
    },
    {
      "caption": "Figure 3: The three conﬁgurations for the proposed attentive cross-modal connections are presented: Type I: ECG to EDA, Type",
      "page": 4
    },
    {
      "caption": "Figure 2: First, we deﬁne AttX with the aim of sharing information from",
      "page": 4
    },
    {
      "caption": "Figure 3: (middle), which we refer to as Type II.",
      "page": 5
    },
    {
      "caption": "Figure 2: ). In this",
      "page": 5
    },
    {
      "caption": "Figure 4: presents the results where WESAD 3-class is depicted in",
      "page": 7
    },
    {
      "caption": "Figure 4: column 1 shows the performance of different",
      "page": 7
    },
    {
      "caption": "Figure 4: presents the performance of AttX for",
      "page": 7
    },
    {
      "caption": "Figure 4: Each plot presents the Accuracy or F1 values versus different stages where AttX has been integrated, for different AttX",
      "page": 8
    },
    {
      "caption": "Figure 4: column 4. The Type I connection",
      "page": 8
    },
    {
      "caption": "Figure 5: Figure 5a shows",
      "page": 8
    },
    {
      "caption": "Figure 5: b, the ideal stage to fuse the modalities for Type II",
      "page": 8
    },
    {
      "caption": "Figure 5: c). Among the multi-",
      "page": 8
    },
    {
      "caption": "Figure 5: Plots for average Accuracy (left) and F1 (right) for",
      "page": 9
    },
    {
      "caption": "Figure 6: Plots for average Accuracy (left) and F1 (right) for",
      "page": 9
    },
    {
      "caption": "Figure 7: Plots for average Accuracy (left) and F1 (right) for",
      "page": 9
    },
    {
      "caption": "Figure 6: shows the AttX connection types and their accumu-",
      "page": 9
    },
    {
      "caption": "Figure 7: shows the performance of all AttX connections",
      "page": 9
    },
    {
      "caption": "Figure 8: Visualization of learned multimodal embeddings, using UMAP, when only feature fusion is used (ﬁrst row) and when",
      "page": 10
    },
    {
      "caption": "Figure 8: shows a comparison between",
      "page": 10
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Block No.": "",
          "Backbone Architecture": "VGG Blocks"
        },
        {
          "Block No.": "Block 1",
          "Backbone Architecture": "Conv1D, 64, 32, 1\nConv1D, 64, 32, 3"
        },
        {
          "Block No.": "Block 2",
          "Backbone Architecture": "Conv1D, 32, 64, 1\nConv1D, 32, 64, 3"
        },
        {
          "Block No.": "Block 3",
          "Backbone Architecture": "Conv1D, 17, 128, 1\nConv1D, 17, 128, 3"
        },
        {
          "Block No.": "Block 4",
          "Backbone Architecture": "Conv1D, 7, 256, 1\nConv1D, 7, 256, 3"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "\u0000+\u0000L\u0000J\u0000K\u0000\u0003\u0000$\u0000U\u0000R\u0000X\u0000V\u0000D\u0000O\n\u0000/\u0000R\u0000Z\u0000\u0003\u0000$\u0000U\u0000R\u0000X\u0000V\u0000D\u0000O": ""
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "\u00001\u0000H\u0000X\u0000W\u0000U\u0000D\u0000O": "\u0000$\u0000P\u0000X\u0000V\u0000H\u0000P\u0000H\u0000Q\u0000W\n\u00006\u0000W\u0000U\u0000H\u0000V\u0000V"
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "\u0000\u0019": "\u0000\u0017\n\u0000\u0015\n\u0000\u0013\n\u0000\u0015\n\u0000\u0017",
          "\u0000+\u0000L\u0000J\u0000K\u0000\u0003\u0000$\u0000U\u0000R\u0000X\u0000V\u0000D\u0000O\n\u0000/\u0000R\u0000Z\u0000\u0003\u0000$\u0000U\u0000R\u0000X\u0000V\u0000D\u0000O": ""
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "[28]": "[56]",
          "AdaBoost": "Gradient Boost",
          "All chest mod.": "All chest mod.",
          "80.34": "80.00",
          "72.51": "79.00"
        },
        {
          "[28]": "[47]",
          "AdaBoost": "Random Forest",
          "All chest mod.": "All chest mod.",
          "80.34": "80.50",
          "72.51": "67.10"
        },
        {
          "[28]": "[55]",
          "AdaBoost": "CNN",
          "All chest mod.": "All chest mod.",
          "80.34": "81.87",
          "72.51": "81.21"
        },
        {
          "[28]": "[23]",
          "AdaBoost": "CNN",
          "All chest mod.": "All chest mod.",
          "80.34": "83.00",
          "72.51": "81.00‡"
        },
        {
          "[28]": "Baselines",
          "AdaBoost": "VGG",
          "All chest mod.": "ECG",
          "80.34": "56.62",
          "72.51": "46.00"
        },
        {
          "[28]": "",
          "AdaBoost": "VGG",
          "All chest mod.": "EDA",
          "80.34": "79.23",
          "72.51": "73.16"
        },
        {
          "[28]": "",
          "AdaBoost": "ResNet",
          "All chest mod.": "ECG",
          "80.34": "45.60",
          "72.51": "36.63"
        },
        {
          "[28]": "",
          "AdaBoost": "ResNet",
          "All chest mod.": "EDA",
          "80.34": "71.01",
          "72.51": "65.15"
        },
        {
          "[28]": "",
          "AdaBoost": "VGG (feat.\nfus.)",
          "All chest mod.": "ECG, EDA",
          "80.34": "74.36",
          "72.51": "67.00"
        },
        {
          "[28]": "",
          "AdaBoost": "VGG (score fus.)",
          "All chest mod.": "ECG, EDA",
          "80.34": "74.27",
          "72.51": "65.70"
        },
        {
          "[28]": "",
          "AdaBoost": "ResNet\n(feat.\nfus.)",
          "All chest mod.": "ECG, EDA",
          "80.34": "74.70",
          "72.51": "69.58"
        },
        {
          "[28]": "",
          "AdaBoost": "ResNet\n(score fus.)",
          "All chest mod.": "ECG, EDA",
          "80.34": "67.68",
          "72.51": "52.56"
        },
        {
          "[28]": "Ours",
          "AdaBoost": "VGG Type II\n[1,2,3]",
          "All chest mod.": "ECG, EDA",
          "80.34": "81.57",
          "72.51": "75.68"
        },
        {
          "[28]": "",
          "AdaBoost": "VGG Type II† [1,2,3]*",
          "All chest mod.": "ECG, EDA, RESP, ST",
          "80.34": "89.57",
          "72.51": "82.57"
        },
        {
          "[28]": "",
          "AdaBoost": "VGG Type II† [1,2,3]",
          "All chest mod.": "ECG, EDA, RESP, ST",
          "80.34": "83.30",
          "72.51": "76.12"
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "[11]": "[40]",
          "SVM": "CNN + Transformer",
          "-": "ECG",
          "58.90": "58.10",
          "–": "58.50"
        },
        {
          "[11]": "Baselines",
          "SVM": "VGG",
          "-": "ECG",
          "58.90": "58.19",
          "–": "47.75"
        },
        {
          "[11]": "",
          "SVM": "VGG",
          "-": "EDA",
          "58.90": "60.33",
          "–": "58.07"
        },
        {
          "[11]": "",
          "SVM": "ResNet",
          "-": "ECG",
          "58.90": "52.65",
          "–": "46.72"
        },
        {
          "[11]": "",
          "SVM": "ResNet",
          "-": "EDA",
          "58.90": "59.03",
          "–": "54.23"
        },
        {
          "[11]": "",
          "SVM": "VGG (feat.\nfus.)",
          "-": "ECG, EDA",
          "58.90": "61.07",
          "–": "56.94"
        },
        {
          "[11]": "",
          "SVM": "VGG (score fus.)",
          "-": "ECG, EDA",
          "58.90": "60.34",
          "–": "50.52"
        },
        {
          "[11]": "",
          "SVM": "ResNet\n(feat.\nfus.)",
          "-": "ECG, EDA",
          "58.90": "57.30",
          "–": "52.10"
        },
        {
          "[11]": "",
          "SVM": "ResNet\n(score fus.)",
          "-": "ECG, EDA",
          "58.90": "53.00",
          "–": "47.19"
        },
        {
          "[11]": "Ours",
          "SVM": "VGG Type II\n[1]",
          "-": "ECG, EDA",
          "58.90": "65.20",
          "–": "62.25"
        }
      ],
      "page": 11
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "[40]": "[57]",
          "CNN": "CNN",
          "ECG": "EDA",
          "80.40": "91.67",
          "69.70": "82.29"
        },
        {
          "[40]": "",
          "CNN": "CNN",
          "ECG": "All modalities",
          "80.40": "83.28",
          "69.70": "67.53"
        },
        {
          "[40]": "[22]",
          "CNN": "QDA (DF)",
          "ECG": "EDA, ECG, BVP, RSP",
          "80.40": "85.80",
          "69.70": "–"
        },
        {
          "[40]": "",
          "CNN": "QDA (FF)",
          "ECG": "EDA, ECG, BVP, RSP",
          "80.40": "87.60",
          "69.70": "19.40"
        },
        {
          "[40]": "[47]",
          "CNN": "Extra Tree",
          "ECG": "All chest mod.",
          "80.40": "91.10",
          "69.70": "90.20"
        },
        {
          "[40]": "[28]",
          "CNN": "LDA",
          "ECG": "All chest mod.",
          "80.40": "93.12",
          "69.70": "91.47"
        },
        {
          "[40]": "[55]",
          "CNN": "CNN",
          "ECG": "All chest mod.",
          "80.40": "93.20",
          "69.70": "92.70"
        },
        {
          "[40]": "Baselines",
          "CNN": "VGG",
          "ECG": "ECG",
          "80.40": "86.54",
          "69.70": "84.93"
        },
        {
          "[40]": "",
          "CNN": "VGG",
          "ECG": "EDA",
          "80.40": "87.32",
          "69.70": "86.76"
        },
        {
          "[40]": "",
          "CNN": "ResNet",
          "ECG": "ECG",
          "80.40": "85.54",
          "69.70": "83.93"
        },
        {
          "[40]": "",
          "CNN": "ResNet",
          "ECG": "EDA",
          "80.40": "84.32",
          "69.70": "80.76"
        },
        {
          "[40]": "",
          "CNN": "VGG (feat.\nfus.)",
          "ECG": "ECG, EDA",
          "80.40": "87.52",
          "69.70": "86.54"
        },
        {
          "[40]": "",
          "CNN": "VGG (score fus.)",
          "ECG": "ECG, EDA",
          "80.40": "92.02",
          "69.70": "90.02"
        },
        {
          "[40]": "",
          "CNN": "ResNet\n(feat.\nfus.)",
          "ECG": "ECG, EDA",
          "80.40": "86.00",
          "69.70": "84.04"
        },
        {
          "[40]": "",
          "CNN": "ResNet\n(score fus.)",
          "ECG": "ECG, EDA",
          "80.40": "85.19",
          "69.70": "81.56"
        },
        {
          "[40]": "Ours",
          "CNN": "VGG Type II\n[2,3]",
          "ECG": "ECG, EDA",
          "80.40": "92.90",
          "69.70": "91.73"
        },
        {
          "[40]": "",
          "CNN": "VGG Type II† [2,3]",
          "ECG": "ECG, EDA, RESP",
          "80.40": "93.70",
          "69.70": "93.28"
        }
      ],
      "page": 11
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Baselines": "",
          "VGG": "VGG",
          "ECG": "EDA",
          "50.44": "56.25",
          "47.38": "52.11"
        },
        {
          "Baselines": "",
          "VGG": "ResNet",
          "ECG": "ECG",
          "50.44": "59.45",
          "47.38": "54.48"
        },
        {
          "Baselines": "",
          "VGG": "ResNet",
          "ECG": "EDA",
          "50.44": "59.63",
          "47.38": "56.69"
        },
        {
          "Baselines": "",
          "VGG": "VGG",
          "ECG": "BVP",
          "50.44": "53.34",
          "47.38": "50.93"
        },
        {
          "Baselines": "",
          "VGG": "ResNet",
          "ECG": "BVP",
          "50.44": "60.48",
          "47.38": "53.26"
        },
        {
          "Baselines": "",
          "VGG": "VGG (feat.\nfus.)",
          "ECG": "ECG, EDA",
          "50.44": "63.02",
          "47.38": "59.11"
        },
        {
          "Baselines": "",
          "VGG": "VGG (score fus.)",
          "ECG": "ECG, EDA",
          "50.44": "52.20",
          "47.38": "49.21"
        },
        {
          "Baselines": "",
          "VGG": "ResNet\n(feat.\nfus.)",
          "ECG": "ECG, EDA",
          "50.44": "63.53",
          "47.38": "57.78"
        },
        {
          "Baselines": "",
          "VGG": "ResNet\n(score fus.)",
          "ECG": "ECG, EDA",
          "50.44": "60.28",
          "47.38": "55.75"
        },
        {
          "Baselines": "",
          "VGG": "VGG (feat.\nfus.)",
          "ECG": "BVP, EDA",
          "50.44": "66.09",
          "47.38": "60.05"
        },
        {
          "Baselines": "",
          "VGG": "VGG (score fus.)",
          "ECG": "BVP, EDA",
          "50.44": "52.57",
          "47.38": "49.36"
        },
        {
          "Baselines": "",
          "VGG": "ResNet\n(feat.\nfus.)",
          "ECG": "BVP, EDA",
          "50.44": "63.95",
          "47.38": "61.00"
        },
        {
          "Baselines": "",
          "VGG": "ResNet\n(score fus.)",
          "ECG": "BVP, EDA",
          "50.44": "60.78",
          "47.38": "55.53"
        },
        {
          "Baselines": "Ours",
          "VGG": "VGG Type II\n[1,2,3]",
          "ECG": "ECG, EDA",
          "50.44": "66.72",
          "47.38": "61.00"
        },
        {
          "Baselines": "",
          "VGG": "VGG Type II\n[2, 3]",
          "ECG": "BVP, EDA",
          "50.44": "67.48",
          "47.38": "62.16"
        },
        {
          "Baselines": "",
          "VGG": "VGG Type II† [2,3]",
          "ECG": "ECG, EDA, ST",
          "50.44": "70.15",
          "47.38": "67.84"
        },
        {
          "Baselines": "",
          "VGG": "VGG Type II† [2,3]",
          "ECG": "BVP, EDA, ST",
          "50.44": "71.00",
          "47.38": "68.89"
        }
      ],
      "page": 11
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Affective computing: challenges",
      "authors": [
        "R Picard"
      ],
      "year": "2003",
      "venue": "International Journal of Human-Computer Studies"
    },
    {
      "citation_id": "2",
      "title": "Affective computing in virtual reality: emotion recognition from brain and heartbeat dynamics using wearable sensors",
      "authors": [
        "J Marín-Morales",
        "J Higuera-Trujillo",
        "A Greco",
        "J Guixeres",
        "C Llinares",
        "E Scilingo",
        "M Alcañiz",
        "G Valenza"
      ],
      "year": "2018",
      "venue": "Scientific reports"
    },
    {
      "citation_id": "3",
      "title": "Affect detection: An interdisciplinary review of models, methods, and their applications",
      "authors": [
        "R Calvo",
        "S Mello"
      ],
      "year": "2010",
      "venue": "IEEE Transactions on affective computing"
    },
    {
      "citation_id": "4",
      "title": "Continuous inference of psychological stress from sensory measurements collected in the natural environment",
      "authors": [
        "K Plarre",
        "A Raij",
        "S Hossain",
        "A Ali",
        "M Nakajima",
        "M Al'absi",
        "E Ertin",
        "T Kamarck",
        "S Kumar",
        "M Scott"
      ],
      "year": "2011",
      "venue": "Proceedings of the 10th ACM/IEEE International Conference on Information Processing in Sensor Networks"
    },
    {
      "citation_id": "5",
      "title": "Self-supervised ecg representation learning for emotion recognition",
      "authors": [
        "P Sarkar",
        "A Etemad"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "6",
      "title": "Detecting stress during real-world driving tasks using physiological sensors",
      "authors": [
        "J Healey",
        "R Picard"
      ],
      "year": "2005",
      "venue": "IEEE Transactions on intelligent transportation systems"
    },
    {
      "citation_id": "7",
      "title": "Parse: Pairwise alignment of representations in semi-supervised eeg learning for emotion recognition",
      "authors": [
        "G Zhang",
        "A Etemad"
      ],
      "year": "2022",
      "venue": "Parse: Pairwise alignment of representations in semi-supervised eeg learning for emotion recognition",
      "arxiv": "arXiv:2202.05400"
    },
    {
      "citation_id": "8",
      "title": "The swell knowledge work dataset for stress and user modeling research",
      "authors": [
        "S Koldijk",
        "M Sappelli",
        "S Verberne",
        "M Neerincx",
        "W Kraaij"
      ],
      "year": "2014",
      "venue": "Proceedings of the 16th international conference on multimodal interaction"
    },
    {
      "citation_id": "9",
      "title": "Personalized multitask learning for predicting tomorrow's mood, stress, and health",
      "authors": [
        "S Taylor",
        "N Jaques",
        "E Nosakhare",
        "A Sano",
        "R Picard"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "10",
      "title": "A deep learning approach to monitoring and detecting atrial fibrillation using wearable technology",
      "authors": [
        "S Shashikumar",
        "A Shah",
        "Q Li",
        "G Clifford",
        "S Nemati"
      ],
      "year": "2017",
      "venue": "2017 IEEE EMBS International Conference on Biomedical & Health Informatics (BHI)"
    },
    {
      "citation_id": "11",
      "title": "Detecting work stress in offices by combining unobtrusive sensors",
      "authors": [
        "S Koldijk",
        "M Neerincx",
        "W Kraaij"
      ],
      "year": "2016",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "12",
      "title": "Detection of maternal and fetal stress from the electrocardiogram with self-supervised representation learning",
      "authors": [
        "P Sarkar",
        "S Lobmaier",
        "B Fabre",
        "D González",
        "A Mueller",
        "M Frasch",
        "M Antonelli",
        "A Etemad"
      ],
      "year": "2021",
      "venue": "Scientific reports"
    },
    {
      "citation_id": "13",
      "title": "The future of simulation-based medical education: Adaptive simulation utilizing a deep multitask neural network",
      "authors": [
        "A Ruberto",
        "D Rodenburg",
        "K Ross",
        "P Sarkar",
        "P Hungler",
        "A Etemad",
        "D Howes",
        "D Clarke",
        "J Mclellan",
        "D Wilson"
      ],
      "year": "2021",
      "venue": "AEM Education and Training"
    },
    {
      "citation_id": "14",
      "title": "Ecg pattern analysis for emotion detection",
      "authors": [
        "F Agrafioti",
        "D Hatzinakos",
        "A Anderson"
      ],
      "year": "2011",
      "venue": "IEEE Transactions on affective computing"
    },
    {
      "citation_id": "15",
      "title": "Stress detection in computer users based on digital signal processing of noninvasive physiological variables",
      "authors": [
        "J Zhai",
        "A Barreto"
      ],
      "year": "2006",
      "venue": "2006 international conference of the IEEE engineering in medicine and biology society"
    },
    {
      "citation_id": "16",
      "title": "Emotion recognition by speech signals",
      "authors": [
        "O.-W Kwon",
        "K Chan",
        "J Hao",
        "T.-W Lee"
      ],
      "year": "2003",
      "venue": "Eighth European conference on speech communication and technology"
    },
    {
      "citation_id": "17",
      "title": "Classification and translation of style and affect in human motion using rbf neural networks",
      "authors": [
        "S Etemad",
        "A Arya"
      ],
      "year": "2014",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "18",
      "title": "A deep framework for facial emotion recognition using light field images",
      "authors": [
        "A Sepas-Moghaddam",
        "A Etemad",
        "P Correia",
        "F Pereira"
      ],
      "year": "2019",
      "venue": "2019 8th International Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "19",
      "title": "Facial emotion recognition using light field images with deep attention-based bidirectional lstm",
      "authors": [
        "A Sepas-Moghaddam",
        "A Etemad",
        "F Pereira",
        "P Correia"
      ],
      "year": "2020",
      "venue": "ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "20",
      "title": "Capsule attention for multimodal eeg and eog spatiotemporal representation learning with application to driver vigilance estimation",
      "authors": [
        "G Zhang",
        "A Etemad"
      ],
      "year": "2019",
      "venue": "Capsule attention for multimodal eeg and eog spatiotemporal representation learning with application to driver vigilance estimation",
      "arxiv": "arXiv:1912.07812"
    },
    {
      "citation_id": "21",
      "title": "Rfnet: Riemannian fusion network for eeg-based brain-computer interfaces",
      "year": "2020",
      "venue": "Rfnet: Riemannian fusion network for eeg-based brain-computer interfaces",
      "arxiv": "arXiv:2008.08633"
    },
    {
      "citation_id": "22",
      "title": "Emotion assessment using feature fusion and decision fusion classification based on physiological data: Are we there yet?",
      "authors": [
        "P Bota",
        "C Wang",
        "A Fred",
        "H Silva"
      ],
      "year": "2020",
      "venue": "Sensors"
    },
    {
      "citation_id": "23",
      "title": "An explainable deep fusion network for affect recognition using physiological signals",
      "authors": [
        "J Lin",
        "S Pan",
        "C Lee",
        "S Oviatt"
      ],
      "year": "2019",
      "venue": "Proceedings of the 28th ACM International Conference on Information and Knowledge Management"
    },
    {
      "citation_id": "24",
      "title": "Dreamer: A database for emotion recognition through eeg and ecg signals from wireless low-cost offthe-shelf devices",
      "authors": [
        "S Katsigiannis",
        "N Ramzan"
      ],
      "year": "2017",
      "venue": "IEEE Journal of Biomedical and Health Informatics"
    },
    {
      "citation_id": "25",
      "title": "Toward dynamically adaptive simulation: Multimodal classification of user expertise using wearable devices",
      "authors": [
        "K Ross",
        "P Sarkar",
        "D Rodenburg",
        "A Ruberto",
        "P Hungler",
        "A Szulewski",
        "D Howes",
        "A Etemad"
      ],
      "year": "2019",
      "venue": "Sensors"
    },
    {
      "citation_id": "26",
      "title": "Detecting naturalistic expressions of nonbasic affect using physiological signals",
      "authors": [
        "O Alzoubi",
        "S Mello",
        "R Calvo"
      ],
      "year": "2012",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "27",
      "title": "Emotion recognition employing ecg and gsr signals as markers of ans",
      "authors": [
        "P Das",
        "A Khasnobish",
        "D Tibarewala"
      ],
      "year": "2016",
      "venue": "2016 Conference on Advances in Signal Processing"
    },
    {
      "citation_id": "28",
      "title": "Introducing wesad, a multimodal dataset for wearable stress and affect detection",
      "authors": [
        "P Schmidt",
        "A Reiss",
        "R Duerichen",
        "C Marberger",
        "K Van Laerhoven"
      ],
      "year": "2018",
      "venue": "Proceedings of the 20th ACM International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "29",
      "title": "Utilizing deep learning towards multi-modal bio-sensing and vision-based affective computing",
      "authors": [
        "S Siddharth",
        "T.-P Jung",
        "T Sejnowski"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "30",
      "title": "An attribute-invariant variational learning for emotion recognition using physiology",
      "authors": [
        "H.-C Yang",
        "C.-C Lee"
      ],
      "year": "2019",
      "venue": "2019 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "31",
      "title": "Unsupervised multi-modal representation learning for affective computing with multi-corpus wearable data",
      "authors": [
        "K Ross",
        "P Hungler",
        "A Etemad"
      ],
      "year": "2020",
      "venue": "Unsupervised multi-modal representation learning for affective computing with multi-corpus wearable data"
    },
    {
      "citation_id": "32",
      "title": "X-cnn: Crossmodal convolutional neural networks for sparse datasets",
      "authors": [
        "P Veličković",
        "D Wang",
        "N Lane",
        "P Liò"
      ],
      "year": "2016",
      "venue": "2016 IEEE Symposium Series on Computational Intelligence (SSCI)"
    },
    {
      "citation_id": "33",
      "title": "Xflow: Cross-modal deep neural networks for audiovisual classification",
      "authors": [
        "C Cangea",
        "P Veličković",
        "P Liò"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Neural Networks and Learning Systems"
    },
    {
      "citation_id": "34",
      "title": "A dataset of continuous affect annotations and physiological signals for emotion analysis",
      "authors": [
        "K Sharma",
        "C Castellini",
        "E Van Den Broek",
        "A Albu-Schaeffer",
        "F Schwenker"
      ],
      "year": "2019",
      "venue": "Scientific data"
    },
    {
      "citation_id": "35",
      "title": "Attentive cross-modal connections for deep multimodal wearable-based emotion recognition",
      "authors": [
        "A Bhatti",
        "B Behinaein",
        "D Rodenburg",
        "P Hungler",
        "A Etemad"
      ],
      "year": "2021",
      "venue": "2021 9th International Conference on Affective Computing and Intelligent Interaction Workshops and Demos"
    },
    {
      "citation_id": "36",
      "title": "Comparing features from ecg pattern and hrv analysis for emotion recognition system",
      "authors": [
        "H Ferdinando",
        "T Seppänen",
        "E Alasaarela"
      ],
      "year": "2016",
      "venue": "2016 IEEE Conference on Computational Intelligence in Bioinformatics and Computational Biology"
    },
    {
      "citation_id": "37",
      "title": "Deep ecgnet: An optimal deep learning framework for monitoring mental stress using ultra short-term ecg signals",
      "authors": [
        "B Hwang",
        "J You",
        "T Vaessen",
        "I Myin-Germeys",
        "C Park",
        "B.-T Zhang"
      ],
      "year": "2018",
      "venue": "TELEMEDICINE and e-HEALTH"
    },
    {
      "citation_id": "38",
      "title": "Classification of cognitive load and expertise for adaptive simulation using deep multitask learning",
      "authors": [
        "P Sarkar",
        "K Ross",
        "A Ruberto",
        "D Rodenbura",
        "P Hungler",
        "A Etemad"
      ],
      "year": "2019",
      "venue": "2019 8th International Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "39",
      "title": "Self-supervised learning for ecg-based emotion recognition",
      "authors": [
        "P Sarkar",
        "A Etemad"
      ],
      "year": "2020",
      "venue": "2020 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "40",
      "title": "A transformer architecture for stress detection from ecg",
      "authors": [
        "B Behinaein",
        "A Bhatti",
        "D Rodenburg",
        "P Hungler",
        "A Etemad"
      ],
      "year": "2021",
      "venue": "2021 International Symposium on Wearable Computers"
    },
    {
      "citation_id": "41",
      "title": "Feature selection framework for xgboost based on electrodermal activity in stress detection",
      "authors": [
        "C.-P Hsieh",
        "Y.-T Chen",
        "W.-K Beh",
        "A.-Y Wu"
      ],
      "year": "2019",
      "venue": "2019 IEEE International Workshop on Signal Processing Systems (SiPS)"
    },
    {
      "citation_id": "42",
      "title": "Discriminating stress from cognitive load using a wearable eda device",
      "authors": [
        "C Setz",
        "B Arnrich",
        "J Schumm",
        "R La Marca",
        "G Tröster",
        "U Ehlert"
      ],
      "year": "2009",
      "venue": "IEEE Transactions on Information Technology in Biomedicine"
    },
    {
      "citation_id": "43",
      "title": "Gsr analysis for stress: Development and validation of an open source tool for noisy naturalistic gsr data",
      "authors": [
        "S Aqajari",
        "E Naeini",
        "M Mehrabadi",
        "S Labbaf",
        "A Rahmani",
        "N Dutt"
      ],
      "year": "2020",
      "venue": "Gsr analysis for stress: Development and validation of an open source tool for noisy naturalistic gsr data"
    },
    {
      "citation_id": "44",
      "title": "An integrated human stress detection sensor using supervised algorithms",
      "authors": [
        "A Mohammadi",
        "M Fakharzadeh",
        "B Baraeinejad"
      ],
      "year": "2022",
      "venue": "IEEE Sensors Journal"
    },
    {
      "citation_id": "45",
      "title": "Stress detection using deep neural networks",
      "authors": [
        "R Li",
        "Z Liu"
      ],
      "year": "2020",
      "venue": "BMC Medical Informatics and Decision Making"
    },
    {
      "citation_id": "46",
      "title": "Benchmarking reservoir and recurrent neural networks for human state and activity recognition",
      "authors": [
        "D Bacciu",
        "D Di",
        "C Sarli",
        "A Gallicchio",
        "N Micheli",
        "Puccinelli"
      ],
      "year": "2021",
      "venue": "International Work-Conference on Artificial Neural Networks"
    },
    {
      "citation_id": "47",
      "title": "Stress detection via sensor translation",
      "authors": [
        "S Samyoun",
        "A Mondol",
        "J Stankovic"
      ],
      "year": "2020",
      "venue": "2020 16th International Conference on Distributed Computing in Sensor Systems (DCOSS)"
    },
    {
      "citation_id": "48",
      "title": "Recognition of emotions using multimodal physiological signals and an ensemble deep learning model",
      "authors": [
        "Z Yin",
        "M Zhao",
        "Y Wang",
        "J Yang",
        "J Zhang"
      ],
      "year": "2017",
      "venue": "Computer Methods and Programs in Biomedicine"
    },
    {
      "citation_id": "49",
      "title": "Attention is all you need",
      "authors": [
        "A Vaswani",
        "N Shazeer",
        "N Parmar",
        "J Uszkoreit",
        "L Jones",
        "A Gomez",
        "Ł Kaiser",
        "I Polosukhin"
      ],
      "year": "2017",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "50",
      "title": "Multi-level multiple attentions for contextual multimodal sentiment analysis",
      "authors": [
        "S Poria",
        "E Cambria",
        "D Hazarika",
        "N Mazumder",
        "A Zadeh",
        "L.-P Morency"
      ],
      "year": "2017",
      "venue": "2017 IEEE International Conference on Data Mining (ICDM)"
    },
    {
      "citation_id": "51",
      "title": "Towards measuring stress with smartphones and wearable devices during workday and sleep",
      "authors": [
        "A Muaremi",
        "B Arnrich",
        "G Tröster"
      ],
      "year": "2013",
      "venue": "BioNanoScience"
    },
    {
      "citation_id": "52",
      "title": "A real-time QRS detection algorithm",
      "authors": [
        "J Pan",
        "W Tompkins"
      ],
      "year": "1985",
      "venue": "IEEE Transaction on Biomedical Engineering"
    },
    {
      "citation_id": "53",
      "title": "Focal loss for dense object detection",
      "authors": [
        "T.-Y Lin",
        "P Goyal",
        "R Girshick",
        "K He",
        "P Dollár"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE International Conference on Computer Vision"
    },
    {
      "citation_id": "54",
      "title": "Umap: Uniform manifold approximation and projection for dimension reduction",
      "authors": [
        "L Mcinnes",
        "J Healy",
        "J Melville"
      ],
      "year": "2018",
      "venue": "Umap: Uniform manifold approximation and projection for dimension reduction",
      "arxiv": "arXiv:1802.03426"
    },
    {
      "citation_id": "55",
      "title": "Human stress detection with wearable sensors using convolutional neural networks",
      "authors": [
        "M Gil-Martin",
        "R San-Segundo",
        "A Mateos",
        "J Ferreiros-Lopez"
      ],
      "year": "2022",
      "venue": "IEEE Aerospace and Electronic Systems Magazine"
    },
    {
      "citation_id": "56",
      "title": "Multi-modal physiological data fusion for affect estimation using deep learning",
      "authors": [
        "M Hssayeni",
        "B Ghoraani"
      ],
      "year": "2021",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "57",
      "title": "Comparing the predictability of sensor modalities to detect stress from wearable sensor data",
      "authors": [
        "R Holder",
        "R Sah",
        "M Cleveland",
        "H Ghasemzadeh"
      ],
      "year": "2022",
      "venue": "2022 IEEE 19th Annual Consumer Communications & Networking Conference (CCNC)"
    },
    {
      "citation_id": "58",
      "title": "Modeling mental stress using a deep learning framework",
      "authors": [
        "K Masood",
        "M Alghamdi"
      ],
      "year": "2019",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "59",
      "title": "Cnnbased stress and emotion recognition in ambulatory settings",
      "authors": [
        "L Liakopoulos",
        "N Stagakis",
        "E Zacharaki",
        "K Moustakas"
      ],
      "year": "2021",
      "venue": "2021 12th International Conference on Information, Intelligence, Systems & Applications (IISA)"
    },
    {
      "citation_id": "60",
      "title": "Strategy for affective computing based on hrv and eda",
      "authors": [
        "V Motogna",
        "G Lupu-Florian",
        "E Lupu"
      ],
      "year": "2021",
      "venue": "2021 International Conference on e-Health and Bioengineering (EHB)"
    },
    {
      "citation_id": "61",
      "title": "Employing multimodal machine learning for stress detection",
      "authors": [
        "R Walambe",
        "P Nayak",
        "A Bhardwaj",
        "K Kotecha"
      ],
      "year": "2021",
      "venue": "Journal of Healthcare Engineering"
    },
    {
      "citation_id": "62",
      "title": "Exploring unsupervised machine learning classification methods for physiological stress detection",
      "authors": [
        "T Iqbal",
        "A Elahi",
        "W Wijns",
        "A Shahzad"
      ],
      "year": "2011",
      "venue": "Frontiers in Medical Technology"
    }
  ]
}