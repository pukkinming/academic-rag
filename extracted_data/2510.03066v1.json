{
  "paper_id": "2510.03066v1",
  "title": "Insideout: An Efficientnetv2-S Based Deep Learning Framework For Robust Multi-Class Facial Emotion Recognition",
  "published": "2025-10-03T14:53:47Z",
  "authors": [
    "Ahsan Farabi",
    "Israt Khandaker",
    "Ibrahim Khalil Shanto",
    "Md Abdul Ahad Minhaz",
    "Tanisha Zaman"
  ],
  "keywords": [
    "Facial Emotion Recognition",
    "EfficientNetV2-S",
    "Transfer Learning",
    "Data Augmentation",
    "Class Imbalance"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Facial Emotion Recognition (FER) is a key task in affective computing, enabling applications in human-computer interaction, e-learning, healthcare, and safety systems. Despite advances in deep learning, FER remains challenging due to occlusions, illumination and pose variations, subtle intra-class differences, and dataset imbalance that hinders recognition of minority emotions. We present InsideOut, a reproducible FER framework built on EfficientNetV2-S with transfer learning, strong data augmentation, and imbalance-aware optimization. The approach standardizes FER2013 images, applies stratified splitting and augmentation, and fine-tunes a lightweight classification head with class-weighted loss to address skewed distributions. InsideOut achieves 62.8% accuracy with a macroaveraged F1 of 0.590 on FER2013, showing competitive results compared to conventional CNN baselines. The novelty lies in demonstrating that efficient architectures, combined with tailored imbalance handling, can provide practical, transparent, and reproducible FER solutions. Code is available at: https://github. com/TheAhsanFarabi/InsideOut",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "Facial expressions are a primary channel of non-verbal communication, providing cues about affect, intention, and social interaction. Automatic Facial Expression Recognition (FER) therefore plays a crucial role in human-computer interaction, clinical screening, driver monitoring, and affect-aware intelligent systems  [6] ,  [14] . Early FER approaches relied on handcrafted features such as Local Binary Patterns (LBP), Gabor filters, and Histogram of Oriented Gradients (HOG) with classifiers like SVMs. While effective in constrained conditions, these systems struggled with robustness to illumination, occlusion, and intra-class variations  [1] . Deep learning revolutionized FER by enabling end-to-end pipelines using CNNs such as VGGNet, ResNet, and DenseNet  [4] ,  [5] , achieving significant performance gains. However, these improvements came at the cost of high computational demand and persistent sensitivity to dataset bias and class imbalance. Despite progress, key challenges remain. First, state-of-the-art CNN and Transformer-based FER models often achieve strong accuracy but require prohibitive computation for real-time or embedded deployment  [9] . Second, benchmark datasets such as FER2013 are highly imbalanced, with minority emotions like Disgust and Fear severely underrepresented, leading to low recall for rare but psychologically significant classes  [21] . Finally, reproducibility and transparent baselines are often lacking in FER research, hindering fair comparison and practical adoption  [15] . To address these gaps, we propose InsideOut, a lightweight FER framework based on EfficientNetV2-S  [9] . InsideOut combines a parameter-efficient backbone with an imbalanceaware training recipe and rigorous evaluation. Leveraging EfficientNetV2's fused-MBConv blocks and progressive learning strategies, the framework balances accuracy with computational efficiency. To mitigate skewed class distributions, we employ class-weighted cross-entropy and data augmentation, improving minority class recognition while maintaining competitive overall accuracy. Importantly, the framework is designed with reproducibility in mind, providing transparent reporting of class-wise performance, diagnostic plots, and implementation details. The main contributions of this paper are:\n\n1) Development of an EfficientNetV2-S based FER baseline with a lightweight classification head, tailored for real-time and resource-constrained scenarios. 2) An imbalance-aware training strategy combining weighted cross-entropy and systematic augmentation, leading to improved minority class recall.\n\n3) Transparent evaluation on FER2013 with class-wise metrics, confusion matrices, qualitative inferences, and reproducible implementation to support future research. The remainder of this paper is organized as follows. Section II reviews related work on FER datasets, architectures, and imbalance handling. Section III details the methodology, including preprocessing, model design, and training. Section IV presents results and analysis. Section V discusses limitations and outlines future research directions. Section VI concludes the study.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Ii. Related Work",
      "text": "A. Classical and CNN-based FER Facial emotion recognition was initially dominated by classical handcrafted feature methods such as Gabor filters, Local Binary Patterns (LBP), and Histogram of Oriented Gradients (HOG), often paired with classifiers like SVMs  [1] ,  [2] . While effective in constrained environments, these methods were highly sensitive to illumination, occlusion, and pose variations. With the advent of deep learning, Convolutional Neural Networks (CNNs) became the state of the art. Architectures such as VGGNet  [3] , ResNet  [4] , and DenseNet  [5]  significantly improved performance on FER tasks by automatically learning hierarchical visual features. Despite these advancements, recognition of minority emotions such as Disgust and Fear remains problematic due to class imbalance and the subtle visual differences between expressions  [6] ,  [12] .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "B. Efficient Architectures And Transformers",
      "text": "Efficient architectures such as MobileNet  [7]  and EfficientNet  [8]  have been explored for real-time FER due to their balance of accuracy and computational efficiency. More recently, EfficientNetV2  [9]  has demonstrated superior performance and faster convergence on large-scale visual tasks. Parallel to CNN developments, Transformer-based models such as Vision Transformers (ViT) and hybrid CNN-Transformer designs have shown promise in FER by capturing global dependencies and contextual cues  [10] ,  [11] ,  [13] . These architectures, often combined with attention mechanisms, provide robustness against noisy labels and intra-class variations. Current research also emphasizes uncertainty-aware modeling and imbalance mitigation strategies  [12] ,  [16] .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "C. Datasets And Imbalance",
      "text": "Benchmark datasets such as FER2013  [17] , RAF-DB  [18] , and AffectNet  [19]  have been widely used in FER research. However, these datasets suffer from challenges such as skewed class distributions (with emotions like Disgust and Fear being severely underrepresented) and label noise. Recent surveys  [14] -  [16]  highlight the importance of improved annotation practices, semi-supervised and self-supervised approaches, and imbalance-handling methods like focal loss  [20]  and dynamic re-weighting  [21] . Addressing class imbalance is critical for real-world deployment, as misclassification of rare but psychologically significant emotions can undermine system reliability.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Iii. Methodology",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "A. Dataset And Preprocessing",
      "text": "This study employs the FER2013 dataset, a widely used benchmark in facial emotion recognition. FER2013 consists of 35,887 grayscale images of size 48 × 48, each labeled with one of seven categories: Anger, Disgust, Fear, Happy, Neutral, Sadness, and Surprise. The dataset is naturally imbalanced, with emotions such as Disgust being underrepresented compared to Happy or Neutral. To standardize input for transfer learning, all images are resized to 224 × 224 and normalized using ImageNet statistics (mean and standard deviation). To enhance generalization and robustness, a comprehensive data augmentation pipeline is applied. Augmentations include random resized cropping, horizontal flipping, small random rotations, and color jitter to simulate variations in illumination and facial orientation. These techniques mitigate overfitting and improve resilience to real-world noise in FER tasks. Figures  1  and 2  show the class distribution of FER2013 and examples of augmented samples.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "B. Model",
      "text": "The proposed InsideOut framework builds upon EfficientNetV2-S, a compact but high-performing convolutional backbone pretrained on ImageNet.\n\nEfficientNetV2 introduces progressive learning, fused-MBConv layers, and optimized scaling strategies that accelerate training while improving representational efficiency compared to earlier CNNs such as ResNet or DenseNet. The pretrained EfficientNetV2-S is adapted by replacing its classifier with a lightweight head consisting of a global average pooling layer, dropout for regularization, and a fully connected layer with seven softmax outputs. This transfer learning approach leverages robust low-level features while finetuning higher-level parameters for FER-specific discriminative cues. The architecture balances accuracy with computational efficiency, making it practical for real-time or embedded deployment scenarios.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "C. Training",
      "text": "Training is conducted using the Adam optimizer with an initial learning rate η = 10 -3 , combined with a cosine annealing schedule to gradually reduce the step size during convergence. The batch size is set to 64, and early stopping is applied based on validation loss to prevent overfitting. The network is finetuned for up to 100 epochs depending on convergence.\n\nTo counteract class imbalance in FER2013, class-weighted categorical cross-entropy is integrated, where weights are inversely proportional to class frequencies. This ensures minority categories such as Disgust and Fear contribute more strongly to the loss function, improving recall for underrepresented emotions. Stratified train-validation-test splits are applied to maintain consistent class distributions across partitions. This strategy, combined with augmentation, enables the model to better capture rare yet psychologically significant expressions.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "D. Metrics",
      "text": "Performance is evaluated using multiple metrics to provide a holistic view of model behavior. In addition to overall accuracy, precision, recall, and F1-score are computed for each class, along with their macro-averages. Macro-averaging treats all classes equally, making it more suitable for imbalanced datasets compared to micro-averaging. Weighted averages are also reported to reflect performance proportional to class distribution. Diagnostic tools such as confusion matrices and learning curves (accuracy and loss per epoch) are presented to analyze misclassifications and monitor convergence. These visualizations reveal common confusions between semantically similar emotions (e.g., Fear vs. Surprise) and validate the stability of training. This multi-faceted evaluation ensures reproducibility and transparency while highlighting both strengths and limitations of the proposed approach.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "E. Overall Pipeline",
      "text": "The overall methodology is summarized in Figure  3 , which illustrates the flow from dataset preprocessing through model training and evaluation.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Iv. Results",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "A. Overall Performance",
      "text": "The performance of the proposed InsideOut framework was evaluated on the FER2013 test set. As shown in Table  I , the model achieved an overall accuracy of 62.8% with a macro-averaged F1-score of 0.590. Compared to traditional CNN-based FER systems such as VGG or ResNet backbones, which typically report accuracies in the 58-62% range on FER2013  [6] ,  [13] , InsideOut demonstrates competitive performance while maintaining computational efficiency due to its EfficientNetV2-S backbone  [9] . Class-wise metrics reveal that Happy is the most reliably recognized emotion (F1: 0.832), reflecting both the abundance of training samples and the strong discriminative features of smiling expressions. Notably, Disgust (Recall: 0.729) and Surprise (Recall: 0.866) achieved strong recall despite their minority representation, highlighting the benefit of classweighted loss and augmentation. In contrast, Fear (F1: 0.418) and Sadness (F1: 0.474) remained challenging, often being confused with semantically similar expressions such as Neutral or Anger.   C. Qualitative Inference  with confidence exceeding 90%. However, ambiguous or lowintensity expressions of Fear and Sadness are frequently misclassified as Neutral, reflecting the subtle boundaries between these emotions. These observations are consistent with quantitative results and highlight the need for richer feature representations, possibly through attention-based mechanisms  [10] .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "D. Discussion",
      "text": "The evaluation demonstrates that InsideOut successfully leverages EfficientNetV2-S to achieve strong overall accuracy and recall on underrepresented classes such as Disgust and Surprise. This performance surpasses many older CNN baselines on FER2013 and validates the use of efficient backbones in FER tasks. Nevertheless, performance gaps remain for subtle and ambiguous classes like Fear and Sadness. These results align with recent findings that emphasize the challenges of class imbalance and the importance of integrating imbalance-aware loss functions, uncertainty modeling, and multi-modal cues (e.g., audio or temporal context)  [14] ,  [21] .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "E. Limitations",
      "text": "Despite its contributions, InsideOut has several limitations. First, the model was evaluated solely on FER2013, which is known to contain noisy annotations and does not fully capture the diversity of real-world facial expressions  [17] . Second, overfitting remains an issue, as evidenced by divergence between training and validation metrics after epoch 10. Third, although class-weighted loss improved recall for minority emotions, performance on Fear and Sadness remains limited, suggesting the need for stronger imbalance remedies such as focal loss  [20]  or dynamic clustering  [21] . Finally, real-world deployment of FER systems requires robustness to occlusions, varying illumination, and adversarial manipulations, which were not explicitly addressed in this study. Future work will extend evaluation to larger datasets such as RAF-DB  [18]  and AffectNet  [19] , incorporate transformer-based architectures  [11] , and explore semi-supervised learning to mitigate dataset noise.\n\nV Expanding evaluation to larger and more diverse datasets such as RAF-DB, AffectNet, and emerging in-the-wild corpora will be critical for establishing generalization. Finally, robustness to real-world challenges-including occlusions, varying illumination, cultural diversity, and adversarial manipulations-remains an essential step toward reliable deployment.\n\nIn conclusion, InsideOut demonstrates that efficient backbones, combined with imbalance-aware design, can deliver competitive and interpretable performance on challenging FER benchmarks. By providing a clear, reproducible baseline and highlighting key challenges, this work contributes a practical foundation for future research aimed at building fair, robust, and deployable affective computing systems.",
      "page_start": 5,
      "page_end": 5
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Class distribution in FER2013, highlighting imbalance across cate-",
      "page": 2
    },
    {
      "caption": "Figure 2: Example preprocessed and augmented samples from FER2013.",
      "page": 2
    },
    {
      "caption": "Figure 4: Confusion matrix of InsideOut on FER2013 test split. The matrix",
      "page": 4
    },
    {
      "caption": "Figure 5: and Figure 6 depict the evolution of accuracy and loss",
      "page": 4
    },
    {
      "caption": "Figure 5: Training and validation accuracy across epochs. Validation accuracy",
      "page": 4
    },
    {
      "caption": "Figure 7: presents qualitative test results, showing predicted",
      "page": 4
    },
    {
      "caption": "Figure 6: Training and validation loss across epochs. Divergence after epoch",
      "page": 4
    },
    {
      "caption": "Figure 7: Example test inferences with predicted labels and confidence scores.",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Class": "Anger\nDisgust\nFear\nHappy\nNeutral\nSadness\nSurprise",
          "Precision": "0.567\n0.361\n0.541\n0.884\n0.519\n0.546\n0.669",
          "Recall": "0.537\n0.729\n0.340\n0.786\n0.751\n0.421\n0.866",
          "F1": "0.552\n0.483\n0.418\n0.832\n0.614\n0.474\n0.755"
        },
        {
          "Class": "Accuracy",
          "Precision": "0.628 (macro avg F1 = 0.590)",
          "Recall": "",
          "F1": ""
        }
      ],
      "page": 3
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Facial expression recognition based on local binary patterns: A comprehensive study",
      "authors": [
        "C Shan",
        "S Gong",
        "P Mcowan"
      ],
      "year": "2009",
      "venue": "Image and Vision Computing"
    },
    {
      "citation_id": "2",
      "title": "Dynamic texture recognition using local binary patterns with an application to facial expressions",
      "authors": [
        "G Zhao",
        "M Pietikäinen"
      ],
      "year": "2007",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "3",
      "title": "Very deep convolutional networks for large-scale image recognition",
      "authors": [
        "K Simonyan",
        "A Zisserman"
      ],
      "year": "2014",
      "venue": "Very deep convolutional networks for large-scale image recognition",
      "arxiv": "arXiv:1409.1556"
    },
    {
      "citation_id": "4",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "K He",
        "X Zhang",
        "S Ren",
        "J Sun"
      ],
      "year": "2016",
      "venue": "Proc. IEEE CVPR"
    },
    {
      "citation_id": "5",
      "title": "Densely connected convolutional networks",
      "authors": [
        "G Huang",
        "Z Liu",
        "L Van Der Maaten",
        "K Weinberger"
      ],
      "year": "2017",
      "venue": "Proc. IEEE CVPR"
    },
    {
      "citation_id": "6",
      "title": "Deeplearning based facial expression recognition: A comprehensive review",
      "authors": [
        "S Minaee",
        "R Kafieh",
        "M Sonka",
        "S Yazdani",
        "G Soufi"
      ],
      "year": "2021",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "7",
      "title": "MobileNets: Efficient convolutional neural networks for mobile vision applications",
      "authors": [
        "A Howard"
      ],
      "year": "2017",
      "venue": "MobileNets: Efficient convolutional neural networks for mobile vision applications",
      "arxiv": "arXiv:1704.04861"
    },
    {
      "citation_id": "8",
      "title": "EfficientNet: Rethinking model scaling for convolutional neural networks",
      "authors": [
        "M Tan",
        "Q Le"
      ],
      "year": "2019",
      "venue": "Proc. ICML"
    },
    {
      "citation_id": "9",
      "title": "EfficientNetV2: Smaller models and faster training",
      "authors": [
        "M Tan",
        "Q Le"
      ],
      "year": "2021",
      "venue": "Proc. ICML"
    },
    {
      "citation_id": "10",
      "title": "Vision Transformer with attentive pooling for robust facial expression recognition",
      "authors": [
        "F Xue",
        "C Zhang",
        "S Li"
      ],
      "year": "2022",
      "venue": "Vision Transformer with attentive pooling for robust facial expression recognition",
      "arxiv": "arXiv:2212.05463"
    },
    {
      "citation_id": "11",
      "title": "Semi-supervised facial expression recognition with confidence-aware pseudo-labeling",
      "authors": [
        "J Jiang",
        "H Wang",
        "Y Li"
      ],
      "year": "2022",
      "venue": "Semi-supervised facial expression recognition with confidence-aware pseudo-labeling",
      "arxiv": "arXiv:2205.14361"
    },
    {
      "citation_id": "12",
      "title": "Combating uncertainty and class imbalance in facial expression recognition",
      "authors": [
        "J Fan",
        "Y Liu",
        "W Zhou"
      ],
      "year": "2022",
      "venue": "Combating uncertainty and class imbalance in facial expression recognition",
      "arxiv": "arXiv:2212.07751"
    },
    {
      "citation_id": "13",
      "title": "Benchmarking deep facial expression recognition",
      "authors": [
        "G Tutuianu",
        "A Birlutiu",
        "M Haindl"
      ],
      "year": "2023",
      "venue": "Benchmarking deep facial expression recognition",
      "arxiv": "arXiv:2311.02910"
    },
    {
      "citation_id": "14",
      "title": "A survey on facial expression recognition: Modality, methodologies, challenges and emerging topics",
      "authors": [
        "A Shahid"
      ],
      "year": "2023",
      "venue": "Journal of Imaging"
    },
    {
      "citation_id": "15",
      "title": "Advances in facial expression recognition: A survey of deep learning methods",
      "authors": [
        "T Kopalidis",
        "G Karanastasis",
        "C Tzelepis"
      ],
      "year": "2024",
      "venue": "Information"
    },
    {
      "citation_id": "16",
      "title": "A survey on facial expression recognition of static and dynamic emotions",
      "authors": [
        "S Li",
        "H Deng",
        "Z Zhang"
      ],
      "year": "2024",
      "venue": "Sensors"
    },
    {
      "citation_id": "17",
      "title": "Challenges in representation learning: A report on three machine learning contests",
      "authors": [
        "I Goodfellow"
      ],
      "year": "2013",
      "venue": "Proc. Intl. Conf. Neural Networks"
    },
    {
      "citation_id": "18",
      "title": "Reliable crowdsourcing and deep localitypreserving learning for expression recognition in the wild",
      "authors": [
        "S Li",
        "W Deng"
      ],
      "year": "2019",
      "venue": "IEEE Trans. Image Processing"
    },
    {
      "citation_id": "19",
      "title": "AffectNet: A database for facial expression, valence, and arousal computing",
      "authors": [
        "A Mollahosseini",
        "B Hasani",
        "M Mahoor"
      ],
      "year": "2019",
      "venue": "IEEE Trans. Affective Computing"
    },
    {
      "citation_id": "20",
      "title": "Focal loss for dense object detection",
      "authors": [
        "T.-Y Lin",
        "P Goyal",
        "R Girshick",
        "K He",
        "P Dollár"
      ],
      "year": "2017",
      "venue": "Proc. IEEE ICCV"
    },
    {
      "citation_id": "21",
      "title": "Optimizing class imbalance in facial expression recognition using dynamic intra-class clustering",
      "authors": [
        "Q Li",
        "F Zhang",
        "H Liu"
      ],
      "year": "2025",
      "venue": "Biomimetics"
    }
  ]
}