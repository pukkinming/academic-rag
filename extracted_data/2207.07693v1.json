{
  "paper_id": "2207.07693v1",
  "title": "Towards Understanding Confusion And Affective States Under Communication Failures In Voice-Based Human-Machine Interaction",
  "published": "2022-07-15T18:32:48Z",
  "authors": [
    "Sujeong Kim",
    "Abhinav Garlapati",
    "Jonah Lubin",
    "Amir Tamrakar",
    "Ajay Divakaran"
  ],
  "keywords": [
    "multimodal data",
    "affective dataset",
    "voice-based interface",
    "human-computer communication",
    "confusion"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "We present a series of two studies conducted to understand user's affective states during voice-based humanmachine interactions. Emphasis is placed on the cases of communication errors or failures. In particular, we are interested in understanding \"confusion\" in relation with other affective states. The studies consist of two types of tasks: (1) related to communication with a voice-based virtual agent: speaking to the machine and understanding what the machine says, (2) non-communication related, problem-solving tasks where the participants solve puzzles and riddles but are asked to verbally explain the answers to the machine. We collected audio-visual data and self-reports of affective states of the participants. We report results of two studies and analysis of the collected data. The first study was analyzed based on the annotator's observation, and the second study was analyzed based on the self-report.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "Voice-base interfaces have become more available with recent advances in natural language processing. Despite the pace of the advances, digital assistants still have shortcomings in understanding the breadth and nuance in natural communication. Commonly reported failures such as general lack of understanding of the users intent, unreliable or inconsistent voice recognition, and inaccurate voice transcription  [1]  could lead to poor user experience and customer dissatisfaction.\n\nThere have been some efforts understanding sources of human-machine communication errors and exploring machine's or human's repair/recovery strategies to handle these errors  [2] -  [5] . Being able to understand and detect user's affective states in these situations could help develop a more reactive and actively engaging human-machine communication system  [6] . There has been extensive work on detecting facial expressions and basic emotions  [7] , such as anger, surprise, happiness, disgust, sadness, and fear, but these categories do not fully describe user states under struggling human-machine communications.\n\nIn this paper, we present a series of two studies conducted to understand user's affective 1 states during voicebased interactions with machine under existing limitations that 1 We use the terms affect to generally refer to feelings or emotional experience  [8] , interchangeably with the term emotion could cause communication failures. Participants are asked to perform two types of tasks: (1) tasks related to humanmachine communication, such as speaking to the machine and understanding the machine's speech, (2) problem-solving type tasks, such as puzzles and riddles. We aim to compare user states between these two scenarios and with varying level of difficulties. We collected multi-modal data (audio-visual recordings) of 35 participants from both studies  2  .\n\nIn the first study, we attempted to categorize commonly exhibited affective states based on the annotations. Confusion was the most commonly observed emotion when participants have difficulty completing the task. We found that confusion is exhibited in few different ways in terms of facial/behavior features, which are similar to the features observed in other emotions such as \"frustrated\", \"irritated\", and \"focused\". It led to a follow-up study to understand confusion and other affective states in more depth. In the second study, we examined user states in three different views, based on the affective state categories revised from the first study, valence and arousal dimensions, and level of confusion, reported by participants after each task. Our results suggest that failures in human-machine communication more often induce confusion than in problem-solving tasks, also accompanying with slightly different emotions (see Figure  1  for example). In the valence and arousal measures, however, we did not find statistically meaningful difference between two types of tasks.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Ii. Related Work",
      "text": "In the area of computer vision and machine learning, most of the existing work on emotion/affect detection has been focused on basic emotions or dimensional models  [7] . Due to its wide application, it has been an active area of research in other fields, too, including robotics  [9] , human-computer interaction  [10]  and learning  [11] ,  [12] , and focuses more on detecting emotions in by uni-/multi-modal data in natural settings.\n\nThere have been studies on user's affective/emotional states outside the basic emotion categories. Some studies suggest agents responding to user's feelings reduce user's frustration  [13] ,  [14] . Hoque et al.  [15]  propose a model to distinguish frustration and delight, Ishimaru et al.  [16]  propose a learning assistant that gives feedback based on self-confidence detection, Halfon et al.  [17]  present a tool to analyze anger, anxiety, pleasure, and sadness in psychotherapy. There have been some studies about understanding learner's states including confusion as a cognitive-affective state  [18]  and integration with an affect-sensitive tutor  [19] . Some studies focus on using these algorithms to study the effect of confusion on learning performance, for example, how confusion and frustration increase the performance of learning  [20] .",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Iii. Method",
      "text": "We designed a web-based program where participants can interact with the computer using a voice interface. We used the WebKit Speech API  [21]  to transcribe the participants' utterances to pass to the system and to synthesize machine speech. For data collection, we set up a typical computer desk: computer with a monitor, a keyboard, a mouse and a headset. We installed a web camera on top of the monitor so it can capture the face and the upper body of a participant. Recorded data contain the task screen and the web-camera input. Additionally, we collected a survey for the first study and self-report for the second study.\n\nThe study starts with a short tutorial of the voice interface and presents four sessions, two containing tasks related to communication with the voice agent, the other two containing problem-solving type tasks: a) Speak to the machine (communication): This session is related to the system's natural language understanding ability, more specifically, correctly taking in user's utterances. Participants are asked to speak a pre-selected sentence to the machine. For difficult tasks, sentences were chosen such that a typical automatic speech recognition (ASR) system would have trouble transcribing.\n\nb) Repeat after the machine (communication): This session is related to system's speech synthesis, more specifically, naturally and clearly delivering speech to the users. Participants are asked to repeat the sentences spoken by the machine. All the sentences were in English, but machine speech is generated with various foreign accents. c) Puzzles (problem solving): Participants are asked to solve puzzles with varying difficulty. They must speak out their solution to the machine. d) Riddles (problem solving): The voice agent shows an image and tells a riddle related to the image. Participants must speak out their solution to the machine.\n\nThe tasks cover range difficulty. Participants were given three chances until they complete the task correctly. There was no time limit for any tasks. It took about 40 minutes to finish the study including training and survey/self-report.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Iv. Study1: Observed User States",
      "text": "We collected data from 25 participants, 9 females and 16 males, age between 20 to 69. 36% of them reported that they almost never use any voice-assistants like Siri, Alexa, or Google Assistant, 24% use almost everyday, 12% few times a year, 8% few times a month, and 20% few times a week.\n\nParticipants are given a brief survey after completing all tasks. For the question asking if they were confused at any circumstances during the study, 76% answered yes. Out of them, 75% indicated that it was during the communication tasks, 17% during the problem-solving tasks.\n\nThe data was annotated by three annotators. Annotators were asked to watch the recordings, segment and label any emotional expressions exhibited. We were particularly interested in finding user state categories that might not be describable with six basic emotion categories, so we asked the annotators to use their judgement and pick a label that best describes each segment. When we merged the annotations, we picked the majority annotations for overlapping segments. When no such majority annotation existed, the more experienced annotator's annotation was chosen. We extracted 2004 video segments in total, with lengths from 1 second to several minutes. Labels suggested by the annotators were: neutral (218), happy (333), surprise (302), apprehensive/anxious (60), confused (730), frustrated (64), irritated (144), disappointed (101), thoughtful (38), realization (1), dissatisfied  (13) .\n\nSome facial/behavioral features that are often observed under challenging tasks were: furrowed eyebrows, raising one or both eyebrows, closing or blinking eyes, pressing lips, tilting head, changing eye gaze from computer screen to outside the screen (left, right, down, or up), getting closer to the screen or speaker to listen closer, leaning forwards or backwards while staring at the screen, and resting chin on hand. About 72% of extracted segments contain more than two features exhibited, while the rest contain only one of the features.\n\nOne of the main challenges in annotation was due to individual difference in their neutral face expression, which serves as the baseline for determining exhibition of other emotions. Different facial appearance, especially related to shape of the lips and eyebrows, made it challenging to distinguish between neutral state and exhibition of certain emotions. Annotating data from a wide age range of participants was also challenging for similar reasons  [22] ,  [23] .\n\nRevised user state categories: We reviewed the extracted clips without context, i.e., only a segment of a video recording without task information or audio, thus solely based on the aforementioned facial/behavioral features . Some clips labeled confused were difficult to distinguish from other",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "V. Study 2: Self-Report Analysis",
      "text": "In the second study, we addressed research questions that arose from the first study by evaluating the user state categories. This time, we collected a self-report from participants after each task about their emotional states, affect state in terms of valence and arousal, level of confusion, and the main challenge of the task. The questionnaire included the following questions and options to choose from. a) Main challenge: \"none or minor\", \"ASR system failure\", \"machine speech\", \"problem solving\", \"memory\", \"other\", with short descriptions for each option.\n\nb) Most significant feelings/emotions during the task: \"indifferent\", \"happy/amused\", \"surprise/realization\", \"unexpected/negative surprise\", \"unsure/uncertain/apprehensive\", \"irritated/annoyed\", \"frustrated\", \"focused/thoughtful\", \"other\". A follow-up question asks to select all other significant feelings/emotions experienced. Same options plus \"no other feelings\" are given.\n\nc) How pleasant the experience was during the task (valence), and how strong it was (arousal): Self-Assessment Manikin (SAM)  [24]  in scale of 1 to 9, each.\n\nd) Level of confusion: in scale of 1 (not confused at all) to 5 (extremely confused).\n\nWe collected total 241 sets of self-report from 10 participants, 5 females and 5 males. 9 of them reported their age range 20-29 (45%), 30-39 (9%), 40-49 (36%), 50-59 (9%). 33% of them answered that they have almost never used voice assistants, 33% answered they use almost everyday, 11% few times a week, 11% few times a month, 11% few times a year.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "A. Emotional States",
      "text": "Table  I  shows the top rated challenge(s), emotion(s), and average level of confusion for each session obtained from self-reports. The \"Speak to Machine\" session was considered relatively easy because of the high accuracy of the ASR engine. The second ranked challenge for this session was ASR system failure, with associated main emotion(s) unexpected/negative surprise. For other sessions, only the topranked items are shown in the table. We can see different main emotions are reported for different type of challenges. Interestingly, challenges in human-machine communication tasks induce more negative emotions than those reported in problem-solving tasks.\n\nIn Table  II , the last row shows the distribution of main emotions reported from all participants. All categories are fairly equally distributed except indifferent at about twice the others. We compare these emotional states in valencearousal dimensions. We visualize valence-arousal scores for selected emotions in Figure  2 (a)-(d) . We can see that amused/happy and surprise/realization are trending positively, whereas frustrated and irritated are more distributed in the negative space along the valence axis. Not included in the figure, other emotions such as unexpected/negative surprise, unsure/uncertain/apprehensive, focused/thoughtful, do not show clear trend in distribution either in negative or positive valence axis. Based only on this result, these states might not be suitable as distinctive affective states, but we believe they could still provide important cues for human-machine interaction. Further analysis in facial/behavioral/audio features from the recordings could help better understand the emotional states.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "B. Level Of Confusion",
      "text": "The average level of confusion for incomplete tasks, i.e., participants failed to correctly perform the task after three trials, was 2.93, which was higher than average level of confusion, 1.67, for complete tasks ( p 0.5, two-sample t-test assuming unequal variance).\n\nIn the first study, we observed ambiguity in the presentation of confusion, sharing similar facial/behavioral features from other user state categories. Table  II  shows distribution of the main emotions for different level of confusion in range 1 (not confused at all) to 5(extremely confused). In the following analysis, we refer to levels 4 and 5 as \"high confusion\". Under high confusion, emotions such as frustration (25%), unsure, uncertain, apprehensive 20%, irritated 18%, focused, thoughtful 15%, unexpected, and negative surprise 11% were reported more often than others.\n\nFigure  1  shows screenshot of three participants under high confusion state. The main emotions they reported were: frustrated, irritated, and focused, respectively. We can see the difference in the exhibited emotions from their facial/behavioral   expressions, such as eye gaze, and shape of the lips and the eyebrows. This confirms the observation in the first study that confusion is exhibited in multiple different forms.\n\nIn the valence-arousal dimension (see Fig.  2  (e)), confusion is mostly distributed in the negative valence and high arousal space, but we observe some data points in other quadrants. The positive ones include main emotions as surprise/realization, indifferent, unsure/uncertain and unexpected, mostly with amused/happy reported as secondary emotion. This suggests that confusion can sometimes be positive experience based on the outcome, but we leave further analysis as future work.\n\nNext, we compare high confusion between human-machine communication tasks and problem-solving tasks. Our results show that human-machine communication tasks generally induced confusion more often than for problem-solving tasks (60% vs. 40%), also in failure cases (55% vs. 45%). Under highly confused states, the two most frequently reported main emotions were frustrated (28%) and irritated (21%) for human-machine communication tasks, and unsure/uncertain/apprehensive (32%) and frustrated (23%) for problem-solving tasks.\n\nIn the valence-arousal measures, we did not find statistically meaningful difference (p > 0.5, two-sample t-test assuming unequal variance) between the two types of tasks. This indicates that based on the data, there is not enough evidence to tell if confusion is exhibited differently between the two types of tasks in terms of valence-arousal measures. A follow-up larger scale study might help confirming this result.\n\nBased on the results, we think that confusion is not an independent emotional state with unique facial/behavioral features or valence/arousal measures. Rather, it can be considered as a higher level user state that we can infer from exhibition of other emotions over time and based on the context. Our results suggest that participants were more often confused and felt negative towards failures in human-machine communication tasks than those in problem-solving tasks. Thus, preventing the communication failures or having a way to recover from these failures might help greatly improving user experience.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Vi. Conclusion And Future Work",
      "text": "We presented results of two studies on user states in voice-based human-machine interaction. In the first study, we analyzed the data based on annotations and suggest user state categories. In the second study, we analyze user states based on self-report, in terms of discrete emotional categories identified in the first study, valence/arousal measures, and level of confusion. Confusion is often induced when humanmachine communication fails, more specifically due to failure of the machine to understand human speech or failure to generate easily understandable machine speech. We found that confusion in these scenarios is a complex state where different emotion can be exhibited. We also found some difference in emotions towards failures in human-machine communication tasks compared to problem-solving tasks, but no statistically meaningful difference between confusion in two different types of tasks in valence/arousal measures.\n\nAs future work, we would like to look at audio-visual features and compare with the self-report analysis. We are planning on training a machine learning model to detect the user states and perform a follow-up study to learn if early intervention in confusion state would improve user experience or help task completion. We are also interested in incorporating individual differences and context information  [25] .",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Example video data showing different emotions",
      "page": 1
    },
    {
      "caption": "Figure 1: for example). In the valence",
      "page": 1
    },
    {
      "caption": "Figure 2: (a)-(d). We can see that",
      "page": 3
    },
    {
      "caption": "Figure 1: shows screenshot of three participants under high",
      "page": 3
    },
    {
      "caption": "Figure 2: Valence (negative/positive) and arousal (low/high) measures for selected emotional states (a)-(d) and high level of",
      "page": 4
    },
    {
      "caption": "Figure 2: (e)), confusion",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "sujeong.kim@sri.com\nabhinavg@fb.com\njonahlubin@uchicago.edu": "Abstract—We\npresent\na\nseries\nof\ntwo\nstudies\nconducted\nto",
          "amir.tamrakar@sri.com\najay.divakaran@sri.com": ""
        },
        {
          "sujeong.kim@sri.com\nabhinavg@fb.com\njonahlubin@uchicago.edu": "understand\nuser’s\naffective\nstates\nduring\nvoice-based\nhuman-",
          "amir.tamrakar@sri.com\najay.divakaran@sri.com": ""
        },
        {
          "sujeong.kim@sri.com\nabhinavg@fb.com\njonahlubin@uchicago.edu": "machine interactions. Emphasis\nis placed on the cases of com-",
          "amir.tamrakar@sri.com\najay.divakaran@sri.com": ""
        },
        {
          "sujeong.kim@sri.com\nabhinavg@fb.com\njonahlubin@uchicago.edu": "munication errors or\nfailures.\nIn particular, we are\ninterested",
          "amir.tamrakar@sri.com\najay.divakaran@sri.com": ""
        },
        {
          "sujeong.kim@sri.com\nabhinavg@fb.com\njonahlubin@uchicago.edu": "in\nunderstanding\n“confusion”\nin\nrelation with\nother\naffective",
          "amir.tamrakar@sri.com\najay.divakaran@sri.com": ""
        },
        {
          "sujeong.kim@sri.com\nabhinavg@fb.com\njonahlubin@uchicago.edu": "states. The\nstudies\nconsist\nof\ntwo\ntypes\nof\ntasks:\n(1)\nrelated",
          "amir.tamrakar@sri.com\najay.divakaran@sri.com": ""
        },
        {
          "sujeong.kim@sri.com\nabhinavg@fb.com\njonahlubin@uchicago.edu": "to\ncommunication with\na\nvoice-based\nvirtual\nagent:\nspeaking",
          "amir.tamrakar@sri.com\najay.divakaran@sri.com": ""
        },
        {
          "sujeong.kim@sri.com\nabhinavg@fb.com\njonahlubin@uchicago.edu": "to the machine and understanding what\nthe machine\nsays,\n(2)",
          "amir.tamrakar@sri.com\najay.divakaran@sri.com": ""
        },
        {
          "sujeong.kim@sri.com\nabhinavg@fb.com\njonahlubin@uchicago.edu": "non-communication\nrelated,\nproblem-solving\ntasks where\nthe",
          "amir.tamrakar@sri.com\najay.divakaran@sri.com": ""
        },
        {
          "sujeong.kim@sri.com\nabhinavg@fb.com\njonahlubin@uchicago.edu": "participants\nsolve puzzles and riddles but are asked to verbally",
          "amir.tamrakar@sri.com\najay.divakaran@sri.com": "video\ndata\nFig.\n1: Example\nshowing\ndifferent\nemotions"
        },
        {
          "sujeong.kim@sri.com\nabhinavg@fb.com\njonahlubin@uchicago.edu": "explain the answers\nto the machine. We\ncollected audio-visual",
          "amir.tamrakar@sri.com\najay.divakaran@sri.com": ""
        },
        {
          "sujeong.kim@sri.com\nabhinavg@fb.com\njonahlubin@uchicago.edu": "",
          "amir.tamrakar@sri.com\najay.divakaran@sri.com": "(frustrated,\nirritated,\nand\nfocused)\nexhibited\nunder\na\nhighly"
        },
        {
          "sujeong.kim@sri.com\nabhinavg@fb.com\njonahlubin@uchicago.edu": "data\nand\nself-reports\nof\naffective\nstates\nof\nthe\nparticipants.",
          "amir.tamrakar@sri.com\najay.divakaran@sri.com": ""
        },
        {
          "sujeong.kim@sri.com\nabhinavg@fb.com\njonahlubin@uchicago.edu": "",
          "amir.tamrakar@sri.com\najay.divakaran@sri.com": "confused state. Both emotional\nstate\nand level of\nconfusion"
        },
        {
          "sujeong.kim@sri.com\nabhinavg@fb.com\njonahlubin@uchicago.edu": "We\nreport\nresults of\ntwo studies and analysis of\nthe\ncollected",
          "amir.tamrakar@sri.com\najay.divakaran@sri.com": ""
        },
        {
          "sujeong.kim@sri.com\nabhinavg@fb.com\njonahlubin@uchicago.edu": "data. The\nﬁrst\nstudy was\nanalyzed\nbased\non\nthe\nannotator’s",
          "amir.tamrakar@sri.com\najay.divakaran@sri.com": "are reported by the participants."
        },
        {
          "sujeong.kim@sri.com\nabhinavg@fb.com\njonahlubin@uchicago.edu": "observation, and the\nsecond study was analyzed based on the",
          "amir.tamrakar@sri.com\najay.divakaran@sri.com": ""
        },
        {
          "sujeong.kim@sri.com\nabhinavg@fb.com\njonahlubin@uchicago.edu": "self-report.",
          "amir.tamrakar@sri.com\najay.divakaran@sri.com": ""
        },
        {
          "sujeong.kim@sri.com\nabhinavg@fb.com\njonahlubin@uchicago.edu": "",
          "amir.tamrakar@sri.com\najay.divakaran@sri.com": "could\ncause\ncommunication\nfailures. Participants\nare\nasked"
        },
        {
          "sujeong.kim@sri.com\nabhinavg@fb.com\njonahlubin@uchicago.edu": "Index Terms—multimodal data, affective dataset, voice-based",
          "amir.tamrakar@sri.com\najay.divakaran@sri.com": ""
        },
        {
          "sujeong.kim@sri.com\nabhinavg@fb.com\njonahlubin@uchicago.edu": "interface, human-computer communication, confusion",
          "amir.tamrakar@sri.com\najay.divakaran@sri.com": "to perform two types of\ntasks:\n(1)\ntasks\nrelated to human-"
        },
        {
          "sujeong.kim@sri.com\nabhinavg@fb.com\njonahlubin@uchicago.edu": "",
          "amir.tamrakar@sri.com\najay.divakaran@sri.com": "machine communication, such as speaking to the machine and"
        },
        {
          "sujeong.kim@sri.com\nabhinavg@fb.com\njonahlubin@uchicago.edu": "I.\nINTRODUCTION",
          "amir.tamrakar@sri.com\najay.divakaran@sri.com": "understanding the machine’s speech, (2) problem-solving type"
        },
        {
          "sujeong.kim@sri.com\nabhinavg@fb.com\njonahlubin@uchicago.edu": "",
          "amir.tamrakar@sri.com\najay.divakaran@sri.com": "tasks,\nsuch as puzzles and riddles. We aim to compare user"
        },
        {
          "sujeong.kim@sri.com\nabhinavg@fb.com\njonahlubin@uchicago.edu": "Voice-base\ninterfaces\nhave\nbecome more\navailable with",
          "amir.tamrakar@sri.com\najay.divakaran@sri.com": ""
        },
        {
          "sujeong.kim@sri.com\nabhinavg@fb.com\njonahlubin@uchicago.edu": "",
          "amir.tamrakar@sri.com\najay.divakaran@sri.com": "states\nbetween\nthese\ntwo\nscenarios\nand with\nvarying\nlevel"
        },
        {
          "sujeong.kim@sri.com\nabhinavg@fb.com\njonahlubin@uchicago.edu": "recent\nadvances\nin natural\nlanguage processing. Despite\nthe",
          "amir.tamrakar@sri.com\najay.divakaran@sri.com": ""
        },
        {
          "sujeong.kim@sri.com\nabhinavg@fb.com\njonahlubin@uchicago.edu": "",
          "amir.tamrakar@sri.com\najay.divakaran@sri.com": "of\ndifﬁculties. We\ncollected multi-modal\ndata\n(audio-visual"
        },
        {
          "sujeong.kim@sri.com\nabhinavg@fb.com\njonahlubin@uchicago.edu": "pace of the advances, digital assistants still have shortcomings",
          "amir.tamrakar@sri.com\najay.divakaran@sri.com": ""
        },
        {
          "sujeong.kim@sri.com\nabhinavg@fb.com\njonahlubin@uchicago.edu": "",
          "amir.tamrakar@sri.com\najay.divakaran@sri.com": "recordings) of 35 participants from both studies2."
        },
        {
          "sujeong.kim@sri.com\nabhinavg@fb.com\njonahlubin@uchicago.edu": "in understanding the breadth and nuance in natural commu-",
          "amir.tamrakar@sri.com\najay.divakaran@sri.com": ""
        },
        {
          "sujeong.kim@sri.com\nabhinavg@fb.com\njonahlubin@uchicago.edu": "nication. Commonly reported failures such as general\nlack of",
          "amir.tamrakar@sri.com\najay.divakaran@sri.com": "In the ﬁrst\nstudy, we\nattempted to categorize\ncommonly"
        },
        {
          "sujeong.kim@sri.com\nabhinavg@fb.com\njonahlubin@uchicago.edu": "understanding of\nthe users\nintent, unreliable or\ninconsistent",
          "amir.tamrakar@sri.com\najay.divakaran@sri.com": "exhibited affective states based on the annotations. Confusion"
        },
        {
          "sujeong.kim@sri.com\nabhinavg@fb.com\njonahlubin@uchicago.edu": "voice recognition, and inaccurate voice transcription [1] could",
          "amir.tamrakar@sri.com\najay.divakaran@sri.com": "was the most commonly observed emotion when participants"
        },
        {
          "sujeong.kim@sri.com\nabhinavg@fb.com\njonahlubin@uchicago.edu": "lead to poor user experience and customer dissatisfaction.",
          "amir.tamrakar@sri.com\najay.divakaran@sri.com": "have difﬁculty completing the task. We found that confusion"
        },
        {
          "sujeong.kim@sri.com\nabhinavg@fb.com\njonahlubin@uchicago.edu": "There\nhave\nbeen\nsome\nefforts\nunderstanding\nsources\nof",
          "amir.tamrakar@sri.com\najay.divakaran@sri.com": "is exhibited in few different ways in terms of\nfacial/behavior"
        },
        {
          "sujeong.kim@sri.com\nabhinavg@fb.com\njonahlubin@uchicago.edu": "human-machine\ncommunication\nerrors\nand\nexploring ma-",
          "amir.tamrakar@sri.com\najay.divakaran@sri.com": "features, which are similar\nto the features observed in other"
        },
        {
          "sujeong.kim@sri.com\nabhinavg@fb.com\njonahlubin@uchicago.edu": "chine’s or human’s\nrepair/recovery strategies\nto handle these",
          "amir.tamrakar@sri.com\najay.divakaran@sri.com": "emotions\nsuch as\n“frustrated”,\n“irritated”,\nand “focused”.\nIt"
        },
        {
          "sujeong.kim@sri.com\nabhinavg@fb.com\njonahlubin@uchicago.edu": "errors\n[2]–[5]. Being\nable\nto\nunderstand\nand\ndetect\nuser’s",
          "amir.tamrakar@sri.com\najay.divakaran@sri.com": "led to a\nfollow-up study to understand confusion and other"
        },
        {
          "sujeong.kim@sri.com\nabhinavg@fb.com\njonahlubin@uchicago.edu": "affective states in these situations could help develop a more",
          "amir.tamrakar@sri.com\najay.divakaran@sri.com": "affective\nstates\nin more depth.\nIn the\nsecond study, we\nex-"
        },
        {
          "sujeong.kim@sri.com\nabhinavg@fb.com\njonahlubin@uchicago.edu": "reactive and actively engaging human-machine communication",
          "amir.tamrakar@sri.com\najay.divakaran@sri.com": "amined\nuser\nstates\nin\nthree\ndifferent\nviews,\nbased\non\nthe"
        },
        {
          "sujeong.kim@sri.com\nabhinavg@fb.com\njonahlubin@uchicago.edu": "system [6]. There has been extensive work on detecting facial",
          "amir.tamrakar@sri.com\najay.divakaran@sri.com": "affective state categories revised from the ﬁrst study, valence"
        },
        {
          "sujeong.kim@sri.com\nabhinavg@fb.com\njonahlubin@uchicago.edu": "expressions and basic emotions\n[7],\nsuch as anger,\nsurprise,",
          "amir.tamrakar@sri.com\najay.divakaran@sri.com": "and arousal dimensions, and level of confusion,\nreported by"
        },
        {
          "sujeong.kim@sri.com\nabhinavg@fb.com\njonahlubin@uchicago.edu": "happiness, disgust, sadness, and fear, but\nthese categories do",
          "amir.tamrakar@sri.com\najay.divakaran@sri.com": "participants after each task. Our results suggest\nthat failures in"
        },
        {
          "sujeong.kim@sri.com\nabhinavg@fb.com\njonahlubin@uchicago.edu": "not fully describe user states under struggling human-machine",
          "amir.tamrakar@sri.com\najay.divakaran@sri.com": "human-machine communication more often induce confusion"
        },
        {
          "sujeong.kim@sri.com\nabhinavg@fb.com\njonahlubin@uchicago.edu": "communications.",
          "amir.tamrakar@sri.com\najay.divakaran@sri.com": "than in problem-solving tasks, also accompanying with slightly"
        },
        {
          "sujeong.kim@sri.com\nabhinavg@fb.com\njonahlubin@uchicago.edu": "In\nthis\npaper, we\npresent\na\nseries\nof\ntwo\nstudies\ncon-",
          "amir.tamrakar@sri.com\najay.divakaran@sri.com": "different emotions (see Figure 1 for example).\nIn the valence"
        },
        {
          "sujeong.kim@sri.com\nabhinavg@fb.com\njonahlubin@uchicago.edu": "ducted\nto\nunderstand\nuser’s\naffective1\nstates\nduring\nvoice-",
          "amir.tamrakar@sri.com\najay.divakaran@sri.com": "and arousal measures, however, we did not ﬁnd statistically"
        },
        {
          "sujeong.kim@sri.com\nabhinavg@fb.com\njonahlubin@uchicago.edu": "based interactions with machine under existing limitations that",
          "amir.tamrakar@sri.com\najay.divakaran@sri.com": "meaningful difference between two types of\ntasks."
        },
        {
          "sujeong.kim@sri.com\nabhinavg@fb.com\njonahlubin@uchicago.edu": "affect\n1We\nuse\nthe\nterms\nto\ngenerally\nrefer\nto\nfeelings\nor\nemotional",
          "amir.tamrakar@sri.com\najay.divakaran@sri.com": ""
        },
        {
          "sujeong.kim@sri.com\nabhinavg@fb.com\njonahlubin@uchicago.edu": "experience [8],\ninterchangeably with the term emotion",
          "amir.tamrakar@sri.com\najay.divakaran@sri.com": "2The anonymized data can be shared for\nresearch purposes."
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "II. RELATED WORK": "",
          "d) Riddles (problem solving): The voice agent shows an": "image and tells a riddle related to the image. Participants must"
        },
        {
          "II. RELATED WORK": "In the area of computer vision and machine learning, most",
          "d) Riddles (problem solving): The voice agent shows an": ""
        },
        {
          "II. RELATED WORK": "",
          "d) Riddles (problem solving): The voice agent shows an": "speak out\ntheir solution to the machine."
        },
        {
          "II. RELATED WORK": "of\nthe\nexisting work\non\nemotion/affect\ndetection\nhas\nbeen",
          "d) Riddles (problem solving): The voice agent shows an": ""
        },
        {
          "II. RELATED WORK": "",
          "d) Riddles (problem solving): The voice agent shows an": "The\ntasks\ncover\nrange\ndifﬁculty. Participants were\ngiven"
        },
        {
          "II. RELATED WORK": "focused on basic\nemotions or dimensional models\n[7]. Due",
          "d) Riddles (problem solving): The voice agent shows an": ""
        },
        {
          "II. RELATED WORK": "",
          "d) Riddles (problem solving): The voice agent shows an": "three\nchances until\nthey complete\nthe\ntask correctly. There"
        },
        {
          "II. RELATED WORK": "to its wide application,\nit has been an active area of\nresearch",
          "d) Riddles (problem solving): The voice agent shows an": ""
        },
        {
          "II. RELATED WORK": "",
          "d) Riddles (problem solving): The voice agent shows an": "was no time limit\nfor any tasks.\nIt\ntook about 40 minutes to"
        },
        {
          "II. RELATED WORK": "in other ﬁelds,\ntoo,\nincluding robotics\n[9], human-computer",
          "d) Riddles (problem solving): The voice agent shows an": ""
        },
        {
          "II. RELATED WORK": "",
          "d) Riddles (problem solving): The voice agent shows an": "ﬁnish the study including training and survey/self-report."
        },
        {
          "II. RELATED WORK": "interaction\n[10]\nand\nlearning\n[11],\n[12],\nand\nfocuses more",
          "d) Riddles (problem solving): The voice agent shows an": ""
        },
        {
          "II. RELATED WORK": "on detecting emotions in by uni-/multi-modal data in natural",
          "d) Riddles (problem solving): The voice agent shows an": ""
        },
        {
          "II. RELATED WORK": "",
          "d) Riddles (problem solving): The voice agent shows an": "IV. STUDY1: OBSERVED USER STATES"
        },
        {
          "II. RELATED WORK": "settings.",
          "d) Riddles (problem solving): The voice agent shows an": ""
        },
        {
          "II. RELATED WORK": "",
          "d) Riddles (problem solving): The voice agent shows an": "We collected data from 25 participants, 9 females and 16"
        },
        {
          "II. RELATED WORK": "There have been studies on user’s affective/emotional states",
          "d) Riddles (problem solving): The voice agent shows an": ""
        },
        {
          "II. RELATED WORK": "",
          "d) Riddles (problem solving): The voice agent shows an": "20\nmales,\nage\nbetween\nto\n69.\n36% of\nthem reported\nthat"
        },
        {
          "II. RELATED WORK": "outside\nthe basic\nemotion categories. Some\nstudies\nsuggest",
          "d) Riddles (problem solving): The voice agent shows an": ""
        },
        {
          "II. RELATED WORK": "",
          "d) Riddles (problem solving): The voice agent shows an": "they almost never use any voice-assistants like Siri, Alexa, or"
        },
        {
          "II. RELATED WORK": "agents\nresponding\nto\nuser’s\nfeelings\nreduce\nuser’s\nfrustra-",
          "d) Riddles (problem solving): The voice agent shows an": ""
        },
        {
          "II. RELATED WORK": "",
          "d) Riddles (problem solving): The voice agent shows an": "Google Assistant, 24% use almost everyday, 12% few times a"
        },
        {
          "II. RELATED WORK": "tion [13],\n[14]. Hoque\net\nal.\n[15] propose\na model\nto dis-",
          "d) Riddles (problem solving): The voice agent shows an": ""
        },
        {
          "II. RELATED WORK": "",
          "d) Riddles (problem solving): The voice agent shows an": "year, 8% few times a month, and 20% few times a week."
        },
        {
          "II. RELATED WORK": "tinguish frustration and delight, Ishimaru et al. [16] propose a",
          "d) Riddles (problem solving): The voice agent shows an": ""
        },
        {
          "II. RELATED WORK": "",
          "d) Riddles (problem solving): The voice agent shows an": "Participants\nare given a brief\nsurvey after\ncompleting all"
        },
        {
          "II. RELATED WORK": "learning assistant\nthat gives feedback based on self-conﬁdence",
          "d) Riddles (problem solving): The voice agent shows an": ""
        },
        {
          "II. RELATED WORK": "",
          "d) Riddles (problem solving): The voice agent shows an": "tasks. For\nthe question asking if\nthey were confused at any"
        },
        {
          "II. RELATED WORK": "detection, Halfon et al.\n[17] present a tool\nto analyze anger,",
          "d) Riddles (problem solving): The voice agent shows an": ""
        },
        {
          "II. RELATED WORK": "",
          "d) Riddles (problem solving): The voice agent shows an": "circumstances during the\nstudy, 76% answered yes. Out of"
        },
        {
          "II. RELATED WORK": "anxiety, pleasure, and sadness\nin psychotherapy. There have",
          "d) Riddles (problem solving): The voice agent shows an": ""
        },
        {
          "II. RELATED WORK": "",
          "d) Riddles (problem solving): The voice agent shows an": "them, 75% indicated that\nit was during the\ncommunication"
        },
        {
          "II. RELATED WORK": "been some studies about understanding learner’s states includ-",
          "d) Riddles (problem solving): The voice agent shows an": ""
        },
        {
          "II. RELATED WORK": "",
          "d) Riddles (problem solving): The voice agent shows an": "tasks, 17% during the problem-solving tasks."
        },
        {
          "II. RELATED WORK": "ing confusion as a cognitive-affective state [18] and integration",
          "d) Riddles (problem solving): The voice agent shows an": ""
        },
        {
          "II. RELATED WORK": "",
          "d) Riddles (problem solving): The voice agent shows an": "The\ndata was\nannotated\nby\nthree\nannotators. Annotators"
        },
        {
          "II. RELATED WORK": "with an affect-sensitive tutor [19]. Some studies focus on using",
          "d) Riddles (problem solving): The voice agent shows an": ""
        },
        {
          "II. RELATED WORK": "",
          "d) Riddles (problem solving): The voice agent shows an": "were\nasked to watch the\nrecordings,\nsegment\nand label\nany"
        },
        {
          "II. RELATED WORK": "these algorithms to study the effect of confusion on learning",
          "d) Riddles (problem solving): The voice agent shows an": ""
        },
        {
          "II. RELATED WORK": "",
          "d) Riddles (problem solving): The voice agent shows an": "emotional\nexpressions\nexhibited. We were\nparticularly\nin-"
        },
        {
          "II. RELATED WORK": "performance,\nfor example, how confusion and frustration in-",
          "d) Riddles (problem solving): The voice agent shows an": ""
        },
        {
          "II. RELATED WORK": "",
          "d) Riddles (problem solving): The voice agent shows an": "terested\nin ﬁnding\nuser\nstate\ncategories\nthat might\nnot\nbe"
        },
        {
          "II. RELATED WORK": "crease the performance of\nlearning [20].",
          "d) Riddles (problem solving): The voice agent shows an": ""
        },
        {
          "II. RELATED WORK": "",
          "d) Riddles (problem solving): The voice agent shows an": "describable with six basic emotion categories, so we asked the"
        },
        {
          "II. RELATED WORK": "III. METHOD",
          "d) Riddles (problem solving): The voice agent shows an": ""
        },
        {
          "II. RELATED WORK": "",
          "d) Riddles (problem solving): The voice agent shows an": "annotators\nto use their\njudgement and pick a label\nthat best"
        },
        {
          "II. RELATED WORK": "We designed a web-based program where participants can",
          "d) Riddles (problem solving): The voice agent shows an": "describes\neach segment. When we merged the\nannotations,"
        },
        {
          "II. RELATED WORK": "interact with the computer using a voice interface. We used",
          "d) Riddles (problem solving): The voice agent shows an": "we picked the majority annotations for overlapping segments."
        },
        {
          "II. RELATED WORK": "the WebKit Speech API\n[21]\nto transcribe\nthe participants’",
          "d) Riddles (problem solving): The voice agent shows an": "When no such majority annotation existed,\nthe more experi-"
        },
        {
          "II. RELATED WORK": "utterances\nto pass\nto the\nsystem and to synthesize machine",
          "d) Riddles (problem solving): The voice agent shows an": "enced annotator’s annotation was chosen. We extracted 2004"
        },
        {
          "II. RELATED WORK": "speech. For\ndata\ncollection, we\nset\nup\na\ntypical\ncomputer",
          "d) Riddles (problem solving): The voice agent shows an": "video segments in total, with lengths from 1 second to several"
        },
        {
          "II. RELATED WORK": "desk: computer with a monitor, a keyboard, a mouse and a",
          "d) Riddles (problem solving): The voice agent shows an": "minutes. Labels\nsuggested\nby\nthe\nannotators were:\nneutral"
        },
        {
          "II. RELATED WORK": "headset. We installed a web camera on top of\nthe monitor so",
          "d) Riddles (problem solving): The voice agent shows an": "(218), happy (333), surprise (302), apprehensive/anxious (60),"
        },
        {
          "II. RELATED WORK": "it can capture the face and the upper body of a participant.",
          "d) Riddles (problem solving): The voice agent shows an": "confused (730),\nfrustrated (64),\nirritated (144), disappointed"
        },
        {
          "II. RELATED WORK": "Recorded data\ncontain the\ntask screen and the web-camera",
          "d) Riddles (problem solving): The voice agent shows an": "(101),\nthoughtful\n(38),\nrealization (1), dissatisﬁed (13)."
        },
        {
          "II. RELATED WORK": "input. Additionally, we collected a survey for\nthe ﬁrst\nstudy",
          "d) Riddles (problem solving): The voice agent shows an": "Some facial/behavioral\nfeatures that are often observed un-"
        },
        {
          "II. RELATED WORK": "and self-report\nfor\nthe second study.",
          "d) Riddles (problem solving): The voice agent shows an": "der challenging tasks were: furrowed eyebrows, raising one or"
        },
        {
          "II. RELATED WORK": "The study starts with a short\ntutorial of\nthe voice interface",
          "d) Riddles (problem solving): The voice agent shows an": "both eyebrows, closing or blinking eyes, pressing lips,\ntilting"
        },
        {
          "II. RELATED WORK": "and\npresents\nfour\nsessions,\ntwo\ncontaining\ntasks\nrelated\nto",
          "d) Riddles (problem solving): The voice agent shows an": "head, changing eye gaze from computer screen to outside the"
        },
        {
          "II. RELATED WORK": "communication with the voice agent,\nthe other two containing",
          "d) Riddles (problem solving): The voice agent shows an": "screen (left, right, down, or up), getting closer to the screen or"
        },
        {
          "II. RELATED WORK": "problem-solving type tasks:",
          "d) Riddles (problem solving): The voice agent shows an": "speaker\nto listen closer,\nleaning forwards or backwards while"
        },
        {
          "II. RELATED WORK": "a) Speak to the machine (communication): This session",
          "d) Riddles (problem solving): The voice agent shows an": "staring at\nthe screen, and resting chin on hand. About 72% of"
        },
        {
          "II. RELATED WORK": "is\nrelated\nto\nthe\nsystem’s\nnatural\nlanguage\nunderstanding",
          "d) Riddles (problem solving): The voice agent shows an": "extracted segments contain more than two features exhibited,"
        },
        {
          "II. RELATED WORK": "ability, more speciﬁcally, correctly taking in user’s utterances.",
          "d) Riddles (problem solving): The voice agent shows an": "while the rest contain only one of\nthe features."
        },
        {
          "II. RELATED WORK": "Participants are asked to speak a pre-selected sentence to the",
          "d) Riddles (problem solving): The voice agent shows an": "One\nof\nthe main\nchallenges\nin\nannotation was\ndue\nto"
        },
        {
          "II. RELATED WORK": "machine. For difﬁcult\ntasks, sentences were chosen such that",
          "d) Riddles (problem solving): The voice agent shows an": "individual difference\nin their neutral\nface\nexpression, which"
        },
        {
          "II. RELATED WORK": "a typical automatic speech recognition (ASR)\nsystem would",
          "d) Riddles (problem solving): The voice agent shows an": "serves\nas\nthe\nbaseline\nfor\ndetermining\nexhibition\nof\nother"
        },
        {
          "II. RELATED WORK": "have trouble transcribing.",
          "d) Riddles (problem solving): The voice agent shows an": "emotions. Different\nfacial\nappearance,\nespecially\nrelated\nto"
        },
        {
          "II. RELATED WORK": "b) Repeat after the machine (communication): This ses-",
          "d) Riddles (problem solving): The voice agent shows an": "shape of the lips and eyebrows, made it challenging to distin-"
        },
        {
          "II. RELATED WORK": "sion is related to system’s speech synthesis, more speciﬁcally,",
          "d) Riddles (problem solving): The voice agent shows an": "guish between neutral state and exhibition of certain emotions."
        },
        {
          "II. RELATED WORK": "naturally and clearly delivering speech to the users. Partici-",
          "d) Riddles (problem solving): The voice agent shows an": "Annotating data from a wide age range of participants was also"
        },
        {
          "II. RELATED WORK": "pants are asked to repeat the sentences spoken by the machine.",
          "d) Riddles (problem solving): The voice agent shows an": "challenging for similar\nreasons [22],\n[23]."
        },
        {
          "II. RELATED WORK": "All\nthe\nsentences were\nin English,\nbut machine\nspeech\nis",
          "d) Riddles (problem solving): The voice agent shows an": "Revised user state categories: We reviewed the extracted"
        },
        {
          "II. RELATED WORK": "generated with various foreign accents.",
          "d) Riddles (problem solving): The voice agent shows an": "clips without context,\ni.e., only a segment of a video record-"
        },
        {
          "II. RELATED WORK": "c) Puzzles\n(problem solving):\nParticipants are asked to",
          "d) Riddles (problem solving): The voice agent shows an": "ing without\ntask information or audio,\nthus\nsolely based on"
        },
        {
          "II. RELATED WORK": "solve puzzles with varying difﬁculty. They must\nspeak out",
          "d) Riddles (problem solving): The voice agent shows an": "the\naforementioned\nfacial/behavioral\nfeatures\n.\nSome\nclips"
        },
        {
          "II. RELATED WORK": "their solution to the machine.",
          "d) Riddles (problem solving): The voice agent shows an": "labeled\nconfused were\ndifﬁcult\nto\ndistinguish\nfrom other"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Session\nTop rated challenge(s)\nTop rated main emotion(s)\nAvg. confusion": "NONE or MINOR: The task was easy (50%)\nIndifferent"
        },
        {
          "Session\nTop rated challenge(s)\nTop rated main emotion(s)\nAvg. confusion": "SpeakToMachine\n1.66"
        },
        {
          "Session\nTop rated challenge(s)\nTop rated main emotion(s)\nAvg. confusion": "ASR SYSTEM FAILURE: The computer did not work as I expected (43%)\nUnexpected, negative surprise"
        },
        {
          "Session\nTop rated challenge(s)\nTop rated main emotion(s)\nAvg. confusion": "RepeatAfterMachine\nMACHINE SPEECH:\nI had difﬁculty understanding what\nthe computer was saying\nIrritated, annoyed\n2.69"
        },
        {
          "Session\nTop rated challenge(s)\nTop rated main emotion(s)\nAvg. confusion": "Riddles\nPROBLEM SOLVING: The question was difﬁcult\nto solve/answer\nUnsure, uncertain, apprehensive\n2.33"
        },
        {
          "Session\nTop rated challenge(s)\nTop rated main emotion(s)\nAvg. confusion": "Puzzles\nPROBLEM SOLVING: The question was difﬁcult\nto solve/answer\nUnsure, uncertain, apprehensive\n2.96"
        },
        {
          "Session\nTop rated challenge(s)\nTop rated main emotion(s)\nAvg. confusion": "TABLE I: Top rated challenge(s), emotion(s), and average level of confusion for each session (different types of tasks). Note that"
        },
        {
          "Session\nTop rated challenge(s)\nTop rated main emotion(s)\nAvg. confusion": "ASR system failure and machine speech failure are categorized as communication failure for analysis, but\nreported separately"
        },
        {
          "Session\nTop rated challenge(s)\nTop rated main emotion(s)\nAvg. confusion": "in this table."
        },
        {
          "Session\nTop rated challenge(s)\nTop rated main emotion(s)\nAvg. confusion": "categories\nlike\nirritated,\nfrustrated,\napprehensive/anxious, or\nrelatively\neasy\nbecause\nof\nthe\nhigh\naccuracy\nof\nthe ASR"
        },
        {
          "Session\nTop rated challenge(s)\nTop rated main emotion(s)\nAvg. confusion": "focused,\nbecause\nof\nsimilarity\nof\nthe\nfacial/behavioral\nfea-\nengine. The\nsecond\nranked\nchallenge\nfor\nthis\nsession was"
        },
        {
          "Session\nTop rated challenge(s)\nTop rated main emotion(s)\nAvg. confusion": "tures. Because\nof\nthis\nambiguity, we\ndecided\nto\nseparate\nASR system failure, with associated main emotion(s) unex-"
        },
        {
          "Session\nTop rated challenge(s)\nTop rated main emotion(s)\nAvg. confusion": "out\nconfusion\nand\nfurther\nanalyze\nit\nin\nrelation with\nother\npected/negative\nsurprise.\nFor\nother\nsessions,\nonly\nthe\ntop-"
        },
        {
          "Session\nTop rated challenge(s)\nTop rated main emotion(s)\nAvg. confusion": "affective\nstates\nin the\nfollow-up study. We\nalso revised the\nranked items\nare\nshown in the\ntable. We\ncan see different"
        },
        {
          "Session\nTop rated challenge(s)\nTop rated main emotion(s)\nAvg. confusion": "user\nstate\ncategories\nto cover\nsubtle differences\nin affective\nmain emotions are reported for different\ntype of challenges."
        },
        {
          "Session\nTop rated challenge(s)\nTop rated main emotion(s)\nAvg. confusion": "states\nunder\nchallenging\ntasks. The\nrevised\ncategories\nare:\nInterestingly,\nchallenges\nin\nhuman-machine\ncommunication"
        },
        {
          "Session\nTop rated challenge(s)\nTop rated main emotion(s)\nAvg. confusion": "indifferent, happy/amused, surprise/realization (e.g., “oh, I got\ntasks\ninduce more negative emotions\nthan those reported in"
        },
        {
          "Session\nTop rated challenge(s)\nTop rated main emotion(s)\nAvg. confusion": "it!, wow!”), negative surprise/unexpected (e.g., “what’s going\nproblem-solving tasks."
        },
        {
          "Session\nTop rated challenge(s)\nTop rated main emotion(s)\nAvg. confusion": "In Table\nII,\nthe\nlast\nrow shows\nthe distribution of main\non?”),\nfocused/thoughtful, uncertain/apprehensive/anxious,\nir-"
        },
        {
          "Session\nTop rated challenge(s)\nTop rated main emotion(s)\nAvg. confusion": "emotions\nreported\nfrom all\nparticipants. All\ncategories\nare\nritated, and frustrated."
        },
        {
          "Session\nTop rated challenge(s)\nTop rated main emotion(s)\nAvg. confusion": "fairly\nequally\ndistributed\nexcept\nindifferent\nat\nabout\ntwice"
        },
        {
          "Session\nTop rated challenge(s)\nTop rated main emotion(s)\nAvg. confusion": "V. STUDY 2: SELF-REPORT ANALYSIS"
        },
        {
          "Session\nTop rated challenge(s)\nTop rated main emotion(s)\nAvg. confusion": "the\nothers. We\ncompare\nthese\nemotional\nstates\nin\nvalence-"
        },
        {
          "Session\nTop rated challenge(s)\nTop rated main emotion(s)\nAvg. confusion": "arousal dimensions. We visualize valence-arousal\nscores\nfor\nIn the second study, we addressed research questions\nthat"
        },
        {
          "Session\nTop rated challenge(s)\nTop rated main emotion(s)\nAvg. confusion": "selected\nemotions\nin\nFigure\n2\n(a)-(d). We\ncan\nsee\nthat\narose from the ﬁrst\nstudy by evaluating the user\nstate cate-"
        },
        {
          "Session\nTop rated challenge(s)\nTop rated main emotion(s)\nAvg. confusion": "amused/happy and surprise/realization are trending positively,\ngories. This time, we collected a self-report\nfrom participants"
        },
        {
          "Session\nTop rated challenge(s)\nTop rated main emotion(s)\nAvg. confusion": "whereas\nfrustrated and irritated are more distributed in the\nafter\neach\ntask\nabout\ntheir\nemotional\nstates,\naffect\nstate\nin"
        },
        {
          "Session\nTop rated challenge(s)\nTop rated main emotion(s)\nAvg. confusion": "negative space along the valence axis. Not\nincluded in the ﬁg-\nterms of valence and arousal,\nlevel of confusion, and the main"
        },
        {
          "Session\nTop rated challenge(s)\nTop rated main emotion(s)\nAvg. confusion": "ure, other emotions such as unexpected/negative surprise, un-\nchallenge of the task. The questionnaire included the following"
        },
        {
          "Session\nTop rated challenge(s)\nTop rated main emotion(s)\nAvg. confusion": "sure/uncertain/apprehensive,\nfocused/thoughtful, do not\nshow\nquestions and options to choose from."
        },
        {
          "Session\nTop rated challenge(s)\nTop rated main emotion(s)\nAvg. confusion": "clear trend in distribution either in negative or positive valence\na) Main\nchallenge:"
        },
        {
          "Session\nTop rated challenge(s)\nTop rated main emotion(s)\nAvg. confusion": "“none\nor minor”,\n“ASR\nsystem"
        },
        {
          "Session\nTop rated challenge(s)\nTop rated main emotion(s)\nAvg. confusion": "axis. Based only on this result,\nthese states might not be suit-"
        },
        {
          "Session\nTop rated challenge(s)\nTop rated main emotion(s)\nAvg. confusion": "failure”,\n“machine\nspeech”,\n“problem solving”,\n“memory”,"
        },
        {
          "Session\nTop rated challenge(s)\nTop rated main emotion(s)\nAvg. confusion": "able as distinctive affective states, but we believe they could"
        },
        {
          "Session\nTop rated challenge(s)\nTop rated main emotion(s)\nAvg. confusion": "“other”, with short descriptions for each option."
        },
        {
          "Session\nTop rated challenge(s)\nTop rated main emotion(s)\nAvg. confusion": "still provide\nimportant\ncues\nfor human-machine\ninteraction."
        },
        {
          "Session\nTop rated challenge(s)\nTop rated main emotion(s)\nAvg. confusion": "b) Most\nsigniﬁcant\nfeelings/emotions during the\ntask:"
        },
        {
          "Session\nTop rated challenge(s)\nTop rated main emotion(s)\nAvg. confusion": "Further\nanalysis\nin facial/behavioral/audio features\nfrom the"
        },
        {
          "Session\nTop rated challenge(s)\nTop rated main emotion(s)\nAvg. confusion": "“indifferent”,\n“happy/amused”,\n“surprise/realization”,\n“unex-"
        },
        {
          "Session\nTop rated challenge(s)\nTop rated main emotion(s)\nAvg. confusion": "recordings could help better understand the emotional states."
        },
        {
          "Session\nTop rated challenge(s)\nTop rated main emotion(s)\nAvg. confusion": "pected/negative\nsurprise”,\n“unsure/uncertain/apprehensive”,"
        },
        {
          "Session\nTop rated challenge(s)\nTop rated main emotion(s)\nAvg. confusion": "B. Level of confusion\n“irritated/annoyed”,\n“frustrated”,\n“focused/thoughtful”,"
        },
        {
          "Session\nTop rated challenge(s)\nTop rated main emotion(s)\nAvg. confusion": "“other”.\nA\nfollow-up\nquestion\nasks\nto\nselect\nall\nother"
        },
        {
          "Session\nTop rated challenge(s)\nTop rated main emotion(s)\nAvg. confusion": "The\naverage\nlevel of\nconfusion for\nincomplete\ntasks,\ni.e.,"
        },
        {
          "Session\nTop rated challenge(s)\nTop rated main emotion(s)\nAvg. confusion": "signiﬁcant\nfeelings/emotions experienced. Same options plus"
        },
        {
          "Session\nTop rated challenge(s)\nTop rated main emotion(s)\nAvg. confusion": "participants\nfailed\nto\ncorrectly\nperform the\ntask\nafter\nthree"
        },
        {
          "Session\nTop rated challenge(s)\nTop rated main emotion(s)\nAvg. confusion": "“no other\nfeelings” are given."
        },
        {
          "Session\nTop rated challenge(s)\nTop rated main emotion(s)\nAvg. confusion": "trials, was\n2.93, which was\nhigher\nthan\naverage\nlevel\nof"
        },
        {
          "Session\nTop rated challenge(s)\nTop rated main emotion(s)\nAvg. confusion": "c) How pleasant\nthe\nexperience was\nduring\nthe\ntask\nconfusion, 1.67,\nfor\ncomplete\ntasks\n( p (cid:28) 0.5,\ntwo-sample"
        },
        {
          "Session\nTop rated challenge(s)\nTop rated main emotion(s)\nAvg. confusion": "(valence), and how strong it was (arousal): Self-Assessment\nt-test assuming unequal variance)."
        },
        {
          "Session\nTop rated challenge(s)\nTop rated main emotion(s)\nAvg. confusion": "Manikin (SAM)\n[24]\nin scale of 1 to 9, each.\nIn the ﬁrst study, we observed ambiguity in the presentation"
        },
        {
          "Session\nTop rated challenge(s)\nTop rated main emotion(s)\nAvg. confusion": "d) Level of confusion:\nin scale of 1 (not confused at all)\nof confusion,\nsharing similar\nfacial/behavioral\nfeatures\nfrom"
        },
        {
          "Session\nTop rated challenge(s)\nTop rated main emotion(s)\nAvg. confusion": "to 5 (extremely confused).\nother user state categories. Table II shows distribution of\nthe"
        },
        {
          "Session\nTop rated challenge(s)\nTop rated main emotion(s)\nAvg. confusion": "We collected total 241 sets of\nself-report\nfrom 10 partici-\nmain emotions for different\nlevel of confusion in range 1 (not"
        },
        {
          "Session\nTop rated challenge(s)\nTop rated main emotion(s)\nAvg. confusion": "pants, 5 females and 5 males. 9 of\nthem reported their age\nconfused at\nall)\nto 5(extremely confused).\nIn the\nfollowing"
        },
        {
          "Session\nTop rated challenge(s)\nTop rated main emotion(s)\nAvg. confusion": "range 20-29 (45%), 30-39 (9%), 40-49 (36%), 50-59 (9%).\nanalysis, we\nrefer\nto\nlevels\n4\nand\n5\nas\n“high\nconfusion“."
        },
        {
          "Session\nTop rated challenge(s)\nTop rated main emotion(s)\nAvg. confusion": "33% of them answered that\nthey have almost never used voice\nUnder high confusion,\nemotions\nsuch as\nfrustration (25%),"
        },
        {
          "Session\nTop rated challenge(s)\nTop rated main emotion(s)\nAvg. confusion": "assistants, 33% answered they use almost everyday, 11% few\nunsure, uncertain, apprehensive 20%,\nirritated 18%,\nfocused,"
        },
        {
          "Session\nTop rated challenge(s)\nTop rated main emotion(s)\nAvg. confusion": "times a week, 11% few times a month, 11% few times a year.\nthoughtful 15%, unexpected, and negative surprise 11% were"
        },
        {
          "Session\nTop rated challenge(s)\nTop rated main emotion(s)\nAvg. confusion": "reported more often than others."
        },
        {
          "Session\nTop rated challenge(s)\nTop rated main emotion(s)\nAvg. confusion": "A. Emotional states"
        },
        {
          "Session\nTop rated challenge(s)\nTop rated main emotion(s)\nAvg. confusion": "Figure 1 shows screenshot of\nthree participants under high"
        },
        {
          "Session\nTop rated challenge(s)\nTop rated main emotion(s)\nAvg. confusion": "Table I\nshows\nthe top rated challenge(s), emotion(s), and\nconfusion state. The main emotions they reported were:\nfrus-"
        },
        {
          "Session\nTop rated challenge(s)\nTop rated main emotion(s)\nAvg. confusion": "average\nlevel\nof\nconfusion\nfor\neach\nsession\nobtained\nfrom\ntrated,\nirritated, and focused, respectively. We can see the dif-"
        },
        {
          "Session\nTop rated challenge(s)\nTop rated main emotion(s)\nAvg. confusion": "self-reports. The “Speak to Machine” session was considered\nference in the exhibited emotions from their\nfacial/behavioral"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Fig. 2: Valence": "confusion (e). Size of the circle is proportional",
          "(negative/positive)": "",
          "and arousal": "",
          "(low/high) measures": "to the data count. (a) and (b) are distributed mostly in the positive valence area.",
          "for": "",
          "selected emotional": "",
          "states": "",
          "(a)-(d)": "",
          "and high level of": ""
        },
        {
          "Fig. 2: Valence": "(c),(d),(e) are mostly concentrated in the negative valence area, with (e) having some data appear",
          "(negative/positive)": "",
          "and arousal": "",
          "(low/high) measures": "",
          "for": "",
          "selected emotional": "",
          "states": "",
          "(a)-(d)": "in positive valence area.",
          "and high level of": ""
        },
        {
          "Fig. 2: Valence": "Confusion",
          "(negative/positive)": "Unexpected",
          "and arousal": "Surprise",
          "(low/high) measures": "Amused",
          "for": "Focused",
          "selected emotional": "Frustrated",
          "states": "Indifferent",
          "(a)-(d)": "Irritated",
          "and high level of": "Total"
        },
        {
          "Fig. 2: Valence": "1",
          "(negative/positive)": "4.68% (5)",
          "and arousal": "8.42% (9)",
          "(low/high) measures": "24.3% (26)",
          "for": "9.35% (10)",
          "selected emotional": "3.74% (4)",
          "states": "38.32% (41)",
          "(a)-(d)": "7.48% (8)",
          "and high level of": "100% (107)"
        },
        {
          "Fig. 2: Valence": "2",
          "(negative/positive)": "14.64% (6)",
          "and arousal": "19.52% (8)",
          "(low/high) measures": "12.2% (5)",
          "for": "4.88% (2)",
          "selected emotional": "7.32% (3)",
          "states": "9.76% (4)",
          "(a)-(d)": "14.64% (6)",
          "and high level of": "100% (41)"
        },
        {
          "Fig. 2: Valence": "3",
          "(negative/positive)": "26.32% (10)",
          "and arousal": "15.79% (6)",
          "(low/high) measures": "7.9% (3)",
          "for": "7.9% (3)",
          "selected emotional": "7.9% (3)",
          "states": "13.16% (5)",
          "(a)-(d)": "13.16% (5)",
          "and high level of": "100% (38)"
        },
        {
          "Fig. 2: Valence": "4",
          "(negative/positive)": "8.34% (3)",
          "and arousal": "2.78% (1)",
          "(low/high) measures": "2.78% (1)",
          "for": "11.12% (4)",
          "selected emotional": "25% (9)",
          "states": "5.56% (2)",
          "(a)-(d)": "22.23% (8)",
          "and high level of": "100% (36)"
        },
        {
          "Fig. 2: Valence": "5",
          "(negative/positive)": "16.67% (3)",
          "and arousal": "5.56% (1)",
          "(low/high) measures": "0% (0)",
          "for": "22.23% (4)",
          "selected emotional": "27.78% (5)",
          "states": "0% (0)",
          "(a)-(d)": "11.12% (2)",
          "and high level of": "100% (18)"
        },
        {
          "Fig. 2: Valence": "Total",
          "(negative/positive)": "11.25% (27)",
          "and arousal": "10.42% (25)",
          "(low/high) measures": "14.59% (35)",
          "for": "9.59% (23)",
          "selected emotional": "10% (24)",
          "states": "21.67% (52)",
          "(a)-(d)": "12.09% (29)",
          "and high level of": "100% (240)"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "confusion is exhibited in multiple different\nforms.": "In the valence-arousal dimension (see Fig. 2 (e)), confusion",
          "tasks\nthan those\nin problem-solving tasks. Thus, preventing": "the communication failures or having a way to recover\nfrom"
        },
        {
          "confusion is exhibited in multiple different\nforms.": "is mostly distributed in the negative valence and high arousal",
          "tasks\nthan those\nin problem-solving tasks. Thus, preventing": "these failures might help greatly improving user experience."
        },
        {
          "confusion is exhibited in multiple different\nforms.": "space, but we observe some data points in other quadrants. The",
          "tasks\nthan those\nin problem-solving tasks. Thus, preventing": ""
        },
        {
          "confusion is exhibited in multiple different\nforms.": "",
          "tasks\nthan those\nin problem-solving tasks. Thus, preventing": "VI. CONCLUSION AND FUTURE WORK"
        },
        {
          "confusion is exhibited in multiple different\nforms.": "positive ones\ninclude main emotions\nas\nsurprise/realization,",
          "tasks\nthan those\nin problem-solving tasks. Thus, preventing": ""
        },
        {
          "confusion is exhibited in multiple different\nforms.": "indifferent,\nunsure/uncertain\nand\nunexpected, mostly with",
          "tasks\nthan those\nin problem-solving tasks. Thus, preventing": "We\npresented\nresults\nof\ntwo\nstudies\non\nuser\nstates\nin"
        },
        {
          "confusion is exhibited in multiple different\nforms.": "amused/happy reported as\nsecondary emotion. This\nsuggests",
          "tasks\nthan those\nin problem-solving tasks. Thus, preventing": "voice-based\nhuman-machine\ninteraction.\nIn\nthe\nﬁrst\nstudy,"
        },
        {
          "confusion is exhibited in multiple different\nforms.": "that confusion can sometimes be positive experience based on",
          "tasks\nthan those\nin problem-solving tasks. Thus, preventing": "we analyzed the data based on annotations and suggest user"
        },
        {
          "confusion is exhibited in multiple different\nforms.": "the outcome, but we leave further analysis as future work.",
          "tasks\nthan those\nin problem-solving tasks. Thus, preventing": "state categories.\nIn the second study, we analyze user\nstates"
        },
        {
          "confusion is exhibited in multiple different\nforms.": "Next, we compare high confusion between human-machine",
          "tasks\nthan those\nin problem-solving tasks. Thus, preventing": "based on self-report,\nin terms of discrete emotional categories"
        },
        {
          "confusion is exhibited in multiple different\nforms.": "communication\ntasks\nand\nproblem-solving\ntasks. Our\nre-",
          "tasks\nthan those\nin problem-solving tasks. Thus, preventing": "identiﬁed\nin\nthe\nﬁrst\nstudy,\nvalence/arousal measures,\nand"
        },
        {
          "confusion is exhibited in multiple different\nforms.": "sults\nshow that human-machine communication tasks gener-",
          "tasks\nthan those\nin problem-solving tasks. Thus, preventing": "level of confusion. Confusion is often induced when human-"
        },
        {
          "confusion is exhibited in multiple different\nforms.": "ally induced confusion more often than for problem-solving",
          "tasks\nthan those\nin problem-solving tasks. Thus, preventing": "machine communication fails, more speciﬁcally due to failure"
        },
        {
          "confusion is exhibited in multiple different\nforms.": "tasks\n(60% vs. 40%),\nalso in failure\ncases\n(55% vs. 45%).",
          "tasks\nthan those\nin problem-solving tasks. Thus, preventing": "of\nthe machine\nto\nunderstand\nhuman\nspeech\nor\nfailure\nto"
        },
        {
          "confusion is exhibited in multiple different\nforms.": "Under\nhighly\nconfused\nstates,\nthe\ntwo most\nfrequently\nre-",
          "tasks\nthan those\nin problem-solving tasks. Thus, preventing": "generate easily understandable machine speech. We found that"
        },
        {
          "confusion is exhibited in multiple different\nforms.": "ported main\nemotions were\nfrustrated\n(28%)\nand\nirritated",
          "tasks\nthan those\nin problem-solving tasks. Thus, preventing": "confusion in these scenarios is a complex state where different"
        },
        {
          "confusion is exhibited in multiple different\nforms.": "(21%)\nfor\nhuman-machine\ncommunication\ntasks,\nand\nun-",
          "tasks\nthan those\nin problem-solving tasks. Thus, preventing": "emotion can be exhibited. We also found some difference in"
        },
        {
          "confusion is exhibited in multiple different\nforms.": "sure/uncertain/apprehensive\n(32%)\nand\nfrustrated\n(23%)\nfor",
          "tasks\nthan those\nin problem-solving tasks. Thus, preventing": "emotions\ntowards\nfailures\nin human-machine communication"
        },
        {
          "confusion is exhibited in multiple different\nforms.": "problem-solving tasks.",
          "tasks\nthan those\nin problem-solving tasks. Thus, preventing": "tasks compared to problem-solving tasks, but no statistically"
        },
        {
          "confusion is exhibited in multiple different\nforms.": "In the valence-arousal measures, we did not ﬁnd statistically",
          "tasks\nthan those\nin problem-solving tasks. Thus, preventing": "meaningful\ndifference\nbetween\nconfusion\nin\ntwo\ndifferent"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "[9] D. McColl, A. Hong, N. Hatakeyama, G. Nejat,\nand B. Benhabib,": "“A survey of\nautonomous human affect detection methods\nfor\nsocial"
        },
        {
          "[9] D. McColl, A. Hong, N. Hatakeyama, G. Nejat,\nand B. Benhabib,": "Journal\nof\nIntelligent & Robotic\nrobots\nengaged\nin\nnatural\nhri,”"
        },
        {
          "[9] D. McColl, A. Hong, N. Hatakeyama, G. Nejat,\nand B. Benhabib,": "Systems, vol. 82, no. 1, pp. 101–133, Apr 2016.\n[Online]. Available:"
        },
        {
          "[9] D. McColl, A. Hong, N. Hatakeyama, G. Nejat,\nand B. Benhabib,": "https://doi.org/10.1007/s10846-015-0259-2"
        },
        {
          "[9] D. McColl, A. Hong, N. Hatakeyama, G. Nejat,\nand B. Benhabib,": "[10] G. Mcintyre\nand R. G¨ocke,\n“Affect\nand emotion in human-computer"
        },
        {
          "[9] D. McColl, A. Hong, N. Hatakeyama, G. Nejat,\nand B. Benhabib,": "interaction,” C. Peter and R. Beale, Eds.\nBerlin, Heidelberg: Springer-"
        },
        {
          "[9] D. McColl, A. Hong, N. Hatakeyama, G. Nejat,\nand B. Benhabib,": "Verlag, 2008, ch. The Composite Sensing of Affect, pp. 104–115.\n[On-"
        },
        {
          "[9] D. McColl, A. Hong, N. Hatakeyama, G. Nejat,\nand B. Benhabib,": "line]. Available: \\url{http://dx.doi.org/10.1007/978-3-540-85099-1 9}"
        },
        {
          "[9] D. McColl, A. Hong, N. Hatakeyama, G. Nejat,\nand B. Benhabib,": "[11] M. Imani and G. A. Montazer, “A survey of emotion recognition methods"
        },
        {
          "[9] D. McColl, A. Hong, N. Hatakeyama, G. Nejat,\nand B. Benhabib,": "Journal of Network and\nwith emphasis on e-learning environments,”"
        },
        {
          "[9] D. McColl, A. Hong, N. Hatakeyama, G. Nejat,\nand B. Benhabib,": "Computer Applications, vol. 147, p. 102423, 08 2019."
        },
        {
          "[9] D. McColl, A. Hong, N. Hatakeyama, G. Nejat,\nand B. Benhabib,": "[12] R. Calvo and S. D’Mello, “Affect detection: An interdisciplinary review"
        },
        {
          "[9] D. McColl, A. Hong, N. Hatakeyama, G. Nejat,\nand B. Benhabib,": "of models, methods,\nand their\napplications,” T. Affective Computing,"
        },
        {
          "[9] D. McColl, A. Hong, N. Hatakeyama, G. Nejat,\nand B. Benhabib,": "vol. 1, pp. 18–37, 01 2010."
        },
        {
          "[9] D. McColl, A. Hong, N. Hatakeyama, G. Nejat,\nand B. Benhabib,": "[13]\nJ. Klein, Y. Moon, and R. W. Picard, “This computer\nresponds to user"
        },
        {
          "[9] D. McColl, A. Hong, N. Hatakeyama, G. Nejat,\nand B. Benhabib,": "frustration: Theory, design,\nand results,”\nInteracting with Computers,"
        },
        {
          "[9] D. McColl, A. Hong, N. Hatakeyama, G. Nejat,\nand B. Benhabib,": "vol. 14, no. 2, pp. 119–140, Feb 2002."
        },
        {
          "[9] D. McColl, A. Hong, N. Hatakeyama, G. Nejat,\nand B. Benhabib,": "[14] K. Hone, “Empathic agents\nto reduce user\nfrustration: The effects of"
        },
        {
          "[9] D. McColl, A. Hong, N. Hatakeyama, G. Nejat,\nand B. Benhabib,": "varying\nagent\ncharacteristics,”\nInteract. Comput.,\nvol.\n18,\nno.\n2,\npp."
        },
        {
          "[9] D. McColl, A. Hong, N. Hatakeyama, G. Nejat,\nand B. Benhabib,": "227–245, Mar.\n2006.\n[Online]. Available:\nhttp://dx.doi.org/10.1016/j."
        },
        {
          "[9] D. McColl, A. Hong, N. Hatakeyama, G. Nejat,\nand B. Benhabib,": "intcom.2005.05.003"
        },
        {
          "[9] D. McColl, A. Hong, N. Hatakeyama, G. Nejat,\nand B. Benhabib,": "[15] M. E. Hoque, D. McDuff, and R. Picard, “Exploring temporal patterns"
        },
        {
          "[9] D. McColl, A. Hong, N. Hatakeyama, G. Nejat,\nand B. Benhabib,": "in classifying frustrated and delighted smiles\n(extended abstract),”\nin"
        },
        {
          "[9] D. McColl, A. Hong, N. Hatakeyama, G. Nejat,\nand B. Benhabib,": "2015 International Conference on Affective Computing and Intelligent"
        },
        {
          "[9] D. McColl, A. Hong, N. Hatakeyama, G. Nejat,\nand B. Benhabib,": "Interaction (ACII), 2015, pp. 505–511."
        },
        {
          "[9] D. McColl, A. Hong, N. Hatakeyama, G. Nejat,\nand B. Benhabib,": "[16]\nS.\nIshimaru, T. Maruichi, A. Dengel, and K. Kise, “Conﬁdence-aware"
        },
        {
          "[9] D. McColl, A. Hong, N. Hatakeyama, G. Nejat,\nand B. Benhabib,": "learning assistant,” 2021."
        },
        {
          "[9] D. McColl, A. Hong, N. Hatakeyama, G. Nejat,\nand B. Benhabib,": "[17]\nS. Halfon, M. Doyran, B. T¨urkmen, E. Aydın, and A. Salah, “Multi-"
        },
        {
          "[9] D. McColl, A. Hong, N. Hatakeyama, G. Nejat,\nand B. Benhabib,": "modal affect analysis of psychodynamic play therapy,” Psychotherapy"
        },
        {
          "[9] D. McColl, A. Hong, N. Hatakeyama, G. Nejat,\nand B. Benhabib,": "Research, vol. 31, 11 2020."
        },
        {
          "[9] D. McColl, A. Hong, N. Hatakeyama, G. Nejat,\nand B. Benhabib,": "International\n[18]\nS. K. D’Mello\nand A. C. Graesser,\n“Confusion,”\nin"
        },
        {
          "[9] D. McColl, A. Hong, N. Hatakeyama, G. Nejat,\nand B. Benhabib,": "handbook\nof\nemotions\nin\neducation, R. Pekrun\nand L. Linnenbrink-"
        },
        {
          "[9] D. McColl, A. Hong, N. Hatakeyama, G. Nejat,\nand B. Benhabib,": "Garcian, Eds.\nNew York: Routledge, 2014, ch. 15, pp. 289–310."
        },
        {
          "[9] D. McColl, A. Hong, N. Hatakeyama, G. Nejat,\nand B. Benhabib,": "[19]\nS. D’Mello, R. W. Picard, and A. Graesser, “Toward an affect-sensitive"
        },
        {
          "[9] D. McColl, A. Hong, N. Hatakeyama, G. Nejat,\nand B. Benhabib,": "autotutor,” IEEE Intelligent Systems, vol. 22, no. 4, pp. 53–61, July 2007."
        },
        {
          "[9] D. McColl, A. Hong, N. Hatakeyama, G. Nejat,\nand B. Benhabib,": "[20]\nZ. Liu, V.\nPataranutaporn,\nJ. Ocumpaugh,\nand R.\nS.\nJ.\nde Baker,"
        },
        {
          "[9] D. McColl, A. Hong, N. Hatakeyama, G. Nejat,\nand B. Benhabib,": "“Sequences of\nfrustration and confusion,\nand learning,”\nin Education"
        },
        {
          "[9] D. McColl, A. Hong, N. Hatakeyama, G. Nejat,\nand B. Benhabib,": "Data Mining, 2013."
        },
        {
          "[9] D. McColl, A. Hong, N. Hatakeyama, G. Nejat,\nand B. Benhabib,": "[21] A. Natal, G.\nShires, M. C´aceres,\nand\nP.\nJ¨agenstedt,\n“Web\nspeech"
        },
        {
          "[9] D. McColl, A. Hong, N. Hatakeyama, G. Nejat,\nand B. Benhabib,": "api\ndraft\ncommunity\ngroup\nreport,”\n2020.\n[Online].\nAvailable:"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "REFERENCES": "",
          "[22] C. Z. Malatesta, M. J. Fiore, and J. J. Messina, “Affect, personality, and": "facial expressive characteristics of older people.” Psychology and Aging,"
        },
        {
          "REFERENCES": "[1]\nJ. Kiseleva, K. Williams, J. Jiang, A. Hassan Awadallah, A. C. Crook,",
          "[22] C. Z. Malatesta, M. J. Fiore, and J. J. Messina, “Affect, personality, and": "vol. 2, no. 1, p. 64, 1987."
        },
        {
          "REFERENCES": "I. Zitouni,\nand T. Anastasakos,\n“Understanding user\nsatisfaction with",
          "[22] C. Z. Malatesta, M. J. Fiore, and J. J. Messina, “Affect, personality, and": "[23] G. Guo, R. Guo, and X. Li, “Facial expression recognition inﬂuenced"
        },
        {
          "REFERENCES": "the 2016 ACM on Conference\nintelligent assistants,” in Proceedings of",
          "[22] C. Z. Malatesta, M. J. Fiore, and J. J. Messina, “Affect, personality, and": "by human aging,” IEEE Transactions on Affective Computing, vol. 4,"
        },
        {
          "REFERENCES": "on Human\nInformation\nInteraction\nand\nRetrieval,\nser. CHIIR ’16.",
          "[22] C. Z. Malatesta, M. J. Fiore, and J. J. Messina, “Affect, personality, and": "no. 3, pp. 291–298, July 2013."
        },
        {
          "REFERENCES": "New York, NY, USA: ACM, 2016, pp. 121–130.\n[Online]. Available:",
          "[22] C. Z. Malatesta, M. J. Fiore, and J. J. Messina, “Affect, personality, and": "[24]\nP. Lang, “The cognitive psychophysiology of emotion: Fear and anxiety.”"
        },
        {
          "REFERENCES": "http://doi.acm.org/10.1145/2854946.2854961",
          "[22] C. Z. Malatesta, M. J. Fiore, and J. J. Messina, “Affect, personality, and": "in Anxiety and the Anxiety Disorders, 1985."
        },
        {
          "REFERENCES": "[2] G. Skantze,\n“Exploring human error handling strategies:\nImplications",
          "[22] C. Z. Malatesta, M. J. Fiore, and J. J. Messina, “Affect, personality, and": "[25] M.\nE. Hoque, R.\nE. Kaliouby,\nand R. W.\nPicard,\n“When\nhuman"
        },
        {
          "REFERENCES": "for spoken dialogue systems,” in ISCA Tutorial and Research Workshop",
          "[22] C. Z. Malatesta, M. J. Fiore, and J. J. Messina, “Affect, personality, and": "coders\n(and machines)\ndisagree\non\nthe meaning\nof\nfacial\naffect\nin"
        },
        {
          "REFERENCES": "on Error Handling in Spoken Dialogue Systems, 2003.",
          "[22] C. Z. Malatesta, M. J. Fiore, and J. J. Messina, “Affect, personality, and": "Intelligent Virtual Agents,\n9th\nInternational\nspontaneous\nvideos,”\nin"
        },
        {
          "REFERENCES": "[3] D. Bohus\nand A.\nI. Rudnicky,\n“Sorry,\ni\ndidn’t\ncatch\nthat!\n–\nan\nin-",
          "[22] C. Z. Malatesta, M. J. Fiore, and J. J. Messina, “Affect, personality, and": "Conference,\nIVA 2009, Amsterdam, The Netherlands,\nSeptember\n14-"
        },
        {
          "REFERENCES": "vestigation of non-understanding errors and recovery strategies,” in In",
          "[22] C. Z. Malatesta, M. J. Fiore, and J. J. Messina, “Affect, personality, and": "16,\n2009,\nProceedings,\nser.\nLecture Notes\nin\nComputer\nScience,"
        },
        {
          "REFERENCES": "Proceedings of\nthe 6th SIGdial Workshop on Discourse and Dialogue,",
          "[22] C. Z. Malatesta, M. J. Fiore, and J. J. Messina, “Affect, personality, and": "Z. Ruttkay, M. Kipp, A. Nijholt,\nand H. H. Vilhj´almsson,\nEds.,"
        },
        {
          "REFERENCES": "2005, pp. 128–143.",
          "[22] C. Z. Malatesta, M. J. Fiore, and J. J. Messina, “Affect, personality, and": "vol.\n5773.\nSpringer,\n2009,\npp.\n337–343.\n[Online].\nAvailable:"
        },
        {
          "REFERENCES": "[4]\nS. Kim, D. Salter, L. DeLuccia, and A. Tamrakar, “Study on text-based",
          "[22] C. Z. Malatesta, M. J. Fiore, and J. J. Messina, “Affect, personality, and": "https://doi.org/10.1007/978-3-642-04380-2\\ 37"
        },
        {
          "REFERENCES": "and voice-based dialogue interfaces for human-computer interactions in",
          "[22] C. Z. Malatesta, M. J. Fiore, and J. J. Messina, “Affect, personality, and": ""
        },
        {
          "REFERENCES": "the 8th International Conference\na blocks world,”\nin Proceedings of",
          "[22] C. Z. Malatesta, M. J. Fiore, and J. J. Messina, “Affect, personality, and": ""
        },
        {
          "REFERENCES": "on Human-Agent\nInteraction,\nser. HAI\n’20.\nNew York, NY, USA:",
          "[22] C. Z. Malatesta, M. J. Fiore, and J. J. Messina, “Affect, personality, and": ""
        },
        {
          "REFERENCES": "Association\nfor Computing Machinery,\n2020,\np.\n227–229.\n[Online].",
          "[22] C. Z. Malatesta, M. J. Fiore, and J. J. Messina, “Affect, personality, and": ""
        },
        {
          "REFERENCES": "Available: https://doi.org/10.1145/3406499.3418754",
          "[22] C. Z. Malatesta, M. J. Fiore, and J. J. Messina, “Affect, personality, and": ""
        },
        {
          "REFERENCES": "[5] M.\nMarge\nand\nA.\nI.\nRudnicky,\n“Miscommunication\ndetection",
          "[22] C. Z. Malatesta, M. J. Fiore, and J. J. Messina, “Affect, personality, and": ""
        },
        {
          "REFERENCES": "ACM\nTrans.\nand\nrecovery\nin\nsituated\nhuman–robot\ndialogue,”",
          "[22] C. Z. Malatesta, M. J. Fiore, and J. J. Messina, “Affect, personality, and": ""
        },
        {
          "REFERENCES": "Interact.\nIntell.\nSyst.,\nvol.\n9,\nno.\n1, Feb.\n2019.\n[Online]. Available:",
          "[22] C. Z. Malatesta, M. J. Fiore, and J. J. Messina, “Affect, personality, and": ""
        },
        {
          "REFERENCES": "https://doi.org/10.1145/3237189",
          "[22] C. Z. Malatesta, M. J. Fiore, and J. J. Messina, “Affect, personality, and": ""
        },
        {
          "REFERENCES": "[6]\nJ. Edlund,\nJ. Gustafson, M. Heldner,\nand A. Hjalmarsson,\n“Towards",
          "[22] C. Z. Malatesta, M. J. Fiore, and J. J. Messina, “Affect, personality, and": ""
        },
        {
          "REFERENCES": "human-like\nspoken dialogue\nsystems,” Speech Commun., vol. 50, pp.",
          "[22] C. Z. Malatesta, M. J. Fiore, and J. J. Messina, “Affect, personality, and": ""
        },
        {
          "REFERENCES": "630–645, 2008.",
          "[22] C. Z. Malatesta, M. J. Fiore, and J. J. Messina, “Affect, personality, and": ""
        },
        {
          "REFERENCES": "[7]\nS. Li\nand W. Deng,\n“Deep facial\nexpression recognition: A survey,”",
          "[22] C. Z. Malatesta, M. J. Fiore, and J. J. Messina, “Affect, personality, and": ""
        },
        {
          "REFERENCES": "IEEE Transactions on Affective Computing, pp. 1–1, 2020.",
          "[22] C. Z. Malatesta, M. J. Fiore, and J. J. Messina, “Affect, personality, and": ""
        },
        {
          "REFERENCES": "[8]\nL. F. Barrett and E. Bliss-Moreau, “Affect as a psychological primitive,”",
          "[22] C. Z. Malatesta, M. J. Fiore, and J. J. Messina, “Affect, personality, and": ""
        },
        {
          "REFERENCES": "Advances in experimental social psychology, vol. 41, pp. 167–218, 2009.",
          "[22] C. Z. Malatesta, M. J. Fiore, and J. J. Messina, “Affect, personality, and": ""
        },
        {
          "REFERENCES": "[9] D. McColl, A. Hong, N. Hatakeyama, G. Nejat,\nand B. Benhabib,",
          "[22] C. Z. Malatesta, M. J. Fiore, and J. J. Messina, “Affect, personality, and": ""
        },
        {
          "REFERENCES": "“A survey of\nautonomous human affect detection methods\nfor\nsocial",
          "[22] C. Z. Malatesta, M. J. Fiore, and J. J. Messina, “Affect, personality, and": ""
        },
        {
          "REFERENCES": "Journal\nof\nIntelligent & Robotic\nrobots\nengaged\nin\nnatural\nhri,”",
          "[22] C. Z. Malatesta, M. J. Fiore, and J. J. Messina, “Affect, personality, and": ""
        },
        {
          "REFERENCES": "Systems, vol. 82, no. 1, pp. 101–133, Apr 2016.\n[Online]. Available:",
          "[22] C. Z. Malatesta, M. J. Fiore, and J. J. Messina, “Affect, personality, and": ""
        },
        {
          "REFERENCES": "https://doi.org/10.1007/s10846-015-0259-2",
          "[22] C. Z. Malatesta, M. J. Fiore, and J. J. Messina, “Affect, personality, and": ""
        },
        {
          "REFERENCES": "[10] G. Mcintyre\nand R. G¨ocke,\n“Affect\nand emotion in human-computer",
          "[22] C. Z. Malatesta, M. J. Fiore, and J. J. Messina, “Affect, personality, and": ""
        },
        {
          "REFERENCES": "interaction,” C. Peter and R. Beale, Eds.\nBerlin, Heidelberg: Springer-",
          "[22] C. Z. Malatesta, M. J. Fiore, and J. J. Messina, “Affect, personality, and": ""
        },
        {
          "REFERENCES": "Verlag, 2008, ch. The Composite Sensing of Affect, pp. 104–115.\n[On-",
          "[22] C. Z. Malatesta, M. J. Fiore, and J. J. Messina, “Affect, personality, and": ""
        },
        {
          "REFERENCES": "line]. Available: \\url{http://dx.doi.org/10.1007/978-3-540-85099-1 9}",
          "[22] C. Z. Malatesta, M. J. Fiore, and J. J. Messina, “Affect, personality, and": ""
        },
        {
          "REFERENCES": "[11] M. Imani and G. A. Montazer, “A survey of emotion recognition methods",
          "[22] C. Z. Malatesta, M. J. Fiore, and J. J. Messina, “Affect, personality, and": ""
        },
        {
          "REFERENCES": "Journal of Network and\nwith emphasis on e-learning environments,”",
          "[22] C. Z. Malatesta, M. J. Fiore, and J. J. Messina, “Affect, personality, and": ""
        },
        {
          "REFERENCES": "Computer Applications, vol. 147, p. 102423, 08 2019.",
          "[22] C. Z. Malatesta, M. J. Fiore, and J. J. Messina, “Affect, personality, and": ""
        },
        {
          "REFERENCES": "[12] R. Calvo and S. D’Mello, “Affect detection: An interdisciplinary review",
          "[22] C. Z. Malatesta, M. J. Fiore, and J. J. Messina, “Affect, personality, and": ""
        },
        {
          "REFERENCES": "of models, methods,\nand their\napplications,” T. Affective Computing,",
          "[22] C. Z. Malatesta, M. J. Fiore, and J. J. Messina, “Affect, personality, and": ""
        },
        {
          "REFERENCES": "vol. 1, pp. 18–37, 01 2010.",
          "[22] C. Z. Malatesta, M. J. Fiore, and J. J. Messina, “Affect, personality, and": ""
        },
        {
          "REFERENCES": "[13]\nJ. Klein, Y. Moon, and R. W. Picard, “This computer\nresponds to user",
          "[22] C. Z. Malatesta, M. J. Fiore, and J. J. Messina, “Affect, personality, and": ""
        },
        {
          "REFERENCES": "frustration: Theory, design,\nand results,”\nInteracting with Computers,",
          "[22] C. Z. Malatesta, M. J. Fiore, and J. J. Messina, “Affect, personality, and": ""
        },
        {
          "REFERENCES": "vol. 14, no. 2, pp. 119–140, Feb 2002.",
          "[22] C. Z. Malatesta, M. J. Fiore, and J. J. Messina, “Affect, personality, and": ""
        },
        {
          "REFERENCES": "[14] K. Hone, “Empathic agents\nto reduce user\nfrustration: The effects of",
          "[22] C. Z. Malatesta, M. J. Fiore, and J. J. Messina, “Affect, personality, and": ""
        },
        {
          "REFERENCES": "varying\nagent\ncharacteristics,”\nInteract. Comput.,\nvol.\n18,\nno.\n2,\npp.",
          "[22] C. Z. Malatesta, M. J. Fiore, and J. J. Messina, “Affect, personality, and": ""
        },
        {
          "REFERENCES": "227–245, Mar.\n2006.\n[Online]. Available:\nhttp://dx.doi.org/10.1016/j.",
          "[22] C. Z. Malatesta, M. J. Fiore, and J. J. Messina, “Affect, personality, and": ""
        },
        {
          "REFERENCES": "intcom.2005.05.003",
          "[22] C. Z. Malatesta, M. J. Fiore, and J. J. Messina, “Affect, personality, and": ""
        },
        {
          "REFERENCES": "[15] M. E. Hoque, D. McDuff, and R. Picard, “Exploring temporal patterns",
          "[22] C. Z. Malatesta, M. J. Fiore, and J. J. Messina, “Affect, personality, and": ""
        },
        {
          "REFERENCES": "in classifying frustrated and delighted smiles\n(extended abstract),”\nin",
          "[22] C. Z. Malatesta, M. J. Fiore, and J. J. Messina, “Affect, personality, and": ""
        },
        {
          "REFERENCES": "2015 International Conference on Affective Computing and Intelligent",
          "[22] C. Z. Malatesta, M. J. Fiore, and J. J. Messina, “Affect, personality, and": ""
        },
        {
          "REFERENCES": "Interaction (ACII), 2015, pp. 505–511.",
          "[22] C. Z. Malatesta, M. J. Fiore, and J. J. Messina, “Affect, personality, and": ""
        },
        {
          "REFERENCES": "[16]\nS.\nIshimaru, T. Maruichi, A. Dengel, and K. Kise, “Conﬁdence-aware",
          "[22] C. Z. Malatesta, M. J. Fiore, and J. J. Messina, “Affect, personality, and": ""
        },
        {
          "REFERENCES": "learning assistant,” 2021.",
          "[22] C. Z. Malatesta, M. J. Fiore, and J. J. Messina, “Affect, personality, and": ""
        },
        {
          "REFERENCES": "[17]\nS. Halfon, M. Doyran, B. T¨urkmen, E. Aydın, and A. Salah, “Multi-",
          "[22] C. Z. Malatesta, M. J. Fiore, and J. J. Messina, “Affect, personality, and": ""
        },
        {
          "REFERENCES": "modal affect analysis of psychodynamic play therapy,” Psychotherapy",
          "[22] C. Z. Malatesta, M. J. Fiore, and J. J. Messina, “Affect, personality, and": ""
        },
        {
          "REFERENCES": "Research, vol. 31, 11 2020.",
          "[22] C. Z. Malatesta, M. J. Fiore, and J. J. Messina, “Affect, personality, and": ""
        },
        {
          "REFERENCES": "International\n[18]\nS. K. D’Mello\nand A. C. Graesser,\n“Confusion,”\nin",
          "[22] C. Z. Malatesta, M. J. Fiore, and J. J. Messina, “Affect, personality, and": ""
        },
        {
          "REFERENCES": "handbook\nof\nemotions\nin\neducation, R. Pekrun\nand L. Linnenbrink-",
          "[22] C. Z. Malatesta, M. J. Fiore, and J. J. Messina, “Affect, personality, and": ""
        },
        {
          "REFERENCES": "Garcian, Eds.\nNew York: Routledge, 2014, ch. 15, pp. 289–310.",
          "[22] C. Z. Malatesta, M. J. Fiore, and J. J. Messina, “Affect, personality, and": ""
        },
        {
          "REFERENCES": "[19]\nS. D’Mello, R. W. Picard, and A. Graesser, “Toward an affect-sensitive",
          "[22] C. Z. Malatesta, M. J. Fiore, and J. J. Messina, “Affect, personality, and": ""
        },
        {
          "REFERENCES": "autotutor,” IEEE Intelligent Systems, vol. 22, no. 4, pp. 53–61, July 2007.",
          "[22] C. Z. Malatesta, M. J. Fiore, and J. J. Messina, “Affect, personality, and": ""
        },
        {
          "REFERENCES": "[20]\nZ. Liu, V.\nPataranutaporn,\nJ. Ocumpaugh,\nand R.\nS.\nJ.\nde Baker,",
          "[22] C. Z. Malatesta, M. J. Fiore, and J. J. Messina, “Affect, personality, and": ""
        },
        {
          "REFERENCES": "“Sequences of\nfrustration and confusion,\nand learning,”\nin Education",
          "[22] C. Z. Malatesta, M. J. Fiore, and J. J. Messina, “Affect, personality, and": ""
        },
        {
          "REFERENCES": "Data Mining, 2013.",
          "[22] C. Z. Malatesta, M. J. Fiore, and J. J. Messina, “Affect, personality, and": ""
        },
        {
          "REFERENCES": "[21] A. Natal, G.\nShires, M. C´aceres,\nand\nP.\nJ¨agenstedt,\n“Web\nspeech",
          "[22] C. Z. Malatesta, M. J. Fiore, and J. J. Messina, “Affect, personality, and": ""
        },
        {
          "REFERENCES": "api\ndraft\ncommunity\ngroup\nreport,”\n2020.\n[Online].\nAvailable:",
          "[22] C. Z. Malatesta, M. J. Fiore, and J. J. Messina, “Affect, personality, and": ""
        },
        {
          "REFERENCES": "https://wicg.github.io/speech-api/",
          "[22] C. Z. Malatesta, M. J. Fiore, and J. J. Messina, “Affect, personality, and": ""
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Understanding user satisfaction with intelligent assistants",
      "authors": [
        "J Kiseleva",
        "K Williams",
        "J Jiang",
        "A Hassan Awadallah",
        "A Crook",
        "I Zitouni",
        "T Anastasakos"
      ],
      "year": "2016",
      "venue": "Proceedings of the 2016 ACM on Conference on Human Information Interaction and Retrieval, ser. CHIIR '16",
      "doi": "10.1145/2854946.2854961"
    },
    {
      "citation_id": "2",
      "title": "Exploring human error handling strategies: Implications for spoken dialogue systems",
      "authors": [
        "G Skantze"
      ],
      "year": "2003",
      "venue": "ISCA Tutorial and Research Workshop on Error Handling in Spoken Dialogue Systems"
    },
    {
      "citation_id": "3",
      "title": "Sorry, i didn't catch that! -an investigation of non-understanding errors and recovery strategies",
      "authors": [
        "D Bohus",
        "A Rudnicky"
      ],
      "year": "2005",
      "venue": "Proceedings of the 6th SIGdial Workshop on Discourse and Dialogue"
    },
    {
      "citation_id": "4",
      "title": "Study on text-based and voice-based dialogue interfaces for human-computer interactions in a blocks world",
      "authors": [
        "S Kim",
        "D Salter",
        "L Deluccia",
        "A Tamrakar"
      ],
      "year": "2020",
      "venue": "Proceedings of the 8th International Conference on Human-Agent Interaction, ser. HAI '20",
      "doi": "10.1145/3406499.3418754"
    },
    {
      "citation_id": "5",
      "title": "Miscommunication detection and recovery in situated human-robot dialogue",
      "authors": [
        "M Marge",
        "A Rudnicky"
      ],
      "year": "2019",
      "venue": "ACM Trans. Interact. Intell. Syst",
      "doi": "10.1145/3237189"
    },
    {
      "citation_id": "6",
      "title": "Towards human-like spoken dialogue systems",
      "authors": [
        "J Edlund",
        "J Gustafson",
        "M Heldner",
        "A Hjalmarsson"
      ],
      "year": "2008",
      "venue": "Speech Commun"
    },
    {
      "citation_id": "7",
      "title": "Deep facial expression recognition: A survey",
      "authors": [
        "S Li",
        "W Deng"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "8",
      "title": "Affect as a psychological primitive",
      "authors": [
        "L Barrett",
        "E Bliss-Moreau"
      ],
      "year": "2009",
      "venue": "Advances in experimental social psychology"
    },
    {
      "citation_id": "9",
      "title": "A survey of autonomous human affect detection methods for social robots engaged in natural hri",
      "authors": [
        "D Mccoll",
        "A Hong",
        "N Hatakeyama",
        "G Nejat",
        "B Benhabib"
      ],
      "year": "2016",
      "venue": "Journal of Intelligent & Robotic Systems",
      "doi": "10.1007/s10846-015-0259-2"
    },
    {
      "citation_id": "10",
      "title": "Affect and emotion in human-computer interaction",
      "authors": [
        "G Mcintyre",
        "R Göcke"
      ],
      "year": "2008",
      "venue": "The Composite Sensing of Affect",
      "doi": "10.1007/978-3-540-85099-19"
    },
    {
      "citation_id": "11",
      "title": "A survey of emotion recognition methods with emphasis on e-learning environments",
      "authors": [
        "M Imani",
        "G Montazer"
      ],
      "year": "2019",
      "venue": "Journal of Network and Computer Applications"
    },
    {
      "citation_id": "12",
      "title": "Affect detection: An interdisciplinary review of models, methods, and their applications",
      "authors": [
        "R Calvo",
        "S Mello"
      ],
      "year": "2010",
      "venue": "T. Affective Computing"
    },
    {
      "citation_id": "13",
      "title": "This computer responds to user frustration: Theory, design, and results",
      "authors": [
        "J Klein",
        "Y Moon",
        "R Picard"
      ],
      "year": "2002",
      "venue": "Interacting with Computers"
    },
    {
      "citation_id": "14",
      "title": "Empathic agents to reduce user frustration: The effects of varying agent characteristics",
      "authors": [
        "K Hone"
      ],
      "year": "2005",
      "venue": "Interact. Comput",
      "doi": "10.1016/j.intcom"
    },
    {
      "citation_id": "15",
      "title": "Exploring temporal patterns in classifying frustrated and delighted smiles (extended abstract)",
      "authors": [
        "M Hoque",
        "D Mcduff",
        "R Picard"
      ],
      "year": "2015",
      "venue": "2015 International Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "16",
      "title": "Confidence-aware learning assistant",
      "authors": [
        "S Ishimaru",
        "T Maruichi",
        "A Dengel",
        "K Kise"
      ],
      "year": "2021",
      "venue": "Confidence-aware learning assistant"
    },
    {
      "citation_id": "17",
      "title": "Multimodal affect analysis of psychodynamic play therapy",
      "authors": [
        "S Halfon",
        "M Doyran",
        "B Türkmen",
        "E Aydın",
        "A Salah"
      ],
      "venue": "Psychotherapy Research"
    },
    {
      "citation_id": "18",
      "title": "in International handbook of emotions in education",
      "authors": [
        "S Mello",
        "A Graesser"
      ],
      "year": "2014",
      "venue": "in International handbook of emotions in education"
    },
    {
      "citation_id": "19",
      "title": "Toward an affect-sensitive autotutor",
      "authors": [
        "S Mello",
        "R Picard",
        "A Graesser"
      ],
      "year": "2007",
      "venue": "IEEE Intelligent Systems"
    },
    {
      "citation_id": "20",
      "title": "Sequences of frustration and confusion, and learning",
      "authors": [
        "Z Liu",
        "V Pataranutaporn",
        "J Ocumpaugh",
        "R Baker"
      ],
      "year": "2013",
      "venue": "Education Data Mining"
    },
    {
      "citation_id": "21",
      "title": "Web speech api draft community group report",
      "authors": [
        "A Natal",
        "G Shires",
        "M Cáceres",
        "P Jägenstedt"
      ],
      "year": "2020",
      "venue": "Web speech api draft community group report"
    },
    {
      "citation_id": "22",
      "title": "Affect, personality, and facial expressive characteristics of older people",
      "authors": [
        "C Malatesta",
        "M Fiore",
        "J Messina"
      ],
      "year": "1987",
      "venue": "Psychology and Aging"
    },
    {
      "citation_id": "23",
      "title": "Facial expression recognition influenced by human aging",
      "authors": [
        "G Guo",
        "R Guo",
        "X Li"
      ],
      "year": "2013",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "24",
      "title": "The cognitive psychophysiology of emotion: Fear and anxiety",
      "authors": [
        "P Lang"
      ],
      "year": "1985",
      "venue": "Anxiety and the Anxiety Disorders"
    },
    {
      "citation_id": "25",
      "title": "When human coders (and machines) disagree on the meaning of facial affect in spontaneous videos",
      "authors": [
        "M Hoque",
        "R Kaliouby",
        "R Picard"
      ],
      "year": "2009",
      "venue": "Intelligent Virtual Agents, 9th International Conference",
      "doi": "10.1007/978-3-642-04380-2\\37"
    }
  ]
}