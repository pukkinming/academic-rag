{
  "paper_id": "2110.04091v1",
  "title": "Affective Burst Detection From Speech Using Kernel-Fusion Dilated Convolutional Neural Networks",
  "published": "2021-10-08T12:40:43Z",
  "authors": [
    "Berkay Kopru",
    "Engin Erzin"
  ],
  "keywords": [
    "Emotion recognition",
    "affective burst detection",
    "kernel fusion",
    "convolutional neural networks",
    "speech analysis"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "As speech-interfaces are getting richer and widespread, speech emotion recognition promises more attractive applications. In the continuous emotion recognition (CER) problem, tracking changes across affective states is an important and desired capability. Although CER studies widely use correlation metrics in evaluations, these metrics do not always capture all the high-intensity changes in the affective domain. In this paper, we define a novel affective burst detection problem to accurately capture high-intensity changes of the affective attributes. For this problem, we formulate a two-class classification approach to isolate affective burst regions over the affective state contour. The proposed classifier is a kernel-fusion dilated convolutional neural network (KFDCNN) architecture driven by speech spectral features to segment the affective attribute contour into idle and burst sections. Experimental evaluations are performed on the RECOLA and CreativeIT datasets. The proposed KFDCNN is observed to outperform baseline feedforward neural networks on both datasets.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Human emotional system is driven by biological stimuli from the external or internal stimuli and interacts with the cognition system in the brain  [1] . Since human behaviours are driven by the emotional system, automatic recognition of emotions has become an attractive and important research field in the last two decades. Speech Emotion Recognition (SER) has drawn significant attention in the recent literature as speech signal is highly correlated with the emotional states, lower-dimensional and occlusion free compared to visual modalities  [2, 3, 4, 5] . SER also keeps attracting attention with the surge of speech interfaces in the Internet-of-Things applications  [2] .\n\nDiscrete emotion recognition (DER) is a subclass of SER, which focuses on the categorical representation of emotions such as anger and happiness. In DER studies, audio signal is widely represented with low-level descriptors (LLDs), such as pitch, energy, zerocrossing rate and spectral features  [6, 7] . Classification task in DER has been addressed using LLD features by hidden Markov model based approaches as in  [6]  and also by recurrent neural networks (RNNs) based approaches as in  [7] . Alternatively in  [8] , raw speech is processed with multiple 1-dimensional convolutions to classify emotions.\n\nAlternatively, continuous emotion recognition (CER) represents emotions in a 3-dimensional continuous affect attribute space whose dimensions are Arousal, Valence and Dominance, respectively representing activeness -passiveness, positiveness -negativeness and dominance-submissiveness  [9] . CER studies use both tailored features as in  [10, 11]  or learned features as in  [12, 13, 14] . In  [10] , 23 LLDs from eGeMAPS  [15]  are extracted from the audio signal. Then, based on these features, a stacked long short-term memory (LSTM)-RNNs model is proposed for CER. In  [11] , audio signal is represented by combination of the Mel-frequency Cepstral Coefficients (MFCCs), delta and acceleration of MFCCs. Then using multi-task learning Arousal, Valence, and Dominance attributes are estimated in parallel. Trigeorgis et al. propose a convolutional recurrent neural network where two convolutional layers are acted as a learned feature extractor on the raw audio signal  [12] . Similarly, Tzirakis et al. adopt convolutional neural networks (CNNs) to produce audio embeddings for CER  [13] . In CER studies, correlation based metrics are widely used for evaluation tasks, since the trend of the predicted attribute is accepted to be more important than the actual level of the prediction. On the other hand, correlation metrics do not always grant effective capturing of all high-intensity changes in the affective domain.\n\nDue to the categorized nature of the DER, while inter-emotion transitions can be observed, intra-emotion transitions do not appear or are not typically available for the DER studies. On the other hand, continuous affect attributes in the CER problem provide the necessary intensity fluctuations for the inter-and intra-emotion transitions.\n\nThe detection of inter-and intra-emotion transitions, i.e., affective change detection, has a significant importance, and it is widely studied in the psychology domain under the mismatch negativity (MMN) literature  [16, 17, 18] . The information processing capability of human beings is limited, so automatic detection of relevant stimuli is crucial to orient attention  [19] . As the emotional expressions drive the communication for possible threats in the environment, even when attention is engaged in a concurrent task, emotional information is prioritized, and automatically processed  [20] . Hence, following changes in the affective domain can be critically important to design natural human-computer interaction applications.\n\nAffective change detection is studied in the DER context by  [21]  and CER context by  [22] . Affective change points are defined as the transition points between the emotions in  [21] . These points are estimated using a Gaussian mixture model based architecture with and without prior emotion class information. In  [22] , emotional hotspots are defined as sections deviated from the median of the affective attribute. They proposed a qualitative agreement-based assessment method to map affective attributes into low, high, neutral, and nonconsensus sections. A section is labeled as a low if that section under the median, and high if it is above the median. Then, bidirectional long short-term memory (BLSTM) is operated on 88 eGeMAPS  [15]  features, which are extracted from acoustic signals, to classify the trend. However, this approach highlights flat sections that are deviated from the median, and it resembles a quantization approach more than tracking the high-intensity regions as the labeling procedure misses all the inter-and some of the intra-emotion transition regions in the affective domain.\n\nIn this study, we address segmentation of the affect contour into affective burst and idle regions. Unlike  [22] , we label sections regarding the intensity of change. Affective burst sections then estimated using spectal features of speech as input and a kernel-fusion dilated convolutional neural network (KFDCNN) as the classier. To summarize, the main contributions of this study are as follows:\n\n• We propose a new labelling mechanism to define high intensity and idle segments of the affect contour.\n\n• We formulate a novel affective burst detection problem capturing the high intensity changes, which can lead to improve the understanding of the inter-and intra-emotion transitions in the scene.\n\n• We propose a novel architecture KFDCNN for affective burst detection from speech.\n\n• We carry out evaluations on the RECOLA and CreativeIT datasets with classification metrics.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Method",
      "text": "We propose a convoltional neural network (CNN) based architecture where the input is processed by parallel convolutions having different dilated kernel lengths to detect affective bursts. In this section first, affective burst detection problem is defined, then feature extraction from speech signals is described. Later, affective burst detection framework based on kernel fusion and CNNs are described.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Affective Bursts",
      "text": "Affective burst detection is a two-class classification problem where the affective contour is segmented into affective burst and idle classes. We define the affective burst segment (ABS) as the region in which the affect attribute contour is changing rapidly with high gradients. Respectively, idle segments (ISs) cover complement of the ABSs and correspond to the regions where the affect attribute contour is changing slowly with low gradients. The ground-truth ABS annotations are generated in two steps. First affective burst points (ABPs) on the affect attribute contours are detected, then these points are extended into segments which are referred as ABSs. Note that all non-ABS regions are referred as idle segments.\n\nAffective burst points (ABPs) are set based on the first order regression coefficients of the arousal and valence attributes as\n\nwhere de[n] is the delta coefficient of attribute e (Arousal or Valence) at sample index n. For simplicity, we will drop e. By selecting a threshold τ , we can define the ABP indicator function p at sample index n as\n\nThen the ground truth binary segment labels are extracted as\n\nwhere an ABS of temporal size ws = 2∆ + 1 is centered for each ABP on the indicator function p[n]. Note that the resulting size of ABSs can be longer than ws when two consecutive ABPs are closer than ws. Sample affect attribute contours for arousal and valence, indicated as ABS and IS in different colors, together with P [n] are depicted in Figure  1 .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Feature Extraction",
      "text": "In this study, we use the extended Geneva Minimalistic Acoustic Parameter Set (eGeMAPS) representing the spectral and temporal characterization of speech signal for ABS detection  [15] . The 88 dimensional eGeMAPS features are calculated as statistics of 25 low-level descriptors (LLDs), such as Mel-Frequency Cepstral Coefficients, Pitch and Loudness, using the OpenSMILE toolkit  [23] .\n\nThe LLDs are extracted using window size of 20 ms, and the 88 dimensional eGeMAPS features are calculated over 500 ms with a hop duration of 40 ms. Hence the eGeMAPS features are extracted at 25 fps and represented at time frame n as",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Kernel Fusion Convolutional Neural Network",
      "text": "The proposed KFDCNN architecture is depicted in Figure  2 . KFD-CNN is composed of both dilated parallel and typical convolutional layers, max-pooling layer, and multiple fully connected layers. Kernel fusion layer at KFDCNN includes dilated convolutional kernels having different lengths that helps to learn temporal relations in different resolutions and enriches representation for the ABS detection. Furthermore, with dilated kernels each layer has longer receptive fields, resulting in convolution outputs that capture long-term information which is especially important for slowly varying emotional processes.\n\nThe proposed KFDCNN architecture processes a window of features which is represented as\n\nwhere s is the dilation rate, and",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Model Training",
      "text": "The architecture is trained for ABS detection through a binary classification task. However, this task has an imbalanced nature. There are a small number of sections labeled as ABS, while a high number of sections are labeled as IS. To overcome the imbalance problem, we adopted weighted negative log-likelihood ratio loss:\n\nwhere θi is the weight for class i, P [n] ∈ {0, 1} is the binary segment label at time frame n, ŷ[n, P [n]] is the P [n]-th component of the KFDCNN output ŷ ∈ R B×2 at frame n, and B is the batch size.\n\nThe class weight θi is extracted as θi = 1 frequency of i th class  (5)  for class index i = 0, 1 corresponding to the IS and ABS.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Experimental Evaluations",
      "text": "The proposed architecture in Section 2 is evaluated on the RECOLA  [24]  and the CreativeIT  [25]  datasets. In this section, datasets and implementation details are introduced. Then evaluation metrics are described. Finally, performance of the KFDCNN is compared against a baseline feed-forward neural network (FFN) together with the CNN and the dilated CNN (DCNN).",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Datasets",
      "text": "We train and evaluate the ABS detection task on the widely used multi-modal datasets RECOLA and CreativeIT. RECOLA dataset is composed of multi-modal recordings of dyadic conversations of 27 French speakers. As a part of AVEC16 challenge, the dataset is divided into uniform-sized training, development, and test sets. While annotations for the training and the development sets are available, the annotations for the test set are not public. Publicly available annotations are for the arousal and valence attributes at 25 Hz rate. USC CreativeIT is a multimodal database of theatrical improvisations. Each interaction on average has a length of 3.5 minutes and is captured by recordings of the body Euler angles and speech from the participants. The dataset includes references for arousal, valence and dominance. It is divided into 5 sessions which are mutually exclusive in terms of speakers. The cross-validation procedure on this dataset is held by leave-one session out to preserve speaker independence.\n\nTable  1  presents statistical characterization of the groundtruth ABS annotations on the arousal and valence contours of the RECOLA and CreativeIT datasets. Total ABS region durations cover around 30% of the datasets with similar mean ABS durations of 3.6 seconds. Mean absolute delta (|d|) values for the ABS regions are observed higher for the RECOLA dataset that indicates higher affect contour changes for the RECOLA.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Implementation Details And Setup",
      "text": "Experimental evaluations of the proposed ABS detection system are executed using cross-validation. For the RECOLA dataset, 2 videos We set the length, L, of the first order regression coefficients to temporally capture 0.8 seconds, the ABS temporal window size, ws, is set to span 2 seconds. In (2), we set two threshold, τ , values, one for each dataset, so to cover 30% of the datasets as the ABS regions.\n\nThe input of the KFDCNN, I[n], is set with T = 100 and s = 5 which spans an 8 seconds temporal window with dilation rate 5. Kernel fusion layer at KFDCNN has 3 parallel 1-dimensional convolutions with kernels sizes 3, 5, and 7, and these kernels have a dilation rate of s = 5. The second 1-dimensional convolution has a kernel length of 3 with a dilation rate of 1. The max-pool layer down-samples the temporal dimension into half. The output of the max-pool layer is flattened from 2-dimension into 1-dimension and feed into fully connected layers with node sizes of 40, 20, and 2 respectively.\n\nSingle kernel and no dilation derivatives of the KFDCNN are also defined and evaluated to better assess the performance of the proposed model. The CNN architecture, which is depicted within the red dashed lines in Figure  2 , has a single kernel set with size of 3 and dilation rate of 1. In order to have comparable complexity with the KFDCNN, input feature of the CNN is set with T = 20 and s = 1 which spans 1.6 seconds temporal window without dilation. A dilated CNN (DCNN) architecture is also defined by setting the input feature representation with T = 100 and s = 5.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Affective Change Point Detection Performances",
      "text": "Unweighted average F-score (UAF1) and Recall (UAR) metrics are computed at frame level via cross-validation and used for the performance evaluations.\n\nTable  2  presents F1-score and Recall performances of the KFD-CNN against the baseline FFN, CNN and DCNN. CNN-based architectures outperform the baseline in all comparison metrics by at least 2% at RECOLA and CreativeIT databases. This result stresses the importance of temporal information for the detection of ABSs. Among the CNN based architectures, KFDCNN distinctly performs better than CNN and DCNN models. Performance improvement for the KFDCNN is highest for arousal and valence in the RECOLA dataset, and for valence in the CreativeIT dataset.\n\nComparing CNN with DCNN, use of dilation improves the Fscore performance by 5% for arousal and 3% for valence in the RECOLA dataset. Moreover, similar improvements are also seen with the CreativeIT dataset. Large temporal context due to dilated kernels is crucial to differentiate idle sections from ABS. This observation supports that consecutive temporal features carry less extrinsic information due to the slowly varying nature of emotional processes.\n\nComparing DCNN with KFDCNN, kernel fusion improves Fscore approximately by 3% for arousal and 2% for valence at the RECOLA database. Similarly at the CreativeIT database, improvements are 2% for valence. On the other hand, DCNN has only 0.2% better performance for arousal at the CreativeIT database. This consistent improvement indicates that learning relationships at different temporal resolutions improves ABS detection. KFDCNN has a superior performance on the RECOLA than CreativeIT, by approximately 10% for arousal and by 3% for valence. This could be due to the fact that RECOLA is not an acted dataset, and as a result it includes more spontaneous changes and exhibits higher mean absolute delta, |d|, values within ABSs compared to the CreativeIT.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Conclusion",
      "text": "In this study, we present the affective burst detection as an important affective computing problem that can introduce the capability of capturing affective fluctuations better in the inter-and intra-emotion domain. We address the affective burst detection as an imbalanced binary problem of segmenting affective contour into burst and idle regions.\n\nFirst, we label the affective contour by first detecting ABPs over the derivatives of the affective attributes. Later, the annotations are generated by extending the ABPs into segment vectors ABSs. The proposed KFDCNN architecture is trained with the generated annotation targets. From the conducted experiments, we observe that the KFDCNN outperforms the baseline architecture for F1-score and Recall on both RECOLA and CreativeIT datasets. Moreover, we depicted the importance of introduced concepts dilation and kernel fusion by comparing KFDCNN with CNN and DCNN. It is seen that larger receptive filed size due to dilation brings at least 3% improvements, and kernel fusion brings at least 2% improvements. Considering these observations, we suggest using KFDCNN for the affective burst detection.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Sample Arousal and Valence contours with affective burst",
      "page": 2
    },
    {
      "caption": "Figure 1: 2.2. Feature Extraction",
      "page": 2
    },
    {
      "caption": "Figure 2: Kernel fusion dilated convolutional neural network for affective burst detection",
      "page": 3
    },
    {
      "caption": "Figure 2: , has a single kernel set with size of 3",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table 2: F-score and Recall performance results of the baseline",
      "data": [
        {
          "# ABSs": "Mean ABS dur (sec)",
          "446\n461": "3.4\n3.8",
          "641\n632": "3.6\n3.6"
        },
        {
          "# ABSs": "Total ABS dur (sec)",
          "446\n461": "1510\n1744",
          "641\n632": "2308\n2296"
        },
        {
          "# ABSs": "Mean |d| of ABSs",
          "446\n461": "0.0030\n0.0017",
          "641\n632": "0.0014\n0.0009"
        },
        {
          "# ABSs": "Total dur (sec)",
          "446\n461": "5400\n5400",
          "641\n632": "7708\n7708"
        }
      ],
      "page": 4
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Cognitive-emotional interactions in the brain",
      "authors": [
        "Joseph Ledoux"
      ],
      "year": "1989",
      "venue": "Cognition and Emotion"
    },
    {
      "citation_id": "3",
      "title": "Features and classifiers for emotion recognition from speech: a survey from 2000 to 2011",
      "authors": [
        "Christos-Nikolaos Anagnostopoulos",
        "Theodoros Iliou",
        "Giannoukos"
      ],
      "year": "2015",
      "venue": "Artif Intell Rev"
    },
    {
      "citation_id": "4",
      "title": "Speech Emotion Recognition Using Deep Learning Techniques: A Review",
      "authors": [
        "Edward Ruhul Amin Khalil",
        "Mohammad Jones",
        "Tariqullah Inayatullah Babar",
        "Mohammad Jan",
        "Thamer Haseeb Zafar",
        "Alhussain"
      ],
      "year": "2019",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "5",
      "title": "Multi-cue fusion for emotion recognition in the wild",
      "authors": [
        "Jingwei Yan",
        "Wenming Zheng",
        "Zhen Cui",
        "Chuangao Tang",
        "Tong Zhang",
        "Yuan Zong"
      ],
      "year": "2018",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "6",
      "title": "Facial expression analysis under partial occlusion: A survey",
      "authors": [
        "Ligang Zhang",
        "Brijesh Verma",
        "Dian Tjondronegoro",
        "Vinod Chandran"
      ],
      "year": "2018",
      "venue": "ACM Computing Surveys"
    },
    {
      "citation_id": "7",
      "title": "Hidden Markov model-based speech emotion recognition",
      "authors": [
        "Björn Schuller",
        "Gerhard Rigoll",
        "Manfred Lang"
      ],
      "year": "2003",
      "venue": "ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing -Proceedings"
    },
    {
      "citation_id": "8",
      "title": "Automatic speech emotion recognition using recurrent neural networks with local attention",
      "authors": [
        "Seyedmahdad Mirsamadi",
        "Emad Barsoum",
        "Cha Zhang"
      ],
      "year": "2017",
      "venue": "ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing -Proceedings"
    },
    {
      "citation_id": "9",
      "title": "End-to-end Triplet Loss based Emotion Embedding System for Speech Emotion Recognition",
      "authors": [
        "Puneet Kumar",
        "Sidharth Jain",
        "Balasubramanian Raman",
        "Partha Roy",
        "Masakazu Iwamura"
      ],
      "year": "2020",
      "venue": "The 25th International Conference on Pattern Recognition"
    },
    {
      "citation_id": "10",
      "title": "Three dimensions of emotion",
      "authors": [
        "Harold Schlosberg"
      ],
      "year": "1954",
      "venue": "Psychological Review"
    },
    {
      "citation_id": "11",
      "title": "Continuous Emotion Recognition in Speech -Do We Need Recurrence?",
      "authors": [
        "Maximilian Schmitt",
        "Nicholas Cummins",
        "Björn Schuller"
      ],
      "year": "2019",
      "venue": "Interspeech 2019, ISCA"
    },
    {
      "citation_id": "12",
      "title": "Multimodal Continuous Emotion Recognition using Deep Multi-Task Learning with Correlation Loss",
      "authors": [
        "Berkay Köprü",
        "Engin Erzin"
      ],
      "year": "2020",
      "venue": "Multimodal Continuous Emotion Recognition using Deep Multi-Task Learning with Correlation Loss"
    },
    {
      "citation_id": "13",
      "title": "Adieu features? End-to-end speech emotion recognition using a deep convolutional recurrent network",
      "authors": [
        "George Trigeorgis",
        "Fabien Ringeval",
        "Raymond Brueckner",
        "Erik Marchi",
        "Mihalis Nicolaou",
        "Bjorn Schuller",
        "Stefanos Zafeiriou"
      ],
      "year": "2016",
      "venue": "IEEE International Conference on Acoustics"
    },
    {
      "citation_id": "14",
      "title": "End-to-end multimodal affect recognition in realworld environments",
      "authors": [
        "Panagiotis Tzirakis",
        "Jiaxin Chen",
        "Stefanos Zafeiriou",
        "Björn Schuller"
      ],
      "year": "2021",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "15",
      "title": "End-to-end speech emotion recognition using deep neural networks",
      "authors": [
        "Panagiotis Tzirakis",
        "Jiehao Zhang",
        "Bjorn Schuller"
      ],
      "year": "2018",
      "venue": "ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing -Proceedings"
    },
    {
      "citation_id": "16",
      "title": "The Geneva minimalistic acoustic parameter set (GeMAPS) for voice research and affective computing",
      "authors": [
        "Florian Eyben",
        "Klaus Scherer",
        "Björn Schuller",
        "Johan Sundberg",
        "Elisabeth André",
        "Carlos Busso",
        "Laurence Devillers",
        "Julien Epps",
        "Petri Laukka",
        "S Shrikanth",
        "Khiet Narayanan",
        "Truong"
      ],
      "year": "2016",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "17",
      "title": "The quest for the genuine visual mismatch negativity (vMMN): Eventrelated potential indications of deviance detection for low-level visual features",
      "authors": [
        "G Alie",
        "Robert Male",
        "Erich O'shea",
        "Dagmar Schröger",
        "Urte Müller",
        "Andreas Roeber",
        "Widmann"
      ],
      "year": "2020",
      "venue": "Psychophysiology"
    },
    {
      "citation_id": "18",
      "title": "Facial Expression Related vMMN: Disentangling Emotional from Neutral Change Detection",
      "authors": [
        "Klara Kovarski",
        "Marianne Latinus",
        "Judith Charpentier",
        "Helen Cléry",
        "Sylvie Roux",
        "Emmanuelle Houy-Durand",
        "Agathe Saby",
        "Frédérique Bonnet-Brilhault",
        "Magali Batty",
        "Marie Gomot"
      ],
      "year": "2017",
      "venue": "Frontiers in Human Neuroscience"
    },
    {
      "citation_id": "19",
      "title": "Visual mismatch negativity and stimulus-specific adaptation: the role of stimulus complexity",
      "authors": [
        "Petia Kojouharova",
        "Domonkos File",
        "István Sulykos",
        "István Czigler"
      ],
      "year": "2019",
      "venue": "Experimental Brain Research"
    },
    {
      "citation_id": "20",
      "title": "Control of goaldirected and stimulus-driven attention in the brain",
      "authors": [
        "Maurizio Corbetta",
        "Gordon Shulman"
      ],
      "year": "2002",
      "venue": "Nature Reviews Neuroscience"
    },
    {
      "citation_id": "21",
      "title": "N170 sensitivity to facial expression: A meta-analysis",
      "authors": [
        "J Hinojosa",
        "F Mercado",
        "L Carretié"
      ],
      "year": "2015",
      "venue": "Neuroscience & Biobehavioral Reviews"
    },
    {
      "citation_id": "22",
      "title": "An Investigation of Emotion Change Detection from Speech",
      "authors": [
        "Zhaocheng Huang",
        "Julien Epps",
        "Eliathamby Ambikairajah"
      ],
      "year": "2015",
      "venue": "Sixteenth Annual Conference of the International Speech Communication Association"
    },
    {
      "citation_id": "23",
      "title": "Predicting Emotionally Salient Regions using Qualitative Agreement of Deep Neural Network Regressors",
      "authors": [
        "Srinivas Parthasarathy",
        "Carlos Busso"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "24",
      "title": "Opensmile: The munich versatile and fast open-source audio feature extractor",
      "authors": [
        "Florian Eyben",
        "Martin Wöllmer",
        "Björn Schuller"
      ],
      "year": "2010",
      "venue": "Proceedings of the 18th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "25",
      "title": "Introducing the RECOLA multimodal corpus of remote collaborative and affective interactions",
      "authors": [
        "Fabien Ringeval",
        "Andreas Sonderegger",
        "Juergen Sauer",
        "Denis Lalanne"
      ],
      "year": "2013",
      "venue": "2013 10th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition"
    },
    {
      "citation_id": "26",
      "title": "The USC CreativeIT database of multimodal dyadic interactions: from speech and full body motion capture to continuous emotional annotations",
      "authors": [
        "Angeliki Metallinou",
        "Zhaojun Yang",
        "Chi-Chun Lee",
        "Carlos Busso",
        "Sharon Carnicke",
        "Shrikanth Narayanan"
      ],
      "year": "2016",
      "venue": "Language Resources and Evaluation"
    }
  ]
}