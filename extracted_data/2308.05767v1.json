{
  "paper_id": "2308.05767v1",
  "title": "Eeg-Based Emotion Style Transfer Network For Cross-Dataset Emotion Recognition",
  "published": "2023-08-09T16:54:40Z",
  "authors": [
    "Yijin Zhou",
    "Fu Li",
    "Yang Li",
    "Youshuo Ji",
    "Lijian Zhang",
    "Yuanfang Chen",
    "Wenming Zheng",
    "Guangming Shi"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "As the key to realizing aBCIs, EEG emotion recognition has been widely studied by many researchers. Previous methods have performed well for intra-subject EEG emotion recognition. However, the style mismatch between source domain (training data) and target domain (test data) EEG samples caused by huge inter-domain differences is still a critical problem for EEG emotion recognition. To solve the problem of cross-dataset EEG emotion recognition, in this paper, we propose an EEG-based Emotion Style Transfer Network (E 2 STN) to obtain EEG representations that contain the content information of source domain and the style information of target domain, which is called stylized emotional EEG representations. The representations are helpful for crossdataset discriminative prediction. Concretely, E 2 STN consists of three modules, i.e., transfer module, transfer evaluation module, and discriminative prediction module. The transfer module encodes the domain-specific information of source and target domains and then re-constructs the source domain's emotional pattern and the target domain's statistical characteristics into the new stylized EEG representations. In this process, the transfer evaluation module is adopted to constrain the generated representations that can more precisely fuse two kinds of complementary information from source and target domains and avoid distorting. Finally, the generated stylized EEG representations are fed into the discriminative prediction module for final classification. Extensive experiments show that the E 2 STN can achieve the state-ofthe-art performance on cross-dataset EEG emotion recognition tasks.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Introduction",
      "text": "As a new way of human-computer interaction in the 21st century, brain-computer interface (BCI) technology provides a new means of communication for us under the hot metaverse background  (Guo and Gao 2022) . Emotion plays a very significant role in human-computer interaction, which has led to the great attention of affective Brain-Computer Interfaces (aBCIs) in the interdisciplinary fields  (Fiorini et al. 2020; Katsigiannis and Ramzan 2018) . Traditional aB-CIs mainly rely on two mediums, behavioral signals and physiological signals, for emotion recognition  (He et al. 2020) . Compared with behavioral signals such facial expressions, speech, and text, it is more reliable to distinguish the spontaneous emotion state through physiological signals such as electrocardiogram (ECG), electrooculogram (EOG), electromyogram (EMG), and electroencephalogram (EEG)  (Zheng, Zhu, and Lu 2017; Song et al. 2021 ). Among them, EEG signals generated in the cerebral cortex are most associated with spontaneous emotional states  (He et al. 2020; Shu and Wang 2017) . With the development of wearable non-invasive EEG acquisition equipment in recent years, more and more researches have focused on the field of EEG-based emotion recognition. The previous EEG emotion recognition mainly focuses on intra-subject tasks  (Xiao et al. 2022) . For instance, Zheng et al. trained deep belief networks (DBNs) with differential entropy (DE) features of SEED dataset and achieved advanced results  (Zheng and Lu 2015) .  Song et al. proposed  the attention-long short-term memory (A-LSTM) to extract more discriminative features from multiple time-frequency domain features  (Song et al. 2019) .  Peng et al.  proposed a self-weighted semi-supervised classification (SWSC) model that utilizes a self-weighted variable to adaptively and quantitatively learn the importance of EEG features for cross-session emotion pattern recognition  (Peng et al. 2021) .  Zhong et al.  proposed a regularized graph neural network (RGNN) that considers the biological topology among different brain regions to capture the local and global relationships between different EEG channels, which achieves advanced performance in cross-subject experiments on SEED and SEED-IV datasets  (Zhong, Wang, and Miao 2020) . Although various EEG emotion recognition methods have been proposed in the past several years, there are still some major issues that should be well investigated to further promote EEG emotion recognition. The first issue is the protocol for EEG emotion recognition. The existing protocols for EEG emotion recognition mostly are intra-subject and crosssubject EEG classification, in which the training and test EEG data come from the same experimental environment. How the performance varies between different experimental environments should be further studied, e.g., the crossdataset EEG emotion recognition. To clearly and intuitively show the differences in the distribution of EEG data, we use the T-SNE technology to visualize the EEG data of different subjects in different datasets, as shown in Fig 1 . Notably, there are significant differences in the distribution of EEG data among different subjects of the same dataset, and further, which are more prominent among diverse datasets.\n\nThe second issue is how to reduce the domain differences. Recently, several studies have attempted to address domain shifts in cross-subject EEG emotion recognition tasks. For example, BiDANN  (Li et al. 2018)  and TANN  (Li et al. 2021)  proposed by  Li et al.  have achieved advanced performance considering the distribution differences between training data and test data in the cross-subject EEG emotion recognition task. However, the inter-domain differences in cross-dataset EEG emotion recognition go well beyond that in the cross-subject EEG emotion recognition task, as shown in Fig.  1 . Narrowing the differences between domains will further improve cross-dataset EEG emotion recognition and the generalization to new EEG emotional data. Therefore, overcoming the significant distribution divergence of EEG signals among different datasets is particularly challenging and promising for cross-dataset EEG emotion recognition.\n\nTo address the aforementioned issues, in this study, we propose an EEG-based Emotion Style Transfer Network (E 2 STN) to obtain stylized emotional EEG representations containing emotion content of the source domain and statistical characteristics of the target domain, and meanwhile, make discriminative predictions for cross-dataset emotional EEG samples. E 2 STN consists of three unique modules, namely the transfer module, the discriminative prediction module, and the transfer evaluation module. To realize the transfer of emotional EEG samples from the source domain to the target domain, we propose a transfer module to reorganize the emotional pattern information of the source domain and the statistical properties of the target domain into new stylized EEG representations. The first layer of the transfer module is two encoders corresponding to the source and target domains to extract their domain-specific information, namely the emotional content information of the source domain and the style statistics characteristics of the target domain. The decoder in the transfer module re-combines emotional content and style characteristics to obtain stylized (transferred) emotional EEG representations. To make the stylized EEG representations contain more accurate emotional content information of the source domain and the statistical characteristic style of the target domain, we design a transfer evaluation module including contentaware loss, style-aware loss, and identity loss to constrain the process of transferring emotional EEG samples from the source domain to the target domain. The content-aware loss and style-aware loss in the transfer evaluation module ensure that the stylized representations can more precisely fuse two kinds of complementary information from source and target domains, respectively. A unique identity loss ensures that the transfer module is unbiased, i.e., the stylized representations remain unchanged when the source and target domains are the same samples. These losses are constructed from features extracted by the multi-layer convolution operations, which is inspired by  (Gatys, Ecker, and Bethge 2016) . The multi-layer convolutions explore the latent relationship between critical frequency bands and extract spatiotemporal information from the stylized representations, respectively. Finally, to extract deep features from both original (source domain) and stylized (transferred) EEG samples for discriminative predictions, we propose a discriminative prediction module that includes a dynamic graph convolutional network and two fully connected (FC) layers. The dynamic graph convolution network extracts spatial features from two types of samples, which are further fed into FC layers to generate discriminative class labels. A cross-entropy loss with the above transfer evaluation losses jointly optimizes the entire model to realize cross-dataset EEG emotion recognition from a global perspective.\n\nMain contributions of this paper are summarized as threefold:\n\n• To our best knowledge, we propose an EEG-based Emotion Style Transfer Network (E 2 STN) for the first time to obtain stylized emotional EEG representations for crossdataset EEG emotion recognition. The model reorganizes the emotional content information of the source domain and the statistical characteristic style of the target domain into new stylized EEG representations, thereby further performing discriminative prediction of cross-dataset emotional EEG samples.\n\n• The proposed E 2 STN is implemented under the joint optimization of the cross-entropy loss and transfer evaluation losses. The transfer evaluation losses constrain the stylized EEG representations that can more precisely fuse two kinds of complementary information from source and target domains and avoid distorting, and meanwhile, the crossentropy loss guides the discriminative prediction for crossdataset EEG emotion recognition.\n\n• Extensive experiments show that the proposed E 2 STN can achieve the state-of-the-art performance on cross-dataset EEG emotion recognition tasks.\n\nThe rest of this paper is organized as follows. Section II summarizes a brief overview of related studies on EEG emotion recognition. Section III specifies the proposed E 2 STN model in detail. Section IV discusses the results of the extensive experiments conducted. Finally, this paper is concluded in Section V.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work Deep Learning In Eeg Emotion Recognition",
      "text": "With the rapid development of deep learning, numerous studies have attempted to solve EEG emotion recognition using deep learning methods. For instance, Song et al. proposed a dynamical graph convolutional neural network (DGCNN) to recognize emotional EEG signals, which can dynamically learn the information transfer relationship between the nodes in a graph and achieve good results on SEED and DREAMER datasets  (Song et al. 2020b) . Considering the abundant spatial information in EEG signals, Song et al. proposed to convert multi-channel EEG signals into images for EEG emotion recognition, which converts the question of EEG-based emotion recognition into image recognition. To this end, they proposed a novel EEG-to-image method and a novel graph-embedded convolutional neural network (GECNN) method. Extensive experiments on four datasets have proved the effectiveness of GECNN  (Song et al. 2022) . Meanwhile, Song et al. presented a novel attention-long short-term memory (A-LSTM) to extract more discriminative features from EEG sequences. A-LSTM shows advanced performance on the MPED dataset proposed in the same paper  (Song et al. 2019 ). In addition, Li et al. proposed a graph-based multitask self-supervised learning model (GMSS), which integrates multiple self-supervised tasks to learn more general representations for EEG emotion recognition. The experimental results of GMSS on SEED, SEED-IV, and MPED datasets show its advanced performance in learning more discriminative and available features for EEG emotional signals  (Li et al. 2022 ). Zheng et al. introduced deep belief networks (DBNs) to investigate the critical frequency bands and channels of EEG signals. The experiment results show that the 4th, 6th, 9th, and 12th channels and the Gamma band are more important in emotion recognition  (Zheng and Lu 2015) . Although the above methods have achieved advanced performance in EEG emotion recognition tasks, due to the massive differences in the distribution of training and test data in practical applications, these methods are disabled to perform well.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Transfer Learning In Eeg Emotion Recognition",
      "text": "Considering that the EEG signals in cross-subject experiments have a considerable domain shift of data distribution, numerous transfer learning methods have been widely used in cross-subject EEG emotion recognition to overcome the significant distribution divergence of EEG signals among individuals. For example, Li et al. proposed a multisource transfer learning method, which regards the existing subjects as sources and the new subject as a target to realize style transfer mapping  (Li et al. 2020 ). Sun et al. proposed a dual-branch dynamic graph convolution-based adaptive transformer feature fusion network with adapter-finetuned transfer learning (DBGC-ATFFNet-AFTL) for EEG emotion recognition. They utilized the transfer learning method to integrate the different domain features and achieved promising performance in cross-subject emotion recognition on three datasets  (Sun et al. 2022 ). Peng et al. proposed a joint feature adaptation and graph adaptive label propagation model (JAGP) for cross-subject emotion recognition. JAGP successfully implemented the inter-domain migration by extracting and integrating the domain-invariant features  (Peng et al. 2022 ). Li et al. proposed a novel bi-hemispheres domain adversarial neural network (BiDANN) for EEG emotion recognition. Inspired by the asymmetry of the left and right hemispheres in the emotional functional regions of the brain, BiDANN maps the EEG signals of the left and right hemispheres to the discriminative feature spaces, so that the model can easily classify the data representations on SEED database.  (Li et al. 2018) . Meanwhile, Li et al. also proposed a transferable attention neural network (TANN) for EEG emotion recognition. TANN is based on the local and global attention mechanism to learn the discriminative information from emotional EEG signals. Extensive experiments on SEED, SEED-IV, and MPED datasets demonstrate the superior performance of TANN  (Li et al. 2021 ). However, the performance of these existing methods decreases significantly when dealing with cross-data EEG emotion recognition tasks. We will compare the representative methods among them with the proposed method in Section .",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Proposed Method For Emotion Recognition",
      "text": "To specify the proposed method clearly, we depict the E 2 STN framework in Fig.  2 . The network aims to reorganize the emotional content information of the source domain and the statistical characteristic style of the target domain to obtain new stylized source domain EEG samples, and finally realize the cross-dataset EEG emotion recognition task. We adopt three modules to achieve this goal, i.e., transfer module, discriminative prediction module, and transfer evaluation module. The transfer module is to obtain emotional EEG representations that contain affective content information of the source domain and statistical characteristics style of the target domain. Subsequently, the original and stylized source samples are fed into the discriminative prediction module for cross-dataset EEG emotion recognition. Meanwhile, the transfer evaluation module extracts multi-scale spatio-temporal features of the stylized EEG samples to construct the multi-dimensional losses constraining the process of emotional EEG style transfer. In the following, we introduce the details of the proposed E 2 STN model.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Obtaining Stylized Emotional Eeg Samples",
      "text": "To obtain emotional EEG samples that contain dynamic content information of the source domain and statistical characteristics style of the target domain, we should decompose the transfer into two steps. The first step is to construct transfer encoders corresponding to the source and target domains to capture the global dependencies in the domain-specific information of different fields (i.e., the emotional content of the source domain and the style characteristics of the target domain), respectively. Inspired by the Transformer method  (Vaswani et al. 2017 ), the encoders of E 2 STN assign dynamic weights to different EEG channels through the multi-head self-attention layer, which selects the more important electrode dependencies in the specific domain.\n\nThe dynamic dependencies with domain-specific information have stronger capabilities in representing their corresponding domain characteristics. The second key step of transfer is to fuse source domain content information and target domain style information in the decoder to obtain stylized source emotional EEG features. The decoder iteratively fuses content and style information by the multi-layer structure, which applies the target domain style to the source domain EEG features. And the residual connection method in the decoder layer ensures that the content information of the source domain will not be distorted. Specifically, the raw EEG signals are first decomposed into five frequency bands, namely δ band (1-4 Hz), θ band (4-8 Hz), α band (8-14 Hz), β band (14-30 Hz) and γ band (30-50 Hz). The emotional EEG samples corresponding to the source and target domains, respectively, are denoted as X s ∈ R C×B and X t ∈ R C×B , where C is the number of EEG channels and B is the number of frequency bands. Subsequently, two encoders extract the domain-specific information from their corresponding source and target domain EEG features, respectively. The encoder layer structure is depicted in Fig.  3 (a) . The emotional EEG samples of the source domain X s are first encoded into query (Q), key (K), and value (V) vectors, as shown in formula 1.\n\nB×m are trainable linear projection matrices. To enable the encoder to pay attention to the information from different channels, Q E s , K E s , and V E s vectors are divided into several attention heads, that\n\n, where h = m p is the number of attention heads. Then the multi-head self-attention (MSA) can be calculated by:\n\nMulti-head Attention\n\nTo preserve domain-specific information, the MSA matrix is added with the Q vector and followed by layer normalization, which can be expressed as:\n\nwhere W 1 , W 2 are trainable weight matrices, and b 1 , b 2 are trainable bias matrices. Similarly, we can easily obtain the domain-specific features of the target domain H E t .\n\nTo fuse the emotional content information of the source domain and the statistical characteristic style of the target domain, we construct a three-layer Transformer decoder, which applies the style of the target domain to the emotional features of the source domain in a progressive manner. The structure of single decoder layer is shown in Fig.  3 (b) . The source domain features H E s containing the emotional content information of the source domain are the main objects of transfer, which are used as query vectors for the first decoder layer. To make the source features more similar to the taget domain style, the target domain features H E t are used as key and value vectors of the first decoder layer, which calculates a similarity matrix with the query vectors to weight the emotional content features H E s . Specifically, we obtain\n\n, and V D 1 through the linear projection, as shown in formula (6).\n\n(6) Following, two MSA layers and one FFN are employed in the first decoder layer with residual connections. The output of the first decoder layer continues to be fed to the second decoder layer, and so on. Therefore, we can easily derive the output H D ∈ R C×m of the Transformer decoder through formulas (3), (4), and (5).\n\nTo restore the dimension of the stylized features, we employ a two-layer CNN decoder to refine the output of the transformer decoder H D . Therefore, we can reshape the stylized EEG features H D ∈ R C×m as generated stylized EEG samples Xs ∈ R C×B .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Obtaining Discriminative Features And Predictions",
      "text": "After obtaining the stylized source-domain EEG samples, we construct a dynamic graph network to extract deep features, which enables E 2 STN to learn discriminative features from the original and stylized source samples. The source EEG samples X s and the corresponding stylized EEG samples Xs are jointly fed into the discriminative prediction module for obtaining discriminative features and predictions. Concretely, the data-driven graph is represented as G = (V, E) according to the standard symbol representation in graph theory, where V = {v i } n i=1 represents the set of n electrodes of the EEG samples, and (v i , v j ) ∈ E denotes the connection weight between electrodes v i and v j in the EEG samples. The relationship between the electrodes is characterized as an adjacency matrix G ∈ R n×n in the E 2 STN. The graph connection G changes adaptively with the input samples X s and Xs . To make G contain the intrachannel spatial information and frequency band information, two trainable matrices W s ∈ R C×C and W f ∈ R B×(C * B) are multiplied left and right by input features, respectively, which can be expressed as follows:\n\nwhere Xs = [X s , Xs ] = {x 1 , . . . , x t , . . . }, ReLU is applied to the output to guarantee non-negative elements, the bias matrix B ∈ R C×B is used to increase the flexibility of graph structure representation. Then, the adjacency matrix G is reshaped into B adjacency matrices, i.e., G = [G * 1 , . . . , G * B ] ∈ R C×C×B , to represent graphs in B frequency bands.\n\nTo avoid the high computational complexity of direct graph Fourier transform based on graph filtering theories, we adopt Chebyshev polynomials to approximate the graph convolution operation  (Kipf and Welling 2017) . Let φ k (G) = G k denotes the k-order polynomial of the adjacency matrix G. Therefore, the high-level features extracted by the dynamic GCN can be expressed as follows:\n\nwhere φ k (G) is the k-th level graph, F is the output dimension for the graph convolution operation. Subsequently, the discriminative prediction module, as the supervision term of the E 2 STN, applies two fully connected (FC) layers to predict the class labels. ReLU activation function is adopted to the first FC layer, and the second FC layer with sof tmax activation function is used to calculate the classification probability. Therefore, the output of the second FC layer H FC = {o 1 , . . . , o p } ∈ R 1×P can be easily deduced, where P is the output dimension of the FC layer. Then, the discriminative predictions from the softmax layer for emotion recognition can be expressed as follows:\n\nwhere Y (p|x t ) denotes the predicted probability that the input sample x t belongs to the pth class in the discriminative prediction module. Consequently, labels l t of sample x t are predicted as follows:\n\nConsequently, the cross-entropy loss function of E 2 STN to achieve cross-dataset EEG emotion recognition can be expressed as:\n\nHere, l g represents the ground-truth label of sample x t ; M 1 is the number of training samples.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Multi-Objective Joint Optimization",
      "text": "To optimize stylized emotional EEG samples, we specially propose a transfer evaluation module to constrain the style transfer process. We mainly consider three factors in the emotional style transfer process, and thus construct three corresponding losses, namely content-aware loss L c , styleaware loss L s , and identity loss L id . The first important point to consider is to preserve the emotional content information of the source domain during the transfer process.\n\nWe regard the features extracted by the convolutional layer as containing the content information of the corresponding domain  (Gatys, Ecker, and Bethge 2016) . Therefore, the content-aware loss is built from the features extracted by three unique convolutional layers in the transfer evaluation module, which can be expressed as:\n\nwhere f i (•) denotes the convolution operation function of the i-th layer in the transfer evaluation module, and ∥•∥ 2 represents the ℓ 2 -norm.\n\nThe second point that cannot be ignored in the transfer process is that the style characteristics of the stylized emotional EEG samples should be as similar as possible to those of the target domain. The Gram matrix of the features extracted by the convolutional layer is regarded as the statistical characteristics of the target domain  (Gatys, Ecker, and Bethge 2016) . Therefore, We can also construct the styleaware loss for the target domain by the statistics (e.g., mean and variance) of each convolutional layer in the transfer evaluation module.\n\nwhere µ(•) and σ(•) denote the mean and variance of the features, respectively. In the last point, considering maintaining more accurate content and style information in self-style transfer, we propose an identity loss to ensure the undistorted stylized EEG samples during the progressive transfer process. Specifically, to ensure the lossless and unbiased transfer of emotional EEG samples, we input the same sample X s (X t ) into the source and target domain, and the obtained stylized emotional EEG sample Xss ( Xtt ) should be identical to the X s (X t ). Hence, the identity loss L id can be defined as:\n\nTo make the features extracted by the multi-layer convolutional layers in the transfer evaluation module contain multidimensional and multi-scale spatio-temporal information, we employ three distinct convolution kernels to construct the convolutional network. The first 2D convolutional layer explores the latent relationship between key bands of stylized EEG features, and H c ∈ R C×B×F1 denotes the overall output feature of the 2D convolution operation, where F 1 is the number of convolutional filters. Subsequently, the depthwise convolution is used to learn the spatial information of stylized EEG features. The depth feature H dc ∈ R 1×B×(F1 * D) can be simply derived, where D is a depth parameter that controls the number of spatial filters in the convolution operation. In the last layer, separable convolution is the extension of depthwise convolution. On the basis of depthwise convolution, F 2 pointwise convolutions are performed to optimally merge the spatial features. Therefore, the feature H dc is further compressed in the channel dimension, H sc ∈ R 1×B×F2 . H c , H dc , and H sc correspond to the features extracted by each convolutional layers in formulas (  13 ), (  14 ), and (15), respectively.\n\nFinally, the transfer losses in the transfer evaluation module and the cross-entropy loss in the discriminative prediction module together form a multi-objective joint optimization loss function L.\n\nwhere λ, µ, ν, ξ are hyper-parameters used to control the proportion between the optimization loss functions. The E 2  STN is optimized by iteratively minimizing L, and the emphasis on transferring tasks and classification tasks is achieved by adjusting the hyperparameters. The procedure used to train the E 2 STN is presented in Algorithm 1 of Appendix A.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Experiments Experimental Settings",
      "text": "Datasets SEED  (Zheng and Lu 2015)  is a commonly used emotion EEG dataset published by SJTU, which contains 15 subjects (7 males and 8 females) in total. Each subject in SEED was asked to attend 3 different sessions. During each session, 3 different kinds of emotional film clips (i.e., positive, neutral and negative emotions) were played in proper order. There are 5 film clips for each kind of emotion, that is, a total of 15 trials (approximately 3400 samples) are included in one session. In each session, The EEG signals and eye movements were collected with the 62-channel ESI NeuroScan System 1  and SMI eye-tracking glasses 2 . The locations of the EEG electrodes are based on the international 10-20 system. The EEG data was downsampled to 200 Hz and divided into 1-s segments. The differential entropy (DE) features extracted from the downsampled EEG signals with 1-second sliding window were used as training samples. SEED-IV  (Zheng et al. 2018 ) contains 15 subjects in total. Similarly, each subject was required to perform three different sessions, but 4 different kinds of emotion were triggered in each session (i.e. happy, sad, fear, and neutral). There are 6 film clips for each kind of emotion, that is, a total of 24 trials are included in one session. The dataset contains EEG and eye movement data, among which the EEG data are collected using a 62-channel ESI neuroscan system. The locations of the EEG electrodes are based on the international 10-20 system. Each session was divided into 1-s segments as a training sample. The DE features at five frequency bands were extracted from all training samples.\n\nThe MPED  (Song et al. 2019 ) collects four modal physiological signals: EEG, galvanic skin response, respiration, and electrocardiogram (ECG). 30 subjects are requested to watch 28 videos, which are divided into 7 categories: joy, funny, anger, fear, disgust, sadness, and neutral emotion. Herein, only the EEG data were used for emotion recognition. To remove the noise and artifacts, a 5-order Butterworth filter was used to filter EEG data at 1-100 Hz. The processed data were decomposed into five frequency bands with a slide non-overlapping window of 1-s, and its 256point STFT features were extracted at the same time.\n\nExperiment protocol The purpose of this paper is to study the cross-dataset EEG emotion recognition tasks. Therefore, following the principles of previous experiments, we set up groups of cross-dataset EEG emotion recognition comparison experiments. Table  1  summarizes the setup details of the cross-dataset EEG emotion recognition experiments. The SEED dataset, which has the fewest categories of emotions of the three datasets we adopted, contains three categories: positive, negative, and neutral. consequently, to ensure the category balance of the training samples, we select the neutral, sad, and happy emotions as the emotional EEG transfer objects for SEED-IV dataset, and neutral, sad, and joy emotions for MPED dataset. The samples of all subjects from one dataset are regarded as source domain data, and the ones in another dataset are used as target domain data. In this way, we can obtain six groups for 3-category cross-dataset EEG emotion recognition experiments, and two groups for 4-category cross-dataset EEG emotion recognition experiments. The eight groups of cross-dataset EEG emotion recognition experiments are denoted as 'MPED 3  → SEED 3 ', 'MPED 3 → SEED-IV 3 ', 'SEED-IV 3 → MPED 3 ', 'SEED-IV 3 → SEED 3 ', 'SEED 3 → MPED 3 ', 'SEED 3 → SEED-IV 3 ', 'MPED 4 → SEED-IV 4 ', and 'SEED-IV 4 → MPED 4 ', respectively.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Experiment Results",
      "text": "",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "3-Category Cross-Dataset Eeg Emotion Recognition",
      "text": "To evaluate the performance of our model in cross-dataset EEG emotion recognition, we conduct extensive experiments using three datasets, namely SEED, SEED-IV, and MPED. To compare the proposed E 2 STN with the other advanced methods of EEG emotion recognition, we also conduct the same experiments using six other methods: linear SVM (Suykens and Vandewalle 1999), A-LSTM  (Song et al. 2019) , IAG  (Song et al. 2020a) , GECNN  (Song et al. 2022) , BiDANN  (Li et al. 2018) , and TANN  (Li et al. 2021) . We quote or reproduce their results from the literature to ensure a convincing comparison with the proposed method. The mean accuracy (ACC) and standard deviation (STD) are used as the evaluation criteria for all subjects in the test dataset. The experiment results are presented in Table  2 . The best result for each row in the Table is highlighted in boldface.\n\nTable  2  shows that the proposed E 2 STN model achieves the best performance in cross-dataset EEG emotion recognition experiments, thus verifying the transfer and recognition effectiveness of the proposed method. Specifically, E 2 STN performs best on the 'MPED 3 → SEED 3 ' task in the the 3-category cross-dataset EEG emotion recognition experiments, with an accuracy rate of 73.51%, which is significantly higher than that of the compared advanced algorithms. Compared with the state-of-the-art domain adaptation method TANN  (Li et al. 2021 ), E 2 STN improves the accuracy by 09.28% (73.51% vs 64.23%) on the 3category classification of the 'MPED 3 → SEED 3 ' task. For the 'SEED-IV 3 → SEED 3 ' experiment, E 2 STN achieves the highest classification accuracy with the smallest standard deviation, which proves that the proposed method has the best performance and stability. In 'MPED 3 → SEED-IV 3 ' and 'SEED 3 → SEED-IV 3 ' experiments (i.e., the 4th and 5th columns of Table  2 ), the recognition performance of the proposed method has declined, which may be due to the reduction of emotional feature discrimination when the SEED-IV dataset collect finer emotion. Meanwhile, the similar classification performance of the two tasks (62.32% vs. 61.24%) proves the effectiveness of the proposed method in eliminating the domain shift problem. Also for the MPED dataset (which contains more emotion categories), the less discrimination between emotions leads to a further decline in model performance (the 6th and 7th columns of Table  2 ). However, since E 2 STN considers both the inter-domain differences between datasets and the dynamic connection relationship of the emotional functional regions of the brain, it still has the highest recognition performance compared with other advanced methods.\n\nTo verify the confidence of our experimental results, we perform the t-test statistical analysis  (Hanusz, Tarasinska, and Zielinski 2016)  on each reproduced accuracy result. First, the Shapro-Wilk test (S-W test)  (Semenick 1990 ) is performed to eliminate the accuracy data that do not follow the normal distribution hypothesis. The statistical analyses are conducted by the SPSS software (IBM SPSS Statistics 3 ), and the significance level of paired t-test  (Hanusz, Tarasinska, and Zielinski 2016)  is defined as p < 0.05. Taking the GECNN method as an example, our proposed E 2 STN shows significantly better (p < 0.05) with it in each cross-dataset task. From this statistical analysis, it can be seen that our proposed method can effectively reduce the inter-domain differences among different datasets and achieve efficient EEG emotion recognition.\n\nTo explore which emotion is more easily recognized by the proposed model, we depict confusion matrices based on the results of the E 2 STN, which are shown in Fig.  4  (1). From this figure, we can have the following observations. Except for the 'SEED 3 → MPED 3 ' experiment (Fig.  4 (f) ), the recognition accuracy of 'happiness' emotion is higher than that of 'sadness' emotion, with an average of 24.56% higher. This proves 'happiness' is more accessible to distinguish than 'sadness' emotions, indicating that the 'happiness' emotion is more easily induced in different datasets. Furthermore, compared with the 'happiness' emotion, the average accuracy of 'sadness' emotion is 40.38%. The lower recognition accuracy of 'sadness' emotion is because it is easy to be mistaken for the 'neutrality' emotion, especially   in Fig.  4 (a), (b), (d), and (f), which may be because the \"sadness\" emotion is weakly stimulated in these experiments.\n\nFor the 'MPED 3 → SEED-IV 3 ' and 'SEED 3 → SEED-IV 3 ' experiments, we can have similar observation between Fig.  4 (c) and (d) with the SEED-IV dataset as the target dataset. Namely, the recognition accuracies of E 2 STN for the three emotions have the following relationship: 'neutrality' >'happiness' >'sadness'. The increase in emotional categories of the SEED-IV dataset may result in more subtle emotional changes thus more difficult to recognize. For the 'SEED 3 → MPED 3 ' experiment in Fig.  4 (f), the recognition accuracy of the 'sadness' emotion is highest, which is the opposite of the other experiment results. The grander difference in the emotional categories between the SEED and MPED datasets may lead to a more significant recognition of the 'sadness' emotions.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "4-Category Cross-Dataset Eeg Emotion Recognition",
      "text": "To evaluate the performance of E 2 STN under more emotional categories, we conduct additional 4-category cross-dataset EEG emotion recognition experiments on SEED-IV and MPED datasets. Similarly, to compare the E 2 STN with other representative methods in previous studies on EEG emotion recognition, we also conduct the same experiments using the comparison methods, i.e. linear SVM  (Suykens and Vandewalle 1999) , A-LSTM  (Song et al. 2019) , IAG  (Song et al. 2020a) , GECNN  (Song et al. 2022) , BiDANN  (Li et al. 2018) , and TANN  (Li et al. 2021 ). We reproduce their results from the literature to ensure a convincing comparison with the proposed method. The mean accuracy (ACC) and standard deviation (STD) of all subjects in the test dataset as the evaluation criteria are shown in Table  3 .\n\nCompared with the 3-category cross-dataset EEG emo- Exploring the importance of emotion-related brain regions To more clearly explore the contribution of different brain functional regions for EEG emotion recognition, we depict the electrode activity maps in Fig.  5 . The contribution of each brain region is reflected in the visualization of advanced features H DG , which is extracted by the dynamic graph convolutional layer in discriminative prediction module. The darker red areas in the figure indicate that higher contributions from corresponding regions of the brain. It can be clearly seen that the frontal and temporal lobes of the brain are activated, which is consistent with existing neuroscience research  (Alarcão and Fonseca 2019) . This reflects that E 2 STN extracts the most important emotion-related features in both stylized and source domain EEG samples, further demonstrating the excellent performance of the proposed method for cross-dataset EEG emotion recognition.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Conclusion",
      "text": "In this study, we propose an EEG-based emotion style transfer network, called E 2 STN, to realize effective cross-dataset EEG emotion recognition. Concretely, we design three modules to accomplish transfer, discriminative prediction, and evaluation tasks, respectively. The proposed transfer module can effectively reduce the inter-domain differences in the data distribution of different datasets and generate the stylized emotional EEG samples of the target domain. The discriminative prediction module is composed of dynamic graph convolution and fully connected layers, which is jointly trained by the source domain and stylized EEG samples to achieve accurate prediction for cross-dataset experiments. Finally, the transfer evaluation module extracts multiscale spatio-temporal features of the stylized EEG samples to construct the multi-dimensional losses constraining the process of emotional EEG style transfer. Extensive experiments have proved the effectiveness of our proposed E 2 STN in cross-dataset EEG emotion recognition tasks. Meanwhile, we have explored the distribution of important brain regions related to emotion, providing a basis for neurophysiology.\n\nIn future research, we hope to further explore the transfer rules of emotional EEG signals, so as to further improve the performance of cross-dataset EEG emotion recognition.  (2) 4-category",
      "page_start": 9,
      "page_end": 9
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Distribution of EEG data in different subjects and",
      "page": 1
    },
    {
      "caption": "Figure 1: . Notably,",
      "page": 2
    },
    {
      "caption": "Figure 1: Narrowing the differences between domains will",
      "page": 2
    },
    {
      "caption": "Figure 2: The network aims to re-",
      "page": 3
    },
    {
      "caption": "Figure 2: Framework of E2STN. The transfer module is applied to obtain emotional EEG representations that contain affective",
      "page": 4
    },
    {
      "caption": "Figure 3: (a). The emotional EEG samples of the",
      "page": 4
    },
    {
      "caption": "Figure 3: Architecture of encoder and decoder layer in",
      "page": 4
    },
    {
      "caption": "Figure 4: (a), (b), (d), and (f), which may be because the ”sad-",
      "page": 8
    },
    {
      "caption": "Figure 4: (c) and (d) with the SEED-IV dataset as the target",
      "page": 8
    },
    {
      "caption": "Figure 4: (f), the recognition",
      "page": 8
    },
    {
      "caption": "Figure 4: (2). For the ’MPED4 →SEED-IV4’ and ’SEED-IV4",
      "page": 9
    },
    {
      "caption": "Figure 4: (h), which is consistent with the neu-",
      "page": 9
    },
    {
      "caption": "Figure 5: The contribu-",
      "page": 9
    },
    {
      "caption": "Figure 4: Confusion matrices of E2STN results on cross-",
      "page": 11
    },
    {
      "caption": "Figure 5: Visualization of HDG distribution in the discrim-",
      "page": 12
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Number of emotional categories": "3-category",
          "The source domain": "MPED3",
          "The target domain": "SEED3",
          "The emotional categories": "neutral, joy, sad"
        },
        {
          "Number of emotional categories": "",
          "The source domain": "MPED3",
          "The target domain": "SEED-IV3",
          "The emotional categories": "neutral, joy(happy), sad"
        },
        {
          "Number of emotional categories": "",
          "The source domain": "SEED-IV3",
          "The target domain": "SEED3",
          "The emotional categories": "neutral, happy, sad"
        },
        {
          "Number of emotional categories": "",
          "The source domain": "SEED-IV3",
          "The target domain": "MPED3",
          "The emotional categories": "neutral, happy(joy), sad"
        },
        {
          "Number of emotional categories": "",
          "The source domain": "SEED3",
          "The target domain": "SEED-IV3",
          "The emotional categories": "neutral, happy, sad"
        },
        {
          "Number of emotional categories": "",
          "The source domain": "SEED3",
          "The target domain": "MPED3",
          "The emotional categories": "neutral, joy, sad"
        },
        {
          "Number of emotional categories": "4-category",
          "The source domain": "MPED4",
          "The target domain": "SEED-IV4",
          "The emotional categories": "neutral, joy(happy), sad, fear"
        },
        {
          "Number of emotional categories": "",
          "The source domain": "SEED-IV4",
          "The target domain": "MPED4",
          "The emotional categories": "neutral, happy(joy), sad, fear"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Method": "",
          "ACC / STD (%)": "MPED3 → SEED3"
        },
        {
          "Method": "SVM (Suykens and Vandewalle 1999)\nA-LSTM (Song et al. 2019)\nIAG (Song et al. 2020a)\nGECNN (Song et al. 2022)\nBiDANN (Li et al. 2018)\nTANN (Li et al. 2021)\nE2STN",
          "ACC / STD (%)": "48.94/04.96\n47.55/07.46\n60.89*/–\n62.90/06.58\n61.30/09.14\n64.23/09.63\n73.51/07.23"
        }
      ],
      "page": 8
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Emotions Recognition Using EEG Signals: A Survey",
      "authors": [
        "S Alarcão",
        "M Fonseca"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "2",
      "title": "Unsupervised emotional state classification through physiological parameters for social robotics applications",
      "authors": [
        "L Fiorini",
        "G Mancioppi",
        "F Semeraro",
        "H Fujita",
        "F Cavallo"
      ],
      "year": "2020",
      "venue": "Unsupervised emotional state classification through physiological parameters for social robotics applications"
    },
    {
      "citation_id": "3",
      "title": "Image Style Transfer Using Convolutional Neural Networks",
      "authors": [
        "L Gatys",
        "A Ecker",
        "M Bethge"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "4",
      "title": "Metaverse-Powered Experiential Situational English-Teaching Design: An Emotion-Based Analysis Method",
      "authors": [
        "H Guo",
        "W Gao"
      ],
      "year": "2022",
      "venue": "Frontiers in Psychology"
    },
    {
      "citation_id": "5",
      "title": "Shapiro-Wilk test with known mean",
      "authors": [
        "Z Hanusz",
        "J Tarasinska",
        "W Zielinski"
      ],
      "year": "2016",
      "venue": "REVSTAT-Statistical Journal"
    },
    {
      "citation_id": "6",
      "title": "Advances in multimodal emotion recognition based on brain-computer interfaces",
      "authors": [
        "Z He",
        "Z Li",
        "F Yang",
        "L Wang",
        "J Li",
        "C Zhou",
        "J Pan"
      ],
      "year": "2020",
      "venue": "Brain sciences"
    },
    {
      "citation_id": "7",
      "title": "DREAMER: A Database for Emotion Recognition Through EEG and ECG Signals From Wireless Low-cost Off-the-Shelf Devices",
      "authors": [
        "S Katsigiannis",
        "N Ramzan"
      ],
      "year": "2018",
      "venue": "IEEE Journal of Biomedical and Health Informatics"
    },
    {
      "citation_id": "8",
      "title": "Semi-Supervised Classification with Graph Convolutional Networks",
      "authors": [
        "T Kipf",
        "M Welling"
      ],
      "year": "2017",
      "venue": "Semi-Supervised Classification with Graph Convolutional Networks",
      "arxiv": "arXiv:1609.02907"
    },
    {
      "citation_id": "9",
      "title": "Decoding the Nature of Emotion in the Brain",
      "authors": [
        "P Kragel",
        "K Labar"
      ],
      "year": "2016",
      "venue": "Trends in Cognitive Sciences"
    },
    {
      "citation_id": "10",
      "title": "Multisource Transfer Learning for Cross-Subject EEG Emotion Recognition",
      "authors": [
        "J Li",
        "S Qiu",
        "Y.-Y Shen",
        "C.-L Liu",
        "H He"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Cybernetics"
    },
    {
      "citation_id": "11",
      "title": "GMSS: Graph-Based Multi-Task Self-Supervised Learning for EEG Emotion Recognition",
      "authors": [
        "Y Li",
        "J Chen",
        "F Li",
        "B Fu",
        "H Wu",
        "Y Ji",
        "Y Zhou",
        "Y Niu",
        "G Shi",
        "W Zheng"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "12",
      "title": "A novel transferability attention neural network model for EEG emotion recognition",
      "authors": [
        "Y Li",
        "B Fu",
        "F Li",
        "G Shi",
        "W Zheng"
      ],
      "year": "2021",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "13",
      "title": "A Novel Neural Network Model based on Cerebral Hemispheric Asymmetry for EEG Emotion Recognition",
      "authors": [
        "Y Li",
        "W Zheng",
        "Z Cui",
        "T Zhang",
        "Y Zong"
      ],
      "year": "2018",
      "venue": "A Novel Neural Network Model based on Cerebral Hemispheric Asymmetry for EEG Emotion Recognition"
    },
    {
      "citation_id": "14",
      "title": "Inferring of human emotional states using multichannel EEG",
      "authors": [
        "M Murugappan",
        "M Rizon",
        "R Nagarajan",
        "S Yaacob"
      ],
      "year": "2010",
      "venue": "European Journal of Scientific Research"
    },
    {
      "citation_id": "15",
      "title": "Self-weighted semi-supervised classification for joint EEG-based emotion recognition and affective activation patterns mining",
      "authors": [
        "Y Peng",
        "W Kong",
        "F Qin",
        "F Nie",
        "J Fang",
        "B.-L Lu",
        "A Cichocki"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Instrumentation and Measurement"
    },
    {
      "citation_id": "16",
      "title": "Joint Feature Adaptation and Graph Adaptive Label Propagation for Cross-Subject Emotion Recognition From EEG Signals",
      "authors": [
        "Y Peng",
        "W Wang",
        "W Kong",
        "F Nie",
        "B.-L Lu",
        "A Cichocki"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "17",
      "title": "Tests and measurements: The T-test",
      "authors": [
        "D Semenick"
      ],
      "year": "1990",
      "venue": "Strength & Conditioning Journal"
    },
    {
      "citation_id": "18",
      "title": "Emotion recognition through integrating EEG and peripheral signals",
      "authors": [
        "Y Shu",
        "S Wang"
      ],
      "year": "2017",
      "venue": "2017 IEEE international conference on acoustics, speech and signal processing (ICASSP)"
    },
    {
      "citation_id": "19",
      "title": "Instance-Adaptive Graph for EEG Emotion Recognition",
      "authors": [
        "T Song",
        "S Liu",
        "W Zheng",
        "Y Zong",
        "Z Cui"
      ],
      "year": "2020",
      "venue": "Instance-Adaptive Graph for EEG Emotion Recognition"
    },
    {
      "citation_id": "20",
      "title": "Variational instance-adaptive graph for EEG emotion recognition",
      "authors": [
        "T Song",
        "S Liu",
        "W Zheng",
        "Y Zong",
        "Z Cui",
        "Y Li",
        "X Zhou"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "21",
      "title": "Graph-Embedded Convolutional Neural Network for Image-Based EEG Emotion Recognition",
      "authors": [
        "T Song",
        "W Zheng",
        "S Liu",
        "Y Zong",
        "Z Cui",
        "Y Li"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Emerging Topics in Computing"
    },
    {
      "citation_id": "22",
      "title": "MPED: A multi-modal physiological emotion database for discrete emotion recognition",
      "authors": [
        "T Song",
        "W Zheng",
        "C Lu",
        "Y Zong",
        "X Zhang",
        "Z Cui"
      ],
      "year": "2019",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "23",
      "title": "EEG Emotion Recognition Using Dynamical Graph Convolutional Neural Networks",
      "authors": [
        "T Song",
        "W Zheng",
        "P Song",
        "Z Cui"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "24",
      "title": "A Dual-Branch Dynamic Graph Convolution Based Adaptive TransFormer Feature Fusion Network for EEG Emotion Recognition",
      "authors": [
        "M Sun",
        "W Cui",
        "S Yu",
        "H Han",
        "B Hu",
        "Y Li"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "25",
      "title": "Least squares support vector machine classifiers",
      "authors": [
        "J Suykens",
        "J Vandewalle"
      ],
      "year": "1999",
      "venue": "Neural Processing Letters"
    },
    {
      "citation_id": "26",
      "title": "Visualizing data using t-SNE",
      "authors": [
        "L Van Der Maaten",
        "G Hinton"
      ],
      "year": "2008",
      "venue": "Journal of machine learning research"
    },
    {
      "citation_id": "27",
      "title": "Attention is All you Need",
      "authors": [
        "A Vaswani",
        "N Shazeer",
        "N Parmar",
        "J Uszkoreit",
        "L Jones",
        "A Gomez",
        "L Kaiser",
        "I Polosukhin",
        "I Guyon",
        "U Luxburg",
        "S Bengio",
        "H Wallach"
      ],
      "year": "2017",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "28",
      "title": "attention-based neural network for EEG emotion recognition",
      "authors": [
        "G Xiao",
        "M Shi",
        "M Ye",
        "B Xu",
        "Z Chen",
        "Q Ren"
      ],
      "year": "2022",
      "venue": "Cognitive Neurodynamics"
    },
    {
      "citation_id": "29",
      "title": "EmotionMeter: A Multimodal Framework for Recognizing Human Emotions",
      "authors": [
        "W Zheng",
        "W Liu",
        "Y Lu",
        "B Lu",
        "A Cichocki"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Cybernetics"
    },
    {
      "citation_id": "30",
      "title": "Investigating critical frequency bands and channels for EEG-based emotion recognition with deep neural networks",
      "authors": [
        "W.-L Zheng",
        "B.-L Lu"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on autonomous mental development"
    },
    {
      "citation_id": "31",
      "title": "Identifying stable patterns over time for emotion recognition from EEG",
      "authors": [
        "W.-L Zheng",
        "J.-Y Zhu",
        "B.-L Lu"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "32",
      "title": "EEG-based emotion recognition using regularized graph neural networks",
      "authors": [
        "P Zhong",
        "D Wang",
        "C Miao"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    }
  ]
}