{
  "paper_id": "2409.17899v2",
  "title": "Exploring Acoustic Similarity In Emotional Speech And Music Via Self-Supervised Representations",
  "published": "2024-09-26T14:49:09Z",
  "authors": [
    "Yujia Sun",
    "Zeyu Zhao",
    "Korin Richmond",
    "Yuanchao Li"
  ],
  "keywords": [
    "Speech Emotion Recognition",
    "Music Emotion Recognition",
    "Self-Supervised Learning",
    "Layerwise Analysis",
    "Domain Adaptation",
    "Fréchet Audio Distance"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Emotion recognition from speech and music shares similarities due to their acoustic overlap, which has led to interest in transferring knowledge between these domains. However, the shared acoustic cues between speech and music, particularly those encoded by Self-Supervised Learning (SSL) models, remain largely unexplored, given the fact that SSL models for speech and music have rarely been applied in crossdomain research. In this work, we revisit the acoustic similarity between emotion speech and music, starting with an analysis of the layerwise behavior of SSL models for Speech Emotion Recognition (SER) and Music Emotion Recognition (MER). Furthermore, we perform crossdomain adaptation by comparing several approaches in a two-stage finetuning process, examining effective ways to utilize music for SER and speech for MER. Lastly, we explore the acoustic similarities between emotional speech and music using Fréchet audio distance for individual emotions, uncovering the issue of emotion bias in both speech and music SSL models. Our findings reveal that while speech and music SSL models do capture shared acoustic features, their behaviors can vary depending on different emotions due to their training strategies and domain-specificities. Additionally, parameter-efficient fine-tuning can enhance SER and MER performance by leveraging knowledge from each other. This study provides new insights into the acoustic similarity between emotional speech and music, and highlights the potential for cross-domain generalization to improve SER and MER systems.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "Emotion recognition from audio signals has become a pivotal task in various applications, ranging from human-computer interaction to affective computing. Two important subfields in this domain are Speech Emotion Recognition (SER) and Music Emotion Recognition (MER). While SER focuses on identifying emotional states from speech, MER aims to recognize emotions conveyed through music. Both tasks share common challenges, such as dealing with subjective emotional labels and the variability of acoustic signals, yet they offer complementary insights into how emotions are expressed and perceived across different modalities  [1] -  [4] .\n\nFurthermore, there has been recognition of the synergy between SER and MER. Both tasks involve the analysis of temporal, spectral, and emotional cues in audio, and advances in one field can benefit the other. For instance, insights gained from modeling emotional dynamics in speech can inform MER, particularly in understanding how rhythm, pitch, and timbre contribute to emotion perception  [5] . By leveraging shared architectures and pretraining strategies, it has led to the feasibility of transfer learning and joint training for SER and MER with cross-domain knowledge  [6] ,  [7] .\n\nRecently, Self-Supervised Learning (SSL) has emerged as a powerful paradigm in audio analysis, addressing the need for large annotated datasets by leveraging unlabeled data  [8] ,  [9] . Self-supervised models have demonstrated remarkable success by pretraining on vast amounts of unlabeled audio data, enabling the extraction of effective representations for downstream tasks, including SER and MER. These models have significantly advanced SER by capturing † Corresponding author. yuanchao.li@ed.ac.uk nuanced prosodic and paralinguistic features  [10] , and are now increasingly being applied to MER, where they show promise in learning emotional patterns from music  [11] .\n\nNevertheless, the shared acoustic cues between speech and music, particularly those encoded by SSL models, remain largely unexplored. While SSL-based models have demonstrated impressive performance in both tasks, the extent to which these models capture and utilize overlapping acoustic features across speech and music is not well understood. This gap limits progress in developing crossdomain insights and hinders the potential for improving SER and MER systems.\n\nTherefore, we address two main research questions: 1) What insights into SSL models can be gained from their performance in SER and MER? 2) Can cross-domain generalization improve performance in SER and MER through domain adaptation? To answer these questions, we undertake the following tasks in both the speech and music domains:\n\n• We conduct cross-domain layerwise probing utilizing SSL models pretrained on either speech or music data.\n\n• We implement domain adaptation techniques in a two-stage process to improve emotion recognition performance with limited data for each task.\n\n• We evaluate the acoustic similarity between speech and music for each emotion using SSL representations with Fréchet audio distance.\n\nNote that in this work we focus on song, a subdomain of music, since the dataset we use contains vocal-only music. This reduces the impact of instrumentation and supports a fair evaluation of the similarity between speech and music in terms of vocal acoustics.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Related Work",
      "text": "Over the past decade, researchers have explored the acoustic similarities and generalizability between speech and music.  [12]  demonstrated that models leveraging cross-domain features outperformed those relying on domain-specific features, suggesting that speech and music share certain emotional characteristics. Subsequently, they applied multitask learning to jointly predict emotions from both speech and music  [6] ,  [7] . In another study,  [5]  investigated transfer learning between the two domains using denoising autoencoderpretrained long short-term memory networks for arousal-valence regression tasks, showing promising cross-domain generalizability. Likewise,  [13]  found that pretraining on speech data enhanced the performance of convolutional neural networks in MER.\n\nNevertheless, the exploration of acoustic similarity between speech and music, as well as the use of transfer learning and domain adaptation between SER and MER, remains limited, especially in the era of SSL models. Although SSL models have demonstrated success in various audio-related tasks, their application to such crossdomain research has yet to be explored. Therefore, we revisit the utility of acoustic correlations between emotional speech and music by using representations from SSL models pretrained on either speech or music. arXiv:2409.17899v2 [eess.AS] 30 Apr 2025",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Iii. Dataset And Ssl Models",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "A. Dataset",
      "text": "We use the RAVDESS dataset  [14] , an audio-visual collection consisting of acted affective recordings of speech and song. The dataset features recordings of 12 female and 12 male professional actors speaking English with a North American accent. In our study, we focus on the audio recordings, excluding the visual input. These recordings contain only vocal performances without accompanying instrumental music, thereby excluding the instrumental influence on the acoustic similarity. Since the song recordings of one female actor are missing, we discard her speech recordings as well to ensure an equal amount of speech and song data for model training. The text content in both the speech and song recordings is semantically neutral and identical, allowing us to concentrate on the acoustic properties by eliminating potential effects caused by lexical content (e.g., prosody variations due to word pronunciation). These are the key reasons we have chosen RAVDESS, despite the availability of larger datasets.\n\nFor emotion categories, we select the ones common to both the speech and music recordings, including neutral, calm, happy, sad, angry, and fearful. This yields 92 files for neutral and 184 files for each of the other emotions in both domains. The higher count for emotions other than neutral is due to their having two intensity levels, whereas neutral is presented with only one normal intensity. To ensure sufficient data samples, we ignore the intensity distinction and randomly select 60% of the data for training, 20% for validation, and 20% for testing, from the total of 1,012 recordings in each domain.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "B. Ssl Models",
      "text": "We select Wav2Vec 2.0-Base-960h (W2V2), HuBERT-Base-ls960 (HuBERT)  [15] , and MERT-v1-95M (MERT) for investigation. All three models have a very similar architecture, consisting of seven convolutional encoders and 12 transformer encoders with around 95M parameters in total. Hidden representations from all 12 layers are used as the input to the downstream classifier for SER and MER.\n\nThe primary difference among the three models lies in their pretraining objectives. W2V2 masks the speech input in the latent space and solves a contrastive task defined over a quantization of the latent representations, which are jointly learned. HuBERT shares the same concept as W2V2 but applies a classification task, requiring the model to classify hidden sequences into predefined K-means clusters. Both W2V2 and HuBERT are pretrained on LibriSpeech  [16] , whereas MERT is pretrained on 160k hours of music recordings from the Internet. Although MERT's pretraining strategy generally aligns with that of HuBERT, its objective is modified to incorporate music-relevant cues, such as harmony, timbre, and musical pitch.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Iv. Methodology",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "A. Layerwise Probing Of Emotion Recognition Performance",
      "text": "To investigate the layerwise behavior of each model on either speech or music data, we first extract features from each of the three upstream SSL models (W2V2, HuBERT, or MERT) and apply mean pooling over all frames along the time dimension to these features, following previous work by  [10] ,  [17] . Subsequently, for each layer, we train a linear classifier on the training set to perform SER or MER. We select the best-performing checkpoint based on the Unweighted Accuracy (UA) on the validation set and evaluate it on the test set.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "B. Domain Adaptation For Cross-Domain Performance Improvement",
      "text": "To investigate whether shared acoustic cues between speech and music can be transferred for emotion recognition in each domain, we combine all 12-layer representations and implement three approaches for transferering the shared acoustics:\n\n• 1) Baseline: we keep the pretrained SSL model frozen and perform mean pooling across the 12-layer representations. Only the linear classifier is trainable.\n\n• 2) Weighted-Sum (WS): we replace mean pooling with learnable 12-dimensional WS parameters, while the SSL models maintain frozen, consistent with the baseline.\n\n• 3) Parameter-Efficient Fine-Tuning (PEFT): we incorporate WS, Weighting-Gate (WG), Low-Rank Adapter (LoRA)  [18] , and Bottleneck Adapter (BA)  [19] . The SSL models are kept frozen, with only the PEFT modules being trainable.\n\nThe same downstream classifier used in layerwise probing is employed in all approaches. As two-stage fine-tuning has demonstrated cross-corpus and cross-lingual SER ability  [20] ,  [21] , we apply this approach to all three methods. Specifically, in Stage One, we train the model on the source domain (either speech or music), save the model with the best validation accuracy, and test it on the source domain. In Stage Two, we load the saved model, further train it on the target domain (music if the source is speech, or vice versa), and again save the model with the highest validation accuracy, testing it on the target domain.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "C. Layerwise Analysis Of Cross-Domain Acoustic Similarity",
      "text": "To explore the extent to which acoustic representations are shared between emotional speech and music, we adopt Fréchet Audio Distance (FAD) as a reference-free measure to assess acoustic similarity. Compared to traditional similarity metrics, such as cosine similarity and Euclidean distance, FAD is specifically designed for audio assessment, capturing the perceptual similarity between two audio embedding distributions  [22] . It has been shown to effectively distinguish between real and synthetic audio  [23] , as well as audio with different emotions  [24] . Therefore, we use FAD to evaluate the acoustic similarity between speech and song. The calculation is as follows.\n\nGiven the embeddings of speech set and music set, X s and X m , the FAD score is calculated using multivariate Gaussians derived from the two embedding sets X s (µs, Σs) and X m (µm, Σm):\n\nwhere tr is the trace of a matrix. Besides using the entire speech and music sets, we also calculate FAD for each emotion to investigate whether the acoustic similarity exhibits different patterns across emotion categories. This analysis is also performed using the representations from each layer.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "V. Experiments",
      "text": "A. Layerwise Probing of Emotion Recognition Performance 1) Experimental Settings: For training the SER and MER models, we use cross-entropy loss as the criterion and the AdamW optimizer with a learning rate of 1e-3. Due to the variation in the number of epochs required to achieve optimal validation performance using different SSL representations (particularly layer 11), we train for 500 epochs to ensure all classifiers reach their full potential.\n\n2) Results and Discussions: Fig.  1  presents the all-emotion results. It can be observed that:\n\n(I) For SER, our results align with previous findings, showing that while W2V2 experiences a performance decline in its final layers, HuBERT does not  [10] ,  [25] . In fact, performance improves in the deeper layers of HuBERT compared to its shallow layers. Additionally, we observe that MERT behaves similarly to HuBERT, exhibiting better performance in deeper layers than W2V2. This is likely due to the pretraining objectives and strategies of MERT aligning closely with those of HuBERT. For MER, all models behave consistently with SER. Moreover, the speech models (W2V2 & HuBERT) perform better at SER, while the music model (MERT) is better at MER, which is reasonable given the differences in their pretraining data. (II) For the speech models, the accuracy gap between SER and MER after the middle layers is more pronounced in HuBERT than in W2V2. Since both models are pretrained on the same data, and the lexical content of RAVDESS does not affect SER or MER, it is likely that HuBERT's deeper layers retain acoustic information that W2V2 misses yet is emotion-relevant. This retained acoustic information could explain the larger performance gap between HuBERT's SER and MER performance from the middle layers. Such a gap is not observed in W2V2, even though its SER performance remains better than its MER performance. (III) MERT shows a significant initial difference between SER and MER, which is not observed on speech models. Despite MERT having similar training objectives and structure to HuBERT, this suggests that the acoustics from the shallow layers of speech models are domain-agnostic and largely shareable, whereas those from the shallow layers of MERT are relatively domain-specific. However, given MERT's fairly good performance on SER, this still indicates shared acoustics between music and speech. Additionally, the drop in SER performance in the deeper layers of MERT suggests that the features in those layers are more favorable for MER than SER. It is possible that the deep layers of MERT encode high-level musicspecific cues, such as those related to tonality and rhythm  [26] ,  [27] . Therefore, speech SSL models, particularly the shallow layers, may be further trainable with music acoustics, whereas further training of music SSL models with speech acoustics may be less effective.\n\nTable  I  and II summarize the overall performances of the speech models and music model on both SER and MER. Since different emotions exhibit distinct paralinguistic patterns, for instance, angry and happy typically have higher intensity and pitch, while sad and calm tend to have lower intensity  [28] ,  [29] ,  [10]  demonstrated that W2V2 shows emotion bias by not encoding different emotional speech equally. In light of this, we compare and contrast emotions by calculating their respective recognition accuracies. Fig.  2  shows the per-emotion results.\n\nFor SER, it can be observed that performance on angry and fearful speech is generally the best, followed by calm, across all speech and music models. This suggests that both speech and music models favor the acoustic characteristics of these emotions in speech. However, for MER, recognizing fearful and calm proves to be more challenging, particularly in the case of fearful with MERT. In contrast, angry music becomes easier to recognize, even for speech models. Moreover, neutral performs much better on music with MERT than any other neutral cases. Additionally, angry music with W2V2 does not show a performance drop, unlike other emotions with W2V2 for either speech or music. We believe that the deep layers of W2V2 might be particularly effective at modeling acoustic characteristics related to angry music, such as intense rhythm. These observations suggest that different emotions in speech and music exhibit distinct acoustic characteristics, highlighting the emotion bias problem in both speech and music SSL models, and strengthening the findings of  [10] . Finally, other emotions all have their respective variations on speech or music with speech models or the music model, but the overall patterns remains similar. This further verifies previous findings that shared acoustic representations exist between speech and music  [6] ,  [13] .\n\nB. Domain Adaptation for Cross-Domain Performance Improvement 1) Experimental Settings: For the baseline and WS models, we use the AdamW optimizer with a learning rate of 1e-3. For the PEFT models, we use the AdamW optimizer with a learning rate of 1e-4. For the same reason as for the layerwise probing, each model is trained for 300 epochs at each stage to achieve its best possible performance.\n\n2) Results and Discussions: Table  III  summarizes the domain adaptation results using all three approaches and models. Note that the results of SER and MER from the same model are not comparable, as they are different tasks. We only compare SER with SER and MER with MER. It can be seen that for the speech models (W2V2 and HuBERT), fine-tuning on either domain in stage one always helps the other domain in stage two. For example, the performances of SER followed by MER (74.88, 80.30, 87.20 for W2V2; 84.24, 82.76, 93.10 for HuBERT) are always better than directly conducting SER (73.89, 79.31, 87.20 for W2V2; 83.25, 80.30, 88.18 for HuBERT). Furthermore, PEFT works extremely effectively for HuBERT, achieving the best performance (93.10) and the largest improvement (4.92). For the music model, while several performances are improved after domain adaptation (e.g., 72.77 vs. 61.85 for SER and 93.60 vs. 91.63), such a phenomenon does not generally hold. This finding further verifies our conclusion of Finding III in Section V-A that speech SSL models are further trainable with music acoustics, yet further training of music SSL models with speech acoustics may be less effective.\n\nC. Layerwise Analysis of Cross-Domain Acoustic Similarity 1) Experimental Settings: As FAD is a reference-free measurement, there is no model training involved. We separate the speech and music sets per emotion, and use the FAD toolkit for the implementation. The lower the FAD score, the more similar the two sets of representations.\n\n2) Results and Discussions: From Fig.  3 , it can be observed that: (I) In terms of distance ranking, all the speech and music models exhibit consistent patterns: angry and fearful have the smallest distances, followed by happy, sad, neutral, and calm. These consistent patterns indicate that the SSL models have a similar ability to capture acoustic similarities related to emotion in their respective representations. Since speech and music share the same text in RAVDESS, the low FAD scores for certain emotions indicate that their acoustics are largely similar between speech and music. For example, angry speech and music are more acoustically similar than calm speech and music.\n\n(II) HuBERT and MERT exhibit aligned patterns not only in the per-emotion ranking but also in the overall trend. The FAD score increases from the first layer to the last, which can be attributed to the similar structures of HuBERT and MERT. However, several middle layers show divergent patterns: while the score continues to rise in HuBERT, it remains stable in MERT. This difference is related to their respective capabilities: HuBERT encodes phoneticlevel information in the middle layers  [30] , whereas such capability has not been found in MERT. Additionally, the FAD scores for the final layers of both HuBERT and MERT rise sharply, indicating that their ability to capture shared acoustic features between speech and music weakens at these layers.\n\n(III) Unlike HuBERT and MERT, the FAD scores for W2V2 exhibit a significant shift starting from the middle layers. One explanation for this is that W2V2 begins encoding word identity and meaning between the middle and final layers, as noted by  [31] . Since speech and music datasets share identical text, the acoustic similarity increases, resulting in lower FAD scores. Additionally, different emotions behave differently in layer 11, where calm, neutral, and sad emotions show a sharp rise, while angry, happy, and fearful do not. It appears that layer 11 amplifies these acoustic differences, consistent with previous findings that observed peculiar behavior in this layer due to its contrastive masked segment prediction function  [10] ,  [31] . We further demonstrate the peculiarity of layer 11 in that it is particularly sensitive to cross-domain acoustic differences.\n\nSince using FAD does not involve model training, thereby avoiding potential impacts on the fairness of comparison across emotions, the results and findings instead further underscore the emotion-bias problem present in SSL representations.",
      "page_start": 2,
      "page_end": 5
    },
    {
      "section_name": "Vi. Conclusion",
      "text": "As the first work exploring acoustic similarity between emotional speech and music through SSL models, we conduct layerwise probing to explore the models' ability to capture emotion-relevant acoustic cues and uncover the commonality and differences between speech and music SSL models. Moreover, aiming to leverage each domain to enhance the emotion recognition performance for the other domain, we implement and compare three approaches, demonstrating efficacy of the PEFT approach for cross-domain adaptation and noticing the domain-specificity of the models. Finally, we employ FAD to measure the acoustic similarity between speech and music representations on each emotion, revealing different behaviors of SSL models and the emotion-bias issue in both domains. Our study provides new insights into the acoustic similarity between emotional speech and music, particularly with SSL models. We also demonstrate the potential to leverage knowledge from each domain to facilitate the other. As there are limited datasets suitable for further investigation and fair comparison, we acknowledge the limitations of our work and plan to collect new datasets for extending our study in the future.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: presents the all-emotion re-",
      "page": 2
    },
    {
      "caption": "Figure 1: Layerwise SER and MER accuracy using SSL representations.",
      "page": 3
    },
    {
      "caption": "Figure 2: Layerwise SER (top) and MER (bottom) accuracy per emotion using",
      "page": 3
    },
    {
      "caption": "Figure 2: shows the per-emotion results.",
      "page": 3
    },
    {
      "caption": "Figure 3: , it can be observed that:",
      "page": 4
    },
    {
      "caption": "Figure 3: Layerwise cross-domain FAD per emotion of the SSL models.",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Approach": "",
          "W2V2": "SER → MER\nMER → SER",
          "HuBERT": "SER → MER\nMER → SER",
          "MERT": "SER → MER\nMER → SER"
        },
        {
          "Approach": "Baseline\nWS\nPEFT",
          "W2V2": "73.89\n69.95\n65.02\n74.88\n79.31\n77.83\n73.40\n80.30\n87.68\n87.20\n87.20\n79.80",
          "HuBERT": "83.25\n76.35\n68.97\n84.24\n80.30\n81.28\n78.82\n82.76\n91.13\n93.10\n88.18\n91.62",
          "MERT": "62.56\n83.74\n85.22\n61.58\n61.85\n89.66\n84.23\n72.77\n93.60\n91.63\n85.22\n82.76"
        }
      ],
      "page": 4
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Emotion recognition in human-computer interaction",
      "authors": [
        "Roddy Cowie",
        "Ellen Douglas-Cowie",
        "Nicolas Tsapatsoulis",
        "George Votsis",
        "Stefanos Kollias",
        "Winfried Fellenz",
        "John Taylor"
      ],
      "year": "2001",
      "venue": "IEEE Signal processing magazine"
    },
    {
      "citation_id": "2",
      "title": "A cross-cultural investigation of emotion inferences from voice and speech: Implications for speech technology",
      "authors": [
        "Klaus Scherer"
      ],
      "venue": "Proc. Interspeech, 2000"
    },
    {
      "citation_id": "3",
      "title": "Music emotion recognition: A state of the art review",
      "authors": [
        "Erik Youngmoo E Kim",
        "Raymond Schmidt",
        "Migneco",
        "Patrick Brandon G Morton",
        "Jeffrey Richardson",
        "Jacquelin Scott",
        "Douglas Speck",
        "Turnbull"
      ],
      "year": "2010",
      "venue": "Proc. ISMIR"
    },
    {
      "citation_id": "4",
      "title": "Machine recognition of music emotion: A review",
      "authors": [
        "Yi-Hsuan Yang",
        "Homer Chen"
      ],
      "year": "2012",
      "venue": "ACM Transactions on Intelligent Systems and Technology (TIST)"
    },
    {
      "citation_id": "5",
      "title": "Shared acoustic codes underlie emotional communication in music and speech-evidence from deep transfer learning",
      "authors": [
        "Eduardo Coutinho",
        "Björn Schuller"
      ],
      "year": "2017",
      "venue": "PloS one"
    },
    {
      "citation_id": "6",
      "title": "Recognizing emotion from singing and speaking using shared models",
      "authors": [
        "Biqiao Zhang",
        "Georg Essl",
        "Emily Provost"
      ],
      "year": "2015",
      "venue": "2015 International Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "7",
      "title": "Cross-corpus acoustic emotion recognition from singing and speaking: A multi-task learning approach",
      "authors": [
        "Biqiao Zhang",
        "Emily Provost",
        "Georg Essl"
      ],
      "year": "2016",
      "venue": "2016 IEEE International conference on acoustics, speech and signal processing"
    },
    {
      "citation_id": "8",
      "title": "data2vec: A general framework for self-supervised learning in speech, vision and language",
      "authors": [
        "Alexei Baevski",
        "Wei-Ning Hsu",
        "Qiantong Xu",
        "Arun Babu",
        "Jiatao Gu",
        "Michael Auli"
      ],
      "year": "2022",
      "venue": "Proceedings of the 39th International Conference on Machine Learning"
    },
    {
      "citation_id": "9",
      "title": "MusicBERT: A selfsupervised learning of music representation",
      "authors": [
        "Hongyuan Zhu",
        "Ye Niu",
        "Di Fu",
        "Hao Wang"
      ],
      "year": "2021",
      "venue": "Proceedings of the 29th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "10",
      "title": "Exploration of a self-supervised speech model: A study on emotional corpora",
      "authors": [
        "Yuanchao Li",
        "Yumnah Mohamied",
        "Peter Bell",
        "Catherine Lai"
      ],
      "year": "2023",
      "venue": "2022 IEEE Spoken Language Technology Workshop (SLT)"
    },
    {
      "citation_id": "11",
      "title": "Mert: Acoustic music understanding model with large-scale selfsupervised training",
      "authors": [
        "Yizhi Li",
        "Ruibin Yuan",
        "Ge Zhang",
        "Yinghao Ma",
        "Xingran Chen",
        "Hanzhi Yin",
        "Chenghao Xiao",
        "Chenghua Lin",
        "Anton Ragni",
        "Emmanouil Benetos"
      ],
      "year": "2023",
      "venue": "Mert: Acoustic music understanding model with large-scale selfsupervised training",
      "arxiv": "arXiv:2306.00107"
    },
    {
      "citation_id": "12",
      "title": "Predicting emotion perception across domains: A study of singing and speaking",
      "authors": [
        "Biqiao Zhang",
        "Emily Provost",
        "Robert Swedberg",
        "Georg Essl"
      ],
      "year": "2015",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "13",
      "title": "Transfer learning from speech to music: towards language-sensitive emotion recognition models",
      "authors": [
        "Juan Sebastián",
        "Gómez Cañón",
        "Estefanía Cano",
        "Perfecto Herrera",
        "Emilia Gómez"
      ],
      "year": "2021",
      "venue": "2020 28th European Signal Processing Conference"
    },
    {
      "citation_id": "14",
      "title": "The ryerson audio-visual database of emotional speech and song (RAVDESS): A dynamic, multimodal set of facial and vocal expressions in north american english",
      "authors": [
        "R Steven",
        "Frank Livingstone",
        "Russo"
      ],
      "year": "2018",
      "venue": "PloS one"
    },
    {
      "citation_id": "15",
      "title": "Hubert: Selfsupervised speech representation learning by masked prediction of hidden units",
      "authors": [
        "Wei-Ning Hsu",
        "Benjamin Bolte",
        "Hubert Yao-Hung",
        "Kushal Tsai",
        "Ruslan Lakhotia",
        "Abdelrahman Salakhutdinov",
        "Mohamed"
      ],
      "year": "2021",
      "venue": "IEEE/ACM transactions on audio, speech, and language processing"
    },
    {
      "citation_id": "16",
      "title": "Libri-light: A benchmark for asr with limited or no supervision",
      "authors": [
        "Jacob Kahn",
        "Morgane Riviere",
        "Weiyi Zheng",
        "Evgeny Kharitonov",
        "Qiantong Xu",
        "Pierre-Emmanuel Mazaré",
        "Julien Karadayi",
        "Vitaliy Liptchinsky",
        "Ronan Collobert",
        "Christian Fuegen"
      ],
      "year": "2020",
      "venue": "2020 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "17",
      "title": "Layer-wise analysis of self-supervised acoustic word embeddings: A study on speech emotion recognition",
      "authors": [
        "Alexandra Saliba",
        "Yuanchao Li",
        "Ramon Sanabria",
        "Catherine Lai"
      ],
      "year": "2024",
      "venue": "2024 IEEE International Conference on Acoustics, Speech, and Signal Processing Workshops (ICASSPW)"
    },
    {
      "citation_id": "18",
      "title": "LoRA: Low-rank adaptation of large language models",
      "authors": [
        "J Edward",
        "Phillip Hu",
        "Zeyuan Wallis",
        "Yuanzhi Allen-Zhu",
        "Shean Li",
        "Lu Wang",
        "Weizhu Wang",
        "Chen"
      ],
      "year": "2021",
      "venue": "International Conference on Learning Representations"
    },
    {
      "citation_id": "19",
      "title": "Parameter-efficient transfer learning for NLP",
      "authors": [
        "Neil Houlsby",
        "Andrei Giurgiu",
        "Stanislaw Jastrzebski",
        "Bruna Morrone",
        "Quentin De Laroussilhe",
        "Andrea Gesmundo",
        "Mona Attariyan",
        "Sylvain Gelly"
      ],
      "year": "2019",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "20",
      "title": "Parameter efficient finetuning for speech emotion recognition and domain adaptation",
      "authors": [
        "Nineli Lashkarashvili",
        "Wen Wu",
        "Guangzhi Sun",
        "Philip Woodland"
      ],
      "year": "2024",
      "venue": "2024 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "21",
      "title": "Cross-lingual speech emotion recognition: Humans vs. self-supervised models",
      "authors": [
        "Zhichen Han",
        "Tianqi Geng",
        "Hui Feng",
        "Jiahong Yuan",
        "Korin Richmond",
        "Yuanchao Li"
      ],
      "year": "2024",
      "venue": "Cross-lingual speech emotion recognition: Humans vs. self-supervised models",
      "arxiv": "arXiv:2409.16920"
    },
    {
      "citation_id": "22",
      "title": "Fréchet audio distance: A reference-free metric for evaluating music enhancement algorithms",
      "authors": [
        "Dominik Roblek",
        "Kevin Kilgour",
        "Matt Sharifi",
        "Mauricio Zuluaga"
      ],
      "year": "2019",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "23",
      "title": "Adapting frechet audio distance for generative music evaluation",
      "authors": [
        "Azalea Gui",
        "Hannes Gamper",
        "Sebastian Braun",
        "Dimitra Emmanouilidou"
      ],
      "year": "2024",
      "venue": "2024 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "24",
      "title": "Rethinking emotion bias in music via frechet audio distance",
      "authors": [
        "Yuanchao Li",
        "Azalea Gui",
        "Dimitra Emmanouilidou",
        "Hannes Gamper"
      ],
      "year": "2024",
      "venue": "Rethinking emotion bias in music via frechet audio distance",
      "arxiv": "arXiv:2409.15545"
    },
    {
      "citation_id": "25",
      "title": "On the utility of self-supervised models for prosody-related tasks",
      "authors": [
        "Guan-Ting Lin",
        "Chi-Luen Feng",
        "Wei-Ping Huang",
        "Yuan Tseng",
        "Tzu-Han Lin",
        "Chen-An Li",
        "Hung-Yi Lee",
        "Nigel Ward"
      ],
      "year": "2023",
      "venue": "2022 IEEE Spoken Language Technology Workshop (SLT)"
    },
    {
      "citation_id": "26",
      "title": "Neural correlates underlying perception of tonality-related emotional contents",
      "authors": [
        "Tomoyuki Mizuno",
        "Morihiro Sugishita"
      ],
      "year": "2007",
      "venue": "Neuroreport"
    },
    {
      "citation_id": "27",
      "title": "The emotional connotations of major versus minor tonality: One or more origins?",
      "authors": [
        "Richard Parncutt"
      ],
      "year": "2014",
      "venue": "Musicae Scientiae"
    },
    {
      "citation_id": "28",
      "title": "The relevance of voice quality features in speaker independent emotion recognition",
      "authors": [
        "Marko Lugger",
        "Bin Yang"
      ],
      "year": "2007",
      "venue": "2007 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "29",
      "title": "Stress and emotion classification using jitter and shimmer features",
      "authors": [
        "Xi Li",
        "Jidong Tao",
        "Joseph Michael T Johnson",
        "Anne Soltis",
        "Kirsten Savage",
        "John Leong",
        "Newman"
      ],
      "year": "2007",
      "venue": "2007 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "30",
      "title": "Comparative layer-wise analysis of self-supervised speech models",
      "authors": [
        "Ankita Pasad",
        "Bowen Shi",
        "Karen Livescu"
      ],
      "year": "2023",
      "venue": "2023 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "31",
      "title": "Layer-wise analysis of a self-supervised speech representation model",
      "authors": [
        "Ankita Pasad",
        "Ju-Chieh Chou",
        "Karen Livescu"
      ],
      "year": "2021",
      "venue": "2021 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)"
    }
  ]
}