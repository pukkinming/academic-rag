{
  "paper_id": "2412.12498v3",
  "title": "Hierarchical Control Of Emotion Rendering In Speech Synthesis",
  "published": "2024-12-17T03:02:05Z",
  "authors": [
    "Sho Inoue",
    "Kun Zhou",
    "Shuai Wang",
    "Haizhou Li"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Emotional text-to-speech synthesis (TTS) aims to generate realistic emotional speech from input text. However, quantitatively controlling multi-level emotion rendering remains challenging. In this paper, we propose a flow-matching based emotional TTS framework with a novel approach for emotion intensity modeling to facilitate fine-grained control over emotion rendering at the phoneme, word, and utterance levels. We introduce a hierarchical emotion distribution (ED) extractor that captures a quantifiable ED embedding across different speech segment levels. Additionally, we explore various acoustic features and assess their impact on emotion intensity modeling. During TTS training, the hierarchical ED embedding effectively captures the variance in emotion intensity from the reference audio and correlates it with linguistic and speaker information. The TTS model not only generates emotional speech during inference, but also quantitatively controls the emotion rendering over the speech constituents. Both objective and subjective evaluations demonstrate the effectiveness of our framework in terms of speech quality, emotional expressiveness, and hierarchical emotion control.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "N Eural text-to-speech (TTS) systems have significantly improved the naturalness of synthesized speech but still struggle with human-like emotional expressions  [1] ,  [2] . We focus on emotional TTS research to produce realistic emotional speech from text input. Emotional TTS has become a vital technology in human-computer interactions  [3] ,  [4] , providing more responsive and natural communication. Emotional TTS supports diverse applications, including virtual assistants, customer support systems, and interactive gaming  [5] .\n\nThe major challenge of emotional TTS is the modeling of the one-to-many function. Unlike traditional TTS systems, Fig.  1 : An example of hierarchical emotion rendering control shows that emotions can be applied either across the entire utterance (ii) or vary at the level of individual words (iii). which primarily focus on capturing phonetic variations across speakers  [6] -  [8] , emotional TTS aims to render diverse emotional styles over text  [9] . As a form of speech expressiveness, speech emotion not only correlates to multiple prosodic features including pitch, energy and speech rate  [10] -  [12] , but also varies among individuals and languages  [13] -  [15] . Emotional TTS studies have benefited from advances in emotional representation learning, evolving from hand-crafted features in tools like OpenSMILE  [16]  to deep emotional features  [17]  and self-supervised learning (SSL) representations  [18] ,  [19] . The improved generalizability of these features enhances the performance of emotional TTS systems. However, controlling the emotion rendering remains a challenging topic.\n\nPrevious emotional TTS frameworks typically treat emotion as a global speech attribute  [20] ,  [21] . However, these approaches often lead to synthesized voices with average emotional styles, providing limited control over emotion. Few studies have focused on controlling emotion intensities by manipulating emotion embedding through interpolation or scaling  [22] -  [25] . Recent research proposes to model emotion as a relative attribute  [26]  by comparing neutral and emotional speech pairs  [27] -  [32] . This method allows for more precise control of emotion intensity  [27] ,  [28]  and facilitates the creation of mixed emotional styles  [29] . However, the above studies are designed to handle emotions primarily at the utterance level, which does not sufficiently accommodate the finer nuances of emotion that can vary significantly within a single utterance due to context, phrasing, or individual words. The hierarchical structure of speech emotions, characterized by prosodic patterns at the phoneme, word, and utterance levels  [33] ,  [34] , includes global prosodic elements such as tempo and speaker traits  [35] , as well as local factors like pitch and lexical emphasis  [36] -  [41] . These elements are crucial for conveying the subtleties of emotional nuances. Therefore, a quantitative approach to controlling multi-level emotion rendering is essential.\n\nIn this paper, motivated by prior studies that emphasize the hierarchical nature of speech emotions  [33] ,  [35] ,  [36] , we introduce a novel emotional TTS framework that explicitly adopts hierarchical emotion modeling and futher enhance the run-time emotion control. We enable users to flexibly adjust not only the style and intensity of emotions, but also their modulation at the phoneme, phrase, or sentence level. Our approach focuses on capturing and controlling the complex emotional layers and transitions that naturally occur in human speech. During training, the framework leverages reference audio to learn hierarchical emotional variations and correlate them with linguistic and speaker information, allowing a fine-grained and quantifiable control over emotion rendering during inference. Compared to our previous work  [31] ,  [32] , we present a robust hierarchical emotion distribution modeling approach that enables finer-grained emotion representations. By integrating multi-level acoustic features, our model captures local emotional nuances through lower-level features and broader emotional contexts via higher-level selfsupervised representations. Additionally, we explore the use of deep neural networks as a replacement for traditional SVMbased models, leading to improved precision and flexibility in emotion intensity modeling. Our novel contributions are summarized as follows:\n\n• We introduce a flow-matching based emotional TTS framework that facilitates fine-grained and quantitative control over emotion rendering at the phoneme, word, and utterance levels; • We design a hierarchical emotion distribution (ED) extraction module, which captures quantifiable ED embeddings across different speech segments. This ED embedding is used to guide the TTS system to recover the reference emotion during training is used to adjust the emotion variance across the speech constituents; • Through comprehensive experiments, we explore different acoustic representations and emotion intensity modeling methods, evaluating their effectiveness in emotion intensity modeling and their impact on emotion control.\n\nThe rest of this paper is organized as follows: In Section 2, we introduce related works. Section 3 describes our proposed methodology. In Section 4, we introduce our experiment setup.\n\nIn section 5, we analyze the results and Section 6 concludes our study. We placed speech demos on the project page 1 .\n\n1 Project Page: https://github.com/shinshoji01/HED-project-page II. RELATED WORKS In this section, we briefly introduce related studies to set the stage for our research and highlight the novelty of our contributions. We first discuss the hierarchical structure of speech emotions, which motivates the use of multi-level representations to better capture emotional nuances. We review related studies on prosody modeling and emotion intensity control in TTS, both of which aim to synthesize speech that varies naturally in pitch, rhythm, and emotional strength. Finally, we highlight how diffusion models have recently been integrated into TTS, inspiring our use in this research.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "A. Speech Emotion And Its Hierarchical Structure",
      "text": "Speech attributes exhibit a hierarchical nature  [33] ,  [35] ,  [36] ,  [42] , with emotional expressions characterized at the utterance, word, and phoneme levels. At the utterance level, emotions are typically conveyed through global prosodic patterns such as pitch features (including contour, range, mean, and intonation), tempo, and rhythm  [35] . At the word level, lexical information contributes to the emotional tone of a word  [36] ,  [43] , where emphasis on words with emotional valence can amplify emotional intensity  [37] . A study  [38]  shows that listeners are more likely to rely on prosodic cues when there is a conflict between perceived lexical and prosodic emotions, highlighting the need to assign unexpected emotions to words for comprehensive emotion control. At the phoneme level, emotion is expressed through the prosodic features of individual speech sounds, such as pitch, energy, and duration  [33] . Various studies have demonstrated the existence of emotion at the phoneme level  [39] -  [41] . This hierarchical emotion structure underscores the importance of considering multi-level emotional information for effective emotion modeling and control.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "B. Prosody Modeling In Tts",
      "text": "Research in expressive Text-to-Speech (TTS) synthesis has demonstrated the effectiveness of multi-level prosody modeling in enhancing speech expressiveness. Some studies utilize a reference encoder, such as Global Style Tokens (GST)  [44]  and Emotion-enhanced GST  [20]  to improve emotionrelated prosody modeling. These frameworks have been further advanced through semi-supervised training to link GST tokens with emotion labels  [21] , alongside other methods for improving expressive TTS quality  [45] -  [47] . Particularly, He et al.  [45]  addressed the limitation of emotional dataset scarcity through semi-supervised training of graph neural networks. Several studies adopt multi-level prosodic features to enhance the performance of TTS. For example, phonelevel content-style disentanglement  [48]  and multi-resolution VAEs  [49]  have been successfully applied to generate multiscale style embeddings, which are then integrated into TTS models based on VAE  [50] , Vision Transformers (VITs)  [51] ,  [52] , or Diffusion with GANs  [53]  to accelerate the decoding process, FastDiff  [54] , which employs a noise scheduling algorithm  [55]  to reduce steps, and MatchaTTS  [56]  which utilizes optimal-transport conditional flow matching (OT-CFM)  [57]  to enhance synthesis speed, aim to improve efficiency. In our experiments, we adopt MatchaTTS as the backbone model due to its lightweight framework design and state-of-the-art performance among neural TTS systems.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Iii. Methodology",
      "text": "Our proposed framework comprises a text encoder, a speaker encoder, a hierarchical emotion distribution (ED) extraction module, and a flow-prediction network, as illustrated in Fig.  2 (b, c ). Given an audio input and its transcript, the hierarchical emotion distribution (ED) extraction module produces a hierarchical ED, which can be modified according to the user's intentions during inference. The speaker encoder, built using Resemblyzer 2  [58] , extracts speaker embeddings. The text encoder generates linguistic embeddings from the input text. Together with the speaker and hierarchical ED embeddings, these embeddings serve as conditions for the flow-prediction network, which iteratively predicts the melspectrogram using a diffusion approach. During TTS training, the hierarchical ED serves as 'soft labels' to the flowprediction model, guiding the model to learn fine-grained emotional information in a quantifiable and hierarchical manner.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "A. Hierarchical Emotion Distribution (Ed) Extraction",
      "text": "The hierarchical ED extraction module comprises speech segmentation and three identical emotion intensity extraction modules for each segment, as illustrated in Fig.  3 (a) . Given input audio and transcription, the hierarchical ED extraction module yields the emotion distribution for any segment. The extraction module is applied at the phoneme, word, and utterance levels, with the resulting features concatenated to create the hierarchical ED (Fig.  4 ). As shown in Fig.  3 (b), the 2 Resemblyzer: https://github.com/resemble-ai/Resemblyzer emotion intensity extraction module comprises an acoustic feature extraction module, a normalization layer, a trained feature extractor, and a classifier module. The acoustic features are derived from the raw input audio using the acoustic feature extraction modules such as OpenSMILE  [16]  or selfsupervised learning (SSL) encoders (like WavLM  [18]  and Hubert  [19] ). Acoustic features are standardized across audio samples in the training dataset. The feature extractor comprises two fully connected layers separated by a ReLU activation layer. After feature extraction, the classifier module determines the classification probability, which we regard as emotion intensity.\n\nWe develop two types of classifier modules: Speech Emotion Recognizer (SER) and Emotion Presence Recognizer (EPR). The SER, a single fully-connected network (FCN), predicts emotion and has an output size of four, corresponding to the four emotions. In contrast, each FCN in the EPR predicts whether the input corresponds to a specific emotion. For instance, the red FCN in Fig.  3(d ) identifies whether the input is Angry or Non-Angry. We adjusted the softmax function's temperature to prevent excessive confidence in emotion prediction, which skews probability values close to 0.0 or 1.0. We employ a modified Softmax function, denoted as s(z i ) =\n\nwhere α is the constant value to control entropy in the softmax distribution, i, j ∈ {0, 1, ..., K-1} are the indices of the output layer's nodes, and K is the number of classes (emotions). Varying α from 1.1 to 3.0 in 0.1 increments, we calculated the KL divergence between the uniform distribution and the emotion intensity distribution of training samples. We then selected the α with the lowest KL divergence score to smooth the distribution of emotion intensities. For each audio sample, we apply emotion intensity extraction at utterance, word, and phoneme levels. This α range meaningfully modifies the softmax function (since α = 1.0 yields no change) and mit-  In TTS training, reference audio and its transcription serve as inputs. From the audio, we extract hierarchical ED and speaker embedding, which are then concatenated with linguistic embedding. This embedding is expanded based on phoneme-level duration to compute the predicted average mel-spectrogram, µ. This µ conditions the diffusion decoding process in a flow-prediction network  [56] , taking inputs",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "C. Hierarchical Emotion Control",
      "text": "During inference, our framework supports hierarchical, user-driven control over emotion rendering in any given utterance (\"emotion transfer\"), as illustrated in Fig.  2 (a), one can increase the intensity of a \"Sad\" emotion for a single phoneme, thereby achieving fine-grained emotion control.\n\nGiven any unseen {audio, text} inputs, we begin by extracting a hierarchical emotion distribution (ED) from the input audio. Building on the hierarchical emotion modeling principles, our framework allows users to adjust the ED at three segmental levels-phoneme, word, and sentence-to fine-tune emotional rendering. These user modifications to the ED are then combined with linguistic embeddings from the text encoder, guiding the generative process within our flowmatching based framework. Conditioned on the average melspectrogram µ, the flow-prediction network iteratively refines a noise-initialized sample into a mel-spectrogram that reflects the user's chosen emotional adjustments. In the experimental sections, we will demonstrate how these manual, hierarchical manipulations translate into practical, quantifiable emotional changes in the synthesized speech, showcasing the flexibility and effectiveness of our approach.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Iv. Experiment Setup",
      "text": "We conducted all the experiments with the Emotional Speech Dataset (ESD)  [59] ,  [60] , which consists of over 29 hours of Chinese and English emotional speech categorized into five emotions: Neutral, Angry, Happy, Sad, and Surprise. We exclusively use the English subset from ESD, all sampled at 16 kHz. For each speaker and emotion, we used 300, 20, and 30 speech samples for the training, validation, and test datasets, respectively, following the division used in the ESD dataset. We sampled random noise x 0 from the Gaussian distribution several times to generate speech samples under the same condition. We generated five samples per test case from 200 samples in the ESD dataset for experiments on speech intelligibility and expressiveness, resulting in 1000 evaluation samples. For other experiments, each of the 50 samples was generated twice, totaling 100 samples.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "A. Model Architecture",
      "text": "We adopted MatchaTTS  [56]  as our backbone TTS framework, which consists of a text encoder, a duration predictor, and a decoder. The text encoder, built on a transformer network  [61] , converts phoneme sequences into linguistic embeddings. The duration predictor, comprising two convolutional layers followed by a projection layer, estimates the required frame count for each phoneme. The decoder is a U-Net architecture with 1D convolutional residual blocks that perform both downsampling and upsampling, with a Transformer block following each residual block. The TTS system is trained using optimal-transport conditional flow matching (OT-CFM)  [57] . We also made two key modifications to the original MatchaTTS: (1) phoneme alignment was derived from Montreal Forced Alignment (MFA)  [62]  to reduce computational demand; and (2) the decoder's parameter set was increased from 16M to 160M to accommodate emotion conditioning, multiple speakers, and an expanded output of mel-spectrogram dimension from 80 to 100.\n\nTo enable multi-speaker scenarios, we integrated speaker embeddings into the output of the linguistic encoder, using Resemblyzer  [63]  for speaker encoding. To enhance speaker disentanglement during training, we extracted speaker embeddings from different audio samples corresponding to the same speaker as in the reference audio. A fully connected layer was then used to process these speaker embeddings along with hierarchical emotion distributions, ensuring alignment with the text encoder's output size. For vocoding, we employed Vocos  [64] , which synthesizes audio signals from melspectrograms. The Mel-spectrogram is configured with 100 dimensions, an FFT size of 1024, and a hop length of 256. The audio samples were downsampled to 16kHz.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "B. Training Configuration",
      "text": "We first trained classifiers for the hierarchical Emotion Distribution (ED) extraction module and then used the extracted hierarchical ED to train the emotion Text-to-Speech (TTS) system. For the emotion classifier, we employed the Adam optimizer  [65]  with an initial learning rate of 0.001, set the batch size to 16, and utilized a scheduler that reduces the learning rate by 0.8 every five steps. The loss function used was cross-entropy, with loss weights adjusted based on the segment count.\n\nTo remove the speaker and the gender information from the emotion intensity, we integrated an adversarial classifier with a Gradient Reversal Layer (GRL)  [66] ,  [67] . The classifier receives the averaged outputs from the shared feature extractor to predict speaker or gender labels. By reversing its gradients (scaled by 0.5), the extractor is trained to suppress speaker-and gender-related cues, promoting disentangled representations. This enhances speaker invariance in our multi-level emotion intensity framework, allowing both the Emotion Presence Recognizer (EPR) and Speech Emotion Recognizer (SER) to better focus on emotional content. We selected the model that achieved the highest validation accuracy in emotion classification and approached random classification accuracy (e.g., 0.2 for a five-label classification) in speaker/gender prediction. To ensure stability in GRL training, we fixed the feature extractor and trained only the classifier for 100 epochs, followed by an evaluation of the validation datasets.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "C. Model Comparison",
      "text": "We provide a thorough comparison between various baseline models and the proposed models under different configurations. We incorporated MsEmoTTS  [30]  (\"Baseline MsE-moTTS\") and SVM-based Hierarchical Emotion Distribution  [31] ,  [32]  (\"SVM-based HED\") into MatchaTTS  [56]  as baseline models. Note that both baseline models utilize OpenS-MILE to extract 88-dimensional acoustic features and relative attributes  [26]  for quantifying emotion intensity. \"Baseline MsEmoTTS\" uses a global emotion label and phoneme-level intensity as conditioning for the backbone TTS framework, whereas \"SVM-based HED\" employs a hierarchical emotion distribution across phone, word, and utterance levels.\n\nAs for our proposed models, we first compare two classifier modules settings: Speech Emotion Recognizer (SER) (\"Proposed w/ SER\") and Emotion Presence Recognizer (EPR) (\"Proposed w/ EPR\"). In terms of acoustic features, the proposed framework combined OpenSMILE and WavLM to enhance both global and local representations. Specifically, WavLM was employed to extract utterance-level emotion distribution, and OpenSMILE was used for words and phonemes.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "V. Results",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "A. Objective Evaluation",
      "text": "We first report the results of objective evaluation to assess the performance in terms of speech expressiveness, emotion controllability, speaker similarity and speech intelligibility. We assessed emotion similarity by measuring the prosodic differences between the synthesized and the reference audio. We employ three objective metrics: (a) Mel-Cepstral Distortion (MCD)  [29] ,  [68]  for spectral similarity, (b) Pitch, and (c) Energy Distortion for prosody alignment. The results are summarized in Table  I . We observe that our proposed framework with emotion presence recognizer (\"Proposed w/ EPR\") outperformed other models. In the meanwhile, \"Baseline MsEmoTTS\" consistently achieves the worst performance among all the frameworks. These results suggest that our proposed framework with emotion presence recognizer (\"Proposed w/ EPR\") could achieve better performance in terms of the emotional similarity with the reference emotion.\n\n2) Emotion Controllability: We evaluated utterance-level emotion controllability with a pre-trained speech emotion recognizer (SER). We trained the SER model  [69]  on the ESD dataset, with data augmentation of Gaussian noise and time and frequency masking strategy  [70] -  [74] .\n\nWe used a Speech Emotion Recognition (SER) system to evaluate synthesized audio samples, where we varied the intensity of one target emotion from 0.0 to 1.0 with an increment of 0.2. All other emotions were kept at a constant intensity. To analyze the SER predictions, we calculated three metrics: \"Positive\", \"Negative\", and \"Score\". \"Positive\" represents how well the SER system's predictions match the intended emotion. It measures the average correlation between the target emotion's intensity and the SER's prediction for the same emotion. For example, if we increase the intensity of \"Angry\", the SER should increase its \"Angry\" prediction accordingly. \"Negative\" represents how much the SER system confuses the target emotion with other emotions. We assign zero to any negative correlations to emphasize when the SER makes incorrect predictions. \"Score\" is a final metric we calculate by subtracting the Negative value from the Positive value. It reflects the overall accuracy of the SER in predicting the target emotion while avoiding confusion with other emotions.\n\nAs shown in Table  III , the baseline model, \"SVM-based HED\", consistently demonstrates the worst performance across all three metrics. In contrast, the \"Proposed w/ EPR\" model outperforms the others in both the Positive and Negative metrics. This suggests that our proposed approach with EPR provides strong control over emotional expression in synthesized audio, yielding both higher accuracy and fewer errors. Meanwhile, the \"Proposed w/ SER\" model achieves the best performance on the Negative metric, indicating superior han-dling of misaligned emotions.\n\n3) Speaker Similarity: In this experiment, we use pretrained speaker verification models to assess the speaker similarity between the synthesized and reference speech. We employ two models: WeSpeaker 3    [75]  and WavLM Base with X-vector 4    [18] . We then calculate the Speaker Encoding Cosine Similarity (SECS), as reported in Table  I . We observe that the proposed models outperform the two baseline models in both speaker verification models as shown in Table  I . The SECS scores for \"Proposed w/ SER\" and \"Proposed w/ EPR\" show only minor differences, suggesting that the hierarchical emotion distribution (ED) generated by our method contributes to superior speaker disentanglement performance.\n\n4) Speech Intelligibility: We further conduct objective evaluation to assess the intelligibility of synthesized speech. We employ Whisper 5    [76]  to generate the transcriptions from the synthesized speech and calculate word error rate (WER) with the ground-truth transcriptions. For both baseline and proposed models, we derived the emotion representation from the reference audio. We also compare WER using different reference speakers to assess the impact of speaker leakage on speech intelligibility. We present the results in Table  I . The \"Proposed w/ EPR\" model outperformed other models when using the original speaker. Interestingly, when using different speakers, the WER scores of our model remained comparable to those obtained with the original speaker, unlike the baseline (SVM+HED). This performance underscores the effectiveness of our speaker disentanglement approach in maintaining speech intelligibility across different speakers.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "B. Subjective Evaluation",
      "text": "We conducted two subjective evaluations: the MUSHRA Test and the Best Worst Scaling (BWS) Test  [77]  with 20 English speakers. Each speaker listens to 386 synthesized samples in total.\n\n1) Speech Naturalness and Emotion Similarity: In our MUSHRA Test, evaluators rated each audio sample from 0 to 100 on speech naturalness and emotion similarity. Participants are asked to focus on the speech naturalness and the emotional style similarity between the reference speech, respectively. The Results summarized in Table IV indicate that \"Proposed w/ EPR\" outperformed the others in speech naturalness and emotion similarity. TABLE II: BWS test results for evaluating emotion controllability at utterance and word levels: The value represents evaluator preferences (%), with blue and orange indicating the heatmap for audio samples selected as the least and the most expressive, respectively. 'Ang', 'Hap', 'Sad', 'Sur', and 'Avg' are abbreviations for 'Angry', 'Happy', 'Sad', 'Surprise', and 'Average', respectively. SVM-based HED (Baseline)  [31]  Proposed   2) Emotion Controllability: We performed the BWS test to assess the controllability of emotion at both word and utterance levels. We randomly selected the audio sample from the test dataset and edited the emotion intensities to three values (0.0, 0.5, 1.0) to synthesize audio samples with low, normal, and high emotion intensities. Then, we instructed human evaluators to identify the least and the most expressive samples among the three. At the word level, we adjusted the intensity for three words with the longest phoneme sequences. We summarized the BWS test results in Table  II . Our proposed model with EPR produced the most distinguishable outcomes across all four emotions (Angry, Happy, Sad, and Surprise), with listeners consistently identifying the least expressive sample at the lowest intensity and the most expressive at the highest, especially at the utterance level. These findings highlight the effectiveness of our method in controlling emotion intensity across different speech segmental levels.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "C. Ablation Study",
      "text": "We conducted ablation studies to validate the effectiveness of each component in our proposed system. Table V displays comprehensive objective evaluation results from these studies, encompassing metrics such as Word Error Rate (WER), speech expressiveness, speaker similarity, and emotion controllability. Additionally, Table  VI  presents results from the Best Worst Scaling (BWS) test, specifically assessing the emotion controllability of models within the ablation study framework.\n\n1) Impact of Acoustic Features: Acoustic features play a pivotal role in our system, particularly the combined use of OpenSMILE and WavLM in our main experiment, referred to as \"Combination\". We evaluated the effectiveness of \"Combination\" compared to the individual use of \"OpenSMILE\" and \"WavLM\". According to objective evaluations and the BWS results, \"Combination\" outperformed the individual features across most metrics (Tables  V  and VI ). Interestingly, while \"WavLM\" surpassed \"OpenSMILE\" in utterance-level emotion controllability, \"OpenSMILE\" was more effective than \"WavLM\" at capturing word-level perceived emotion intensity. This highlights the synergistic benefit of combining two acoustic features in enhancing the overall system performance.\n\n2) Impact of Emotion Intensity Extraction Methods: We explored various methods for extracting emotion intensities, including our proposed Emotion Presence Recognition (EPR) model and the Speech Emotion Recognition (SER) model, and compared them against a baseline Support Vector Machine (SVM) model used for hierarchical emotion detection  [31] ,  [32] . To ensure a fair comparison, all models utilized the same acoustic feature set, OpenSMILE. Our findings show that \"EPR\" outperforms in metrics such as Word Error Rate (WER), speaker similarity, and utterance-level emotion control. Conversely, \"SVM\" excels in speech expressiveness. In the Best Worst Scaling (BWS) test, both \"EPR\" and \"SER\" significantly outperformed \"SVM\", as documented in Tables  II  and VI .  3) Impact of Gradient Reverse Layers: We investigate the use of Gradient Reverse Layer (GRL)  [66]  to enhance speaker disentanglement. We tested 4 different settings in our proposed framework: \"EPR w/o GRL\", \"EPR w/ GRL\", \"SER w/o GRL\", and \"SER w/ GRL\". The results indicated that EPR-based models consistently outperformed SER-based models across all metrics. Notably, the inclusion of GRL generally enhanced the performance of the models, particularly contributing to improvements in the Speaker Emotion Classification Score (SECS). This underscores the effectiveness of GRL in facilitating speaker disentanglement.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "4) Impact Of Different Self-Supervised Learning Features:",
      "text": "We further explored the effects of different self-supervised learning (SSL) features including WavLM and Hubert  [19] . Additionally, we evaluated the performance of combining these SSL features with OpenSMILE features, resulting in combinations such as \"C-WavLM\" and \"C-HuBERT,\" similar to the approach used in our proposed methods. In this context, \"C\" denotes the integration of emotion distributions derived from each SSL feature at the utterance level, combined with OpenSMILE features applied at the phoneme and word levels. As shown in Table  V , we observe that SSL features, when combined using our proposed method, consistently outperform those using single features. Specifically, C-HuBERT demonstrates superior performance in WER, SECS, and SER, while C-WavLM excels in capturing emotional expressiveness. Our findings demonstrate that combined acoustic features consistently outperform the individual SSL features. Both WavLM and Hubert showed comparable performance across most metrics. We opted to employ WavLM in our systems primarily due to its superior emotion controllability.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "D. Analysis Of Hierarchical Emotion Distribution",
      "text": "We analyze the hierarchical distribution of emotions by assessing its alignment with ground-truth labels and exploring the disentanglement of speaker characteristics.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "1) Emotion Classification Accuracy:",
      "text": "We evaluated emotion classifiers through binary classification for emotion presence using SVM-based and EPR-based models. Each classifier determines whether a speech segment conveys a specific emotion. Table  VII  summarizes the accuracy under different conditions, where \"U\", \"W\", and \"P\" correspond to utterance-,  word-, and phoneme-level segments, respectively. Our results demonstrate that acoustic features critically influence performance; WavLM outperforms OpenSMILE. Longer segments yield higher accuracy, although WavLM exhibits a larger drop from utterance-to word-and phoneme-level segments. Additionally, the EPR-based model without GRL performs comparably to the SVM baseline, whereas the EPR with GRL shows reduced accuracy, likely due to adversarial learning of speaker and gender attributes. Subsequently, we examined the alignment between the hierarchical emotion distribution and the ground-truth labels by computing the ratio of instances in which the emotion with the highest intensity matches the ground-truth label. Table VIII presents these results, which follow trends similar to those observed in the emotion presence binary classification, with SER values comparable to the EPR models.\n\n2) Speaker Disentanglement: We evaluated speaker disentanglement to quantify speaker leakage in the hierarchical ED, a critical factor for improving emotion representation and controllability. We employed various metrics  [78] -  [81] . In the information-based evaluation, we computed the Mutual Information Gap (MIG)  [79] . For each factor v i , we estimated the mutual information with latent code z j via where b v and b z denote bins in the factor and code spaces, respectively. They defined I(v i , z ⋆ ) as the highest and I(v i , z • ) as the second highest mutual information, and computed the MIG as\n\nwith H(v i ) representing the entropy of v i . The overall MIG score is the average over all factors. In our study, the latent code is derived from a phoneme-level segment of the hierarchical ED with shape (1, 12), treating each segment (originally of shape (# of phonemes, 12)) as independent, while the factor corresponds to speaker labels from 10 speakers. We varied the bin count (30, 50, and 100) to ensure reliable estimation. We computed predictor-based metrics, namely Disentanglement  [80]  and Explicitness  [81]  scores, using Random Forest and Lasso Logistic Regressor classifiers, following the approach in  [80] . We extracted sample-level acoustic features comprising utterance-level emotion intensities and the characteristics of word-level and phoneme-level emotion intensities.\n\nFor the latter, we computed features including basic statistics (mean, median, standard deviation, maximum, minimum, and interquartile range), linear trend (slope), peak-based metrics (number of peaks and average peak prominence), and first-lag autocorrelation.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Vi. Discussion",
      "text": "",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "A. Analysis Of Emotion Controllability",
      "text": "We further analyzed our model's controllability over both global and local emotional variations, evaluating it at the utterance and word levels. In line with our previous setup, the emotional intensity of words and phonemes remained consistent. At the word level, we adjusted the emotional intensities for the three words with the longest phonemes, while keeping the utterance intensity constant. We gradually increased the emotion intensity from 0.0 to 1.0, measuring various prosodic features, including duration, pitch (mean and standard deviation), and energy (mean and standard deviation), as illustrated in Fig.  6 . These prosodic attributes are strongly correlated with speech emotion, as noted in previous studies  [82] . For example, anger often manifests in a slower speaking rate and higher values for energy mean/standard deviation.\n\nWe analyzed the relationship between acoustic features and various emotions, as illustrated in Fig.  6  using color to represent positive and negative correlations. A red background indicates an expected negative trend, while a blue background signifies a positive trend with increasing intensity, based on our data analysis from the ESD dataset. For example, anger is typically associated with a slower speaking rate and higher values in both the mean and standard deviation of energy. Our findings show that synthesized emotional speech closely follows the anticipated trends in most prosodic features. Specifically, we observed a positive correlation between happiness intensity Fig.  7 : Spectrograms of synthesized audio samples across different emotion intensities with pitch (blue) and energy (black) contours: the y-axis for energy contours are not relevant. and mean pitch, as well as a positive correlation between anger and both the mean and standard deviation of energy. These results demonstrate that our model effectively adjusts acoustic features in response to varying levels of emotional intensity. Fig.  7  shows the spectrograms of synthesized audio samples with varying emotion intensities. Pitch and energy contours are represented by blue and black lines, respectively. Note that the y-axis for the energy contours is not relevant. Each row corresponds to a different emotion, with the first column depicting acoustic features at an emotion intensity of 0.0, and the second column at 1.0. The three highlighted areas indicate regions where the intensity has been modified. For anger, we observe more pronounced energy spikes, especially in the first and third words, at higher intensities. In happiness, pitch and energy patterns are similar, with higher pitch values at an intensity of 1.0. Sadness is marked by a longer duration and a decline in the pitch contour as intensity increases. For surprise, we note a rise in pitch contours along with a slight energy spike. This demonstrates that our model can effectively manipulate pitch and energy contours in response to changes in emotion intensity.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "B. Future Work",
      "text": "Since the dataset used in our study is content-parallel, our hierarchical architecture (at the utterance, word, and phoneme levels) primarily focuses on modeling acoustic dynamics. We acknowledge that lexical content naturally conveys prosodic cues that influence emotional expression, particularly along the valence dimension  [83] . In future work, we plan to further explore the role of linguistic prosody to enhance emotion control. We also aim to extend our model to naturalistic and multi-modal signals (e.g., visual cues  [84] ), thereby improving its applicability in real-world affective speech synthesis.",
      "page_start": 10,
      "page_end": 11
    },
    {
      "section_name": "Vii. Conclusion",
      "text": "In this work, we introduce a flow-matching based emotional text-to-speech framework that offers hierarchical emotion intensity control, enabling precise modulation of emotion rendering at the phoneme, word, and utterance levels. By developing a quantifiable emotion distribution embedding, we provide users with real-time, fine-grained control over emotional rendering during inference. Our investigation of various acoustic features and emotion intensity extractor architectures highlights their significant influence on synthesis quality. Both subjective and objective evaluations demonstrate the effectiveness of our model in achieving high-quality, expressive, and controllable synthesized speech.",
      "page_start": 11,
      "page_end": 11
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: An example of hierarchical emotion rendering control",
      "page": 1
    },
    {
      "caption": "Figure 2: The overall diagram of the proposed TTS framework. (a) An example of emotion modification (b) The style conditioning",
      "page": 3
    },
    {
      "caption": "Figure 3: Diagrams of (a) Hierarchical ED Extraction Module; (b) Emotion Intensity Extraction Module; (c) SER classifier module;",
      "page": 4
    },
    {
      "caption": "Figure 4: An example of hierarchical emotion distribution (ED),",
      "page": 4
    },
    {
      "caption": "Figure 5: (b), we freeze the parameters of the acoustic feature",
      "page": 4
    },
    {
      "caption": "Figure 5: (d)) modules. We then extracted the hierarchical",
      "page": 4
    },
    {
      "caption": "Figure 5: Training diagrams emotion intensity extractors (a)",
      "page": 4
    },
    {
      "caption": "Figure 6: The illustration of prosodic variants with changes in intensity. A red background indicates the expected negative trend,",
      "page": 9
    },
    {
      "caption": "Figure 7: Spectrograms of synthesized audio samples across dif-",
      "page": 10
    },
    {
      "caption": "Figure 7: shows the spectrograms of synthesized audio samples",
      "page": 10
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "0.0\nLeast\n0.5\n1.0\nUtterance-Level\n0.0\nMost\n0.5\n1.0": "",
          "47\n59\n41\n88\n59": "28\n12\n25\n6\n18\n19\n22\n28\n0\n17\n16\n16\n53\n3\n22\n25\n44\n25\n28\n30\n53\n34\n16\n62\n41",
          "44\n38\n47\n62\n48": "25\n50\n34\n12\n30\n25\n6\n12\n19\n16\n19\n12\n6\n16\n13\n19\n19\n22\n22\n20\n56\n62\n66\n56\n60",
          "69\n72\n62\n66\n67": "19\n6\n25\n19\n17\n6\n16\n6\n9\n9\n3\n6\n6\n16\n8\n12\n34\n6\n16\n17\n78\n53\n81\n62\n69"
        },
        {
          "0.0\nLeast\n0.5\n1.0\nUtterance-Level\n0.0\nMost\n0.5\n1.0": "0.0\nLeast\n0.5\n1.0\nWord-Level\n0.0\nMost\n0.5\n1.0",
          "47\n59\n41\n88\n59": "",
          "44\n38\n47\n62\n48": "",
          "69\n72\n62\n66\n67": ""
        },
        {
          "0.0\nLeast\n0.5\n1.0\nUtterance-Level\n0.0\nMost\n0.5\n1.0": "",
          "47\n59\n41\n88\n59": "66\n59\n72\n84\n70",
          "44\n38\n47\n62\n48": "91\n47\n72\n97\n77",
          "69\n72\n62\n66\n67": "94\n62\n91\n97\n86"
        },
        {
          "0.0\nLeast\n0.5\n1.0\nUtterance-Level\n0.0\nMost\n0.5\n1.0": "",
          "47\n59\n41\n88\n59": "28\n19\n22\n6\n19\n6\n22\n6\n9\n11\n12\n19\n9\n3\n11\n12\n12\n31\n25\n20",
          "44\n38\n47\n62\n48": "6\n25\n3\n3\n9\n3\n28\n25\n0\n14\n0\n41\n12\n0\n13\n12\n12\n38\n25\n22",
          "69\n72\n62\n66\n67": "3\n38\n9\n3\n13\n3\n0\n0\n0\n1\n0\n6\n0\n0\n2\n16\n9\n19\n31\n19"
        },
        {
          "0.0\nLeast\n0.5\n1.0\nUtterance-Level\n0.0\nMost\n0.5\n1.0": "",
          "47\n59\n41\n88\n59": "75\n69\n59\n72\n69",
          "44\n38\n47\n62\n48": "88\n47\n50\n75\n65",
          "69\n72\n62\n66\n67": "84\n84\n81\n69\n80"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "0.0\nLeast\n0.5\n1.0\nUtterance\n0.0\nMost\n0.5\n1.0": "",
          "66\n78\n69\n53\n66": "16\n12\n19\n0\n12\n12\n3\n6\n41\n16\n16\n3\n12\n19\n12\n25\n28\n16\n38\n27\n53\n62\n66\n38\n55",
          "62\n34\n81\n62\n60\n22\n19\n9\n19\n17\n9\n41\n3\n12\n16\n12\n28\n3\n16\n15\n28\n44\n12\n22\n27\n53\n22\n78\n56\n52": "",
          "53\n31\n47\n88\n55\n19\n16\n34\n3\n18\n22\n47\n12\n3\n21": ""
        },
        {
          "0.0\nLeast\n0.5\n1.0\nUtterance\n0.0\nMost\n0.5\n1.0": "",
          "66\n78\n69\n53\n66": "",
          "62\n34\n81\n62\n60\n22\n19\n9\n19\n17\n9\n41\n3\n12\n16\n12\n28\n3\n16\n15\n28\n44\n12\n22\n27\n53\n22\n78\n56\n52": "",
          "53\n31\n47\n88\n55\n19\n16\n34\n3\n18\n22\n47\n12\n3\n21": "28\n44\n19\n3\n23\n31\n25\n6\n47\n27\n34\n25\n69\n44\n43"
        },
        {
          "0.0\nLeast\n0.5\n1.0\nUtterance\n0.0\nMost\n0.5\n1.0": "0.0\nLeast\n0.5\n1.0\nWord\n0.0\nMost\n0.5\n1.0",
          "66\n78\n69\n53\n66": "",
          "62\n34\n81\n62\n60\n22\n19\n9\n19\n17\n9\n41\n3\n12\n16\n12\n28\n3\n16\n15\n28\n44\n12\n22\n27\n53\n22\n78\n56\n52": "",
          "53\n31\n47\n88\n55\n19\n16\n34\n3\n18\n22\n47\n12\n3\n21": ""
        },
        {
          "0.0\nLeast\n0.5\n1.0\nUtterance\n0.0\nMost\n0.5\n1.0": "",
          "66\n78\n69\n53\n66": "94\n78\n62\n88\n80",
          "62\n34\n81\n62\n60\n22\n19\n9\n19\n17\n9\n41\n3\n12\n16\n12\n28\n3\n16\n15\n28\n44\n12\n22\n27\n53\n22\n78\n56\n52": "62\n75\n69\n69\n69",
          "53\n31\n47\n88\n55\n19\n16\n34\n3\n18\n22\n47\n12\n3\n21": "69\n28\n84\n94\n69\n22\n34\n9\n6\n18\n9\n38\n6\n0\n13\n6\n22\n9\n6\n11\n6\n38\n25\n41\n27\n88\n41\n66\n53\n62"
        },
        {
          "0.0\nLeast\n0.5\n1.0\nUtterance\n0.0\nMost\n0.5\n1.0": "",
          "66\n78\n69\n53\n66": "3\n19\n38\n9\n17\n3\n3\n0\n3\n2\n0\n16\n3\n0\n5\n19\n25\n9\n25\n20",
          "62\n34\n81\n62\n60\n22\n19\n9\n19\n17\n9\n41\n3\n12\n16\n12\n28\n3\n16\n15\n28\n44\n12\n22\n27\n53\n22\n78\n56\n52": "25\n12\n16\n19\n18\n12\n12\n16\n12\n13\n16\n6\n3\n0\n6\n19\n47\n56\n22\n36\n66\n47\n41\n78\n58",
          "53\n31\n47\n88\n55\n19\n16\n34\n3\n18\n22\n47\n12\n3\n21": ""
        },
        {
          "0.0\nLeast\n0.5\n1.0\nUtterance\n0.0\nMost\n0.5\n1.0": "",
          "66\n78\n69\n53\n66": "81\n59\n88\n75\n76",
          "62\n34\n81\n62\n60\n22\n19\n9\n19\n17\n9\n41\n3\n12\n16\n12\n28\n3\n16\n15\n28\n44\n12\n22\n27\n53\n22\n78\n56\n52": "",
          "53\n31\n47\n88\n55\n19\n16\n34\n3\n18\n22\n47\n12\n3\n21": ""
        }
      ],
      "page": 8
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "An overview of affective speech synthesis and conversion in the deep learning era",
      "authors": [
        "A Triantafyllopoulos",
        "B Schuller",
        "G İymen",
        "M Sezgin",
        "X He",
        "Z Yang",
        "P Tzirakis",
        "S Liu",
        "S Mertes",
        "E André"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE"
    },
    {
      "citation_id": "2",
      "title": "The age of artificial emotional intelligence",
      "authors": [
        "D Schuller",
        "B Schuller"
      ],
      "year": "2018",
      "venue": "Computer"
    },
    {
      "citation_id": "3",
      "title": "Handling emotions in human-computer dialogues",
      "authors": [
        "J Pittermann",
        "A Pittermann",
        "W Minker"
      ],
      "year": "2010",
      "venue": "Handling emotions in human-computer dialogues"
    },
    {
      "citation_id": "4",
      "title": "Emotion modelling for speech generation",
      "authors": [
        "Z Kun"
      ],
      "year": "2022",
      "venue": "Emotion modelling for speech generation"
    },
    {
      "citation_id": "5",
      "title": "An overview of affective speech synthesis and conversion in the deep learning era",
      "authors": [
        "A Triantafyllopoulos",
        "B Schuller",
        "G Iymen",
        "M Sezgin",
        "X He",
        "Z Yang",
        "P Tzirakis",
        "S Liu",
        "S Mertes",
        "E Andr'e",
        "R Fu",
        "J Tao"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE"
    },
    {
      "citation_id": "6",
      "title": "Fastspeech 2: Fast and high-quality end-to-end text to speech",
      "authors": [
        "Y Ren",
        "C Hu",
        "X Tan",
        "T Qin",
        "S Zhao",
        "Z Zhao",
        "T.-Y Liu"
      ],
      "year": "2022",
      "venue": "Fastspeech 2: Fast and high-quality end-to-end text to speech"
    },
    {
      "citation_id": "7",
      "title": "Phonetic enhanced language modeling for text-to-speech synthesis",
      "authors": [
        "K Zhou",
        "S Zhao",
        "Y Ma",
        "C Zhang",
        "H Wang",
        "D Ng",
        "C Ni",
        "T Nguyen",
        "J Yip",
        "B Ma"
      ],
      "venue": "Phonetic enhanced language modeling for text-to-speech synthesis"
    },
    {
      "citation_id": "8",
      "title": "Text-to-speech for low-resource agglutinative language with morphology-aware language model pre-training",
      "authors": [
        "R Liu",
        "Y Hu",
        "H Zuo",
        "Z Luo",
        "L Wang",
        "G Gao"
      ],
      "year": "2024",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "9",
      "title": "Emotional speech synthesis: A review",
      "authors": [
        "M Schröder"
      ],
      "year": "2001",
      "venue": "Seventh European Conference on Speech Communication and Technology"
    },
    {
      "citation_id": "10",
      "title": "A survey on neural speech synthesis",
      "authors": [
        "X Tan",
        "T Qin",
        "F Soong",
        "T.-Y Liu"
      ],
      "year": "2021",
      "venue": "A survey on neural speech synthesis",
      "arxiv": "arXiv:2106.15561"
    },
    {
      "citation_id": "11",
      "title": "The handbook of pragmatics",
      "authors": [
        "J Hirschberg"
      ],
      "year": "2004",
      "venue": "The handbook of pragmatics"
    },
    {
      "citation_id": "12",
      "title": "Perception of affective and linguistic prosody: an ale meta-analysis of neuroimaging studies",
      "authors": [
        "M Belyk",
        "S Brown"
      ],
      "year": "2014",
      "venue": "Social cognitive and affective neuroscience"
    },
    {
      "citation_id": "13",
      "title": "Vocal Communication of Emotion",
      "authors": [
        "P Laukka"
      ],
      "year": "2017",
      "venue": "Vocal Communication of Emotion",
      "doi": "10.1007/978-3-319-28099-8_562-1"
    },
    {
      "citation_id": "14",
      "title": "Vaw-gan for disentanglement and recomposition of emotional elements in speech",
      "authors": [
        "K Zhou",
        "B Sisman",
        "H Li"
      ],
      "year": "2021",
      "venue": "2021 IEEE Spoken Language Technology Workshop (SLT)"
    },
    {
      "citation_id": "15",
      "title": "Converting anyone's emotion: Towards speaker-independent emotional voice conversion",
      "authors": [
        "K Zhou",
        "B Sisman",
        "M Zhang",
        "H Li"
      ],
      "year": "2020",
      "venue": "Proc. Interspeech 2020"
    },
    {
      "citation_id": "16",
      "title": "opensmile -the munich versatile and fast open-source audio feature extractor",
      "authors": [
        "F Eyben",
        "M Wöllmer",
        "B Schuller"
      ],
      "year": "2010",
      "venue": "opensmile -the munich versatile and fast open-source audio feature extractor"
    },
    {
      "citation_id": "17",
      "title": "emotion2vec: Self-supervised pre-training for speech emotion representation",
      "authors": [
        "Z Ma",
        "Z Zheng",
        "J Ye",
        "J Li",
        "Z Gao",
        "S Zhang",
        "X Chen"
      ],
      "year": "2023",
      "venue": "emotion2vec: Self-supervised pre-training for speech emotion representation"
    },
    {
      "citation_id": "18",
      "title": "Wavlm: Large-scale self-supervised pre-training for full stack speech processing",
      "authors": [
        "S Chen",
        "C Wang",
        "Z Chen",
        "Y Wu",
        "S Liu",
        "Z Chen",
        "J Li",
        "N Kanda",
        "T Yoshioka",
        "X Xiao",
        "J Wu",
        "L Zhou",
        "S Ren",
        "Y Qian",
        "Y Qian",
        "M Zeng",
        "F Wei"
      ],
      "year": "2021",
      "venue": "IEEE Journal of Selected Topics in Signal Processing"
    },
    {
      "citation_id": "19",
      "title": "Hubert: Self-supervised speech representation learning by masked prediction of hidden units",
      "authors": [
        "W.-N Hsu",
        "B Bolte",
        "Y.-H Tsai",
        "K Lakhotia",
        "R Salakhutdinov",
        "A Mohamed"
      ],
      "year": "2021",
      "venue": "Hubert: Self-supervised speech representation learning by masked prediction of hidden units"
    },
    {
      "citation_id": "20",
      "title": "Emotion controllable speech synthesis using emotion-unlabeled dataset with the assistance of cross-domain speech emotion recognition",
      "authors": [
        "X Cai",
        "D Dai",
        "Z Wu",
        "X Li",
        "J Li",
        "H Meng"
      ],
      "year": "2021",
      "venue": "Emotion controllable speech synthesis using emotion-unlabeled dataset with the assistance of cross-domain speech emotion recognition"
    },
    {
      "citation_id": "21",
      "title": "End-to-end emotional speech synthesis using style tokens and semi-supervised training",
      "authors": [
        "P Wu",
        "Z Ling",
        "L Liu",
        "Y Jiang",
        "H Wu",
        "L Dai"
      ],
      "year": "2019",
      "venue": "End-to-end emotional speech synthesis using style tokens and semi-supervised training"
    },
    {
      "citation_id": "22",
      "title": "Controllable emotion transfer for end-to-end speech synthesis",
      "authors": [
        "T Li",
        "S Yang",
        "L Xue",
        "L Xie"
      ],
      "year": "2020",
      "venue": "Controllable emotion transfer for end-to-end speech synthesis"
    },
    {
      "citation_id": "23",
      "title": "Semi-supervised learning for continuous emotional intensity controllable speech synthesis with disentangled representations",
      "authors": [
        "Y Oh",
        "J Lee",
        "Y Han",
        "K Lee"
      ],
      "year": "2023",
      "venue": "Semi-supervised learning for continuous emotional intensity controllable speech synthesis with disentangled representations"
    },
    {
      "citation_id": "24",
      "title": "iemotts: Toward robust cross-speaker emotion transfer and control for speech synthesis based on disentanglement between prosody and timbre",
      "authors": [
        "G Zhang",
        "Y Qin",
        "W Zhang",
        "J Wu",
        "M Li",
        "Y Gai",
        "F Jiang",
        "T Lee"
      ],
      "year": "2023",
      "venue": "iemotts: Toward robust cross-speaker emotion transfer and control for speech synthesis based on disentanglement between prosody and timbre"
    },
    {
      "citation_id": "25",
      "title": "Cross-speaker emotion disentangling and transfer for end-to-end speech synthesis",
      "authors": [
        "T Li",
        "X Wang",
        "Q Xie",
        "Z Wang",
        "L Xie"
      ],
      "year": "2022",
      "venue": "Cross-speaker emotion disentangling and transfer for end-to-end speech synthesis"
    },
    {
      "citation_id": "26",
      "title": "Relative attributes",
      "authors": [
        "D Parikh",
        "K Grauman"
      ],
      "year": "2011",
      "venue": "2011 International Conference on Computer Vision. IEEE"
    },
    {
      "citation_id": "27",
      "title": "Emotion intensity and its control for emotional voice conversion",
      "authors": [
        "K Zhou",
        "B Sisman",
        "R Rana",
        "B Schuller",
        "H Li"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "28",
      "title": "Controlling emotion strength with relative attribute for end-to-end speech synthesis",
      "authors": [
        "X Zhu",
        "S Yang",
        "G Yang",
        "L Xie"
      ],
      "year": "2019",
      "venue": "2019 IEEE Automatic Speech Recognition and Understanding Workshop"
    },
    {
      "citation_id": "29",
      "title": "Speech synthesis with mixed emotions",
      "authors": [
        "K Zhou",
        "B Sisman",
        "R Rana",
        "B Schuller",
        "H Li"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "30",
      "title": "Msemotts: Multi-scale emotion transfer, prediction, and control for emotional speech synthesis",
      "authors": [
        "Y Lei",
        "S Yang",
        "X Wang",
        "L Xie"
      ],
      "year": "2022",
      "venue": "Msemotts: Multi-scale emotion transfer, prediction, and control for emotional speech synthesis"
    },
    {
      "citation_id": "31",
      "title": "Hierarchical emotion prediction and control in text-to-speech synthesis",
      "authors": [
        "S Inoue",
        "K Zhou",
        "S Wang",
        "H Li"
      ],
      "year": "2024",
      "venue": "ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "32",
      "title": "Fine-grained quantitative emotion editing for speech generation",
      "year": "2024",
      "venue": "Fine-grained quantitative emotion editing for speech generation"
    },
    {
      "citation_id": "33",
      "title": "Emotion Recognition Using Prosodic Information",
      "authors": [
        "S Krothapalli",
        "S Koolagudi"
      ],
      "year": "2013",
      "venue": "Emotion Recognition Using Prosodic Information",
      "doi": "10.1007/978-1-4614-5143-3_5"
    },
    {
      "citation_id": "34",
      "title": "Transforming spectrum and prosody for emotional voice conversion with non-parallel training data",
      "authors": [
        "K Zhou",
        "B Sisman",
        "H Li"
      ],
      "year": "2020",
      "venue": "Proc. Odyssey 2020 The Speaker and Language Recognition Workshop"
    },
    {
      "citation_id": "35",
      "title": "Intonation and emotion: Influence of pitch levels and contour type on creating emotions",
      "authors": [
        "E Rodero"
      ],
      "year": "2011",
      "venue": "Journal of Voice"
    },
    {
      "citation_id": "36",
      "title": "Norms of valence, arousal, and dominance for 13,915 english lemmas",
      "authors": [
        "A Warriner",
        "V Kuperman",
        "M Brysbaert"
      ],
      "year": "2013",
      "venue": "Behavior research methods"
    },
    {
      "citation_id": "37",
      "title": "The influence of pitch range, duration, amplitude and spectral features on the interpretation of the rise-fall-rise intonation contour in english",
      "authors": [
        "J Hirschberg",
        "G Ward"
      ],
      "year": "1992",
      "venue": "Journal of Phonetics"
    },
    {
      "citation_id": "38",
      "title": "Using prosody to avoid ambiguity: Effects of speaker awareness and referential context",
      "authors": [
        "J Snedeker",
        "J Trueswell"
      ],
      "venue": "Journal of Memory and Language"
    },
    {
      "citation_id": "39",
      "title": "Analysis of emotionally salient aspects of fundamental frequency for emotion detection",
      "authors": [
        "C Busso",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2009",
      "venue": "IEEE Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "40",
      "title": "Survey on speech emotion recognition: Features, classification schemes, and databases",
      "authors": [
        "M Ayadi",
        "M Kamel",
        "F Karray"
      ],
      "year": "2011",
      "venue": "Pattern Recognition"
    },
    {
      "citation_id": "41",
      "title": "Expression of emotional-motivational connotations with a one-word utterance",
      "authors": [
        "L Leinonen",
        "T Hiltunen",
        "I Linnankoski",
        "M.-L Laakso"
      ],
      "year": "1997",
      "venue": "Journal of the Acoustical Society of America",
      "doi": "10.1121/1.420109"
    },
    {
      "citation_id": "42",
      "title": "Controllable accented textto-speech synthesis with fine and coarse-grained intensity rendering",
      "authors": [
        "R Liu",
        "B Sisman",
        "G Gao",
        "H Li"
      ],
      "year": "2024",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "43",
      "title": "Connecting cross-modal representations for compact and robust multimodal sentiment analysis with sentiment word substitution error",
      "authors": [
        "Q Sun",
        "H Zuo",
        "R Liu",
        "H Li"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "44",
      "title": "Style tokens: Unsupervised style modeling, control and transfer in end-to-end speech synthesis",
      "authors": [
        "Y Wang",
        "D Stanton",
        "Y Zhang",
        "R Skerry-Ryan",
        "E Battenberg",
        "J Shor",
        "Y Xiao",
        "F Ren",
        "Y Jia",
        "R Saurous"
      ],
      "year": "2018",
      "venue": "Style tokens: Unsupervised style modeling, control and transfer in end-to-end speech synthesis"
    },
    {
      "citation_id": "45",
      "title": "Improve emotional speech synthesis quality by learning explicit and implicit representations with semi-supervised training",
      "authors": [
        "J He",
        "C Gong",
        "L Wang",
        "X Wang",
        "J Xu",
        "J Dang"
      ],
      "year": "2022",
      "venue": "Improve emotional speech synthesis quality by learning explicit and implicit representations with semi-supervised training"
    },
    {
      "citation_id": "46",
      "title": "Multi-speaker emotional speech synthesis with fine-grained prosody modeling",
      "authors": [
        "C Lu",
        "X Wen",
        "R Liu",
        "X Chen"
      ],
      "year": "2021",
      "venue": "ICASSP 2021 -2021 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "47",
      "title": "Language model-based emotion prediction methods for emotional speech synthesis systems",
      "authors": [
        "H.-W Yoon",
        "O Kwon",
        "H Lee",
        "R Yamamoto",
        "E Song",
        "J.-M Kim",
        "M.-J Hwang"
      ],
      "year": "2022",
      "venue": "Language model-based emotion prediction methods for emotional speech synthesis systems"
    },
    {
      "citation_id": "48",
      "title": "Fine-grained style modeling, transfer and prediction in text-to-speech synthesis via phone-level content-style disentanglement",
      "authors": [
        "D Tan",
        "T Lee"
      ],
      "year": "2021",
      "venue": "Interspeech 2021, ser. interspeech2021. ISCA",
      "doi": "10.21437/Interspeech.2021-1129"
    },
    {
      "citation_id": "49",
      "title": "Hierarchical multi-grained generative model for expressive speech synthesis",
      "authors": [
        "Y Hono",
        "K Tsuboi",
        "K Sawada",
        "K Hashimoto",
        "K Oura",
        "Y Nankaku",
        "K Tokuda"
      ],
      "year": "2021",
      "venue": "Hierarchical multi-grained generative model for expressive speech synthesis"
    },
    {
      "citation_id": "50",
      "title": "Towards multi-scale style control for expressive speech synthesis",
      "authors": [
        "X Li",
        "C Song",
        "J Li",
        "Z Wu",
        "J Jia",
        "H Meng"
      ],
      "year": "2021",
      "venue": "Towards multi-scale style control for expressive speech synthesis"
    },
    {
      "citation_id": "51",
      "title": "Conditional variational autoencoder with adversarial learning for end-to-end text-to-speech",
      "authors": [
        "J Kim",
        "J Kong",
        "J Son"
      ],
      "year": "2021",
      "venue": "Conditional variational autoencoder with adversarial learning for end-to-end text-to-speech"
    },
    {
      "citation_id": "52",
      "title": "An emotion speech synthesis method based on vits",
      "authors": [
        "W Zhao",
        "Z Yang"
      ],
      "venue": "Applied Sciences"
    },
    {
      "citation_id": "53",
      "title": "Generative adversarial networks",
      "authors": [
        "I Goodfellow",
        "J Pouget-Abadie",
        "M Mirza",
        "B Xu",
        "D Warde-Farley",
        "S Ozair",
        "A Courville",
        "Y Bengio"
      ],
      "year": "2014",
      "venue": "Generative adversarial networks"
    },
    {
      "citation_id": "54",
      "title": "FastDiff 2: Revisiting and incorporating GANs and diffusion models in high-fidelity speech synthesis",
      "authors": [
        "R Huang",
        "Y Ren",
        "Z Jiang",
        "C Cui",
        "J Liu",
        "Z Zhao"
      ],
      "year": "2023",
      "venue": "Findings of the Association for Computational Linguistics: ACL 2023"
    },
    {
      "citation_id": "55",
      "title": "Bddm: Bilateral denoising diffusion models for fast and high-quality speech synthesis",
      "authors": [
        "M Lam",
        "J Wang",
        "D Su",
        "D Yu"
      ],
      "year": "2022",
      "venue": "Bddm: Bilateral denoising diffusion models for fast and high-quality speech synthesis"
    },
    {
      "citation_id": "56",
      "title": "Matcha-tts: A fast tts architecture with conditional flow matching",
      "authors": [
        "S Mehta",
        "R Tu",
        "J Beskow",
        "Éva Székely",
        "G Henter"
      ],
      "year": "2024",
      "venue": "Matcha-tts: A fast tts architecture with conditional flow matching"
    },
    {
      "citation_id": "57",
      "title": "Flow matching for generative modeling",
      "authors": [
        "Y Lipman",
        "R Chen",
        "H Ben-Hamu",
        "M Nickel",
        "M Le"
      ],
      "year": "2023",
      "venue": "Flow matching for generative modeling"
    },
    {
      "citation_id": "58",
      "title": "Generalized end-to-end loss for speaker verification",
      "authors": [
        "L Wan",
        "Q Wang",
        "A Papir",
        "I Moreno"
      ],
      "year": "2020",
      "venue": "Generalized end-to-end loss for speaker verification"
    },
    {
      "citation_id": "59",
      "title": "Seen and unseen emotional style transfer for voice conversion with a new emotional speech dataset",
      "authors": [
        "K Zhou",
        "B Sisman",
        "R Liu",
        "H Li"
      ],
      "year": "2021",
      "venue": "ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "60",
      "title": "Emotional voice conversion: Theory, databases and esd",
      "year": "2022",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "61",
      "title": "Attention is all you need",
      "authors": [
        "A Vaswani",
        "N Shazeer",
        "N Parmar",
        "J Uszkoreit",
        "L Jones",
        "A Gomez",
        "L Kaiser",
        "I Polosukhin"
      ],
      "year": "2017",
      "venue": "Attention is all you need"
    },
    {
      "citation_id": "62",
      "title": "Glow-tts: A generative flow for text-to-speech via monotonic alignment search",
      "authors": [
        "J Kim",
        "S Kim",
        "J Kong",
        "S Yoon"
      ],
      "year": "2020",
      "venue": "Glow-tts: A generative flow for text-to-speech via monotonic alignment search"
    },
    {
      "citation_id": "63",
      "title": "Generalized end-to-end loss for speaker verification",
      "authors": [
        "L Wan",
        "Q Wang",
        "A Papir",
        "I Moreno"
      ],
      "year": "2020",
      "venue": "Generalized end-to-end loss for speaker verification"
    },
    {
      "citation_id": "64",
      "title": "Vocos: Closing the gap between time-domain and fourierbased neural vocoders for high-quality audio synthesis",
      "authors": [
        "H Siuzdak"
      ],
      "year": "2023",
      "venue": "Vocos: Closing the gap between time-domain and fourierbased neural vocoders for high-quality audio synthesis"
    },
    {
      "citation_id": "65",
      "title": "Adam: A method for stochastic optimization",
      "authors": [
        "D Kingma",
        "J Ba"
      ],
      "year": "2017",
      "venue": "Adam: A method for stochastic optimization"
    },
    {
      "citation_id": "66",
      "title": "Unsupervised domain adaptation by backpropagation",
      "authors": [
        "Y Ganin",
        "V Lempitsky"
      ],
      "year": "2015",
      "venue": "Unsupervised domain adaptation by backpropagation"
    },
    {
      "citation_id": "67",
      "title": "Adversarial domain generalized transformer for cross-corpus speech emotion recognition",
      "authors": [
        "Y Gao",
        "L Wang",
        "J Liu",
        "J Dang",
        "S Okada"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "68",
      "title": "Mel-cepstral distance measure for objective speech quality assessment",
      "authors": [
        "R Kubichek"
      ],
      "year": "1993",
      "venue": "Proceedings of IEEE Pacific Rim Conference on Communications Computers and Signal Processing"
    },
    {
      "citation_id": "69",
      "title": "3-d convolutional recurrent neural networks with attention model for speech emotion recognition",
      "authors": [
        "M Chen",
        "X He",
        "J Yang",
        "H Zhang"
      ],
      "year": "2018",
      "venue": "IEEE Signal Processing Letters"
    },
    {
      "citation_id": "70",
      "title": "Multi-conditioning and data augmentation using generative noise model for speech emotion recognition in noisy conditions",
      "authors": [
        "U Tiwari",
        "M Soni",
        "R Chakraborty",
        "A Panda",
        "S Kopparapu"
      ],
      "year": "2020",
      "venue": "ICASSP 2020 -2020 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "71",
      "title": "Speech emotion recognition in noisy and reverberant environments",
      "authors": [
        "P Heracleous",
        "K Yasuda",
        "F Sugaya",
        "M Hashimoto",
        "A Yoneyama"
      ],
      "year": "2017",
      "venue": "Speech emotion recognition in noisy and reverberant environments"
    },
    {
      "citation_id": "72",
      "title": "Deep learning techniques for speech emotion recognition",
      "authors": [
        "T Muni",
        "R Tata",
        "J Narasimharao",
        "M Kalipindi",
        "H Arora"
      ],
      "year": "2022",
      "venue": "Deep learning techniques for speech emotion recognition"
    },
    {
      "citation_id": "73",
      "title": "Improved emotion recognition using gaussian mixture model and extreme learning machine in speech and glottal signals",
      "authors": [
        "K Polat",
        "S Yaacob"
      ],
      "venue": "Mathematical Problems in Engineering"
    },
    {
      "citation_id": "74",
      "title": "Specaugment: A simple data augmentation method for automatic speech recognition",
      "authors": [
        "D Park",
        "W Chan",
        "Y Zhang",
        "C.-C Chiu",
        "B Zoph",
        "E Cubuk",
        "Q Le"
      ],
      "year": "2019",
      "venue": "Specaugment: A simple data augmentation method for automatic speech recognition"
    },
    {
      "citation_id": "75",
      "title": "Wespeaker: A research and production oriented speaker embedding learning toolkit",
      "authors": [
        "H Wang",
        "C Liang",
        "S Wang",
        "Z Chen",
        "B Zhang",
        "X Xiang",
        "Y Deng",
        "Y Qian"
      ],
      "year": "2023",
      "venue": "ICASSP 2023 -2023 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "76",
      "title": "Robust speech recognition via large-scale weak supervision",
      "authors": [
        "A Radford",
        "J Kim",
        "T Xu",
        "G Brockman",
        "C Mcleavey",
        "I Sutskever"
      ],
      "year": "2022",
      "venue": "Robust speech recognition via large-scale weak supervision"
    },
    {
      "citation_id": "77",
      "title": "Best-worst scaling more reliable than rating scales: A case study on sentiment intensity annotation",
      "authors": [
        "S Kiritchenko",
        "S Mohammad"
      ],
      "year": "2017",
      "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "78",
      "title": "Measuring disentanglement: A review of metrics",
      "authors": [
        "M.-A Carbonneau",
        "J Zaïdi",
        "J Boilard",
        "G Gagnon"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Neural Networks and Learning Systems"
    },
    {
      "citation_id": "79",
      "title": "Isolating sources of disentanglement in vaes",
      "authors": [
        "T Chen",
        "X Li",
        "R Grosse",
        "D Duvenaud"
      ],
      "year": "2018",
      "venue": "Isolating sources of disentanglement in vaes"
    },
    {
      "citation_id": "80",
      "title": "A framework for the quantitative evaluation of disentangled representations",
      "authors": [
        "C Eastwood",
        "C Williams"
      ],
      "year": "2018",
      "venue": "International Conference on Learning Representations"
    },
    {
      "citation_id": "81",
      "title": "Learning deep disentangled embeddings with the f-statistic loss",
      "authors": [
        "K Ridgeway",
        "M Mozer"
      ],
      "year": "2018",
      "venue": "Neural Information Processing Systems"
    },
    {
      "citation_id": "82",
      "title": "Speech emotion recognition: Two decades in a nutshell, benchmarks, and ongoing trends",
      "authors": [
        "B Schuller"
      ],
      "year": "2018",
      "venue": "Communications of the ACM"
    },
    {
      "citation_id": "83",
      "title": "Dawn of the transformer era in speech emotion recognition: Closing the valence gap",
      "authors": [
        "J Wagner",
        "A Triantafyllopoulos",
        "H Wierstorf",
        "M Schmitt",
        "F Burkhardt",
        "F Eyben",
        "B Schuller"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "84",
      "title": "Contrastive learning based modality-invariant feature acquisition for robust multimodal emotion recognition with missing modalities",
      "authors": [
        "R Liu",
        "H Zuo",
        "Z Lian",
        "B Schuller",
        "H Li"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Affective Computing"
    }
  ]
}