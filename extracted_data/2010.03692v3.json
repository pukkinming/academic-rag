{
  "paper_id": "2010.03692v3",
  "title": "An Audio-Video Deep And Transfer Learning Framework For Multimodal Emotion Recognition In The Wild",
  "published": "2020-10-07T23:45:24Z",
  "authors": [
    "Denis Dresvyanskiy",
    "Elena Ryumina",
    "Heysem Kaya",
    "Maxim Markitantov",
    "Alexey Karpov",
    "Wolfgang Minker"
  ],
  "keywords": [
    "Emotion recognition",
    "deep neural network",
    "information fusion",
    "transfer learning",
    "end-to-end models"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "In this paper, we present our contribution to ABAW facial expression challenge. We report the proposed system and the official challenge results adhering to the challenge protocol. Using end-to-end deep learning and benefiting from transfer learning approaches, we reached a test set challenge performance measure of 42.10%.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Emotions play a vital role in daily human-human interactions and automated recognition of emotions from multi-modal signals has attracted increasing attention in the last decade with applications in domains ranging from intelligent call centers to intelligent tutoring systems. Emotion recognition is studied in the broader affective computing field, where the studies of emotion is the focal point. The research in the domain is shifting to more \"in-the-wild\", namely out of lab-controlled studies, thanks to new and challenging datasets collected and introduced over competitions such as Affective Facial Expressions in the Wild (AFEW)  [1, 2]  and Affective Behavior Analysis in the Wild (ABAW)  [3, 4, 5, 6, 7, 8] .\n\nMotivated from the recent outstanding general performance of deep learning on audio and video domains as well as the efficacy of deep transfer learning to alleviate data scarcity in the target problem  [9, 10, 5] , in this study we employ both deep end-to-end learning and deep transfer learning for both audio and video modalities, fusing the scores of the uni-modal sub-systems for multi-modal affect recognition in out-of-lab conditions. We experiment on and use the official challenge protocol for the ABAW challenge -Facial Expressions Sub-challenge, originally run for Face and Gesture 2020, but was later extended until October. This subchallenge features Ekman's six basic emotions plus neutral, thus featuring a seven-class classification task.\n\nThe contributions of this paper include 1) a novel multimodal framework that leverages deep and transfer learning in audio and video modalities 2) extensive analysis of unimodal systems followed by a multimodal score fusion.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Proposed Approaches",
      "text": "For the emotion recognition, we used both audio and video modalities. As a final solution, we introduce fusion system, which fuses probabilities from both modalities with different strategies.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Video-Based Deep Networks",
      "text": "As video-based models, we chose 2 same models in terms of structure, but different in terms of pretraining data were used.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Vggface2 Based Cnn",
      "text": "VGGFace2  [11]  model was pretrained on the VGGFace2 dataset, which is intended to learn models recognize identities by their faces. It contains 8631 identities in training set with 362.6 images on average. We took the pretrained model, cut the last layer provides class probabilities of the face, and stack two dense layers with 1024 and 7 neurons accordingly. The last layer allows the model to predict the probability of the emotional category.\n\nTo make the model more robust to interference, we used data augmentation, which is implemented via ImageDataGenerator included in keras  [12]  framework. In total, we used such augmentation techniques as rotation (up to 40 degrees), horizontal flip, and brightness (the value ranged from 0.2 to 1).\n\nMoreover, to decrease the effect of class disbalance on loss function, we applied logarithmic weighting.\n\nFor training, the optimizer AdamW  [13]  was chosen. The values of learning rate and weight decay were set to 0.00001. Also, the dropout with probability equaled 0.5 was applied to the last 2 layers. The model with the smallest loss value on the validation dataset across epochs was chosen. We used the predictions from this model called further non-temporal VGGFace2 as a first submission.\n\nTo capture temporal information across video frames, we implemented a 2D CNN + SVM model, which operates with windows of video. It was implemented in the following way: deep embeddings extracted from each frame by nontemporal VGGFace2 were formed in windows with specified size. Then, means and standart deviations (STDs) deep embeddings withing formed window were calculated. Thus, for every window we had 2048 features -1024 means and 1024 STDs. We experimented with different sizes of windows and the value of best one equalled 4 seconds (it corresponds to 30 frames). As earlier, only one labels per whole window was chosen as a major category within corresponding window.\n\nNext the Support Vector Machine (SVM) was trained on formed features. We conducted a lot of experiments with various kernels and regularization parameter and the best result was obtained with following parameters: kernelpoly, gamma = 0.1, C = 3. It should be noted that we also applied logarithmic weighting, as earlier, but with parameter r = 0.47. The predictions from this model were a third submission.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Vgg-Fer Cnn Based Video Features",
      "text": "Facial features are extracted over short non-overlapping video segments (e.g. 2s-4s) and summarized by statistical functionals. A deep neural network pre-trained with VGG-Face  [14]  and fine-tuned with FER-2013 database  [15]  is used. We use the aligned faces given by the ABAW challenge organizers to extract image-level deep features. The aforementioned transfer learning of VGG-Face based CNN by fine-tuning on FER-2013 is proposed in  [9] , and is successfully applied to a set of video based affect recognition challenges ranging from emotion recognition in the wild using AFEW corpora  [1]  to apparent personality recognition  [16, 10]  using ChaLearn LAP-First Impression corpus  [17] .\n\nThe fine-tuned CNN can be reached over github repository 1 . The deep CNN has a 37-layer architecture (involving 16 convolution layers and 5 pooling layers). The response of the 33 rd layer is used, which is the lowest-level 4 096dimensional descriptor. This output of the finetuned model is used inline with the former works  [9, 16, 10]  After extracting frame-level features from each aligned face using the deep neural network, non-overlapping short chunks of the original videos are summarized by computing mean and standard deviation statistics of each dimension over time. These features are subsequently modeled using Kernel Extreme Learning Machine (KELM), which is a fast and robust learning method  [18] . 1 https://github.com/frkngrpnr/jcs",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Audio-Based Deep Networks",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Audio Separation",
      "text": "It is well-known that different extraneous sounds in audio recordings that are not speech (e.g. office, city, street noises, music) can significantly decrease the efficiency of the training process. To avoid it, we applied special separation library Spleeter  [19] , which contains a lot of pretrained deep neural models and is able to separate voice from other sounds. Since we wanted to separate just voice from all others sounds, we used a model, which allows us to divide audio on vocals (voice) and accompaniment (all other sounds including music). We downsampled original extracted audios to 16 kHz, because the Spleeter model is only able to work with no more then 16 kHz and then applied separation. Extracted vocals were used in further experiments.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Labels Preprocessing",
      "text": "Because of the certain circumstances, different videos differ in their frame rate. As annotations were done in per-frame manner, they also have different sample rates. In order to equalize them, we down-sampled all labels to sample rate equals 5, since the smallest one frame rate is only 7.5 frames per second and the category of emotion switches very rarely. Thus, this value of the sample rate should be enough to catch all changes in emotions.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Pann-Based Deep Network",
      "text": "Pretrained audio neural networks (PANNs)  [20]  have demonstrated state-of-the-art performance in the audio pattern recognition domain. These models extract features from raw waveforms, a process that data, and return predictions in real-time. In this study, we used the CNN-14 model, which consists of one layer for extracting features, and six convolutional blocks, inspired by the VGG-like CNNs  [21] . Each convolutional block consists of two convolutional layers with a kernel size of 3x3. Batch normalization is applied between each convolutional layer, and the ReLU nonlinearity is used to speed up and stabilize the training. Average pooling of a size of 2x2 is applied to each convolutional block for downsampling. After the last convolutional layer global pooling is applied to summarize the feature maps into a fixed-length vector. CNN-14 model were preliminary trained on the largescale AudioSet dataset  [22] . We fine-tuned the PANN model for the Expression Challenge. We replaced the last fullyconnected layer with a new one with seven neurons to get the probability of each category of emotion. All parameters are initialized from the CNN-14, except the final fully-connected layer which is randomly initialized. Raw waveforms with a window width of 3 seconds and a step of 1 second were fed to the input of the PANN model.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "1D Cnn + Lstm Based Deep Network",
      "text": "Since there are no available pretrained 1D CNNs on raw audio to use it for transfer learning, we constructed and trained our own. Moreover, to catch temporal information from 1D CNN embeddings, we stacked two LSTM layers above it and seven neurons as a last layer of network to get the probability of each emotion category. In general, two schemes were realized:\n\n• Sequence-to-sequence modelling • Sequence-to-one modeling (one emotion per whole window)\n\nThus, we implemented models be able to map input acoustic raw data into emotional category probabilities. The number of parameters of both models were equaled about 4.5 M. During the testing, the sequence-to-sequence model gave worse results, therefore in this work we present only sequence-to-one model.\n\nAs in video-based modeling, we conducted tests with different sizes of windows (from 2 to 12 seconds with step 2) and the best value was 4 seconds. To train final model, all audios were cut on parts of 4 seconds (with the intersection of 2/5 size of the window), which corresponds to 48000 values in waveform and 20 labels. The labels of window was selected as the most frequent category in it.\n\nThe training process was conducted with following parameters: optimizer was Adam with learning rate = 0.00025, loss functioncategorical cross-entropy, after every convolutional layer the dropout with rate equalled 0.3 was applied.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Fusion",
      "text": "We used two weighted score fusion methods. The first one assigns one weight for confidence scores of each model, where the weights sum up to 1. If we have L models, then the optimized weights are a vector of length L. We use Dirichlet distribution to generate these weights randomly and try to optimize these weights with respect to the challenge measure on the validation set. We call this first approach \"Simple Weighted Fusion\" (SWF).\n\nIn the second approach, we extend the first approach and have a fusion matrix of L × K, where K denotes the number of classes. That is we have an importance weight for each class of each model, separately. Similar to the SWF, the weights are randomly generated using Dirichlet distribution for each class, such that the sum of weights for each class over models sum up to unity. We call this \"Model and Class based Weighed Fusion\" (MCWF). The latter approach has been successfully applied in former video based affect recognition inthe-wild challenges  [23, 9] .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Experimental Results",
      "text": "The official performance measure used in challenge is defined by formula 1.\n\nwhere F 1 is a weighted average of the recall and the precision (also known as F-measure) and the Accuracy is the fraction of predictions that the model classified correctly.\n\nWe conducted extensive experiments with different unimodal video and audio systems separately and then fused them with different combinations. The results of experiments are presented in the Table  1 .\n\nAs a baseline for our work, we took the results of nontemporal VGGFace2-based CNN, which was described in Section 2.1.1. We used the predictions from this model as a first submission. It reached a challenge performance measure of 50.23% on the validation set and 40.60% on the test set. For the subsequent submissions, to grasp the temporal dependencies we used VGGFface2 + SVM approach, which is also described in Section 2.1.1. Since our second approach VGGFER-based CNN + KELM reached the result a little bit lower than the modified VGGFace2-based CNN, we decided to fuse three systems on the decision level. The fusion was done by SWF approach. The results of the fusion system are 56.56% on the validation set and the predictions from this system were a second submission. On the test set the fused predictions showed 41.90% of the challenge performance metric.\n\nFor the third submission, we chose predictions from modified VGGFace2-based CNN to check how it works standalone. The value of the competition metric of the model is 55.66% on the validation set and 42.0% on the test set. For the next 3 submissions we used MCWF technique described in our paper with different combinations of models:",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Conclusions",
      "text": "The paper has researched the efficiency of deep neural networks and the application of transfer learning and fusion techniques on emotion recognition problem. We have found that video-based MCWF fusion of different pretrained models with correctly fitted weights can increase the efficiency of the system compare to standalone models. However, audio-based models did not contribute to performance within the fusion process, which can be caused by information insufficiency of audios (the subjects often silence) or incorrectness of audio data such as music, a voice from another source, and noises. In future work, it will be good to annotate audios manually for distinguishing different persons in audio and use semantic information extracted from a person's speech to add new modality in the fusion system.",
      "page_start": 6,
      "page_end": 6
    }
  ],
  "figures": [],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "3 St. Petersburg Federal Research Center of the Russian Academy of Sciences, Russia": "4 Department of Information and Computing Sciences, Utrecht University, Utrecht, The Netherlands"
        },
        {
          "3 St. Petersburg Federal Research Center of the Russian Academy of Sciences, Russia": "ABSTRACT"
        },
        {
          "3 St. Petersburg Federal Research Center of the Russian Academy of Sciences, Russia": ""
        },
        {
          "3 St. Petersburg Federal Research Center of the Russian Academy of Sciences, Russia": "In this paper, we present our contribution to ABAW facial ex-"
        },
        {
          "3 St. Petersburg Federal Research Center of the Russian Academy of Sciences, Russia": ""
        },
        {
          "3 St. Petersburg Federal Research Center of the Russian Academy of Sciences, Russia": "pression challenge. We report\nthe proposed system and the"
        },
        {
          "3 St. Petersburg Federal Research Center of the Russian Academy of Sciences, Russia": ""
        },
        {
          "3 St. Petersburg Federal Research Center of the Russian Academy of Sciences, Russia": "ofﬁcial challenge results adhering to the challenge protocol."
        },
        {
          "3 St. Petersburg Federal Research Center of the Russian Academy of Sciences, Russia": ""
        },
        {
          "3 St. Petersburg Federal Research Center of the Russian Academy of Sciences, Russia": "Using end-to-end deep learning and beneﬁting from transfer"
        },
        {
          "3 St. Petersburg Federal Research Center of the Russian Academy of Sciences, Russia": "learning approaches, we reached a test set challenge perfor-"
        },
        {
          "3 St. Petersburg Federal Research Center of the Russian Academy of Sciences, Russia": "mance measure of 42.10%."
        },
        {
          "3 St. Petersburg Federal Research Center of the Russian Academy of Sciences, Russia": "Index Terms— Emotion recognition, deep neural net-"
        },
        {
          "3 St. Petersburg Federal Research Center of the Russian Academy of Sciences, Russia": ""
        },
        {
          "3 St. Petersburg Federal Research Center of the Russian Academy of Sciences, Russia": "work, information fusion, transfer learning, end-to-end mod-"
        },
        {
          "3 St. Petersburg Federal Research Center of the Russian Academy of Sciences, Russia": ""
        },
        {
          "3 St. Petersburg Federal Research Center of the Russian Academy of Sciences, Russia": "els"
        },
        {
          "3 St. Petersburg Federal Research Center of the Russian Academy of Sciences, Russia": ""
        },
        {
          "3 St. Petersburg Federal Research Center of the Russian Academy of Sciences, Russia": ""
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Also,\nthe dropout with probability equaled 0.5 was applied": "to the last 2 layers. The model with the smallest\nloss value",
          "2.2. Audio-based deep networks": ""
        },
        {
          "Also,\nthe dropout with probability equaled 0.5 was applied": "",
          "2.2. Audio-based deep networks": "2.2.1. Audio separation"
        },
        {
          "Also,\nthe dropout with probability equaled 0.5 was applied": "on the validation dataset across epochs was chosen. We used",
          "2.2. Audio-based deep networks": ""
        },
        {
          "Also,\nthe dropout with probability equaled 0.5 was applied": "the predictions from this model called further non-temporal",
          "2.2. Audio-based deep networks": ""
        },
        {
          "Also,\nthe dropout with probability equaled 0.5 was applied": "",
          "2.2. Audio-based deep networks": "It\nis well-known that different extraneous\nsounds\nin audio"
        },
        {
          "Also,\nthe dropout with probability equaled 0.5 was applied": "VGGFace2 as a ﬁrst submission.",
          "2.2. Audio-based deep networks": ""
        },
        {
          "Also,\nthe dropout with probability equaled 0.5 was applied": "",
          "2.2. Audio-based deep networks": "recordings that are not speech (e.g. ofﬁce, city, street noises,"
        },
        {
          "Also,\nthe dropout with probability equaled 0.5 was applied": "To\ncapture\ntemporal\ninformation across video frames,",
          "2.2. Audio-based deep networks": ""
        },
        {
          "Also,\nthe dropout with probability equaled 0.5 was applied": "",
          "2.2. Audio-based deep networks": "music) can signiﬁcantly decrease the efﬁciency of the train-"
        },
        {
          "Also,\nthe dropout with probability equaled 0.5 was applied": "we implemented a 2D CNN + SVM model, which operates",
          "2.2. Audio-based deep networks": ""
        },
        {
          "Also,\nthe dropout with probability equaled 0.5 was applied": "",
          "2.2. Audio-based deep networks": "ing process. To avoid it, we applied special separation library"
        },
        {
          "Also,\nthe dropout with probability equaled 0.5 was applied": "with windows of video.\nIt was implemented in the following",
          "2.2. Audio-based deep networks": ""
        },
        {
          "Also,\nthe dropout with probability equaled 0.5 was applied": "",
          "2.2. Audio-based deep networks": "Spleeter\n[19], which contains a lot of pretrained deep neu-"
        },
        {
          "Also,\nthe dropout with probability equaled 0.5 was applied": "way:\ndeep embeddings extracted from each frame by non-",
          "2.2. Audio-based deep networks": ""
        },
        {
          "Also,\nthe dropout with probability equaled 0.5 was applied": "",
          "2.2. Audio-based deep networks": "ral models and is able to separate voice from other sounds."
        },
        {
          "Also,\nthe dropout with probability equaled 0.5 was applied": "temporal VGGFace2 were formed in windows with speciﬁed",
          "2.2. Audio-based deep networks": ""
        },
        {
          "Also,\nthe dropout with probability equaled 0.5 was applied": "",
          "2.2. Audio-based deep networks": "Since we wanted to separate just voice from all others sounds,"
        },
        {
          "Also,\nthe dropout with probability equaled 0.5 was applied": "size. Then, means and standart deviations (STDs) deep em-",
          "2.2. Audio-based deep networks": ""
        },
        {
          "Also,\nthe dropout with probability equaled 0.5 was applied": "",
          "2.2. Audio-based deep networks": "we used a model, which allows us to divide audio on vocals"
        },
        {
          "Also,\nthe dropout with probability equaled 0.5 was applied": "beddings withing formed window were calculated. Thus, for",
          "2.2. Audio-based deep networks": ""
        },
        {
          "Also,\nthe dropout with probability equaled 0.5 was applied": "",
          "2.2. Audio-based deep networks": "(voice) and accompaniment (all other sounds including mu-"
        },
        {
          "Also,\nthe dropout with probability equaled 0.5 was applied": "every window we had 2048 features - 1024 means and 1024",
          "2.2. Audio-based deep networks": ""
        },
        {
          "Also,\nthe dropout with probability equaled 0.5 was applied": "",
          "2.2. Audio-based deep networks": "sic). We downsampled original extracted audios to 16 kHz,"
        },
        {
          "Also,\nthe dropout with probability equaled 0.5 was applied": "STDs. We experimented with different sizes of windows and",
          "2.2. Audio-based deep networks": ""
        },
        {
          "Also,\nthe dropout with probability equaled 0.5 was applied": "",
          "2.2. Audio-based deep networks": "because the Spleeter model is only able to work with no more"
        },
        {
          "Also,\nthe dropout with probability equaled 0.5 was applied": "the value of best one equalled 4 seconds (it corresponds to 30",
          "2.2. Audio-based deep networks": ""
        },
        {
          "Also,\nthe dropout with probability equaled 0.5 was applied": "",
          "2.2. Audio-based deep networks": "then 16 kHz and then applied separation.\nExtracted vocals"
        },
        {
          "Also,\nthe dropout with probability equaled 0.5 was applied": "frames). As earlier, only one labels per whole window was",
          "2.2. Audio-based deep networks": ""
        },
        {
          "Also,\nthe dropout with probability equaled 0.5 was applied": "",
          "2.2. Audio-based deep networks": "were used in further experiments."
        },
        {
          "Also,\nthe dropout with probability equaled 0.5 was applied": "chosen as a major category within corresponding window.",
          "2.2. Audio-based deep networks": ""
        },
        {
          "Also,\nthe dropout with probability equaled 0.5 was applied": "Next\nthe Support Vector Machine (SVM) was trained on",
          "2.2. Audio-based deep networks": ""
        },
        {
          "Also,\nthe dropout with probability equaled 0.5 was applied": "formed features. We conducted a lot of experiments with",
          "2.2. Audio-based deep networks": "2.2.2.\nLabels preprocessing"
        },
        {
          "Also,\nthe dropout with probability equaled 0.5 was applied": "various kernels and regularization parameter and the best re-",
          "2.2. Audio-based deep networks": ""
        },
        {
          "Also,\nthe dropout with probability equaled 0.5 was applied": "",
          "2.2. Audio-based deep networks": "Because of the certain circumstances, different videos differ"
        },
        {
          "Also,\nthe dropout with probability equaled 0.5 was applied": "sult was obtained with following parameters: kernel\n- poly,",
          "2.2. Audio-based deep networks": ""
        },
        {
          "Also,\nthe dropout with probability equaled 0.5 was applied": "",
          "2.2. Audio-based deep networks": "in their\nframe rate. As annotations were done in per-frame"
        },
        {
          "Also,\nthe dropout with probability equaled 0.5 was applied": "gamma = 0.1, C = 3.\nIt should be noted that we also applied",
          "2.2. Audio-based deep networks": ""
        },
        {
          "Also,\nthe dropout with probability equaled 0.5 was applied": "",
          "2.2. Audio-based deep networks": "manner,\nthey also have different sample rates.\nIn order\nto"
        },
        {
          "Also,\nthe dropout with probability equaled 0.5 was applied": "logarithmic weighting, as earlier, but with parameter r = 0.47.",
          "2.2. Audio-based deep networks": ""
        },
        {
          "Also,\nthe dropout with probability equaled 0.5 was applied": "",
          "2.2. Audio-based deep networks": "equalize them, we down-sampled all\nlabels\nto sample rate"
        },
        {
          "Also,\nthe dropout with probability equaled 0.5 was applied": "The predictions from this model were a third submission.",
          "2.2. Audio-based deep networks": ""
        },
        {
          "Also,\nthe dropout with probability equaled 0.5 was applied": "",
          "2.2. Audio-based deep networks": "equals 5, since the smallest one frame rate is only 7.5 frames"
        },
        {
          "Also,\nthe dropout with probability equaled 0.5 was applied": "",
          "2.2. Audio-based deep networks": "per second and the category of emotion switches very rarely."
        },
        {
          "Also,\nthe dropout with probability equaled 0.5 was applied": "2.1.2. VGG-FER CNN based Video Features",
          "2.2. Audio-based deep networks": "Thus, this value of the sample rate should be enough to catch"
        },
        {
          "Also,\nthe dropout with probability equaled 0.5 was applied": "",
          "2.2. Audio-based deep networks": "all changes in emotions."
        },
        {
          "Also,\nthe dropout with probability equaled 0.5 was applied": "Facial features are extracted over short non-overlapping video",
          "2.2. Audio-based deep networks": ""
        },
        {
          "Also,\nthe dropout with probability equaled 0.5 was applied": "segments (e.g. 2s-4s) and summarized by statistical function-",
          "2.2. Audio-based deep networks": ""
        },
        {
          "Also,\nthe dropout with probability equaled 0.5 was applied": "als. A deep neural network pre-trained with VGG-Face [14]",
          "2.2. Audio-based deep networks": "2.2.3. PANN-based deep network"
        },
        {
          "Also,\nthe dropout with probability equaled 0.5 was applied": "and ﬁne-tuned with FER-2013 database [15] is used. We use",
          "2.2. Audio-based deep networks": ""
        },
        {
          "Also,\nthe dropout with probability equaled 0.5 was applied": "the aligned faces given by the ABAW challenge organizers to",
          "2.2. Audio-based deep networks": "Pretrained audio neural networks (PANNs) [20] have demon-"
        },
        {
          "Also,\nthe dropout with probability equaled 0.5 was applied": "extract image-level deep features. The aforementioned trans-",
          "2.2. Audio-based deep networks": "strated\nstate-of-the-art\nperformance\nin\nthe\naudio\npattern"
        },
        {
          "Also,\nthe dropout with probability equaled 0.5 was applied": "fer learning of VGG-Face based CNN by ﬁne-tuning on FER-",
          "2.2. Audio-based deep networks": "recognition domain.\nThese models\nextract\nfeatures\nfrom"
        },
        {
          "Also,\nthe dropout with probability equaled 0.5 was applied": "2013 is proposed in [9], and is successfully applied to a set of",
          "2.2. Audio-based deep networks": "raw waveforms, a process that data, and return predictions in"
        },
        {
          "Also,\nthe dropout with probability equaled 0.5 was applied": "video based affect recognition challenges ranging from emo-",
          "2.2. Audio-based deep networks": "real-time.\nIn this study, we used the CNN-14 model, which"
        },
        {
          "Also,\nthe dropout with probability equaled 0.5 was applied": "tion recognition in the wild using AFEW corpora [1] to ap-",
          "2.2. Audio-based deep networks": "consists of one layer for extracting features, and six convo-"
        },
        {
          "Also,\nthe dropout with probability equaled 0.5 was applied": "parent personality recognition [16, 10] using ChaLearn LAP-",
          "2.2. Audio-based deep networks": "lutional blocks,\ninspired by the VGG-like CNNs [21]. Each"
        },
        {
          "Also,\nthe dropout with probability equaled 0.5 was applied": "First Impression corpus [17].",
          "2.2. Audio-based deep networks": "convolutional block consists of two convolutional layers with"
        },
        {
          "Also,\nthe dropout with probability equaled 0.5 was applied": "",
          "2.2. Audio-based deep networks": "a kernel size of 3x3. Batch normalization is applied between"
        },
        {
          "Also,\nthe dropout with probability equaled 0.5 was applied": "The ﬁne-tuned CNN can be reached over github reposi-",
          "2.2. Audio-based deep networks": ""
        },
        {
          "Also,\nthe dropout with probability equaled 0.5 was applied": "",
          "2.2. Audio-based deep networks": "each convolutional layer, and the ReLU nonlinearity is used"
        },
        {
          "Also,\nthe dropout with probability equaled 0.5 was applied": "tory1. The deep CNN has a 37-layer architecture (involving",
          "2.2. Audio-based deep networks": ""
        },
        {
          "Also,\nthe dropout with probability equaled 0.5 was applied": "",
          "2.2. Audio-based deep networks": "to speed up and stabilize the training. Average pooling of a"
        },
        {
          "Also,\nthe dropout with probability equaled 0.5 was applied": "16 convolution layers and 5 pooling layers).\nThe response",
          "2.2. Audio-based deep networks": ""
        },
        {
          "Also,\nthe dropout with probability equaled 0.5 was applied": "of\nthe 33rd\nlayer\nis used, which is\nthe lowest-level 4 096-",
          "2.2. Audio-based deep networks": "size of 2x2 is applied to each convolutional block for down-"
        },
        {
          "Also,\nthe dropout with probability equaled 0.5 was applied": "",
          "2.2. Audio-based deep networks": "sampling. After\nthe last convolutional\nlayer global pooling"
        },
        {
          "Also,\nthe dropout with probability equaled 0.5 was applied": "dimensional descriptor. This output of the ﬁnetuned model is",
          "2.2. Audio-based deep networks": ""
        },
        {
          "Also,\nthe dropout with probability equaled 0.5 was applied": "",
          "2.2. Audio-based deep networks": "is applied to summarize the feature maps into a ﬁxed-length"
        },
        {
          "Also,\nthe dropout with probability equaled 0.5 was applied": "used inline with the former works [9, 16, 10] After extracting",
          "2.2. Audio-based deep networks": ""
        },
        {
          "Also,\nthe dropout with probability equaled 0.5 was applied": "",
          "2.2. Audio-based deep networks": "vector. CNN-14 model were preliminary trained on the large-"
        },
        {
          "Also,\nthe dropout with probability equaled 0.5 was applied": "frame-level\nfeatures from each aligned face using the deep",
          "2.2. Audio-based deep networks": ""
        },
        {
          "Also,\nthe dropout with probability equaled 0.5 was applied": "",
          "2.2. Audio-based deep networks": "scale AudioSet dataset [22]. We ﬁne-tuned the PANN model"
        },
        {
          "Also,\nthe dropout with probability equaled 0.5 was applied": "neural network, non-overlapping short chunks of\nthe origi-",
          "2.2. Audio-based deep networks": ""
        },
        {
          "Also,\nthe dropout with probability equaled 0.5 was applied": "",
          "2.2. Audio-based deep networks": "for\nthe Expression Challenge. We replaced the last\nfully-"
        },
        {
          "Also,\nthe dropout with probability equaled 0.5 was applied": "nal videos are summarized by computing mean and standard",
          "2.2. Audio-based deep networks": ""
        },
        {
          "Also,\nthe dropout with probability equaled 0.5 was applied": "",
          "2.2. Audio-based deep networks": "connected layer with a new one with seven neurons to get the"
        },
        {
          "Also,\nthe dropout with probability equaled 0.5 was applied": "deviation statistics of each dimension over time. These fea-",
          "2.2. Audio-based deep networks": ""
        },
        {
          "Also,\nthe dropout with probability equaled 0.5 was applied": "",
          "2.2. Audio-based deep networks": "probability of each category of emotion. All parameters are"
        },
        {
          "Also,\nthe dropout with probability equaled 0.5 was applied": "tures are subsequently modeled using Kernel Extreme Learn-",
          "2.2. Audio-based deep networks": ""
        },
        {
          "Also,\nthe dropout with probability equaled 0.5 was applied": "",
          "2.2. Audio-based deep networks": "initialized from the CNN-14, except the ﬁnal fully-connected"
        },
        {
          "Also,\nthe dropout with probability equaled 0.5 was applied": "ing Machine (KELM), which is a fast and robust\nlearning",
          "2.2. Audio-based deep networks": ""
        },
        {
          "Also,\nthe dropout with probability equaled 0.5 was applied": "",
          "2.2. Audio-based deep networks": "layer which is randomly initialized. Raw waveforms with a"
        },
        {
          "Also,\nthe dropout with probability equaled 0.5 was applied": "method [18].",
          "2.2. Audio-based deep networks": ""
        },
        {
          "Also,\nthe dropout with probability equaled 0.5 was applied": "",
          "2.2. Audio-based deep networks": "window width of 3 seconds and a step of 1 second were fed"
        },
        {
          "Also,\nthe dropout with probability equaled 0.5 was applied": "1https://github.com/frkngrpnr/jcs",
          "2.2. Audio-based deep networks": "to the input of the PANN model."
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "2.2.4.\n1D CNN + LSTM based deep network": "Since there are no available pretrained 1D CNNs on raw au-",
          "3. EXPERIMENTAL RESULTS": "The ofﬁcial performance measure used in challenge is deﬁned"
        },
        {
          "2.2.4.\n1D CNN + LSTM based deep network": "dio to use it for transfer learning, we constructed and trained",
          "3. EXPERIMENTAL RESULTS": "by formula 1."
        },
        {
          "2.2.4.\n1D CNN + LSTM based deep network": "our own. Moreover,\nto catch temporal\ninformation from 1D",
          "3. EXPERIMENTAL RESULTS": ""
        },
        {
          "2.2.4.\n1D CNN + LSTM based deep network": "CNN embeddings, we stacked two LSTM layers above it and",
          "3. EXPERIMENTAL RESULTS": "CP M = 0.67 ∗ F1 + 0.33 ∗ Accuracy\n(1)"
        },
        {
          "2.2.4.\n1D CNN + LSTM based deep network": "seven neurons as a last\nlayer of network to get\nthe probabil-",
          "3. EXPERIMENTAL RESULTS": ""
        },
        {
          "2.2.4.\n1D CNN + LSTM based deep network": "",
          "3. EXPERIMENTAL RESULTS": "where F1 is a weighted average of the recall and the precision"
        },
        {
          "2.2.4.\n1D CNN + LSTM based deep network": "ity of each emotion category.\nIn general,\ntwo schemes were",
          "3. EXPERIMENTAL RESULTS": ""
        },
        {
          "2.2.4.\n1D CNN + LSTM based deep network": "",
          "3. EXPERIMENTAL RESULTS": "(also known as F-measure) and the Accuracy is the fraction"
        },
        {
          "2.2.4.\n1D CNN + LSTM based deep network": "realized:",
          "3. EXPERIMENTAL RESULTS": ""
        },
        {
          "2.2.4.\n1D CNN + LSTM based deep network": "",
          "3. EXPERIMENTAL RESULTS": "of predictions that the model classiﬁed correctly."
        },
        {
          "2.2.4.\n1D CNN + LSTM based deep network": "",
          "3. EXPERIMENTAL RESULTS": "We conducted extensive experiments with different uni-"
        },
        {
          "2.2.4.\n1D CNN + LSTM based deep network": "• Sequence-to-sequence modelling",
          "3. EXPERIMENTAL RESULTS": ""
        },
        {
          "2.2.4.\n1D CNN + LSTM based deep network": "",
          "3. EXPERIMENTAL RESULTS": "modal video and audio systems\nseparately and then fused"
        },
        {
          "2.2.4.\n1D CNN + LSTM based deep network": "",
          "3. EXPERIMENTAL RESULTS": "them with different combinations. The results of experiments"
        },
        {
          "2.2.4.\n1D CNN + LSTM based deep network": "• Sequence-to-one modeling\n(one\nemotion\nper whole",
          "3. EXPERIMENTAL RESULTS": ""
        },
        {
          "2.2.4.\n1D CNN + LSTM based deep network": "",
          "3. EXPERIMENTAL RESULTS": "are presented in the Table 1."
        },
        {
          "2.2.4.\n1D CNN + LSTM based deep network": "window)",
          "3. EXPERIMENTAL RESULTS": ""
        },
        {
          "2.2.4.\n1D CNN + LSTM based deep network": "",
          "3. EXPERIMENTAL RESULTS": "As a baseline for our work, we took the results of non-"
        },
        {
          "2.2.4.\n1D CNN + LSTM based deep network": "",
          "3. EXPERIMENTAL RESULTS": "temporal VGGFace2-based CNN, which was described in"
        },
        {
          "2.2.4.\n1D CNN + LSTM based deep network": "Thus, we\nimplemented models\nbe\nable\nto map\ninput",
          "3. EXPERIMENTAL RESULTS": ""
        },
        {
          "2.2.4.\n1D CNN + LSTM based deep network": "",
          "3. EXPERIMENTAL RESULTS": "Section 2.1.1. We used the predictions from this model as"
        },
        {
          "2.2.4.\n1D CNN + LSTM based deep network": "acoustic raw data into emotional category probabilities. The",
          "3. EXPERIMENTAL RESULTS": ""
        },
        {
          "2.2.4.\n1D CNN + LSTM based deep network": "",
          "3. EXPERIMENTAL RESULTS": "a ﬁrst submission.\nIt reached a challenge performance mea-"
        },
        {
          "2.2.4.\n1D CNN + LSTM based deep network": "number of parameters of both models were equaled about",
          "3. EXPERIMENTAL RESULTS": ""
        },
        {
          "2.2.4.\n1D CNN + LSTM based deep network": "",
          "3. EXPERIMENTAL RESULTS": "sure of 50.23% on the validation set and 40.60% on the test"
        },
        {
          "2.2.4.\n1D CNN + LSTM based deep network": "4.5 M. During the testing,\nthe sequence-to-sequence model",
          "3. EXPERIMENTAL RESULTS": ""
        },
        {
          "2.2.4.\n1D CNN + LSTM based deep network": "",
          "3. EXPERIMENTAL RESULTS": "set.\nFor\nthe subsequent submissions,\nto grasp the temporal"
        },
        {
          "2.2.4.\n1D CNN + LSTM based deep network": "gave worse results,\ntherefore in this work we present only",
          "3. EXPERIMENTAL RESULTS": ""
        },
        {
          "2.2.4.\n1D CNN + LSTM based deep network": "",
          "3. EXPERIMENTAL RESULTS": "dependencies we used VGGFface2 + SVM approach, which"
        },
        {
          "2.2.4.\n1D CNN + LSTM based deep network": "sequence-to-one model.",
          "3. EXPERIMENTAL RESULTS": ""
        },
        {
          "2.2.4.\n1D CNN + LSTM based deep network": "",
          "3. EXPERIMENTAL RESULTS": "is also described in Section 2.1.1. Since our second approach"
        },
        {
          "2.2.4.\n1D CNN + LSTM based deep network": "As in video-based modeling, we conducted tests with dif-",
          "3. EXPERIMENTAL RESULTS": ""
        },
        {
          "2.2.4.\n1D CNN + LSTM based deep network": "",
          "3. EXPERIMENTAL RESULTS": "VGGFER-based CNN + KELM reached the result a little bit"
        },
        {
          "2.2.4.\n1D CNN + LSTM based deep network": "ferent sizes of windows (from 2 to 12 seconds with step 2) and",
          "3. EXPERIMENTAL RESULTS": ""
        },
        {
          "2.2.4.\n1D CNN + LSTM based deep network": "",
          "3. EXPERIMENTAL RESULTS": "lower than the modiﬁed VGGFace2-based CNN, we decided"
        },
        {
          "2.2.4.\n1D CNN + LSTM based deep network": "the best value was 4 seconds. To train ﬁnal model, all audios",
          "3. EXPERIMENTAL RESULTS": ""
        },
        {
          "2.2.4.\n1D CNN + LSTM based deep network": "",
          "3. EXPERIMENTAL RESULTS": "to fuse three systems on the decision level. The fusion was"
        },
        {
          "2.2.4.\n1D CNN + LSTM based deep network": "were cut on parts of 4 seconds (with the intersection of 2/5",
          "3. EXPERIMENTAL RESULTS": ""
        },
        {
          "2.2.4.\n1D CNN + LSTM based deep network": "",
          "3. EXPERIMENTAL RESULTS": "done by SWF approach. The results of the fusion system are"
        },
        {
          "2.2.4.\n1D CNN + LSTM based deep network": "size of\nthe window), which corresponds to 48000 values in",
          "3. EXPERIMENTAL RESULTS": ""
        },
        {
          "2.2.4.\n1D CNN + LSTM based deep network": "",
          "3. EXPERIMENTAL RESULTS": "56.56% on the validation set and the predictions from this"
        },
        {
          "2.2.4.\n1D CNN + LSTM based deep network": "waveform and 20 labels. The labels of window was selected",
          "3. EXPERIMENTAL RESULTS": ""
        },
        {
          "2.2.4.\n1D CNN + LSTM based deep network": "",
          "3. EXPERIMENTAL RESULTS": "system were a second submission. On the test set\nthe fused"
        },
        {
          "2.2.4.\n1D CNN + LSTM based deep network": "as the most frequent category in it.",
          "3. EXPERIMENTAL RESULTS": ""
        },
        {
          "2.2.4.\n1D CNN + LSTM based deep network": "",
          "3. EXPERIMENTAL RESULTS": "predictions\nshowed 41.90% of\nthe\nchallenge performance"
        },
        {
          "2.2.4.\n1D CNN + LSTM based deep network": "The training process was conducted with following pa-",
          "3. EXPERIMENTAL RESULTS": "metric."
        },
        {
          "2.2.4.\n1D CNN + LSTM based deep network": "rameters: optimizer was Adam with learning rate = 0.00025,",
          "3. EXPERIMENTAL RESULTS": "For\nthe\nthird\nsubmission, we\nchose\npredictions\nfrom"
        },
        {
          "2.2.4.\n1D CNN + LSTM based deep network": "loss function - categorical cross-entropy, after every convolu-",
          "3. EXPERIMENTAL RESULTS": "modiﬁed VGGFace2-based CNN to check how it works stan-"
        },
        {
          "2.2.4.\n1D CNN + LSTM based deep network": "tional layer the dropout with rate equalled 0.3 was applied.",
          "3. EXPERIMENTAL RESULTS": "dalone. The value of the competition metric of the model\nis"
        },
        {
          "2.2.4.\n1D CNN + LSTM based deep network": "",
          "3. EXPERIMENTAL RESULTS": "55.66% on the validation set and 42.0% on the test set."
        },
        {
          "2.2.4.\n1D CNN + LSTM based deep network": "",
          "3. EXPERIMENTAL RESULTS": "For\nthe next 3 submissions we used MCWF technique de-"
        },
        {
          "2.2.4.\n1D CNN + LSTM based deep network": "2.3. Fusion",
          "3. EXPERIMENTAL RESULTS": "scribed in our paper with different combinations of models:"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "For\nthe next 3 submissions we used MCWF technique de-": "scribed in our paper with different combinations of models:"
        },
        {
          "For\nthe next 3 submissions we used MCWF technique de-": "• VGGFace2-based CNN + SVM and VGGFER-based"
        },
        {
          "For\nthe next 3 submissions we used MCWF technique de-": "CNN models\nfusion:\nthe value of\nchallenge perfor-"
        },
        {
          "For\nthe next 3 submissions we used MCWF technique de-": "mance metric is 55.70% for validation set and 41.70%"
        },
        {
          "For\nthe next 3 submissions we used MCWF technique de-": "for test set."
        },
        {
          "For\nthe next 3 submissions we used MCWF technique de-": ""
        },
        {
          "For\nthe next 3 submissions we used MCWF technique de-": "• VGGFace2-based CNN + SVM and 1D CNN + LSTM"
        },
        {
          "For\nthe next 3 submissions we used MCWF technique de-": ""
        },
        {
          "For\nthe next 3 submissions we used MCWF technique de-": "models\nfusion:\nthe value of\nchallenge performance"
        },
        {
          "For\nthe next 3 submissions we used MCWF technique de-": ""
        },
        {
          "For\nthe next 3 submissions we used MCWF technique de-": "metric is 55.87% on validation set and 42.10% on test"
        },
        {
          "For\nthe next 3 submissions we used MCWF technique de-": ""
        },
        {
          "For\nthe next 3 submissions we used MCWF technique de-": "set.\nIt\nis\nthe best performance value reached by our"
        },
        {
          "For\nthe next 3 submissions we used MCWF technique de-": ""
        },
        {
          "For\nthe next 3 submissions we used MCWF technique de-": "system in Expression challenge."
        },
        {
          "For\nthe next 3 submissions we used MCWF technique de-": ""
        },
        {
          "For\nthe next 3 submissions we used MCWF technique de-": ""
        },
        {
          "For\nthe next 3 submissions we used MCWF technique de-": "• VGGFace2-based CNN + SVM, 1D CNN + LSTM"
        },
        {
          "For\nthe next 3 submissions we used MCWF technique de-": ""
        },
        {
          "For\nthe next 3 submissions we used MCWF technique de-": "and PANN-based models\nfusion:\nthe value of chal-"
        },
        {
          "For\nthe next 3 submissions we used MCWF technique de-": ""
        },
        {
          "For\nthe next 3 submissions we used MCWF technique de-": "lenge performance metric is 54.78% for validation set"
        },
        {
          "For\nthe next 3 submissions we used MCWF technique de-": ""
        },
        {
          "For\nthe next 3 submissions we used MCWF technique de-": "and 41.60% for test set."
        },
        {
          "For\nthe next 3 submissions we used MCWF technique de-": ""
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Table 1. The performance of models performed on ABAW challenge (Expressions sub-challenge) validation set. CPM: Chal-": ""
        },
        {
          "Table 1. The performance of models performed on ABAW challenge (Expressions sub-challenge) validation set. CPM: Chal-": "Features/Input"
        },
        {
          "Table 1. The performance of models performed on ABAW challenge (Expressions sub-challenge) validation set. CPM: Chal-": "Visual"
        },
        {
          "Table 1. The performance of models performed on ABAW challenge (Expressions sub-challenge) validation set. CPM: Chal-": "Visual"
        },
        {
          "Table 1. The performance of models performed on ABAW challenge (Expressions sub-challenge) validation set. CPM: Chal-": "Visual"
        },
        {
          "Table 1. The performance of models performed on ABAW challenge (Expressions sub-challenge) validation set. CPM: Chal-": "Raw audio"
        },
        {
          "Table 1. The performance of models performed on ABAW challenge (Expressions sub-challenge) validation set. CPM: Chal-": "Raw audio"
        },
        {
          "Table 1. The performance of models performed on ABAW challenge (Expressions sub-challenge) validation set. CPM: Chal-": "Visual"
        },
        {
          "Table 1. The performance of models performed on ABAW challenge (Expressions sub-challenge) validation set. CPM: Chal-": "Visual"
        },
        {
          "Table 1. The performance of models performed on ABAW challenge (Expressions sub-challenge) validation set. CPM: Chal-": "Visual & Raw audio"
        },
        {
          "Table 1. The performance of models performed on ABAW challenge (Expressions sub-challenge) validation set. CPM: Chal-": "Visual & Raw audio"
        },
        {
          "Table 1. The performance of models performed on ABAW challenge (Expressions sub-challenge) validation set. CPM: Chal-": "Visual & Raw audio"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "1 - VGGFace2-based 2D CNN, 2 - VGGFace2-based 2D CNN + SVM,": "3 - VGGFER-based 2D CNN 4 - 1D CNN + LSTM, 5 - PANN-based 2D CNN"
        },
        {
          "1 - VGGFace2-based 2D CNN, 2 - VGGFace2-based 2D CNN + SVM,": "and VGGFER-based CNN features + ELM, PANN-based and"
        },
        {
          "1 - VGGFace2-based 2D CNN, 2 - VGGFace2-based 2D CNN + SVM,": "1D CNN + LSTM models. The challenge performance metric"
        },
        {
          "1 - VGGFace2-based 2D CNN, 2 - VGGFace2-based 2D CNN + SVM,": "on the validation set is 55.54% and 41.80% on the test set."
        },
        {
          "1 - VGGFace2-based 2D CNN, 2 - VGGFace2-based 2D CNN + SVM,": ""
        },
        {
          "1 - VGGFace2-based 2D CNN, 2 - VGGFace2-based 2D CNN + SVM,": ""
        },
        {
          "1 - VGGFace2-based 2D CNN, 2 - VGGFace2-based 2D CNN + SVM,": "4. CONCLUSIONS"
        },
        {
          "1 - VGGFace2-based 2D CNN, 2 - VGGFace2-based 2D CNN + SVM,": ""
        },
        {
          "1 - VGGFace2-based 2D CNN, 2 - VGGFace2-based 2D CNN + SVM,": "The paper has researched the efﬁciency of deep neural net-"
        },
        {
          "1 - VGGFace2-based 2D CNN, 2 - VGGFace2-based 2D CNN + SVM,": ""
        },
        {
          "1 - VGGFace2-based 2D CNN, 2 - VGGFace2-based 2D CNN + SVM,": "works\nand the\napplication of\ntransfer\nlearning and fusion"
        },
        {
          "1 - VGGFace2-based 2D CNN, 2 - VGGFace2-based 2D CNN + SVM,": ""
        },
        {
          "1 - VGGFace2-based 2D CNN, 2 - VGGFace2-based 2D CNN + SVM,": "techniques on emotion recognition problem. We have found"
        },
        {
          "1 - VGGFace2-based 2D CNN, 2 - VGGFace2-based 2D CNN + SVM,": ""
        },
        {
          "1 - VGGFace2-based 2D CNN, 2 - VGGFace2-based 2D CNN + SVM,": "that video-based MCWF fusion of different pretrained models"
        },
        {
          "1 - VGGFace2-based 2D CNN, 2 - VGGFace2-based 2D CNN + SVM,": ""
        },
        {
          "1 - VGGFace2-based 2D CNN, 2 - VGGFace2-based 2D CNN + SVM,": "with correctly ﬁtted weights can increase the efﬁciency of the"
        },
        {
          "1 - VGGFace2-based 2D CNN, 2 - VGGFace2-based 2D CNN + SVM,": "system compare to standalone models. However, audio-based"
        },
        {
          "1 - VGGFace2-based 2D CNN, 2 - VGGFace2-based 2D CNN + SVM,": "models did not contribute to performance within the fusion"
        },
        {
          "1 - VGGFace2-based 2D CNN, 2 - VGGFace2-based 2D CNN + SVM,": "process, which can be caused by information insufﬁciency of"
        },
        {
          "1 - VGGFace2-based 2D CNN, 2 - VGGFace2-based 2D CNN + SVM,": "audios (the subjects often silence) or incorrectness of audio"
        },
        {
          "1 - VGGFace2-based 2D CNN, 2 - VGGFace2-based 2D CNN + SVM,": "data such as music, a voice from another source, and noises."
        },
        {
          "1 - VGGFace2-based 2D CNN, 2 - VGGFace2-based 2D CNN + SVM,": "In future work,\nit will be good to annotate audios manually"
        },
        {
          "1 - VGGFace2-based 2D CNN, 2 - VGGFace2-based 2D CNN + SVM,": ""
        },
        {
          "1 - VGGFace2-based 2D CNN, 2 - VGGFace2-based 2D CNN + SVM,": "for distinguishing different persons in audio and use seman-"
        },
        {
          "1 - VGGFace2-based 2D CNN, 2 - VGGFace2-based 2D CNN + SVM,": ""
        },
        {
          "1 - VGGFace2-based 2D CNN, 2 - VGGFace2-based 2D CNN + SVM,": "tic information extracted from a person’s speech to add new"
        },
        {
          "1 - VGGFace2-based 2D CNN, 2 - VGGFace2-based 2D CNN + SVM,": ""
        },
        {
          "1 - VGGFace2-based 2D CNN, 2 - VGGFace2-based 2D CNN + SVM,": "modality in the fusion system."
        },
        {
          "1 - VGGFace2-based 2D CNN, 2 - VGGFace2-based 2D CNN + SVM,": ""
        },
        {
          "1 - VGGFace2-based 2D CNN, 2 - VGGFace2-based 2D CNN + SVM,": ""
        },
        {
          "1 - VGGFace2-based 2D CNN, 2 - VGGFace2-based 2D CNN + SVM,": "5. ACKNOWLEDGEMENTS"
        },
        {
          "1 - VGGFace2-based 2D CNN, 2 - VGGFace2-based 2D CNN + SVM,": ""
        },
        {
          "1 - VGGFace2-based 2D CNN, 2 - VGGFace2-based 2D CNN + SVM,": "The research was supported by the German Federal Ministry"
        },
        {
          "1 - VGGFace2-based 2D CNN, 2 - VGGFace2-based 2D CNN + SVM,": ""
        },
        {
          "1 - VGGFace2-based 2D CNN, 2 - VGGFace2-based 2D CNN + SVM,": "of Education and Research project ”RobotKoop: Coopera-"
        },
        {
          "1 - VGGFace2-based 2D CNN, 2 - VGGFace2-based 2D CNN + SVM,": ""
        },
        {
          "1 - VGGFace2-based 2D CNN, 2 - VGGFace2-based 2D CNN + SVM,": "tive Interaction Strategies and Goal Negotiations with Learn-"
        },
        {
          "1 - VGGFace2-based 2D CNN, 2 - VGGFace2-based 2D CNN + SVM,": ""
        },
        {
          "1 - VGGFace2-based 2D CNN, 2 - VGGFace2-based 2D CNN + SVM,": "ing Autonomous Robots”. The experimental VGGFace2 re-"
        },
        {
          "1 - VGGFace2-based 2D CNN, 2 - VGGFace2-based 2D CNN + SVM,": "search part was supported by the Russian Science Foundation"
        },
        {
          "1 - VGGFace2-based 2D CNN, 2 - VGGFace2-based 2D CNN + SVM,": "(project No. 18-11-00145)."
        },
        {
          "1 - VGGFace2-based 2D CNN, 2 - VGGFace2-based 2D CNN + SVM,": ""
        },
        {
          "1 - VGGFace2-based 2D CNN, 2 - VGGFace2-based 2D CNN + SVM,": "6. REFERENCES"
        },
        {
          "1 - VGGFace2-based 2D CNN, 2 - VGGFace2-based 2D CNN + SVM,": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "[9] Heysem Kaya, Furkan G¨urpınar, and Albert Ali Salah,": "“Video-based emotion recognition in\nthe wild\nusing",
          "[20] Qiuqiang Kong, Yin Cao, Turab Iqbal, Yuxuan Wang,": "Wenwu Wang, and Mark D Plumbley,\n“Panns: Large-"
        },
        {
          "[9] Heysem Kaya, Furkan G¨urpınar, and Albert Ali Salah,": "deep transfer learning and score fusion,” Image and Vi-",
          "[20] Qiuqiang Kong, Yin Cao, Turab Iqbal, Yuxuan Wang,": "scale pretrained audio neural networks for audio pattern"
        },
        {
          "[9] Heysem Kaya, Furkan G¨urpınar, and Albert Ali Salah,": "sion Computing, vol. 65, pp. 66–75, 2017.",
          "[20] Qiuqiang Kong, Yin Cao, Turab Iqbal, Yuxuan Wang,": "recognition,” arXiv preprint arXiv:1912.10211, 2019."
        },
        {
          "[9] Heysem Kaya, Furkan G¨urpınar, and Albert Ali Salah,": "[10] Heysem Kaya, Furkan Gurpinar, and Albert Ali Salah,",
          "[20] Qiuqiang Kong, Yin Cao, Turab Iqbal, Yuxuan Wang,": "[21] Karen Simonyan and Andrew Zisserman,\n“Very deep"
        },
        {
          "[9] Heysem Kaya, Furkan G¨urpınar, and Albert Ali Salah,": "“Multi-modal\nscore fusion and decision trees\nfor ex-",
          "[20] Qiuqiang Kong, Yin Cao, Turab Iqbal, Yuxuan Wang,": "convolutional networks for\nlarge-scale image recogni-"
        },
        {
          "[9] Heysem Kaya, Furkan G¨urpınar, and Albert Ali Salah,": "plainable automatic job candidate screening from video",
          "[20] Qiuqiang Kong, Yin Cao, Turab Iqbal, Yuxuan Wang,": "tion,” arXiv preprint arXiv:1409.1556, 2014."
        },
        {
          "[9] Heysem Kaya, Furkan G¨urpınar, and Albert Ali Salah,": "cvs,” in IEEE CVPRW, 2017, pp. 1–9.",
          "[20] Qiuqiang Kong, Yin Cao, Turab Iqbal, Yuxuan Wang,": ""
        },
        {
          "[9] Heysem Kaya, Furkan G¨urpınar, and Albert Ali Salah,": "",
          "[20] Qiuqiang Kong, Yin Cao, Turab Iqbal, Yuxuan Wang,": "[22]\nJort F Gemmeke, Daniel PW Ellis, Dylan Freedman,"
        },
        {
          "[9] Heysem Kaya, Furkan G¨urpınar, and Albert Ali Salah,": "[11] Qiong Cao, Li Shen, Weidi Xie, Omkar M Parkhi, and",
          "[20] Qiuqiang Kong, Yin Cao, Turab Iqbal, Yuxuan Wang,": ""
        },
        {
          "[9] Heysem Kaya, Furkan G¨urpınar, and Albert Ali Salah,": "",
          "[20] Qiuqiang Kong, Yin Cao, Turab Iqbal, Yuxuan Wang,": "Aren\nJansen, Wade Lawrence, R Channing Moore,"
        },
        {
          "[9] Heysem Kaya, Furkan G¨urpınar, and Albert Ali Salah,": "Andrew Zisserman,\n“VGGFace2: A dataset for recog-",
          "[20] Qiuqiang Kong, Yin Cao, Turab Iqbal, Yuxuan Wang,": ""
        },
        {
          "[9] Heysem Kaya, Furkan G¨urpınar, and Albert Ali Salah,": "",
          "[20] Qiuqiang Kong, Yin Cao, Turab Iqbal, Yuxuan Wang,": "Manoj Plakal,\nand Marvin Ritter,\n“Audio set:\nAn"
        },
        {
          "[9] Heysem Kaya, Furkan G¨urpınar, and Albert Ali Salah,": "nising faces across pose and age,”\nin IEEE FG, 2018,",
          "[20] Qiuqiang Kong, Yin Cao, Turab Iqbal, Yuxuan Wang,": ""
        },
        {
          "[9] Heysem Kaya, Furkan G¨urpınar, and Albert Ali Salah,": "",
          "[20] Qiuqiang Kong, Yin Cao, Turab Iqbal, Yuxuan Wang,": "ontology and human-labeled dataset\nfor audio events,”"
        },
        {
          "[9] Heysem Kaya, Furkan G¨urpınar, and Albert Ali Salah,": "pp. 67–74.",
          "[20] Qiuqiang Kong, Yin Cao, Turab Iqbal, Yuxuan Wang,": ""
        },
        {
          "[9] Heysem Kaya, Furkan G¨urpınar, and Albert Ali Salah,": "",
          "[20] Qiuqiang Kong, Yin Cao, Turab Iqbal, Yuxuan Wang,": "in 2017 IEEE International Conference on Acoustics,"
        },
        {
          "[9] Heysem Kaya, Furkan G¨urpınar, and Albert Ali Salah,": "",
          "[20] Qiuqiang Kong, Yin Cao, Turab Iqbal, Yuxuan Wang,": "Speech and Signal Processing (ICASSP).\nIEEE, 2017,"
        },
        {
          "[9] Heysem Kaya, Furkan G¨urpınar, and Albert Ali Salah,": "[12] Franc¸ois\nChollet\net\nal.,\n“Keras,”",
          "[20] Qiuqiang Kong, Yin Cao, Turab Iqbal, Yuxuan Wang,": ""
        },
        {
          "[9] Heysem Kaya, Furkan G¨urpınar, and Albert Ali Salah,": "",
          "[20] Qiuqiang Kong, Yin Cao, Turab Iqbal, Yuxuan Wang,": "pp. 776–780."
        },
        {
          "[9] Heysem Kaya, Furkan G¨urpınar, and Albert Ali Salah,": "https://keras.io, 2015.",
          "[20] Qiuqiang Kong, Yin Cao, Turab Iqbal, Yuxuan Wang,": ""
        },
        {
          "[9] Heysem Kaya, Furkan G¨urpınar, and Albert Ali Salah,": "",
          "[20] Qiuqiang Kong, Yin Cao, Turab Iqbal, Yuxuan Wang,": "[23] Heysem Kaya,\nFurkan G¨urpinar,\nSadaf Afshar,\nand"
        },
        {
          "[9] Heysem Kaya, Furkan G¨urpınar, and Albert Ali Salah,": "[13]\nIlya\nLoshchilov\nand\nFrank Hutter,\n“Decoupled",
          "[20] Qiuqiang Kong, Yin Cao, Turab Iqbal, Yuxuan Wang,": ""
        },
        {
          "[9] Heysem Kaya, Furkan G¨urpınar, and Albert Ali Salah,": "",
          "[20] Qiuqiang Kong, Yin Cao, Turab Iqbal, Yuxuan Wang,": "Albert Ali Salah,\n“Contrasting and combining least"
        },
        {
          "[9] Heysem Kaya, Furkan G¨urpınar, and Albert Ali Salah,": "arXiv\npreprint\nweight\ndecay\nregularization,”",
          "[20] Qiuqiang Kong, Yin Cao, Turab Iqbal, Yuxuan Wang,": ""
        },
        {
          "[9] Heysem Kaya, Furkan G¨urpınar, and Albert Ali Salah,": "",
          "[20] Qiuqiang Kong, Yin Cao, Turab Iqbal, Yuxuan Wang,": "squares based learners for emotion recognition in the"
        },
        {
          "[9] Heysem Kaya, Furkan G¨urpınar, and Albert Ali Salah,": "arXiv:1711.05101, 2017.",
          "[20] Qiuqiang Kong, Yin Cao, Turab Iqbal, Yuxuan Wang,": ""
        },
        {
          "[9] Heysem Kaya, Furkan G¨urpınar, and Albert Ali Salah,": "",
          "[20] Qiuqiang Kong, Yin Cao, Turab Iqbal, Yuxuan Wang,": "wild,” in ACM ICMI, 2015, pp. 459–466."
        },
        {
          "[9] Heysem Kaya, Furkan G¨urpınar, and Albert Ali Salah,": "[14] O. M. Parkhi, A. Vedaldi, and A. Zisserman,\n“Deep",
          "[20] Qiuqiang Kong, Yin Cao, Turab Iqbal, Yuxuan Wang,": ""
        },
        {
          "[9] Heysem Kaya, Furkan G¨urpınar, and Albert Ali Salah,": "face recognition,”\nin British Machine Vision Confer-",
          "[20] Qiuqiang Kong, Yin Cao, Turab Iqbal, Yuxuan Wang,": ""
        },
        {
          "[9] Heysem Kaya, Furkan G¨urpınar, and Albert Ali Salah,": "ence, 2015.",
          "[20] Qiuqiang Kong, Yin Cao, Turab Iqbal, Yuxuan Wang,": ""
        },
        {
          "[9] Heysem Kaya, Furkan G¨urpınar, and Albert Ali Salah,": "[15]\nIan\nJ Goodfellow, Dumitru Erhan,\nPierre Luc Car-",
          "[20] Qiuqiang Kong, Yin Cao, Turab Iqbal, Yuxuan Wang,": ""
        },
        {
          "[9] Heysem Kaya, Furkan G¨urpınar, and Albert Ali Salah,": "rier, Aaron Courville, Mehdi Mirza, Ben Hamner, Will",
          "[20] Qiuqiang Kong, Yin Cao, Turab Iqbal, Yuxuan Wang,": ""
        },
        {
          "[9] Heysem Kaya, Furkan G¨urpınar, and Albert Ali Salah,": "Cukierski, Yichuan Tang, David Thaler, Dong-Hyun",
          "[20] Qiuqiang Kong, Yin Cao, Turab Iqbal, Yuxuan Wang,": ""
        },
        {
          "[9] Heysem Kaya, Furkan G¨urpınar, and Albert Ali Salah,": "Lee, et al.,\n“Challenges in representation learning: A",
          "[20] Qiuqiang Kong, Yin Cao, Turab Iqbal, Yuxuan Wang,": ""
        },
        {
          "[9] Heysem Kaya, Furkan G¨urpınar, and Albert Ali Salah,": "report on three machine learning contests,”\nin Inter-",
          "[20] Qiuqiang Kong, Yin Cao, Turab Iqbal, Yuxuan Wang,": ""
        },
        {
          "[9] Heysem Kaya, Furkan G¨urpınar, and Albert Ali Salah,": "national Conference on Neural Information Processing.",
          "[20] Qiuqiang Kong, Yin Cao, Turab Iqbal, Yuxuan Wang,": ""
        },
        {
          "[9] Heysem Kaya, Furkan G¨urpınar, and Albert Ali Salah,": "Springer, 2013, pp. 117–124.",
          "[20] Qiuqiang Kong, Yin Cao, Turab Iqbal, Yuxuan Wang,": ""
        },
        {
          "[9] Heysem Kaya, Furkan G¨urpınar, and Albert Ali Salah,": "[16] Furkan G¨urpınar, Heysem Kaya, and Albert Ali Salah,",
          "[20] Qiuqiang Kong, Yin Cao, Turab Iqbal, Yuxuan Wang,": ""
        },
        {
          "[9] Heysem Kaya, Furkan G¨urpınar, and Albert Ali Salah,": "“Multimodal Fusion of Audio, Scene, and Face Features",
          "[20] Qiuqiang Kong, Yin Cao, Turab Iqbal, Yuxuan Wang,": ""
        },
        {
          "[9] Heysem Kaya, Furkan G¨urpınar, and Albert Ali Salah,": "for First Impression Estimation,”\nin 23rd International",
          "[20] Qiuqiang Kong, Yin Cao, Turab Iqbal, Yuxuan Wang,": ""
        },
        {
          "[9] Heysem Kaya, Furkan G¨urpınar, and Albert Ali Salah,": "Conference on Pattern Recognition, Cancun, Mexico,",
          "[20] Qiuqiang Kong, Yin Cao, Turab Iqbal, Yuxuan Wang,": ""
        },
        {
          "[9] Heysem Kaya, Furkan G¨urpınar, and Albert Ali Salah,": "December 2016, pp. 43–48.",
          "[20] Qiuqiang Kong, Yin Cao, Turab Iqbal, Yuxuan Wang,": ""
        },
        {
          "[9] Heysem Kaya, Furkan G¨urpınar, and Albert Ali Salah,": "[17] V´ıctor Ponce-L´opez, Baiyu Chen, Marc Oliu, Ciprian",
          "[20] Qiuqiang Kong, Yin Cao, Turab Iqbal, Yuxuan Wang,": ""
        },
        {
          "[9] Heysem Kaya, Furkan G¨urpınar, and Albert Ali Salah,": "Corneanu, Albert Clap´es, Isabelle Guyon, Xavier Bar´o,",
          "[20] Qiuqiang Kong, Yin Cao, Turab Iqbal, Yuxuan Wang,": ""
        },
        {
          "[9] Heysem Kaya, Furkan G¨urpınar, and Albert Ali Salah,": "Hugo Jair Escalante, and Sergio Escalera,\n“Chalearn",
          "[20] Qiuqiang Kong, Yin Cao, Turab Iqbal, Yuxuan Wang,": ""
        },
        {
          "[9] Heysem Kaya, Furkan G¨urpınar, and Albert Ali Salah,": "lap 2016: First\nround challenge on ﬁrst\nimpressions -",
          "[20] Qiuqiang Kong, Yin Cao, Turab Iqbal, Yuxuan Wang,": ""
        },
        {
          "[9] Heysem Kaya, Furkan G¨urpınar, and Albert Ali Salah,": "dataset and results,”\nin Computer Vision – ECCV 2016",
          "[20] Qiuqiang Kong, Yin Cao, Turab Iqbal, Yuxuan Wang,": ""
        },
        {
          "[9] Heysem Kaya, Furkan G¨urpınar, and Albert Ali Salah,": "Workshops, Gang Hua and Herv´e J´egou, Eds., Cham,",
          "[20] Qiuqiang Kong, Yin Cao, Turab Iqbal, Yuxuan Wang,": ""
        },
        {
          "[9] Heysem Kaya, Furkan G¨urpınar, and Albert Ali Salah,": "2016, pp. 400–418, Springer International Publishing.",
          "[20] Qiuqiang Kong, Yin Cao, Turab Iqbal, Yuxuan Wang,": ""
        },
        {
          "[9] Heysem Kaya, Furkan G¨urpınar, and Albert Ali Salah,": "[18] Guang-Bin Huang, Hongming Zhou, Xiaojian Ding,",
          "[20] Qiuqiang Kong, Yin Cao, Turab Iqbal, Yuxuan Wang,": ""
        },
        {
          "[9] Heysem Kaya, Furkan G¨urpınar, and Albert Ali Salah,": "and Rui Zhang, “Extreme Learning Machine for Regres-",
          "[20] Qiuqiang Kong, Yin Cao, Turab Iqbal, Yuxuan Wang,": ""
        },
        {
          "[9] Heysem Kaya, Furkan G¨urpınar, and Albert Ali Salah,": "IEEE Transactions\nsion and Multiclass Classiﬁcation,”",
          "[20] Qiuqiang Kong, Yin Cao, Turab Iqbal, Yuxuan Wang,": ""
        },
        {
          "[9] Heysem Kaya, Furkan G¨urpınar, and Albert Ali Salah,": "on Systems, Man, and Cybernetics, Part B: Cybernetics,",
          "[20] Qiuqiang Kong, Yin Cao, Turab Iqbal, Yuxuan Wang,": ""
        },
        {
          "[9] Heysem Kaya, Furkan G¨urpınar, and Albert Ali Salah,": "vol. 42, no. 2, pp. 513–529, 2012.",
          "[20] Qiuqiang Kong, Yin Cao, Turab Iqbal, Yuxuan Wang,": ""
        },
        {
          "[9] Heysem Kaya, Furkan G¨urpınar, and Albert Ali Salah,": "[19] Romain Hennequin, Anis Khlif,\nFelix Voituret,\nand",
          "[20] Qiuqiang Kong, Yin Cao, Turab Iqbal, Yuxuan Wang,": ""
        },
        {
          "[9] Heysem Kaya, Furkan G¨urpınar, and Albert Ali Salah,": "Manuel Moussallam,\n“Spleeter:\na\nfast\nand efﬁcient",
          "[20] Qiuqiang Kong, Yin Cao, Turab Iqbal, Yuxuan Wang,": ""
        },
        {
          "[9] Heysem Kaya, Furkan G¨urpınar, and Albert Ali Salah,": "music source separation tool with pre-trained models,”",
          "[20] Qiuqiang Kong, Yin Cao, Turab Iqbal, Yuxuan Wang,": ""
        },
        {
          "[9] Heysem Kaya, Furkan G¨urpınar, and Albert Ali Salah,": "Journal of Open Source Software, vol. 5, no. 50, pp.",
          "[20] Qiuqiang Kong, Yin Cao, Turab Iqbal, Yuxuan Wang,": ""
        },
        {
          "[9] Heysem Kaya, Furkan G¨urpınar, and Albert Ali Salah,": "2154, 2020, Deezer Research.",
          "[20] Qiuqiang Kong, Yin Cao, Turab Iqbal, Yuxuan Wang,": ""
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "This figure \"model_1_d_cnn.png\" is available in \"png\"(cid:10) format from:": "http://arxiv.org/ps/2010.03692v3"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "This figure \"pipeline_fusion_system.png\" is available in \"png\"(cid:10) format from:": "http://arxiv.org/ps/2010.03692v3"
        }
      ],
      "page": 7
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Collecting large, richly annotated facial-expression databases from movies",
      "authors": [
        "Abhinav Dhall",
        "Roland Goecke",
        "Simon Lucey",
        "Tom Gedeon"
      ],
      "year": "2012",
      "venue": "IEEE MultiMedia"
    },
    {
      "citation_id": "3",
      "title": "From individual to group-level emotion recognition: Emotiw 5.0",
      "authors": [
        "Abhinav Dhall",
        "Roland Goecke",
        "Shreya Ghosh",
        "Jyoti Joshi",
        "Jesse Hoey",
        "Tom Gedeon"
      ],
      "year": "2017",
      "venue": "Proc.ACM ICMI"
    },
    {
      "citation_id": "4",
      "title": "Recognition of affect in the wild using deep neural networks",
      "authors": [
        "Dimitrios Kollias",
        "A Mihalis",
        "Irene Nicolaou",
        "Guoying Kotsia",
        "Stefanos Zhao",
        "Zafeiriou"
      ],
      "year": "2017",
      "venue": "IEEE"
    },
    {
      "citation_id": "5",
      "title": "Aff-wild: Valence and arousal 'in-thewild'challenge",
      "authors": [
        "Stefanos Zafeiriou",
        "Dimitrios Kollias",
        "A Mihalis",
        "Athanasios Nicolaou",
        "Guoying Papaioannou",
        "Irene Zhao",
        "Kotsia"
      ],
      "year": "2017",
      "venue": "IEEE"
    },
    {
      "citation_id": "6",
      "title": "Deep affect prediction in-the-wild: Aff-wild database and challenge, deep architectures, and beyond",
      "authors": [
        "Dimitrios Kollias",
        "Panagiotis Tzirakis",
        "A Mihalis",
        "Athanasios Nicolaou",
        "Guoying Papaioannou",
        "Björn Zhao",
        "Irene Schuller",
        "Stefanos Kotsia",
        "Zafeiriou"
      ],
      "year": "2019",
      "venue": "International Journal of Computer Vision"
    },
    {
      "citation_id": "7",
      "title": "A multitask learning & generation framework: Valence-arousal, action units & primary expressions",
      "authors": [
        "Dimitrios Kollias",
        "Stefanos Zafeiriou"
      ],
      "year": "2018",
      "venue": "A multitask learning & generation framework: Valence-arousal, action units & primary expressions",
      "arxiv": "arXiv:1811.07771"
    },
    {
      "citation_id": "8",
      "title": "Aff-wild2: Extending the aff-wild database for affect recognition",
      "authors": [
        "Dimitrios Kollias",
        "Stefanos Zafeiriou"
      ],
      "year": "2018",
      "venue": "Aff-wild2: Extending the aff-wild database for affect recognition",
      "arxiv": "arXiv:1811.07770"
    },
    {
      "citation_id": "9",
      "title": "Expression, affect, action unit recognition: Aff-wild2, multi-task learning and arcface",
      "authors": [
        "Dimitrios Kollias",
        "Stefanos Zafeiriou"
      ],
      "year": "2019",
      "venue": "Expression, affect, action unit recognition: Aff-wild2, multi-task learning and arcface",
      "arxiv": "arXiv:1910.04855"
    },
    {
      "citation_id": "10",
      "title": "Video-based emotion recognition in the wild using deep transfer learning and score fusion",
      "authors": [
        "Heysem Kaya",
        "Furkan Gürpınar",
        "Albert Salah"
      ],
      "year": "2017",
      "venue": "Image and Vision Computing"
    },
    {
      "citation_id": "11",
      "title": "Multi-modal score fusion and decision trees for explainable automatic job candidate screening from video cvs",
      "authors": [
        "Heysem Kaya",
        "Furkan Gurpinar",
        "Albert Salah"
      ],
      "year": "2017",
      "venue": "IEEE"
    },
    {
      "citation_id": "12",
      "title": "VGGFace2: A dataset for recognising faces across pose and age",
      "authors": [
        "Qiong Cao",
        "Li Shen",
        "Weidi Xie",
        "Omkar Parkhi",
        "Andrew Zisserman"
      ],
      "year": "2018",
      "venue": "IEEE FG"
    },
    {
      "citation_id": "13",
      "title": "Keras",
      "authors": [
        "Chollet Franc"
      ],
      "year": "2015",
      "venue": "Keras"
    },
    {
      "citation_id": "14",
      "title": "Decoupled weight decay regularization",
      "authors": [
        "Ilya Loshchilov",
        "Frank Hutter"
      ],
      "year": "2017",
      "venue": "Decoupled weight decay regularization",
      "arxiv": "arXiv:1711.05101"
    },
    {
      "citation_id": "15",
      "title": "Deep face recognition",
      "authors": [
        "O Parkhi",
        "A Vedaldi",
        "A Zisserman"
      ],
      "year": "2015",
      "venue": "British Machine Vision Conference"
    },
    {
      "citation_id": "16",
      "title": "Challenges in representation learning: A report on three machine learning contests",
      "authors": [
        "Ian Goodfellow",
        "Dumitru Erhan",
        "Pierre Carrier",
        "Aaron Courville",
        "Mehdi Mirza",
        "Ben Hamner",
        "Will Cukierski",
        "Yichuan Tang",
        "David Thaler",
        "Dong-Hyun Lee"
      ],
      "year": "2013",
      "venue": "International Conference on Neural Information Processing"
    },
    {
      "citation_id": "17",
      "title": "Multimodal Fusion of Audio, Scene, and Face Features for First Impression Estimation",
      "authors": [
        "Furkan Gürpınar",
        "Heysem Kaya",
        "Albert Salah"
      ],
      "year": "2016",
      "venue": "23rd International Conference on Pattern Recognition"
    },
    {
      "citation_id": "18",
      "title": "Chalearn lap 2016: First round challenge on first impressionsdataset and results",
      "authors": [
        "Víctor Ponce-López",
        "Baiyu Chen",
        "Marc Oliu",
        "Ciprian Corneanu",
        "Albert Clapés",
        "Isabelle Guyon",
        "Xavier Baró",
        "Hugo Escalante",
        "Sergio Escalera"
      ],
      "year": "2016",
      "venue": "Computer Vision -ECCV"
    },
    {
      "citation_id": "19",
      "title": "Extreme Learning Machine for Regression and Multiclass Classification",
      "authors": [
        "Guang-Bin Huang",
        "Hongming Zhou",
        "Xiaojian Ding",
        "Rui Zhang"
      ],
      "year": "2012",
      "venue": "IEEE Transactions on Systems, Man, and Cybernetics, Part B: Cybernetics"
    },
    {
      "citation_id": "20",
      "title": "Spleeter: a fast and efficient music source separation tool with pre-trained models",
      "authors": [
        "Romain Hennequin",
        "Anis Khlif",
        "Felix Voituret",
        "Manuel Moussallam"
      ],
      "year": "2020",
      "venue": "Journal of Open Source Software"
    },
    {
      "citation_id": "21",
      "title": "Panns: Largescale pretrained audio neural networks for audio pattern recognition",
      "authors": [
        "Qiuqiang Kong",
        "Yin Cao",
        "Turab Iqbal",
        "Yuxuan Wang",
        "Wenwu Wang",
        "Mark Plumbley"
      ],
      "year": "2019",
      "venue": "Panns: Largescale pretrained audio neural networks for audio pattern recognition",
      "arxiv": "arXiv:1912.10211"
    },
    {
      "citation_id": "22",
      "title": "Very deep convolutional networks for large-scale image recognition",
      "authors": [
        "Karen Simonyan",
        "Andrew Zisserman"
      ],
      "year": "2014",
      "venue": "Very deep convolutional networks for large-scale image recognition",
      "arxiv": "arXiv:1409.1556"
    },
    {
      "citation_id": "23",
      "title": "Audio set: An ontology and human-labeled dataset for audio events",
      "authors": [
        "Jort F Gemmeke",
        "P Daniel",
        "Dylan Ellis",
        "Aren Freedman",
        "Wade Jansen",
        "R Channing Lawrence",
        "Manoj Moore",
        "Marvin Plakal",
        "Ritter"
      ],
      "year": "2017",
      "venue": "2017 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "24",
      "title": "Contrasting and combining least squares based learners for emotion recognition in the wild",
      "authors": [
        "Heysem Kaya",
        "Furkan Gürpinar",
        "Sadaf Afshar",
        "Albert Salah"
      ],
      "year": "2015",
      "venue": "ACM ICMI"
    },
    {
      "citation_id": "25",
      "title": "This figure \"model_1_d_cnn.png\" is available in \"png\" format from",
      "venue": "This figure \"model_1_d_cnn.png\" is available in \"png\" format from"
    },
    {
      "citation_id": "26",
      "title": "This figure \"pipeline_fusion_system.png\" is available in \"png\" format from",
      "venue": "This figure \"pipeline_fusion_system.png\" is available in \"png\" format from"
    }
  ]
}