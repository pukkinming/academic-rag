{
  "paper_id": "2104.08806v2",
  "title": "Best Practices For Noise-Based Augmentation To Improve The Performance Of Deployable Speech-Based Emotion Recognition Systems",
  "published": "2021-04-18T10:33:38Z",
  "authors": [
    "Mimansa Jaiswal",
    "Emily Mower Provost"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Speech emotion recognition is an important component of any human centered system. But speech characteristics produced and perceived by a person can be influenced by a multitude of reasons, both desirable such as emotion, and undesirable such as noise. To train robust emotion recognition models, we need a large, yet realistic data distribution, but emotion datasets are often small and hence are augmented with noise. Often noise augmentation makes one important assumption, that the prediction label should remain the same in presence or absence of noise, which is true for automatic speech recognition but not necessarily true for perception based tasks. In this paper we make three novel contributions. We validate through crowdsourcing that the presence of noise does change the annotation label and hence may alter the original ground truth label. We then show how disregarding this knowledge and assuming consistency in ground truth labels propagates to downstream evaluation of ML models, both for performance evaluation and robustness testing. We end the paper with a set of recommendations for noise augmentations in speech emotion recognition datasets.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Speech emotion recognition is increasingly included as a component in many real-world humancentered machine learning models. Modulations in speech can be produced for a multitude of reasons, both desirable and undesirable. In our case desirable modulations encode information that we want our model to learn and be informed by, such as speaker characteristics or emotion. Undesirable modulations encode information that are extrinsic factors change with the environment, such as noise. In order to handle these modulations, we need large datasets that capture the range of possible speech variations and their relationship to emotion expression. But, such datasets are generally not available for emotion tasks. To bridge this gap, researchers have proposed various methods to generate larger datasets. One of the most common is noise augmentation. The baseline assumption of noise augmentation is that the labels of the emotion examples do not change once noise has been added  (Pappagari et al., 2021) . While this assumption can be confidently made for tasks such as automatic speech recognition (ASR), the same cannot be said for perception-based tasks, such as emotion recognition.\n\nIn this paper, we question the assumption that the annotation label remains the same in the presence of noise. We first create a noise augmented dataset and conduct a perception study to label the emotion of these augmented samples, focused on the type of noise in samples whose perception has changed or remained the same given the agumentation. We use the results from this study to classify the complete set of augmentation noises into two categories, perception-altering (i.e., noises that may change the perception of emotion) and perception-retaining (i.e., noises that do not change the perception of emotion). We propose that the perception-altering noises should not be used in supervised learning or evaluation frameworks because we cannot confidently maintain that the original annotation holds for a given sample. We evaluate the effects of disregarding emotion perception changes by examining how the performance of emotion recognition models and analyses of their robustness change in unpredictable manners when we include samples that alter human perception in the training of these models. Lastly, we provide a set of recommendations for noise based augmentation of speech emotion recognition datasets based on our results.\n\nResearchers have considered the impact of noise on emotion perception and thereby the annotation of emotions. [X] looked at how pink and white noises in varying intensities change the perception of emotion. Another set of research has concen-arXiv:2104.08806v2 [cs.SD] 31 Aug 2023 trated on training and validating noise robust models with the assumption that intent label prediction remains consistent in the presence of noise. For example, [X] have looked at training student teacher models that aim to ignore the effect of noise introduced to the model. On the other hand [X] have proposed copy pasting various emotion segments together along with neutral noise to balance the classes in an emotion dataset, thus improving performance.\n\nIn this paper, we claim that the standard assumption about perception and hence, label retention of emotion in the presence of noise may not hold true in a multiple noise categories. To understand which noises impact emotion perception, we use a common emotion dataset, IEMOCAP and introduce various kinds of noises to it, at varying signal to noise ratio (SNR) levels as well as at different positions in the sample. We then perform a crowdsourcing experiment that asks workers to annotate their perception of emotion for both the clean and the corresponding noise-augmented sample. This enables us to divide noise augmentation options into groups characterized by their potential to either influence or not influence human perception.\n\nThe results of the crowdsourcing experiments inform a series of empircal analyses focused on model performance and model robustness. We first present an empirical evaluation of the effects of including perception-altering noises in training. It will allow us to observe how the inclusion of perception-altering noises creates an impression of performance improvement. We will discuss how this improvement is a myth, this new model will have learned to predict labels that are not truly associated with a given sample due to the perceptual effects of these noises. We consider both a general recurrent neural network (RNN) model and an endto-end model for this purpose. We evaluate conditions in which novel augmentation noises are either introduced during training (matched) or seen for the first time during testing (mismatched). The second empirical evaluation analyzes whether the gap in performance between the matched and mismatched conditions can be bridged using noise robust modeling techniques. The third and final evaluation is focused on the robustness of the model. It will allow us to observe how the inclusion of these perception altering noises ultimately leads to a model that is more susceptible to attack compared to a model that does not include these noises. We train an attack model for robustness testing. It considers a pool of noises and picks the best noise with a minimal SNR degradation that is able to change a model's prediction. We consider a condition in which the attack model has black-box access to the trained model. The attack has a fixed number of allowed queries to the trained model, but not the internal gradients or structure (i.e., the attack model can only provide input and can only access the trained model's prediction). We test and monitor the difference in the observed robustness of these aforementioned models.\n\nWe find that the crowdsourced labels do change in the presence of some kinds of noise. We then verify that the models perform worse on noisy samples when trained only on clean datasets. But, we show that this decrease in performance is different when using the complete set of noises for augmenting the test set vs. when only using the perceptionretaining noises for augmentation. We show similar patterns for noise-robust models, specifically showing how there is an increased drop in performance for the end-to-end noise-robust model when excluding performance-altering noises during augmentation. We then discuss how our conventional metrics, those that look only at model performance, may be incorrectly asserting improvements as the model is learning to predict an emotion measure that is not in line with human perception. Troublingly, we find that the attack model is generally more effective when it has access to the set of all noises as compared to when excluding perception-altering noises for allowed augmentations. We also specifically find that given just a pool of carefully crafted reverberation modulations, the attack model can be successful in almost 65% of the cases with minimal degradation in SNR and in less than ten queries to the trained model. We end the paper with a general set of recommendations for noise augmentations in speech emotion recognition datasets.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Research Questions",
      "text": "In this paper, we investigate five research questions:\n\nPurpose 1: Premise Validation through Crowdsourcing RQ1: Does the presence of noise affect emotion perception as evaluated by human raters? Is this effect dependent on the utterance length, loudness, the type of the added noise, and the original emotion? Reason: Noise has been known to have masking effect on humans in specific situations. Hence, humans can often understand verbalized content even in presence of noise. Our goal is to understand whether the same masking effect extends to paralinguistic cues such as emotion, and to what extent. Our continuing claim from hereon remains that only noises that do not change human perception should be used for the training and evaluation of machine learning models. Not doing so, can lead to gains or drops in performance measurement that may not actually extend to real world settings. We call these changes \"unverified\" because we cannot, with certainity, be sure that the model should have predicted the original label (i.e., the label of the sample before noise was added) because the human did not neccessarily label the noisy instance with that same label.\n\nPurpose 2: Noise Impact Quantification RQ2: Can we verify previous findings that the presence of noise affects the performance of emotion recognition models? Does this effect vary based on the type of the added noise? Reason: We have known that presence of noise in data shifts the data distribution  (Chenchah and Lachiri, 2016) . This shift often leads to poor performance by machine learning models. We aim to quantify the amount of performance drop based on the type of noise in these systems, both, for any kind of noise, and then, specifically for noises that do not change human perception (perceptionretaining).\n\nPurpose 3: Denoising and Augmentation Benefits Evaluation RQ3: Does dataset augmentation (Q3a) and/or sample denoising (Q3b) help improve the robustness of emotion recognition models to unseen noise? Reason: We test whether the commonly-used methods for improving the performance of these models under distribution shifts is helpful. We focus on two main methods, augmentation and denoising. We specifically look at how performance changes when we augment with noises that include those that are perception-altering vs. when we exclude such noises.\n\nPurpose 4: Model Robustness Testing Conditions RQ4: How does the robustness of a model to attacks compare when we are using test samples that with are augmented with perception-retaining noise vs. samples that are augmented with all types of noise, regardless of their effect on perception? Reason: Another major metric for any deployable machine learning algorithm is its performance on \"unseen situations\" or handling incoming data shifts (i.e., robustness testing). We test robustness using a noise augmentation algorithm that aims to forcefully and efficiently change a model's output by augmenting test samples with noise. We look at how often this algorithm is unsuccessful in being able to \"fool\" a model with its augmented samples. We look at the changes in frequency with which a model is successfully able to defend itself when the attack algorithm uses a set that includes all types of noises vs. when it only uses perception-retaining noises.\n\nPurpose 5: Recommendations RQ5: What are the recommended practices for speech emotion dataset augmentation and model deployment? Reason: We then provide a set of recommendations based on our empirical studies for deploying emotion recognition models in real world situations.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Related Work",
      "text": "Previous work that has focused on measuring how noise impacts the performance of machine learning models can be classified into three main directions: emotion recognition and noise-robust models, speech augmentation for classification purposes, and robustness testing in speech-based model training.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Emotion Recognition",
      "text": "End-to-End models are a recent paradigm for audio classification. The two major end-to-end models often used for speech classification tasks are: DeepSpeech  (Hannun et al., 2014)     and wav2vec  (1.0  (Schneider et al., 2019) , 2.0  (Baevski et al., 2020) ). Researchers have used the latest version of these pretrained models for recognizing speech emotion in various languages  (Pepino et al., 2021; Mohamed and Aly, 2021) . Researchers have also looked at extending traditional deep learning models to end-to-end models using multiple cojoined networks. For example, Amirhossein et al. investigated how attention mechanisms could be processed at different layers of an end-to-end model to improve both speaker and emotion recognition  (Hajavi and Etemad, 2021) . In another approach, researchers have also analyzed how waveforms could be treated as concatenated image blocks and used this mechanism to perform real-time speech emotion recognition from an incoming audio stream  (Lech et al., 2020) . Other researchers have also looked at comparisons between performance of wav2vec2, wavBert, and HuBert, to understand which models perform the best for speech emotion recognition given a set of pre-known conditions  (Mohamed and Aly, 2021) .\n\nResearchers have explored techniques for noisebased data augmentation to improve noise robustness  (Kim and Kim) , focusing on how model training can be improved to yield better performance. However, these augmentation techniques tend to focus on acoustic event detection, speaker verification or speech recognition  (Ko et al., 2015) , and have been sparingly used in audio-based paralinguistic classification tasks.\n\nThe common way to deal with noise in any audio signal is to use denoising algorithms. Hence, it is important to understand how machine learning models trained to recognize emotions perform if they are tested on denoised samples. Two common approaches include: Denoising Feature Space  (Valin, 2018)  and Speech Enhancement  (Chakraborty et al., 2019) . Denoising feature space algorithms seek to remove noise from the extracted front end features. Speech enhancement algorithms seek to convert noisy speech to more intelligible speech. Both techniques are associated with challenges, from signal to harmonic dissonance  (Valin, 2018) .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Noise Augmentation For Model Robustness",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Noise Augmentation In Speech Recognition",
      "text": "Research in speech recognition, rather than emotion recognition, has also tackled this problem.\n\nResearchers have investigated methods to build speech recognition systems that are robust to various kinds and levels of noise  (Li et al., 2014) . The common themes are a concentration on either data augmentation or gathering more real-world data to produce accurate transcripts  (Zheng et al., 2016) .\n\nOther lines of work have looked into preventing various attacks, e.g., spoofing or recording playback, on speaker verification systems  (Shim et al.) .\n\nNoise in these systems is usually considered to be caused by reverberations or channel-based modulations  (Zhao et al., 2014) .",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Noise Robustness In Emotion",
      "text": "The general method in emotion recognition, like speech recognition, involves the addition of noise to the original sample. Researchers make use of publicly available and hierarchically categorized noise samples  (Chenchah and Lachiri, 2016)  or introduce signal distortions to the sample itself.\n\nFor example, researchers have looked at speed perturbation, additive white noise, vocal tract length perturbation and temp perturbation for augmenting speech emotion recognition datasets  (Nicolás et al., 2022) .\n\nThe main concern with this approach is the underlying assumption that the addition of any noise in the background would not change the emotion label. It is important to note that most emotion recognition datasets are labelled as \"perception of others\" (i.e., annotated by an outside group of obervers, rather than the speaker themself) and are not self-reported. Intuitively, one might think that we should still be able to predict the \"correct\" label in the presence of noise because the speaker's emotion did not change. But given that the labels do not represent a speaker's evaluation of their emotion, rather, fall into the category of how others perceive another person's emotion, noise can (and does, as we show later and as supported by prior work in psychology  (Ma and Thompson, 2015) ), change the emotion label.\n\nAnother recent approach, CopyPaste  (Pappagari et al., 2021) , takes a speaker-specific approach, augmenting samples from a single speaker with neutral examples from that speaker. They argue that this approach will maintain the emotion label and, as a result of data augmentation, help improve performance.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Robustness Testing",
      "text": "A separate line of research has used noise to evaluate the robustness of a given model. For example, researchers have focused on adversarial example generation, aiming to create audio samples that change the output of the classifier. However, these methods assume white-box access to the network and create modifications in either feature vectors or actual wav files  (Carlini and Wagner) . This generally results in samples that either have perceptible differences when played back to a human, or are imperceptible to a human but fail to attack the model when played over-air  (Carlini and Wagner) .\n\nModel robustness to noise or an adversarial at-tack can be evaluated by adding noise to the dataset and testing performance of the model. This method is commonly used for various tasks, such as, speech recognition, or speaker identification  (Abdullah et al., 2019) , whose perception is ideally independent of noise as discussed before. The closest task to ours, where the ground truth varies based on noise introduction, is sentiment analysis. In this case, the adversarial robustness is usually tested by flipping words to their synonyms, such that the meaning of the text remains the same, and analyzing how the predictions of the model change  (Ebrahimi et al., 2017) . There is not a direct parallel for the acoustics of speech. Hence, the introduction of noise for emotion recognition while assuring that the perception remains the same can be more difficult.\n\nTo the best of our knowledge, this is the first work that has studied the effect of different kinds of real-world noise and varying amounts of noise contamination on the human perception of emotion and the implication of training on these datasets from the perspective of machine performance and robustness to adversarial attacks.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Datasets",
      "text": "For our study, we use the IEMOCAP dataset  (Busso et al., 2008) , created to explore the relationship between emotion, gestures, and speech. The data contain recordings from five pairs of actors (one male and one female), 10 actors in total. The actors either performed from scripted scenes or improvised based on target scenarios. The data were segmented by speaker turn, resulting in a total of 10,039 utterances (5,255 scripted turns and 4,784 improvised turns). IEMOCAP contains audio, video, and associated manual transcriptions.\n\nThe data were evaluated in terms of dimensional and categorical labels. The dimensional evaluations included valence (positive vs. negative), activation (calm vs. excited), and dominance (passive vs. dominant). Each utterance was evaluated by at least two evaluators. The final dimensional labels are the average over the individual evaluators. We bin the labels into three classes {low, mid, high}, which we defined as {(low:[1,2.75]),  (mid:(2.75,3.25] ), (high:(3.25,5])}.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Noise",
      "text": "We investigate the effects of two types of noise, environmental and signal distortion. Environmental noises are additive, while signal distortion noise involves other types of signal manipulation.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Environmental Noise",
      "text": "We define environmental noises (ENV) as additive background noise, obtained from the ESC-50 dataset(Piczak). ESC-50 is generally used for noise contamination and environmental sound classification  (Xu et al., 2021) . These environmental sounds are representative of many types of noise seen in real world deployments, especially in the context of virtual and smart home conversational agents. We use the following categories:\n\n• Natural soundscapes (Nat), e.g., rain, wind.\n\n• Human, non-speech sounds (Hum), e.g., sneezing, coughing, laughing or crying in the background etc.\n\n• Interior/domestic sounds (Int), e.g., door creaks, clock ticks etc.\n\nWe manipulate three factors when adding the noise sources:\n\n• Position: The position of the introduction of sound that: (i) starts and then fades out in loudness or (ii) occurs during the entirety of the duration of the utterance. In the second case, this complete additive background would represent a consistent noise source in real world (e.g., fan rotation).\n\n• Quality Degradation: The decrease in the signal to noise ratio (SNR) caused by the addition of the additive background noise at levels of 20dB, 10dB and 0dB. This is used only when noise is added to the entirety of the utterance.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Signal Distortion",
      "text": "We define signal distortion noise as modulations that aren't additive in the background. These kinds of noise in the audio signal can occur from linguistic/paralinguistic factors, room environment, internet lags, or the physical locomotion of the speaker.\n\nWe use the nine following categories:\n\n• SpeedUtt: The utterance is sped up by either 1.25× or 0.75×.\n\n• SpeedSeg: A random segment within an utterance is sped up by 1.25×. The package pyAudio that we used to speed up a segment did not permit slowing a segment down. Thus, the 0.75× was not used here.\n\n• Fade: The loudness of the utterance is faded by 2% every second, which emulates the scenario of a user moving away from the speaker.\n\nThe loudness is increased for fade in, and decreased for fade out.\n\n• Filler: Non-verbal short fillers such as 'uh', 'umm' (from the same speaker) are inserted in the middle of a sentence. The insertion is either just the filler or succeeded and preceded by a long pause Fillers are obtained by parsing audio files for a given speaker and finding occurrences of any of the options from the above mentioned set. We will release the extracted fillers per speaker for IEMOCAP\n\n• DropWord: A randomly selected set of nonessential words belonging to the set: {a, the, an, so, like, and} are dropped from an utterance using word-aligned boundaries and stiching the audio segments together.\n\n• DropLetters: Following the same approach as drop word, letters are dropped in accordance with various linguistic styles chosen from the set: {/h/+vowel, vowel+/nd/+consonant(next word), consonant+/t/+consonant(next word), vowel+/r/+consonant, /ihng/}. This is supported by research that has studied phonological deletion or dropping of letters in the native US-English dialect (pho; Yuan and Liberman).\n\n• Laugh/Cry: \"Sob\" and \"short-laughter\" sounds are added to the utterance. They are obtained from AudioSet (Gemmeke et al., 2017).\n\n• Pitch: The pitch is changed by ± 3 half octaves using the pyAudio library.\n\n• Rev: Room reverberation is added to the utterance using py-audio-effects (pysndfx). We vary metrics such as reverberation ratio or room size to vary the type and intensity of reverberation added.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Sampling And Noise-Perturbations",
      "text": "We randomly select 900 samples from the IEMO-CAP dataset, which is far larger than the ones used for previous perception studies (  Parada-Cabaleiro et al., 2017; Scharenborg et al., 2018) . We select 100 samples from each activation and valence pair bin, i.e., 100 samples from the bin with activation: low, valence: low; 100 samples from the bin with activation: low, and valence: mid, and so on. This ensures that the chosen 900 samples cover the range of emotions expressed. We impose another constraint on these 100 samples from each bin, 30 of them are shorter than the first quartile or greater than fourth quartile of utterance length in seconds to cover both extremities of the spectrum, and the remaining 70 belong in the middle. We also ensure that the selected samples had a 50-50 even split amongst gender. We introduce noise to the 900 samples (Section 4). Each sample is modulated in ten ways: four randomly chosen types of environmental noise and six randomly chosen signal distortion noise modulations, giving us a total of 9,000 noisy samples.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "User Study",
      "text": "We first analyze the effects of noise on human perception by relabeling the noise-enhanced data using the Amazon Mechanical Turk (AMT) platform. We use insights from this experiment to guide the machine learning analyses that follow.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Crowdsourcing Setup",
      "text": "We recruited 147 workers using Amazon Mechanical Turk who self-identify as being from the United States and as native English speakers, to reduce the impact of cultural variability. We ensured that each worker had > 98% approval rating and more than 500 approved Human Intelligence Tasks (HITs).\n\nWe ensured that all workers understood the meaning of activation and valence using a qualification task that asked workers to rank emotion content similar to  (Jaiswal et al., 2019) . The qualification task has two parts: (i) we explain the difference between valence and activation and how to identify those, and, (ii) we ask them to identify which of the two samples has a higher/lower valence and a higher/lower activation, to ensure that they have understood the concept of activation and valence annotations. All HIT workers were paid a minimum wage ($9.45/hr), pro-rated to the minute. Each HIT was annotated by three workers.\n\nFor our main task, we created pairs that contained one original and one modulated sample. We then asked each worker to annotate whether or not they perceived the pair to have the same emotion. If they said yes for both activation and valence, the noisy sample was labeled same and they could directly move to the next HIT. If they said no, the noisy sample was labeled different. In this case, they were asked to assess the activation and valence of the noisy sample using Self Assessment Manikins (Bradley and Lang, 1994) on a scale of  [1, 5]  (similar to the original IEMOCAP annotation).\n\nWe also include three kinds of attention checks:\n\n1. We show two samples that have not been modified and ask them to decide if the emotion represented was different. If the person says yes, then the experiment ends.\n\n2. We observe the time spent on the task. If the time spent on the task is less than the combined length of both samples, then the user's qualification to annotate the HITs is rescinded and their responses are discarded.\n\n3. We show two samples, one which has a gold standard label, and another, which has been contaminated with significant noise (performance degradation >30dB), such that the resulting sample is incomprehensible. If people do not mark this set of samples as being different, the experiment ends.\n\nThe failure rate based on the above criteria was 8%.\n\nWe ensured the quality of the annotations by paying bonuses based on time spent, not just number of HITs, and by disqualifying annotators if they annotated any sample (including those outside of the attention checks) more quickly than the combined length of the audio samples.\n\nWe then created two sets of labels for each noiseaugmented clip. The first type of label compared a noise-augmented clip to its original. The noiseaugmented clip was labeled the same if the modified and original clip were perceived to have the same valence or activation, otherwise it was labeled different. We created this label by taking the majority vote over all evaluations. The second type of label included valence and activation. A noiseaugmented clip was given the average valence and activation over all evaluations.\n\nThe inter-annotator agreement was measured using Cohen's kappa. Conventionally, when estimating Cohen's kappa, annotators are not considered as individuals, instead reducing annotators to the generic 1, 2, and 3. The challenge is that this often leads to artificially inflated inter-annotator agreement because individual characteristics and behavior of a particular worker are not taken under consideration  (Hoek and Scholman, 2017) . We take a different approach, creating a table for the calculation of the statistic that considers annotators as individuals with separate entries for each clip, following the approach of  (Hoek and Scholman, 2017) . If an annotator didn't evaluate a given clip, the cell has a null (missing data) value. We found that the Cohen's kappa was 79% for activation and 76% for valence.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Methods",
      "text": "We now describe the emotion recognition approaches, presenting two separate pipelines, one that relies upon direct feature extraction (Section 7.2) and the other that is end-to-end (Section 7.3). This allows us to investigate whether noise has a consistent effect. We discuss approaches to improve noise robustness by training models with noise-augmented data or denoised data (Section 7.4). Finally, we describe the setup and evaluation of the model robustness using an untargeted model misclassification test, which measures a model's fragility in terms of how likely it is that the model's decisions will change when specific types of noise are observed at test time (Section 7.5).",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Creation Of Data Partitions",
      "text": "We use a subject-independent five-fold cross validation scheme to select our train, test and validation sets. In the first iteration, sessions 1-3 are used for training, session 4 is used as validation, and session 5 is used for testing. This is repeated in a round-robin fashion, resulting in each session serving as a validation and a test fold. We also divide possible noises in two different categories based on results of crowdsourcing study (see Section 8.1). The first category is perception-altering, those that changed perception of humans and hence cannot be used for model training or evaluation with the old annotations. The second category is perception-retraining, those that did not change human perception, and hence, the model should produce no change in predictions when using those noise categories for sample augmentation.\n\nWe use the noise categories (seeSection 5) in two varying circumstances. The first category is matched, where both the training and testing sets",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Traditional Deep Learning Network",
      "text": "We first explore a common \"traditional\" deep learning network that is used in speech emotion recognition. In this method we extract Mel Filterbank (MFB) features as input to a model composed of convolutional and gated recurrent unit (GRU) layers.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Features",
      "text": "We extract 40-dimensional Mel Filterbank (MFB) features using a 25-millisecond Hamming window with a step-size of 10-milliseconds using pythonspeech-features. Each utterance is represented as a sequence of 40-dimensional feature vectors. We z-normalize the acoustic features using parameters extracted from the training dataset. During each cross-validation fold, the parameters are chosen from the training data and are applied to both the validation and testing data.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Network",
      "text": "Our baseline network is a state-of-art single utterance emotion classification model which has been used in previous research  (Aldeneh et al., 2017; Khorram et al., 2017; Krishna et al., 2018) . The extracted MFBs are processed using a set of convolution layers and GRUs (see Table  1  for the hyperparameters used for these layers). The output of these layers is then fed through a mean pooling layer to produce an acoustic representation which is then fed into a set of dense layers to classify activation or valence.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Training.",
      "text": "We implement the models using the Keras library  (Chollet, 2015) . We use a cross-entropy loss function for each task (e.g., valence or activation).\n\nWe learn the model parameters using the RMSProp optimizer. We train our networks for a maximum of 50 epochs and use early stopping if the validation loss does not improve after five consecutive epochs. Once the training process ends, we revert the network's weights to those that achieved the lowest validation loss. We repeat the experiment five times. We report the results in terms of Unweighted Average Recall (UAR, chance is 0.33), averaged over all test samples and five repetitions.\n\nWe compare the performance of different models or the same model in different noisy conditions/partitions using a paired t-test using the Bonferroni correction, asserting significance when p ≤ 0.05.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "End-To-End Deep Learning Networks",
      "text": "Next, we explore a transformer-based model. In this method the raw audio signal is used as input to a pre-trained and fine-tuned network and the emotion prediction is directly obtained as an output. These models do not require us to perform manual or domain knowledge-based extraction of features. They instead have a feature encoder component inside the model, which is dynamic in nature, and hence, can change its output for the same signal based on the dataset and nature of the task.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Features",
      "text": "For the end-to-end deep learning models, we do not need to extract audio features. Instead we rely on the network itself to both normalize and extract features, that are later passed onto the deeper layers of the network. The feature set here is the original wav files that are not modified in any capacity. The eventual representations are of size 512, reproducing the setup in the state-of-the-art implementation  (Pepino et al., 2021) .",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Network",
      "text": "Our baseline network is the state-of-the-art wav2vec2.0 emotion recognition model  (Pepino et al., 2021) . The wav2vec model is comprised of three parts: (i) a convolutional neural network (CNN) that acts as feature encoder, (ii) a quantizier module, and (iii) a transformer module. The input to the model is raw audio data (16kHz) that is passed to a multi-block 1-d CNN to generate audio representations (25ms). The quantizer is similar to a variational autoencoder that encodes and extracts features using a contrastive loss. The transformer is used for masked sequence prediction and encodes the bi-directional temporal context of the features.\n\nWe use the base model, which has not been finetuned for ASR (wav2vec2.0-PT). We then fine-tune the base model to predict the binned emotion labels. We use the final representation of the output as an input to dense layers to produce the final output.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Training",
      "text": "We implement the model provided in the speech brain library. As in the other pipeline (Section 7.2), we use cross-entropy loss for each task and learn the dense layer parameters. Reproducing the state of the art model  (Pepino et al., 2021)  = 2,021 We run the model for a maximum of eight epochs. We revert the network's state to the one that achieved the lowest validation loss. We repeat this experiment five times. Again, we use UAR and report the results averaged over both subjects and repetitions.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Noise Augmentation Overview",
      "text": "We will be assessing a model's ability to classify emotion given either environmental or signal distortion noise. We perform two kinds of analysis, one when using the set of noises that includes those that do alter human perception, and another when only using noises that are perception-retaining. We report overall model performances for both of these categories.\n\nFor a more thorough analysis, we then specifically focus on the categories of noise that do not significantly affect human perception. This allows us to evaluate a model's robustness, or its fragility, with respect to variations that wouldn't alter a human's perception of emotion. This is important because the overwhelming majority of the noiseaugmented utterances in the IEMOCAP dataset were not included in the user study and, therefore, do not have perceptual labels (Section 6). We consider three types of environmental noise {Human (Hum), Interior (Int), Natural (Nat)} and three types of signal distortion noise {Speeding a segment (SpeedSeg), Fade, Reverberation (Reverb)}.\n\nWe use two separate testing paradigms: (i) matched testing, in which all noise types are introduced to the training, testing, and validation data and (ii) mismatched testing, in which n-1 types of noise are introduced to the training and validation sets and the heldout type of noise is introduced to the test set. In all cases, we analyze the test data in terms of specific noise categories. Therefore, the test sets are the same between the two paradigms.\n\nWe run both the matched and mismatched experiments twice, first with the noise-augmented data and second with a noise-robust/denoising pipeline. The first iteration will allow us to quantify the effect of the noise on the traditional and end-to-end classification pipelines. We then repeat the experiment with either denoised data for the traditional classifier (Section 7.4.1) or using the noise-robust implementation of wav2vec2.0 for the end-to-end classifier (Section 7.4.2). This allows us to investigate how, or if, noise-robust implementations can offset the effects of background noise.",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "Denoising",
      "text": "We implement denoising using the well-known Recurrent Neural Network Noise Suppression (RNNNoise, denoising feature space) approach, proposed in 2017 for noise suppression  (Valin, 2018) . RNNNoise is trained on environmental noise, and these noises overlap considerably with those in our dataset. We use the algorithm's default parameters and use it on an 'as-is' basis for our experiments. We assume that the system does not have the knowledge of which noise, from the set of available noise categories, is introduced and, therefore, we do not compare with other denoising algorithms that assume a priori knowledge of noise category. The result is a set of 'noise-suppressed' samples in the training, validation and testing sets.\n\nWe pass all the data, including both the original and noise-augmented data, through a denoising algorithm. This allows us to ensure that acoustic artifacts, if any, are introduced to both the original and noise-augmented data. We then train the traditional deep learning model as described in Section 7.2.",
      "page_start": 15,
      "page_end": 15
    },
    {
      "section_name": "Using A Noise-Robust Model",
      "text": "In the end-to-end model, we need to use a different denoising approach because the approach described in the previous section does not return a wav file, but instead is applied to the feature-space directly. Here, we enforce robustness to noise using a model trained to be noise-robust in an end-to-end fashion. We use the noise-robust version (Wav2Vec2-Large-Robust) of the aforementioned wav2vec2.0 model  (Hsu et al., 2021) . The noise-robust large model was pretrained on 16kHz sampled speech audio. Noisy speech datasets from multiple domains were used to pretrain the model: Libri-Light, Com-monVoice, Switchboard, and, Fisher  (Hsu et al., 2021) . We then train the end-to-end model as described in Section 7.3.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Model Robustness Testing",
      "text": "Deployed emotion recognition models must be robust. One of the major scenarios that we robustness test any speech-based model for is the presence of noise. But the set of noises we choose to test robustness on can lead to different conclusions about the robustness of the models. In our case, we consider two different scenarios:\n\n1. Robustness evaluation when using perceptionretaining samples, noise samples that do not change human perception 2. Robustness evaluation when using any kind of noise (i.e., both perception-retaining and perception-altering)\n\nWe perform robustness evaluation of a model by using the model's output predictions to create new noise-enhanced samples that change the model's output, compared to the original clean sample. We do this using an untargeted model misclassification test, in which we add noise to the samples. The intentional misclassification algorithm assumes black-box model access. For our purposes, it needs to have access to: (i) a subset of the dataset, (ii) noises to add to create perturbed samples, and (iii) model input and output.\n\nAs in any other perturbation-based robustness testing, the goal is to introduce perturbations to the samples such that the resulting samples are as close to the original sample as possible. The minimally perturbed sample should be the one that causes a classifier to change its original classification. We measure the amount of perturbation using SNR, calculated using the logarithmic value of the ratio between the original signal and the noise-augmented signal's power. We note that the lower the decrease in SNR, the more minimally perturbed a sample is. The maximal decrease in SNR that we use in the algorithm is a difference of 10 dB. This condition ensures that the sample is not audibly judged as contaminated by humans  (Kidd Jr et al., 2016) .\n\nThe algorithm to choose this minimally perturbed sample has four major components:\n\n1. Requirements: some labelled samples, noise files, model input and output access, unlabelled samples for testing, and, optionally, correlation between noise type and performance degradation for a given model.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Looping:",
      "text": "The algorithm then loops over each noise category to figure out whether it can successfully force the model to misclassify. The noise category order is random if we do not have access to the optional performance degradation correlations.\n\n3. Minimizing: The algorithm then aims to find the lowest decrease in dB, such that the model still misclassifies. This ensures that the resultant noisy sample is as imperceptible to humans as possible.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "End Condition:",
      "text": "The algorithm ends if a noise addition has been found, or if it runs out of number of tries allowed for model access.\n\nPlease see Algorithm 1 for more details. In the algorithm, numAttempts is the number of times the algorithm is allowed to access the model's inputoutput pairs. Classifier output refers to the prediction made by the model when the attack algorithm sends an input to the model to be classified. Classifier output changes is true when the model predicted the emotion label differently after noise was added to the sample, compared to the original clean sample. Success implies that the algorithm was successfully able to force the model to misclassify a particular sample in the allowed number of attempts. Failure implies that the algorithm could not force the model to misclassify in the allowed number of attempts and that the model can be considered robust for that sample.\n\nWe use the above algorithm in two different settings, under two different pre-known assumptions, with four levels of allowed queries, and four models (64 categories):\n\n1. Settings {All vs.\n\nNot-Altering Human Perception} 2. Pre-Known Assumptions {No Knowledge vs.\n\nKnowledge About Noise Category Degradation Level} 3. Allowed Queries {5, 15, 20, inf}  For all the test samples, we execute five runs of the above algorithm to account for randomization in noise choices. These five runs are then averaged to obtain the average success of misclassification or average robustness for a given sample (1-average success of misclassification). We then average the robustness value over all the test samples. We report our obtained results for the above mentioned scenarios.\n\nTable  5 : Success of misclassification attempts on different models with varying number of allowed attempts (lower is better). As a reminder, samples in the all noises category have an uncertain ground truth, the row is marked with two stars ( * * ). Reverberation (reverb) is a perception-retaining noise that is also analyzed separately. Is this effect dependent on the utterance length, loudness, the type of the added noise, and the original emotion?\n\nWe find that the presence of environmental noise, even when loud, rarely affects annotator perception, suggesting that annotators are able to psychoacoustically mask the background noise in various cases, as also shown in prior work (e.g.,  (Stenback, 2016) ). We find that the addition of signal distortion noise alters human perception. The reported change in valence and activation values is on a scale of -1 to 1 (normalized). The addition of laughter changes the activation perception of 16% of the utterances, with an average change of +22% (+.26). The valence perception is altered in 17% of the utterances, with an average change of +14% (+.11). Similarly for crying, valence is altered in 20% of the cases, with an average change of -21% (-.20). Crying changes activation perception in 22% of the cases, with an average change of -32% (-.43). Raises in pitch also alter the perception of emotion. In 22% of utterances, the perception of activation is changed. This contrasts with the perception of valence, which was altered only in 7% of utterances. In this scenario, activation increases by an average of 26% (+.19), and valence decreases by 12% (-.11). On the other hand, decreases in pitch change the perception of activation in 10% of the cases and of valence in 29% of the cases. In this scenario, activation decreases by an average of 16% (-.15), and valence decreases by 7% (-.07). This ties into previous work  (Busso et al., 2009) , which looked into how changes and fluctuations in pitch levels influenced the perception of emotions. Changes in the speed of an utterance affect human perception of valence in 13% (average of -.13) of the cases when speed is increased, and 28% (average of -.23) when speed is decreased. On the other hand, changes in the speed of an utterance do not affect activation as often, specifically, 3% in case of increase and 6% in case of decrease.\n\nWe ensured that our crowsdourcing samples had an even distribution over gender of the speaker and the length of the sample (see Section 5.3). We performed paired t-test to evaluate whether these variables influenced the outcome of emotion per-ception change in presence of noise. We found that the changes in perception were not tied to characteristics of the speakers. For example, there was no correlation between changes in perception and variables such as, the original emotion of the utterance, the gender of the speaker, and the length of the utterance.\n\nThe human perception study provides insight into how emotion perception changes given noise. This also provides information about the potential effects of noise addition on model behavior. In the sections that follow, we will evaluate how machine perception changes given these sources of noise.\n\n8.2 RQ2: Can we verify previous findings that the presence of noise affects the performance of emotion recognition models? Does this effect vary based on the type of the added noise?\n\nWe first assess the performance of the model on the original IEMOCAP data and find that the traditional model obtains a performance of 0.67 UAR on the activation and 0.59 UAR on the valence task.\n\nOn the other hand, the end-to-end model obtains a performance of 0.73 on activation and 0.64 on the valence task. We hypothesize that the wav2vec2 model has an added advantage of being trained to recognize word structures that can incorporate some paralinguistic/langauge information in the fine-tuned model. Next, we augment the test samples of each fold with each of the noise types (Section 5) and investigate how the performance of the model changes. We include two cases: (i) only perception-retaining noises and,(ii) all noises.\n\nIn the first scenario, we do not include noise types that were found to affect human perception (e.g., Pitch, SpeedUtt, Laugh) because once these noises are added, the ground truth is no longer reliable. This lack of reliable ground-truth data hinders the evaluation of the model's performance on these samples because the majority of the utterances were not part of the original crowdsourcing experiment and are thus unlabeled. The remainder of this section focuses on the second scenario only.\n\nWe find that for matched train and test noise conditions, the traditional machine learning model's performance decreases by an average of 28% for environmental noise while it drops by 32% for signal manipulation. On the other hand, for end-toend deep learning model, the model's performance decreases by an average of 22% and 26% for environmental and manipulated noises, respectively. In mismatched noise conditions, the models' performance decreases by an average of 33% for environmental noise, fading, and reverberation. There is also a smaller drop in performance for speeding up parts of the utterance and dropping words, showing the brittleness of these models. Table  3  reports the percentage change in performance when testing on noisy test data, compared to clean test data.\n\nWe see that the end-to-end deep learning model is less affected by environmental noise, but has a larger drop due to fading and reverberation. We observe a larger drop on performance when dropping words, which possibly can be attributed to the change in audio-structure and non-controllable feature extraction for this model.\n\nIn the second scenario, we observe a large drop in performance for both the traditional and the end to end machine learning model. For example, in the case of a traditional deep learning model, the valence prediction performance drops to a nearchance performance when including kinds of noises (see Table  3 ).\n\nWe specifically want to point out how the inclusion of all noises in the test conditions changes the observed model performance. Primarily, the models on an average seem to do 20% worse than they would if we only consider noises that do not alter human perception. We note the discrepancy between the results of the two noise addition scenarios and that results should be described with respect to the perceptual effects of noise, if noise augmentation is used.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Rq3A: Does Dataset Augmentation Help",
      "text": "improve the robustness of emotion recognition models to unseen noise?\n\nWe first report results for only perception-retaining noises. When the training datasets are augmented with noise, we observe an average performance drop of 26% and 10% for matched noise conditions when using the traditional and the end-to-end deep learning model, respectively. For the mismatched noise conditions, we observe an average performance drop of 31% and 16% for the traditional and end-to-end deep learning models, respectively. Both models see improved performance when the training dataset is augmented with continuous background noise in the matched noise setting. We find that data augmentation improves performance on mismatched noisy test data over a baseline system trained only on the clean IEMOCAP data. For example, the end-to-end model tested on environmental noise-augmented dataset (as compared to traditional deep learning model), reduces the performance drop to nearly zero. This improvement is particularly pronounced (an increase of 22% as compared to when trained on the clean partition) when the environmental noise is introduced at the start of the utterance (e.g., when the test set is introduced with nature-based noises at the start of the utterance and the train set is introduced with human and interior noises at the start of the utterance). We speculate that the network learns to assign different weights to the start and end of the utterance to account for the initial noise.\n\nHowever, we find that in both matched and mismatched conditions, it is hard to handle utterances contaminated with reverberation, a common use case, even when the training set is augmented with other types of noise. We find that this improvement in performance is even more reduced when using the wav2vec model, alluding to the model's fragility towards data integrity/structural changes. This can be because reverberation adds a continuous human speech signal in the background delayed by a small period of time. None of the other kind of noise types have speech in them, and hence augmentation doesn't aid the model to learn robustness to this kind of noise.\n\nFinally, we investigate the differences in model performance when we use all types of noise vs. those that are perception-retaining. Specifically, we focus on the perception-altering noises because samples augmented with noises in this category no longer have a known ground truth. We inquire as to whether the use of samples that alter perception may lead to the appearance of model performance improvement (note: appearance because the samples now have uncertain ground truth). To maintain equivalence, we ensure that the training and validation dataset sizes are equal even when they are augmented with more noise conditions. We observe that many cases of performance improvement occur when the noises include those that are perception-altering (see \"Al noises\" in Table  3 ). We observe a difference of 12% to 25% between the numbers that we obtain for the perception-retaining noises vs. when not distinguishing between the two noise categories. This supports our claim that the choice between types of noises used for data aug-mentation during model training and performance evaluation affects the empirical observations and should be carefully considered. We hypothesize that this improvement in performance may be due to the inherent nature of noises that change emotion perception, if they are perceptible enough to change emotion perception, then they may stand out enough that the model can adequately learn to separate them out and improve its prediction towards the original ground truth annotation. However, if the noise alteration truly does change perception, then the model is learning to ignore this natural human perceptual phenomenon. This may have negative consequences during model deployment.",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "Rq3B: Does Sample Denoising Help",
      "text": "improve the robustness of emotion recognition models to unseen noise?\n\nIn the matched training testing condition, we find that the traditional deep learning model has an average performance of 0.57 across all the datasets and testing setups, while the end-to-end models do substantially better at 0.61 UAR. See Table  4  for details.\n\nIn the mismatched training testing condition, we find that for the traditional deep learning model, adding a denoising component to the models leads to a significant improvement when the original SNR is high (e.g., after continuous noise introduction the SNR decreases only by 10dB). In this case, we see an average improvement of 23% ± 3% across all environmental noise categories, compared to when there is no denoising or augmentation performed.\n\nHowever, when the SNR decreases by 20dB, we observe a decline in performance when using the noise suppression algorithms. We believe that this decline in performance is reflective of the mismatch in goals: the goal of noise suppression is to maintain, or improve, the comprehensibility of the speech itself, not necessarily highlight emotion. As a result, it may end up masking emotional information, as discussed in  (Ma and Thompson, 2015) .\n\nWe further show that the addition of a denoising component does not significantly improve performance in the presence of signal distortion noise (other than reverberation) as compared to the presence of environmental noise (noise addition). For example, when samples were faded in or out or segments were sped up, the performance is significantly lower (-28%) than when tested on a clean test set. However, we did see an improvement in the performance for unseen reverberation contaminated samples as compared to data augmentation (an average of +36%). Finally, we observe a general trend of increase in emotion recognition performance for the combined dataset (noisy and nonnoisy samples), as compared to when the model is trained on the clean training set, which supports the findings from previous dataset augmentation research  (Aldeneh and Provost, 2017) .\n\nFor the end-to-end deep learning model, we use the noise-robust version. We find that the model is effective at countering environmental noise when trained on a dataset augmented with environmental noise, even in the mismatched condition. The performance is equivalent to the model evaluated on the clean data. We further delve into the amount of noise augmentation needed to achieve this equivalency. We consider all of the original training data. We augment a percentage of the training data, starting by augmenting a random sample of 10% with perception-retaining noise and increasing by 10% each time. We find that we obtain equivalency after augmenting with only 30% of the training data. We compare this compares to the traditional model, in which all of the training data are noise-augmented and we still do not see equivalency.\n\nWe separately consider the signal distortion noise samples. These were not part of the training of the wav2vec2-Large-robust model. However, this model only sees a 6% loss in performance, where the traditional robust model saw a 20% loss in performance.\n\nHowever, as discussed in the original traditional model, the end-to-end noise-robust model also fails on reverberation-based contamination even when trained on a similarly augmented dataset (note that the denoised traditional model could effectively handle reverberation). We believe that this may be because the wav2vec model is trained on continuous speech and relies on the underlying linguistic structure of speech. However, in reverberation, there is an implicit doubling of the underlying information, which is at odds with how this model was trained. This may explain why it is not able to compensate for this type of signal manipulation.\n\nNext, we analyze whether the perception category of noise used for data augmentation of the samples, in both, the train and test dataset influ-ences the reported results for noise-robust model improvement. We find that there is a significant difference in performance when the testing dataset is augmented with any kind of noise vs. when augmented with perception-retaining noise. Specifically, we observe that the maximal gains in performance when testing on matched noisy conditions are for samples for which we do not know whether or not the ground truth holds (i.e., both noise categories). For example, when using the noise robust traditional deep learning model, where the test and train dataset is augmented with any type of noises, we observe a performance improvement, as compared to that using a clean train dataset, of 12%. Similarly for noise robust end to end models, the performance improvement difference when using all noises vs. only perception retaining ones is 15% for activation and 13% for valence. Again, this is a problem when we think about deploying models in the real world because although the perception of these emotion expressions may change, we are assuming that the system should think of these perception labels as rigid and unchanging.",
      "page_start": 15,
      "page_end": 15
    },
    {
      "section_name": "Rq4: How Does The Robustness Of A Model",
      "text": "to attacks compare when we are using test samples that with are augmented with perception-retaining noise vs. samples that are augmented with all types of noise, regardless of their effect on perception?\n\nIn this section, we aim to show the effects of noise augmentation in general and specifically highlight noise categories that do not alter human perception. We will show that if we are not careful with the selection of our noise types, moving from noise that we know not to alter perception to noise that may, the resulting noise sources can not only impact the brittleness of models, but also lead to inaccurate evaluation metrics. We also specifically report robustness performance when using reverberationbased contamination, as we observed that it is the most likely noise category to degrade the robustness of the model. We allow the decision boundary attack algorithm a maximum of five queries to create a noise augmented sample that will change the model output. We find that if the attacker is also given perception-altering noises, compared to perceptionretaining, it can more effectively corrupt the sample. It achieves an increase in success rate from 35% (only perception-retaining) to 48.5% (all noise cat-egories). This increase in the success rate when perception-altering noises are included implies that the model does not remain robust when the effects of noise on human perception are not considered.\n\nWe next consider the type of noise (environmental vs. signal manipulation). We find that the success rate of flipping a model's output is 18% for noises belonging to the environmental category, which is generally a category of perceptionretaining noise. The success rate of flipping a model's output is 37% for all noises belonging to the signal manipulation category. When we constrain our possible noise choices to perceptionretaining signal manipulations, we see that the success rate of the intentional misclassification algorithm drops to 24%. On the other hand, we observe that when we also consider the signal manipulations that are perception-altering, the success rate of flipping a model output is 39%. See Table  5  for more details.\n\nWe previously discussed the fragility of end-toend models towards reverberation-based noise contamination, noise that is perception-retaining for human evaluators. Here, we specifically run an experiment to use only that particular noise category for the model fragility testing. If the attacker knows that the model is susceptible to reverberation-based prediction changes, the intentional misclassification algorithm can land on an optimal set of room and reverberation parameters in a maximum of five queries to be able to produce a flipped output for that particular sample. It achieves a success rate of 24%, compared to 12% for other perceptionretaining noises. The traditional noise-robust deep learning model is even more challenged, compared to the end-to-end model. The number of queries required to flip the output is three, vs. five for the end-to-end model, suggesting that it is less robust. This empirical evaluation is performed primarily to demonstrate how such noise inclusions can not only invalidate the ground truth but also lead to inaccurate and fragile benchmarking and evaluation of adversarial efficiency and robustness. We propose a set of recommendations, for both augmentation and deployment of emotion recognition models in the wild, that are grounded in human perception. For augmentation, we suggest that:\n\n1. Environmental noise should be added to datasets to improve generalizability to varied noise conditions, whether using denoising, augmentation, or a combination of both.\n\n2. It is good to augment datasets by fading the loudness of the segments, dropping letters or words, and speeding up small (no more than 25% of the total sample length) segments of the complete sound samples in the dataset. But it is important to note that these augmented samples should not be passed through the denoising component as the denoised version loses emotion information.\n\n3. One should not change the speed of the entire utterance more than 5% and should not add intentional pauses or any background noises that elicit emotion behavior, e.g., sobs or laughter.\n\nRegarding deployment, we suggest that:\n\n1. Noisy starts and ends of utterances can be handled by augmentation, hence, if the training set included these augmentations, there should be no issue for deployed emotion recognition systems.\n\n2. Reverberation is hard to handle for even augmented emotion recognition models. Hence, the samples must either be cleaned to remove the reverberation effect, or must be identified as low confidence for classification.\n\n3. Deploy complementary models that identify the presence of noise that would change a human's perception.",
      "page_start": 16,
      "page_end": 16
    },
    {
      "section_name": "Conclusion",
      "text": "In this work, we study how the presence of real world noise, environmental or signal distortion, affects human emotion perception. We identify noise sources that do not affect human perception, such that they can be confidently used for data augmentation. We look at the change in performance of the models that are trained on the original IEMOCAP dataset, but tested on noisy samples and if augmentation of the training set leads to an improvement in performance. We conclude that, unlike humans, machine learning models are extremely brittle to the introduction of many kinds of noise. While the performance of the machine learning model on noisy samples is aided from augmentation, the performance is still significantly lower when the noise in the train and test environments does not match. In this paper, we demonstrate fragility of the emotion recognition systems and valid methods to augment the datasets, which is a critical concern in real world deployment.",
      "page_start": 17,
      "page_end": 17
    }
  ],
  "figures": [],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "mimansa@umich.edu": "Abstract",
          "emilykmp@umich.edu": "this gap, researchers have proposed various meth-"
        },
        {
          "mimansa@umich.edu": "",
          "emilykmp@umich.edu": "ods to generate larger datasets. One of the most"
        },
        {
          "mimansa@umich.edu": "Speech emotion recognition is an important",
          "emilykmp@umich.edu": ""
        },
        {
          "mimansa@umich.edu": "",
          "emilykmp@umich.edu": "common is noise augmentation. The baseline as-"
        },
        {
          "mimansa@umich.edu": "component of any human centered system. But",
          "emilykmp@umich.edu": ""
        },
        {
          "mimansa@umich.edu": "speech characteristics produced and perceived",
          "emilykmp@umich.edu": "sumption of noise augmentation is that the labels"
        },
        {
          "mimansa@umich.edu": "by a person can be influenced by a multitude",
          "emilykmp@umich.edu": "of the emotion examples do not change once noise"
        },
        {
          "mimansa@umich.edu": "of reasons, both desirable such as emotion, and",
          "emilykmp@umich.edu": "has been added (Pappagari et al., 2021). While this"
        },
        {
          "mimansa@umich.edu": "undesirable such as noise. To train robust emo-",
          "emilykmp@umich.edu": ""
        },
        {
          "mimansa@umich.edu": "",
          "emilykmp@umich.edu": "assumption can be confidently made for tasks such"
        },
        {
          "mimansa@umich.edu": "tion recognition models, we need a large, yet",
          "emilykmp@umich.edu": ""
        },
        {
          "mimansa@umich.edu": "",
          "emilykmp@umich.edu": "as automatic speech recognition (ASR), the same"
        },
        {
          "mimansa@umich.edu": "realistic data distribution, but emotion datasets",
          "emilykmp@umich.edu": ""
        },
        {
          "mimansa@umich.edu": "",
          "emilykmp@umich.edu": "cannot be said for perception-based tasks, such as"
        },
        {
          "mimansa@umich.edu": "are often small and hence are augmented with",
          "emilykmp@umich.edu": ""
        },
        {
          "mimansa@umich.edu": "",
          "emilykmp@umich.edu": "emotion recognition."
        },
        {
          "mimansa@umich.edu": "noise. Often noise augmentation makes one",
          "emilykmp@umich.edu": ""
        },
        {
          "mimansa@umich.edu": "important assumption, that the prediction label",
          "emilykmp@umich.edu": "In this paper, we question the assumption that"
        },
        {
          "mimansa@umich.edu": "should remain the same in presence or absence",
          "emilykmp@umich.edu": ""
        },
        {
          "mimansa@umich.edu": "",
          "emilykmp@umich.edu": "the annotation label remains the same in the pres-"
        },
        {
          "mimansa@umich.edu": "of noise, which is true for automatic speech",
          "emilykmp@umich.edu": ""
        },
        {
          "mimansa@umich.edu": "",
          "emilykmp@umich.edu": "ence of noise. We first create a noise augmented"
        },
        {
          "mimansa@umich.edu": "recognition but not necessarily true for per-",
          "emilykmp@umich.edu": ""
        },
        {
          "mimansa@umich.edu": "",
          "emilykmp@umich.edu": "dataset and conduct a perception study to label"
        },
        {
          "mimansa@umich.edu": "ception based tasks.\nIn this paper we make",
          "emilykmp@umich.edu": ""
        },
        {
          "mimansa@umich.edu": "",
          "emilykmp@umich.edu": "the emotion of these augmented samples, focused"
        },
        {
          "mimansa@umich.edu": "three novel contributions. We validate through",
          "emilykmp@umich.edu": ""
        },
        {
          "mimansa@umich.edu": "",
          "emilykmp@umich.edu": "on the type of noise in samples whose perception"
        },
        {
          "mimansa@umich.edu": "crowdsourcing that the presence of noise does",
          "emilykmp@umich.edu": ""
        },
        {
          "mimansa@umich.edu": "change the annotation label and hence may al-",
          "emilykmp@umich.edu": "has changed or remained the same given the agu-"
        },
        {
          "mimansa@umich.edu": "ter\nthe original ground truth label. We then",
          "emilykmp@umich.edu": "mentation. We use the results from this study to"
        },
        {
          "mimansa@umich.edu": "show how disregarding this knowledge and as-",
          "emilykmp@umich.edu": "classify the complete set of augmentation noises"
        },
        {
          "mimansa@umich.edu": "suming consistency in ground truth labels prop-",
          "emilykmp@umich.edu": ""
        },
        {
          "mimansa@umich.edu": "",
          "emilykmp@umich.edu": "into two categories, perception-altering (i.e., noises"
        },
        {
          "mimansa@umich.edu": "agates to downstream evaluation of ML models,",
          "emilykmp@umich.edu": ""
        },
        {
          "mimansa@umich.edu": "",
          "emilykmp@umich.edu": "that may change the perception of emotion) and"
        },
        {
          "mimansa@umich.edu": "both for performance evaluation and robustness",
          "emilykmp@umich.edu": ""
        },
        {
          "mimansa@umich.edu": "",
          "emilykmp@umich.edu": "perception-retaining (i.e., noises that do not change"
        },
        {
          "mimansa@umich.edu": "testing. We end the paper with a set of recom-",
          "emilykmp@umich.edu": ""
        },
        {
          "mimansa@umich.edu": "",
          "emilykmp@umich.edu": "the perception of emotion). We propose that\nthe"
        },
        {
          "mimansa@umich.edu": "mendations for noise augmentations in speech",
          "emilykmp@umich.edu": ""
        },
        {
          "mimansa@umich.edu": "",
          "emilykmp@umich.edu": "perception-altering noises should not be used in"
        },
        {
          "mimansa@umich.edu": "emotion recognition datasets.",
          "emilykmp@umich.edu": ""
        },
        {
          "mimansa@umich.edu": "",
          "emilykmp@umich.edu": "supervised learning or evaluation frameworks be-"
        },
        {
          "mimansa@umich.edu": "1\nIntroduction",
          "emilykmp@umich.edu": ""
        },
        {
          "mimansa@umich.edu": "",
          "emilykmp@umich.edu": "cause we cannot confidently maintain that the orig-"
        },
        {
          "mimansa@umich.edu": "",
          "emilykmp@umich.edu": "inal annotation holds for a given sample. We eval-"
        },
        {
          "mimansa@umich.edu": "Speech emotion recognition is\nincreasingly in-",
          "emilykmp@umich.edu": ""
        },
        {
          "mimansa@umich.edu": "",
          "emilykmp@umich.edu": "uate the effects of disregarding emotion percep-"
        },
        {
          "mimansa@umich.edu": "cluded as a component in many real-world human-",
          "emilykmp@umich.edu": ""
        },
        {
          "mimansa@umich.edu": "",
          "emilykmp@umich.edu": "tion changes by examining how the performance of"
        },
        {
          "mimansa@umich.edu": "centered machine learning models. Modulations",
          "emilykmp@umich.edu": ""
        },
        {
          "mimansa@umich.edu": "",
          "emilykmp@umich.edu": "emotion recognition models and analyses of their"
        },
        {
          "mimansa@umich.edu": "in speech can be produced for a multitude of rea-",
          "emilykmp@umich.edu": ""
        },
        {
          "mimansa@umich.edu": "",
          "emilykmp@umich.edu": "robustness change in unpredictable manners when"
        },
        {
          "mimansa@umich.edu": "sons, both desirable and undesirable.\nIn our case",
          "emilykmp@umich.edu": ""
        },
        {
          "mimansa@umich.edu": "",
          "emilykmp@umich.edu": "we include samples that alter human perception in"
        },
        {
          "mimansa@umich.edu": "desirable modulations encode information that we",
          "emilykmp@umich.edu": ""
        },
        {
          "mimansa@umich.edu": "",
          "emilykmp@umich.edu": "the training of these models. Lastly, we provide a"
        },
        {
          "mimansa@umich.edu": "want our model to learn and be informed by, such",
          "emilykmp@umich.edu": ""
        },
        {
          "mimansa@umich.edu": "",
          "emilykmp@umich.edu": "set of recommendations for noise based augmenta-"
        },
        {
          "mimansa@umich.edu": "as speaker characteristics or emotion. Undesirable",
          "emilykmp@umich.edu": ""
        },
        {
          "mimansa@umich.edu": "",
          "emilykmp@umich.edu": "tion of speech emotion recognition datasets based"
        },
        {
          "mimansa@umich.edu": "modulations encode information that are extrin-",
          "emilykmp@umich.edu": ""
        },
        {
          "mimansa@umich.edu": "",
          "emilykmp@umich.edu": "on our results."
        },
        {
          "mimansa@umich.edu": "sic factors change with the environment, such as",
          "emilykmp@umich.edu": ""
        },
        {
          "mimansa@umich.edu": "noise.\nIn order to handle these modulations, we",
          "emilykmp@umich.edu": "Researchers have considered the impact of noise"
        },
        {
          "mimansa@umich.edu": "need large datasets that capture the range of pos-",
          "emilykmp@umich.edu": "on emotion perception and thereby the annotation"
        },
        {
          "mimansa@umich.edu": "sible speech variations and their\nrelationship to",
          "emilykmp@umich.edu": "of emotions.\n[X]\nlooked at how pink and white"
        },
        {
          "mimansa@umich.edu": "emotion expression. But, such datasets are gen-",
          "emilykmp@umich.edu": "noises in varying intensities change the perception"
        },
        {
          "mimansa@umich.edu": "erally not available for emotion tasks. To bridge",
          "emilykmp@umich.edu": "of emotion. Another set of research has concen-"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "tion changes by examining how the performance of": ""
        },
        {
          "tion changes by examining how the performance of": "emotion recognition models and analyses of their"
        },
        {
          "tion changes by examining how the performance of": ""
        },
        {
          "tion changes by examining how the performance of": "robustness change in unpredictable manners when"
        },
        {
          "tion changes by examining how the performance of": ""
        },
        {
          "tion changes by examining how the performance of": "we include samples that alter human perception in"
        },
        {
          "tion changes by examining how the performance of": ""
        },
        {
          "tion changes by examining how the performance of": "the training of these models. Lastly, we provide a"
        },
        {
          "tion changes by examining how the performance of": ""
        },
        {
          "tion changes by examining how the performance of": "set of recommendations for noise based augmenta-"
        },
        {
          "tion changes by examining how the performance of": ""
        },
        {
          "tion changes by examining how the performance of": "tion of speech emotion recognition datasets based"
        },
        {
          "tion changes by examining how the performance of": ""
        },
        {
          "tion changes by examining how the performance of": "on our results."
        },
        {
          "tion changes by examining how the performance of": ""
        },
        {
          "tion changes by examining how the performance of": "Researchers have considered the impact of noise"
        },
        {
          "tion changes by examining how the performance of": "on emotion perception and thereby the annotation"
        },
        {
          "tion changes by examining how the performance of": "of emotions.\n[X]\nlooked at how pink and white"
        },
        {
          "tion changes by examining how the performance of": "noises in varying intensities change the perception"
        },
        {
          "tion changes by examining how the performance of": "of emotion. Another set of research has concen-"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "trated on training and validating noise robust mod-": "els with the assumption that intent label prediction",
          "an attack model for robustness testing. It considers": "a pool of noises and picks the best noise with a"
        },
        {
          "trated on training and validating noise robust mod-": "remains consistent in the presence of noise. For ex-",
          "an attack model for robustness testing. It considers": "minimal SNR degradation that\nis able to change"
        },
        {
          "trated on training and validating noise robust mod-": "ample, [X] have looked at training student teacher",
          "an attack model for robustness testing. It considers": "a model’s prediction. We consider a condition in"
        },
        {
          "trated on training and validating noise robust mod-": "models that aim to ignore the effect of noise intro-",
          "an attack model for robustness testing. It considers": "which the attack model has black-box access to"
        },
        {
          "trated on training and validating noise robust mod-": "duced to the model. On the other hand [X] have",
          "an attack model for robustness testing. It considers": "the trained model. The attack has a fixed number"
        },
        {
          "trated on training and validating noise robust mod-": "proposed copy pasting various emotion segments",
          "an attack model for robustness testing. It considers": "of allowed queries to the trained model, but not"
        },
        {
          "trated on training and validating noise robust mod-": "together along with neutral noise to balance the",
          "an attack model for robustness testing. It considers": "the internal gradients or structure (i.e.,\nthe attack"
        },
        {
          "trated on training and validating noise robust mod-": "classes in an emotion dataset, thus improving per-",
          "an attack model for robustness testing. It considers": "model can only provide input and can only access"
        },
        {
          "trated on training and validating noise robust mod-": "formance.",
          "an attack model for robustness testing. It considers": "the trained model’s prediction). We test and mon-"
        },
        {
          "trated on training and validating noise robust mod-": "",
          "an attack model for robustness testing. It considers": "itor\nthe difference in the observed robustness of"
        },
        {
          "trated on training and validating noise robust mod-": "In this paper, we claim that the standard assump-",
          "an attack model for robustness testing. It considers": ""
        },
        {
          "trated on training and validating noise robust mod-": "",
          "an attack model for robustness testing. It considers": "these aforementioned models."
        },
        {
          "trated on training and validating noise robust mod-": "tion about perception and hence,\nlabel\nretention",
          "an attack model for robustness testing. It considers": ""
        },
        {
          "trated on training and validating noise robust mod-": "",
          "an attack model for robustness testing. It considers": "We find that the crowdsourced labels do change"
        },
        {
          "trated on training and validating noise robust mod-": "of emotion in the presence of noise may not hold",
          "an attack model for robustness testing. It considers": ""
        },
        {
          "trated on training and validating noise robust mod-": "",
          "an attack model for robustness testing. It considers": "in the presence of some kinds of noise. We then"
        },
        {
          "trated on training and validating noise robust mod-": "true in a multiple noise categories. To understand",
          "an attack model for robustness testing. It considers": ""
        },
        {
          "trated on training and validating noise robust mod-": "",
          "an attack model for robustness testing. It considers": "verify that the models perform worse on noisy sam-"
        },
        {
          "trated on training and validating noise robust mod-": "which noises impact emotion perception, we use",
          "an attack model for robustness testing. It considers": ""
        },
        {
          "trated on training and validating noise robust mod-": "",
          "an attack model for robustness testing. It considers": "ples when trained only on clean datasets. But, we"
        },
        {
          "trated on training and validating noise robust mod-": "a common emotion dataset, IEMOCAP and intro-",
          "an attack model for robustness testing. It considers": ""
        },
        {
          "trated on training and validating noise robust mod-": "",
          "an attack model for robustness testing. It considers": "show that this decrease in performance is different"
        },
        {
          "trated on training and validating noise robust mod-": "duce various kinds of noises to it, at varying signal",
          "an attack model for robustness testing. It considers": ""
        },
        {
          "trated on training and validating noise robust mod-": "",
          "an attack model for robustness testing. It considers": "when using the complete set of noises for augment-"
        },
        {
          "trated on training and validating noise robust mod-": "to noise ratio (SNR) levels as well as at different",
          "an attack model for robustness testing. It considers": ""
        },
        {
          "trated on training and validating noise robust mod-": "",
          "an attack model for robustness testing. It considers": "ing the test set vs. when only using the perception-"
        },
        {
          "trated on training and validating noise robust mod-": "positions in the sample. We then perform a crowd-",
          "an attack model for robustness testing. It considers": ""
        },
        {
          "trated on training and validating noise robust mod-": "",
          "an attack model for robustness testing. It considers": "retaining noises for augmentation. We show similar"
        },
        {
          "trated on training and validating noise robust mod-": "sourcing experiment that asks workers to annotate",
          "an attack model for robustness testing. It considers": ""
        },
        {
          "trated on training and validating noise robust mod-": "",
          "an attack model for robustness testing. It considers": "patterns for noise-robust models, specifically show-"
        },
        {
          "trated on training and validating noise robust mod-": "their perception of emotion for both the clean and",
          "an attack model for robustness testing. It considers": ""
        },
        {
          "trated on training and validating noise robust mod-": "",
          "an attack model for robustness testing. It considers": "ing how there is an increased drop in performance"
        },
        {
          "trated on training and validating noise robust mod-": "the corresponding noise-augmented sample. This",
          "an attack model for robustness testing. It considers": ""
        },
        {
          "trated on training and validating noise robust mod-": "",
          "an attack model for robustness testing. It considers": "for the end-to-end noise-robust model when exclud-"
        },
        {
          "trated on training and validating noise robust mod-": "enables us to divide noise augmentation options",
          "an attack model for robustness testing. It considers": ""
        },
        {
          "trated on training and validating noise robust mod-": "",
          "an attack model for robustness testing. It considers": "ing performance-altering noises during augmenta-"
        },
        {
          "trated on training and validating noise robust mod-": "into groups characterized by their potential\nto ei-",
          "an attack model for robustness testing. It considers": ""
        },
        {
          "trated on training and validating noise robust mod-": "",
          "an attack model for robustness testing. It considers": "tion. We then discuss how our conventional metrics,"
        },
        {
          "trated on training and validating noise robust mod-": "ther influence or not influence human perception.",
          "an attack model for robustness testing. It considers": ""
        },
        {
          "trated on training and validating noise robust mod-": "",
          "an attack model for robustness testing. It considers": "those that look only at model performance, may be"
        },
        {
          "trated on training and validating noise robust mod-": "The results of the crowdsourcing experiments",
          "an attack model for robustness testing. It considers": ""
        },
        {
          "trated on training and validating noise robust mod-": "",
          "an attack model for robustness testing. It considers": "incorrectly asserting improvements as the model"
        },
        {
          "trated on training and validating noise robust mod-": "inform a series of empircal analyses focused on",
          "an attack model for robustness testing. It considers": ""
        },
        {
          "trated on training and validating noise robust mod-": "",
          "an attack model for robustness testing. It considers": "is learning to predict an emotion measure that\nis"
        },
        {
          "trated on training and validating noise robust mod-": "model performance and model\nrobustness. We",
          "an attack model for robustness testing. It considers": ""
        },
        {
          "trated on training and validating noise robust mod-": "",
          "an attack model for robustness testing. It considers": "not in line with human perception. Troublingly, we"
        },
        {
          "trated on training and validating noise robust mod-": "first present an empirical evaluation of the effects",
          "an attack model for robustness testing. It considers": ""
        },
        {
          "trated on training and validating noise robust mod-": "",
          "an attack model for robustness testing. It considers": "find that the attack model is generally more effec-"
        },
        {
          "trated on training and validating noise robust mod-": "of including perception-altering noises in training.",
          "an attack model for robustness testing. It considers": ""
        },
        {
          "trated on training and validating noise robust mod-": "",
          "an attack model for robustness testing. It considers": "tive when it has access to the set of all noises as"
        },
        {
          "trated on training and validating noise robust mod-": "It will allow us to observe how the inclusion of",
          "an attack model for robustness testing. It considers": ""
        },
        {
          "trated on training and validating noise robust mod-": "",
          "an attack model for robustness testing. It considers": "compared to when excluding perception-altering"
        },
        {
          "trated on training and validating noise robust mod-": "perception-altering noises creates an impression of",
          "an attack model for robustness testing. It considers": ""
        },
        {
          "trated on training and validating noise robust mod-": "",
          "an attack model for robustness testing. It considers": "noises for allowed augmentations. We also specifi-"
        },
        {
          "trated on training and validating noise robust mod-": "performance improvement. We will discuss how",
          "an attack model for robustness testing. It considers": ""
        },
        {
          "trated on training and validating noise robust mod-": "",
          "an attack model for robustness testing. It considers": "cally find that given just a pool of carefully crafted"
        },
        {
          "trated on training and validating noise robust mod-": "this improvement\nis a myth,\nthis new model will",
          "an attack model for robustness testing. It considers": ""
        },
        {
          "trated on training and validating noise robust mod-": "",
          "an attack model for robustness testing. It considers": "reverberation modulations, the attack model can be"
        },
        {
          "trated on training and validating noise robust mod-": "have learned to predict labels that are not truly as-",
          "an attack model for robustness testing. It considers": ""
        },
        {
          "trated on training and validating noise robust mod-": "",
          "an attack model for robustness testing. It considers": "successful in almost 65% of the cases with minimal"
        },
        {
          "trated on training and validating noise robust mod-": "sociated with a given sample due to the perceptual",
          "an attack model for robustness testing. It considers": ""
        },
        {
          "trated on training and validating noise robust mod-": "",
          "an attack model for robustness testing. It considers": "degradation in SNR and in less than ten queries to"
        },
        {
          "trated on training and validating noise robust mod-": "effects of these noises. We consider both a general",
          "an attack model for robustness testing. It considers": ""
        },
        {
          "trated on training and validating noise robust mod-": "",
          "an attack model for robustness testing. It considers": "the trained model. We end the paper with a general"
        },
        {
          "trated on training and validating noise robust mod-": "recurrent neural network (RNN) model and an end-",
          "an attack model for robustness testing. It considers": ""
        },
        {
          "trated on training and validating noise robust mod-": "",
          "an attack model for robustness testing. It considers": "set of recommendations for noise augmentations in"
        },
        {
          "trated on training and validating noise robust mod-": "to-end model for this purpose. We evaluate condi-",
          "an attack model for robustness testing. It considers": ""
        },
        {
          "trated on training and validating noise robust mod-": "",
          "an attack model for robustness testing. It considers": "speech emotion recognition datasets."
        },
        {
          "trated on training and validating noise robust mod-": "tions in which novel augmentation noises are either",
          "an attack model for robustness testing. It considers": ""
        },
        {
          "trated on training and validating noise robust mod-": "introduced during training (matched) or seen for the",
          "an attack model for robustness testing. It considers": ""
        },
        {
          "trated on training and validating noise robust mod-": "",
          "an attack model for robustness testing. It considers": "2\nResearch Questions"
        },
        {
          "trated on training and validating noise robust mod-": "first time during testing (mismatched). The second",
          "an attack model for robustness testing. It considers": ""
        },
        {
          "trated on training and validating noise robust mod-": "empirical evaluation analyzes whether the gap in",
          "an attack model for robustness testing. It considers": "In this paper, we investigate five research questions:"
        },
        {
          "trated on training and validating noise robust mod-": "performance between the matched and mismatched",
          "an attack model for robustness testing. It considers": "Purpose 1: Premise Validation through Crowd-"
        },
        {
          "trated on training and validating noise robust mod-": "conditions can be bridged using noise robust mod-",
          "an attack model for robustness testing. It considers": "sourcing"
        },
        {
          "trated on training and validating noise robust mod-": "eling techniques. The third and final evaluation",
          "an attack model for robustness testing. It considers": "RQ1: Does the presence of noise affect emotion"
        },
        {
          "trated on training and validating noise robust mod-": "is focused on the robustness of the model.\nIt will",
          "an attack model for robustness testing. It considers": "perception as evaluated by human raters? Is this"
        },
        {
          "trated on training and validating noise robust mod-": "allow us to observe how the inclusion of these per-",
          "an attack model for robustness testing. It considers": "effect dependent on the utterance length, loudness,"
        },
        {
          "trated on training and validating noise robust mod-": "ception altering noises ultimately leads to a model",
          "an attack model for robustness testing. It considers": "the type of the added noise, and the original emo-"
        },
        {
          "trated on training and validating noise robust mod-": "that\nis more susceptible to attack compared to a",
          "an attack model for robustness testing. It considers": "tion?"
        },
        {
          "trated on training and validating noise robust mod-": "model that does not include these noises. We train",
          "an attack model for robustness testing. It considers": "Reason: Noise has been known to have masking"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "effect on humans in specific situations.\nHence,": "humans can often understand verbalized content",
          "of noise, regardless of their effect on perception?": "Reason:\nAnother major metric for any deploy-"
        },
        {
          "effect on humans in specific situations.\nHence,": "even in presence of noise. Our goal\nis to under-",
          "of noise, regardless of their effect on perception?": "able machine learning algorithm is its performance"
        },
        {
          "effect on humans in specific situations.\nHence,": "stand whether the same masking effect extends to",
          "of noise, regardless of their effect on perception?": "on \"unseen situations\" or handling incoming data"
        },
        {
          "effect on humans in specific situations.\nHence,": "paralinguistic cues such as emotion, and to what",
          "of noise, regardless of their effect on perception?": "shifts (i.e., robustness testing). We test robustness"
        },
        {
          "effect on humans in specific situations.\nHence,": "extent. Our continuing claim from hereon remains",
          "of noise, regardless of their effect on perception?": "using a noise augmentation algorithm that aims to"
        },
        {
          "effect on humans in specific situations.\nHence,": "that only noises that do not change human percep-",
          "of noise, regardless of their effect on perception?": "forcefully and efficiently change a model’s output"
        },
        {
          "effect on humans in specific situations.\nHence,": "tion should be used for the training and evaluation",
          "of noise, regardless of their effect on perception?": "by augmenting test samples with noise. We look at"
        },
        {
          "effect on humans in specific situations.\nHence,": "of machine learning models. Not doing so, can",
          "of noise, regardless of their effect on perception?": "how often this algorithm is unsuccessful in being"
        },
        {
          "effect on humans in specific situations.\nHence,": "lead to gains or drops in performance measurement",
          "of noise, regardless of their effect on perception?": "able to \"fool\" a model with its augmented samples."
        },
        {
          "effect on humans in specific situations.\nHence,": "that may not actually extend to real world settings.",
          "of noise, regardless of their effect on perception?": "We look at the changes in frequency with which a"
        },
        {
          "effect on humans in specific situations.\nHence,": "We call these changes \"unverified\" because we can-",
          "of noise, regardless of their effect on perception?": "model is successfully able to defend itself when the"
        },
        {
          "effect on humans in specific situations.\nHence,": "not, with certainity, be sure that the model should",
          "of noise, regardless of their effect on perception?": "attack algorithm uses a set that includes all types of"
        },
        {
          "effect on humans in specific situations.\nHence,": "have predicted the original label (i.e., the label of",
          "of noise, regardless of their effect on perception?": "noises vs. when it only uses perception-retaining"
        },
        {
          "effect on humans in specific situations.\nHence,": "the sample before noise was added) because the",
          "of noise, regardless of their effect on perception?": "noises."
        },
        {
          "effect on humans in specific situations.\nHence,": "human did not neccessarily label the noisy instance",
          "of noise, regardless of their effect on perception?": "Purpose 5: Recommendations"
        },
        {
          "effect on humans in specific situations.\nHence,": "with that same label.",
          "of noise, regardless of their effect on perception?": "RQ5: What are the recommended practices for"
        },
        {
          "effect on humans in specific situations.\nHence,": "",
          "of noise, regardless of their effect on perception?": "speech emotion dataset augmentation and model"
        },
        {
          "effect on humans in specific situations.\nHence,": "Purpose 2: Noise Impact Quantification",
          "of noise, regardless of their effect on perception?": ""
        },
        {
          "effect on humans in specific situations.\nHence,": "",
          "of noise, regardless of their effect on perception?": "deployment?"
        },
        {
          "effect on humans in specific situations.\nHence,": "RQ2:\nCan we verify previous findings that\nthe",
          "of noise, regardless of their effect on perception?": ""
        },
        {
          "effect on humans in specific situations.\nHence,": "",
          "of noise, regardless of their effect on perception?": "Reason: We then provide a set of recommenda-"
        },
        {
          "effect on humans in specific situations.\nHence,": "presence of noise affects the performance of emo-",
          "of noise, regardless of their effect on perception?": ""
        },
        {
          "effect on humans in specific situations.\nHence,": "",
          "of noise, regardless of their effect on perception?": "tions based on our empirical studies for deploying"
        },
        {
          "effect on humans in specific situations.\nHence,": "tion recognition models?\nDoes\nthis effect vary",
          "of noise, regardless of their effect on perception?": ""
        },
        {
          "effect on humans in specific situations.\nHence,": "",
          "of noise, regardless of their effect on perception?": "emotion recognition models in real world situa-"
        },
        {
          "effect on humans in specific situations.\nHence,": "based on the type of the added noise?",
          "of noise, regardless of their effect on perception?": ""
        },
        {
          "effect on humans in specific situations.\nHence,": "",
          "of noise, regardless of their effect on perception?": "tions."
        },
        {
          "effect on humans in specific situations.\nHence,": "Reason: We have known that presence of noise",
          "of noise, regardless of their effect on perception?": ""
        },
        {
          "effect on humans in specific situations.\nHence,": "in data shifts the data distribution (Chenchah and",
          "of noise, regardless of their effect on perception?": ""
        },
        {
          "effect on humans in specific situations.\nHence,": "",
          "of noise, regardless of their effect on perception?": "3\nRelated Work"
        },
        {
          "effect on humans in specific situations.\nHence,": "Lachiri, 2016). This shift often leads to poor per-",
          "of noise, regardless of their effect on perception?": ""
        },
        {
          "effect on humans in specific situations.\nHence,": "formance by machine learning models. We aim",
          "of noise, regardless of their effect on perception?": ""
        },
        {
          "effect on humans in specific situations.\nHence,": "",
          "of noise, regardless of their effect on perception?": "Previous work that has focused on measuring how"
        },
        {
          "effect on humans in specific situations.\nHence,": "to quantify the amount of performance drop based",
          "of noise, regardless of their effect on perception?": ""
        },
        {
          "effect on humans in specific situations.\nHence,": "",
          "of noise, regardless of their effect on perception?": "noise impacts the performance of machine learn-"
        },
        {
          "effect on humans in specific situations.\nHence,": "on the type of noise in these systems, both,\nfor",
          "of noise, regardless of their effect on perception?": ""
        },
        {
          "effect on humans in specific situations.\nHence,": "",
          "of noise, regardless of their effect on perception?": "ing models can be classified into three main di-"
        },
        {
          "effect on humans in specific situations.\nHence,": "any kind of noise, and then, specifically for noises",
          "of noise, regardless of their effect on perception?": ""
        },
        {
          "effect on humans in specific situations.\nHence,": "",
          "of noise, regardless of their effect on perception?": "rections:\nemotion recognition and noise-robust"
        },
        {
          "effect on humans in specific situations.\nHence,": "that do not change human perception (perception-",
          "of noise, regardless of their effect on perception?": ""
        },
        {
          "effect on humans in specific situations.\nHence,": "",
          "of noise, regardless of their effect on perception?": "models,\nspeech augmentation for\nclassification"
        },
        {
          "effect on humans in specific situations.\nHence,": "retaining).",
          "of noise, regardless of their effect on perception?": ""
        },
        {
          "effect on humans in specific situations.\nHence,": "",
          "of noise, regardless of their effect on perception?": "purposes, and robustness testing in speech-based"
        },
        {
          "effect on humans in specific situations.\nHence,": "Purpose 3: Denoising and Augmentation Bene-",
          "of noise, regardless of their effect on perception?": "model training."
        },
        {
          "effect on humans in specific situations.\nHence,": "fits Evaluation",
          "of noise, regardless of their effect on perception?": ""
        },
        {
          "effect on humans in specific situations.\nHence,": "RQ3:\nDoes dataset augmentation (Q3a) and/or",
          "of noise, regardless of their effect on perception?": "3.1\nEmotion Recognition"
        },
        {
          "effect on humans in specific situations.\nHence,": "sample denoising (Q3b) help improve the robust-",
          "of noise, regardless of their effect on perception?": ""
        },
        {
          "effect on humans in specific situations.\nHence,": "",
          "of noise, regardless of their effect on perception?": "End-to-End models are a recent paradigm for au-"
        },
        {
          "effect on humans in specific situations.\nHence,": "ness of\nemotion recognition models\nto unseen",
          "of noise, regardless of their effect on perception?": ""
        },
        {
          "effect on humans in specific situations.\nHence,": "",
          "of noise, regardless of their effect on perception?": "dio classification. The two major end-to-end mod-"
        },
        {
          "effect on humans in specific situations.\nHence,": "noise?",
          "of noise, regardless of their effect on perception?": ""
        },
        {
          "effect on humans in specific situations.\nHence,": "",
          "of noise, regardless of their effect on perception?": "els often used for speech classification tasks are:"
        },
        {
          "effect on humans in specific situations.\nHence,": "Reason: We\ntest whether\nthe\ncommonly-used",
          "of noise, regardless of their effect on perception?": ""
        },
        {
          "effect on humans in specific situations.\nHence,": "",
          "of noise, regardless of their effect on perception?": "DeepSpeech (Hannun et al., 2014) and wav2vec"
        },
        {
          "effect on humans in specific situations.\nHence,": "methods for improving the performance of these",
          "of noise, regardless of their effect on perception?": ""
        },
        {
          "effect on humans in specific situations.\nHence,": "",
          "of noise, regardless of their effect on perception?": "(1.0 (Schneider et al., 2019), 2.0 (Baevski et al.,"
        },
        {
          "effect on humans in specific situations.\nHence,": "models under distribution shifts is helpful. We",
          "of noise, regardless of their effect on perception?": ""
        },
        {
          "effect on humans in specific situations.\nHence,": "",
          "of noise, regardless of their effect on perception?": "2020)). Researchers have used the latest version"
        },
        {
          "effect on humans in specific situations.\nHence,": "focus on two main methods, augmentation and de-",
          "of noise, regardless of their effect on perception?": ""
        },
        {
          "effect on humans in specific situations.\nHence,": "",
          "of noise, regardless of their effect on perception?": "of these pretrained models for recognizing speech"
        },
        {
          "effect on humans in specific situations.\nHence,": "noising. We specifically look at how performance",
          "of noise, regardless of their effect on perception?": ""
        },
        {
          "effect on humans in specific situations.\nHence,": "",
          "of noise, regardless of their effect on perception?": "emotion in various languages (Pepino et al., 2021;"
        },
        {
          "effect on humans in specific situations.\nHence,": "changes when we augment with noises that include",
          "of noise, regardless of their effect on perception?": ""
        },
        {
          "effect on humans in specific situations.\nHence,": "",
          "of noise, regardless of their effect on perception?": "Mohamed and Aly, 2021). Researchers have also"
        },
        {
          "effect on humans in specific situations.\nHence,": "those that are perception-altering vs. when we ex-",
          "of noise, regardless of their effect on perception?": ""
        },
        {
          "effect on humans in specific situations.\nHence,": "",
          "of noise, regardless of their effect on perception?": "looked at extending traditional deep learning mod-"
        },
        {
          "effect on humans in specific situations.\nHence,": "clude such noises.",
          "of noise, regardless of their effect on perception?": ""
        },
        {
          "effect on humans in specific situations.\nHence,": "",
          "of noise, regardless of their effect on perception?": "els to end-to-end models using multiple cojoined"
        },
        {
          "effect on humans in specific situations.\nHence,": "Purpose 4: Model Robustness Testing Condi-",
          "of noise, regardless of their effect on perception?": "networks. For example, Amirhossein et al.\ninves-"
        },
        {
          "effect on humans in specific situations.\nHence,": "tions",
          "of noise, regardless of their effect on perception?": "tigated how attention mechanisms could be pro-"
        },
        {
          "effect on humans in specific situations.\nHence,": "RQ4:\nHow does the robustness of a model\nto",
          "of noise, regardless of their effect on perception?": "cessed at different layers of an end-to-end model to"
        },
        {
          "effect on humans in specific situations.\nHence,": "attacks compare when we are using test samples",
          "of noise, regardless of their effect on perception?": "improve both speaker and emotion recognition (Ha-"
        },
        {
          "effect on humans in specific situations.\nHence,": "that with are augmented with perception-retaining",
          "of noise, regardless of their effect on perception?": "javi\nand Etemad, 2021).\nIn another\napproach,"
        },
        {
          "effect on humans in specific situations.\nHence,": "noise vs. samples that are augmented with all types",
          "of noise, regardless of their effect on perception?": "researchers have also analyzed how waveforms"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "could be treated as concatenated image blocks": "and used this mechanism to perform real-time",
          "3.2.2\nNoise Robustness in Emotion": ""
        },
        {
          "could be treated as concatenated image blocks": "",
          "3.2.2\nNoise Robustness in Emotion": "The general method in emotion recognition,\nlike"
        },
        {
          "could be treated as concatenated image blocks": "speech emotion recognition from an incoming au-",
          "3.2.2\nNoise Robustness in Emotion": ""
        },
        {
          "could be treated as concatenated image blocks": "",
          "3.2.2\nNoise Robustness in Emotion": "speech recognition, involves the addition of noise"
        },
        {
          "could be treated as concatenated image blocks": "dio stream (Lech et al., 2020). Other researchers",
          "3.2.2\nNoise Robustness in Emotion": ""
        },
        {
          "could be treated as concatenated image blocks": "",
          "3.2.2\nNoise Robustness in Emotion": "to the original sample. Researchers make use of"
        },
        {
          "could be treated as concatenated image blocks": "have also looked at comparisons between perfor-",
          "3.2.2\nNoise Robustness in Emotion": ""
        },
        {
          "could be treated as concatenated image blocks": "",
          "3.2.2\nNoise Robustness in Emotion": "publicly available and hierarchically categorized"
        },
        {
          "could be treated as concatenated image blocks": "mance of wav2vec2, wavBert, and HuBert, to un-",
          "3.2.2\nNoise Robustness in Emotion": ""
        },
        {
          "could be treated as concatenated image blocks": "",
          "3.2.2\nNoise Robustness in Emotion": "noise samples (Chenchah and Lachiri, 2016) or"
        },
        {
          "could be treated as concatenated image blocks": "derstand which models perform the best for speech",
          "3.2.2\nNoise Robustness in Emotion": ""
        },
        {
          "could be treated as concatenated image blocks": "",
          "3.2.2\nNoise Robustness in Emotion": "introduce signal distortions to the sample itself."
        },
        {
          "could be treated as concatenated image blocks": "emotion recognition given a set of pre-known con-",
          "3.2.2\nNoise Robustness in Emotion": ""
        },
        {
          "could be treated as concatenated image blocks": "",
          "3.2.2\nNoise Robustness in Emotion": "For example, researchers have looked at speed per-"
        },
        {
          "could be treated as concatenated image blocks": "ditions (Mohamed and Aly, 2021).",
          "3.2.2\nNoise Robustness in Emotion": ""
        },
        {
          "could be treated as concatenated image blocks": "",
          "3.2.2\nNoise Robustness in Emotion": "turbation, additive white noise, vocal tract length"
        },
        {
          "could be treated as concatenated image blocks": "Researchers have explored techniques for noise-",
          "3.2.2\nNoise Robustness in Emotion": "perturbation and temp perturbation for augmenting"
        },
        {
          "could be treated as concatenated image blocks": "based data augmentation to improve noise robust-",
          "3.2.2\nNoise Robustness in Emotion": "speech emotion recognition datasets (Nicolás et al.,"
        },
        {
          "could be treated as concatenated image blocks": "ness (Kim and Kim), focusing on how model train-",
          "3.2.2\nNoise Robustness in Emotion": "2022)."
        },
        {
          "could be treated as concatenated image blocks": "ing can be improved to yield better performance.",
          "3.2.2\nNoise Robustness in Emotion": ""
        },
        {
          "could be treated as concatenated image blocks": "",
          "3.2.2\nNoise Robustness in Emotion": "The main concern with this approach is the un-"
        },
        {
          "could be treated as concatenated image blocks": "However,\nthese augmentation techniques tend to",
          "3.2.2\nNoise Robustness in Emotion": ""
        },
        {
          "could be treated as concatenated image blocks": "",
          "3.2.2\nNoise Robustness in Emotion": "derlying assumption that the addition of any noise"
        },
        {
          "could be treated as concatenated image blocks": "focus on acoustic event detection, speaker verifi-",
          "3.2.2\nNoise Robustness in Emotion": ""
        },
        {
          "could be treated as concatenated image blocks": "",
          "3.2.2\nNoise Robustness in Emotion": "in the background would not change the emotion"
        },
        {
          "could be treated as concatenated image blocks": "cation or speech recognition (Ko et al., 2015), and",
          "3.2.2\nNoise Robustness in Emotion": ""
        },
        {
          "could be treated as concatenated image blocks": "",
          "3.2.2\nNoise Robustness in Emotion": "label.\nIt\nis important\nto note that most emotion"
        },
        {
          "could be treated as concatenated image blocks": "have been sparingly used in audio-based paralin-",
          "3.2.2\nNoise Robustness in Emotion": ""
        },
        {
          "could be treated as concatenated image blocks": "",
          "3.2.2\nNoise Robustness in Emotion": "recognition datasets are labelled as\n\"perception"
        },
        {
          "could be treated as concatenated image blocks": "guistic classification tasks.",
          "3.2.2\nNoise Robustness in Emotion": ""
        },
        {
          "could be treated as concatenated image blocks": "",
          "3.2.2\nNoise Robustness in Emotion": "of others\" (i.e., annotated by an outside group of"
        },
        {
          "could be treated as concatenated image blocks": "The common way to deal with noise in any au-",
          "3.2.2\nNoise Robustness in Emotion": "obervers, rather than the speaker themself) and are"
        },
        {
          "could be treated as concatenated image blocks": "dio signal is to use denoising algorithms. Hence,",
          "3.2.2\nNoise Robustness in Emotion": "not self-reported. Intuitively, one might think that"
        },
        {
          "could be treated as concatenated image blocks": "it\nis important\nto understand how machine learn-",
          "3.2.2\nNoise Robustness in Emotion": "we should still be able to predict the “correct” label"
        },
        {
          "could be treated as concatenated image blocks": "ing models\ntrained to recognize\nemotions per-",
          "3.2.2\nNoise Robustness in Emotion": "in the presence of noise because the speaker’s emo-"
        },
        {
          "could be treated as concatenated image blocks": "form if\nthey\nare\ntested\non\ndenoised\nsamples.",
          "3.2.2\nNoise Robustness in Emotion": "tion did not change. But given that the labels do not"
        },
        {
          "could be treated as concatenated image blocks": "Two common approaches include: Denoising Fea-",
          "3.2.2\nNoise Robustness in Emotion": "represent a speaker’s evaluation of their emotion,"
        },
        {
          "could be treated as concatenated image blocks": "ture Space\n(Valin, 2018)\nand Speech Enhance-",
          "3.2.2\nNoise Robustness in Emotion": "rather, fall into the category of how others perceive"
        },
        {
          "could be treated as concatenated image blocks": "ment\n(Chakraborty et al., 2019). Denoising fea-",
          "3.2.2\nNoise Robustness in Emotion": "another person’s emotion, noise can (and does, as"
        },
        {
          "could be treated as concatenated image blocks": "ture space algorithms seek to remove noise from",
          "3.2.2\nNoise Robustness in Emotion": "we show later and as supported by prior work in"
        },
        {
          "could be treated as concatenated image blocks": "the extracted front end features. Speech enhance-",
          "3.2.2\nNoise Robustness in Emotion": "psychology (Ma and Thompson, 2015)), change"
        },
        {
          "could be treated as concatenated image blocks": "ment algorithms seek to convert noisy speech to",
          "3.2.2\nNoise Robustness in Emotion": "the emotion label."
        },
        {
          "could be treated as concatenated image blocks": "more intelligible speech. Both techniques are as-",
          "3.2.2\nNoise Robustness in Emotion": "Another recent approach, CopyPaste (Pappagari"
        },
        {
          "could be treated as concatenated image blocks": "sociated with challenges, from signal to harmonic",
          "3.2.2\nNoise Robustness in Emotion": "et al., 2021), takes a speaker-specific approach, aug-"
        },
        {
          "could be treated as concatenated image blocks": "dissonance (Valin, 2018).",
          "3.2.2\nNoise Robustness in Emotion": "menting samples from a single speaker with neutral"
        },
        {
          "could be treated as concatenated image blocks": "",
          "3.2.2\nNoise Robustness in Emotion": "examples from that speaker. They argue that this"
        },
        {
          "could be treated as concatenated image blocks": "",
          "3.2.2\nNoise Robustness in Emotion": "approach will maintain the emotion label and, as a"
        },
        {
          "could be treated as concatenated image blocks": "3.2\nNoise Augmentation for Model Robustness",
          "3.2.2\nNoise Robustness in Emotion": ""
        },
        {
          "could be treated as concatenated image blocks": "",
          "3.2.2\nNoise Robustness in Emotion": "result of data augmentation, help improve perfor-"
        },
        {
          "could be treated as concatenated image blocks": "3.2.1\nNoise Augmentation in Speech",
          "3.2.2\nNoise Robustness in Emotion": ""
        },
        {
          "could be treated as concatenated image blocks": "",
          "3.2.2\nNoise Robustness in Emotion": "mance."
        },
        {
          "could be treated as concatenated image blocks": "Recognition",
          "3.2.2\nNoise Robustness in Emotion": ""
        },
        {
          "could be treated as concatenated image blocks": "",
          "3.2.2\nNoise Robustness in Emotion": "3.3\nRobustness Testing"
        },
        {
          "could be treated as concatenated image blocks": "Research in speech recognition, rather than emo-",
          "3.2.2\nNoise Robustness in Emotion": ""
        },
        {
          "could be treated as concatenated image blocks": "tion recognition, has also tackled this problem.",
          "3.2.2\nNoise Robustness in Emotion": "A separate line of research has used noise to evalu-"
        },
        {
          "could be treated as concatenated image blocks": "Researchers have investigated methods\nto build",
          "3.2.2\nNoise Robustness in Emotion": "ate the robustness of a given model. For example,"
        },
        {
          "could be treated as concatenated image blocks": "speech recognition systems that are robust to vari-",
          "3.2.2\nNoise Robustness in Emotion": "researchers have focused on adversarial example"
        },
        {
          "could be treated as concatenated image blocks": "ous kinds and levels of noise (Li et al., 2014). The",
          "3.2.2\nNoise Robustness in Emotion": "generation, aiming to create audio samples that"
        },
        {
          "could be treated as concatenated image blocks": "common themes are a concentration on either data",
          "3.2.2\nNoise Robustness in Emotion": "change the output of the classifier. However, these"
        },
        {
          "could be treated as concatenated image blocks": "augmentation or gathering more real-world data to",
          "3.2.2\nNoise Robustness in Emotion": "methods assume white-box access to the network"
        },
        {
          "could be treated as concatenated image blocks": "produce accurate transcripts (Zheng et al., 2016).",
          "3.2.2\nNoise Robustness in Emotion": "and create modifications in either feature vectors"
        },
        {
          "could be treated as concatenated image blocks": "Other lines of work have looked into preventing",
          "3.2.2\nNoise Robustness in Emotion": "or actual wav files\n(Carlini and Wagner).\nThis"
        },
        {
          "could be treated as concatenated image blocks": "various attacks, e.g., spoofing or recording play-",
          "3.2.2\nNoise Robustness in Emotion": "generally results in samples that either have percep-"
        },
        {
          "could be treated as concatenated image blocks": "back, on speaker verification systems (Shim et al.).",
          "3.2.2\nNoise Robustness in Emotion": "tible differences when played back to a human, or"
        },
        {
          "could be treated as concatenated image blocks": "Noise in these systems is usually considered to be",
          "3.2.2\nNoise Robustness in Emotion": "are imperceptible to a human but fail to attack the"
        },
        {
          "could be treated as concatenated image blocks": "caused by reverberations or channel-based modula-",
          "3.2.2\nNoise Robustness in Emotion": "model when played over-air (Carlini and Wagner)."
        },
        {
          "could be treated as concatenated image blocks": "tions (Zhao et al., 2014).",
          "3.2.2\nNoise Robustness in Emotion": "Model robustness to noise or an adversarial at-"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "tack can be evaluated by adding noise to the dataset": "and testing performance of the model. This method",
          "noises are additive, while signal distortion noise": "involves other types of signal manipulation."
        },
        {
          "tack can be evaluated by adding noise to the dataset": "is commonly used for various tasks, such as, speech",
          "noises are additive, while signal distortion noise": ""
        },
        {
          "tack can be evaluated by adding noise to the dataset": "",
          "noises are additive, while signal distortion noise": "5.1\nEnvironmental Noise"
        },
        {
          "tack can be evaluated by adding noise to the dataset": "recognition, or\nspeaker\nidentification (Abdullah",
          "noises are additive, while signal distortion noise": ""
        },
        {
          "tack can be evaluated by adding noise to the dataset": "et al., 2019), whose perception is\nideally inde-",
          "noises are additive, while signal distortion noise": "We define environmental noises (ENV) as addi-"
        },
        {
          "tack can be evaluated by adding noise to the dataset": "pendent of noise as discussed before.\nThe clos-",
          "noises are additive, while signal distortion noise": "tive background noise, obtained from the ESC-50"
        },
        {
          "tack can be evaluated by adding noise to the dataset": "est\ntask to ours, where the ground truth varies",
          "noises are additive, while signal distortion noise": "dataset(Piczak). ESC-50 is generally used for noise"
        },
        {
          "tack can be evaluated by adding noise to the dataset": "based on noise introduction,\nis sentiment analy-",
          "noises are additive, while signal distortion noise": "contamination and environmental sound classifica-"
        },
        {
          "tack can be evaluated by adding noise to the dataset": "sis. In this case, the adversarial robustness is usu-",
          "noises are additive, while signal distortion noise": "tion (Xu et al., 2021). These environmental sounds"
        },
        {
          "tack can be evaluated by adding noise to the dataset": "ally tested by flipping words to their synonyms,",
          "noises are additive, while signal distortion noise": "are representative of many types of noise seen in"
        },
        {
          "tack can be evaluated by adding noise to the dataset": "such that the meaning of the text remains the same,",
          "noises are additive, while signal distortion noise": "real world deployments, especially in the context"
        },
        {
          "tack can be evaluated by adding noise to the dataset": "and analyzing how the predictions of\nthe model",
          "noises are additive, while signal distortion noise": "of virtual and smart home conversational agents."
        },
        {
          "tack can be evaluated by adding noise to the dataset": "change (Ebrahimi et al., 2017). There is not a di-",
          "noises are additive, while signal distortion noise": "We use the following categories:"
        },
        {
          "tack can be evaluated by adding noise to the dataset": "rect parallel for the acoustics of speech. Hence, the",
          "noises are additive, while signal distortion noise": ""
        },
        {
          "tack can be evaluated by adding noise to the dataset": "",
          "noises are additive, while signal distortion noise": "• Natural soundscapes (Nat), e.g., rain, wind."
        },
        {
          "tack can be evaluated by adding noise to the dataset": "introduction of noise for emotion recognition while",
          "noises are additive, while signal distortion noise": ""
        },
        {
          "tack can be evaluated by adding noise to the dataset": "assuring that the perception remains the same can",
          "noises are additive, while signal distortion noise": ""
        },
        {
          "tack can be evaluated by adding noise to the dataset": "",
          "noises are additive, while signal distortion noise": "• Human,\nnon-speech\nsounds\n(Hum),\ne.g.,"
        },
        {
          "tack can be evaluated by adding noise to the dataset": "be more difficult.",
          "noises are additive, while signal distortion noise": ""
        },
        {
          "tack can be evaluated by adding noise to the dataset": "",
          "noises are additive, while signal distortion noise": "sneezing, coughing, laughing or crying in the"
        },
        {
          "tack can be evaluated by adding noise to the dataset": "To the best of our knowledge,\nthis is the first",
          "noises are additive, while signal distortion noise": ""
        },
        {
          "tack can be evaluated by adding noise to the dataset": "",
          "noises are additive, while signal distortion noise": "background etc."
        },
        {
          "tack can be evaluated by adding noise to the dataset": "work that has studied the effect of different kinds",
          "noises are additive, while signal distortion noise": ""
        },
        {
          "tack can be evaluated by adding noise to the dataset": "of real-world noise and varying amounts of noise",
          "noises are additive, while signal distortion noise": ""
        },
        {
          "tack can be evaluated by adding noise to the dataset": "",
          "noises are additive, while signal distortion noise": "•\nInterior/domestic\nsounds\n(Int),\ne.g.,\ndoor"
        },
        {
          "tack can be evaluated by adding noise to the dataset": "contamination on the human perception of emotion",
          "noises are additive, while signal distortion noise": ""
        },
        {
          "tack can be evaluated by adding noise to the dataset": "",
          "noises are additive, while signal distortion noise": "creaks, clock ticks etc."
        },
        {
          "tack can be evaluated by adding noise to the dataset": "and the implication of training on these datasets",
          "noises are additive, while signal distortion noise": ""
        },
        {
          "tack can be evaluated by adding noise to the dataset": "from the perspective of machine performance and",
          "noises are additive, while signal distortion noise": "We manipulate three factors when adding the"
        },
        {
          "tack can be evaluated by adding noise to the dataset": "robustness to adversarial attacks.",
          "noises are additive, while signal distortion noise": "noise sources:"
        },
        {
          "tack can be evaluated by adding noise to the dataset": "4\nDatasets",
          "noises are additive, while signal distortion noise": "• Position: The position of the introduction of"
        },
        {
          "tack can be evaluated by adding noise to the dataset": "",
          "noises are additive, while signal distortion noise": "sound that: (i) starts and then fades out in loud-"
        },
        {
          "tack can be evaluated by adding noise to the dataset": "For our study, we use the IEMOCAP dataset (Busso",
          "noises are additive, while signal distortion noise": ""
        },
        {
          "tack can be evaluated by adding noise to the dataset": "",
          "noises are additive, while signal distortion noise": "ness or (ii) occurs during the entirety of the"
        },
        {
          "tack can be evaluated by adding noise to the dataset": "et al., 2008), created to explore the relationship be-",
          "noises are additive, while signal distortion noise": ""
        },
        {
          "tack can be evaluated by adding noise to the dataset": "",
          "noises are additive, while signal distortion noise": "duration of the utterance. In the second case,"
        },
        {
          "tack can be evaluated by adding noise to the dataset": "tween emotion, gestures, and speech. The data con-",
          "noises are additive, while signal distortion noise": ""
        },
        {
          "tack can be evaluated by adding noise to the dataset": "",
          "noises are additive, while signal distortion noise": "this complete additive background would rep-"
        },
        {
          "tack can be evaluated by adding noise to the dataset": "tain recordings from five pairs of actors (one male",
          "noises are additive, while signal distortion noise": ""
        },
        {
          "tack can be evaluated by adding noise to the dataset": "",
          "noises are additive, while signal distortion noise": "resent a consistent noise source in real world"
        },
        {
          "tack can be evaluated by adding noise to the dataset": "and one female), 10 actors in total. The actors ei-",
          "noises are additive, while signal distortion noise": ""
        },
        {
          "tack can be evaluated by adding noise to the dataset": "",
          "noises are additive, while signal distortion noise": "(e.g., fan rotation)."
        },
        {
          "tack can be evaluated by adding noise to the dataset": "ther performed from scripted scenes or improvised",
          "noises are additive, while signal distortion noise": ""
        },
        {
          "tack can be evaluated by adding noise to the dataset": "based on target scenarios. The data were segmented",
          "noises are additive, while signal distortion noise": "• Quality Degradation: The decrease in the sig-"
        },
        {
          "tack can be evaluated by adding noise to the dataset": "by speaker turn, resulting in a total of 10,039 utter-",
          "noises are additive, while signal distortion noise": "nal to noise ratio (SNR) caused by the addition"
        },
        {
          "tack can be evaluated by adding noise to the dataset": "ances (5,255 scripted turns and 4,784 improvised",
          "noises are additive, while signal distortion noise": "of the additive background noise at levels of"
        },
        {
          "tack can be evaluated by adding noise to the dataset": "turns). IEMOCAP contains audio, video, and asso-",
          "noises are additive, while signal distortion noise": "20dB, 10dB and 0dB. This is used only when"
        },
        {
          "tack can be evaluated by adding noise to the dataset": "ciated manual transcriptions.",
          "noises are additive, while signal distortion noise": "noise is added to the entirety of the utterance."
        },
        {
          "tack can be evaluated by adding noise to the dataset": "The data were evaluated in terms of dimensional",
          "noises are additive, while signal distortion noise": ""
        },
        {
          "tack can be evaluated by adding noise to the dataset": "",
          "noises are additive, while signal distortion noise": "5.2\nSignal Distortion"
        },
        {
          "tack can be evaluated by adding noise to the dataset": "and categorical\nlabels.\nThe dimensional evalua-",
          "noises are additive, while signal distortion noise": ""
        },
        {
          "tack can be evaluated by adding noise to the dataset": "tions included valence (positive vs. negative), ac-",
          "noises are additive, while signal distortion noise": "We define signal distortion noise as modulations"
        },
        {
          "tack can be evaluated by adding noise to the dataset": "tivation (calm vs.\nexcited), and dominance (pas-",
          "noises are additive, while signal distortion noise": "that aren’t additive in the background. These kinds"
        },
        {
          "tack can be evaluated by adding noise to the dataset": "sive vs. dominant). Each utterance was evaluated",
          "noises are additive, while signal distortion noise": "of noise in the audio signal can occur\nfrom lin-"
        },
        {
          "tack can be evaluated by adding noise to the dataset": "by at least two evaluators. The final dimensional",
          "noises are additive, while signal distortion noise": "guistic/paralinguistic factors,\nroom environment,"
        },
        {
          "tack can be evaluated by adding noise to the dataset": "labels are the average over\nthe individual evalu-",
          "noises are additive, while signal distortion noise": "internet\nlags, or\nthe physical\nlocomotion of\nthe"
        },
        {
          "tack can be evaluated by adding noise to the dataset": "ators. We bin the labels into three classes {low,",
          "noises are additive, while signal distortion noise": "speaker."
        },
        {
          "tack can be evaluated by adding noise to the dataset": "mid, high}, which we defined as {(low:[1,2.75]),",
          "noises are additive, while signal distortion noise": "We use the nine following categories:"
        },
        {
          "tack can be evaluated by adding noise to the dataset": "(mid:(2.75,3.25]), (high:(3.25,5])}.",
          "noises are additive, while signal distortion noise": ""
        },
        {
          "tack can be evaluated by adding noise to the dataset": "",
          "noises are additive, while signal distortion noise": "• SpeedUtt: The utterance is sped up by either"
        },
        {
          "tack can be evaluated by adding noise to the dataset": "5\nNoise",
          "noises are additive, while signal distortion noise": "1.25× or 0.75×."
        },
        {
          "tack can be evaluated by adding noise to the dataset": "We investigate the effects of two types of noise, en-",
          "noises are additive, while signal distortion noise": "• SpeedSeg: A random segment within an ut-"
        },
        {
          "tack can be evaluated by adding noise to the dataset": "vironmental and signal distortion. Environmental",
          "noises are additive, while signal distortion noise": "terance is sped up by 1.25×. The package"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "pyAudio that we used to speed up a segment": "did not permit slowing a segment down. Thus,",
          "for previous perception studies (Parada-Cabaleiro": "et al., 2017; Scharenborg et al., 2018). We select"
        },
        {
          "pyAudio that we used to speed up a segment": "the 0.75× was not used here.",
          "for previous perception studies (Parada-Cabaleiro": "100 samples from each activation and valence pair"
        },
        {
          "pyAudio that we used to speed up a segment": "",
          "for previous perception studies (Parada-Cabaleiro": "bin,\ni.e., 100 samples from the bin with activa-"
        },
        {
          "pyAudio that we used to speed up a segment": "• Fade: The loudness of the utterance is faded",
          "for previous perception studies (Parada-Cabaleiro": ""
        },
        {
          "pyAudio that we used to speed up a segment": "",
          "for previous perception studies (Parada-Cabaleiro": "tion:\nlow, valence:\nlow; 100 samples from the bin"
        },
        {
          "pyAudio that we used to speed up a segment": "by 2% every second, which emulates the sce-",
          "for previous perception studies (Parada-Cabaleiro": ""
        },
        {
          "pyAudio that we used to speed up a segment": "",
          "for previous perception studies (Parada-Cabaleiro": "with activation:\nlow, and valence: mid, and so on."
        },
        {
          "pyAudio that we used to speed up a segment": "nario of a user moving away from the speaker.",
          "for previous perception studies (Parada-Cabaleiro": ""
        },
        {
          "pyAudio that we used to speed up a segment": "",
          "for previous perception studies (Parada-Cabaleiro": "This ensures that the chosen 900 samples cover the"
        },
        {
          "pyAudio that we used to speed up a segment": "The loudness is increased for fade in, and de-",
          "for previous perception studies (Parada-Cabaleiro": ""
        },
        {
          "pyAudio that we used to speed up a segment": "",
          "for previous perception studies (Parada-Cabaleiro": "range of emotions expressed. We impose another"
        },
        {
          "pyAudio that we used to speed up a segment": "creased for fade out.",
          "for previous perception studies (Parada-Cabaleiro": ""
        },
        {
          "pyAudio that we used to speed up a segment": "",
          "for previous perception studies (Parada-Cabaleiro": "constraint on these 100 samples from each bin, 30"
        },
        {
          "pyAudio that we used to speed up a segment": "",
          "for previous perception studies (Parada-Cabaleiro": "of them are shorter than the first quartile or greater"
        },
        {
          "pyAudio that we used to speed up a segment": "• Filler: Non-verbal short fillers such as ‘uh’,",
          "for previous perception studies (Parada-Cabaleiro": ""
        },
        {
          "pyAudio that we used to speed up a segment": "",
          "for previous perception studies (Parada-Cabaleiro": "than fourth quartile of utterance length in seconds"
        },
        {
          "pyAudio that we used to speed up a segment": "‘umm’ (from the same speaker) are inserted",
          "for previous perception studies (Parada-Cabaleiro": ""
        },
        {
          "pyAudio that we used to speed up a segment": "",
          "for previous perception studies (Parada-Cabaleiro": "to cover both extremities of the spectrum, and the"
        },
        {
          "pyAudio that we used to speed up a segment": "in the middle of a sentence. The insertion is",
          "for previous perception studies (Parada-Cabaleiro": ""
        },
        {
          "pyAudio that we used to speed up a segment": "",
          "for previous perception studies (Parada-Cabaleiro": "remaining 70 belong in the middle. We also ensure"
        },
        {
          "pyAudio that we used to speed up a segment": "either just the filler or succeeded and preceded",
          "for previous perception studies (Parada-Cabaleiro": ""
        },
        {
          "pyAudio that we used to speed up a segment": "",
          "for previous perception studies (Parada-Cabaleiro": "that\nthe selected samples had a 50-50 even split"
        },
        {
          "pyAudio that we used to speed up a segment": "by a long pause Fillers are obtained by parsing",
          "for previous perception studies (Parada-Cabaleiro": ""
        },
        {
          "pyAudio that we used to speed up a segment": "",
          "for previous perception studies (Parada-Cabaleiro": "amongst gender. We introduce noise to the 900"
        },
        {
          "pyAudio that we used to speed up a segment": "audio files for a given speaker and finding oc-",
          "for previous perception studies (Parada-Cabaleiro": ""
        },
        {
          "pyAudio that we used to speed up a segment": "",
          "for previous perception studies (Parada-Cabaleiro": "samples (Section 4). Each sample is modulated"
        },
        {
          "pyAudio that we used to speed up a segment": "currences of any of the options from the above",
          "for previous perception studies (Parada-Cabaleiro": ""
        },
        {
          "pyAudio that we used to speed up a segment": "",
          "for previous perception studies (Parada-Cabaleiro": "in ten ways:\nfour randomly chosen types of envi-"
        },
        {
          "pyAudio that we used to speed up a segment": "mentioned set. We will release the extracted",
          "for previous perception studies (Parada-Cabaleiro": ""
        },
        {
          "pyAudio that we used to speed up a segment": "",
          "for previous perception studies (Parada-Cabaleiro": "ronmental noise and six randomly chosen signal"
        },
        {
          "pyAudio that we used to speed up a segment": "fillers per speaker for IEMOCAP",
          "for previous perception studies (Parada-Cabaleiro": ""
        },
        {
          "pyAudio that we used to speed up a segment": "",
          "for previous perception studies (Parada-Cabaleiro": "distortion noise modulations, giving us a total of"
        },
        {
          "pyAudio that we used to speed up a segment": "• DropWord: A randomly selected set of non-",
          "for previous perception studies (Parada-Cabaleiro": "9,000 noisy samples."
        },
        {
          "pyAudio that we used to speed up a segment": "essential words belonging to the set: {a, the,",
          "for previous perception studies (Parada-Cabaleiro": ""
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "5] (similar to the original IEMOCAP annotation).": "We also include three kinds of attention checks:",
          "2017). If an annotator didn’t evaluate a given clip,": "the cell has a null (missing data) value. We found"
        },
        {
          "5] (similar to the original IEMOCAP annotation).": "",
          "2017). If an annotator didn’t evaluate a given clip,": "that the Cohen’s kappa was 79% for activation and"
        },
        {
          "5] (similar to the original IEMOCAP annotation).": "1. We show two samples that have not been mod-",
          "2017). If an annotator didn’t evaluate a given clip,": ""
        },
        {
          "5] (similar to the original IEMOCAP annotation).": "",
          "2017). If an annotator didn’t evaluate a given clip,": "76% for valence."
        },
        {
          "5] (similar to the original IEMOCAP annotation).": "ified and ask them to decide if the emotion",
          "2017). If an annotator didn’t evaluate a given clip,": ""
        },
        {
          "5] (similar to the original IEMOCAP annotation).": "represented was different. If the person says",
          "2017). If an annotator didn’t evaluate a given clip,": ""
        },
        {
          "5] (similar to the original IEMOCAP annotation).": "",
          "2017). If an annotator didn’t evaluate a given clip,": "7\nMethods"
        },
        {
          "5] (similar to the original IEMOCAP annotation).": "yes, then the experiment ends.",
          "2017). If an annotator didn’t evaluate a given clip,": ""
        },
        {
          "5] (similar to the original IEMOCAP annotation).": "",
          "2017). If an annotator didn’t evaluate a given clip,": "We\nnow describe\nthe\nemotion\nrecognition\nap-"
        },
        {
          "5] (similar to the original IEMOCAP annotation).": "2. We observe the time spent on the task. If the",
          "2017). If an annotator didn’t evaluate a given clip,": ""
        },
        {
          "5] (similar to the original IEMOCAP annotation).": "",
          "2017). If an annotator didn’t evaluate a given clip,": "proaches, presenting two separate pipelines, one"
        },
        {
          "5] (similar to the original IEMOCAP annotation).": "time spent on the task is less than the com-",
          "2017). If an annotator didn’t evaluate a given clip,": ""
        },
        {
          "5] (similar to the original IEMOCAP annotation).": "",
          "2017). If an annotator didn’t evaluate a given clip,": "that\nrelies upon direct\nfeature\nextraction (Sec-"
        },
        {
          "5] (similar to the original IEMOCAP annotation).": "bined length of both samples, then the user’s",
          "2017). If an annotator didn’t evaluate a given clip,": ""
        },
        {
          "5] (similar to the original IEMOCAP annotation).": "",
          "2017). If an annotator didn’t evaluate a given clip,": "tion 7.2) and the other\nthat\nis end-to-end (Sec-"
        },
        {
          "5] (similar to the original IEMOCAP annotation).": "qualification to annotate the HITs is rescinded",
          "2017). If an annotator didn’t evaluate a given clip,": ""
        },
        {
          "5] (similar to the original IEMOCAP annotation).": "",
          "2017). If an annotator didn’t evaluate a given clip,": "tion 7.3).\nThis allows us to investigate whether"
        },
        {
          "5] (similar to the original IEMOCAP annotation).": "and their responses are discarded.",
          "2017). If an annotator didn’t evaluate a given clip,": ""
        },
        {
          "5] (similar to the original IEMOCAP annotation).": "",
          "2017). If an annotator didn’t evaluate a given clip,": "noise has\na\nconsistent\neffect.\nWe discuss\nap-"
        },
        {
          "5] (similar to the original IEMOCAP annotation).": "",
          "2017). If an annotator didn’t evaluate a given clip,": "proaches to improve noise robustness by training"
        },
        {
          "5] (similar to the original IEMOCAP annotation).": "3. We show two samples, one which has a gold",
          "2017). If an annotator didn’t evaluate a given clip,": ""
        },
        {
          "5] (similar to the original IEMOCAP annotation).": "",
          "2017). If an annotator didn’t evaluate a given clip,": "models with noise-augmented data or denoised data"
        },
        {
          "5] (similar to the original IEMOCAP annotation).": "standard label, and another, which has been",
          "2017). If an annotator didn’t evaluate a given clip,": ""
        },
        {
          "5] (similar to the original IEMOCAP annotation).": "",
          "2017). If an annotator didn’t evaluate a given clip,": "(Section 7.4). Finally, we describe the setup and"
        },
        {
          "5] (similar to the original IEMOCAP annotation).": "contaminated with significant noise (perfor-",
          "2017). If an annotator didn’t evaluate a given clip,": ""
        },
        {
          "5] (similar to the original IEMOCAP annotation).": "",
          "2017). If an annotator didn’t evaluate a given clip,": "evaluation of\nthe model\nrobustness using an un-"
        },
        {
          "5] (similar to the original IEMOCAP annotation).": "mance degradation >30dB), such that the re-",
          "2017). If an annotator didn’t evaluate a given clip,": ""
        },
        {
          "5] (similar to the original IEMOCAP annotation).": "",
          "2017). If an annotator didn’t evaluate a given clip,": "targeted model misclassification test, which mea-"
        },
        {
          "5] (similar to the original IEMOCAP annotation).": "sulting sample is incomprehensible. If people",
          "2017). If an annotator didn’t evaluate a given clip,": ""
        },
        {
          "5] (similar to the original IEMOCAP annotation).": "",
          "2017). If an annotator didn’t evaluate a given clip,": "sures a model’s fragility in terms of how likely"
        },
        {
          "5] (similar to the original IEMOCAP annotation).": "do not mark this set of samples as being dif-",
          "2017). If an annotator didn’t evaluate a given clip,": ""
        },
        {
          "5] (similar to the original IEMOCAP annotation).": "",
          "2017). If an annotator didn’t evaluate a given clip,": "it\nis that\nthe model’s decisions will change when"
        },
        {
          "5] (similar to the original IEMOCAP annotation).": "ferent, the experiment ends.",
          "2017). If an annotator didn’t evaluate a given clip,": ""
        },
        {
          "5] (similar to the original IEMOCAP annotation).": "",
          "2017). If an annotator didn’t evaluate a given clip,": "specific types of noise are observed at\ntest\ntime"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "they perceived the pair to have the same emotion.": "If they said yes for both activation and valence, the",
          "often leads to artificially inflated inter-annotator": "agreement because individual characteristics and"
        },
        {
          "they perceived the pair to have the same emotion.": "noisy sample was labeled same and they could di-",
          "often leads to artificially inflated inter-annotator": "behavior of a particular worker are not\ntaken un-"
        },
        {
          "they perceived the pair to have the same emotion.": "rectly move to the next HIT. If they said no,\nthe",
          "often leads to artificially inflated inter-annotator": "der consideration (Hoek and Scholman, 2017). We"
        },
        {
          "they perceived the pair to have the same emotion.": "noisy sample was labeled different.\nIn this case,",
          "often leads to artificially inflated inter-annotator": "take a different approach, creating a table for the"
        },
        {
          "they perceived the pair to have the same emotion.": "they were asked to assess the activation and va-",
          "often leads to artificially inflated inter-annotator": "calculation of the statistic that considers annotators"
        },
        {
          "they perceived the pair to have the same emotion.": "lence of the noisy sample using Self Assessment",
          "often leads to artificially inflated inter-annotator": "as individuals with separate entries for each clip,"
        },
        {
          "they perceived the pair to have the same emotion.": "Manikins (Bradley and Lang, 1994) on a scale of [1,",
          "often leads to artificially inflated inter-annotator": "following the approach of (Hoek and Scholman,"
        },
        {
          "they perceived the pair to have the same emotion.": "5] (similar to the original IEMOCAP annotation).",
          "often leads to artificially inflated inter-annotator": "2017). If an annotator didn’t evaluate a given clip,"
        },
        {
          "they perceived the pair to have the same emotion.": "We also include three kinds of attention checks:",
          "often leads to artificially inflated inter-annotator": "the cell has a null (missing data) value. We found"
        },
        {
          "they perceived the pair to have the same emotion.": "",
          "often leads to artificially inflated inter-annotator": "that the Cohen’s kappa was 79% for activation and"
        },
        {
          "they perceived the pair to have the same emotion.": "1. We show two samples that have not been mod-",
          "often leads to artificially inflated inter-annotator": ""
        },
        {
          "they perceived the pair to have the same emotion.": "",
          "often leads to artificially inflated inter-annotator": "76% for valence."
        },
        {
          "they perceived the pair to have the same emotion.": "ified and ask them to decide if the emotion",
          "often leads to artificially inflated inter-annotator": ""
        },
        {
          "they perceived the pair to have the same emotion.": "represented was different. If the person says",
          "often leads to artificially inflated inter-annotator": ""
        },
        {
          "they perceived the pair to have the same emotion.": "",
          "often leads to artificially inflated inter-annotator": "7\nMethods"
        },
        {
          "they perceived the pair to have the same emotion.": "yes, then the experiment ends.",
          "often leads to artificially inflated inter-annotator": ""
        },
        {
          "they perceived the pair to have the same emotion.": "",
          "often leads to artificially inflated inter-annotator": "We\nnow describe\nthe\nemotion\nrecognition\nap-"
        },
        {
          "they perceived the pair to have the same emotion.": "2. We observe the time spent on the task. If the",
          "often leads to artificially inflated inter-annotator": ""
        },
        {
          "they perceived the pair to have the same emotion.": "",
          "often leads to artificially inflated inter-annotator": "proaches, presenting two separate pipelines, one"
        },
        {
          "they perceived the pair to have the same emotion.": "time spent on the task is less than the com-",
          "often leads to artificially inflated inter-annotator": ""
        },
        {
          "they perceived the pair to have the same emotion.": "",
          "often leads to artificially inflated inter-annotator": "that\nrelies upon direct\nfeature\nextraction (Sec-"
        },
        {
          "they perceived the pair to have the same emotion.": "bined length of both samples, then the user’s",
          "often leads to artificially inflated inter-annotator": ""
        },
        {
          "they perceived the pair to have the same emotion.": "",
          "often leads to artificially inflated inter-annotator": "tion 7.2) and the other\nthat\nis end-to-end (Sec-"
        },
        {
          "they perceived the pair to have the same emotion.": "qualification to annotate the HITs is rescinded",
          "often leads to artificially inflated inter-annotator": ""
        },
        {
          "they perceived the pair to have the same emotion.": "",
          "often leads to artificially inflated inter-annotator": "tion 7.3).\nThis allows us to investigate whether"
        },
        {
          "they perceived the pair to have the same emotion.": "and their responses are discarded.",
          "often leads to artificially inflated inter-annotator": ""
        },
        {
          "they perceived the pair to have the same emotion.": "",
          "often leads to artificially inflated inter-annotator": "noise has\na\nconsistent\neffect.\nWe discuss\nap-"
        },
        {
          "they perceived the pair to have the same emotion.": "",
          "often leads to artificially inflated inter-annotator": "proaches to improve noise robustness by training"
        },
        {
          "they perceived the pair to have the same emotion.": "3. We show two samples, one which has a gold",
          "often leads to artificially inflated inter-annotator": ""
        },
        {
          "they perceived the pair to have the same emotion.": "",
          "often leads to artificially inflated inter-annotator": "models with noise-augmented data or denoised data"
        },
        {
          "they perceived the pair to have the same emotion.": "standard label, and another, which has been",
          "often leads to artificially inflated inter-annotator": ""
        },
        {
          "they perceived the pair to have the same emotion.": "",
          "often leads to artificially inflated inter-annotator": "(Section 7.4). Finally, we describe the setup and"
        },
        {
          "they perceived the pair to have the same emotion.": "contaminated with significant noise (perfor-",
          "often leads to artificially inflated inter-annotator": ""
        },
        {
          "they perceived the pair to have the same emotion.": "",
          "often leads to artificially inflated inter-annotator": "evaluation of\nthe model\nrobustness using an un-"
        },
        {
          "they perceived the pair to have the same emotion.": "mance degradation >30dB), such that the re-",
          "often leads to artificially inflated inter-annotator": ""
        },
        {
          "they perceived the pair to have the same emotion.": "",
          "often leads to artificially inflated inter-annotator": "targeted model misclassification test, which mea-"
        },
        {
          "they perceived the pair to have the same emotion.": "sulting sample is incomprehensible. If people",
          "often leads to artificially inflated inter-annotator": ""
        },
        {
          "they perceived the pair to have the same emotion.": "",
          "often leads to artificially inflated inter-annotator": "sures a model’s fragility in terms of how likely"
        },
        {
          "they perceived the pair to have the same emotion.": "do not mark this set of samples as being dif-",
          "often leads to artificially inflated inter-annotator": ""
        },
        {
          "they perceived the pair to have the same emotion.": "",
          "often leads to artificially inflated inter-annotator": "it\nis that\nthe model’s decisions will change when"
        },
        {
          "they perceived the pair to have the same emotion.": "ferent, the experiment ends.",
          "often leads to artificially inflated inter-annotator": ""
        },
        {
          "they perceived the pair to have the same emotion.": "",
          "often leads to artificially inflated inter-annotator": "specific types of noise are observed at\ntest\ntime"
        },
        {
          "they perceived the pair to have the same emotion.": "",
          "often leads to artificially inflated inter-annotator": "(Section 7.5)."
        },
        {
          "they perceived the pair to have the same emotion.": "The failure rate based on the above criteria was 8%.",
          "often leads to artificially inflated inter-annotator": ""
        },
        {
          "they perceived the pair to have the same emotion.": "We ensured the quality of the annotations by pay-",
          "often leads to artificially inflated inter-annotator": ""
        },
        {
          "they perceived the pair to have the same emotion.": "",
          "often leads to artificially inflated inter-annotator": "7.1\nCreation of Data Partitions"
        },
        {
          "they perceived the pair to have the same emotion.": "ing bonuses based on time spent, not just number",
          "often leads to artificially inflated inter-annotator": ""
        },
        {
          "they perceived the pair to have the same emotion.": "of HITs, and by disqualifying annotators if they an-",
          "often leads to artificially inflated inter-annotator": "We use a subject-independent five-fold cross valida-"
        },
        {
          "they perceived the pair to have the same emotion.": "notated any sample (including those outside of the",
          "often leads to artificially inflated inter-annotator": "tion scheme to select our train, test and validation"
        },
        {
          "they perceived the pair to have the same emotion.": "attention checks) more quickly than the combined",
          "often leads to artificially inflated inter-annotator": "sets.\nIn the first\niteration, sessions 1-3 are used"
        },
        {
          "they perceived the pair to have the same emotion.": "length of the audio samples.",
          "often leads to artificially inflated inter-annotator": "for training, session 4 is used as validation, and"
        },
        {
          "they perceived the pair to have the same emotion.": "We then created two sets of labels for each noise-",
          "often leads to artificially inflated inter-annotator": "session 5 is used for testing. This is repeated in"
        },
        {
          "they perceived the pair to have the same emotion.": "augmented clip. The first type of label compared",
          "often leads to artificially inflated inter-annotator": "a round-robin fashion,\nresulting in each session"
        },
        {
          "they perceived the pair to have the same emotion.": "a noise-augmented clip to its original. The noise-",
          "often leads to artificially inflated inter-annotator": "serving as a validation and a test\nfold. We also"
        },
        {
          "they perceived the pair to have the same emotion.": "augmented clip was labeled the same if the mod-",
          "often leads to artificially inflated inter-annotator": "divide possible noises in two different categories"
        },
        {
          "they perceived the pair to have the same emotion.": "ified and original clip were perceived to have the",
          "often leads to artificially inflated inter-annotator": "based on results of crowdsourcing study (see Sec-"
        },
        {
          "they perceived the pair to have the same emotion.": "same valence or activation, otherwise it was labeled",
          "often leads to artificially inflated inter-annotator": "tion 8.1). The first category is perception-altering,"
        },
        {
          "they perceived the pair to have the same emotion.": "different. We created this label by taking the ma-",
          "often leads to artificially inflated inter-annotator": "those that changed perception of humans and hence"
        },
        {
          "they perceived the pair to have the same emotion.": "jority vote over all evaluations. The second type",
          "often leads to artificially inflated inter-annotator": "cannot be used for model\ntraining or evaluation"
        },
        {
          "they perceived the pair to have the same emotion.": "of label included valence and activation. A noise-",
          "often leads to artificially inflated inter-annotator": "with the old annotations. The second category is"
        },
        {
          "they perceived the pair to have the same emotion.": "augmented clip was given the average valence and",
          "often leads to artificially inflated inter-annotator": "perception-retraining,\nthose that did not change"
        },
        {
          "they perceived the pair to have the same emotion.": "activation over all evaluations.",
          "often leads to artificially inflated inter-annotator": "human perception, and hence,\nthe model should"
        },
        {
          "they perceived the pair to have the same emotion.": "The inter-annotator agreement was measured us-",
          "often leads to artificially inflated inter-annotator": "produce no change in predictions when using those"
        },
        {
          "they perceived the pair to have the same emotion.": "ing Cohen’s kappa. Conventionally, when estimat-",
          "often leads to artificially inflated inter-annotator": "noise categories for sample augmentation."
        },
        {
          "they perceived the pair to have the same emotion.": "ing Cohen’s kappa, annotators are not considered",
          "often leads to artificially inflated inter-annotator": "We use the noise categories (seeSection 5)\nin"
        },
        {
          "they perceived the pair to have the same emotion.": "as individuals, instead reducing annotators to the",
          "often leads to artificially inflated inter-annotator": "two varying circumstances. The first category is"
        },
        {
          "they perceived the pair to have the same emotion.": "generic 1, 2, and 3.\nThe challenge is\nthat\nthis",
          "often leads to artificially inflated inter-annotator": "matched, where both the training and testing sets"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "PSEUDO-CODE FOR TESTING MODEL ROBUSTNESS. Exit Code IS": "",
          "Table 1: Hyper-parameters used to select the best per-": "forming model on validation subset whilst training the"
        },
        {
          "PSEUDO-CODE FOR TESTING MODEL ROBUSTNESS. Exit Code IS": "Success WHEN THE ALGORITHM FINDS A NOISE-AUGMENTED VERSION",
          "Table 1: Hyper-parameters used to select the best per-": ""
        },
        {
          "PSEUDO-CODE FOR TESTING MODEL ROBUSTNESS. Exit Code IS": "",
          "Table 1: Hyper-parameters used to select the best per-": "traditional deep learning model."
        },
        {
          "PSEUDO-CODE FOR TESTING MODEL ROBUSTNESS. Exit Code IS": "OF THE SAMPLE THAT THE MODEL CHANGES PREDICTION FOR. Exit",
          "Table 1: Hyper-parameters used to select the best per-": ""
        },
        {
          "PSEUDO-CODE FOR TESTING MODEL ROBUSTNESS. Exit Code IS": "Code IS Failure WHEN THE MODEL MAINTAINS ITS PREDICTIONS OVER",
          "Table 1: Hyper-parameters used to select the best per-": ""
        },
        {
          "PSEUDO-CODE FOR TESTING MODEL ROBUSTNESS. Exit Code IS": "",
          "Table 1: Hyper-parameters used to select the best per-": "Hyper-parameter\nValues"
        },
        {
          "PSEUDO-CODE FOR TESTING MODEL ROBUSTNESS. Exit Code IS": "THE ANY OF THE NOISE-AUGMENTED VERSIONS TRIED.",
          "Table 1: Hyper-parameters used to select the best per-": ""
        },
        {
          "PSEUDO-CODE FOR TESTING MODEL ROBUSTNESS. Exit Code IS": "",
          "Table 1: Hyper-parameters used to select the best per-": "Traditional"
        },
        {
          "PSEUDO-CODE FOR TESTING MODEL ROBUSTNESS. Exit Code IS": "Randomly sample 1 noise variation from each category",
          "Table 1: Hyper-parameters used to select the best per-": ""
        },
        {
          "PSEUDO-CODE FOR TESTING MODEL ROBUSTNESS. Exit Code IS": "mentioned in Section 5.;",
          "Table 1: Hyper-parameters used to select the best per-": ""
        },
        {
          "PSEUDO-CODE FOR TESTING MODEL ROBUSTNESS. Exit Code IS": "",
          "Table 1: Hyper-parameters used to select the best per-": "No. of Convolution Kernels\n{64, 128}"
        },
        {
          "PSEUDO-CODE FOR TESTING MODEL ROBUSTNESS. Exit Code IS": "numAttempts = 0;",
          "Table 1: Hyper-parameters used to select the best per-": ""
        },
        {
          "PSEUDO-CODE FOR TESTING MODEL ROBUSTNESS. Exit Code IS": "",
          "Table 1: Hyper-parameters used to select the best per-": "Convolution Kernels Width\n{2}"
        },
        {
          "PSEUDO-CODE FOR TESTING MODEL ROBUSTNESS. Exit Code IS": "for each noise in selected random noises: do",
          "Table 1: Hyper-parameters used to select the best per-": ""
        },
        {
          "PSEUDO-CODE FOR TESTING MODEL ROBUSTNESS. Exit Code IS": "",
          "Table 1: Hyper-parameters used to select the best per-": "Number of Convolution Layers\n{5}"
        },
        {
          "PSEUDO-CODE FOR TESTING MODEL ROBUSTNESS. Exit Code IS": "Add noise to the sample such that the decrease in SNR",
          "Table 1: Hyper-parameters used to select the best per-": ""
        },
        {
          "PSEUDO-CODE FOR TESTING MODEL ROBUSTNESS. Exit Code IS": "",
          "Table 1: Hyper-parameters used to select the best per-": "Number of GRU layers\n{1, 2, 3}"
        },
        {
          "PSEUDO-CODE FOR TESTING MODEL ROBUSTNESS. Exit Code IS": "is 1.;",
          "Table 1: Hyper-parameters used to select the best per-": ""
        },
        {
          "PSEUDO-CODE FOR TESTING MODEL ROBUSTNESS. Exit Code IS": "",
          "Table 1: Hyper-parameters used to select the best per-": "Pooling Kernel Width\n{2, 4}"
        },
        {
          "PSEUDO-CODE FOR TESTING MODEL ROBUSTNESS. Exit Code IS": "Get the classifier output with this new sample",
          "Table 1: Hyper-parameters used to select the best per-": ""
        },
        {
          "PSEUDO-CODE FOR TESTING MODEL ROBUSTNESS. Exit Code IS": "",
          "Table 1: Hyper-parameters used to select the best per-": "GRU Layers Width\n{32, 64}"
        },
        {
          "PSEUDO-CODE FOR TESTING MODEL ROBUSTNESS. Exit Code IS": "variation.;",
          "Table 1: Hyper-parameters used to select the best per-": ""
        },
        {
          "PSEUDO-CODE FOR TESTING MODEL ROBUSTNESS. Exit Code IS": "",
          "Table 1: Hyper-parameters used to select the best per-": "Number of Dense Layers\n{1, 2, 3}"
        },
        {
          "PSEUDO-CODE FOR TESTING MODEL ROBUSTNESS. Exit Code IS": "numAttempts+ = 1;",
          "Table 1: Hyper-parameters used to select the best per-": ""
        },
        {
          "PSEUDO-CODE FOR TESTING MODEL ROBUSTNESS. Exit Code IS": "if numAttempts > k then",
          "Table 1: Hyper-parameters used to select the best per-": "End to End"
        },
        {
          "PSEUDO-CODE FOR TESTING MODEL ROBUSTNESS. Exit Code IS": "return Exit Code = Failure",
          "Table 1: Hyper-parameters used to select the best per-": ""
        },
        {
          "PSEUDO-CODE FOR TESTING MODEL ROBUSTNESS. Exit Code IS": "",
          "Table 1: Hyper-parameters used to select the best per-": "No. of Dense Layers\n{1, 2}"
        },
        {
          "PSEUDO-CODE FOR TESTING MODEL ROBUSTNESS. Exit Code IS": "end",
          "Table 1: Hyper-parameters used to select the best per-": ""
        },
        {
          "PSEUDO-CODE FOR TESTING MODEL ROBUSTNESS. Exit Code IS": "if classifier output changes then",
          "Table 1: Hyper-parameters used to select the best per-": ""
        },
        {
          "PSEUDO-CODE FOR TESTING MODEL ROBUSTNESS. Exit Code IS": "return Exit Code = Success",
          "Table 1: Hyper-parameters used to select the best per-": ""
        },
        {
          "PSEUDO-CODE FOR TESTING MODEL ROBUSTNESS. Exit Code IS": "end",
          "Table 1: Hyper-parameters used to select the best per-": ""
        },
        {
          "PSEUDO-CODE FOR TESTING MODEL ROBUSTNESS. Exit Code IS": "",
          "Table 1: Hyper-parameters used to select the best per-": "are augmented with same kinds of noise (e.g., both"
        },
        {
          "PSEUDO-CODE FOR TESTING MODEL ROBUSTNESS. Exit Code IS": "end",
          "Table 1: Hyper-parameters used to select the best per-": ""
        },
        {
          "PSEUDO-CODE FOR TESTING MODEL ROBUSTNESS. Exit Code IS": "",
          "Table 1: Hyper-parameters used to select the best per-": "have nature-based sounds in them). The second"
        },
        {
          "PSEUDO-CODE FOR TESTING MODEL ROBUSTNESS. Exit Code IS": "for each noise in selected random noises: do",
          "Table 1: Hyper-parameters used to select the best per-": ""
        },
        {
          "PSEUDO-CODE FOR TESTING MODEL ROBUSTNESS. Exit Code IS": "Add noise to the sample such that the decrease in SNR",
          "Table 1: Hyper-parameters used to select the best per-": "category is mismatched, where the testing set\nis"
        },
        {
          "PSEUDO-CODE FOR TESTING MODEL ROBUSTNESS. Exit Code IS": "is 5.;",
          "Table 1: Hyper-parameters used to select the best per-": ""
        },
        {
          "PSEUDO-CODE FOR TESTING MODEL ROBUSTNESS. Exit Code IS": "",
          "Table 1: Hyper-parameters used to select the best per-": "augmented with a noise category not used for aug-"
        },
        {
          "PSEUDO-CODE FOR TESTING MODEL ROBUSTNESS. Exit Code IS": "Get the classifier output with this new sample",
          "Table 1: Hyper-parameters used to select the best per-": ""
        },
        {
          "PSEUDO-CODE FOR TESTING MODEL ROBUSTNESS. Exit Code IS": "variation.;",
          "Table 1: Hyper-parameters used to select the best per-": "menting the training set (e.g., only the test set\nis"
        },
        {
          "PSEUDO-CODE FOR TESTING MODEL ROBUSTNESS. Exit Code IS": "numAttempts+ = 1;",
          "Table 1: Hyper-parameters used to select the best per-": ""
        },
        {
          "PSEUDO-CODE FOR TESTING MODEL ROBUSTNESS. Exit Code IS": "",
          "Table 1: Hyper-parameters used to select the best per-": "augmented with nature-based noise while the train"
        },
        {
          "PSEUDO-CODE FOR TESTING MODEL ROBUSTNESS. Exit Code IS": "if numAttempts > k then",
          "Table 1: Hyper-parameters used to select the best per-": ""
        },
        {
          "PSEUDO-CODE FOR TESTING MODEL ROBUSTNESS. Exit Code IS": "",
          "Table 1: Hyper-parameters used to select the best per-": "set is augmented with human or interior noises)."
        },
        {
          "PSEUDO-CODE FOR TESTING MODEL ROBUSTNESS. Exit Code IS": "return Exit Code = Failure",
          "Table 1: Hyper-parameters used to select the best per-": ""
        },
        {
          "PSEUDO-CODE FOR TESTING MODEL ROBUSTNESS. Exit Code IS": "end",
          "Table 1: Hyper-parameters used to select the best per-": ""
        },
        {
          "PSEUDO-CODE FOR TESTING MODEL ROBUSTNESS. Exit Code IS": "if classifier output changes then",
          "Table 1: Hyper-parameters used to select the best per-": "7.2\nTraditional Deep Learning Network"
        },
        {
          "PSEUDO-CODE FOR TESTING MODEL ROBUSTNESS. Exit Code IS": "return Exit Code = Success",
          "Table 1: Hyper-parameters used to select the best per-": ""
        },
        {
          "PSEUDO-CODE FOR TESTING MODEL ROBUSTNESS. Exit Code IS": "end",
          "Table 1: Hyper-parameters used to select the best per-": "We first explore a common “traditional” deep learn-"
        },
        {
          "PSEUDO-CODE FOR TESTING MODEL ROBUSTNESS. Exit Code IS": "if classifier output changes then",
          "Table 1: Hyper-parameters used to select the best per-": ""
        },
        {
          "PSEUDO-CODE FOR TESTING MODEL ROBUSTNESS. Exit Code IS": "",
          "Table 1: Hyper-parameters used to select the best per-": "ing network that is used in speech emotion recog-"
        },
        {
          "PSEUDO-CODE FOR TESTING MODEL ROBUSTNESS. Exit Code IS": "while classifier output does not change do",
          "Table 1: Hyper-parameters used to select the best per-": ""
        },
        {
          "PSEUDO-CODE FOR TESTING MODEL ROBUSTNESS. Exit Code IS": "",
          "Table 1: Hyper-parameters used to select the best per-": "nition.\nIn this method we extract Mel Filterbank"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "if classifier output changes then": "",
          "7.2\nTraditional Deep Learning Network": ""
        },
        {
          "if classifier output changes then": "",
          "7.2\nTraditional Deep Learning Network": "We first explore a common “traditional” deep learn-"
        },
        {
          "if classifier output changes then": "if classifier output changes then",
          "7.2\nTraditional Deep Learning Network": ""
        },
        {
          "if classifier output changes then": "",
          "7.2\nTraditional Deep Learning Network": "ing network that is used in speech emotion recog-"
        },
        {
          "if classifier output changes then": "",
          "7.2\nTraditional Deep Learning Network": ""
        },
        {
          "if classifier output changes then": "",
          "7.2\nTraditional Deep Learning Network": "nition.\nIn this method we extract Mel Filterbank"
        },
        {
          "if classifier output changes then": "",
          "7.2\nTraditional Deep Learning Network": ""
        },
        {
          "if classifier output changes then": "",
          "7.2\nTraditional Deep Learning Network": "(MFB) features as input to a model composed of"
        },
        {
          "if classifier output changes then": "",
          "7.2\nTraditional Deep Learning Network": ""
        },
        {
          "if classifier output changes then": "",
          "7.2\nTraditional Deep Learning Network": "convolutional and gated recurrent unit (GRU) lay-"
        },
        {
          "if classifier output changes then": "",
          "7.2\nTraditional Deep Learning Network": ""
        },
        {
          "if classifier output changes then": "",
          "7.2\nTraditional Deep Learning Network": "ers."
        },
        {
          "if classifier output changes then": "",
          "7.2\nTraditional Deep Learning Network": ""
        },
        {
          "if classifier output changes then": "",
          "7.2\nTraditional Deep Learning Network": "7.2.1\nFeatures"
        },
        {
          "if classifier output changes then": "",
          "7.2\nTraditional Deep Learning Network": ""
        },
        {
          "if classifier output changes then": "",
          "7.2\nTraditional Deep Learning Network": "We extract 40-dimensional Mel Filterbank (MFB)"
        },
        {
          "if classifier output changes then": "",
          "7.2\nTraditional Deep Learning Network": ""
        },
        {
          "if classifier output changes then": "end",
          "7.2\nTraditional Deep Learning Network": "features using a 25-millisecond Hamming window"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "layer to produce an acoustic representation which": "is then fed into a set of dense layers to classify",
          "module, and (iii) a transformer module. The in-": "put to the model is raw audio data (16kHz) that is"
        },
        {
          "layer to produce an acoustic representation which": "activation or valence.",
          "module, and (iii) a transformer module. The in-": "passed to a multi-block 1-d CNN to generate audio"
        },
        {
          "layer to produce an acoustic representation which": "",
          "module, and (iii) a transformer module. The in-": "representations (25ms). The quantizer is similar to"
        },
        {
          "layer to produce an acoustic representation which": "7.2.3\nTraining.",
          "module, and (iii) a transformer module. The in-": ""
        },
        {
          "layer to produce an acoustic representation which": "",
          "module, and (iii) a transformer module. The in-": "a variational autoencoder that encodes and extracts"
        },
        {
          "layer to produce an acoustic representation which": "We\nimplement\nthe models\nusing\nthe Keras\nli-",
          "module, and (iii) a transformer module. The in-": "features using a contrastive loss. The transformer is"
        },
        {
          "layer to produce an acoustic representation which": "brary (Chollet, 2015). We use a cross-entropy loss",
          "module, and (iii) a transformer module. The in-": "used for masked sequence prediction and encodes"
        },
        {
          "layer to produce an acoustic representation which": "function for each task (e.g., valence or activation).",
          "module, and (iii) a transformer module. The in-": "the bi-directional temporal context of the features."
        },
        {
          "layer to produce an acoustic representation which": "We learn the model parameters using the RMSProp",
          "module, and (iii) a transformer module. The in-": "We use the base model, which has not been fine-"
        },
        {
          "layer to produce an acoustic representation which": "optimizer. We train our networks for a maximum",
          "module, and (iii) a transformer module. The in-": "tuned for ASR (wav2vec2.0-PT). We then fine-tune"
        },
        {
          "layer to produce an acoustic representation which": "of 50 epochs and use early stopping if the valida-",
          "module, and (iii) a transformer module. The in-": "the base model to predict the binned emotion labels."
        },
        {
          "layer to produce an acoustic representation which": "tion loss does not\nimprove after five consecutive",
          "module, and (iii) a transformer module. The in-": "We use the final representation of the output as an"
        },
        {
          "layer to produce an acoustic representation which": "epochs. Once the training process ends, we revert",
          "module, and (iii) a transformer module. The in-": "input to dense layers to produce the final output."
        },
        {
          "layer to produce an acoustic representation which": "the network’s weights to those that achieved the",
          "module, and (iii) a transformer module. The in-": ""
        },
        {
          "layer to produce an acoustic representation which": "",
          "module, and (iii) a transformer module. The in-": "7.3.3\nTraining"
        },
        {
          "layer to produce an acoustic representation which": "lowest validation loss. We repeat the experiment",
          "module, and (iii) a transformer module. The in-": ""
        },
        {
          "layer to produce an acoustic representation which": "five times. We report\nthe results in terms of Un-",
          "module, and (iii) a transformer module. The in-": "We implement\nthe model provided in the speech"
        },
        {
          "layer to produce an acoustic representation which": "weighted Average Recall (UAR, chance is 0.33),",
          "module, and (iii) a transformer module. The in-": "brain library. As in the other pipeline (Section 7.2),"
        },
        {
          "layer to produce an acoustic representation which": "averaged over all test samples and five repetitions.",
          "module, and (iii) a transformer module. The in-": "we use cross-entropy loss for each task and learn"
        },
        {
          "layer to produce an acoustic representation which": "We compare the performance of different models",
          "module, and (iii) a transformer module. The in-": "the dense layer parameters. Reproducing the state"
        },
        {
          "layer to produce an acoustic representation which": "or the same model\nin different noisy conditions/-",
          "module, and (iii) a transformer module. The in-": "of the art model\n(Pepino et al., 2021) = 2,021 We"
        },
        {
          "layer to produce an acoustic representation which": "partitions using a paired t-test using the Bonferroni",
          "module, and (iii) a transformer module. The in-": "run the model for a maximum of eight epochs. We"
        },
        {
          "layer to produce an acoustic representation which": "correction, asserting significance when p ≤ 0.05.",
          "module, and (iii) a transformer module. The in-": "revert the network’s state to the one that achieved"
        },
        {
          "layer to produce an acoustic representation which": "",
          "module, and (iii) a transformer module. The in-": "the lowest validation loss. We repeat\nthis experi-"
        },
        {
          "layer to produce an acoustic representation which": "7.3\nEnd-to-End Deep Learning Networks",
          "module, and (iii) a transformer module. The in-": "ment five times. Again, we use UAR and report the"
        },
        {
          "layer to produce an acoustic representation which": "",
          "module, and (iii) a transformer module. The in-": "results averaged over both subjects and repetitions."
        },
        {
          "layer to produce an acoustic representation which": "Next, we explore a transformer-based model.\nIn",
          "module, and (iii) a transformer module. The in-": ""
        },
        {
          "layer to produce an acoustic representation which": "this method the raw audio signal is used as input",
          "module, and (iii) a transformer module. The in-": ""
        },
        {
          "layer to produce an acoustic representation which": "",
          "module, and (iii) a transformer module. The in-": "7.4\nNoise Augmentation Overview"
        },
        {
          "layer to produce an acoustic representation which": "to a pre-trained and fine-tuned network and the",
          "module, and (iii) a transformer module. The in-": ""
        },
        {
          "layer to produce an acoustic representation which": "",
          "module, and (iii) a transformer module. The in-": "We will be assessing a model’s ability to classify"
        },
        {
          "layer to produce an acoustic representation which": "emotion prediction is directly obtained as an output.",
          "module, and (iii) a transformer module. The in-": ""
        },
        {
          "layer to produce an acoustic representation which": "",
          "module, and (iii) a transformer module. The in-": "emotion given either environmental or signal dis-"
        },
        {
          "layer to produce an acoustic representation which": "These models do not require us to perform manual",
          "module, and (iii) a transformer module. The in-": ""
        },
        {
          "layer to produce an acoustic representation which": "",
          "module, and (iii) a transformer module. The in-": "tortion noise. We perform two kinds of analysis,"
        },
        {
          "layer to produce an acoustic representation which": "or domain knowledge-based extraction of features.",
          "module, and (iii) a transformer module. The in-": ""
        },
        {
          "layer to produce an acoustic representation which": "",
          "module, and (iii) a transformer module. The in-": "one when using the set of noises that includes those"
        },
        {
          "layer to produce an acoustic representation which": "They instead have a feature encoder component",
          "module, and (iii) a transformer module. The in-": ""
        },
        {
          "layer to produce an acoustic representation which": "",
          "module, and (iii) a transformer module. The in-": "that do alter human perception, and another when"
        },
        {
          "layer to produce an acoustic representation which": "inside the model, which is dynamic in nature, and",
          "module, and (iii) a transformer module. The in-": ""
        },
        {
          "layer to produce an acoustic representation which": "",
          "module, and (iii) a transformer module. The in-": "only using noises that are perception-retaining. We"
        },
        {
          "layer to produce an acoustic representation which": "hence, can change its output for the same signal",
          "module, and (iii) a transformer module. The in-": ""
        },
        {
          "layer to produce an acoustic representation which": "",
          "module, and (iii) a transformer module. The in-": "report overall model performances for both of these"
        },
        {
          "layer to produce an acoustic representation which": "based on the dataset and nature of the task.",
          "module, and (iii) a transformer module. The in-": ""
        },
        {
          "layer to produce an acoustic representation which": "",
          "module, and (iii) a transformer module. The in-": "categories."
        },
        {
          "layer to produce an acoustic representation which": "7.3.1\nFeatures",
          "module, and (iii) a transformer module. The in-": ""
        },
        {
          "layer to produce an acoustic representation which": "",
          "module, and (iii) a transformer module. The in-": "For a more thorough analysis, we then specifi-"
        },
        {
          "layer to produce an acoustic representation which": "For the end-to-end deep learning models, we do",
          "module, and (iii) a transformer module. The in-": "cally focus on the categories of noise that do not"
        },
        {
          "layer to produce an acoustic representation which": "not need to extract audio features. Instead we rely",
          "module, and (iii) a transformer module. The in-": "significantly affect human perception. This allows"
        },
        {
          "layer to produce an acoustic representation which": "on the network itself\nto both normalize and ex-",
          "module, and (iii) a transformer module. The in-": "us to evaluate a model’s robustness, or its fragility,"
        },
        {
          "layer to produce an acoustic representation which": "tract features, that are later passed onto the deeper",
          "module, and (iii) a transformer module. The in-": "with respect to variations that wouldn’t alter a hu-"
        },
        {
          "layer to produce an acoustic representation which": "layers of the network. The feature set here is the",
          "module, and (iii) a transformer module. The in-": "man’s perception of emotion. This is important"
        },
        {
          "layer to produce an acoustic representation which": "original wav files that are not modified in any ca-",
          "module, and (iii) a transformer module. The in-": "because the overwhelming majority of the noise-"
        },
        {
          "layer to produce an acoustic representation which": "pacity.\nThe eventual\nrepresentations are of size",
          "module, and (iii) a transformer module. The in-": "augmented utterances in the IEMOCAP dataset"
        },
        {
          "layer to produce an acoustic representation which": "512, reproducing the setup in the state-of-the-art",
          "module, and (iii) a transformer module. The in-": "were not\nincluded in the user\nstudy and,\nthere-"
        },
        {
          "layer to produce an acoustic representation which": "implementation (Pepino et al., 2021).",
          "module, and (iii) a transformer module. The in-": "fore, do not have perceptual labels (Section 6). We"
        },
        {
          "layer to produce an acoustic representation which": "",
          "module, and (iii) a transformer module. The in-": "consider three types of environmental noise {Hu-"
        },
        {
          "layer to produce an acoustic representation which": "7.3.2\nNetwork",
          "module, and (iii) a transformer module. The in-": ""
        },
        {
          "layer to produce an acoustic representation which": "",
          "module, and (iii) a transformer module. The in-": "man (Hum), Interior (Int), Natural (Nat)} and three"
        },
        {
          "layer to produce an acoustic representation which": "Our\nbaseline\nnetwork\nis\nthe\nstate-of-the-art",
          "module, and (iii) a transformer module. The in-": "types of signal distortion noise {Speeding a seg-"
        },
        {
          "layer to produce an acoustic representation which": "wav2vec2.0 emotion recognition model\n(Pepino",
          "module, and (iii) a transformer module. The in-": "ment (SpeedSeg), Fade, Reverberation (Reverb)}."
        },
        {
          "layer to produce an acoustic representation which": "et al., 2021). The wav2vec model\nis comprised",
          "module, and (iii) a transformer module. The in-": "We use\ntwo separate\ntesting paradigms:\n(i)"
        },
        {
          "layer to produce an acoustic representation which": "of three parts:\n(i) a convolutional neural network",
          "module, and (iii) a transformer module. The in-": "matched testing, in which all noise types are intro-"
        },
        {
          "layer to produce an acoustic representation which": "(CNN) that acts as feature encoder, (ii) a quantizier",
          "module, and (iii) a transformer module. The in-": "duced to the training, testing, and validation data"
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "and (ii) mismatched testing, in which n-1 types of": "noise are introduced to the training and validation",
          "model (Hsu et al., 2021). The noise-robust\nlarge": "model was pretrained on 16kHz sampled speech au-"
        },
        {
          "and (ii) mismatched testing, in which n-1 types of": "sets and the heldout type of noise is introduced to",
          "model (Hsu et al., 2021). The noise-robust\nlarge": "dio. Noisy speech datasets from multiple domains"
        },
        {
          "and (ii) mismatched testing, in which n-1 types of": "the test set. In all cases, we analyze the test data in",
          "model (Hsu et al., 2021). The noise-robust\nlarge": "were used to pretrain the model: Libri-Light, Com-"
        },
        {
          "and (ii) mismatched testing, in which n-1 types of": "terms of specific noise categories. Therefore, the",
          "model (Hsu et al., 2021). The noise-robust\nlarge": "monVoice, Switchboard, and, Fisher (Hsu et al.,"
        },
        {
          "and (ii) mismatched testing, in which n-1 types of": "test sets are the same between the two paradigms.",
          "model (Hsu et al., 2021). The noise-robust\nlarge": "2021). We then train the end-to-end model as de-"
        },
        {
          "and (ii) mismatched testing, in which n-1 types of": "We run both the matched and mismatched exper-",
          "model (Hsu et al., 2021). The noise-robust\nlarge": "scribed in Section 7.3."
        },
        {
          "and (ii) mismatched testing, in which n-1 types of": "iments twice, first with the noise-augmented data",
          "model (Hsu et al., 2021). The noise-robust\nlarge": ""
        },
        {
          "and (ii) mismatched testing, in which n-1 types of": "",
          "model (Hsu et al., 2021). The noise-robust\nlarge": "7.5\nModel Robustness Testing"
        },
        {
          "and (ii) mismatched testing, in which n-1 types of": "and second with a noise-robust/denoising pipeline.",
          "model (Hsu et al., 2021). The noise-robust\nlarge": ""
        },
        {
          "and (ii) mismatched testing, in which n-1 types of": "The first iteration will allow us to quantify the ef-",
          "model (Hsu et al., 2021). The noise-robust\nlarge": "Deployed emotion recognition models must be ro-"
        },
        {
          "and (ii) mismatched testing, in which n-1 types of": "fect of the noise on the traditional and end-to-end",
          "model (Hsu et al., 2021). The noise-robust\nlarge": "bust. One of the major scenarios that we robustness"
        },
        {
          "and (ii) mismatched testing, in which n-1 types of": "classification pipelines. We then repeat the experi-",
          "model (Hsu et al., 2021). The noise-robust\nlarge": "test any speech-based model for is the presence of"
        },
        {
          "and (ii) mismatched testing, in which n-1 types of": "ment with either denoised data for the traditional",
          "model (Hsu et al., 2021). The noise-robust\nlarge": "noise. But the set of noises we choose to test robust-"
        },
        {
          "and (ii) mismatched testing, in which n-1 types of": "classifier (Section 7.4.1) or using the noise-robust",
          "model (Hsu et al., 2021). The noise-robust\nlarge": "ness on can lead to different conclusions about the"
        },
        {
          "and (ii) mismatched testing, in which n-1 types of": "implementation of wav2vec2.0 for the end-to-end",
          "model (Hsu et al., 2021). The noise-robust\nlarge": "robustness of the models. In our case, we consider"
        },
        {
          "and (ii) mismatched testing, in which n-1 types of": "classifier (Section 7.4.2). This allows us to investi-",
          "model (Hsu et al., 2021). The noise-robust\nlarge": "two different scenarios:"
        },
        {
          "and (ii) mismatched testing, in which n-1 types of": "gate how, or if, noise-robust implementations can",
          "model (Hsu et al., 2021). The noise-robust\nlarge": ""
        },
        {
          "and (ii) mismatched testing, in which n-1 types of": "",
          "model (Hsu et al., 2021). The noise-robust\nlarge": "1. Robustness evaluation when using perception-"
        },
        {
          "and (ii) mismatched testing, in which n-1 types of": "offset the effects of background noise.",
          "model (Hsu et al., 2021). The noise-robust\nlarge": ""
        },
        {
          "and (ii) mismatched testing, in which n-1 types of": "",
          "model (Hsu et al., 2021). The noise-robust\nlarge": "retaining samples, noise samples that do not"
        },
        {
          "and (ii) mismatched testing, in which n-1 types of": "7.4.1\nDenoising",
          "model (Hsu et al., 2021). The noise-robust\nlarge": "change human perception"
        },
        {
          "and (ii) mismatched testing, in which n-1 types of": "We implement denoising using the well-known",
          "model (Hsu et al., 2021). The noise-robust\nlarge": ""
        },
        {
          "and (ii) mismatched testing, in which n-1 types of": "",
          "model (Hsu et al., 2021). The noise-robust\nlarge": "2. Robustness evaluation when using any kind"
        },
        {
          "and (ii) mismatched testing, in which n-1 types of": "Recurrent Neural Network Noise\nSuppression",
          "model (Hsu et al., 2021). The noise-robust\nlarge": ""
        },
        {
          "and (ii) mismatched testing, in which n-1 types of": "",
          "model (Hsu et al., 2021). The noise-robust\nlarge": "of noise (i.e., both perception-retaining and"
        },
        {
          "and (ii) mismatched testing, in which n-1 types of": "(RNNNoise, denoising feature space) approach,",
          "model (Hsu et al., 2021). The noise-robust\nlarge": ""
        },
        {
          "and (ii) mismatched testing, in which n-1 types of": "",
          "model (Hsu et al., 2021). The noise-robust\nlarge": "perception-altering)"
        },
        {
          "and (ii) mismatched testing, in which n-1 types of": "proposed in 2017 for noise suppression (Valin,",
          "model (Hsu et al., 2021). The noise-robust\nlarge": ""
        },
        {
          "and (ii) mismatched testing, in which n-1 types of": "2018).\nRNNNoise is\ntrained on environmental",
          "model (Hsu et al., 2021). The noise-robust\nlarge": "We perform robustness evaluation of a model"
        },
        {
          "and (ii) mismatched testing, in which n-1 types of": "noise, and these noises overlap considerably with",
          "model (Hsu et al., 2021). The noise-robust\nlarge": "by using the model’s output predictions\nto cre-"
        },
        {
          "and (ii) mismatched testing, in which n-1 types of": "those in our dataset. We use the algorithm’s de-",
          "model (Hsu et al., 2021). The noise-robust\nlarge": "ate new noise-enhanced samples that change the"
        },
        {
          "and (ii) mismatched testing, in which n-1 types of": "fault parameters and use it on an ‘as-is’ basis for",
          "model (Hsu et al., 2021). The noise-robust\nlarge": "model’s output, compared to the original clean sam-"
        },
        {
          "and (ii) mismatched testing, in which n-1 types of": "our experiments. We assume that the system does",
          "model (Hsu et al., 2021). The noise-robust\nlarge": "ple. We do this using an untargeted model misclas-"
        },
        {
          "and (ii) mismatched testing, in which n-1 types of": "not have the knowledge of which noise, from the",
          "model (Hsu et al., 2021). The noise-robust\nlarge": "sification test,\nin which we add noise to the sam-"
        },
        {
          "and (ii) mismatched testing, in which n-1 types of": "set of available noise categories, is introduced and,",
          "model (Hsu et al., 2021). The noise-robust\nlarge": "ples. The intentional misclassification algorithm"
        },
        {
          "and (ii) mismatched testing, in which n-1 types of": "therefore, we do not compare with other denoising",
          "model (Hsu et al., 2021). The noise-robust\nlarge": "assumes black-box model access. For our purposes,"
        },
        {
          "and (ii) mismatched testing, in which n-1 types of": "algorithms that assume a priori knowledge of noise",
          "model (Hsu et al., 2021). The noise-robust\nlarge": "it needs to have access to: (i) a subset of the dataset,"
        },
        {
          "and (ii) mismatched testing, in which n-1 types of": "category. The result is a set of ‘noise-suppressed’",
          "model (Hsu et al., 2021). The noise-robust\nlarge": "(ii) noises to add to create perturbed samples, and"
        },
        {
          "and (ii) mismatched testing, in which n-1 types of": "samples in the training, validation and testing sets.",
          "model (Hsu et al., 2021). The noise-robust\nlarge": "(iii) model input and output."
        },
        {
          "and (ii) mismatched testing, in which n-1 types of": "We pass all\nthe data,\nincluding both the origi-",
          "model (Hsu et al., 2021). The noise-robust\nlarge": "As in any other perturbation-based robustness"
        },
        {
          "and (ii) mismatched testing, in which n-1 types of": "nal and noise-augmented data, through a denoising",
          "model (Hsu et al., 2021). The noise-robust\nlarge": "testing, the goal is to introduce perturbations to the"
        },
        {
          "and (ii) mismatched testing, in which n-1 types of": "algorithm. This allows us to ensure that acoustic",
          "model (Hsu et al., 2021). The noise-robust\nlarge": "samples such that the resulting samples are as close"
        },
        {
          "and (ii) mismatched testing, in which n-1 types of": "artifacts,\nif any, are introduced to both the origi-",
          "model (Hsu et al., 2021). The noise-robust\nlarge": "to the original sample as possible. The minimally"
        },
        {
          "and (ii) mismatched testing, in which n-1 types of": "nal and noise-augmented data. We then train the",
          "model (Hsu et al., 2021). The noise-robust\nlarge": "perturbed sample should be the one that causes a"
        },
        {
          "and (ii) mismatched testing, in which n-1 types of": "traditional deep learning model as described in Sec-",
          "model (Hsu et al., 2021). The noise-robust\nlarge": "classifier to change its original classification. We"
        },
        {
          "and (ii) mismatched testing, in which n-1 types of": "tion 7.2.",
          "model (Hsu et al., 2021). The noise-robust\nlarge": "measure the amount of perturbation using SNR, cal-"
        },
        {
          "and (ii) mismatched testing, in which n-1 types of": "",
          "model (Hsu et al., 2021). The noise-robust\nlarge": "culated using the logarithmic value of the ratio be-"
        },
        {
          "and (ii) mismatched testing, in which n-1 types of": "7.4.2\nUsing a Noise-Robust Model",
          "model (Hsu et al., 2021). The noise-robust\nlarge": ""
        },
        {
          "and (ii) mismatched testing, in which n-1 types of": "",
          "model (Hsu et al., 2021). The noise-robust\nlarge": "tween the original signal and the noise-augmented"
        },
        {
          "and (ii) mismatched testing, in which n-1 types of": "In the end-to-end model, we need to use a different",
          "model (Hsu et al., 2021). The noise-robust\nlarge": "signal’s power. We note that the lower the decrease"
        },
        {
          "and (ii) mismatched testing, in which n-1 types of": "denoising approach because the approach described",
          "model (Hsu et al., 2021). The noise-robust\nlarge": "in SNR, the more minimally perturbed a sample is."
        },
        {
          "and (ii) mismatched testing, in which n-1 types of": "in the previous section does not return a wav file,",
          "model (Hsu et al., 2021). The noise-robust\nlarge": "The maximal decrease in SNR that we use in the"
        },
        {
          "and (ii) mismatched testing, in which n-1 types of": "but instead is applied to the feature-space directly.",
          "model (Hsu et al., 2021). The noise-robust\nlarge": "algorithm is a difference of 10 dB. This condition"
        },
        {
          "and (ii) mismatched testing, in which n-1 types of": "Here, we enforce robustness to noise using a model",
          "model (Hsu et al., 2021). The noise-robust\nlarge": "ensures that\nthe sample is not audibly judged as"
        },
        {
          "and (ii) mismatched testing, in which n-1 types of": "trained to be noise-robust\nin an end-to-end fash-",
          "model (Hsu et al., 2021). The noise-robust\nlarge": "contaminated by humans (Kidd Jr et al., 2016)."
        },
        {
          "and (ii) mismatched testing, in which n-1 types of": "ion. We use the noise-robust version (Wav2Vec2-",
          "model (Hsu et al., 2021). The noise-robust\nlarge": "The algorithm to choose this minimally per-"
        },
        {
          "and (ii) mismatched testing, in which n-1 types of": "Large-Robust) of the aforementioned wav2vec2.0",
          "model (Hsu et al., 2021). The noise-robust\nlarge": "turbed sample has four major components:"
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "change in Valence on addition of noise, δA:Average": ""
        },
        {
          "change in Valence on addition of noise, δA:Average": "change in Activation on addition of noise."
        },
        {
          "change in Valence on addition of noise, δA:Average": ""
        },
        {
          "change in Valence on addition of noise, δA:Average": ""
        },
        {
          "change in Valence on addition of noise, δA:Average": ""
        },
        {
          "change in Valence on addition of noise, δA:Average": ""
        },
        {
          "change in Valence on addition of noise, δA:Average": ""
        },
        {
          "change in Valence on addition of noise, δA:Average": ""
        },
        {
          "change in Valence on addition of noise, δA:Average": ""
        },
        {
          "change in Valence on addition of noise, δA:Average": "-10dB"
        },
        {
          "change in Valence on addition of noise, δA:Average": "Same"
        },
        {
          "change in Valence on addition of noise, δA:Average": "+10dB"
        },
        {
          "change in Valence on addition of noise, δA:Average": ""
        },
        {
          "change in Valence on addition of noise, δA:Average": ""
        },
        {
          "change in Valence on addition of noise, δA:Average": ""
        },
        {
          "change in Valence on addition of noise, δA:Average": "-10dB"
        },
        {
          "change in Valence on addition of noise, δA:Average": "Same"
        },
        {
          "change in Valence on addition of noise, δA:Average": "+10dB"
        },
        {
          "change in Valence on addition of noise, δA:Average": ""
        },
        {
          "change in Valence on addition of noise, δA:Average": ""
        },
        {
          "change in Valence on addition of noise, δA:Average": ""
        },
        {
          "change in Valence on addition of noise, δA:Average": "-10dB"
        },
        {
          "change in Valence on addition of noise, δA:Average": "Same"
        },
        {
          "change in Valence on addition of noise, δA:Average": "+10dB"
        },
        {
          "change in Valence on addition of noise, δA:Average": ""
        },
        {
          "change in Valence on addition of noise, δA:Average": ""
        },
        {
          "change in Valence on addition of noise, δA:Average": ""
        },
        {
          "change in Valence on addition of noise, δA:Average": ""
        },
        {
          "change in Valence on addition of noise, δA:Average": "In"
        },
        {
          "change in Valence on addition of noise, δA:Average": ""
        },
        {
          "change in Valence on addition of noise, δA:Average": "Out"
        },
        {
          "change in Valence on addition of noise, δA:Average": ""
        },
        {
          "change in Valence on addition of noise, δA:Average": ""
        },
        {
          "change in Valence on addition of noise, δA:Average": ""
        },
        {
          "change in Valence on addition of noise, δA:Average": ""
        },
        {
          "change in Valence on addition of noise, δA:Average": "L"
        },
        {
          "change in Valence on addition of noise, δA:Average": "S"
        },
        {
          "change in Valence on addition of noise, δA:Average": ""
        },
        {
          "change in Valence on addition of noise, δA:Average": ""
        },
        {
          "change in Valence on addition of noise, δA:Average": ""
        },
        {
          "change in Valence on addition of noise, δA:Average": ""
        },
        {
          "change in Valence on addition of noise, δA:Average": "1.25x"
        },
        {
          "change in Valence on addition of noise, δA:Average": "0.75x"
        },
        {
          "change in Valence on addition of noise, δA:Average": ""
        },
        {
          "change in Valence on addition of noise, δA:Average": "1.25x"
        },
        {
          "change in Valence on addition of noise, δA:Average": ""
        },
        {
          "change in Valence on addition of noise, δA:Average": "0.75x"
        },
        {
          "change in Valence on addition of noise, δA:Average": ""
        }
      ],
      "page": 11
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Table 2: The table shows the ratio of samples marked by": ""
        },
        {
          "Table 2: The table shows the ratio of samples marked by": "human evaluators as imperceptible to difference in emo-"
        },
        {
          "Table 2: The table shows the ratio of samples marked by": ""
        },
        {
          "Table 2: The table shows the ratio of samples marked by": "tion perception. V: Valence, A: Activation, δV:Average"
        },
        {
          "Table 2: The table shows the ratio of samples marked by": ""
        },
        {
          "Table 2: The table shows the ratio of samples marked by": "change in Valence on addition of noise, δA:Average"
        },
        {
          "Table 2: The table shows the ratio of samples marked by": ""
        },
        {
          "Table 2: The table shows the ratio of samples marked by": "change in Activation on addition of noise."
        },
        {
          "Table 2: The table shows the ratio of samples marked by": ""
        },
        {
          "Table 2: The table shows the ratio of samples marked by": ""
        },
        {
          "Table 2: The table shows the ratio of samples marked by": ""
        },
        {
          "Table 2: The table shows the ratio of samples marked by": ""
        },
        {
          "Table 2: The table shows the ratio of samples marked by": ""
        },
        {
          "Table 2: The table shows the ratio of samples marked by": ""
        },
        {
          "Table 2: The table shows the ratio of samples marked by": ""
        },
        {
          "Table 2: The table shows the ratio of samples marked by": "-10dB"
        },
        {
          "Table 2: The table shows the ratio of samples marked by": "Same"
        },
        {
          "Table 2: The table shows the ratio of samples marked by": "+10dB"
        },
        {
          "Table 2: The table shows the ratio of samples marked by": ""
        },
        {
          "Table 2: The table shows the ratio of samples marked by": ""
        },
        {
          "Table 2: The table shows the ratio of samples marked by": ""
        },
        {
          "Table 2: The table shows the ratio of samples marked by": "-10dB"
        },
        {
          "Table 2: The table shows the ratio of samples marked by": "Same"
        },
        {
          "Table 2: The table shows the ratio of samples marked by": "+10dB"
        },
        {
          "Table 2: The table shows the ratio of samples marked by": ""
        },
        {
          "Table 2: The table shows the ratio of samples marked by": ""
        },
        {
          "Table 2: The table shows the ratio of samples marked by": ""
        },
        {
          "Table 2: The table shows the ratio of samples marked by": "-10dB"
        },
        {
          "Table 2: The table shows the ratio of samples marked by": "Same"
        },
        {
          "Table 2: The table shows the ratio of samples marked by": "+10dB"
        },
        {
          "Table 2: The table shows the ratio of samples marked by": ""
        },
        {
          "Table 2: The table shows the ratio of samples marked by": ""
        },
        {
          "Table 2: The table shows the ratio of samples marked by": ""
        },
        {
          "Table 2: The table shows the ratio of samples marked by": ""
        },
        {
          "Table 2: The table shows the ratio of samples marked by": "In"
        },
        {
          "Table 2: The table shows the ratio of samples marked by": ""
        },
        {
          "Table 2: The table shows the ratio of samples marked by": "Out"
        },
        {
          "Table 2: The table shows the ratio of samples marked by": ""
        },
        {
          "Table 2: The table shows the ratio of samples marked by": ""
        },
        {
          "Table 2: The table shows the ratio of samples marked by": ""
        },
        {
          "Table 2: The table shows the ratio of samples marked by": ""
        },
        {
          "Table 2: The table shows the ratio of samples marked by": "L"
        },
        {
          "Table 2: The table shows the ratio of samples marked by": "S"
        },
        {
          "Table 2: The table shows the ratio of samples marked by": ""
        },
        {
          "Table 2: The table shows the ratio of samples marked by": ""
        },
        {
          "Table 2: The table shows the ratio of samples marked by": ""
        },
        {
          "Table 2: The table shows the ratio of samples marked by": ""
        },
        {
          "Table 2: The table shows the ratio of samples marked by": "1.25x"
        },
        {
          "Table 2: The table shows the ratio of samples marked by": "0.75x"
        },
        {
          "Table 2: The table shows the ratio of samples marked by": ""
        },
        {
          "Table 2: The table shows the ratio of samples marked by": "1.25x"
        },
        {
          "Table 2: The table shows the ratio of samples marked by": ""
        },
        {
          "Table 2: The table shows the ratio of samples marked by": "0.75x"
        },
        {
          "Table 2: The table shows the ratio of samples marked by": ""
        },
        {
          "Table 2: The table shows the ratio of samples marked by": ""
        },
        {
          "Table 2: The table shows the ratio of samples marked by": ""
        },
        {
          "Table 2: The table shows the ratio of samples marked by": ""
        },
        {
          "Table 2: The table shows the ratio of samples marked by": ""
        },
        {
          "Table 2: The table shows the ratio of samples marked by": "4. Models {Traditional Deep Neural Network (T-"
        },
        {
          "Table 2: The table shows the ratio of samples marked by": ""
        },
        {
          "Table 2: The table shows the ratio of samples marked by": "DNN), End to End Deep Neural Network (E-"
        },
        {
          "Table 2: The table shows the ratio of samples marked by": ""
        },
        {
          "Table 2: The table shows the ratio of samples marked by": "DNN), Noise-Robust T-DNN (T-DNN-NR),"
        },
        {
          "Table 2: The table shows the ratio of samples marked by": ""
        },
        {
          "Table 2: The table shows the ratio of samples marked by": "Noise-Robust E-DNN (E-DNN-NR)}"
        },
        {
          "Table 2: The table shows the ratio of samples marked by": ""
        },
        {
          "Table 2: The table shows the ratio of samples marked by": ""
        },
        {
          "Table 2: The table shows the ratio of samples marked by": ""
        },
        {
          "Table 2: The table shows the ratio of samples marked by": ""
        },
        {
          "Table 2: The table shows the ratio of samples marked by": ""
        },
        {
          "Table 2: The table shows the ratio of samples marked by": ""
        },
        {
          "Table 2: The table shows the ratio of samples marked by": "For all the test samples, we execute five runs of"
        }
      ],
      "page": 11
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Table 3: State of the art model performance in terms of UAR when using the general versions of traditional deep": "learning and end-to-end deep learning models. No noise refers to clean speech, all noises refers to the combined set"
        },
        {
          "Table 3: State of the art model performance in terms of UAR when using the general versions of traditional deep": "of perception-retaining and perception-altering noise. The environmental and signal distortion categories shown"
        },
        {
          "Table 3: State of the art model performance in terms of UAR when using the general versions of traditional deep": "include only the perception-retaining noises. As a reminder, samples in the all noises category have an uncertain"
        },
        {
          "Table 3: State of the art model performance in terms of UAR when using the general versions of traditional deep": "ground truth, the row is marked with two stars (∗∗). V: Valence, A: Activation, Clean: Training on clean dataset,"
        },
        {
          "Table 3: State of the art model performance in terms of UAR when using the general versions of traditional deep": "Clean+Noise | Mismatch: Cleaning on noisy dataset and testing on mismatched noisy partition, Clean+Noise |"
        },
        {
          "Table 3: State of the art model performance in terms of UAR when using the general versions of traditional deep": "Match: Cleaning on noisy dataset and testing on matched noisy partition. Random chance UAR is 0.33."
        }
      ],
      "page": 12
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Clean": ""
        },
        {
          "Clean": "V"
        },
        {
          "Clean": "0.63"
        },
        {
          "Clean": "0.63"
        },
        {
          "Clean": "0.45"
        },
        {
          "Clean": "0.48"
        },
        {
          "Clean": "0.42"
        },
        {
          "Clean": "0.38"
        },
        {
          "Clean": "0.39"
        },
        {
          "Clean": "0.47"
        },
        {
          "Clean": "0.39"
        },
        {
          "Clean": "0.39"
        },
        {
          "Clean": "0.37"
        },
        {
          "Clean": "0.47"
        },
        {
          "Clean": "0.40"
        },
        {
          "Clean": "0.38"
        },
        {
          "Clean": "0.38"
        },
        {
          "Clean": "0.55"
        },
        {
          "Clean": "0.55"
        },
        {
          "Clean": "0.54"
        },
        {
          "Clean": "0.58"
        },
        {
          "Clean": "0.59"
        },
        {
          "Clean": "0.34"
        }
      ],
      "page": 12
    },
    {
      "caption": "Table 4: Noise-Robust (NR) state of the art model performance in terms of UAR when using the noise-robust",
      "data": [
        {
          "Table 4: Noise-Robust (NR) state of the art model performance in terms of UAR when using the noise-robust": "versions of traditional deep learning and end-to-end deep learning models. No noise refers to clean speech, all"
        },
        {
          "Table 4: Noise-Robust (NR) state of the art model performance in terms of UAR when using the noise-robust": "noises refers to the combined set of perception-retaining and perception-altering noise. The environmental and"
        },
        {
          "Table 4: Noise-Robust (NR) state of the art model performance in terms of UAR when using the noise-robust": "signal distortion categories shown include only the perception-retaining noises. As a reminder, samples in the all"
        },
        {
          "Table 4: Noise-Robust (NR) state of the art model performance in terms of UAR when using the noise-robust": "noises category have an uncertain ground truth, the row is marked with two stars (∗∗). V: Valence, A: Activation,"
        },
        {
          "Table 4: Noise-Robust (NR) state of the art model performance in terms of UAR when using the noise-robust": "Clean: Training on clean dataset, Clean+Noise | Mismatch: Cleaning on noisy dataset and testing on mismatched"
        },
        {
          "Table 4: Noise-Robust (NR) state of the art model performance in terms of UAR when using the noise-robust": "noisy partition, Clean+Noise | Match: Cleaning on noisy dataset and testing on matched noisy partition, NR: Noise"
        },
        {
          "Table 4: Noise-Robust (NR) state of the art model performance in terms of UAR when using the noise-robust": "Robust versions of the corresponding models Random chance UAR is 0.33."
        }
      ],
      "page": 13
    },
    {
      "caption": "Table 4: Noise-Robust (NR) state of the art model performance in terms of UAR when using the noise-robust",
      "data": [
        {
          "Clean": ""
        },
        {
          "Clean": "V"
        },
        {
          "Clean": "0.63"
        },
        {
          "Clean": "0.40"
        },
        {
          "Clean": "0.48"
        },
        {
          "Clean": "0.50"
        },
        {
          "Clean": "0.42"
        },
        {
          "Clean": "0.46"
        },
        {
          "Clean": "0.43"
        },
        {
          "Clean": "0.52"
        },
        {
          "Clean": "0.42"
        },
        {
          "Clean": "0.44"
        },
        {
          "Clean": "0.40"
        },
        {
          "Clean": "0.49"
        },
        {
          "Clean": "0.42"
        },
        {
          "Clean": "0.38"
        },
        {
          "Clean": "0.38"
        },
        {
          "Clean": "0.58"
        },
        {
          "Clean": "0.58"
        },
        {
          "Clean": "0.57"
        },
        {
          "Clean": "0.60"
        },
        {
          "Clean": "0.64"
        },
        {
          "Clean": "0.40"
        }
      ],
      "page": 13
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "No.of": "",
          "At-\ntempts": "",
          "Activation": "NR-Trad",
          "Valence": "NR-Trad"
        },
        {
          "No.of": "",
          "At-\ntempts": "5",
          "Activation": "0.15",
          "Valence": "0.13"
        },
        {
          "No.of": "",
          "At-\ntempts": "15",
          "Activation": "0.31",
          "Valence": "0.22"
        },
        {
          "No.of": "",
          "At-\ntempts": "25",
          "Activation": "0.40",
          "Valence": "0.21"
        },
        {
          "No.of": "",
          "At-\ntempts": "inf",
          "Activation": "0.44",
          "Valence": "0.26"
        },
        {
          "No.of": "",
          "At-\ntempts": "5",
          "Activation": "0.07",
          "Valence": "0.02"
        },
        {
          "No.of": "",
          "At-\ntempts": "15",
          "Activation": "0.25",
          "Valence": "0.18"
        },
        {
          "No.of": "",
          "At-\ntempts": "25",
          "Activation": "0.32",
          "Valence": "0.11"
        },
        {
          "No.of": "",
          "At-\ntempts": "inf",
          "Activation": "0.36",
          "Valence": "0.17"
        },
        {
          "No.of": "",
          "At-\ntempts": "5",
          "Activation": "0.22",
          "Valence": "0.22"
        },
        {
          "No.of": "",
          "At-\ntempts": "15",
          "Activation": "0.30",
          "Valence": "0.34"
        },
        {
          "No.of": "",
          "At-\ntempts": "5",
          "Activation": "0.24",
          "Valence": "0.12"
        },
        {
          "No.of": "",
          "At-\ntempts": "15",
          "Activation": "0.32",
          "Valence": "0.18"
        },
        {
          "No.of": "",
          "At-\ntempts": "25",
          "Activation": "0.44",
          "Valence": "0.18"
        },
        {
          "No.of": "",
          "At-\ntempts": "inf",
          "Activation": "0.46",
          "Valence": "0.22"
        },
        {
          "No.of": "",
          "At-\ntempts": "5",
          "Activation": "0.22",
          "Valence": "0.15"
        },
        {
          "No.of": "",
          "At-\ntempts": "15",
          "Activation": "0.32",
          "Valence": "0.16"
        },
        {
          "No.of": "",
          "At-\ntempts": "25",
          "Activation": "0.47",
          "Valence": "0.23"
        },
        {
          "No.of": "",
          "At-\ntempts": "inf",
          "Activation": "0.50",
          "Valence": "0.25"
        },
        {
          "No.of": "",
          "At-\ntempts": "5",
          "Activation": "0.33",
          "Valence": "0.31"
        },
        {
          "No.of": "",
          "At-\ntempts": "15",
          "Activation": "0.41",
          "Valence": "0.35"
        }
      ],
      "page": 14
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "8\nAnalysis": "",
          "ception change in presence of noise. We found that": "the changes in perception were not tied to charac-"
        },
        {
          "8\nAnalysis": "8.1\nResearch Question 1 (Q1): Does the",
          "ception change in presence of noise. We found that": ""
        },
        {
          "8\nAnalysis": "",
          "ception change in presence of noise. We found that": "teristics of the speakers. For example,\nthere was"
        },
        {
          "8\nAnalysis": "presence of noise affect emotion",
          "ception change in presence of noise. We found that": ""
        },
        {
          "8\nAnalysis": "",
          "ception change in presence of noise. We found that": "no correlation between changes in perception and"
        },
        {
          "8\nAnalysis": "perception as evaluated by human raters?",
          "ception change in presence of noise. We found that": ""
        },
        {
          "8\nAnalysis": "",
          "ception change in presence of noise. We found that": "variables such as, the original emotion of the utter-"
        },
        {
          "8\nAnalysis": "Is this effect dependent on the utterance",
          "ception change in presence of noise. We found that": ""
        },
        {
          "8\nAnalysis": "",
          "ception change in presence of noise. We found that": "ance, the gender of the speaker, and the length of"
        },
        {
          "8\nAnalysis": "length, loudness, the type of the added",
          "ception change in presence of noise. We found that": ""
        },
        {
          "8\nAnalysis": "",
          "ception change in presence of noise. We found that": "the utterance."
        },
        {
          "8\nAnalysis": "noise, and the original emotion?",
          "ception change in presence of noise. We found that": ""
        },
        {
          "8\nAnalysis": "",
          "ception change in presence of noise. We found that": "The human perception study provides insight"
        },
        {
          "8\nAnalysis": "We find that the presence of environmental noise,",
          "ception change in presence of noise. We found that": ""
        },
        {
          "8\nAnalysis": "",
          "ception change in presence of noise. We found that": "into how emotion perception changes given noise."
        },
        {
          "8\nAnalysis": "even when loud,\nrarely affects annotator percep-",
          "ception change in presence of noise. We found that": ""
        },
        {
          "8\nAnalysis": "",
          "ception change in presence of noise. We found that": "This also provides information about the potential"
        },
        {
          "8\nAnalysis": "tion, suggesting that annotators are able to psycho-",
          "ception change in presence of noise. We found that": ""
        },
        {
          "8\nAnalysis": "",
          "ception change in presence of noise. We found that": "effects of noise addition on model behavior. In the"
        },
        {
          "8\nAnalysis": "acoustically mask the background noise in various",
          "ception change in presence of noise. We found that": ""
        },
        {
          "8\nAnalysis": "",
          "ception change in presence of noise. We found that": "sections that follow, we will evaluate how machine"
        },
        {
          "8\nAnalysis": "cases, as also shown in prior work (e.g., (Stenback,",
          "ception change in presence of noise. We found that": ""
        },
        {
          "8\nAnalysis": "",
          "ception change in presence of noise. We found that": "perception changes given these sources of noise."
        },
        {
          "8\nAnalysis": "2016)).",
          "ception change in presence of noise. We found that": ""
        },
        {
          "8\nAnalysis": "We find that\nthe addition of signal distortion",
          "ception change in presence of noise. We found that": ""
        },
        {
          "8\nAnalysis": "",
          "ception change in presence of noise. We found that": "8.2\nRQ2: Can we verify previous findings that"
        },
        {
          "8\nAnalysis": "noise\nalters\nhuman\nperception.\nThe\nreported",
          "ception change in presence of noise. We found that": ""
        },
        {
          "8\nAnalysis": "",
          "ception change in presence of noise. We found that": "the presence of noise affects the"
        },
        {
          "8\nAnalysis": "change in valence and activation values is on a scale",
          "ception change in presence of noise. We found that": ""
        },
        {
          "8\nAnalysis": "",
          "ception change in presence of noise. We found that": "performance of emotion recognition"
        },
        {
          "8\nAnalysis": "of -1 to 1 (normalized). The addition of laughter",
          "ception change in presence of noise. We found that": ""
        },
        {
          "8\nAnalysis": "",
          "ception change in presence of noise. We found that": "models? Does this effect vary based on the"
        },
        {
          "8\nAnalysis": "changes the activation perception of 16% of\nthe",
          "ception change in presence of noise. We found that": ""
        },
        {
          "8\nAnalysis": "",
          "ception change in presence of noise. We found that": "type of the added noise?"
        },
        {
          "8\nAnalysis": "utterances, with an average change of +22% (+.26).",
          "ception change in presence of noise. We found that": ""
        },
        {
          "8\nAnalysis": "",
          "ception change in presence of noise. We found that": "We first assess the performance of the model on"
        },
        {
          "8\nAnalysis": "The valence perception is altered in 17% of\nthe",
          "ception change in presence of noise. We found that": ""
        },
        {
          "8\nAnalysis": "",
          "ception change in presence of noise. We found that": "the original IEMOCAP data and find that the tradi-"
        },
        {
          "8\nAnalysis": "utterances, with an average change of +14% (+.11).",
          "ception change in presence of noise. We found that": ""
        },
        {
          "8\nAnalysis": "",
          "ception change in presence of noise. We found that": "tional model obtains a performance of 0.67 UAR"
        },
        {
          "8\nAnalysis": "Similarly for crying, valence is altered in 20% of",
          "ception change in presence of noise. We found that": ""
        },
        {
          "8\nAnalysis": "",
          "ception change in presence of noise. We found that": "on the activation and 0.59 UAR on the valence task."
        },
        {
          "8\nAnalysis": "the cases, with an average change of -21% (-.20).",
          "ception change in presence of noise. We found that": ""
        },
        {
          "8\nAnalysis": "",
          "ception change in presence of noise. We found that": "On the other hand, the end-to-end model obtains a"
        },
        {
          "8\nAnalysis": "Crying changes activation perception in 22% of",
          "ception change in presence of noise. We found that": ""
        },
        {
          "8\nAnalysis": "",
          "ception change in presence of noise. We found that": "performance of 0.73 on activation and 0.64 on the"
        },
        {
          "8\nAnalysis": "the cases, with an average change of -32% (-.43).",
          "ception change in presence of noise. We found that": ""
        },
        {
          "8\nAnalysis": "",
          "ception change in presence of noise. We found that": "valence task. We hypothesize that\nthe wav2vec2"
        },
        {
          "8\nAnalysis": "Raises in pitch also alter the perception of emotion.",
          "ception change in presence of noise. We found that": ""
        },
        {
          "8\nAnalysis": "",
          "ception change in presence of noise. We found that": "model has an added advantage of being trained"
        },
        {
          "8\nAnalysis": "In 22% of utterances, the perception of activation is",
          "ception change in presence of noise. We found that": ""
        },
        {
          "8\nAnalysis": "",
          "ception change in presence of noise. We found that": "to recognize word structures that can incorporate"
        },
        {
          "8\nAnalysis": "changed. This contrasts with the perception of va-",
          "ception change in presence of noise. We found that": ""
        },
        {
          "8\nAnalysis": "",
          "ception change in presence of noise. We found that": "some paralinguistic/langauge information in the"
        },
        {
          "8\nAnalysis": "lence, which was altered only in 7% of utterances.",
          "ception change in presence of noise. We found that": ""
        },
        {
          "8\nAnalysis": "",
          "ception change in presence of noise. We found that": "fine-tuned model."
        },
        {
          "8\nAnalysis": "In this scenario, activation increases by an average",
          "ception change in presence of noise. We found that": ""
        },
        {
          "8\nAnalysis": "",
          "ception change in presence of noise. We found that": "Next, we augment the test samples of each fold"
        },
        {
          "8\nAnalysis": "of 26% (+.19), and valence decreases by 12% (-",
          "ception change in presence of noise. We found that": ""
        },
        {
          "8\nAnalysis": "",
          "ception change in presence of noise. We found that": "with each of the noise types (Section 5) and inves-"
        },
        {
          "8\nAnalysis": ".11). On the other hand, decreases in pitch change",
          "ception change in presence of noise. We found that": ""
        },
        {
          "8\nAnalysis": "",
          "ception change in presence of noise. We found that": "tigate how the performance of the model changes."
        },
        {
          "8\nAnalysis": "the perception of activation in 10% of the cases and",
          "ception change in presence of noise. We found that": ""
        },
        {
          "8\nAnalysis": "",
          "ception change in presence of noise. We found that": "We include two cases: (i) only perception-retaining"
        },
        {
          "8\nAnalysis": "of valence in 29% of the cases. In this scenario, ac-",
          "ception change in presence of noise. We found that": ""
        },
        {
          "8\nAnalysis": "",
          "ception change in presence of noise. We found that": "noises and,(ii) all noises."
        },
        {
          "8\nAnalysis": "tivation decreases by an average of 16% (-.15), and",
          "ception change in presence of noise. We found that": ""
        },
        {
          "8\nAnalysis": "valence decreases by 7% (-.07). This ties into pre-",
          "ception change in presence of noise. We found that": "In the first scenario, we do not\ninclude noise"
        },
        {
          "8\nAnalysis": "vious work (Busso et al., 2009), which looked into",
          "ception change in presence of noise. We found that": "types that were found to affect human perception"
        },
        {
          "8\nAnalysis": "how changes and fluctuations in pitch levels influ-",
          "ception change in presence of noise. We found that": "(e.g., Pitch, SpeedUtt, Laugh) because once these"
        },
        {
          "8\nAnalysis": "enced the perception of emotions. Changes in the",
          "ception change in presence of noise. We found that": "noises are added,\nthe ground truth is no longer"
        },
        {
          "8\nAnalysis": "speed of an utterance affect human perception of",
          "ception change in presence of noise. We found that": "reliable.\nThis lack of\nreliable ground-truth data"
        },
        {
          "8\nAnalysis": "valence in 13% (average of -.13) of the cases when",
          "ception change in presence of noise. We found that": "hinders the evaluation of the model’s performance"
        },
        {
          "8\nAnalysis": "speed is increased, and 28% (average of -.23) when",
          "ception change in presence of noise. We found that": "on these samples because the majority of the utter-"
        },
        {
          "8\nAnalysis": "speed is decreased. On the other hand, changes in",
          "ception change in presence of noise. We found that": "ances were not part of the original crowdsourcing"
        },
        {
          "8\nAnalysis": "the speed of an utterance do not affect activation as",
          "ception change in presence of noise. We found that": "experiment and are thus unlabeled. The remainder"
        },
        {
          "8\nAnalysis": "often, specifically, 3% in case of increase and 6%",
          "ception change in presence of noise. We found that": "of this section focuses on the second scenario only."
        },
        {
          "8\nAnalysis": "in case of decrease.",
          "ception change in presence of noise. We found that": "We find that for matched train and test noise con-"
        },
        {
          "8\nAnalysis": "We ensured that our crowsdourcing samples had",
          "ception change in presence of noise. We found that": "ditions,\nthe traditional machine learning model’s"
        },
        {
          "8\nAnalysis": "an even distribution over gender of the speaker and",
          "ception change in presence of noise. We found that": "performance decreases by an average of 28% for"
        },
        {
          "8\nAnalysis": "the length of\nthe sample (see Section 5.3). We",
          "ception change in presence of noise. We found that": "environmental noise while it drops by 32% for sig-"
        },
        {
          "8\nAnalysis": "performed paired t-test to evaluate whether these",
          "ception change in presence of noise. We found that": "nal manipulation. On the other hand, for end-to-"
        },
        {
          "8\nAnalysis": "variables influenced the outcome of emotion per-",
          "ception change in presence of noise. We found that": "end deep learning model, the model’s performance"
        }
      ],
      "page": 15
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "decreases by an average of 22% and 26% for envi-": "ronmental and manipulated noises, respectively. In",
          "on mismatched noisy test data over a baseline sys-": "tem trained only on the clean IEMOCAP data. For"
        },
        {
          "decreases by an average of 22% and 26% for envi-": "mismatched noise conditions, the models’ perfor-",
          "on mismatched noisy test data over a baseline sys-": "example, the end-to-end model tested on environ-"
        },
        {
          "decreases by an average of 22% and 26% for envi-": "mance decreases by an average of 33% for environ-",
          "on mismatched noisy test data over a baseline sys-": "mental noise-augmented dataset (as compared to"
        },
        {
          "decreases by an average of 22% and 26% for envi-": "mental noise, fading, and reverberation. There is",
          "on mismatched noisy test data over a baseline sys-": "traditional deep learning model), reduces the per-"
        },
        {
          "decreases by an average of 22% and 26% for envi-": "also a smaller drop in performance for speeding up",
          "on mismatched noisy test data over a baseline sys-": "formance drop to nearly zero. This improvement"
        },
        {
          "decreases by an average of 22% and 26% for envi-": "parts of the utterance and dropping words, showing",
          "on mismatched noisy test data over a baseline sys-": "is particularly pronounced (an increase of 22% as"
        },
        {
          "decreases by an average of 22% and 26% for envi-": "the brittleness of these models. Table 3 reports the",
          "on mismatched noisy test data over a baseline sys-": "compared to when trained on the clean partition)"
        },
        {
          "decreases by an average of 22% and 26% for envi-": "percentage change in performance when testing on",
          "on mismatched noisy test data over a baseline sys-": "when the environmental noise is introduced at the"
        },
        {
          "decreases by an average of 22% and 26% for envi-": "noisy test data, compared to clean test data.",
          "on mismatched noisy test data over a baseline sys-": "start of the utterance (e.g., when the test set is intro-"
        },
        {
          "decreases by an average of 22% and 26% for envi-": "We see that the end-to-end deep learning model",
          "on mismatched noisy test data over a baseline sys-": "duced with nature-based noises at the start of the"
        },
        {
          "decreases by an average of 22% and 26% for envi-": "is less affected by environmental noise, but has a",
          "on mismatched noisy test data over a baseline sys-": "utterance and the train set is introduced with human"
        },
        {
          "decreases by an average of 22% and 26% for envi-": "larger drop due to fading and reverberation. We",
          "on mismatched noisy test data over a baseline sys-": "and interior noises at the start of the utterance). We"
        },
        {
          "decreases by an average of 22% and 26% for envi-": "observe a larger drop on performance when drop-",
          "on mismatched noisy test data over a baseline sys-": "speculate that the network learns to assign differ-"
        },
        {
          "decreases by an average of 22% and 26% for envi-": "ping words, which possibly can be attributed to",
          "on mismatched noisy test data over a baseline sys-": "ent weights to the start and end of the utterance to"
        },
        {
          "decreases by an average of 22% and 26% for envi-": "the change in audio-structure and non-controllable",
          "on mismatched noisy test data over a baseline sys-": "account for the initial noise."
        },
        {
          "decreases by an average of 22% and 26% for envi-": "feature extraction for this model.",
          "on mismatched noisy test data over a baseline sys-": ""
        },
        {
          "decreases by an average of 22% and 26% for envi-": "",
          "on mismatched noisy test data over a baseline sys-": "However, we find that in both matched and mis-"
        },
        {
          "decreases by an average of 22% and 26% for envi-": "In the second scenario, we observe a large drop",
          "on mismatched noisy test data over a baseline sys-": ""
        },
        {
          "decreases by an average of 22% and 26% for envi-": "",
          "on mismatched noisy test data over a baseline sys-": "matched conditions, it is hard to handle utterances"
        },
        {
          "decreases by an average of 22% and 26% for envi-": "in performance for both the traditional and the end",
          "on mismatched noisy test data over a baseline sys-": ""
        },
        {
          "decreases by an average of 22% and 26% for envi-": "",
          "on mismatched noisy test data over a baseline sys-": "contaminated with reverberation, a common use"
        },
        {
          "decreases by an average of 22% and 26% for envi-": "to end machine learning model. For example,\nin",
          "on mismatched noisy test data over a baseline sys-": ""
        },
        {
          "decreases by an average of 22% and 26% for envi-": "",
          "on mismatched noisy test data over a baseline sys-": "case, even when the training set is augmented with"
        },
        {
          "decreases by an average of 22% and 26% for envi-": "the case of a traditional deep learning model, the",
          "on mismatched noisy test data over a baseline sys-": ""
        },
        {
          "decreases by an average of 22% and 26% for envi-": "",
          "on mismatched noisy test data over a baseline sys-": "other\ntypes of noise. We find that\nthis improve-"
        },
        {
          "decreases by an average of 22% and 26% for envi-": "valence prediction performance drops to a near-",
          "on mismatched noisy test data over a baseline sys-": ""
        },
        {
          "decreases by an average of 22% and 26% for envi-": "",
          "on mismatched noisy test data over a baseline sys-": "ment in performance is even more reduced when"
        },
        {
          "decreases by an average of 22% and 26% for envi-": "chance performance when including all kinds of",
          "on mismatched noisy test data over a baseline sys-": ""
        },
        {
          "decreases by an average of 22% and 26% for envi-": "",
          "on mismatched noisy test data over a baseline sys-": "using the wav2vec model, alluding to the model’s"
        },
        {
          "decreases by an average of 22% and 26% for envi-": "noises (see Table 3).",
          "on mismatched noisy test data over a baseline sys-": ""
        },
        {
          "decreases by an average of 22% and 26% for envi-": "",
          "on mismatched noisy test data over a baseline sys-": "fragility towards data integrity/structural changes."
        },
        {
          "decreases by an average of 22% and 26% for envi-": "We specifically want to point out how the inclu-",
          "on mismatched noisy test data over a baseline sys-": "This can be because reverberation adds a continu-"
        },
        {
          "decreases by an average of 22% and 26% for envi-": "sion of all noises in the test conditions changes",
          "on mismatched noisy test data over a baseline sys-": "ous human speech signal in the background delayed"
        },
        {
          "decreases by an average of 22% and 26% for envi-": "the observed model performance. Primarily,\nthe",
          "on mismatched noisy test data over a baseline sys-": "by a small period of time. None of the other kind"
        },
        {
          "decreases by an average of 22% and 26% for envi-": "models on an average seem to do 20% worse than",
          "on mismatched noisy test data over a baseline sys-": "of noise types have speech in them, and hence aug-"
        },
        {
          "decreases by an average of 22% and 26% for envi-": "they would if we only consider noises that do not",
          "on mismatched noisy test data over a baseline sys-": "mentation doesn’t aid the model to learn robustness"
        },
        {
          "decreases by an average of 22% and 26% for envi-": "alter human perception. We note the discrepancy",
          "on mismatched noisy test data over a baseline sys-": "to this kind of noise."
        },
        {
          "decreases by an average of 22% and 26% for envi-": "between the results of the two noise addition sce-",
          "on mismatched noisy test data over a baseline sys-": ""
        },
        {
          "decreases by an average of 22% and 26% for envi-": "",
          "on mismatched noisy test data over a baseline sys-": "Finally, we investigate the differences in model"
        },
        {
          "decreases by an average of 22% and 26% for envi-": "narios and that\nresults should be described with",
          "on mismatched noisy test data over a baseline sys-": ""
        },
        {
          "decreases by an average of 22% and 26% for envi-": "",
          "on mismatched noisy test data over a baseline sys-": "performance when we use all\ntypes of noise vs."
        },
        {
          "decreases by an average of 22% and 26% for envi-": "respect to the perceptual effects of noise, if noise",
          "on mismatched noisy test data over a baseline sys-": ""
        },
        {
          "decreases by an average of 22% and 26% for envi-": "",
          "on mismatched noisy test data over a baseline sys-": "those that are perception-retaining.\nSpecifically,"
        },
        {
          "decreases by an average of 22% and 26% for envi-": "augmentation is used.",
          "on mismatched noisy test data over a baseline sys-": ""
        },
        {
          "decreases by an average of 22% and 26% for envi-": "",
          "on mismatched noisy test data over a baseline sys-": "we focus on the perception-altering noises because"
        },
        {
          "decreases by an average of 22% and 26% for envi-": "",
          "on mismatched noisy test data over a baseline sys-": "samples augmented with noises in this category no"
        },
        {
          "decreases by an average of 22% and 26% for envi-": "8.3\nRQ3a: Does dataset augmentation help",
          "on mismatched noisy test data over a baseline sys-": ""
        },
        {
          "decreases by an average of 22% and 26% for envi-": "",
          "on mismatched noisy test data over a baseline sys-": "longer have a known ground truth. We inquire as"
        },
        {
          "decreases by an average of 22% and 26% for envi-": "improve the robustness of emotion",
          "on mismatched noisy test data over a baseline sys-": ""
        },
        {
          "decreases by an average of 22% and 26% for envi-": "",
          "on mismatched noisy test data over a baseline sys-": "to whether the use of samples that alter perception"
        },
        {
          "decreases by an average of 22% and 26% for envi-": "recognition models to unseen noise?",
          "on mismatched noisy test data over a baseline sys-": ""
        },
        {
          "decreases by an average of 22% and 26% for envi-": "",
          "on mismatched noisy test data over a baseline sys-": "may lead to the appearance of model performance"
        },
        {
          "decreases by an average of 22% and 26% for envi-": "We first report results for only perception-retaining",
          "on mismatched noisy test data over a baseline sys-": "improvement (note: appearance because the sam-"
        },
        {
          "decreases by an average of 22% and 26% for envi-": "noises. When the training datasets are augmented",
          "on mismatched noisy test data over a baseline sys-": "ples now have uncertain ground truth). To main-"
        },
        {
          "decreases by an average of 22% and 26% for envi-": "with noise, we observe an average performance",
          "on mismatched noisy test data over a baseline sys-": "tain equivalence, we ensure that\nthe training and"
        },
        {
          "decreases by an average of 22% and 26% for envi-": "drop of 26% and 10% for matched noise conditions",
          "on mismatched noisy test data over a baseline sys-": "validation dataset sizes are equal even when they"
        },
        {
          "decreases by an average of 22% and 26% for envi-": "when using the traditional and the end-to-end deep",
          "on mismatched noisy test data over a baseline sys-": "are augmented with more noise conditions. We"
        },
        {
          "decreases by an average of 22% and 26% for envi-": "learning model, respectively. For the mismatched",
          "on mismatched noisy test data over a baseline sys-": "observe that many cases of performance improve-"
        },
        {
          "decreases by an average of 22% and 26% for envi-": "noise conditions, we observe an average perfor-",
          "on mismatched noisy test data over a baseline sys-": "ment occur when the noises include those that are"
        },
        {
          "decreases by an average of 22% and 26% for envi-": "mance drop of 31% and 16% for the traditional and",
          "on mismatched noisy test data over a baseline sys-": "perception-altering (see “Al noises” in Table 3). We"
        },
        {
          "decreases by an average of 22% and 26% for envi-": "end-to-end deep learning models, respectively.",
          "on mismatched noisy test data over a baseline sys-": "observe a difference of 12% to 25% between the"
        },
        {
          "decreases by an average of 22% and 26% for envi-": "Both models see improved performance when",
          "on mismatched noisy test data over a baseline sys-": "numbers that we obtain for the perception-retaining"
        },
        {
          "decreases by an average of 22% and 26% for envi-": "the training dataset is augmented with continuous",
          "on mismatched noisy test data over a baseline sys-": "noises vs. when not distinguishing between the two"
        },
        {
          "decreases by an average of 22% and 26% for envi-": "background noise in the matched noise setting. We",
          "on mismatched noisy test data over a baseline sys-": "noise categories. This supports our claim that the"
        },
        {
          "decreases by an average of 22% and 26% for envi-": "find that data augmentation improves performance",
          "on mismatched noisy test data over a baseline sys-": "choice between types of noises used for data aug-"
        }
      ],
      "page": 16
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "mentation during model training and performance": "evaluation affects the empirical observations and",
          "segments were sped up, the performance is signifi-": "cantly lower (−28%) than when tested on a clean"
        },
        {
          "mentation during model training and performance": "should be carefully considered. We hypothesize",
          "segments were sped up, the performance is signifi-": "test set. However, we did see an improvement in"
        },
        {
          "mentation during model training and performance": "that this improvement in performance may be due",
          "segments were sped up, the performance is signifi-": "the performance for unseen reverberation contami-"
        },
        {
          "mentation during model training and performance": "to the inherent nature of noises that change emo-",
          "segments were sped up, the performance is signifi-": "nated samples as compared to data augmentation"
        },
        {
          "mentation during model training and performance": "tion perception, if they are perceptible enough to",
          "segments were sped up, the performance is signifi-": "(an average of +36%). Finally, we observe a gen-"
        },
        {
          "mentation during model training and performance": "change emotion perception,\nthen they may stand",
          "segments were sped up, the performance is signifi-": "eral trend of increase in emotion recognition per-"
        },
        {
          "mentation during model training and performance": "out enough that\nthe model can adequately learn",
          "segments were sped up, the performance is signifi-": "formance for the combined dataset (noisy and non-"
        },
        {
          "mentation during model training and performance": "to separate them out and improve its prediction",
          "segments were sped up, the performance is signifi-": "noisy samples), as compared to when the model is"
        },
        {
          "mentation during model training and performance": "towards the original ground truth annotation. How-",
          "segments were sped up, the performance is signifi-": "trained on the clean training set, which supports"
        },
        {
          "mentation during model training and performance": "ever, if the noise alteration truly does change per-",
          "segments were sped up, the performance is signifi-": "the findings from previous dataset augmentation"
        },
        {
          "mentation during model training and performance": "ception,\nthen the model\nis learning to ignore this",
          "segments were sped up, the performance is signifi-": "research (Aldeneh and Provost, 2017)."
        },
        {
          "mentation during model training and performance": "natural human perceptual phenomenon. This may",
          "segments were sped up, the performance is signifi-": ""
        },
        {
          "mentation during model training and performance": "",
          "segments were sped up, the performance is signifi-": "For the end-to-end deep learning model, we use"
        },
        {
          "mentation during model training and performance": "have negative consequences during model deploy-",
          "segments were sped up, the performance is signifi-": ""
        },
        {
          "mentation during model training and performance": "",
          "segments were sped up, the performance is signifi-": "the noise-robust version. We find that the model is"
        },
        {
          "mentation during model training and performance": "ment.",
          "segments were sped up, the performance is signifi-": ""
        },
        {
          "mentation during model training and performance": "",
          "segments were sped up, the performance is signifi-": "effective at countering environmental noise when"
        },
        {
          "mentation during model training and performance": "",
          "segments were sped up, the performance is signifi-": "trained on a dataset augmented with environmental"
        },
        {
          "mentation during model training and performance": "8.4\nRQ3b: Does sample denoising help",
          "segments were sped up, the performance is signifi-": ""
        },
        {
          "mentation during model training and performance": "",
          "segments were sped up, the performance is signifi-": "noise, even in the mismatched condition. The per-"
        },
        {
          "mentation during model training and performance": "improve the robustness of emotion",
          "segments were sped up, the performance is signifi-": ""
        },
        {
          "mentation during model training and performance": "",
          "segments were sped up, the performance is signifi-": "formance is equivalent to the model evaluated on"
        },
        {
          "mentation during model training and performance": "recognition models to unseen noise?",
          "segments were sped up, the performance is signifi-": ""
        },
        {
          "mentation during model training and performance": "",
          "segments were sped up, the performance is signifi-": "the clean data. We further delve into the amount of"
        },
        {
          "mentation during model training and performance": "",
          "segments were sped up, the performance is signifi-": "noise augmentation needed to achieve this equiva-"
        },
        {
          "mentation during model training and performance": "In the matched training testing condition, we find",
          "segments were sped up, the performance is signifi-": ""
        },
        {
          "mentation during model training and performance": "",
          "segments were sped up, the performance is signifi-": "lency. We consider all of the original training data."
        },
        {
          "mentation during model training and performance": "that the traditional deep learning model has an av-",
          "segments were sped up, the performance is signifi-": ""
        },
        {
          "mentation during model training and performance": "",
          "segments were sped up, the performance is signifi-": "We augment a percentage of the training data, start-"
        },
        {
          "mentation during model training and performance": "erage performance of 0.57 across all the datasets",
          "segments were sped up, the performance is signifi-": ""
        },
        {
          "mentation during model training and performance": "",
          "segments were sped up, the performance is signifi-": "ing by augmenting a random sample of 10% with"
        },
        {
          "mentation during model training and performance": "and testing setups, while the end-to-end models do",
          "segments were sped up, the performance is signifi-": ""
        },
        {
          "mentation during model training and performance": "",
          "segments were sped up, the performance is signifi-": "perception-retaining noise and increasing by 10%"
        },
        {
          "mentation during model training and performance": "substantially better at 0.61 UAR. See Table 4 for",
          "segments were sped up, the performance is signifi-": ""
        },
        {
          "mentation during model training and performance": "",
          "segments were sped up, the performance is signifi-": "each time. We find that we obtain equivalency after"
        },
        {
          "mentation during model training and performance": "details.",
          "segments were sped up, the performance is signifi-": ""
        },
        {
          "mentation during model training and performance": "",
          "segments were sped up, the performance is signifi-": "augmenting with only 30% of the training data. We"
        },
        {
          "mentation during model training and performance": "In the mismatched training testing condition, we",
          "segments were sped up, the performance is signifi-": ""
        },
        {
          "mentation during model training and performance": "",
          "segments were sped up, the performance is signifi-": "compare this compares to the traditional model, in"
        },
        {
          "mentation during model training and performance": "find that for the traditional deep learning model,",
          "segments were sped up, the performance is signifi-": ""
        },
        {
          "mentation during model training and performance": "",
          "segments were sped up, the performance is signifi-": "which all of the training data are noise-augmented"
        },
        {
          "mentation during model training and performance": "adding a denoising component to the models leads",
          "segments were sped up, the performance is signifi-": ""
        },
        {
          "mentation during model training and performance": "",
          "segments were sped up, the performance is signifi-": "and we still do not see equivalency."
        },
        {
          "mentation during model training and performance": "to a significant\nimprovement when the original",
          "segments were sped up, the performance is signifi-": ""
        },
        {
          "mentation during model training and performance": "",
          "segments were sped up, the performance is signifi-": "We\nseparately consider\nthe\nsignal distortion"
        },
        {
          "mentation during model training and performance": "SNR is high (e.g., after continuous noise intro-",
          "segments were sped up, the performance is signifi-": ""
        },
        {
          "mentation during model training and performance": "",
          "segments were sped up, the performance is signifi-": "noise samples. These were not part of the training"
        },
        {
          "mentation during model training and performance": "duction the SNR decreases only by 10dB). In this",
          "segments were sped up, the performance is signifi-": ""
        },
        {
          "mentation during model training and performance": "",
          "segments were sped up, the performance is signifi-": "of\nthe wav2vec2-Large-robust model. However,"
        },
        {
          "mentation during model training and performance": "case, we see an average improvement of 23% ± 3%",
          "segments were sped up, the performance is signifi-": ""
        },
        {
          "mentation during model training and performance": "",
          "segments were sped up, the performance is signifi-": "this model only sees a 6% loss in performance,"
        },
        {
          "mentation during model training and performance": "across all environmental noise categories,\ncom-",
          "segments were sped up, the performance is signifi-": ""
        },
        {
          "mentation during model training and performance": "",
          "segments were sped up, the performance is signifi-": "where the traditional robust model saw a 20% loss"
        },
        {
          "mentation during model training and performance": "pared to when there is no denoising or augmen-",
          "segments were sped up, the performance is signifi-": ""
        },
        {
          "mentation during model training and performance": "",
          "segments were sped up, the performance is signifi-": "in performance."
        },
        {
          "mentation during model training and performance": "tation performed.",
          "segments were sped up, the performance is signifi-": ""
        },
        {
          "mentation during model training and performance": "However, when the SNR decreases by 20dB,",
          "segments were sped up, the performance is signifi-": "However, as discussed in the original traditional"
        },
        {
          "mentation during model training and performance": "we observe a decline in performance when using",
          "segments were sped up, the performance is signifi-": "model, the end-to-end noise-robust model also fails"
        },
        {
          "mentation during model training and performance": "the noise suppression algorithms. We believe that",
          "segments were sped up, the performance is signifi-": "on reverberation-based contamination even when"
        },
        {
          "mentation during model training and performance": "this decline in performance is reflective of the mis-",
          "segments were sped up, the performance is signifi-": "trained on a similarly augmented dataset (note that"
        },
        {
          "mentation during model training and performance": "match in goals:\nthe goal of noise suppression is",
          "segments were sped up, the performance is signifi-": "the denoised traditional model could effectively"
        },
        {
          "mentation during model training and performance": "to maintain, or improve, the comprehensibility of",
          "segments were sped up, the performance is signifi-": "handle reverberation). We believe that this may be"
        },
        {
          "mentation during model training and performance": "the speech itself, not necessarily highlight emo-",
          "segments were sped up, the performance is signifi-": "because the wav2vec model is trained on continu-"
        },
        {
          "mentation during model training and performance": "tion. As a result, it may end up masking emotional",
          "segments were sped up, the performance is signifi-": "ous speech and relies on the underlying linguistic"
        },
        {
          "mentation during model training and performance": "information, as discussed in (Ma and Thompson,",
          "segments were sped up, the performance is signifi-": "structure of speech.\nHowever,\nin reverberation,"
        },
        {
          "mentation during model training and performance": "2015).",
          "segments were sped up, the performance is signifi-": "there is an implicit doubling of the underlying in-"
        },
        {
          "mentation during model training and performance": "",
          "segments were sped up, the performance is signifi-": "formation, which is at odds with how this model"
        },
        {
          "mentation during model training and performance": "We further show that the addition of a denoising",
          "segments were sped up, the performance is signifi-": ""
        },
        {
          "mentation during model training and performance": "",
          "segments were sped up, the performance is signifi-": "was trained. This may explain why it is not able to"
        },
        {
          "mentation during model training and performance": "component does not significantly improve perfor-",
          "segments were sped up, the performance is signifi-": ""
        },
        {
          "mentation during model training and performance": "",
          "segments were sped up, the performance is signifi-": "compensate for this type of signal manipulation."
        },
        {
          "mentation during model training and performance": "mance in the presence of signal distortion noise",
          "segments were sped up, the performance is signifi-": ""
        },
        {
          "mentation during model training and performance": "(other than reverberation) as compared to the pres-",
          "segments were sped up, the performance is signifi-": "Next, we analyze whether the perception cate-"
        },
        {
          "mentation during model training and performance": "ence of environmental noise (noise addition). For",
          "segments were sped up, the performance is signifi-": "gory of noise used for data augmentation of\nthe"
        },
        {
          "mentation during model training and performance": "example, when samples were faded in or out or",
          "segments were sped up, the performance is signifi-": "samples,\nin both,\nthe train and test dataset\ninflu-"
        }
      ],
      "page": 17
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "ences the reported results for noise-robust model": "improvement. We find that\nthere is a significant",
          "egories). This increase in the success rate when": "perception-altering noises are included implies that"
        },
        {
          "ences the reported results for noise-robust model": "difference in performance when the testing dataset",
          "egories). This increase in the success rate when": "the model does not remain robust when the effects"
        },
        {
          "ences the reported results for noise-robust model": "is augmented with any kind of noise vs. when",
          "egories). This increase in the success rate when": "of noise on human perception are not considered."
        },
        {
          "ences the reported results for noise-robust model": "augmented with perception-retaining noise. Specif-",
          "egories). This increase in the success rate when": "We next consider\nthe type of noise (environ-"
        },
        {
          "ences the reported results for noise-robust model": "ically, we observe that the maximal gains in perfor-",
          "egories). This increase in the success rate when": "mental vs.\nsignal manipulation). We find that"
        },
        {
          "ences the reported results for noise-robust model": "mance when testing on matched noisy conditions",
          "egories). This increase in the success rate when": "the success\nrate of flipping a model’s output\nis"
        },
        {
          "ences the reported results for noise-robust model": "are for samples for which we do not know whether",
          "egories). This increase in the success rate when": "18% for noises belonging to the environmental cat-"
        },
        {
          "ences the reported results for noise-robust model": "or not the ground truth holds (i.e., both noise cate-",
          "egories). This increase in the success rate when": "egory, which is generally a category of perception-"
        },
        {
          "ences the reported results for noise-robust model": "gories). For example, when using the noise robust",
          "egories). This increase in the success rate when": "retaining noise.\nThe success\nrate of flipping a"
        },
        {
          "ences the reported results for noise-robust model": "traditional deep learning model, where the test and",
          "egories). This increase in the success rate when": "model’s output\nis 37% for all noises belonging"
        },
        {
          "ences the reported results for noise-robust model": "train dataset is augmented with any type of noises,",
          "egories). This increase in the success rate when": "to the signal manipulation category. When we"
        },
        {
          "ences the reported results for noise-robust model": "we observe a performance improvement, as com-",
          "egories). This increase in the success rate when": "constrain our possible noise choices to perception-"
        },
        {
          "ences the reported results for noise-robust model": "pared to that using a clean train dataset, of 12%.",
          "egories). This increase in the success rate when": "retaining signal manipulations, we see that the suc-"
        },
        {
          "ences the reported results for noise-robust model": "Similarly for noise robust end to end models, the",
          "egories). This increase in the success rate when": "cess rate of the intentional misclassification algo-"
        },
        {
          "ences the reported results for noise-robust model": "performance improvement difference when using",
          "egories). This increase in the success rate when": "rithm drops to 24%. On the other hand, we observe"
        },
        {
          "ences the reported results for noise-robust model": "all noises vs. only perception retaining ones is 15%",
          "egories). This increase in the success rate when": "that when we also consider the signal manipula-"
        },
        {
          "ences the reported results for noise-robust model": "for activation and 13% for valence. Again, this is",
          "egories). This increase in the success rate when": "tions that are perception-altering, the success rate"
        },
        {
          "ences the reported results for noise-robust model": "a problem when we think about deploying models",
          "egories). This increase in the success rate when": "of flipping a model output is 39%. See Table 5 for"
        },
        {
          "ences the reported results for noise-robust model": "in the real world because although the perception",
          "egories). This increase in the success rate when": "more details."
        },
        {
          "ences the reported results for noise-robust model": "of these emotion expressions may change, we are",
          "egories). This increase in the success rate when": ""
        },
        {
          "ences the reported results for noise-robust model": "",
          "egories). This increase in the success rate when": "We previously discussed the fragility of end-to-"
        },
        {
          "ences the reported results for noise-robust model": "assuming that the system should think of these per-",
          "egories). This increase in the success rate when": ""
        },
        {
          "ences the reported results for noise-robust model": "",
          "egories). This increase in the success rate when": "end models towards reverberation-based noise con-"
        },
        {
          "ences the reported results for noise-robust model": "ception labels as rigid and unchanging.",
          "egories). This increase in the success rate when": ""
        },
        {
          "ences the reported results for noise-robust model": "",
          "egories). This increase in the success rate when": "tamination, noise that\nis perception-retaining for"
        },
        {
          "ences the reported results for noise-robust model": "",
          "egories). This increase in the success rate when": "human evaluators. Here, we specifically run an ex-"
        },
        {
          "ences the reported results for noise-robust model": "8.5\nRQ4: How does the robustness of a model",
          "egories). This increase in the success rate when": "periment to use only that particular noise category"
        },
        {
          "ences the reported results for noise-robust model": "to attacks compare when we are using test",
          "egories). This increase in the success rate when": "for the model fragility testing. If the attacker knows"
        },
        {
          "ences the reported results for noise-robust model": "samples that with are augmented with",
          "egories). This increase in the success rate when": "that the model is susceptible to reverberation-based"
        },
        {
          "ences the reported results for noise-robust model": "perception-retaining noise vs. samples",
          "egories). This increase in the success rate when": "prediction changes,\nthe intentional misclassifica-"
        },
        {
          "ences the reported results for noise-robust model": "that are augmented with all types of noise,",
          "egories). This increase in the success rate when": "tion algorithm can land on an optimal set of room"
        },
        {
          "ences the reported results for noise-robust model": "regardless of their effect on perception?",
          "egories). This increase in the success rate when": "and reverberation parameters in a maximum of five"
        },
        {
          "ences the reported results for noise-robust model": "",
          "egories). This increase in the success rate when": "queries to be able to produce a flipped output for"
        },
        {
          "ences the reported results for noise-robust model": "In this section, we aim to show the effects of noise",
          "egories). This increase in the success rate when": ""
        },
        {
          "ences the reported results for noise-robust model": "",
          "egories). This increase in the success rate when": "that particular sample.\nIt achieves a success rate"
        },
        {
          "ences the reported results for noise-robust model": "augmentation in general and specifically highlight",
          "egories). This increase in the success rate when": ""
        },
        {
          "ences the reported results for noise-robust model": "",
          "egories). This increase in the success rate when": "of 24%, compared to 12% for other perception-"
        },
        {
          "ences the reported results for noise-robust model": "noise categories that do not alter human perception.",
          "egories). This increase in the success rate when": ""
        },
        {
          "ences the reported results for noise-robust model": "",
          "egories). This increase in the success rate when": "retaining noises. The traditional noise-robust deep"
        },
        {
          "ences the reported results for noise-robust model": "We will show that if we are not careful with the se-",
          "egories). This increase in the success rate when": ""
        },
        {
          "ences the reported results for noise-robust model": "",
          "egories). This increase in the success rate when": "learning model is even more challenged, compared"
        },
        {
          "ences the reported results for noise-robust model": "lection of our noise types, moving from noise that",
          "egories). This increase in the success rate when": ""
        },
        {
          "ences the reported results for noise-robust model": "",
          "egories). This increase in the success rate when": "to the end-to-end model. The number of queries"
        },
        {
          "ences the reported results for noise-robust model": "we know not to alter perception to noise that may,",
          "egories). This increase in the success rate when": ""
        },
        {
          "ences the reported results for noise-robust model": "",
          "egories). This increase in the success rate when": "required to flip the output is three, vs. five for the"
        },
        {
          "ences the reported results for noise-robust model": "the resulting noise sources can not only impact",
          "egories). This increase in the success rate when": ""
        },
        {
          "ences the reported results for noise-robust model": "",
          "egories). This increase in the success rate when": "end-to-end model, suggesting that it is less robust."
        },
        {
          "ences the reported results for noise-robust model": "the brittleness of models, but also lead to inaccu-",
          "egories). This increase in the success rate when": ""
        },
        {
          "ences the reported results for noise-robust model": "",
          "egories). This increase in the success rate when": "This empirical evaluation is performed primarily"
        },
        {
          "ences the reported results for noise-robust model": "rate evaluation metrics. We also specifically report",
          "egories). This increase in the success rate when": ""
        },
        {
          "ences the reported results for noise-robust model": "",
          "egories). This increase in the success rate when": "to demonstrate how such noise inclusions can not"
        },
        {
          "ences the reported results for noise-robust model": "robustness performance when using reverberation-",
          "egories). This increase in the success rate when": ""
        },
        {
          "ences the reported results for noise-robust model": "",
          "egories). This increase in the success rate when": "only invalidate the ground truth but also lead to in-"
        },
        {
          "ences the reported results for noise-robust model": "based contamination, as we observed that it is the",
          "egories). This increase in the success rate when": ""
        },
        {
          "ences the reported results for noise-robust model": "",
          "egories). This increase in the success rate when": "accurate and fragile benchmarking and evaluation"
        },
        {
          "ences the reported results for noise-robust model": "most\nlikely noise category to degrade the robust-",
          "egories). This increase in the success rate when": ""
        },
        {
          "ences the reported results for noise-robust model": "",
          "egories). This increase in the success rate when": "of adversarial efficiency and robustness."
        },
        {
          "ences the reported results for noise-robust model": "ness of the model.",
          "egories). This increase in the success rate when": ""
        },
        {
          "ences the reported results for noise-robust model": "We allow the decision boundary attack algorithm",
          "egories). This increase in the success rate when": ""
        },
        {
          "ences the reported results for noise-robust model": "",
          "egories). This increase in the success rate when": "8.6\nRQ5: What are the recommended"
        },
        {
          "ences the reported results for noise-robust model": "a maximum of five queries to create a noise aug-",
          "egories). This increase in the success rate when": ""
        },
        {
          "ences the reported results for noise-robust model": "",
          "egories). This increase in the success rate when": "practices for speech emotion dataset"
        }
      ],
      "page": 18
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "1. Environmental\nnoise\nshould\nbe\nadded\nto": "datasets\nto improve generalizability to var-",
          "performance is still significantly lower when the": "noise in the train and test environments does not"
        },
        {
          "1. Environmental\nnoise\nshould\nbe\nadded\nto": "ied noise conditions, whether using denoising,",
          "performance is still significantly lower when the": "match.\nIn this paper, we demonstrate fragility of"
        },
        {
          "1. Environmental\nnoise\nshould\nbe\nadded\nto": "augmentation, or a combination of both.",
          "performance is still significantly lower when the": "the emotion recognition systems and valid methods"
        },
        {
          "1. Environmental\nnoise\nshould\nbe\nadded\nto": "",
          "performance is still significantly lower when the": "to augment the datasets, which is a critical concern"
        },
        {
          "1. Environmental\nnoise\nshould\nbe\nadded\nto": "2.\nIt is good to augment datasets by fading the",
          "performance is still significantly lower when the": ""
        },
        {
          "1. Environmental\nnoise\nshould\nbe\nadded\nto": "",
          "performance is still significantly lower when the": "in real world deployment."
        },
        {
          "1. Environmental\nnoise\nshould\nbe\nadded\nto": "loudness of the segments, dropping letters or",
          "performance is still significantly lower when the": ""
        },
        {
          "1. Environmental\nnoise\nshould\nbe\nadded\nto": "words, and speeding up small (no more than",
          "performance is still significantly lower when the": ""
        },
        {
          "1. Environmental\nnoise\nshould\nbe\nadded\nto": "25% of the total sample length) segments of",
          "performance is still significantly lower when the": "References"
        },
        {
          "1. Environmental\nnoise\nshould\nbe\nadded\nto": "the complete sound samples in the dataset.",
          "performance is still significantly lower when the": ""
        },
        {
          "1. Environmental\nnoise\nshould\nbe\nadded\nto": "",
          "performance is still significantly lower when the": "Phonological history of english consonant clusters."
        },
        {
          "1. Environmental\nnoise\nshould\nbe\nadded\nto": "But\nit\nis\nimportant\nto note that\nthese aug-",
          "performance is still significantly lower when the": ""
        },
        {
          "1. Environmental\nnoise\nshould\nbe\nadded\nto": "",
          "performance is still significantly lower when the": "Hadi Abdullah, Washington Garcia, Christian Peeters,"
        },
        {
          "1. Environmental\nnoise\nshould\nbe\nadded\nto": "mented samples should not be passed through",
          "performance is still significantly lower when the": ""
        },
        {
          "1. Environmental\nnoise\nshould\nbe\nadded\nto": "",
          "performance is still significantly lower when the": "Patrick Traynor, Kevin RB Butler, and Joseph Wilson."
        },
        {
          "1. Environmental\nnoise\nshould\nbe\nadded\nto": "the denoising component as the denoised ver-",
          "performance is still significantly lower when the": "2019. Practical hidden voice attacks against speech"
        },
        {
          "1. Environmental\nnoise\nshould\nbe\nadded\nto": "sion loses emotion information.",
          "performance is still significantly lower when the": "arXiv preprint\nand speaker\nrecognition systems."
        },
        {
          "1. Environmental\nnoise\nshould\nbe\nadded\nto": "",
          "performance is still significantly lower when the": "arXiv:1904.05734."
        },
        {
          "1. Environmental\nnoise\nshould\nbe\nadded\nto": "3. One should not change the speed of the entire",
          "performance is still significantly lower when the": ""
        },
        {
          "1. Environmental\nnoise\nshould\nbe\nadded\nto": "",
          "performance is still significantly lower when the": "Zakaria Aldeneh, Soheil Khorram, Dimitrios Dimitri-"
        },
        {
          "1. Environmental\nnoise\nshould\nbe\nadded\nto": "utterance more than 5% and should not add in-",
          "performance is still significantly lower when the": ""
        },
        {
          "1. Environmental\nnoise\nshould\nbe\nadded\nto": "",
          "performance is still significantly lower when the": "adis,\nand Emily Mower Provost. 2017.\nPooling"
        },
        {
          "1. Environmental\nnoise\nshould\nbe\nadded\nto": "tentional pauses or any background noises that",
          "performance is still significantly lower when the": "acoustic and lexical\nfeatures for\nthe prediction of"
        },
        {
          "1. Environmental\nnoise\nshould\nbe\nadded\nto": "",
          "performance is still significantly lower when the": "the 19th ACM Interna-\nvalence.\nIn Proceedings of"
        },
        {
          "1. Environmental\nnoise\nshould\nbe\nadded\nto": "elicit emotion behavior, e.g., sobs or laughter.",
          "performance is still significantly lower when the": ""
        },
        {
          "1. Environmental\nnoise\nshould\nbe\nadded\nto": "",
          "performance is still significantly lower when the": "tional Conference on Multimodal Interaction, pages"
        },
        {
          "1. Environmental\nnoise\nshould\nbe\nadded\nto": "",
          "performance is still significantly lower when the": "68–72. Acm."
        },
        {
          "1. Environmental\nnoise\nshould\nbe\nadded\nto": "Regarding deployment, we suggest that:",
          "performance is still significantly lower when the": ""
        },
        {
          "1. Environmental\nnoise\nshould\nbe\nadded\nto": "",
          "performance is still significantly lower when the": "Zakaria Aldeneh and Emily Mower Provost. 2017. Us-"
        },
        {
          "1. Environmental\nnoise\nshould\nbe\nadded\nto": "1. Noisy starts and ends of utterances can be han-",
          "performance is still significantly lower when the": "ing regional saliency for speech emotion recognition."
        },
        {
          "1. Environmental\nnoise\nshould\nbe\nadded\nto": "dled by augmentation, hence,\nif the training",
          "performance is still significantly lower when the": "In 2017 IEEE International Conference on Acoustics,"
        },
        {
          "1. Environmental\nnoise\nshould\nbe\nadded\nto": "",
          "performance is still significantly lower when the": "Speech and Signal Processing (ICASSP). Ieee."
        },
        {
          "1. Environmental\nnoise\nshould\nbe\nadded\nto": "set included these augmentations, there should",
          "performance is still significantly lower when the": ""
        },
        {
          "1. Environmental\nnoise\nshould\nbe\nadded\nto": "be no issue for deployed emotion recognition",
          "performance is still significantly lower when the": "Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed,"
        },
        {
          "1. Environmental\nnoise\nshould\nbe\nadded\nto": "systems.",
          "performance is still significantly lower when the": "and Michael Auli. 2020. wav2vec 2.0: A framework"
        },
        {
          "1. Environmental\nnoise\nshould\nbe\nadded\nto": "",
          "performance is still significantly lower when the": "for self-supervised learning of speech representations."
        },
        {
          "1. Environmental\nnoise\nshould\nbe\nadded\nto": "",
          "performance is still significantly lower when the": "Advances in Neural Information Processing Systems."
        },
        {
          "1. Environmental\nnoise\nshould\nbe\nadded\nto": "2. Reverberation is hard to handle for even aug-",
          "performance is still significantly lower when the": ""
        },
        {
          "1. Environmental\nnoise\nshould\nbe\nadded\nto": "mented emotion recognition models. Hence,",
          "performance is still significantly lower when the": "Margaret M Bradley and Peter J Lang. 1994. Measur-"
        },
        {
          "1. Environmental\nnoise\nshould\nbe\nadded\nto": "the samples must either be cleaned to remove",
          "performance is still significantly lower when the": "ing emotion:\nthe self-assessment manikin and the"
        },
        {
          "1. Environmental\nnoise\nshould\nbe\nadded\nto": "",
          "performance is still significantly lower when the": "Journal of behavior therapy\nsemantic differential."
        },
        {
          "1. Environmental\nnoise\nshould\nbe\nadded\nto": "the reverberation effect, or must be identified",
          "performance is still significantly lower when the": ""
        },
        {
          "1. Environmental\nnoise\nshould\nbe\nadded\nto": "",
          "performance is still significantly lower when the": "and experimental psychiatry, 25(1):49–59."
        },
        {
          "1. Environmental\nnoise\nshould\nbe\nadded\nto": "as low confidence for classification.",
          "performance is still significantly lower when the": ""
        },
        {
          "1. Environmental\nnoise\nshould\nbe\nadded\nto": "",
          "performance is still significantly lower when the": "Carlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe"
        },
        {
          "1. Environmental\nnoise\nshould\nbe\nadded\nto": "3. Deploy complementary models that identify",
          "performance is still significantly lower when the": "Kazemzadeh, Emily Mower, Samuel Kim,\nJean-"
        },
        {
          "1. Environmental\nnoise\nshould\nbe\nadded\nto": "",
          "performance is still significantly lower when the": "nette N Chang,\nSungbok Lee,\nand Shrikanth S"
        },
        {
          "1. Environmental\nnoise\nshould\nbe\nadded\nto": "the presence of noise that would change a",
          "performance is still significantly lower when the": ""
        },
        {
          "1. Environmental\nnoise\nshould\nbe\nadded\nto": "",
          "performance is still significantly lower when the": "Narayanan. 2008.\nIemocap:\nInteractive emotional"
        },
        {
          "1. Environmental\nnoise\nshould\nbe\nadded\nto": "human’s perception.",
          "performance is still significantly lower when the": ""
        },
        {
          "1. Environmental\nnoise\nshould\nbe\nadded\nto": "",
          "performance is still significantly lower when the": "dyadic motion capture database. Language resources"
        },
        {
          "1. Environmental\nnoise\nshould\nbe\nadded\nto": "",
          "performance is still significantly lower when the": "and evaluation, 42(4):335."
        },
        {
          "1. Environmental\nnoise\nshould\nbe\nadded\nto": "9\nConclusion",
          "performance is still significantly lower when the": ""
        },
        {
          "1. Environmental\nnoise\nshould\nbe\nadded\nto": "",
          "performance is still significantly lower when the": "Carlos Busso, Murtaza Bulut, and Sungbok Lee. 2009."
        },
        {
          "1. Environmental\nnoise\nshould\nbe\nadded\nto": "In this work, we study how the presence of real",
          "performance is still significantly lower when the": "Shrikanth narayanan fundamental frequency analysis"
        },
        {
          "1. Environmental\nnoise\nshould\nbe\nadded\nto": "",
          "performance is still significantly lower when the": "for speech emotion processing. The role of prosody"
        },
        {
          "1. Environmental\nnoise\nshould\nbe\nadded\nto": "world noise, environmental or signal distortion, af-",
          "performance is still significantly lower when the": ""
        },
        {
          "1. Environmental\nnoise\nshould\nbe\nadded\nto": "",
          "performance is still significantly lower when the": "in affective speech."
        },
        {
          "1. Environmental\nnoise\nshould\nbe\nadded\nto": "fects human emotion perception. We identify noise",
          "performance is still significantly lower when the": ""
        },
        {
          "1. Environmental\nnoise\nshould\nbe\nadded\nto": "sources that do not affect human perception, such",
          "performance is still significantly lower when the": "Nicholas Carlini and David Wagner. Audio adversarial"
        },
        {
          "1. Environmental\nnoise\nshould\nbe\nadded\nto": "",
          "performance is still significantly lower when the": "examples: Targeted attacks on speech-to-text.\nIn"
        },
        {
          "1. Environmental\nnoise\nshould\nbe\nadded\nto": "that they can be confidently used for data augmen-",
          "performance is still significantly lower when the": ""
        },
        {
          "1. Environmental\nnoise\nshould\nbe\nadded\nto": "",
          "performance is still significantly lower when the": "2018 IEEE Security and Privacy Workshops (SPW)."
        },
        {
          "1. Environmental\nnoise\nshould\nbe\nadded\nto": "tation. We look at the change in performance of the",
          "performance is still significantly lower when the": ""
        },
        {
          "1. Environmental\nnoise\nshould\nbe\nadded\nto": "models that are trained on the original IEMOCAP",
          "performance is still significantly lower when the": "Rupayan Chakraborty, Ashish Panda, Meghna Pand-"
        },
        {
          "1. Environmental\nnoise\nshould\nbe\nadded\nto": "",
          "performance is still significantly lower when the": "haripande, Sonal Joshi, and Sunil Kumar Kopparapu."
        },
        {
          "1. Environmental\nnoise\nshould\nbe\nadded\nto": "dataset, but tested on noisy samples and if augmen-",
          "performance is still significantly lower when the": ""
        },
        {
          "1. Environmental\nnoise\nshould\nbe\nadded\nto": "",
          "performance is still significantly lower when the": "2019. Front-end feature compensation and denois-"
        },
        {
          "1. Environmental\nnoise\nshould\nbe\nadded\nto": "tation of the training set leads to an improvement",
          "performance is still significantly lower when the": ""
        },
        {
          "1. Environmental\nnoise\nshould\nbe\nadded\nto": "",
          "performance is still significantly lower when the": "ing for noise robust speech emotion recognition.\nIn"
        },
        {
          "1. Environmental\nnoise\nshould\nbe\nadded\nto": "in performance. We conclude that, unlike humans,",
          "performance is still significantly lower when the": ""
        },
        {
          "1. Environmental\nnoise\nshould\nbe\nadded\nto": "",
          "performance is still significantly lower when the": "Interspeech."
        },
        {
          "1. Environmental\nnoise\nshould\nbe\nadded\nto": "machine learning models are extremely brittle to",
          "performance is still significantly lower when the": ""
        },
        {
          "1. Environmental\nnoise\nshould\nbe\nadded\nto": "",
          "performance is still significantly lower when the": "Farah Chenchah and Zied Lachiri. 2016. Speech emo-"
        },
        {
          "1. Environmental\nnoise\nshould\nbe\nadded\nto": "the introduction of many kinds of noise. While",
          "performance is still significantly lower when the": ""
        },
        {
          "1. Environmental\nnoise\nshould\nbe\nadded\nto": "",
          "performance is still significantly lower when the": "tion recognition in noisy environment.\nIn 2016 2nd"
        },
        {
          "1. Environmental\nnoise\nshould\nbe\nadded\nto": "the performance of\nthe machine learning model",
          "performance is still significantly lower when the": ""
        },
        {
          "1. Environmental\nnoise\nshould\nbe\nadded\nto": "",
          "performance is still significantly lower when the": "International Conference on Advanced Technologies"
        },
        {
          "1. Environmental\nnoise\nshould\nbe\nadded\nto": "on noisy samples is aided from augmentation, the",
          "performance is still significantly lower when the": "for Signal and Image Processing (ATSIP). Ieee."
        }
      ],
      "page": 19
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "François Chollet. 2015. keras. https : //github.com/": "fchollet/keras.",
          "Tom Ko, Vijayaditya Peddinti, Daniel Povey, and San-": "jeev Khudanpur. 2015.\nAudio augmentation for"
        },
        {
          "François Chollet. 2015. keras. https : //github.com/": "",
          "Tom Ko, Vijayaditya Peddinti, Daniel Povey, and San-": "speech recognition.\nIn Sixteenth Annual Conference"
        },
        {
          "François Chollet. 2015. keras. https : //github.com/": "Javid Ebrahimi, Anyi Rao, Daniel Lowd,\nand De-",
          "Tom Ko, Vijayaditya Peddinti, Daniel Povey, and San-": "of the International Speech Communication Associa-"
        },
        {
          "François Chollet. 2015. keras. https : //github.com/": "jing Dou. 2017.\nHotflip: White-box adversarial",
          "Tom Ko, Vijayaditya Peddinti, Daniel Povey, and San-": "tion."
        },
        {
          "François Chollet. 2015. keras. https : //github.com/": "arXiv preprint\nexamples\nfor\ntext\nclassification.",
          "Tom Ko, Vijayaditya Peddinti, Daniel Povey, and San-": ""
        },
        {
          "François Chollet. 2015. keras. https : //github.com/": "",
          "Tom Ko, Vijayaditya Peddinti, Daniel Povey, and San-": "Kalpesh Krishna, Liang Lu, Kevin Gimpel, and Karen"
        },
        {
          "François Chollet. 2015. keras. https : //github.com/": "arXiv:1712.06751.",
          "Tom Ko, Vijayaditya Peddinti, Daniel Povey, and San-": ""
        },
        {
          "François Chollet. 2015. keras. https : //github.com/": "",
          "Tom Ko, Vijayaditya Peddinti, Daniel Povey, and San-": "Livescu. 2018. A study of all-convolutional encoders"
        },
        {
          "François Chollet. 2015. keras. https : //github.com/": "",
          "Tom Ko, Vijayaditya Peddinti, Daniel Povey, and San-": "for connectionist\ntemporal classification.\nIn 2018"
        },
        {
          "François Chollet. 2015. keras. https : //github.com/": "Jort F. Gemmeke, Daniel P. W. Ellis, Dylan Freedman,",
          "Tom Ko, Vijayaditya Peddinti, Daniel Povey, and San-": ""
        },
        {
          "François Chollet. 2015. keras. https : //github.com/": "",
          "Tom Ko, Vijayaditya Peddinti, Daniel Povey, and San-": "IEEE International Conference on Acoustics, Speech"
        },
        {
          "François Chollet. 2015. keras. https : //github.com/": "Aren Jansen, et al. 2017. Audio set: An ontology",
          "Tom Ko, Vijayaditya Peddinti, Daniel Povey, and San-": ""
        },
        {
          "François Chollet. 2015. keras. https : //github.com/": "",
          "Tom Ko, Vijayaditya Peddinti, Daniel Povey, and San-": "and Signal Processing (ICASSP). Ieee."
        },
        {
          "François Chollet. 2015. keras. https : //github.com/": "and human-labeled dataset for audio events.\nIn 2017",
          "Tom Ko, Vijayaditya Peddinti, Daniel Povey, and San-": ""
        },
        {
          "François Chollet. 2015. keras. https : //github.com/": "IEEE International Conference on Acoustics, Speech",
          "Tom Ko, Vijayaditya Peddinti, Daniel Povey, and San-": ""
        },
        {
          "François Chollet. 2015. keras. https : //github.com/": "",
          "Tom Ko, Vijayaditya Peddinti, Daniel Povey, and San-": "Margaret Lech, Melissa Stolar, Christopher Best, and"
        },
        {
          "François Chollet. 2015. keras. https : //github.com/": "and Signal Processing, ICASSP 2017, New Orleans,",
          "Tom Ko, Vijayaditya Peddinti, Daniel Povey, and San-": ""
        },
        {
          "François Chollet. 2015. keras. https : //github.com/": "",
          "Tom Ko, Vijayaditya Peddinti, Daniel Povey, and San-": "Robert Bolia. 2020. Real-time speech emotion recog-"
        },
        {
          "François Chollet. 2015. keras. https : //github.com/": "LA, USA, March 5-9, 2017. Ieee.",
          "Tom Ko, Vijayaditya Peddinti, Daniel Povey, and San-": ""
        },
        {
          "François Chollet. 2015. keras. https : //github.com/": "",
          "Tom Ko, Vijayaditya Peddinti, Daniel Povey, and San-": "nition using a pre-trained image classification net-"
        },
        {
          "François Chollet. 2015. keras. https : //github.com/": "",
          "Tom Ko, Vijayaditya Peddinti, Daniel Povey, and San-": "work: Effects of bandwidth reduction and compand-"
        },
        {
          "François Chollet. 2015. keras. https : //github.com/": "Amirhossein Hajavi and Ali Etemad. 2021.\nSiamese",
          "Tom Ko, Vijayaditya Peddinti, Daniel Povey, and San-": "ing. Frontiers in Computer Science."
        },
        {
          "François Chollet. 2015. keras. https : //github.com/": "capsule network for end-to-end speaker recognition",
          "Tom Ko, Vijayaditya Peddinti, Daniel Povey, and San-": ""
        },
        {
          "François Chollet. 2015. keras. https : //github.com/": "in the wild.\nIn ICASSP 2021-2021 IEEE Interna-",
          "Tom Ko, Vijayaditya Peddinti, Daniel Povey, and San-": "Jinyu Li, Li Deng, Yifan Gong, and Reinhold Haeb-"
        },
        {
          "François Chollet. 2015. keras. https : //github.com/": "tional Conference on Acoustics, Speech and Signal",
          "Tom Ko, Vijayaditya Peddinti, Daniel Povey, and San-": "Umbach. 2014. An overview of noise-robust auto-"
        },
        {
          "François Chollet. 2015. keras. https : //github.com/": "Processing (ICASSP). Ieee.",
          "Tom Ko, Vijayaditya Peddinti, Daniel Povey, and San-": "IEEE/ACM Transactions\nmatic speech recognition."
        },
        {
          "François Chollet. 2015. keras. https : //github.com/": "",
          "Tom Ko, Vijayaditya Peddinti, Daniel Povey, and San-": "on Audio, Speech, and Language Processing."
        },
        {
          "François Chollet. 2015. keras. https : //github.com/": "Awni Hannun, Carl Case, Jared Casper, Bryan Catan-",
          "Tom Ko, Vijayaditya Peddinti, Daniel Povey, and San-": ""
        },
        {
          "François Chollet. 2015. keras. https : //github.com/": "",
          "Tom Ko, Vijayaditya Peddinti, Daniel Povey, and San-": "Weiyi Ma and William Forde Thompson. 2015. Human"
        },
        {
          "François Chollet. 2015. keras. https : //github.com/": "zaro, Greg Diamos, Erich Elsen, Ryan Prenger, San-",
          "Tom Ko, Vijayaditya Peddinti, Daniel Povey, and San-": ""
        },
        {
          "François Chollet. 2015. keras. https : //github.com/": "",
          "Tom Ko, Vijayaditya Peddinti, Daniel Povey, and San-": "emotions track changes in the acoustic environment."
        },
        {
          "François Chollet. 2015. keras. https : //github.com/": "jeev Satheesh, Shubho Sengupta, Adam Coates, et al.",
          "Tom Ko, Vijayaditya Peddinti, Daniel Povey, and San-": ""
        },
        {
          "François Chollet. 2015. keras. https : //github.com/": "",
          "Tom Ko, Vijayaditya Peddinti, Daniel Povey, and San-": "Proceedings of the National Academy of Sciences."
        },
        {
          "François Chollet. 2015. keras. https : //github.com/": "2014. Deep speech: Scaling up end-to-end speech",
          "Tom Ko, Vijayaditya Peddinti, Daniel Povey, and San-": ""
        },
        {
          "François Chollet. 2015. keras. https : //github.com/": "recognition. arXiv preprint arXiv:1412.5567.",
          "Tom Ko, Vijayaditya Peddinti, Daniel Povey, and San-": ""
        },
        {
          "François Chollet. 2015. keras. https : //github.com/": "",
          "Tom Ko, Vijayaditya Peddinti, Daniel Povey, and San-": "Omar Mohamed and Salah A Aly. 2021.\nArabic"
        },
        {
          "François Chollet. 2015. keras. https : //github.com/": "",
          "Tom Ko, Vijayaditya Peddinti, Daniel Povey, and San-": "speech emotion recognition employing wav2vec2."
        },
        {
          "François Chollet. 2015. keras. https : //github.com/": "Jet Hoek and Merel Scholman. 2017. Evaluating dis-",
          "Tom Ko, Vijayaditya Peddinti, Daniel Povey, and San-": "0 and hubert based on baved dataset. arXiv preprint"
        },
        {
          "François Chollet. 2015. keras. https : //github.com/": "course annotation:\nSome recent\ninsights and new",
          "Tom Ko, Vijayaditya Peddinti, Daniel Povey, and San-": "arXiv:2110.04425."
        },
        {
          "François Chollet. 2015. keras. https : //github.com/": "the 13th Joint ISO-\napproaches.\nIn Proceedings of",
          "Tom Ko, Vijayaditya Peddinti, Daniel Povey, and San-": ""
        },
        {
          "François Chollet. 2015. keras. https : //github.com/": "ACL Workshop on Interoperable Semantic Annotation",
          "Tom Ko, Vijayaditya Peddinti, Daniel Povey, and San-": "José Antonio Nicolás,\nJavier de Lope,\nand Manuel"
        },
        {
          "François Chollet. 2015. keras. https : //github.com/": "(isa-13).",
          "Tom Ko, Vijayaditya Peddinti, Daniel Povey, and San-": "Graña. 2022.\nData augmentation techniques\nfor"
        },
        {
          "François Chollet. 2015. keras. https : //github.com/": "",
          "Tom Ko, Vijayaditya Peddinti, Daniel Povey, and San-": "speech emotion recognition and deep learning.\nIn"
        },
        {
          "François Chollet. 2015. keras. https : //github.com/": "",
          "Tom Ko, Vijayaditya Peddinti, Daniel Povey, and San-": "International Work-Conference on the Interplay Be-"
        },
        {
          "François Chollet. 2015. keras. https : //github.com/": "Wei-Ning Hsu, Anuroop Sriram, Alexei Baevski, Ta-",
          "Tom Ko, Vijayaditya Peddinti, Daniel Povey, and San-": ""
        },
        {
          "François Chollet. 2015. keras. https : //github.com/": "",
          "Tom Ko, Vijayaditya Peddinti, Daniel Povey, and San-": "tween Natural and Artificial Computation. Springer."
        },
        {
          "François Chollet. 2015. keras. https : //github.com/": "tiana Likhomanenko, Qiantong Xu, Vineel Pratap,",
          "Tom Ko, Vijayaditya Peddinti, Daniel Povey, and San-": ""
        },
        {
          "François Chollet. 2015. keras. https : //github.com/": "Jacob Kahn, Ann Lee, Ronan Collobert, Gabriel Syn-",
          "Tom Ko, Vijayaditya Peddinti, Daniel Povey, and San-": ""
        },
        {
          "François Chollet. 2015. keras. https : //github.com/": "",
          "Tom Ko, Vijayaditya Peddinti, Daniel Povey, and San-": "˙"
        },
        {
          "François Chollet. 2015. keras. https : //github.com/": "",
          "Tom Ko, Vijayaditya Peddinti, Daniel Povey, and San-": "Raghavendra Pappagari, Jesús Villalba, Piotr\nZelasko,"
        },
        {
          "François Chollet. 2015. keras. https : //github.com/": "naeve, et al. 2021. Robust wav2vec 2.0: Analyzing",
          "Tom Ko, Vijayaditya Peddinti, Daniel Povey, and San-": ""
        },
        {
          "François Chollet. 2015. keras. https : //github.com/": "",
          "Tom Ko, Vijayaditya Peddinti, Daniel Povey, and San-": "Laureano Moro-Velazquez, and Najim Dehak. 2021."
        },
        {
          "François Chollet. 2015. keras. https : //github.com/": "domain shift\nin self-supervised pre-training. arXiv",
          "Tom Ko, Vijayaditya Peddinti, Daniel Povey, and San-": ""
        },
        {
          "François Chollet. 2015. keras. https : //github.com/": "",
          "Tom Ko, Vijayaditya Peddinti, Daniel Povey, and San-": "Copypaste: An augmentation method for\nspeech"
        },
        {
          "François Chollet. 2015. keras. https : //github.com/": "preprint arXiv:2104.01027.",
          "Tom Ko, Vijayaditya Peddinti, Daniel Povey, and San-": ""
        },
        {
          "François Chollet. 2015. keras. https : //github.com/": "",
          "Tom Ko, Vijayaditya Peddinti, Daniel Povey, and San-": "emotion recognition.\nIn IEEE International Con-"
        },
        {
          "François Chollet. 2015. keras. https : //github.com/": "",
          "Tom Ko, Vijayaditya Peddinti, Daniel Povey, and San-": "ference on Acoustics, Speech and Signal Processing"
        },
        {
          "François Chollet. 2015. keras. https : //github.com/": "Mimansa Jaiswal, Zakaria Aldeneh, Cristian-Paul Bara,",
          "Tom Ko, Vijayaditya Peddinti, Daniel Povey, and San-": ""
        },
        {
          "François Chollet. 2015. keras. https : //github.com/": "",
          "Tom Ko, Vijayaditya Peddinti, Daniel Povey, and San-": "(ICASSP)."
        },
        {
          "François Chollet. 2015. keras. https : //github.com/": "Yuanhang Luo, Mihai Burzo, Rada Mihalcea, and",
          "Tom Ko, Vijayaditya Peddinti, Daniel Povey, and San-": ""
        },
        {
          "François Chollet. 2015. keras. https : //github.com/": "Emily Mower Provost. 2019. Muse-ing on the im-",
          "Tom Ko, Vijayaditya Peddinti, Daniel Povey, and San-": "Emilia Parada-Cabaleiro, Alice Baird, Anton Batliner,"
        },
        {
          "François Chollet. 2015. keras. https : //github.com/": "pact of utterance ordering on crowdsourced emotion",
          "Tom Ko, Vijayaditya Peddinti, Daniel Povey, and San-": "Nicholas Cummins,\net al. 2017.\nThe perception"
        },
        {
          "François Chollet. 2015. keras. https : //github.com/": "annotations.\nIn 2019 Icassp. Ieee.",
          "Tom Ko, Vijayaditya Peddinti, Daniel Povey, and San-": "of emotions in noisified nonsense speech.\nIn Inter-"
        },
        {
          "François Chollet. 2015. keras. https : //github.com/": "",
          "Tom Ko, Vijayaditya Peddinti, Daniel Povey, and San-": "speech 2017, 18th Annual Conference of\nthe Inter-"
        },
        {
          "François Chollet. 2015. keras. https : //github.com/": "Soheil Khorram, Zakaria Aldeneh, Dimitrios Dimitri-",
          "Tom Ko, Vijayaditya Peddinti, Daniel Povey, and San-": "national Speech Communication Association, Stock-"
        },
        {
          "François Chollet. 2015. keras. https : //github.com/": "adis, Melvin McInnis, and Emily Mower Provost.",
          "Tom Ko, Vijayaditya Peddinti, Daniel Povey, and San-": "holm, Sweden, August 20-24, 2017. Isca."
        },
        {
          "François Chollet. 2015. keras. https : //github.com/": "2017. Capturing long-term temporal dependencies",
          "Tom Ko, Vijayaditya Peddinti, Daniel Povey, and San-": ""
        },
        {
          "François Chollet. 2015. keras. https : //github.com/": "",
          "Tom Ko, Vijayaditya Peddinti, Daniel Povey, and San-": "Leonardo Pepino, Pablo Riera, and Luciana Ferrer. 2021."
        },
        {
          "François Chollet. 2015. keras. https : //github.com/": "with convolutional networks for continuous emotion",
          "Tom Ko, Vijayaditya Peddinti, Daniel Povey, and San-": ""
        },
        {
          "François Chollet. 2015. keras. https : //github.com/": "",
          "Tom Ko, Vijayaditya Peddinti, Daniel Povey, and San-": "Emotion recognition from speech using wav2vec 2.0"
        },
        {
          "François Chollet. 2015. keras. https : //github.com/": "recognition. Proc. Interspeech.",
          "Tom Ko, Vijayaditya Peddinti, Daniel Povey, and San-": ""
        },
        {
          "François Chollet. 2015. keras. https : //github.com/": "",
          "Tom Ko, Vijayaditya Peddinti, Daniel Povey, and San-": "embeddings.\nIn Interspeech 2021, 22nd Annual Con-"
        },
        {
          "François Chollet. 2015. keras. https : //github.com/": "",
          "Tom Ko, Vijayaditya Peddinti, Daniel Povey, and San-": "ference of the International Speech Communication"
        },
        {
          "François Chollet. 2015. keras. https : //github.com/": "Gerald Kidd Jr, Christine R Mason, Jayaganesh Swami-",
          "Tom Ko, Vijayaditya Peddinti, Daniel Povey, and San-": ""
        },
        {
          "François Chollet. 2015. keras. https : //github.com/": "",
          "Tom Ko, Vijayaditya Peddinti, Daniel Povey, and San-": "Association, Brno, Czechia, 30 August - 3 September"
        },
        {
          "François Chollet. 2015. keras. https : //github.com/": "nathan, Elin Roverud, Kameron K Clayton, and Vir-",
          "Tom Ko, Vijayaditya Peddinti, Daniel Povey, and San-": ""
        },
        {
          "François Chollet. 2015. keras. https : //github.com/": "",
          "Tom Ko, Vijayaditya Peddinti, Daniel Povey, and San-": "2021. Isca."
        },
        {
          "François Chollet. 2015. keras. https : //github.com/": "ginia Best. 2016. Determining the energetic and in-",
          "Tom Ko, Vijayaditya Peddinti, Daniel Povey, and San-": ""
        },
        {
          "François Chollet. 2015. keras. https : //github.com/": "formational components of speech-on-speech mask-",
          "Tom Ko, Vijayaditya Peddinti, Daniel Povey, and San-": ""
        },
        {
          "François Chollet. 2015. keras. https : //github.com/": "",
          "Tom Ko, Vijayaditya Peddinti, Daniel Povey, and San-": "Karol J. Piczak. ESC: Dataset for Environmental Sound"
        },
        {
          "François Chollet. 2015. keras. https : //github.com/": "ing. The Journal of the Acoustical Society of Amer-",
          "Tom Ko, Vijayaditya Peddinti, Daniel Povey, and San-": ""
        },
        {
          "François Chollet. 2015. keras. https : //github.com/": "",
          "Tom Ko, Vijayaditya Peddinti, Daniel Povey, and San-": "the 23rd Annual\nClassification.\nIn Proceedings of"
        },
        {
          "François Chollet. 2015. keras. https : //github.com/": "ica.",
          "Tom Ko, Vijayaditya Peddinti, Daniel Povey, and San-": ""
        },
        {
          "François Chollet. 2015. keras. https : //github.com/": "",
          "Tom Ko, Vijayaditya Peddinti, Daniel Povey, and San-": "ACM Conference on Multimedia."
        },
        {
          "François Chollet. 2015. keras. https : //github.com/": "Hyoung-Gook Kim and Jin Young Kim. Acoustic event",
          "Tom Ko, Vijayaditya Peddinti, Daniel Povey, and San-": "Odette Scharenborg, Sofoklis Kakouros, and Jiska Koe-"
        },
        {
          "François Chollet. 2015. keras. https : //github.com/": "detection in multichannel audio using gated recur-",
          "Tom Ko, Vijayaditya Peddinti, Daniel Povey, and San-": "mans. 2018. The effect of noise on emotion percep-"
        },
        {
          "François Chollet. 2015. keras. https : //github.com/": "rent neural networks with high-resolution spectral",
          "Tom Ko, Vijayaditya Peddinti, Daniel Povey, and San-": "tion in an unknown language.\nIn Speech Prosody"
        },
        {
          "François Chollet. 2015. keras. https : //github.com/": "features. ETRI Journal.",
          "Tom Ko, Vijayaditya Peddinti, Daniel Povey, and San-": "2018. Isca."
        }
      ],
      "page": 20
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Steffen Schneider, Alexei Baevski, Ronan Collobert,": "and Michael Auli. 2019. wav2vec: Unsupervised"
        },
        {
          "Steffen Schneider, Alexei Baevski, Ronan Collobert,": "pre-training for speech recognition. arXiv preprint"
        },
        {
          "Steffen Schneider, Alexei Baevski, Ronan Collobert,": "arXiv:1904.05862."
        },
        {
          "Steffen Schneider, Alexei Baevski, Ronan Collobert,": "Hye-Jin Shim,\nJee-Weon Jung, Hee-Soo Heo, Sung-"
        },
        {
          "Steffen Schneider, Alexei Baevski, Ronan Collobert,": "Hyun Yoon, and Ha-Jin Yu. Replay spoofing detec-"
        },
        {
          "Steffen Schneider, Alexei Baevski, Ronan Collobert,": "tion system for automatic speaker verification using"
        },
        {
          "Steffen Schneider, Alexei Baevski, Ronan Collobert,": "multi-task learning of noise classes.\nIn 2018 Taai."
        },
        {
          "Steffen Schneider, Alexei Baevski, Ronan Collobert,": "Victoria Stenback. 2016. Speech masking speech in ev-"
        },
        {
          "Steffen Schneider, Alexei Baevski, Ronan Collobert,": "eryday communication: The role of inhibitory control"
        },
        {
          "Steffen Schneider, Alexei Baevski, Ronan Collobert,": "and working memory capacity. Linkoping University"
        },
        {
          "Steffen Schneider, Alexei Baevski, Ronan Collobert,": "Electronic Press."
        },
        {
          "Steffen Schneider, Alexei Baevski, Ronan Collobert,": "Jean-Marc Valin. 2018. A hybrid dsp/deep learning"
        },
        {
          "Steffen Schneider, Alexei Baevski, Ronan Collobert,": "approach to real-time full-band speech enhancement."
        },
        {
          "Steffen Schneider, Alexei Baevski, Ronan Collobert,": "In 2018 IEEE 20th International Workshop on Multi-"
        },
        {
          "Steffen Schneider, Alexei Baevski, Ronan Collobert,": "media Signal Processing (MMSP). Ieee."
        },
        {
          "Steffen Schneider, Alexei Baevski, Ronan Collobert,": "Mingke Xu, Fan Zhang, and Wei Zhang. 2021. Head"
        },
        {
          "Steffen Schneider, Alexei Baevski, Ronan Collobert,": "fusion:\nImproving the accuracy and robustness of"
        },
        {
          "Steffen Schneider, Alexei Baevski, Ronan Collobert,": "speech emotion recognition on the\niemocap and"
        },
        {
          "Steffen Schneider, Alexei Baevski, Ronan Collobert,": "ravdess dataset.\nIEEE Access."
        },
        {
          "Steffen Schneider, Alexei Baevski, Ronan Collobert,": "Jiahong Yuan and Mark Liberman. Automatic detection"
        },
        {
          "Steffen Schneider, Alexei Baevski, Ronan Collobert,": "of “g-dropping” in american english using forced"
        },
        {
          "Steffen Schneider, Alexei Baevski, Ronan Collobert,": "alignment.\nIn 2011 IEEE Workshop on Automatic"
        },
        {
          "Steffen Schneider, Alexei Baevski, Ronan Collobert,": "Speech Recognition & Understanding. Ieee."
        },
        {
          "Steffen Schneider, Alexei Baevski, Ronan Collobert,": "Xiaojia Zhao, Yuxuan Wang, and DeLiang Wang. 2014."
        },
        {
          "Steffen Schneider, Alexei Baevski, Ronan Collobert,": "Robust speaker identification in noisy and reverber-"
        },
        {
          "Steffen Schneider, Alexei Baevski, Ronan Collobert,": "IEEE/ACM Transactions on Audio,\nant conditions."
        },
        {
          "Steffen Schneider, Alexei Baevski, Ronan Collobert,": "Speech, and Language Processing."
        },
        {
          "Steffen Schneider, Alexei Baevski, Ronan Collobert,": "Stephan Zheng, Yang Song, Thomas Leung, and Ian"
        },
        {
          "Steffen Schneider, Alexei Baevski, Ronan Collobert,": "Goodfellow. 2016.\nImproving the robustness of deep"
        },
        {
          "Steffen Schneider, Alexei Baevski, Ronan Collobert,": "neural networks via stability training.\nIn Proceedings"
        },
        {
          "Steffen Schneider, Alexei Baevski, Ronan Collobert,": "of CVPR."
        }
      ],
      "page": 21
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Practical hidden voice attacks against speech and speaker recognition systems",
      "authors": [
        "Hadi Abdullah",
        "Washington Garcia",
        "Christian Peeters",
        "Patrick Traynor",
        "Kevin Rb Butler",
        "Joseph Wilson"
      ],
      "year": "2019",
      "venue": "Practical hidden voice attacks against speech and speaker recognition systems",
      "arxiv": "arXiv:1904.05734"
    },
    {
      "citation_id": "2",
      "title": "Pooling acoustic and lexical features for the prediction of valence",
      "authors": [
        "Zakaria Aldeneh",
        "Soheil Khorram",
        "Dimitrios Dimitriadis",
        "Emily Provost"
      ],
      "year": "2017",
      "venue": "Proceedings of the 19th ACM International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "3",
      "title": "Using regional saliency for speech emotion recognition",
      "authors": [
        "Zakaria Aldeneh",
        "Emily Provost"
      ],
      "year": "2017",
      "venue": "2017 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "4",
      "title": "wav2vec 2.0: A framework for self-supervised learning of speech representations",
      "authors": [
        "Alexei Baevski",
        "Yuhao Zhou",
        "Abdelrahman Mohamed",
        "Michael Auli"
      ],
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "5",
      "title": "Measuring emotion: the self-assessment manikin and the semantic differential",
      "authors": [
        "M Margaret",
        "Peter Bradley",
        "Lang"
      ],
      "year": "1994",
      "venue": "Journal of behavior therapy and experimental psychiatry"
    },
    {
      "citation_id": "6",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "Carlos Busso",
        "Murtaza Bulut",
        "Chi-Chun Lee",
        "Abe Kazemzadeh",
        "Emily Mower",
        "Samuel Kim",
        "Jeannette Chang",
        "Sungbok Lee",
        "Shrikanth S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "7",
      "title": "Shrikanth narayanan fundamental frequency analysis for speech emotion processing. The role of prosody in affective speech",
      "authors": [
        "Carlos Busso",
        "Murtaza Bulut",
        "Sungbok Lee"
      ],
      "year": "2009",
      "venue": "Shrikanth narayanan fundamental frequency analysis for speech emotion processing. The role of prosody in affective speech"
    },
    {
      "citation_id": "8",
      "title": "Audio adversarial examples: Targeted attacks on speech-to-text",
      "authors": [
        "Nicholas Carlini",
        "David Wagner"
      ],
      "year": "2018",
      "venue": "IEEE Security and Privacy Workshops"
    },
    {
      "citation_id": "9",
      "title": "Front-end feature compensation and denoising for noise robust speech emotion recognition",
      "authors": [
        "Rupayan Chakraborty",
        "Ashish Panda",
        "Meghna Pandharipande",
        "Sonal Joshi",
        "Sunil Kumar"
      ],
      "year": "2019",
      "venue": "Front-end feature compensation and denoising for noise robust speech emotion recognition"
    },
    {
      "citation_id": "10",
      "title": "Speech emotion recognition in noisy environment",
      "authors": [
        "Farah Chenchah",
        "Zied Lachiri"
      ],
      "year": "2016",
      "venue": "2016 2nd International Conference on Advanced Technologies for Signal and Image Processing"
    },
    {
      "citation_id": "11",
      "title": "",
      "authors": [
        "François Chollet"
      ],
      "year": "2015",
      "venue": ""
    },
    {
      "citation_id": "12",
      "title": "Hotflip: White-box adversarial examples for text classification",
      "authors": [
        "Javid Ebrahimi",
        "Anyi Rao",
        "Daniel Lowd",
        "Dejing Dou"
      ],
      "year": "2017",
      "venue": "Hotflip: White-box adversarial examples for text classification",
      "arxiv": "arXiv:1712.06751"
    },
    {
      "citation_id": "13",
      "title": "Audio set: An ontology and human-labeled dataset for audio events",
      "authors": [
        "F Jort",
        "Gemmeke",
        "P Daniel",
        "Dylan Ellis",
        "Aren Freedman",
        "Jansen"
      ],
      "year": "2017",
      "venue": "2017 IEEE International Conference on Acoustics, Speech and Signal Processing",
      "doi": "10.1109/icassp.2017.7952261"
    },
    {
      "citation_id": "14",
      "title": "Siamese capsule network for end-to-end speaker recognition in the wild",
      "authors": [
        "Amirhossein Hajavi",
        "Ali Etemad"
      ],
      "year": "2021",
      "venue": "ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "15",
      "title": "Deep speech: Scaling up end-to-end speech recognition",
      "authors": [
        "Awni Hannun",
        "Carl Case",
        "Jared Casper",
        "Bryan Catanzaro",
        "Greg Diamos",
        "Erich Elsen",
        "Ryan Prenger",
        "Sanjeev Satheesh",
        "Shubho Sengupta",
        "Adam Coates"
      ],
      "year": "2014",
      "venue": "Deep speech: Scaling up end-to-end speech recognition",
      "arxiv": "arXiv:1412.5567"
    },
    {
      "citation_id": "16",
      "title": "Evaluating discourse annotation: Some recent insights and new approaches",
      "authors": [
        "Jet Hoek",
        "Merel Scholman"
      ],
      "year": "2017",
      "venue": "Proceedings of the 13th Joint ISO-ACL Workshop on Interoperable Semantic Annotation"
    },
    {
      "citation_id": "17",
      "title": "Robust wav2vec 2.0: Analyzing domain shift in self-supervised pre-training",
      "authors": [
        "Wei-Ning Hsu",
        "Anuroop Sriram",
        "Alexei Baevski",
        "Tatiana Likhomanenko",
        "Qiantong Xu",
        "Vineel Pratap",
        "Jacob Kahn",
        "Ann Lee",
        "Ronan Collobert",
        "Gabriel Synnaeve"
      ],
      "year": "2021",
      "venue": "Robust wav2vec 2.0: Analyzing domain shift in self-supervised pre-training",
      "arxiv": "arXiv:2104.01027"
    },
    {
      "citation_id": "18",
      "title": "Muse-ing on the impact of utterance ordering on crowdsourced emotion annotations",
      "authors": [
        "Mimansa Jaiswal",
        "Zakaria Aldeneh",
        "Cristian-Paul Bara",
        "Yuanhang Luo",
        "Mihai Burzo",
        "Rada Mihalcea",
        "Emily Provost"
      ],
      "year": "2019",
      "venue": "Icassp"
    },
    {
      "citation_id": "19",
      "title": "Capturing long-term temporal dependencies with convolutional networks for continuous emotion recognition",
      "authors": [
        "Soheil Khorram",
        "Zakaria Aldeneh",
        "Dimitrios Dimitriadis",
        "Melvin Mcinnis",
        "Emily Provost"
      ],
      "year": "2017",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "20",
      "title": "Determining the energetic and informational components of speech-on-speech masking",
      "authors": [
        "Gerald Kidd",
        "Christine Mason",
        "Jayaganesh Swaminathan",
        "Elin Roverud",
        "Virginia Kameron K Clayton",
        "Best"
      ],
      "year": "2016",
      "venue": "The Journal of the Acoustical Society of America"
    },
    {
      "citation_id": "21",
      "title": "Acoustic event detection in multichannel audio using gated recurrent neural networks with high-resolution spectral features",
      "authors": [
        "Hyoung-Gook Kim",
        "Jin Kim"
      ],
      "venue": "Acoustic event detection in multichannel audio using gated recurrent neural networks with high-resolution spectral features"
    },
    {
      "citation_id": "22",
      "title": "Audio augmentation for speech recognition",
      "authors": [
        "Tom Ko",
        "Vijayaditya Peddinti",
        "Daniel Povey",
        "Sanjeev Khudanpur"
      ],
      "year": "2015",
      "venue": "Sixteenth Annual Conference of the International Speech Communication Association"
    },
    {
      "citation_id": "23",
      "title": "A study of all-convolutional encoders for connectionist temporal classification",
      "authors": [
        "Kalpesh Krishna",
        "Liang Lu",
        "Kevin Gimpel",
        "Karen Livescu"
      ],
      "year": "2018",
      "venue": "2018 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "24",
      "title": "Real-time speech emotion recognition using a pre-trained image classification network: Effects of bandwidth reduction and companding",
      "authors": [
        "Margaret Lech",
        "Melissa Stolar",
        "Christopher Best",
        "Robert Bolia"
      ],
      "year": "2020",
      "venue": "Frontiers in Computer Science"
    },
    {
      "citation_id": "25",
      "title": "An overview of noise-robust automatic speech recognition",
      "authors": [
        "Jinyu Li",
        "Li Deng",
        "Yifan Gong",
        "Reinhold Haeb-Umbach"
      ],
      "year": "2014",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing",
      "doi": "10.1109/taslp.2014.2304637"
    },
    {
      "citation_id": "26",
      "title": "Human emotions track changes in the acoustic environment",
      "authors": [
        "Weiyi Ma",
        "William Thompson"
      ],
      "year": "2015",
      "venue": "Human emotions track changes in the acoustic environment"
    },
    {
      "citation_id": "27",
      "title": "Arabic speech emotion recognition employing wav2vec2. 0 and hubert based on baved dataset",
      "authors": [
        "Omar Mohamed",
        "A Salah",
        "Aly"
      ],
      "year": "2021",
      "venue": "Arabic speech emotion recognition employing wav2vec2. 0 and hubert based on baved dataset",
      "arxiv": "arXiv:2110.04425"
    },
    {
      "citation_id": "28",
      "title": "Data augmentation techniques for speech emotion recognition and deep learning",
      "authors": [
        "José Antonio Nicolás",
        "Javier De Lope",
        "Manuel Graña"
      ],
      "year": "2022",
      "venue": "International Work-Conference on the Interplay Between Natural and Artificial Computation"
    },
    {
      "citation_id": "29",
      "title": "Copypaste: An augmentation method for speech emotion recognition",
      "authors": [
        "Raghavendra Pappagari",
        "Jesús Villalba",
        "Piotr Żelasko"
      ],
      "year": "2021",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "30",
      "title": "The perception of emotions in noisified nonsense speech",
      "authors": [
        "Emilia Parada-Cabaleiro",
        "Alice Baird",
        "Anton Batliner",
        "Nicholas Cummins"
      ],
      "year": "2017",
      "venue": "Interspeech 2017, 18th Annual Conference of the International Speech Communication Association",
      "doi": "10.21437/interspeech.2017-104"
    },
    {
      "citation_id": "31",
      "title": "Emotion recognition from speech using wav2vec 2.0 embeddings",
      "authors": [
        "Leonardo Pepino",
        "Pablo Riera",
        "Luciana Ferrer"
      ],
      "year": "2021",
      "venue": "Interspeech 2021, 22nd Annual Conference of the International Speech Communication Association",
      "doi": "10.21437/Interspeech.2021-703"
    },
    {
      "citation_id": "32",
      "title": "ESC: Dataset for Environmental Sound Classification",
      "authors": [
        "J Karol",
        "Piczak"
      ],
      "venue": "Proceedings of the 23rd Annual ACM Conference on Multimedia"
    },
    {
      "citation_id": "33",
      "title": "The effect of noise on emotion perception in an unknown language",
      "authors": [
        "Odette Scharenborg"
      ],
      "year": "2018",
      "venue": "Sofoklis Kakouros, and Jiska Koemans",
      "doi": "10.21437/speechprosody.2018-74"
    },
    {
      "citation_id": "34",
      "title": "wav2vec: Unsupervised pre-training for speech recognition",
      "authors": [
        "Steffen Schneider",
        "Alexei Baevski",
        "Ronan Collobert",
        "Michael Auli"
      ],
      "year": "2019",
      "venue": "wav2vec: Unsupervised pre-training for speech recognition",
      "arxiv": "arXiv:1904.05862"
    },
    {
      "citation_id": "35",
      "title": "Replay spoofing detection system for automatic speaker verification using multi-task learning of noise classes",
      "authors": [
        "Hye-Jin Shim",
        "Jee-Weon Jung",
        "Hee-Soo Heo",
        "Sung-Hyun Yoon",
        "Ha-Jin Yu"
      ],
      "year": "2018",
      "venue": "Replay spoofing detection system for automatic speaker verification using multi-task learning of noise classes"
    },
    {
      "citation_id": "36",
      "title": "Speech masking speech in everyday communication: The role of inhibitory control and working memory capacity",
      "authors": [
        "Victoria Stenback"
      ],
      "year": "2016",
      "venue": "Speech masking speech in everyday communication: The role of inhibitory control and working memory capacity"
    },
    {
      "citation_id": "37",
      "title": "A hybrid dsp/deep learning approach to real-time full-band speech enhancement",
      "authors": [
        "Jean-Marc Valin"
      ],
      "year": "2018",
      "venue": "2018 IEEE 20th International Workshop on Multimedia Signal Processing"
    },
    {
      "citation_id": "38",
      "title": "Head fusion: Improving the accuracy and robustness of speech emotion recognition on the iemocap and ravdess dataset",
      "authors": [
        "Mingke Xu",
        "Fan Zhang",
        "Wei Zhang"
      ],
      "year": "2021",
      "venue": "Head fusion: Improving the accuracy and robustness of speech emotion recognition on the iemocap and ravdess dataset"
    },
    {
      "citation_id": "39",
      "title": "Automatic detection of \"g-dropping\" in american english using forced alignment",
      "authors": [
        "Jiahong Yuan",
        "Mark Liberman"
      ],
      "venue": "2011 IEEE Workshop on Automatic Speech Recognition & Understanding"
    },
    {
      "citation_id": "40",
      "title": "Robust speaker identification in noisy and reverberant conditions",
      "authors": [
        "Xiaojia Zhao",
        "Yuxuan Wang",
        "Deliang Wang"
      ],
      "year": "2014",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "41",
      "title": "Improving the robustness of deep neural networks via stability training",
      "authors": [
        "Stephan Zheng",
        "Yang Song",
        "Thomas Leung",
        "Ian Goodfellow"
      ],
      "year": "2016",
      "venue": "Proceedings of CVPR"
    }
  ]
}