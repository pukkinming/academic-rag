{
  "paper_id": "2503.05379v2",
  "title": "R1-Omni: Explainable Omni-Multimodal Emotion Recognition With Reinforcement Learning",
  "published": "2025-03-07T12:46:42Z",
  "authors": [
    "Jiaxing Zhao",
    "Xihan Wei",
    "Liefeng Bo"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "In this work, we present the first application of Reinforcement Learning with Verifiable Reward (RLVR) to an Omni-multimodal large language model in the context of emotion recognition, a task where both visual and audio modalities play crucial roles. We leverage RLVR to optimize the Omni model, significantly enhancing its performance in three key aspects: reasoning capability, emotion recognition accuracy, and generalization ability. The introduction of RLVR not only improves the model's overall performance on in-distribution data but also demonstrates superior robustness when evaluated on out-of-distribution datasets. More importantly, the improved reasoning capability enables clear analysis of the contributions of different modalities, particularly visual and audio information, in the emotion recognition process. This provides valuable insights into the optimization of multimodal large language models.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "With the advent of DeepSeek R1  [2] , the potential of Reinforcement Learning (RL) has garnered increasing attention from researchers working on large models. A key innovation introduced by DeepSeek R1 is Reinforcement Learning with Verifiable Reward (RLVR), which leverages rule-based reward mechanisms to optimize models in a highly efficient and reliable manner. This approach has demonstrated remarkable success in enhancing the capabilities of large language models (LLMs) even with limited training data. Recent studies have extended this methodology to multimodal large language models (MLLMs), further showcasing its versatility. For instance, R1-V  [1]  has been applied to tasks such as geometry reasoning and visual counting, where MLLMs trained with RLVR not only exhibit strong reasoning abilities but also achieve performance comparable to Supervised Fine-Tuning (SFT) on in-domain tests, while significantly outperforming SFT models on out-of-distribution (OOD) evaluations.\n\nIn another notable work, Visual-RFT  [6] , the authors validated the effectiveness of RLVR on classical computer vision tasks such as image classification and object detection. Their results demonstrated that RLVR consistently outperforms SFT across nearly all categories, highlighting its broad applicability and robustness. Despite these advancements, the integration of RLVR with MLLMs has thus far been limited to image-text modalities. To the best of our knowledge, no prior work has explored the application of RLVR to video-based multimodal models that incorporate richer sources of information, such as audio and dynamic visual content. Bridging this gap, we present the first exploration of RLVR in conjunction with Video Omni-multimodal Models, focusing on the task of emotion recognition-a domain where both visual and audio modalities provide critical cues for accurate predictions.\n\nTech report. Preprint.\n\nIn this study, we build upon HumanOmni  [8] , a first open-source Omni model designed for humancentric scene understanding. By applying RLVR to HumanOmni, we aim to investigate its potential in enhancing emotion recognition performance. Our findings reveal several key insights:\n\n• Enhanced Reasoning Capability: R1-Omni demonstrate superior reasoning abilities, enabling a clearer understanding of how visual and audio information contribute to emotion recognition. • Improved Understanding Capability: Compared to SFT, RLVR significantly boosts performance on emotion recognition tasks. • Stronger Generalization Capability: RLVR models exhibit markedly better generalization capabilities, particularly excelling in out-of-distribution scenarios. At its core, RLVR simplifies the reward mechanism while ensuring alignment with the inherent correctness standards of the task. Given an input question q, the policy model π θ generates a response o, which is then evaluated using a verifiable reward function R(q, o). This reward function determines whether the generated output matches the ground truth, assigning a binary score:\n\nThe optimization objective of RLVR is formulated as follows:\n\nwhere\n\nHere, π ref denotes the reference model prior to optimization, R(q, o) is the verifiable reward function, and β is a hyperparameter controlling the trade-off between maximizing the reward and maintaining proximity to the reference model via KL-divergence regularization.\n\nIn this work, we extend the application of RLVR beyond traditional domains like math and coding to the realm of multimodal emotion recognition, where both visual and audio modalities contribute to the final prediction. Specifically, we utilize the training sets from the MAFW  [5]  and DFEW  [3]  datasets, comprising a total of 15,306 video samples, to train our Omni-multimodal model. Notably, these datasets only provide annotations for emotion categories, without any explicit labels or supervision for the reasoning process. Despite this limitation, by leveraging RLVR, we aim to enhance the reasoning capabilities, performance, and generalization of the Omni-multimodal model in this challenging task.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Group Relative Policy Optimization (Grpo)",
      "text": "Group Relative Policy Optimization (GRPO) represents a novel approach to reinforcement learning that diverges from traditional methods like Proximal Policy Optimization (PPO). Unlike PPO, which relies on a critic model to evaluate the performance of candidate policies, GRPO eliminates the need for an additional critic by directly comparing groups of generated responses. This streamlined mechanism simplifies the training process while maintaining robust optimization capabilities.\n\nThe core idea behind GRPO is to assess the relative quality of multiple candidate responses within a group. For a given input question q, GRPO first generates G distinct responses {o 1 , o 2 , . . . , o G } using the current policy π θold . These responses are then evaluated based on their corresponding rewards {r 1 , r 2 , . . . , r G }, which are obtained through a predefined reward function. To determine the relative quality of each response, GRPO normalizes the rewards by computing their mean and standard deviation:\n\nwhere A i represents the normalized score indicating the relative quality of the i-th response.\n\nBy leveraging this normalized scoring mechanism, GRPO encourages the model to prioritize responses with higher reward values within the group. This approach not only reduces the dependency on external critic models but also enhances the model's ability to differentiate between high-quality and low-quality outputs effectively.\n\nFollowing the approach proposed in DeepSeek R1, we combine GRPO with RLVR to leverage the strengths of both methods. This integration allows us to achieve superior reasoning, generalization, and emotion recognition capabilities.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "R1-Omni",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Cold Start With Emer Dataset",
      "text": "To ensure the smooth training of RLVR, we adopt a cold start strategy inspired by the training approach used in DeepSeek R1. Specifically, we initialize the HumanOmni-0.5B by fine-tuning it on a combined dataset consisting of 232 samples from the Explainable Multimodal Emotion Reasoning (EMER)  [4]  dataset and 348 samples from our manually annotated HumanOmni dataset, resulting in a total of 580 video samples. This step is crucial for equipping the model with preliminary reasoning capabilities before proceeding to RLVR training.\n\nThe EMER dataset is specifically designed for explainable emotion reasoning tasks. It contains multimodal data (visual and audio) paired with detailed annotations that describe the reasoning process behind emotion recognition. By leveraging this dataset during the cold start phase, we enable the HumanOmni model to develop an initial understanding of how visual and audio cues contribute to emotion recognition. The specific format of the EMER data used during the cold start phase is presented below.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Human:",
      "text": "As an emotional recognition expert; throughout the video, which emotion conveyed by the characters is the most obvious to you? Output the thinking process in \" <think></think>\" tags and the final emotion in \"<answer></answer>\" tags. GPT: <think>In the video, in the opening scene, we see a male character with a slight frown, his gaze directly focused on the person opposite him, eyes wide open, seemingly speaking or expressing a certain viewpoint. In the final scene, the frown becomes more pronounced, and the male character's expression becomes more irritable. Based on these scenes, it can be inferred that the male character in this video is dissatisfied and angry with the other person. In the audio, the tone is high-pitched and emotional, with a fast pace, giving a sense of excitement. In the text, the subtitle reads: \"I didn't think I was that special, I just can't understand why you're treating me like this.\" This sentence expresses the male character's dissatisfaction and anger. Based on the video clues of the male character's slight frown, direct gaze towards the person opposite him, wide open eyes, and the more pronounced frown and irritable expression in the final scene, it can be inferred that the male character is dissatisfied and angry with the other person. Additionally, the audio clue describing the high-pitched and emotional tone, fast pace also supports this inference. Therefore, this sentence expresses the male character's dissatisfaction and anger towards the other person.</think> <answer>angry</answer> After completing the cold start phase, the HumanOmni model is ready for further optimization using RLVR, which will be discussed in the subsequent sections. HumanOmni-0.5B: happy EMER-SFT: <think>In the video, we see a woman in an indoor setting that appears to be a personal space or a study room. She is wearing a red headband and a beige blazer over a white top, with her hair neatly tied back. Her facial expression shows slight confusion and surprise, as if she has encountered something unexpected. As time goes on, her emotions seem to calm down, but the specific reasons for this are not explained. In the audio, there is a long silence between \"but\" and \"it was,\" indicating hesitation and uncertainty. In the text, the subtitle reads: \"But it was interesting.\" This sentence may express the confusion and surprise of the person. Based on the video clues of the woman's slight ly confused and surprised expression, as well as her overall appearance and facial expression showing minor confusion and surprise, we can infer that this sentence may have a hesitant or uncertain tone. Additionally, the audio clue describing a long silence between \"but\" and \"it wa s\" also indicates some hesitation and uncertainty. Therefore, this sentence expresses the woman's interest and curiosity in something, while a lso showing a certain level of hesitation and uncertainty.</think> <answer>surprise</answer> MAFW-DFEW-SFT: happy R1-Omni: <think>In the video, in the opening scene, we see a woman with her eyes slightly closed and mouth slowly opening as if she is l aughing. Her facial expression appears somewhat joyful, which may indicate that she is experiencing some pleasant or amusing situation at that moment. In the audio, there are no pauses between sentences, they flow continuously, and the tone is light and cheerful. Combined with the text content, it can be felt that the character is in a very happy and positive emotional state. In the text, the subtitle reads:",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Rlvr",
      "text": "In our framework, the RLVR training process is designed to optimize the HumanOmni-0.5B for emotion recognition tasks using multimodal inputs that include both video and audio data. The RLVR pipeline begins with the policy model π θ , which processes the multimodal input data comprising video frames and corresponding audio streams and generates a set of candidate responses. Each response is accompanied by a detailed reasoning process, providing insights into how the model integrates visual and auditory information to arrive at its predictions. These candidate responses are then evaluated using the verifiable reward functions.\n\nThe reward function in our RLVR framework is inspired by the design principles of DeepSeek R1, which decomposes the reward into two components: accuracy reward (R acc ) and format reward (R format ). These components are combined to form the total reward R, defined as:\n\nFigure  2 : Performance comparison of models on emotion recognition datasets.\n\nThe accuracy reward (R acc ) evaluates the correctness of the predicted emotion compared to the ground truth (GT). To generate predictions, we use a specific prompt: \"As an emotional recognition expert; throughout the video, which emotion is the most obvious to you? Output the thinking process in <think> </think> and final emotion in <answer> </answer> tags.\"\n\nThe model's output is expected to include two parts. A reasoning process enclosed within \"<think></think>\" tags, explaining how the model integrates visual and audio cues to arrive at its prediction. A final emotion label enclosed within \"<answer></answer>\" tags, representing the predicted emotion.\n\nThe accuracy reward is computed as follows:\n\nR acc = 1, if the predicted emotion matches the ground truth, 0, otherwise.\n\nThis binary scoring mechanism ensures that the model is directly incentivized to produce correct emotion predictions.\n\nThe format reward enforces strict adherence to the required output structure, ensuring that the model's predictions conform to the specified HTML-like tag format.\n\nIf the output satisfies these formatting constraints, the format reward is assigned a value of 1; otherwise, it is set to 0:\n\nThis constraint ensures that the model generates structured and interpretable outputs, facilitating downstream analysis and evaluation.\n\nBy combining these two components, the reward function not only encourages the model to produce accurate predictions but also ensures that the outputs are well-structured and aligned with the desired format.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Experiments",
      "text": "In this section, we present the experimental results to validate the effectiveness of our RLVR in enhancing the capabilities of the HumanOmni-0.5B. We compare R1-Omni (RLVR-trained) with three models. 1. HumanOmni-0.5B. Our experiments systematically evaluate the performance of the R1-Omni and reveal three key strengths: (1) it demonstrates enhanced reasoning capability by generating detailed and interpretable explanations for its predictions; (2) it achieves improved understanding of multimodal data, resulting in higher accuracy in emotion recognition tasks; and (3) it exhibits stronger generalization to out-ofdistribution data, showcasing robustness across diverse scenarios.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Enhanced Reasoning Capability",
      "text": "One of the most significant advantages of our R1-Omni is its superior reasoning ability. To illustrate the reasoning capability of R1-Omni, we present a set of visualization examples in Figure  1 , comparing its outputs with those of three other models. These examples provide an intuitive sense of how R1-Omni performs relative to other approaches in terms of reasoning coherence and interpretability.\n\nWe can observe that the original HumanOmni-0.5B and the MAFW-DFEW-SFT models exhibit limited reasoning capabilities. While EMER-SFT demonstrates some level of reasoning ability, its reasoning process suffers from relatively poor coherence and is prone to generating hallucinations. For example, when tested on the MAFW and DFEW datasets, the R1-Omni consistently outperforms these baselines by providing more coherent, accurate, and interpretable reasoning processes. This enhanced reasoning capability not only improves the model's overall performance but also offers deeper insights into how visual and audio information interact in emotion recognition tasks.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Improved Understanding Capability",
      "text": "To quantitatively evaluate the understanding capability of the R1-Omni, we compared its performance with other models on the MAFW and DFEW datasets. The metrics used for evaluation are Unweighted Average Recall (UAR) and Weighted Average Recall (WAR), which measure the model's ability to accurately classify emotions across different classes. Importantly, all evaluations were conducted using an open-vocabulary emotion testing (OV-emotion) protocol. In this setting, the model is not provided with predefined emotion categories but instead generates the emotion labels directly from the input data.\n\nThe results of the UAR and WAR metrics on the MAFW and DFEW datasets are summarized in Table  1 .\n\nFor a more intuitive comparison, please refer to Figure  2 .\n\nFrom the results presented in Table  1 , we observe that The R1-Omni consistently outperforms other models on both datasets, achieving the highest UAR and WAR scores. The experimental results highlight the superior performance of the R1-Omni model compared to the SFT on MAFW and DFEW model. On the DFEW dataset, the R1-Omni achieves a UAR of 65.83% and a WAR of 56.27%, surpassing the SFT model's performance of 60.23% UAR and 44.39% WAR. Similarly, on the MAFW dataset, the R1-Omni demonstrates significant improvements with a UAR of 57.68% and a WAR of 40.04%, outperforming the SFT model, which achieves only 50.44% UAR and 30.39% WAR. These results clearly demonstrate that the RLVR approach not only leverages task-specific data more effectively but also enhances the model's general understanding and reasoning capabilities, leading to consistently higher performance across both datasets.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Stronger Generalization Capability",
      "text": "To evaluate the generalization capability of the R1-Omni, we conducted experiments on the RAVDESS  [7]  dataset, which serves as an out-of-distribution (OOD) test set. Unlike the MAFW and DFEW datasets, which consist primarily of movie clips, the RAVDESS dataset features professional actors vocalizing lexically-matched statements in a neutral North American accent. This significant difference in data distribution makes RAVDESS an ideal benchmark for assessing the model's ability to generalize to unseen scenarios. From his actions and expressions, it can be inferred that he is engaged in a tense conversation or argument. In the audio, there is a high pitch and fast pace at the beginning of the sentence, accompanied by a loud tone when expressing \"the other's word.\" This indicates that the chara cter is very angry and emotionally excited. In the text, the subtitle reads: \"You're just a petty official, let's have a fight!\" This sentence expre sses the character's anger and excitement. Based on the video clues of the fierce facial expression, leaning posture, and rigid body, as well as the description of a high-pitched and fast-paced tone when expressing \"the other's word,\" we can infer that this sentence carries a sense of a nger and agitation. The character's intense emotional state and aggressive attitude are clearly displayed through his facial expressions, body language, and vocal characteristics.  From the experimental results presented in Table  1 , it is evident that the R1-Omni model demonstrates a substantial improvement over the MAFW-DFEW-SFT model on the RAVDESS dataset. Specifically, the R1-Omni model achieves a UAR of 43.00% and a WAR of 44.69%, surpassing the SFT model's performance of 29.33% UAR and 30.75% WAR. This significant gap highlights the superior generalization capability of the R1-Omni.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Limitations",
      "text": "Despite the significant improvements achieved by the R1-Omni, there remain several limitations that warrant further investigation. To illustrate these challenges, we present three representative examples in Figure  3 .",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Inaccurate Subtitle Recognition",
      "text": "In the first example, although the model produces a correct emotion prediction, we observe that inaccuracies in subtitle recognition remain a potential limitation. This issue arises because neither the HumanOmni base model nor the subsequent SFT and RLVR training processes explicitly focus on improving subtitle recognition capabilities. Addressing this limitation will require integrating more robust subtitle processing techniques, such as fine-tuning on specialized datasets or incorporating advanced natural language understanding models.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Hallucination In Reasoning",
      "text": "The second example demonstrates a common issue hallucination, where the model generates reasoning outputs that are not grounded in the actual content of the video. For instance, the statement \"The voiceover reveals her neutral initial reaction, which gradually turns into mild excitement and anger over time\" does not align with the video's actual emotional trajectory. This fabricated reasoning leads the model to incorrectly predict the emotion as surprise, highlighting the need for mechanisms to ensure the model's outputs remain faithful to the input data.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Underutilization Of Audio Cues",
      "text": "The third example underscores the model's limited ability to fully utilize audio cues, such as tone and intonation, which are critical for accurate emotion recognition. Although our model is capable of reasoning about emotions by integrating both audio and visual information, it appears that in certain cases, the use of audio features is not as thorough or effective as the use of visual cues. In this specific instance, the character's vocal delivery provides strong emotional signals, yet the model fails to adequately incorporate these nuances into its reasoning process.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Implications For Future Research",
      "text": "The limitations identified in our analysis highlight several promising directions for future research to further enhance the capabilities of R1-Omni. Specifically, we propose the following key areas of exploration:\n\n1. Strengthening the Foundation Model's Capabilities While RLVR significantly enhances the reasoning and generalization abilities of the base model, the inherent performance of the foundation model remains a critical determinant of overall success. Therefore, continuous efforts to improve the underlying Omni model such as through larger-scale pretraining, more diverse datasets, or advanced architectural designs are essential to unlock the full potential of RLVR-based approaches.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Mitigating Hallucination In Reasoning Outputs",
      "text": "Due to the inherent challenges of multimodal data, such as the weaker causal relationships within video and audio tokens compared to text tokens, as well as the lack of explicit supervision for reasoning content, hallucinations can occur during the model's reasoning process. These inaccuracies not only degrade performance but also negatively impact user experience. Developing mechanisms to detect and mitigate hallucinations will be crucial for improving the reliability and usability of the model.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Enhancing Audio Cue Utilization",
      "text": "The underutilization of audio cues, such as tone and intonation, represents a limitation in the current model. Future work should focus on improving the model's ability to extract and integrate audio features effectively.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Enhancing Reasoning Depth And Emotional Intelligence",
      "text": "The current reasoning process tends to be somewhat mechanistic, focusing primarily on directly observable features such as visual cues and audio signals. However, human emotion recognition often involves deeper psychological insights, such as understanding the motivations, intentions, or internal states of individuals. By guiding the model to explore more nuanced aspects of reasoning, such as inferring psychological activities or emotional drivers, we can elevate its emotional intelligence and enhance its ability to capture complex emotional dynamics. This advancement would enable the model to better simulate human-like empathy and reasoning in real-world scenarios.",
      "page_start": 8,
      "page_end": 8
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Visualization comparison.",
      "page": 4
    },
    {
      "caption": "Figure 2: Performance comparison of models on emotion recognition datasets.",
      "page": 5
    },
    {
      "caption": "Figure 2: From the results presented in Table 1, we observe that The R1-Omni consistently outperforms other",
      "page": 6
    },
    {
      "caption": "Figure 3: Challenging Cases.",
      "page": 7
    }
  ],
  "tables": [
    {
      "caption": "Table 1: Results on emotion recognition datasets.",
      "page": 6
    },
    {
      "caption": "Table 1: For a more intuitive comparison, please refer to Figure 2.",
      "page": 6
    },
    {
      "caption": "Table 1: , we observe that The R1-Omni consistently outperforms other",
      "page": 6
    },
    {
      "caption": "Table 1: , it is evident that the R1-Omni model demon-",
      "page": 7
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "R1-v: Reinforcing super generalization ability in vision-language models with less than $3",
      "authors": [
        "Liang Chen",
        "Lei Li",
        "Haozhe Zhao",
        "Yifan Song",
        "Vinci"
      ],
      "venue": "R1-v: Reinforcing super generalization ability in vision-language models with less than $3"
    },
    {
      "citation_id": "2",
      "title": "Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning",
      "year": "2025",
      "venue": "Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning"
    },
    {
      "citation_id": "3",
      "title": "Dfew: A large-scale database for recognizing dynamic facial expressions in the wild",
      "authors": [
        "Xingxun Jiang",
        "Yuan Zong",
        "Wenming Zheng",
        "Chuangao Tang",
        "Wanchuang Xia",
        "Cheng Lu",
        "Jiateng Liu"
      ],
      "year": "2020",
      "venue": "Proceedings of the 28th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "4",
      "title": "Explainable multimodal emotion recognition",
      "authors": [
        "Zheng Lian",
        "Haiyang Sun",
        "Licai Sun",
        "Hao Gu",
        "Zhuofan Wen",
        "Siyuan Zhang",
        "Shun Chen",
        "Mingyu Xu",
        "Ke Xu",
        "Kang Chen",
        "Lan Chen",
        "Shan Liang",
        "Ya Li",
        "Jiangyan Yi",
        "Bin Liu",
        "Jianhua Tao"
      ],
      "year": "2024",
      "venue": "Explainable multimodal emotion recognition"
    },
    {
      "citation_id": "5",
      "title": "MAFW: A Large-scale, Multi-modal, Compound Affective Database for Dynamic Facial Expression Recognition in the Wild",
      "authors": [
        "Yuanyuan Liu",
        "Wei Dai",
        "Chuanxu Feng",
        "Wenbin Wang",
        "Guanghao Yin",
        "Jiabei Zeng",
        "Shiguang Shan"
      ],
      "year": "2022",
      "venue": "MAFW: A Large-scale, Multi-modal, Compound Affective Database for Dynamic Facial Expression Recognition in the Wild"
    },
    {
      "citation_id": "6",
      "title": "Visual-rft: Visual reinforcement fine-tuning",
      "authors": [
        "Ziyu Liu",
        "Zeyi Sun",
        "Yuhang Zang",
        "Xiaoyi Dong",
        "Yuhang Cao",
        "Haodong Duan",
        "Dahua Lin",
        "Jiaqi Wang"
      ],
      "year": "2025",
      "venue": "Visual-rft: Visual reinforcement fine-tuning"
    },
    {
      "citation_id": "7",
      "title": "The ryerson audio-visual database of emotional speech and song (ravdess): A dynamic, multimodal set of facial and vocal expressions in north american english",
      "authors": [
        "Steven Livingstone",
        "Frank Russo"
      ],
      "venue": "PLOS ONE"
    },
    {
      "citation_id": "8",
      "title": "A large vision-speech language model for human-centric video understanding",
      "authors": [
        "Jiaxing Zhao",
        "Qize Yang",
        "Yixing Peng",
        "Detao Bai",
        "Shimin Yao",
        "Boyuan Sun",
        "Xiang Chen",
        "Shenghao Fu",
        "Xihan Wei",
        "Liefeng Bo"
      ],
      "year": "2025",
      "venue": "A large vision-speech language model for human-centric video understanding",
      "arxiv": "arXiv:2501.15111"
    }
  ]
}