{
  "paper_id": "2312.15583v3",
  "title": "Iteach-Net: Inverted Teacher-Student Search Network For Emotion Recognition In Conversation",
  "published": "2023-12-25T01:57:22Z",
  "authors": [
    "Haiyang Sun",
    "Zheng Lian",
    "Chenglong Wang",
    "Kang Chen",
    "Licai Sun",
    "Bin Liu",
    "Jianhua Tao"
  ],
  "keywords": [
    "emotion recognition in conversation",
    "incomplete multimodal learning",
    "emotion context changing",
    "inverted teacher-student",
    "neural architecture search"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Due to the widespread applications of conversations in human-computer interaction, Emotion Recognition in Conversation (ERC) has attracted increasing attention from researchers. Although many studies have achieved notable results, there remain two critical challenges that hinder the development of ERC. Firstly, there is a lack of exploration into mining deeper insights from the data itself for conversational emotion tasks. Secondly, the systems exhibit vulnerability to random modality feature missing, which is a common occurrence in realistic settings. Focusing on these two key challenges, we propose a novel framework for incomplete multimodal learning in ERC, called \"Inverted Teacher-studEnt seArCH Network (ITEACH-Net).\" ITEACH-Net comprises two novel components: the Emotion Context Changing Encoder (ECCE) and the Inverted Teacher-Student (ITS) framework. Specifically, leveraging the tendency for emotional states to exhibit local stability within conversational contexts, ECCE captures these patterns and further perceives their evolution over time. Recognizing the varying challenges of handling incomplete versus complete data, ITS employs a teacher-student framework to decouple the respective computations. Subsequently, through Neural Architecture Search, the student model develops enhanced computational capabilities for handling incomplete data compared to the teacher model. During testing, we design a novel evaluation method, testing the model's performance under different missing rate conditions without altering the model weights. We conduct experiments on three benchmark ERC datasets, and the results demonstrate that our ITEACH-Net outperforms existing methods in incomplete multimodal ERC. We believe ITEACH-Net can inspire relevant research on the intrinsic nature of emotions within conversation scenarios and pave a more robust route for incomplete learning techniques. Codes will be made available.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Introduction",
      "text": "Emotion Recognition in Conversation (ERC) aims to analyze the emotional states of speakers in conversational contexts. It has found extensive applications in areas such as social media analysis and human-computer interaction  [1, 2, 3] . Recent studies have achieved promising results in ERC  [4, 5, 6] . However, there are still some limitations that need to be addressed.\n\nFirst, unlike sentence-level emotion understanding tasks, conversations feature longer sequences and important context information. Long-sequence conversations often entail complex context changes, the model need to carefully consider these changes to better understand conversation data. While many works have strengthened the ability of models to process context information  [7, 8, 9, 10] , these methods are general in conversational tasks. They have not deeply considered the unique patterns of emotional expression in conversations, and there is an inability to leverage such prior knowledge to fully exploit data.\n\nSecond, real-world scenarios introduce various challenges that impact the collection of multimodal data. Factors such as background noise in speech, obstacles in images, and recognition errors in text may result in the omission of modal information, introducing uncertainty to the completeness of the data. Consequently, the model must exhibit robustness in addressing incomplete data. Although many works have performed well in incomplete learning  [11, 12, 13, 14] , they overlook the difference in computational power requirements between models for incomplete data tasks and complete data tasks, failing to endow models with sufficient capability to handle missing data.\n\nThird, the disturbances data encounters are uncertain. Therefore, when evaluating the robustness of models, it is essential to consider their performance in scenarios where the incompleteness of the data dynamically changes. Existing evaluation methods, which involve training model weights separately for testing under various missing rate scenarios  [15, 16, 17, 18] , fail to authentically reflect the model's robustness.\n\nIn this paper, we propose a novel framework for incomplete learning in ERC, called \"Inverted Teacher-studEnt seArCH Network (ITEACH-Net)\". Figure  1  illustrates the overall structure of our method. To better understand the unique patterns in emotional conversations, as described in Figure  3 , we design a novel encoder based on the context changes that enables the model to simultaneously consider them from both local and global perspectives, called Emotion Context Change Encoder (ECCE). To enhance the model's ability to handle missing data, we propose a novel training framework, called Inverted Teacher-Student (ITS), which allows the student model to utilize computationally intensive techniques, such as Neural Architecture Search (NAS)  [19, 20, 21, 22] , to acquire knowledge from the teacher trained on complete data, thereby enhancing its capability in processing incomplete data.\n\nTo verify the actual robustness of models, we propose a novel evaluation method to simulate dynamic data missing scenarios and conduct experiments on three benchmark ERC datasets. Specifically, we fix the model weights and assess its performance across different missing rates.\n\nThrough quantitative and qualitative analysis, we demonstrate that our ITEACH-Net outperforms currently advanced approaches. The main contributions of this paper are summarized as follows:\n\n• We introduce a focus on local stable emotional patterns within conversational data, and design a novel encoding method, called Emotion Context Change Encoder, that enables the model to simultaneously consider them from both local and global perspectives.\n\n• We propose a novel Inverted Teacher-Student training method as the foundational framework for incomplete data, which decouples the computation of complete and incomplete data, and leverages NAS to endow the student model with stronger encoding capabilities, thereby improving its robustness in the face of incomplete data.\n\n• Unlike existing works that train individual models for specific missing rates, we propose a novel evaluation approach, namely assessing the average performance of a unified model across all missing rates. This evaluation method is closer to the dynamic data missing scenario and fills the gap in current research.\n\n• Experimental results on three benchmark datasets demonstrate that our method, ITEACH-Net, is superior to existing state-of-the-art approaches in ERC.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Related Work",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Context Modeling In Conversation",
      "text": "In comparison to sentence-level emotion recognition tasks, context information in conversations is extremely crucial  [7] . Utilizing temporal modeling methods to consider context information is simple and effective, whether using positional encoding  [23]  or temporal models  [24, 25, 8] , both can do this. Based on these methods, many works have further enhanced the modeling of context information. Yang et al.  [26]  introduce context information by embedding previous statements between interlocutors, thereby enhancing the emotional representation of the current utterance. Liu et al.  [27]  use a BLSTM  [28]  layer to learn long-term dependencies and utterance-level context information,  and a multi-head self-attention layer to make the model focus on the features most related to the emotions. Song et al.  [9]  innovatively introduce the concept of global emotional atmosphere, using the most dominant emotion in the conversation as guiding information, and use graph neural networks  [29]  and GRUs  [30]  in a concatenated framework to consider local and long-term context information. However, these context modeling methods are universally applicable across various conversational tasks, they do not delve into the unique characteristics inherent in conversational emotional data. Further, Liu et al.  [10]  consider context modeling from the perspectives of emotional inertia and contagion, designing two independent modules to capture identity-based global context dependencies and extract speaker-and temporal-aware local context information. Tu et al.  [31]  enhance the model's ability to capture emotional information by starting from the dependency relationship between commonsense knowledge and context. Peng et al.  [32]  explore the interrelationship between intention, action, and emotion, while Ai et al.  [33]  use graph neural networks to model the dependencies between speakers and events in conversations. These methods, although considering the characteristics of emotional information in conversational tasks, primarily focus on the association with external knowledge and lack an in-depth exploration of the intrinsic properties of the data itself.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Incomplete Multimodal Learning",
      "text": "Learning from incomplete multimodal data is a fundamental research area in machine learning. One simple approach involves data imputation, followed by utilizing existing classification methods on the imputed data. Adding zero vectors or using the mean values of the same modality within a class as missing data are the most straightforward methods  [34, 35, 36] . However, the performance of such approaches inevitably reaches an upper limit, as they cannot leverage deeper multimodal correlations to improve downstream task performance. Consequently, some studies start leveraging the correlations between different modalities to optimize this process, manifesting as a low-rank data matrix resulting from high correlation  [37, 38] . Many works, therefore, project the data into a common space using low-rankness  [39, 38] . These methods have shown promising results in incomplete multimodal learning.\n\nHowever, with the advancement of deep learning, methods based on deep learning have significantly surpassed the aforementioned approaches. For instance, Duan et al.  [40]  utilize autoencoders to estimate missing data, achieving commendable results in incomplete multimodal learning. Tran et al.  [41]  further propose the Cascaded Residual Autoencoder (CRA) to enhance the imputation capability of autoencoders. Additionally, Zhao et al.  [11]  combine CRA with cycle-consistency loss for cross-modal imputation, outperforming existing methods. Furthermore, Zuo et al.  [17]  and Sun et al.  [14]  enhance the imputation ability of autoencoders for missing data from both unimodal and multimodal perspectives, or both high-and low-level features, achieving outstanding performance. Meanwhile, Lian et al.  [13]  delve deeper into considering temporal and speaker information in conversations, achieving state-of-the-art performance in ERC. Although these works have shown excellent performance in handling incomplete data, using the same network structure to model both complete and incomplete data means that the model lacks additional capabilities to strengthen learning from incomplete data.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Method",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Input Format",
      "text": "In this task, a conversation C = {(u i , y i )} L i=1 consists of L utterances, and the i th utterance u i with a label y i . For each utterance, we extract features x m ∈ R 1×d m , m ∈ {t, a, v}, from text, audio and visual modalities, where d represents the feature dimension, and concatenate them in the order of speaking to obtain X m ∈ R L×d m , to represent the entire conversation.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Inverted Teacher-Student",
      "text": "To ensure that the model achieves performance akin to that with complete data when processing incomplete information, existing approaches primarily focus on encoding the incomplete data into deep representations to approximate those derived from complete data. This introduces additional computational burden to the model in a multitask format, necessitating greater computational capacity than models operating on complete data. Therefore, we introduce the Teacher-Student framework to decouple these two computations, where the teacher model is trained on complete data, and the student model focuses on utilizing incomplete data to emulate the teacher model's performance. Furthermore, we design an Inverted Teacher-Student framework, providing the student model with a more complex structure than the teacher, enabling it to possess sufficient computational capacity to follow the teacher's performance.\n\nTeacher-Student training framework is designed to distill knowledge from a complex teacher model into a simpler student model  [42, 43, 44] , Inverted Teacher-Student serves as a framework allowing a complex student to emulate the performance of a simple teacher. Our framework is illustrated in Figure  1 .\n\nWe define complete multimodal features as {X t , X a , X v } and incomplete multimodal data as {X s t , X s a , X s v }. We first capture their emotion context changing information:\n\nI just-I know I don't make you happy.\n\nYou're the one ……, who could enjoy herself.\n\nYou know, actually now that you mention it, no I don't.\n\nSo maybe we are in the wrong spot, but we are with the right person.\n\nWe have a great view of the moon from here, though, huh?\n\nwhere E m ∈ R L×L . Subsequently, they pass through multiple Encoder Layers in their respective models, generating hidden features under the guidance of E m :\n\nL tea and L stu denote the numbers of Encoder Layers for the Teacher Model and Student Model, respectively. It's worth noting that, for ease of understanding, we label the hidden feature closest to the classification layer as H 1 . To enhance the computing capability of the Student Model, we allocate more Encoder Layers to it.\n\nIn the end, the final layer of hidden features H 1 and H s 1 is employed by the classifier to predict emotion labels. Simultaneously, the masked portions of the last three layers of hidden features from the Student Model, {H s 1 , H s 2 , H s 3 }, are computed for distance loss, aiming to acquire the knowledge from the Teacher Model, {H 1 , H 2 , H 3 }.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Emotion Context Changing Encoder",
      "text": "As depicted in Figure  3 , the emotional state of speakers in a conversation tends to maintain a stable pattern locally, which evolves with context changes. To enhance model's perception of these dynamics, we design a novel encoding method, Emotion Context Changing Encoder (ECCE). The encoder consists of two specific steps: encoding local emotion context and encoding global context changing.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Encoding Local Emotion Context",
      "text": "Given a context window size w, for the i th utterance, the calculation process for its local context feature z i m can be described as follows:\n\nF m local represents a replaceable function used for processing features within the window. After calculating for each utterance, we obtain the local context feature Z m ∈ R L×d z .",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Encoding Global Context Changing",
      "text": "The local context feature considers only the emotion context within a limited window. However, to account for the dynamics of the context changing over time, further calculations are needed for the Z m . For the context changing feature e i, j m from the i th to j th utterances, the calculation process can be described as follows:\n\nF m global represents a replaceable function used for processing context features. During computation, the maximum allowable distance dist m between i and j can be set to mitigate excessive computational complexity. After calculating for each utterance pair, we obtain the context changing feature E m ∈ R L×L×d e .\n\nAlgorithm 1 presents the pseudocode for the ECCE. In our experiments, we use a 1D convolution operation as F local and a straightforward average pooling operation as F global .",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Teacher-Student Model",
      "text": "To enhance the model's perception of complex contexts in conversations, we leverage emotion context changing information E m to guide the modeling of single-modal sequence information. Additionally, to improve the model's robustness for incomplete data, we propose the use of NAS to enhance the mimetic capabilities of the student model. In this section, we will provide a detailed explanation of these processes. As illustrated in Figure  2 , the Encoder Layer and ITEACH Encoder subgraphs depict the model we designed, while the Token Mixer subgraph illustrates the guidance process of E m .",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Encoder Layer",
      "text": "As shown in Figure  2 , multimodal features undergo encoding in their respective ITEACH Encoder at each Encoder Layer. Subsequently, the Cross Attention module merges various modal information into text features. The fused text features will serve as the latent vector H in Figure  1 . Subsequently, the three modal data will continue to be fed into the next Encoder Layer. The calculation process of Cross Attention can be described as follows:\n\nC represents the dimension of q. In both rounds of Cross Attention modules, text features consistently serve as q, while audio and visual features respectively function as k and v for the two iterations.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Iteach Encoder",
      "text": "In the design of the ITEACH Encoder, we retain the overall framework of Metaformer  [45]  and conduct a search for the Token Mixer  [20] . Unlike previous workes, to maximize the model's encoding capacity, we utilize the Router module  [46]  to extend the architecture search process to input-dependent. As shown in Figure  4 , different inputs can choose different architectures as needed, rather than opting for a single architecture for all inputs. Given the candidate module set O, where |O| represents the number of modules, the search process through the Router can be described as follows:\n\nwhere X m α ∈ R L×|O| and output ∈ R L×d m . While there have been numerous excellent designs for Token Mixers recently, our focus is on the introduction of NAS methods rather than algorithm design. Therefore, we select 4 simple and lightweight operations as candidates. Please refer to the appendix for a detailed description of them.\n\nIn our experiments, when the Router is a linear layer and the Token Mixer comprises the selected four candidates, we denote the ITEACH Encoder as NAS. Conversely, when the Router is an Identity operation and the Token Mixer includes only Self-Attention, we denote it as Transformer.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Guidance Of Ecce",
      "text": "The role of the Token Mixer is to interact with information from different utterances in conversation. A category of mixers, represented by attention operation, typically generates a Mix Map, which is normalized and used as weights for fusing utterance feature. The E m serves to adjust the Mix Map, thereby guiding the interaction of sequential information  [47] . The specific calculation process can be described as follows:",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Joint Optimization",
      "text": "To simultaneously train the model for both the target task and handling incomplete data, we jointly optimize the emotion loss, L emo , and the distance loss, L rec .\n\nConsistent with prior work, we employ cross-entropy loss as the emotion loss for labeled category data. The calculation formula is shown as follows:\n\ny i ∈ R y is the true label. For labeled dimensional value data, we use the mean square error as the emotion loss.\n\nThe predicted values for this data are transformed into category labels based on a domain range conversion before evaluating classification performance. The loss calculation formula is shown as follows:\n\nTo enable the student model to better emulate the teacher model, we simultaneously calculate the vector distance loss for its last three layers of hidden features. We assign a progressively weighted adjustment to the model's attention to each layer's reconstruction loss:\n\nThe hidden layer vectors of the teacher model will be treated as constants during the calculation; therefore, the reconstruction loss will not backpropagate any gradients to the teacher model. Finally, we merge these two loss functions into a unified objective function. This combined loss is employed to optimize all trainable parameters in an end-to-end manner.\n\n4. Experiments",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Datasets",
      "text": "In this section, we provide a comprehensive overview of the dataset's content and the data preprocessing methods employed in our study.\n\nCMU-MOSI  [48]  contains 2,199 utterance-level video clips from 93 conversations. Each video clip is annotated with an emotion intensity score ranging from -3 (strongly negative) to 3 (strongly positive).\n\nCMU-MOSEI  [49]  is an iterative version of CMU-MOSI and includes 23,453 utterance-level video clips from 3225 conversations with the same annotation format as CMU-MOSI.\n\nIEMOCAP  [50]  has two annotation formats: a four-class format and a six-class format. The four-class format consists of 5,531 utterance-level video clips from 151 conversations, with each clip labeled as neutral, happy, sad, or angry. The six-class format includes 7,433 utterance-level video clips from 151 conversations, with additional labels for excited and frustrated emotions.\n\nFollowing  [13]   1  , we extract audio features using wav2vec  [51] , text features using DeBERTa  [52] , and visual features using MA-Net  [53] . Similar to  [13] , taking the case of 70% data missing as an example, when reading each mini-batch, each modality randomly resets 70% of the data to zero, but at least one modality is retained for each utterance.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Evaluation Metrics",
      "text": "Following  [13] , for CMU-MOSI and CMU-MOSEI, we use MAE as the loss function and select the model based on the best weighted average F1-score (WAF) on the validation set, reporting its WAF on the test set. For IEMOCAP, we adopt a 5-fold leave-one-session strategy and report the mean of the best WAF on the test sets. WAF is computed by excluding the samples with label 0.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Evaluation Method",
      "text": "When evaluating the robustness of a model, it is crucial to consider its performance across various missing scenarios. Previous studies would define different scenarios, assuming that incidental factors hindering data collection would impact data at a fixed rate  [16, 13, 14] . Consequently, these works masked training and testing data at the same rate to simulate incomplete data scenarios, allowing them train the model and test its performance under this missing rate. This process will repeat with different missing rates to ensure experimental coverage of diverse missing scenarios. In this paper, we refer to this evaluation method as Independent Model Evaluation (IME).\n\nHowever, in real-world scenarios, the aforementioned assumption is invalid, as the occurrence of incidental factors is uncertain. Therefore, the testing data that the model needs to handle will not maintain a fixed missing rate. To fill this gap, we propose a more realistic evaluation method called Unified Model Evaluation (UME). Once we obtain a well-trained model, we assume it will face a dynamic missing data scenario, where the missing rate of the testing data is no longer fixed. Specifically, we mask the testing data at a fixed missing rate and then test the model's performance under this missing rate without optimizing its weights. Subsequently, this process will repeat with different missing rates, and the average is taken as a comprehensive performance metric for the robustness.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Setup",
      "text": "We conduct experiments on all datasets using a unified setting, incorporating three modalities: text, audio, and video. The kernelsize of the F local is set to 7, and dist m is set to 30. Each Encoder Layer has a hidden dimension of 128, an FFN hidden layer ratio of 4.0, and Self-Attention with 8 head nums. The weights of Teacher model and Student model are optimized using AdaW with a learning rate of 5e-4, and Router are optimized using AdaW with a learning rate of 5e-3. The model is trained using a batch size of 32 for 100 epochs. We run each model three times and report average results. It's worth noting that, in this paper, unless otherwise specified, the Teacher model is uniformly set to three Encoder Layers, while the Student model is uniformly set to four.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Baselines",
      "text": "To evaluate the performance of our proposed ITEACH-Net, we consider the following state-of-the-art incomplete multimodal learning methods as baselines.\n\nCPM-Net  [36]  projects all samples into a common space without considering missing patterns. It learns wellstructured features by equipping with a clustering-like classification loss.\n\nAE  [54]  employs an autoencoder to estimate missing data from partially observed inputs and jointly optimizes the autoencoder's reconstruction loss with the classification loss of the downstream task.\n\nCRA  [41]  combines a series of residual autoencoders into a cascading architecture for data imputation and jointly optimizes imputation and downstream tasks end-to-end.\n\nMMIN  [11]  combines CRA with cycle consistency learning to predict the latent representations of missing modalities, exhibiting excellent performance under various missing conditions.\n\nGCNet  [13]  employs graph neural networks to model speaker and context information separately, and uses LSTM to model temporal information, finally jointly optimizing for imputation and downstream tasks. This method outperforms others in incomplete learning for ERC.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Results",
      "text": "",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Emotion Context Changing Encoder Performance",
      "text": "Recognizing the unique pattern of context changes in Emotion Recognition in Conversational (ERC), we propose a novel encoder called Emotion Context Change Encoder (ECCE). This method takes into account context change information from both local and global perspectives. To comprehensively analyze the performance of ECCE, we compare the performance of teacher models under different settings, as ECCE is not influenced by data missing. The results are presented in Table  1 , where \"Local\" denotes the encoding of local emotion context, and \"Global\" signifies the encoding of global context changes. As the local emotion context information cannot be directly utilized, we are unable to conduct independent performance testing on it. Firstly, the incorporation of ECCE, derived from two-stage encoding involving Local and Global encoding, exhibits a consistent improvement in model performance across the four tasks compared to vanilla Self-Attention. Simultaneously, forgoing Local stage encoding and directly utilizing utterance features for encoding global context changes results in a decline in model performance across the four tasks.\n\nFurthermore, as illustrated in Figure  5 , we employ heatmaps for a more intuitive analysis of ECCE. Figure  5 (a) depicts the attention map generated by vanilla Self-Attention, Figure  5  As shown in Figure  5 (a), the vanilla Self-Attention, when computing attention maps, tends to allocate higher weights to certain utterances that are more crucial throughout the entire conversation. However, it fails to consider the context in which the two utterances are situated and the changing between them. In contrast, the content in Figure  5 (b) demonstrates a certain degree of local attention capability. However, this attention behavior appears somewhat ambiguous and does not accurately discern the emotional context.",
      "page_start": 9,
      "page_end": 10
    },
    {
      "section_name": "Inverted Teacher-Student Performance",
      "text": "Dealing with incomplete data poses a significantly greater challenge than handling complete data. However, current methods do not equip models for handling incomplete data with more stronger abilities to follow the performance of models trained with complete data. Therefore, we propose the Inverted Teacher-Student traning framework, which decouples the two computations, and employs a student model with complex structures to handle incomplete data while emulating the performance of a teacher model with simpler structures trained with complete data. To comprehensively analyze the performance of ITS, we compare different training frameworks under different settings. The results are presented in Table  2 , where the \"PRE\" represents previous training framework, which uses only one model to handle complete and incomplete data during the training phase. The \"TS\" training framework represents the vanilla Teacher-Student framework, where the teacher model is more complex than the student model. \"TF\" is an abbreviation for Transformer. It's worth noting that, in the TS training framework, the teacher model is equipped with four encoder layers, while the student model is equipped with three.\n\nFirstly, the performance of the Transformer under PRE training framework is not satisfactory. It not only fails to achieve outstanding performance with complete data but also exhibits poor capability in handling incomplete data. The IST training framework brings about a noticeable improvement in performance. The student model, guided by a simpler teacher model, not only performs better with complete data, surpassing the baselines across all four tasks, but also demonstrates significant performance improvements in handling incomplete data. In transitioning from complete data to a severe missing scenario with a 70% data loss, the ITS training framework exhibits less performance degradation. Compared to the PRE training framework, ITS incurs a smaller loss of 2.88% in the four-class IEMOCAP task, 5.24% in the six-class IEMOCAP task, and 1.28% in the CMU-MOSI task. While it shows a slight disadvantage in the CMU-MOSEI task, the difference is negligible due to the similar performance under various missing rates for both frameworks. These results demonstrate the shortcomings of the PRE training framework, while the ITS training framework can better enhance the model's robustness when dealing with incomplete data. Furthermore, we propose the efficient augmentation of the student model's complexity using Neural Architecture Search (NAS), aiming to enhance its capability in handling incomplete data. While the introduction of NAS doesn't necessarily result in superior performance on complete data, it significantly enhances the model's capability to handle incomplete data. The student model enhanced through NAS achieved the smallest performance decline across all four tasks, with reductions of 3.63% in the four-class IEMOCAP, 3.08% in the six-class IEMOCAP, 15.97% in CMU-MOSI, and 6.50% in CMU-MOSEI. This substantial surpasses the previous settings. These results effectively demonstrate that compared to models processing complete data, models handling incomplete data require stronger computational power to process the incomplete data more effectively. Even when trained alongside the same teacher, student models with greater computational capabilities will perform better.\n\nFinally, we present additional results showcasing the performance of a simple student model under the guidance of a complex teacher model. The results indicate that a simple student model struggles to effectively acquire the knowledge from the teacher model, resulting in a more pronounced decline in performance. This comparison highlights the disparity between the ITS and vanilla teacher-student training frameworks in incomplete learning tasks.",
      "page_start": 10,
      "page_end": 11
    },
    {
      "section_name": "Distance Loss Performance",
      "text": "Within the ITS framework, the student model primarily acquires knowledge from the teacher model through distance loss. To further investigate the benefits of distance loss on model robustness, we compare the vector mean absolute distance between the deep encodings H s 1 of student models of varying complexities and the encoding H 1 of the teacher model. The results are presented in Figure  6 , where the red lines signifie the student model under the NAS setup, and the blue lines represent the student model in the Transformer configuration, with the x-axis denoting the data missing rate.\n\nIt is observable that, with the exception of the CMU-MOSEI task, the student models without NAS enhancement exhibit superior performance in predicting the hidden layer representations of the teacher model. However, while the NAS-enhanced student models show competitive reconstruction performance on the CMU-MOSEI task at data missing rates of 10% to 40%, they significantly underperform under conditions of extreme data incompleteness.\n\nIntegrating the findings with the results presented in Table  2 , it is evident that a more robust student model performs poorly in reconstruction tasks. Therefore, we can conclude that while the knowledge imparted by the teacher model within the ITS framework is crucial, it only serves as a guide for the student model. In this guidance process, the student model can gradually learn the correct way to handle incomplete data suitable for itself, rather than merely focusing on imitating the teacher model. This also reveals the difference between the ITS method and imputationbased methods, where reconstruction performance does not represent the model's robustness. incomplete data. In contrast, ITEACH-Net learns to handle complete data effectively, not only improving performance as the missing rate decreases but also maintaining stable performance across different missing rates. These results aptly reflect the necessity of testing model robustness using UME. However, the optimal training approach to fully leverage the potential of the model within the UME method remains an open question. In this paper, we compare the performance of models under two different training strategies:\n\nRandom Strategy: To equip the model with the capability to process data with varying degrees of incompleteness, a straightforward approach involves blending training data with different missing rates. Under this training strategy, each batch of training data sampled by the model is randomly assigned a missing rate ranging from 0% to 70%, ensuring that each batch experiences a distinct level of incompleteness.\n\nAs illustrated in Table  4 , it is observable that ITEACH-Net achieves optimal results across the Average metrics of four tasks, particularly demonstrating a significant performance advantage in the six-class IEMOCAP and CMU-MOSI tasks. Similarly, ITEACH-Net consistently outperformed all baselines across various missing rates in these two tasks, while in the four-class IEMOCAP and CMU-MOSEI tasks, it alternately excels or is comparable to GCNet.\n\nProgressive Strategy: Compared to handling incomplete data, classifying tasks with complete data is evidently simpler. Hence, we aim to familiarize the model with complete data modeling initially, gradually introducing it to incomplete data in the hope that it can leverage previously acquired knowledge for better handling of incomplete data. Specifically, we start with a 0% missing rate and increase it of the training data by 10% every 10 epochs, up to a maximum of 70%. This progressive strategy allows the model to adapt to incomplete data more smoothly.\n\nAs depicted in Table  5 , ITEACH-Net consistently achieved optimal results on the Average metrics across all four tasks, with a particularly distinct performance advantage in all tasks except the four-class IEMOCAP. Although GCNet outperforms ITEACH-Net in the four-class IEMOCAP, six-class IEMOCAP and CMU-MOSEI tasks under severe data missing scenarios, it fails to maintain performance at lower missing rates, highlighting a drawback of the previous training frameworks. ITEACH-Net effectively mitigates this issue.\n\nFurthermore, examining the performance of different methods across various missing rates, baselines under the Random training strategy show a balanced performance across different data missing rate scenarios but rarely achieve standout results. With the Progressive training strategy, baselines tend to exhibit a biased proficiency, performing well at a certain missing rate but declining as the missing rate changes, whether it increases or decreases. This indicates that the model is unable to learn how to process data at different missing rates simultaneously. However, regardless of the training strategy employed, ITEACH-Net consistently demonstrates a balanced performance, exhibiting exceptional capability in processing both incomplete and complete data.",
      "page_start": 11,
      "page_end": 15
    },
    {
      "section_name": "Visualization Of Embedding Space",
      "text": "To qualitatively analyze the robustness of ITEACH-Net in the face of dynamic data missing rates, we visualized the latent representations under different missing scenarios on the four-class IEMOCAP test set. Figure  9  presents the t-SNE  [55]  visualization results. Subfigure 9(a) represents the encoding results of the Teacher model using complete data, while subfigures 9(b) to 9(i) depict the encoding results of the Student model under various data missing rates. It is observable that, in dynamic missing scenarios, the Student model still effectively emulates the knowledge of the Teacher during the encoding complete data. Moreover, as the data missing rate increases, although the variance of the same class data representation gradually enlarges, the overall distribution pattern remains unchanged, and there is still a clear distance between different class data representations. The stable data embedding space distribution demonstrates the robustness of ITEACH-Net in handling different missing scenarios.",
      "page_start": 15,
      "page_end": 16
    },
    {
      "section_name": "Visualization Of Confusion Matrix",
      "text": "To delve into the processing results of ITEACH-Net on imbalanced category data under different missing rates, we further visualized the confusion matrix of the model's prediction results in the Four-class IEMOCAP. The results are shown in Figure  10 . It can be seen that the prediction accuracy for the Sad, Neutral, and Angry categories decreases with the increase in data missing rates. Among them, the Neutral category has the largest number of samples, totaling 1708, while the Angry category has the least, with only 1103 samples. In contrast, the Happy category, which has a larger number of samples, maintains a relatively stable prediction accuracy under different missing rates. Based on these results, it can be concluded that ITEACH-Net performs stably in the relatively simple task of Happy emotion detection in dynamic missing scenarios, and does not suffer from model collapse due to data imbalance in other strate that previous approaches struggle to effectively handle varying data missing rate scenarios when dealing with incomplete data. Concurrently, experimental results showcase the superiority of ITEACH-Net in incomplete learning.\n\nIn the future, we aim to expand ITEACH-Net to focus on real-time analysis scenarios, considering how to determine the emotional context patterns of the current time node based on data from previous time nodes. Additionally, beyond emotion recognition in conversation, we will also utilize ITEACH-Net to address the issue of modality missing in other types of conversation understanding tasks.",
      "page_start": 15,
      "page_end": 17
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: illustrates the overall structure of our method. To better understand the",
      "page": 2
    },
    {
      "caption": "Figure 3: , we design a novel encoder based on the context",
      "page": 2
    },
    {
      "caption": "Figure 1: The overall structure of Inverted Teacher-studEnt seArCH Network (ITEACH-Net) with the trimodal setting. The Inverted Teacher-",
      "page": 3
    },
    {
      "caption": "Figure 2: The computational modules within the Teacher Model and the Student Model.",
      "page": 4
    },
    {
      "caption": "Figure 1: We define complete multimodal features as {Xt, Xa, Xv} and incomplete multimodal data as {Xs",
      "page": 4
    },
    {
      "caption": "Figure 3: In conversations, the speakers’ states tend to maintain a relatively stable pattern within the local context. As the context change, this",
      "page": 5
    },
    {
      "caption": "Figure 3: , the emotional state of speakers in a conversation tends to maintain a stable pattern locally,",
      "page": 5
    },
    {
      "caption": "Figure 2: , the Encoder",
      "page": 6
    },
    {
      "caption": "Figure 2: , multimodal features undergo encoding in their respective ITEACH Encoder at each Encoder",
      "page": 6
    },
    {
      "caption": "Figure 1: Subsequently, the three modal data will continue to be fed into",
      "page": 6
    },
    {
      "caption": "Figure 4: , different inputs can",
      "page": 6
    },
    {
      "caption": "Figure 4: When there are three Token Mixers, the Router’s search process involves different utterance features selecting different operations. The",
      "page": 7
    },
    {
      "caption": "Figure 5: Heatmaps generated under different model settings. (a) illustrates the attention map computed by vanilla Self-Attention. (b) illustrates",
      "page": 10
    },
    {
      "caption": "Figure 5: , we employ heatmaps for a more intuitive analysis of ECCE. Figure 5(a)",
      "page": 10
    },
    {
      "caption": "Figure 5: (b) represents ECCE without Local stage en-",
      "page": 10
    },
    {
      "caption": "Figure 5: (c) illustrates the complete ECCE. Darker colors indicate higher weights. All maps originate",
      "page": 10
    },
    {
      "caption": "Figure 5: (a), the vanilla Self-Attention, when computing attention maps, tends to allocate higher",
      "page": 10
    },
    {
      "caption": "Figure 5: (b) demonstrates a certain degree of local attention capability. However, this attention behavior appears somewhat",
      "page": 10
    },
    {
      "caption": "Figure 5: (c) illustrates the complete ECCE, which takes into account these aspects effectively. It can be observed",
      "page": 10
    },
    {
      "caption": "Figure 6: , where the red lines signifie the student model under the NAS",
      "page": 11
    },
    {
      "caption": "Figure 6: The vector distance between Hs",
      "page": 12
    },
    {
      "caption": "Figure 7: Comparison of classification performance under Independent Model Evaluation (IME). We present WAF scores (%).",
      "page": 12
    },
    {
      "caption": "Figure 8: Comparison of classification performance at different missing rates for models trained with 70% missing data.",
      "page": 13
    },
    {
      "caption": "Figure 7: , enabling a fair comparison with the baselines.",
      "page": 13
    },
    {
      "caption": "Figure 8: the performance of different methods at various",
      "page": 13
    },
    {
      "caption": "Figure 9: presents the",
      "page": 15
    },
    {
      "caption": "Figure 9: (a) represents the encoding results of the Teacher model using complete",
      "page": 15
    },
    {
      "caption": "Figure 10: It can be seen that the prediction accuracy for the Sad, Neutral, and Angry categories decreases",
      "page": 15
    },
    {
      "caption": "Figure 9: t-SNE visualization results on the four-class IEMOCAP test set with the increasing data missing rates. Student-0.1 represents the results",
      "page": 16
    },
    {
      "caption": "Figure 10: Visualization results of confusion matrices on the four-class IEMOCAP with the increasing data missing rates.",
      "page": 16
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "IEMOCAP\n(four-class)": "IEMOCAP\n(six-class)",
          "✕\n✕\n✕\n✓\n✓\n✓": "✕\n✕\n✕\n✓\n✓\n✓",
          "77.83\n77.69\n78.15": "60.28\n59.89\n60.66"
        },
        {
          "IEMOCAP\n(four-class)": "CMU-MOSI",
          "✕\n✕\n✕\n✓\n✓\n✓": "✕\n✕\n✕\n✓\n✓\n✓",
          "77.83\n77.69\n78.15": "85.46\n85.16\n85.74"
        },
        {
          "IEMOCAP\n(four-class)": "CMU-MOSEI",
          "✕\n✕\n✕\n✓\n✓\n✓": "✕\n✕\n✕\n✓\n✓\n✓",
          "77.83\n77.69\n78.15": "87.31\n87.12\n87.59"
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "IEMOCAP\n(four-class)": "IEMOCAP\n(six-class)",
          "PRE\nITS\nITS\nTS": "PRE\nITS\nITS\nTS",
          "TF\nTF\nTF\nNAS": "TF\nTF\nTF\nNAS",
          "-\nTF\nNAS\nTF": "-\nTF\nNAS\nTF",
          "73.89\n77.28\n77.47\n76.15": "56.98\n58.59\n59.02\n58.13",
          "72.86\n76.26\n76.87\n75.48": "56.02\n58.30\n58.69\n57.11",
          "71.52\n75.59\n76.44\n74.70": "54.79\n57.29\n58.53\n56.41",
          "70.53\n74.76\n76.02\n74.00": "53.11\n56.79\n57.95\n55.73",
          "69.15\n74.00\n75.48\n73.37": "51.67\n55.89\n57.55\n55.27",
          "68.65\n73.39\n74.72\n72.86": "49.84\n55.18\n56.75\n54.50",
          "67.62\n72.54\n74.19\n72.33": "47.85\n54.74\n56.03\n53.90",
          "65.79\n72.06\n73.84\n71.77": "47.46\n54.31\n55.94\n53.16",
          "8.10\n5.22\n3.63\n4.38": "9.52\n4.28\n3.08\n4.97"
        },
        {
          "IEMOCAP\n(four-class)": "CMU-MOSI",
          "PRE\nITS\nITS\nTS": "PRE\nITS\nITS\nTS",
          "TF\nTF\nTF\nNAS": "TF\nTF\nTF\nNAS",
          "-\nTF\nNAS\nTF": "-\nTF\nNAS\nTF",
          "73.89\n77.28\n77.47\n76.15": "84.99\n85.93\n84.90\n85.53",
          "72.86\n76.26\n76.87\n75.48": "81.65\n82.45\n83.35\n81.04",
          "71.52\n75.59\n76.44\n74.70": "75.95\n79.49\n81.04\n78.31",
          "70.53\n74.76\n76.02\n74.00": "75.99\n77.21\n77.90\n75.45",
          "69.15\n74.00\n75.48\n73.37": "72.27\n72.97\n76.99\n72.24",
          "68.65\n73.39\n74.72\n72.86": "69.11\n69.46\n71.94\n66.26",
          "67.62\n72.54\n74.19\n72.33": "62.79\n65.68\n70.27\n62.58",
          "65.79\n72.06\n73.84\n71.77": "64.66\n66.88\n68.93\n62.18",
          "8.10\n5.22\n3.63\n4.38": "20.33\n19.05\n15.97\n23.35"
        },
        {
          "IEMOCAP\n(four-class)": "CMU-MOSEI",
          "PRE\nITS\nITS\nTS": "PRE\nITS\nITS\nTS",
          "TF\nTF\nTF\nNAS": "TF\nTF\nTF\nNAS",
          "-\nTF\nNAS\nTF": "-\nTF\nNAS\nTF",
          "73.89\n77.28\n77.47\n76.15": "86.81\n87.16\n86.71\n86.72",
          "72.86\n76.26\n76.87\n75.48": "85.64\n85.86\n86.40\n86.22",
          "71.52\n75.59\n76.44\n74.70": "85.01\n84.80\n84.97\n84.35",
          "70.53\n74.76\n76.02\n74.00": "83.80\n83.87\n84.40\n84.06",
          "69.15\n74.00\n75.48\n73.37": "82.93\n83.12\n83.25\n82.52",
          "68.65\n73.39\n74.72\n72.86": "81.89\n81.53\n82.00\n82.02",
          "67.62\n72.54\n74.19\n72.33": "80.15\n80.63\n80.65\n80.10",
          "65.79\n72.06\n73.84\n71.77": "79.50\n79.42\n80.21\n79.04",
          "8.10\n5.22\n3.63\n4.38": "7.31\n7.74\n6.50\n7.68"
        }
      ],
      "page": 11
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "AE[54]\nCRA[41]\nMMIN[11]\nGCNet[13]": "ITEACH-Net",
          "-\n-\n-\n-": "Teacher\nStudent",
          "1.29M\n2.11M\n3.87M\n34.14M": "2.19M\n3.88M"
        }
      ],
      "page": 13
    },
    {
      "caption": "Table 4: , it is observable that ITEACH-Net achieves optimal results across the Average metrics",
      "data": [
        {
          "IEMOCAP\n(four-class)": "IEMOCAP\n(six-class)",
          "AE[54]\nCRA[41]\nMMIN[11]\nGCNet[13]\nITEACH-Net": "AE[54]\nCRA[41]\nMMIN[11]\nGCNet[13]\nITEACH-Net",
          "57.55\n36.89\n74.71\n77.09\n77.61": "28.10\n24.15\n57.00\n56.05\n59.90",
          "56.43\n35.76\n72.88\n77.00\n77.23": "27.82\n23.65\n54.85\n56.41\n59.32",
          "55.26\n34.77\n70.81\n76.47\n76.46": "27.51\n23.04\n52.79\n56.18\n58.61",
          "53.59\n33.45\n68.15\n76.19\n75.97": "27.49\n22.16\n50.13\n56.03\n57.86",
          "51.50\n32.28\n65.49\n75.30\n75.12": "27.00\n21.39\n47.33\n55.87\n57.01",
          "49.53\n30.72\n62.22\n74.71\n73.99": "26.27\n20.65\n44.62\n54.42\n56.36",
          "47.10\n29.22\n58.53\n72.24\n72.40": "25.78\n19.38\n41.24\n52.93\n55.29",
          "45.16\n28.07\n55.85\n69.94\n71.73": "25.05\n18.53\n39.16\n51.87\n54.01",
          "52.02\n32.65\n66.08\n74.87\n75.06": "26.88\n21.62\n48.39\n54.97\n57.30"
        },
        {
          "IEMOCAP\n(four-class)": "CMUMOSI",
          "AE[54]\nCRA[41]\nMMIN[11]\nGCNet[13]\nITEACH-Net": "AE[54]\nCRA[41]\nMMIN[11]\nGCNet[13]\nITEACH-Net",
          "57.55\n36.89\n74.71\n77.09\n77.61": "83.84\n76.84\n84.57\n84.37\n86.23",
          "56.43\n35.76\n72.88\n77.00\n77.23": "79.80\n73.87\n80.74\n82.62\n84.22",
          "55.26\n34.77\n70.81\n76.47\n76.46": "77.07\n70.75\n76.63\n79.64\n81.65",
          "53.59\n33.45\n68.15\n76.19\n75.97": "70.78\n66.82\n74.69\n77.86\n79.27",
          "51.50\n32.28\n65.49\n75.30\n75.12": "66.53\n62.96\n69.57\n75.50\n76.43",
          "49.53\n30.72\n62.22\n74.71\n73.99": "63.67\n58.93\n64.28\n73.12\n75.18",
          "47.10\n29.22\n58.53\n72.24\n72.40": "56.56\n55.75\n59.68\n71.34\n70.65",
          "45.16\n28.07\n55.85\n69.94\n71.73": "51.91\n50.90\n57.05\n67.11\n68.96",
          "52.02\n32.65\n66.08\n74.87\n75.06": "68.77\n64.60\n70.90\n76.44\n77.82"
        },
        {
          "IEMOCAP\n(four-class)": "CMUMOSEI",
          "AE[54]\nCRA[41]\nMMIN[11]\nGCNet[13]\nITEACH-Net": "AE[54]\nCRA[41]\nMMIN[11]\nGCNet[13]\nITEACH-Net",
          "57.55\n36.89\n74.71\n77.09\n77.61": "84.48\n75.48\n85.75\n86.84\n87.40",
          "56.43\n35.76\n72.88\n77.00\n77.23": "82.82\n74.06\n83.12\n86.06\n86.50",
          "55.26\n34.77\n70.81\n76.47\n76.46": "81.20\n72.56\n80.55\n85.25\n85.11",
          "53.59\n33.45\n68.15\n76.19\n75.97": "79.13\n70.95\n77.52\n84.48\n84.59",
          "51.50\n32.28\n65.49\n75.30\n75.12": "77.33\n69.18\n75.10\n83.80\n82.81",
          "49.53\n30.72\n62.22\n74.71\n73.99": "75.49\n67.24\n71.44\n82.13\n82.30",
          "47.10\n29.22\n58.53\n72.24\n72.40": "73.13\n64.84\n68.17\n80.66\n80.66",
          "45.16\n28.07\n55.85\n69.94\n71.73": "71.24\n63.56\n65.38\n79.64\n79.79",
          "52.02\n32.65\n66.08\n74.87\n75.06": "78.10\n69.73\n75.88\n83.61\n83.65"
        }
      ],
      "page": 14
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "IEMOCAP\n(four-class)": "IEMOCAP\n(six-class)",
          "AE[54]\nCRA[41]\nMMIN[11]\nGCNet[13]\nITEACH-Net": "AE[54]\nCRA[41]\nMMIN[11]\nGCNet[13]\nITEACH-Net",
          "67.60\n61.34\n74.77\n71.67\n77.47": "48.08\n36.01\n57.16\n51.88\n59.09",
          "66.23\n59.12\n72.74\n73.53\n76.86": "46.38\n34.95\n54.81\n53.88\n58.72",
          "64.09\n56.59\n70.34\n74.90\n76.78": "44.71\n34.07\n53.05\n55.34\n58.13",
          "62.28\n53.90\n67.59\n75.96\n75.86": "43.20\n32.60\n50.15\n55.53\n57.53",
          "59.91\n51.12\n64.98\n75.72\n75.26": "41.08\n31.17\n47.85\n55.91\n57.18",
          "56.97\n48.10\n61.81\n76.20\n74.33": "39.07\n29.68\n44.79\n56.22\n56.39",
          "53.73\n43.98\n58.30\n75.48\n73.73": "36.37\n26.51\n41.39\n56.16\n55.53",
          "51.41\n41.51\n56.05\n74.80\n71.79": "34.28\n25.24\n38.90\n55.27\n55.06",
          "60.28\n51.96\n65.82\n74.78\n75.26": "41.64\n31.28\n48.51\n55.03\n57.21"
        },
        {
          "IEMOCAP\n(four-class)": "CMUMOSI",
          "AE[54]\nCRA[41]\nMMIN[11]\nGCNet[13]\nITEACH-Net": "AE[54]\nCRA[41]\nMMIN[11]\nGCNet[13]\nITEACH-Net",
          "67.60\n61.34\n74.77\n71.67\n77.47": "85.66\n85.00\n84.62\n68.78\n84.40",
          "66.23\n59.12\n72.74\n73.53\n76.86": "81.72\n80.90\n79.84\n70.91\n82.39",
          "64.09\n56.59\n70.34\n74.90\n76.78": "78.29\n77.01\n76.32\n70.72\n80.54",
          "62.28\n53.90\n67.59\n75.96\n75.86": "74.14\n72.03\n71.39\n70.11\n78.86",
          "59.91\n51.12\n64.98\n75.72\n75.26": "69.46\n67.46\n68.05\n69.45\n75.62",
          "56.97\n48.10\n61.81\n76.20\n74.33": "62.81\n61.86\n61.95\n69.72\n74.30",
          "53.73\n43.98\n58.30\n75.48\n73.73": "58.13\n56.75\n56.40\n68.28\n69.92",
          "51.41\n41.51\n56.05\n74.80\n71.79": "54.23\n52.24\n52.93\n68.16\n65.77",
          "60.28\n51.96\n65.82\n74.78\n75.26": "70.55\n69.16\n68.94\n69.52\n76.48"
        },
        {
          "IEMOCAP\n(four-class)": "CMUMOSEI",
          "AE[54]\nCRA[41]\nMMIN[11]\nGCNet[13]\nITEACH-Net": "AE[54]\nCRA[41]\nMMIN[11]\nGCNet[13]\nITEACH-Net",
          "67.60\n61.34\n74.77\n71.67\n77.47": "85.19\n85.73\n85.21\n82.81\n86.91",
          "66.23\n59.12\n72.74\n73.53\n76.86": "83.14\n83.18\n82.83\n82.67\n85.78",
          "64.09\n56.59\n70.34\n74.90\n76.78": "81.50\n80.61\n80.19\n82.72\n84.86",
          "62.28\n53.90\n67.59\n75.96\n75.86": "79.33\n77.51\n78.09\n82.31\n84.16",
          "59.91\n51.12\n64.98\n75.72\n75.26": "77.00\n74.69\n74.53\n82.03\n82.77",
          "56.97\n48.10\n61.81\n76.20\n74.33": "74.43\n71.94\n71.53\n81.81\n81.60",
          "53.73\n43.98\n58.30\n75.48\n73.73": "71.52\n67.84\n67.90\n81.12\n79.80",
          "51.41\n41.51\n56.05\n74.80\n71.79": "70.19\n65.49\n66.08\n79.99\n78.91",
          "60.28\n51.96\n65.82\n74.78\n75.26": "77.79\n75.88\n75.79\n81.93\n83.10"
        }
      ],
      "page": 15
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Multimodal analysis and prediction of persuasiveness in online social multimedia",
      "authors": [
        "S Park",
        "H Shim",
        "M Chatterjee",
        "K Sagae",
        "L.-P Morency"
      ],
      "year": "2016",
      "venue": "ACM Transactions on Interactive Intelligent Systems, (TiiS)"
    },
    {
      "citation_id": "2",
      "title": "Bieru: Bidirectional emotional recurrent unit for conversational sentiment analysis",
      "authors": [
        "W Li",
        "W Shao",
        "S Ji",
        "E Cambria"
      ],
      "year": "2022",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "3",
      "title": "Pirnet: Personality-enhanced iterative refinement network for emotion recognition in conversation",
      "authors": [
        "Z Lian",
        "B Liu",
        "J Tao"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Neural Networks and Learning Systems"
    },
    {
      "citation_id": "4",
      "title": "M2fnet: Multi-modal fusion network for emotion recognition in conversation",
      "authors": [
        "V Chudasama",
        "P Kar",
        "A Gudmalwar",
        "N Shah",
        "P Wasnik",
        "N Onoe"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "5",
      "title": "Conversational transfer learning for emotion recognition",
      "authors": [
        "D Hazarika",
        "S Poria",
        "R Zimmermann",
        "R Mihalcea"
      ],
      "year": "2021",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "6",
      "title": "Smin: Semi-supervised multi-modal interaction network for conversational emotion recognition",
      "authors": [
        "Z Lian",
        "B Liu",
        "J Tao"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "7",
      "title": "A comprehensive survey on multi-modal conversational emotion recognition with deep learning",
      "authors": [
        "Y Shou",
        "T Meng",
        "W Ai",
        "N Yin",
        "K Li"
      ],
      "year": "2023",
      "venue": "A comprehensive survey on multi-modal conversational emotion recognition with deep learning",
      "arxiv": "arXiv:2312.05735"
    },
    {
      "citation_id": "8",
      "title": "Ctnet: Conversational transformer network for emotion recognition",
      "authors": [
        "Z Lian",
        "B Liu",
        "J Tao"
      ],
      "year": "2021",
      "venue": "IEEE ACM Trans. Audio Speech Lang. Process"
    },
    {
      "citation_id": "9",
      "title": "Multi-stage graph representation learning for dialogue-level speech emotion recognition",
      "authors": [
        "Y Song",
        "J Liu",
        "L Wang",
        "R Yu",
        "J Dang"
      ],
      "year": "2022",
      "venue": "International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "10",
      "title": "Emotionic: Emotional inertia and contagion-driven dependency modelling for emotion recognition in conversation",
      "authors": [
        "Y Liu",
        "J Li",
        "X Wang",
        "Z Zeng"
      ],
      "year": "2023",
      "venue": "Emotionic: Emotional inertia and contagion-driven dependency modelling for emotion recognition in conversation",
      "arxiv": "arXiv:2303.11117"
    },
    {
      "citation_id": "11",
      "title": "Missing modality imagination network for emotion recognition with uncertain missing modalities",
      "authors": [
        "J Zhao",
        "R Li",
        "Q Jin"
      ],
      "year": "2021",
      "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "12",
      "title": "Transformer-based feature reconstruction network for robust multimodal sentiment analysis",
      "authors": [
        "Z Yuan",
        "W Li",
        "H Xu",
        "W Yu"
      ],
      "year": "2021",
      "venue": "Proceedings of the ACM International Conference on Multimedia, ACM MM"
    },
    {
      "citation_id": "13",
      "title": "Gcnet: Graph completion network for incomplete multimodal learning in conversation",
      "authors": [
        "Z Lian",
        "L Chen",
        "L Sun",
        "B Liu",
        "J Tao"
      ],
      "year": "2023",
      "venue": "IEEE Trans. Pattern Anal. Mach. Intell, TPAMI"
    },
    {
      "citation_id": "14",
      "title": "Efficient multimodal transformer with dual-level feature restoration for robust multimodal sentiment analysis",
      "authors": [
        "L Sun",
        "Z Lian",
        "B Liu",
        "J Tao"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "15",
      "title": "Learning modality-specific representations with self-supervised multi-task learning for multimodal sentiment analysis",
      "authors": [
        "W Yu",
        "H Xu",
        "Z Yuan",
        "J Wu"
      ],
      "venue": "AAAI Conference on Artificial Intelligence, AAAI, 2021"
    },
    {
      "citation_id": "16",
      "title": "Multi-modality information fusion for radiomics-based neural architecture search",
      "authors": [
        "Y Peng",
        "L Bi",
        "M Fulham",
        "D Feng",
        "J Kim"
      ],
      "year": "2020",
      "venue": "Medical Image Computing and Computer Assisted Intervention"
    },
    {
      "citation_id": "17",
      "title": "Exploiting modality-invariant feature for robust multimodal emotion recognition with missing modalities",
      "authors": [
        "H Zuo",
        "R Liu",
        "J Zhao",
        "G Gao",
        "H Li"
      ],
      "year": "2023",
      "venue": "ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "18",
      "title": "Transformer-based feature reconstruction network for robust multimodal sentiment analysis",
      "authors": [
        "Z Yuan",
        "W Li",
        "H Xu",
        "W Yu"
      ],
      "year": "2021",
      "venue": "Proceedings of the ACM International Conference on Multimedia, ACM MM"
    },
    {
      "citation_id": "19",
      "title": "DARTS: differentiable architecture search",
      "authors": [
        "H Liu",
        "K Simonyan",
        "Y Yang"
      ],
      "year": "2019",
      "venue": "th International Conference on Learning Representations"
    },
    {
      "citation_id": "20",
      "title": "Searching for burgerformer with micro-meso-macro space design",
      "authors": [
        "L Yang",
        "Y Hu",
        "S Lu",
        "Z Sun",
        "J Mei",
        "Y Han",
        "X Li"
      ],
      "year": "2022",
      "venue": "International Conference on Machine Learning, ICML, PMLR"
    },
    {
      "citation_id": "21",
      "title": "Emotionnas: Two-stream architecture search for speech emotion recognition",
      "authors": [
        "H Sun",
        "Z Lian",
        "B Liu",
        "Y Li",
        "L Sun",
        "C Cai",
        "J Tao",
        "M Wang",
        "Y Cheng"
      ],
      "year": "2023",
      "venue": "Emotionnas: Two-stream architecture search for speech emotion recognition"
    },
    {
      "citation_id": "22",
      "title": "Emotion recognition through multiple perspectives fusion architecture search emulating human cognition",
      "authors": [
        "H Sun",
        "F Zhang",
        "Z Lian",
        "Y Guo",
        "S Zhang"
      ],
      "year": "2023",
      "venue": "Emotion recognition through multiple perspectives fusion architecture search emulating human cognition",
      "arxiv": "arXiv:2306.09361"
    },
    {
      "citation_id": "23",
      "title": "Relation-aware graph attention networks with relational position encodings for emotion recognition in conversations",
      "authors": [
        "T Ishiwatari",
        "Y Yasuda",
        "T Miyazaki",
        "J Goto"
      ],
      "year": "2020",
      "venue": "Proceedings of the 2020 conference on empirical methods in natural language processing"
    },
    {
      "citation_id": "24",
      "title": "Conversational emotion analysis via attention mechanisms",
      "authors": [
        "Z Lian",
        "J Tao",
        "B Liu",
        "J Huang"
      ],
      "year": "2019",
      "venue": "Conversational emotion analysis via attention mechanisms"
    },
    {
      "citation_id": "25",
      "title": "Conversational emotion recognition using self-attention mechanisms and graph neural networks",
      "authors": [
        "Z Lian",
        "J Tao",
        "B Liu",
        "J Huang",
        "Z Yang",
        "R Li"
      ],
      "year": "2020",
      "venue": "Conversational emotion recognition using self-attention mechanisms and graph neural networks"
    },
    {
      "citation_id": "26",
      "title": "Contextual and cross-modal interaction for multi-modal speech emotion recognition",
      "authors": [
        "D Yang",
        "S Huang",
        "Y Liu",
        "L Zhang"
      ],
      "year": "2022",
      "venue": "IEEE Signal Processing Letters"
    },
    {
      "citation_id": "27",
      "title": "Multi-modal speech emotion recognition using self-attention mechanism and multi-scale fusion framework",
      "authors": [
        "Y Liu",
        "H Sun",
        "W Guan",
        "Y Xia",
        "Z Zhao"
      ],
      "year": "2022",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "28",
      "title": "Long short-term memory",
      "authors": [
        "S Hochreiter",
        "J Schmidhuber"
      ],
      "year": "1997",
      "venue": "Neural computation"
    },
    {
      "citation_id": "29",
      "title": "Spectral networks and locally connected networks on graphs, in: 2nd International Conference on Learning Representations, ICLR",
      "authors": [
        "J Bruna",
        "W Zaremba",
        "A Szlam",
        "Y Lecun"
      ],
      "year": "2014",
      "venue": "Spectral networks and locally connected networks on graphs, in: 2nd International Conference on Learning Representations, ICLR"
    },
    {
      "citation_id": "30",
      "title": "On the properties of neural machine translation: Encoder-decoder approaches",
      "authors": [
        "K Cho",
        "B Van Merriënboer",
        "D Bahdanau",
        "Y Bengio"
      ],
      "year": "2014",
      "venue": "On the properties of neural machine translation: Encoder-decoder approaches",
      "arxiv": "arXiv:1409.1259"
    },
    {
      "citation_id": "31",
      "title": "Context-and sentiment-aware networks for emotion recognition in conversation",
      "authors": [
        "G Tu",
        "J Wen",
        "C Liu",
        "D Jiang",
        "E Cambria"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Artificial Intelligence"
    },
    {
      "citation_id": "32",
      "title": "Modeling intention, emotion and external world in dialogue systems",
      "authors": [
        "W Peng",
        "Y Hu",
        "L Xing",
        "Y Xie",
        "X Zhang",
        "Y Sun"
      ],
      "year": "2022",
      "venue": "International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "33",
      "title": "Der-gcn: Dialog and event relation-aware graph convolutional neural network for multimodal dialog emotion recognition",
      "authors": [
        "W Ai",
        "Y Shou",
        "T Meng",
        "K Li"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Neural Networks and Learning Systems"
    },
    {
      "citation_id": "34",
      "title": "Training strategies to handle missing modalities for audio-visual expression recognition",
      "authors": [
        "S Parthasarathy",
        "S Sundaram"
      ],
      "year": "2020",
      "venue": "Companion Publication of the 2020 International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "35",
      "title": "Deep partial multi-view learning",
      "authors": [
        "C Zhang",
        "Y Cui",
        "Z Han",
        "J Zhou",
        "H Fu",
        "Q Hu"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "36",
      "title": "Deep partial multi-view learning",
      "authors": [
        "C Zhang",
        "Y Cui",
        "Z Han",
        "J Zhou",
        "H Fu",
        "Q Hu"
      ],
      "year": "2022",
      "venue": "IEEE Trans. Pattern Anal. Mach. Intell"
    },
    {
      "citation_id": "37",
      "title": "Learning to extract semantic structure from documents using multimodal fully convolutional neural networks",
      "authors": [
        "X Yang",
        "E Yumer",
        "P Asente",
        "M Kraley",
        "D Kifer",
        "C Giles"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "38",
      "title": "Learning representations from imperfect time series data via tensor rank regularization",
      "authors": [
        "P Liang",
        "Z Liu",
        "Y.-H Tsai",
        "Q Zhao",
        "R Salakhutdinov",
        "L.-P Morency"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "39",
      "title": "Hyperspectral image restoration using low-rank tensor recovery",
      "authors": [
        "H Fan",
        "Y Chen",
        "Y Guo",
        "H Zhang",
        "G Kuang"
      ],
      "year": "2017",
      "venue": "IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing"
    },
    {
      "citation_id": "40",
      "title": "A deep learning based approach for traffic data imputation",
      "authors": [
        "Y Duan",
        "Y Lv",
        "W Kang",
        "Y Zhao"
      ],
      "year": "2014",
      "venue": "th International IEEE Conference on Intelligent Transportation Systems (ITSC)"
    },
    {
      "citation_id": "41",
      "title": "Missing modalities imputation via cascaded residual autoencoder",
      "authors": [
        "L Tran",
        "X Liu",
        "J Zhou",
        "R Jin"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "42",
      "title": "Distilling the knowledge in a neural network",
      "authors": [
        "G Hinton",
        "O Vinyals",
        "J Dean"
      ],
      "year": "2015",
      "venue": "Distilling the knowledge in a neural network",
      "arxiv": "arXiv:1503.02531"
    },
    {
      "citation_id": "43",
      "title": "Training data-efficient image transformers & distillation through attention",
      "authors": [
        "H Touvron",
        "M Cord",
        "M Douze",
        "F Massa",
        "A Sablayrolles",
        "H Jégou"
      ],
      "year": "2021",
      "venue": "International Conference on Machine Learning, ICML"
    },
    {
      "citation_id": "44",
      "title": "Kd-zero: Evolving knowledge distiller for any teacher-student pairs",
      "authors": [
        "L Li",
        "P Dong",
        "A Li",
        "Z Wei",
        "Y Yang"
      ],
      "year": "2024",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "45",
      "title": "Metaformer is actually what you need for vision",
      "authors": [
        "W Yu",
        "M Luo",
        "P Zhou",
        "C Si",
        "Y Zhou",
        "X Wang",
        "J Feng",
        "S Yan"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "46",
      "title": "Efficient language modeling with sparse all-mlp",
      "authors": [
        "P Yu",
        "M Artetxe",
        "M Ott",
        "S Shleifer",
        "H Gong",
        "V Stoyanov",
        "X Li"
      ],
      "year": "2022",
      "venue": "Efficient language modeling with sparse all-mlp",
      "arxiv": "arXiv:2203.06850"
    },
    {
      "citation_id": "47",
      "title": "Do transformers really perform badly for graph representation?",
      "authors": [
        "C Ying",
        "T Cai",
        "S Luo",
        "S Zheng",
        "G Ke",
        "D He",
        "Y Shen",
        "T.-Y Liu"
      ],
      "year": "2021",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "48",
      "title": "Multimodal sentiment intensity analysis in videos: Facial gestures and verbal messages",
      "authors": [
        "A Zadeh",
        "R Zellers",
        "E Pincus",
        "L Morency"
      ],
      "year": "2016",
      "venue": "IEEE Intell. Syst"
    },
    {
      "citation_id": "49",
      "title": "Multimodal language analysis in the wild: CMU-MOSEI dataset and interpretable dynamic fusion graph",
      "authors": [
        "A Zadeh",
        "P Liang",
        "S Poria",
        "E Cambria",
        "L Morency"
      ],
      "year": "2018",
      "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "50",
      "title": "IEMOCAP: interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Lang. Resour. Evaluation"
    },
    {
      "citation_id": "51",
      "title": "wav2vec: Unsupervised pre-training for speech recognition",
      "authors": [
        "S Schneider",
        "A Baevski",
        "R Collobert",
        "M Auli"
      ],
      "year": "2019",
      "venue": "wav2vec: Unsupervised pre-training for speech recognition"
    },
    {
      "citation_id": "52",
      "title": "A robustly optimized BERT pretraining approach",
      "authors": [
        "Y Liu",
        "M Ott",
        "N Goyal",
        "J Du",
        "M Joshi",
        "D Chen",
        "O Levy",
        "M Lewis",
        "L Zettlemoyer",
        "V Stoyanov"
      ],
      "year": "2019",
      "venue": "A robustly optimized BERT pretraining approach"
    },
    {
      "citation_id": "53",
      "title": "Learning deep global multi-scale and local attention features for facial expression recognition in the wild",
      "authors": [
        "Z Zhao",
        "Q Liu",
        "S Wang"
      ],
      "year": "2021",
      "venue": "IEEE Trans. Image Process"
    },
    {
      "citation_id": "54",
      "title": "Greedy layer-wise training of deep networks",
      "authors": [
        "Y Bengio",
        "P Lamblin",
        "D Popovici",
        "H Larochelle"
      ],
      "year": "2006",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "55",
      "title": "Visualizing data using t-sne",
      "authors": [
        "L Van Der Maaten",
        "G Hinton"
      ],
      "year": "2008",
      "venue": "Journal of machine learning research"
    }
  ]
}