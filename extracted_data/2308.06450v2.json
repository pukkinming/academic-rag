{
  "paper_id": "2308.06450v2",
  "title": "Ernetcl: A Novel Emotion Recognition Network In Textual Conversation Based On Curriculum Learning Strategy",
  "published": "2023-08-12T03:05:44Z",
  "authors": [
    "Jiang Li",
    "Xiaoping Wang",
    "Yingjian Liu",
    "Zhigang Zeng"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Emotion recognition in conversation (ERC) has emerged as a research hotspot in domains such as conversational robots and question-answer systems. How to efficiently and adequately retrieve contextual emotional cues has been one of the key challenges in the ERC task. Existing efforts do not fully model the context and employ complex network structures, resulting in limited performance gains. In this paper, we propose a novel emotion recognition network based on curriculum learning strategy (ERNetCL). The proposed ERNetCL primarily consists of temporal encoder (TE), spatial encoder (SE), and curriculum learning (CL) loss. We utilize TE and SE to combine the strengths of previous methods in a simplistic manner to efficiently capture temporal and spatial contextual information in the conversation. To ease the harmful influence resulting from emotion shift and simulate the way humans learn curriculum from easy to hard, we apply the idea of CL to the ERC task to progressively optimize the network parameters. At the beginning of training, we assign lower learning weights to difficult samples. As the epoch increases, the learning weights for these samples are gradually raised. Extensive experiments on four datasets exhibit that our proposed method is effective and dramatically beats other baseline models.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Emotional dialogue system strives to enable chatbots to recognize, understand, and express emotions when interacting with users, yielding more attractive and various reactions. Since emotional dialogue system empowers robots to interact with users in a more empathetic and human-like fashion, it has the potential to become a hot research field. Emotion is regarded as a brief but intense physiological response of the brain to a stimulus, and it is expressed in the form of facial expression, vocal tone, and behavioral change  [1] . Enabling robots to identify and understand emotions is a key aspect of realizing an emotional dialogue system. Hence, emotion recognition in conversation (ERC) has become a centerpiece technology to augment the performance of many applications and has received much attention in realms such as question-answer systems  [2] , opinion mining  [3] , recommender systems  [4] , and conversational robots  [5] . The target of ERC is to automatically recognize the emotions of the participants as they say each utterance according to the conversational content. The modeling style of ERC is dissimilar to that of non-conversational emotion recognition due to its multi-turn conversational scenario and the presence of natural emotion transition  [6, 7] . As shown in Figure  1 , it is extremely hard for the ERC system to accurately derive the corresponding emotional state based on the current utterance alone, but rather, it needs to make a comprehensive judgment by combining the contextual information in the conversation. Accordingly, how to maximally model the context in the conversation is of paramount importance to the ERC task.\n\nThere are a number of ERC efforts based on contextual modeling. Depending on the network structure of these works, they are primarily categorized into recurrent neural network (RNN)-based methods, graph neural network (GNN)-based methods, and multi-head attention (MHA)based methods. RNN-based models typically construct different modules utilizing long short-term memory (LSTM) or gated recurrent unit (GRU) to capture emotional cues at different aspects. Ghosal et al.  [8]  utilized different GRUs to establish five distinct states in a dialogue while adding commonsense knowledge to improve performance. Hu et al.  [9]  drew inspiration from the emotion cognition theory and constructed LSTM-based multi-turn inference module to extract emotional cues. These methods tend to extract limited information from the nearest neighbors, causing unsatisfactory performance improvements. GNN-based models construct the conversation as a graph from the spatial perspective to extract emotional cues between utterances. Lee et al.  [10]  considered the ERC task as a dialogue-based relation extraction and constructed a heterogeneous graph to capture the relationships between arguments in the dialogue. Li et al.  [11]  presented a psychological-knowledge-aware interaction graph by constructing a locally connected graph and introducing different types of edges to simulate the psychological interaction between utterances. These approaches We are actually not even allowed--you know, like we don't have toâ€¦  [",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "ğ’–ğ’– ğŸğŸ",
      "text": "You know, the airlines can not be held responsible for every bag ofâ€¦  [frustrated]  ğ’–ğ’– ğŸ“ğŸ“ Figure  1 : A conversational scenario. Combining current and contextual information is needed to comprehensively determine the emotion of the utterance to be predicted. can capture long-distance contextual information, thus effectively mitigating the shortcomings of RNN-based methods. Analogous to GNN-based models, MHA-based ones exploit global attention to retrieve long-distance emotional cues. Indeed, MHA is a network structure that operates graph attention  [12]  on the fully connected graph, i.e., MHA can be regarded as a special type of GNNs. HiTrans  [13]  employed two separate Transformer networks to model a conversation, where the low-level Transformer was utilized to capture local utterance information, and the high-level Transformer was leveraged to extract global contextual information. CoG-BART  [14]  modeled the conversation with the powerful comprehension and generation capabilities of BART  [15]  while employing contrastive learning to further upgrade performance.\n\nAlthough GNN-and MHA-based approaches address the drawback of the inability to capture long-distance contextual information, they ignore temporal sequence information in the conversation. To summarize, most of the existing efforts often fail to model the conversation from both temporal and spatial perspectives, i.e., they do not take into account the combination of temporal and spatial contextual information, making them infeasible to adequately extract contextual information. On the other side, the network structure of most ERC works is overly complex, e.g., employing encoder-decoder architecture  [13, 14] , containing too many components  [9, 10] , and integrating commonsense knowledge  [8, 11] , causing limited performance improvements to be obtained. This suggests that some complex structures may be redundant and useless in current ERC models. We try to leverage GRU and MHA networks in a straightforward manner to achieve a combination of RNN-and MHA-based approaches, which in turn can effectively extract spatiotemporal contextual information.\n\nPrevious studies  [8, 16, 17]  have demonstrated that emotion shift is one of reasons plaguing the failure of the ERC system. The emotion-shift problem refers to the difficulty for the ERC system to effectively handle scenarios where the utterances are consecutive but their true emotions are different. Theoretically, the higher the frequency of emotion shifts that occur in a conversation, the larger the difficulty of that conversation, i.e., the more difficult it is to classify samples (i.e., utterances) in that conversation. To mitigate the negative effects caused by emotion shift, we introduce a curriculum learning (CL)  [18]  strategy in the ERC task. Firstly, we adopt the frequency of emotion shifts in the conversation to measure the difficulty score of CL. Then, lower learning weights are assigned to these difficult samples at the beginning of training. As the increase of epoch, the learning weights for these samples are gradually grown. The main thought behind using CL is to first train the model with simpler data subsets, and then gradually employ more difficult subsets until the entire dataset is fully exploited to obtain an optimal model.\n\nIn a nutshell, we propose a novel Emotion Recognition Network in textual conversation based on the Curriculum Learning strategy (ERNetCL). The proposed method not only thoroughly models a conversation from both temporal and spatial perspectives but also alleviates the emotion-shift problem by utilizing the idea of \"training from simple to difficult data\" in CL. We conduct extensive comparison and ablation experiments on four public emotion datasets, and the results validate the effectiveness of our ERNetCL. Our contributions are summarized below:\n\nâ€¢ A novel conversational emotion recognition network (i.e., ERNetCL) based on curriculum learning is proposed. ERNetCL combines RNN-and MHA-based methods in a simplistic fashion to capture temporal and spatial contextual information.\n\nâ€¢ We apply curriculum learning strategy to the ERC task in order to progressively optimize the network parameters of ERNetCL from easy to hard. The frequency of emotion shift is used to measure the difficulty score of curriculum learning.\n\nâ€¢ We perform extensive comparative and ablative experiments on four baseline datasets, and the empirical results confirm that our proposed ERNetCL is superior to other baselines.\n\nThe rest of this paper is structured as follows. In Section 2, we describe related works. Section 3 corresponds to the methodology of this work, i.e., ERNetCL. Sections 4 is the description of the experimental setup. In Section 5, we report, discuss, and analyze experimental results. Section 6 is the summary of this work.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "2. Related Work",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "2.1. Emotion Recognition In Conversation",
      "text": "The goal of emotion recognition in conversation (ERC) is to assign a specific emotion to each utterance based on the content of the conversation. ERC has emerged as an influential research issue owing to its widespread implementation in numerous scenarios such as conversational robots  [19]  and mental health services  [20] . The ERC mission differs from non-conversational emotion recognition  [6, 7]  with isolated utterances in that it needs to integrate conversational intent and context. Existing ERC models can be categorized into three groups: recurrent neural network (RNN) based approaches, graph neural network (GNN) based approaches, and multi-head attention (MHA) based approaches. Gan et al.  [21]  proposed a hierarchical feature interactive fusion network that learns the cross impact of conversational emotion recognition and conversational intent recognition through collaborative attention to obtain deep semantic information. Jiao et al.  [22]  proposed a GRU-based ERC model in which a hierarchical memory network was utilized to extract the interactive information between historical utterances, and a bidirectional GRU was employed to summarize the recent and long-term memory attention weights. Zhao et al.  [23]  explicitly modeled intra-and inter-speaker dependencies by introducing commonsense knowledge as a cue for emotional cause detection in conversations. Bao et al.  [24]  presented a speaker-guided encoder-decoder framework called SGED to leverage speaker information for emotion decoding. These methods tend to focus on the near contexts and ignore the long-distance contexts, leading to limited performance improvements of models.\n\nTo address this issue, numerous models based on GNN and MHA have been proposed. Ghosal et al.  [25]  proposed the first ERC model based on GNN, which utilized utterances and their associations in a conversation to construct a graph. Shen et al.  [16]  modeled the dialogue by constructing the directed graph for the task of emotion recognition. Ren et al.  [26]  utilized a relationship graph network to integrate contextual information and speaker dependencies, and then extracted potential associations between utterances with the help of a multi-attention mechanism. Yang et al.  [27]  utilized pre-trained knowledge adapters to integrate linguistic and factual knowledge, and meanwhile they simplified the highdimensional supervised contrastive learning space into a three-dimensional affective representation space. Zhong et al.  [28]  implemented ERC using a hierarchical Transformer, where utterance-level self-attention and context-level selfattention modules were utilized to calculate utterance and context representations, respectively. Song et al.  [29]  employed the supervised prototypical contrastive learning to strengthen the classification ability of the model while designing a metric function based on inter-class distance to mitigate the effect of extreme samples. Son et al.  [30]  proposed a relational semantic based prompt guidance model that utilized the large pre-trained model to compensate for the low information density of multi-person conversation.\n\nYang et al.  [31]  proposed an auxiliary task for target utterance reconstruction based on the variational auto-encoder to improve model performance and regularize the latent spaces.\n\nAlthough ERC models based on GNN and MHA can capture long-distance contextual information, they are prone to ignore the temporal sequence information in the conversation. Conclusively, most extant ERC methods do not adequately mine contextual emotional cues, and they contain complex network structures. In our work, we attempt to utilize the advantages of RNN and MHA to fully extract contextual information from both temporal and spatial perspectives.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Curriculum Learning Strategy",
      "text": "Curriculum learning (CL) is a training strategy that mimics the way humans learn curriculum step by step from easy to difficult. The basic idea of CL is to train a model using simpler data subsets, and then gradually adopt harder subsets until an optimal model is obtained. Bengio et al.  [18]  first proposed the idea of CL under the background of machine learning, which was then widely applied in various fields such as image classification  [32, 33] , object detection  [34, 35] , and speech processing  [36, 37]  due to its effectiveness. The benefits of applying CL to these realms are mainly summarized as improving the performance of the target task and accelerating the training process  [38] .\n\nDogan et al.  [32]  proposed a label similarity curriculum method for image classification. This approach did not employ true labels to train the model in the early stages of the learning process, instead using the probability distribution of classes. Cascante-Bonilla et al.  [33]  presented a curriculum labeling method that enhanced the process of selecting the correct pseudo-labels through the use of curriculum based on the extreme value theory. Wang et al.  [34]  introduced an easy-to-hard method for weakly-and semi-supervised object detection that fine-tuned weakly annotated images. For unsupervised cross-domain object detection, Soviany et al.  [35]  utilized a self-paced curriculum approach. They proposed a multi-stage technique for better knowledge transfer from the source domain to target domain. Zhang et al.  [36]  presented a teacher-student strategy for digital modulation classification, which trained the teacher network with feedback from a preinitialized student network. Lotfian et al.  [37]  utilized CL for speech emotion detection, which applied an easy-to-hard batch technique and fine-tuned the learning rate for each bin.\n\nCL has also seen widespread application in natural language processing. Wang et al.  [39]  proposed a joint curriculum technique for neural machine translation that incorporated two levels of heuristics to build domain and denoising curriculums. Zhan et al.  [40]  provided a meta-CL strategy for tackling neural machine translation issue in cross-domain settings. Liu et al.  [41]  introduced a normbased CL method for increasing the training efficiency of neural machine translation system. With the use of the word embedding norm, this method measured the difficulty of the sentence, the ability of the model, and the weight of the sentence. Zhou et al.  [42]  established an easy-todifficult ranking by using data-level uncertainty and employed model-level uncertainty to determine the correct timing for augmenting the training set. Tay et al.  [43]  offered a generative curriculum pre-training strategy for tackling reading comprehension of long narratives. Shen et al.  [44]  suggested a curriculum dual learning model, which extended emotion-controlled response generation to a dual task for alternately generating emotional responses and emotional queries.\n\nTo the best of our knowledge, only Yang et al.  [45]  and Song et al.  [29]  have applied CL to ERC tasks. Yang et al.  [45]  proposed a CL framework for ERC that progressively enhanced the ability to recognize emotion through conversation-level and utterance-level curriculums. Song et al.  [29]  designed a difficulty metric function based on the distance between classes and introduced CL to mitigate the effects of extreme samples. In order to complement the research in this area, we design a new CL scheme for the ERC mission.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Proposed Ernetcl",
      "text": "Modeling the contexts from both temporal and spatial perspectives is conducive to extracting richer emotional information. To achieve this purpose, we adopt GRU-and MHA-based modules in ERNetCL to extract temporal and spatial information, respectively. The overall architecture of the proposed ERNetCL is shown in Figure  2 , which mainly includes temporal encoder (TE), spatial encoder (SE), and Emotional Classifier. First, we regard the conversation as a temporal sequence and utilize a GRU-based network, i.e., TE, to extract temporal contextual information; second, to enrich the emotional expressions of utterances from the spatial perspective, an MHA-based network, i.e., SE, is used to capture spatial contextual information in the conversation; and lastly, the features encoded sequentially by TE and SE are applied to emotion classification. Furthermore, in order to simulate the way humans learn curriculum from easy to hard, we replace the original loss with a curriculum learning one to progressively optimize the network parameters of ERNetCL.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Problem Definition",
      "text": "Before introducing the model, let us define the ERC task. Suppose a conversation îˆ¯ contains ğ‘› utterances, i.e., îˆ¯ = {ğ‘ 1 , ğ‘ 2 , â‹¯ , ğ‘ ğ‘› }. The goal of ERC is to predict the corresponding emotion ğ‘’ ğ‘– based on an utterance ğ‘ ğ‘– . If we notate îˆ± as the set of emotion labels, then ğ‘’ ğ‘– belongs to îˆ±. The number of elements in îˆ± depending on the dataset. For example, in the IEMOCAP dataset, the number of elements is 6, while in the MELD dataset, that is 7.\n\nThe definitions related to emotion shift are as follows. If two consecutive utterances to be determined have the same emotion, then it indicates that there is no emotion shift in these two utterances; otherwise, it indicates that there is an emotion shift in them. In our work, we take into account the speaker identity to determine the number of speakerspecific emotion shifts. More specifically, in a conversation, the utterances said by the same speaker are first formed into a sequence chronologically, and then the number of emotion shifts for that speaker is recorded.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Temporal Encoder",
      "text": "Existing efforts  [8, 9, 23]  have evidenced that the emotional state of the current utterance is affected by its context. To capture the contextual information in the conversation, we adopt a GRU-based temporal encoder (TE) to model that conversation. Concretely, the conversation is first considered as a temporal sequence, where each element of the sequence is an utterance; then, the GRU network, fully connected layer, dropout operation, residual operation, and normalization operation are sequentially employed to extract temporal contextual information. The temporal encoder in Figure  2  depicts the above process. Our TE can be expressed by the following equations:\n\nwhere ğ‘‹ ğ‘™ denotes the ğ‘™-th layer feature matrix of utterances in TE; ğ™±ğš’ğ™¶ğšğš„(â‹…) denotes the single-layer bidirectional GRU; ğ‘Š ğ¹ is the learnable parameter, which is used to reduce the feature dimension for ğ‘‹ ğ‘™ ğº to half of the original one; ğ™³ğ™¿(â‹…) and ğ™»ğ™½(â‹…) denote the dropout and normalization operations, respectively. Note that the residual operation sums the input of BiGRU and the output of dropout operation, and our normalization operation is layer normalization.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Spatial Encoder",
      "text": "It is not enough to only consider the contextual information in a conversation from the temporal perspective. Existing studies  [46, 47]  have demonstrated that the MHA network has a powerful capability for feature extraction. In this subsection, we exploit an MHA-based spatial encoder (SE) to extract spatial contextual information. Specifically, we first assume that the current utterance is directly associated with all utterances in the conversation, i.e., all contextual utterances are regarded as direct neighbors of the current utterance; then, the MHA network, dropout operation, residual operation, and normalization operation are applied to compute spatial contextual information. The above process is depicted by the spatial encoder in Figure  2 . Our SE can be formulated as follows:\n\nwhere X ğ‘™ denotes the ğ‘™-th layer feature matrix of utterances in SE; similar to TE, ğ™³ğ™¿(â‹…) and ğ™»ğ™½(â‹…) are the dropout and normalization operations, respectively; The residual operation performs a summation between the input of MHA and the The overall architecture of our proposed ERNetCL. The proposed method sequentially abstracts temporal and spatial contextual cues through temporal and spatial encoders. In the training phase, the curriculum learning loss is adopted to optimize the network parameters instead of the original loss. output of dropout operation; ğ™¼ğ™·ğ™°(â‹…) denotes the multi-head attention network, it can be expressed as follows,\n\nwhere ğ‘Š ğ‘™ ğ¶ , ğ‘„ ğ‘™ ğ‘– , ğ¾ ğ‘™ ğ‘– , and ğ‘‰ ğ‘™ ğ‘– denote the trainable parameters; head ğ‘™ ğ‘– is the ğ‘™-th layer output of the ğ‘–-th head attention network; ğ» is the number of head in MHA; ğ‘‘ ğ‘˜ denotes the dimension of ğ¾ ğ‘™ ğ‘– X ğ‘™ or ğ‘‰ ğ‘™ ğ‘– X ğ‘™ ; ğ™²ğ™°ğšƒ(â‹…) denotes the concatenation operation, and ğš‚ğš˜ğšğšğ™¼ğšŠğš¡(â‹…) denotes the softmax function. It is worth noting that we do not incorporate positional encoding when extracting spatial context information since TE contains temporal sequence information.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Emotional Classifier",
      "text": "After multiple layers of temporal and spatial encoding, we can obtain the feature matrix H of utterances. Finally, the feature dimension of the utterance is reduced to |îˆ±| by the fully connected layer, which in turn yields the predicted emotion ğ‘’ â€² ğ‘– . The process can be formulated as follows:\n\nwhere h ğ‘– is the feature representation of the ğ‘–-th utterance, h ğ‘– âˆˆ H; ğ‘Š ğ‘† is the learnable parameter, and ğ™°ğš›ğšğ™¼ğšŠğš¡(â‹…) is the argmax function. To optimize the parameters of the model, the loss function is defined as:\n\nwhere ğ‘›(ğ‘–) is the number of utterances in the ğ‘–-th conversation, and ğ‘ îˆ¯ is the number of all conversations in training set; ğ‘¦ â€² ğ‘–,ğ‘— denotes the probability distribution of predicted emotion label of the ğ‘—-th utterance in the ğ‘–-th conversation, and ğ‘¦ ğ‘–,ğ‘— denotes the ground truth label.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Curriculum Learning Loss",
      "text": "Several studies  [8, 17, 16]  have revealed that the ERC task suffers from emotion-shift problem. If the number of emotion shifts in a conversation is higher, the probability that utterances in that conversation are correctly classified is lower. To put it differently, the higher the frequency of emotion shifts in a conversation, the greater the classification difficulty of that conversation. Since the architectural design of existing neural networks draws inspiration from the human brain, their training process should also be inspired by the way humans learn. Curriculum learning (CL) is developed precisely with the goal of mimicking the way humans learn curriculum in an easy-to-hard sequence. To alleviate the emotion-shift problem, we adopt the idea of CL to progressively optimize the network parameters of ERNetCL from simple to difficult.\n\nOne of the pivotal issues in CL is how to measure difficulty score. Based on the previous analysis, we utilize the frequency of emotion shifts in each conversation to measure the classification difficulty of that conversation (i.e., difficulty score of CL). Our difficulty score is defined as follows:\n\nwhere ğ·(îˆ¯ ğ‘– ) takes the value range of [0, 1]; ğ‘ ğ‘ ğ‘ (îˆ¯ ğ‘– ) denotes the number of speakers in conversation îˆ¯ ğ‘– , ğ‘ ğ‘ â„ (ğ‘ , îˆ¯ ğ‘– ) and ğ‘ ğ‘¢ğ‘¡ (ğ‘ , îˆ¯ ğ‘– ) denote the number of emotion shifts for speaker ğ‘  and the total number of utterances uttered by ğ‘  in conversation îˆ¯ ğ‘– , respectively. The implication of the above equation is that the higher the average frequency of emotion shifts per participant in a conversation, the greater the classification difficulty of utterances. We construct a weight function ğœ” ğ‘– (ğ‘¡) that varies with the epoch ğ‘¡ and classification difficulty ğ·(îˆ¯ ğ‘– ). The objective of ğœ” ğ‘– (ğ‘¡) is to assign smaller learning weights to difficult utterances (also known as difficult samples) at the beginning of training. As the epoch increases, the learning weights of difficult samples are gradually increased. Consequently, ğœ” ğ‘– (ğ‘¡) is defined as follows:\n\nwhere\n\nwhere ğœ” ğ‘–,1 (ğ‘¡) = ğœ” ğ‘–,2 (ğ‘¡) = â‹¯ = ğœ” ğ‘–,ğ‘›(ğ‘–) (ğ‘¡), and ğ‘¡ stands for the current epoch; as with the original loss, ğ‘›(ğ‘–) indicates the number of utterances in the ğ‘–-th conversation, and ğ‘ îˆ¯ represents the total number of conversations in training set; ğ‘¦ â€² ğ‘–,ğ‘— (ğ‘¡) denotes the ğ‘¡-th epoch probability distribution of the ğ‘—th utterance in the ğ‘–-th conversation, and ğ‘¦ ğ‘–,ğ‘— is the true label. In the training phase, we employ the curriculum learning loss îˆ¸(ğ‘¡) instead of the original loss îˆ¸.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Experimental Setup",
      "text": "",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Datasets",
      "text": "We evaluate our ERNetCL on four public datasets, including MELD  [48] , IEMOCAP  [49] , EmoryNLP  [50] , and DailyDialog  [51] .\n\nMELD contains 1,433 dialogues with a total of 13,708 utterances. The dataset is composed of multi-party conversation videos sourced from the TV series Friends. Each utterance in the dataset is annotated with one of seven emotions, namely joy, anger, fear, disgust, sadness, surprise, and neutral.\n\nIEMOCAP contains 151 conversations with a total of 7,433 utterances. The dataset comprises dyadic conversation videos with ten distinct speakers, with the first eight speakers included in the training set and the remaining two included in the test set. The utterance is labeled with one of six emotions, namely happy, sad, neutral, angry, excited, and frustrated.\n\nEmoryNLP contains 827 dialogues with a total of 9,489 utterances. The dataset is another one collected from the TV series Friends with only textual modality. The utterances in this dataset are annotated into seven classes, and they are neutral, joyful, peaceful, powerful, scared, mad, and sad.\n\nDailyDialog contains 13,118 conversations with a total of 102,979 utterances. The dataset is a large-scale multi-turn dyadic dialogue dataset with the conversations reflecting various topics in daily life. Each utterance in the dataset is labeled with one of seven emotion categories: neutral, happiness, surprise, sadness, anger, disgust, and fear. The Table  1  The statistics for these four emotion datasets. Here, #Con and #Utt denote the number of conversations and utterances, respectively. dataset suffers from a severe class imbalance, with over 83% of emotion labels being neutral.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Dataset",
      "text": "The statistics are reported in Table  1 . According to COSMIC  [8] , we use utterance-level textual features which are fine-tuned adopting RoBERTa  [52]  to implement ERC task. Note that we don't adopt the features of other modalities (i.e., acoustic and visual modalities), and the split of training/validation/testing is consistent with COSMIC.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Baselines",
      "text": "To demonstrate the validity of the proposed ERNetCL, we choose some representative baselines for comparison, as follows:\n\nCOSMIC  [8]  leveraged multiple GRUs and incorporated commonsense knowledge to capture complex interactions.\n\nHiTrans  [13]  proposed a hierarchical framework that consists of two different Transformers to capture contextand speaker-sensitive information.\n\nAGHMN  [22]  proposed a hierarchical memory network and attention GRU to better utilize the attention weights to improve the performance.\n\nDialogueCRN  [9]  was a cognitive-inspired network that utilized LSTM networks to construct multi-turn reasoning modules and capture implicit emotional clues in the conversation.\n\nSKAIG-ERC  [11]  generated the knowledge representation of edges with the aid of commonsense knowledge to enhance emotional expression.\n\nDialogXL  [17]  utilized a modified XLNet  [53]  to encode multi-turn dialogues that were organized in a sliding window, and its dialog-aware self-attention contained four distinct attention modules.\n\nCauAIN  [23]  retrieved causal clues in commonsense knowledge to enrich the modeling of intra-and inter-speaker dependencies.\n\nCoG-BART  [14]  proposed a novel ERC approach that adopted both contrastive learning and generative modeling to ensure the fact that different emotions were mutually exclusive.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Training Settings",
      "text": "The operating system which we use is Ubuntu 20.04, and the programming language is Python 3.9.12. Our experiments are conducted on a single NVIDIA GeForce RTX",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Results And Analysis",
      "text": "In this section, we report the experimental results of the proposed ERNetCL on these four datasets. We compare these results with those of the baselines. For the MELD, IEMOCAP, and EmoryNLP datasets, we utilize weighted F1 and micro F1 scores as evaluation metrics. For the DailyDialog dataset, we adopt macro F1 score and micro F1 score without neutral to evaluate our model since neutral accounts for about 83% in the dataset. In addition, we investigate the impact of different modules or settings on the performance of ERNetCL.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Overall Comparison",
      "text": "The experimental results of ERNetCL and comparative models are reported in Table  3 . From these results, we can draw the following conclusions:\n\n(1) On the MELD dataset, ERNetCL attains a weighted F1 score of 66.31%, which is 0.85% higher than that of 65.46% obtained by CauAIN. This suggests that even though CauAIN introduces causal cues, it still does not thoroughly model context. The proposed method achieves a micro F1 score of 67.43%, which is 1.48% higher relative to that of CoG-BART. A probable explanation for the discrepancy is the additional noise introduced by the response generation module in CoG-BART. (2) On the IEMOCAP dataset, the weighted F1 score and micro F1 score of our ERNetCL are 69.73% and 69.75%, respectively, which are much higher than those of other models. For example, ERNetCL's weighted F1 score is 3.79% higher than DialogXL's, while its micro F1 score improves by 3.04% with respect to CoG-BART's. These superior results are facilitated by the fact that ERNetCL can incorporate the idea of curriculum learning and extract contextual information from both temporal and spatial perspectives. (3) On the EmoryNLP dataset, the weighted F1 score of HiTrans is 36.75%, which is 2.96% lower than that of ERNetCL's 39.71%; and the micro F1 score of CoG-BART is 42.58%, while the score of our method is 44.21%, which is an improvement of 1.63%. One possible reason is that HiTrans and CoG-BART have difficulty extracting chronological information from the conversation. (4) On the DailyDialog dataset, the macro F1 score of our ERNetCL is 53.09%, which is 0.76% lower than that of CauAIN; but the micro F1 score of CauAIN is 58.21%, which is 1.96% lower than that of our method, which is 60.05%. The main reason for the limited performance of ERNetCL is due to the class imbalance problem in the DailyDialog dataset.\n\nIn most cases, our model achieves optimal performance on these four datasets compared to all baselines. This indicates that our proposed ERNetCL can utilize simple encoding structures to effectively capture temporal and spatial contextual information in the conversation.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Results For Each Emotion",
      "text": "As shown in Figure  3 , we show the F1 scores for each emotion on the MELD and IEMOCAP datasets. On the MELD dataset, ERNetCL's F1 scores for all emotions are higher than AGHMN's results. For instance, the result of ERNetCL for surprise is 58.42%, which is 8.72% higher than that of AGHMN. It is worth noting that ERNetCL's F1 scores for the minority class fear and disgust are significantly higher than AGHMN's results. A possible reason is that the curriculum learning loss of ERNetCL mitigates the classimbalanced problem of the MELD dataset to some extent. On the IEMOCAP dataset, the scores of our ERNetCL for all emotions are higher than those of AGHMN. For emotion sad, the F1 score of AGHMN is 73.30%, while that of ERNetCL is 83.82%, which is a significant improvement of 10.52%.\n\nIn addition, on the MELD dataset, ERNetCL obtains the highest F1 score for neutral in comparison to those for other emotions. By examining the class distribution of MELD, we find that neutral belongs to the majority class by an absolute proportional advantage with a share of about 46.95%. The emotion that achieves the highest F1 score on the IEMOCAP dataset is sad. However, sad is not the category with the highest percentage in the IEMOCAP dataset. The main reason for this phenomenon is that the class distribution of IEMOCAP is relatively balanced, and the slight classimbalanced problem barely affects the performance of the model.\n\nFigure  4  shows the T-SNE visualization of the IEMO-CAP dataset. It can be seen that each emotion can be distinguished more easily after feature extraction with the proposed ERNetCL. This situation shows that ERNetCL is powerful in feature extraction.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Ablation Studies",
      "text": "In order to explore the contribution of each component (i.e., SE, TE, and CL) separately, we report the experimental results regarding the removal of different components in    Table  4  and Figure  5 . Overall, regardless of which component is removed, the performance of ERNetCL suffers a degradation.\n\nEffectiveness of TE: Table  4  shows the results on the MELD and IEMOCAP datasets. When TE is removed, the weighted F1 scores of our model on these two datasets Effectiveness of SE: As can be seen in Table  4 , when we remove SE, there are decreases of 0.46% and 3.88% in the micro F1 scores on the MELD and IEMOCAP datasets, respectively. It can also be observed from Figure  5  that the similar declines occur on the EmoryNLP and DailyDialog datasets when SE is removed. The above results indicate that SE can effectively extract spatial context information. Effectiveness of CL: We remove the CL loss and replace it with the standard cross-entropy loss (i.e., Equation  5 ). As shown in Table  4 , non-use of the CL loss brings about 0.42% and 1.79% drops in micro F1 scores on the MELD and IEMOCAP datasets, respectively. A similar situation appears on the other two datasets, as shown in Figure  5 . The above phenomenon reveals that CL can reduce the learning difficulty of the network and effectively promote the performance of the model. Thus, it can be concluded that CL loss prevails over the standard loss and somewhat eases the detrimental impacts associated with emotion shift.  To examine the impact of TE with different network depths (number of layers) on the performance, we first fix the network depth of SE, then adjust that of TE and report the micro F1 scores. As shown in Figure  6 , the yellow line is the experimental result for testing the network depths of TE. It can be viewed that as the network depth increases, the micro F1 score shows a tendency to increase and then decrease. Similarly, by fixing the network depth of TE, we can explore the effect of SE with different network depths on the performance. The experimental results are shown with the orange line in Figure  6 , where the performance increases and then decreases as the number of network layers grows.  Instead of using emotion labels, we use sentiment labels for the three-classification task. As can be seen in Table  5 , all the experimental results show significant improvements relative to those for emotion classification. For example, the weighted F1 scores on the MELD and EmoryNLP datasets are improved by 6.96% and 17.35%, respectively. This is due to the fact that the classification difficulty is reduced after the emotion labels are coarsened to sentiments. This phenomenon can be illustrated with the T-SNE visualization in Figure  7 . The sentiment classification is easier to distinguish each category than the emotion classification. Compared to the weighted F1 scores of COSMIC under sentiment classification, our scores are only marginally improved. A possible explanation is that the lower the difficulty of the dataset, the   less the feature extraction capability is required of the model. Thus, ERNetCL-S and COSMIC-S have similar results.",
      "page_start": 7,
      "page_end": 10
    },
    {
      "section_name": "Impact Of Network Depths",
      "text": "",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Results Of Sentiment Classification",
      "text": "",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Limitations",
      "text": "The categories in most emotion datasets are unbalanced with each other, i.e., there is a class-imbalanced problem. Figure  8  illustrates the number of utterances for each emotion class in the MELD and DailyDialog datasets. It can be seen that in the MELD dataset, neutral is the majority class with about 47%; in the DailyDialog dataset, neutral is the majority class with approximately 83%. Like previous ERC models, the performance of ERNetCL is limited by the class-imbalanced problem. Figure  9  shows confusion matrices on the MELD and DailyDialog datasets. In the MELD dataset, three minority classes, i.e., fear, sad, and disgust, are easily recognized as the majority class neutral. Since DailyDialog is an extremely class-imbalanced dataset, most emotions tend to be detected as the majority class neutral. Additionally, another limitation is the similar-emotional problem, that is, an emotion class is easily categorized as the similar emotion in some cases. As shown in Figure  9a , the true emotion disgust is easily recognized as anger.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Conclusion",
      "text": "Previous ERC approaches fail to model the context from both temporal and spatial perspectives, and the network designs of these models are overly complex, e.g., requiring commonsense knowledge, employing encoder-decoder architecture, and containing too many components. The above issues may cause these models to generate redundant information, leading to weak performance enhancements. In this work, we propose a novel emotion recognition method, ER-NetCL, to fully extract contextual emotional cues from the conversation. ERNetCL combines RNN-and MHA-based approaches straightforwardly, employing temporal and spatial encoders to capture the contextual information of the utterance from the temporal and spatial perspectives, respectively. Furthermore, to relieve the adverse affects caused by emotion shift and further enhance the performance of the model, we employ the CL strategy to train the proposed model in a fashion that simulates humans learning curriculum. We define the difficulty score of CL using the frequency of emotion shifts in the conversation, assigning different learning weights to the samples in the training set. We conduct extensive experiments on four widely used datasets to evaluate the proposed method. Empirical results attest that ERNetCL can effectively model context in a simple manner and significantly outperforms other comparative models in most cases.\n\nIn future work, we will investigate the ERC tasks based on multimodal fusion and explore the further application of curriculum learning in them. Moreover, reinforcement learning and contrast learning have recently received a great deal of attention. The former can utilize the reward mechanism to return a higher score for the minority class and a lower score for the majority class. Contrastive learning can pull the same classes closer together and push different classes farther apart. Therefore, we intend to integrate these techniques into the ERC model to overcome the class-imbalanced and similar-emotional problems.",
      "page_start": 10,
      "page_end": 11
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: , it is extremely hard for the ERC system to",
      "page": 1
    },
    {
      "caption": "Figure 1: A conversational scenario. Combining current and",
      "page": 2
    },
    {
      "caption": "Figure 2: , which mainly",
      "page": 4
    },
    {
      "caption": "Figure 2: depicts the above process. Our TE can be expressed by the",
      "page": 4
    },
    {
      "caption": "Figure 2: Our SE can be",
      "page": 4
    },
    {
      "caption": "Figure 2: The overall architecture of our proposed ERNetCL. The proposed method sequentially abstracts temporal and spatial",
      "page": 5
    },
    {
      "caption": "Figure 3: , we show the F1 scores for each",
      "page": 7
    },
    {
      "caption": "Figure 4: shows the T-SNE visualization of the IEMO-",
      "page": 7
    },
    {
      "caption": "Figure 3: F1 score for each emotion on the MELD and IEMOCAP datasets. Our modelâ€™s F1 scores for all emotions are higher",
      "page": 8
    },
    {
      "caption": "Figure 4: T-SNE visualization of IEMOCAP before and after",
      "page": 8
    },
    {
      "caption": "Figure 5: Overall, regardless of which com-",
      "page": 8
    },
    {
      "caption": "Figure 5: Results after removing each component on the",
      "page": 9
    },
    {
      "caption": "Figure 5: The above phenomenon reveals that CL can reduce the",
      "page": 9
    },
    {
      "caption": "Figure 6: The impact of the network depths on the perfor-",
      "page": 9
    },
    {
      "caption": "Figure 6: , the yellow line",
      "page": 9
    },
    {
      "caption": "Figure 6: , where the performance increases",
      "page": 9
    },
    {
      "caption": "Figure 7: T-SNE visualization of emotion and sentiment",
      "page": 9
    },
    {
      "caption": "Figure 7: The sentiment classification is easier to distinguish",
      "page": 9
    },
    {
      "caption": "Figure 8: The number of utterances for each emotion in the MELD and DailyDialog datasets. The data annotation includes",
      "page": 10
    },
    {
      "caption": "Figure 9: The confusion matrices on the MELD and DailyDialog datasets. The vertical and horizontal axes denote the true and",
      "page": 10
    },
    {
      "caption": "Figure 8: illustrates the number of utterances for each emo-",
      "page": 10
    },
    {
      "caption": "Figure 9: shows confusion",
      "page": 10
    }
  ],
  "tables": [
    {
      "caption": "Table 1: of difficult samples are gradually increased. Consequently,",
      "data": [
        {
          "Dataset": "#Con",
          "MELD IEMOCAP\nEmoryNLP DailyDialog": "1,039\n659\n11,118\n120\n114\n89\n1,000\n280\n31\n79\n1,000"
        },
        {
          "Dataset": "#Utt",
          "MELD IEMOCAP\nEmoryNLP DailyDialog": "9,989\n7,551\n87,170\n5,810\n1,109\n954\n8,069\n2,610\n1,623\n984\n7,740"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table 3: PerformancecomparisonofERNetCLwithallbaselines.Resultsforallbaselinesareobtainedfromtheoriginalpaper.Bestresults",
      "data": [
        {
          "Method": "",
          "MELD": "weighted-F1\nmicro-F1",
          "IEMOCAP": "weighted-F1\nmicro-F1",
          "EmoryNLP": "weighted-F1\nmicro-F1",
          "DailyDialog": "macro-F1\nmicro-F1"
        },
        {
          "Method": "COSMIC [8]\nHiTrans [13]\nAGHMN [22]\nDialogueCRN [9]\nSKAIG-ERC [11]\nDialogXL [17]\nCauAIN [23]\nCoG-BART [14]",
          "MELD": "65.21\n-\n61.94\n-\n58.10\n-\n58.39\n-\n65.18\n-\n62.41\n-\n65.46\n-\n64.81\n65.95",
          "IEMOCAP": "65.28\n-\n64.50\n-\n63.50\n-\n66.20\n-\n66.96\n-\n65.94\n-\n67.61\n-\n66.18\n66.71",
          "EmoryNLP": "38.11\n-\n36.75\n-\n-\n-\n-\n-\n38.88\n-\n34.73\n-\n-\n-\n39.04\n42.58",
          "DailyDialog": "51.05\n58.48\n-\n-\n-\n-\n-\n-\n51.95\n59.75\n-\n54.93\n53.85\n58.21\n-\n56.29"
        },
        {
          "Method": "ERNetCL",
          "MELD": "66.31\n67.43",
          "IEMOCAP": "69.73\n69.75",
          "EmoryNLP": "39.71\n44.21",
          "DailyDialog": "60.17\n53.09"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table 3: PerformancecomparisonofERNetCLwithallbaselines.Resultsforallbaselinesareobtainedfromtheoriginalpaper.Bestresults",
      "data": [
        {
          "Method": "ERNetCL",
          "MELD\nweighted-F1 micro-F1": "66.31\n67.43",
          "IEMOCAP\nweighted-F1 micro-F1": "69.73\n69.75"
        },
        {
          "Method": "-w/o TE\n-w/o SE\n-w/o CL",
          "MELD\nweighted-F1 micro-F1": "64.47\n65.86\n65.98\n66.97\n65.98\n67.01",
          "IEMOCAP\nweighted-F1 micro-F1": "65.89\n65.99\n65.58\n65.87\n67.84\n67.96"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table 4: , non-use of the CL loss brings about",
      "data": [
        {
          "Method": "",
          "MELD": "weighted-F1 micro-F1 weighted-F1 micro-F1",
          "EmoryNLP": ""
        },
        {
          "Method": "COSMIC-E\nCOSMIC-S",
          "MELD": "65.21\n-\n73.20\n-",
          "EmoryNLP": "38.11\n-\n56.51\n-"
        },
        {
          "Method": "ERNetCL-E\nERNetCL-S",
          "MELD": "66.31\n67.43\n73.27\n73.26",
          "EmoryNLP": "39.71\n44.21\n57.06\n57.11"
        }
      ],
      "page": 9
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Beyond Emotion: A multi-modal dataset for human desire understanding",
      "authors": [
        "A Jia",
        "Y He",
        "Y Zhang",
        "S Uprety",
        "D Song",
        "C Lioma"
      ],
      "year": "2022",
      "venue": "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
      "doi": "10.18653/v1/2022.naacl-main.108"
    },
    {
      "citation_id": "2",
      "title": "Sentiment enhanced answer generation and information fusing for product-related question answering",
      "authors": [
        "Y Du",
        "X Jin",
        "R Yan",
        "J Yan"
      ],
      "year": "2023",
      "venue": "Information Sciences",
      "doi": "10.1016/j.ins.2023.01.098"
    },
    {
      "citation_id": "3",
      "title": "An enhanced guided lda model augmented with bert based semantic strength for aspect term extraction in sentiment analysis",
      "authors": [
        "M Venugopalan",
        "D Gupta"
      ],
      "year": "2022",
      "venue": "Knowledge-Based Systems",
      "doi": "10.1016/j.knosys.2022.108668"
    },
    {
      "citation_id": "4",
      "title": "Towards emotion-aware recommender systems: an affective coherence model based on emotion-driven behaviors",
      "authors": [
        "M Polignano",
        "F Narducci",
        "M De Gemmis",
        "G Semeraro"
      ],
      "year": "2021",
      "venue": "Expert Systems with Applications",
      "doi": "10.1016/j.eswa.2020.114382"
    },
    {
      "citation_id": "5",
      "title": "Knowing what to say: Towards knowledge grounded code-mixed response generation for open-domain conversations",
      "authors": [
        "G Singh",
        "M Firdaus",
        "S Shambhavi",
        "A Mishra",
        "Ekbal"
      ],
      "year": "2022",
      "venue": "Knowledge-Based Systems",
      "doi": "10.1016/j.knosys.2022.108900"
    },
    {
      "citation_id": "6",
      "title": "Relational graph attention network for aspect-based sentiment analysis",
      "authors": [
        "K Wang",
        "W Shen",
        "Y Yang",
        "X Quan",
        "R Wang"
      ],
      "year": "2020",
      "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
      "doi": "10.18653/v1/2020.acl-main.295"
    },
    {
      "citation_id": "7",
      "title": "Relation-aware collaborative learning for unified aspect-based sentiment analysis",
      "authors": [
        "Z Chen",
        "T Qian"
      ],
      "year": "2020",
      "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
      "doi": "10.18653/v1/2020.acl-main.340"
    },
    {
      "citation_id": "8",
      "title": "COS-MIC: Commonsense knowledge for emotion identification in conversations",
      "authors": [
        "D Ghosal",
        "N Majumder",
        "A Gelbukh",
        "R Mihalcea",
        "S Poria"
      ],
      "year": "2020",
      "venue": "Findings of the Association for Computational Linguistics: EMNLP 2020"
    },
    {
      "citation_id": "9",
      "title": "Contextual reasoning networks for emotion recognition in conversations",
      "authors": [
        "D Hu",
        "L Wei",
        "X Huai",
        "Dialoguecrn"
      ],
      "year": "2021",
      "venue": "Proceedings of the Annual Meeting of the Association for Computational Linguistics and the International Joint Conference on Natural Language Processing",
      "doi": "10.18653/v1/2021.acl-long.547"
    },
    {
      "citation_id": "10",
      "title": "Graph based network with contextualized representations of turns in dialogue",
      "authors": [
        "B Lee",
        "Y Choi"
      ],
      "year": "2021",
      "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics, Online and Punta Cana",
      "doi": "10.18653/v1/2021.emnlp-main.36"
    },
    {
      "citation_id": "11",
      "title": "Past, Present, and Future: Conversational emotion recognition through structural modeling of psychological knowledge",
      "authors": [
        "J Li",
        "Z Lin",
        "P Fu",
        "W Wang"
      ],
      "year": "2021",
      "venue": "Findings of the Association for Computational Linguistics: EMNLP 2021",
      "doi": "10.18653/v1/2021.findings-emnlp.104"
    },
    {
      "citation_id": "12",
      "title": "Graph attention networks",
      "authors": [
        "P VeliÄkoviÄ‡",
        "G Cucurull",
        "A Casanova",
        "A Romero",
        "P LiÃ²",
        "Y Bengio"
      ],
      "year": "2018",
      "venue": "Proceedings of International Conference on Learning Representations"
    },
    {
      "citation_id": "13",
      "title": "HiTrans: A transformerbased context-and speaker-sensitive model for emotion detection in conversations",
      "authors": [
        "J Li",
        "D Ji",
        "F Li",
        "M Zhang",
        "Y Liu"
      ],
      "year": "2020",
      "venue": "Proceedings of the 28th International Conference on Computational Linguistics, International Committee on Computational Linguistics"
    },
    {
      "citation_id": "14",
      "title": "Contrast and generation make BART a good dialogue emotion recognizer",
      "authors": [
        "S Li",
        "H Yan",
        "X Qiu"
      ],
      "year": "2022",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "15",
      "title": "Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension",
      "authors": [
        "M Lewis",
        "Y Liu",
        "N Goyal",
        "M Ghazvininejad",
        "A Mohamed",
        "O Levy",
        "V Stoyanov",
        "L Zettlemoyer"
      ],
      "year": "2020",
      "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
      "doi": "10.18653/v1/2020.acl-main.703"
    },
    {
      "citation_id": "16",
      "title": "Directed acyclic graph network for conversational emotion recognition",
      "authors": [
        "W Shen",
        "S Wu",
        "Y Yang",
        "X Quan"
      ],
      "year": "2021",
      "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing"
    },
    {
      "citation_id": "17",
      "title": "All-in-one xlnet for multi-party conversation emotion recognition",
      "authors": [
        "W Shen",
        "J Chen",
        "X Quan",
        "Z Xie"
      ],
      "year": "2021",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence",
      "doi": "10.1609/aaai.v35i15.17625"
    },
    {
      "citation_id": "18",
      "title": "Curriculum learning",
      "authors": [
        "Y Bengio",
        "J Louradour",
        "R Collobert",
        "J Weston"
      ],
      "year": "2009",
      "venue": "Proceedings of the 26th Annual International Conference on Machine Learning, ICML '09, Association for Computing Machinery",
      "doi": "10.1145/1553374.1553380"
    },
    {
      "citation_id": "19",
      "title": "The rise of emotion-aware conversational agents: Threats in digital emotions",
      "authors": [
        "M Mensio",
        "G Rizzo",
        "M Morisio"
      ],
      "year": "2018",
      "venue": "Proceedings of the Web Conference, WWW '18, International World Wide Web Conferences Steering Committee, Republic and Canton of Geneva",
      "doi": "10.1145/3184558.3191607"
    },
    {
      "citation_id": "20",
      "title": "Deep convolution network based emotion analysis towards mental health care",
      "authors": [
        "Z Fei",
        "E Yang",
        ".-U Li",
        "S Butler",
        "W Ijomah",
        "X Li",
        "H Zhou"
      ],
      "year": "2020",
      "venue": "Neurocomputing",
      "doi": "10.1016/j.neucom.2020.01.034"
    },
    {
      "citation_id": "21",
      "title": "Dhf-net: A hierarchical feature interactive fusion network for dialogue emotion recognition",
      "authors": [
        "C Gan",
        "Y Yang",
        "Q Zhu",
        "D Jain",
        "V Struc"
      ],
      "year": "2022",
      "venue": "Expert Systems with Applications",
      "doi": "10.1016/j.eswa.2022.118525"
    },
    {
      "citation_id": "22",
      "title": "Real-Time emotion recognition via attention gated hierarchical memory network",
      "authors": [
        "W Jiao",
        "M Lyu",
        "I King"
      ],
      "year": "2020",
      "venue": "Proceedings of the Thirty-Fourth AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "23",
      "title": "Causal aware interaction network for emotion recognition in conversations",
      "authors": [
        "W Zhao",
        "Y Zhao",
        "X Lu"
      ],
      "year": "2022",
      "venue": "Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence, IJCAI-22",
      "doi": "10.24963/ijcai.2022/628"
    },
    {
      "citation_id": "24",
      "title": "Speaker-guided encoderdecoder framework for emotion recognition in conversation",
      "authors": [
        "Y Bao",
        "Q Ma",
        "L Wei",
        "W Zhou",
        "S Hu"
      ],
      "year": "2022",
      "venue": "Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence, IJCAI 2022, ijcai.org",
      "doi": "10.24963/IJCAI.2022/562"
    },
    {
      "citation_id": "25",
      "title": "Dia-logueGCN: A graph convolutional neural network for emotion recognition in conversation",
      "authors": [
        "D Ghosal",
        "N Majumder",
        "S Poria",
        "N Chhaya",
        "A Gelbukh"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
      "doi": "10.18653/v1/D19-1015"
    },
    {
      "citation_id": "26",
      "title": "Latent relationaware graph convolutional network for conversational emotion recognition",
      "authors": [
        "M Ren",
        "X Huang",
        "W Li",
        "D Song",
        "W Nie"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Multimedia",
      "doi": "10.1109/TMM.2021.3117062"
    },
    {
      "citation_id": "27",
      "title": "Cluster-level contrastive learning for emotion recognition in conversations",
      "authors": [
        "K Yang",
        "T Zhang",
        "H Alhuzali",
        "S Ananiadou"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Affective Computing",
      "doi": "10.1109/TAFFC.2023.3243463"
    },
    {
      "citation_id": "28",
      "title": "Knowledge-enriched transformer for emotion detection in textual conversations",
      "authors": [
        "P Zhong",
        "D Wang",
        "C Miao"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
      "doi": "10.18653/v1/D19-1016"
    },
    {
      "citation_id": "29",
      "title": "Supervised prototypical contrastive learning for emotion recognition in conversation",
      "authors": [
        "X Song",
        "L Huang",
        "H Xue",
        "S Hu"
      ],
      "year": "2022",
      "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "30",
      "title": "GRASP: Guiding model with Re-lAtional semantics using prompt for dialogue relation extraction",
      "authors": [
        "J Son",
        "J Kim",
        "J Lim",
        "H Lim"
      ],
      "year": "2022",
      "venue": "Proceedings of the 29th International Conference on Computational Linguistics, International Committee on Computational Linguistics"
    },
    {
      "citation_id": "31",
      "title": "Disentangled variational autoencoder for emotion recognition in conversations",
      "authors": [
        "K Yang",
        "T Zhang",
        "S Ananiadou"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Affective Computing",
      "doi": "10.1109/TAFFC.2023.3280038"
    },
    {
      "citation_id": "32",
      "title": "Label-similarity curriculum learning",
      "authors": [
        "Ãœ Dogan",
        "A Deshmukh",
        "M Machura",
        "C Igel"
      ],
      "year": "2020",
      "venue": "Computer Vision -ECCV 2020"
    },
    {
      "citation_id": "33",
      "title": "Curriculum labeling: Revisiting pseudo-labeling for semi-supervised learning",
      "authors": [
        "P Cascante-Bonilla",
        "F Tan",
        "Y Qi",
        "V Ordonez"
      ],
      "year": "2021",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence",
      "doi": "10.1609/aaai.v35i8.16852"
    },
    {
      "citation_id": "34",
      "title": "Weakly-and semi-supervised faster r-cnn with curriculum learning",
      "authors": [
        "J Wang",
        "X Wang",
        "W Liu"
      ],
      "year": "2018",
      "venue": "24th International Conference on Pattern Recognition (ICPR)",
      "doi": "10.1109/ICPR.2018.8546088"
    },
    {
      "citation_id": "35",
      "title": "Curriculum self-paced learning for cross-domain object detection",
      "authors": [
        "P Soviany",
        "R Ionescu",
        "P Rota",
        "N Sebe"
      ],
      "year": "2021",
      "venue": "Computer Vision and Image Understanding",
      "doi": "10.1016/j.cviu.2021.103166"
    },
    {
      "citation_id": "36",
      "title": "Automatic digital modulation classification based on curriculum learning",
      "authors": [
        "M Zhang",
        "Z Yu",
        "H Wang",
        "H Qin",
        "W Zhao",
        "Y Liu"
      ],
      "year": "2019",
      "venue": "Applied Sciences",
      "doi": "10.3390/app9102171"
    },
    {
      "citation_id": "37",
      "title": "Curriculum learning for speech emotion recognition from crowdsourced labels",
      "authors": [
        "R Lotfian",
        "C Busso"
      ],
      "year": "2019",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing",
      "doi": "10.1109/TASLP.2019.2898816"
    },
    {
      "citation_id": "38",
      "title": "A survey on curriculum learning",
      "authors": [
        "X Wang",
        "Y Chen",
        "W Zhu"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
      "doi": "10.1109/TPAMI.2021.3069908"
    },
    {
      "citation_id": "39",
      "title": "Dynamically composing domaindata selection with clean-data selection by \"co-curricular learning\" for neural machine translation",
      "authors": [
        "W Wang",
        "I Caswell",
        "C Chelba"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
      "doi": "10.18653/v1/P19-1123"
    },
    {
      "citation_id": "40",
      "title": "Meta-curriculum learning for domain adaptation in neural machine translation",
      "authors": [
        "R Zhan",
        "X Liu",
        "D Wong",
        "L Chao"
      ],
      "year": "2021",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "41",
      "title": "Norm-based curriculum learning for neural machine translation",
      "authors": [
        "X Liu",
        "H Lai",
        "D Wong",
        "L Chao"
      ],
      "year": "2020",
      "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
      "doi": "10.18653/v1/2020.acl-main.41"
    },
    {
      "citation_id": "42",
      "title": "Uncertainty-aware curriculum learning for neural machine translation",
      "authors": [
        "Y Zhou",
        "B Yang",
        "D Wong",
        "Y Wan",
        "L Chao"
      ],
      "year": "2020",
      "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
      "doi": "10.18653/v1/2020.acl-main.620"
    },
    {
      "citation_id": "43",
      "title": "Simple and effective curriculum pointer-generator networks for reading comprehension over long narratives",
      "authors": [
        "Y Tay",
        "S Wang",
        "A Luu",
        "J Fu",
        "M Phan",
        "X Yuan",
        "J Rao",
        "S Hui",
        "A Zhang"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
      "doi": "10.18653/v1/P19-1486"
    },
    {
      "citation_id": "44",
      "title": "CDL: Curriculum dual learning for emotioncontrollable response generation",
      "authors": [
        "L Shen",
        "Y Feng"
      ],
      "year": "2020",
      "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics",
      "doi": "10.18653/v1/2020.acl-main.52"
    },
    {
      "citation_id": "45",
      "title": "Hybrid curriculum learning for emotion recognition in conversation",
      "authors": [
        "L Yang",
        "Y Shen",
        "Y Mao",
        "L Cai"
      ],
      "year": "2022",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence",
      "doi": "10.1609/aaai.v36i10.21413"
    },
    {
      "citation_id": "46",
      "title": "Attention is all you need",
      "authors": [
        "A Vaswani",
        "N Shazeer",
        "N Parmar",
        "J Uszkoreit",
        "L Jones",
        "A Gomez",
        "L Kaiser",
        "I Polosukhin"
      ],
      "year": "2017",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "47",
      "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
      "authors": [
        "A Dosovitskiy",
        "L Beyer",
        "A Kolesnikov",
        "D Weissenborn",
        "X Zhai",
        "T Unterthiner",
        "M Dehghani",
        "M Minderer",
        "G Heigold",
        "S Gelly",
        "J Uszkoreit",
        "N Houlsby"
      ],
      "year": "2021",
      "venue": "International Conference on Learning Representations"
    },
    {
      "citation_id": "48",
      "title": "MELD: A multimodal multi-party dataset for emotion recognition in conversations",
      "authors": [
        "S Poria",
        "D Hazarika",
        "N Majumder",
        "G Naik",
        "E Cambria",
        "R Mihalcea"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
      "doi": "10.18653/v1/p19-1050"
    },
    {
      "citation_id": "49",
      "title": "IEMOCAP: interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language Resources and Evaluation",
      "doi": "10.1007/s10579-008-9076-6"
    },
    {
      "citation_id": "50",
      "title": "Emotion detection on tv show transcripts with sequence-based convolutional neural networks",
      "authors": [
        "S Zahiri",
        "J Choi"
      ],
      "year": "2018",
      "venue": "The Workshops of the Thirty-Second AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "51",
      "title": "DailyDialog: A manually labelled multi-turn dialogue dataset",
      "authors": [
        "Y Li",
        "H Su",
        "X Shen",
        "W Li",
        "Z Cao",
        "S Niu"
      ],
      "year": "2017",
      "venue": "Proceedings of the Eighth International Joint Conference on Natural Language Processing"
    },
    {
      "citation_id": "52",
      "title": "A robustly optimized BERT pretraining approach",
      "authors": [
        "Y Liu",
        "M Ott",
        "N Goyal",
        "J Du",
        "M Joshi",
        "D Chen",
        "O Levy",
        "M Lewis",
        "L Zettlemoyer",
        "V Stoyanov"
      ],
      "year": "2020",
      "venue": "A robustly optimized BERT pretraining approach"
    },
    {
      "citation_id": "53",
      "title": "Generalized autoregressive pretraining for language understanding",
      "authors": [
        "Z Yang",
        "Z Dai",
        "Y Yang",
        "J Carbonell",
        "R Salakhutdinov",
        "Q Le"
      ],
      "year": "2019",
      "venue": "Proceedings of the 33rd International Conference on Neural Information Processing Systems"
    },
    {
      "citation_id": "54",
      "title": "Decoupled weight decay regularization",
      "authors": [
        "I Loshchilov",
        "F Hutter"
      ],
      "year": "2019",
      "venue": "Proceedings of International Conference on Learning Representations"
    }
  ]
}