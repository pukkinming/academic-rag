{
  "paper_id": "2002.10804v1",
  "title": "> Replace This Line With Your Paper Identification Number (Double-Click Here To Edit) <",
  "published": "2020-02-25T11:47:53Z",
  "authors": [
    "Cunbo Li",
    "Peiyang Li",
    "Yangsong Zhang",
    "Ning Li",
    "Yajing Si",
    "Fali Li",
    "Dezhong Yao",
    "Peng Xu"
  ],
  "keywords": [
    "Brain neural network",
    "Emotion recognition",
    "emotional intelligence",
    "MESNP",
    "Network topology"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Brain neural networks characterize various information propagation patterns for different emotional states. However, the statistical features based on traditional graph theory may ignore the spacial network difference. To reveal these inherent spatial features and increase the stability of emotional recognition, we proposed a hierarchical framework that can perform the multiple emotion recognitions with the multiple emotion-related spatial network topology patterns (MESNP) by combining a supervised learning with ensemble co-decision strategy. To evaluate the performance of our proposed MESNP approach, we conduct both off-line and simulated on-line experiments with two public datasets i.e., MAHNOB and DEAP. The experiment results demonstrated that MESNP can significantly enhance the classification performance for the multiple emotions. The highest accuracies of off-line experiments for MAHNOB-HCI and DEAP achieved 99.93% (3 classes) and 83.66% (4 classes), respectively. For simulated on-line experiments, we also obtained the best classification accuracies with 100% (3 classes) for MAHNOB and 99.22% (4 classes) for DEAP by proposed MESNP. These results further proved the efficiency of MESNP for structured feature extraction in multclassification emotional task.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "MOTION processing plays a crucial role in meditation and social behavior of human beings  [5] . A series of studies have converged to evidences that emotional intelligence is one of the most important intelligence in humans' daily life rather than logical intelligence  [8] . Scientific American reported that whether we can detect human emotions automatically is one of the twenty big questions about the future of humanity  [10] . In the last decade, researches have proposed a large variety of strategies to automatically detect different emotional states through physiological signals  [12] . These studies aim to enhance the capability of affective brain computer interfaces (aBCI) to effectively detect, process and respond to users satisfactory experimental result  [2] .  Zhao et al.  studied the possible relationship between emotion and personality inference with EEG, and predicted personality traits in five dimensions with PSD feature, achieving the best 86.11% accuracy in the dimension of agreeableness  [26] .  Zheng et al.  used DE feature to emotion classification and achieved 70.58% accuracy with all 62 channels of EEG data  [10] . However, these features mainly reflect the local information of brain activities, ignoring the information propagation patterns between multiple brain regions that is important to form the human emotions.\n\nIn essence, emotion processing in the brain relates to a complex dynamic interactive process involving many brain regions  [27] [28] [29] [30] . Therefore, the more optimal feature extraction methods should be capable of capturing these interaction patterns in brain. In recent years, the functional connections among different brain regions have been proven to hold close relationship with different emotional states  [24, 30] . Y.-Y. Lee and S. Hsieh classified different emotional states with the EEGbased functional connectivity patterns and found that the functional connections are significantly different when the emotional states changes  [30] . Li et al adopted a multiple feature fusion approach to combine the activation patterns and connection patterns for emotion recognition, and the performed analysis has shown the functional connection patterns can significantly enhance the classification performance  [31] . Y. Dasdemir et al analyzed the functional brain connections for positive and negative emotions states and reported that the control of emotion may be related with the functional connectivity of left frontal electrodes  [24] . These findings highlight the importance of the brain functional connectivity for the emotion related researches.\n\nHowever, the statistical measurements of functional networks though determined by the spatial network topology may fail to reflect the intact spatial information of networks  [32] . In fact, identifying the essential spatial network topologies may be potentially helpful to differentiate different emotions. In this work, we proposed a hierarchical framework that can perform the multiple emotion recognitions with the multiple emotionrelated spatial network topology patterns (MESNP) by combining a supervised learning with ensemble co-decision strategy. To evaluate the validity and stability of the proposed method, we used two public emotion EEG datasets, i.e., MAHNOB-HCI that provided by M. Soleymani et al  [33]  and DEAP by Koelstra et al  [34]  to verify the proposed method. For MAHNOB-HCI, there are three emotion labels, namely negative, neutral and positive. For DEAP, there are four emotion labels, which includes low arousal-low valence (LALV), high arousal-low valence (HALV), low arousal-high valence (LAHV), and high arousal-high valence (HAHV), according to the ratings of valence-arousal (VA) space.\n\nThe main contributions of this paper can be summarized as two aspects:\n\n1) We proposed a hierarchical framework to realize the multiple emotion recognition by combining a supervised learning with an ensemble co-decision strategy based on the topological patterns of emotions.\n\n2) A simulated on-line emotion detection system was established for real time emotional recognition, which could also be applied to clinical diagnosis and intervention.\n\nThe remainder of this paper is as follows. In Section 2, we briefly introduced the mostly adopted features for emotional recognition. In Section 3, the structure of proposed MESNP are presented elaborately. In Section 4, both off-line and on-line experiments on MAHNOB and DEAP are conducted to evaluate the efficiency of our proposed emotional recognition strategy. An elaborate analysis and discussion on experiment results are given in Sections 5 and 6 to demonstrate the robustness of the proposed method. Finally, conclusions and future works are described in Section 7.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Related Works",
      "text": "Currently, the widely used EEG features for emotional recognition are generally derived from the spectrum power represented by power spectral density (PSD) and differential entropy (DE). In this work, these two widely applied features were utilized as the benchmarks for comparison, and the details of these two methods are introduced as follows.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "A. Power Spectral Density",
      "text": "Spectrum analysis is widely used in diverse fields such as pattern recognition and signal processing  [35]  which is usually carried out by the estimation of PSD. In this work, the Welch algorithm was adopted to estimate PSD features in theta, alpha, beta and gamma bands, respectively. The power spectral density   Psd f of EEG signal   X t is calculated by the periodogram method, shown as follow:\n\nwhere j is an imaginary unit, T is the length of time series, and   F t is the Hamming window function to reduce spectral leakage. Besides, in W is the regularization coefficients of windows defined as follow, which is used to reduce the influence of window function on the power spectrum estimated.\n\n ",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "B. Differential Entropy",
      "text": "DE is proposed to extract features from EEG data  [36, 37] , which is based on the Shannon entropy that defined as\n\nwhere   f x is the probability density function of   X t . We assume that the EEG signals are the time series of   X t , which obeys the Gaussian distribution of   2 , N   , its differential entropy is defined as  [9]  \n\nwhere   X t follows the Gaussian distribution   2 , N   , and  and e are constants.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Iii. Methods",
      "text": "In this work, motivated from the network difference involved in different emotions processing, we proposed a hierarchical emotional recognition framework based on spatial network patterns as shown in Fig.  1 . This procedure consists of two subprocedures, i.e., training and prediction. As shown in Fig.  1 , the original EEG data was divided into training and testing dataset firstly, and then the network analysis method was adopted to construct functional brain neural networks for training and testing sets. For the training procedure, we divided the training set into pair-wise groups according to label information and a supervised learning is adopted to extract the corresponding MESNP filters and training features for each pair-wise group respectively. Finally, the ensemble SVM system was trained from each group MESNP features. During the prediction procedure, each brain neural network sample would be fed into every group MESNP filters to extract the corresponding MESNP features, and these MESNP features were inputted into the corresponding trained SVMs to predict labels. Finally, the final emotion label would be predicted by the result of ensemble co-decision voting of all the SVM outputs. Compared to the existing emotion prediction approaches, the improvement of proposed one can be attributed to the combination of a supervised learning strategy and ensemble co-decision to extract the discriminative spatial network patterns of multiclasses emotions. The overall procedure for proposed MESNP is summarized in Algorithm 1. In the following sub-sections, we will introduce the related aspects in detail, and in order to facilitate reading, the nomenclature mentioned in this section are shown in Table  Ⅰ .",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "A. Brain Neural Network Construction",
      "text": "In current work, we estimated functional brain neural network with Phase Locked Value (PLV)  [38] . According to the Gabor analysis  [39] , for the time signal  \n\nwhere . . PV is the Cauchy principal value  [40] .\n\nThe phase   t  and amplitude   A t of signal   X t can be uniquely determined by  (5)  and  (6) . The calculation of the instantaneous phase is as\n\nFrom (  5 )-(  7 ), we can respectively calculate the instantaneous phases of two different channel EEG time signals\n\nAfter calculating the PLV for each paired EEG channels, we can get the weighted adjacency matrix to reflect the phase couplings among the recorded EEG signals. Due to the fact that both the two public emotion datasets used 32-channel EEG acquisition systems, the PLV brain neural networks are 32×32 weighted adjacency matrix, i.e., PLV brain neural network consists of 32 nodes.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "B. Network Properties",
      "text": "There are four mostly used statistical brain neural network measurements, i.e., clustering coefficients ( Cc ), the shortest path length ( L ), global efficiency ( Ge ) and local efficiency ( Le )  [41, 42] . Here, Let  \n\n \n\nIn current work, we used these four neural network metrics as features for multiple emotions recognition.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "C. Multiple Emotion-Related Spatial Network Topology Patterns For Emotion Recognition",
      "text": "Although the network properties can be used to measure the statistical properties of the emotion-related brain neural networks to some degree, the statistical properties may weaken some spatial topology information of the networks and cannot reflect the intact cyberspace information  [43] . Considering that the weighted adjacency matrix of the brain neural network contains fundamental spatial information of networks, we fuse supervised learning and ensemble co-decision strategy to extract the discriminative multiple emotion-related spatial network topology patterns (MESNP). The implementation of MESNP consists of two stages, i.e., the training stage and testing stage. During training stage, the parameters for both feature extraction (i.e., spatial filters) and classification are obtained from training data, which will be further utilized for testing data. Fig.  1  illustrates the proposed framework. In essence, the core of MESNP is to learn and identify the discriminative features from the spatial patterns of brain neural networks. In our current work, one-versus-one with max-win strategy is utilized to realize a hierarchal structure for multiple emotions recognition, and the details of the MESNP training and testing implementation are introduced as follows.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "1) Training Stage",
      "text": "where W represents the pair-coupling spatial filters that trained with supervised method from\n\n, and\n\nwhere i U and j U indicate two different network matrices datasets of the i-th and the j-th emotional states from training dataset\n\n, which can be described as\n\n(1,2,3,..., )\n\nIn essence, the goal of MESNP is to find the optimal projection to maximize the difference between two different emotional states by maximizing the variance of brain neural networks from one emotional states while minimizing the variance of another  [44] . The theoretical implementation is mainly accomplished by computing the diagonalization of the covariance matrix  [45] . Actually, the solution of optimal projection problem can be equalized to maximize the following function: arg max ( )\n\nDue to the fact that the scaling of the projection w has no effect on the object value, the optimal projection problem of (  19 ) can be converted to the constrained optimization problem as follows:\n\nFurthermore, introducing the Lagrange multiplier, the constrained optimization problem of (20) can be rewritten as:\n\n( , ) , the projection of w , can be estimated with the generalized eigenvalue equation, as follow:\n\nwhere  denotes the eigenvalue of generalized eigenvalue equation, and w is the corresponding eigenvector  [46] . For multiple spatial filters, equation (  22 ) can be solved as:\n\nwhere W : consists of the eigenvectors of  \n\n is a diagonal matrix with  being the corresponding singular values. In fact, the diagonal values in  represent the differential capabilities of spatial filters, and the first and last spatial filters correspond to the largest and smallest eigenvalues which consist of the most discriminative spatial filter pair. In this work, three pairs of spatial filters were adopted to extract MESNP feature for each label pair-wise group. Thus, the trained spatial filters for the i-th and j-th emotional states can be concatenated as:\n\nFilter Filter Filter Filter Filter W Filter :  (24)  where\n\nis the eigenvectors in  (23) . With these filters, the training MESNP features corresponding to the combination of i-th and j-th emotional states can be extracted as: U U can be defined as\n\nWhere each element in  (26)      sets of spatial filters and MESNP features, respectively. Finally, the trained pair-coupling spatial filters and their corresponding features can be integrated as\n\nObviously, corresponding to the training groups, totally\n\n   sub-classifiers need to be further trained for the ensemble system so as that the system can be used for multiclass identification tasks. The ensemble system is defined as\n\n,",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "2) Testing Stage",
      "text": "During testing stage, for a given testing brain neural network test Plv , its MESNP features can be extracted with the trained spatial filters set W , which shown in (27), and the testing MESNP features can be computed as\n\nAs shown in  (27) , due to the fact that there are  ",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "  ",
      "text": "MESNP feature groups. The testing MESNP feature group can be estimated as\n\nwhere ij W corresponds to the pair-coupling spatial filters, which trained from the training sub-classification task with the pair-coupling of i-th and j-th emotional states, and the testing MESNP feature\n\nAs  (33)  shows, the test MESNP will be put into the ensemble system\n\n, and each testing MESNP feature will be predicted by the corresponding subclassification SVM model already trained with features extracted from the same spatial filters. After predicted from each sub-classification SVM model of the ensemble system, the final predicting emotion label will be predicted by ensemble co-decision voting as\n\nwhere k\n\nV indicates the frequency of the k-th class identified by the ensemble system, i.e. the vote of the ensemble codecision voting, and V is the voting results set of all\n\n| max , ,..., ,...,\n\nemotional states for current testing sample. Ideally, the final output of testing emotional label for the k-th class can be expressed as\n\n| max , ,..., ,...,\n\nAs shown in  (35) , the final label prediction result for the testing brain neural network test Plv is the class having the highest voting value in the ensemble co-decision system. Fig.  1  demonstrates the implementation procedure of MESNP for MAHNOB-HCI dataset, which includes three emotional states, i.e. Negative, Positive and Neutral. Similar to MAHNOB-HCI, the MESNP of DEAP includes six pair-wise groups and the ensemble system includes six sub-models, and the final prediction for testing sample will be decided from the voting of six sub-models prediction. The testing procedure can be summarized in Algorithm 3.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Iv. Materials",
      "text": "In this work, two public emotional datasets are utilized, i.e., MAHNOB-HCI and DEAP. Videos were used as experimental stimulus for these two datasets. When participants watching these the EEG-acquisition equipment will record the emotion-related EEG signals. MAHNOB-HCI dataset consists of three emotion categories, while DEAP dataset has 4 emotion states evoked.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "A. Mahnob-Hci Dataset",
      "text": "The MAHNOB-HCI dataset contains EEG, peripheral physiological signals, functional near-infrared spectroscopy (fNIRS) and facial videos of 27 participants (11 male and 16 female, aged between 19 and 40). During experiments, 32channel electrodes were assigned on the participants' scalp in accordance with the international standard 10-20 system to collect EEG data and the sampling frequency is 256 Hz. The experimental stimuli videos were selected by participants via an on-line self-assessment manikins (SAM) system from 155 video clips. Through statistical assessment of participants' selfemotion, 20 video segments from 155 video segments were selected as the stimuli of experiment  [33] . In the experiment, 20 video segments were played randomly and the participants had to use some emotional labels to express their emotions when they watched the video, the labels include neutral, anxiety, amusement, sadness, joy, disgust, anger, surprise, fear, happiness and so on. The labels were divided into three categories from the dimensions of Arousal and Valence, including positive, neutral and negative emotions, and the specific division is shown in Table  Ⅱ . The detail of MAHNOB-HCI dataset could refer to  [33] .",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "B. Deap Dataset",
      "text": "The DEAP database contains EEG and peripheral physiological signals of 32 healthy participants (16 male and 16 females, aged between 19 and 37). The experiment used 48channel electrodes (32 EEG channels, 12 peripheral channels, 3 unused channels, and 1 status channel) to collect data, and the sampling rate is 512HZ. The EEG channels were placed according to standard 10-20 system. This experiment first selected 120 initial stimuli, half of which were chosen semiautomatically and the rest manually, and then used a web-based subjective emotion assessment interface to choose 40 test video clips as the stimuli. Stimuli induce emotions in the four quadrants of the valence-arousal (VA) space (LALV, HALV, LAHV, and HAHV) as shown in Fig.  2 . The EEG signals of 32 participants were recorded as each watched 40 one-minute long excerpts of music videos. At the end of each trial, participants performed a self-assessment of their levels of arousal, valence, liking, and dominance  [34] .",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "C. Data Processing",
      "text": "To reduce the noise influence and enhance the stability of EEG data, we performed following four steps for EEG preprocessing: 1) segmenting EEG dataset with 10-s moving window; 2) converting segmented EEG data to the average reference; 3) adopting baseline correction to reduce the impact of baseline drift by using the first 1 second signals in each segment; 4) filtering each EEG segment into in four frequency bands with band-pass filters, i.e., 4-8 HZ (theta band), 8-12 HZ (alpha band), 12-30 HZ (beta band), and 30-48 HZ (gamma band).\n\nIn the feature extraction stage, we firstly divided EEG segments into training and testing datasets and then constructed brain neural networks with PLV from segmented EEG data. Based on training datasets, we trained MESNP filters and the corresponding ensemble classifiers. In order to verify the effectiveness and feasibility of our method, we also used the widely adopted features, like power spectral density (PSD), differential entropy (DE) and the network properties features as benchmarks for comparison.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "D. Deap Dataset",
      "text": "In this experiment, the brain neural networks are constructed for all segments. To evaluate the efficiency of our proposed emotional recognition strategy, we designed two classification tasks, i.e., the off-line classification task and the simulated on-  To simulate the condition of on-line data collection and processing, the preprocessed EEG data were divided into two parts, where the first 50% of the time series was used for off-line analysis to train the model and the last 50% of the time series was used as on-line real-time data acquisition simulation. Specifically, the MESNP filters and the ensemble co-decision classification system are trained in the off-line analysis. Subsequently, the on-line MESNP features are extracted from brain neural networks with the trained MESNP spatial filters, and the final on-line emotional states will be predicted from the ensemble co-decision classification system. In fact, the main difference between off-line and on-line classifications rises from the fact that whether the temporal sequence information is considered or not. For off-line classification task, the order of time sequence in EEG data is not considered and the 10-fold cross-validation scheme is used to randomly divide the EEG segments into training and testing sets. To get the robust and convinced result, the 10-fold crossvalidation is repeated for 10 times for each classification approach, and the mean accuracy across the 10 times is reported. For on-line classification task, the sequential order of EEG data must be considered to simulate the situation of on-line real-time data acquisition (i.e., those segments recorded in the early stage will be used as training samples, while other segments recorded in the relatively later stage will be served as the testing samples).",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "V. Experiment Results",
      "text": "",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "A. Experiment Results On Mahnob-Hci Dataset",
      "text": "For the MAHNOB-HCI Dataset, three classes of emotions (positive, neutral and negative) were labeled in the whole experiment. Based on the four frequency bands (i.e., theta, alpha, beta and gamma), respectively, we utilized Network properties, PSD, DE and MESNP features to perform the prediction of the three emotions. The corresponding classification accuracies are shown in Table  Ⅲ  and Fig.  3 , respectively.\n\nAs shown in Table  Ⅲ , we can find that the optimal recognition rates are 55.21%(gamma), 60.62% (beta) and 71.25% (gamma) for the Network Properties features, PSD and DE features in all the four frequency bands, respectively. For MESNP features, however, the prediction accuracy has achieved 99.93%±0.13, 99.85%±0.42, 98.41%±1.84 and 96.20%±2.60 in theta, alpha, beta and gamma bands, respectively. In addition, the results in lower frequency bands (i.e., theta and alpha) are better than that in the higher frequency bands (i.e., beta and gamma).\n\nTable Ⅲ clearly shows that the recognition based on MESNP features are better than other features widely used for emotion classification in previous studies. Based on MESNP features, we further explored the prediction performances across all participants in the four frequency bands (Fig.  3 ). Fig.  3  presents the variability across participants (i.e., 99.47%-100% for the theta band; 98.30%-100% for the alpha band; 93.32%-100% for the beta band; 91.26%-100% for the gamma band). Importantly, all the participants had the prediction accuracies over 90%, and the best prediction performance was also exhibited in the low frequency bands (theta and alpha), which was consistent with the result in Table  Ⅲ .\n\nWe further compared the recognition accuracy of various systems using MAHNOB-HCI dataset and presented in Table  Ⅳ .\n\nKoelstra et al.  [2]  presented a multi-modal approach that analyzed both facial expressions and electroencephalography (EEG) signals for the generation of affective tags. They performed binary classification on the arousal, valence and control ratings, which are threshold into high (rating 6-9) and low (rating 1-5) classes. For arousal, valence, and control, video tag classification rates of 80.00%, 80.00%, and 86.70% are obtained respectively when aggregating across all participants. Huang et al. fused the facial expression features and EEG features for emotion recognition and the best classification rate for valence and arousal are 66.28% and 63.22% for 2 classes. Wang et al.  [7] proposed a novel emotion recognition approach with privileged information by exploiting relations between EEG signals and stimulus videos. The best accuracies for arousal and valence (2 classes) are 61.35% and 61.22%. Most of those reported studies focused on the 2-classes emotion recognition. Our method for classification of 3 classes (positive,",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Koelstra Et Al.[2]",
      "text": "Average classification rates of 80.00%, 80.00% and 86.70% for arousal, valence, and control ratings (2 classes) with 24 participants. Huang et al.  [4]  The best classification rate (66.28% for valence and 63.22% for arousal) (2 classes) is achieved. Wang et al.  [7]  For valence and arousal (2 classes), the classification rates are 61.35% (arousal) and 60.22% (valence).",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Our Method",
      "text": "Average classification rate of 99.93% for 3 classes (positive, neutral and negative) with all 27 participants. neutral and negative) has achieved the average accuracy of 99.93% on the same dataset.\n\nThe average on-line simulation experimental recognition result of all participants for MAHNOB-HCI dataset is shown in Fig.  4 . We can find that the experimental results in theta and alpha bands have achieved 100% for every participant. Though the results in beta and gamma are less stable than that in theta and alpha bands, the average classification results are still above 99% in both bands. Combined with the off-line experimental results, we could discover that the results of two experiments are consistent and the better recognition results are both in theta and alpha bands for this dataset.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "B. Experiment Results On Deap Dataset",
      "text": "For DEAP Dataset, we divided the valence-arousal (VA) space into four quadrants (rating 1-5 is defined as low, rating 5-9 is defined as high, resulting in LALV, LAHV, HALV, HAHV 4 classes, shown in Fig.  3 ). The experimental results for different features of all 32 participants are shown in Table Ⅴ and Fig.  5 .\n\nAs shown in Table  Ⅴ , the best recognition rates are 73.33% ±4.47 (alpha), 57.57% ±6.59 (beta) and 55.56%±6.47 (beta) for the Network Properties features, PSD and DE features in all four frequency bands, respectively. As for MESNP features, the best accuracy rate was 83.66% ± 2.43 (alpha), which is significantly higher than other features (Network Properties features, PSD and DE).\n\nThe recognition accuracy for every participant is shown in Fig.  5 . The accuracies of low bands (theta and alpha) apparently are better than that of high bands (beta and gamma) for MESNP based approach. In Table Ⅴ, the average accuracy also shows the obvious disparities among different bands, where the average accuracies reach up to 81.81% and 83.66% in theta and alpha bands, however, 36.64% and 42.78% in beta and gamma bands. Comparing Table  Ⅲ  with Table Ⅴ, the classification results of DEAP are highly consistent with that of MAHNOB-HCI, which could verify our proposed method to be efficient and stable for emotion recognition. We compared our work with previously reported studies for DEAP dataset as shown in Table  Ⅵ .\n\nYoon et al.  [1]  defined a probabilistic classifier based on Bayes' theorem for valence and arousal classification with DEAP dataset. The best classification rates for their experiment are 70.90% (2 classes) and 55.40% (3 classes). Liu et al.  [3]  proposed a real-time fractal dimension (FD) based valence level recognition algorithm for EEG signals. In this study, they selected 10 participants from DEAP dataset as the material for their experiments and reached the best mean accuracy of 63.04% for arousal-dominance recognition (4 classes). Zhang et al.  [6]  designed an ontological model to represent and integrate EEG data, which achieved an average recognition rate of 75.19% on valence and 81.74% on arousal (2 classes). Zheng et al.  [9]  used a newly developed pattern classifier named discriminative Graph regularized Extreme Learning Machine (GELM) and extracted the differential entropy (DE) as their training feature. Their method achieved an average accuracy of 69.67% on the DEAP dataset for quadrants of VA space (4 classes) in theta frequency band for all participants. Arnau-González et al.  [11]  combined both connectivity-based and channel-based features with a selection method and achieved 67.70% (arousal) and 69.90% (valence) on arousal and valence (2 classes). Compared with their work, we have the largest",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Yoon Et Al. [1]",
      "text": "The classification rates of 70.90%, 70.10% for valence and arousal (2 classes), 55.40%, 55.20% for valence and arousal (3 classes) with all 32 participants.\n\nLiu et al.  [3]  Selected 10 participants and achieved 63.04% for arousal-dominance recognition (4 classes). Zhang et al.  [6]  Chose 8 participants and achieved 75.19 % and 81.74 % on valence and arousal (2 classes). Zheng et al.  [9]  69.67% for quadrants of VA space (4 classes) with all 32 participants. Arnau-González et al.  [11]  67.70% and 69.90% on arousal and valence (2 classes) with all 32 participants.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Our Method",
      "text": "Average classification rates of 83.66% on valencearousal (VA) space (4 classes) with all 32 participants. number of categories (4 classes) and the highest recognition accuracy (83.66%), which highlights the superiority and stability of our method. Fig.  6  shows the on-line simulation experimental recognition of all participants on DEAP dataset. The similar results to MAHNOB-HCI dataset can be found in DEAP dataset, where the results in theta and alpha bands are more stable than those in beta and gamma bands. The best on-line experimental classification result on DEAP dataset has achieved 99.22% in the alpha band, which is consistent with the off-line classification result.\n\nMoreover, in order to investigate how MESNP can effectively extract the discriminative spatial network patterns, we performed the analysis of variance (ANOVA) to explore the possible relationship between the learned spatial filters and network patterns of emotion states. The statistically significant difference edges (p<0.01) between different emotion brain neural networks and the learned two most discriminative spatial filters in alpha band for one subject in MAHNOB-HCI dataset are shown in Fig.  7 , where (a), (b) and (c) correspond to positive vs negative, positive vs neutral and negative vs neutral, respectively. Specifically investigating Fig.  7 , we could find that the different emotion pairs exhibited the different distinct network patterns. The proposed MESNP can adaptively learn the specific discriminative spatial network patterns, where the vital network nodes of spatial filters are emphasized with large values (i.e., those marked with either the red or deep blue colors), while other less important nodes are compressed by giving the small weight values.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Vi. Discussion",
      "text": "The challenge for the reliable classification of different emotions is mainly due to the very limited knowledge regarding the underlying neural mechanism of emotions. In this work, we mainly investigate the feature extraction and classification based on emotion-related EEG brain neural networks that can measure the information propragration and exchange among different brain areas. Considering that different emotional processing involves different brain network patterns, this work established an approach termed as MESNP to extract EEGrelated brain neural networks topology features of different emotional states. We specifically constructed the emotion-related brain neural networks based on EEG data, and then combined the supervised learning and the ensemble co-decision together to train the spatial filters to extract spatial topology difference features from brain neural networks, and also train the co-decision classification system.\n\nIn this work, based on the public emotional MAHNOB-HCI and DEAP datasets, we performed the off-line and on-line simulation experiments to investigate the feasibility of proposed method. From the experiment results of Table  Ⅲ  and Table  Ⅴ , we can find that MESNP can achieve the higher classification accuracy than the other conventional approaches. Moreover, MESNP consistently reveals the relatively better performances in the lower frequency bands (theta and alpha) compared to other two higher frequency bands (beta and gamma) for both two emotion datasets. The performance improvement of MESNP infers that the MESNP features extracted from brain neural networks may contain more discriminative information to differentiate the human emotions. Combining the network differences between two paired emotional states and the corresponding spatial MNESP filters in Fig.  7 , the working mechanism of MNESP could be revealed. Fig.  7  shows that there exists network topology difference between brain neural networks (positive, neutral and negative) for different emotions, which means that the spatial discriminative information exists in the network topologies. Comparing the differential network topological patterns with the spatial filters, we can discover that the nodes exhibiting large topology differences are imposed with the larger filter weights, while other less important nodes are given with the smaller filter weights. Essentially, when multiplying the filters with the network adjacency matrix, the spatial filters function like a sub-network-seeking filters, automatically selecting modules with significant spatial differences between emotions in the brain neural network space by giving them larger emphasis and compressing other modules by providing smaller coefficients. Therefore, this band-pass-like filter can be used to effectively extract the spatial topological differences between different emotions, so as to obtain more reliable distinctions. Specifically, investigating Fig.  7  we can also find that though the network topology difference mainly exists in the frontal, occipital and parietal regions that are proved to be associated with emotions  [48] [49] [50] [51] [52] , the different emotion pairs actually show the distinct discriminative network patterns. Due to the adoption of the supervised learning strategy, MNESP can adaptively capture these differences, resulting in different spatial MNESP patterns for different emotion pairs. Besides the differences in network topology analysis in alpha band as reported in Fig.  7 , we also performed the same analysis for other three frequency bands. The comparison among the four frequency bands reveals that there are significant differences between different emotions network in the theta and alpha bands of two datasets while the differences in the beta and gamma bands are relatively weak, which accounts for the highest classification results obtained in the theta and alpha bands. Previous studies also confirmed that the more discriminative information for emotion recognition are revealed in the theta and alpha bands. Lin et al. used SVM to classify emotions and the results of asymmetry feature in alpha and theta bands have achieved the highest accuracy among all four frequency bands  [25] . Shahabi et al. found that the frontal-toparietal connectivity in the alpha band specifically increased when emotions changed from neutral to joy and in almost every case, the meaningful differences were observed in theta band for emotion shift from melancholic to joy  [53] . In the theta band, compared to happiness emotion, significant higher coherence is found in healthy participants during sadness, fear, disgust, and anger emotion states, and in the alpha band, sadness and disgust appeared to have higher coherence than happiness and surprise emotions  [54] . Moreover, the asymmetries at the F3-F4 pair related to valence emotion are observed in both alpha and theta bands when analyzing the emotional arousal during affectivepictures stimuli  [48, 49] .\n\nBesides the conventional fold cross-validation based evaluation, this work newly performed a simulated on-line analysis, which is much closer to the practical application. Similar to the off-line analysis, the analysis for the simulated on-line protocol also achieves the relatively high accuracy above 88%, which may provide the promising tool for the realization of the effective on-line affective recognition system. In the filed of BCI, how to improve the emotional interaction between computers and humans is the biggest chalenage for researchers. The simulated on-line experimental results also shows the stability and effectiveness of the framework we proposed, and this framework may provide a possibility for the realization of intelligent affective brain computer interfaces system.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Vii. Conclusion",
      "text": "In the current study, based on the public datasets, we have systematically compared the performance differences between conventional features and the discriminative network features proposed in this work for emotional recognition. The results in both off-line and on-line classification tasks showed that the proposed feature extraction method MESNP can robustly and reliably differentiate different emotional states, and the MESNP features can be used to make better categorical predictions of emotions.",
      "page_start": 12,
      "page_end": 12
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The overall structure of multiple hierarchical emotional recognition framework.",
      "page": 4
    },
    {
      "caption": "Figure 1: illustrates the proposed framework. In",
      "page": 5
    },
    {
      "caption": "Figure 2: The valence-arousal (VA) space model",
      "page": 8
    },
    {
      "caption": "Figure 3: The classification accuracies with MESNP features for each",
      "page": 9
    },
    {
      "caption": "Figure 3: ). The experimental results for",
      "page": 10
    },
    {
      "caption": "Figure 4: The on-line simulation classification accuracy for MAHNOB-HCI",
      "page": 10
    },
    {
      "caption": "Figure 5: The classification accuracies with MESNP features for each",
      "page": 10
    },
    {
      "caption": "Figure 6: The on-line simulation classification accuracy for DEAP dataset",
      "page": 11
    },
    {
      "caption": "Figure 7: The scalp topologies for the two most discriminative MESNP filters",
      "page": 11
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Study": "Koelstra et \nal.[2]",
          "Results": "Average  classification  rates  of  80.00%,  80.00%  and \n86.70%  for  arousal,  valence,  and  control  ratings  (2 \nclasses) with 24 participants."
        },
        {
          "Study": "Huang et \nal.[4]",
          "Results": "The  best  classification  rate  (66.28%  for  valence  and \n63.22% for arousal) (2 classes) is achieved."
        },
        {
          "Study": "Wang et al. \n[7]",
          "Results": "For valence and arousal (2 classes), the classification \nrates are 61.35% (arousal) and 60.22% (valence)."
        },
        {
          "Study": "Our method",
          "Results": "Average  classification  rate  of  99.93%  for  3  classes \n(positive, \nneutral \nand \nnegative) \nwith \nall \n27 \nparticipants."
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Study": "Yoon et al. \n[1]",
          "Results": "The classification rates of 70.90%, 70.10% for valence \nand arousal (2 classes), 55.40%, 55.20% for valence \nand arousal (3 classes) with all 32 participants."
        },
        {
          "Study": "Liu et al. [3]",
          "Results": "Selected  10  participants  and  achieved  63.04%  for \narousal-dominance recognition (4 classes)."
        },
        {
          "Study": "Zhang et al. \n[6]",
          "Results": "Chose  8  participants  and  achieved  75.19  %  and \n81.74 % on valence and arousal (2 classes)."
        },
        {
          "Study": "Zheng et \nal.[9]",
          "Results": "69.67% for quadrants of VA space (4 classes) with all \n32 participants."
        },
        {
          "Study": "Arnau-\nGonzález et \nal.[11]",
          "Results": "67.70% and 69.90% on arousal and valence (2 classes) \nwith all 32 participants."
        },
        {
          "Study": "Our method",
          "Results": "Average  classification  rates  of  83.66%  on  valence-\narousal (VA) space (4 classes) with all 32 participants."
        }
      ],
      "page": 10
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "EEG-based emotion estimation using Bayesian weighted-log-posterior function and perceptron convergence algorithm",
      "authors": [
        "H Yoon",
        "S Chung"
      ],
      "year": "2013",
      "venue": "Computers in Biology & Medicine"
    },
    {
      "citation_id": "2",
      "title": "Fusion of facial expressions and EEG for implicit affective tagging",
      "authors": [
        "S Koelstra",
        "I Patras"
      ],
      "year": "2013",
      "venue": "Image and Vision Computing"
    },
    {
      "citation_id": "3",
      "title": "Real-time fractal-based valence level recognition from EEG",
      "authors": [
        "Y Liu",
        "O Sourina"
      ],
      "year": "2013",
      "venue": "Transactions on computational science"
    },
    {
      "citation_id": "4",
      "title": "Multi-modal emotion analysis from facial expressions and electroencephalogram",
      "authors": [
        "X Huang",
        "J Kortelainen",
        "G Zhao",
        "X Li",
        "A Moilanen",
        "T Seppänen",
        "M Pietikäinen"
      ],
      "year": "2016",
      "venue": "Computer Vision and Image Understanding"
    },
    {
      "citation_id": "5",
      "title": "Descartes' Error: Emotion, Reason and the Human Brain",
      "authors": [
        "S Spence"
      ],
      "year": "1995",
      "venue": "Bmj Clinical Research"
    },
    {
      "citation_id": "6",
      "title": "Ontology-based context modeling for emotion recognition in an;intelligent web",
      "authors": [
        "Xiaowei Zhang",
        "Hu",
        "Bin",
        "Jing Chen"
      ],
      "year": "2013",
      "venue": "World Wide Web-internet & Web Information Systems"
    },
    {
      "citation_id": "7",
      "title": "Emotion Recognition with the Help of Privileged Information",
      "authors": [
        "S Wang",
        "Y Zhu",
        "L Yue",
        "J Qiang"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on Autonomous Mental Development"
    },
    {
      "citation_id": "8",
      "title": "Emotional intelligence",
      "authors": [
        "P Salovey",
        "J Mayer"
      ],
      "year": "1990",
      "venue": "Imagination, cognition and personality"
    },
    {
      "citation_id": "9",
      "title": "Identifying Stable Patterns over Time for Emotion Recognition from EEG",
      "authors": [
        "W Zheng",
        "J Zhu",
        "B Lu"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "10",
      "title": "Emotionmeter: A multimodal framework for recognizing human emotions",
      "authors": [
        "W.-L Zheng",
        "W Liu",
        "Y Lu",
        "B.-L Lu",
        "A Cichocki"
      ],
      "year": "2018",
      "venue": "IEEE transactions on cybernetics"
    },
    {
      "citation_id": "11",
      "title": "Fusing highly dimensional energy and connectivity features to identify affective states from EEG signals",
      "authors": [
        "P Arnau-González",
        "M Arevalillo-Herráez",
        "N Ramzan"
      ],
      "year": "2017",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "12",
      "title": "A survey of affective brain computer interfaces: principles, state-of-the-art, and challenges",
      "authors": [
        "C Mühl",
        "B Allison",
        "A Nijholt",
        "G Chanel"
      ],
      "year": "2014",
      "venue": "Brain-Computer Interfaces"
    },
    {
      "citation_id": "13",
      "title": "Review and classification of emotion recognition based on EEG braincomputer interface system research: a systematic review",
      "authors": [
        "A Al-Nafjan",
        "M Hosny",
        "Y Al-Ohali",
        "A Al-Wabil"
      ],
      "year": "2017",
      "venue": "Applied Sciences"
    },
    {
      "citation_id": "14",
      "title": "An EEG-Based Brain Computer Interface for Emotion Recognition and Its Application in Patients with Disorder of Consciousness",
      "authors": [
        "H Huang",
        "Q Xie",
        "J Pan",
        "Y He",
        "Z Wen",
        "R Yu",
        "Y Li"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "15",
      "title": "Improving BCI-based emotion recognition by combining EEG feature selection and kernel classifiers",
      "authors": [
        "J Atkinson",
        "D Campos"
      ],
      "year": "2016",
      "venue": "Expert Systems with Applications"
    },
    {
      "citation_id": "16",
      "title": "Using brain-computer interfaces to detect human satisfaction in human-robot interaction",
      "authors": [
        "E Esfahani",
        "V Sundararajan"
      ],
      "year": "2011",
      "venue": "International Journal of Humanoid Robotics"
    },
    {
      "citation_id": "17",
      "title": "Human emotion: A functional view",
      "authors": [
        "R Levenson"
      ],
      "year": "1994",
      "venue": "The nature of emotion: Fundamental questions"
    },
    {
      "citation_id": "18",
      "title": "Feelings and emotions as dynamic factors in personality integration",
      "authors": [
        "M Arnold",
        "J Gasson"
      ],
      "year": "1954",
      "venue": "Feelings and emotions as dynamic factors in personality integration"
    },
    {
      "citation_id": "19",
      "title": "Emotional state classification from EEG data using machine learning approach",
      "authors": [
        "X.-W Wang",
        "D Nie",
        "B.-L Lu"
      ],
      "year": "2014",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "20",
      "title": "Feature Extraction and Selection for Emotion Recognition from EEG",
      "authors": [
        "R Jenke",
        "A Peer",
        "M Buss"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "21",
      "title": "Emotion recognition based on physiological changes in music listening",
      "authors": [
        "K Jonghwa",
        "A Elisabeth"
      ],
      "year": "2008",
      "venue": "IEEE Transactions on Pattern Analysis & Machine Intelligence"
    },
    {
      "citation_id": "22",
      "title": "A survey of affect recognition methods: audio, visual, and spontaneous expressions",
      "authors": [
        "Z Zhihong",
        "P Maja",
        "R Glenn",
        "H Thomas"
      ],
      "year": "2009",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "23",
      "title": "Using Black Hole Algorithm to Improve EEG-Based Emotion Recognition",
      "authors": [
        "R Munoz",
        "R Olivares",
        "C Taramasco",
        "R Villarroel",
        "R Soto",
        "T Barcelos",
        "E Merino",
        "M Alonso-Sánchez"
      ],
      "year": "2018",
      "venue": "Computational Intelligence and Neuroscience"
    },
    {
      "citation_id": "24",
      "title": "Analysis of functional brain connections for positive-negative emotions using phase locking value",
      "authors": [
        "Y Dasdemir",
        "E Yildirim",
        "S Yildirim"
      ],
      "year": "2017",
      "venue": "Cognitive neurodynamics"
    },
    {
      "citation_id": "25",
      "title": "EEG-based emotion recognition in music listening",
      "authors": [
        "L Yuan-Pin",
        "W Chi-Hong",
        "J Tzyy-Ping",
        "W Tien-Lin",
        "J Shyh-Kang",
        "D Jeng-Ren",
        "C Jyh-Horng"
      ],
      "year": "2010",
      "venue": "IEEE Transactions on Biomedical Engineering"
    },
    {
      "citation_id": "26",
      "title": "Emotion Analysis for Personality Inference from EEG Signals",
      "authors": [
        "G Zhao",
        "G Yan",
        "B Shen",
        "X Wei",
        "W Hao"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "27",
      "title": "Large-scale brain networks in cognition: emerging methods and principles",
      "authors": [
        "S Bressler",
        "V Menon"
      ],
      "year": "2010",
      "venue": "Trends in cognitive sciences"
    },
    {
      "citation_id": "28",
      "title": "Altered cerebellar functional connectivity with intrinsic connectivity networks in adults with major depressive disorder",
      "authors": [
        "L Liu",
        "L.-L Zeng",
        "Y Li",
        "Q Ma",
        "B Li",
        "H Shen",
        "D Hu"
      ],
      "year": "2012",
      "venue": "PloS one"
    },
    {
      "citation_id": "29",
      "title": "Amygdala-frontal connectivity during emotion regulation",
      "authors": [
        "S Banks",
        "K Eddy",
        "M Angstadt",
        "P Nathan",
        "K Phan"
      ],
      "year": "2007",
      "venue": "Soc Cogn Affect Neurosci"
    },
    {
      "citation_id": "30",
      "title": "Classifying different emotional states by means of EEG-based functional connectivity patterns",
      "authors": [
        "Y.-Y Lee",
        "S Hsieh"
      ],
      "year": "2014",
      "venue": "PloS one"
    },
    {
      "citation_id": "31",
      "title": "EEG based emotion recognition by combining functional connectivity network and local activations",
      "authors": [
        "P Li",
        "H Liu",
        "Y Si",
        "C Li",
        "F Li",
        "X Zhu",
        "X Huang",
        "Y Zeng",
        "D Yao",
        "Y Zhang",
        "P Xu"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Biomedical Engineering"
    },
    {
      "citation_id": "32",
      "title": "Differentiating between psychogenic nonepileptic seizures and epilepsy based on common spatial pattern of weighted EEG resting networks",
      "authors": [
        "P Xu",
        "X Xiong",
        "Q Xue",
        "P Li",
        "R Zhang",
        "Z Wang",
        "P Valdes-Sosa",
        "Y Wang",
        "D Yao"
      ],
      "year": "2014",
      "venue": "IEEE Trans Biomed Eng"
    },
    {
      "citation_id": "33",
      "title": "A Multimodal Database for Affect Recognition and Implicit Tagging",
      "authors": [
        "M Soleymani",
        "J Lichtenauer",
        "T Pun",
        "M Pantic"
      ],
      "year": "2012",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "34",
      "title": "DEAP: A Database for Emotion Analysis Using Physiological Signals",
      "authors": [
        "S Koelstra",
        "C Muhl",
        "M Soleymani",
        "J Lee",
        "A Yazdani",
        "T Ebrahimi",
        "T Pun",
        "A Nijholt",
        "I Patras"
      ],
      "year": "2012",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "35",
      "title": "Fully digital spectrum analyzer using time compression and discrete fourier transform techniques",
      "authors": [
        "J Lehmann",
        "F Lynch"
      ],
      "year": "1975",
      "venue": "Fully digital spectrum analyzer using time compression and discrete fourier transform techniques"
    },
    {
      "citation_id": "36",
      "title": "EEG-based emotion recognition with manifold regularized extreme learning machine",
      "authors": [
        "Y Peng",
        "J Zhu",
        "W Zheng",
        "B Lu"
      ],
      "venue": "EEG-based emotion recognition with manifold regularized extreme learning machine"
    },
    {
      "citation_id": "37",
      "title": "EEG-based emotion classification using deep belief networks",
      "authors": [
        "W Zheng",
        "J Zhu",
        "Y Peng",
        "B Lu"
      ],
      "venue": "EEG-based emotion classification using deep belief networks"
    },
    {
      "citation_id": "38",
      "title": "Review of advanced techniques for the estimation of brain connectivity measured with EEG/MEG",
      "authors": [
        "V Sakkalis"
      ],
      "year": "2011",
      "venue": "Computers in Biology & Medicine"
    },
    {
      "citation_id": "39",
      "title": "Measuring phase synchrony in brain signals",
      "authors": [
        "J Lachaux",
        "E Rodriguez",
        "J Martinerie",
        "F Varela"
      ],
      "year": "1999",
      "venue": "Human brain mapping"
    },
    {
      "citation_id": "40",
      "title": "A Note on the Phase Locking Value and its Properties",
      "authors": [
        "S Aydore",
        "D Pantazis",
        "R Leahy"
      ],
      "year": "2013",
      "venue": "Neuroimage"
    },
    {
      "citation_id": "41",
      "title": "Graph theoretical analysis of magnetoencephalographic functional connectivity in Alzheimer's disease",
      "year": "2009",
      "venue": "Brain"
    },
    {
      "citation_id": "42",
      "title": "Energy Efficiency of Downlink Transmission Strategies for Cloud Radio Access Networks",
      "authors": [
        "B Dai",
        "W Yu"
      ],
      "year": "2016",
      "venue": "IEEE Journal on Selected Areas in Communications"
    },
    {
      "citation_id": "43",
      "title": "Opportunities and methodological challenges in EEG and MEG resting state functional brain network research",
      "authors": [
        "E Van Diessen",
        "T Numan",
        "E Van Dellen",
        "A Van Der Kooi",
        "M Boersma",
        "D Hofman",
        "R Van Lutterveld",
        "B Van Dijk",
        "E Van Straaten",
        "A Hillebrand"
      ],
      "year": "2015",
      "venue": "Clinical Neurophysiology"
    },
    {
      "citation_id": "44",
      "title": "Spatial patterns underlying population differences in the background EEG",
      "authors": [
        "Z Koles",
        "M Lazar",
        "S Zhou"
      ],
      "year": "1990",
      "venue": "Brain Topography"
    },
    {
      "citation_id": "45",
      "title": "Optimizing spatial filters for robust EEG single-trial analysis",
      "authors": [
        "B Blankertz",
        "R Tomioka",
        "S Lemm",
        "M Kawanabe",
        "K Müller"
      ],
      "year": "2007",
      "venue": "IEEE Signal Processing Magazine"
    },
    {
      "citation_id": "46",
      "title": "The non-invasive Berlin brain-computer interface: fast acquisition of effective performance in untrained subjects",
      "authors": [
        "B Blankertz",
        "G Dornhege",
        "M Krauledat",
        "K.-R Müller",
        "G Curio"
      ],
      "year": "2007",
      "venue": "NeuroImage"
    },
    {
      "citation_id": "47",
      "title": "SVM parameter optimization using grid search and genetic algorithm to improve classification performance",
      "authors": [
        "I Syarif",
        "A Prugel-Bennett",
        "G Wills"
      ],
      "year": "2016",
      "venue": "Telkomnika"
    },
    {
      "citation_id": "48",
      "title": "Issues and assumptions on the road from raw signals to metrics of frontal EEG asymmetry in emotion",
      "authors": [
        "J Allen",
        "J Coan"
      ],
      "year": "2004",
      "venue": "Biological Psychology"
    },
    {
      "citation_id": "49",
      "title": "Analysis of evoked EEG synchronization and desynchronization in conditions of emotional activation in humans: temporal and topographic characteristics",
      "authors": [
        "L Aftanas",
        "N Reva",
        "A Varlamov",
        "S Pavlov",
        "V Makhnev"
      ],
      "year": "2004",
      "venue": "Neuroscience and behavioral physiology"
    },
    {
      "citation_id": "50",
      "title": "Prefrontal brain electrical asymmetry predicts the evaluation of affective stimuli",
      "authors": [
        "S Sutton",
        "R Davidson"
      ],
      "year": "2000",
      "venue": "Neuropsychologia"
    },
    {
      "citation_id": "51",
      "title": "Different Decision-Making Responses Occupy Different Brain Networks for Information Processing: A Study Based on EEG and TMS",
      "authors": [
        "Y Si",
        "X Wu",
        "F Li",
        "L Zhang",
        "K Duan",
        "P Li",
        "L Song",
        "Y Jiang",
        "T Zhang",
        "Y Zhang",
        "P Xu"
      ],
      "venue": "Cerebral Cortex"
    },
    {
      "citation_id": "52",
      "title": "Hits to the left, flops to the right: different emotions during listening to music are reflected in cortical lateralisation patterns",
      "authors": [
        "E Altenmüller",
        "K Schürmann",
        "V Lim",
        "D Parlitz"
      ],
      "year": "2002",
      "venue": "Neuropsychologia"
    },
    {
      "citation_id": "53",
      "title": "Toward automatic detection of brain responses to emotional music through analysis of EEG effective connectivity",
      "authors": [
        "H Shahabi",
        "S Moghimi"
      ],
      "year": "2016",
      "venue": "Computers in Human Behavior"
    },
    {
      "citation_id": "54",
      "title": "Brain functional connectivity patterns for emotional state classification in Parkinson's disease patients without dementia",
      "authors": [
        "R Yuvaraj",
        "M Murugappan",
        "U Acharya",
        "H Adeli",
        "N Ibrahim",
        "E Mesquita"
      ],
      "year": "2016",
      "venue": "Behav Brain Res"
    }
  ]
}