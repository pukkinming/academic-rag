{
  "paper_id": "2108.08663v2",
  "title": "Unsupervised Cross-Lingual Speech Emotion Recognition Using Pseudo Multilabel",
  "published": "2021-08-19T12:49:35Z",
  "authors": [
    "Jin Li",
    "Nan Yan",
    "Lan Wang"
  ],
  "keywords": [
    "speech emotion recognition",
    "humancomputer interaction",
    "cross-domain adaptation",
    "cross-lingual speech emotion recognition"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Speech Emotion Recognition (SER) in a single language has achieved remarkable results through deep learning approaches in the last decade. However, cross-lingual SER remains a challenge in real-world applications due to a great difference between the source and target domain distributions. To address this issue, we propose an unsupervised crosslingual Neural Network with Pseudo Multilabel (NNPM) that is trained to learn the emotion similarities between source domain features inside an external memory adjusted to identify emotion in cross-lingual databases. NNPM introduces a novel approach that leverages external memory to store source domain features and generates pseudo multilabel for each target domain data by computing the similarities between the external memory and the target domain features. We evaluate our approach on multiple different languages of speech emotion databases. Experimental results show our proposed approach significantly improves the weighted accuracy (WA) across multiple low-resource languages on Urdu, Skropus, ShEMO, and EMO-DB corpus. To facilitate further research, code is available at https://github.com/happyjin/NNPM",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Speech emotion recognition(SER) is the recognition of different human emotions from a given speech and is gaining increasing interest in the areas of human-computer interaction, computational neuroscience, cognitive psychology, intelligent tutor for children, and medical healthcare  [1, 2, 3] . Advancements in machine learning methods in recent years have allowed SER systems to exhibit excellent performance when training and testing data from corpora of the same language. However, the development of more robust SER systems for the cross-lingual scenarios remains an open problem due to the domain mismatch problem.\n\nTo date, several supervised methods have been proposed to reduce the domain mismatch problem for cross-lingual SER systems. One approach involves training with a combination of diverse corpora, which is able to reduce the factors from varying conditions such as acoustic environment and improves the model performance  [4] . Class-wise adversarial domain adaptation is another method to solve this problem by reducing the domain shift for all classes between different corpora  [5] . However, labels of some languages are costly to acquire, especially for low-resource languages, leading to a lack of available data for developing SER systems. This drawback limits the application of supervised methods.\n\nUnsupervised methods that do not need data labels in the target domain provide an alternative solution for cross-lingual SER, especially for low-resource languages. Some crosslingual SER systems based on unsupervised learning methods are introduced recently  [6, 7, 8, 9] . A mainstream method is to utilize the adversarial-based method since it can learn the corpus-invariant representations using domain adversarial training. Generative Adversarial Network (GAN)-based method is proposed by unsupervised domain adaptation for multilingual SER and learns language invariant feature representations from source language features to target language features  [6] . However, GAN is hard to train and prone to suffer from convergence failure  [10] . Domain adversarial neural network (DANN) is also a method by generating a domain invariant feature representation that reduces the gap between features in the source and target domain  [8] . However, the effectiveness of domain adversarial training is highly correlated with the distribution of two databases so that adversarial attacks and instabilities may occur during training if the data points are sharply different from each other  [11] . Multi-task learning training methodology for unsupervised cross-lingual is another approach to improve the generalisability of the model by incorporating information on gender and naturalness  [9] . An alternative approach is proposed by training to learn the emotion similarities from source domain samples through few-shot learning adapted to the target domain  [7] .\n\nFig.  1 . The pipeline of our proposed NNPM. We first compute spectrograms for both source domain audios and query audio from the current minibatch. Then both spectrograms are input into Siamese Network with Self Attention (SNSA) and the feature is extracted from this block. The source-domain features are dynamically stored in and read from the dynamic external memory. To learn the emotion similarities by computing the cosine distance between features inside dynamic external memory and the query vector. The query vector is a feature extract from the SNSA of the target domain. After that, the query vector from the target domain is assigned with pseudo multilabel based on the similarity scores. The model is optimized through loss from the source domain and the target domain using a hard negative sample mining strategy.\n\nHowever, samples in few-shot learning significantly depend on the choice of support set that might make the challenging to apply to practical settings. Moreover, because of the strong assumption of the support set sampled uniformly from a single distribution, it leads to the unstable number of samples selected for each class during the training process  [12] .\n\nIn an attempt to provide a solution to the challenges described above, we build a novel framework that models the unsupervised cross-lingual SER as a multilabel classification problem. This idea is inspired by ensemble learning  [13] , which averages the decisions from multiple related emotion features and decreases the effect from one specific emotion label to improve the model robustness. In addition, a dynamic external memory was also designed to store and update source domain features stably so that the number of samples from the source domain would be all utilized during training. As illustrated in Figure  1 , given two groups of audios from the source domain and the target domain, a model is first trained based on the source domain in a supervised way. Then the source domain features are saved in dynamic external memory and pseudo multilabel is assigned to target domain data by computing the similarities between features of the dynamic external memory and features of the target domain. Finally, the unsupervised training phase is optimized by combining the source domain cross entropy loss and loss from the target domain. Experimental results show the effectiveness of our NNPM in series of experiments.\n\nThe main contributions of this paper are: (1) a novel unsupervised cross-lingual SER framework with pseudo multilabel in the target domain; (2) a dynamic external memory design with memory update mechanism; (3) vastly exceed the baseline and other approaches for unweighted accuracy cross different languages.\n\nThe rest of this paper is organized as follows. The baseline model and the proposed method are described in Section 2. The experimental setup and results analysis are present in Section 3. Section 4 accomplishes the study with conclusions and directions.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Methodology",
      "text": "2.1. Siamese Network with Self Attention (SNSA) Let (X s , Y s ) := {(x s i , y s i )} n i=1 denotes the spectrogram x s ∈ X s and its corresponding label y s ∈ Y s in the source domain, where n is the number of data in the source domain, and X t = {x t i } m i=1 denotes the spectrogram of the target domain corpus without labeling, where x s i ∈ R L×d and x t i ∈ R L×d represent L the temporal length of the spectrogram in the source and the target domain separately and d is the dimension of a spectrogram feature vector.\n\nThe SNSA is used to extract the features of the source domain spectrogram X s and the target domain spectrogram X t . Our SNSA consists of two identical self-attention modules with shared weights for each other. For each self-attention module, we follow the previous work  [14]  that mainly consists of 3 components, namely a 2-layer convolutional neural network (CNN), a 2-layer bidirectional LSTM (BLSTM)  [15] , and a self-attention network. A temporal MaxPooling is applied to each convolution layer for dimensionality reduction and ReLU  [16]  is applied as an activation function after each MaxPooling operation to enhance the model nonlinearity. The BLSTM with forward and backward hidden states concatenation is represented by H blstm . The selfattention a is the weighted sum of the hidden states given H blstm and is described by the following equations\n\nwhere W 1 and W 2 are the parameters to be optimized during training. h attn can be either source domain feature h attn s or target domain feature h attn t .",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Dynamic External Memory",
      "text": "Dynamic external memory is proposed to store and load source domain features dynamically. Writing operation stores and updates the source domain features into the dynamic external memory. Specifically, it is updated by removing previous features and writing new features with a decay rate into the dynamic external memory. One crucial problem for the dynamic external memory update can be correlated with the stability-plasticity dilemma  [17] . On the one hand, dynamic external memory updates should be stable to avoid mismatching problems. Frequently changing the features in the dynamic external memory in every iteration decrease the training stability. On the other hand, the system also needs enough plasticity to effectively incorporate new features from the source domain into the dynamic external memory. In response to this issue, we purpose a weighted memory update strategy for the writing operation, which can be formulated as follows:\n\nwhere || • || 2 represents 2 norm, M ∈ R n×d is a dynamic external memory to save features and M[A i ] = h attn i , A is an index set described in Equation  4 , β is an updating rate that controls how fast the source domain features are written into the dynamic external memory. The reading operation loads features from the dynamic external memory and can be formulated as M read ← M iter [A].",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Pseudo-Labeling",
      "text": "Since the target domain labels are not available in the proposed framework, pseudo multilabel is assigned for each data in the target domain. Pseudo-multi-labeling is a process of picking up multilabel for the target domain data. Each x t i ∈ X t is assigned with a pseudo label ỹt i . The assigning pseudo multilabel process can be expressed as follows\n\nwhere A is an index set of source labels and for each index satisfies the similarity scores between source domain features of the read dynamic external memory as well as a feature from the target domain is higher than the similarity score threshold γ. This process can be formulated by the inner product between the external memory and target domain feature\n\nWith each unlabeled data in the target domain assigned pseudo label ỹt i , we can reformulate the target domain corpus as\n\n. With the pseudo multilabel ỹt i , data from the source domain and the target domain can be combined and train them together by the supervised loss function.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Loss Functions",
      "text": "The loss function of the proposed NNPM consists of two parts, one from the source domain, and one from the target domain.\n\nThe source domain loss function aims to bridge the gap between ground truth labels and predictions from source domain data under supervised learning. This function can be expressed by cross-entropy loss function as\n\nwhere y s i is the ground-truth label of source domain data, n is the batch size and p(x s k ) denotes the predicted classification probability after softmax function.\n\nFor target domain loss, the mean square loss (MSE) function is utilized as follows\n\nwhere y t k is the prediction by matrix multiplication between the dynamic external memory features M read and target domain features in the current batch.\n\nThe dynamic external memory contains a large number of features that are not assigned with labels and are treated as negative samples after the pseudo-multi-labeling process. As a result, the target domain suffers from a sample imbalance problem between positive and negative samples. To moderate this problem, hard negative sample mining  [18]  is applied to focus more on hard negative samples than easy negative samples to make the model more robust and discriminative. This process can be formulated as\n\nwhere N neg is the index of negative sample score, sort(•) represents sorted negative score from large to small and λ is the hard negative sample ratio. With this hard negative sample mining process, Equation (  6 ) can be reformulated as the summation of two parts\n\nwhere L t pos is the MSE for positive samples and L t neg is MSE for the positive samples and H neg hard negative samples.\n\nFinally, the overall training loss of this work is given by:\n\n3. EXPERIMENTS 3.1. Databases",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Iemocap",
      "text": "The IEMOCAP  [19]  was an audiovisual database developed to investigate the connection between gestures, speech, and emotions and contained a total of five sessions recorded by 10 professional actors (5 males and 5 females). The database was segmented by speaker turn and 10,039 utterances were generated with 5,255 scripted turns and 4,787 improvised turns respectively. Four out of ten emotional categories provided by the corpus were used in the experiment. They are angry, happy, sad, and neutral, where the happy comprises the samples labeled as excited.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Urdu",
      "text": "The Urdu language is an official language in Pakistan. The Urdu database was comprised of spontaneous emotional speech collected from Urdu TV talk show  [20] . The database contained a total of 400 utterances from 38 speakers (27 males and 11 females) with four categorical emotional categories: angry, happy, sad, and, neutral. The corpus included emotional excerpts from spontaneous unscripted discussions among different speakers on the TV talk show. The data were split into training and test datasets using the Scikit-learn toolkit  [21]  with a ratio of 67% for the training set and 33% for the test set respectively.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Estonian",
      "text": "The Estonian emotional speech corpus (Ekropus) contained recordings of read speech sentences of four categorical emotions: anger, joy, sadness, and neutral  [22] . Out of the 173 sentences (1473 tokens) in total, 45.7% was anger, 11.6% was sadness, 1.7% was joy (happiness), and 14.4% was neutral. The dataset partition followed the same ratio that for the Urdu corpus.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Persian",
      "text": "The Sharif Emotional Speech Database (ShEMO) was a database for the Persian language  [23]  and consisted of 3000 utterances from 87 native Persian speakers (31 females, 56 males). Six emotional categories were provided in the database: surprise, happiness, sadness, fear, anger, and neutral. In this experiment, we only consider happiness, sadness, anger, and neural. The dataset partition followed the same ratio as the ones above.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "German",
      "text": "EMO-DB was an emotional speech database in the German language  [24]  and contained 10 German sentences from daily life communication produced by 10 actors (5 male and 5 female) in 7 emotions including anger, neutral, fear, joy, sadness, disgust, and boredom. Consistent with the data from the previous databases, four emotional categories (joy (happy), sadness, anger, and neutral) were selected for use in the experiment. Following the practice in  [9] , audios from actors #15 and # 16 were used as test partition and validation partition respectively while audios from the rest of the actors were used as the training partition.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Spectrogram Extraction",
      "text": "Firstly, the utterance duration is unified to 7.5 seconds by padding zeros for a short duration short than the unified duration and cropping along the time axis for long utterances which longer than the unified duration. Then, a Hanning window with a length of 400 is applied to the audio signals. The sampling rate is set at 16000Hz. For every frame, a shortterm Fourier transform (STFT) of length 512 with hop length 160 is computed. Finally, Mel-scale is used to mimic the nonlinear human ear perception of sound. We have also tried several different lengths of sampling window but the results had no clear differences.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Experimental Setup",
      "text": "The proposed model is trained in an end-to-end manner using the PyTorch toolkit  [25] . The IEMOCAP database is used for the source domain corpus and the rest databases are used as the target domain corpora. The model is trained for 50 epochs. In each iteration during the target domain training, the batch size is set to 32 and the model is optimized through the total loss by the Adam optimizer  [26]  with a learning rate of 10 -4 , a decay rate of 5 -5 . A dropout is applied after every BLSTM layer with a 0.5 dropout probability. The dimensions of each SNSA module for the source and target domain setting are the same as  [14] . The similarity score threshold γ is set to 0.9 and its influence will be studied in the ablation study of the experimental results. The memory updating rate β starts from 0 and grows linearly to 0.4 through the 50 The proposed NNPM model is compared against the following baselines:\n\n• SNSA-F: the SNSA is trained on the source domain corpus. The parameters are frozen in order to save the source domain knowledge, which is then adapt to the target domain corpus directly by reusing the parameters of the source domain and a four emotional classifiers of the source domain. We refer to this model as the Frozen SNSA (SNSA-F).\n\n• SNSA-wo-SL: this model consists of SNSA and external memory with a pseudo-multilabeling process. The loss only contains the target domain loss L t hard and no source domain loss. We refer to this model as the SNSA without Source domain Loss (SNSA-wo-SL).\n\n• SNSA-wo-HL: the setting of this baseline is similar to the SNSA-wo-SL, but the loss consists of both source domain loss and target domain loss without hard negative mining. We refer to this model as the SNSA without Hard mining Loss (SNSA-wo-HL).",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Experimental Results",
      "text": "Both weighted accuracy (WA) and unweighted accuracy (UA) are measured as the evaluation criteria for evaluating the performances of the proposed and the baseline models. Our NNPM achieves 52.0 % UA that training with IEMOCAP on the source domain and it contains 0.46 million parameters. Table  1  summarizes the UAs and WAs of the proposed NNPM and the three baseline models for the four lowresource corpora. Out of the four models examined in the experiment, the SNSA-F has the lowest performance across all corpora, suggesting that the source domain knowledge inside the SNSA-F alone is not enough for direct adaptation to the target domain. In comparison, the SNSA-wo-SL outperforms the SNSA-F by an absolute UA improvement of 20.27% for Urdu, and 12.5% for EMO-DB. This is likely due to the assignment of multilabel for target domain data by computing the similarities between features in the external memory and the target domain, which may have an improvement in the reliability of the target domain labels. But its performance is worse than NNPM. Since the source domain loss can guarantee the accuracy of the source domain module. The SNSA-wo-HL performance is worse than NNPM, which is because a large number of negative samples harm the model performance. The NNPM achieves the best overall performance, outperforming the SNSA-F by an absolute WA improvement of 17.77% for Urdu, 12.50% for Ekropus, 15.27% for ShEMO, and 14.70% for EMO-DB. These performance gains demonstrate the effectiveness of hard negative mining compares with SNSA-wo-SL. However, the performance of the NNPM is worse for Urdu compared to SNSA-wo-SL, which we speculate is due to a large difference in distribution between IEMOCAP and Urdu so that the source domain loss in NNPM may actually hurt the performance of target domain training. The performance for Ekropus and ShEMO under SNSA-F baseline is below 25% due to a great domain mismatch between the English and Estonian or Persian. In addition, the backbone contains only knowledge of the source domain if the Siamese parameters are frozen without any fine-tuning for the target domain. The worse performance from Ekropus under SNSA-wo-HL is due to the greater unbalance in data distribution with almost 46% of data being anger and only 1.7% being joy. In comparison, the distribution of IEMOCAP is balanced so that it achieved 52.0% UA on the source domain.\n\nTable  2 . Comparison UA with other state-of-the-art methods on the EMO-DB corpus. The DANN method  [8]  and AL method  [9]  results are taken from the re-implementation in paper  [7] . Therefore, we cite those results from  [7]  directly.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Method",
      "text": "EMO-DB FLUDA  [7]  34.9 DANN  [7]  28.5 AL  [7]  42.5 Ours (NNPM) 50.6\n\nIn addition to the experiment above, we also compare the proposed NNPM with three recent state-of-the-art methods, FLUDA  [7] , DANN  [8]  and AL  [9] , for the EMO-DB database and the results are summarized in Table  2 . The FLUDA is an unsupervised cross-corpus SER model based on few-shot learning  [7] . The NNPM largely outperforms this few-shot based method by a relative UA increase of 45.5%. The domain adversarial neural network (DANN) and the aggregated multi-task learning (AL) are not publicly available, we do not implement these systems but instead cite these results of their performance on EMO-DB from re-implementations in  [7] . In comparisons, the NNPM outperforms adversarial-based method, that is DANN, by a ralative UA improvement of 77.5% and outperformes multi-task learning based method, that is AL, with 19.1%. The results show that the NNPM outperforms recent state-of-the-art methods.\n\nFinally, ablation studies are conducted to examine the influence of the modules of and hyperparameters in the NNPM on the system performance. Influence of freezing the first N convolutional layers for SNSA. Freezing a different number of the first N convolutional layers of SNSA and fine-tune the rest has different effects on the NNPM performance (see Table  3 ). In specifies, freezing the first two layers layer and fine-tuning the rest gives the best performance and has a relative UA improvement of 9.9% compared to freezing the first convolutional layer of SNSA and a relative UA improvement of 15.6% compare to not freezing any convolutional layer on the EMO-DB corpus. Layer freezing can preserve knowledge from the source domain which can be reused for the target domain during the transfer learning. Because the first two layers usually contain more knowledge from the source domain, it is reasonable that freezing the first two layers in this experiment leads to the best model performance compared to freezing only the first layer or no layer freezing at all.\n\nThe similarity score threshold γ. Hyper-parameter γ controls the degree of similarity between features in the dynamic external memory and features from the target domain. The threshold is varied from 0.4 to 0.9 and evaluate on the ShEMO and EMO-DB corpora and the result is illustrated in Figure  2 . For the ShEMO corpus, the UA increases almost linearly from small to large γ, and the best UA is achieved when γ = 0.9. For the EMO-DB corpus, a similar increasing trend is also found although the UA increases relatively slow at first and does not accelerate until γ = 0.6. The best performance is also achieved when γ = 0.9. This result shows that similarity between two features has a positive correlation with threshold γ and that a large similarity score threshold may offer a more stable pseudo multilabel for the target do-",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Conclusions",
      "text": "In this paper, we propose a novel unsupervised framework to improve the performance of cross-lingual SER with dynamic external memory and hard negative sample mining strategy. Experiments show that our NNPM exceeds baselines and other approaches across different low-resource corpora.",
      "page_start": 6,
      "page_end": 6
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The pipeline of our proposed NNPM. We ﬁrst compute spectrograms for both source domain audios and query audio",
      "page": 2
    },
    {
      "caption": "Figure 1: , given two groups of audios from the",
      "page": 2
    },
    {
      "caption": "Figure 2: For the ShEMO corpus, the UA increases almost",
      "page": 6
    },
    {
      "caption": "Figure 2: Ablation study of similarity score threshold γ on",
      "page": 6
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "{li.jin,nan.yan,lan.wang}@siat.ac.cn": "ABSTRACT"
        },
        {
          "{li.jin,nan.yan,lan.wang}@siat.ac.cn": "Speech Emotion Recognition\n(SER)\nin\na\nsingle\nlanguage"
        },
        {
          "{li.jin,nan.yan,lan.wang}@siat.ac.cn": "has achieved remarkable results\nthrough deep learning ap-"
        },
        {
          "{li.jin,nan.yan,lan.wang}@siat.ac.cn": "proaches\nin the\nlast decade.\nHowever,\ncross-lingual SER"
        },
        {
          "{li.jin,nan.yan,lan.wang}@siat.ac.cn": "remains a challenge in real-world applications due to a great"
        },
        {
          "{li.jin,nan.yan,lan.wang}@siat.ac.cn": "difference between the source and target domain distributions."
        },
        {
          "{li.jin,nan.yan,lan.wang}@siat.ac.cn": "To address\nthis\nissue, we propose\nan unsupervised cross-"
        },
        {
          "{li.jin,nan.yan,lan.wang}@siat.ac.cn": "lingual Neural Network with Pseudo Multilabel (NNPM) that"
        },
        {
          "{li.jin,nan.yan,lan.wang}@siat.ac.cn": "is trained to learn the emotion similarities between source do-"
        },
        {
          "{li.jin,nan.yan,lan.wang}@siat.ac.cn": "main features inside an external memory adjusted to identify"
        },
        {
          "{li.jin,nan.yan,lan.wang}@siat.ac.cn": "emotion in cross-lingual databases. NNPM introduces a novel"
        },
        {
          "{li.jin,nan.yan,lan.wang}@siat.ac.cn": "approach that\nleverages external memory to store source do-"
        },
        {
          "{li.jin,nan.yan,lan.wang}@siat.ac.cn": "main features and generates pseudo multilabel for each target"
        },
        {
          "{li.jin,nan.yan,lan.wang}@siat.ac.cn": ""
        },
        {
          "{li.jin,nan.yan,lan.wang}@siat.ac.cn": "domain data by computing the similarities between the exter-"
        },
        {
          "{li.jin,nan.yan,lan.wang}@siat.ac.cn": ""
        },
        {
          "{li.jin,nan.yan,lan.wang}@siat.ac.cn": "nal memory and the target domain features. We evaluate our"
        },
        {
          "{li.jin,nan.yan,lan.wang}@siat.ac.cn": ""
        },
        {
          "{li.jin,nan.yan,lan.wang}@siat.ac.cn": "approach on multiple different\nlanguages of speech emotion"
        },
        {
          "{li.jin,nan.yan,lan.wang}@siat.ac.cn": ""
        },
        {
          "{li.jin,nan.yan,lan.wang}@siat.ac.cn": "databases. Experimental results show our proposed approach"
        },
        {
          "{li.jin,nan.yan,lan.wang}@siat.ac.cn": ""
        },
        {
          "{li.jin,nan.yan,lan.wang}@siat.ac.cn": "signiﬁcantly improves\nthe weighted accuracy (WA)\nacross"
        },
        {
          "{li.jin,nan.yan,lan.wang}@siat.ac.cn": ""
        },
        {
          "{li.jin,nan.yan,lan.wang}@siat.ac.cn": "multiple low-resource languages on Urdu, Skropus, ShEMO,"
        },
        {
          "{li.jin,nan.yan,lan.wang}@siat.ac.cn": ""
        },
        {
          "{li.jin,nan.yan,lan.wang}@siat.ac.cn": "and EMO-DB corpus. To facilitate further research, code is"
        },
        {
          "{li.jin,nan.yan,lan.wang}@siat.ac.cn": ""
        },
        {
          "{li.jin,nan.yan,lan.wang}@siat.ac.cn": "available at https://github.com/happyjin/NNPM"
        },
        {
          "{li.jin,nan.yan,lan.wang}@siat.ac.cn": ""
        },
        {
          "{li.jin,nan.yan,lan.wang}@siat.ac.cn": "Index Terms— speech\nemotion\nrecognition,\nhuman-"
        },
        {
          "{li.jin,nan.yan,lan.wang}@siat.ac.cn": "computer interaction, cross-domain adaptation, cross-lingual"
        },
        {
          "{li.jin,nan.yan,lan.wang}@siat.ac.cn": "speech emotion recognition"
        },
        {
          "{li.jin,nan.yan,lan.wang}@siat.ac.cn": ""
        },
        {
          "{li.jin,nan.yan,lan.wang}@siat.ac.cn": "1.\nINTRODUCTION"
        },
        {
          "{li.jin,nan.yan,lan.wang}@siat.ac.cn": ""
        },
        {
          "{li.jin,nan.yan,lan.wang}@siat.ac.cn": "Speech emotion recognition(SER)\nis the recognition of dif-"
        },
        {
          "{li.jin,nan.yan,lan.wang}@siat.ac.cn": "ferent human emotions from a given speech and is gaining"
        },
        {
          "{li.jin,nan.yan,lan.wang}@siat.ac.cn": "increasing interest\nin the areas of human-computer\ninterac-"
        },
        {
          "{li.jin,nan.yan,lan.wang}@siat.ac.cn": "tion, computational neuroscience, cognitive psychology,\nin-"
        },
        {
          "{li.jin,nan.yan,lan.wang}@siat.ac.cn": "telligent\ntutor for children, and medical healthcare [1, 2, 3]."
        },
        {
          "{li.jin,nan.yan,lan.wang}@siat.ac.cn": "Advancements in machine learning methods in recent years"
        },
        {
          "{li.jin,nan.yan,lan.wang}@siat.ac.cn": "have allowed SER systems to exhibit excellent performance"
        },
        {
          "{li.jin,nan.yan,lan.wang}@siat.ac.cn": "when training and testing data from corpora of the same lan-"
        },
        {
          "{li.jin,nan.yan,lan.wang}@siat.ac.cn": "guage. However,\nthe development of more robust SER sys-"
        },
        {
          "{li.jin,nan.yan,lan.wang}@siat.ac.cn": "tems for the cross-lingual scenarios remains an open problem"
        },
        {
          "{li.jin,nan.yan,lan.wang}@siat.ac.cn": "due to the domain mismatch problem."
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Fig. 1. The pipeline of our proposed NNPM. We ﬁrst compute spectrograms for both source domain audios and query audio": "from the current minibatch.\nThen both spectrograms are input\ninto Siamese Network with Self Attention (SNSA) and the"
        },
        {
          "Fig. 1. The pipeline of our proposed NNPM. We ﬁrst compute spectrograms for both source domain audios and query audio": "feature is extracted from this block. The source-domain features are dynamically stored in and read from the dynamic external"
        },
        {
          "Fig. 1. The pipeline of our proposed NNPM. We ﬁrst compute spectrograms for both source domain audios and query audio": "memory. To learn the emotion similarities by computing the cosine distance between features inside dynamic external memory"
        },
        {
          "Fig. 1. The pipeline of our proposed NNPM. We ﬁrst compute spectrograms for both source domain audios and query audio": "and the query vector. The query vector is a feature extract from the SNSA of the target domain. After that,\nthe query vector"
        },
        {
          "Fig. 1. The pipeline of our proposed NNPM. We ﬁrst compute spectrograms for both source domain audios and query audio": "from the target domain is assigned with pseudo multilabel based on the similarity scores. The model is optimized through loss"
        },
        {
          "Fig. 1. The pipeline of our proposed NNPM. We ﬁrst compute spectrograms for both source domain audios and query audio": "from the source domain and the target domain using a hard negative sample mining strategy."
        },
        {
          "Fig. 1. The pipeline of our proposed NNPM. We ﬁrst compute spectrograms for both source domain audios and query audio": "However, samples in few-shot\nlearning signiﬁcantly depend\ndomain. Experimental\nresults show the effectiveness of our"
        },
        {
          "Fig. 1. The pipeline of our proposed NNPM. We ﬁrst compute spectrograms for both source domain audios and query audio": "on the choice of support set\nthat might make the challenging\nNNPM in series of experiments."
        },
        {
          "Fig. 1. The pipeline of our proposed NNPM. We ﬁrst compute spectrograms for both source domain audios and query audio": "to apply to practical settings. Moreover, because of the strong\nThe main contributions of this paper are:\n(1) a novel un-"
        },
        {
          "Fig. 1. The pipeline of our proposed NNPM. We ﬁrst compute spectrograms for both source domain audios and query audio": "assumption of the support set sampled uniformly from a sin-\nsupervised cross-lingual SER framework with pseudo mul-"
        },
        {
          "Fig. 1. The pipeline of our proposed NNPM. We ﬁrst compute spectrograms for both source domain audios and query audio": "gle distribution,\nit\nleads to the unstable number of samples\ntilabel\nin the target domain;\n(2) a dynamic external memory"
        },
        {
          "Fig. 1. The pipeline of our proposed NNPM. We ﬁrst compute spectrograms for both source domain audios and query audio": "selected for each class during the training process [12].\ndesign with memory update mechanism; (3) vastly exceed the"
        },
        {
          "Fig. 1. The pipeline of our proposed NNPM. We ﬁrst compute spectrograms for both source domain audios and query audio": "baseline and other approaches for unweighted accuracy cross"
        },
        {
          "Fig. 1. The pipeline of our proposed NNPM. We ﬁrst compute spectrograms for both source domain audios and query audio": "In an attempt\nto provide a solution to the challenges de-"
        },
        {
          "Fig. 1. The pipeline of our proposed NNPM. We ﬁrst compute spectrograms for both source domain audios and query audio": "different languages."
        },
        {
          "Fig. 1. The pipeline of our proposed NNPM. We ﬁrst compute spectrograms for both source domain audios and query audio": "scribed above, we build a novel\nframework that models the"
        },
        {
          "Fig. 1. The pipeline of our proposed NNPM. We ﬁrst compute spectrograms for both source domain audios and query audio": "The rest of this paper is organized as follows. The base-"
        },
        {
          "Fig. 1. The pipeline of our proposed NNPM. We ﬁrst compute spectrograms for both source domain audios and query audio": "unsupervised cross-lingual SER as a multilabel classiﬁcation"
        },
        {
          "Fig. 1. The pipeline of our proposed NNPM. We ﬁrst compute spectrograms for both source domain audios and query audio": "line model and the proposed method are described in Section"
        },
        {
          "Fig. 1. The pipeline of our proposed NNPM. We ﬁrst compute spectrograms for both source domain audios and query audio": "problem.\nThis\nidea is\ninspired by ensemble learning [13],"
        },
        {
          "Fig. 1. The pipeline of our proposed NNPM. We ﬁrst compute spectrograms for both source domain audios and query audio": "2. The experimental setup and results analysis are present\nin"
        },
        {
          "Fig. 1. The pipeline of our proposed NNPM. We ﬁrst compute spectrograms for both source domain audios and query audio": "which averages the decisions from multiple related emotion"
        },
        {
          "Fig. 1. The pipeline of our proposed NNPM. We ﬁrst compute spectrograms for both source domain audios and query audio": "Section 3. Section 4 accomplishes the study with conclusions"
        },
        {
          "Fig. 1. The pipeline of our proposed NNPM. We ﬁrst compute spectrograms for both source domain audios and query audio": "features and decreases the effect\nfrom one speciﬁc emotion"
        },
        {
          "Fig. 1. The pipeline of our proposed NNPM. We ﬁrst compute spectrograms for both source domain audios and query audio": "and directions."
        },
        {
          "Fig. 1. The pipeline of our proposed NNPM. We ﬁrst compute spectrograms for both source domain audios and query audio": "label to improve the model robustness. In addition, a dynamic"
        },
        {
          "Fig. 1. The pipeline of our proposed NNPM. We ﬁrst compute spectrograms for both source domain audios and query audio": "external memory was also designed to store and update source"
        },
        {
          "Fig. 1. The pipeline of our proposed NNPM. We ﬁrst compute spectrograms for both source domain audios and query audio": "2. METHODOLOGY"
        },
        {
          "Fig. 1. The pipeline of our proposed NNPM. We ﬁrst compute spectrograms for both source domain audios and query audio": "domain features stably so that\nthe number of samples from"
        },
        {
          "Fig. 1. The pipeline of our proposed NNPM. We ﬁrst compute spectrograms for both source domain audios and query audio": "the source domain would be all utilized during training. As"
        },
        {
          "Fig. 1. The pipeline of our proposed NNPM. We ﬁrst compute spectrograms for both source domain audios and query audio": "2.1.\nSiamese Network with Self Attention (SNSA)"
        },
        {
          "Fig. 1. The pipeline of our proposed NNPM. We ﬁrst compute spectrograms for both source domain audios and query audio": "illustrated in Figure 1, given two groups of audios from the"
        },
        {
          "Fig. 1. The pipeline of our proposed NNPM. We ﬁrst compute spectrograms for both source domain audios and query audio": "source domain and the target domain, a model is ﬁrst trained\nLet (Xs, Ys) := {(xs\ni , ys\ni )}n\ni=1 denotes the spectrogram xs ∈"
        },
        {
          "Fig. 1. The pipeline of our proposed NNPM. We ﬁrst compute spectrograms for both source domain audios and query audio": "in the source do-\nbased on the source domain in a supervised way. Then the\nXs and its corresponding label ys ∈ Ys"
        },
        {
          "Fig. 1. The pipeline of our proposed NNPM. We ﬁrst compute spectrograms for both source domain audios and query audio": "main, where n is the number of data in the source domain, and\nsource domain features are saved in dynamic external mem-"
        },
        {
          "Fig. 1. The pipeline of our proposed NNPM. We ﬁrst compute spectrograms for both source domain audios and query audio": "ory and pseudo multilabel is assigned to target domain data by\nXt = {xt\ni}m\ni=1 denotes the spectrogram of the target domain"
        },
        {
          "Fig. 1. The pipeline of our proposed NNPM. We ﬁrst compute spectrograms for both source domain audios and query audio": "computing the similarities between features of\nthe dynamic\ncorpus without\nlabeling, where xs\ni ∈ RL×d and xt\ni ∈ RL×d"
        },
        {
          "Fig. 1. The pipeline of our proposed NNPM. We ﬁrst compute spectrograms for both source domain audios and query audio": "external memory and features of the target domain. Finally,\nrepresent L the temporal\nlength of\nthe spectrogram in the"
        },
        {
          "Fig. 1. The pipeline of our proposed NNPM. We ﬁrst compute spectrograms for both source domain audios and query audio": "the unsupervised training phase is optimized by combining\nsource and the target domain separately and d is the dimen-"
        },
        {
          "Fig. 1. The pipeline of our proposed NNPM. We ﬁrst compute spectrograms for both source domain audios and query audio": "the source domain cross entropy loss and loss from the target\nsion of a spectrogram feature vector."
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "The SNSA is used to extract the features of the source do-": "main spectrogram Xs and the target domain spectrogram Xt.",
          "in the target domain.\nPseudo-multi-labeling is a process of": "picking up multilabel for the target domain data."
        },
        {
          "The SNSA is used to extract the features of the source do-": "Our SNSA consists of\ntwo identical\nself-attention modules",
          "in the target domain.\nPseudo-multi-labeling is a process of": "Each xt\nis assigned with a pseudo label ˜yt\nThe\ni ∈ Xt\ni ."
        },
        {
          "The SNSA is used to extract the features of the source do-": "with shared weights for each other.\nFor each self-attention",
          "in the target domain.\nPseudo-multi-labeling is a process of": "assigning pseudo multilabel process can be expressed as fol-"
        },
        {
          "The SNSA is used to extract the features of the source do-": "module, we follow the previous work [14]\nthat mainly con-",
          "in the target domain.\nPseudo-multi-labeling is a process of": "lows"
        },
        {
          "The SNSA is used to extract the features of the source do-": "sists of 3 components, namely a 2-layer convolutional neural",
          "in the target domain.\nPseudo-multi-labeling is a process of": "yt\nfor j ∈ A\n(4)\ni = Ys[j]"
        },
        {
          "The SNSA is used to extract the features of the source do-": "network\n(CNN),\na\n2-layer\nbidirectional LSTM (BLSTM)",
          "in the target domain.\nPseudo-multi-labeling is a process of": ""
        },
        {
          "The SNSA is used to extract the features of the source do-": "",
          "in the target domain.\nPseudo-multi-labeling is a process of": "where A is an index set of source labels and for each index sat-"
        },
        {
          "The SNSA is used to extract the features of the source do-": "[15], and a self-attention network. A temporal MaxPooling",
          "in the target domain.\nPseudo-multi-labeling is a process of": ""
        },
        {
          "The SNSA is used to extract the features of the source do-": "",
          "in the target domain.\nPseudo-multi-labeling is a process of": "isﬁes the similarity scores between source domain features of"
        },
        {
          "The SNSA is used to extract the features of the source do-": "is applied to each convolution layer\nfor dimensionality re-",
          "in the target domain.\nPseudo-multi-labeling is a process of": ""
        },
        {
          "The SNSA is used to extract the features of the source do-": "",
          "in the target domain.\nPseudo-multi-labeling is a process of": "the read dynamic external memory as well as a feature from"
        },
        {
          "The SNSA is used to extract the features of the source do-": "duction and ReLU [16]\nis applied as an activation function",
          "in the target domain.\nPseudo-multi-labeling is a process of": ""
        },
        {
          "The SNSA is used to extract the features of the source do-": "",
          "in the target domain.\nPseudo-multi-labeling is a process of": "the target domain is higher\nthan the similarity score thresh-"
        },
        {
          "The SNSA is used to extract the features of the source do-": "after each MaxPooling operation to enhance the model non-",
          "in the target domain.\nPseudo-multi-labeling is a process of": ""
        },
        {
          "The SNSA is used to extract the features of the source do-": "",
          "in the target domain.\nPseudo-multi-labeling is a process of": "old γ.\nThis process can be formulated by the inner prod-"
        },
        {
          "The SNSA is used to extract the features of the source do-": "linearity.\nThe BLSTM with forward and backward hidden",
          "in the target domain.\nPseudo-multi-labeling is a process of": ""
        },
        {
          "The SNSA is used to extract the features of the source do-": "",
          "in the target domain.\nPseudo-multi-labeling is a process of": "uct between the external memory and target domain feature"
        },
        {
          "The SNSA is used to extract the features of the source do-": "states\nconcatenation\nis\nrepresented\nby H blstm.\nThe\nself-",
          "in the target domain.\nPseudo-multi-labeling is a process of": ""
        },
        {
          "The SNSA is used to extract the features of the source do-": "",
          "in the target domain.\nPseudo-multi-labeling is a process of": "||(cid:104) Mread, hattn\n(cid:105)||2 ≥ γ, where < ·, · > is the inner prod-"
        },
        {
          "The SNSA is used to extract the features of the source do-": "",
          "in the target domain.\nPseudo-multi-labeling is a process of": "j"
        },
        {
          "The SNSA is used to extract the features of the source do-": "attention a is\nthe weighted sum of\nthe hidden states given",
          "in the target domain.\nPseudo-multi-labeling is a process of": ""
        },
        {
          "The SNSA is used to extract the features of the source do-": "",
          "in the target domain.\nPseudo-multi-labeling is a process of": "uct. With each unlabeled data in the target domain assigned"
        },
        {
          "The SNSA is used to extract the features of the source do-": "H blstm and is described by the following equations",
          "in the target domain.\nPseudo-multi-labeling is a process of": ""
        },
        {
          "The SNSA is used to extract the features of the source do-": "",
          "in the target domain.\nPseudo-multi-labeling is a process of": "pseudo label ˜yt\ni , we can reformulate the target domain cor-"
        },
        {
          "The SNSA is used to extract the features of the source do-": "(1)\na = softmax(W2 tanh(W1H blstm))",
          "in the target domain.\nPseudo-multi-labeling is a process of": "pus as (Xt, ˜Yt) = {(xt\ni, ˜yt\ni )}n\ni=1. With the pseudo multilabel"
        },
        {
          "The SNSA is used to extract the features of the source do-": "",
          "in the target domain.\nPseudo-multi-labeling is a process of": "yt"
        },
        {
          "The SNSA is used to extract the features of the source do-": "",
          "in the target domain.\nPseudo-multi-labeling is a process of": "i , data from the source domain and the target domain can"
        },
        {
          "The SNSA is used to extract the features of the source do-": "hattn = aH blstm\n(2)",
          "in the target domain.\nPseudo-multi-labeling is a process of": ""
        },
        {
          "The SNSA is used to extract the features of the source do-": "",
          "in the target domain.\nPseudo-multi-labeling is a process of": "be combined and train them together by the supervised loss"
        },
        {
          "The SNSA is used to extract the features of the source do-": "where W1 and W2 are the parameters to be optimized during",
          "in the target domain.\nPseudo-multi-labeling is a process of": "function."
        },
        {
          "The SNSA is used to extract the features of the source do-": "or\ntraining. hattn can be either source domain feature hattn\ns",
          "in the target domain.\nPseudo-multi-labeling is a process of": ""
        },
        {
          "The SNSA is used to extract the features of the source do-": ".\ntarget domain feature hattn",
          "in the target domain.\nPseudo-multi-labeling is a process of": ""
        },
        {
          "The SNSA is used to extract the features of the source do-": "t",
          "in the target domain.\nPseudo-multi-labeling is a process of": "2.4. Loss Functions"
        },
        {
          "The SNSA is used to extract the features of the source do-": "",
          "in the target domain.\nPseudo-multi-labeling is a process of": "The loss\nfunction of\nthe proposed NNPM consists of\ntwo"
        },
        {
          "The SNSA is used to extract the features of the source do-": "2.2. Dynamic External Memory",
          "in the target domain.\nPseudo-multi-labeling is a process of": ""
        },
        {
          "The SNSA is used to extract the features of the source do-": "",
          "in the target domain.\nPseudo-multi-labeling is a process of": "parts, one from the source domain, and one from the target"
        },
        {
          "The SNSA is used to extract the features of the source do-": "Dynamic\nexternal memory\nis\nproposed\nto\nstore\nand\nload",
          "in the target domain.\nPseudo-multi-labeling is a process of": ""
        },
        {
          "The SNSA is used to extract the features of the source do-": "",
          "in the target domain.\nPseudo-multi-labeling is a process of": "domain."
        },
        {
          "The SNSA is used to extract the features of the source do-": "source domain features dynamically. Writing operation stores",
          "in the target domain.\nPseudo-multi-labeling is a process of": ""
        },
        {
          "The SNSA is used to extract the features of the source do-": "",
          "in the target domain.\nPseudo-multi-labeling is a process of": "The source domain loss function aims to bridge the gap"
        },
        {
          "The SNSA is used to extract the features of the source do-": "and updates\nthe\nsource domain features\ninto the dynamic",
          "in the target domain.\nPseudo-multi-labeling is a process of": ""
        },
        {
          "The SNSA is used to extract the features of the source do-": "",
          "in the target domain.\nPseudo-multi-labeling is a process of": "between ground truth labels and predictions from source do-"
        },
        {
          "The SNSA is used to extract the features of the source do-": "external memory.\nSpeciﬁcally,\nit\nis updated by removing",
          "in the target domain.\nPseudo-multi-labeling is a process of": ""
        },
        {
          "The SNSA is used to extract the features of the source do-": "",
          "in the target domain.\nPseudo-multi-labeling is a process of": "main data under supervised learning.\nThis function can be"
        },
        {
          "The SNSA is used to extract the features of the source do-": "previous features and writing new features with a decay rate",
          "in the target domain.\nPseudo-multi-labeling is a process of": ""
        },
        {
          "The SNSA is used to extract the features of the source do-": "",
          "in the target domain.\nPseudo-multi-labeling is a process of": "expressed by cross-entropy loss function as"
        },
        {
          "The SNSA is used to extract the features of the source do-": "into the dynamic external memory. One crucial problem for",
          "in the target domain.\nPseudo-multi-labeling is a process of": ""
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "where Nneg is the index of negative sample score, sort(·) rep-": "resents sorted negative score from large to small and λ is the",
          "3.1.4. Persian": ""
        },
        {
          "where Nneg is the index of negative sample score, sort(·) rep-": "",
          "3.1.4. Persian": "The Sharif Emotional Speech Database\n(ShEMO) was\na"
        },
        {
          "where Nneg is the index of negative sample score, sort(·) rep-": "hard negative sample ratio. With this hard negative sample",
          "3.1.4. Persian": ""
        },
        {
          "where Nneg is the index of negative sample score, sort(·) rep-": "",
          "3.1.4. Persian": "database for the Persian language [23] and consisted of 3000"
        },
        {
          "where Nneg is the index of negative sample score, sort(·) rep-": "mining process, Equation (6) can be reformulated as the sum-",
          "3.1.4. Persian": ""
        },
        {
          "where Nneg is the index of negative sample score, sort(·) rep-": "",
          "3.1.4. Persian": "utterances\nfrom 87\nnative\nPersian\nspeakers\n(31\nfemales,"
        },
        {
          "where Nneg is the index of negative sample score, sort(·) rep-": "mation of two parts",
          "3.1.4. Persian": ""
        },
        {
          "where Nneg is the index of negative sample score, sort(·) rep-": "",
          "3.1.4. Persian": "56 males).\nSix emotional categories were provided in the"
        },
        {
          "where Nneg is the index of negative sample score, sort(·) rep-": "",
          "3.1.4. Persian": "database:\nsurprise, happiness, sadness,\nfear, anger, and neu-"
        },
        {
          "where Nneg is the index of negative sample score, sort(·) rep-": "Lt\n(8)\nhard = Lt\npos + Lt\nneg",
          "3.1.4. Persian": ""
        },
        {
          "where Nneg is the index of negative sample score, sort(·) rep-": "",
          "3.1.4. Persian": "tral. In this experiment, we only consider happiness, sadness,"
        },
        {
          "where Nneg is the index of negative sample score, sort(·) rep-": "",
          "3.1.4. Persian": "anger, and neural.\nThe dataset partition followed the same"
        },
        {
          "where Nneg is the index of negative sample score, sort(·) rep-": "where Lt\nis the MSE for positive samples and Lt\npos\nneg is MSE",
          "3.1.4. Persian": ""
        },
        {
          "where Nneg is the index of negative sample score, sort(·) rep-": "",
          "3.1.4. Persian": "ratio as the ones above."
        },
        {
          "where Nneg is the index of negative sample score, sort(·) rep-": "for the positive samples and Hneg hard negative samples.",
          "3.1.4. Persian": ""
        },
        {
          "where Nneg is the index of negative sample score, sort(·) rep-": "Finally, the overall training loss of this work is given by:",
          "3.1.4. Persian": ""
        },
        {
          "where Nneg is the index of negative sample score, sort(·) rep-": "",
          "3.1.4. Persian": "3.1.5. German"
        },
        {
          "where Nneg is the index of negative sample score, sort(·) rep-": "L = Ls\n(9)\nCE + Lt\nhard",
          "3.1.4. Persian": ""
        },
        {
          "where Nneg is the index of negative sample score, sort(·) rep-": "",
          "3.1.4. Persian": "EMO-DB was an emotional speech database in the German"
        },
        {
          "where Nneg is the index of negative sample score, sort(·) rep-": "",
          "3.1.4. Persian": "language [24] and contained 10 German sentences from daily"
        },
        {
          "where Nneg is the index of negative sample score, sort(·) rep-": "3. EXPERIMENTS",
          "3.1.4. Persian": "life communication produced by 10 actors (5 male and 5 fe-"
        },
        {
          "where Nneg is the index of negative sample score, sort(·) rep-": "",
          "3.1.4. Persian": "male)\nin 7 emotions including anger, neutral,\nfear,\njoy, sad-"
        },
        {
          "where Nneg is the index of negative sample score, sort(·) rep-": "3.1. Databases",
          "3.1.4. Persian": "ness, disgust, and boredom. Consistent with the data from the"
        },
        {
          "where Nneg is the index of negative sample score, sort(·) rep-": "",
          "3.1.4. Persian": "previous databases,\nfour emotional categories (joy (happy),"
        },
        {
          "where Nneg is the index of negative sample score, sort(·) rep-": "3.1.1.\nIEMOCAP",
          "3.1.4. Persian": ""
        },
        {
          "where Nneg is the index of negative sample score, sort(·) rep-": "",
          "3.1.4. Persian": "sadness, anger, and neutral) were selected for use in the ex-"
        },
        {
          "where Nneg is the index of negative sample score, sort(·) rep-": "",
          "3.1.4. Persian": "periment.\nFollowing the practice in [9], audios from actors"
        },
        {
          "where Nneg is the index of negative sample score, sort(·) rep-": "The IEMOCAP [19] was an audiovisual database developed",
          "3.1.4. Persian": ""
        },
        {
          "where Nneg is the index of negative sample score, sort(·) rep-": "",
          "3.1.4. Persian": "#15 and # 16 were used as test partition and validation parti-"
        },
        {
          "where Nneg is the index of negative sample score, sort(·) rep-": "to investigate the connection between gestures, speech, and",
          "3.1.4. Persian": ""
        },
        {
          "where Nneg is the index of negative sample score, sort(·) rep-": "",
          "3.1.4. Persian": "tion respectively while audios from the rest of the actors were"
        },
        {
          "where Nneg is the index of negative sample score, sort(·) rep-": "emotions and contained a total of ﬁve sessions recorded by 10",
          "3.1.4. Persian": ""
        },
        {
          "where Nneg is the index of negative sample score, sort(·) rep-": "",
          "3.1.4. Persian": "used as the training partition."
        },
        {
          "where Nneg is the index of negative sample score, sort(·) rep-": "professional actors (5 males and 5 females). The database was",
          "3.1.4. Persian": ""
        },
        {
          "where Nneg is the index of negative sample score, sort(·) rep-": "segmented by speaker turn and 10,039 utterances were gen-",
          "3.1.4. Persian": ""
        },
        {
          "where Nneg is the index of negative sample score, sort(·) rep-": "erated with 5,255 scripted turns and 4,787 improvised turns",
          "3.1.4. Persian": "3.2.\nSpectrogram Extraction"
        },
        {
          "where Nneg is the index of negative sample score, sort(·) rep-": "respectively.\nFour out of\nten emotional categories provided",
          "3.1.4. Persian": ""
        },
        {
          "where Nneg is the index of negative sample score, sort(·) rep-": "",
          "3.1.4. Persian": "Firstly,\nthe utterance duration is uniﬁed to 7.5 seconds by"
        },
        {
          "where Nneg is the index of negative sample score, sort(·) rep-": "by the corpus were used in the experiment. They are angry,",
          "3.1.4. Persian": ""
        },
        {
          "where Nneg is the index of negative sample score, sort(·) rep-": "",
          "3.1.4. Persian": "padding zeros for a short duration short\nthan the uniﬁed du-"
        },
        {
          "where Nneg is the index of negative sample score, sort(·) rep-": "happy, sad, and neutral, where the happy comprises the sam-",
          "3.1.4. Persian": ""
        },
        {
          "where Nneg is the index of negative sample score, sort(·) rep-": "",
          "3.1.4. Persian": "ration and cropping along the time axis for\nlong utterances"
        },
        {
          "where Nneg is the index of negative sample score, sort(·) rep-": "ples labeled as excited.",
          "3.1.4. Persian": ""
        },
        {
          "where Nneg is the index of negative sample score, sort(·) rep-": "",
          "3.1.4. Persian": "which longer than the uniﬁed duration. Then, a Hanning win-"
        },
        {
          "where Nneg is the index of negative sample score, sort(·) rep-": "",
          "3.1.4. Persian": "dow with a length of 400 is applied to the audio signals. The"
        },
        {
          "where Nneg is the index of negative sample score, sort(·) rep-": "3.1.2. Urdu",
          "3.1.4. Persian": "sampling rate is set at 16000Hz.\nFor every frame, a short-"
        },
        {
          "where Nneg is the index of negative sample score, sort(·) rep-": "",
          "3.1.4. Persian": "term Fourier transform (STFT) of length 512 with hop length"
        },
        {
          "where Nneg is the index of negative sample score, sort(·) rep-": "The Urdu language is an ofﬁcial\nlanguage in Pakistan. The",
          "3.1.4. Persian": ""
        },
        {
          "where Nneg is the index of negative sample score, sort(·) rep-": "",
          "3.1.4. Persian": "160 is computed. Finally, Mel-scale is used to mimic the non-"
        },
        {
          "where Nneg is the index of negative sample score, sort(·) rep-": "Urdu\ndatabase was\ncomprised\nof\nspontaneous\nemotional",
          "3.1.4. Persian": ""
        },
        {
          "where Nneg is the index of negative sample score, sort(·) rep-": "",
          "3.1.4. Persian": "linear human ear perception of sound. We have also tried sev-"
        },
        {
          "where Nneg is the index of negative sample score, sort(·) rep-": "speech collected from Urdu TV talk show [20]. The database",
          "3.1.4. Persian": ""
        },
        {
          "where Nneg is the index of negative sample score, sort(·) rep-": "",
          "3.1.4. Persian": "eral different lengths of sampling window but the results had"
        },
        {
          "where Nneg is the index of negative sample score, sort(·) rep-": "contained a\ntotal of 400 utterances\nfrom 38 speakers\n(27",
          "3.1.4. Persian": ""
        },
        {
          "where Nneg is the index of negative sample score, sort(·) rep-": "",
          "3.1.4. Persian": "no clear differences."
        },
        {
          "where Nneg is the index of negative sample score, sort(·) rep-": "males and 11 females) with four categorical emotional cate-",
          "3.1.4. Persian": ""
        },
        {
          "where Nneg is the index of negative sample score, sort(·) rep-": "gories: angry, happy, sad, and, neutral. The corpus included",
          "3.1.4. Persian": ""
        },
        {
          "where Nneg is the index of negative sample score, sort(·) rep-": "",
          "3.1.4. Persian": "3.3. Experimental Setup"
        },
        {
          "where Nneg is the index of negative sample score, sort(·) rep-": "emotional excerpts from spontaneous unscripted discussions",
          "3.1.4. Persian": ""
        },
        {
          "where Nneg is the index of negative sample score, sort(·) rep-": "among different\nspeakers on the TV talk show.\nThe data",
          "3.1.4. Persian": ""
        },
        {
          "where Nneg is the index of negative sample score, sort(·) rep-": "",
          "3.1.4. Persian": "The proposed model\nis trained in an end-to-end manner us-"
        },
        {
          "where Nneg is the index of negative sample score, sort(·) rep-": "were split into training and test datasets using the Scikit-learn",
          "3.1.4. Persian": ""
        },
        {
          "where Nneg is the index of negative sample score, sort(·) rep-": "",
          "3.1.4. Persian": "ing the PyTorch toolkit [25]. The IEMOCAP database is used"
        },
        {
          "where Nneg is the index of negative sample score, sort(·) rep-": "toolkit [21] with a ratio of 67% for the training set and 33%",
          "3.1.4. Persian": ""
        },
        {
          "where Nneg is the index of negative sample score, sort(·) rep-": "",
          "3.1.4. Persian": "for the source domain corpus and the rest databases are used"
        },
        {
          "where Nneg is the index of negative sample score, sort(·) rep-": "for the test set respectively.",
          "3.1.4. Persian": ""
        },
        {
          "where Nneg is the index of negative sample score, sort(·) rep-": "",
          "3.1.4. Persian": "as the target domain corpora.\nThe model\nis trained for 50"
        },
        {
          "where Nneg is the index of negative sample score, sort(·) rep-": "",
          "3.1.4. Persian": "epochs.\nIn each iteration during the target domain training,"
        },
        {
          "where Nneg is the index of negative sample score, sort(·) rep-": "",
          "3.1.4. Persian": "the batch size is set to 32 and the model is optimized through"
        },
        {
          "where Nneg is the index of negative sample score, sort(·) rep-": "3.1.3. Estonian",
          "3.1.4. Persian": ""
        },
        {
          "where Nneg is the index of negative sample score, sort(·) rep-": "",
          "3.1.4. Persian": "the total loss by the Adam optimizer [26] with a learning rate"
        },
        {
          "where Nneg is the index of negative sample score, sort(·) rep-": "The Estonian emotional speech corpus (Ekropus) contained",
          "3.1.4. Persian": "of 10−4, a decay rate of 5−5. A dropout\nis applied after ev-"
        },
        {
          "where Nneg is the index of negative sample score, sort(·) rep-": "recordings of read speech sentences of four categorical emo-",
          "3.1.4. Persian": "ery BLSTM layer with a 0.5 dropout probability. The dimen-"
        },
        {
          "where Nneg is the index of negative sample score, sort(·) rep-": "tions:\nanger,\njoy, sadness, and neutral\n[22]. Out of\nthe 173",
          "3.1.4. Persian": "sions of each SNSA module for the source and target domain"
        },
        {
          "where Nneg is the index of negative sample score, sort(·) rep-": "sentences (1473 tokens) in total, 45.7% was anger, 11.6% was",
          "3.1.4. Persian": "setting are the same as [14]. The similarity score threshold"
        },
        {
          "where Nneg is the index of negative sample score, sort(·) rep-": "sadness, 1.7% was joy (happiness), and 14.4% was neutral.",
          "3.1.4. Persian": "γ is set\nto 0.9 and its inﬂuence will be studied in the abla-"
        },
        {
          "where Nneg is the index of negative sample score, sort(·) rep-": "The dataset partition followed the same ratio that for the Urdu",
          "3.1.4. Persian": "tion study of the experimental results. The memory updating"
        },
        {
          "where Nneg is the index of negative sample score, sort(·) rep-": "corpus.",
          "3.1.4. Persian": "rate β starts from 0 and grows linearly to 0.4 through the 50"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 2: Comparison UA with other state-of-the-art meth-",
      "data": [
        {
          "Table 1. UA and WA for unsupervised cross-lingual results.": "Urdu"
        },
        {
          "Table 1. UA and WA for unsupervised cross-lingual results.": ""
        },
        {
          "Table 1. UA and WA for unsupervised cross-lingual results.": "UA"
        },
        {
          "Table 1. UA and WA for unsupervised cross-lingual results.": "36.36"
        },
        {
          "Table 1. UA and WA for unsupervised cross-lingual results.": "56.63"
        },
        {
          "Table 1. UA and WA for unsupervised cross-lingual results.": "48.12"
        },
        {
          "Table 1. UA and WA for unsupervised cross-lingual results.": "54.55"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table 2: Comparison UA with other state-of-the-art meth-",
      "data": [
        {
          "SNSA-wo-HL\n48.12\n47.72\n24.21": "28.34\nNNPM\n54.55\n51.31",
          "24.67\n28.18\n28.87\n46.02\n50.00": "35.62\n28.19\n35.51\n50.57\n55.88"
        },
        {
          "SNSA-wo-HL\n48.12\n47.72\n24.21": "epochs of\ntraining.\nThe hard negative sample ratio λ is set",
          "24.67\n28.18\n28.87\n46.02\n50.00": "memory and the target domain, which may have an improve-"
        },
        {
          "SNSA-wo-HL\n48.12\n47.72\n24.21": "to 0.01. The parameters of the ﬁrst two convolution modules",
          "24.67\n28.18\n28.87\n46.02\n50.00": "ment\nin the reliability of\nthe target domain labels.\nBut\nits"
        },
        {
          "SNSA-wo-HL\n48.12\n47.72\n24.21": "of SNSA are frozen during training, and the inﬂuence of this",
          "24.67\n28.18\n28.87\n46.02\n50.00": "performance is worse than NNPM. Since the source domain"
        },
        {
          "SNSA-wo-HL\n48.12\n47.72\n24.21": "setup will be studied in the ensuing ablation study of the ex-",
          "24.67\n28.18\n28.87\n46.02\n50.00": "loss can guarantee the accuracy of\nthe source domain mod-"
        },
        {
          "SNSA-wo-HL\n48.12\n47.72\n24.21": "perimental results.",
          "24.67\n28.18\n28.87\n46.02\n50.00": "ule.\nThe SNSA-wo-HL performance is worse than NNPM,"
        },
        {
          "SNSA-wo-HL\n48.12\n47.72\n24.21": "The proposed NNPM model\nis compared against\nthe fol-",
          "24.67\n28.18\n28.87\n46.02\n50.00": "which is because a large number of negative samples harm"
        },
        {
          "SNSA-wo-HL\n48.12\n47.72\n24.21": "lowing baselines:",
          "24.67\n28.18\n28.87\n46.02\n50.00": "the model performance. The NNPM achieves the best over-"
        },
        {
          "SNSA-wo-HL\n48.12\n47.72\n24.21": "",
          "24.67\n28.18\n28.87\n46.02\n50.00": "all performance, outperforming the SNSA-F by an absolute"
        },
        {
          "SNSA-wo-HL\n48.12\n47.72\n24.21": "• SNSA-F:\nthe SNSA is trained on the source domain",
          "24.67\n28.18\n28.87\n46.02\n50.00": ""
        },
        {
          "SNSA-wo-HL\n48.12\n47.72\n24.21": "",
          "24.67\n28.18\n28.87\n46.02\n50.00": "WA improvement of 17.77% for Urdu,\n12.50% for Ekro-"
        },
        {
          "SNSA-wo-HL\n48.12\n47.72\n24.21": "corpus. The parameters are frozen in order to save the",
          "24.67\n28.18\n28.87\n46.02\n50.00": ""
        },
        {
          "SNSA-wo-HL\n48.12\n47.72\n24.21": "",
          "24.67\n28.18\n28.87\n46.02\n50.00": "pus, 15.27% for ShEMO, and 14.70% for EMO-DB. These"
        },
        {
          "SNSA-wo-HL\n48.12\n47.72\n24.21": "source domain knowledge, which is then adapt\nto the",
          "24.67\n28.18\n28.87\n46.02\n50.00": ""
        },
        {
          "SNSA-wo-HL\n48.12\n47.72\n24.21": "",
          "24.67\n28.18\n28.87\n46.02\n50.00": "performance gains demonstrate the effectiveness of hard neg-"
        },
        {
          "SNSA-wo-HL\n48.12\n47.72\n24.21": "target domain corpus directly by reusing the parameters",
          "24.67\n28.18\n28.87\n46.02\n50.00": ""
        },
        {
          "SNSA-wo-HL\n48.12\n47.72\n24.21": "",
          "24.67\n28.18\n28.87\n46.02\n50.00": "ative mining\ncompares with SNSA-wo-SL. However,\nthe"
        },
        {
          "SNSA-wo-HL\n48.12\n47.72\n24.21": "of the source domain and a four emotional classiﬁers of",
          "24.67\n28.18\n28.87\n46.02\n50.00": ""
        },
        {
          "SNSA-wo-HL\n48.12\n47.72\n24.21": "",
          "24.67\n28.18\n28.87\n46.02\n50.00": "performance of\nthe NNPM is worse for Urdu compared to"
        },
        {
          "SNSA-wo-HL\n48.12\n47.72\n24.21": "the source domain. We refer to this model as the Frozen",
          "24.67\n28.18\n28.87\n46.02\n50.00": ""
        },
        {
          "SNSA-wo-HL\n48.12\n47.72\n24.21": "",
          "24.67\n28.18\n28.87\n46.02\n50.00": "SNSA-wo-SL, which we speculate is due to a large differ-"
        },
        {
          "SNSA-wo-HL\n48.12\n47.72\n24.21": "SNSA (SNSA-F).",
          "24.67\n28.18\n28.87\n46.02\n50.00": ""
        },
        {
          "SNSA-wo-HL\n48.12\n47.72\n24.21": "",
          "24.67\n28.18\n28.87\n46.02\n50.00": "ence in distribution between IEMOCAP and Urdu so that"
        },
        {
          "SNSA-wo-HL\n48.12\n47.72\n24.21": "",
          "24.67\n28.18\n28.87\n46.02\n50.00": "the source domain loss in NNPM may actually hurt\nthe per-"
        },
        {
          "SNSA-wo-HL\n48.12\n47.72\n24.21": "• SNSA-wo-SL: this model consists of SNSA and exter-",
          "24.67\n28.18\n28.87\n46.02\n50.00": ""
        },
        {
          "SNSA-wo-HL\n48.12\n47.72\n24.21": "",
          "24.67\n28.18\n28.87\n46.02\n50.00": "formance of\ntarget domain training.\nThe performance for"
        },
        {
          "SNSA-wo-HL\n48.12\n47.72\n24.21": "nal memory with a pseudo-multilabeling process. The",
          "24.67\n28.18\n28.87\n46.02\n50.00": ""
        },
        {
          "SNSA-wo-HL\n48.12\n47.72\n24.21": "",
          "24.67\n28.18\n28.87\n46.02\n50.00": "Ekropus and ShEMO under SNSA-F baseline is below 25%"
        },
        {
          "SNSA-wo-HL\n48.12\n47.72\n24.21": "loss only contains the target domain loss Lt",
          "24.67\n28.18\n28.87\n46.02\n50.00": ""
        },
        {
          "SNSA-wo-HL\n48.12\n47.72\n24.21": "hard and no",
          "24.67\n28.18\n28.87\n46.02\n50.00": ""
        },
        {
          "SNSA-wo-HL\n48.12\n47.72\n24.21": "",
          "24.67\n28.18\n28.87\n46.02\n50.00": "due to a great domain mismatch between the English and"
        },
        {
          "SNSA-wo-HL\n48.12\n47.72\n24.21": "source domain loss. We refer to this model as the SNSA",
          "24.67\n28.18\n28.87\n46.02\n50.00": ""
        },
        {
          "SNSA-wo-HL\n48.12\n47.72\n24.21": "",
          "24.67\n28.18\n28.87\n46.02\n50.00": "Estonian or Persian.\nIn addition,\nthe backbone contains only"
        },
        {
          "SNSA-wo-HL\n48.12\n47.72\n24.21": "without Source domain Loss (SNSA-wo-SL).",
          "24.67\n28.18\n28.87\n46.02\n50.00": ""
        },
        {
          "SNSA-wo-HL\n48.12\n47.72\n24.21": "",
          "24.67\n28.18\n28.87\n46.02\n50.00": "knowledge of\nthe source domain if\nthe Siamese parameters"
        },
        {
          "SNSA-wo-HL\n48.12\n47.72\n24.21": "• SNSA-wo-HL: the setting of this baseline is similar to",
          "24.67\n28.18\n28.87\n46.02\n50.00": ""
        },
        {
          "SNSA-wo-HL\n48.12\n47.72\n24.21": "",
          "24.67\n28.18\n28.87\n46.02\n50.00": "are frozen without any ﬁne-tuning for the target domain. The"
        },
        {
          "SNSA-wo-HL\n48.12\n47.72\n24.21": "the SNSA-wo-SL, but\nthe loss consists of both source",
          "24.67\n28.18\n28.87\n46.02\n50.00": ""
        },
        {
          "SNSA-wo-HL\n48.12\n47.72\n24.21": "",
          "24.67\n28.18\n28.87\n46.02\n50.00": "worse performance from Ekropus under SNSA-wo-HL is due"
        },
        {
          "SNSA-wo-HL\n48.12\n47.72\n24.21": "domain loss and target domain loss without hard nega-",
          "24.67\n28.18\n28.87\n46.02\n50.00": ""
        },
        {
          "SNSA-wo-HL\n48.12\n47.72\n24.21": "",
          "24.67\n28.18\n28.87\n46.02\n50.00": "to the greater unbalance in data distribution with almost 46%"
        },
        {
          "SNSA-wo-HL\n48.12\n47.72\n24.21": "tive mining. We refer to this model as the SNSA with-",
          "24.67\n28.18\n28.87\n46.02\n50.00": ""
        },
        {
          "SNSA-wo-HL\n48.12\n47.72\n24.21": "",
          "24.67\n28.18\n28.87\n46.02\n50.00": "of data being anger and only 1.7% being joy.\nIn comparison,"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table 2: Comparison UA with other state-of-the-art meth-",
      "data": [
        {
          "Both weighted accuracy (WA) and unweighted accuracy (UA)": "are measured as the evaluation criteria for evaluating the per-",
          "Table 2.\nComparison UA with other state-of-the-art meth-": "ods on the EMO-DB corpus. The DANN method [8] and AL"
        },
        {
          "Both weighted accuracy (WA) and unweighted accuracy (UA)": "formances of\nthe proposed and the baseline models.\nOur",
          "Table 2.\nComparison UA with other state-of-the-art meth-": "method [9]\nresults are taken from the re-implementation in"
        },
        {
          "Both weighted accuracy (WA) and unweighted accuracy (UA)": "NNPM achieves 52.0 % UA that training with IEMOCAP on",
          "Table 2.\nComparison UA with other state-of-the-art meth-": "paper [7]. Therefore, we cite those results from [7] directly."
        },
        {
          "Both weighted accuracy (WA) and unweighted accuracy (UA)": "the source domain and it contains 0.46 million parameters.",
          "Table 2.\nComparison UA with other state-of-the-art meth-": ""
        },
        {
          "Both weighted accuracy (WA) and unweighted accuracy (UA)": "",
          "Table 2.\nComparison UA with other state-of-the-art meth-": "Method\nEMO-DB"
        },
        {
          "Both weighted accuracy (WA) and unweighted accuracy (UA)": "Table 1 summarizes the UAs and WAs of\nthe proposed",
          "Table 2.\nComparison UA with other state-of-the-art meth-": ""
        },
        {
          "Both weighted accuracy (WA) and unweighted accuracy (UA)": "",
          "Table 2.\nComparison UA with other state-of-the-art meth-": "FLUDA [7]\n34.9"
        },
        {
          "Both weighted accuracy (WA) and unweighted accuracy (UA)": "NNPM and\nthe\nthree\nbaseline models\nfor\nthe\nfour\nlow-",
          "Table 2.\nComparison UA with other state-of-the-art meth-": ""
        },
        {
          "Both weighted accuracy (WA) and unweighted accuracy (UA)": "",
          "Table 2.\nComparison UA with other state-of-the-art meth-": "DANN [7]\n28.5"
        },
        {
          "Both weighted accuracy (WA) and unweighted accuracy (UA)": "resource corpora. Out of\nthe four models examined in the",
          "Table 2.\nComparison UA with other state-of-the-art meth-": ""
        },
        {
          "Both weighted accuracy (WA) and unweighted accuracy (UA)": "",
          "Table 2.\nComparison UA with other state-of-the-art meth-": "AL [7]\n42.5"
        },
        {
          "Both weighted accuracy (WA) and unweighted accuracy (UA)": "experiment,\nthe SNSA-F has the lowest performance across",
          "Table 2.\nComparison UA with other state-of-the-art meth-": ""
        },
        {
          "Both weighted accuracy (WA) and unweighted accuracy (UA)": "",
          "Table 2.\nComparison UA with other state-of-the-art meth-": "50.6\nOurs (NNPM)"
        },
        {
          "Both weighted accuracy (WA) and unweighted accuracy (UA)": "all\ncorpora,\nsuggesting that\nthe\nsource domain knowledge",
          "Table 2.\nComparison UA with other state-of-the-art meth-": ""
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "on few-shot\nlearning [7].\nThe NNPM largely outperforms": "this\nfew-shot based method by a\nrelative UA increase of"
        },
        {
          "on few-shot\nlearning [7].\nThe NNPM largely outperforms": "45.5%.\nThe domain adversarial neural network (DANN)"
        },
        {
          "on few-shot\nlearning [7].\nThe NNPM largely outperforms": "and the aggregated multi-task learning (AL) are not publicly"
        },
        {
          "on few-shot\nlearning [7].\nThe NNPM largely outperforms": "available, we do not\nimplement\nthese\nsystems but\ninstead"
        },
        {
          "on few-shot\nlearning [7].\nThe NNPM largely outperforms": "cite\nthese\nresults of\ntheir performance on EMO-DB from"
        },
        {
          "on few-shot\nlearning [7].\nThe NNPM largely outperforms": "re-implementations in [7].\nIn comparisons,\nthe NNPM out-"
        },
        {
          "on few-shot\nlearning [7].\nThe NNPM largely outperforms": "performs adversarial-based method, that is DANN, by a rala-"
        },
        {
          "on few-shot\nlearning [7].\nThe NNPM largely outperforms": "tive UA improvement of 77.5% and outperformes multi-task"
        },
        {
          "on few-shot\nlearning [7].\nThe NNPM largely outperforms": "learning based method,\nthat\nis AL, with 19.1%.\nThe\nre-"
        },
        {
          "on few-shot\nlearning [7].\nThe NNPM largely outperforms": "sults show that the NNPM outperforms recent state-of-the-art"
        },
        {
          "on few-shot\nlearning [7].\nThe NNPM largely outperforms": "methods."
        },
        {
          "on few-shot\nlearning [7].\nThe NNPM largely outperforms": "Finally, ablation studies are conducted to examine the in-"
        },
        {
          "on few-shot\nlearning [7].\nThe NNPM largely outperforms": "ﬂuence of the modules of and hyperparameters in the NNPM"
        },
        {
          "on few-shot\nlearning [7].\nThe NNPM largely outperforms": "on the system performance."
        },
        {
          "on few-shot\nlearning [7].\nThe NNPM largely outperforms": "Table 3. Ablation study on freezing the ﬁrst N convolutional"
        },
        {
          "on few-shot\nlearning [7].\nThe NNPM largely outperforms": "layers of SNSA on the EMO-DB corpus."
        },
        {
          "on few-shot\nlearning [7].\nThe NNPM largely outperforms": "freeze N\nUA\nWA"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "6. REFERENCES": "",
          "of adversarial training and the blind-spot attack,” arXiv": "preprint arXiv:1901.04684, 2019."
        },
        {
          "6. REFERENCES": "[1] Moataz El Ayadi, Mohamed S Kamel, and Fakhri Kar-",
          "of adversarial training and the blind-spot attack,” arXiv": ""
        },
        {
          "6. REFERENCES": "",
          "of adversarial training and the blind-spot attack,” arXiv": "[12] Mateusz Ochal,\nJose Vazquez, Yvan Petillot, and Sen"
        },
        {
          "6. REFERENCES": "ray,\n“Survey on speech emotion recognition: Features,",
          "of adversarial training and the blind-spot attack,” arXiv": ""
        },
        {
          "6. REFERENCES": "",
          "of adversarial training and the blind-spot attack,” arXiv": "Wang,\n“A comparison of\nfew-shot\nlearning methods"
        },
        {
          "6. REFERENCES": "classiﬁcation schemes, and databases,” Pattern recogni-",
          "of adversarial training and the blind-spot attack,” arXiv": ""
        },
        {
          "6. REFERENCES": "",
          "of adversarial training and the blind-spot attack,” arXiv": "for underwater optical and sonar\nimage classiﬁcation,”"
        },
        {
          "6. REFERENCES": "tion, vol. 44, no. 3, pp. 572–587, 2011.",
          "of adversarial training and the blind-spot attack,” arXiv": ""
        },
        {
          "6. REFERENCES": "",
          "of adversarial training and the blind-spot attack,” arXiv": "arXiv preprint arXiv:2005.04621, 2020."
        },
        {
          "6. REFERENCES": "[2] Roddy Cowie, Ellen Douglas-Cowie, Nicolas Tsapat-",
          "of adversarial training and the blind-spot attack,” arXiv": ""
        },
        {
          "6. REFERENCES": "soulis, George Votsis, Stefanos Kollias, Winfried Fel-",
          "of adversarial training and the blind-spot attack,” arXiv": "[13] David Opitz and Richard Maclin,\n“Popular ensemble"
        },
        {
          "6. REFERENCES": "",
          "of adversarial training and the blind-spot attack,” arXiv": "Journal of artiﬁcial\nin-\nmethods: An empirical study,”"
        },
        {
          "6. REFERENCES": "lenz,\nand\nJohn G Taylor,\n“Emotion\nrecognition\nin",
          "of adversarial training and the blind-spot attack,” arXiv": ""
        },
        {
          "6. REFERENCES": "IEEE Signal processing\nhuman-computer interaction,”",
          "of adversarial training and the blind-spot attack,” arXiv": "telligence research, vol. 11, pp. 169–198, 1999."
        },
        {
          "6. REFERENCES": "magazine, vol. 18, no. 1, pp. 32–80, 2001.",
          "of adversarial training and the blind-spot attack,” arXiv": ""
        },
        {
          "6. REFERENCES": "",
          "of adversarial training and the blind-spot attack,” arXiv": "[14] Yuanchao Li, Tianyu Zhao, and Tatsuya Kawahara, “Im-"
        },
        {
          "6. REFERENCES": "[3] Felix Albu, Daniela Hagiescu,\nLiviu Vladutu,\nand",
          "of adversarial training and the blind-spot attack,” arXiv": "proved\nend-to-end\nspeech\nemotion\nrecognition\nusing"
        },
        {
          "6. REFERENCES": "Mihaela-Alexandra Puica, “Neural network approaches",
          "of adversarial training and the blind-spot attack,” arXiv": "self attention mechanism and multitask learning.,”\nin"
        },
        {
          "6. REFERENCES": "for children’s emotion recognition in intelligent learning",
          "of adversarial training and the blind-spot attack,” arXiv": "Interspeech, 2019, pp. 2803–2807."
        },
        {
          "6. REFERENCES": "applications,” in EDULEARN15 7th Annu Int Conf Educ",
          "of adversarial training and the blind-spot attack,” arXiv": ""
        },
        {
          "6. REFERENCES": "",
          "of adversarial training and the blind-spot attack,” arXiv": "[15] Sepp Hochreiter and J¨urgen Schmidhuber, “Long short-"
        },
        {
          "6. REFERENCES": "New Learn Technol Barcelona, Spain, 6th-8th, 2015.",
          "of adversarial training and the blind-spot attack,” arXiv": ""
        },
        {
          "6. REFERENCES": "",
          "of adversarial training and the blind-spot attack,” arXiv": "term memory,” Neural computation, vol. 9, no. 8, pp."
        },
        {
          "6. REFERENCES": "[4] Bjorn Schuller, Bogdan Vlasenko, Florian Eyben, Mar-",
          "of adversarial training and the blind-spot attack,” arXiv": "1735–1780, 1997."
        },
        {
          "6. REFERENCES": "tin W¨ollmer, Andre Stuhlsatz, Andreas Wendemuth, and",
          "of adversarial training and the blind-spot attack,” arXiv": ""
        },
        {
          "6. REFERENCES": "",
          "of adversarial training and the blind-spot attack,” arXiv": "[16] Vinod Nair and Geoffrey E Hinton,\n“Rectiﬁed linear"
        },
        {
          "6. REFERENCES": "Gerhard Rigoll, “Cross-corpus acoustic emotion recog-",
          "of adversarial training and the blind-spot attack,” arXiv": ""
        },
        {
          "6. REFERENCES": "",
          "of adversarial training and the blind-spot attack,” arXiv": "units improve restricted boltzmann machines,”\nin Icml,"
        },
        {
          "6. REFERENCES": "nition: Variances and strategies,” IEEE Transactions on",
          "of adversarial training and the blind-spot attack,” arXiv": ""
        },
        {
          "6. REFERENCES": "",
          "of adversarial training and the blind-spot attack,” arXiv": "2010."
        },
        {
          "6. REFERENCES": "Affective Computing, vol. 1, no. 2, pp. 119–131, 2010.",
          "of adversarial training and the blind-spot attack,” arXiv": ""
        },
        {
          "6. REFERENCES": "",
          "of adversarial training and the blind-spot attack,” arXiv": "[17] Stephen Grossberg,\n“Competitive learning:\nFrom in-"
        },
        {
          "6. REFERENCES": "[5] Hao Zhou and Ke Chen, “Transferable positive/negative",
          "of adversarial training and the blind-spot attack,” arXiv": ""
        },
        {
          "6. REFERENCES": "",
          "of adversarial training and the blind-spot attack,” arXiv": "Cognitive\nteractive activation to adaptive resonance,”"
        },
        {
          "6. REFERENCES": "speech emotion recognition via class-wise adversarial",
          "of adversarial training and the blind-spot attack,” arXiv": ""
        },
        {
          "6. REFERENCES": "",
          "of adversarial training and the blind-spot attack,” arXiv": "science, vol. 11, no. 1, pp. 23–63, 1987."
        },
        {
          "6. REFERENCES": "domain adaptation,”\nin ICASSP 2019-2019 IEEE In-",
          "of adversarial training and the blind-spot attack,” arXiv": ""
        },
        {
          "6. REFERENCES": "ternational Conference on Acoustics, Speech and Signal",
          "of adversarial training and the blind-spot attack,” arXiv": ""
        },
        {
          "6. REFERENCES": "",
          "of adversarial training and the blind-spot attack,” arXiv": "[18] Abhinav Shrivastava, Abhinav Gupta,\nand Ross Gir-"
        },
        {
          "6. REFERENCES": "Processing (ICASSP). IEEE, 2019, pp. 3732–3736.",
          "of adversarial training and the blind-spot attack,” arXiv": ""
        },
        {
          "6. REFERENCES": "",
          "of adversarial training and the blind-spot attack,” arXiv": "shick, “Training region-based object detectors with on-"
        },
        {
          "6. REFERENCES": "",
          "of adversarial training and the blind-spot attack,” arXiv": "line hard example mining,” in Proceedings of the IEEE"
        },
        {
          "6. REFERENCES": "[6] Siddique Latif,\nJunaid Qadir,\nand Muhammad Bilal,",
          "of adversarial training and the blind-spot attack,” arXiv": ""
        },
        {
          "6. REFERENCES": "",
          "of adversarial training and the blind-spot attack,” arXiv": "conference on computer vision and pattern recognition,"
        },
        {
          "6. REFERENCES": "“Unsupervised adversarial domain adaptation for cross-",
          "of adversarial training and the blind-spot attack,” arXiv": ""
        },
        {
          "6. REFERENCES": "",
          "of adversarial training and the blind-spot attack,” arXiv": "2016, pp. 761–769."
        },
        {
          "6. REFERENCES": "lingual speech emotion recognition,” in 2019 8th Inter-",
          "of adversarial training and the blind-spot attack,” arXiv": ""
        },
        {
          "6. REFERENCES": "national Conference on Affective Computing and Intel-",
          "of adversarial training and the blind-spot attack,” arXiv": ""
        },
        {
          "6. REFERENCES": "",
          "of adversarial training and the blind-spot attack,” arXiv": "[19] Carlos Busso, Murtaza Bulut,\nChi-Chun Lee, Abe"
        },
        {
          "6. REFERENCES": "ligent Interaction (ACII). IEEE, 2019, pp. 732–737.",
          "of adversarial training and the blind-spot attack,” arXiv": ""
        },
        {
          "6. REFERENCES": "",
          "of adversarial training and the blind-spot attack,” arXiv": "Kazemzadeh, Emily Mower, Samuel Kim, Jeannette N"
        },
        {
          "6. REFERENCES": "",
          "of adversarial training and the blind-spot attack,” arXiv": "Chang,\nSungbok Lee,\nand\nShrikanth\nS Narayanan,"
        },
        {
          "6. REFERENCES": "[7] Youngdo Ahn,\nSung\nJoo Lee,\nand\nJong Won Shin,",
          "of adversarial training and the blind-spot attack,” arXiv": ""
        },
        {
          "6. REFERENCES": "",
          "of adversarial training and the blind-spot attack,” arXiv": "“Iemocap:\nInteractive emotional dyadic motion capture"
        },
        {
          "6. REFERENCES": "“Cross-corpus\nspeech\nemotion\nrecognition\nbased\non",
          "of adversarial training and the blind-spot attack,” arXiv": ""
        },
        {
          "6. REFERENCES": "",
          "of adversarial training and the blind-spot attack,” arXiv": "database,” Language resources and evaluation, vol. 42,"
        },
        {
          "6. REFERENCES": "IEEE Sig-\nfew-shot\nlearning and domain adaptation,”",
          "of adversarial training and the blind-spot attack,” arXiv": ""
        },
        {
          "6. REFERENCES": "",
          "of adversarial training and the blind-spot attack,” arXiv": "no. 4, pp. 335–359, 2008."
        },
        {
          "6. REFERENCES": "nal Processing Letters, 2021.",
          "of adversarial training and the blind-spot attack,” arXiv": ""
        },
        {
          "6. REFERENCES": "",
          "of adversarial training and the blind-spot attack,” arXiv": "[20] Siddique Latif, Adnan Qayyum, Muhammad Usman,"
        },
        {
          "6. REFERENCES": "[8] Mohammed Abdelwahab\nand Carlos Busso,\n“Do-",
          "of adversarial training and the blind-spot attack,” arXiv": ""
        },
        {
          "6. REFERENCES": "",
          "of adversarial training and the blind-spot attack,” arXiv": "and Junaid Qadir, “Cross lingual speech emotion recog-"
        },
        {
          "6. REFERENCES": "main\nadversarial\nfor\nacoustic\nemotion\nrecognition,”",
          "of adversarial training and the blind-spot attack,” arXiv": ""
        },
        {
          "6. REFERENCES": "",
          "of adversarial training and the blind-spot attack,” arXiv": "nition: Urdu vs. western languages,”\nin 2018 Interna-"
        },
        {
          "6. REFERENCES": "IEEE/ACM Transactions on Audio, Speech, and Lan-",
          "of adversarial training and the blind-spot attack,” arXiv": ""
        },
        {
          "6. REFERENCES": "",
          "of adversarial training and the blind-spot attack,” arXiv": "tional Conference on Frontiers of Information Technol-"
        },
        {
          "6. REFERENCES": "guage Processing, vol. 26, no. 12, pp. 2423–2435, 2018.",
          "of adversarial training and the blind-spot attack,” arXiv": ""
        },
        {
          "6. REFERENCES": "",
          "of adversarial training and the blind-spot attack,” arXiv": "ogy (FIT). IEEE, 2018, pp. 88–93."
        },
        {
          "6. REFERENCES": "[9]\nJaebok Kim, Gwenn Englebienne, Khiet P Truong, and",
          "of adversarial training and the blind-spot attack,” arXiv": ""
        },
        {
          "6. REFERENCES": "",
          "of adversarial training and the blind-spot attack,” arXiv": "[21] Fabian Pedregosa, Ga¨el Varoquaux, Alexandre Gram-"
        },
        {
          "6. REFERENCES": "Vanessa Evers,\n“Towards speech emotion recognition”",
          "of adversarial training and the blind-spot attack,” arXiv": ""
        },
        {
          "6. REFERENCES": "",
          "of adversarial training and the blind-spot attack,” arXiv": "fort, Vincent Michel, Bertrand Thirion, Olivier Grisel,"
        },
        {
          "6. REFERENCES": "in the wild” using aggregated corpora and deep multi-",
          "of adversarial training and the blind-spot attack,” arXiv": ""
        },
        {
          "6. REFERENCES": "",
          "of adversarial training and the blind-spot attack,” arXiv": "Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vin-"
        },
        {
          "6. REFERENCES": "task learning,” arXiv preprint arXiv:1708.03920, 2017.",
          "of adversarial training and the blind-spot attack,” arXiv": ""
        },
        {
          "6. REFERENCES": "",
          "of adversarial training and the blind-spot attack,” arXiv": "cent Dubourg, et al., “Scikit-learn: Machine learning in"
        },
        {
          "6. REFERENCES": "[10]\nIan Goodfellow,\n“Nips 2016 tutorial: Generative ad-",
          "of adversarial training and the blind-spot attack,” arXiv": "python,” the Journal of machine Learning research, vol."
        },
        {
          "6. REFERENCES": "versarial networks,”\narXiv preprint arXiv:1701.00160,",
          "of adversarial training and the blind-spot attack,” arXiv": "12, pp. 2825–2830, 2011."
        },
        {
          "6. REFERENCES": "2016.",
          "of adversarial training and the blind-spot attack,” arXiv": ""
        },
        {
          "6. REFERENCES": "",
          "of adversarial training and the blind-spot attack,” arXiv": "ˇ"
        },
        {
          "6. REFERENCES": "",
          "of adversarial training and the blind-spot attack,” arXiv": "[22] Frantiˇsek\nCermak,\nR¯uta\nMarcinkeviˇcien˙e,\nErika"
        },
        {
          "6. REFERENCES": "[11] Huan Zhang, Hongge Chen, Zhao Song, Duane Boning,",
          "of adversarial training and the blind-spot attack,” arXiv": "Rimkut˙e,\nand Jolanta Zabarskait˙e,\n“Hlt’2007:\nThe"
        },
        {
          "6. REFERENCES": "Inderjit S Dhillon, and Cho-Jui Hsieh, “The limitations",
          "of adversarial training and the blind-spot attack,” arXiv": "estonian emotional speech corpus: Release,” ."
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "[23] Omid Mohamad Nezami, Paria Jamshid Lou, and Man-": "soureh Karami,\n“Shemo:\na\nlarge-scale\nvalidated"
        },
        {
          "[23] Omid Mohamad Nezami, Paria Jamshid Lou, and Man-": "Lan-\ndatabase for persian speech emotion detection,”"
        },
        {
          "[23] Omid Mohamad Nezami, Paria Jamshid Lou, and Man-": "guage Resources and Evaluation, vol. 53, no. 1, pp. 1–"
        },
        {
          "[23] Omid Mohamad Nezami, Paria Jamshid Lou, and Man-": "16, 2019."
        },
        {
          "[23] Omid Mohamad Nezami, Paria Jamshid Lou, and Man-": "[24] Felix Burkhardt, Astrid Paeschke, Miriam Rolfes, Wal-"
        },
        {
          "[23] Omid Mohamad Nezami, Paria Jamshid Lou, and Man-": "ter F Sendlmeier, and Benjamin Weiss,\n“A database of"
        },
        {
          "[23] Omid Mohamad Nezami, Paria Jamshid Lou, and Man-": "german emotional speech,”\nin Ninth European Confer-"
        },
        {
          "[23] Omid Mohamad Nezami, Paria Jamshid Lou, and Man-": "ence on Speech Communication and Technology, 2005."
        },
        {
          "[23] Omid Mohamad Nezami, Paria Jamshid Lou, and Man-": "[25] Adam Paszke,\nSam Gross,\nFrancisco Massa, Adam"
        },
        {
          "[23] Omid Mohamad Nezami, Paria Jamshid Lou, and Man-": "Lerer,\nJames\nBradbury,\nGregory\nChanan,\nTrevor"
        },
        {
          "[23] Omid Mohamad Nezami, Paria Jamshid Lou, and Man-": "Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga,"
        },
        {
          "[23] Omid Mohamad Nezami, Paria Jamshid Lou, and Man-": "et\nal.,\n“Pytorch:\nAn\nimperative\nstyle,\nhigh-"
        },
        {
          "[23] Omid Mohamad Nezami, Paria Jamshid Lou, and Man-": "arXiv\npreprint\nperformance\ndeep\nlearning\nlibrary,”"
        },
        {
          "[23] Omid Mohamad Nezami, Paria Jamshid Lou, and Man-": "arXiv:1912.01703, 2019."
        },
        {
          "[23] Omid Mohamad Nezami, Paria Jamshid Lou, and Man-": "[26] Diederik\nP Kingma\nand\nJimmy Ba,\n“Adam:\nA"
        },
        {
          "[23] Omid Mohamad Nezami, Paria Jamshid Lou, and Man-": "arXiv preprint\nmethod for\nstochastic optimization,”"
        },
        {
          "[23] Omid Mohamad Nezami, Paria Jamshid Lou, and Man-": "arXiv:1412.6980, 2014."
        }
      ],
      "page": 8
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Survey on speech emotion recognition: Features, classification schemes, and databases",
      "authors": [
        "Moataz Ayadi",
        "Mohamed Kamel",
        "Fakhri Karray"
      ],
      "year": "2011",
      "venue": "Survey on speech emotion recognition: Features, classification schemes, and databases"
    },
    {
      "citation_id": "3",
      "title": "Emotion recognition in human-computer interaction",
      "authors": [
        "Roddy Cowie",
        "Ellen Douglas-Cowie",
        "Nicolas Tsapatsoulis",
        "George Votsis",
        "Stefanos Kollias",
        "Winfried Fellenz",
        "John Taylor"
      ],
      "year": "2001",
      "venue": "IEEE Signal processing magazine"
    },
    {
      "citation_id": "4",
      "title": "Neural network approaches for children's emotion recognition in intelligent learning applications",
      "authors": [
        "Felix Albu",
        "Daniela Hagiescu",
        "Liviu Vladutu",
        "Mihaela-Alexandra Puica"
      ],
      "year": "2015",
      "venue": "EDULEARN15 7th Annu Int Conf Educ New Learn Technol"
    },
    {
      "citation_id": "5",
      "title": "Cross-corpus acoustic emotion recognition: Variances and strategies",
      "authors": [
        "Bjorn Schuller",
        "Bogdan Vlasenko",
        "Florian Eyben",
        "Martin Wöllmer",
        "Andre Stuhlsatz",
        "Andreas Wendemuth",
        "Gerhard Rigoll"
      ],
      "year": "2010",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "6",
      "title": "Transferable positive/negative speech emotion recognition via class-wise adversarial domain adaptation",
      "authors": [
        "Hao Zhou",
        "Ke Chen"
      ],
      "year": "2019",
      "venue": "ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "7",
      "title": "Unsupervised adversarial domain adaptation for crosslingual speech emotion recognition",
      "authors": [
        "Siddique Latif",
        "Junaid Qadir",
        "Muhammad Bilal"
      ],
      "year": "2019",
      "venue": "2019 8th International Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "8",
      "title": "Cross-corpus speech emotion recognition based on few-shot learning and domain adaptation",
      "authors": [
        "Youngdo Ahn",
        "Sung Lee",
        "Jong Shin"
      ],
      "year": "2021",
      "venue": "IEEE Signal Processing Letters"
    },
    {
      "citation_id": "9",
      "title": "Domain adversarial for acoustic emotion recognition",
      "authors": [
        "Mohammed Abdelwahab",
        "Carlos Busso"
      ],
      "year": "2018",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "10",
      "title": "Towards speech emotion recognition\" in the wild\" using aggregated corpora and deep multitask learning",
      "authors": [
        "Jaebok Kim",
        "Gwenn Englebienne",
        "P Khiet",
        "Vanessa Truong",
        "Evers"
      ],
      "year": "2017",
      "venue": "Towards speech emotion recognition\" in the wild\" using aggregated corpora and deep multitask learning",
      "arxiv": "arXiv:1708.03920"
    },
    {
      "citation_id": "11",
      "title": "Nips 2016 tutorial: Generative adversarial networks",
      "authors": [
        "Ian Goodfellow"
      ],
      "year": "2016",
      "venue": "Nips 2016 tutorial: Generative adversarial networks",
      "arxiv": "arXiv:1701.00160"
    },
    {
      "citation_id": "12",
      "title": "The limitations of adversarial training and the blind-spot attack",
      "authors": [
        "Huan Zhang",
        "Hongge Chen",
        "Zhao Song",
        "Duane Boning",
        "Cho-Jui Inderjit S Dhillon",
        "Hsieh"
      ],
      "year": "2019",
      "venue": "The limitations of adversarial training and the blind-spot attack",
      "arxiv": "arXiv:1901.04684"
    },
    {
      "citation_id": "13",
      "title": "A comparison of few-shot learning methods for underwater optical and sonar image classification",
      "authors": [
        "Mateusz Ochal",
        "Jose Vazquez",
        "Yvan Petillot",
        "Sen Wang"
      ],
      "year": "2020",
      "venue": "A comparison of few-shot learning methods for underwater optical and sonar image classification",
      "arxiv": "arXiv:2005.04621"
    },
    {
      "citation_id": "14",
      "title": "Popular ensemble methods: An empirical study",
      "authors": [
        "David Opitz",
        "Richard Maclin"
      ],
      "year": "1999",
      "venue": "Journal of artificial intelligence research"
    },
    {
      "citation_id": "15",
      "title": "Improved end-to-end speech emotion recognition using self attention mechanism and multitask learning",
      "authors": [
        "Yuanchao Li",
        "Tianyu Zhao",
        "Tatsuya Kawahara"
      ],
      "year": "2019",
      "venue": "Improved end-to-end speech emotion recognition using self attention mechanism and multitask learning"
    },
    {
      "citation_id": "16",
      "title": "Long shortterm memory",
      "authors": [
        "Sepp Hochreiter",
        "Jürgen Schmidhuber"
      ],
      "year": "1997",
      "venue": "Neural computation"
    },
    {
      "citation_id": "17",
      "title": "Rectified linear units improve restricted boltzmann machines",
      "authors": [
        "Vinod Nair",
        "Geoffrey Hinton"
      ],
      "year": "2010",
      "venue": "Icml"
    },
    {
      "citation_id": "18",
      "title": "Competitive learning: From interactive activation to adaptive resonance",
      "authors": [
        "Stephen Grossberg"
      ],
      "year": "1987",
      "venue": "Cognitive science"
    },
    {
      "citation_id": "19",
      "title": "Training region-based object detectors with online hard example mining",
      "authors": [
        "Abhinav Shrivastava",
        "Abhinav Gupta",
        "Ross Girshick"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "20",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "Carlos Busso",
        "Murtaza Bulut",
        "Chi-Chun Lee",
        "Abe Kazemzadeh",
        "Emily Mower",
        "Samuel Kim",
        "Jeannette Chang",
        "Sungbok Lee",
        "Shrikanth S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "21",
      "title": "Cross lingual speech emotion recognition: Urdu vs. western languages",
      "authors": [
        "Siddique Latif",
        "Adnan Qayyum",
        "Muhammad Usman",
        "Junaid Qadir"
      ],
      "year": "2018",
      "venue": "2018 International Conference on Frontiers of Information Technology (FIT)"
    },
    {
      "citation_id": "22",
      "title": "Scikit-learn: Machine learning in python",
      "authors": [
        "Fabian Pedregosa",
        "Gaël Varoquaux",
        "Alexandre Gramfort",
        "Vincent Michel",
        "Bertrand Thirion",
        "Olivier Grisel",
        "Mathieu Blondel",
        "Peter Prettenhofer",
        "Ron Weiss",
        "Vincent Dubourg"
      ],
      "year": "2011",
      "venue": "the Journal of machine Learning research"
    },
    {
      "citation_id": "23",
      "title": "Hlt'2007: The estonian emotional speech corpus: Release",
      "authors": [
        "František Čermak",
        "Rūta Marcinkevičienė",
        "Erika Rimkutė",
        "Jolanta Zabarskaitė"
      ],
      "venue": "Hlt'2007: The estonian emotional speech corpus: Release"
    },
    {
      "citation_id": "24",
      "title": "Shemo: a large-scale validated database for persian speech emotion detection",
      "authors": [
        "Mohamad Omid",
        "Paria Jamshid Nezami",
        "Mansoureh Lou",
        "Karami"
      ],
      "year": "2019",
      "venue": "Language Resources and Evaluation"
    },
    {
      "citation_id": "25",
      "title": "A database of german emotional speech",
      "authors": [
        "Felix Burkhardt",
        "Astrid Paeschke",
        "Miriam Rolfes",
        "Walter Sendlmeier",
        "Benjamin Weiss"
      ],
      "year": "2005",
      "venue": "Ninth European Conference on Speech Communication and Technology"
    },
    {
      "citation_id": "26",
      "title": "Pytorch: An imperative style, highperformance deep learning library",
      "authors": [
        "Adam Paszke",
        "Sam Gross",
        "Francisco Massa",
        "Adam Lerer",
        "James Bradbury",
        "Gregory Chanan",
        "Trevor Killeen",
        "Zeming Lin",
        "Natalia Gimelshein",
        "Luca Antiga"
      ],
      "year": "2019",
      "venue": "Pytorch: An imperative style, highperformance deep learning library",
      "arxiv": "arXiv:1912.01703"
    },
    {
      "citation_id": "27",
      "title": "Adam: A method for stochastic optimization",
      "authors": [
        "P Diederik",
        "Jimmy Kingma",
        "Ba"
      ],
      "year": "2014",
      "venue": "Adam: A method for stochastic optimization",
      "arxiv": "arXiv:1412.6980"
    }
  ]
}