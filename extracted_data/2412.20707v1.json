{
  "paper_id": "2412.20707v1",
  "title": "Metadata-Enhanced Speech Emotion Recognition: Augmented Residual Integration And Co-Attention In Two-Stage Fine-Tuning",
  "published": "2024-12-30T04:49:43Z",
  "authors": [
    "Zixiang Wan",
    "Ziyue Qiu",
    "Yiyang Liu",
    "Wei-Qiang Zhang"
  ],
  "keywords": [
    "speech emotion recognition",
    "metadata",
    "multi-task learning",
    "self-supervised learning model",
    "fine-tuning"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Speech Emotion Recognition (SER) involves analyzing vocal expressions to determine the emotional state of speakers, where the comprehensive and thorough utilization of audio information is paramount. Therefore, we propose a novel approach on self-supervised learning (SSL) models that employs all available auxiliary information-specifically metadata-to enhance performance. Through a two-stage fine-tuning method in multi-task learning, we introduce the Augmented Residual Integration (ARI) module, which enhances transformer layers in encoder of SSL models. The module efficiently preserves acoustic features across all different levels, thereby significantly improving the performance of metadata-related auxiliary tasks that require various levels of features. Moreover, the Co-attention module is incorporated due to its complementary nature with ARI, enabling the model to effectively utilize multidimensional information and contextual relationships from metadata-related auxiliary tasks. Under pre-trained base models and speaker-independent setup, our approach consistently surpasses state-ofthe-art (SOTA) models on multiple SSL encoders for the IEMOCAP dataset.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "Speech Emotion Recognition (SER) is an advanced technology that uses artificial intelligence techniques to recognize emotional states in speech. It has applications in areas  [1] -  [5]  such as virtual assistants  [6] , customer service  [7] , and healthcare  [8] -  [10] . Studies have highlighted its importance in enhancing user experience and personalized services.\n\nHowever, since audio signal is a complex, non-linear, highdimensional data type  [11] , and emotional expression is conveyed through the combination and variation of multiple features instead of a single acoustic feature, it becomes challenging for AI models to identify clear patterns, which makes SER tasks particularly difficult.\n\nResearchers have identified that incorporating auxiliary information, or metadata, into SER can significantly enhance its performance  [4] , which reveals that effective SER requires a comprehensive understanding of the diverse factors that influence emotional expression in speech, including gender, speaker information, the style of speech, semantic content of speech etc  [4] ,  [12] -  [15] .\n\nThus, given that SER can benefit from this metadata, multitask learning becomes natural choice  [16] . However, there are still challenges remaining in traditional MTL approaches  [17] ,  [18] . One challenge is the quality of auxiliary tasks if the auxiliary tasks are not accurate, they can lead to negative transfer, degrading the performance of the primary task  [19] ,  [20] . Another challenge is that incorporating too many auxiliary tasks may overwhelm the model, leading to poor performance due to an inability to efficiently integrate high-dimensional information  [21] ,  [22] . Furthermore, the common practice of merging auxiliary tasks in a simplistic manner like weighted sum  [23]  can lead to suboptimal use of the data, reducing overall model efficiency  [24] .\n\nTo address the challenges in multi-task learning for Speech Emotion Recognition (SER), we introduce improvements within a two-stage fine-tuning approach on transformer-based self-supervised learning (SSL) models, which extract high-level speech features from raw audio and are fine-tuned with a small amount of labeled data. The main contributions of our work are summarized as follows:\n\n• An Augmented Residual Integration (ARI) module is introduced to enhance the transformer-based encoder in SSL models, which effectively preserving multi-level acoustic feature suited for different metadata-related auxiliary tasks and outperforming the traditional weighted sum approach for every auxiliary tasks. • Co-attention module is novelly incorporated into the framework of two-stage fine-tuning. The module is superior at utilizing the multidimensional information from the ARI-enhanced encoder and the contextual relationships between all auxiliary tasks and their associated metadata.\n\nThe complementary nature of the two modules lies in their distinct strengths: the ARI module excels at contributing high-quality multidimensional information to auxiliary tasks by preserving features at various levels, while the Co-attention module efficiently utilizes this multidimensional information. This synergy enables the proposed approach to accommodate each type of available metadata (e.g., gender, speech style) as an auxiliary task or modality  [25] . The enhanced ability to process multimodal information extracted from metadata, with performance improvements positively correlating with the number of auxiliary tasks, is observed and validated on different transformer-based SSL models. Ultimately, under pre-trained base SSL models and a speaker-independent setup, our approach consistently outperforms state-of-the-art (SOTA) models on the IEMOCAP dataset.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Proposed Method",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "A. Model Architecture",
      "text": "We propose an end-to-end model that takes speech as raw waveform input and outputs predicted emotions. Figure  1  shows the model architecture. 1) Time Domain Spectral Augmentation: Initially, raw waveforms undergo Time Domain Spectral Augmentation (TDSA) for data augment. This module applies speed perturbation by resampling the waveform at varying rates (80, 100, and 120), thereby improving neural model robustness during training. Notably, TDSA is utilized exclusively in the training phase.\n\n2) Feature Encoder: To obtain high-level contextual representations of speech, pretrained SSL-based models were utilized as the original waveform encoder. Three representative transformer-based SSL models-Wav2Vec-2.0-base, HuBert-base  [26] , and WavLMbase  [27] -were selected to validate the generality of the proposed method. These models, all based on self-supervised learning, aim to extract rich representations from raw audio without requiring large amounts of labeled data. Furthermore, all selected models were pretrained on 960 hours of LibriSpeech and share a similar architecture, consisting of: 1) convolutional layers that extract deep features from raw audio, and 2) twelve Transformer layers that capture contextual information from the output of the CNN layers.\n\nSince splitting MTL into stages effectively avoids gradient conflicts and allows the model to better adapt to the current stage of the task  [4] , a two-stage fine-tuning strategy was adopted for training. In the first stage, the convolutional layers were frozen, and all Transformer layers were fine-tuned to embed auxiliary task information. In the second stage, to maintain stability for tasks that require lower-level features, the last 8 Transformer layers were further fine-tuned while keeping the convolutional layers and the first 4 Transformer layers frozen. By freezing the first a few layers, the emotion recognition task was able to benefit from the information learned in the first stage.\n\n3) Multi-task Learning: As described in the introduction, all available metadata in IEMOCAP were incorporated to facilitate feature extraction. In this work, we incorporated gender, speaker, speech style (impromptu vs. scripted) and Automatic Speech Recognition (ASR) tasks as auxiliary tasks in the MTL model.\n\nFor all auxiliary tasks except ASR, as well as SER, cross-entropy loss function was used.\n\nFor ASR, the Connectionist Temporal Classification (CTC) loss function  [28]  was used, as it is particularly suited for the sequenceto-sequence learning process inherent in speech recognition.\n\nIn the first stage of training, the weighted sum of the loss functions for all auxiliary tasks was minimized:\n\nwhere α, β, γ are hyper-parameters.\n\nIn the second stage of training, the following loss function was minimized:\n\nwhere Lemotion is the loss function of SER.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "4) Augmented Residual Integration Module (Ari):",
      "text": "In traditional multi-task learning models, the simple weighted sum method often has shortcomings such as information dilution, where mixing information from all layers can lead to the loss of particular features, and increased complexity, as directly summing outputs from different layers may introduce conflicts, making optimization more challenging.\n\nIn order to enhance the feature integration and utilization capabilities of the model, the Augmented Residual Integration (ARI) module was introduced for the Transformer layers in the encoder of transformer-based SSL models. This design is inspired by previous research  [27]  on WavLM, which identified that different Transformer layers exert significant effects on different tasks. For instance, layer 1 and layer 4 are especially effective for extracting speaker information, layer 8 through layer 10 are more proficient for emotion recognition, and layer 11 contains the majority of the textual information. The detailed formula is presented as follows:\n\nwhere Fi denotes the output of the i-th Transformer layer; W ∈ R 1×11 denotes weighted matrix.\n\nIn contrast to merely weighting and summing all layers, the ARI module selectively integrates the output of the initial 11 layers and connects it to the output of the last (12-th) layer. This design enables the capture of a more expansive range of task-relevant feature information, while the output of the last layer provides high-level semantic information that is essential for complex tasks.\n\nIt is postulated that this module design can be better adapted to different feature-demanding tasks such as gender recognition (GR), speaker recognition (SpkR), ASR, etc. The results of our ablation experiments in IV-D demonstrate that the ARI module outperforms traditional weighted sum method in terms of performance, providing a more robust and task-specific feature representation that is effective across all tasks. Moreover, the results in IV-A demonstrate that the proposed method can be generalized to all transformer-based SSL models.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "5) Co-Attention Module:",
      "text": "The Co-attention module integrates the output from the ARI module with the hidden layers of the auxiliary tasks. Recognizing the auxiliary tasks' synergistic impact on Speech Emotion Recognition, we leverage their interplay to guide feature weighting, thereby generating weighted emotion features. Subsequently, these weighted emotion features are combined with the hidden layer outputs from the auxiliary tasks. This composite input is then fed into the SER classifier to generate the predicted labels ŷ.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Iii. Experimental Setup",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "A. Data Preparation",
      "text": "In this study, the Interactive Emotional Dyadic Motion Capture (IEMOCAP) dataset was utilized to validate the effectiveness of the proposed method  [29] . The dataset contains 12 hours of recordings from 10 speakers, with each conversation featuring a male and a female speaker, including both scripted and improvised segments. It covers five emotion labels: neutral, happy, angry, sad, and excited.\n\nRegarding data preparation, to maintain consistency with current state-of-the-art (SOTA) research  [4] ,  [30] -  [34] , the excited class was merged with the happy class in the IEMOCAP dataset, resulting in four primary emotion categories: neutral (1,708 samples), happy (1,636 samples), angry (1,103 samples), and sad (1,084 samples). The 5-fold cross-validation with speaker-independent constraint (with one pair of speakers per fold) was applied. The results were compared using weighted accuracy (WA) and unweighted accuracy (UA).\n\nIt is crucial to highlight that all metadata within the dataset were utilized, as they are integral to this research. The metadata in IEMOCAP includes speaker gender, speaker ID, speech style, and audio transcription.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "B. Implementations",
      "text": "The proposed model was implemented using PyTorch and Speech-Brain  [35] ,  [36] . Building on previous research  [4] ,  [37] , three representative transformer-based SSL models-Wav2Vec-2.0-base, HuBert-base, and WavLM-base-were selected. All three models are pre-trained on 960 hours of LibriSpeech  [38] . These models shared a similar architecture including a CNN feature extractor for mapping raw audio to latent representations, as well as twelve Transformers layers for learning contextual information. All sentences in each batch were padded to ensure compatibility with the input length requirements of the SSL models. During training, the CNN feature extractor was frozen, and the Transformer layers were fine-tuned for 100 epochs. To reduce the dimensionality of features, mean pooling and two FC layers with dropout were employed for SER. The downstream models used a learning rate of 10e-4, while the SSL models used a learning rate of 10e-5. The batch size was set to 4. For the hyperparameter settings of the metadata auxiliary tasks, IEMOCAP includes gender, speaker, speech style (impromptu vs. scripted), and Automatic Speech Recognition (ASR) tasks. Each task was assigned a hyperparameter independently. The hyperparameter was subject to change with different encoders.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Iv. Experimental Results And Analysis",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "A. Evaluation Results",
      "text": "Table I presents a comparison between the proposed method, baseline models, and methods with leading performance. First, compared to the baseline models which leverage the base version of each SSL encoder followed by a fully connected layer as classifier, the proposed method, using the corresponding encoder of the baseline models, achieves a significant improvement, with an average increase of 4.09% in UA and 3.90% in WA. Second, under speaker-independent conditions and using different SSL encoders, the proposed method consistently outperforms the leading studies, achieving state-of-theart results with improvements ranging from 0.54% to 1.64% in UA and from 0.9% to 1.39% in WA. These results demonstrate the effectiveness and superiority of the proposed method in enhancing the performance of pretrained transformer-based SSL models.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "B. Ablation Study On Highlighted Modules",
      "text": "Table  II  presents the results of the ablation study on the ARI and Co-attention modules. Performance improvements are observed across the three transformer-based encoders with both metrics when the ARI and Co-attention modules are included. Specifically, UA increases by 0.44% and WA by 0.61% with the addition of the ARI module. When the Co-attention module is added, there is an increase of 0.64% in UA and 0.82% in WA. This indicates that the ARI module's capability to preserve multi-level features and the Co-attention module's ability to capture contextual information from tasks both contribute to performance enhancement.\n\nHowever, when both the ARI and Co-attention modules are included together, the increase in UA reaches 1.31%, and WA improves by 1.29%, both surpassing the performance gains of adding ARI or Co-attention individually. This demonstrates that the ARI module effectively preserves multidimensional, high-quality auxiliary information, while the Co-attention module excels at utilizing multidimensional and contextual information from metadata-related auxiliary tasks, working in a complementary manner.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "C. Analysis Of Ari Modules",
      "text": "Table  III  presents the performance of the weighting and summing all 12 layers (weighted sum) method and the ARI module on metadata-related tasks, alongside the final results after applying the two-stage fine-tuning framework. GR, StyR and SpkR correspond to the tasks of gender recognition, speech style recognition and speaker recognition, respectively. The results in Table  III  show that, compared to the weighted sum method, the ARI module demonstrates superior performance across every task. Specifically, for ASR (automatic speech recognition) tasks, the improvement is most significant, with a relative average improvement of 24.40% in CER and an average improvement of 29.75% in WER. However, for gender recognition and style recognition tasks, the relative average improvement is only 0.66% and 1.55%, respectively. The substantial improvement in ASR tasks supports the observation that the higher the weight assigned to the ASR task during finetuning, the more likely the model is to achieve better performance.\n\nOn the IEMOCAP dataset, the four shown metrics exhibit an average improvement of 5.41% over the weighted sum method. Furthermore, the final results after applying the two-stage fine-tuning method also maintain the same trend.\n\nAfter validating in three representative transformer-based encoders with 4 different auxiliary tasks, it is safe to conclude that the ARI module is more effective and robust than the weighted sum approach in selecting features better suited for different feature-demanding tasks, thereby providing the Co-attention module with higher-quality auxiliary information derived from multimodal metadata.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "D. Analysis Of Auxiliary Task Combinations",
      "text": "In our investigation, we integrated various auxiliary metadata into the proposed SER model. Ablation studies in Table IV examine performance under different auxiliary task configurations. A clear improvement in model efficacy correlates with the incremental integration of auxiliary tasks. Notably, in IEMOCAP, speaker identification accuracy was zero due to the speaker-independent setup (mutually exclusive speakers in training and test sets). However, integrating speaker identification still improved performance in the multi-task learning (MTL) framework (e.g., from speaker + gender to speaker + gender + style), showing that the Co-attention module effectively captures relationships between auxiliary tasks, thereby contributing to the primary task. This also explains the performance drop when fewer auxiliary tasks are used-limited modalities restrict contextual information, and some metadata (e.g., gender) may be weakly related to SER. This can be mitigated by freezing the ARI module in Stage 1. Encouragingly, with enough modalities, the multi-task learning (MTL) framework consistently outperforms SER-only model. Additionally, in the MTL framework, each additional modality, regardless of its relevance to the primary task, improves performance, further demonstrating the Co-attention module's effectiveness on capturing and utilizing the relationships between all auxiliary tasks and their associated metadata.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "V. Conclusion And Limitations",
      "text": "In this paper, we introduce a metadata-enhanced approach for transformer-based SSL models through the ARI and Co-attention modules, based on a two-stage fine-tuning framework. The experiment highlights the ARI module's capability to provide more robust and task-specific feature selection, enabling the extraction of higherquality and more diverse auxiliary information from metadata. The ablation study reveals a promising correlation between auxiliary tasks and model performance, demonstrating the Co-attention module's ability to efficiently capture and utilize the relationships between various metadata auxiliary tasks. The ARI and Co-attention modules exhibit complementary strengths in preserving and utilizing multimodal information, unveiling a methodology that enhances the model's ability to process multidimensional information based on transformerbased SSL models. Moreover, the fact that this method outperforms SOTA across different SSL encoders under pre-trained base models and a speaker-independent setup further validates its effectiveness and superiority. In future work, the potential improvements with larger pretrained models and the generalizability of this methodology to other tasks in the speech domain will be further explored.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: shows the model",
      "page": 1
    },
    {
      "caption": "Figure 1: The proposed model is trained in two stages, the first stage trains the auxiliary tasks and the second stage trains the SER with the auxiliary task",
      "page": 2
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "UA": "73.79",
          "WA": "72.08"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Metric\nTask": "",
          "UA": "74.65",
          "WA": "73.35"
        },
        {
          "Metric\nTask": "",
          "UA": "74.11\n74.67\n75.62\n76.60",
          "WA": "71.37\n70.24\n73.32\n74.68"
        },
        {
          "Metric\nTask": "",
          "UA": "73.99\n74.90\n77.06\n74.82\n77.14\n77.02",
          "WA": "70.73\n73.55\n75.23\n72.17\n75.23\n75.51"
        },
        {
          "Metric\nTask": "",
          "UA": "75.06\n77.35\n77.24\n77.57",
          "WA": "72.26\n75.82\n75.12\n75.62"
        },
        {
          "Metric\nTask": "",
          "UA": "77.74",
          "WA": "75.88"
        }
      ],
      "page": 4
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "A multilingual framework based on pre-training model for speech emotion recognition",
      "authors": [
        "Z Zhang",
        "X Zhang",
        "M Guo",
        "W.-Q Zhang",
        "K Li",
        "Y Huang"
      ],
      "year": "2021",
      "venue": "Proc. Asia-Pacific Signal and Information Processing Association Annual Summit and Conference"
    },
    {
      "citation_id": "2",
      "title": "Speech emotion recognition using self-supervised features",
      "authors": [
        "E Morais",
        "R Hoory",
        "W Zhu",
        "I Gat",
        "M Damasceno",
        "H Aronowitz"
      ],
      "year": "2022",
      "venue": "Proc. IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "3",
      "title": "Multimodal emotion recognition using transfer learning from speaker recognition and bert-based models",
      "authors": [
        "S Padi",
        "S Sadjadi",
        "D Manocha",
        "R Sriram"
      ],
      "year": "2022",
      "venue": "Multimodal emotion recognition using transfer learning from speaker recognition and bert-based models",
      "arxiv": "arXiv:2202.08974"
    },
    {
      "citation_id": "4",
      "title": "Two-stage finetuning of wav2vec 2.0 for speech emotion recognition with ASR and gender pretraining",
      "authors": [
        "Y Gao",
        "C Chu",
        "T Kawahara"
      ],
      "year": "2023",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "5",
      "title": "A critical review of state-of-theart chatbot designs and applications",
      "authors": [
        "B Luo",
        "R Lau",
        "C Li",
        "Y.-W Si"
      ],
      "year": "2022",
      "venue": "Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery"
    },
    {
      "citation_id": "6",
      "title": "IDER: Unified query rewriting for steering, intent carryover, disfluencies, entity carryover and repair",
      "authors": [
        "J Lu",
        "B.-H Tseng",
        "J Moniz",
        "S Li",
        "X Zhu",
        "H Yu",
        "M Akbacak"
      ],
      "year": "2023",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "7",
      "title": "Crosslingual/cross-channel intent detection in contact-center conversations",
      "authors": [
        "S Agrawal",
        "A Sachdeva",
        "S Jain",
        "C George",
        "J Vepa"
      ],
      "year": "2023",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "8",
      "title": "Automated neural nursing assistant (ANNA): An over-the-phone system for cognitive monitoring",
      "authors": [
        "J Solinsky",
        "R Finzel",
        "M Michalowski",
        "S Pakhomov"
      ],
      "year": "2023",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "9",
      "title": "Whisper-based transfer learning for alzheimer disease classification: Leveraging speech segments with full transcripts as prompts",
      "authors": [
        "J Li",
        "W.-Q Zhang"
      ],
      "year": "2024",
      "venue": "Proc. IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "10",
      "title": "Cross-lingual alzheimer's disease detection based on paralinguistic and pre-trained features",
      "authors": [
        "X Chen",
        "Y Pu",
        "J Li",
        "W.-Q Zhang"
      ],
      "venue": "Proc. IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "11",
      "title": "Exploiting emotion information in speaker embeddings for expressive text-to-speech",
      "authors": [
        "Z Shaheen",
        "T Sadekova",
        "Y Matveeva",
        "A Shirshova",
        "M Kudinov"
      ],
      "year": "2023",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "12",
      "title": "Fusing ASR outputs in joint training for speech emotion recognition",
      "authors": [
        "Y Li",
        "P Bell",
        "C Lai"
      ],
      "year": "2022",
      "venue": "Proc. IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "13",
      "title": "Speaker-aware training of speech emotion classifier with speaker recognition",
      "authors": [
        "L Savchenko",
        "A Savchenko"
      ],
      "year": "2021",
      "venue": "Proc. International Conference on Speech and Computer (SPECOM)"
    },
    {
      "citation_id": "14",
      "title": "Speech emotion recognition combining acoustic features and linguistic information in a hybrid support vector machine-belief network architecture",
      "authors": [
        "B Schuller",
        "G Rigoll",
        "M Lang"
      ],
      "year": "2004",
      "venue": "Proc. IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "15",
      "title": "Mmer: Multimodal multi-task learning for speech emotion recognition",
      "authors": [
        "S Ghosh",
        "U Tyagi",
        "S Ramaneswaran",
        "H Srivastava",
        "D Manocha"
      ],
      "year": "2023",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "16",
      "title": "Speech emotion recognition using decomposed speech via multi-task learning",
      "authors": [
        "J.-H Hsu",
        "C.-H Wu",
        "Y.-H Wei"
      ],
      "year": "2023",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "17",
      "title": "Multi-task learning based end-to-end speaker recognition",
      "authors": [
        "Y Pan",
        "W.-Q Zhang"
      ],
      "year": "2019",
      "venue": "Proc. International Conference on Signal Processing and Machine Learning (SPML)"
    },
    {
      "citation_id": "18",
      "title": "Mutitask learning based muti-examples keywords spotting in low resource condition",
      "authors": [
        "J Yang",
        "J Kang",
        "W.-Q Zhang",
        "J Liu"
      ],
      "year": "2018",
      "venue": "Proc. IEEE International Conference on Signal Processing"
    },
    {
      "citation_id": "19",
      "title": "MMER: Multimodal multi-task learning for speech emotion recognition",
      "authors": [
        "S Ghosh",
        "U Tyagi",
        "S Ramaneswaran",
        "H Srivastava",
        "D Manocha"
      ],
      "year": "2023",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "20",
      "title": "A review of speech emotion recognition: Datasets, features, and machine learning algorithms",
      "authors": [
        "B Schuller",
        "A Batliner",
        "S Steidl"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "21",
      "title": "Gradient surgery for multi-task learning",
      "authors": [
        "T Yu",
        "S Kumar",
        "A Gupta",
        "S Levine",
        "K Hausman",
        "C Finn"
      ],
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "22",
      "title": "Deep auxiliary learning for visual localization and odometry",
      "authors": [
        "A Valada",
        "W Burgard"
      ],
      "year": "2018",
      "venue": "Deep auxiliary learning for visual localization and odometry"
    },
    {
      "citation_id": "23",
      "title": "Exploring large scale pre-trained models for robust machine anomalous sound detection",
      "authors": [
        "B Han",
        "Z Lv",
        "A Jiang",
        "W Huang",
        "Z Chen",
        "Y Deng",
        "J Ding",
        "C Lu",
        "W.-Q Zhang",
        "P Fan",
        "J Liu",
        "Y Qian"
      ],
      "year": "2024",
      "venue": "Proc. IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "24",
      "title": "Efficient multi-task auxiliary learning: Selecting auxiliary data by feature similarity",
      "authors": [
        "P.-N Kung",
        "S.-S Yin",
        "Y.-C Chen",
        "T.-H Yang",
        "Y.-N Chen"
      ],
      "year": "2021",
      "venue": "Proc. 2021 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "25",
      "title": "Towards discriminative representations and unbiased predictions: Class-specific angular softmax for speech emotion recognition",
      "authors": [
        "Z Li",
        "L He",
        "J Li",
        "L Wang",
        "W.-Q Zhang"
      ],
      "year": "2019",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "26",
      "title": "HuBERT: Self-supervised speech representation learning by masked prediction of hidden units",
      "authors": [
        "W.-N Hsu",
        "B Bolte",
        "Y.-H Tsai",
        "K Lakhotia",
        "R Salakhutdinov",
        "A Mohamed"
      ],
      "year": "2021",
      "venue": "Proc. IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "27",
      "title": "WavLM: Large-scale self-supervised pretraining for full stack speech processing",
      "authors": [
        "S Chen",
        "C Wang",
        "Z Chen",
        "Y Wu",
        "S Liu",
        "Z Chen",
        "J Li",
        "N Kanda",
        "T Yoshioka",
        "X Xiao",
        "J Wu",
        "L Zhou",
        "S Ren",
        "Y Qian",
        "Y Qian",
        "J Wu",
        "M Zeng",
        "X Yu",
        "F Wei"
      ],
      "year": "2022",
      "venue": "IEEE Journal of Selected Topics in Signal Processing"
    },
    {
      "citation_id": "28",
      "title": "Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks",
      "authors": [
        "A Graves",
        "S Fernández",
        "F Gomez",
        "J Schmidhuber"
      ],
      "year": "2006",
      "venue": "Proc. International Conference on Machine Learning (ICML)"
    },
    {
      "citation_id": "29",
      "title": "IEMOCAP: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "IEMOCAP: Interactive emotional dyadic motion capture database"
    },
    {
      "citation_id": "30",
      "title": "Mingling or misalignment? temporal shift for speech emotion recognition with pre-trained representations",
      "authors": [
        "S Shen",
        "F Liu",
        "A Zhou"
      ],
      "venue": "Proc. IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "31",
      "title": "Temporal modeling matters: A novel temporal emotional modeling approach for speech emotion recognition",
      "authors": [
        "J Ye",
        "X.-C Wen",
        "Y Wei",
        "Y Xu",
        "K Liu",
        "H Shan"
      ],
      "venue": "Proc. IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "32",
      "title": "DST: Deformable speech transformer for emotion recognition",
      "authors": [
        "W Chen",
        "X Xing",
        "X Xu",
        "J Pang",
        "L Du"
      ],
      "venue": "Proc. IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "33",
      "title": "DWFormer: Dynamic window transformer for speech emotion recognition",
      "authors": [
        "S Chen",
        "X Xing",
        "W Zhang",
        "W Chen",
        "X Xu"
      ],
      "venue": "Proc. 2023 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "34",
      "title": "Multiple acoustic features speech emotion recognition using cross-attention transformer",
      "authors": [
        "Y He",
        "N Minematsu",
        "D Saito"
      ],
      "venue": "Proc. IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "35",
      "title": "PyTorch: An imperative style, high-performance deep learning library",
      "authors": [
        "A Paszke",
        "S Gross",
        "F Massa",
        "A Lerer",
        "J Bradbury",
        "G Chanan",
        "T Killeen",
        "Z Lin",
        "N Gimelshein",
        "L Antiga"
      ],
      "year": "2019",
      "venue": "Advances in Neural Information Processing systems"
    },
    {
      "citation_id": "36",
      "title": "SpeechBrain: A general-purpose speech toolkit",
      "authors": [
        "M Ravanelli",
        "T Parcollet",
        "P Plantinga",
        "A Rouhe",
        "S Cornell",
        "L Lugosch",
        "C Subakan",
        "N Dawalatabad",
        "A Heba",
        "J Zhong"
      ],
      "year": "2021",
      "venue": "SpeechBrain: A general-purpose speech toolkit",
      "arxiv": "arXiv:2106.04624"
    },
    {
      "citation_id": "37",
      "title": "Improving automatic speech recognition performance for low-resource languages with self-supervised models",
      "authors": [
        "J Zhao",
        "W.-Q Zhang"
      ],
      "year": "2022",
      "venue": "IEEE Journal of Selected Topics in Signal Processing"
    },
    {
      "citation_id": "38",
      "title": "Librispeech: an ASR corpus based on public domain audio books",
      "authors": [
        "V Panayotov",
        "G Chen",
        "D Povey",
        "S Khudanpur"
      ],
      "year": "2015",
      "venue": "Proc. IEEE international conference on acoustics, speech and signal processing"
    }
  ]
}