{
  "paper_id": "2412.00082v2",
  "title": "Pl-Dcp: A Pairwise Learning Framework With Domain And Class Prototypes For Eeg Emotion Recognition Under Unseen Target Conditions",
  "published": "2024-11-27T00:56:43Z",
  "authors": [
    "Guangli Li",
    "Canbiao Wu",
    "Zhehao Zhou",
    "Tuo Sun",
    "Ping Tan",
    "Li Zhang",
    "Zhen Liang"
  ],
  "keywords": [
    "EEG",
    "Prototype Inference",
    "Transfer learning",
    "Unseen target",
    "Emotion Recognition"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Electroencephalogram (EEG) signals serve as a powerful tool in affective Brain-Computer Interfaces (aBCIs) and play a crucial role in affective computing. In recent years, the introduction of deep learning techniques has significantly advanced the development of aBCIs. However, the current emotion recognition methods based on deep transfer learning face the challenge of the dual dependence of the model on source domain and target domain, As well as being affected by label noise, which seriously affects the performance and generalization ability of the model. To overcome this limitation, we proposes a Pairwise Learning framework with Domain and Category Prototypes for EEG emotion recognition under unseen target conditions (PL-DCP), and integrating concepts of feature disentanglement and prototype inference. Here, the feature disentanglement module extracts and decouples the emotional EEG features to form domain features and class features, and further calculates the dual prototype representation. The Domainpprototype captures the individual variations across subjects, while the class-prototype captures the cross-individual commonality of emotion categories. In addition, the pairwise learning strategy effectively reduces the noise effect caused by wrong labels. The PL-DCP framework conducts a systematic experimental evaluation on the published datasets SEED, SEED-IV and SEED-V, and the accuracy are 82.88%, 65.15% and 61.29%, respectively. The results show that compared with other State-of-the-Art(SOTA) Methods, the PL-DCP model still achieves slightly better performance than the deep transfer learning method that requires both source and target data, although the target domain is completely unseen during the training. This work provides an effective and robust potential solution for emotion recognition. The source code is available at https://github.com/WuCB-BCI/PL_DCP.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Affective, as a basic psychological state, plays a crucial role in daily life. It not only affects people's feelings, thoughts and behaviors, but also affects people's physical and mental health.  [1]  Extensive research has established a strong link between emotional states and mental health disorders.  [2]  As a physiological signal, Electroencephalogram (EEG) has the characteristics of high time resolution and objectivity, which provides more direct and objective clues for understanding and evaluating emotional states.  [3, 4]  Affective computing is a rapidly developing interdisciplinary research field. Accurately recognizing emotions and providing personalized treatment plans are key challenges in the field of emotion computing, which is crucial for daily life, mental health management, human-computer interaction and so on.  [5]  In recent years, with the further development of deep learning technology, the field of emotion recognition has also been vigorously developed,  [6]  and has attracted more and more attention from researchers in different fields such as computer science and psychology. It has become a research hotspot in understanding and recognizing human emotions.  [7]   [8]  For ex-ample: Feng et al.  [9]  designed a hybrid model called ST-GCLSTM, which comprises a spatial-graph convolutional network (SGCN) module and an attention-enhanced bi-directional Long Short-Term Memory (LSTM) module, which can be used to extract representative spatial-temporal features from multiple EEG channels. Yang et al.  [10]  proposed spectral-spatial attention alignment multi-source domain adaptation (S 2 A 2 -MSD), which constructs domain attention to represent affective cognition attributes in spatial and spectral domains and utilizes domain consistent loss to align them between domains. Zhang et al.  [11]  introduced both cascade and parallel convolutional recurrent neural network models for precisely identifying human intended movements and instructions, effectively learning the compositional spatio-temporal representations of raw EEG streams.\n\nDespite these advances, two key challenges remain that limit the practical application of EEG-based emotion recognition systems. (1) EEG signals are highly personalized. Existing transfer learning models rely heavily on target domain data, which makes the model vulnerable to data preferences, thus reducing its adaptability in the real world environment and in-  creasing the cost of model training. Therefore, we should create emotion recognition models under the condition of unseen target domain to adapt to a large number of individual differences, so as to improve real-world usability. (2) Currently, EEG emotion experiments are basically induced by emotional stimulation material, such as video, pictures, audio, etc. Due to individual physiological factors, subjects may not always respond correctly to emotions. Label noise remains a pressing issue that affects the reliability of the model. It is very important to build an emotion recognition model with the ability to resist label noise to improve the robustness of emotion recognition system. Therefore, addressing these two challenges is crucial to advance the field and achieve more adaptable, accurate and reliable emotion recognition techniques.\n\nPrevious studies have emphasized that EEG signals are highly subject-dependent,  [12]   [13]  and that the way individuals perceive and express emotions can vary significantly  [14] . These individual differences extend to neural processes involved in emotional regulation, further complicating the task of emotion recognition.  [15]  A fundamental assumption in many machine learning and deep learning methods is that training and testing data share the same feature space and follow the same distribution, thus satisfying the independent and identically distributed (IID) condition.  [16] [17] However, individual variability in EEG signals often disrupts this assumption, leading to substantial performance degradation or even model failure when traditional emotion recognition models are applied to new subjects. This variability presents a significant challenge for the effectiveness and generalizability of existing models, underscoring the need for approaches that can adapt to diverse individual EEG patterns without compromising performance.\n\nTo address the challenges posed by individual variability in EEG signals, a growing number of researchers are employing transfer learning methodologies, which have shown promising results.  [3, [18] [19] [20] [21] [22]  Transfer learning accommodates variations in domains, tasks and data distributions between training and testing phases. By considering the feature distributions of both the source domain (with labeled data and known distribution) and the target domain (with unlabeled data and unknown distribution), transfer learning approach leverages knowledge from the source domain to improve predictive accuracy in the target domain.  [23]  Given the success of transfer learning in addressing individual differences in EEG signals, most current EEGbased emotion recognition models are developed within a transfer learning framework.  [24] [25] [26] [27]  In addition, transfer learning models generally require simultaneous access to both source and target domain data during training, which necessitates retraining the model before it can adapt to new subjects. This requirement significantly increases the practical costs associated with model deployment, particularly when dealing with large datasets or complex models with numerous parameters. Retraining in these cases can be timeconsuming and computationally intensive, posing a challenge for efficient deployment and limiting the scalability of these models in real-world, diverse scenarios. Addressing this limitation is crucial for developing more adaptable and cost-effective EEG-based emotion recognition systems. Therefore, addressing the individual differences in EEG signals while avoiding dependence on target domain data is a crucial step for EEGbased emotion recognition models to advance towards practical applications.\n\nIn order to solve the above problems, We proposes a Pairwise Learning transfer framework based on Domain and Class Prototype (PL-DCP). In this framework, we propose a domain-class dual feature decoupling mechanism for EEG signals, where individual differences are modeled as the superimposed offset of domain features and class features, and fine-grained feature separation is achieved through semantic structure encoding. Domain prototypes (representing the individual differences of subjects, as shown in Fig.  1 .(a)) and class prototypes (represent-ing the commonality across subjects' emotional categories, as shown in Fig.  1 .(b)) are innovatively constructed, and the sample distribution of the target domain is dynamically calibrated by using prototype similarity matching to alleviate the bottleneck of model generalization caused by individual differences of EEG signals. Alternatively, to reduce the impact of label noise, classification is described as a pairwise learning task that evaluates the relationship between samples and different prototypes. It is worth noting that the target domain data is not needed in the training process, which breaks through the dependence of traditional deep transfer learning on target domain data. Overall, The main contributions are summarized below:\n\n• EEG features are represented through a novel dual prototype of domain and class prototypes, enabling individual variability in EEG signals to be conceptualized as feature shifts resulting from interactions between these domains and class prototypes.\n\n• Proposed the concepts of feature disentanglement and prototype reasoning, obtained domain features and class features from EEG signals. And it does not need to touch the target domain data during the training process.\n\n• We rigorously tested model using public databases. Experimental results show that, in the absence of target domain data, PL-DCP still achieves comparable or even better performance than classical deep transfer learning models. In addition, we perform a thorough analysis of our performance on the model and feature visualization to deepen our understanding of the model and results.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work",
      "text": "To overcome the limitations of the IID assumption, which is challenging to uphold due to significant individual variability in EEG signals, an increasing number of researchers are turning to transfer learning methods. In transfer learning, the labeled training data is source domain, and the unknown test data is target domain. Current transfer learning algorithms for EEGbased emotion recognition can generally be divided into two categories: non-deep transfer learning models and deep transfer learning models.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Non-Deep Transfer Learning Models",
      "text": "To facilitate knowledge transfer between the source and target domains, Pan et al.  [28]  proposed the Transfer Component Analysis (TCA) method, which minimizes the maximum mean discrepancy (MMD) to learn transferable components across domains. Fernando et al.  [29]  introduced the Subspace Alignment (SA) method, which learns a mapping function to align source and target subspaces. The results showed SA could reduce the domain gap and enhance the adaptability of the model. Zheng et al.  [30]  proposed two transfer learning approaches specifically for EEG signals. The first combines TCA with kernel principal component analysis (KPCA) to identify a shared feature space. The second approach, Transductive Parameter Transfer (TPT), constructs multiple classifiers in the source domain and transfers knowledge to the target subject by learning a mapping from distributions to classifier parameters, which are then applied to the target domain. Additionally, Gong et al.  [31]  introduced the Geodesic Flow Kernel (GFK), which maps EEG signals to a kernel space that captures domain shifts using geodesic flow. This approach enhances feature alignment by integrating multiple subspaces and identifying domain-invariant directions, thereby supporting more robust cross-domain adaptation.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Deep Transfer Learning Models",
      "text": "Non-deep transfer models would be limited in complexity and capacity, constraining their ability to fully meet the practical demands of emotion recognition. With advances in deep learning theories and technologies, deep learning-based transfer algorithms have been introduced, offering enhanced model capabilities in terms of performance and generalizability. These algorithms have been widely applied in EEG-based emotion recognition, and most contemporary models now leverage deep transfer learning. For examples, Lin et al.  [32]  proposed a novel emotion recognition method based on a novel deep learning model (ERDL). The model fuses graph convolutional neural network (GCNN) and long-short term memories neural networks (LSTM), to extract graph domain features and temporal features. Ye et al.  [3]  introduced a semi-supervised Dual-Stream Self-Attentive Adversarial Graph Contrastive learning framework (DS-AGC) to enhance feature representation in scenarios with limited labeled data. The DS-AGC includes a graph contrastive learning method to extract effective graph-based feature representations from multiple EEG channels. Additionally, it incorporates a self-attentive fusion module for feature fusion, sample selection, and emotion recognition.\n\nIn recent year, many recent deep transfer learning methods for EEG emotion recognition have been developed based on the Domain-Adversarial Neural Network (DANN  [33] ) structure, leveraging its capacity for effective domain adaptation. For example, Li et al.  [24]  introduced the Bi-Domain Adversarial Neural Network (BiDANN), which accounts for asymmetrical emotional responses in the left and right hemispheres of the brain, leveraging neuroscientific insights to improve emotion recognition performance. Zhang et al.  [34]  introduced a crosssubject emotion recognition method that utilizes CNNs with DDC. This method constructs an Electrode-Frequency Distribution Map (EFDM) from EEG signals, using a CNN to extract emotion-related features while employing DDC to minimize distribution differences between source and target domains. Gokhale et al.  [35]  proposed an adversarial training approach which learns to generate new samples to maximize exposure of the classifier to the attribute-space, which enables deep neural networks to be robust against a wide range of naturally occurring perturbations.\n\nThese deep transfer learning methods has provided valuable insights and strategies for addressing individual differences in EEG signals and achieving significant results in EEG-based emotion recognition. However, a key challenge remains: the dependence of the model on the target domain data during the training process may lead to the model being influenced by the target domain data preference, and increases the practical application cost, which limits the scalability of these models in real-world scenarios.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Prototype Learning",
      "text": "The core concept of prototype learning is that each class is represented by a prototype (a feature vector that acts as a central, representative feature for that class). Data points belonging to a specific class are clustered around this prototype, enabling classification by evaluating the proximity or similarity of data points to their respective class prototypes. For example, Snell et al.  [36]  proposed prototypical networks, which learn a metric space where samples from the same class are clustered around their respective class prototypes. In Pinheiro et al. 's work  [37] , prototype representations are computed for each class, and target domain images are classified by comparing their feature representations to these prototypes, assigning the label of the most similar prototype. Ji et al.  [38]  tackled proposed Semantic-guided Attentive Prototypes Network (SAPNet) framework to address the challenges of extreme imbalance and combinatorial explosion in Human-Object Interaction (HOI) tasks. Liu et al.  [39]  developed a refined prototypical contrastive learning network for few-shot learning (RPCL-FSL), which combines contrastive learning with few-shot learning in an end-to-end network for enhanced performance in lowdata scenarios. Yang et al.  [40]  introduced the Two-Stream Prototypical Learning Network (TSPLN), which simultaneously considers the quality of support images and their relevance to query images, thereby optimizing the learning of class prototypes. These studies show that prototype learning is particularly effective in few-shot learning and unsupervised tasks, significantly improving the robustness of the model and providing a potential approach for the field of affective computing.\n\nTherefore, in the latest research, Zhou et al.  [18]  proposed a prototype representation based pairwise learning framework (PRPL), which applies prototype learning to EEG emotion recognition, and interacts sample features with prototype features through bilinear transformation. However, PR-PL considers only class prototypes, assuming that source domain data follow a uniform distribution, a limitation that does not align with real-world variability. In addition, PR-PL still relies on source and target data to align sample features during training, which may limit its applicability in real-world scenarios.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Methodology",
      "text": "The source domain is defined as S={S 1 ,S 2 ,S 3 ,...,S n } N d n=1 , where N d denotes the number of subjects in the source domain. For each individual subject in the source domain, we have S n ={x i n ,y i n } N s i=1 , where x i n denotes the i-th sample of n-th subject, y i n represents the corresponding emotion label, and N s is the sample size for the n-th subject. The target domain is represented as T={x i t ,y i t } N t i=1 , where N t denotes the number of EEG samples in the target domain. For clarity, Tab.1 summarizes the commonly used notations.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Notation",
      "text": "Description\n\nthe number of subjects in the source domain N c the number of classes S bilinear transformation matrix in h(•)\n\nInspired by the work of Peng et al.  [41]  and Cai et al.  [42] , we hypothesize that EEG features involves two types of deep features: domain-invariant class features and class-invariant domain features. The domain-invariant class features capture semantic information about the class to which a sample belongs(Such as, negative emotions induce larger LPP amplitudes, and this encoding mechanism is related to individual emotional semantic processing, but does not depend on specific domains.  [43] ). The class-invariant domain features convey the domain or subject-specific information of the sample(Such as, anatomical differences such as the thickness of the skull and the morphology of the brain regions lead to individual specificity in the spatial distribution of EEG signals. Such characteristics are independent of the emotion class, but affect the baseline of signal amplitude.  [44][45] ). Overall, original EEG features can be viewed as an integration of these two types of features.\n\nThe distributional differences in EEG signals across subjects can be attributed to variations in the domain features, causing a shift in the distribution of class features. Since EEG classification adheres to a common standard, the class features from different subjects should ideally occupy the same feature space and follow a consistent distribution. Traditional methods assume that all test data come from a single, unified domain, effectively focusing only on class features while overlooking the presence of domain features. This assumption limits model generalization across subjects. Feature extraction methods such as DANN can be interpreted as an attempt to remove the domain-specific components from sample features, thereby aligning the class features across subjects and improving the generalization performance of the model.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Feature Disentanglement",
      "text": "Based on the above theories and assumptions, we consider both domain-specific and class-specific components in EEG feature extraction to improve the robustness and generalization ability across different subjects. To be specific, we start with a shallow feature extractor f g to obtain shallow features from the EEG samples x. Then, we introduce a class feature disentangler f c (•) and a domain feature disentangler f d (•) to disentangle the semantic information within these shallow features, resulting in The structural framework of the proposed PL-DCP model. In the feature disentanglement module, we disentangle domain features and class features from shallow EEG features. In the prototype inference module, we obtain the domain and class prototypes for each subject and then assess the similarity between the sample's domain features and each domain prototype. After selecting the most similar domain, we measure the similarity of the class features of the samples with each class prototype in the domain. In the pairwise learning module, we capture the pairwise relationships between different samples, thereby enhancing the model's anti-interference to label noise.\n\nclass features x c and domain features x d , expressed as:\n\nTo improve the effectiveness of the disentanglers in separating the two types of features, we introduce a domain discriminator D d (•) and a class discriminator D c (•). The domain discriminator is designed to determine the domain of the input features, while the class discriminator ascertains the class of the input features. Our goal is for the domain discriminator to accurately identify the domain of the input when given domain features, while the class discriminator should be unable to identify the class based solely on domain features. This inability to classify based on domain features indicates successful disentanglement, where domain features contain only domain-specific information and no class-related information. Similarly, class features only contain class-related information, without domain-specific information.\n\nTo achieve this, before the class features are passed into the domain discriminator and the domain features into the class discriminator, they pass through a Gradient Reversal Layer (GRL) to facilitate adversarial training. We use a binary cross-entropy loss function to optimize the discriminators. The output from each discriminator is first passed through a sigmoid layer to obtain probability values, which are then compared to the true labels. This approach converts the multi-class problem into several independent binary classification tasks. The binary cross-entropy loss function is defined as follows:\n\nhere, y i represents the true class labels, and z i denotes the predicted class labels from the discriminator. Specifically, for the class discriminator, the binary cross-entropy loss can be defined as:\n\nwhere R(•) represents the GRL. y i c represents the true class labels of i-th sample, x i c represents the class feature of i-th sample, and x i d represents the domain feature i-th sample. This loss function is iteratively optimized during model training to help the class discriminator accurately distinguish the class labels, ensure that the class features only retain class-related information, and minimize the interference of domain-specific features. Similarly, for the domain discriminator, We define the binary cross-entropy loss function as:\n\nhere, y i d represents the true domain labels of i-th sample. This loss function is optimized to ensure that the domain discriminator accurately identifies the domain-specific information, encouraging the domain features to be disentangled from classrelated information. By applying the GRL before the domain features enter the class discriminator, adversarial training is facilitated, further promoting the separation of domain and class features. In the implementation, the shallow feature extractor, domain feature disentangler, and class feature disentangler are all designed as multi-layer perceptrons (MLPs). The disentangled class features x c and domain features x d are then utilized in the subsequent prototype inference module. This module learns domain prototypes to represent each domain, and learns class prototypes to capture each class within those domains.",
      "page_start": 4,
      "page_end": 6
    },
    {
      "section_name": "Prototype Inference",
      "text": "For domain features, we assume that each domain has a prototype representation, which we refer to as the domain prototype. This prototype represents the key characteristics of that domain, with the distribution of domain features centered around it. As shown in Fig.  1 (a), For each domain, the domain prototype can be considered the \"centroid\" of all its domain features. Similarly, for each class within a domain, we derive class prototypes through prototype inference. As shown in Fig.  1(b) , the class prototypes capture the essential properties of each class within the domain and serve as the \"centroid\" of the class features. Both types of prototypes, domain and class, can be computed as the average value of their respective sample features, denoted as µ c . Specifically, the estimation of domain prototypes for each domain is given by:\n\nwhere\n\nrepresents the collection of domain features for all samples from the n-th subject in the source domain. Here, |X n | denotes the number of samples from this subject, x i d is the the domain feature of the i-th sample, and y i d is the corresponding domain label for that sample's domain feature. For data from the same domain, the domain labels y i d are identical. For the class features within a single domain d n , the class prototype is defined as:\n\nwhere After obtaining the domain prototypes and class prototypes, we proceed with prototype inference. For each sample, as shown in Fig.  3 .(a), we after feature decoupling and extracting the corresponding domain and class features, we first perform domain prototype inference to identify the most suitable domain. Then,as shown in Fig.  3 .(b), class prototype inference within the selected domain to determine the class label. Specifically, for the domain feature x i d , we compare its similarity with each class domain prototype using a bilinear transformation h(•) as:\n\nwhere S ∈ R d×d is a trainable, randomly initialized bilinear transformation matrix that is not constrained by positive definiteness or symmetry. The model updates the weights of this bilinear matrix through backpropagation, with the purpose of enhancing the feature representation capability for downstream tasks. We compare the similarity between the sample's domain feature and each domain prototype, defined as:\n\nhere, µ n d (n = 1, ..., N d ) represents the domain prototype of the n-th subject.The most similar domain d * is determined based on the µ n d corresponding to the maximum value in the vector D sim . For each training epoch, once the most similar domain d * for the sample is identified, the class prototypes µ d * ,k c (k = 1, ..., N c ) for 6 that domain are used to measure the similarity between the sample's class features and each class prototype. When comparing class features with class prototypes, we use cosine similarity, defined as:\n\nwhere d cos (•) denotes the cosine similarity computation.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Pairwise Learning",
      "text": "To solve the label noise problem and enhance the model's resistance to label noise, we employ a pairwise learning strategy to replace pointwise learning. Unlike pointwise learning, pairwise learning takes into account the relationships between pairs of samples, capturing their relative associations through pairwise comparisons. The pairwise loss function used is defined as follows.\n\nwhere L(•) is the binary cross-entropy function, defined in Eq.3.\n\nN b represents the number of samples in a batch. r i j is determined based on the class labels of samples i and sample j. For class labels y i c and y j c of samples i and j, if y i c = y j c , then r i j = 1; otherwise, r i j = 0. The r i j derived from the sample labels enhances the model's stability during the training process as well as its generalization capability. The term g(x i c , x j c ; θ) represents the similarity measure between the class features of samples x i and x j , given as:\n\nHere, l i and l j are the feature vectors of the class feature of samples x i and x j , obtained through prototype inference (Eq.10). The symbol (•) represents the dot product operation. The result of g(x i c , x j c ; θ) falls within the range [0,1], representing the similarity between the two feature vectors l i and l j . In summary, the objective function for the pairwise learning is defined as follows:\n\nCompared to pointwise learning, pairwise learning has a stronger resistance to label noise. Furthermore, a soft regularization term R is introduced to prevent the model from overfitting, with its weight parameter β as:\n\nwhere each row of the matrix P represents the domain prototype belonging to a source domain subject, || • || F denotes the Frobenius norm of the matrix, and I represents the identity matrix.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Experimental Results",
      "text": "",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Dataset And Data Preprocessing",
      "text": "We validate the proposed PL-DCP using the widely recognized public databases SEED  [46] , SEED-IV  [47]  and SEED-V  [48] . The SEED dataset includes 15 subjects, each participating in three experimental sessions conducted on different dates, with each session containing 15 trials. During these sessions, video clips were shown to evoke emotional responses (negative, neutral, and positive) while EEG signals were simultaneously recorded. For the SEED-IV dataset, it includes 15 subjects, each participating in three sessions held on different dates, with each session consisting of 24 trials. In this dataset, video clips were used to induce emotions of happiness, sadness, calmness, and fear in the subjects. In the SEED-V database, a total of 16 subjects were participated, with each completing three sessions on different dates. Each session comprised 15 trials and include five emotions: happiness, neutral, sadness, disgust and fear.\n\nThe acquired EEG signals undergo preprocessing as follows. First, the EEG signals are downsampled to a 200 Hz sampling rate, and noise is manually removed, such as electromyography (EMG) and electrooculography (EOG). The denoised data is then filtered using a bandpass filter with a range of 0.3 Hz to 50 Hz. For each experiment, the signals are segmented using a 1-second window, and differential entropy (DE) features, representing the logarithmic energy spectrum of specific frequency bands, are extracted based on five frequency bands: Delta (1-3 Hz), Theta (4-7 Hz), Alpha (8-12Hz), Beta (14-30Hz), and Gamma (31-50Hz), resulting in 310 features for each EEG segment (5 frequency bands × 62 channels). Finally, a Linear Dynamic System (LDS) is applied to smooth all obtained features, leveraging the temporal dependency of emotional changes to filter out EEG components unrelated to emotions and those contaminated by noise.  [49]  The EEG preprocessing procedure adheres to the same standards as previous studies to enable fair comparisons with models presented in previous literature. All models are executed under the following configuration: NVIDIA GeForce RTX 3090, CUDA=11.6, PyTorch =1.12.1.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Experiment Protocols",
      "text": "To thoroughly evaluate the model's performance and enable a comprehensive comparison with existing methods, we adopt two different cross-validation protocols. (1) Cross-Subject Single-Session Leave-One-Subject-Out Cross-Validation. This is the most widely used validation method in EEG-based emotion recognition tasks. In this approach, data from a single session of one subject in the dataset is designated as the target, while data from single sessions of the remaining subjects serve as the source. To ensure consistency with other studies, we use only the first session for the cross-subject single-session cross-validation. (2) Cross-Subject Cross-Session Leave-One-Subject-Out Cross-Validation. To more closely simulate practical application scenarios, we also assess the model's performance for unknown subjects and unknown sessions. Similar to the previous method, all session data from one subject in the  dataset is assigned as the target domain, while data from all sessions of the remaining subjects serve as the source domain.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Cross-Subject Single-Session Leave-One-Subject-Out",
      "text": "Cross-Validation.\n\nThe experimental results of PL-DCP on the SEED database are shown in Tab.2, our proposed PL-DCP model demonstrates a significant performance advantage over both traditional machine learning and non-deep transfer learning methods. Compared to the best-performing non-deep transfer learning method (CORAL), our model shows an improvement of 11.40% (CORAL: 71.48%; PL-DCP: 82.88%). Notably, while other deep transfer learning methods incorporate target domain data during training, our model trains without using any target domain data. Despite this, our method achieves results comparable to, and frequently surpassing, other deep transfer learning approaches that use target domain data, with a 7.46% improvement over DDC and a 5.23% improvement over MS-MDA.  The results on the SEED-IV dataset are shown in Tab.3, PL-DCP also achieves superior results without target domain data in training. Compared to the highest-performing traditional machine learning method (RF: 52.67%), and deep learning methods that utilized target domain data during training (MMD: 59.34%), PL-DCP achieves an accuracy of 65.15% ± 10.34%, outperforming all other methods in both the traditional and deep learning methods. The results on the SEED-V database are shown in Tab.4. The results of PL-DCP are better than those of traditional machine learning methods. The PL-DCP model achieves an accuracy of 61.29%, which is 1.93% higher than the suboptimal method (DAN: 59.36%). All these results prove that PL-DCP achieves superior performance even without relying on target domain data, and has strong robustness and generalization ability.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Cross-Subject Cross-Session Leave-One-Subject-Out",
      "text": "Cross-Validation. Compared to cross-subject single-session, cross-subject cross-session not only accounts for variability among subjects but also incorporates differences across sessions. In EEG-based emotion recognition tasks, this evaluation scheme presents the   is completely unknown during training, the PL-DCP model still achieves slightly better performance than the deep transfer learning method that requires both source and target data. All these results suggest that the proposed PL-DCP can maintain robust performance independently of target domain data, effectively handling the challenges posed by inter-subject and inter-session variability in EEG-based emotion recognition tasks, which demonstrates the model's strong validity and generalization capabilities.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Confusion Matrix",
      "text": "The overall accuracy reflects the global performance of the model, but in the practical application of emotional braincomputer interface, the misjudgment of a specific emotion class may cause completely different interaction consequences. In order to qualitatively evaluate the performance of the proposed model on different sentiment categories, we visualize the results of leave-one-out cross-validation on the SEED, SEED-IV and SEED-V datasets using confusion matrices, and compare  the results with deep transfer learning methods. To measure the recognition performance of the model for different emotions.\n\nOn the SEED dataset, as shown in Fig.  4 .(a)∼(d), all models have the best performance in recognizing positive emotion, which are 81.86%, 80.16%, 81.50%, 78.85%, respectively. In contrast, the recognition accuracy of negative and neutral emotions is slightly lower. In addition, the performance difference between positive and neutral emotion of DANN model is 8.69%, the performance difference between positive and neutral emotion of DAN model is 8.77%, and the performance difference between positive and negative emotion of DCORAL model is 10.4%. However, the maximum performance difference of PL-DCP between emotion categories is only 4.09%. Obviously, PL-DCP showed more robust and balanced performance, while other deep transfer learning methods showed lower stability with fluctuating performance on different emotion categories. On the SEED-IV dataset, as shown in Fig.  4 .(e)∼(h), On the SEED-IV dataset, the recognition accuracy of all models for neutral emotion is slightly higher, reaching 76.53%, 65.96%, 64.12% and 61.35%, respectively, while the recognition accuracy for happy emotion is slightly lower. In addition, on this dataset, each model is easy to confuse sadness and fear. On the SEED-V dataset, as shown in Fig.  5 .(a)∼(d), all the models have slightly lower recognition accuracy performance for happy emotions, while they have higher recognition accuracy for neutral, sad, and fear emotions. All models easily confuse happy emotions with fear and neutral emotions. Compared with other models, the PL-DCP model has the best performance in recognizing neutral emotions. Overall, our proposed PL-DCP model has superior emotion recognition performance and stability.",
      "page_start": 9,
      "page_end": 10
    },
    {
      "section_name": "Discussion And Conclusion",
      "text": "",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Ablation Study",
      "text": "To comprehensively evaluate the performance of the model and evaluate the impact of each module in the proposed PL-DCP model, we performed ablation experiments. Ablation results under cross-subject single-session leave-one-subject-out cross-validation based on the SEED dataset are shown in Tab.8. After removing domain prototype, it is observed that the performance of the model decreases by 8.21% when only the class prototype is used, indicating that the domain and class dual prototype representation method has performance advantages compared with the single class prototype representation method. After removing the domain discriminator loss, we observe a 7.53% decrease in model performance. After removing the class discriminator loss, the model performance decreases from 82.88% to 79.18%, with a decrease of 3.70%. After removing domain discriminator loss and class discriminator loss, the model performance decreases significantly by 8.39%, which indicates that the interaction of domain discriminator and class discriminator helps to extract relevant features and significantly improves the recognition performance for the target domain.\n\nAfter removing the pairwise learning strategy and replacing it with the pointwise learning strategy, the model performance decreases by 5.27%, indicating that the paired learning strategy enhances the recognition performance of the model by capturing the relationship between sample pairs. Removing bilinear transformation matrix S , the model performance decreases by 2.24%, indicating that the bilinear transformation matrix S contributes to the model performance in the formula Eq. 8. After removing the soft regularization term R, the model performance decreases by 1.15%. Overall, All these results demonstrate the effectiveness of the individual components in the PL-DCP model and their combined impact on the overall performance.",
      "page_start": 10,
      "page_end": 11
    },
    {
      "section_name": "Visualization Of Domain And Class Features",
      "text": "To intuitively understand the extracted domain and class features, we use T-SNE  [62]  to visualize these features for the re-  In the visualization of domain features from (a)∼(c), different colors represent the domain features for each domain, while diamonds (⋄) in corresponding colors denote the domain prototypes for each domain. The target data are represented as semitransparent black crosses (×) to avoid excessive overlap with other domain features. Comparing the feature distribution from Fig.  6 .(a)∼(c), it is evident that the domain features of the same subject are clustered more and more closely, forming separate groups with the domain prototypes (⋄) located at the center of each cluster. This differentiation in domain feature distributions across subjects supports our hypothesis that domain features, derived through feature disentanglement of shallow features, can effectively distinguish between subjects. A similar trend is observed in the class feature visualization as shown in To further illustrate the relationships between domain prototypes and class prototypes, we analyze both close pairs and distant pairs of domain samples. As shown in Fig.  7 .(a)∼(b), we visualizing their respective representations in the class prototype space.(a) shows that the domain features of the subject pairs are relatively close, with a high degree of overlap, and the class feature distributions are therefore relatively close. (b) shows that the domain features of the subject pairs are relatively far away, with a low degree of overlap, and the class feature distributions are therefore relatively far away. The results reveal that Samples that are close to each other in the domain prototype space also tend to remain close in the class prototype space, indicating consistency and coherence in the mapping across the two prototype spaces. This observation reinforces the effectiveness of the proposed framework in preserving the intrinsic relationships between samples during dual prototype learning.",
      "page_start": 10,
      "page_end": 11
    },
    {
      "section_name": "Effect Of Noisy Labels",
      "text": "We further evaluate the performance of the model under label noise to evaluate the robustness and noise immunity introduced by pairwise learning. During the experiment, the labels of η% (5%, 10%, 20% and 30%) of the source domain data are randomly replaced with random error labels, which simulates   the situation that the data labels contain noise in the real scene. We tested the PL-DCP model using cross-subject single-session leave-one-out cross validation on the SEED database, and compared it on pointwise learning and pairwise learning strategies. The results are shown in Tab.9. When using the pointwise learning strategy, The accuracy of the model in different proportions of label noise was 77.61%, 76.76%, 73.49%, 69.17%, 66.53%. The performance of the model decreases rapidly with the increase of η, and the performance of the model decreases by 11.08% when the label noise rate reaches 30%. When using pairwise learning strategy, the accuracy rates are 81.46%, 80.32%, 79.79% and 79.01%, respectively. As the label noise rate gradually increases from 5% to 30%, the overall performance of the model only decreases by 3.87%. The results show that the performance of the PL-DCP model only slightly decreases in the label noise environment, showing superior performance, and the introduction of pairwise learning strategy shows strong robustness under label noise.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Conclusion",
      "text": "This study proposes a novel pairwise learning framework with domain and class prototypes (PL-DCP) for EEG-based emotion recognition in unseen target conditions. Unlike existing transfer learning methods that require both source and target data for feature alignment, PL-DCP relies solely on source data for model training. Experimental results show that the proposed method achieves promising results even without using target domain data for training, with performance approaching or even surpassing some deep transfer learning models that heavily rely on target domain data. This suggests that combining feature disentanglement with domain and class prototypes helps generalize more reliable and stable characteristics of individual subjects. Additionally, the introduction of pairwise learning enhances the anti-interference ability of the model to label noise. These findings underscore the potential of this method for practical applications inAffective Brain-Computer Interfaces(aBCIs). In the future work, we will further explore aBCIs models with better performance and more adaptability.",
      "page_start": 12,
      "page_end": 12
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Schematic of the domain prototype and class prototype. (a) Domain Prototype Inference. Colored circles represent domain features, while colored stars",
      "page": 2
    },
    {
      "caption": "Figure 2: The structural framework of the proposed PL-DCP model. In the feature disentanglement module, we disentangle domain features and class features",
      "page": 5
    },
    {
      "caption": "Figure 3: Schematic of domain prototype inference and class prototype infer-",
      "page": 6
    },
    {
      "caption": "Figure 4: Confusion matrices of different model settings under cross-subject single-session leave-one-subject-out cross-validation. The Seed database contains three",
      "page": 9
    },
    {
      "caption": "Figure 5: Confusion matrices of different model settings under cross-subject single-session leave-one-subject-out cross-validation. The Seed-V database contains",
      "page": 10
    },
    {
      "caption": "Figure 6: Visualization of domain and class features at different training stages. (a)∼(c) show the domain features, where the diamond shape (⋄) represents the",
      "page": 11
    },
    {
      "caption": "Figure 7: A visualization of closer pair (a) and distant pair (b) of domain features in the class prototype space.Here, blue hollow circles (◦) represent domain features",
      "page": 12
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "fAddress correspondence to: janezliang@szu.edu.cn": "Abstract"
        },
        {
          "fAddress correspondence to: janezliang@szu.edu.cn": "Electroencephalogram (EEG) signals serve as a powerful tool in affective Brain-Computer Interfaces (aBCIs) and play a crucial"
        },
        {
          "fAddress correspondence to: janezliang@szu.edu.cn": "role in affective computing. In recent years, the introduction of deep learning techniques has significantly advanced the development"
        },
        {
          "fAddress correspondence to: janezliang@szu.edu.cn": "of aBCIs.\nHowever,\nthe current emotion recognition methods based on deep transfer\nlearning face the challenge of\nthe dual"
        },
        {
          "fAddress correspondence to: janezliang@szu.edu.cn": "dependence of the model on source domain and target domain, As well as being affected by label noise, which seriously affects the"
        },
        {
          "fAddress correspondence to: janezliang@szu.edu.cn": "performance and generalization ability of the model. To overcome this limitation, we proposes a Pairwise Learning framework with"
        },
        {
          "fAddress correspondence to: janezliang@szu.edu.cn": "Domain and Category Prototypes for EEG emotion recognition under unseen target conditions (PL-DCP), and integrating concepts"
        },
        {
          "fAddress correspondence to: janezliang@szu.edu.cn": "of feature disentanglement and prototype inference. Here, the feature disentanglement module extracts and decouples the emotional"
        },
        {
          "fAddress correspondence to: janezliang@szu.edu.cn": "EEG features to form domain features and class features, and further calculates the dual prototype representation. The Domain-"
        },
        {
          "fAddress correspondence to: janezliang@szu.edu.cn": "pprototype captures the individual variations across subjects, while the class-prototype captures the cross-individual commonality"
        },
        {
          "fAddress correspondence to: janezliang@szu.edu.cn": "of emotion categories.\nIn addition,\nthe pairwise learning strategy effectively reduces the noise effect caused by wrong labels. The"
        },
        {
          "fAddress correspondence to: janezliang@szu.edu.cn": "PL-DCP framework conducts a systematic experimental evaluation on the published datasets SEED, SEED-IV and SEED-V, and"
        },
        {
          "fAddress correspondence to: janezliang@szu.edu.cn": "the accuracy are 82.88%, 65.15% and 61.29%,\nrespectively. The results show that compared with other State-of-the-Art(SOTA)"
        },
        {
          "fAddress correspondence to: janezliang@szu.edu.cn": "Methods,\nthe PL-DCP model still achieves slightly better performance than the deep transfer learning method that requires both"
        },
        {
          "fAddress correspondence to: janezliang@szu.edu.cn": "source and target data, although the target domain is completely unseen during the training. This work provides an effective and"
        },
        {
          "fAddress correspondence to: janezliang@szu.edu.cn": "robust potential solution for emotion recognition. The source code is available at https://github.com/WuCB-BCI/PL_DCP."
        },
        {
          "fAddress correspondence to: janezliang@szu.edu.cn": "Keywords: EEG, Prototype Inference, Transfer learning, Unseen target, Emotion Recognition."
        },
        {
          "fAddress correspondence to: janezliang@szu.edu.cn": "1.\nIntroduction\nample:\nFeng et al.\n[9] designed a hybrid model called ST-"
        },
        {
          "fAddress correspondence to: janezliang@szu.edu.cn": "GCLSTM, which comprises a spatial-graph convolutional net-"
        },
        {
          "fAddress correspondence to: janezliang@szu.edu.cn": "Affective, as a basic psychological state, plays a crucial role\nwork (SGCN) module and an attention-enhanced bi-directional"
        },
        {
          "fAddress correspondence to: janezliang@szu.edu.cn": "in daily life.\nIt not only affects people’s\nfeelings,\nthoughts\nLong Short-Term Memory (LSTM) module, which can be used"
        },
        {
          "fAddress correspondence to: janezliang@szu.edu.cn": "and behaviors,\nbut\nalso affects people’s physical\nand men-\nto extract representative spatial-temporal features from multiple"
        },
        {
          "fAddress correspondence to: janezliang@szu.edu.cn": "tal health.[1] Extensive research has established a strong link\nEEG channels. Yang et al. [10] proposed spectral-spatial atten-"
        },
        {
          "fAddress correspondence to: janezliang@szu.edu.cn": "between emotional\nstates and mental health disorders.[2] As\ntion alignment multi-source domain adaptation (S2A2-MSD),"
        },
        {
          "fAddress correspondence to: janezliang@szu.edu.cn": "a physiological\nsignal, Electroencephalogram (EEG) has\nthe\nwhich constructs domain attention to represent affective cogni-"
        },
        {
          "fAddress correspondence to: janezliang@szu.edu.cn": "characteristics of high time resolution and objectivity, which\ntion attributes in spatial and spectral domains and utilizes do-"
        },
        {
          "fAddress correspondence to: janezliang@szu.edu.cn": "provides more direct\nand objective\nclues\nfor understanding\nmain consistent\nloss to align them between domains.\nZhang"
        },
        {
          "fAddress correspondence to: janezliang@szu.edu.cn": "and evaluating emotional states.[3, 4] Affective computing is\net al.\n[11] introduced both cascade and parallel convolutional"
        },
        {
          "fAddress correspondence to: janezliang@szu.edu.cn": "a\nrapidly developing interdisciplinary research field.\nAccu-\nrecurrent neural network models for precisely identifying hu-"
        },
        {
          "fAddress correspondence to: janezliang@szu.edu.cn": "rately recognizing emotions and providing personalized treat-\nman intended movements and instructions, effectively learning"
        },
        {
          "fAddress correspondence to: janezliang@szu.edu.cn": "ment plans are key challenges in the field of emotion comput-\nthe compositional spatio-temporal representations of raw EEG"
        },
        {
          "fAddress correspondence to: janezliang@szu.edu.cn": "ing, which is crucial for daily life, mental health management,\nstreams."
        },
        {
          "fAddress correspondence to: janezliang@szu.edu.cn": "human-computer interaction and so on.[5]\nDespite these advances, two key challenges remain that limit"
        },
        {
          "fAddress correspondence to: janezliang@szu.edu.cn": "In recent years, with the further development of deep learn-\nthe practical\napplication of EEG-based emotion recognition"
        },
        {
          "fAddress correspondence to: janezliang@szu.edu.cn": "ing technology,\nthe field of emotion recognition has also been\nsystems.\n(1) EEG signals are highly personalized.\nExisting"
        },
        {
          "fAddress correspondence to: janezliang@szu.edu.cn": "vigorously developed,[6] and has attracted more and more at-\ntransfer\nlearning models\nrely heavily on target domain data,"
        },
        {
          "fAddress correspondence to: janezliang@szu.edu.cn": "tention from researchers\nin different fields\nsuch as computer\nwhich makes\nthe model vulnerable to data preferences,\nthus"
        },
        {
          "fAddress correspondence to: janezliang@szu.edu.cn": "science and psychology.\nIt has become a research hotspot\nin\nreducing its adaptability in the real world environment and in-"
        },
        {
          "fAddress correspondence to: janezliang@szu.edu.cn": "understanding and recognizing human emotions.[7][8] For ex-"
        },
        {
          "fAddress correspondence to: janezliang@szu.edu.cn": "Preprint submitted to arXiv\nAugust 8, 2025"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "(a) Domain Prototype Representation\n(b) Class Prototype Representation": "Figure 1: Schematic of the domain prototype and class prototype.\n(a) Domain Prototype Inference. Colored circles represent domain features, while colored stars"
        },
        {
          "(a) Domain Prototype Representation\n(b) Class Prototype Representation": "represent domain prototypes. Different colors represent different domain prototypes.\n(b) Class Prototype Inference. Hollow shapes represent class features, and"
        },
        {
          "(a) Domain Prototype Representation\n(b) Class Prototype Representation": "solid shapes represent class prototypes within each domain. Different shapes represent different class prototypes"
        },
        {
          "(a) Domain Prototype Representation\n(b) Class Prototype Representation": "in domains,\ntasks and data distributions between training and\ncreasing the cost of model\ntraining. Therefore, we should cre-"
        },
        {
          "(a) Domain Prototype Representation\n(b) Class Prototype Representation": "testing phases. By considering the feature distributions of both\nate emotion recognition models under the condition of unseen"
        },
        {
          "(a) Domain Prototype Representation\n(b) Class Prototype Representation": "the source domain (with labeled data and known distribution)\ntarget domain to adapt\nto a large number of\nindividual differ-"
        },
        {
          "(a) Domain Prototype Representation\n(b) Class Prototype Representation": "ences, so as to improve real-world usability. (2) Currently, EEG\nand the target domain (with unlabeled data and unknown distri-"
        },
        {
          "(a) Domain Prototype Representation\n(b) Class Prototype Representation": "bution),\ntransfer\nlearning approach leverages knowledge from\nemotion experiments are basically induced by emotional stim-"
        },
        {
          "(a) Domain Prototype Representation\n(b) Class Prototype Representation": "the source domain to improve predictive accuracy in the target\nulation material,\nsuch as video, pictures, audio, etc.\nDue to"
        },
        {
          "(a) Domain Prototype Representation\n(b) Class Prototype Representation": "domain.[23] Given the success of transfer learning in address-\nindividual physiological\nfactors,\nsubjects may not always re-"
        },
        {
          "(a) Domain Prototype Representation\n(b) Class Prototype Representation": "ing individual differences in EEG signals, most current EEG-\nspond correctly to emotions.\nLabel noise remains a pressing"
        },
        {
          "(a) Domain Prototype Representation\n(b) Class Prototype Representation": "issue that affects the reliability of the model.\nIt\nis very impor-\nbased emotion recognition models are developed within a trans-"
        },
        {
          "(a) Domain Prototype Representation\n(b) Class Prototype Representation": "fer learning framework.[24–27]\ntant\nto build an emotion recognition model with the ability to"
        },
        {
          "(a) Domain Prototype Representation\n(b) Class Prototype Representation": "In addition, transfer learning models generally require simul-\nresist\nlabel noise to improve the robustness of emotion recog-"
        },
        {
          "(a) Domain Prototype Representation\n(b) Class Prototype Representation": "taneous access to both source and target domain data during\nnition system.\nTherefore, addressing these two challenges is"
        },
        {
          "(a) Domain Prototype Representation\n(b) Class Prototype Representation": "training, which necessitates retraining the model before it can\ncrucial\nto advance the field and achieve more adaptable, accu-"
        },
        {
          "(a) Domain Prototype Representation\n(b) Class Prototype Representation": "adapt to new subjects. This requirement significantly increases\nrate and reliable emotion recognition techniques."
        },
        {
          "(a) Domain Prototype Representation\n(b) Class Prototype Representation": "Previous\nstudies\nhave\nemphasized\nthat EEG signals\nare\nthe practical costs associated with model deployment, particu-"
        },
        {
          "(a) Domain Prototype Representation\n(b) Class Prototype Representation": "larly when dealing with large datasets or complex models with\nhighly subject-dependent,[12][13] and that\nthe way individu-"
        },
        {
          "(a) Domain Prototype Representation\n(b) Class Prototype Representation": "numerous parameters. Retraining in these cases can be time-\nals perceive and express emotions can vary significantly [14]."
        },
        {
          "(a) Domain Prototype Representation\n(b) Class Prototype Representation": "consuming and computationally intensive, posing a challenge\nThese\nindividual\ndifferences\nextend\nto\nneural\nprocesses\nin-"
        },
        {
          "(a) Domain Prototype Representation\n(b) Class Prototype Representation": "for efficient deployment and limiting the scalability of\nthese\nvolved in emotional regulation, further complicating the task of"
        },
        {
          "(a) Domain Prototype Representation\n(b) Class Prototype Representation": "models in real-world, diverse scenarios. Addressing this limita-\nemotion recognition.\n[15] A fundamental assumption in many"
        },
        {
          "(a) Domain Prototype Representation\n(b) Class Prototype Representation": "machine learning and deep learning methods\nis\nthat\ntraining\ntion is crucial for developing more adaptable and cost-effective"
        },
        {
          "(a) Domain Prototype Representation\n(b) Class Prototype Representation": "EEG-based emotion recognition systems. Therefore, address-\nand testing data share the same feature space and follow the"
        },
        {
          "(a) Domain Prototype Representation\n(b) Class Prototype Representation": "ing the individual differences\nin EEG signals while avoiding\nsame distribution,\nthus satisfying the independent and identi-"
        },
        {
          "(a) Domain Prototype Representation\n(b) Class Prototype Representation": "dependence on target domain data is a crucial step for EEG-\ncally distributed (IID) condition.[16][17] However,\nindividual"
        },
        {
          "(a) Domain Prototype Representation\n(b) Class Prototype Representation": "based emotion recognition models to advance towards practical\nvariability in EEG signals often disrupts this assumption, lead-"
        },
        {
          "(a) Domain Prototype Representation\n(b) Class Prototype Representation": "applications.\ning to substantial performance degradation or even model fail-"
        },
        {
          "(a) Domain Prototype Representation\n(b) Class Prototype Representation": "ure when traditional emotion recognition models are applied to\nIn order to solve the above problems, We proposes a Pairwise"
        },
        {
          "(a) Domain Prototype Representation\n(b) Class Prototype Representation": "Learning transfer framework based on Domain and Class Proto-\nnew subjects. This variability presents a significant challenge"
        },
        {
          "(a) Domain Prototype Representation\n(b) Class Prototype Representation": "type (PL-DCP). In this framework, we propose a domain-class\nfor\nthe effectiveness and generalizability of existing models,"
        },
        {
          "(a) Domain Prototype Representation\n(b) Class Prototype Representation": "dual\nfeature decoupling mechanism for EEG signals, where\nunderscoring the need for approaches that can adapt\nto diverse"
        },
        {
          "(a) Domain Prototype Representation\n(b) Class Prototype Representation": "individual differences are modeled as the superimposed offset\nindividual EEG patterns without compromising performance."
        },
        {
          "(a) Domain Prototype Representation\n(b) Class Prototype Representation": "To address the challenges posed by individual variability in\nof domain features and class features, and fine-grained feature"
        },
        {
          "(a) Domain Prototype Representation\n(b) Class Prototype Representation": "separation is\nachieved through semantic\nstructure\nencoding.\nEEG signals, a growing number of researchers are employing"
        },
        {
          "(a) Domain Prototype Representation\n(b) Class Prototype Representation": "Domain prototypes (representing the individual differences of\ntransfer learning methodologies, which have shown promising"
        },
        {
          "(a) Domain Prototype Representation\n(b) Class Prototype Representation": "subjects, as shown in Fig.1.(a)) and class prototypes (represent-\nresults.[3, 18–22] Transfer\nlearning accommodates variations"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "pendence of traditional deep transfer learning on target domain": "data. Overall, The main contributions are summarized below:"
        },
        {
          "pendence of traditional deep transfer learning on target domain": ""
        },
        {
          "pendence of traditional deep transfer learning on target domain": "• EEG features are represented through a novel dual proto-"
        },
        {
          "pendence of traditional deep transfer learning on target domain": "type of domain and class prototypes, enabling individual"
        },
        {
          "pendence of traditional deep transfer learning on target domain": "variability in EEG signals to be conceptualized as feature"
        },
        {
          "pendence of traditional deep transfer learning on target domain": "shifts resulting from interactions between these domains"
        },
        {
          "pendence of traditional deep transfer learning on target domain": "and class prototypes."
        },
        {
          "pendence of traditional deep transfer learning on target domain": ""
        },
        {
          "pendence of traditional deep transfer learning on target domain": "• Proposed the concepts of feature disentanglement and pro-"
        },
        {
          "pendence of traditional deep transfer learning on target domain": ""
        },
        {
          "pendence of traditional deep transfer learning on target domain": "totype reasoning, obtained domain features and class fea-"
        },
        {
          "pendence of traditional deep transfer learning on target domain": ""
        },
        {
          "pendence of traditional deep transfer learning on target domain": "tures from EEG signals. And it does not need to touch the"
        },
        {
          "pendence of traditional deep transfer learning on target domain": ""
        },
        {
          "pendence of traditional deep transfer learning on target domain": "target domain data during the training process."
        },
        {
          "pendence of traditional deep transfer learning on target domain": ""
        },
        {
          "pendence of traditional deep transfer learning on target domain": ""
        },
        {
          "pendence of traditional deep transfer learning on target domain": "• We rigorously tested model using public databases. Exper-"
        },
        {
          "pendence of traditional deep transfer learning on target domain": ""
        },
        {
          "pendence of traditional deep transfer learning on target domain": "imental results show that,\nin the absence of target domain"
        },
        {
          "pendence of traditional deep transfer learning on target domain": ""
        },
        {
          "pendence of traditional deep transfer learning on target domain": "data, PL-DCP still achieves comparable or even better per-"
        },
        {
          "pendence of traditional deep transfer learning on target domain": ""
        },
        {
          "pendence of traditional deep transfer learning on target domain": "formance than classical deep transfer learning models.\nIn"
        },
        {
          "pendence of traditional deep transfer learning on target domain": ""
        },
        {
          "pendence of traditional deep transfer learning on target domain": "addition, we perform a thorough analysis of our perfor-"
        },
        {
          "pendence of traditional deep transfer learning on target domain": ""
        },
        {
          "pendence of traditional deep transfer learning on target domain": "mance on the model and feature visualization to deepen"
        },
        {
          "pendence of traditional deep transfer learning on target domain": ""
        },
        {
          "pendence of traditional deep transfer learning on target domain": "our understanding of the model and results."
        },
        {
          "pendence of traditional deep transfer learning on target domain": ""
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "ing the commonality across subjects’ emotional categories, as": "shown in Fig.1.(b)) are innovatively constructed, and the sam-",
          "Transfer (TPT), constructs multiple classifiers in the source do-": "main and transfers knowledge to the target subject by learning a"
        },
        {
          "ing the commonality across subjects’ emotional categories, as": "ple distribution of the target domain is dynamically calibrated",
          "Transfer (TPT), constructs multiple classifiers in the source do-": "mapping from distributions to classifier parameters, which are"
        },
        {
          "ing the commonality across subjects’ emotional categories, as": "by using prototype similarity matching to alleviate the bottle-",
          "Transfer (TPT), constructs multiple classifiers in the source do-": "then applied to the target domain. Additionally, Gong et al."
        },
        {
          "ing the commonality across subjects’ emotional categories, as": "neck of model generalization caused by individual differences",
          "Transfer (TPT), constructs multiple classifiers in the source do-": "[31] introduced the Geodesic Flow Kernel (GFK), which maps"
        },
        {
          "ing the commonality across subjects’ emotional categories, as": "of EEG signals. Alternatively,\nto reduce the impact of\nlabel",
          "Transfer (TPT), constructs multiple classifiers in the source do-": "EEG signals to a kernel space that captures domain shifts using"
        },
        {
          "ing the commonality across subjects’ emotional categories, as": "noise, classification is described as a pairwise learning task that",
          "Transfer (TPT), constructs multiple classifiers in the source do-": "geodesic flow. This approach enhances feature alignment by in-"
        },
        {
          "ing the commonality across subjects’ emotional categories, as": "evaluates the relationship between samples and different pro-",
          "Transfer (TPT), constructs multiple classifiers in the source do-": "tegrating multiple subspaces and identifying domain-invariant"
        },
        {
          "ing the commonality across subjects’ emotional categories, as": "totypes.\nIt\nis worth noting that\nthe target domain data is not",
          "Transfer (TPT), constructs multiple classifiers in the source do-": "directions, thereby supporting more robust cross-domain adap-"
        },
        {
          "ing the commonality across subjects’ emotional categories, as": "needed in the training process, which breaks through the de-",
          "Transfer (TPT), constructs multiple classifiers in the source do-": "tation."
        },
        {
          "ing the commonality across subjects’ emotional categories, as": "pendence of traditional deep transfer learning on target domain",
          "Transfer (TPT), constructs multiple classifiers in the source do-": ""
        },
        {
          "ing the commonality across subjects’ emotional categories, as": "data. Overall, The main contributions are summarized below:",
          "Transfer (TPT), constructs multiple classifiers in the source do-": "2.2. Deep transfer learning models"
        },
        {
          "ing the commonality across subjects’ emotional categories, as": "",
          "Transfer (TPT), constructs multiple classifiers in the source do-": "Non-deep transfer models would be limited in complexity"
        },
        {
          "ing the commonality across subjects’ emotional categories, as": "• EEG features are represented through a novel dual proto-",
          "Transfer (TPT), constructs multiple classifiers in the source do-": "and capacity, constraining their ability to fully meet\nthe prac-"
        },
        {
          "ing the commonality across subjects’ emotional categories, as": "type of domain and class prototypes, enabling individual",
          "Transfer (TPT), constructs multiple classifiers in the source do-": "tical demands of emotion recognition. With advances in deep"
        },
        {
          "ing the commonality across subjects’ emotional categories, as": "variability in EEG signals to be conceptualized as feature",
          "Transfer (TPT), constructs multiple classifiers in the source do-": "learning theories and technologies, deep learning-based trans-"
        },
        {
          "ing the commonality across subjects’ emotional categories, as": "shifts resulting from interactions between these domains",
          "Transfer (TPT), constructs multiple classifiers in the source do-": "fer algorithms have been introduced, offering enhanced model"
        },
        {
          "ing the commonality across subjects’ emotional categories, as": "and class prototypes.",
          "Transfer (TPT), constructs multiple classifiers in the source do-": "capabilities in terms of performance and generalizability. These"
        },
        {
          "ing the commonality across subjects’ emotional categories, as": "",
          "Transfer (TPT), constructs multiple classifiers in the source do-": "algorithms have been widely applied in EEG-based emotion"
        },
        {
          "ing the commonality across subjects’ emotional categories, as": "• Proposed the concepts of feature disentanglement and pro-",
          "Transfer (TPT), constructs multiple classifiers in the source do-": ""
        },
        {
          "ing the commonality across subjects’ emotional categories, as": "",
          "Transfer (TPT), constructs multiple classifiers in the source do-": "recognition, and most contemporary models now leverage deep"
        },
        {
          "ing the commonality across subjects’ emotional categories, as": "totype reasoning, obtained domain features and class fea-",
          "Transfer (TPT), constructs multiple classifiers in the source do-": ""
        },
        {
          "ing the commonality across subjects’ emotional categories, as": "",
          "Transfer (TPT), constructs multiple classifiers in the source do-": "transfer\nlearning.\nFor examples, Lin et al.\n[32] proposed a"
        },
        {
          "ing the commonality across subjects’ emotional categories, as": "tures from EEG signals. And it does not need to touch the",
          "Transfer (TPT), constructs multiple classifiers in the source do-": ""
        },
        {
          "ing the commonality across subjects’ emotional categories, as": "",
          "Transfer (TPT), constructs multiple classifiers in the source do-": "novel emotion recognition method based on a novel deep learn-"
        },
        {
          "ing the commonality across subjects’ emotional categories, as": "target domain data during the training process.",
          "Transfer (TPT), constructs multiple classifiers in the source do-": ""
        },
        {
          "ing the commonality across subjects’ emotional categories, as": "",
          "Transfer (TPT), constructs multiple classifiers in the source do-": "ing model (ERDL). The model fuses graph convolutional neu-"
        },
        {
          "ing the commonality across subjects’ emotional categories, as": "",
          "Transfer (TPT), constructs multiple classifiers in the source do-": "ral network (GCNN) and long-short term memories neural net-"
        },
        {
          "ing the commonality across subjects’ emotional categories, as": "• We rigorously tested model using public databases. Exper-",
          "Transfer (TPT), constructs multiple classifiers in the source do-": ""
        },
        {
          "ing the commonality across subjects’ emotional categories, as": "",
          "Transfer (TPT), constructs multiple classifiers in the source do-": "works (LSTM),\nto extract graph domain features and tempo-"
        },
        {
          "ing the commonality across subjects’ emotional categories, as": "imental results show that,\nin the absence of target domain",
          "Transfer (TPT), constructs multiple classifiers in the source do-": ""
        },
        {
          "ing the commonality across subjects’ emotional categories, as": "",
          "Transfer (TPT), constructs multiple classifiers in the source do-": "ral features. Ye et al.\n[3] introduced a semi-supervised Dual-"
        },
        {
          "ing the commonality across subjects’ emotional categories, as": "data, PL-DCP still achieves comparable or even better per-",
          "Transfer (TPT), constructs multiple classifiers in the source do-": ""
        },
        {
          "ing the commonality across subjects’ emotional categories, as": "",
          "Transfer (TPT), constructs multiple classifiers in the source do-": "Stream Self-Attentive Adversarial Graph Contrastive learning"
        },
        {
          "ing the commonality across subjects’ emotional categories, as": "formance than classical deep transfer learning models.\nIn",
          "Transfer (TPT), constructs multiple classifiers in the source do-": ""
        },
        {
          "ing the commonality across subjects’ emotional categories, as": "",
          "Transfer (TPT), constructs multiple classifiers in the source do-": "framework (DS-AGC) to enhance feature representation in sce-"
        },
        {
          "ing the commonality across subjects’ emotional categories, as": "addition, we perform a thorough analysis of our perfor-",
          "Transfer (TPT), constructs multiple classifiers in the source do-": ""
        },
        {
          "ing the commonality across subjects’ emotional categories, as": "",
          "Transfer (TPT), constructs multiple classifiers in the source do-": "narios with limited labeled data. The DS-AGC includes a graph"
        },
        {
          "ing the commonality across subjects’ emotional categories, as": "mance on the model and feature visualization to deepen",
          "Transfer (TPT), constructs multiple classifiers in the source do-": ""
        },
        {
          "ing the commonality across subjects’ emotional categories, as": "",
          "Transfer (TPT), constructs multiple classifiers in the source do-": "contrastive\nlearning method to extract\neffective graph-based"
        },
        {
          "ing the commonality across subjects’ emotional categories, as": "our understanding of the model and results.",
          "Transfer (TPT), constructs multiple classifiers in the source do-": ""
        },
        {
          "ing the commonality across subjects’ emotional categories, as": "",
          "Transfer (TPT), constructs multiple classifiers in the source do-": "feature representations from multiple EEG channels. Addition-"
        },
        {
          "ing the commonality across subjects’ emotional categories, as": "",
          "Transfer (TPT), constructs multiple classifiers in the source do-": "ally,\nit\nincorporates a self-attentive fusion module for\nfeature"
        },
        {
          "ing the commonality across subjects’ emotional categories, as": "2. Related Work",
          "Transfer (TPT), constructs multiple classifiers in the source do-": ""
        },
        {
          "ing the commonality across subjects’ emotional categories, as": "",
          "Transfer (TPT), constructs multiple classifiers in the source do-": "fusion, sample selection, and emotion recognition."
        },
        {
          "ing the commonality across subjects’ emotional categories, as": "",
          "Transfer (TPT), constructs multiple classifiers in the source do-": "In recent year, many recent deep transfer\nlearning methods"
        },
        {
          "ing the commonality across subjects’ emotional categories, as": "To overcome the limitations of the IID assumption, which is",
          "Transfer (TPT), constructs multiple classifiers in the source do-": ""
        },
        {
          "ing the commonality across subjects’ emotional categories, as": "",
          "Transfer (TPT), constructs multiple classifiers in the source do-": "for EEG emotion recognition have been developed based on"
        },
        {
          "ing the commonality across subjects’ emotional categories, as": "challenging to uphold due to significant individual variability in",
          "Transfer (TPT), constructs multiple classifiers in the source do-": ""
        },
        {
          "ing the commonality across subjects’ emotional categories, as": "",
          "Transfer (TPT), constructs multiple classifiers in the source do-": "the Domain-Adversarial Neural Network (DANN[33])\nstruc-"
        },
        {
          "ing the commonality across subjects’ emotional categories, as": "EEG signals, an increasing number of researchers are turning",
          "Transfer (TPT), constructs multiple classifiers in the source do-": ""
        },
        {
          "ing the commonality across subjects’ emotional categories, as": "",
          "Transfer (TPT), constructs multiple classifiers in the source do-": "ture,\nleveraging its capacity for effective domain adaptation."
        },
        {
          "ing the commonality across subjects’ emotional categories, as": "to transfer\nlearning methods.\nIn transfer\nlearning,\nthe labeled",
          "Transfer (TPT), constructs multiple classifiers in the source do-": ""
        },
        {
          "ing the commonality across subjects’ emotional categories, as": "",
          "Transfer (TPT), constructs multiple classifiers in the source do-": "For example, Li et al. [24] introduced the Bi-Domain Adversar-"
        },
        {
          "ing the commonality across subjects’ emotional categories, as": "training data is source domain, and the unknown test data is",
          "Transfer (TPT), constructs multiple classifiers in the source do-": ""
        },
        {
          "ing the commonality across subjects’ emotional categories, as": "",
          "Transfer (TPT), constructs multiple classifiers in the source do-": "ial Neural Network (BiDANN), which accounts for asymmetri-"
        },
        {
          "ing the commonality across subjects’ emotional categories, as": "target domain. Current\ntransfer\nlearning algorithms for EEG-",
          "Transfer (TPT), constructs multiple classifiers in the source do-": ""
        },
        {
          "ing the commonality across subjects’ emotional categories, as": "",
          "Transfer (TPT), constructs multiple classifiers in the source do-": "cal emotional responses in the left and right hemispheres of the"
        },
        {
          "ing the commonality across subjects’ emotional categories, as": "based emotion recognition can generally be divided into two",
          "Transfer (TPT), constructs multiple classifiers in the source do-": ""
        },
        {
          "ing the commonality across subjects’ emotional categories, as": "",
          "Transfer (TPT), constructs multiple classifiers in the source do-": "brain,\nleveraging neuroscientific insights to improve emotion"
        },
        {
          "ing the commonality across subjects’ emotional categories, as": "categories: non-deep transfer learning models and deep transfer",
          "Transfer (TPT), constructs multiple classifiers in the source do-": ""
        },
        {
          "ing the commonality across subjects’ emotional categories, as": "",
          "Transfer (TPT), constructs multiple classifiers in the source do-": "recognition performance. Zhang et al.\n[34] introduced a cross-"
        },
        {
          "ing the commonality across subjects’ emotional categories, as": "learning models.",
          "Transfer (TPT), constructs multiple classifiers in the source do-": ""
        },
        {
          "ing the commonality across subjects’ emotional categories, as": "",
          "Transfer (TPT), constructs multiple classifiers in the source do-": "subject emotion recognition method that utilizes CNNs with"
        },
        {
          "ing the commonality across subjects’ emotional categories, as": "",
          "Transfer (TPT), constructs multiple classifiers in the source do-": "DDC. This method constructs an Electrode-Frequency Distri-"
        },
        {
          "ing the commonality across subjects’ emotional categories, as": "2.1. Non-deep transfer learning models",
          "Transfer (TPT), constructs multiple classifiers in the source do-": ""
        },
        {
          "ing the commonality across subjects’ emotional categories, as": "",
          "Transfer (TPT), constructs multiple classifiers in the source do-": "bution Map (EFDM)\nfrom EEG signals, using a CNN to ex-"
        },
        {
          "ing the commonality across subjects’ emotional categories, as": "To facilitate knowledge transfer between the source and tar-",
          "Transfer (TPT), constructs multiple classifiers in the source do-": "tract emotion-related features while employing DDC to min-"
        },
        {
          "ing the commonality across subjects’ emotional categories, as": "get domains, Pan et al.\n[28] proposed the Transfer Component",
          "Transfer (TPT), constructs multiple classifiers in the source do-": "imize distribution differences between source and target do-"
        },
        {
          "ing the commonality across subjects’ emotional categories, as": "Analysis (TCA) method, which minimizes the maximum mean",
          "Transfer (TPT), constructs multiple classifiers in the source do-": "mains. Gokhale et al.\n[35] proposed an adversarial\ntraining"
        },
        {
          "ing the commonality across subjects’ emotional categories, as": "discrepancy (MMD)\nto learn transferable components across",
          "Transfer (TPT), constructs multiple classifiers in the source do-": "approach which learns to generate new samples to maximize"
        },
        {
          "ing the commonality across subjects’ emotional categories, as": "domains. Fernando et al.\n[29] introduced the Subspace Align-",
          "Transfer (TPT), constructs multiple classifiers in the source do-": "exposure of the classifier to the attribute-space, which enables"
        },
        {
          "ing the commonality across subjects’ emotional categories, as": "ment\n(SA) method, which learns a mapping function to align",
          "Transfer (TPT), constructs multiple classifiers in the source do-": "deep neural networks to be robust against a wide range of natu-"
        },
        {
          "ing the commonality across subjects’ emotional categories, as": "source and target subspaces. The results showed SA could re-",
          "Transfer (TPT), constructs multiple classifiers in the source do-": "rally occurring perturbations."
        },
        {
          "ing the commonality across subjects’ emotional categories, as": "duce the domain gap and enhance the adaptability of the model.",
          "Transfer (TPT), constructs multiple classifiers in the source do-": "These deep transfer learning methods has provided valuable"
        },
        {
          "ing the commonality across subjects’ emotional categories, as": "Zheng et al.\n[30] proposed two transfer\nlearning approaches",
          "Transfer (TPT), constructs multiple classifiers in the source do-": "insights and strategies for addressing individual differences in"
        },
        {
          "ing the commonality across subjects’ emotional categories, as": "specifically for EEG signals. The first combines TCA with ker-",
          "Transfer (TPT), constructs multiple classifiers in the source do-": "EEG signals and achieving significant\nresults\nin EEG-based"
        },
        {
          "ing the commonality across subjects’ emotional categories, as": "nel principal component analysis (KPCA) to identify a shared",
          "Transfer (TPT), constructs multiple classifiers in the source do-": "emotion recognition. However, a key challenge remains:\nthe"
        },
        {
          "ing the commonality across subjects’ emotional categories, as": "feature space.\nThe second approach, Transductive Parameter",
          "Transfer (TPT), constructs multiple classifiers in the source do-": "dependence of the model on the target domain data during the"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Table 1: Frequently used notations and descriptions.": ""
        },
        {
          "Table 1: Frequently used notations and descriptions.": ""
        },
        {
          "Table 1: Frequently used notations and descriptions.": "Notation\nDescription"
        },
        {
          "Table 1: Frequently used notations and descriptions.": ""
        },
        {
          "Table 1: Frequently used notations and descriptions.": "S\\T"
        },
        {
          "Table 1: Frequently used notations and descriptions.": "source\\target domain"
        },
        {
          "Table 1: Frequently used notations and descriptions.": "domain \\ discriminator\nDd(·)\\Dc(·)"
        },
        {
          "Table 1: Frequently used notations and descriptions.": "domain\\class feature disentangler\nfd(·)\\ fc(·)"
        },
        {
          "Table 1: Frequently used notations and descriptions.": "domain\\class prototype\nµd\\µc"
        },
        {
          "Table 1: Frequently used notations and descriptions.": ""
        },
        {
          "Table 1: Frequently used notations and descriptions.": "domain\\class feature\nxd\\xc"
        },
        {
          "Table 1: Frequently used notations and descriptions.": ""
        },
        {
          "Table 1: Frequently used notations and descriptions.": "domain\\class label\nyd\\yc"
        },
        {
          "Table 1: Frequently used notations and descriptions.": ""
        },
        {
          "Table 1: Frequently used notations and descriptions.": "f (·)\nshallow feature extractor"
        },
        {
          "Table 1: Frequently used notations and descriptions.": ""
        },
        {
          "Table 1: Frequently used notations and descriptions.": "the number of subjects in the source domain\nNd"
        },
        {
          "Table 1: Frequently used notations and descriptions.": ""
        },
        {
          "Table 1: Frequently used notations and descriptions.": "the number of classes\nNc"
        },
        {
          "Table 1: Frequently used notations and descriptions.": ""
        },
        {
          "Table 1: Frequently used notations and descriptions.": "S\nbilinear transformation matrix in h(·)"
        },
        {
          "Table 1: Frequently used notations and descriptions.": ""
        },
        {
          "Table 1: Frequently used notations and descriptions.": ""
        },
        {
          "Table 1: Frequently used notations and descriptions.": ""
        },
        {
          "Table 1: Frequently used notations and descriptions.": "Inspired by the work of Peng et al.\n[41] and Cai et al.\n[42],"
        },
        {
          "Table 1: Frequently used notations and descriptions.": "we hypothesize that EEG features involves two types of deep"
        },
        {
          "Table 1: Frequently used notations and descriptions.": "domain-invariant class\nfeatures:\nfeatures and class-invariant"
        },
        {
          "Table 1: Frequently used notations and descriptions.": "domain features. The domain-invariant class features capture"
        },
        {
          "Table 1: Frequently used notations and descriptions.": "semantic information about\nthe class\nto which a sample be-"
        },
        {
          "Table 1: Frequently used notations and descriptions.": "longs(Such as,\nnegative\nemotions\ninduce\nlarger LPP ampli-"
        },
        {
          "Table 1: Frequently used notations and descriptions.": "tudes,\nand this encoding mechanism is\nrelated to individual"
        },
        {
          "Table 1: Frequently used notations and descriptions.": "emotional semantic processing, but does not depend on specific"
        },
        {
          "Table 1: Frequently used notations and descriptions.": "domains.[43]). The class-invariant domain features convey the"
        },
        {
          "Table 1: Frequently used notations and descriptions.": "domain or subject-specific information of the sample(Such as,"
        },
        {
          "Table 1: Frequently used notations and descriptions.": "anatomical differences such as the thickness of the skull and the"
        },
        {
          "Table 1: Frequently used notations and descriptions.": "morphology of the brain regions lead to individual specificity in"
        },
        {
          "Table 1: Frequently used notations and descriptions.": "the spatial distribution of EEG signals. Such characteristics are"
        },
        {
          "Table 1: Frequently used notations and descriptions.": "independent of the emotion class, but affect the baseline of sig-"
        },
        {
          "Table 1: Frequently used notations and descriptions.": "nal amplitude.[44][45]). Overall, original EEG features can be"
        },
        {
          "Table 1: Frequently used notations and descriptions.": "viewed as an integration of these two types of features."
        },
        {
          "Table 1: Frequently used notations and descriptions.": "The distributional differences in EEG signals across subjects"
        },
        {
          "Table 1: Frequently used notations and descriptions.": "can be attributed to variations\nin the domain features,\ncaus-"
        },
        {
          "Table 1: Frequently used notations and descriptions.": "ing a shift\nin the distribution of class\nfeatures.\nSince EEG"
        },
        {
          "Table 1: Frequently used notations and descriptions.": "classification adheres to a common standard,\nthe class features"
        },
        {
          "Table 1: Frequently used notations and descriptions.": "from different subjects should ideally occupy the same feature"
        },
        {
          "Table 1: Frequently used notations and descriptions.": "space and follow a consistent distribution.\nTraditional meth-"
        },
        {
          "Table 1: Frequently used notations and descriptions.": "ods assume that all\ntest data come from a single, unified do-"
        },
        {
          "Table 1: Frequently used notations and descriptions.": "main, effectively focusing only on class\nfeatures while over-"
        },
        {
          "Table 1: Frequently used notations and descriptions.": "looking the presence of domain features. This assumption lim-"
        },
        {
          "Table 1: Frequently used notations and descriptions.": "its model generalization across\nsubjects.\nFeature\nextraction"
        },
        {
          "Table 1: Frequently used notations and descriptions.": "methods such as DANN can be interpreted as an attempt to re-"
        },
        {
          "Table 1: Frequently used notations and descriptions.": "move the domain-specific components\nfrom sample features,"
        },
        {
          "Table 1: Frequently used notations and descriptions.": "thereby aligning the class features across subjects and improv-"
        },
        {
          "Table 1: Frequently used notations and descriptions.": "ing the generalization performance of the model."
        },
        {
          "Table 1: Frequently used notations and descriptions.": ""
        },
        {
          "Table 1: Frequently used notations and descriptions.": "3.1. Feature Disentanglement"
        },
        {
          "Table 1: Frequently used notations and descriptions.": ""
        },
        {
          "Table 1: Frequently used notations and descriptions.": ""
        },
        {
          "Table 1: Frequently used notations and descriptions.": "Based on the above theories and assumptions, we consider"
        },
        {
          "Table 1: Frequently used notations and descriptions.": "both domain-specific\nand class-specific\ncomponents\nin EEG"
        },
        {
          "Table 1: Frequently used notations and descriptions.": "feature extraction to improve the robustness and generalization"
        },
        {
          "Table 1: Frequently used notations and descriptions.": ""
        },
        {
          "Table 1: Frequently used notations and descriptions.": "ability across different subjects. To be specific, we start with a"
        },
        {
          "Table 1: Frequently used notations and descriptions.": "shallow feature extractor\nfg to obtain shallow features from the"
        },
        {
          "Table 1: Frequently used notations and descriptions.": "EEG samples x. Then, we introduce a class feature disentangler"
        },
        {
          "Table 1: Frequently used notations and descriptions.": ""
        },
        {
          "Table 1: Frequently used notations and descriptions.": "fc(·) and a domain feature disentangler\nfd(·) to disentangle the"
        },
        {
          "Table 1: Frequently used notations and descriptions.": "semantic information within these shallow features, resulting in"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "GRL": "",
          "...\n1": "",
          "...\n0": "",
          "0": "",
          "1": ""
        },
        {
          "GRL": "Dc",
          "...\n1": "",
          "...\n0": "",
          "0": "",
          "1": ""
        },
        {
          "GRL": "",
          "...\n1": "",
          "...\n0": "",
          "0": "",
          "1": ""
        },
        {
          "GRL": "",
          "...\n1": "",
          "...\n0": "",
          "0": "",
          "1": ""
        },
        {
          "GRL": "",
          "...\n1": "0.26",
          "...\n0": "0.76",
          "0": "0.70",
          "1": "0.92"
        },
        {
          "GRL": "",
          "...\n1": "",
          "...\n0": "",
          "0": "",
          "1": ""
        },
        {
          "GRL": "",
          "...\n1": "0.25",
          "...\n0": "1.00",
          "0": "0.47",
          "1": "0.67"
        },
        {
          "GRL": "",
          "...\n1": "",
          "...\n0": "",
          "0": "",
          "1": ""
        },
        {
          "GRL": "",
          "...\n1": "",
          "...\n0": "",
          "0": "",
          "1": ""
        },
        {
          "GRL": "",
          "...\n1": "",
          "...\n0": "0.47",
          "0": "1.00",
          "1": "0.38"
        },
        {
          "GRL": "",
          "...\n1": "0.16",
          "...\n0": "",
          "0": "",
          "1": ""
        },
        {
          "GRL": "",
          "...\n1": "...\n0.32",
          "...\n0": "0.67",
          "0": "...\n0.38",
          "1": "1.00"
        },
        {
          "GRL": "",
          "...\n1": "",
          "...\n0": "",
          "0": "",
          "1": ""
        },
        {
          "GRL": "",
          "...\n1": "",
          "...\n0": "",
          "0": "",
          "1": ""
        },
        {
          "GRL": "",
          "...\n1": "",
          "...\n0": "",
          "0": "",
          "1": ""
        },
        {
          "GRL": "",
          "...\n1": "",
          "...\n0": "",
          "0": "",
          "1": ""
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "class features xc and domain features xd, expressed as:": "",
          "entropy loss function is defined as follows:": "N"
        },
        {
          "class features xc and domain features xd, expressed as:": "",
          "entropy loss function is defined as follows:": "(cid:88)"
        },
        {
          "class features xc and domain features xd, expressed as:": "(1)\nxc = fc( fg(x))",
          "entropy loss function is defined as follows:": ""
        },
        {
          "class features xc and domain features xd, expressed as:": "",
          "entropy loss function is defined as follows:": "1 N\nL = −\n(3)\n(yi\n· log(zi) + (1 − yi) · log(1 − zi))"
        },
        {
          "class features xc and domain features xd, expressed as:": "",
          "entropy loss function is defined as follows:": "i=1"
        },
        {
          "class features xc and domain features xd, expressed as:": "(2)\nxd = fd( fg(x))",
          "entropy loss function is defined as follows:": ""
        },
        {
          "class features xc and domain features xd, expressed as:": "",
          "entropy loss function is defined as follows:": "here, yi\nrepresents the true class labels, and zi denotes the pre-"
        },
        {
          "class features xc and domain features xd, expressed as:": "To improve the effectiveness of the disentanglers in separating",
          "entropy loss function is defined as follows:": "dicted class labels from the discriminator. Specifically, for the"
        },
        {
          "class features xc and domain features xd, expressed as:": "the two types of features, we introduce a domain discriminator",
          "entropy loss function is defined as follows:": "class discriminator, the binary cross-entropy loss can be defined"
        },
        {
          "class features xc and domain features xd, expressed as:": "Dd(·) and a class discriminator Dc(·). The domain discrimina-",
          "entropy loss function is defined as follows:": "as:"
        },
        {
          "class features xc and domain features xd, expressed as:": "tor\nis designed to determine the domain of\nthe input\nfeatures,",
          "entropy loss function is defined as follows:": ""
        },
        {
          "class features xc and domain features xd, expressed as:": "",
          "entropy loss function is defined as follows:": "(4)\nLcls(θc) = L(yi\nc, xi\nc) + L(yi\nc, R(xi\nd))"
        },
        {
          "class features xc and domain features xd, expressed as:": "while the class discriminator ascertains the class of\nthe input",
          "entropy loss function is defined as follows:": ""
        },
        {
          "class features xc and domain features xd, expressed as:": "",
          "entropy loss function is defined as follows:": "where R(·) represents the GRL. yi\nrepresents the true class la-\nc"
        },
        {
          "class features xc and domain features xd, expressed as:": "features. Our goal is for the domain discriminator to accurately",
          "entropy loss function is defined as follows:": ""
        },
        {
          "class features xc and domain features xd, expressed as:": "",
          "entropy loss function is defined as follows:": "bels of i-th sample, xi\nrepresents the class feature of i-th sam-\nc"
        },
        {
          "class features xc and domain features xd, expressed as:": "identify the domain of the input when given domain features,",
          "entropy loss function is defined as follows:": ""
        },
        {
          "class features xc and domain features xd, expressed as:": "",
          "entropy loss function is defined as follows:": "ple, and xi"
        },
        {
          "class features xc and domain features xd, expressed as:": "",
          "entropy loss function is defined as follows:": "d represents the domain feature i-th sample. This loss"
        },
        {
          "class features xc and domain features xd, expressed as:": "while the class discriminator should be unable to identify the",
          "entropy loss function is defined as follows:": ""
        },
        {
          "class features xc and domain features xd, expressed as:": "",
          "entropy loss function is defined as follows:": "function is iteratively optimized during model\ntraining to help"
        },
        {
          "class features xc and domain features xd, expressed as:": "class based solely on domain features. This inability to classify",
          "entropy loss function is defined as follows:": ""
        },
        {
          "class features xc and domain features xd, expressed as:": "",
          "entropy loss function is defined as follows:": "the class discriminator accurately distinguish the class labels,"
        },
        {
          "class features xc and domain features xd, expressed as:": "based on domain features indicates successful disentanglement,",
          "entropy loss function is defined as follows:": ""
        },
        {
          "class features xc and domain features xd, expressed as:": "",
          "entropy loss function is defined as follows:": "ensure that the class features only retain class-related informa-"
        },
        {
          "class features xc and domain features xd, expressed as:": "where domain features contain only domain-specific informa-",
          "entropy loss function is defined as follows:": ""
        },
        {
          "class features xc and domain features xd, expressed as:": "",
          "entropy loss function is defined as follows:": "tion, and minimize the interference of domain-specific features."
        },
        {
          "class features xc and domain features xd, expressed as:": "tion and no class-related information. Similarly, class features",
          "entropy loss function is defined as follows:": ""
        },
        {
          "class features xc and domain features xd, expressed as:": "",
          "entropy loss function is defined as follows:": "Similarly,\nfor\nthe domain discriminator, We define the binary"
        },
        {
          "class features xc and domain features xd, expressed as:": "only contain class-related information, without domain-specific",
          "entropy loss function is defined as follows:": ""
        },
        {
          "class features xc and domain features xd, expressed as:": "",
          "entropy loss function is defined as follows:": "cross-entropy loss function as:"
        },
        {
          "class features xc and domain features xd, expressed as:": "information.",
          "entropy loss function is defined as follows:": ""
        },
        {
          "class features xc and domain features xd, expressed as:": "To achieve this, before the class features are passed into the",
          "entropy loss function is defined as follows:": ""
        },
        {
          "class features xc and domain features xd, expressed as:": "",
          "entropy loss function is defined as follows:": "Ldom(θd) = L(yi"
        },
        {
          "class features xc and domain features xd, expressed as:": "",
          "entropy loss function is defined as follows:": "(5)\nc))\nd, xi\nd) + L(yi\nd, R(xi"
        },
        {
          "class features xc and domain features xd, expressed as:": "domain discriminator and the domain features into the class dis-",
          "entropy loss function is defined as follows:": ""
        },
        {
          "class features xc and domain features xd, expressed as:": "criminator, they pass through a Gradient Reversal Layer (GRL)",
          "entropy loss function is defined as follows:": "represents the true domain labels of i-th sample. This\nhere, yi"
        },
        {
          "class features xc and domain features xd, expressed as:": "",
          "entropy loss function is defined as follows:": "d"
        },
        {
          "class features xc and domain features xd, expressed as:": "to facilitate adversarial training. We use a binary cross-entropy",
          "entropy loss function is defined as follows:": "loss function is optimized to ensure that\nthe domain discrimi-"
        },
        {
          "class features xc and domain features xd, expressed as:": "loss function to optimize the discriminators. The output from",
          "entropy loss function is defined as follows:": "nator accurately identifies the domain-specific information, en-"
        },
        {
          "class features xc and domain features xd, expressed as:": "each discriminator\nis first passed through a sigmoid layer\nto",
          "entropy loss function is defined as follows:": "couraging the domain features to be disentangled from class-"
        },
        {
          "class features xc and domain features xd, expressed as:": "obtain probability values, which are then compared to the true",
          "entropy loss function is defined as follows:": "related information. By applying the GRL before the domain"
        },
        {
          "class features xc and domain features xd, expressed as:": "labels. This approach converts the multi-class problem into sev-",
          "entropy loss function is defined as follows:": "features enter the class discriminator, adversarial training is fa-"
        },
        {
          "class features xc and domain features xd, expressed as:": "eral\nindependent binary classification tasks. The binary cross-",
          "entropy loss function is defined as follows:": "cilitated, further promoting the separation of domain and class"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "features.\nIn the implementation,\nthe shallow feature extractor,": "domain feature disentangler, and class feature disentangler are",
          "the estimation of domain\nfeatures, denoted as µc. Specifically,": "prototypes for each domain is given by:"
        },
        {
          "features.\nIn the implementation,\nthe shallow feature extractor,": "all designed as multi-layer perceptrons (MLPs). The disentan-",
          "the estimation of domain\nfeatures, denoted as µc. Specifically,": ""
        },
        {
          "features.\nIn the implementation,\nthe shallow feature extractor,": "",
          "the estimation of domain\nfeatures, denoted as µc. Specifically,": "(cid:88)\n1"
        },
        {
          "features.\nIn the implementation,\nthe shallow feature extractor,": "gled class features xc and domain features xd are then utilized in",
          "the estimation of domain\nfeatures, denoted as µc. Specifically,": ""
        },
        {
          "features.\nIn the implementation,\nthe shallow feature extractor,": "",
          "the estimation of domain\nfeatures, denoted as µc. Specifically,": "xi\n(6)\nµd =\nd"
        },
        {
          "features.\nIn the implementation,\nthe shallow feature extractor,": "",
          "the estimation of domain\nfeatures, denoted as µc. Specifically,": "|Xn|"
        },
        {
          "features.\nIn the implementation,\nthe shallow feature extractor,": "the subsequent prototype inference module. This module learns",
          "the estimation of domain\nfeatures, denoted as µc. Specifically,": ""
        },
        {
          "features.\nIn the implementation,\nthe shallow feature extractor,": "",
          "the estimation of domain\nfeatures, denoted as µc. Specifically,": "xi"
        },
        {
          "features.\nIn the implementation,\nthe shallow feature extractor,": "",
          "the estimation of domain\nfeatures, denoted as µc. Specifically,": "d∈Xn"
        },
        {
          "features.\nIn the implementation,\nthe shallow feature extractor,": "domain prototypes to represent each domain, and learns class",
          "the estimation of domain\nfeatures, denoted as µc. Specifically,": ""
        },
        {
          "features.\nIn the implementation,\nthe shallow feature extractor,": "",
          "the estimation of domain\nfeatures, denoted as µc. Specifically,": "(cid:111)|Xn|"
        },
        {
          "features.\nIn the implementation,\nthe shallow feature extractor,": "prototypes to capture each class within those domains.",
          "the estimation of domain\nfeatures, denoted as µc. Specifically,": "(xi\nwhere Xn = (cid:110)"
        },
        {
          "features.\nIn the implementation,\nthe shallow feature extractor,": "",
          "the estimation of domain\nfeatures, denoted as µc. Specifically,": "represents\nthe collection of domain\nd, yi\nd)\ni=1"
        },
        {
          "features.\nIn the implementation,\nthe shallow feature extractor,": "",
          "the estimation of domain\nfeatures, denoted as µc. Specifically,": "features for all samples from the n-th subject\nin the source do-"
        },
        {
          "features.\nIn the implementation,\nthe shallow feature extractor,": "",
          "the estimation of domain\nfeatures, denoted as µc. Specifically,": "main. Here,\n|Xn| denotes the number of samples from this sub-"
        },
        {
          "features.\nIn the implementation,\nthe shallow feature extractor,": "",
          "the estimation of domain\nfeatures, denoted as µc. Specifically,": "ject, xi\nis the the domain feature of the i-th sample, and yi\nis"
        },
        {
          "features.\nIn the implementation,\nthe shallow feature extractor,": "",
          "the estimation of domain\nfeatures, denoted as µc. Specifically,": "d\nd"
        },
        {
          "features.\nIn the implementation,\nthe shallow feature extractor,": "",
          "the estimation of domain\nfeatures, denoted as µc. Specifically,": "the corresponding domain label for that sample’s domain fea-"
        },
        {
          "features.\nIn the implementation,\nthe shallow feature extractor,": "",
          "the estimation of domain\nfeatures, denoted as µc. Specifically,": "the domain labels yi"
        },
        {
          "features.\nIn the implementation,\nthe shallow feature extractor,": "",
          "the estimation of domain\nfeatures, denoted as µc. Specifically,": "d are"
        },
        {
          "features.\nIn the implementation,\nthe shallow feature extractor,": "",
          "the estimation of domain\nfeatures, denoted as µc. Specifically,": "identical. For the class features within a single domain dn,\nthe"
        },
        {
          "features.\nIn the implementation,\nthe shallow feature extractor,": "",
          "the estimation of domain\nfeatures, denoted as µc. Specifically,": "class prototype is defined as:"
        },
        {
          "features.\nIn the implementation,\nthe shallow feature extractor,": "",
          "the estimation of domain\nfeatures, denoted as µc. Specifically,": "(cid:88)\n1"
        },
        {
          "features.\nIn the implementation,\nthe shallow feature extractor,": "",
          "the estimation of domain\nfeatures, denoted as µc. Specifically,": "=\nxi\nµdn,c∗\n(7)\n(cid:12)\n(cid:12)\nc\nc"
        },
        {
          "features.\nIn the implementation,\nthe shallow feature extractor,": "",
          "the estimation of domain\nfeatures, denoted as µc. Specifically,": "(cid:12)"
        },
        {
          "features.\nIn the implementation,\nthe shallow feature extractor,": "",
          "the estimation of domain\nfeatures, denoted as µc. Specifically,": "(cid:12)Xc∗\n(cid:12)"
        },
        {
          "features.\nIn the implementation,\nthe shallow feature extractor,": "",
          "the estimation of domain\nfeatures, denoted as µc. Specifically,": "xi\nc∈Xc∗"
        },
        {
          "features.\nIn the implementation,\nthe shallow feature extractor,": "",
          "the estimation of domain\nfeatures, denoted as µc. Specifically,": "(cid:111)|Xc∗"
        },
        {
          "features.\nIn the implementation,\nthe shallow feature extractor,": "",
          "the estimation of domain\nfeatures, denoted as µc. Specifically,": "|\n= (cid:110)"
        },
        {
          "features.\nIn the implementation,\nthe shallow feature extractor,": "",
          "the estimation of domain\nfeatures, denoted as µc. Specifically,": "where Xc∗\n(xi\nrepresents the set of class features for\nn\nc, yi\nc)"
        },
        {
          "features.\nIn the implementation,\nthe shallow feature extractor,": "",
          "the estimation of domain\nfeatures, denoted as µc. Specifically,": "i=1"
        },
        {
          "features.\nIn the implementation,\nthe shallow feature extractor,": "",
          "the estimation of domain\nfeatures, denoted as µc. Specifically,": "samples that classified as c∗\nfrom the n-th subject’s samples."
        },
        {
          "features.\nIn the implementation,\nthe shallow feature extractor,": "",
          "the estimation of domain\nfeatures, denoted as µc. Specifically,": "Here, |Xc∗\nn | denotes the number of samples classified as c∗ in the"
        },
        {
          "features.\nIn the implementation,\nthe shallow feature extractor,": "(a) Domain Prototype Inference",
          "the estimation of domain\nfeatures, denoted as µc. Specifically,": "xi\nn-th subject’s data.\nis the class feature of\nthe i-th sample,"
        },
        {
          "features.\nIn the implementation,\nthe shallow feature extractor,": "",
          "the estimation of domain\nfeatures, denoted as µc. Specifically,": "c"
        },
        {
          "features.\nIn the implementation,\nthe shallow feature extractor,": "",
          "the estimation of domain\nfeatures, denoted as µc. Specifically,": "and yi\nis the class label for that sample.\nIn summary, for each\nc"
        },
        {
          "features.\nIn the implementation,\nthe shallow feature extractor,": "",
          "the estimation of domain\nfeatures, denoted as µc. Specifically,": "sample, we will obtain the corresponding domain prototype µd"
        },
        {
          "features.\nIn the implementation,\nthe shallow feature extractor,": "",
          "the estimation of domain\nfeatures, denoted as µc. Specifically,": ", µdn,2\n, · · ·\n, µdn,Nc\nand class prototypes [µdn,1\nrepre-\n]. Here, Nc"
        },
        {
          "features.\nIn the implementation,\nthe shallow feature extractor,": "",
          "the estimation of domain\nfeatures, denoted as µc. Specifically,": "sents the number of classes. During training,\nthe prototype of"
        },
        {
          "features.\nIn the implementation,\nthe shallow feature extractor,": "",
          "the estimation of domain\nfeatures, denoted as µc. Specifically,": "each subject\nis computed based on the features of all\nits sam-"
        },
        {
          "features.\nIn the implementation,\nthe shallow feature extractor,": "",
          "the estimation of domain\nfeatures, denoted as µc. Specifically,": "ples. Prototype features are updated iteratively throughout\nthe"
        },
        {
          "features.\nIn the implementation,\nthe shallow feature extractor,": "",
          "the estimation of domain\nfeatures, denoted as µc. Specifically,": "training process to better capture the feature distribution. Note-"
        },
        {
          "features.\nIn the implementation,\nthe shallow feature extractor,": "",
          "the estimation of domain\nfeatures, denoted as µc. Specifically,": "worthy, these prototypes are fixed during the testing phase."
        },
        {
          "features.\nIn the implementation,\nthe shallow feature extractor,": "",
          "the estimation of domain\nfeatures, denoted as µc. Specifically,": "After obtaining the domain prototypes and class prototypes,"
        },
        {
          "features.\nIn the implementation,\nthe shallow feature extractor,": "",
          "the estimation of domain\nfeatures, denoted as µc. Specifically,": "we proceed with prototype\ninference.\nFor\neach sample,\nas"
        },
        {
          "features.\nIn the implementation,\nthe shallow feature extractor,": "",
          "the estimation of domain\nfeatures, denoted as µc. Specifically,": "shown in Fig.3.(a), we after feature decoupling and extracting"
        },
        {
          "features.\nIn the implementation,\nthe shallow feature extractor,": "",
          "the estimation of domain\nfeatures, denoted as µc. Specifically,": "the corresponding domain and class features, we first perform"
        },
        {
          "features.\nIn the implementation,\nthe shallow feature extractor,": "",
          "the estimation of domain\nfeatures, denoted as µc. Specifically,": "domain prototype inference to identify the most\nsuitable do-"
        },
        {
          "features.\nIn the implementation,\nthe shallow feature extractor,": "",
          "the estimation of domain\nfeatures, denoted as µc. Specifically,": "main.\nThen,as\nshown in Fig.3.(b), class prototype inference"
        },
        {
          "features.\nIn the implementation,\nthe shallow feature extractor,": "(b) Class Prototype Inference",
          "the estimation of domain\nfeatures, denoted as µc. Specifically,": ""
        },
        {
          "features.\nIn the implementation,\nthe shallow feature extractor,": "",
          "the estimation of domain\nfeatures, denoted as µc. Specifically,": "within the selected domain to determine the class label. Specif-"
        },
        {
          "features.\nIn the implementation,\nthe shallow feature extractor,": "",
          "the estimation of domain\nfeatures, denoted as µc. Specifically,": "ically, for the domain feature xi"
        },
        {
          "features.\nIn the implementation,\nthe shallow feature extractor,": "Figure 3: Schematic of domain prototype inference and class prototype infer-",
          "the estimation of domain\nfeatures, denoted as µc. Specifically,": "d, we compare its similarity with"
        },
        {
          "features.\nIn the implementation,\nthe shallow feature extractor,": "ence. Firstly,\nthe prototypes were inferred based on the domain prototypes.\n.",
          "the estimation of domain\nfeatures, denoted as µc. Specifically,": "each class domain prototype using a bilinear transformation h(·)"
        },
        {
          "features.\nIn the implementation,\nthe shallow feature extractor,": "Then, in this domain, the emotion class is inferred based on the class prototype.",
          "the estimation of domain\nfeatures, denoted as µc. Specifically,": "as:"
        },
        {
          "features.\nIn the implementation,\nthe shallow feature extractor,": "",
          "the estimation of domain\nfeatures, denoted as µc. Specifically,": "h(xi\n(8)\nd, µd) = (xi\nd)T S µd"
        },
        {
          "features.\nIn the implementation,\nthe shallow feature extractor,": "",
          "the estimation of domain\nfeatures, denoted as µc. Specifically,": "∈ Rd×d\nwhere S\nis a trainable,\nrandomly initialized bilinear"
        },
        {
          "features.\nIn the implementation,\nthe shallow feature extractor,": "3.2. Prototype Inference",
          "the estimation of domain\nfeatures, denoted as µc. Specifically,": "transformation matrix that\nis not constrained by positive defi-"
        },
        {
          "features.\nIn the implementation,\nthe shallow feature extractor,": "",
          "the estimation of domain\nfeatures, denoted as µc. Specifically,": "niteness or symmetry. The model updates the weights of\nthis"
        },
        {
          "features.\nIn the implementation,\nthe shallow feature extractor,": "For domain features, we assume that each domain has a pro-",
          "the estimation of domain\nfeatures, denoted as µc. Specifically,": ""
        },
        {
          "features.\nIn the implementation,\nthe shallow feature extractor,": "",
          "the estimation of domain\nfeatures, denoted as µc. Specifically,": "bilinear matrix through backpropagation, with the purpose of"
        },
        {
          "features.\nIn the implementation,\nthe shallow feature extractor,": "totype representation, which we refer\nto as\nthe domain pro-",
          "the estimation of domain\nfeatures, denoted as µc. Specifically,": ""
        },
        {
          "features.\nIn the implementation,\nthe shallow feature extractor,": "",
          "the estimation of domain\nfeatures, denoted as µc. Specifically,": "enhancing the feature representation capability for downstream"
        },
        {
          "features.\nIn the implementation,\nthe shallow feature extractor,": "totype.\nThis prototype\nrepresents\nthe key characteristics of",
          "the estimation of domain\nfeatures, denoted as µc. Specifically,": ""
        },
        {
          "features.\nIn the implementation,\nthe shallow feature extractor,": "",
          "the estimation of domain\nfeatures, denoted as µc. Specifically,": "tasks. We compare the similarity between the sample’s domain"
        },
        {
          "features.\nIn the implementation,\nthe shallow feature extractor,": "that domain, with the distribution of domain features centered",
          "the estimation of domain\nfeatures, denoted as µc. Specifically,": ""
        },
        {
          "features.\nIn the implementation,\nthe shallow feature extractor,": "",
          "the estimation of domain\nfeatures, denoted as µc. Specifically,": "feature and each domain prototype, defined as:"
        },
        {
          "features.\nIn the implementation,\nthe shallow feature extractor,": "around it. As shown in Fig.1(a), For each domain,\nthe domain",
          "the estimation of domain\nfeatures, denoted as µc. Specifically,": ""
        },
        {
          "features.\nIn the implementation,\nthe shallow feature extractor,": "prototype can be considered the ”centroid” of all\nits domain",
          "the estimation of domain\nfeatures, denoted as µc. Specifically,": ""
        },
        {
          "features.\nIn the implementation,\nthe shallow feature extractor,": "",
          "the estimation of domain\nfeatures, denoted as µc. Specifically,": "Dsim = so f tmax([h(xi"
        },
        {
          "features.\nIn the implementation,\nthe shallow feature extractor,": "",
          "the estimation of domain\nfeatures, denoted as µc. Specifically,": "(9)\nd), . . . , h(xi\nd, µ1\nd, µn\nd)])"
        },
        {
          "features.\nIn the implementation,\nthe shallow feature extractor,": "features.\nSimilarly,\nfor each class within a domain, we de-",
          "the estimation of domain\nfeatures, denoted as µc. Specifically,": ""
        },
        {
          "features.\nIn the implementation,\nthe shallow feature extractor,": "rive class prototypes through prototype inference. As shown",
          "the estimation of domain\nfeatures, denoted as µc. Specifically,": "here, µn\nd(n = 1, ..., Nd) represents the domain prototype of the"
        },
        {
          "features.\nIn the implementation,\nthe shallow feature extractor,": "in Fig.1(b), the class prototypes capture the essential properties",
          "the estimation of domain\nfeatures, denoted as µc. Specifically,": "n-th subject.The most similar domain d∗ is determined based on"
        },
        {
          "features.\nIn the implementation,\nthe shallow feature extractor,": "",
          "the estimation of domain\nfeatures, denoted as µc. Specifically,": "the µn"
        },
        {
          "features.\nIn the implementation,\nthe shallow feature extractor,": "of each class within the domain and serve as the ”centroid” of",
          "the estimation of domain\nfeatures, denoted as µc. Specifically,": "d corresponding to the maximum value in the vector Dsim."
        },
        {
          "features.\nIn the implementation,\nthe shallow feature extractor,": "the class features. Both types of prototypes, domain and class,",
          "the estimation of domain\nfeatures, denoted as µc. Specifically,": "For each training epoch, once the most similar domain d∗ for the"
        },
        {
          "features.\nIn the implementation,\nthe shallow feature extractor,": "can be computed as the average value of their respective sample",
          "the estimation of domain\nfeatures, denoted as µc. Specifically,": "sample is identified, the class prototypes µd∗,k\n(k = 1, ..., Nc) for"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "that domain are used to measure the similarity between the sam-": "ple’s class features and each class prototype. When comparing",
          "4. Experimental Results": ""
        },
        {
          "that domain are used to measure the similarity between the sam-": "class features with class prototypes, we use cosine similarity,",
          "4. Experimental Results": "4.1. Dataset and Data Preprocessing"
        },
        {
          "that domain are used to measure the similarity between the sam-": "defined as:",
          "4. Experimental Results": ""
        },
        {
          "that domain are used to measure the similarity between the sam-": "",
          "4. Experimental Results": "We validate the proposed PL-DCP using the widely recog-"
        },
        {
          "that domain are used to measure the similarity between the sam-": "",
          "4. Experimental Results": "nized public databases SEED [46], SEED-IV [47] and SEED-"
        },
        {
          "that domain are used to measure the similarity between the sam-": ")])\n(10)\nli = so f tmax([dcos(xi\n), . . . , dcos(xi\nc, µd∗,1\nc, µd∗,k",
          "4. Experimental Results": ""
        },
        {
          "that domain are used to measure the similarity between the sam-": "",
          "4. Experimental Results": "V[48]. The SEED dataset includes 15 subjects, each participat-"
        },
        {
          "that domain are used to measure the similarity between the sam-": "",
          "4. Experimental Results": "ing in three experimental sessions conducted on different dates,"
        },
        {
          "that domain are used to measure the similarity between the sam-": "where dcos(·) denotes the cosine similarity computation.",
          "4. Experimental Results": ""
        },
        {
          "that domain are used to measure the similarity between the sam-": "",
          "4. Experimental Results": "with each session containing 15 trials. During these sessions,"
        },
        {
          "that domain are used to measure the similarity between the sam-": "",
          "4. Experimental Results": "video clips were shown to evoke emotional responses (negative,"
        },
        {
          "that domain are used to measure the similarity between the sam-": "3.3. Pairwise Learning",
          "4. Experimental Results": "neutral, and positive) while EEG signals were simultaneously"
        },
        {
          "that domain are used to measure the similarity between the sam-": "",
          "4. Experimental Results": "recorded.\nFor\nthe SEED-IV dataset,\nit\nincludes 15 subjects,"
        },
        {
          "that domain are used to measure the similarity between the sam-": "To solve the label noise problem and enhance the model’s re-",
          "4. Experimental Results": ""
        },
        {
          "that domain are used to measure the similarity between the sam-": "",
          "4. Experimental Results": "each participating in three sessions held on different dates, with"
        },
        {
          "that domain are used to measure the similarity between the sam-": "sistance to label noise, we employ a pairwise learning strategy",
          "4. Experimental Results": ""
        },
        {
          "that domain are used to measure the similarity between the sam-": "",
          "4. Experimental Results": "each session consisting of 24 trials.\nIn this dataset, video clips"
        },
        {
          "that domain are used to measure the similarity between the sam-": "to replace pointwise learning. Unlike pointwise learning, pair-",
          "4. Experimental Results": ""
        },
        {
          "that domain are used to measure the similarity between the sam-": "",
          "4. Experimental Results": "were used to induce emotions of happiness, sadness, calmness,"
        },
        {
          "that domain are used to measure the similarity between the sam-": "wise learning takes into account the relationships between pairs",
          "4. Experimental Results": ""
        },
        {
          "that domain are used to measure the similarity between the sam-": "",
          "4. Experimental Results": "and fear in the subjects.\nIn the SEED-V database, a total of 16"
        },
        {
          "that domain are used to measure the similarity between the sam-": "of samples, capturing their\nrelative associations through pair-",
          "4. Experimental Results": ""
        },
        {
          "that domain are used to measure the similarity between the sam-": "",
          "4. Experimental Results": "subjects were participated, with each completing three sessions"
        },
        {
          "that domain are used to measure the similarity between the sam-": "wise comparisons. The pairwise loss function used is defined",
          "4. Experimental Results": ""
        },
        {
          "that domain are used to measure the similarity between the sam-": "",
          "4. Experimental Results": "on different dates. Each session comprised 15 trials and include"
        },
        {
          "that domain are used to measure the similarity between the sam-": "as follows.",
          "4. Experimental Results": ""
        },
        {
          "that domain are used to measure the similarity between the sam-": "",
          "4. Experimental Results": "five emotions: happiness, neutral, sadness, disgust and fear."
        },
        {
          "that domain are used to measure the similarity between the sam-": "",
          "4. Experimental Results": "The acquired EEG signals undergo preprocessing as follows."
        },
        {
          "that domain are used to measure the similarity between the sam-": "(cid:88)\n1",
          "4. Experimental Results": ""
        },
        {
          "that domain are used to measure the similarity between the sam-": "L(rs\nLclass =\nc, x j",
          "4. Experimental Results": ""
        },
        {
          "that domain are used to measure the similarity between the sam-": "(11)\nc; θ))\ni j, g(xi",
          "4. Experimental Results": "First,\nthe EEG signals are downsampled to a 200 Hz sampling"
        },
        {
          "that domain are used to measure the similarity between the sam-": "NbNb",
          "4. Experimental Results": ""
        },
        {
          "that domain are used to measure the similarity between the sam-": "i, j∈Nb",
          "4. Experimental Results": "rate, and noise is manually removed,\nsuch as electromyogra-"
        },
        {
          "that domain are used to measure the similarity between the sam-": "",
          "4. Experimental Results": "phy (EMG) and electrooculography (EOG). The denoised data"
        },
        {
          "that domain are used to measure the similarity between the sam-": "where L(·) is the binary cross-entropy function, defined in Eq.3.",
          "4. Experimental Results": ""
        },
        {
          "that domain are used to measure the similarity between the sam-": "",
          "4. Experimental Results": "is then filtered using a bandpass filter with a range of 0.3 Hz"
        },
        {
          "that domain are used to measure the similarity between the sam-": "represents the number of samples in a batch.\nis deter-\nNb\nri j",
          "4. Experimental Results": ""
        },
        {
          "that domain are used to measure the similarity between the sam-": "",
          "4. Experimental Results": "to 50 Hz. For each experiment,\nthe signals are segmented us-"
        },
        {
          "that domain are used to measure the similarity between the sam-": "mined based on the class labels of samples i and sample j. For",
          "4. Experimental Results": ""
        },
        {
          "that domain are used to measure the similarity between the sam-": "",
          "4. Experimental Results": "ing a 1-second window, and differential entropy (DE) features,"
        },
        {
          "that domain are used to measure the similarity between the sam-": "= y j\nclass labels yi\nc, then ri j = 1;\nc of samples i and j, if yi\nc and y j",
          "4. Experimental Results": ""
        },
        {
          "that domain are used to measure the similarity between the sam-": "c",
          "4. Experimental Results": "representing the logarithmic energy spectrum of\nspecific fre-"
        },
        {
          "that domain are used to measure the similarity between the sam-": "otherwise, ri j = 0. The ri j derived from the sample labels en-",
          "4. Experimental Results": ""
        },
        {
          "that domain are used to measure the similarity between the sam-": "",
          "4. Experimental Results": "quency bands,\nare extracted based on five frequency bands:"
        },
        {
          "that domain are used to measure the similarity between the sam-": "hances the model’s stability during the training process as well",
          "4. Experimental Results": ""
        },
        {
          "that domain are used to measure the similarity between the sam-": "",
          "4. Experimental Results": "Delta (1-3 Hz), Theta (4-7 Hz), Alpha (8-12Hz), Beta (14-"
        },
        {
          "that domain are used to measure the similarity between the sam-": "as its generalization capability. The term g(xi\nc; θ) represents\nc, x j",
          "4. Experimental Results": ""
        },
        {
          "that domain are used to measure the similarity between the sam-": "",
          "4. Experimental Results": "30Hz),\nand Gamma (31-50Hz),\nresulting in 310 features\nfor"
        },
        {
          "that domain are used to measure the similarity between the sam-": "the similarity measure between the class features of samples xi",
          "4. Experimental Results": ""
        },
        {
          "that domain are used to measure the similarity between the sam-": "",
          "4. Experimental Results": "each EEG segment\n(5 frequency bands × 62 channels).\nFi-"
        },
        {
          "that domain are used to measure the similarity between the sam-": "and x j, given as:",
          "4. Experimental Results": ""
        },
        {
          "that domain are used to measure the similarity between the sam-": "",
          "4. Experimental Results": "nally, a Linear Dynamic System (LDS)\nis applied to smooth"
        },
        {
          "that domain are used to measure the similarity between the sam-": "",
          "4. Experimental Results": "all obtained features,\nleveraging the temporal dependency of"
        },
        {
          "that domain are used to measure the similarity between the sam-": "li\n· l j",
          "4. Experimental Results": ""
        },
        {
          "that domain are used to measure the similarity between the sam-": "g(xi\n(12)\nc; θ) =\n(cid:13)\n(cid:13)\nc, x j",
          "4. Experimental Results": "emotional changes to filter out EEG components unrelated to"
        },
        {
          "that domain are used to measure the similarity between the sam-": "(cid:13)\n(cid:13)",
          "4. Experimental Results": ""
        },
        {
          "that domain are used to measure the similarity between the sam-": "∥li∥2\n(cid:13)l j",
          "4. Experimental Results": ""
        },
        {
          "that domain are used to measure the similarity between the sam-": "(cid:13)2",
          "4. Experimental Results": "emotions and those contaminated by noise.[49] The EEG pre-"
        },
        {
          "that domain are used to measure the similarity between the sam-": "",
          "4. Experimental Results": "processing procedure adheres to the same standards as previ-"
        },
        {
          "that domain are used to measure the similarity between the sam-": "Here, li and l j are the feature vectors of the class feature of sam-",
          "4. Experimental Results": ""
        },
        {
          "that domain are used to measure the similarity between the sam-": "",
          "4. Experimental Results": "ous studies to enable fair comparisons with models presented in"
        },
        {
          "that domain are used to measure the similarity between the sam-": "ples xi and x j, obtained through prototype inference (Eq.10).",
          "4. Experimental Results": ""
        },
        {
          "that domain are used to measure the similarity between the sam-": "",
          "4. Experimental Results": "previous literature. All models are executed under the follow-"
        },
        {
          "that domain are used to measure the similarity between the sam-": "The symbol\n(·)\nrepresents the dot product operation.\nThe re-",
          "4. Experimental Results": ""
        },
        {
          "that domain are used to measure the similarity between the sam-": "",
          "4. Experimental Results": "ing configuration: NVIDIA GeForce RTX 3090, CUDA=11.6,"
        },
        {
          "that domain are used to measure the similarity between the sam-": "sult of g(xi\nc; θ) falls within the range [0,1], representing the\nc, x j",
          "4. Experimental Results": ""
        },
        {
          "that domain are used to measure the similarity between the sam-": "",
          "4. Experimental Results": "PyTorch =1.12.1."
        },
        {
          "that domain are used to measure the similarity between the sam-": "In sum-\nsimilarity between the two feature vectors li and l j.",
          "4. Experimental Results": ""
        },
        {
          "that domain are used to measure the similarity between the sam-": "mary, the objective function for the pairwise learning is defined",
          "4. Experimental Results": ""
        },
        {
          "that domain are used to measure the similarity between the sam-": "",
          "4. Experimental Results": "4.2. Experiment Protocols"
        },
        {
          "that domain are used to measure the similarity between the sam-": "as follows:",
          "4. Experimental Results": ""
        },
        {
          "that domain are used to measure the similarity between the sam-": "",
          "4. Experimental Results": "To thoroughly evaluate the model’s performance and enable"
        },
        {
          "that domain are used to measure the similarity between the sam-": "(cid:88)\n1",
          "4. Experimental Results": ""
        },
        {
          "that domain are used to measure the similarity between the sam-": "",
          "4. Experimental Results": "a comprehensive comparison with existing methods, we adopt"
        },
        {
          "that domain are used to measure the similarity between the sam-": "(13)\nLpairwise(θ) =\nL(ri j, g(xi\nc; θ)) + βR\nc, x j",
          "4. Experimental Results": ""
        },
        {
          "that domain are used to measure the similarity between the sam-": "NbNb",
          "4. Experimental Results": "(1) Cross-Subject\ntwo different\ncross-validation protocols."
        },
        {
          "that domain are used to measure the similarity between the sam-": "i, j∈Nb",
          "4. Experimental Results": ""
        },
        {
          "that domain are used to measure the similarity between the sam-": "",
          "4. Experimental Results": "Single-Session\nLeave-One-Subject-Out\nCross-Validation."
        },
        {
          "that domain are used to measure the similarity between the sam-": "Compared\nto\npointwise\nlearning,\npairwise\nlearning\nhas\na",
          "4. Experimental Results": "This is the most widely used validation method in EEG-based"
        },
        {
          "that domain are used to measure the similarity between the sam-": "stronger resistance to label noise. Furthermore, a soft regular-",
          "4. Experimental Results": "emotion recognition tasks.\nIn this approach, data from a single"
        },
        {
          "that domain are used to measure the similarity between the sam-": "ization term R is introduced to prevent the model from overfit-",
          "4. Experimental Results": "session of one subject\nin the dataset\nis designated as the tar-"
        },
        {
          "that domain are used to measure the similarity between the sam-": "ting, with its weight parameter β as:",
          "4. Experimental Results": "get, while data from single sessions of the remaining subjects"
        },
        {
          "that domain are used to measure the similarity between the sam-": "",
          "4. Experimental Results": "serve as the source. To ensure consistency with other studies,"
        },
        {
          "that domain are used to measure the similarity between the sam-": "(14)\nR = ||PT P − I||F",
          "4. Experimental Results": "we use only the first session for the cross-subject single-session"
        },
        {
          "that domain are used to measure the similarity between the sam-": "",
          "4. Experimental Results": "(2) Cross-Subject Cross-Session Leave-\ncross-validation."
        },
        {
          "that domain are used to measure the similarity between the sam-": "where each row of\nthe matrix P represents the domain proto-",
          "4. Experimental Results": "One-Subject-Out Cross-Validation. To more closely simulate"
        },
        {
          "that domain are used to measure the similarity between the sam-": "||\n·\ntype belonging to a source domain subject,\n||F denotes the",
          "4. Experimental Results": "practical application scenarios, we also assess the model’s per-"
        },
        {
          "that domain are used to measure the similarity between the sam-": "Frobenius norm of the matrix, and I represents the identity ma-",
          "4. Experimental Results": "formance for unknown subjects and unknown sessions. Similar"
        },
        {
          "that domain are used to measure the similarity between the sam-": "trix.",
          "4. Experimental Results": "to the previous method, all session data from one subject in the"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table 2: Cross-subject single-session leave-one-subject-out cross-validation Table 4: Cross-subject single-session leave-one-subject-out cross-validation",
      "data": [
        {
          "Table 2:\nCross-subject\nsingle-session leave-one-subject-out": "results\non\nSEED\ndataset,\nexpressed\nas",
          "cross-validation": "(Mean-Accuracy%±Standard-",
          "Table 4:\nCross-subject\nsingle-session leave-one-subject-out\ncross-validation": "results\non\nSEED-V dataset,\nexpressed\nas\n(Mean-Accuracy%±Standard-"
        },
        {
          "Table 2:\nCross-subject\nsingle-session leave-one-subject-out": "Deviation%). Here,\n’*’",
          "cross-validation": "indicates the results are obtained by our own imple-",
          "Table 4:\nCross-subject\nsingle-session leave-one-subject-out\ncross-validation": "Deviation%). Here,\n’*’\nindicates the results are obtained by our own imple-"
        },
        {
          "Table 2:\nCross-subject\nsingle-session leave-one-subject-out": "mentation.",
          "cross-validation": "",
          "Table 4:\nCross-subject\nsingle-session leave-one-subject-out\ncross-validation": "mentation."
        },
        {
          "Table 2:\nCross-subject\nsingle-session leave-one-subject-out": "Methods\nMethods\nPacc(%)",
          "cross-validation": "Pacc(%)",
          "Table 4:\nCross-subject\nsingle-session leave-one-subject-out\ncross-validation": "Methods\nMethods\nPacc(%)\nPacc(%)"
        },
        {
          "Table 2:\nCross-subject\nsingle-session leave-one-subject-out": "Traditional machine learning methods",
          "cross-validation": "",
          "Table 4:\nCross-subject\nsingle-session leave-one-subject-out\ncross-validation": "Traditional machine learning methods"
        },
        {
          "Table 2:\nCross-subject\nsingle-session leave-one-subject-out": "KNN*[50]\n55.26±12.43\nKPCA*[51]",
          "cross-validation": "48.07±09.97",
          "Table 4:\nCross-subject\nsingle-session leave-one-subject-out\ncross-validation": "KNN*[50]\n35.73±07.98\nKPCA*[51]\n35.47±09.39"
        },
        {
          "Table 2:\nCross-subject\nsingle-session leave-one-subject-out": "SA*[29]\nSVM*[52]\n70.62±09.02",
          "cross-validation": "59.73±05.40",
          "Table 4:\nCross-subject\nsingle-session leave-one-subject-out\ncross-validation": "SVM*[52]\n53.14±10.10\nSA*[29]\n36.06±11.55"
        },
        {
          "Table 2:\nCross-subject\nsingle-session leave-one-subject-out": "TCA*[28]\n58.12±09.52\nCORAL*[53]",
          "cross-validation": "71.48±11.57",
          "Table 4:\nCross-subject\nsingle-session leave-one-subject-out\ncross-validation": "TCA*[28]\n37.57±13.47\nCORAL*[53]\n55.18±07.42"
        },
        {
          "Table 2:\nCross-subject\nsingle-session leave-one-subject-out": "GFK*[31]\n56.71±12.29\nRF*[54]",
          "cross-validation": "62.78±06.60",
          "Table 4:\nCross-subject\nsingle-session leave-one-subject-out\ncross-validation": "GFK*[31]\n38.32±10.11\nRF*[54]\n42.29±16.02"
        },
        {
          "Table 2:\nCross-subject\nsingle-session leave-one-subject-out": "Deep learning methods",
          "cross-validation": "",
          "Table 4:\nCross-subject\nsingle-session leave-one-subject-out\ncross-validation": "Deep learning methods"
        },
        {
          "Table 2:\nCross-subject\nsingle-session leave-one-subject-out": "DAN*[55]\n82.54±09.25\nDANN*[56]",
          "cross-validation": "81.57±07.21",
          "Table 4:\nCross-subject\nsingle-session leave-one-subject-out\ncross-validation": "DAN*[55]\n59.36±16.83\nDANN*[56]\n56.28±16.25"
        },
        {
          "Table 2:\nCross-subject\nsingle-session leave-one-subject-out": "DCORAL*[57]\n82.90±06.97\nDDC*[58]",
          "cross-validation": "75.42±10.15",
          "Table 4:\nCross-subject\nsingle-session leave-one-subject-out\ncross-validation": "DDC*[58]\n56.54±18.35\nDCORAL*[57]\n56.26±14.56"
        },
        {
          "Table 2:\nCross-subject\nsingle-session leave-one-subject-out": "MMD [60]\nDGCNN[59]\n79.95±09.02",
          "cross-validation": "80.88/10.10",
          "Table 4:\nCross-subject\nsingle-session leave-one-subject-out\ncross-validation": ""
        },
        {
          "Table 2:\nCross-subject\nsingle-session leave-one-subject-out": "",
          "cross-validation": "",
          "Table 4:\nCross-subject\nsingle-session leave-one-subject-out\ncross-validation": "PL-DCP\n61.29±09.61"
        },
        {
          "Table 2:\nCross-subject\nsingle-session leave-one-subject-out": "BiDANN[24]\n83.28±09.60\nR2G-STNN[61]",
          "cross-validation": "84.16±07.10",
          "Table 4:\nCross-subject\nsingle-session leave-one-subject-out\ncross-validation": ""
        },
        {
          "Table 2:\nCross-subject\nsingle-session leave-one-subject-out": "EFDMs[34]\n78.40±06.76\nMS-MDA*[26]",
          "cross-validation": "77.65±11.32",
          "Table 4:\nCross-subject\nsingle-session leave-one-subject-out\ncross-validation": ""
        },
        {
          "Table 2:\nCross-subject\nsingle-session leave-one-subject-out": "PL-DCP",
          "cross-validation": "82.88±05.23",
          "Table 4:\nCross-subject\nsingle-session leave-one-subject-out\ncross-validation": "Table\n5:\nCross-subject\ncross-session\nleave-one-subject-out\ncross-validation"
        },
        {
          "Table 2:\nCross-subject\nsingle-session leave-one-subject-out": "",
          "cross-validation": "",
          "Table 4:\nCross-subject\nsingle-session leave-one-subject-out\ncross-validation": "results\non\nSEED\ndataset,\nexpressed\nas\n(Mean-Accuracy%±Standard-"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table 2: Cross-subject single-session leave-one-subject-out cross-validation Table 4: Cross-subject single-session leave-one-subject-out cross-validation",
      "data": [
        {
          "Table 3:\nCross-subject": "results\non\nSEED-IV dataset,",
          "single-session leave-one-subject-out\ncross-validation": "expressed\nas\n(Mean-Accuracy%±Standard-"
        },
        {
          "Table 3:\nCross-subject": "Deviation%). Here,\n’*’",
          "single-session leave-one-subject-out\ncross-validation": "indicates the results are obtained by our own imple-"
        },
        {
          "Table 3:\nCross-subject": "mentation.",
          "single-session leave-one-subject-out\ncross-validation": ""
        },
        {
          "Table 3:\nCross-subject": "",
          "single-session leave-one-subject-out\ncross-validation": ""
        },
        {
          "Table 3:\nCross-subject": "",
          "single-session leave-one-subject-out\ncross-validation": ""
        },
        {
          "Table 3:\nCross-subject": "Methods",
          "single-session leave-one-subject-out\ncross-validation": "Methods\nPacc(%)\nPacc(%)"
        },
        {
          "Table 3:\nCross-subject": "",
          "single-session leave-one-subject-out\ncross-validation": ""
        },
        {
          "Table 3:\nCross-subject": "",
          "single-session leave-one-subject-out\ncross-validation": "Traditional machine learning methods"
        },
        {
          "Table 3:\nCross-subject": "",
          "single-session leave-one-subject-out\ncross-validation": ""
        },
        {
          "Table 3:\nCross-subject": "KNN*[50]",
          "single-session leave-one-subject-out\ncross-validation": "41.77±09.53\nKPCA*[51]\n29.25±09.73"
        },
        {
          "Table 3:\nCross-subject": "SVM*[52]",
          "single-session leave-one-subject-out\ncross-validation": "50.50±12.03\nSA*[29]\n34.74±05.29"
        },
        {
          "Table 3:\nCross-subject": "",
          "single-session leave-one-subject-out\ncross-validation": ""
        },
        {
          "Table 3:\nCross-subject": "TCA*[28]",
          "single-session leave-one-subject-out\ncross-validation": "44.11±10.76\nCORAL*[53]\n48.14±10.38"
        },
        {
          "Table 3:\nCross-subject": "",
          "single-session leave-one-subject-out\ncross-validation": ""
        },
        {
          "Table 3:\nCross-subject": "GFK*[31]",
          "single-session leave-one-subject-out\ncross-validation": "RF*[54]\n52.67±13.85\n43.10±09.77"
        },
        {
          "Table 3:\nCross-subject": "",
          "single-session leave-one-subject-out\ncross-validation": ""
        },
        {
          "Table 3:\nCross-subject": "",
          "single-session leave-one-subject-out\ncross-validation": "Deep learning methods"
        },
        {
          "Table 3:\nCross-subject": "",
          "single-session leave-one-subject-out\ncross-validation": ""
        },
        {
          "Table 3:\nCross-subject": "DAN*[55]",
          "single-session leave-one-subject-out\ncross-validation": "59.27±14.45\nDANN*[56]\n57.16±12.61"
        },
        {
          "Table 3:\nCross-subject": "DCORAL*[57]",
          "single-session leave-one-subject-out\ncross-validation": "56.05±15.60\nDDC*[58]\n58.02±15.14"
        },
        {
          "Table 3:\nCross-subject": "MS-MDA*[26]",
          "single-session leave-one-subject-out\ncross-validation": "57.36±11.76\nMMD[60]\n59.34±05.48"
        },
        {
          "Table 3:\nCross-subject": "PL-DCP",
          "single-session leave-one-subject-out\ncross-validation": "65.15±10.34"
        },
        {
          "Table 3:\nCross-subject": "",
          "single-session leave-one-subject-out\ncross-validation": ""
        }
      ],
      "page": 8
    },
    {
      "caption": "Table 6: Cross-subject cross-session leave-one-subject-out cross-validation Table 7: Cross-subject cross-session leave-one-subject-out cross-validation",
      "data": [
        {
          "(e)\n(f)\n(g)": "Figure 4: Confusion matrices of different model settings under cross-subject single-session leave-one-subject-out cross-validation. The Seed database contains three",
          "(h)": ""
        },
        {
          "(e)\n(f)\n(g)": "emotion categories: negative, neutral and positive. Among them, (a) PL-DCP; (b) DANN; (c) DAN; (d) DCORAL.",
          "(h)": "the Seed-IV database contains four emotion"
        },
        {
          "(e)\n(f)\n(g)": "categories: happy, sad, calm and fear. Among them, (e) PL-DCP; (f) DANN; (g) DAN; (h) DCORAL.",
          "(h)": ""
        },
        {
          "(e)\n(f)\n(g)": "Table\n6:\nCross-subject\ncross-session\nleave-one-subject-out\ncross-validation\nTable\n7:\nCross-subject",
          "(h)": "cross-session\nleave-one-subject-out\ncross-validation"
        },
        {
          "(e)\n(f)\n(g)": "results\non\nSEED-IV dataset,\nexpressed\nas\n(Mean-Accuracy%±Standard-\nresults\non\nSEED-V dataset,",
          "(h)": "expressed\nas\n(Mean-Accuracy%±Standard-"
        },
        {
          "(e)\n(f)\n(g)": "Deviation%). Here,\n’*’\nindicates the results are obtained by our own imple-\nDeviation%). Here,\n’*’",
          "(h)": "indicates the results are obtained by our own imple-"
        },
        {
          "(e)\n(f)\n(g)": "mentation.\nmentation.",
          "(h)": ""
        }
      ],
      "page": 9
    },
    {
      "caption": "Table 6: Cross-subject cross-session leave-one-subject-out cross-validation Table 7: Cross-subject cross-session leave-one-subject-out cross-validation",
      "data": [
        {
          "Table\n6:\nCross-subject": "results\non",
          "cross-session": "SEED-V dataset,\nexpressed",
          "leave-one-subject-out": "as",
          "cross-validation": "(Mean-Accuracy%±Standard-",
          "Table\n7:\nCross-subject": "results\non"
        },
        {
          "Table\n6:\nCross-subject": "Deviation%). Here,",
          "cross-session": "’*’",
          "leave-one-subject-out": "indicates the results are obtained by our own imple-",
          "cross-validation": "",
          "Table\n7:\nCross-subject": "Deviation%). Here,"
        },
        {
          "Table\n6:\nCross-subject": "mentation.",
          "cross-session": "",
          "leave-one-subject-out": "",
          "cross-validation": "",
          "Table\n7:\nCross-subject": "mentation."
        },
        {
          "Table\n6:\nCross-subject": "Methods",
          "cross-session": "Pacc(%)",
          "leave-one-subject-out": "Methods",
          "cross-validation": "Pacc(%)",
          "Table\n7:\nCross-subject": "Methods"
        },
        {
          "Table\n6:\nCross-subject": "",
          "cross-session": "Traditional machine learning methods",
          "leave-one-subject-out": "",
          "cross-validation": "",
          "Table\n7:\nCross-subject": ""
        },
        {
          "Table\n6:\nCross-subject": "KNN*[50]",
          "cross-session": "35.28±07.57",
          "leave-one-subject-out": "KPCA*[51]",
          "cross-validation": "39.68±11.28",
          "Table\n7:\nCross-subject": "KNN*[50]"
        },
        {
          "Table\n6:\nCross-subject": "SVM*[52]",
          "cross-session": "41.20±10.76",
          "leave-one-subject-out": "SA*[29]",
          "cross-validation": "31.87±09.87",
          "Table\n7:\nCross-subject": "SVM*[52]"
        },
        {
          "Table\n6:\nCross-subject": "TCA*[28]",
          "cross-session": "37.68±08.40",
          "leave-one-subject-out": "CORAL*[53]",
          "cross-validation": "54.68±07.44",
          "Table\n7:\nCross-subject": "TCA*[28]"
        },
        {
          "Table\n6:\nCross-subject": "GFK*[31]",
          "cross-session": "37.89±09.84",
          "leave-one-subject-out": "RF*[54]",
          "cross-validation": "43.63±11.38",
          "Table\n7:\nCross-subject": "GFK*[31]"
        },
        {
          "Table\n6:\nCross-subject": "",
          "cross-session": "Deep learning methods",
          "leave-one-subject-out": "",
          "cross-validation": "",
          "Table\n7:\nCross-subject": ""
        },
        {
          "Table\n6:\nCross-subject": "DAN*[55]",
          "cross-session": "54.27±10.42",
          "leave-one-subject-out": "DANN*[56]",
          "cross-validation": "52.83±13.90",
          "Table\n7:\nCross-subject": "DAN*[55]"
        },
        {
          "Table\n6:\nCross-subject": "DCORAL*[57]",
          "cross-session": "52.23±12.76",
          "leave-one-subject-out": "DDC*[58]",
          "cross-validation": "43.89±11.84",
          "Table\n7:\nCross-subject": "DCORAL*[57]"
        },
        {
          "Table\n6:\nCross-subject": "PL-DCP",
          "cross-session": "",
          "leave-one-subject-out": "",
          "cross-validation": "57.53±09.72",
          "Table\n7:\nCross-subject": "PL-DCP"
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "(a)\n(b)": "Figure 5: Confusion matrices of different model settings under cross-subject single-session leave-one-subject-out cross-validation. The Seed-V database contains",
          "(d)\n(c)": ""
        },
        {
          "(a)\n(b)": "five emotion categories: happiness, neutral, sadness, disgust and fear. Among them, (a) PL-DCP; (b) DANN; (c) DAN; (d) DCORAL.",
          "(d)\n(c)": ""
        },
        {
          "(a)\n(b)": "",
          "(d)\n(c)": "Table 8: Results of ablation experiments with the PL-DCP model, expressed as"
        },
        {
          "(a)\n(b)": "the results with deep transfer learning methods. To measure the",
          "(d)\n(c)": ""
        },
        {
          "(a)\n(b)": "",
          "(d)\n(c)": "(Mean-Accuracy%±Standard-Deviation%)"
        },
        {
          "(a)\n(b)": "recognition performance of the model for different emotions.",
          "(d)\n(c)": ""
        },
        {
          "(a)\n(b)": "On the SEED dataset, as shown in Fig.4.(a)∼(d), all mod-",
          "(d)\n(c)": ""
        },
        {
          "(a)\n(b)": "",
          "(d)\n(c)": "Ablation study about training strategy\nPacc(%)"
        },
        {
          "(a)\n(b)": "els have\nthe best performance\nin recognizing positive\nemo-",
          "(d)\n(c)": ""
        },
        {
          "(a)\n(b)": "tion, which are 81.86%,\n80.16%,\n81.50%,\n78.85%,\nrespec-",
          "(d)\n(c)": "w/o domain prototype\n74.67±07.62"
        },
        {
          "(a)\n(b)": "tively.\nIn contrast,\nthe recognition accuracy of negative and",
          "(d)\n(c)": "w/o domain disc.\nloss\n75.24±10.52"
        },
        {
          "(a)\n(b)": "neutral emotions is slightly lower. In addition, the performance",
          "(d)\n(c)": "w/o class disc.\nloss\n79.18±06.62"
        },
        {
          "(a)\n(b)": "difference\nbetween\npositive\nand\nneutral\nemotion\nof DANN",
          "(d)\n(c)": "w/o domain disc.\nloss and class disc.\nloss\n74.49±05.49"
        },
        {
          "(a)\n(b)": "model\nis 8.69%,\nthe performance difference between positive",
          "(d)\n(c)": "w/o pairwise learning\n77.61±09.20"
        },
        {
          "(a)\n(b)": "and neutral\nemotion of DAN model\nis 8.77%,\nand the per-",
          "(d)\n(c)": "w/o the bilinear transformation matrix S\n80.64±04.60"
        },
        {
          "(a)\n(b)": "formance difference between positive and negative emotion of",
          "(d)\n(c)": "w/o soft regularization R\n81.73±03.31"
        },
        {
          "(a)\n(b)": "DCORAL model\nis 10.4%.\nHowever,\nthe maximum perfor-",
          "(d)\n(c)": ""
        },
        {
          "(a)\n(b)": "",
          "(d)\n(c)": "PL-DCP\n82.88±05.32"
        },
        {
          "(a)\n(b)": "mance difference of PL-DCP between emotion categories\nis",
          "(d)\n(c)": ""
        },
        {
          "(a)\n(b)": "only 4.09%. Obviously, PL-DCP showed more robust and bal-",
          "(d)\n(c)": ""
        },
        {
          "(a)\n(b)": "anced performance, while other deep transfer learning methods",
          "(d)\n(c)": "prototype is used, indicating that the domain and class dual pro-"
        },
        {
          "(a)\n(b)": "showed lower stability with fluctuating performance on differ-",
          "(d)\n(c)": "totype representation method has performance advantages com-"
        },
        {
          "(a)\n(b)": "ent emotion categories. On the SEED-IV dataset, as shown in",
          "(d)\n(c)": "pared with the single class prototype representation method."
        },
        {
          "(a)\n(b)": "Fig.4.(e)∼(h), On the SEED-IV dataset,\nthe recognition accu-",
          "(d)\n(c)": "After\nremoving the domain discriminator loss, we observe a"
        },
        {
          "(a)\n(b)": "racy of all models for neutral emotion is slightly higher, reach-",
          "(d)\n(c)": "7.53% decrease in model performance.\nAfter\nremoving the"
        },
        {
          "(a)\n(b)": "ing 76.53%, 65.96%, 64.12% and 61.35%, respectively, while",
          "(d)\n(c)": "class discriminator loss, the model performance decreases from"
        },
        {
          "(a)\n(b)": "the recognition accuracy for happy emotion is slightly lower. In",
          "(d)\n(c)": "82.88% to 79.18%, with a decrease of 3.70%. After\nremov-"
        },
        {
          "(a)\n(b)": "addition, on this dataset, each model is easy to confuse sadness",
          "(d)\n(c)": "ing domain discriminator loss and class discriminator loss, the"
        },
        {
          "(a)\n(b)": "and fear. On the SEED-V dataset, as shown in Fig.5.(a)∼(d),",
          "(d)\n(c)": "model performance decreases significantly by 8.39%, which in-"
        },
        {
          "(a)\n(b)": "all the models have slightly lower recognition accuracy perfor-",
          "(d)\n(c)": "dicates that\nthe interaction of domain discriminator and class"
        },
        {
          "(a)\n(b)": "mance for happy emotions, while they have higher recognition",
          "(d)\n(c)": "discriminator helps to extract relevant features and significantly"
        },
        {
          "(a)\n(b)": "accuracy for neutral, sad, and fear emotions. All models easily",
          "(d)\n(c)": "improves\nthe recognition performance for\nthe target domain."
        },
        {
          "(a)\n(b)": "confuse happy emotions with fear and neutral emotions. Com-",
          "(d)\n(c)": "After removing the pairwise learning strategy and replacing it"
        },
        {
          "(a)\n(b)": "pared with other models, the PL-DCP model has the best perfor-",
          "(d)\n(c)": "with the pointwise learning strategy, the model performance de-"
        },
        {
          "(a)\n(b)": "mance in recognizing neutral emotions. Overall, our proposed",
          "(d)\n(c)": "creases by 5.27%,\nindicating that\nthe paired learning strategy"
        },
        {
          "(a)\n(b)": "PL-DCP model has superior emotion recognition performance",
          "(d)\n(c)": "enhances the recognition performance of the model by captur-"
        },
        {
          "(a)\n(b)": "and stability.",
          "(d)\n(c)": "ing the relationship between sample pairs. Removing bilinear"
        },
        {
          "(a)\n(b)": "",
          "(d)\n(c)": "transformation matrix S ,\nthe model performance decreases by"
        },
        {
          "(a)\n(b)": "",
          "(d)\n(c)": "2.24%, indicating that the bilinear transformation matrix S con-"
        },
        {
          "(a)\n(b)": "5. Discussion and Conclusion",
          "(d)\n(c)": ""
        },
        {
          "(a)\n(b)": "",
          "(d)\n(c)": "tributes to the model performance in the formula Eq. 8. After"
        },
        {
          "(a)\n(b)": "",
          "(d)\n(c)": "removing the soft regularization term R, the model performance"
        },
        {
          "(a)\n(b)": "5.1. Ablation Study",
          "(d)\n(c)": ""
        },
        {
          "(a)\n(b)": "",
          "(d)\n(c)": "decreases by 1.15%."
        },
        {
          "(a)\n(b)": "To comprehensively evaluate the performance of the model",
          "(d)\n(c)": "Overall, All these results demonstrate the effectiveness of the"
        },
        {
          "(a)\n(b)": "and evaluate the impact of each module in the proposed PL-",
          "(d)\n(c)": "individual components in the PL-DCP model and their com-"
        },
        {
          "(a)\n(b)": "DCP model, we performed ablation experiments. Ablation re-",
          "(d)\n(c)": "bined impact on the overall performance."
        },
        {
          "(a)\n(b)": "sults under cross-subject\nsingle-session leave-one-subject-out",
          "(d)\n(c)": ""
        },
        {
          "(a)\n(b)": "",
          "(d)\n(c)": "5.2. Visualization of Domain and Class Features"
        },
        {
          "(a)\n(b)": "cross-validation based on the SEED dataset are shown in Tab.8.",
          "(d)\n(c)": ""
        },
        {
          "(a)\n(b)": "After\nremoving domain prototype,\nit\nis observed that\nthe per-",
          "(d)\n(c)": "To intuitively understand the extracted domain and class fea-"
        },
        {
          "(a)\n(b)": "formance of the model decreases by 8.21% when only the class",
          "(d)\n(c)": "tures, we use T-SNE [62] to visualize these features for the re-"
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "(d)\n(e)\n(f)": "Figure 6: Visualization of domain and class features at different\ntraining stages.\n(a)∼(c) show the domain features, where the diamond shape (⋄) represents the"
        },
        {
          "(d)\n(e)\n(f)": "domain prototype of each domain, different colors represent\nthe domain features from different domains,\nthe same color represents the domain features from the"
        },
        {
          "(d)\n(e)\n(f)": "same domain, and the target domain features are represented as translucent black crosses (×).\n(d)∼(f) show the class features, where different colors represent"
        },
        {
          "(d)\n(e)\n(f)": "different class features. Here, the first column represents the initial stage of training, the second column represents the results after 50 epochs of training, the third"
        },
        {
          "(d)\n(e)\n(f)": "column represents the results end of training."
        },
        {
          "(d)\n(e)\n(f)": "spective samples, enabling us a clear observation of how the\nindicating that the model learns a strong feature alignment abil-"
        },
        {
          "(d)\n(e)\n(f)": "features and prototypes evolve over\ntraining.\nThe visualiza-\nity based on class prototypes during training."
        },
        {
          "(d)\n(e)\n(f)": "tions of domain and class features at different\ntraining stages\nTo further illustrate the relationships between domain proto-"
        },
        {
          "(d)\n(e)\n(f)": "are shown in Fig.6.(a)∼(c) and (d)∼(f), respectively. These fig-\ntypes and class prototypes, we analyze both close pairs and dis-"
        },
        {
          "(d)\n(e)\n(f)": "ures capture the features at\nthe beginning of training (first col-\ntant pairs of domain samples. As shown in Fig.7.(a)∼(b), we vi-"
        },
        {
          "(d)\n(e)\n(f)": "umn), after 50 training epochs (second column), and at the end\nsualizing their respective representations in the class prototype"
        },
        {
          "(d)\n(e)\n(f)": "of training (third column).\nspace.(a) shows that the domain features of the subject pairs are"
        },
        {
          "(d)\n(e)\n(f)": "In the visualization of domain features from (a)∼(c), differ-\nrelatively close, with a high degree of overlap, and the class fea-"
        },
        {
          "(d)\n(e)\n(f)": "ent colors represent the domain features for each domain, while\nture distributions are therefore relatively close.\n(b) shows that"
        },
        {
          "(d)\n(e)\n(f)": "diamonds (⋄) in corresponding colors denote the domain proto-\nthe domain features of the subject pairs are relatively far away,"
        },
        {
          "(d)\n(e)\n(f)": "types for each domain. The target data are represented as semi-\nwith a low degree of overlap, and the class feature distributions"
        },
        {
          "(d)\n(e)\n(f)": "transparent black crosses (×)\nto avoid excessive overlap with\nare therefore relatively far away. The results reveal\nthat Sam-"
        },
        {
          "(d)\n(e)\n(f)": "other domain features. Comparing the feature distribution from\nples that are close to each other in the domain prototype space"
        },
        {
          "(d)\n(e)\n(f)": "Fig.6.(a)∼(c), it is evident that the domain features of the same\nalso tend to remain close in the class prototype space,\nindicat-"
        },
        {
          "(d)\n(e)\n(f)": "subject are clustered more and more closely, forming separate\ning consistency and coherence in the mapping across the two"
        },
        {
          "(d)\n(e)\n(f)": "groups with the domain prototypes (⋄) located at\nthe center of\nprototype spaces. This observation reinforces the effectiveness"
        },
        {
          "(d)\n(e)\n(f)": "each cluster.\nThis differentiation in domain feature distribu-\nof the proposed framework in preserving the intrinsic relation-"
        },
        {
          "(d)\n(e)\n(f)": "tions across subjects supports our hypothesis that domain fea-\nships between samples during dual prototype learning."
        },
        {
          "(d)\n(e)\n(f)": "tures, derived through feature disentanglement of shallow fea-"
        },
        {
          "(d)\n(e)\n(f)": "tures, can effectively distinguish between subjects. A similar\n5.3. Effect of Noisy Labels"
        },
        {
          "(d)\n(e)\n(f)": "trend is observed in the class feature visualization as shown in"
        },
        {
          "(d)\n(e)\n(f)": "We further evaluate the performance of the model under la-"
        },
        {
          "(d)\n(e)\n(f)": "Fig.6.(d)∼(f). Here, different colors represent different classes,"
        },
        {
          "(d)\n(e)\n(f)": "bel noise to evaluate the robustness and noise immunity intro-"
        },
        {
          "(d)\n(e)\n(f)": "with the semi-transparent black crosses (×) again indicating the"
        },
        {
          "(d)\n(e)\n(f)": "duced by pairwise learning. During the experiment,\nthe labels"
        },
        {
          "(d)\n(e)\n(f)": "target data. As the training progresses, clearer class boundaries"
        },
        {
          "(d)\n(e)\n(f)": "of η% (5%, 10%, 20% and 30%) of the source domain data are"
        },
        {
          "(d)\n(e)\n(f)": "between different categories become more and more obvious,"
        },
        {
          "(d)\n(e)\n(f)": "randomly replaced with random error\nlabels, which simulates"
        }
      ],
      "page": 11
    },
    {
      "caption": "Table 9: Results of the PL-DCP model adding different proportions(η%) of emotionrecognitioninunseentargetconditions. Unlikeexist-",
      "data": [
        {
          "Deviation%).": "Noisy Ratio"
        },
        {
          "Deviation%).": "η (%)"
        },
        {
          "Deviation%).": "0%"
        },
        {
          "Deviation%).": "5%"
        },
        {
          "Deviation%).": "10%"
        },
        {
          "Deviation%).": "20%"
        },
        {
          "Deviation%).": "30%"
        }
      ],
      "page": 12
    },
    {
      "caption": "Table 9: Results of the PL-DCP model adding different proportions(η%) of emotionrecognitioninunseentargetconditions. Unlikeexist-",
      "data": [
        {
          "(a)": "Figure 7: A visualization of closer pair (a) and distant pair (b) of domain features in the class prototype space.Here, blue hollow circles (◦) represent domain features",
          "(b)": ""
        },
        {
          "(a)": "from the same subject, and purple hollow triangles (△) represent domain features from another subject. The three clusters with different colors in the figure represent",
          "(b)": ""
        },
        {
          "(a)": "the class feature distribution of the three emotion categories.",
          "(b)": ""
        },
        {
          "(a)": "Table 9: Results of\nthe PL-DCP model adding different proportions(η%) of",
          "(b)": ""
        },
        {
          "(a)": "",
          "(b)": "emotion recognition in unseen target conditions. Unlike exist-"
        },
        {
          "(a)": "label noise to the source domain, expressed as (Mean-Accuracy%±Standard-",
          "(b)": ""
        },
        {
          "(a)": "",
          "(b)": "ing transfer learning methods that require both source and tar-"
        },
        {
          "(a)": "Deviation%).",
          "(b)": ""
        },
        {
          "(a)": "",
          "(b)": "get data for feature alignment, PL-DCP relies solely on source"
        },
        {
          "(a)": "",
          "(b)": "data for model training. Experimental results show that the pro-"
        },
        {
          "(a)": "Noisy Ratio\nPointwise Learning\nPairwise Learning",
          "(b)": ""
        },
        {
          "(a)": "",
          "(b)": "posed method achieves promising results even without using"
        },
        {
          "(a)": "η (%)\nPacc(%)\nPacc(%)",
          "(b)": ""
        },
        {
          "(a)": "",
          "(b)": ""
        },
        {
          "(a)": "0%\n77.61±09.20\n82.88±05.32",
          "(b)": ""
        },
        {
          "(a)": "",
          "(b)": "ing or even surpassing some deep transfer learning models that"
        },
        {
          "(a)": "5%\n76.76±08.64\n81.46±05.54",
          "(b)": ""
        },
        {
          "(a)": "",
          "(b)": "heavily rely on target domain data. This suggests that combin-"
        },
        {
          "(a)": "10%\n73.49±07.96\n80.32±06.39",
          "(b)": ""
        },
        {
          "(a)": "",
          "(b)": "ing feature disentanglement with domain and class prototypes"
        },
        {
          "(a)": "20%\n69.17±08.31\n79.79±06.09",
          "(b)": ""
        },
        {
          "(a)": "",
          "(b)": "helps generalize more reliable and stable characteristics of in-"
        },
        {
          "(a)": "30%\n66.53±06.63\n79.01±07.46",
          "(b)": ""
        },
        {
          "(a)": "",
          "(b)": "Additionally,"
        },
        {
          "(a)": "",
          "(b)": "learning enhances the anti-interference ability of the model"
        },
        {
          "(a)": "",
          "(b)": "These findings underscore the potential of"
        },
        {
          "(a)": "",
          "(b)": "method for practical applications inAffective Brain-Computer"
        },
        {
          "(a)": "the situation that the data labels contain noise in the real scene.",
          "(b)": ""
        },
        {
          "(a)": "",
          "(b)": ""
        },
        {
          "(a)": "We tested the PL-DCP model using cross-subject single-session",
          "(b)": ""
        },
        {
          "(a)": "",
          "(b)": "aBCIs models with better performance and more adaptability."
        },
        {
          "(a)": "leave-one-out cross validation on the SEED database, and com-",
          "(b)": ""
        },
        {
          "(a)": "pared it on pointwise learning and pairwise learning strategies.",
          "(b)": ""
        },
        {
          "(a)": "The results are shown in Tab.9. When using the pointwise",
          "(b)": ""
        },
        {
          "(a)": "learning strategy, The accuracy of\nthe model\nin different pro-",
          "(b)": ""
        },
        {
          "(a)": "",
          "(b)": ""
        },
        {
          "(a)": "portions of label noise was 77.61%, 76.76%, 73.49%, 69.17%,",
          "(b)": ""
        }
      ],
      "page": 12
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "References": "",
          "436–444doi:10.1038/nature14539.": "URL http://dx.doi.org/10.1038/nature14539"
        },
        {
          "References": "",
          "436–444doi:10.1038/nature14539.": "[18] R. Zhou, Z. Zhang, H. Fu, L. Zhang, L. Li, G. Huang, F. Li, X. Yang,"
        },
        {
          "References": "[1]\nS. PARADISO, Affective neuroscience: The foundations of human and",
          "436–444doi:10.1038/nature14539.": ""
        },
        {
          "References": "",
          "436–444doi:10.1038/nature14539.": "Y\n. Dong, Y.-T. Zhang, et al., Pr-pl: A novel prototypical representation"
        },
        {
          "References": "animal emotions, American Journal of Psychiatry 159 (10) (2002) 1805–",
          "436–444doi:10.1038/nature14539.": ""
        },
        {
          "References": "",
          "436–444doi:10.1038/nature14539.": "based pairwise learning framework for emotion recognition using eeg sig-"
        },
        {
          "References": "1805. doi:10.1176/appi.ajp.159.10.1805.",
          "436–444doi:10.1038/nature14539.": ""
        },
        {
          "References": "",
          "436–444doi:10.1038/nature14539.": "nals, IEEE Transactions on Affective Computing 15 (2) (2023) 657–670."
        },
        {
          "References": "[2]\nL. F. Barrett, J. Gross, T. C. Christensen, M. Benvenuto, Knowing what",
          "436–444doi:10.1038/nature14539.": ""
        },
        {
          "References": "",
          "436–444doi:10.1038/nature14539.": "doi:10.1109/TAFFC.2023.3288118."
        },
        {
          "References": "you’re feeling and knowing what\nto do about\nit: Mapping the relation",
          "436–444doi:10.1038/nature14539.": ""
        },
        {
          "References": "",
          "436–444doi:10.1038/nature14539.": "[19] V. Jayaram, M. Alamgir, Y. Altun, B. Scholkopf, M. Grosse-Wentrup,"
        },
        {
          "References": "between emotion differentiation and emotion regulation, Cognition &",
          "436–444doi:10.1038/nature14539.": ""
        },
        {
          "References": "",
          "436–444doi:10.1038/nature14539.": "Transfer learning in brain-computer interfaces,\nIEEE Computational In-"
        },
        {
          "References": "Emotion 15 (6)\n(2001) 713–724.\ndoi:https://doi.org/10.1080/",
          "436–444doi:10.1038/nature14539.": ""
        },
        {
          "References": "",
          "436–444doi:10.1038/nature14539.": "telligence Magazine 11 (1)\n(2016) 20–31.\ndoi:10.1109/MCI.2015."
        },
        {
          "References": "02699930143000239.",
          "436–444doi:10.1038/nature14539.": ""
        },
        {
          "References": "",
          "436–444doi:10.1038/nature14539.": "2501545."
        },
        {
          "References": "[3] W. Ye, Z. Zhang, F. Teng, M. Zhang,\nJ. Wang, D. Ni, F. Li, P. Xu,",
          "436–444doi:10.1038/nature14539.": ""
        },
        {
          "References": "",
          "436–444doi:10.1038/nature14539.": "[20] K. Zhang, N. Robinson, S.-W. Lee, C. Guan, Adaptive transfer learning"
        },
        {
          "References": "Z. Liang, Semi-supervised dual-stream self-attentive adversarial graph",
          "436–444doi:10.1038/nature14539.": ""
        },
        {
          "References": "",
          "436–444doi:10.1038/nature14539.": "for eeg motor imagery classification with deep convolutional neural net-"
        },
        {
          "References": "contrastive\nlearning\nfor\ncross-subject\neeg-based\nemotion\nrecognition,",
          "436–444doi:10.1038/nature14539.": ""
        },
        {
          "References": "",
          "436–444doi:10.1038/nature14539.": "work, Neural Networks 136 (2021) 1–10. doi:https://doi.org/10."
        },
        {
          "References": "IEEE Transactions on Affective Computing 16 (1) (2025) 290–305. doi:",
          "436–444doi:10.1038/nature14539.": ""
        },
        {
          "References": "",
          "436–444doi:10.1038/nature14539.": "1016/j.neunet.2020.12.013."
        },
        {
          "References": "10.1109/TAFFC.2024.3433470.",
          "436–444doi:10.1038/nature14539.": ""
        },
        {
          "References": "",
          "436–444doi:10.1038/nature14539.": "[21] W. Li, W. Huan, S. Shao, B. Hou, A. Song, Ms-fran: A novel multi-source"
        },
        {
          "References": "[4] W. Ye,\nJ. Wang, L. Chen, L. Dai, Z. Sun, Z. Liang, Adaptive\nspa-",
          "436–444doi:10.1038/nature14539.": ""
        },
        {
          "References": "",
          "436–444doi:10.1038/nature14539.": "domain adaptation method for eeg-based emotion recognition, IEEE Jour-"
        },
        {
          "References": "tial–temporal\naware\ngraph\nlearning\nfor\neeg-based\nemotion\nrecogni-",
          "436–444doi:10.1038/nature14539.": ""
        },
        {
          "References": "",
          "436–444doi:10.1038/nature14539.": "nal of Biomedical and Health Informatics 27 (11)\n(2023) 5302–5313."
        },
        {
          "References": "tion, Cyborg and Bionic Systems 6 (2025) 0088.\narXiv:https://",
          "436–444doi:10.1038/nature14539.": ""
        },
        {
          "References": "",
          "436–444doi:10.1038/nature14539.": "doi:10.1109/JBHI.2023.3311338."
        },
        {
          "References": "spj.science.org/doi/pdf/10.34133/cbsystems.0088, doi:10.",
          "436–444doi:10.1038/nature14539.": ""
        },
        {
          "References": "",
          "436–444doi:10.1038/nature14539.": "[22] Y. Peng, H. Liu, W. Kong, F. Nie, B.-L. Lu, A. Cichocki,\nJoint eeg"
        },
        {
          "References": "34133/cbsystems.0088.",
          "436–444doi:10.1038/nature14539.": ""
        },
        {
          "References": "",
          "436–444doi:10.1038/nature14539.": "feature transfer and semisupervised cross-subject emotion recognition,"
        },
        {
          "References": "[5]\nSiddharth, T.-P. Jung, T. J. Sejnowski, Utilizing deep learning towards",
          "436–444doi:10.1038/nature14539.": ""
        },
        {
          "References": "",
          "436–444doi:10.1038/nature14539.": "IEEE Transactions on Industrial\nInformatics 19 (7)\n(2023) 8104–8115."
        },
        {
          "References": "multi-modal\nbio-sensing\nand\nvision-based\naffective\ncomputing,\nIEEE",
          "436–444doi:10.1038/nature14539.": ""
        },
        {
          "References": "",
          "436–444doi:10.1038/nature14539.": "doi:10.1109/TII.2022.3217120."
        },
        {
          "References": "Transactions on Affective Computing 13 (01)\n(2022) 96–107.\ndoi:",
          "436–444doi:10.1038/nature14539.": ""
        },
        {
          "References": "",
          "436–444doi:10.1038/nature14539.": "[23]\nS. J. Pan, Q. Yang, A survey on transfer learning, IEEE Transactions on"
        },
        {
          "References": "10.1109/TAFFC.2019.2916015.",
          "436–444doi:10.1038/nature14539.": ""
        },
        {
          "References": "",
          "436–444doi:10.1038/nature14539.": "Knowledge and Data Engineering 22 (10) (2010) 1345–1359. doi:10."
        },
        {
          "References": "[6]\nE. H. Houssein, A. Hammad, A. A. Ali, Human\nemotion\nrecogni-",
          "436–444doi:10.1038/nature14539.": ""
        },
        {
          "References": "",
          "436–444doi:10.1038/nature14539.": "1109/TKDE.2009.191."
        },
        {
          "References": "tion\nfrom eeg-based\nbrain–computer\ninterface\nusing machine\nlearn-",
          "436–444doi:10.1038/nature14539.": ""
        },
        {
          "References": "",
          "436–444doi:10.1038/nature14539.": "[24] Y. Li, W. Zheng, Y. Zong, Z. Cui, T. Zhang, X. Zhou, A bi-hemisphere"
        },
        {
          "References": "ing:\na\ncomprehensive\nreview, Neural Computing\nand Applications",
          "436–444doi:10.1038/nature14539.": ""
        },
        {
          "References": "",
          "436–444doi:10.1038/nature14539.": "domain adversarial neural network model\nfor eeg emotion recognition,"
        },
        {
          "References": "34\n(15)\n(2022)\n12527–12557.\ndoi:https://doi.org/10.1007/",
          "436–444doi:10.1038/nature14539.": ""
        },
        {
          "References": "",
          "436–444doi:10.1038/nature14539.": "IEEE Transactions on Affective Computing 12 (2) (2021) 494–504. doi:"
        },
        {
          "References": "s00521-022-07292-4.",
          "436–444doi:10.1038/nature14539.": ""
        },
        {
          "References": "",
          "436–444doi:10.1038/nature14539.": "10.1109/TAFFC.2018.2885474."
        },
        {
          "References": "[7] W. Hu, G. Huang, L. Li, L. Zhang, Z. Zhang, Z. Liang, Video-triggered",
          "436–444doi:10.1038/nature14539.": ""
        },
        {
          "References": "",
          "436–444doi:10.1038/nature14539.": "[25]\nJ. Li, S. Qiu, Y.-Y. Shen, C.-L. Liu, H. He, Multisource transfer learning"
        },
        {
          "References": "eeg-emotion public databases and current methods: A survey, Brain Sci-",
          "436–444doi:10.1038/nature14539.": ""
        },
        {
          "References": "",
          "436–444doi:10.1038/nature14539.": "for cross-subject eeg emotion recognition, IEEE transactions on cybernet-"
        },
        {
          "References": "ence Advances 6 (3)\n(2020) 255–287.\ndoi:https://doi.org/10.",
          "436–444doi:10.1038/nature14539.": ""
        },
        {
          "References": "",
          "436–444doi:10.1038/nature14539.": "ics 50 (7) (2019) 3281–3293. doi:10.1109/TCYB.2019.2904052."
        },
        {
          "References": "26599/BSA.2020.9050026.",
          "436–444doi:10.1038/nature14539.": ""
        },
        {
          "References": "",
          "436–444doi:10.1038/nature14539.": "[26] H. Chen, M.\nJin, Z. Li, C. Fan,\nJ. Li, H. He, Ms-mda: Multisource"
        },
        {
          "References": "[8]\nS. M. Alarcao, M. J. Fonseca, Emotions recognition using eeg signals: A",
          "436–444doi:10.1038/nature14539.": ""
        },
        {
          "References": "",
          "436–444doi:10.1038/nature14539.": "marginal distribution adaptation for cross-subject and cross-session eeg"
        },
        {
          "References": "survey,\nIEEE Transactions on Affective Computing 10 (3)\n(2017) 374–",
          "436–444doi:10.1038/nature14539.": ""
        },
        {
          "References": "",
          "436–444doi:10.1038/nature14539.": "emotion recognition, Frontiers in Neuroscience 15 (2021) 778488. doi:"
        },
        {
          "References": "393. doi:10.1109/TAFFC.2017.2714671.",
          "436–444doi:10.1038/nature14539.": ""
        },
        {
          "References": "",
          "436–444doi:10.1038/nature14539.": "https://doi.org/10.3389/fnins.2021.778488."
        },
        {
          "References": "[9]\nL. Feng, C. Cheng, M. Zhao, H. Deng, Y. Zhang, Eeg-based emotion",
          "436–444doi:10.1038/nature14539.": ""
        },
        {
          "References": "",
          "436–444doi:10.1038/nature14539.": "[27] X. Gu, W.\nCai, M. Gao,\nY\n.\nJiang,\nX. Ning,\nP. Qian, Multi-"
        },
        {
          "References": "recognition using spatial-temporal graph convolutional\nlstm with atten-",
          "436–444doi:10.1038/nature14539.": ""
        },
        {
          "References": "",
          "436–444doi:10.1038/nature14539.": "source domain transfer discriminative dictionary learning modeling for"
        },
        {
          "References": "tion mechanism,\nIEEE Journal of Biomedical and Health Informatics",
          "436–444doi:10.1038/nature14539.": ""
        },
        {
          "References": "",
          "436–444doi:10.1038/nature14539.": "electroencephalogram-based emotion recognition, IEEE Transactions on"
        },
        {
          "References": "(2022) 5406–5417doi:10.1109/jbhi.2022.3198688.",
          "436–444doi:10.1038/nature14539.": ""
        },
        {
          "References": "",
          "436–444doi:10.1038/nature14539.": "Computational Social Systems 9 (6) (2022) 1604–1612. doi:10.1109/"
        },
        {
          "References": "URL http://dx.doi.org/10.1109/jbhi.2022.3198688",
          "436–444doi:10.1038/nature14539.": ""
        },
        {
          "References": "",
          "436–444doi:10.1038/nature14539.": "TCSS.2022.3153660."
        },
        {
          "References": "[10] Y. Yang, Z. Wang, W. Tao, X. Liu, Z. Jia, B. Wang, F. Wan, Spectral-",
          "436–444doi:10.1038/nature14539.": ""
        },
        {
          "References": "",
          "436–444doi:10.1038/nature14539.": "[28]\nS. J. Pan, I. W. Tsang, J. T. Kwok, Q. Yang, Domain adaptation via trans-"
        },
        {
          "References": "spatial attention alignment\nfor multi-source domain adaptation in eeg-",
          "436–444doi:10.1038/nature14539.": ""
        },
        {
          "References": "",
          "436–444doi:10.1038/nature14539.": "fer component analysis,\nIEEE Transactions on Neural Networks 22 (2)"
        },
        {
          "References": "based emotion recognition,\nIEEE Transactions on Affective Computing",
          "436–444doi:10.1038/nature14539.": ""
        },
        {
          "References": "",
          "436–444doi:10.1038/nature14539.": "(2011) 199–210. doi:10.1109/TNN.2010.2091281."
        },
        {
          "References": "15 (4) (2024) 2012–2024. doi:10.1109/TAFFC.2024.3394436.",
          "436–444doi:10.1038/nature14539.": ""
        },
        {
          "References": "",
          "436–444doi:10.1038/nature14539.": "[29] B. Fernando, A. Habrard, M. Sebban, T. Tuytelaars, Unsupervised visual"
        },
        {
          "References": "[11] D. Zhang, L. Yao, X. Zhang, S. Wang, W. Chen, R. Boots, Cascade and",
          "436–444doi:10.1038/nature14539.": ""
        },
        {
          "References": "",
          "436–444doi:10.1038/nature14539.": "domain adaptation using subspace alignment, in: 2013 IEEE International"
        },
        {
          "References": "parallel convolutional\nrecurrent neural networks on eeg-based intention",
          "436–444doi:10.1038/nature14539.": ""
        },
        {
          "References": "",
          "436–444doi:10.1038/nature14539.": "Conference on Computer Vision, 2013, pp. 2960–2967. doi:10.1109/"
        },
        {
          "References": "recognition for brain computer interface,\nin: 32nd AAAI Conference on",
          "436–444doi:10.1038/nature14539.": ""
        },
        {
          "References": "",
          "436–444doi:10.1038/nature14539.": "ICCV.2013.368."
        },
        {
          "References": "Artificial Intelligence, AAAI 2018, AAAI Press, United States, 2018, pp.",
          "436–444doi:10.1038/nature14539.": ""
        },
        {
          "References": "",
          "436–444doi:10.1038/nature14539.": "[30] W.-L. Zheng, B.-L. Lu, Personalizing eeg-based affective models with"
        },
        {
          "References": "1703–1710. doi:10.1609/aaai.v32i1.11496.",
          "436–444doi:10.1038/nature14539.": ""
        },
        {
          "References": "",
          "436–444doi:10.1038/nature14539.": "transfer\nlearning,\nin: Proceedings of\nthe twenty-fifth international\njoint"
        },
        {
          "References": "URL http://dx.doi.org/10.1609/aaai.v32i1.11496",
          "436–444doi:10.1038/nature14539.": ""
        },
        {
          "References": "",
          "436–444doi:10.1038/nature14539.": "conference on artificial\nintelligence, 2016, pp. 2732–2738.\ndoi:10."
        },
        {
          "References": "[12]\nJ. Berkhout, D. O. Walter, Temporal stability and individual differences",
          "436–444doi:10.1038/nature14539.": ""
        },
        {
          "References": "",
          "436–444doi:10.1038/nature14539.": "5555/3060832.3061003."
        },
        {
          "References": "in the human eeg: An analysis of variance of\nspectral values,\nIEEE",
          "436–444doi:10.1038/nature14539.": ""
        },
        {
          "References": "",
          "436–444doi:10.1038/nature14539.": "[31] B. Gong, Y. Shi, F. Sha, K. Grauman, Geodesic flow kernel for unsuper-"
        },
        {
          "References": "Transactions on Biomedical Engineering (3)\n(1968) 165–168.\ndoi:",
          "436–444doi:10.1038/nature14539.": ""
        },
        {
          "References": "",
          "436–444doi:10.1038/nature14539.": "vised domain adaptation, in: 2012 IEEE Conference on Computer Vision"
        },
        {
          "References": "10.1109/tbme.1968.4502560.",
          "436–444doi:10.1038/nature14539.": ""
        },
        {
          "References": "",
          "436–444doi:10.1038/nature14539.": "and Pattern Recognition, 2012, pp. 2066–2073.\ndoi:10.1109/CVPR."
        },
        {
          "References": "[13] R. Paranjape,\nJ. Mahovsky, L. Benedicenti, Z. Koles, The\nelectroen-",
          "436–444doi:10.1038/nature14539.": ""
        },
        {
          "References": "",
          "436–444doi:10.1038/nature14539.": "2012.6247911."
        },
        {
          "References": "cephalogram as\na\nbiometric,\nin:\nCanadian Conference\non Electrical",
          "436–444doi:10.1038/nature14539.": ""
        },
        {
          "References": "",
          "436–444doi:10.1038/nature14539.": "[32] Y. Yin, X. Zheng, B. Hu, Y. Zhang, X. Cui, Eeg emotion recognition using"
        },
        {
          "References": "and Computer Engineering\n2001. Conference Proceedings\n(Cat. No.",
          "436–444doi:10.1038/nature14539.": ""
        },
        {
          "References": "",
          "436–444doi:10.1038/nature14539.": "fusion model of graph convolutional neural networks and lstm, Applied"
        },
        {
          "References": "01TH8555), Vol. 2, IEEE, 2001, pp. 1363–1366. doi:10.1109/CCECE.",
          "436–444doi:10.1038/nature14539.": ""
        },
        {
          "References": "",
          "436–444doi:10.1038/nature14539.": "Soft Computing (2021) 106954.doi:10.1016/j.asoc.2020.106954."
        },
        {
          "References": "2001.933649.",
          "436–444doi:10.1038/nature14539.": ""
        },
        {
          "References": "",
          "436–444doi:10.1038/nature14539.": "[33] Y.-M. Jin, Y.-D. Luo, W.-L. Zheng, B.-L. Lu, Eeg-based emotion recog-"
        },
        {
          "References": "[14]\nJ. J. Gross, O. P. John, Revealing feelings: facets of emotional expressiv-",
          "436–444doi:10.1038/nature14539.": ""
        },
        {
          "References": "",
          "436–444doi:10.1038/nature14539.": "nition using domain adaptation network,\nin:\n2017 International Con-"
        },
        {
          "References": "ity in self-reports, peer ratings, and behavior., Journal of personality and",
          "436–444doi:10.1038/nature14539.": ""
        },
        {
          "References": "",
          "436–444doi:10.1038/nature14539.": "ference on Orange Technologies\n(ICOT), 2017,\npp. 222–225.\ndoi:"
        },
        {
          "References": "social psychology 72 (2) (1997) 435. doi:10.1037//0022-3514.72.",
          "436–444doi:10.1038/nature14539.": ""
        },
        {
          "References": "",
          "436–444doi:10.1038/nature14539.": "10.1109/ICOT.2017.8336126."
        },
        {
          "References": "2.435.",
          "436–444doi:10.1038/nature14539.": ""
        },
        {
          "References": "",
          "436–444doi:10.1038/nature14539.": "[34] W. Zhang, F. Wang, Y.\nJiang, Z. Xu, S. Wu, Y. Zhang, Cross-subject"
        },
        {
          "References": "[15] C. Morawetz, U. Basten, Neural underpinnings of individual differences",
          "436–444doi:10.1038/nature14539.": ""
        },
        {
          "References": "",
          "436–444doi:10.1038/nature14539.": "eeg-based emotion recognition with deep domain confusion,\nin:\nIn-"
        },
        {
          "References": "in emotion regulation:\nA systematic\nreview, Neuroscience & Biobe-",
          "436–444doi:10.1038/nature14539.": ""
        },
        {
          "References": "",
          "436–444doi:10.1038/nature14539.": "telligent Robotics\nand Applications:\n12th\nInternational Conference,"
        },
        {
          "References": "havioral Reviews (2024) 105727doi:https://doi.org/10.1016/j.",
          "436–444doi:10.1038/nature14539.": ""
        },
        {
          "References": "",
          "436–444doi:10.1038/nature14539.": "ICIRA 2019, Shenyang, China, August 8–11, 2019, Proceedings, Part"
        },
        {
          "References": "neubiorev.2024.105727.",
          "436–444doi:10.1038/nature14539.": ""
        },
        {
          "References": "",
          "436–444doi:10.1038/nature14539.": "I 12, Springer, 2019, pp. 558–570. doi:https://doi.org/10.1007/"
        },
        {
          "References": "[16]\nS.\nShalev-Shwartz,\nS. Ben-David, Understanding machine\nlearning:",
          "436–444doi:10.1038/nature14539.": ""
        },
        {
          "References": "",
          "436–444doi:10.1038/nature14539.": "978-3-030-27526-6_49."
        },
        {
          "References": "From theory to algorithms, Cambridge university press, 2014.",
          "436–444doi:10.1038/nature14539.": ""
        },
        {
          "References": "",
          "436–444doi:10.1038/nature14539.": "[35]\nT. Gokhale, R. Anirudh, B. Kailkhura,\nJ.\nJ. Thiagarajan, C. Baral,"
        },
        {
          "References": "[17] Y.\nLeCun,\nY\n. Bengio,\nG. Hinton,\nDeep\nlearning,\nNature\n(2015)",
          "436–444doi:10.1038/nature14539.": ""
        }
      ],
      "page": 13
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Y\n. Yang, Attribute-guided adversarial\ntraining for\nrobustness to natural": "perturbations, Proceedings of the AAAI Conference on Artificial Intelli-",
          "[53] B. Sun, J. Feng, K. Saenko, Return of\nfrustratingly easy domain adap-": "tation,\nin: Proceedings of\nthe Thirtieth AAAI Conference on Artificial"
        },
        {
          "Y\n. Yang, Attribute-guided adversarial\ntraining for\nrobustness to natural": "gence (2022) 7574–7582doi:10.1609/aaai.v35i9.16927.",
          "[53] B. Sun, J. Feng, K. Saenko, Return of\nfrustratingly easy domain adap-": "Intelligence, AAAI’16, AAAI Press, 2016, p. 2058–2065."
        },
        {
          "Y\n. Yang, Attribute-guided adversarial\ntraining for\nrobustness to natural": "[36]\nJ. Snell, K. Swersky, R. Zemel, Prototypical networks for few-shot learn-",
          "[53] B. Sun, J. Feng, K. Saenko, Return of\nfrustratingly easy domain adap-": "[54]\nL. Breiman, Random forests, Machine learning 45 (2001) 5–32.\ndoi:"
        },
        {
          "Y\n. Yang, Attribute-guided adversarial\ntraining for\nrobustness to natural": "ing, in: Proceedings of the 31st International Conference on Neural Infor-",
          "[53] B. Sun, J. Feng, K. Saenko, Return of\nfrustratingly easy domain adap-": "https://doi.org/10.1023/A:1010933404324."
        },
        {
          "Y\n. Yang, Attribute-guided adversarial\ntraining for\nrobustness to natural": "mation Processing Systems, NIPS’17, Curran Associates Inc., Red Hook,",
          "[53] B. Sun, J. Feng, K. Saenko, Return of\nfrustratingly easy domain adap-": "[55] H. Li, Y.-M. Jin, W.-L. Zheng, B.-L. Lu, Cross-subject emotion recog-"
        },
        {
          "Y\n. Yang, Attribute-guided adversarial\ntraining for\nrobustness to natural": "NY, USA, 2017, p. 4080–4090. doi:10.5555/3294996.3295163.",
          "[53] B. Sun, J. Feng, K. Saenko, Return of\nfrustratingly easy domain adap-": "nition using deep adaptation networks,\nin: L. Cheng, A. C. S. Leung,"
        },
        {
          "Y\n. Yang, Attribute-guided adversarial\ntraining for\nrobustness to natural": "[37]\nP. O. Pinheiro, Unsupervised domain adaptation with similarity learning,",
          "[53] B. Sun, J. Feng, K. Saenko, Return of\nfrustratingly easy domain adap-": "S. Ozawa (Eds.), Neural Information Processing, Springer International"
        },
        {
          "Y\n. Yang, Attribute-guided adversarial\ntraining for\nrobustness to natural": "in: 2018 IEEE/CVF Conference on Computer Vision and Pattern Recog-",
          "[53] B. Sun, J. Feng, K. Saenko, Return of\nfrustratingly easy domain adap-": "Publishing, Cham, 2018, pp. 403–413.\ndoi:https://doi.org/10."
        },
        {
          "Y\n. Yang, Attribute-guided adversarial\ntraining for\nrobustness to natural": "nition, 2018, pp. 8004–8013. doi:10.1109/CVPR.2018.00835.",
          "[53] B. Sun, J. Feng, K. Saenko, Return of\nfrustratingly easy domain adap-": "1007/978-3-030-04221-9_36."
        },
        {
          "Y\n. Yang, Attribute-guided adversarial\ntraining for\nrobustness to natural": "[38]\nZ. Ji, X. Liu, Y. Pang, W. Ouyang, X. Li, Few-shot human-object\nin-",
          "[53] B. Sun, J. Feng, K. Saenko, Return of\nfrustratingly easy domain adap-": "[56]\nJ. Li, S. Qiu, C. Du, Y. Wang, H. He, Domain adaptation for eeg emotion"
        },
        {
          "Y\n. Yang, Attribute-guided adversarial\ntraining for\nrobustness to natural": "teraction recognition with semantic-guided attentive prototypes network,",
          "[53] B. Sun, J. Feng, K. Saenko, Return of\nfrustratingly easy domain adap-": "recognition based on latent representation similarity,\nIEEE Transactions"
        },
        {
          "Y\n. Yang, Attribute-guided adversarial\ntraining for\nrobustness to natural": "IEEE Transactions on Image Processing 30 (2020) 1648–1661.\ndoi:",
          "[53] B. Sun, J. Feng, K. Saenko, Return of\nfrustratingly easy domain adap-": "on Cognitive and Developmental Systems 12 (2) (2020) 344–353. doi:"
        },
        {
          "Y\n. Yang, Attribute-guided adversarial\ntraining for\nrobustness to natural": "10.1109/TIP.2020.3046861.",
          "[53] B. Sun, J. Feng, K. Saenko, Return of\nfrustratingly easy domain adap-": "10.1109/TCDS.2019.2949306."
        },
        {
          "Y\n. Yang, Attribute-guided adversarial\ntraining for\nrobustness to natural": "[39] Q. Liu, J. Peng, Y. Ning, N. Chen, W. Sun, Q. Du, Y. Zhou, Refined pro-",
          "[53] B. Sun, J. Feng, K. Saenko, Return of\nfrustratingly easy domain adap-": "[57] B. Sun, K. Saenko, Deep coral: Correlation alignment for deep domain"
        },
        {
          "Y\n. Yang, Attribute-guided adversarial\ntraining for\nrobustness to natural": "totypical contrastive learning for few-shot hyperspectral image classifica-",
          "[53] B. Sun, J. Feng, K. Saenko, Return of\nfrustratingly easy domain adap-": "adaptation,\nin: G. Hua, H. J´egou (Eds.), Computer Vision – ECCV 2016"
        },
        {
          "Y\n. Yang, Attribute-guided adversarial\ntraining for\nrobustness to natural": "tion,\nIEEE Transactions on Geoscience and Remote Sensing 61 (2023)",
          "[53] B. Sun, J. Feng, K. Saenko, Return of\nfrustratingly easy domain adap-": "Workshops, Springer International Publishing, Cham, 2016, pp. 443–450."
        },
        {
          "Y\n. Yang, Attribute-guided adversarial\ntraining for\nrobustness to natural": "1–14. doi:10.1109/TGRS.2023.3257341.",
          "[53] B. Sun, J. Feng, K. Saenko, Return of\nfrustratingly easy domain adap-": "[58]\nE. Tzeng, J. Hoffman, N. Zhang, K. Saenko, T. Darrell, Deep domain con-"
        },
        {
          "Y\n. Yang, Attribute-guided adversarial\ntraining for\nrobustness to natural": "[40] X. Yang, M. Han, Y. Luo, H. Hu, Y. Wen, Two-stream prototype learning",
          "[53] B. Sun, J. Feng, K. Saenko, Return of\nfrustratingly easy domain adap-": "fusion: Maximizing for domain invariance, ArXiv abs/1412.3474 (2014)."
        },
        {
          "Y\n. Yang, Attribute-guided adversarial\ntraining for\nrobustness to natural": "network for\nfew-shot\nface recognition under occlusions,\nIEEE Transac-",
          "[53] B. Sun, J. Feng, K. Saenko, Return of\nfrustratingly easy domain adap-": "URL https://api.semanticscholar.org/CorpusID:17169365"
        },
        {
          "Y\n. Yang, Attribute-guided adversarial\ntraining for\nrobustness to natural": "tions on Multimedia 25 (2023) 1555–1563. doi:10.1109/TMM.2023.",
          "[53] B. Sun, J. Feng, K. Saenko, Return of\nfrustratingly easy domain adap-": "[59]\nT. Song, W. Zheng, P. Song, Z. Cui, Eeg emotion recognition using dy-"
        },
        {
          "Y\n. Yang, Attribute-guided adversarial\ntraining for\nrobustness to natural": "3253054.",
          "[53] B. Sun, J. Feng, K. Saenko, Return of\nfrustratingly easy domain adap-": "namical graph convolutional neural networks, IEEE Transactions on Af-"
        },
        {
          "Y\n. Yang, Attribute-guided adversarial\ntraining for\nrobustness to natural": "[41] X. Peng, Z. Huang, X. Sun, K. Saenko, Domain agnostic learning with",
          "[53] B. Sun, J. Feng, K. Saenko, Return of\nfrustratingly easy domain adap-": "fective Computing 11 (3) (2020) 532–541. doi:10.1109/TAFFC.2018."
        },
        {
          "Y\n. Yang, Attribute-guided adversarial\ntraining for\nrobustness to natural": "disentangled representations,\nin: K. Chaudhuri, R. Salakhutdinov (Eds.),",
          "[53] B. Sun, J. Feng, K. Saenko, Return of\nfrustratingly easy domain adap-": "2817622."
        },
        {
          "Y\n. Yang, Attribute-guided adversarial\ntraining for\nrobustness to natural": "Proceedings of the 36th International Conference on Machine Learning,",
          "[53] B. Sun, J. Feng, K. Saenko, Return of\nfrustratingly easy domain adap-": "[60] D. Sejdinovic, B. Sriperumbudur, A. Gretton, K. Fukumizu, Equivalence"
        },
        {
          "Y\n. Yang, Attribute-guided adversarial\ntraining for\nrobustness to natural": "Vol. 97 of Proceedings of Machine Learning Research, PMLR, 2019, pp.",
          "[53] B. Sun, J. Feng, K. Saenko, Return of\nfrustratingly easy domain adap-": "of distance-based and rkhs-based statistics in hypothesis testing, Annals"
        },
        {
          "Y\n. Yang, Attribute-guided adversarial\ntraining for\nrobustness to natural": "5102–5112.",
          "[53] B. Sun, J. Feng, K. Saenko, Return of\nfrustratingly easy domain adap-": "of Stats 41 (5) (2013) 2263–2291. doi:10.1214/13-AOS1140."
        },
        {
          "Y\n. Yang, Attribute-guided adversarial\ntraining for\nrobustness to natural": "[42] R. Cai, Z. Li, P. Wei, J. Qiao, K. Zhang, Z. Hao, Learning disentangled",
          "[53] B. Sun, J. Feng, K. Saenko, Return of\nfrustratingly easy domain adap-": "[61] Y. Li, W. Zheng, L. Wang, Y. Zong, Z. Cui, From regional\nto global"
        },
        {
          "Y\n. Yang, Attribute-guided adversarial\ntraining for\nrobustness to natural": "semantic representation for domain adaptation,\nin:\nProceedings of\nthe",
          "[53] B. Sun, J. Feng, K. Saenko, Return of\nfrustratingly easy domain adap-": "brain: A novel hierarchical spatial-temporal neural network model for eeg"
        },
        {
          "Y\n. Yang, Attribute-guided adversarial\ntraining for\nrobustness to natural": "28th International Joint Conference on Artificial Intelligence, IJCAI’19,",
          "[53] B. Sun, J. Feng, K. Saenko, Return of\nfrustratingly easy domain adap-": "emotion recognition,\nIEEE Transactions on Affective Computing 13 (2)"
        },
        {
          "Y\n. Yang, Attribute-guided adversarial\ntraining for\nrobustness to natural": "AAAI Press, 2019, p. 2060–2066.",
          "[53] B. Sun, J. Feng, K. Saenko, Return of\nfrustratingly easy domain adap-": "(2022) 568–578. doi:10.1109/TAFFC.2019.2922912."
        },
        {
          "Y\n. Yang, Attribute-guided adversarial\ntraining for\nrobustness to natural": "[43] H. T. Schupp, B. N. Cuthbert, M. M. Bradley, J. T. Cacioppo, T. Ito, P. J.",
          "[53] B. Sun, J. Feng, K. Saenko, Return of\nfrustratingly easy domain adap-": "[62]\nL. van der Maaten, G. Hinton, Visualizing data using t-sne,\nJournal of"
        },
        {
          "Y\n. Yang, Attribute-guided adversarial\ntraining for\nrobustness to natural": "Lang, Affective picture processing: The late positive potential\nis modu-",
          "[53] B. Sun, J. Feng, K. Saenko, Return of\nfrustratingly easy domain adap-": "Machine Learning Research 9 (86) (2008) 2579–2605."
        },
        {
          "Y\n. Yang, Attribute-guided adversarial\ntraining for\nrobustness to natural": "lated by motivational relevance, Psychophysiology (2000) 257–261doi:",
          "[53] B. Sun, J. Feng, K. Saenko, Return of\nfrustratingly easy domain adap-": "URL http://jmlr.org/papers/v9/vandermaaten08a.html"
        },
        {
          "Y\n. Yang, Attribute-guided adversarial\ntraining for\nrobustness to natural": "10.1111/1469-8986.3720257.",
          "[53] B. Sun, J. Feng, K. Saenko, Return of\nfrustratingly easy domain adap-": ""
        },
        {
          "Y\n. Yang, Attribute-guided adversarial\ntraining for\nrobustness to natural": "[44]\nP. A. Vald´es-Hern´andez,\nN.\nvon\nEllenrieder,\nA. Ojeda-Gonzalez,",
          "[53] B. Sun, J. Feng, K. Saenko, Return of\nfrustratingly easy domain adap-": ""
        },
        {
          "Y\n. Yang, Attribute-guided adversarial\ntraining for\nrobustness to natural": "S. Kochen, Y. Alem´an-G´omez, C. Muravchik, P. A. Vald´es-Sosa, Ap-",
          "[53] B. Sun, J. Feng, K. Saenko, Return of\nfrustratingly easy domain adap-": ""
        },
        {
          "Y\n. Yang, Attribute-guided adversarial\ntraining for\nrobustness to natural": "proximate average head models for eeg source imaging, Journal of Neuro-",
          "[53] B. Sun, J. Feng, K. Saenko, Return of\nfrustratingly easy domain adap-": ""
        },
        {
          "Y\n. Yang, Attribute-guided adversarial\ntraining for\nrobustness to natural": "science Methods 185 (1) (2009) 125–132. doi:10.1016/j.jneumeth.",
          "[53] B. Sun, J. Feng, K. Saenko, Return of\nfrustratingly easy domain adap-": ""
        },
        {
          "Y\n. Yang, Attribute-guided adversarial\ntraining for\nrobustness to natural": "2009.09.005.",
          "[53] B. Sun, J. Feng, K. Saenko, Return of\nfrustratingly easy domain adap-": ""
        },
        {
          "Y\n. Yang, Attribute-guided adversarial\ntraining for\nrobustness to natural": "[45]\nJ. Haueisen, D. Tuch, C. Ramon, P. Schimpf, V. Wedeen,\nJ. George,",
          "[53] B. Sun, J. Feng, K. Saenko, Return of\nfrustratingly easy domain adap-": ""
        },
        {
          "Y\n. Yang, Attribute-guided adversarial\ntraining for\nrobustness to natural": "J. Belliveau, The influence of brain tissue anisotropy on human eeg and",
          "[53] B. Sun, J. Feng, K. Saenko, Return of\nfrustratingly easy domain adap-": ""
        },
        {
          "Y\n. Yang, Attribute-guided adversarial\ntraining for\nrobustness to natural": "meg, NeuroImage 15 (1) (2002) 159–166. doi:10.1006/nimg.2001.",
          "[53] B. Sun, J. Feng, K. Saenko, Return of\nfrustratingly easy domain adap-": ""
        },
        {
          "Y\n. Yang, Attribute-guided adversarial\ntraining for\nrobustness to natural": "0962.",
          "[53] B. Sun, J. Feng, K. Saenko, Return of\nfrustratingly easy domain adap-": ""
        },
        {
          "Y\n. Yang, Attribute-guided adversarial\ntraining for\nrobustness to natural": "[46] W.-L. Zheng, B.-L. Lu, Investigating critical frequency bands and chan-",
          "[53] B. Sun, J. Feng, K. Saenko, Return of\nfrustratingly easy domain adap-": ""
        },
        {
          "Y\n. Yang, Attribute-guided adversarial\ntraining for\nrobustness to natural": "nels for eeg-based emotion recognition with deep neural networks, IEEE",
          "[53] B. Sun, J. Feng, K. Saenko, Return of\nfrustratingly easy domain adap-": ""
        },
        {
          "Y\n. Yang, Attribute-guided adversarial\ntraining for\nrobustness to natural": "Transactions on Autonomous Mental Development 7 (3) (2015) 162–175.",
          "[53] B. Sun, J. Feng, K. Saenko, Return of\nfrustratingly easy domain adap-": ""
        },
        {
          "Y\n. Yang, Attribute-guided adversarial\ntraining for\nrobustness to natural": "doi:10.1109/TAMD.2015.2431497.",
          "[53] B. Sun, J. Feng, K. Saenko, Return of\nfrustratingly easy domain adap-": ""
        },
        {
          "Y\n. Yang, Attribute-guided adversarial\ntraining for\nrobustness to natural": "[47] W.-L. Zheng, W. Liu, Y. Lu, B.-L. Lu, A. Cichocki, Emotionmeter: A",
          "[53] B. Sun, J. Feng, K. Saenko, Return of\nfrustratingly easy domain adap-": ""
        },
        {
          "Y\n. Yang, Attribute-guided adversarial\ntraining for\nrobustness to natural": "multimodal framework for recognizing human emotions, IEEE Transac-",
          "[53] B. Sun, J. Feng, K. Saenko, Return of\nfrustratingly easy domain adap-": ""
        },
        {
          "Y\n. Yang, Attribute-guided adversarial\ntraining for\nrobustness to natural": "tions on Cybernetics 49 (3)\n(2019) 1110–1122.\ndoi:10.1109/TCYB.",
          "[53] B. Sun, J. Feng, K. Saenko, Return of\nfrustratingly easy domain adap-": ""
        },
        {
          "Y\n. Yang, Attribute-guided adversarial\ntraining for\nrobustness to natural": "2018.2797176.",
          "[53] B. Sun, J. Feng, K. Saenko, Return of\nfrustratingly easy domain adap-": ""
        },
        {
          "Y\n. Yang, Attribute-guided adversarial\ntraining for\nrobustness to natural": "[48] Y. Li, W. Zheng, Y. Zong, Z. Cui, T. Zhang, X. Zhou, A bi-hemisphere",
          "[53] B. Sun, J. Feng, K. Saenko, Return of\nfrustratingly easy domain adap-": ""
        },
        {
          "Y\n. Yang, Attribute-guided adversarial\ntraining for\nrobustness to natural": "domain adversarial neural network model\nfor eeg emotion recognition,",
          "[53] B. Sun, J. Feng, K. Saenko, Return of\nfrustratingly easy domain adap-": ""
        },
        {
          "Y\n. Yang, Attribute-guided adversarial\ntraining for\nrobustness to natural": "IEEE Transactions on Affective Computing (2021) 494–504doi:10.",
          "[53] B. Sun, J. Feng, K. Saenko, Return of\nfrustratingly easy domain adap-": ""
        },
        {
          "Y\n. Yang, Attribute-guided adversarial\ntraining for\nrobustness to natural": "1109/taffc.2018.2885474.",
          "[53] B. Sun, J. Feng, K. Saenko, Return of\nfrustratingly easy domain adap-": ""
        },
        {
          "Y\n. Yang, Attribute-guided adversarial\ntraining for\nrobustness to natural": "[49]\nL.-C. Shi, B.-L. Lu, Off-line and on-line vigilance estimation based on",
          "[53] B. Sun, J. Feng, K. Saenko, Return of\nfrustratingly easy domain adap-": ""
        },
        {
          "Y\n. Yang, Attribute-guided adversarial\ntraining for\nrobustness to natural": "linear dynamical system and manifold learning,\nin: 2010 Annual\nInter-",
          "[53] B. Sun, J. Feng, K. Saenko, Return of\nfrustratingly easy domain adap-": ""
        },
        {
          "Y\n. Yang, Attribute-guided adversarial\ntraining for\nrobustness to natural": "national Conference of the IEEE Engineering in Medicine and Biology,",
          "[53] B. Sun, J. Feng, K. Saenko, Return of\nfrustratingly easy domain adap-": ""
        },
        {
          "Y\n. Yang, Attribute-guided adversarial\ntraining for\nrobustness to natural": "2010, pp. 6587–6590. doi:10.1109/IEMBS.2010.5627125.",
          "[53] B. Sun, J. Feng, K. Saenko, Return of\nfrustratingly easy domain adap-": ""
        },
        {
          "Y\n. Yang, Attribute-guided adversarial\ntraining for\nrobustness to natural": "[50] D. Coomans, D. L. Massart, Alternative k-nearest neighbour rules in su-",
          "[53] B. Sun, J. Feng, K. Saenko, Return of\nfrustratingly easy domain adap-": ""
        },
        {
          "Y\n. Yang, Attribute-guided adversarial\ntraining for\nrobustness to natural": "pervised pattern recognition: Part 1. k-nearest neighbour classification by",
          "[53] B. Sun, J. Feng, K. Saenko, Return of\nfrustratingly easy domain adap-": ""
        },
        {
          "Y\n. Yang, Attribute-guided adversarial\ntraining for\nrobustness to natural": "using alternative voting rules, Analytica Chimica Acta 136 (1982) 15–27.",
          "[53] B. Sun, J. Feng, K. Saenko, Return of\nfrustratingly easy domain adap-": ""
        },
        {
          "Y\n. Yang, Attribute-guided adversarial\ntraining for\nrobustness to natural": "doi:https://doi.org/10.1016/S0003-2670(01)95359-0.",
          "[53] B. Sun, J. Feng, K. Saenko, Return of\nfrustratingly easy domain adap-": ""
        },
        {
          "Y\n. Yang, Attribute-guided adversarial\ntraining for\nrobustness to natural": "[51]\nS. Mika, B. Sch¨olkopf, A. Smola, K.-R. M¨uller, M. Scholz, G. R¨atsch,",
          "[53] B. Sun, J. Feng, K. Saenko, Return of\nfrustratingly easy domain adap-": ""
        },
        {
          "Y\n. Yang, Attribute-guided adversarial\ntraining for\nrobustness to natural": "Kernel pca\nand de-noising in feature\nspaces,\nin:\nProceedings of\nthe",
          "[53] B. Sun, J. Feng, K. Saenko, Return of\nfrustratingly easy domain adap-": ""
        },
        {
          "Y\n. Yang, Attribute-guided adversarial\ntraining for\nrobustness to natural": "12th International Conference on Neural\nInformation Processing Sys-",
          "[53] B. Sun, J. Feng, K. Saenko, Return of\nfrustratingly easy domain adap-": ""
        },
        {
          "Y\n. Yang, Attribute-guided adversarial\ntraining for\nrobustness to natural": "tems, NIPS’98, MIT Press, Cambridge, MA, USA, 1998, p. 536–542.",
          "[53] B. Sun, J. Feng, K. Saenko, Return of\nfrustratingly easy domain adap-": ""
        },
        {
          "Y\n. Yang, Attribute-guided adversarial\ntraining for\nrobustness to natural": "[52] Y. Jin, Y.-D. Luo, W.-L. Zheng, B.-L. Lu, Eeg-based emotion recognition",
          "[53] B. Sun, J. Feng, K. Saenko, Return of\nfrustratingly easy domain adap-": ""
        },
        {
          "Y\n. Yang, Attribute-guided adversarial\ntraining for\nrobustness to natural": "using domain adaptation network, 2017, pp. 222–225.\ndoi:10.1109/",
          "[53] B. Sun, J. Feng, K. Saenko, Return of\nfrustratingly easy domain adap-": ""
        },
        {
          "Y\n. Yang, Attribute-guided adversarial\ntraining for\nrobustness to natural": "ICOT.2017.8336126.",
          "[53] B. Sun, J. Feng, K. Saenko, Return of\nfrustratingly easy domain adap-": ""
        }
      ],
      "page": 14
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Affective neuroscience: The foundations of human and animal emotions",
      "authors": [
        "S Paradiso"
      ],
      "year": "2002",
      "venue": "American Journal of Psychiatry",
      "doi": "10.1176/appi.ajp.159.10.1805"
    },
    {
      "citation_id": "2",
      "title": "Knowing what you're feeling and knowing what to do about it: Mapping the relation between emotion differentiation and emotion regulation",
      "authors": [
        "L Barrett",
        "J Gross",
        "T Christensen",
        "M Benvenuto"
      ],
      "year": "2001",
      "venue": "Cognition & Emotion",
      "doi": "10.1080/02699930143000239"
    },
    {
      "citation_id": "3",
      "title": "Semi-supervised dual-stream self-attentive adversarial graph contrastive learning for cross-subject eeg-based emotion recognition",
      "authors": [
        "W Ye",
        "Z Zhang",
        "F Teng",
        "M Zhang",
        "J Wang",
        "D Ni",
        "F Li",
        "P Xu",
        "Z Liang"
      ],
      "year": "2025",
      "venue": "IEEE Transactions on Affective Computing",
      "doi": "10.1109/TAFFC.2024.3433470"
    },
    {
      "citation_id": "4",
      "title": "Adaptive spatial-temporal aware graph learning for eeg-based emotion recognition",
      "authors": [
        "W Ye",
        "J Wang",
        "L Chen",
        "L Dai",
        "Z Sun",
        "Z Liang"
      ],
      "year": "2025",
      "venue": "Cyborg and Bionic Systems",
      "doi": "10.34133/cbsystems.0088"
    },
    {
      "citation_id": "5",
      "title": "Utilizing deep learning towards multi-modal bio-sensing and vision-based affective computing",
      "authors": [
        "T.-P Siddharth",
        "T Jung",
        "Sejnowski"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing",
      "doi": "10.1109/TAFFC.2019.2916015"
    },
    {
      "citation_id": "6",
      "title": "Human emotion recognition from eeg-based brain-computer interface using machine learning: a comprehensive review",
      "authors": [
        "E Houssein",
        "A Hammad",
        "A Ali"
      ],
      "year": "2022",
      "venue": "Neural Computing and Applications",
      "doi": "10.1007/s00521-022-07292-4"
    },
    {
      "citation_id": "7",
      "title": "Video-triggered eeg-emotion public databases and current methods: A survey",
      "authors": [
        "W Hu",
        "G Huang",
        "L Li",
        "L Zhang",
        "Z Zhang",
        "Z Liang"
      ],
      "year": "2020",
      "venue": "Brain Science Advances",
      "doi": "10.26599/BSA.2020.9050026"
    },
    {
      "citation_id": "8",
      "title": "Emotions recognition using eeg signals: A survey",
      "authors": [
        "S Alarcao",
        "M Fonseca"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing",
      "doi": "10.1109/TAFFC.2017.2714671"
    },
    {
      "citation_id": "9",
      "title": "Eeg-based emotion recognition using spatial-temporal graph convolutional lstm with attention mechanism",
      "authors": [
        "L Feng",
        "C Cheng",
        "M Zhao",
        "H Deng",
        "Y Zhang"
      ],
      "year": "2022",
      "venue": "IEEE Journal of Biomedical and Health Informatics",
      "doi": "10.1109/jbhi.2022.3198688"
    },
    {
      "citation_id": "10",
      "title": "Spectralspatial attention alignment for multi-source domain adaptation in eegbased emotion recognition",
      "authors": [
        "Y Yang",
        "Z Wang",
        "W Tao",
        "X Liu",
        "Z Jia",
        "B Wang",
        "F Wan"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Affective Computing",
      "doi": "10.1109/TAFFC.2024.3394436"
    },
    {
      "citation_id": "11",
      "title": "Cascade and parallel convolutional recurrent neural networks on eeg-based intention recognition for brain computer interface",
      "authors": [
        "D Zhang",
        "L Yao",
        "X Zhang",
        "S Wang",
        "W Chen",
        "R Boots"
      ],
      "year": "2018",
      "venue": "32nd AAAI Conference on Artificial Intelligence, AAAI 2018",
      "doi": "10.1609/aaai.v32i1.11496"
    },
    {
      "citation_id": "12",
      "title": "Temporal stability and individual differences in the human eeg: An analysis of variance of spectral values",
      "authors": [
        "J Berkhout",
        "D Walter"
      ],
      "year": "1968",
      "venue": "IEEE Transactions on Biomedical Engineering",
      "doi": "10.1109/tbme.1968.4502560"
    },
    {
      "citation_id": "13",
      "title": "The electroencephalogram as a biometric",
      "authors": [
        "R Paranjape",
        "J Mahovsky",
        "L Benedicenti",
        "Z Koles"
      ],
      "year": "2001",
      "venue": "Canadian Conference on Electrical and Computer Engineering",
      "doi": "10.1109/CCECE.2001.933649"
    },
    {
      "citation_id": "14",
      "title": "Revealing feelings: facets of emotional expressivity in self-reports, peer ratings, and behavior",
      "authors": [
        "J Gross",
        "O John"
      ],
      "year": "1997",
      "venue": "Journal of personality and social psychology",
      "doi": "10.1037//0022-3514.72.2.435"
    },
    {
      "citation_id": "15",
      "title": "Neural underpinnings of individual differences in emotion regulation: A systematic review",
      "authors": [
        "C Morawetz",
        "U Basten"
      ],
      "year": "2024",
      "venue": "Neuroscience & Biobehavioral Reviews",
      "doi": "10.1016/j.neubiorev.2024.105727"
    },
    {
      "citation_id": "16",
      "title": "Understanding machine learning: From theory to algorithms",
      "authors": [
        "S Shalev-Shwartz",
        "S Ben-David"
      ],
      "year": "2014",
      "venue": "Understanding machine learning: From theory to algorithms"
    },
    {
      "citation_id": "17",
      "title": "Deep learning",
      "authors": [
        "Y Lecun",
        "Y Bengio",
        "G Hinton"
      ],
      "year": "2015",
      "venue": "Nature",
      "doi": "10.1038/nature14539"
    },
    {
      "citation_id": "18",
      "title": "Pr-pl: A novel prototypical representation based pairwise learning framework for emotion recognition using eeg signals",
      "authors": [
        "R Zhou",
        "Z Zhang",
        "H Fu",
        "L Zhang",
        "L Li",
        "G Huang",
        "F Li",
        "X Yang",
        "Y Dong",
        "Y.-T Zhang"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Affective Computing",
      "doi": "10.1109/TAFFC.2023.3288118"
    },
    {
      "citation_id": "19",
      "title": "Transfer learning in brain-computer interfaces",
      "authors": [
        "V Jayaram",
        "M Alamgir",
        "Y Altun",
        "B Scholkopf",
        "M Grosse-Wentrup"
      ],
      "year": "2016",
      "venue": "IEEE Computational Intelligence Magazine",
      "doi": "10.1109/MCI.2015.2501545"
    },
    {
      "citation_id": "20",
      "title": "Adaptive transfer learning for eeg motor imagery classification with deep convolutional neural network",
      "authors": [
        "K Zhang",
        "N Robinson",
        "S.-W Lee",
        "C Guan"
      ],
      "year": "2021",
      "venue": "Neural Networks",
      "doi": "10.1016/j.neunet.2020.12.013"
    },
    {
      "citation_id": "21",
      "title": "Ms-fran: A novel multi-source domain adaptation method for eeg-based emotion recognition",
      "authors": [
        "W Li",
        "W Huan",
        "S Shao",
        "B Hou",
        "A Song"
      ],
      "year": "2023",
      "venue": "IEEE Journal of Biomedical and Health Informatics",
      "doi": "10.1109/JBHI.2023.3311338"
    },
    {
      "citation_id": "22",
      "title": "Joint eeg feature transfer and semisupervised cross-subject emotion recognition",
      "authors": [
        "Y Peng",
        "H Liu",
        "W Kong",
        "F Nie",
        "B.-L Lu",
        "A Cichocki"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Industrial Informatics",
      "doi": "10.1109/TII.2022.3217120"
    },
    {
      "citation_id": "23",
      "title": "A survey on transfer learning",
      "authors": [
        "S Pan",
        "Q Yang"
      ],
      "year": "2010",
      "venue": "IEEE Transactions on Knowledge and Data Engineering",
      "doi": "10.1109/TKDE.2009.191"
    },
    {
      "citation_id": "24",
      "title": "A bi-hemisphere domain adversarial neural network model for eeg emotion recognition",
      "authors": [
        "Y Li",
        "W Zheng",
        "Y Zong",
        "Z Cui",
        "T Zhang",
        "X Zhou"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Affective Computing",
      "doi": "10.1109/TAFFC.2018.2885474"
    },
    {
      "citation_id": "25",
      "title": "Multisource transfer learning for cross-subject eeg emotion recognition",
      "authors": [
        "J Li",
        "S Qiu",
        "Y.-Y Shen",
        "C.-L Liu",
        "H He"
      ],
      "year": "2019",
      "venue": "IEEE transactions on cybernetics",
      "doi": "10.1109/TCYB.2019.2904052"
    },
    {
      "citation_id": "26",
      "title": "Ms-mda: Multisource marginal distribution adaptation for cross-subject and cross-session eeg emotion recognition",
      "authors": [
        "H Chen",
        "M Jin",
        "Z Li",
        "C Fan",
        "J Li",
        "H He"
      ],
      "year": "2021",
      "venue": "Frontiers in Neuroscience",
      "doi": "10.3389/fnins.2021.778488"
    },
    {
      "citation_id": "27",
      "title": "Multisource domain transfer discriminative dictionary learning modeling for electroencephalogram-based emotion recognition",
      "authors": [
        "X Gu",
        "W Cai",
        "M Gao",
        "Y Jiang",
        "X Ning",
        "P Qian"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Computational Social Systems",
      "doi": "10.1109/TCSS.2022.3153660"
    },
    {
      "citation_id": "28",
      "title": "Domain adaptation via transfer component analysis",
      "authors": [
        "S Pan",
        "I Tsang",
        "J Kwok",
        "Q Yang"
      ],
      "year": "2011",
      "venue": "IEEE Transactions on Neural Networks",
      "doi": "10.1109/TNN.2010.2091281"
    },
    {
      "citation_id": "29",
      "title": "Unsupervised visual domain adaptation using subspace alignment",
      "authors": [
        "B Fernando",
        "A Habrard",
        "M Sebban",
        "T Tuytelaars"
      ],
      "year": "2013",
      "venue": "2013 IEEE International Conference on Computer Vision",
      "doi": "10.1109/ICCV.2013.368"
    },
    {
      "citation_id": "30",
      "title": "Personalizing eeg-based affective models with transfer learning",
      "authors": [
        "W.-L Zheng",
        "B.-L Lu"
      ],
      "year": "2016",
      "venue": "Proceedings of the twenty-fifth international joint conference on artificial intelligence",
      "doi": "10.5555/3060832.3061003"
    },
    {
      "citation_id": "31",
      "title": "Geodesic flow kernel for unsupervised domain adaptation",
      "authors": [
        "B Gong",
        "Y Shi",
        "F Sha",
        "K Grauman"
      ],
      "year": "2012",
      "venue": "2012 IEEE Conference on Computer Vision and Pattern Recognition",
      "doi": "10.1109/CVPR.2012.6247911"
    },
    {
      "citation_id": "32",
      "title": "Eeg emotion recognition using fusion model of graph convolutional neural networks and lstm",
      "authors": [
        "Y Yin",
        "X Zheng",
        "B Hu",
        "Y Zhang",
        "X Cui"
      ],
      "year": "2021",
      "venue": "Applied Soft Computing",
      "doi": "10.1016/j.asoc.2020.106954"
    },
    {
      "citation_id": "33",
      "title": "Eeg-based emotion recognition using domain adaptation network",
      "authors": [
        "Y.-M Jin",
        "Y.-D Luo",
        "W.-L Zheng",
        "B.-L Lu"
      ],
      "year": "2017",
      "venue": "Eeg-based emotion recognition using domain adaptation network",
      "doi": "10.1109/ICOT.2017.8336126"
    },
    {
      "citation_id": "34",
      "title": "Cross-subject eeg-based emotion recognition with deep domain confusion",
      "authors": [
        "W Zhang",
        "F Wang",
        "Y Jiang",
        "Z Xu",
        "S Wu",
        "Y Zhang"
      ],
      "year": "2019",
      "venue": "telligent Robotics and Applications: 12th International Conference",
      "doi": "10.1007/978-3-030-27526-6_49"
    },
    {
      "citation_id": "35",
      "title": "Attribute-guided adversarial training for robustness to natural perturbations",
      "authors": [
        "T Gokhale",
        "R Anirudh",
        "B Kailkhura",
        "J Thiagarajan",
        "C Baral",
        "Y Yang"
      ],
      "year": "2022",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence",
      "doi": "10.1609/aaai.v35i9.16927"
    },
    {
      "citation_id": "36",
      "title": "Prototypical networks for few-shot learning",
      "authors": [
        "J Snell",
        "K Swersky",
        "R Zemel"
      ],
      "year": "2017",
      "venue": "Proceedings of the 31st International Conference on Neural Information Processing Systems, NIPS'17",
      "doi": "10.5555/3294996.3295163"
    },
    {
      "citation_id": "37",
      "title": "Unsupervised domain adaptation with similarity learning",
      "authors": [
        "P Pinheiro"
      ],
      "year": "2018",
      "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition",
      "doi": "10.1109/CVPR.2018.00835"
    },
    {
      "citation_id": "38",
      "title": "Few-shot human-object interaction recognition with semantic-guided attentive prototypes network",
      "authors": [
        "Z Ji",
        "X Liu",
        "Y Pang",
        "W Ouyang",
        "X Li"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Image Processing",
      "doi": "10.1109/TIP.2020.3046861"
    },
    {
      "citation_id": "39",
      "title": "Refined prototypical contrastive learning for few-shot hyperspectral image classification",
      "authors": [
        "Q Liu",
        "J Peng",
        "Y Ning",
        "N Chen",
        "W Sun",
        "Q Du",
        "Y Zhou"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Geoscience and Remote Sensing",
      "doi": "10.1109/TGRS.2023.3257341"
    },
    {
      "citation_id": "40",
      "title": "Two-stream prototype learning network for few-shot face recognition under occlusions",
      "authors": [
        "X Yang",
        "M Han",
        "Y Luo",
        "H Hu",
        "Y Wen"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Multimedia",
      "doi": "10.1109/TMM.2023.3253054"
    },
    {
      "citation_id": "41",
      "title": "Domain agnostic learning with disentangled representations",
      "authors": [
        "X Peng",
        "Z Huang",
        "X Sun",
        "K Saenko"
      ],
      "year": "2019",
      "venue": "Proceedings of the 36th International Conference on Machine Learning"
    },
    {
      "citation_id": "42",
      "title": "Learning disentangled semantic representation for domain adaptation",
      "authors": [
        "R Cai",
        "Z Li",
        "P Wei",
        "J Qiao",
        "K Zhang",
        "Z Hao"
      ],
      "year": "2019",
      "venue": "Proceedings of the 28th International Joint Conference on Artificial Intelligence, IJCAI'19"
    },
    {
      "citation_id": "43",
      "title": "Affective picture processing: The late positive potential is modulated by motivational relevance",
      "authors": [
        "H Schupp",
        "B Cuthbert",
        "M Bradley",
        "J Cacioppo",
        "T Ito",
        "P Lang"
      ],
      "year": "2000",
      "venue": "Psychophysiology",
      "doi": "10.1111/1469-8986.3720257"
    },
    {
      "citation_id": "44",
      "title": "Approximate average head models for eeg source imaging",
      "authors": [
        "P Valdés-Hernández",
        "N Ellenrieder",
        "A Ojeda-Gonzalez",
        "S Kochen",
        "Y Alemán-Gómez",
        "C Muravchik",
        "P Valdés-Sosa"
      ],
      "year": "2009",
      "venue": "Journal of Neuroscience Methods",
      "doi": "10.1016/j.jneumeth.2009.09.005"
    },
    {
      "citation_id": "45",
      "title": "The influence of brain tissue anisotropy on human eeg and meg",
      "authors": [
        "J Haueisen",
        "D Tuch",
        "C Ramon",
        "P Schimpf",
        "V Wedeen",
        "J George",
        "J Belliveau"
      ],
      "year": "2002",
      "venue": "NeuroImage",
      "doi": "10.1006/nimg.2001.0962"
    },
    {
      "citation_id": "46",
      "title": "Investigating critical frequency bands and channels for eeg-based emotion recognition with deep neural networks",
      "authors": [
        "W.-L Zheng",
        "B.-L Lu"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on Autonomous Mental Development",
      "doi": "10.1109/TAMD.2015.2431497"
    },
    {
      "citation_id": "47",
      "title": "Emotionmeter: A multimodal framework for recognizing human emotions",
      "authors": [
        "W.-L Zheng",
        "W Liu",
        "Y Lu",
        "B.-L Lu",
        "A Cichocki"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Cybernetics",
      "doi": "10.1109/TCYB.2018.2797176"
    },
    {
      "citation_id": "48",
      "title": "A bi-hemisphere domain adversarial neural network model for eeg emotion recognition",
      "authors": [
        "Y Li",
        "W Zheng",
        "Y Zong",
        "Z Cui",
        "T Zhang",
        "X Zhou"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Affective Computing",
      "doi": "10.1109/taffc.2018.2885474"
    },
    {
      "citation_id": "49",
      "title": "Off-line and on-line vigilance estimation based on linear dynamical system and manifold learning",
      "authors": [
        "L.-C Shi",
        "B.-L Lu"
      ],
      "year": "2010",
      "venue": "2010 Annual International Conference of the IEEE Engineering in Medicine and Biology",
      "doi": "10.1109/IEMBS.2010.5627125"
    },
    {
      "citation_id": "50",
      "title": "Alternative k-nearest neighbour rules in supervised pattern recognition: Part 1. k-nearest neighbour classification by using alternative voting rules",
      "authors": [
        "D Coomans",
        "D Massart"
      ],
      "year": "1982",
      "venue": "Analytica Chimica Acta",
      "doi": "10.1016/S0003-2670(01)95359-0"
    },
    {
      "citation_id": "51",
      "title": "Kernel pca and de-noising in feature spaces",
      "authors": [
        "S Mika",
        "B Schölkopf",
        "A Smola",
        "K.-R Müller",
        "M Scholz",
        "G Rätsch"
      ],
      "year": "1998",
      "venue": "Proceedings of the 12th International Conference on Neural Information Processing Systems, NIPS'98"
    },
    {
      "citation_id": "52",
      "title": "Eeg-based emotion recognition using domain adaptation network",
      "authors": [
        "Y Jin",
        "Y.-D Luo",
        "W.-L Zheng",
        "B.-L Lu"
      ],
      "year": "2017",
      "venue": "Eeg-based emotion recognition using domain adaptation network",
      "doi": "10.1109/ICOT.2017.8336126"
    },
    {
      "citation_id": "53",
      "title": "Return of frustratingly easy domain adaptation",
      "authors": [
        "B Sun",
        "J Feng",
        "K Saenko"
      ],
      "year": "2016",
      "venue": "Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence, AAAI'16"
    },
    {
      "citation_id": "54",
      "title": "Random forests",
      "authors": [
        "L Breiman"
      ],
      "year": "2001",
      "venue": "Machine learning",
      "doi": "10.1023/A:1010933404324"
    },
    {
      "citation_id": "55",
      "title": "Cross-subject emotion recognition using deep adaptation networks",
      "authors": [
        "H Li",
        "Y.-M Jin",
        "W.-L Zheng",
        "B.-L Lu"
      ],
      "year": "2018",
      "venue": "Neural Information Processing",
      "doi": "10.1007/978-3-030-04221-9_36"
    },
    {
      "citation_id": "56",
      "title": "Domain adaptation for eeg emotion recognition based on latent representation similarity",
      "authors": [
        "J Li",
        "S Qiu",
        "C Du",
        "Y Wang",
        "H He"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Cognitive and Developmental Systems",
      "doi": "10.1109/TCDS.2019.2949306"
    },
    {
      "citation_id": "57",
      "title": "Deep coral: Correlation alignment for deep domain adaptation",
      "authors": [
        "B Sun",
        "K Saenko"
      ],
      "year": "2016",
      "venue": "Computer Vision -ECCV"
    },
    {
      "citation_id": "58",
      "title": "",
      "authors": [
        "Workshops"
      ],
      "year": "2016",
      "venue": ""
    },
    {
      "citation_id": "59",
      "title": "Deep domain confusion: Maximizing for domain invariance",
      "authors": [
        "E Tzeng",
        "J Hoffman",
        "N Zhang",
        "K Saenko",
        "T Darrell"
      ],
      "year": "2014",
      "venue": "Deep domain confusion: Maximizing for domain invariance"
    },
    {
      "citation_id": "60",
      "title": "Eeg emotion recognition using dynamical graph convolutional neural networks",
      "authors": [
        "T Song",
        "W Zheng",
        "P Song",
        "Z Cui"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing",
      "doi": "10.1109/TAFFC.2018.2817622"
    },
    {
      "citation_id": "61",
      "title": "Equivalence of distance-based and rkhs-based statistics in hypothesis testing",
      "authors": [
        "D Sejdinovic",
        "B Sriperumbudur",
        "A Gretton",
        "K Fukumizu"
      ],
      "year": "2013",
      "venue": "Annals of Stats",
      "doi": "10.1214/13-AOS1140"
    },
    {
      "citation_id": "62",
      "title": "From regional to global brain: A novel hierarchical spatial-temporal neural network model for eeg emotion recognition",
      "authors": [
        "Y Li",
        "W Zheng",
        "L Wang",
        "Y Zong",
        "Z Cui"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing",
      "doi": "10.1109/TAFFC.2019.2922912"
    },
    {
      "citation_id": "63",
      "title": "Visualizing data using t-sne",
      "authors": [
        "L Van Der Maaten",
        "G Hinton"
      ],
      "year": "2008",
      "venue": "Journal of Machine Learning Research"
    }
  ]
}