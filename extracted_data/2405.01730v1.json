{
  "paper_id": "2405.01730v1",
  "title": "Converting Anyone'S Voice: End-To-End Expressive Voice Conversion With A Conditional Diffusion Model",
  "published": "2024-05-02T20:51:53Z",
  "authors": [
    "Zongyang Du",
    "Junchen Lu",
    "Kun Zhou",
    "Lakshmish Kaushik",
    "Berrak Sisman"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Expressive voice conversion (VC) conducts speaker identity conversion for emotional speakers by jointly converting speaker identity and emotional style. Emotional style modeling for arbitrary speakers in expressive VC has not been extensively explored. Previous approaches have relied on vocoders for speech reconstruction, which makes speech quality heavily dependent on the performance of vocoders. A major challenge of expressive VC lies in emotion prosody modeling. To address these challenges, this paper proposes a fully end-to-end expressive VC framework based on a conditional denoising diffusion probabilistic model (DDPM). We utilize speech units derived from self-supervised speech models as content conditioning, along with deep features extracted from speech emotion recognition and speaker verification systems to model emotional style and speaker identity. Objective and subjective evaluations show the effectiveness of our framework. Codes and samples are publicly available.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Emotions play a vital role in natural speech, as they convey one's feelings, moods, and personality  [1] . Speech expressiveness encompasses a diverse range of emotions  [2] . Expressive voice conversion aims to jointly perform speaker identity and style transfer for emotional speakers, which poses huge potential in movie dubbing, voice acting, and human-computer interaction  [3] [4] [5] .\n\nVoice conversion (VC) is the task of converting speaker identity while preserving linguistic content  [6] . Early VC studies focus on learning a statistical mapping between speech recordings of source and target speakers  [7, 8] . Recent advancements in deep learning-based approaches significantly improve the performance of VC by learning the disentanglement across various speech characteristics. For instance, separate encoders guided by appropriate constraints have been explored to learn disentangled content and speaker representations  [9] [10] [11] . Several studies introduce information bottleneck  [10, [12] [13] [14] , instance normalization  [15, 16]  and mutual information  [11, 17]  to achieve better disentanglement between content and speaker components. We note that most VC studies primarily focus on neutral speech, overlooking the prosodic variations manifested in different emotions. Expressive VC aims to fill this gap.\n\nIt remains a formidable undertaking to model emotional styles for expressive VC due to the hierarchical structure, in-\n\nCodes & Speech Samples: https://a2023aa.github.io/DEVC/ Junchen Lu and Kun Zhou contributed to this work during their internship at UT Dallas.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Expressive Voice Conversion",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Speaker Identity With Emotion Cues Voice Conversion",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Speaker Identity Emotional Voice Conversion Emotion State",
      "text": "Figure  1 : A comparison between the conversion of different speech components across VC  [6] , expressive VC  [3] , and emotional VC  [18] .\n\nherent complexity, subjective nature, and variability of human emotional speech  [3] . Moreover, emotional style contains both speaker-independent and speaker-dependent features  [19] [20] [21] . Speaker-independent emotional features often exhibit consistent associations with the same emotion across different speakers  [22] , while speaker-dependent ones are manifested in speaker characteristics and are unique to each speaker  [19] . Despite their significant roles in conveying emotional style, speaker-dependent emotional features have been overlooked in previous VC studies  [4, 5] . In this paper, we aim to enhance the expressiveness of converted speech for VC systems by exploring both speaker-dependent and speaker-independent emotional features.\n\nAnother limitation of previous approaches  [3] [4] [5]  of expressive voice conversion stems from their reliance on vocoders to reconstruct speech waveform from acoustic features. Consequently, the quality of the synthesized speech is intricately influenced by vocoder's performance  [23] , which typically requires a substantial amount of high-quality speech data for training. To address these issues, we propose a fully end-to-end diffusionbased framework for expressive VC for the first time. We also demonstrate the flexibility of our model on any-to-any conversion.\n\nIn this paper, we introduce an expressive voice conversion framework, namely DEVC, which enables effective speaker identity conversion for emotionally expressive speakers. To achieve this, we employ three encoders dedicated to handling content representation, speaker representation incorporating speaker-dependent emotional cues, and speaker-independent emotion representation, respectively. Content conditioning is facilitated through speech units, while deep features derived from speaker verification (SV) and speech emotion recognition (SER) tasks are utilized to capture speaker-dependent and speaker-independent emotional information. Furthermore, we • We propose a fully end-to-end expressive voice conversion framework based on a conditional diffusion model without the need for large-scale training data and manual annotations;\n\n• Our findings reveal that speaker embeddings derived from an SV model pre-trained on neutral data effectively capture speaker-dependent emotional cues, thereby demonstrating their utility in enhancing expressive voice conversion; and\n\n• Our proposed framework shows flexibility in identity conversion for both seen and unseen emotional speakers, achieving any-to-any expressive voice conversion.\n\nThe rest of this paper is organized as follows: Section 2 provides an introduction to the related work. In Section 3, we undertake a comprehensive analysis of the emotion and speaker representations employed in our framework. Section 4 presents the details of our proposed framework. The experimental results are reported in Section 5. In Section 7, we conclude the study.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Related Work",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Expressive Voice Conversion",
      "text": "Expressive speech is a complex composition of various components, including speaker identity, emotional style, and linguistic content. Previous models in expressive voice conversion  [4, 5]  primarily focus on eliminating redundant information between emotional style and other speech components, such as content and speaker identity. This objective has been achieved through techniques like mutual information loss  [5]  and prosody filters  [4] . In another work  [3] , a StarGAN-based model incorporates deep features from an SER model as emotion representation and a simple one-hot vector as the speaker representation. However, this simplistic speaker representation may not adequately capture the nuances of emotional speakers, thus limiting its ability to achieve any-to-any voice conversion for arbitrary speakers.\n\nAnother related task is emotional voice conversion, which focuses on converting the emotional state of the speaker while preserving the speaker's identity  [18, 24] . In contrast, expressive voice conversion aims to simultaneously convert both speaker identity and speech style for speakers displaying emotional expression  [3] , as shown in Figure  1 . This paper will concentrate on expressive voice conversion.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Denoising Diffusion Probabilistic Models In Speech Synthesis",
      "text": "Denoising diffusion probabilistic models  [25]  are a class of diffusion models that uses a Markov chain to gradually convert a simple distribution into a complicated data distribution. It contains two processes: a diffusion process where clean input data is converted to an isotropic Gaussian distribution by adding noise step by step; and a reverse process where clean input can be recovered from Gaussian noise by predicting and removing the noise introduced in each step of the diffusion process. Denoising diffusion probabilistic models have achieved great success in various speech synthesis tasks, such as vocoding  [26] , text-to-speech  [27] , and VC  [28] [29] [30] . We further develop the idea and introduce an end-to-end framework for expressive voice conversion, which will be elaborated in Section 4.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Representation Learning With Self-Supervised Models",
      "text": "Collecting labeled speech data is difficult, expensive, and timeconsuming  [31] [32] [33] . Self-supervised speech models pre-trained on a large amount of unlabeled data can learn high-level speech representations without relying on labeled data  [34] [35] [36] [37] [38] . Selfsupervised representations have been proven to be highly effective in various speech synthesis tasks due to their ability to capture essential speech content information  [39, 40] . The majority of existing VC methods leveraging such representations separate content from speaker-related information through discretization  [41, 42] . To retain more linguistic content in VC, van Niekerk et al.  [43]  propose to learn soft speech units by predicting a distribution over discretized representations, further improving the intelligibility of converted speech. Inspired by this, we incorporate soft speech units as content conditioning in our proposed method. Expressive speech introduces variations in the acoustic features, resulting in increased complexity for speaker identity among emotional speakers  [44] . In expressive voice conversion, it is expected that speaker identity contains both speaker-dependent emotional cues and other speaker-related information. Speakerdependent emotional cues refer to the emotional information that is specific to an individual speaker. These cues include variations in intonation, rhythm, and voice quality that reflect the speaker's emotional state.\n\nTo incorporate such speaker-dependent emotional cues into our expressive voice conversion model, we extract speaker representations from a pre-trained SV model  [45] . These speaker representations serve as conditioning information for our expressive voice conversion model, allowing it to generate converted speech that preserves both target speaker identity and their emotional nuances.\n\nBy employing the t-SNE algorithm  [46] , we visualize the speaker representations of two female speakers and two male speakers, as shown in Fig.  2(a) . We observe that the speaker representations form separate clusters for each speaker, indicating successful differentiation of speaker identities. We also notice the formation of smaller clusters within the same speaker and emotional state, indicating the presence of speakerdependent emotional information.\n\nTo further evaluate the similarity of speaker representations, we perform a comprehensive analysis. We randomly selected 240 emotional speech utterances from the ESD dataset  [18]  and divided them equally into two groups. We calculate the Euclidean distance between utterance pairs from these groups and obtain the mean distance of emotion pairs. As presented in Table  1 , the results consistently demonstrate that the distances within the same emotional state are lower compared to those between different emotions. This consistent pattern strongly supports our hypothesis that the speaker representations not only capture speaker-related information but also encompass speaker-dependent emotional cues.\n\nIn our proposed method, we adopt deep features obtained from a pre-trained speaker-independent SER model  [47]  as speaker-independent emotion representations, following the approach in  [48] . To visually analyze the speaker-independent emotion representations, we employ the t-SNE algorithm  [46]  and present the results in Figure  2(b) . The visualization demonstrates that the emotion representations derived from different emotions form well-separated clusters. Within each cluster, representations from the same emotion but different speakers tend to be mixed up. This observation confirms that our emotion representations encompass speaker-independent emotional style information, allowing for generalization across different individuals.\n\nOur analysis provides valuable insights into the nature of our emotion and speaker representations. By incorporating these representations, our framework enables a single DDPM to synthesize expressive speech for any given speaker by effectively modeling both common characteristics of each emotional state across different speakers and emotional nuances associated with each individual. Further details on this approach will be provided in Section 4.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Proposed Method -Devc",
      "text": "In this section, we introduce our fully end-to-end expressive VC model, Diffusion-based Expressive Voice Conversion (DEVC). DEVC consists of a content encoder, a speaker encoder, a speaker-independent emotion encoder, and a diffusion-based decoder, as illustrated in Figure  3 . These three encoders extract content representations, speaker representation with emotional cues, and speaker-independent emotion representations from input expressive speech, respectively. Utilizing these auxiliary representations as conditions, the diffusion-based decoder iteratively generates the converted expressive speech, starting from Gaussian noise.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Training Stage",
      "text": "Given an expressive speech utterance with S audio segments, the content encoder extracts a 256-dimensional content representation Rc ∈ R S×256 from the waveform. The speaker encoder extracts a 256-dimension utterance-level speaker representation Rs with emotional cues, as reported in Table  1  and Figure  2\n\nThe conditional DDPM-based decoder has two subprocesses: the diffusion process and the reverse process. Given a segment of waveform x0, the diffusion process is the process of x0 being gradually corrupted to Gaussian noise xT within finite T steps. Assume {x1, ..., xT -1} is a sequence of latent variables where xt is transformed from xt-1 by adding Gaussian noises at each timestep t ∈ [1, T ]:\n\nwhere {β1, ...βT } is a fixed variance schedule. Given clean data x0, sampling of xt can be written in a closed form:\n\nwhere αt = 1 -βt and ᾱt = t s=1 αs. Noise ϵ ∼ N (0, I) has the same dimensionality as data x0 and latent variables x1, ...xT .\n\nThe reverse process generates a reverse sequence by sampling the posteriors q(xt-1|xt), starting from a Gaussian noise sample xT . However, since q(xt-1|xt) is intractable, the decoder learn parameterized Gaussian transitions p θ (xt-1|xt) with a learned mean µ θ (xt, t, c) and a fixed variance σ 2 t I [25]:\n\nwhere µ θ (xt, t, c) is the function of a noise approximator ϵ θ (xt, t, c).\n\nBased on speech-conditioning pairs, we then learn the conditional diffusion-based decoder via:\n\nwhere ϵ ∼ N (0, I) is the noise and ϵ θ denotes the decoder with learnable parameters θ.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Run-Time Conversion",
      "text": "At run-time, DEVC takes a source utterance from one speaker and a reference utterance from a target speaker, which is either seen or unseen during training, as shown in Figure  4 . The encoders extract content representation and speaker-independent emotional style representation from the source utterance and speaker representation with emotional cues from the reference utterance and the diffusion-based decoder generates converted expressive speech from Gaussian noise and timestep conditioned on those representations. We note that emotion representations are derived from a speaker-independent emotion recognizer, thereby inherently assumed to be speaker-independent (as shown in Section 3). Through our experiments, we illustrate that the emotion encoder can take input from either the source utterance or the reference.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Experiments",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Dataset",
      "text": "We evaluate the proposed DEVC on the ESD dataset  [18] . For each emotion and each speaker, there are 300 training utterances, 20 reference utterances, and 30 test utterances. In our experiments, we select four emotions (Neutral, Angry, Happy, and Sad) and four speakers (0013, 0016, 0018, and 0020) as seen speakers for training, while randomly selecting four other speakers as unseen speakers. We evaluate our model in three scenarios: the conversion between seen speakers (S2S), the conversion from seen speakers to unseen speakers (S2U), and the conversion between unseen speakers (U2U), commonly referred to as the any-to-any task. All speech samples are sampled at 16 kHz and saved in 16-bit format.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Experimental Setup",
      "text": "DEVC consists of a content encoder, a speaker encoder, an emotion encoder, and a diffusion-based decoder. The content encoder is based on the HuBERT-Base backbone network, augmented with a linear layer  [43]  and pre-trained on the LibriSpeech-960 dataset  [49] . The emotion encoder consists of a three-dimensional CNN layer, a BLSTM layer, an attention layer, and a fully-connected (FC) layer. It is initially pre-trained on the IEMOCAP dataset  [50]  and subsequently fine-tuned using the ESD dataset  [48] . The speaker encoder follows the architecture in  [45] , utilizing a 3-layer LSTM with projection to generate 256-dimensional representations.\n\nFor the diffusion decoder, the architecture is similar to that described in  [26] . The step encoder comprises a 128dimensional sinusoidal position encoding and two FC layers with Switch activations. The PreNet consists of a Conv1×1 layer followed by ReLU activation. The output of the step encoder and latent variables are then fed into a stack of 64 residual blocks with 128 residual channels. The skip connections from all residual layers are summed before entering the Post-Net, which includes two Conv1×1 layers. DEVC is trained on a single Nvidia 3080Ti GPU for 1.2 million steps using Adam optimizer with a batch size of 16 and a learning rate of 0.0002.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Baselines",
      "text": "In a comparative study, we adopt the following two expressive VC models as our baseline:\n\n• Baseline: JES-StarGAN  [3] , a many-to-many expressive voice conversion framework for S2S scenarios; and\n\n• Baseline-U: We replace the one-hot speaker label in JES-StarGAN  [3]  with speaker representations for S2U and U2U settings.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Objective Evaluations",
      "text": "Table  2  presents the objective evaluation results of our proposed framework and the baselines. Mel-cepstral distortion (MCD)  [51]  is utilized to measure spectral distortion between synthesized samples and target samples. Lower MCD values indicate better quality and closer similarity between the synthesized samples and the target samples  [52] . Our proposed method outperforms the baselines in all settings (S2S, S2U, and U2U) by achieving consistently lower MCD values, which indicates a better conversion performance.\n\nTo evaluate speaker similarity, we utilize a pre-trained SV model 1 and report the SV accuracy in Table  2 . Higher SV accuracy indicates better conversion of speaker identity  [31] . Our proposed framework achieves higher speaker similarity compared to the baselines, highlighting its capability to accurately convert the characteristics of emotional speakers.\n\nIn addition, we employ two pitch similarity metrics: Voicing Decision Error (VDE)  [53]  and F0 Frame Error (FFE)  [54] . VDE measures the proportion of voiced/unvoiced decision difference between the converted and target utterances, while FFE captures both pitch similarity and voiced/unvoiced decision differences  [42] . Our proposed method outperforms the baselines, indicating its effectiveness in capturing and reproducing the desired characteristics in the converted speech.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Subjective Evaluation",
      "text": "We conduct listening experiments with 13 subjects to assess the speech quality, speaker similarity, and emotional style similarity. Each of them listens to 192 converted utterances in total. In the Mean Opinion Score (MOS) test, participants evaluated the converted utterances using a rating scale ranging from 1 (poor) to 5 (excellent) to assess speech quality. The results for S2S, S2U, and U2U are presented in Table  3 . As shown in Table  3 , DEVC outperforms the baselines in terms of speech quality.\n\nIn addition, we perform ABX preference tests to evaluate speaker similarity in the S2S, S2U, and U2U settings. The listeners compare the converted utterances and select the utterance that sounds closer to the reference in terms of speaker identity. The results, illustrated in Figure  5(a) , demonstrate that our proposed framework significantly outperforms the baseline, indicating that our method excels in accurately converting speaker identities for both seen and unseen speakers.\n\nWe further conduct ABX preference tests to evaluate the emotional style similarity in S2S, S2U, and U2U settings. These tests require listeners to compare the emotional styles of the reference and converted utterances. Figure  5 (b) presents the results, clearly demonstrating the superior performance of our proposed framework over the baseline. These findings highlight the capability of our method to successfully convert emotional 1 https://github.com/resemble-ai/Resemblyzer  styles for arbitrary speakers, further validating the effectiveness of our emotional style modeling approach.\n\nIt is worth mentioning that the conversion between unseen speakers to unseen speakers is particularly challenging due to the lack of training data. However, even in this challenging scenario, our proposed method achieves excellent results in subjective evaluation, further affirming its ability to conduct highquality conversion in any-to-any setting.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Ablation Studies",
      "text": "We conduct ablation studies to analyze the impact of our speaker representations with emotion cues and speakerindependent emotion representations in the context of expressive voice conversion. Specifically, we investigate three different models, all sharing the same diffusion-based decoder architecture: the first model includes content representations and speaker representations, denoted as Rc, Rs. The second model incorporates content representations, speaker-independent emotional style representations, and one-hot speaker labels, denoted as Rc, Re. The third model represents our proposed method, incorporating content representations, speaker representations with emotion cues, and speaker-independent emotion representations, denoted as Rc, Re, Rs.\n\nIn the S2S setting, we evaluate the performance of these models by calculating metrics MCD, SV accuracy, and F0 root mean squared errors (F0-RMSE)  [55] , as presented in Table  4 . From the results, our proposed method, which utilizes speaker representations instead of one-hot speaker labels, exhibits superior performance compared to the model incorporating one-hot speaker vectors. This highlights the advantage of our speaker representation with emotion cues, indicating that it captures and represents speaker-dependent emotion cues more effectively. In addition, we observe that our proposed method outperforms the other models. This finding suggests the effectiveness of our emotional style modeling method in the expressive voice conversion task.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Discussion",
      "text": "We note that our proposed method has the potential to be extended to convert emotion states for different speakers with various reference utterance settings. However, in this paper, our focus is solely on expressive VC. We intend to broaden the scope of this work to include the conversion of emotion states for the same speaker (emotional VC) or different speakers in our future research.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Conclusion",
      "text": "In this paper, we present a novel framework for expressive voice conversion using a diffusion-based approach that supports anyto-any conversion. Our framework leverages speech units as content representations and incorporates deep features extracted from speech emotion recognition and speaker verification tasks to capture emotion and speaker characteristics. A key finding of our research is that speaker embeddings obtained from a pretrained SV model using neutral data inherently contain speakerdependent emotional features, thereby proving advantageous for the expressive voice conversion task. Our proposed framework exhibits remarkable flexibility in converting both seen and unseen speakers without the need for vocoders, marking it as the first end-to-end diffusion model-based expressive voice conversion framework known to us.",
      "page_start": 6,
      "page_end": 6
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: A comparison between the conversion of different",
      "page": 1
    },
    {
      "caption": "Figure 2: Visualization of speaker representations and speaker-independent emotion representations of 50 randomly selected utterances",
      "page": 2
    },
    {
      "caption": "Figure 1: This paper will concentrate",
      "page": 2
    },
    {
      "caption": "Figure 2: (a) Speaker: 0013 (M1)",
      "page": 3
    },
    {
      "caption": "Figure 2: (a). We observe that the speaker",
      "page": 3
    },
    {
      "caption": "Figure 2: (b). The visualization demon-",
      "page": 3
    },
    {
      "caption": "Figure 3: These three encoders extract",
      "page": 3
    },
    {
      "caption": "Figure 2: (a). The emotion encoder extracts an utterance-",
      "page": 3
    },
    {
      "caption": "Figure 3: An illustration of the training phase of the proposed DEVC, where the green boxes represent the modules that are involved in",
      "page": 4
    },
    {
      "caption": "Figure 4: An illustration of the run-time phase of the proposed",
      "page": 4
    },
    {
      "caption": "Figure 5: (a), demonstrate that our pro-",
      "page": 5
    },
    {
      "caption": "Figure 5: (b) presents the",
      "page": 5
    },
    {
      "caption": "Figure 5: ABX preference results for S2S, S2U, and U2S set-",
      "page": 6
    }
  ],
  "tables": [
    {
      "caption": "Table 2: presents the objective evaluation results of our pro-",
      "data": [
        {
          "Neutral": "MCD↓\nSV↑\nFFE↓\nVDE↓",
          "Angry": "MCD↓\nSV↑\nFFE↓\nVDE↓",
          "Happy": "MCD↓\nSV↑\nFFE↓\nVDE↓",
          "Sad": "MCD↓\nSV↑\nFFE↓\nVDE↓"
        },
        {
          "Neutral": "8.86\n0.72\n0.39\n0.34\n8.18\n0.84\n0.32\n0.29",
          "Angry": "8.79\n0.75\n0.39\n0.34\n7.99\n0.87\n0.33\n0.30",
          "Happy": "8.84\n0.72\n0.47\n0.37\n8.42\n0.82\n0.41\n0.34",
          "Sad": "9.08\n0.73\n0.41\n0.34\n8.01\n0.80\n0.34\n0.30"
        },
        {
          "Neutral": "8.81\n0.68\n0.42\n0.37\n8.80\n0.80\n0.34\n0.30",
          "Angry": "8.74\n0.53\n0.46\n0.39\n8.61\n0.71\n0.39\n0.34",
          "Happy": "8.54\n0.71\n0.47\n0.39\n8.43\n0.73\n0.42\n0.36",
          "Sad": "8.51\n0.67\n0.43\n0.35\n8.12\n0.76\n0.39\n0.33"
        },
        {
          "Neutral": "9.02\n0.65\n0.42\n0.37\n8.67\n0.79\n0.35\n0.31",
          "Angry": "9.26\n0.69\n0.47\n0.40\n8.62\n0.84\n0.38\n0.33",
          "Happy": "8.72\n0.70\n0.48\n0.39\n8.39\n0.79\n0.41\n0.35",
          "Sad": "8.62\n0.67\n0.45\n0.38\n8.19\n0.77\n0.40\n0.35"
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "A review on five recent and near-future developments in computational processing of emotion in the human voice",
      "authors": [
        "M Dagmar",
        "Björn Schuller",
        "Schuller"
      ],
      "year": "2020",
      "venue": "Emotion Review"
    },
    {
      "citation_id": "3",
      "title": "Speech synthesis with mixed emotions",
      "authors": [
        "Kun Zhou",
        "Berrak Sisman",
        "Rajib Rana",
        "Björn Schuller",
        "Haizhou Li"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "4",
      "title": "Expressive voice conversion: A joint framework for speaker identity and emotional style transfer",
      "authors": [
        "Zongyang Du",
        "Berrak Sisman",
        "Kun Zhou",
        "Haizhou Li"
      ],
      "year": "2021",
      "venue": "2021 IEEE Automatic Speech Recognition and Understanding Workshop"
    },
    {
      "citation_id": "5",
      "title": "Iqdubbing: Prosody modeling based on discrete self-supervised speech representation for expressive voice conversion",
      "authors": [
        "Wendong Gan",
        "Bolong Wen",
        "Ying Yan",
        "Haitao Chen",
        "Zhichao Wang",
        "Hongqiang Du",
        "Lei Xie",
        "Kaixuan Guo",
        "Hai Li"
      ],
      "year": "2022",
      "venue": "Iqdubbing: Prosody modeling based on discrete self-supervised speech representation for expressive voice conversion",
      "arxiv": "arXiv:2201.00269"
    },
    {
      "citation_id": "6",
      "title": "Disentanglement of Emotional Style and Speaker Identity for Expressive Voice Conversion",
      "authors": [
        "Zongyang Du",
        "Berrak Sisman",
        "Kun Zhou",
        "Haizhou Li"
      ],
      "year": "2022",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "7",
      "title": "An overview of voice conversion and its challenges: From statistical modeling to deep learning",
      "authors": [
        "Berrak Sisman",
        "Junichi Yamagishi",
        "Simon King",
        "Haizhou Li"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "8",
      "title": "Voice conversion based on maximum-likelihood estimation of spectral parameter trajectory",
      "authors": [
        "Tomoki Toda",
        "Alan Black",
        "Keiichi Tokuda"
      ],
      "year": "2007",
      "venue": "IEEE Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "9",
      "title": "Gmm-based emotional voice conversion using spectrum and prosody features",
      "authors": [
        "Ryo Aihara",
        "Ryoichi Takashima",
        "Tetsuya Takiguchi",
        "Yasuo Ariki"
      ],
      "year": "2012",
      "venue": "American Journal of Signal Processing"
    },
    {
      "citation_id": "10",
      "title": "Voice conversion across arbitrary speakers based on a single target-speaker utterance",
      "authors": [
        "Songxiang Liu",
        "Jinghua Zhong",
        "Lifa Sun",
        "Xixin Wu",
        "Xunying Liu",
        "Helen Meng"
      ],
      "year": "2018",
      "venue": "Voice conversion across arbitrary speakers based on a single target-speaker utterance"
    },
    {
      "citation_id": "11",
      "title": "Autovc: Zero-shot voice style transfer with only autoencoder loss",
      "authors": [
        "Kaizhi Qian",
        "Yang Zhang",
        "Shiyu Chang",
        "Xuesong Yang",
        "Mark Hasegawa-Johnson"
      ],
      "year": "2019",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "12",
      "title": "VQMIVC: Vector Quantization and Mutual Information-Based Unsupervised Speech Representation Disentanglement for One-Shot Voice Conversion",
      "authors": [
        "Disong Wang",
        "Liqun Deng",
        "Yu Yeung",
        "Xiao Chen",
        "Xunying Liu",
        "Helen Meng"
      ],
      "year": "2021",
      "venue": "Proc. Interspeech 2021"
    },
    {
      "citation_id": "13",
      "title": "Vqvae unsupervised unit discovery and multi-scale code2spec inverter for zerospeech challenge 2019",
      "authors": [
        "Andros Tjandra",
        "Berrak Sisman",
        "Mingyang Zhang",
        "Sakriani Sakti",
        "Haizhou Li",
        "Satoshi Nakamura"
      ],
      "year": "2019",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "14",
      "title": "crank: An open-source software for nonparallel voice conversion based on vector-quantized variational autoencoder",
      "authors": [
        "Kazuhiro Kobayashi",
        "Wen-Chin Huang",
        "Yi-Chiao Wu",
        "Patrick Lumban Tobing",
        "Tomoki Hayashi",
        "Tomoki Toda"
      ],
      "year": "2021",
      "venue": "ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "15",
      "title": "Tvqvc: Transformer based vector quantized variational autoencoder with ctc loss for voice conversion",
      "authors": [
        "Ziyi Chen",
        "Pengyuan Zhang"
      ],
      "year": "2021",
      "venue": "Tvqvc: Transformer based vector quantized variational autoencoder with ctc loss for voice conversion"
    },
    {
      "citation_id": "16",
      "title": "Again-vc: A one-shot voice conversion using activation guidance and adaptive instance normalization",
      "authors": [
        "Yen-Hao Chen",
        "Da-Yi Wu",
        "Tsung-Han Wu",
        "Hung-Yi Lee"
      ],
      "year": "2021",
      "venue": "ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "17",
      "title": "One-Shot Voice Conversion by Separating Speaker and Content Representations with Instance Normalization",
      "authors": [
        "Chou Ju",
        "Hung-Yi Lee"
      ],
      "year": "2019",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "18",
      "title": "Limi-vc: A light weight voice conversion model with mutual information disentanglement",
      "authors": [
        "Liangjie Huang",
        "Tian Yuan",
        "Yunming Liang",
        "Zeyu Chen",
        "Can Wen",
        "Yanlu Xie",
        "Jinsong Zhang",
        "Dengfeng Ke"
      ],
      "year": "2023",
      "venue": "ICASSP 2023 -2023 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "19",
      "title": "Emotional voice conversion: Theory, databases and esd",
      "authors": [
        "Kun Zhou",
        "Berrak Sisman",
        "Rui Liu",
        "Haizhou Li"
      ],
      "year": "2022",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "20",
      "title": "Unsupervised personalization of an emotion recognition system: The unique properties of the externalization of valence in speech",
      "authors": [
        "Kusha Sridhar",
        "Carlos Busso"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "21",
      "title": "Vaw-gan for disentanglement and recomposition of emotional elements in speech",
      "authors": [
        "Kun Zhou",
        "Berrak Sisman",
        "Haizhou Li"
      ],
      "year": "2021",
      "venue": "2021 IEEE Spoken Language Technology Workshop (SLT)"
    },
    {
      "citation_id": "22",
      "title": "Emotion intensity and its control for emotional voice conversion",
      "authors": [
        "Kun Zhou",
        "Berrak Sisman",
        "Rajib Rana",
        "Björn Schuller",
        "Haizhou Li"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "23",
      "title": "Converting anyone's emotion: Towards speaker-independent emotional voice conversion",
      "authors": [
        "Kun Zhou",
        "Berrak Sisman",
        "Mingyang Zhang",
        "Haizhou Li"
      ],
      "year": "2020",
      "venue": "Proc. Interspeech 2020"
    },
    {
      "citation_id": "24",
      "title": "Nvc-net: End-to-end adversarial voice conversion",
      "authors": [
        "Bac Nguyen",
        "Fabien Cardinaux"
      ],
      "year": "2022",
      "venue": "ICASSP 2022 -2022 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "25",
      "title": "Nonparallel emotional voice conversion for unseen speaker-emotion pairs using dual domain adversarial network & virtual domain pairing",
      "authors": [
        "Nirmesh Shah",
        "Mayank Singh",
        "Naoya Takahashi",
        "Naoyuki Onoe"
      ],
      "year": "2023",
      "venue": "ICASSP 2023 -2023 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "26",
      "title": "Denoising diffusion probabilistic models",
      "authors": [
        "Jonathan Ho",
        "Ajay Jain",
        "Pieter Abbeel"
      ],
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "27",
      "title": "Diffwave: A versatile diffusion model for audio synthesis",
      "authors": [
        "Zhifeng Kong",
        "Wei Ping",
        "Jiaji Huang",
        "Kexin Zhao",
        "Bryan Catanzaro"
      ],
      "year": "2021",
      "venue": "International Conference on Learning Representations"
    },
    {
      "citation_id": "28",
      "title": "Fastdiff: A fast conditional diffusion model for high-quality speech synthesis",
      "authors": [
        "Rongjie Huang",
        "Max Lam",
        "Jun Wang",
        "Dan Su",
        "Dong Yu",
        "Yi Ren",
        "Zhou Zhao"
      ],
      "year": "2022",
      "venue": "Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence, IJCAI-22"
    },
    {
      "citation_id": "29",
      "title": "Diffsvc: A diffusion probabilistic model for singing voice conversion",
      "authors": [
        "Songxiang Liu",
        "Yuewen Cao",
        "Dan Su",
        "Helen Meng"
      ],
      "year": "2021",
      "venue": "2021 IEEE Automatic Speech Recognition and Understanding Workshop"
    },
    {
      "citation_id": "30",
      "title": "Diffusionbased voice conversion with fast maximum likelihood sampling scheme",
      "authors": [
        "Vadim Popov",
        "Ivan Vovk",
        "Vladimir Gogoryan",
        "Tasnima Sadekova",
        "Mikhail Sergeevich Kudinov",
        "Jiansheng Wei"
      ],
      "year": "2021",
      "venue": "International Conference on Learning Representations"
    },
    {
      "citation_id": "31",
      "title": "A unified system for voice cloning and voice conversion through diffusion probabilistic modeling",
      "authors": [
        "Tasnima Sadekova",
        "Vladimir Gogoryan",
        "Ivan Vovk",
        "Vadim Popov",
        "Mikhail Kudinov",
        "Jiansheng Wei"
      ],
      "year": "2022",
      "venue": "A unified system for voice cloning and voice conversion through diffusion probabilistic modeling"
    },
    {
      "citation_id": "32",
      "title": "S2vc: A framework for any-to-any voice conversion with self-supervised pretrained representations",
      "authors": [
        "Jheng-Hao Lin",
        "Chung-Ming Yist Y Lin",
        "Hung-Yi Chien",
        "Lee"
      ],
      "year": "2021",
      "venue": "S2vc: A framework for any-to-any voice conversion with self-supervised pretrained representations",
      "arxiv": "arXiv:2104.02901"
    },
    {
      "citation_id": "33",
      "title": "Self-supervised speech representation learning: A review",
      "authors": [
        "Abdelrahman Mohamed",
        "Hung-Yi Lee",
        "Lasse Borgholt",
        "Jakob Havtorn",
        "Joakim Edin",
        "Christian Igel",
        "Katrin Kirchhoff",
        "Shang-Wen",
        "Karen Li",
        "Lars Livescu",
        "Maaløe"
      ],
      "year": "2022",
      "venue": "IEEE Journal of Selected Topics in Signal Processing"
    },
    {
      "citation_id": "34",
      "title": "FeaRLESS: Feature Refinement Loss for Ensembling Self-Supervised Learning Features in Robust End-to-end Speech Recognition",
      "authors": [
        "Szu-Jui Chen",
        "Jiamin Xie",
        "John Hansen"
      ],
      "year": "2022",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "35",
      "title": "vqwav2vec: Self-supervised learning of discrete speech representations",
      "authors": [
        "Alexei Baevski",
        "Steffen Schneider",
        "Michael Auli"
      ],
      "year": "2019",
      "venue": "vqwav2vec: Self-supervised learning of discrete speech representations",
      "arxiv": "arXiv:1910.05453"
    },
    {
      "citation_id": "36",
      "title": "wav2vec: Unsupervised pre-training for speech recognition",
      "authors": [
        "Steffen Schneider",
        "Alexei Baevski",
        "Ronan Collobert",
        "Michael Auli"
      ],
      "year": "2019",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "37",
      "title": "wav2vec 2.0: A framework for self-supervised learning of speech representations",
      "authors": [
        "Alexei Baevski",
        "Yuhao Zhou",
        "Abdelrahman Mohamed",
        "Michael Auli"
      ],
      "year": "2020",
      "venue": "Proc. NeurIPS"
    },
    {
      "citation_id": "38",
      "title": "Hubert: Self-supervised speech representation learning by masked prediction of hidden units",
      "authors": [
        "Wei-Ning Hsu",
        "Benjamin Bolte",
        "Hubert Yao-Hung",
        "Kushal Tsai",
        "Ruslan Lakhotia",
        "Abdelrahman Salakhutdinov",
        "Mohamed"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "39",
      "title": "What Can an Accent Identifier Learn? Probing Phonetic and Prosodic Information in a Wav2vec2-based Accent Identification Model",
      "authors": [
        "Mu Yang",
        "Ram Shekar",
        "Okim Kang",
        "John Hansen"
      ],
      "year": "2023",
      "venue": "Proc. INTERSPEECH 2023"
    },
    {
      "citation_id": "40",
      "title": "Textless speech emotion conversion using discrete & decomposed representations",
      "authors": [
        "Felix Kreuk",
        "Adam Polyak",
        "Jade Copet",
        "Eugene Kharitonov",
        "Anh Tu",
        "Morgan Nguyen",
        "Wei-Ning Rivière",
        "Abdelrahman Hsu",
        "Emmanuel Mohamed",
        "Yossi Dupoux",
        "Adi"
      ],
      "year": "2022",
      "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "41",
      "title": "High-Quality Automatic Voice Over with Accurate Alignment: Supervision through Self-Supervised Discrete Speech Units",
      "authors": [
        "Junchen Lu",
        "Berrak Sisman",
        "Mingyang Zhang",
        "Haizhou Li"
      ],
      "year": "2023",
      "venue": "Proc. INTERSPEECH 2023"
    },
    {
      "citation_id": "42",
      "title": "Any-toone sequence-to-sequence voice conversion using self-supervised discrete speech representations",
      "authors": [
        "Wen-Chin Huang",
        "Yi-Chiao Wu",
        "Tomoki Hayashi"
      ],
      "year": "2021",
      "venue": "ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "43",
      "title": "Speech Resynthesis from Discrete Disentangled Self-Supervised Representations",
      "authors": [
        "Adam Polyak",
        "Yossi Adi",
        "Jade Copet",
        "Eugene Kharitonov",
        "Kushal Lakhotia",
        "Wei-Ning Hsu",
        "Abdelrahman Mohamed",
        "Emmanuel Dupoux"
      ],
      "year": "2021",
      "venue": "Proc. Interspeech 2021"
    },
    {
      "citation_id": "44",
      "title": "A comparison of discrete and soft speech units for improved voice conversion",
      "authors": [
        "Marc-André Benjamin Van Niekerk",
        "Julian Carbonneau",
        "Matthew Zaïdi",
        "Hugo Baas",
        "Herman Seuté",
        "Kamper"
      ],
      "year": "2022",
      "venue": "ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "45",
      "title": "A study of speaker verification performance with expressive speech",
      "authors": [
        "Srinivas Parthasarathy",
        "Chunlei Zhang",
        "John Hl Hansen",
        "Carlos Busso"
      ],
      "year": "2017",
      "venue": "2017 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "46",
      "title": "Generalized end-to-end loss for speaker verification",
      "authors": [
        "Li Wan",
        "Quan Wang",
        "Alan Papir",
        "Ignacio Moreno"
      ],
      "year": "2018",
      "venue": "2018 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "47",
      "title": "Visualizing data using t-sne",
      "authors": [
        "Laurens Van Der Maaten",
        "Geoffrey Hinton"
      ],
      "year": "2008",
      "venue": "Journal of machine learning research"
    },
    {
      "citation_id": "48",
      "title": "3-d convolutional recurrent neural networks with attention model for speech emotion recognition",
      "authors": [
        "Mingyi Chen",
        "Xuanji He",
        "Jing Yang",
        "Han Zhang"
      ],
      "year": "2018",
      "venue": "IEEE Signal Processing Letters"
    },
    {
      "citation_id": "49",
      "title": "Seen and unseen emotional style transfer for voice conversion with a new emotional speech dataset",
      "authors": [
        "Kun Zhou",
        "Berrak Sisman",
        "Rui Liu",
        "Haizhou Li"
      ],
      "year": "2021",
      "venue": "ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "50",
      "title": "Librispeech: An asr corpus based on public domain audio books",
      "authors": [
        "Vassil Panayotov",
        "Guoguo Chen",
        "Daniel Povey",
        "Sanjeev Khudanpur"
      ],
      "year": "2015",
      "venue": "2015 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "51",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "Carlos Busso",
        "Murtaza Bulut",
        "Chi-Chun Lee",
        "Abe Kazemzadeh",
        "Emily Mower",
        "Samuel Kim",
        "Jeannette Chang",
        "Sungbok Lee",
        "Shrikanth S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "52",
      "title": "Mel-cepstral distance measure for objective speech quality assessment",
      "authors": [
        "Robert Kubichek"
      ],
      "year": "1993",
      "venue": "Proceedings of IEEE Pacific Rim Conference on Communications Computers and Signal Processing"
    },
    {
      "citation_id": "53",
      "title": "Spectrum and prosody conversion for cross-lingual voice conversion with cyclegan",
      "authors": [
        "Zongyang Du",
        "Kun Zhou",
        "Berrak Sisman",
        "Haizhou Li"
      ],
      "year": "2020",
      "venue": "2020 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference"
    },
    {
      "citation_id": "54",
      "title": "A method for fundamental frequency estimation and voicing decision: Application to infant utterances recorded in real acoustical environments",
      "authors": [
        "Tomohiro Nakatani",
        "Shigeaki Amano",
        "Toshio Irino",
        "Kentaro Ishizuka",
        "Tadahisa Kondo"
      ],
      "year": "2008",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "55",
      "title": "Reducing f0 frame error of f0 tracking algorithms under noisy conditions with an unvoiced/voiced classification frontend",
      "authors": [
        "Wei Chu",
        "Abeer Alwan"
      ],
      "year": "2009",
      "venue": "2009 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "56",
      "title": "Neutral-to-emotional voice conversion with cross-wavelet transform f0 using generative adversarial networks",
      "authors": [
        "Zhaojie Luo",
        "Jinhui Chen",
        "Tetsuya Takiguchi",
        "Yasuo Ariki"
      ],
      "year": "2019",
      "venue": "APSIPA Transactions on Signal and Information Processing"
    }
  ]
}