{
  "paper_id": "2506.18402v1",
  "title": "Infant Cry Emotion Recognition Using Improved Ecapa-Tdnn With Multiscale Feature Fusion And Attention Enhancement",
  "published": "2025-06-23T08:39:48Z",
  "authors": [
    "Junyu Zhou",
    "Yanxiong Li",
    "Haolin Yu"
  ],
  "keywords": [
    "ECAPA-TDNN",
    "multi-scale feature fusion",
    "attention mechanism",
    "infant cry emotion recognition"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Infant cry emotion recognition is crucial for parenting and medical applications. It faces many challenges, such as subtle emotional variations, noise interference, and limited data. The existing methods lack the ability to effectively integrate multiscale features and temporal-frequency relationships. In this study, we propose a method for infant cry emotion recognition using an improved Emphasized Channel Attention, Propagation and Aggregation in Time Delay Neural Network (ECAPA-TDNN) with both multi-scale feature fusion and attention enhancement. Experiments on a public dataset show that the proposed method achieves accuracy of 82.20%, number of parameters of 1.43 MB and FLOPs of 0.32 Giga. Moreover, our method has advantage over the baseline methods in terms of accuracy. The code is at https://github.com/kkpretend/IETMA.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Infant cries encode biologically significant information through acoustic features like frequency, duration, and timbre, correlating with emotional/physical states and influenced by individual variability and environmental noise. Dunstan's theory identifies primal, reflexive vocalizations in infants aged 0-3 months linked to specific physiological needs, verified in later studies  [1] . Post-neonatal stages involve complex cries. Practical applications require multimodal integration of cry acoustics, behavioral cues, and contextual data.\n\nRecent advances in signal processing and data-driven algorithms have enabled infant cry analysis for detection  [2] -  [5] , cause classification  [6] -  [13] , and pathological diagnosis  [14] -  [16] . Traditional methods rely on handcrafted acoustic features (e.g., Mel-frequency cepstral coefficients, linear prediction coefficient, spectral centroid) combined with classifiers (e.g., support vector machine, random forest, logistic regression  [2] ,  [6] -  [8] . Hybrid methods, such as multistage ensemble models, achieve accuracies up to 93.7%  [8] .\n\nDeep-learning based methods exceed the above traditional methods by extracting deep embeddings from samples. Transfer learning with pretrained Convolutional Neural Network (CNNs) or spectrogram-based network of long-short term memory attains accuracy of 92~99% for cry detection  [5]  and cry classification  [9] . CNNs struggle with long-term dependencies, whereas recurrent neural networks (RNNs) are computationally inefficient. Models combining CNNs, RNNs, and attention mechanisms can further improve the performance of emotion recognition  [10] -  [12] .\n\nDesplanques et al. propose the ECAPA-TDNN with an attention module of Squeeze Excitation (SE), and win first place at the VoxSRC2020 competition  [17] . In a recent study  [18] , the ECAPA-TDNN is used for speaker verification based on infant cry, and an equal error rate of 25.8% is obtained. It is shown that the ECAPA-TDNN can learn effective embeddings from samples, and obtain higher accuracy than other models.\n\nInfant cries are emotionally nuanced, whose characteristics are hard to be captured by the model. Environmental noises further disrupt infant cries, and thus degrade the model's performance. In addition, training datasets of infant cries are relatively small-scale, which leads to a decrease in the model's generalization ability and adaptability. The ECAPA-TDNN has three critical limitations for emotion recognition of infant cries.\n\nFirst, its single-scale convolutional design with fixed kernel sizes and fixed dilation rates restricts the receptive field adaptability. As a result, multi-scale emotional information cannot be effectively captured. For instance, pain-induced highfrequency spikes require small receptive fields for precise localization, while hunger-induced rhythmic patterns demand larger scales to discern trends. Hence, single-scale convolution will result in blurry information and incomplete utilization of multi-scale emotional information.\n\nSecond, the ECAPA-TDNN lacks effective interlayer feature fusion mechanisms. Shallow layers capture low-level acoustic features, while deep layers extract abstract representations. The absence of cross-layer integration hampers the comprehensive interaction of features.\n\nThird, the temporal and channel attention mechanisms operate in isolation without synergistic interaction. Temporal attention focuses on time-series importance, while channel attention emphasizes frequency-band significance. However, the isolated usage of temporal and channel attentions prevents the model from effectively integrating the time-frequency intertwined emotional cues. The disconnection between temporal and channel attention modules limits the model's holistic feature integration capability and decision-making capability, and finally reduces the model's recognition accuracy.\n\nIn short, the limitations in scale adaptability, hierarchical feature fusion, and attention synergy constrain the ECAPA-TDNN's performance in recognizing infant cry emotion. To overcome the above deficiencies of the ECAPA-TDNN, we propose an improved ECAPA-TDNN for infant cry emotion recognition in this paper.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Method 2.1 Method Framework",
      "text": "The framework of the improved ECAPA-TDNN is the same as that of the ECAPA-TDNN, except for three synergistic modules. Figure  1  illustrates the frameworks of both ECAPA-TDNN and improved ECAPA-TDNN.\n\nIn the improved ECAPA-TDNN, the modules of Residual Squeeze-and-Excitation (RSE), Multi-scale Channel Attention (MCA) and differential attention are incorporated into the architecture of the ECAPA-TDNN. In addition, the global average pooling and Softmax substitute for the attentive statistics pooling and AAM-Softmax of the ECAPA-TDNN, respectively. The RSE module implements cross-dimensional interaction between the time-varying patterns and critical frequency bands, and thus effectively bridges the temporal and spectral domains. The MCA module enhances the feature extraction backbone by parallel multi-scale dilated convolution, and thus captures both localized spectro-temporal structures and extended contextual dynamics in the infant vocalizations. The global average pooling compresses the channel-wise feature representations, followed by the differential attention module, which can correct the fused channel weights by emphasizing emotion-relevant channels based on learned interdependencies. In summary, the layers within the blue dotted-line box in Figure  1 (b ) not only enhance the interaction of features between layers but also prevent the emergence of issues such as gradient vanishing.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Multi-Scale Channel Attention",
      "text": "To enhance the model's ability to extract discriminative features across different receptive fields, we propose a MCA module that integrates multi-scale feature extraction with channel-wise adaptive weighting  [13] ,  [19] . As illustrated in Figure  2 , the MCA module consists of two key components: parallel convolutional branches with varying dilation rates for multi-scale context aggregation, and a channel attention mechanism for feature recalibration.\n\nFor an input feature map ùëø ‚àà ‚Ñù ( √ó ) , the multi-scale transformation is defined by\n\nwhere ùê∂ùëúùëõùë£ √ó ( ) denotes a convolution with kernel size 1√ó ùëò (\n\nwhere T denotes the number of time steps. Subsequently, channel dependencies are learned through a gating mechanism using two fully-connected layers: ùíî = ùúé ùëæ ùüê ùõø(ùëæ ùüè ùíõ) ‚àà ‚Ñù , (5) where ùõø (‚Äß) and ùúé (‚Äß) denote ReLU function and Sigmoid activation function, respectively; ùëæ ‚àà ‚Ñù ( / )√ó and ùëæ ‚àà ‚Ñù √ó( / ) represent trainable weight matrices with reduction ratio r. The output of the block is obtained through channelwise scaling: ùë≠ = ùíî ‚äô ùë≠ ‚àà ‚Ñù √ó (6) where ‚äô denotes channel-wise multiplication. The original spatial resolution can be preserved, and discriminative channel responses can be enhanced. This approach for multi-scale feature extraction, coupled with the SE mechanism to adjust channel weights, not only addresses the issue of ECAPA-TDNN's single-scale feature extraction but also maintains the coherence of the attention mechanism. This is crucial for the classification of infant crying causes, because the usage of global pooling operations avoids the premature reduction of dimensions, which will lead to the neglect of the importance of feature extraction.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Temporal-Channel Interactive Attention Mechanism",
      "text": "The temporal-channel interactive attention module performs information interaction between temporal dimension and channel dimension via a RSE structure. Given an input feature ùëø ‚àà ‚Ñù ( √ó ) , where T and C denote numbers of time steps and channels, respectively. The processing pipeline is formalized by ùëø = ùëÖùëÜùê∏(ùëø) = ùëø + ùëÜùê∏(ùëø) ,  (7)  where ùëø represents the features after the merging of time and frequency domains, and ùëÜùê∏(‚Ä¢) denotes the Squeeze-and-Excitation operation, and is defined by\n\nwith\n\nwhere ùëæ ‚àà ‚Ñù ( / √ó ) and ùëæ ‚àà ‚Ñù ( √ó / ) represent learnable parameters; œÉ (‚Ä¢) , ‚äô and T stand for a Sigmoid function, channel-wise multiplication and the number of time steps, respectively. The RSE module preserves original temporal resolution and enhances channel adaptability. Temporal attention output ùë® ‚àà ‚Ñù ( √ó ) and channel attention output ùë® ‚àà ‚Ñù ( √ó ) are fused by ùë¥ = (ùë® ùë® ) ‚äô ùëø .\n\n(10) The final output integrates original temporal features by ùëø = ùëø + ùê∂ùëúùëõùë£1ùê∑(ùë¥) . (  11 ) For pain-related infant cries (e.g., 0.8-1.2 kHz abrupt onsets), temporal attention peaks at onset t_0 induce elevated weights for 0.8-1.2 kHz channels via ùë® ùë® correlation.\n\nThe MCA and RSE Res2Block incorporates the RSE and MCA mechanisms into the original Res2Block of the ECAPA-TDNN. Its structure is illustrated in Figure  3 . As illustrated in Figure  3 , after passing through the RSEattention module, the features are further processed by a onedimensional convolution and a Muti-Res2DilatedConv1D module. Afterwards, the channel weights are adjusted via the SE-attention module.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Channel Recalibration Via Differential Attention",
      "text": "Like global average pooling and multi-scale feature fusion, the concatenated feature tensor ùíÅ ‚àà ‚Ñù ( √ó )  where ùúÜ ‚àà ‚Ñù is a learnable scalar optimized via gradient descent; T denotes the transpose of matrix; and Split(‚Äß) is an operation for dividing a matrix into smaller matrices along a specified dimension. Each smaller matrix is then multiplied by corresponding weight matrices ùëæ ùë∏ and ùëæ ùë≤ to create multiple query and key matrices.\n\nThe dual Softmax subtraction ( ùë® -ùúÜùë® ) implements contrastive channel weighting, suppressing attention noise and amplifying discriminative spectral patterns  [20] ,  [21] . This is achieved by adaptively canceling attention scores shared across both pathways ( ùë® and ùë® ) , which correspond to noninformative features. In the fused channels of baby crying, different channels contribute to emotional expression to varying degrees. The incorporation of this module further enhances the model's ability to capture information from specific channels.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Experiments 3.1 Experimental Datasets",
      "text": "The experiments are conducted on a public dataset which is released by iFLYTEK and the University of Science and Technology of China in 2020. This dataset contains a total of 3000 audio clips. The duration of each audio clip ranges from 3 to 5 seconds. There are six types of infant cry emotions in this dataset, namely \"awake\", \"diaper\", \"hug\", \"hungry\", \"sleepy\", and \"uncomfortable\". The ratio of training set to testing set is equal to 8:2. The detailed information of this dataset can be found at: https://aistudio.baidu.com/datasetdetail/41960.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Experimental Setup",
      "text": "The hardware for performing the experiments mainly consists of an NVIDIA GeForce RTX 3060 GPU. The software mainly includes Python 3.10 and the deep learning framework of PyTorch 2.4. The audio clips are preprocessed by removing silent segments and normalizing the lengths to ensure uniformity. The model is trained for 700 epochs using the Adam optimizer with a batch size of 64, and the learning rate is set to 2 . Additionally, 13 filters are adopted for spectral feature extraction, while 128 channels are used in the neural network.\n\nTo verify the effectiveness of the improved ECAPA-TDNN, some necessary experiments are conducted. First, we conduct ablation experiments to show the performance contributions of different key modules of the improved ECAPA-TDNN. Then, we compare the improved ECAPA-TDNN with the ECAPA-TDNN and Resnet18 using multiple performance metrics. The performance metrics include accuracy, number of model parameters (Param), and Floating-Point Operations (FLOPs) required for inference per sample. In addition, the confusion matrices of different models are presented for illustrating the details of confusions between different types of emotions obtained by different models.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Experimental Results",
      "text": "Table  1  presents the ablation experimental results obtained by the improved ECAPA-TDNN with different combinations of MCA, RSE and differential-attention modules. From the results of Table  1 , we can draw two conclusions. First, each of the above three modules has an impact on the accuracy obtained by the proposed model for infant crying emotion recognition. Moreover, the MCA module has the greatest impact on the accuracy, since the accuracy drops from 82.20% to 78.37% after removing it. Second, when all the three modules are used together, the proposed model obtains the highest accuracy score of 82.20%. Hence, each module has complementarity and can work together to improve the accuracy of the proposed model. The improved ECAPA-TDNN is compared with two state-of the-art models, namely the ECAPA-TDNN and the Resnet18. The results obtained by different models are listed in Table  2 . The improved ECAPA-TDNN obtains the accuracy score of 82.20%. Its accuracy score is higher than that obtained by both ECAPA-TDNN and Resnet18. The accuracy gain obtained by the improved ECAPA-TDNN is mainly attributed to the integration of multi-scale feature extraction, optimization of the connection and coherence of the attention mechanism. The multi-scale feature extraction module can capture rich feature representations of the samples. The cross-layer connection module can realize the comprehensive interaction of features. The interaction between temporal and channel attention modules can improve the model's abilities of holistic feature integration and decision-making. Hence, the improved ECAPA-TDNN achieves higher accuracy scores compared to the two state-of-the-art models, namely ECAPA-TDNN and Resnet18. The number of parameters and the FLOPs of the improved ECAPA-TDNN are 1.43 MB and 0.32 Giga, respectively. These two values are higher than the counterparts of the ECAPA-TDNN but lower than the counterparts of the Resnet18. That is, it doesn't and does have advantage over the ECAPA-TDNN and the Resnet18 in complexity, respectively.\n\nThe confusion matrix of the improved ECAPA-TDNN for classifying infant cry emotions is shown in Figure  5 . There are differences in the degree of confusion between different categories. The confusion between \"awake\" and \"hungry\" is the highest (reaching 12.15%). However, the confusion between \"sleepy\" and \"diaper\", and the confusion between \"awake\" and \"uncomfortable\", reach the lowest value of 0. For comparison convenience, Figures  6  and 7  depict the confusion matrices of the ECAPA-TDNN and Resnet18, respectively. From the results in Figure  5  to Figure  7 , it can be known that the ECAPA-TDNN and Resnet18 have higher levels of confusion for various categories of emotions than the improved ECAPA-TDNN.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Conclusions",
      "text": "In this paper, we design a model of the improved ECAPA-TDNN for infant cry emotion recognition. It integrates multiscale feature fusion module and bidirectional temporal-channel co-attention modules. Experimental results demonstrate that the proposed model achieves the accuracy score of 82.20% which is higher than that obtained by two state-of-the-art models, namely ECAPA-TDNN and Restnet18. Hence, it has advantage over other two models in accuracy.\n\nHowever, the proposed model still has its own limitations. First, its complexity is higher than the ECAPA-TDNN. Second, its attention modules mainly concentrate on the temporalchannel interactions, but spatial dependencies in spectrogram representations remain underutilized. In the future work, we will explore hybrid attention architectures by incorporating spatial-spectral relationships. We will also take measures to reduce the complexity of the model for deploying it on edge devices, such as knowledge distillation, pruning, and parameter compression of models.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: illustrates the frameworks of both ECAPA-",
      "page": 1
    },
    {
      "caption": "Figure 1: (b) not only enhance the interaction",
      "page": 2
    },
    {
      "caption": "Figure 1: The frameworks of (a) ECAPA-TDNN and (b)",
      "page": 2
    },
    {
      "caption": "Figure 2: , the MCA module consists of two key components:",
      "page": 2
    },
    {
      "caption": "Figure 2: Structure of MCA convolution layer.",
      "page": 2
    },
    {
      "caption": "Figure 3: RSE-attention",
      "page": 3
    },
    {
      "caption": "Figure 3: Structure of MCA and RSE Res2Block.",
      "page": 3
    },
    {
      "caption": "Figure 3: , after passing through the RSE-",
      "page": 3
    },
    {
      "caption": "Figure 5: There are",
      "page": 4
    },
    {
      "caption": "Figure 5: Confusion matrix of the improved ECAPA-TDNN.",
      "page": 4
    },
    {
      "caption": "Figure 5: to Figure 7, it can be",
      "page": 4
    },
    {
      "caption": "Figure 6: Confusion matrix of the ECAPA-TDNN.",
      "page": 4
    },
    {
      "caption": "Figure 7: Confusion matrix of the improved Resnet18.",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "School of Electronic and Information Engineering, South China University of Technology, Guangzhou, China": "202230324417@mail.scut.edu.cn, eeyxli@scut.edu.cn, 1430876441@qq.com"
        },
        {
          "School of Electronic and Information Engineering, South China University of Technology, Guangzhou, China": ""
        },
        {
          "School of Electronic and Information Engineering, South China University of Technology, Guangzhou, China": "Abstract"
        },
        {
          "School of Electronic and Information Engineering, South China University of Technology, Guangzhou, China": ""
        },
        {
          "School of Electronic and Information Engineering, South China University of Technology, Guangzhou, China": ""
        },
        {
          "School of Electronic and Information Engineering, South China University of Technology, Guangzhou, China": "Infant  cry  emotion  recognition  is  crucial  for  parenting  and"
        },
        {
          "School of Electronic and Information Engineering, South China University of Technology, Guangzhou, China": ""
        },
        {
          "School of Electronic and Information Engineering, South China University of Technology, Guangzhou, China": "medical applications. It faces many challenges, such as subtle"
        },
        {
          "School of Electronic and Information Engineering, South China University of Technology, Guangzhou, China": ""
        },
        {
          "School of Electronic and Information Engineering, South China University of Technology, Guangzhou, China": "emotional variations, noise interference, and limited data. The"
        },
        {
          "School of Electronic and Information Engineering, South China University of Technology, Guangzhou, China": ""
        },
        {
          "School of Electronic and Information Engineering, South China University of Technology, Guangzhou, China": "existing methods lack the ability to effectively integrate multi-"
        },
        {
          "School of Electronic and Information Engineering, South China University of Technology, Guangzhou, China": ""
        },
        {
          "School of Electronic and Information Engineering, South China University of Technology, Guangzhou, China": "scale  features  and  temporal-frequency  relationships.  In  this"
        },
        {
          "School of Electronic and Information Engineering, South China University of Technology, Guangzhou, China": ""
        },
        {
          "School of Electronic and Information Engineering, South China University of Technology, Guangzhou, China": "study, we propose a method for infant cry emotion recognition"
        },
        {
          "School of Electronic and Information Engineering, South China University of Technology, Guangzhou, China": ""
        },
        {
          "School of Electronic and Information Engineering, South China University of Technology, Guangzhou, China": "using an improved Emphasized Channel Attention, Propagation"
        },
        {
          "School of Electronic and Information Engineering, South China University of Technology, Guangzhou, China": ""
        },
        {
          "School of Electronic and Information Engineering, South China University of Technology, Guangzhou, China": "and  Aggregation  in  Time  Delay  Neural  Network  (ECAPA-"
        },
        {
          "School of Electronic and Information Engineering, South China University of Technology, Guangzhou, China": ""
        },
        {
          "School of Electronic and Information Engineering, South China University of Technology, Guangzhou, China": "TDNN)  with  both  multi-scale  feature  fusion  and  attention"
        },
        {
          "School of Electronic and Information Engineering, South China University of Technology, Guangzhou, China": ""
        },
        {
          "School of Electronic and Information Engineering, South China University of Technology, Guangzhou, China": "enhancement.  Experiments  on  a  public  dataset  show  that  the"
        },
        {
          "School of Electronic and Information Engineering, South China University of Technology, Guangzhou, China": ""
        },
        {
          "School of Electronic and Information Engineering, South China University of Technology, Guangzhou, China": "proposed  method  achieves  accuracy  of  82.20%,  number  of"
        },
        {
          "School of Electronic and Information Engineering, South China University of Technology, Guangzhou, China": ""
        },
        {
          "School of Electronic and Information Engineering, South China University of Technology, Guangzhou, China": "parameters of 1.43 MB and FLOPs of 0.32 Giga. Moreover, our"
        },
        {
          "School of Electronic and Information Engineering, South China University of Technology, Guangzhou, China": ""
        },
        {
          "School of Electronic and Information Engineering, South China University of Technology, Guangzhou, China": "method  has  advantage  over  the  baseline  methods  in  terms  of"
        },
        {
          "School of Electronic and Information Engineering, South China University of Technology, Guangzhou, China": ""
        },
        {
          "School of Electronic and Information Engineering, South China University of Technology, Guangzhou, China": "accuracy. The code is at https://github.com/kkpretend/IETMA."
        },
        {
          "School of Electronic and Information Engineering, South China University of Technology, Guangzhou, China": ""
        },
        {
          "School of Electronic and Information Engineering, South China University of Technology, Guangzhou, China": "Index  Terms:  ECAPA-TDNN,  multi-scale \nfeature \nfusion,"
        },
        {
          "School of Electronic and Information Engineering, South China University of Technology, Guangzhou, China": ""
        },
        {
          "School of Electronic and Information Engineering, South China University of Technology, Guangzhou, China": "attention mechanism, infant cry emotion recognition"
        },
        {
          "School of Electronic and Information Engineering, South China University of Technology, Guangzhou, China": ""
        },
        {
          "School of Electronic and Information Engineering, South China University of Technology, Guangzhou, China": ""
        },
        {
          "School of Electronic and Information Engineering, South China University of Technology, Guangzhou, China": "1. \nIntroduction"
        },
        {
          "School of Electronic and Information Engineering, South China University of Technology, Guangzhou, China": ""
        },
        {
          "School of Electronic and Information Engineering, South China University of Technology, Guangzhou, China": "Infant  cries  encode  biologically \nsignificant \ninformation"
        },
        {
          "School of Electronic and Information Engineering, South China University of Technology, Guangzhou, China": "through acoustic features like frequency, duration, and timbre,"
        },
        {
          "School of Electronic and Information Engineering, South China University of Technology, Guangzhou, China": "correlating  with  emotional/physical  states  and  influenced  by"
        },
        {
          "School of Electronic and Information Engineering, South China University of Technology, Guangzhou, China": "individual  variability  and  environmental  noise.  Dunstan‚Äôs"
        },
        {
          "School of Electronic and Information Engineering, South China University of Technology, Guangzhou, China": "theory identifies primal, reflexive vocalizations in infants aged"
        },
        {
          "School of Electronic and Information Engineering, South China University of Technology, Guangzhou, China": "0-3 months linked to specific physiological needs, verified in"
        },
        {
          "School of Electronic and Information Engineering, South China University of Technology, Guangzhou, China": "later  studies  [1].  Post-neonatal  stages  involve  complex  cries."
        },
        {
          "School of Electronic and Information Engineering, South China University of Technology, Guangzhou, China": "Practical  applications  require  multimodal  integration  of  cry"
        },
        {
          "School of Electronic and Information Engineering, South China University of Technology, Guangzhou, China": "acoustics, behavioral cues, and contextual data."
        },
        {
          "School of Electronic and Information Engineering, South China University of Technology, Guangzhou, China": "Recent \nadvances \nin \nsignal  processing \nand  data-driven"
        },
        {
          "School of Electronic and Information Engineering, South China University of Technology, Guangzhou, China": "algorithms have enabled infant  cry  analysis  for detection [2]-"
        },
        {
          "School of Electronic and Information Engineering, South China University of Technology, Guangzhou, China": "[5],  cause  classification  [6]-[13],  and  pathological  diagnosis"
        },
        {
          "School of Electronic and Information Engineering, South China University of Technology, Guangzhou, China": "[14]-[16].  Traditional  methods  rely  on  handcrafted  acoustic"
        },
        {
          "School of Electronic and Information Engineering, South China University of Technology, Guangzhou, China": "features \n(e.g.,  Mel-frequency \ncepstral \ncoefficients, \nlinear"
        },
        {
          "School of Electronic and Information Engineering, South China University of Technology, Guangzhou, China": "prediction \ncoefficient, \nspectral \ncentroid) \ncombined  with"
        },
        {
          "School of Electronic and Information Engineering, South China University of Technology, Guangzhou, China": "classifiers (e.g., support vector machine, random forest, logistic"
        },
        {
          "School of Electronic and Information Engineering, South China University of Technology, Guangzhou, China": "regression  [2],  [6]-[8].  Hybrid  methods,  such  as  multistage"
        },
        {
          "School of Electronic and Information Engineering, South China University of Technology, Guangzhou, China": "ensemble models, achieve accuracies up to 93.7% [8]."
        },
        {
          "School of Electronic and Information Engineering, South China University of Technology, Guangzhou, China": "Deep-learning  based  methods  exceed  the  above  traditional"
        },
        {
          "School of Electronic and Information Engineering, South China University of Technology, Guangzhou, China": "methods by extracting deep embeddings from samples. Transfer"
        },
        {
          "School of Electronic and Information Engineering, South China University of Technology, Guangzhou, China": "learning with pretrained Convolutional Neural Network (CNNs)"
        },
        {
          "School of Electronic and Information Engineering, South China University of Technology, Guangzhou, China": "or  spectrogram-based  network  of \nlong-short \nterm  memory"
        },
        {
          "School of Electronic and Information Engineering, South China University of Technology, Guangzhou, China": "attains  accuracy  of  92~99%  for  cry  detection  [5]  and  cry"
        },
        {
          "School of Electronic and Information Engineering, South China University of Technology, Guangzhou, China": "classification [9]. CNNs struggle with long-term dependencies,"
        },
        {
          "School of Electronic and Information Engineering, South China University of Technology, Guangzhou, China": "whereas recurrent neural networks (RNNs) are computationally"
        },
        {
          "School of Electronic and Information Engineering, South China University of Technology, Guangzhou, China": ""
        },
        {
          "School of Electronic and Information Engineering, South China University of Technology, Guangzhou, China": "inefficient.  Models  combining  CNNs,  RNNs,  and  attention"
        },
        {
          "School of Electronic and Information Engineering, South China University of Technology, Guangzhou, China": ""
        },
        {
          "School of Electronic and Information Engineering, South China University of Technology, Guangzhou, China": "mechanisms can further improve the performance of  emotion"
        },
        {
          "School of Electronic and Information Engineering, South China University of Technology, Guangzhou, China": ""
        },
        {
          "School of Electronic and Information Engineering, South China University of Technology, Guangzhou, China": "recognition [10]-[12]."
        },
        {
          "School of Electronic and Information Engineering, South China University of Technology, Guangzhou, China": ""
        },
        {
          "School of Electronic and Information Engineering, South China University of Technology, Guangzhou, China": "Desplanques  et  al.  propose \nthe  ECAPA-TDNN  with  an"
        },
        {
          "School of Electronic and Information Engineering, South China University of Technology, Guangzhou, China": ""
        },
        {
          "School of Electronic and Information Engineering, South China University of Technology, Guangzhou, China": "attention  module  of  Squeeze  Excitation  (SE),  and  win  first"
        },
        {
          "School of Electronic and Information Engineering, South China University of Technology, Guangzhou, China": ""
        },
        {
          "School of Electronic and Information Engineering, South China University of Technology, Guangzhou, China": "place at the VoxSRC2020 competition [17]. In a recent study"
        },
        {
          "School of Electronic and Information Engineering, South China University of Technology, Guangzhou, China": ""
        },
        {
          "School of Electronic and Information Engineering, South China University of Technology, Guangzhou, China": "ÔÄ™ Corresponding author: Yanxiong Li (eeyxli@scut.edu.cn)"
        },
        {
          "School of Electronic and Information Engineering, South China University of Technology, Guangzhou, China": ""
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "output": ""
        },
        {
          "output": ""
        },
        {
          "output": "SE-attention"
        },
        {
          "output": ""
        },
        {
          "output": ""
        },
        {
          "output": ""
        },
        {
          "output": "Concat"
        },
        {
          "output": ""
        },
        {
          "output": ""
        },
        {
          "output": ""
        },
        {
          "output": ""
        },
        {
          "output": ""
        },
        {
          "output": ""
        },
        {
          "output": ""
        },
        {
          "output": ""
        },
        {
          "output": ""
        },
        {
          "output": ""
        },
        {
          "output": ""
        },
        {
          "output": ""
        },
        {
          "output": ""
        },
        {
          "output": ""
        },
        {
          "output": "Input"
        },
        {
          "output": ""
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "dotted-line box in Figure 1 (b) not only enhance the interaction": "of  features  between  layers  but  also  prevent  the  emergence  of"
        },
        {
          "dotted-line box in Figure 1 (b) not only enhance the interaction": ""
        },
        {
          "dotted-line box in Figure 1 (b) not only enhance the interaction": "issues such as gradient vanishing."
        },
        {
          "dotted-line box in Figure 1 (b) not only enhance the interaction": ""
        },
        {
          "dotted-line box in Figure 1 (b) not only enhance the interaction": ""
        },
        {
          "dotted-line box in Figure 1 (b) not only enhance the interaction": ""
        },
        {
          "dotted-line box in Figure 1 (b) not only enhance the interaction": "SE-attention"
        },
        {
          "dotted-line box in Figure 1 (b) not only enhance the interaction": ""
        },
        {
          "dotted-line box in Figure 1 (b) not only enhance the interaction": "+"
        },
        {
          "dotted-line box in Figure 1 (b) not only enhance the interaction": "output"
        },
        {
          "dotted-line box in Figure 1 (b) not only enhance the interaction": ""
        },
        {
          "dotted-line box in Figure 1 (b) not only enhance the interaction": "Conv1d+ReLu+BN\nConv1D+ReLU+BN"
        },
        {
          "dotted-line box in Figure 1 (b) not only enhance the interaction": ""
        },
        {
          "dotted-line box in Figure 1 (b) not only enhance the interaction": "RSE-attention"
        },
        {
          "dotted-line box in Figure 1 (b) not only enhance the interaction": "SE-Res2Block"
        },
        {
          "dotted-line box in Figure 1 (b) not only enhance the interaction": "+"
        },
        {
          "dotted-line box in Figure 1 (b) not only enhance the interaction": "MSERes2Block"
        },
        {
          "dotted-line box in Figure 1 (b) not only enhance the interaction": "SE-Res2Block"
        },
        {
          "dotted-line box in Figure 1 (b) not only enhance the interaction": "+"
        },
        {
          "dotted-line box in Figure 1 (b) not only enhance the interaction": ""
        },
        {
          "dotted-line box in Figure 1 (b) not only enhance the interaction": "MSERes2Block"
        },
        {
          "dotted-line box in Figure 1 (b) not only enhance the interaction": "SE-Res2Block"
        },
        {
          "dotted-line box in Figure 1 (b) not only enhance the interaction": "+\n+"
        },
        {
          "dotted-line box in Figure 1 (b) not only enhance the interaction": "MSERes2Block"
        },
        {
          "dotted-line box in Figure 1 (b) not only enhance the interaction": "Conv1d+ReLU"
        },
        {
          "dotted-line box in Figure 1 (b) not only enhance the interaction": "+"
        },
        {
          "dotted-line box in Figure 1 (b) not only enhance the interaction": "Attentive Stat Pooling+BN"
        },
        {
          "dotted-line box in Figure 1 (b) not only enhance the interaction": "Global Average Pooling+BN"
        },
        {
          "dotted-line box in Figure 1 (b) not only enhance the interaction": ""
        },
        {
          "dotted-line box in Figure 1 (b) not only enhance the interaction": "Differential-attention\nFC+BN"
        },
        {
          "dotted-line box in Figure 1 (b) not only enhance the interaction": "FC+BN"
        },
        {
          "dotted-line box in Figure 1 (b) not only enhance the interaction": "AAM-Softmax"
        },
        {
          "dotted-line box in Figure 1 (b) not only enhance the interaction": "Soft-max"
        },
        {
          "dotted-line box in Figure 1 (b) not only enhance the interaction": ""
        },
        {
          "dotted-line box in Figure 1 (b) not only enhance the interaction": "output\noutput"
        },
        {
          "dotted-line box in Figure 1 (b) not only enhance the interaction": ""
        },
        {
          "dotted-line box in Figure 1 (b) not only enhance the interaction": "(a)\n(b)"
        },
        {
          "dotted-line box in Figure 1 (b) not only enhance the interaction": ""
        },
        {
          "dotted-line box in Figure 1 (b) not only enhance the interaction": "Figure  1  The \nframeworks  of \n(a)  ECAPA-TDNN  and \n(b)"
        },
        {
          "dotted-line box in Figure 1 (b) not only enhance the interaction": ""
        },
        {
          "dotted-line box in Figure 1 (b) not only enhance the interaction": "improved  ECAPA-TDNN.  As  shown \nin  subgraph \n(b), \nthe"
        },
        {
          "dotted-line box in Figure 1 (b) not only enhance the interaction": ""
        },
        {
          "dotted-line box in Figure 1 (b) not only enhance the interaction": "improved parts include modules in the green background box,"
        },
        {
          "dotted-line box in Figure 1 (b) not only enhance the interaction": ""
        },
        {
          "dotted-line box in Figure 1 (b) not only enhance the interaction": "modules in the blue background box, and modules in the blue"
        },
        {
          "dotted-line box in Figure 1 (b) not only enhance the interaction": ""
        },
        {
          "dotted-line box in Figure 1 (b) not only enhance the interaction": "dotted-line box."
        },
        {
          "dotted-line box in Figure 1 (b) not only enhance the interaction": ""
        },
        {
          "dotted-line box in Figure 1 (b) not only enhance the interaction": ""
        },
        {
          "dotted-line box in Figure 1 (b) not only enhance the interaction": "2.2 Multi-Scale Channel Attention"
        },
        {
          "dotted-line box in Figure 1 (b) not only enhance the interaction": "To  enhance  the  model‚Äôs  ability  to  extract  discriminative"
        },
        {
          "dotted-line box in Figure 1 (b) not only enhance the interaction": "features  across  different  receptive fields,  we  propose  a  MCA"
        },
        {
          "dotted-line box in Figure 1 (b) not only enhance the interaction": ""
        },
        {
          "dotted-line box in Figure 1 (b) not only enhance the interaction": "module \nthat \nintegrates  multi-scale \nfeature  extraction  with"
        },
        {
          "dotted-line box in Figure 1 (b) not only enhance the interaction": ""
        },
        {
          "dotted-line box in Figure 1 (b) not only enhance the interaction": "channel-wise  adaptive  weighting  [13],  [19].  As  illustrated  in"
        },
        {
          "dotted-line box in Figure 1 (b) not only enhance the interaction": ""
        },
        {
          "dotted-line box in Figure 1 (b) not only enhance the interaction": "Figure  2,  the  MCA  module  consists  of  two  key  components:"
        },
        {
          "dotted-line box in Figure 1 (b) not only enhance the interaction": ""
        },
        {
          "dotted-line box in Figure 1 (b) not only enhance the interaction": "parallel convolutional branches with varying dilation rates for"
        },
        {
          "dotted-line box in Figure 1 (b) not only enhance the interaction": ""
        },
        {
          "dotted-line box in Figure 1 (b) not only enhance the interaction": "multi-scale \ncontext \naggregation, \nand \na \nchannel \nattention"
        },
        {
          "dotted-line box in Figure 1 (b) not only enhance the interaction": ""
        },
        {
          "dotted-line box in Figure 1 (b) not only enhance the interaction": "mechanism for feature recalibration."
        },
        {
          "dotted-line box in Figure 1 (b) not only enhance the interaction": ""
        },
        {
          "dotted-line box in Figure 1 (b) not only enhance the interaction": "For  an \ninput \n, \nthe  multi-scale \nfeature  map  ùëø ‚àà ‚Ñù((cid:3004)√ó(cid:3021))"
        },
        {
          "dotted-line box in Figure 1 (b) not only enhance the interaction": ""
        },
        {
          "dotted-line box in Figure 1 (b) not only enhance the interaction": "transformation is defined by"
        },
        {
          "dotted-line box in Figure 1 (b) not only enhance the interaction": ""
        },
        {
          "dotted-line box in Figure 1 (b) not only enhance the interaction": "(1) \n((cid:3031)(cid:2880)(cid:3031)(cid:3286))(ùê∂ùëúùëõùë£(cid:2869)√ó(cid:2869)(ùëø))"
        },
        {
          "dotted-line box in Figure 1 (b) not only enhance the interaction": "ùë≠‚Çñ = ùê∂ùëúùëõùë£(cid:2869)√ó(cid:3038)"
        },
        {
          "dotted-line box in Figure 1 (b) not only enhance the interaction": "((cid:3031)(cid:2880)(cid:3031)(cid:3286)) denotes a convolution with kernel size 1√ó ùëò"
        },
        {
          "dotted-line box in Figure 1 (b) not only enhance the interaction": "where ùê∂ùëúùëõùë£(cid:2869)√ó(cid:3038)"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table 1: presents the ablation experimental results obtained",
      "data": [
        {
          "with": "(cid:3021)",
          "descent;  T  denotes  the  transpose  of  matrix;  and  Split(‚Äß)  is  an": ""
        },
        {
          "with": "",
          "descent;  T  denotes  the  transpose  of  matrix;  and  Split(‚Äß)  is  an": "operation  for  dividing  a  matrix  into  smaller  matrices  along  a"
        },
        {
          "with": "ùíõ  = (1/ùëá) (cid:3533) ùëø(ùë°)\n‚àà ‚Ñù(cid:3004) ,\n(9)",
          "descent;  T  denotes  the  transpose  of  matrix;  and  Split(‚Äß)  is  an": "specified dimension. Each smaller matrix is then multiplied by"
        },
        {
          "with": "(cid:3047)(cid:2880)(cid:2869)",
          "descent;  T  denotes  the  transpose  of  matrix;  and  Split(‚Äß)  is  an": "corresponding weight matrices  ùëæùë∏ and ùëæùë≤ to create multiple"
        },
        {
          "with": "where ùëæ(cid:2869) ‚àà ‚Ñù((cid:3004)/(cid:3045)√ó(cid:3004)) and ùëæ(cid:2870) ‚àà ‚Ñù((cid:3004)√ó(cid:3004)/(cid:3045)) represent  learnable",
          "descent;  T  denotes  the  transpose  of  matrix;  and  Split(‚Äß)  is  an": "query and key matrices."
        },
        {
          "with": "parameters;  œÉ (¬∑) ,  ‚äô  and  T  stand  for  a  Sigmoid  function,",
          "descent;  T  denotes  the  transpose  of  matrix;  and  Split(‚Äß)  is  an": ""
        },
        {
          "with": "",
          "descent;  T  denotes  the  transpose  of  matrix;  and  Split(‚Äß)  is  an": "The  dual  Softmax \nsubtraction  ( ùë®(cid:2869)  ‚àí ùúÜùë®(cid:2870))  implements"
        },
        {
          "with": "channel-wise  multiplication  and \nthe  number  of \ntime  steps,",
          "descent;  T  denotes  the  transpose  of  matrix;  and  Split(‚Äß)  is  an": "contrastive channel weighting, suppressing attention noise and"
        },
        {
          "with": "respectively.  The  RSE  module  preserves  original \ntemporal",
          "descent;  T  denotes  the  transpose  of  matrix;  and  Split(‚Äß)  is  an": "amplifying  discriminative spectral  patterns  [20],  [21].  This  is"
        },
        {
          "with": "resolution \nand \nenhances \nchannel \nadaptability.  Temporal",
          "descent;  T  denotes  the  transpose  of  matrix;  and  Split(‚Äß)  is  an": "achieved by adaptively canceling attention scores shared across"
        },
        {
          "with": "attention output ùë®(cid:3047) ‚àà ‚Ñù((cid:3021)√ó(cid:2869)) and channel attention output ùë®(cid:3030) ‚àà",
          "descent;  T  denotes  the  transpose  of  matrix;  and  Split(‚Äß)  is  an": "correspond \nto  non-\nboth  pathways  ( ùë®(cid:2869) and ùë®(cid:2870)) ,  which"
        },
        {
          "with": "‚Ñù((cid:2869)√ó(cid:3004)) are fused by",
          "descent;  T  denotes  the  transpose  of  matrix;  and  Split(‚Äß)  is  an": "informative  features.  In  the  fused  channels  of  baby  crying,"
        },
        {
          "with": "",
          "descent;  T  denotes  the  transpose  of  matrix;  and  Split(‚Äß)  is  an": "different channels contribute to emotional expression to varying"
        },
        {
          "with": "(10) \nùë¥  = (ùë®(cid:3047) ùë®(cid:3030)\n(cid:3021)) ‚äô ùëø(cid:3045)(cid:3046)(cid:3032) .",
          "descent;  T  denotes  the  transpose  of  matrix;  and  Split(‚Äß)  is  an": ""
        },
        {
          "with": "The final output integrates original temporal features by",
          "descent;  T  denotes  the  transpose  of  matrix;  and  Split(‚Äß)  is  an": "degrees. The incorporation of this module further enhances the"
        },
        {
          "with": "",
          "descent;  T  denotes  the  transpose  of  matrix;  and  Split(‚Äß)  is  an": "model's ability to capture information from specific channels."
        },
        {
          "with": "(11) \nùëø(cid:3042)(cid:3048)(cid:3047) =  ùëø  + ùê∂ùëúùëõùë£1ùê∑(ùë¥) .",
          "descent;  T  denotes  the  transpose  of  matrix;  and  Split(‚Äß)  is  an": ""
        },
        {
          "with": "For pain-related infant cries (e.g., 0.8-1.2 kHz abrupt onsets),",
          "descent;  T  denotes  the  transpose  of  matrix;  and  Split(‚Äß)  is  an": ""
        },
        {
          "with": "temporal  attention peaks  at  onset t_0 induce  elevated  weights",
          "descent;  T  denotes  the  transpose  of  matrix;  and  Split(‚Äß)  is  an": "3. Experiments"
        },
        {
          "with": "(cid:3021) correlation. \nfor 0.8-1.2 kHz channels via ùë®(cid:3047)ùë®(cid:3030)",
          "descent;  T  denotes  the  transpose  of  matrix;  and  Split(‚Äß)  is  an": ""
        },
        {
          "with": "",
          "descent;  T  denotes  the  transpose  of  matrix;  and  Split(‚Äß)  is  an": "3.1 Experimental Datasets"
        },
        {
          "with": "The  MCA  and  RSE  Res2Block  incorporates  the  RSE  and",
          "descent;  T  denotes  the  transpose  of  matrix;  and  Split(‚Äß)  is  an": ""
        },
        {
          "with": "MCA mechanisms into the original Res2Block of the ECAPA-",
          "descent;  T  denotes  the  transpose  of  matrix;  and  Split(‚Äß)  is  an": ""
        },
        {
          "with": "",
          "descent;  T  denotes  the  transpose  of  matrix;  and  Split(‚Äß)  is  an": "The experiments are conducted on a public dataset which is"
        },
        {
          "with": "TDNN. Its structure is illustrated in Figure 3.",
          "descent;  T  denotes  the  transpose  of  matrix;  and  Split(‚Äß)  is  an": ""
        },
        {
          "with": "",
          "descent;  T  denotes  the  transpose  of  matrix;  and  Split(‚Äß)  is  an": "released  by \niFLYTEK  and \nthe  University  of  Science  and"
        },
        {
          "with": "",
          "descent;  T  denotes  the  transpose  of  matrix;  and  Split(‚Äß)  is  an": "Technology of China in 2020. This dataset contains a total of"
        },
        {
          "with": "input",
          "descent;  T  denotes  the  transpose  of  matrix;  and  Split(‚Äß)  is  an": ""
        },
        {
          "with": "",
          "descent;  T  denotes  the  transpose  of  matrix;  and  Split(‚Äß)  is  an": "3000 audio clips. The duration of each audio clip ranges from 3"
        },
        {
          "with": "",
          "descent;  T  denotes  the  transpose  of  matrix;  and  Split(‚Äß)  is  an": "to 5 seconds. There are six types of infant cry emotions in this"
        },
        {
          "with": "RSE-attention",
          "descent;  T  denotes  the  transpose  of  matrix;  and  Split(‚Äß)  is  an": ""
        },
        {
          "with": "C*T",
          "descent;  T  denotes  the  transpose  of  matrix;  and  Split(‚Äß)  is  an": ""
        },
        {
          "with": "",
          "descent;  T  denotes  the  transpose  of  matrix;  and  Split(‚Äß)  is  an": "dataset, namely ‚Äúawake‚Äù, ‚Äúdiaper‚Äù, ‚Äúhug‚Äù, ‚Äúhungry‚Äù, ‚Äúsleepy‚Äù,"
        },
        {
          "with": "",
          "descent;  T  denotes  the  transpose  of  matrix;  and  Split(‚Äß)  is  an": "and ‚Äúuncomfortable‚Äù. The ratio of training set to testing set is"
        },
        {
          "with": "MCA+BN+ReLU\n+",
          "descent;  T  denotes  the  transpose  of  matrix;  and  Split(‚Äß)  is  an": ""
        },
        {
          "with": "",
          "descent;  T  denotes  the  transpose  of  matrix;  and  Split(‚Äß)  is  an": "equal  to  8:2.  The  detailed  information  of  this  dataset  can  be"
        },
        {
          "with": "C/2*T\nConv1d+BN+ReLU",
          "descent;  T  denotes  the  transpose  of  matrix;  and  Split(‚Äß)  is  an": "found at: https://aistudio.baidu.com/datasetdetail/41960."
        },
        {
          "with": "Conv1d+BN",
          "descent;  T  denotes  the  transpose  of  matrix;  and  Split(‚Äß)  is  an": ""
        },
        {
          "with": "",
          "descent;  T  denotes  the  transpose  of  matrix;  and  Split(‚Äß)  is  an": "3.2 Experimental Setup"
        },
        {
          "with": "Muti-Res2DilatedConv1D",
          "descent;  T  denotes  the  transpose  of  matrix;  and  Split(‚Äß)  is  an": ""
        },
        {
          "with": "+",
          "descent;  T  denotes  the  transpose  of  matrix;  and  Split(‚Äß)  is  an": "The hardware for performing the experiments mainly consists"
        },
        {
          "with": "C*T",
          "descent;  T  denotes  the  transpose  of  matrix;  and  Split(‚Äß)  is  an": "of an NVIDIA GeForce RTX 3060 GPU. The software mainly"
        },
        {
          "with": "output\nConv1d+BN",
          "descent;  T  denotes  the  transpose  of  matrix;  and  Split(‚Äß)  is  an": ""
        },
        {
          "with": "",
          "descent;  T  denotes  the  transpose  of  matrix;  and  Split(‚Äß)  is  an": "includes  Python  3.10  and \nthe  deep \nlearning  framework  of"
        },
        {
          "with": "",
          "descent;  T  denotes  the  transpose  of  matrix;  and  Split(‚Äß)  is  an": "PyTorch  2.4.  The  audio  clips  are  preprocessed  by  removing"
        },
        {
          "with": "",
          "descent;  T  denotes  the  transpose  of  matrix;  and  Split(‚Äß)  is  an": "silent \nsegments \nand \nnormalizing \nthe \nlengths \nto \nensure"
        },
        {
          "with": "SE-attention",
          "descent;  T  denotes  the  transpose  of  matrix;  and  Split(‚Äß)  is  an": ""
        },
        {
          "with": "",
          "descent;  T  denotes  the  transpose  of  matrix;  and  Split(‚Äß)  is  an": "uniformity. The model is trained for 700 epochs using the Adam"
        },
        {
          "with": "",
          "descent;  T  denotes  the  transpose  of  matrix;  and  Split(‚Äß)  is  an": "optimizer with a batch size of  64, and the learning rate is set"
        },
        {
          "with": "+",
          "descent;  T  denotes  the  transpose  of  matrix;  and  Split(‚Äß)  is  an": ""
        },
        {
          "with": "",
          "descent;  T  denotes  the  transpose  of  matrix;  and  Split(‚Äß)  is  an": "to 2(cid:2879)(cid:2873). Additionally, 13 filters are adopted for spectral feature"
        },
        {
          "with": "output",
          "descent;  T  denotes  the  transpose  of  matrix;  and  Split(‚Äß)  is  an": "extraction, while 128 channels are used in the neural network."
        },
        {
          "with": "",
          "descent;  T  denotes  the  transpose  of  matrix;  and  Split(‚Äß)  is  an": "To verify the effectiveness of the improved ECAPA-TDNN,"
        },
        {
          "with": "Figure 3 Structure of MCA and RSE Res2Block.",
          "descent;  T  denotes  the  transpose  of  matrix;  and  Split(‚Äß)  is  an": ""
        },
        {
          "with": "",
          "descent;  T  denotes  the  transpose  of  matrix;  and  Split(‚Äß)  is  an": "some necessary experiments are conducted. First, we conduct"
        },
        {
          "with": "",
          "descent;  T  denotes  the  transpose  of  matrix;  and  Split(‚Äß)  is  an": "ablation experiments to show the performance contributions of"
        },
        {
          "with": "As  illustrated  in  Figure  3,  after  passing  through  the  RSE-",
          "descent;  T  denotes  the  transpose  of  matrix;  and  Split(‚Äß)  is  an": ""
        },
        {
          "with": "",
          "descent;  T  denotes  the  transpose  of  matrix;  and  Split(‚Äß)  is  an": "different key modules of the improved ECAPA-TDNN. Then,"
        },
        {
          "with": "attention module, the features are further processed by a one-",
          "descent;  T  denotes  the  transpose  of  matrix;  and  Split(‚Äß)  is  an": ""
        },
        {
          "with": "",
          "descent;  T  denotes  the  transpose  of  matrix;  and  Split(‚Äß)  is  an": "we compare  the  improved  ECAPA-TDNN with  the  ECAPA-"
        },
        {
          "with": "dimensional \nconvolution \nand \na  Muti-Res2DilatedConv1D",
          "descent;  T  denotes  the  transpose  of  matrix;  and  Split(‚Äß)  is  an": ""
        },
        {
          "with": "",
          "descent;  T  denotes  the  transpose  of  matrix;  and  Split(‚Äß)  is  an": "TDNN and Resnet18 using multiple performance metrics. The"
        },
        {
          "with": "module. Afterwards,  the channel weights  are adjusted via the",
          "descent;  T  denotes  the  transpose  of  matrix;  and  Split(‚Äß)  is  an": ""
        },
        {
          "with": "",
          "descent;  T  denotes  the  transpose  of  matrix;  and  Split(‚Äß)  is  an": "performance  metrics \ninclude  accuracy,  number  of  model"
        },
        {
          "with": "SE-attention module.",
          "descent;  T  denotes  the  transpose  of  matrix;  and  Split(‚Äß)  is  an": ""
        },
        {
          "with": "",
          "descent;  T  denotes  the  transpose  of  matrix;  and  Split(‚Äß)  is  an": "parameters  (Param),  and  Floating-Point  Operations  (FLOPs)"
        },
        {
          "with": "2.4 Channel Recalibration via Differential Attention",
          "descent;  T  denotes  the  transpose  of  matrix;  and  Split(‚Äß)  is  an": "required  for  inference  per  sample.  In  addition,  the  confusion"
        },
        {
          "with": "",
          "descent;  T  denotes  the  transpose  of  matrix;  and  Split(‚Äß)  is  an": "matrices  of  different  models  are presented  for  illustrating  the"
        },
        {
          "with": "Like global average pooling and multi-scale feature fusion,",
          "descent;  T  denotes  the  transpose  of  matrix;  and  Split(‚Äß)  is  an": ""
        },
        {
          "with": "",
          "descent;  T  denotes  the  transpose  of  matrix;  and  Split(‚Äß)  is  an": "details  of  confusions  between  different \ntypes  of  emotions"
        },
        {
          "with": "the concatenated feature tensor ùíÅ ‚àà ‚Ñù((cid:3015)√ó(cid:3004)) undergoes channel-",
          "descent;  T  denotes  the  transpose  of  matrix;  and  Split(‚Äß)  is  an": ""
        },
        {
          "with": "",
          "descent;  T  denotes  the  transpose  of  matrix;  and  Split(‚Äß)  is  an": "obtained by different models."
        },
        {
          "with": "wise recalibration by a differential attention operation of",
          "descent;  T  denotes  the  transpose  of  matrix;  and  Split(‚Äß)  is  an": ""
        },
        {
          "with": "",
          "descent;  T  denotes  the  transpose  of  matrix;  and  Split(‚Äß)  is  an": "3.3 Experimental Results"
        },
        {
          "with": "(12) \nùíÅ(cid:3042)(cid:3048)(cid:3047) = ( ùë®(cid:2869)  ‚àí ùúÜùë®(cid:2870) ) ùëΩ √ó ùíÅ",
          "descent;  T  denotes  the  transpose  of  matrix;  and  Split(‚Äß)  is  an": ""
        },
        {
          "with": "where:",
          "descent;  T  denotes  the  transpose  of  matrix;  and  Split(‚Äß)  is  an": ""
        },
        {
          "with": "",
          "descent;  T  denotes  the  transpose  of  matrix;  and  Split(‚Äß)  is  an": "Table 1 presents the ablation experimental results obtained"
        },
        {
          "with": "(cid:2904)/‚àöùëë(cid:3439)\n(13)",
          "descent;  T  denotes  the  transpose  of  matrix;  and  Split(‚Äß)  is  an": "by  the improved ECAPA-TDNN with  different  combinations"
        },
        {
          "with": "ùë®(cid:2869)  = ùëÜùëúùëìùë°ùëöùëéùë•(cid:3435) ùë∏(cid:2869)ùë≤(cid:2869)",
          "descent;  T  denotes  the  transpose  of  matrix;  and  Split(‚Äß)  is  an": ""
        },
        {
          "with": "",
          "descent;  T  denotes  the  transpose  of  matrix;  and  Split(‚Äß)  is  an": "of  MCA,  RSE  and  differential-attention  modules.  From  the"
        },
        {
          "with": "ùë∏(cid:2869) = ùëÜùëùùëôùëñùë°(cid:3435) ùíÅ ùëæùë∏ùíõ(cid:3117)(cid:3439)",
          "descent;  T  denotes  the  transpose  of  matrix;  and  Split(‚Äß)  is  an": "results of Table 1, we can draw two conclusions. First, each of"
        },
        {
          "with": "(cid:4682)\n(14)",
          "descent;  T  denotes  the  transpose  of  matrix;  and  Split(‚Äß)  is  an": ""
        },
        {
          "with": "ùë≤(cid:2869) = ùëÜùëùùëôùëñùë°(cid:3435) ùíÅ ùëæùë≤(cid:3117)(cid:3439)",
          "descent;  T  denotes  the  transpose  of  matrix;  and  Split(‚Äß)  is  an": "the above three modules has an impact on the accuracy obtained"
        },
        {
          "with": "",
          "descent;  T  denotes  the  transpose  of  matrix;  and  Split(‚Äß)  is  an": "by the proposed model for infant crying emotion recognition."
        },
        {
          "with": "(cid:2904)/‚àöùëë(cid:3439)\n(15) \nùë®(cid:2870)  = ùëÜùëúùëìùë°ùëöùëéùë•(cid:3435) ùë∏(cid:2870)ùë≤(cid:2870)",
          "descent;  T  denotes  the  transpose  of  matrix;  and  Split(‚Äß)  is  an": ""
        },
        {
          "with": "",
          "descent;  T  denotes  the  transpose  of  matrix;  and  Split(‚Äß)  is  an": "Moreover,  the  MCA  module  has  the  greatest  impact  on  the"
        },
        {
          "with": "ùë∏(cid:2870) = ùëÜùëùùëôùëñùë°(cid:3435) ùíÅ ùëæùë∏(cid:3118)(cid:3439)",
          "descent;  T  denotes  the  transpose  of  matrix;  and  Split(‚Äß)  is  an": "accuracy,  since  the  accuracy  drops  from  82.20%  to  78.37%"
        },
        {
          "with": "(cid:4682)\n(16)",
          "descent;  T  denotes  the  transpose  of  matrix;  and  Split(‚Äß)  is  an": ""
        },
        {
          "with": "",
          "descent;  T  denotes  the  transpose  of  matrix;  and  Split(‚Äß)  is  an": "after removing it. Second, when all the three modules are used"
        },
        {
          "with": "ùë≤(cid:2870) = ùëÜùëùùëôùëñùë°(cid:3435) ùíÅ ùëæùë≤(cid:3118)(cid:3439)",
          "descent;  T  denotes  the  transpose  of  matrix;  and  Split(‚Äß)  is  an": ""
        },
        {
          "with": "",
          "descent;  T  denotes  the  transpose  of  matrix;  and  Split(‚Äß)  is  an": "together, the proposed model obtains the highest accuracy score"
        },
        {
          "with": "ùëΩ  =  ùíÅ ùëæ(cid:3023) ‚àà ‚Ñù((cid:3015)√ó(cid:3004))\n(17)",
          "descent;  T  denotes  the  transpose  of  matrix;  and  Split(‚Äß)  is  an": ""
        },
        {
          "with": "",
          "descent;  T  denotes  the  transpose  of  matrix;  and  Split(‚Äß)  is  an": "of 82.20%. Hence, each module has complementarity and can"
        },
        {
          "with": "learnable  scalar  optimized  via  gradient \nwhere  ùúÜ ‚àà ‚Ñù  is  a",
          "descent;  T  denotes  the  transpose  of  matrix;  and  Split(‚Äß)  is  an": ""
        }
      ],
      "page": 3
    },
    {
      "caption": "Table 1: Ablation experimental results.",
      "data": [
        {
          "work together to improve the accuracy of the proposed model.": "",
          "For  comparison  convenience,  Figures  6  and  7  depict  the": "confusion  matrices  of \nthe  ECAPA-TDNN  and  Resnet18,"
        },
        {
          "work together to improve the accuracy of the proposed model.": "Table 1 Ablation experimental results.",
          "For  comparison  convenience,  Figures  6  and  7  depict  the": ""
        },
        {
          "work together to improve the accuracy of the proposed model.": "",
          "For  comparison  convenience,  Figures  6  and  7  depict  the": "respectively. From the results in Figure 5 to Figure 7, it can be"
        },
        {
          "work together to improve the accuracy of the proposed model.": "Model \nAccuracy",
          "For  comparison  convenience,  Figures  6  and  7  depict  the": ""
        },
        {
          "work together to improve the accuracy of the proposed model.": "",
          "For  comparison  convenience,  Figures  6  and  7  depict  the": "known \nthat \nthe  ECAPA-TDNN  and  Resnet18  have  higher"
        },
        {
          "work together to improve the accuracy of the proposed model.": "Improved ECAPA-TDNN with all modules \n82.20%",
          "For  comparison  convenience,  Figures  6  and  7  depict  the": ""
        },
        {
          "work together to improve the accuracy of the proposed model.": "",
          "For  comparison  convenience,  Figures  6  and  7  depict  the": "levels of confusion for various categories of emotions than the"
        },
        {
          "work together to improve the accuracy of the proposed model.": "Improved ECAPA-TDNN w/o MCA \n78.37%",
          "For  comparison  convenience,  Figures  6  and  7  depict  the": ""
        },
        {
          "work together to improve the accuracy of the proposed model.": "",
          "For  comparison  convenience,  Figures  6  and  7  depict  the": "improved ECAPA-TDNN."
        },
        {
          "work together to improve the accuracy of the proposed model.": "Improved ECAPA-TDNN w/o RSE \n78.54%",
          "For  comparison  convenience,  Figures  6  and  7  depict  the": ""
        },
        {
          "work together to improve the accuracy of the proposed model.": "Improved ECAPA-TDNN w/o differential-attention \n78.70%",
          "For  comparison  convenience,  Figures  6  and  7  depict  the": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 1: Ablation experimental results.",
      "data": [
        {
          "integration and decision-making. Hence, the improved ECAPA-": "TDNN  achieves  higher  accuracy  scores  compared  to  the  two"
        },
        {
          "integration and decision-making. Hence, the improved ECAPA-": "state-of-the-art models, namely ECAPA-TDNN and Resnet18."
        },
        {
          "integration and decision-making. Hence, the improved ECAPA-": "Table 2 The results obtained by different models."
        },
        {
          "integration and decision-making. Hence, the improved ECAPA-": "Model"
        },
        {
          "integration and decision-making. Hence, the improved ECAPA-": ""
        },
        {
          "integration and decision-making. Hence, the improved ECAPA-": "Improved ECAPA-TDNN"
        },
        {
          "integration and decision-making. Hence, the improved ECAPA-": "ECAPA-TDNN"
        },
        {
          "integration and decision-making. Hence, the improved ECAPA-": "Resnet18"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Kong, 2022, pp. 1-7,": ""
        },
        {
          "Kong, 2022, pp. 1-7,": "X. Shen, B. Lv, T. Liu, and Q. Cheng, \"Infant speech emotion"
        },
        {
          "Kong, 2022, pp. 1-7,": "recognition based on channel attention mechanism with ResNet-"
        },
        {
          "Kong, 2022, pp. 1-7,": ""
        },
        {
          "Kong, 2022, pp. 1-7,": "BiLSTM,\" in Proc. of International Conference on Information"
        },
        {
          "Kong, 2022, pp. 1-7,": ""
        },
        {
          "Kong, 2022, pp. 1-7,": "Science, Parallel and Distributed Systems (ISPDS), Guangzhou,"
        },
        {
          "Kong, 2022, pp. 1-7,": ""
        },
        {
          "Kong, 2022, pp. 1-7,": "China, 2024, pp. 54-57."
        },
        {
          "Kong, 2022, pp. 1-7,": ""
        },
        {
          "Kong, 2022, pp. 1-7,": "S.  G.  A,  G.  S,  G.  Tharagarani,  S.P, and  S.  B,  \"An  automated"
        },
        {
          "Kong, 2022, pp. 1-7,": ""
        },
        {
          "Kong, 2022, pp. 1-7,": "mood analysis of crying infants through sound recognition using"
        },
        {
          "Kong, 2022, pp. 1-7,": "hybrid deep learning,\" in Proc. of International Conference on"
        },
        {
          "Kong, 2022, pp. 1-7,": "Smart \nTechnologies \nand \nSystems \nfor \nNext  Generation"
        },
        {
          "Kong, 2022, pp. 1-7,": "Computing (ICSTSN), Villupuram, India, 2024, pp. 1-6."
        },
        {
          "Kong, 2022, pp. 1-7,": "Y.  Liu,  B.  Lv,  S.  Xu,  and  X.  Shen,  \"Emotion  recognition  of"
        },
        {
          "Kong, 2022, pp. 1-7,": ""
        },
        {
          "Kong, 2022, pp. 1-7,": "infant  crying \nsounds  using  convolutional \nrecurrent  neural"
        },
        {
          "Kong, 2022, pp. 1-7,": "network with multi-scale joint attention mechanism,\" in Proc. of"
        },
        {
          "Kong, 2022, pp. 1-7,": "International  Conference  on  Information  Systems  Engineering"
        },
        {
          "Kong, 2022, pp. 1-7,": "(ICISE), Dalian, China, 2023, pp. 615-619."
        },
        {
          "Kong, 2022, pp. 1-7,": "J.  Yang,  Z.  Zhang, \nJ.  Li, \nand  C.  Lin, \n\"A  multi-scale"
        },
        {
          "Kong, 2022, pp. 1-7,": "convolutional attention neural network based on residual block"
        },
        {
          "Kong, 2022, pp. 1-7,": "downsampling \nfor \ninfant  cry  classification  and  detection,\""
        },
        {
          "Kong, 2022, pp. 1-7,": "in Proc.  of  International  Conference  on  Internet  of  Things,"
        },
        {
          "Kong, 2022, pp. 1-7,": "Automation  and  Artificial  Intelligence  (IoTAAI),  Guangzhou,"
        },
        {
          "Kong, 2022, pp. 1-7,": "China, 2024, pp. 9-14."
        },
        {
          "Kong, 2022, pp. 1-7,": "C. Ji, T. B. Mudiyanselage, Y. Gao, and Y. Pan, \"A review of"
        },
        {
          "Kong, 2022, pp. 1-7,": "infant  cry  analysis  and  classification,\" EURASIP  Journal  on"
        },
        {
          "Kong, 2022, pp. 1-7,": "Audio, Speech, and Music Processing, vol. 2021, no. 1, Art. no."
        },
        {
          "Kong, 2022, pp. 1-7,": "8, 2021."
        },
        {
          "Kong, 2022, pp. 1-7,": "J.  J.  Parga,  S.  K.  Lewis,  and  M.  H.  Goldstein,  \"Defining  and"
        },
        {
          "Kong, 2022, pp. 1-7,": "distinguishing \ninfant  behavioral \nstates  using \nacoustic \ncry"
        },
        {
          "Kong, 2022, pp. 1-7,": "analysis: Is colic painful ?,\" Pediatric Research, vol. 87, no. 3,"
        },
        {
          "Kong, 2022, pp. 1-7,": "pp. 440-447, 2020."
        },
        {
          "Kong, 2022, pp. 1-7,": "A. Gorin, C. Subakan, S. Abdoli, J. Wang, S. Latremouille, and"
        },
        {
          "Kong, 2022, pp. 1-7,": "C.  C.  Onu,  \"Self-supervised  learning  for  infant  cry  analysis,\""
        },
        {
          "Kong, 2022, pp. 1-7,": "in Proc. of ICASSP Workshop on Safety and Security in Speech"
        },
        {
          "Kong, 2022, pp. 1-7,": "and Biomedical Signal Processing (SASB), IEEE, 2023, pp. 1-5."
        },
        {
          "Kong, 2022, pp. 1-7,": "B.  Desplanques,  J.  Thienpondt,  and  K.  Demuynck,  \"ECAPA-"
        },
        {
          "Kong, 2022, pp. 1-7,": "TDNN: \nEmphasized \nchannel \nattention, \npropagation \nand"
        },
        {
          "Kong, 2022, pp. 1-7,": "aggregation  in  TDNN  based  speaker  verification,\"  in Proc.  of"
        },
        {
          "Kong, 2022, pp. 1-7,": "INTERSPEECH, 2020, pp. 3830-3834."
        },
        {
          "Kong, 2022, pp. 1-7,": "D. Budaghyan, C. C. Onu, A. Gorin, C. Subakan, and D. Precup,"
        },
        {
          "Kong, 2022, pp. 1-7,": "\"CryCeleb:  A  speaker  verification  dataset  based  on  infant  cry"
        },
        {
          "Kong, 2022, pp. 1-7,": "sounds,\" \nin Proc. \nof \nIEEE \nInternational  Conference \non"
        },
        {
          "Kong, 2022, pp. 1-7,": "Acoustics, Speech and Signal  Processing,  Seoul, Korea, 2024,"
        },
        {
          "Kong, 2022, pp. 1-7,": "pp. 11966-11970."
        },
        {
          "Kong, 2022, pp. 1-7,": "S.  Dixit,  A. \nJain, \nand  R. \nSingh, \n\"Improving \nspeaker"
        },
        {
          "Kong, 2022, pp. 1-7,": "representations \nusing \ncontrastive \nlosses \non \nmulti-scale"
        },
        {
          "Kong, 2022, pp. 1-7,": "features,\" arXiv preprint arXiv:2410.05037, 2024."
        },
        {
          "Kong, 2022, pp. 1-7,": "T.  Ye,  L.  Wang,  and  H.  Li,  \"Differential  transformer,\" arXiv"
        },
        {
          "Kong, 2022, pp. 1-7,": "preprint arXiv:2410.05258, 2024."
        },
        {
          "Kong, 2022, pp. 1-7,": "F. Li, C. Cui, and Y. Hu, \"Classification of Infant Crying Sounds"
        },
        {
          "Kong, 2022, pp. 1-7,": "Using  SE-ResNet-Transformer,\" Sensors,  vol.  24,  no.  20,  pp."
        },
        {
          "Kong, 2022, pp. 1-7,": "6575, 2024."
        },
        {
          "Kong, 2022, pp. 1-7,": ""
        },
        {
          "Kong, 2022, pp. 1-7,": ""
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Dunstan baby language classification with CNN",
      "authors": [
        "C Bratan",
        "M Gheorghe",
        "I Ispas",
        "E Franti",
        "M Dascalu",
        "S Stoicescu",
        "I Ro»ôca",
        "F Gherghiceanu",
        "D Dumitrache",
        "L Nastase"
      ],
      "year": "2021",
      "venue": "Proc. of International Conference on Speech Technology and Human-Computer Dialogue (SpeD)"
    },
    {
      "citation_id": "3",
      "title": "Infant crying detection in real-world environments",
      "authors": [
        "X Yao",
        "M Micheletti",
        "M Johnson",
        "E Thomaz",
        "K De Barbaro"
      ],
      "year": "2022",
      "venue": "Proc. of IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "4",
      "title": "Is your baby fine at home? Baby cry sound detection in domestic environments",
      "authors": [
        "T Khandelwal",
        "R Das",
        "E Chng"
      ],
      "year": "2022",
      "venue": "Proc. of Asia-Pacific Signal and Information Processing Association Annual Summit and Conference"
    },
    {
      "citation_id": "5",
      "title": "Real-time baby crying detection in the noisy everyday environment",
      "authors": [
        "L Foo",
        "W.-S Yap",
        "Y Hum",
        "Z Kadim",
        "H Hon",
        "Y Tee"
      ],
      "year": "2020",
      "venue": "Proc. of IEEE Control and System Graduate Research Colloquium (ICSGRC)"
    },
    {
      "citation_id": "6",
      "title": "Spectrogram and LSTM based infant cry detection method for infant wellness monitoring systems",
      "authors": [
        "S Narayanan",
        "M Manikandan",
        "L Cenkeramaddi"
      ],
      "year": "2024",
      "venue": "Proc. of International Conference on Human System Interaction (HSI)"
    },
    {
      "citation_id": "7",
      "title": "A machine learning approach to classify biomedical acoustic features for baby cries",
      "authors": [
        "G Aggarwal",
        "K Jhajharia",
        "J Izhar",
        "M Kumar",
        "L Abualigah"
      ],
      "year": "2023",
      "venue": "Journal of Voice"
    },
    {
      "citation_id": "8",
      "title": "Infant cry language analysis and recognition: an experimental approach",
      "authors": [
        "L Liu",
        "W Li",
        "X Wu",
        "B Zhou"
      ],
      "year": "2019",
      "venue": "IEEE/CAA Journal of Automatica Sinica"
    },
    {
      "citation_id": "9",
      "title": "A multistage heterogeneous stacking ensemble model for augmented infant cry classification",
      "authors": [
        "V Joshi",
        "K Srinivasan",
        "P Vincent",
        "V Rajinikanth",
        "C Chang"
      ],
      "year": "2022",
      "venue": "Frontiers in Public Health"
    },
    {
      "citation_id": "10",
      "title": "Infant cry classification using transfer learning",
      "authors": [
        "G Anjali",
        "S Sanjeev",
        "A Mounika",
        "G Suhas",
        "G Reddy",
        "Y Kshiraja"
      ],
      "year": "2022",
      "venue": "Proc. of IEEE Region 10 Conference on TENCON"
    },
    {
      "citation_id": "11",
      "title": "Infant speech emotion recognition based on channel attention mechanism with ResNet-BiLSTM",
      "authors": [
        "X Shen",
        "B Lv",
        "T Liu",
        "Q Cheng"
      ],
      "year": "2024",
      "venue": "Proc. of International Conference on Information Science, Parallel and Distributed Systems (ISPDS)"
    },
    {
      "citation_id": "12",
      "title": "An automated mood analysis of crying infants through sound recognition using hybrid deep learning",
      "authors": [
        "G Tharagarani"
      ],
      "year": "2024",
      "venue": "Proc. of International Conference on Smart Technologies and Systems for Next Generation Computing (ICSTSN)"
    },
    {
      "citation_id": "13",
      "title": "Emotion recognition of infant crying sounds using convolutional recurrent neural network with multi-scale joint attention mechanism",
      "authors": [
        "Y Liu",
        "B Lv",
        "S Xu",
        "X Shen"
      ],
      "year": "2023",
      "venue": "Proc. of International Conference on Information Systems Engineering (ICISE)"
    },
    {
      "citation_id": "14",
      "title": "A multi-scale convolutional attention neural network based on residual block downsampling for infant cry classification and detection",
      "authors": [
        "J Yang",
        "Z Zhang",
        "J Li",
        "C Lin"
      ],
      "year": "2024",
      "venue": "Proc. of International Conference on Internet of Things, Automation and Artificial Intelligence (IoTAAI)"
    },
    {
      "citation_id": "15",
      "title": "A review of infant cry analysis and classification",
      "authors": [
        "C Ji",
        "T Mudiyanselage",
        "Y Gao",
        "Y Pan"
      ],
      "year": "2021",
      "venue": "EURASIP Journal on Audio, Speech, and Music Processing"
    },
    {
      "citation_id": "16",
      "title": "Defining and distinguishing infant behavioral states using acoustic cry analysis: Is colic painful ?",
      "authors": [
        "J Parga",
        "S Lewis",
        "M Goldstein"
      ],
      "year": "2020",
      "venue": "Pediatric Research"
    },
    {
      "citation_id": "17",
      "title": "Self-supervised learning for infant cry analysis",
      "authors": [
        "A Gorin",
        "C Subakan",
        "S Abdoli",
        "J Wang",
        "S Latremouille",
        "C Onu"
      ],
      "year": "2023",
      "venue": "Proc. of ICASSP Workshop on Safety and Security in Speech and Biomedical Signal Processing (SASB)"
    },
    {
      "citation_id": "18",
      "title": "ECAPA-TDNN: Emphasized channel attention, propagation and aggregation in TDNN based speaker verification",
      "authors": [
        "B Desplanques",
        "J Thienpondt",
        "K Demuynck"
      ],
      "year": "2020",
      "venue": "Proc. of INTERSPEECH"
    },
    {
      "citation_id": "19",
      "title": "CryCeleb: A speaker verification dataset based on infant cry sounds",
      "authors": [
        "D Budaghyan",
        "C Onu",
        "A Gorin",
        "C Subakan",
        "D Precup"
      ],
      "year": "2024",
      "venue": "Proc. of IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "20",
      "title": "Improving speaker representations using contrastive losses on multi-scale features",
      "authors": [
        "S Dixit",
        "A Jain",
        "R Singh"
      ],
      "year": "2024",
      "venue": "Improving speaker representations using contrastive losses on multi-scale features",
      "arxiv": "arXiv:2410.05037"
    },
    {
      "citation_id": "21",
      "title": "Differential transformer",
      "authors": [
        "T Ye",
        "L Wang",
        "H Li"
      ],
      "year": "2024",
      "venue": "Differential transformer",
      "arxiv": "arXiv:2410.05258"
    },
    {
      "citation_id": "22",
      "title": "Classification of Infant Crying Sounds Using SE-ResNet-Transformer",
      "authors": [
        "F Li",
        "C Cui",
        "Y Hu"
      ],
      "year": "2024",
      "venue": "Sensors"
    }
  ]
}