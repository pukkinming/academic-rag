{
  "paper_id": "2406.06781v1",
  "title": "Persona: An Application For Emotion Recognition, Gender Recognition And Age Estimation",
  "published": "2024-06-10T20:38:48Z",
  "authors": [
    "Devyani Koshal",
    "Orchid Chetia Phukan",
    "Sarthak Jain",
    "Arun Balaji Buduru",
    "Rajesh Sharma"
  ],
  "keywords": [
    "Emotion Recognition",
    "Gender Recognition",
    "Age Estimation",
    "Computational Paralinguistics"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Emotion Recognition (ER), Gender Recognition (GR), and Age Estimation (AE) constitute paralinguistic tasks that rely not on the spoken content but primarily on speech characteristics such as pitch and tone. While previous research has made significant strides in developing models for each task individually, there has been comparatively less emphasis on concurrently learning these tasks, despite their inherent interconnectedness. As such in this demonstration, we present PERSONA, an application for predicting ER, GR, and AE with a single model in the backend. One notable point is we show that representations from speaker recognition pre-trained model (PTM) is better suited for such a multi-task learning format than the state-of-the-art (SOTA) self-supervised (SSL) PTM by carrying out a comparative study. Our methodology obviates the need for deploying separate models for each task and can potentially conserve resources and time during the training and deployment phases.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Speech analysis exhibits remarkable versatility across various domains, including customer service, market research, legal proceedings, healthcare, and entertainment. Within this expansive landscape, Emotion Recognition (ER), Gender Recognition (GR), and Age Estimation (AE) emerge as pivotal tasks. These tasks play an instrumental role in extracting valuable demographic insights to craft personalized recommendations and even providing the means to assess users' sentiments and satisfaction levels.\n\nPrevious studies have done substantial research into developing these tasks by leveraging various methodologies. Yang et al.  [1]  used representations from various self-supervised learning (SSL) pre-trained models (PTMs) for ER. Lebourdais et al.  [2]  used SOTA SSL PTM WavLM representations, while Abdul et al.  [3]  used K-NN classifier with MFCC features for GR, followed by Zazo et al.  [4]  using LSTM-based network for AE.\n\nHowever, developing distinct models for each task individually presents significant resource, cost, and time implications. We tackle this challenge of resource constraints and timeintensive model development by adopting a multi-task learning strategy for the modeling and present PERSONA application. We explore two speech pre-trained models (PTMs) and evaluate their representations for jointly learning ER, GR, and AE. This approach maximises the utility of shared parameters, paving the way for more robust and scalable modeling solutions in speech processing.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "End-To-End Multi-Task Learning Model",
      "text": "This section discusses the PTM representations used for our experiment, followed by modeling networks. Lastly, we discuss the database used and the experimental results. Pre-Trained Representations: We use WavLM  [5]  as SSL PTM for our experiments due to its performance in SUPERB. WavLM shows SOTA performance in various speech processing tasks such as ER, ASR, deepfake detection, and so on. We use x-vector  [6] , a SOTA speaker recognition model for its topmost performance in various tasks such as ER, shout intensity prediction, and so on shown by previous works  [7, 8] . We extract representations of 768-dimension from WavLM from the last hidden state through average-pooling and from x-vector, we extract representations of 512-dimension. Model Architecture: We use CNN and FCN as model architectures on top of the extracted representations for our experiments. The proposed model is shown in Figure  1 . Combination of two 1D-CNN and maxpooling layer is embedded on top of the extracted representations. This is succeeded by a fully connected network (FCN) consisting of three layers with 200, 128, and 56 neurons. The preceding layers are shared by the three tasks, followed by a task-specific head added on top, providing the task-specific output. For ER and GR, we add softmax function in the output layer that outputs the probabilities of the classes. For AE, we use a linear activation function in output layer for estimating the age. We use cross-entropy as loss function for ER, GR and mean-squared-error (MSE) for AE. For FCN, we follow the same modeling as the FCN used in the CNN model. The total loss is the summation of three losses for each specific task, and we train the model for 20 epochs with 1e-3 learning rate. We train the models in a 5-fold manner with 4-fold for training and 1-fold for testing. We leverage early stopping and learning rate decay techniques to optimise the training process. Database and Results: We use CREMA-D  [9]  database for our experiments. We resample the audios to 16KHz before passing through the PTMs. The evaluation results of our experiments are presented in Table  1 . We use accuracy as metric for ER and GR and RMSE in AE. We observed that the x-vector shows the topmost performance in the three tasks, and this behaviour can be traced to its effectiveness in capturing paralinguistic cues such as pitch, tone, etc, needed for these tasks.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Persona Application",
      "text": "In this section, we discuss how users can interact with PER-SONA to get the desired outputs and the complete workflow behind it. We have used React.Js for building the front end and Flask as a backend for exposing the model input and output as API. Users can upload a raw audio file in .wav or .mp3 format by clicking Click to choose button, and then when the  user clicks the button Make predictions, the raw audio will be passed through the pre-processing stage followed by input to the model. We employ the best model CNN with x-vector representations in PERSONA backend. The outputs given by the model are shown at the front end that will be referred to by the user, and it is shown in Figure  1 . PERSONA takes average 1sec for 1min incoming audio for inference.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Conclusion",
      "text": "In this demonstration, we present, PERSONA, an application that predicts emotion, gender, and age from uploaded audio. Our work solves the persistent challenges associated with building individual models for ER, GR, and AE. Our work emphasizes using representations from speaker recognition PTM, xvector instead of SSL PTM for building related multi-task applications.",
      "page_start": 2,
      "page_end": 2
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Combination",
      "page": 1
    },
    {
      "caption": "Figure 1: PERSONA Workflow ; Here, only the CNN model is shown",
      "page": 2
    },
    {
      "caption": "Figure 1: PERSONA takes average 1sec",
      "page": 2
    }
  ],
  "tables": [
    {
      "caption": "Table 1: We use accuracy as metric for ER",
      "page": 1
    },
    {
      "caption": "Table 1: Evaluation results; Scores given are in %; Scores are",
      "page": 2
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Superb: Speech processing universal performance benchmark",
      "authors": [
        "S.-W Yang",
        "P.-H Chi",
        "Y.-S Chuang",
        "C.-I Lai",
        "K Lakhotia",
        "Y Lin",
        "A Liu",
        "J Shi",
        "X Chang",
        "G.-T Lin"
      ],
      "year": "2021",
      "venue": "Superb: Speech processing universal performance benchmark",
      "arxiv": "arXiv:2105.01051"
    },
    {
      "citation_id": "3",
      "title": "Overlapped speech and gender detection with WavLM pre-trained features",
      "authors": [
        "M Lebourdais",
        "M Tahon",
        "A Laurent",
        "S Meignier"
      ],
      "year": "2022",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "4",
      "title": "Age and gender recognition from speech signals",
      "authors": [
        "A Abdulsatar",
        "V Davydov",
        "V Yushkova",
        "A Glinushkin",
        "V Rud"
      ],
      "year": "2019",
      "venue": "Journal of Physics: Conference Series"
    },
    {
      "citation_id": "5",
      "title": "Age estimation in short speech utterances based on lstm recurrent neural networks",
      "authors": [
        "R Zazo",
        "P Nidadavolu",
        "N Chen",
        "J Gonzalez-Rodriguez",
        "N Dehak"
      ],
      "year": "2018",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "6",
      "title": "Wavlm: Large-scale self-supervised pre-training for full stack speech processing",
      "authors": [
        "S Chen",
        "C Wang",
        "Z Chen",
        "Y Wu",
        "S Liu",
        "Z Chen",
        "J Li",
        "N Kanda",
        "T Yoshioka",
        "X Xiao"
      ],
      "year": "2022",
      "venue": "IEEE Journal of Selected Topics in Signal Processing"
    },
    {
      "citation_id": "7",
      "title": "X-vectors: Robust dnn embeddings for speaker recognition",
      "authors": [
        "D Snyder",
        "D Garcia-Romero",
        "G Sell",
        "D Povey",
        "S Khudanpur"
      ],
      "year": "2018",
      "venue": "2018 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "8",
      "title": "Transforming the Embeddings: A Lightweight Technique for Speech Emotion Recognition Tasks",
      "authors": [
        "O Chetia Phukan",
        "A Balaji",
        "R Buduru",
        "Sharma"
      ],
      "year": "2023",
      "venue": "Proc. INTER-SPEECH 2023"
    },
    {
      "citation_id": "9",
      "title": "Investigating the effectiveness of speaker embeddings for shout intensity prediction",
      "authors": [
        "T Fukumori",
        "T Ishida",
        "Y Yamashita"
      ],
      "year": "2023",
      "venue": "2023 Asia Pacific Signal and Information Processing Association Annual Summit and Conference"
    },
    {
      "citation_id": "10",
      "title": "Crema-d: Crowd-sourced emotional multimodal actors dataset",
      "authors": [
        "H Cao",
        "D Cooper",
        "M Keutmann",
        "R Gur",
        "A Nenkova",
        "R Verma"
      ],
      "year": "2014",
      "venue": "IEEE transactions on affective computing"
    }
  ]
}