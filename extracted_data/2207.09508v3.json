{
  "paper_id": "2207.09508v3",
  "title": "Hse-Nn Team At The 4Th Abaw Competition: Multi-Task Emotion Recognition And Learning From Synthetic Images",
  "published": "2022-07-19T18:43:14Z",
  "authors": [
    "Andrey V. Savchenko"
  ],
  "keywords": [
    "Facial expression recognition",
    "multi-task learning",
    "learning from synthetic data",
    "4th Affective Behavior Analysis in-the-Wild (ABAW)",
    "EfficientNet"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "In this paper, we present the results of the HSE-NN team in the 4th competition on Affective Behavior Analysis in-the-wild (ABAW). The novel multi-task EfficientNet model is trained for simultaneous recognition of facial expressions and prediction of valence and arousal on static photos. The resulting MT-EmotiEffNet extracts visual features that are fed into simple feed-forward neural networks in the multi-task learning challenge. We obtain performance measure 1.3 on the validation set, which is significantly greater when compared to either performance of baseline (0.3) or existing models that are trained only on the s-Aff-Wild2 database. In the learning from synthetic data challenge, the quality of the original synthetic training set is increased by using the super-resolution techniques, such as Real-ESRGAN. Next, the MT-EmotiEffNet is finetuned on the new training set. The final prediction is a simple blending ensemble of pre-trained and fine-tuned MT-EmotiEffNets. Our average validation F1 score is 18% greater than the baseline convolutional neural network. As a result, our team took the first place in the learning from synthetic data challenge and the third place in the multi-task learning challenge.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "The problem of affective behavior analysis in-the-wild is to understand people's feelings, emotions, and behaviors. Human emotions are typically represented using a small set of basic categories, such as anger or happiness. More advanced representations include a discrete set of Action Units (AUs) from the Facial Action Coding System (FACS) Ekman's model and Russell's continuous encoding of affect in the 2-D space of arousal and valence. The former shows how passive or active an emotional state is, whilst the latter shows how positive or negative it is. Though the emotion of a person may be identified using various signals, such as voice, pronounced utterance, body language, etc., the most accurate results are obtained with facial analytics.\n\nDue to the high complexity of labeling emotions, existing emotional datasets for facial expression recognition (FER) are small and dirty. As a result, the trained models learn too many features specific to a concrete dataset, which is not practical for in-the-wild settings  [6] . Indeed, they typically remain not robust to the diversity of environments and video recording conditions, so they can be hardly used in real-world settings with uncontrolled observation conditions. Hence, a lot of attention has been recently brought towards mitigating algorithmic bias in models and, in particular, cross-dataset studies.\n\nThis problem has become a focus of many researchers since an appearance of a sequence of the Affective Behavior Analysis in-the-wild (ABAW) challenges that involve different parts of the Aff-Wild  [11, 23]  and Aff-Wild2  [9, 12]  databases. The organizers encourage participants to actively pre-train the models on other datasets by introducing all the new requirements. For example, one of the tasks of the third ABAW competition was the multi-task-learning (MTL) for simultaneous prediction of facial expressions, valence, arousal, and AUs  [6, 10] . Its winners  [2]  did not use the MTL small static training set (s-Aff-Wild2). Indeed, they proposed the Transformer-based Sign-and-Message Multi-Emotion Net that was trained on a large set of all video frames from the Aff-Wild2. The runner-up proposed the auditory-visual representations  [4]  that were also trained on initial video files. Similarly, the team that took fourth place in the MTL challenge developed the transformer-based multimodal framework  [24]  trained on the video frames from the Aff-Wild2 dataset that let them become the winners of FER and AU sub-challenges.\n\nAs a result, in the fourth ABAW competition  [5] , such usage of other data from Aff-Wild2 except the small training set is prohibited. It seems that only one successful participant (the third place) of the previous MTL challenge, namely, multi-head EfficientNet  [19] , and the baseline VGGFACE convolutional neural network (CNN)  [6]  satisfy new requirements. Moreover, a new challenge has been introduced that encourages the refinement of the pre-trained models on a set with synthetic faces generated from a small part of the Aff-Wild2 dataset  [8, 13, 7] .\n\nIn this paper, we discuss our solution for all tasks from the ABAW4 competition. It is proposed to improve the EfficientNet-based model  [19]  by pretraining a model on the AffectNet dataset  [15]  not only for FER but for additional prediction of valence and arousal. The visual embeddings are extracted from the penultimate layer of the resulting MT-EmotiEffNet, while the valence, arousal, and logits for each emotion are obtained at the output of its last layer. The multi-output feed-forward neural network is trained using the s-Aff-Wild2 database for the MTL challenge. The best validation results are obtained by simple blending  [17]  of its predictions with action unit features from the Open-Face 2 toolkit  [1] . In the Learning from Synthetic Data (LSD) challenge, we propose to increase the quality of the original synthetic training set by using the super-resolution techniques, such as Real-ESRGAN  [21] , and fine-tuning the MT-EmotiEffNet on the new training set. The final prediction is a simple blend-",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Proposed Approach",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Multi-Task Learning Challenge",
      "text": "The main task of human affective behavior analysis is FER. It is a typical problem of image recognition in which an input facial image X should be associated with one of C EXP R > 1 categories (classes), such as anger, surprise, etc. There exist several commonly-used expression representations, such as estimation of Valence V and Arousal A (typically, V, A ∈ [-1, 1]) and AU detection  [6] . The latter task is a multi-label classification problem, i.e., prediction of a binary vector AU = [AU 1 , ..., AU C AU ], where C AU is the total number of AUs, and AU i ∈ {0, 1} is a binary indicator of the presence of the i-th AU in the photo.\n\nIn this paper, we propose the novel model (Fig.  1 ) for the MTL challenge from ABAW4  [6] , in which the method from  [19]  was modified as follows:\n\n1. Emotional feature extractor is implemented with the novel MT-EmotiEffNet model based on EfficientNet-B0 architecture  [20] , which was pre-trained not only for FER but for additional valence-arousal estimation. 2. The valence and arousal for the input facial photo are predicted by using only the output of the last layer of the MT-EmotiEffNet, i.e., they are not concatenated with embeddings extracted at its penultimate layer. 3. Three heads of the model for facial expressions, Valence-Arousal and AUs, respectively, are trained sequentially, i.e., we do not need additional masks for missing data.\n\nLet us consider the details of this pipeline. Its main part, namely, the MT-EmotiEffNet model, was pre-trained using PyTorch framework identically to EfficientNet-B0 from  [18]  on face identification task. The facial regions in the training set are simply cropped by a face detector without margins or face alignment. Next, the last classification layer is replaced by a dense layer with 10 units for valence, arousal, and C (e) = 8 emotional categories (Neutral, Happy, Sad, Surprise, Fear, Anger, Disgust, Contempt) from the AffectNet dataset  [15] , respectively. The loss function is computed as a sum of Concordance Correlation Coefficients (CCCs)  [14]  and the weighted categorical cross-entropy  [15] :\n\nwhere X is the training image, y (e) ∈ {1, ..., C e } is its emotional class label, y (V ) and y (A) are the ground-truth valence and arousal, N c is the total number of training examples of the c-th class, z y (e) is the FER score, i.e., y (e) -th output of the last (logits) layer, z (V ) and z (A) are the outputs of the last two units in the output layer, and sof tmax is the softmax activation function.\n\nThe imbalanced training set with 287,651 facial photos provided by the authors of the AffectNet  [15]  was used to train the MT-EmotiEffNet model, while the official balanced set of 4000 images (500 per category) was used for validation. At first, all weights except the new head were frozen, and the model was learned in 3 epochs using the Adam optimizer with a learning rate of 0.001 and SAM (Sharpness-Aware Minimization)  [3] . Finally, we trained all weights of the model totally of 6 epochs with a lower learning rate (0.0001).\n\nIt is important to emphasize that the MT-EmotiEffNet feature extractor is not refined on the s-Aff-Wild2 dataset for the MTL challenge. Thus, every input X and reference X n image is resized to 224x224 and fed into our CNN. We examine two types of features: (1) facial image embeddings (output of the penultimate layer)  [18] ; and (2) logits (predictions of emotional unnormalized probabilities at the output of the last layer). The outputs of penultimate layer  [18]  are stored in the D = 1280-dimensional embeddings x and x n , respectively. The concatenation of 8 FER logits, Valence, and Arousal at the output of the model are stored in the 10-dimensional logits l and l n . We experimentally noticed that the valence and arousal for the MTL challenge are better predicted by using the logits only, while the facial expression and AUs are more accurately detected if the embeddings x and logits l are concatenated.\n\nThe remaining part of our neural network (Fig.  1 ) contains three output layers, namely, (1) C EXP R = 8 units with softmax activation for recognition of one of eight emotions (Neutral, Anger, Disgust, Fear, Happiness, Sadness, Surprise, Other); (2) two neurons with tanh activation functions for Valence-Arousal prediction; and (3) C AU = 12 output units with sigmoid activation for AU detection. The model was trained using the s-Aff-Wild2 cropped aligned set provided by the organizers  [6] . This set contains N = 142, 333 facial images {X n }, n ∈ {1, ..., N }, for which the expression e n ∈ {1, ..., C EXP R }, C AU -dimensional binary vector AU n of AUs, and/or valence V n and arousal A n are known. Some labels are missed, so only 90,645 emotional labels, 103,316 AU labels, and 103,917 values of Valence-Arousal are available for training. The validation set contains 26,876 facial frames, for which AU and VA are known, but only 15,440 facial expressions are provided. The three heads of the model, i.e., three fully-connected (FC) layers, were trained separately by using the Tensorflow 2 framework and the procedure described in the paper  [19]  for the uni-task learning.\n\nFinally, we examined the possibility to improve the quality by using additional facial features. The OpenFace 2 toolkit  [1]  extracted pose, gaze, eye, and AU features from each image. It was experimentally found that only the latter features are suitable for the FER part of the MTL challenge. Hence, we trained MLP (multi-layered perceptron) that contains an input layer with 35 AUs from the OpenFace, 1 hidden layer with 128 units and ReLU activation, and 1 output layer with C EXP R units and softmax activation. The component-wise weighted sum of the C EXP R outputs of this model and the emotional scores (posterior probabilities) at the output of \"FC layer, softmax\" (Fig.  1 ) is computed in the \"Blending\" layer, which returns the emotional class that corresponds to the maximal component of this weighted sum. The best weight in a blending is estimated by maximizing the average F1 score of FER on the validation set.",
      "page_start": 3,
      "page_end": 5
    },
    {
      "section_name": "Learning From Synthetic Data Challenge",
      "text": "In the second sub-challenge, it was required to solve the FER task and associate an input facial image with one of C EXP R = 6 basic expressions (Surprise, Fear, Disgust, Anger, Happiness, Sadness). The task is to refine the pre-trained model by using only information from 277,251 synthetic images that have been generated from some specific frames from the Aff-Wild2 database. The validation set contains 4,670 images for the same subjects.\n\nThe proposed pipeline (Fig.  2 ) uses the MT-EmotiEffNet from the previous Subsection in a straightforward way. Foremost, we noticed that the quality of provided synthetic facial images with a resolution of 112x112 is too low for our model that was trained on photos with a rather large resolution (224x224). Hence, it was proposed to enhance these images by using contemporary super- Since the required set of 6 expressions is a subset of eight emotions from AffectNet, the simplest solution is to ignore the training set of synthetic images, and predict expression for an image X with the pre-trained MT-EmotiEffNet by using only 6 scores (emotional posterior probabilities) s (pre) (X) from the last (softmax) layer  [18]  that is associated with required categories.\n\nTo use the provided synthetic data, we additionally fine-tuned the MT-EmotiEffNet model on the training set at the output of Real-ESRGAN. At first, the output layer was replaced with the new fully-connected layer with C EXP R units, and the weights of the new head were learned during 3 epochs. The remaining weights were frozen. The weighted cross-entropy similar to (1) was minimized by the SAM  [3]  with Adam optimizer and learning rate of 0.001. Moreover, we examine the subsequent fine-tuning of all weights on synthetic data during 6 epochs with a learning rate of 0.0001. We can use either model with a new head or completely fine-tuned EfficientNet to predict C EXP R -dimensional vector of scores s (f t) (X) for the input image. To make a final decision, blending of pre-trained and fine-tune models is again applied by computing the weighted sum of scores:  [17] . The final decision is made in favor of the expression that corresponds to the maximal component of the vector s(X). The best value of the weight hyperparameter w ∈ [0, 1], is estimated by maximizing the average F1 score on the validation set.   [18]  61.32 ----SL + SSL inpanting-pl (B0)  [16]  61.72 ----Distract Your Attention  [22]  62.09 ----EfficientNet-B2  [19]  63.03 ----MT-EmotiEffNet 61.93 0.434 0.594 0.387 0.549\n\n3 Experimental study",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Fer For Static Images",
      "text": "In this subsection, we compare the proposed MT-EmotiEffNet with several existing techniques for static photos from the validation set of AffectNet  [15] . The accuracy of FER with 8 emotional categories and CCC and RMSE (root mean squared error) for valence and arousal estimates are summarized in Table  1 .\n\nThough the MT-EmotiEffNet is not the best FER model, it has 0.6% greater accuracy when compared to a single-task model with identical training settings and architecture (EfficientNet-B0)  [18] . One can conclude that taking valencearousal into account while training the model makes it possible to simultaneously solve multiple affect prediction tasks and extract better emotional features. The RMSE for valence prediction of the AlexNet  [15]  is slightly better than the RMSE of our model. However, as we optimized the CCC for valence and arousal (1), these metric is higher when compared to baseline EfficientNet in all cases.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Multi-Task-Learning Challenge",
      "text": "In this subsection, the proposed pipeline (Fig.  1 ) for the MTL challenge is examined. At first, we study various features extracted by either known models (OpenFace AUs, EfficientNet-B0  [18]  ) and our MT-EmotiEffNet. Visual embeddings x and/or logits l are fed into MLP with 3 heads. Three training datasets were used, namely, cropped and cropped aligned (hereinafter \"aligned) provided by the organizers of this challenge and the aligned faces after Real-ESRGAN  [21]  as we proposed in Subsection 2.2.\n\nThe ablation results are presented in Table  2 . Here, we used the performance metrics recommended by the organizers of the challenge, namely, P V A is the average CCC for valence and arousal, P EXP R is the macro-average F1 score for FER, P AU is the macro-average F1 score for AU detection, and P M T L = P V A + P EXP R + P AU . The best result in each column is marked in bold. The proposed MT-EmotiEffNet has 0.06-0.07 greater overall quality P M T L when compared to EfficientNet-B0 features  [18]  even if all heads are trained simultaneously as described in the original paper  [19] . If the heads are trained separately without adding masks for missing values, P M T L is increased by 0.02-0.04. Moreover, it was experimentally found that valence and arousal are better predicted by using the logits only. Hence, we developed a multi-head MLP (Fig.  1 ) that feeds logits to the Valence-Arousal head but concatenates logits with embeddings for FER and AU heads. As a result, P V A was increased to 0.447, and the whole model reached P M T L = 1.276. Finally, we noticed that AU features at the output of OpenFace may be used for rather accurate FER, so that their ensemble with our model is characterized by the best average F1 score for facial expressions P EXP R = 0.357. It is remarkable that AU features from OpenFace are not well suited for the AU detection task in the MTL challenge, and their blending with our model cannot significantly increase P AU . F1 score, thresholds 0.5 0.58 0.41 0.58 0.59 0.73 0.72 0.68 0.17 0.13 0.17 0.85 0.35\n\nThe best thresholds 0.6 0.7 0.5 0.3 0.4 0.4 0.5 0.9 0.8 0.7 0.2 0.7 F1 score, the best thresholds 0.58 0.47 0.58 0.60 0.74 0.72 0.68 0.25 0.18 0.21 0.88 0.36 The detailed results of the proposed model (Fig.  1 ) for AU detection with the additional tuning of thresholds for AU detection, FER, and Valence-Arousal estimation are shown in Table  3 , Table  ,4  and Table 5 , respectively. A comparison of our best model to existing baselines is summarized in Table  6 . As one can notice, the usage of the proposed MT-EmotiEffNet significantly increased the performance of valence-arousal prediction and expression recognition. The AUs are also detected with up to 0.03 greater F1 scores. As a result, the performance measure P M T L of our model is 0.15 and 1.02 points greater when compared to the best model from the third ABAW challenge trained on s-Aff-Wild2 dataset only  [19]  and the baseline of the organizers  [5] , respectively.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Learning From Synthetic Data Challenge",
      "text": "In this Subsection, our results for the LSD challenge are presented. We examine various EfficientNet-based models that have been pre-trained on AffectNet and fine-tuned on both an original set of synthetic images provided by the organizers and its enhancement by using SR techniques  [21] . The official validation set of the organizers without SR was used in all experiments to make the metrics directly comparable. The macro-averaged F1 scores of individual models are presented in Table  7 . Here, even the best pre-trained model is 9-10% more accurate than the baseline of the organizers (ImageNet ResNet-50) that reached an F1 score of 0.5 on the validation set  [5] . As a result, if we replace the last classification layer with a new one and refine the model by training only the weights of the new head, the resulting F1 score will not be increased significantly. It is worth noting that the MT-EmotiEffNet is preferable to other emotional models in all these cases. Thus, our multi-task pre-training seems to provide better emotional features when compared to existing models. As was expected, the fine-tuning of all weights lead to approximately the same results for the same EfficientNet-B0 architecture. It is important to emphasize that the fine-tuning of the model on the training set after Real-ESRGAN leads to a 3-4% greater F1 score after tuning the whole model. Even if only a new head is learned, performance is increased by 1% when SR is applied.\n\nThe ablation study of the proposed ensemble (Fig.  2 ) is provided in Table  8 . We noticed that as the synthetic data have been generated from subjects of the validation set, but not of the test set, the baseline's performance on the validation and test sets (real data from Aff-Wild2) is equal to 0.50 and 0.30, respectively  [5] . Thus, the high accuracy on the validation set does not necessarily lead to the high testing quality. Hence, we estimated the F1 score not only on the official validation set from this sub-challenge but also on the validation set from the MTL competition. The frames with Neutral and Other expressions were removed to obtain a new validation set of 8,953 images with 6 expressions.\n\nAccording to these results, the original validation set seems to be not representative. For example, fine-tuning all weights leads to an increase in the validation F1 score by 6-9%. However, it decreases the performance by 6% for the new validation set if facial expressions from other subjects should be predicted. It is important to highlight that the pre-trained models are characterized by high F1 scores (41-42%) in the latter case. The proposed MT-EmotiEffNet is still 1% more accurate than EfficientNet-B0  [18] . What is more important, the",
      "page_start": 9,
      "page_end": 10
    },
    {
      "section_name": "Conclusions",
      "text": "In this paper, we presented the multi-task EfficientNet-based model for simultaneous recognition of facial expressions, valence, and arousal that was pre-trained on static photos from the AffectNet dataset. Based on this model, we introduced two novel pipelines for MTL (Fig.  1 ) and LSD (Fig.  2 ) challenges in the fourth ABAW competition  [6] . It was experimentally demonstrated that the proposed model significantly improves the results of either baseline VGGFACE CNN  [5]  or single-task EfficientNet-B0  [18]  for both tasks (Tables  6, 8 ). It is worth noting that the best performance in both challenges is obtained by original pre-trained MT-EmotiEffNet without a need for fine-tuning of all weights on Aff-Wild2 data. Thus, the well-known issue of affect analysis techniques, namely, the subgroup distribution shift, is partially overcome in our models by training simple MLP-based predictors on top of facial emotional features extracted by the MT-EmotiEffNet.",
      "page_start": 12,
      "page_end": 12
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Proposed model for the multi-task-learning challenge",
      "page": 3
    },
    {
      "caption": "Figure 1: ) for the MTL challenge",
      "page": 4
    },
    {
      "caption": "Figure 1: ) contains three output lay-",
      "page": 5
    },
    {
      "caption": "Figure 1: ) is computed in the",
      "page": 5
    },
    {
      "caption": "Figure 2: ) uses the MT-EmotiEffNet from the previous",
      "page": 5
    },
    {
      "caption": "Figure 2: Proposed pipeline for learning from synthetic data",
      "page": 6
    },
    {
      "caption": "Figure 1: ) for the MTL challenge is ex-",
      "page": 7
    },
    {
      "caption": "Figure 1: ) that feeds logits to the Valence-Arousal head but concatenates logits",
      "page": 8
    },
    {
      "caption": "Figure 1: ) for AU detection with",
      "page": 9
    },
    {
      "caption": "Figure 2: ) is provided in Table 8.",
      "page": 10
    },
    {
      "caption": "Figure 3: Confusion matrix for learning from synthetic data: (a) pre-trained MT-",
      "page": 11
    },
    {
      "caption": "Figure 3: Hence, our final submission included both",
      "page": 11
    },
    {
      "caption": "Figure 1: ) and LSD (Fig. 2) challenges in the fourth",
      "page": 12
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Nizhny Novgorod, Russia": "avsavchenko@hse.ru"
        },
        {
          "Nizhny Novgorod, Russia": "Abstract.\nIn this paper, we present the results of the HSE-NN team in"
        },
        {
          "Nizhny Novgorod, Russia": "the 4th competition on Affective Behavior Analysis in-the-wild (ABAW)."
        },
        {
          "Nizhny Novgorod, Russia": "The novel multi-task EfficientNet model is trained for simultaneous recog-"
        },
        {
          "Nizhny Novgorod, Russia": "nition of facial expressions and prediction of valence and arousal on static"
        },
        {
          "Nizhny Novgorod, Russia": "photos. The resulting MT-EmotiEffNet extracts visual\nfeatures that are"
        },
        {
          "Nizhny Novgorod, Russia": "fed into simple feed-forward neural networks in the multi-task learning"
        },
        {
          "Nizhny Novgorod, Russia": "challenge. We obtain performance measure 1.3 on the validation set,"
        },
        {
          "Nizhny Novgorod, Russia": "which is\nsignificantly greater when compared to either performance of"
        },
        {
          "Nizhny Novgorod, Russia": "baseline (0.3) or existing models that are trained only on the s-Aff-Wild2"
        },
        {
          "Nizhny Novgorod, Russia": "database. In the learning from synthetic data challenge, the quality of the"
        },
        {
          "Nizhny Novgorod, Russia": "original synthetic training set is increased by using the super-resolution"
        },
        {
          "Nizhny Novgorod, Russia": "techniques, such as Real-ESRGAN. Next, the MT-EmotiEffNet is fine-"
        },
        {
          "Nizhny Novgorod, Russia": "tuned on the new training set. The final prediction is a simple blending"
        },
        {
          "Nizhny Novgorod, Russia": "ensemble of pre-trained and fine-tuned MT-EmotiEffNets. Our average"
        },
        {
          "Nizhny Novgorod, Russia": "validation F1 score is 18% greater than the baseline convolutional neural"
        },
        {
          "Nizhny Novgorod, Russia": "network. As a result, our team took the first place in the learning from"
        },
        {
          "Nizhny Novgorod, Russia": "synthetic data challenge and the third place in the multi-task learning"
        },
        {
          "Nizhny Novgorod, Russia": "challenge."
        },
        {
          "Nizhny Novgorod, Russia": "Keywords: Facial\nexpression recognition, multi-task\nlearning,\nlearn-"
        },
        {
          "Nizhny Novgorod, Russia": "ing from synthetic data, 4th Affective Behavior Analysis\nin-the-Wild"
        },
        {
          "Nizhny Novgorod, Russia": "(ABAW), EfficientNet"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "(ABAW), EfficientNet": "Introduction"
        },
        {
          "(ABAW), EfficientNet": ""
        },
        {
          "(ABAW), EfficientNet": ""
        },
        {
          "(ABAW), EfficientNet": ""
        },
        {
          "(ABAW), EfficientNet": ""
        },
        {
          "(ABAW), EfficientNet": ""
        },
        {
          "(ABAW), EfficientNet": ""
        },
        {
          "(ABAW), EfficientNet": ""
        },
        {
          "(ABAW), EfficientNet": ""
        },
        {
          "(ABAW), EfficientNet": ""
        },
        {
          "(ABAW), EfficientNet": ""
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "2\nA.V. Savchenko": "Due to the high complexity of labeling emotions, existing emotional datasets"
        },
        {
          "2\nA.V. Savchenko": "for\nfacial\nexpression recognition (FER) are\nsmall and dirty. As a result,\nthe"
        },
        {
          "2\nA.V. Savchenko": "trained models learn too many features specific to a concrete dataset, which is"
        },
        {
          "2\nA.V. Savchenko": "not practical\nfor\nin-the-wild settings\n[6].\nIndeed,\nthey typically remain not\nro-"
        },
        {
          "2\nA.V. Savchenko": "bust\nto the diversity of environments and video recording conditions,\nso they"
        },
        {
          "2\nA.V. Savchenko": "can be hardly used in real-world settings with uncontrolled observation condi-"
        },
        {
          "2\nA.V. Savchenko": "tions. Hence, a lot of attention has been recently brought\ntowards mitigating"
        },
        {
          "2\nA.V. Savchenko": "algorithmic bias in models and,\nin particular, cross-dataset studies."
        },
        {
          "2\nA.V. Savchenko": "This problem has become a focus of many researchers since an appearance of a"
        },
        {
          "2\nA.V. Savchenko": "sequence of the Affective Behavior Analysis in-the-wild (ABAW) challenges that"
        },
        {
          "2\nA.V. Savchenko": "involve different parts of\nthe Aff-Wild [11,23] and Aff-Wild2 [9,12] databases."
        },
        {
          "2\nA.V. Savchenko": "The organizers encourage participants to actively pre-train the models on other"
        },
        {
          "2\nA.V. Savchenko": "datasets by introducing all the new requirements. For example, one of the tasks"
        },
        {
          "2\nA.V. Savchenko": "of the third ABAW competition was the multi-task-learning (MTL) for simul-"
        },
        {
          "2\nA.V. Savchenko": "taneous prediction of\nfacial\nexpressions, valence, arousal, and AUs\n[6,10].\nIts"
        },
        {
          "2\nA.V. Savchenko": "winners [2] did not use the MTL small static training set (s-Aff-Wild2). Indeed,"
        },
        {
          "2\nA.V. Savchenko": "they proposed the Transformer-based Sign-and-Message Multi-Emotion Net that"
        },
        {
          "2\nA.V. Savchenko": "was trained on a large set of all video frames from the Aff-Wild2. The runner-up"
        },
        {
          "2\nA.V. Savchenko": "proposed the auditory-visual representations [4] that were also trained on initial"
        },
        {
          "2\nA.V. Savchenko": "video files. Similarly, the team that took fourth place in the MTL challenge de-"
        },
        {
          "2\nA.V. Savchenko": "veloped the transformer-based multimodal\nframework [24] trained on the video"
        },
        {
          "2\nA.V. Savchenko": "frames\nfrom the Aff-Wild2 dataset\nthat\nlet\nthem become the winners of FER"
        },
        {
          "2\nA.V. Savchenko": "and AU sub-challenges."
        },
        {
          "2\nA.V. Savchenko": "As a result,\nin the fourth ABAW competition [5], such usage of other data"
        },
        {
          "2\nA.V. Savchenko": "from Aff-Wild2 except the small training set is prohibited. It seems that only one"
        },
        {
          "2\nA.V. Savchenko": "successful participant (the third place) of the previous MTL challenge, namely,"
        },
        {
          "2\nA.V. Savchenko": "multi-head EfficientNet [19], and the baseline VGGFACE convolutional neural"
        },
        {
          "2\nA.V. Savchenko": "network (CNN) [6] satisfy new requirements. Moreover, a new challenge has been"
        },
        {
          "2\nA.V. Savchenko": "introduced that encourages the refinement of the pre-trained models on a set with"
        },
        {
          "2\nA.V. Savchenko": "synthetic faces generated from a small part of the Aff-Wild2 dataset [8,13,7]."
        },
        {
          "2\nA.V. Savchenko": "In this paper, we discuss our\nsolution for all\ntasks\nfrom the ABAW4 com-"
        },
        {
          "2\nA.V. Savchenko": "petition.\nIt\nis proposed to improve\nthe EfficientNet-based model\n[19] by pre-"
        },
        {
          "2\nA.V. Savchenko": "training a model on the AffectNet dataset [15] not only for FER but for addi-"
        },
        {
          "2\nA.V. Savchenko": "tional prediction of valence and arousal. The visual embeddings are extracted"
        },
        {
          "2\nA.V. Savchenko": "from the penultimate layer of the resulting MT-EmotiEffNet, while the valence,"
        },
        {
          "2\nA.V. Savchenko": "arousal, and logits for each emotion are obtained at the output of\nits last layer."
        },
        {
          "2\nA.V. Savchenko": "The multi-output feed-forward neural network is trained using the s-Aff-Wild2"
        },
        {
          "2\nA.V. Savchenko": "database\nfor\nthe MTL challenge. The best validation results are obtained by"
        },
        {
          "2\nA.V. Savchenko": "simple blending [17] of\nits predictions with action unit features from the Open-"
        },
        {
          "2\nA.V. Savchenko": "Face 2 toolkit\n[1].\nIn the Learning from Synthetic Data (LSD)\nchallenge, we"
        },
        {
          "2\nA.V. Savchenko": "propose to increase the quality of\nthe original\nsynthetic training set by using"
        },
        {
          "2\nA.V. Savchenko": "the super-resolution techniques, such as Real-ESRGAN [21], and fine-tuning the"
        },
        {
          "2\nA.V. Savchenko": "MT-EmotiEffNet on the new training set. The final prediction is a simple blend-"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "HSE-NN Team at the 4th ABAW Competition...\n3": ">/-’(1%$!-(%41415!*’-!"
        },
        {
          "HSE-NN Team at the 4th ABAW Competition...\n3": "D(%41!.<$-4E-%*J!"
        },
        {
          "HSE-NN Team at the 4th ABAW Competition...\n3": "B18<-!4.%5’!"
        },
        {
          "HSE-NN Team at the 4th ABAW Competition...\n3": "#??!"
        },
        {
          "HSE-NN Team at the 4th ABAW Competition...\n3": "CDE>.+-4>,,?’-!"
        },
        {
          "HSE-NN Team at the 4th ABAW Competition...\n3": "\"%7’!"
        },
        {
          "HSE-NN Team at the 4th ABAW Competition...\n3": ">,,474’1-?’-!"
        },
        {
          "HSE-NN Team at the 4th ABAW Competition...\n3": "6’-’7-+(!"
        },
        {
          "HSE-NN Team at the 4th ABAW Competition...\n3": ">.@’66415*!"
        },
        {
          "HSE-NN Team at the 4th ABAW Competition...\n3": "\">A!$+54-*!\n9%$’17’!\n;(+<*%$!"
        },
        {
          "HSE-NN Team at the 4th ABAW Competition...\n3": "#+17%-’1%-’!"
        },
        {
          "HSE-NN Team at the 4th ABAW Competition...\n3": "H8’1\"%7’!"
        },
        {
          "HSE-NN Team at the 4th ABAW Competition...\n3": ";=*!"
        },
        {
          "HSE-NN Team at the 4th ABAW Competition...\n3": "#+17%-’1%-’!"
        },
        {
          "HSE-NN Team at the 4th ABAW Competition...\n3": "CFG!"
        },
        {
          "HSE-NN Team at the 4th ABAW Competition...\n3": "\"#!$%&’()!30!\n\"#!$%&’()!\n\"#!$%&’()!"
        },
        {
          "HSE-NN Team at the 4th ABAW Competition...\n3": "*45.+46*!\n*+,-.%/!\n0!-%12!"
        },
        {
          "HSE-NN Team at the 4th ABAW Competition...\n3": "I$’16415!"
        },
        {
          "HSE-NN Team at the 4th ABAW Competition...\n3": "9%$’17’:;(+<*%$!\n\"%74%$!’/8(’**4+1!\n;7-4+1!=14-*!"
        },
        {
          "HSE-NN Team at the 4th ABAW Competition...\n3": "Fig. 1: Proposed model\nfor the multi-task-learning challenge"
        },
        {
          "HSE-NN Team at the 4th ABAW Competition...\n3": "ing of scores at the output of pre-trained and fine-tuned MT-EmotiEffNets. The"
        },
        {
          "HSE-NN Team at the 4th ABAW Competition...\n3": "source code for the proposed solutions are made publicly available1."
        },
        {
          "HSE-NN Team at the 4th ABAW Competition...\n3": "This paper is organized as follows. Section 2 introduces the MT-EmotiEffNet"
        },
        {
          "HSE-NN Team at the 4th ABAW Competition...\n3": "model and the training procedures for both tasks. Experimental results are pre-"
        },
        {
          "HSE-NN Team at the 4th ABAW Competition...\n3": "sented in Section 3. Concluding comments are discussed in Section 4."
        },
        {
          "HSE-NN Team at the 4th ABAW Competition...\n3": "2\nProposed Approach"
        },
        {
          "HSE-NN Team at the 4th ABAW Competition...\n3": "2.1\nMulti-task learning challenge"
        },
        {
          "HSE-NN Team at the 4th ABAW Competition...\n3": "The main task of human affective behavior analysis is FER. It is a typical prob-"
        },
        {
          "HSE-NN Team at the 4th ABAW Competition...\n3": "lem of\nimage recognition in which an input facial\nimage X should be associated"
        },
        {
          "HSE-NN Team at the 4th ABAW Competition...\n3": "with one of CEXP R > 1 categories (classes), such as anger, surprise, etc. There"
        },
        {
          "HSE-NN Team at the 4th ABAW Competition...\n3": "exist\nseveral commonly-used expression representations,\nsuch as estimation of"
        },
        {
          "HSE-NN Team at the 4th ABAW Competition...\n3": "Valence V\nand Arousal A (typically, V, A ∈ [−1, 1]) and AU detection [6]. The"
        },
        {
          "HSE-NN Team at the 4th ABAW Competition...\n3": "latter\ntask is a multi-label\nclassification problem,\ni.e., prediction of a binary"
        },
        {
          "HSE-NN Team at the 4th ABAW Competition...\n3": "is\nthe\ntotal number of AUs, and\nvector AU = [AU1, ..., AUCAU ], where CAU"
        },
        {
          "HSE-NN Team at the 4th ABAW Competition...\n3": "AUi ∈ {0, 1} is a binary indicator of the presence of the i-th AU in the photo."
        },
        {
          "HSE-NN Team at the 4th ABAW Competition...\n3": "1 https://github.com/HSE-asavchenko/face-emotion-recognition/blob/main/"
        },
        {
          "HSE-NN Team at the 4th ABAW Competition...\n3": "src/ABAW"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "4\nA.V. Savchenko": "In this paper, we propose the novel model\n(Fig. 1)\nfor\nthe MTL challenge"
        },
        {
          "4\nA.V. Savchenko": "from ABAW4 [6],\nin which the method from [19] was modified as follows:"
        },
        {
          "4\nA.V. Savchenko": "1. Emotional feature extractor is implemented with the novel MT-EmotiEffNet"
        },
        {
          "4\nA.V. Savchenko": "model based on EfficientNet-B0 architecture [20], which was pre-trained not"
        },
        {
          "4\nA.V. Savchenko": "only for FER but for additional valence-arousal estimation."
        },
        {
          "4\nA.V. Savchenko": "2. The valence and arousal\nfor\nthe input\nfacial photo are predicted by using"
        },
        {
          "4\nA.V. Savchenko": "only the output of the last layer of the MT-EmotiEffNet,\ni.e., they are not"
        },
        {
          "4\nA.V. Savchenko": "concatenated with embeddings extracted at its penultimate layer."
        },
        {
          "4\nA.V. Savchenko": "3. Three heads of the model\nfor facial expressions, Valence-Arousal and AUs,"
        },
        {
          "4\nA.V. Savchenko": "respectively, are trained sequentially,\ni.e., we do not need additional masks"
        },
        {
          "4\nA.V. Savchenko": "for missing data."
        },
        {
          "4\nA.V. Savchenko": "Let us consider the details of this pipeline.\nIts main part, namely, the MT-"
        },
        {
          "4\nA.V. Savchenko": "EmotiEffNet model, was pre-trained using PyTorch framework identically to"
        },
        {
          "4\nA.V. Savchenko": "EfficientNet-B0 from [18] on face identification task. The facial\nregions\nin the"
        },
        {
          "4\nA.V. Savchenko": "training set are simply cropped by a face detector without margins or face align-"
        },
        {
          "4\nA.V. Savchenko": "ment. Next, the last classification layer is replaced by a dense layer with 10 units"
        },
        {
          "4\nA.V. Savchenko": "for valence, arousal, and C (e) = 8 emotional categories\n(Neutral, Happy, Sad,"
        },
        {
          "4\nA.V. Savchenko": "Surprise, Fear, Anger, Disgust, Contempt) from the AffectNet dataset [15], re-"
        },
        {
          "4\nA.V. Savchenko": "spectively. The loss function is computed as a sum of Concordance Correlation"
        },
        {
          "4\nA.V. Savchenko": "Coefficients (CCCs) [14] and the weighted categorical cross-entropy [15]:"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "HSE-NN Team at the 4th ABAW Competition...\n5": "stored in the 10-dimensional\nlogits l and ln. We experimentally noticed that the"
        },
        {
          "HSE-NN Team at the 4th ABAW Competition...\n5": "valence and arousal\nfor\nthe MTL challenge are better predicted by using the"
        },
        {
          "HSE-NN Team at the 4th ABAW Competition...\n5": "logits only, while the facial expression and AUs are more accurately detected if"
        },
        {
          "HSE-NN Team at the 4th ABAW Competition...\n5": "the embeddings x and logits l are concatenated."
        },
        {
          "HSE-NN Team at the 4th ABAW Competition...\n5": "The remaining part of our neural network (Fig. 1) contains three output lay-"
        },
        {
          "HSE-NN Team at the 4th ABAW Competition...\n5": "ers, namely, (1) CEXP R = 8 units with softmax activation for recognition of one"
        },
        {
          "HSE-NN Team at the 4th ABAW Competition...\n5": "of eight emotions (Neutral, Anger, Disgust, Fear, Happiness, Sadness, Surprise,"
        },
        {
          "HSE-NN Team at the 4th ABAW Competition...\n5": "Other); (2) two neurons with tanh activation functions for Valence-Arousal pre-"
        },
        {
          "HSE-NN Team at the 4th ABAW Competition...\n5": "diction; and (3) CAU = 12 output units with sigmoid activation for AU detection."
        },
        {
          "HSE-NN Team at the 4th ABAW Competition...\n5": "The model was trained using the s-Aff-Wild2 cropped aligned set provided by the"
        },
        {
          "HSE-NN Team at the 4th ABAW Competition...\n5": "organizers [6]. This set contains N = 142, 333 facial images {Xn}, n ∈ {1, ..., N },"
        },
        {
          "HSE-NN Team at the 4th ABAW Competition...\n5": "for which the expression en ∈ {1, ..., CEXP R}, CAU -dimensional binary vector"
        },
        {
          "HSE-NN Team at the 4th ABAW Competition...\n5": "AUn of AUs, and/or valence Vn and arousal An are known. Some labels are"
        },
        {
          "HSE-NN Team at the 4th ABAW Competition...\n5": "missed, so only 90,645 emotional\nlabels, 103,316 AU labels, and 103,917 values"
        },
        {
          "HSE-NN Team at the 4th ABAW Competition...\n5": "of Valence-Arousal are available for training. The validation set contains 26,876"
        },
        {
          "HSE-NN Team at the 4th ABAW Competition...\n5": "facial\nframes,\nfor which AU and VA are known, but only 15,440 facial expres-"
        },
        {
          "HSE-NN Team at the 4th ABAW Competition...\n5": "sions are provided. The\nthree heads of\nthe model,\ni.e.,\nthree\nfully-connected"
        },
        {
          "HSE-NN Team at the 4th ABAW Competition...\n5": "(FC) layers, were trained separately by using the Tensorflow 2 framework and"
        },
        {
          "HSE-NN Team at the 4th ABAW Competition...\n5": "the procedure described in the paper [19]\nfor the uni-task learning."
        },
        {
          "HSE-NN Team at the 4th ABAW Competition...\n5": "Finally, we examined the possibility to improve the quality by using addi-"
        },
        {
          "HSE-NN Team at the 4th ABAW Competition...\n5": "tional\nfacial\nfeatures. The OpenFace 2 toolkit [1] extracted pose, gaze, eye, and"
        },
        {
          "HSE-NN Team at the 4th ABAW Competition...\n5": "AU features from each image. It was experimentally found that only the latter"
        },
        {
          "HSE-NN Team at the 4th ABAW Competition...\n5": "features are suitable for the FER part of the MTL challenge. Hence, we trained"
        },
        {
          "HSE-NN Team at the 4th ABAW Competition...\n5": "MLP (multi-layered perceptron) that contains an input layer with 35 AUs from"
        },
        {
          "HSE-NN Team at the 4th ABAW Competition...\n5": "the OpenFace, 1 hidden layer with 128 units and ReLU activation, and 1 output"
        },
        {
          "HSE-NN Team at the 4th ABAW Competition...\n5": "layer with CEXP R units and softmax activation. The component-wise weighted"
        },
        {
          "HSE-NN Team at the 4th ABAW Competition...\n5": "sum of\nthis model and the emotional\nscores\n(posterior\nthe CEXP R outputs of"
        },
        {
          "HSE-NN Team at the 4th ABAW Competition...\n5": "probabilities) at the output of “FC layer, softmax” (Fig. 1) is computed in the"
        },
        {
          "HSE-NN Team at the 4th ABAW Competition...\n5": "“Blending” layer, which returns the emotional class that corresponds to the max-"
        },
        {
          "HSE-NN Team at the 4th ABAW Competition...\n5": "imal component of this weighted sum. The best weight in a blending is estimated"
        },
        {
          "HSE-NN Team at the 4th ABAW Competition...\n5": "by maximizing the average F1 score of FER on the validation set."
        },
        {
          "HSE-NN Team at the 4th ABAW Competition...\n5": "2.2\nLearning from synthetic data challenge"
        },
        {
          "HSE-NN Team at the 4th ABAW Competition...\n5": "In the second sub-challenge,\nit was required to solve the FER task and associate"
        },
        {
          "HSE-NN Team at the 4th ABAW Competition...\n5": "an input facial\nimage with one of CEXP R = 6 basic expressions (Surprise, Fear,"
        },
        {
          "HSE-NN Team at the 4th ABAW Competition...\n5": "Disgust, Anger, Happiness, Sadness). The task is to refine the pre-trained model"
        },
        {
          "HSE-NN Team at the 4th ABAW Competition...\n5": "by using only information from 277,251 synthetic images that have been gener-"
        },
        {
          "HSE-NN Team at the 4th ABAW Competition...\n5": "ated from some specific frames from the Aff-Wild2 database. The validation set"
        },
        {
          "HSE-NN Team at the 4th ABAW Competition...\n5": "contains 4,670 images for the same subjects."
        },
        {
          "HSE-NN Team at the 4th ABAW Competition...\n5": "The proposed pipeline (Fig. 2) uses the MT-EmotiEffNet from the previous"
        },
        {
          "HSE-NN Team at the 4th ABAW Competition...\n5": "Subsection in a straightforward way. Foremost, we noticed that\nthe quality of"
        },
        {
          "HSE-NN Team at the 4th ABAW Competition...\n5": "provided synthetic\nfacial\nimages with a resolution of 112x112 is\ntoo low for"
        },
        {
          "HSE-NN Team at the 4th ABAW Competition...\n5": "our model that was trained on photos with a rather large resolution (224x224)."
        },
        {
          "HSE-NN Team at the 4th ABAW Competition...\n5": "Hence,\nit was proposed to enhance these images by using contemporary super-"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "resolution (SR) techniques, such as Real-ESRGAN [21].\nIn particular, the pre-": "trained model RealESRGAN x4plus with scale 2 was used for all training images."
        },
        {
          "resolution (SR) techniques, such as Real-ESRGAN [21].\nIn particular, the pre-": "Since the required set of 6 expressions\nis a subset of eight emotions\nfrom"
        },
        {
          "resolution (SR) techniques, such as Real-ESRGAN [21].\nIn particular, the pre-": "AffectNet, the simplest solution is to ignore the training set of synthetic images,"
        },
        {
          "resolution (SR) techniques, such as Real-ESRGAN [21].\nIn particular, the pre-": "and predict expression for an image X with the pre-trained MT-EmotiEffNet"
        },
        {
          "resolution (SR) techniques, such as Real-ESRGAN [21].\nIn particular, the pre-": "by using only 6 scores (emotional posterior probabilities) s(pre)(X) from the last"
        },
        {
          "resolution (SR) techniques, such as Real-ESRGAN [21].\nIn particular, the pre-": "(softmax) layer [18] that is associated with required categories."
        },
        {
          "resolution (SR) techniques, such as Real-ESRGAN [21].\nIn particular, the pre-": "To use\nthe provided synthetic data, we\nadditionally fine-tuned the MT-"
        },
        {
          "resolution (SR) techniques, such as Real-ESRGAN [21].\nIn particular, the pre-": "EmotiEffNet model on the\ntraining set at\nthe output of Real-ESRGAN. At"
        },
        {
          "resolution (SR) techniques, such as Real-ESRGAN [21].\nIn particular, the pre-": "first,\nthe output\nlayer was\nreplaced with the new fully-connected layer with"
        },
        {
          "resolution (SR) techniques, such as Real-ESRGAN [21].\nIn particular, the pre-": "the new head were learned during 3 epochs.\nCEXP R units, and the weights of"
        },
        {
          "resolution (SR) techniques, such as Real-ESRGAN [21].\nIn particular, the pre-": "The remaining weights were frozen. The weighted cross-entropy similar\nto (1)"
        },
        {
          "resolution (SR) techniques, such as Real-ESRGAN [21].\nIn particular, the pre-": "was minimized by the SAM [3] with Adam optimizer and learning rate of 0.001."
        },
        {
          "resolution (SR) techniques, such as Real-ESRGAN [21].\nIn particular, the pre-": "Moreover, we examine the subsequent fine-tuning of all weights on synthetic data"
        },
        {
          "resolution (SR) techniques, such as Real-ESRGAN [21].\nIn particular, the pre-": "during 6 epochs with a learning rate of 0.0001. We can use either model with a"
        },
        {
          "resolution (SR) techniques, such as Real-ESRGAN [21].\nIn particular, the pre-": "new head or completely fine-tuned EfficientNet to predict CEXP R-dimensional"
        },
        {
          "resolution (SR) techniques, such as Real-ESRGAN [21].\nIn particular, the pre-": "vector of scores s(f t)(X) for the input image. To make a final decision, blending"
        },
        {
          "resolution (SR) techniques, such as Real-ESRGAN [21].\nIn particular, the pre-": "of pre-trained and fine-tune models is again applied by computing the weighted"
        },
        {
          "resolution (SR) techniques, such as Real-ESRGAN [21].\nIn particular, the pre-": "sum of scores: s(X) = w · s(pre)(X) + (1 − w) · s(f t)(X) [17]. The final decision"
        },
        {
          "resolution (SR) techniques, such as Real-ESRGAN [21].\nIn particular, the pre-": "is made in favor of the expression that corresponds to the maximal component"
        },
        {
          "resolution (SR) techniques, such as Real-ESRGAN [21].\nIn particular, the pre-": "of the vector s(X). The best value of the weight hyperparameter w ∈ [0, 1],\nis"
        },
        {
          "resolution (SR) techniques, such as Real-ESRGAN [21].\nIn particular, the pre-": "estimated by maximizing the average F1 score on the validation set."
        }
      ],
      "page": 6
    },
    {
      "caption": "Table 1: Results for the AffectNet validation set (high accuracy and CCC are",
      "data": [
        {
          "Table 1: Results\nfor": "better,\nlow RMSE is better)",
          "the AffectNet validation set": "",
          "(high accuracy and CCC are": ""
        },
        {
          "Table 1: Results\nfor": "",
          "the AffectNet validation set": "Facial expressions",
          "(high accuracy and CCC are": "Valence"
        },
        {
          "Table 1: Results\nfor": "Model",
          "the AffectNet validation set": "8-class accuracy, %",
          "(high accuracy and CCC are": "RMSE"
        },
        {
          "Table 1: Results\nfor": "AlexNet [15]",
          "the AffectNet validation set": "58.0",
          "(high accuracy and CCC are": "0.394"
        },
        {
          "Table 1: Results\nfor": "EfficientNet-B0 [18]",
          "the AffectNet validation set": "61.32",
          "(high accuracy and CCC are": "-"
        },
        {
          "Table 1: Results\nfor": "SL + SSL inpanting-pl (B0) [16]",
          "the AffectNet validation set": "61.72",
          "(high accuracy and CCC are": "-"
        },
        {
          "Table 1: Results\nfor": "Distract Your Attention [22]",
          "the AffectNet validation set": "62.09",
          "(high accuracy and CCC are": "-"
        },
        {
          "Table 1: Results\nfor": "EfficientNet-B2 [19]",
          "the AffectNet validation set": "63.03",
          "(high accuracy and CCC are": "-"
        },
        {
          "Table 1: Results\nfor": "MT-EmotiEffNet",
          "the AffectNet validation set": "61.93",
          "(high accuracy and CCC are": "0.434"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table 2: Ablation study for the MTL challenge",
      "data": [
        {
          "8\nA.V. Savchenko": ""
        },
        {
          "8\nA.V. Savchenko": "CNN"
        },
        {
          "8\nA.V. Savchenko": ""
        },
        {
          "8\nA.V. Savchenko": "EfficientNet-B0 [18]"
        },
        {
          "8\nA.V. Savchenko": ""
        },
        {
          "8\nA.V. Savchenko": "MT-EmotiEffNet"
        },
        {
          "8\nA.V. Savchenko": "(simultaneous"
        },
        {
          "8\nA.V. Savchenko": "training"
        },
        {
          "8\nA.V. Savchenko": "of heads)"
        },
        {
          "8\nA.V. Savchenko": "MT-EmotiEffNet"
        },
        {
          "8\nA.V. Savchenko": "(separate"
        },
        {
          "8\nA.V. Savchenko": "training"
        },
        {
          "8\nA.V. Savchenko": "of heads)"
        },
        {
          "8\nA.V. Savchenko": ""
        },
        {
          "8\nA.V. Savchenko": ""
        }
      ],
      "page": 8
    },
    {
      "caption": "Table 2: Ablation study for the MTL challenge",
      "data": [
        {
          "The proposed MT-EmotiEffNet has 0.06-0.07 greater overall quality PM T L": ""
        },
        {
          "The proposed MT-EmotiEffNet has 0.06-0.07 greater overall quality PM T L": "If\nthe heads are trained"
        },
        {
          "The proposed MT-EmotiEffNet has 0.06-0.07 greater overall quality PM T L": ""
        },
        {
          "The proposed MT-EmotiEffNet has 0.06-0.07 greater overall quality PM T L": "it was experimentally found that valence and arousal are bet-"
        },
        {
          "The proposed MT-EmotiEffNet has 0.06-0.07 greater overall quality PM T L": ""
        },
        {
          "The proposed MT-EmotiEffNet has 0.06-0.07 greater overall quality PM T L": "to the Valence-Arousal head but concatenates\nlogits"
        },
        {
          "The proposed MT-EmotiEffNet has 0.06-0.07 greater overall quality PM T L": "increased to"
        },
        {
          "The proposed MT-EmotiEffNet has 0.06-0.07 greater overall quality PM T L": ""
        },
        {
          "The proposed MT-EmotiEffNet has 0.06-0.07 greater overall quality PM T L": "rather accurate FER,\nso"
        },
        {
          "The proposed MT-EmotiEffNet has 0.06-0.07 greater overall quality PM T L": ""
        },
        {
          "The proposed MT-EmotiEffNet has 0.06-0.07 greater overall quality PM T L": "remarkable that AU features\nfrom"
        },
        {
          "The proposed MT-EmotiEffNet has 0.06-0.07 greater overall quality PM T L": ""
        },
        {
          "The proposed MT-EmotiEffNet has 0.06-0.07 greater overall quality PM T L": ""
        }
      ],
      "page": 8
    },
    {
      "caption": "Table 4: Class-level F1 score of the MT-EmotiEffNet for the FER task",
      "data": [
        {
          "Table 6: Multi-Task-Learning challenge results on the s-Aff-Wild2 validation set": "Model"
        },
        {
          "Table 6: Multi-Task-Learning challenge results on the s-Aff-Wild2 validation set": "VGGFACE Baseline [5]"
        },
        {
          "Table 6: Multi-Task-Learning challenge results on the s-Aff-Wild2 validation set": "EfficientNet-B2 [19]"
        },
        {
          "Table 6: Multi-Task-Learning challenge results on the s-Aff-Wild2 validation set": "MT-EmotiEffNet"
        },
        {
          "Table 6: Multi-Task-Learning challenge results on the s-Aff-Wild2 validation set": ""
        },
        {
          "Table 6: Multi-Task-Learning challenge results on the s-Aff-Wild2 validation set": ""
        },
        {
          "Table 6: Multi-Task-Learning challenge results on the s-Aff-Wild2 validation set": "Proposed complete"
        },
        {
          "Table 6: Multi-Task-Learning challenge results on the s-Aff-Wild2 validation set": ""
        },
        {
          "Table 6: Multi-Task-Learning challenge results on the s-Aff-Wild2 validation set": "model (Fig. 1)"
        }
      ],
      "page": 9
    },
    {
      "caption": "Table 7: Table7:F1scoreofsingle models forthelearningfromsyntheticdata challenge",
      "data": [
        {
          "10\nA.V. Savchenko": "fine-tuned on both an original set of synthetic images provided by the organizers"
        },
        {
          "10\nA.V. Savchenko": "and its enhancement by using SR techniques [21]. The official validation set of the"
        },
        {
          "10\nA.V. Savchenko": "organizers without SR was used in all experiments to make the metrics directly"
        },
        {
          "10\nA.V. Savchenko": "comparable. The macro-averaged F1 scores of"
        },
        {
          "10\nA.V. Savchenko": "in Table 7."
        },
        {
          "10\nA.V. Savchenko": "Table 7: F1 score of single models for the learning from synthetic data challenge"
        },
        {
          "10\nA.V. Savchenko": "Pre-\nFine-tuned (orig)"
        }
      ],
      "page": 10
    },
    {
      "caption": "Table 7: Table7:F1scoreofsingle models forthelearningfromsyntheticdata challenge",
      "data": [
        {
          "Table 7: F1 score of single models for the learning from synthetic data challenge": ""
        },
        {
          "Table 7: F1 score of single models for the learning from synthetic data challenge": "CNN"
        },
        {
          "Table 7: F1 score of single models for the learning from synthetic data challenge": "EfficientNet-B0 [18]"
        },
        {
          "Table 7: F1 score of single models for the learning from synthetic data challenge": "EfficientNet-B2 [19]"
        },
        {
          "Table 7: F1 score of single models for the learning from synthetic data challenge": "MT-EmotiEffNet"
        }
      ],
      "page": 10
    },
    {
      "caption": "Table 8: F1 score of ensemble models for the learning from synthetic data chal-",
      "data": [
        {
          "Table 8: F1 score of ensemble models for the learning from synthetic data chal-": "lenge, CNN fine-tuned on the training set with super-resolution"
        },
        {
          "Table 8: F1 score of ensemble models for the learning from synthetic data chal-": "CNN"
        },
        {
          "Table 8: F1 score of ensemble models for the learning from synthetic data chal-": ""
        },
        {
          "Table 8: F1 score of ensemble models for the learning from synthetic data chal-": ""
        },
        {
          "Table 8: F1 score of ensemble models for the learning from synthetic data chal-": "EfficientNet-B0 [18]"
        },
        {
          "Table 8: F1 score of ensemble models for the learning from synthetic data chal-": ""
        },
        {
          "Table 8: F1 score of ensemble models for the learning from synthetic data chal-": ""
        },
        {
          "Table 8: F1 score of ensemble models for the learning from synthetic data chal-": ""
        },
        {
          "Table 8: F1 score of ensemble models for the learning from synthetic data chal-": ""
        },
        {
          "Table 8: F1 score of ensemble models for the learning from synthetic data chal-": "MT-EmotiEffNet"
        },
        {
          "Table 8: F1 score of ensemble models for the learning from synthetic data chal-": ""
        },
        {
          "Table 8: F1 score of ensemble models for the learning from synthetic data chal-": ""
        }
      ],
      "page": 11
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "References": "1. Baltrusaitis, T., Zadeh, A., Lim, Y.C., Morency, L.P.: OpenFace 2.0: Facial behav-"
        },
        {
          "References": "ior analysis toolkit. In: Proceedings of the 13th IEEE International Conference on"
        },
        {
          "References": "Automatic Face & Gesture Recognition (FG 2018). pp. 59–66. IEEE (2018)"
        },
        {
          "References": "2. Deng, D., Shi, B.E.: Estimating multiple emotion descriptors by separating descrip-"
        },
        {
          "References": "tion and inference.\nIn: Proceedings of\nthe IEEE/CVF Conference on Computer"
        },
        {
          "References": "Vision and Pattern Recognition (CVPR) Workshops. pp. 2392–2400 (2022)"
        },
        {
          "References": "3. Foret, P., Kleiner, A., Mobahi, H., Neyshabur, B.: Sharpness-aware minimization"
        },
        {
          "References": "for efficiently improving generalization. arXiv preprint arXiv:2010.01412 (2020)"
        },
        {
          "References": "4.\nJeong, E., Oh, G., Lim, S.: Multi-task learning for human affect prediction with"
        },
        {
          "References": "auditory-visual\nsynchronized representation.\nIn: Proceedings of\nthe\nIEEE/CVF"
        },
        {
          "References": "Conference on Computer Vision and Pattern Recognition (CVPR) Workshops."
        },
        {
          "References": "pp. 2438–2445 (2022)"
        },
        {
          "References": "5. Kollias, D.: ABAW: Learning from synthetic data & multi-task learning challenges."
        },
        {
          "References": "arXiv preprint arXiv:2207.01138 (2022)"
        },
        {
          "References": "6. Kollias, D.: ABAW: Valence-arousal estimation, expression recognition, action unit"
        },
        {
          "References": "detection & multi-task learning challenges. In: Proceedings of the IEEE/CVF Con-"
        },
        {
          "References": "ference on Computer Vision and Pattern Recognition (CVPR) Workshops. pp."
        },
        {
          "References": "2328–2336 (2022)"
        },
        {
          "References": "7. Kollias, D., Cheng, S., Pantic, M., Zafeiriou, S.: Photorealistic\nfacial\nsynthesis"
        },
        {
          "References": "in the dimensional affect\nspace.\nIn: Proceedings of\nthe European Conference on"
        },
        {
          "References": "Computer Vision (ECCV) Workshops. pp. 0–0 (2018)"
        }
      ],
      "page": 12
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "HSE-NN Team at the 4th ABAW Competition...\n13": "8. Kollias, D., Cheng, S., Ververas, E., Kotsia, I., Zafeiriou, S.: Deep neural network"
        },
        {
          "HSE-NN Team at the 4th ABAW Competition...\n13": "augmentation: Generating faces for affect analysis. International Journal of Com-"
        },
        {
          "HSE-NN Team at the 4th ABAW Competition...\n13": "puter Vision 128(5), 1455–1484 (2020)"
        },
        {
          "HSE-NN Team at the 4th ABAW Competition...\n13": "9. Kollias, D., Nicolaou, M.A., Kotsia,\nI., Zhao, G., Zafeiriou, S.: Recognition of"
        },
        {
          "HSE-NN Team at the 4th ABAW Competition...\n13": "affect in the wild using deep neural networks.\nIn: Proceedings of the IEEE/CVF"
        },
        {
          "HSE-NN Team at the 4th ABAW Competition...\n13": "Conference on Computer Vision and Pattern Recognition (CVPR) Workshops. pp."
        },
        {
          "HSE-NN Team at the 4th ABAW Competition...\n13": "1972–1979. IEEE (2017)"
        },
        {
          "HSE-NN Team at the 4th ABAW Competition...\n13": "10. Kollias, D., Sharmanska, V., Zafeiriou, S.: Distribution matching for heteroge-"
        },
        {
          "HSE-NN Team at the 4th ABAW Competition...\n13": "neous multi-task learning: a large-scale face study. arXiv preprint arXiv:2105.03790"
        },
        {
          "HSE-NN Team at the 4th ABAW Competition...\n13": "(2021)"
        },
        {
          "HSE-NN Team at the 4th ABAW Competition...\n13": "11. Kollias, D., Tzirakis, P., Nicolaou, M.A., Papaioannou, A., Zhao, G., Schuller, B.,"
        },
        {
          "HSE-NN Team at the 4th ABAW Competition...\n13": "Kotsia, I., Zafeiriou, S.: Deep affect prediction in-the-wild: Aff-Wild database and"
        },
        {
          "HSE-NN Team at the 4th ABAW Competition...\n13": "challenge, deep architectures, and beyond pp. 1–23 (2019)"
        },
        {
          "HSE-NN Team at the 4th ABAW Competition...\n13": "12. Kollias, D., Zafeiriou, S.: Expression, affect, action unit\nrecognition: Aff-Wild2,"
        },
        {
          "HSE-NN Team at the 4th ABAW Competition...\n13": "multi-task learning and arcface. arXiv preprint arXiv:1910.04855 (2019)"
        },
        {
          "HSE-NN Team at the 4th ABAW Competition...\n13": "13. Kollias, D., Zafeiriou, S.: Va-stargan: Continuous affect generation.\nIn:\nInterna-"
        },
        {
          "HSE-NN Team at the 4th ABAW Competition...\n13": "tional Conference on Advanced Concepts for Intelligent Vision Systems. pp. 227–"
        },
        {
          "HSE-NN Team at the 4th ABAW Competition...\n13": "238. Springer (2020)"
        },
        {
          "HSE-NN Team at the 4th ABAW Competition...\n13": "14. Kollias, D., Zafeiriou, S.: Affect analysis in-the-wild: Valence-arousal, expressions,"
        },
        {
          "HSE-NN Team at the 4th ABAW Competition...\n13": "action units and a unified framework. arXiv preprint arXiv:2103.15792 (2021)"
        },
        {
          "HSE-NN Team at the 4th ABAW Competition...\n13": "15. Mollahosseini, A., Hasani, B., Mahoor, M.H.: AffectNet: A database\nfor\nfacial"
        },
        {
          "HSE-NN Team at the 4th ABAW Competition...\n13": "expression, valence, and arousal\ncomputing in the wild.\nIEEE Trans. Affective"
        },
        {
          "HSE-NN Team at the 4th ABAW Competition...\n13": "Computing 10(1), 18–31 (2017)"
        },
        {
          "HSE-NN Team at the 4th ABAW Competition...\n13": "16. Pourmirzaei, M., Montazer, G.A., Esmaili, F.: Using self-supervised auxiliary tasks"
        },
        {
          "HSE-NN Team at the 4th ABAW Competition...\n13": "to\nimprove\nfine-grained\nfacial\nrepresentation.\narXiv\npreprint\narXiv:2105.06421"
        },
        {
          "HSE-NN Team at the 4th ABAW Competition...\n13": "(2021)"
        },
        {
          "HSE-NN Team at the 4th ABAW Competition...\n13": "17. Savchenko, A., Alekseev, A., Kwon, S., Tutubalina, E., Myasnikov, E., Nikolenko,"
        },
        {
          "HSE-NN Team at the 4th ABAW Competition...\n13": "S.: Ad lingua: Text classification improves symbolism prediction in image adver-"
        },
        {
          "HSE-NN Team at the 4th ABAW Competition...\n13": "tisements. In: Proceedings of the 28th International Conference on Computational"
        },
        {
          "HSE-NN Team at the 4th ABAW Competition...\n13": "Linguistics (COLING). pp. 1886–1892 (2020)"
        },
        {
          "HSE-NN Team at the 4th ABAW Competition...\n13": "18. Savchenko, A.V.: Facial expression and attributes recognition based on multi-task"
        },
        {
          "HSE-NN Team at the 4th ABAW Competition...\n13": "learning of\nlightweight neural networks.\nIn: 19th Int. Symposium on Intelligent"
        },
        {
          "HSE-NN Team at the 4th ABAW Competition...\n13": "Systems and Informatics (SISY). pp. 119–124. IEEE (2021)"
        },
        {
          "HSE-NN Team at the 4th ABAW Competition...\n13": "19. Savchenko, A.V.: Video-based frame-level\nfacial analysis of affective behavior on"
        },
        {
          "HSE-NN Team at the 4th ABAW Competition...\n13": "mobile devices using EfficientNets. In: Proceedings of the IEEE/CVF Conference"
        },
        {
          "HSE-NN Team at the 4th ABAW Competition...\n13": "on Computer Vision and Pattern Recognition (CVPR) Workshops. pp. 2359–2366"
        },
        {
          "HSE-NN Team at the 4th ABAW Competition...\n13": "(2022)"
        },
        {
          "HSE-NN Team at the 4th ABAW Competition...\n13": "20. Tan, M., Le, Q.: EfficientNet: Rethinking model\nscaling for convolutional neural"
        },
        {
          "HSE-NN Team at the 4th ABAW Competition...\n13": "networks. In: Int. Conf. Mach. Learn. pp. 6105–6114 (2019)"
        },
        {
          "HSE-NN Team at the 4th ABAW Competition...\n13": "21. Wang, X., Xie, L., Dong, C., Shan, Y.: Real-ESRGAN: Training real-world blind"
        },
        {
          "HSE-NN Team at the 4th ABAW Competition...\n13": "super-resolution with pure synthetic data.\nIn: Proceedings of the IEEE/CVF In-"
        },
        {
          "HSE-NN Team at the 4th ABAW Competition...\n13": "ternational Conference on Computer Vision (ICCV). pp. 1905–1914 (2021)"
        },
        {
          "HSE-NN Team at the 4th ABAW Competition...\n13": "22. Wen, Z., Lin, W., Wang, T., Xu, G.: Distract your attention: Multi-head cross at-"
        },
        {
          "HSE-NN Team at the 4th ABAW Competition...\n13": "tention network for facial expression recognition. arXiv preprint arXiv:2109.07270"
        },
        {
          "HSE-NN Team at the 4th ABAW Competition...\n13": "(2021)"
        },
        {
          "HSE-NN Team at the 4th ABAW Competition...\n13": "23. Zafeiriou, S., Kollias, D., Nicolaou, M.A., Papaioannou, A., Zhao, G., Kotsia,"
        },
        {
          "HSE-NN Team at the 4th ABAW Competition...\n13": "I.: Aff-Wild: Valence and arousal\n‘in-the-wild’\nchallenge.\nIn: Proceedings of\nthe"
        },
        {
          "HSE-NN Team at the 4th ABAW Competition...\n13": "IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
        },
        {
          "HSE-NN Team at the 4th ABAW Competition...\n13": "Workshops. pp. 1980–1987. IEEE (2017)"
        }
      ],
      "page": 13
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "14": "24. Zhang, W., Qiu, F., Wang, S., Zeng, H., Zhang, Z., An, R., Ma, B., Ding, Y.:",
          "A.V. Savchenko": ""
        },
        {
          "14": "",
          "A.V. Savchenko": "Transformer-based multimodal\ninformation fusion for\nfacial\nexpression analysis."
        },
        {
          "14": "",
          "A.V. Savchenko": "In: Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern"
        },
        {
          "14": "",
          "A.V. Savchenko": "Recognition (CVPR) Workshops. pp. 2428–2437 (2022)"
        }
      ],
      "page": 14
    }
  ],
  "citations": [
    {
      "citation_id": "2",
      "title": "7-+(! #+17%-'1%-'! CDE>",
      "venue": "7-+(! #+17%-'1%-'! CDE>"
    },
    {
      "citation_id": "3",
      "title": "OpenFace 2.0: Facial behavior analysis toolkit",
      "authors": [
        "T Baltrusaitis",
        "A Zadeh",
        "Y Lim",
        "L Morency"
      ],
      "year": "2018",
      "venue": "Proceedings of the 13th IEEE International Conference on Automatic Face & Gesture Recognition"
    },
    {
      "citation_id": "4",
      "title": "Estimating multiple emotion descriptors by separating description and inference",
      "authors": [
        "D Deng",
        "B Shi"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops"
    },
    {
      "citation_id": "5",
      "title": "Sharpness-aware minimization for efficiently improving generalization",
      "authors": [
        "P Foret",
        "A Kleiner",
        "H Mobahi",
        "B Neyshabur"
      ],
      "year": "2020",
      "venue": "Sharpness-aware minimization for efficiently improving generalization",
      "arxiv": "arXiv:2010.01412"
    },
    {
      "citation_id": "6",
      "title": "Multi-task learning for human affect prediction with auditory-visual synchronized representation",
      "authors": [
        "E Jeong",
        "G Oh",
        "S Lim"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops"
    },
    {
      "citation_id": "7",
      "title": "ABAW: Learning from synthetic data & multi-task learning challenges",
      "authors": [
        "D Kollias"
      ],
      "year": "2022",
      "venue": "ABAW: Learning from synthetic data & multi-task learning challenges",
      "arxiv": "arXiv:2207.01138"
    },
    {
      "citation_id": "8",
      "title": "ABAW: Valence-arousal estimation, expression recognition, action unit detection & multi-task learning challenges",
      "authors": [
        "D Kollias"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops"
    },
    {
      "citation_id": "9",
      "title": "Photorealistic facial synthesis in the dimensional affect space",
      "authors": [
        "D Kollias",
        "S Cheng",
        "M Pantic",
        "S Zafeiriou"
      ],
      "year": "2018",
      "venue": "Proceedings of the European Conference on Computer Vision (ECCV) Workshops"
    },
    {
      "citation_id": "10",
      "title": "Deep neural network augmentation: Generating faces for affect analysis",
      "authors": [
        "D Kollias",
        "S Cheng",
        "E Ververas",
        "I Kotsia",
        "S Zafeiriou"
      ],
      "year": "2020",
      "venue": "International Journal of Computer Vision"
    },
    {
      "citation_id": "11",
      "title": "Recognition of affect in the wild using deep neural networks",
      "authors": [
        "D Kollias",
        "M Nicolaou",
        "I Kotsia",
        "G Zhao",
        "S Zafeiriou"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops"
    },
    {
      "citation_id": "12",
      "title": "Distribution matching for heterogeneous multi-task learning: a large-scale face study",
      "authors": [
        "D Kollias",
        "V Sharmanska",
        "S Zafeiriou"
      ],
      "year": "2021",
      "venue": "Distribution matching for heterogeneous multi-task learning: a large-scale face study",
      "arxiv": "arXiv:2105.03790"
    },
    {
      "citation_id": "13",
      "title": "Deep affect prediction in-the-wild: Aff-Wild database and challenge",
      "authors": [
        "D Kollias",
        "P Tzirakis",
        "M Nicolaou",
        "A Papaioannou",
        "G Zhao",
        "B Schuller",
        "I Kotsia",
        "S Zafeiriou"
      ],
      "year": "2019",
      "venue": "Deep affect prediction in-the-wild: Aff-Wild database and challenge"
    },
    {
      "citation_id": "14",
      "title": "Expression, affect, action unit recognition: Aff-Wild2, multi-task learning and arcface",
      "authors": [
        "D Kollias",
        "S Zafeiriou"
      ],
      "year": "2019",
      "venue": "Expression, affect, action unit recognition: Aff-Wild2, multi-task learning and arcface",
      "arxiv": "arXiv:1910.04855"
    },
    {
      "citation_id": "15",
      "title": "Va-stargan: Continuous affect generation",
      "authors": [
        "D Kollias",
        "S Zafeiriou"
      ],
      "year": "2020",
      "venue": "International Conference on Advanced Concepts for Intelligent Vision Systems"
    },
    {
      "citation_id": "16",
      "title": "Affect analysis in-the-wild: Valence-arousal, expressions, action units and a unified framework",
      "authors": [
        "D Kollias",
        "S Zafeiriou"
      ],
      "year": "2021",
      "venue": "Affect analysis in-the-wild: Valence-arousal, expressions, action units and a unified framework",
      "arxiv": "arXiv:2103.15792"
    },
    {
      "citation_id": "17",
      "title": "AffectNet: A database for facial expression, valence, and arousal computing in the wild",
      "authors": [
        "A Mollahosseini",
        "B Hasani",
        "M Mahoor"
      ],
      "year": "2017",
      "venue": "IEEE Trans. Affective Computing"
    },
    {
      "citation_id": "18",
      "title": "Using self-supervised auxiliary tasks to improve fine-grained facial representation",
      "authors": [
        "M Pourmirzaei",
        "G Montazer",
        "F Esmaili"
      ],
      "year": "2021",
      "venue": "Using self-supervised auxiliary tasks to improve fine-grained facial representation",
      "arxiv": "arXiv:2105.06421"
    },
    {
      "citation_id": "19",
      "title": "Ad lingua: Text classification improves symbolism prediction in image advertisements",
      "authors": [
        "A Savchenko",
        "A Alekseev",
        "S Kwon",
        "E Tutubalina",
        "E Myasnikov",
        "S Nikolenko"
      ],
      "year": "2020",
      "venue": "Proceedings of the 28th International Conference on Computational Linguistics (COLING)"
    },
    {
      "citation_id": "20",
      "title": "Facial expression and attributes recognition based on multi-task learning of lightweight neural networks",
      "authors": [
        "A Savchenko"
      ],
      "year": "2021",
      "venue": "19th Int. Symposium on Intelligent Systems and Informatics (SISY)"
    },
    {
      "citation_id": "21",
      "title": "Video-based frame-level facial analysis of affective behavior on mobile devices using EfficientNets",
      "authors": [
        "A Savchenko"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops"
    },
    {
      "citation_id": "22",
      "title": "EfficientNet: Rethinking model scaling for convolutional neural networks",
      "authors": [
        "M Tan",
        "Q Le"
      ],
      "year": "2019",
      "venue": "Int. Conf. Mach. Learn"
    },
    {
      "citation_id": "23",
      "title": "Real-ESRGAN: Training real-world blind super-resolution with pure synthetic data",
      "authors": [
        "X Wang",
        "L Xie",
        "C Dong",
        "Y Shan"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)"
    },
    {
      "citation_id": "24",
      "title": "Distract your attention: Multi-head cross attention network for facial expression recognition",
      "authors": [
        "Z Wen",
        "W Lin",
        "T Wang",
        "G Xu"
      ],
      "year": "2021",
      "venue": "Distract your attention: Multi-head cross attention network for facial expression recognition",
      "arxiv": "arXiv:2109.07270"
    },
    {
      "citation_id": "25",
      "title": "Aff-Wild: Valence and arousal 'in-the-wild' challenge",
      "authors": [
        "S Zafeiriou",
        "D Kollias",
        "M Nicolaou",
        "A Papaioannou",
        "G Zhao",
        "I Kotsia"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "26",
      "title": "Transformer-based multimodal information fusion for facial expression analysis",
      "authors": [
        "W Zhang",
        "F Qiu",
        "S Wang",
        "H Zeng",
        "Z Zhang",
        "R An",
        "B Ma",
        "Y Ding"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    }
  ]
}