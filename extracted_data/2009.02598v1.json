{
  "paper_id": "2009.02598v1",
  "title": "Semi-Supervised Multi-Modal Emotion Recognition With Cross-Modal Distribution Matching",
  "published": "2020-09-05T20:51:01Z",
  "authors": [
    "Jingjun Liang",
    "Ruichen Li",
    "Qin Jin"
  ],
  "keywords": [
    "Multimodal Emotion Recognition",
    "Cross-Modality Distribution Matching",
    "Semi-supervised Learning"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Automatic emotion recognition is an active research topic with wide range of applications. Due to the high manual annotation cost and inevitable label ambiguity, the development of emotion recognition dataset is limited in both scale and quality. Therefore, one of the key challenges is how to build effective models with limited data resource. Previous works have explored different approaches to tackle this challenge including data enhancement, transfer learning, and semi-supervised learning etc. However, the weakness of these existing approaches includes such as training instability, large performance loss during transfer, or marginal improvement. In this work, we propose a novel semi-supervised multi-modal emotion recognition model based on cross-modality distribution matching, which leverages abundant unlabeled data to enhance the model training under the assumption that the inner emotional status is consistent at the utterance level across modalities. We conduct extensive experiments to evaluate the proposed model on two benchmark datasets, IEMOCAP and MELD. The experiment results prove that the proposed semi-supervised learning model can effectively utilize unlabeled data and combine multi-modalities to boost the emotion recognition performance, which outperforms other state-of-the-art approaches under the same condition. The proposed model also achieves competitive capacity compared with existing approaches which take advantage of additional auxiliary information such as speaker and interaction context. \n CCS Concepts • Computing methodologies → Semi-supervised learning settings; Semantic networks; • Human-centered computing → HCI design and evaluation methods.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Introduction",
      "text": "Emotion is an important part of daily interpersonal human interactions. Automatic recognition or detection of human emotions have attracted much research interest in the field of computer vision, speech processing, and multimedia computing. Emotion recognition technology has a wide range of applications including assisting mental health analysis  [14] , improving natural human machine interaction  [15] , enabling emotional robot design and intelligent education tutoring  [31, 54]  etc.\n\nEmotion recognition can be generally categorized into two types of tasks, namely discrete (categorical) emotion recognition and continuous (dimensional) emotion recognition. The discrete emotion recognition normally divides the emotion space into several basic emotion classes such as happiness, sadness, anger and neutral etc  [12] , while the continuous emotion recognition treats emotional state as distribution in a continuous space, which is normally described by two or three dimensions such as arousal, valence and dominance  [37] . Although continuous emotion representation can model more flexible and complicated emotional state, it is not as easy to understand as discrete emotion representation, which is reflected in the quite high variance in continuous emotion human annotations from different annotators  [8, 41] . We therefore focus on the discrete emotion modeling in this work.\n\nWe humans convey emotions in various ways including both spoken words and nonverbal behaviors, such as facial expression and body language etc  [16] . Such rich information from multimodalities could be used to understand the emotional state  [29] . Previous research works have shown that different modalities are complementary for emotion recognition  [23, 36] . Different modalities all carry emotion relevant information and how to effectively combine multiple modalities has been an active research focus.\n\nBesides multi-modality, another challenge for emotion recognition is the limitation of supervised data. Although we can easily collect large amount of emotional data from online social media, the emotion annotation requires heavy manual efforts and usually involves inevitable label ambiguity. Therefore shortage of high quality supervised data has been a big obstacle for developing generalized and robust emotion models. There have been some endeavors to tackle the data shortage challenge. For example, Albanie et al.  [1]  apply transfer learning to obtain supervision from another labeled modality. However, the improvement is very marginal. Data augmentation and semi-supervised learning through generative adversarial network  [8, 18, 39]  have also been explored. However, such models are hard to optimize due to the unstable training procedure and non-intuitive synthetic samples.\n\nInspired by the research in cross-modal retrieval task  [6, 48, 55] , in this paper, we propose a novel semi-supervised training strategy for discrete multi-modal emotion recognition. We assume that different modalities are expected to express similar emotion information at the coarse-grained level (such as the utterance level) under a certain scenario as shown in Figure  1 . Under this assumption, we can regard this latent relationship as an auxiliary task to obtain guidance from unlabeled data to enhance the fully-supervised training procedure. Specifically, we use auto-encoder structure  [38]  to extract utterance-level representation from different modalities and apply Maximum Mean Discrepancy(MMD)  [43]  to restrict their distribution difference. We conduct extensive experiments to compare with other state-of-the-art techniques on two benchmark datasets, IEMOCAP  [5]  and MELD  [35] , and demonstrate the effectiveness of our proposed semi-supervised learning approach. We also carry out detailed analysis experiments to study the performance impact from each model component.\n\nThe remainder of this paper is organized as follows. Section 2 introduces some related works. Section 3 describes the details of our emotion recognition system based on the proposed semi-supervised learning strategy, including the representation learning with DAE and the design of multiple loss functions. Section 4 then shows the extensive experiment results on two benchmark datasets. Finally, Section 6 presents our conclusions.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Related Works 2.1 Multi-Modal Emotion Recognition",
      "text": "The quality of multi-modal features plays a decisive role in emotion recognition. Thus previous works have explored effective features in acoustic, visual and lexical modalities for emotion recognition tasks. Brady et al.  [4]  derive high-level acoustic, visual and physiological features from the low-level descriptors using sparse coding and deep learning. Seng et al.  [44]  uses a mixture of rulebased and machine learning techniques upon prosodic and spectral features to determine the emotion state contained in the audio and visual signal. The granularity of these features varies from frame level to sentence level.\n\nFor modality aggregation, Viktor et al.  [36]  use early fusion to concatenate multi-modal features as the input for the inference models. But it ignores the mismatch between different modalities.\n\nConsidering the inner relationship alignment, Yoon et al.  [51]  propose deep dual recurrent encoder to combine text information and speech signals concurrently to gain a better understanding of emotion recognition. Xu et al.  [50]  propose to learn the frame-level alignment between speech and text signal via attention mechanism. They both learn the similarity between these two modalities to compress acoustic sequence and align the speech with text. However, the length of speech sequence is much larger than text sequence so that the accurate alignment is quite hard to learn. To avoid this problem, we tend to use utterance acoustic feature in this work.\n\nFor modeling the long-term dependency in emotion expression, Mao et al.  [30]  aggregate the segment-level decisions to improve utterance-level classification. Li et al.  [27]  propose an novel representation learning component with residual convolutional network, multi-head self-attention and global context-aware attention LSTM. Following their suggestion, we utilize self-attention mechanism to capture temporal information as well.\n\nBesides, emotion recognition in conversational scenario has become a popular sub-task recently. It emphasizes the interaction contextual information extraction and modeling in a human dialogue. Several approaches have been proposed to capture contextual and speaker cues to assist emotion recognition  [20, 21, 28, 33, 53] . Although our proposed work is applied in non-interactive scenario, we compare the emotion recognition performance on the same dataset with these interactive models as well.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Semi-Supervised Emotion Recognition",
      "text": "Du et al.  [11]  propose a semi-supervised multi-modal generative framework with non-uniformly weighted Gaussian mixture posterior approximation for the shared latent variable. They use a conditional probabilistic distribution for the unknown labels in the semisupervised classification algorithm. Salimans et al.  [39]  use generative adversarial networks to implement semi-supervised learning. And Chang et al.  [8]  apply similar GAN-based semi-supervised framework on acoustic representations learning and it helps to improve emotion recognition. Besides generative models, Albanie et al.  [2]  explore to transfer emotion label from one modality to the other modality assuming that the supervised annotation does exist in one modality.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Distribution Matching",
      "text": "Distribution matching  [17, 24]  has been proposed and developed for cross-modal retrieval recently. Several methods  [6, 55]  are proposed to map distribution from different domains into a shared space so that the representations of similar distribution from different domains can be aligned. They use various similarity metrics such as Asymmetric Quantizer Distance (AQD)  [25]  and Maximum Mean Discrepancy (MMD)  [43] . Wang et al.  [48]  use distribution matching loss to incorporate the labeled and unlabeled data into one framework simultaneously and apply the semi-supervised crossmodal retrieval. Although all these previous works address the cross-modal search problem, we find that this concept can be seamlessly integrated with the semi-supervised emotion recognition. So we conduct the experiment and prove the efficiency of distribution matching across modality.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Method",
      "text": "We assume that a video database naturally consists of information from three modalities (acoustic, visual and lexical). Given a\n\n, where x a , x v , x l denote the feature representation from the acoustic, visual and lexical modalities respectively, L and uL are used to distinguish labeled and unlabeled data, and n L and n uL denote the size of the labeled and unlabeled dataset respectively, our goal is to involve the unlabeled data in training to improve model performance.\n\nPrevious study  [5]  has shown that the emotional status is kept unchanged during an utterance and the average duration of utterances in the dataset is 4.5 seconds. Rigoulot et al  [45]  has also studied the time course for human emotion expression and found that 4 seconds of speech emotions can usually be classified correctly. Based on such discovery, we make the following assumption: although the emotion expression is not necessarily aligned at the frame level across modalities, the overall emotional state should be similar at the coarse-grained utterance level. We can utilize this assumption to extract supervision from unlabeled data. During training, we improve the accuracy of classification on labeled data, and reduce the difference of inter modality distribution on both labeled and unlabeled data simultaneously.\n\nAs shown in Fig.  2 , two types of data (labeled and unlabeled data) both participate in the model learning (solid line for labeled data, dash line for unlabeled data). The additional unlabeled data can help learn more robust and emotion-salient latent representation. We use Maximum Mean Discrepancy (MMD)  [43]  to measure the distribution similarity, which is motivated by its previous success in transfer learning and feature representation learning.\n\nFormally, the training objective of our semi-supervised model (Eq. 1) consists of three components corresponding to the emotion classification, data reconstruction and data distribution matching respectively, among which only the emotion classification component requires labeled data while the other two components are unsupervised. We present the details of model architecture (Section 3.1) and loss function design (Section 3.2) in following subsections.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Multi-Modal Network Architecture",
      "text": "As the model structure is related to the feature characteristics of each modality, we first introduce the features and then present the network design.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Multi-Modal Features",
      "text": "We first extract raw features from acoustic, visual and lexical modalities respectively.\n\n• Acoustic: We utilize the toolkit OpenSMILE  [13]  to extract the utterance-level features with the configuration of INTERSPEECH2010  [42] . The extracted feature vector is in 1,582 dimension. • Visual: We utilize the state-of-the-art Dense Convolutional Neural Networks (DenseNet)  [22]  to extract the facial features. The DenseNet is pretrained on the FERPlus  [3]  dataset for facial expression recognition. We extract the 342 dimensional activation from the last pooling layer for each face image as in  [9] . • Lexical: We use the state-of-the-art word embedding trained by Bidirectional Encoder Representation from Transformers (BERT)  [10] . Each word is represented as a 1,024-dimensional vector.\n\nWe apply z-normalization on each dimension of the raw features to reduce data discrepancy.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Dae For Representation Learning",
      "text": "Deep Auto-encoder (DAE) is proposed to learn high quality latent representation by encoding and reconstructing its input data. It can capture the data manifolds smoothly without losing too much original information  [38] . Cross modal distribution matching is applied to avoid the latent representation collapsing into zero space.\n\nAcoustic: As the extracted acoustic features are at the utterance level, we consider the stacked linear layers as the encoder structure. We first transform the input acoustic feature x a to the latent representation z a with a set of linear layers and then get the reconstructed output xa with symmetric layers. The network structure is shown in Figure  3 . Please note that we do not apply frame-level distribution matching across modalities due to two reasons: Firstly, as mentioned above, previous research has shown that emotion expression is not necessarily aligned at the frame level or word level across modalities  [19, 32, 34, 46] , forcing the matching at the frame level will lead to inherently poor optimization of the model.\n\nSecondly, reconstruction at the frame level will result in very large amount of trainable parameters.\n\nVisual and Lexical: As both the raw visual and lexical features are a sequence of features, we consider seq2seq type of structure for the encoder and decoder. Transformer  [47]  is one type of the stateof-the-art Seq2Seq models, which is completely based on attention mechanism and does not need recurrence and convolution. It captures the relative dependencies between elements of the sequence. We therefore design the transformer architecture for DAE of the visual and lexical modalities. The detailed component structure of visual and lexical DAE is shown in Figure  4 .\n\n(1) Firstly, the input and output of the visual/lexical DAE model are the raw and reconstructed visual/lexical features, we therefore drop the embedding layer of the input and set the number of hidden units as 1 in the last linear output layer. (2) Secondly, following Srivastava et al.  [45] , we take the reversed input sequence as the reconstruction target instead of the original sequence. Reversing the reconstruction target makes the optimization easier because the model can get off the ground by looking at low range correlations. (3) Lastly, the latent representations for different modalities are expected to follow the same shape. But the encoder output in the visual/lexical DAE is still stacked by time order. So in the middle of transformer, we set up a set of convolutional layers for down sampling and, symmetrically, a set of deconvolutional layers for reconstruction.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Loss Function Design",
      "text": "The training objective of our semi-supervised model (Eq. 1) consists of three components corresponding to the emotion classification, data reconstruction and data distribution matching respectively.\n\nReconstruction loss. The loss function of reconstruction is Mean Squared Error (MSE) as follows:\n\nwhere x is the input to DAE and rec(x) is the reconstructed output from the corresponding DAE.\n\nUnsupervised Distribution Matching Loss. Given a numbers of video samples segmented by utterance, we assume that the latent representation of acoustic, visual and lexical modalities from the same video can be mapped into a similar space, while the distribution of modalities from videos with different emotion status should be diverse. We employ Maximum Mean Discrepancy (MMD)  [43]  to measure the distribution similarity. The distribution matching loss is as follows:\n\nwhere p, q are the latent representation from two different modalities. The latent representation of each modality is mapped to Reproducing Kernel Hilbert Space (RKHS) before computing the distance.\n\nWe use the Gaussian kernel (Eq. 4) to calculate the dot product in RKHS. This formula dose not contain any annotation information, so it can be forced on both labeled and unlabeled data to build up unsupervised training target.\n\nSupervised Emotion Classification Loss. As we assume the emotion status is aligned across modalities at the utterance level, we can apply multi-modal fusion through directly concatenating the latent representation and then feed it into the classifier. For the labeled data, we compute the cross-entropy loss (Eq. 5) for optimization.\n\nwhere C is a neural network based emotion classifier, K is the number of emotion classes, n L is the total number of supervised samples, y i and p i are the annotated and predicted emotion class probability for input data x i and [z a i ; z v i ; z l i ] is the concatenation of the latent representation of x i from all the modalities. Joint Loss Function. We combine all the losses into the joint loss function as below. For the supervised part, the loss function is set as:\n\nHowever, we find that the latent representation still collapses into zero space. To avoid meaningless matching of modality representation, we add unpaired samples into the training. The 'unpaired' means the feature extracted from different modalities doesn't belong to the same video or are not aligned. The gap between latent distribution of paired and unpaired should be obviously large. We shuffle the features across the acoustic, visual and lexical modalities so that they are not aligned anymore. We thus can form unpaired samples in this way. We hope the unpaired samples are mapped into different emotion space which means we enlarge the distribution distance during training. Based on such idea, the loss function is modified as:\n\nWe then add unsupervised data in the training and the loss function for the unsupervised part is set as:\n\nwhere α, β are hyper-parameters. Finally, we form the semi-supervised loss function by combining the supervised and unsupervised losses:\n\nwhere ω is the hyper-parameter to balance the two losses.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Experiment",
      "text": "In this section, we present a series of comparison experiments on discrete emotion recognition task under fully-supervised and semi-supervised settings.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Data Description",
      "text": "We utilize both labeled and unlabeled data for our experiments.\n\nIEMOCAP  [5]  contains 12 hours of video recordings of situational dialogues. The videos are divided into five sessions. Each session contains only two actors so that in total there are ten actors in the database. The recorded dialogues are manually segmented into 10039 utterances with 9 discrete emotion classes, namely happiness, anger, sadness, fear, surprise, excitement, frustration, neutral and others. To compare with the state-of-the-art approaches, we follow the data split setting as in  [27]  and use 5531 utterances from the top 4 emotion classes: happiness, anger, sadness and neutral (the 'excitement' utterances are merged into the 'happiness' class). The data distribution is shown at Table  1  We follow the speakerindependent setting to avoid actor overlap in the validation and testing set. Under this consideration, four sessions are chosen as MELD  [35]  is a multi-modal conversational dataset. It extracts more than 1300 dialogues and 13000 utterances from Friends TV series with total 304 speakers. Each utterance segment contains audio track, visual scene and text transcript. And it is labeled with one of seven discrete emotions which are joy, anger, sadness, surprise, disgust, fear and neutral. The data distribution is shown at Table  2 . The video contains multiple faces in a scene and the speaker label is not provided in the dataset. Thus, in this work, we can not match the speaker with his/her face exactly and we only use acoustic and lexical modalities in related experiments. AMI  [7]  dataset consists of about 100 hours of unlabeled meeting recordings. It provides video recordings of each speaker, voice track and transcripts of their speech. But there is no emotion annotation in the dataset, so we use it as the unsupervised dataset.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Implementation Details",
      "text": "Unlabeled Sample Sampling: We summarize several basic rules to select unsupervised data and perform semi-supervised learning: firstly, the spoken language, cultural background and age range of speakers should be similar. Secondly, the camera setting should be as consistent as possible (e.g. lighting, shooting position, resolution, etc.). To avoid the case that the sampled region from AMI is silence, we randomly select three continuous words in the transcript and look up the time region in the video. We then extract the audio and video segment with middle word as the center of the segment. Due to various video duration and utterance length in AMI and IEMOCAP datasets, we pre-define the video duration of unlabeled sample in advance. Because the duration of 80% utterances in the IEMOCAP dataset is limited to 7.2s, we therefore extract unlabeled sample with 7.2s as the crop width for the experiments on the IEMOCAP dataset. Similarly, for experiments on the MELD dataset whose utterances are shorter, the crop width is set as 3.5s. This step ensures that there is no significant difference in the duration between labeled samples and unlabeled samples. Additionally, people tend to keep neutral emotion status during meetings and less likely to express sad or anger emotions. We apply a pre-trained vanilla emotion classifier with IEMOCAP dataset on the AMI dataset and observe that about 80% of the 20,000 unlabeled samples are classified as happiness and neutral. We therefore apply sub-sampling Table  3 : Model architecture setting. In convolutional and deconvolutional layers, we denote kernel size as k, stride length as s, padding length as p and the number of channels as c. In Transformer, we denote the number of self-attention heads as h, the number of transformer blocks as b and the size of hidden embedding as e. on these two emotion types which selects 5000 samples for the happiness and neutral classes respectively. The number of samples for sadness and anger classes is less than 5000, we therefore apply over-sampling on these two classes and collect 5000 samples for each class respectively. After the filtering process, the unlabeled dataset is more balanced.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Modality",
      "text": "Face Extraction: We apply face detection and extraction on all the datasets with the toolkit Seetaface  [49] . Each face image is transformed into the gray scale with size of 64x64. These videos contain 30 frames per second and there is almost no change between adjacent frames. To reduce computation cost without losing too much information, we set the sampling rate to 1/10, which means we can get about 3 frames per second. For those frames where faces cannot be detected, we use the detection results of the previous frame. However, for the AMI dataset, there are about 11% videos where faces can not be detected. We simply drop these samples. Finally, we get 20,000 unlabeled samples from AMI in total. Hyper-parameters: For the IEMOCAP dataset, the number of face images and words in one utterance is fixed as 18 and 22. For the MELD dataset, the number of words in a single utterance is fixed as 12. We pad zeros when the utterance is not long enough and cut off if it is too long. We set the batch size as 128, the weight of reconstruction loss α as 0.2, the weight of MMD loss β as 0.1 and the weight of unsupervised part ω as 0.3. We apply Adam algorithm with learning rate of 1e-3 to optimize the parameters. The detailed setting of network is presented in Table  3 . The structure of the encoder and decoder is completely symmetric. We select the best model based on 5-fold cross validation on the validation set and report its performance on the testing set.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Comparison Experiments And Results",
      "text": "We first compare the proposed semi-supervised framework with several recent state-of-the-art approaches on the IEMOCAP dataset.\n\n1): Yoon et al.  [51]  propose a deep dual recurrent encoder to combine the text information and audio signals. They first investigate the performance of uni-modal recurrent encoder on audio and text (ARE, TRE). Then they propose multi-modal dual recurrent encoder with and without attention techique (MDRE+Att, MDRE). All the results are reported under speaker-independent settings.\n\n2): Xu et al.  [50]  propose to learn the frame-level alignment between audio and text via the attention mechanism in order to produce more accurate multi-modal feature representations. They conduct uni-modal experiments on acoustic and lexical data using LSTM with attention (LSTM+Att). For multi-modal settings, they compare the performance of direct concatenation (Concat) and applying alignment (Alignment) via attention computing. They report the results under speaker-independent settings as well.\n\nFor fair comparison with above baselines, we also conduct speakerindependent experiments and report the performance with weighted average precision (WAP) and unweighted accuracy (UA)  [26] .\n\nTable  4  presents the experiment results under the fully supervised speaker-independent setting. We can see from the results that our method outperforms recurrent encoder and LSTM with attention under the uni-modal scenario, which indicates that the feature selection and DAE architecture can capture emotional characteristics well and the long-term dependency in transcript can    5 . We compare the performance between fully-supervised setting and semisupervised setting to verify the feasibility of our assumption of modality distribution matching. Since the proposed semi-supervised approach needs at least two modalities, we didn't implement semisupervised experiments for uni-modal settings. Our semi-supervised training strategy boosts the model capability in all modality combination, which outperforms all the baseline approaches. It demonstrates the rationality and effectiveness of the proposed assumption and model, which can take advantages of unsupervised data and extract more emotion-salient latent representation.\n\nWe then present the experiment results on the MELD dataset. Because MELD dataset is the new emotion dataset collected in the interaction scenario, majority of existing approaches validating on it consider auxiliary information from interaction, such as interaction context or speaker information. However, in this work, our focus is on how to effectively utilize unlabeled data, our proposed approach doesn't consider the context or speaker information. So in this set of experiments, we not only compare to approaches without using interaction related information, but also to those interactive approaches. We use following state-of-the-art baselines for comparison.\n\n(1) Zadeh et al. propose Memory Fusion Network (MFN)  [52]  which focuses on improving multi-modal fusion effectiveness. This method does not use any context or speaker information. Since the number of each emotion category is unbalanced in the MELD dataset, following Zhang et al.  [53] , we report performance with weighted average F1 score  [40] .\n\nAs shown in Table  6 , our semi-supervised model significantly outperforms MFN which also does not use any conversation context and speaker information. Our model also achieves better performance than CMN, ICON and BC-LSTM, which uses additional auxiliary information, either conversation context information or speaker information. It demonstrates the advantage of our model in isolated emotion recognition scenario. Furthermore, our model achieves comparable performance with DialogueRNN and ConGCN, the two state-of-the-art interactive emotion recognition approaches proposed recently. Although ConGCN outperforms our model by 2.3% when it makes full usage of both speaker and contextual information, the overall experiment results show that even though our model does not utilize any auxiliary interaction information, it is also very competitive in conversational scenarios.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Analysis And Discussion",
      "text": "Unlabeled data quantity analysis. To gain more insights about the impact of unlabeled data, we analyze the classification performance change with different quantity of unlabeled training data. We conduct the semi-supervised experiment with combining all the three modalities. To show the impact of unlabeled data, we keep the hyper-parameter unchanged except the number of unlabeled data. We train a fully-supervised model at first and then add 5000 unlabeled samples step by step till all the 20,000 unlabeled samples from the AMI corpus are used. As shown in Fig.  7 (a), the performance of semi-supervised model gradually improves with the increase of unlabeled samples, which indicates that the additional samples benefit the generalization and robustness of the recognition model. Confusion matrix analysis. As shown in Fig.  6 , neutral samples are more likely to be identified as emotional categories. According to the confusion matrices, multi-modal combination boosts the performance on all classes and semi-supervised learning mainly improves performance on the neutral class.\n\nTraining procedure analysis. We present the loss curves in Fig.  7(b) . We can see that the classification loss, our main optimization target, decreases rapidly and converges within 15 epochs. Due to the limited scale of training data, most of the best models on the validation set appear between 10th and 15th epoch in our experiments. Too many training iterations will lead to over fitting on the training set. The curve of reconstruction loss changes smoothly and converges stably. The change of distribution loss also meets our expectation that the paired one and the unpaired one change towards the opposite direction and then converge to a similar value.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Ablation Study",
      "text": "To explore the contribution of each loss component, we fix the supervised emotion classification loss and do an ablation study on reconstruction loss and unsupervised distribution matching loss. We select the speaker-independent experiment of the acoustic, visual and lexical modalities on the IEMOCAP dataset as example. Table  7  presents the results. The experiments are divided into fullysupervised part and semi-supervised part. In the fully-supervised experiments, the model only employing distribution matching loss  achieves worst performance. During the model structure design, we have a concern that distribution matching loss may harm the training with zero collapsing. Since the model aims to narrow the gap between two distributions, one trick is to drop most information and map all the distributions into a simple space (e.g. zero space). This experiment verifies our concern so that we need to avoid this misguidance. After we add the reconstruction target, the model improves and outperforms the vanilla model. When both reconstruction and distribution matching are employed, it further boosts the recognition performance. For the semi-supervised model, the distribution matching component is essential, so we only remove the reconstruction component. Similar to the fully-supervised experiment, the single distribution matching target misleads the training process and gets worse performance compared with the vanilla model. According to this study, distribution matching needs to be constrained to provide proper supervision for training.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Conclusion",
      "text": "In this work, under the assumption that the emotion status is consistent across different modalities at the coarse utterance level, we propose a novel semi-supervised learning method based on crossmodal distribution matching for multi-modal emotion recognition. We jointly optimize the emotion classification, utterance-level crossmodal distribution matching and feature reconstruction objectives. Extensive experiments on IEMOCAP and MELD datasets prove the effectiveness of our proposed semi-supervised model and demonstrate that unlabeled data and multi-modality fusion both benefit the classification performance. Our model without contextual information outperforms existing state-of-the-art models in non-interactive scenario and is competitive with interactive methods.",
      "page_start": 8,
      "page_end": 8
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The latent representation of acoustic, visual and",
      "page": 1
    },
    {
      "caption": "Figure 1: Under this assumption, we",
      "page": 2
    },
    {
      "caption": "Figure 2: The overall structure of the proposed semi-supervised multi-modal emotion recognition framework. Both labeled",
      "page": 3
    },
    {
      "caption": "Figure 2: , two types of data (labeled and unlabeled data)",
      "page": 3
    },
    {
      "caption": "Figure 3: Please note that we do not apply frame-level",
      "page": 3
    },
    {
      "caption": "Figure 3: Acoustic DAE: symmetric stacked linear layers.",
      "page": 4
    },
    {
      "caption": "Figure 4: (1) Firstly, the input and output of the visual/lexical DAE model are",
      "page": 4
    },
    {
      "caption": "Figure 4: Visual and Lexical DAE: modified Transformer",
      "page": 4
    },
    {
      "caption": "Figure 5: An example frame from videos in IEMOCAP",
      "page": 7
    },
    {
      "caption": "Figure 5: The low quality of face images limits visual model performance,",
      "page": 7
    },
    {
      "caption": "Figure 6: Confusion matrix of experiments on IEMOCAP",
      "page": 8
    },
    {
      "caption": "Figure 7: (a), the performance",
      "page": 8
    },
    {
      "caption": "Figure 6: , neutral sam-",
      "page": 8
    },
    {
      "caption": "Figure 7: (b). We can see that the classification loss, our main optimiza-",
      "page": 8
    },
    {
      "caption": "Figure 7: (a) Influence of unlabeled data quantity. (b) The",
      "page": 8
    }
  ],
  "tables": [
    {
      "caption": "Table 3: Model architecture setting. In convolutional and Table4:Fully-supervisedexperimentresultsunderspeaker-",
      "data": [
        {
          "Modality": "A",
          "Input": "1 × 1582",
          "Encoder": "Linear:1582 → 512 → 256 → 128"
        },
        {
          "Modality": "V",
          "Input": "18 × 342",
          "Encoder": "Transformer Encoder h=4,b=2,e=342"
        },
        {
          "Modality": "",
          "Input": "",
          "Encoder": "Convolutional layers\nk=4,s=2,p=1,c=16\nk=5,s=2,p=1,c=64\nk=3,s=3,p=1,c=32"
        },
        {
          "Modality": "",
          "Input": "",
          "Encoder": "Flatten layer\nLinear:1856 → 128"
        },
        {
          "Modality": "L",
          "Input": "22 × 1024",
          "Encoder": "Transformer Encoder h=4,b=2,e=1024"
        },
        {
          "Modality": "",
          "Input": "",
          "Encoder": "convolutional layers\nk=4,s=2,p=1,c=64\nk=4,s=3,p=1,c=4"
        },
        {
          "Modality": "",
          "Input": "",
          "Encoder": "Flatten layer\nLinear:2736 → 512 → 128"
        },
        {
          "Modality": "",
          "Input": "Input",
          "Encoder": "Decoder"
        },
        {
          "Modality": "A",
          "Input": "1 × 128",
          "Encoder": "Linear:128 → 256 → 512 → 1582"
        },
        {
          "Modality": "V",
          "Input": "1 × 128",
          "Encoder": "Linear:128 → 1856\nReshape to 32 × 2 × 29"
        },
        {
          "Modality": "",
          "Input": "",
          "Encoder": "Deconvolutional layers\nk=3,s=3,p=1,c=64\nk=5,s=2,p=1,c=16\nk=4,s=2,p=1,c=1"
        },
        {
          "Modality": "",
          "Input": "",
          "Encoder": "Transformer Decoder h=4,b=2,e=342"
        },
        {
          "Modality": "L",
          "Input": "1 × 128",
          "Encoder": "Linear:128 → 512 → 2736\nReshape to 4 × 4 × 171"
        },
        {
          "Modality": "",
          "Input": "",
          "Encoder": "Deconvolutional layers\nk=4,s=3,p=1,c=64\nk=4,s=2,p=1,c=1"
        },
        {
          "Modality": "",
          "Input": "",
          "Encoder": "Transformer Decoder h=4,b=2,e=1024"
        }
      ],
      "page": 6
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Emotion Recognition in Speech using Cross-Modal Transfer in the Wild",
      "authors": [
        "Arsha Samuel Albanie",
        "Andrea Nagrani",
        "Andrew Vedaldi",
        "Zisserman"
      ],
      "year": "2018",
      "venue": "2018 ACM Multimedia Conference on Multimedia Conference, MM 2018",
      "doi": "10.1145/3240508.3240578"
    },
    {
      "citation_id": "2",
      "title": "Emotion recognition in speech using cross-modal transfer in the wild",
      "authors": [
        "Arsha Samuel Albanie",
        "Andrea Nagrani",
        "Andrew Vedaldi",
        "Zisserman"
      ],
      "year": "2018",
      "venue": "Proceedings of the 26th ACM international conference on Multimedia"
    },
    {
      "citation_id": "3",
      "title": "Training Deep Networks for Facial Expression Recognition with Crowd-Sourced Label Distribution",
      "authors": [
        "Emad Barsoum",
        "Cha Zhang",
        "Cristian Canton Ferrer",
        "Zhengyou Zhang"
      ],
      "year": "2016",
      "venue": "ACM International Conference on Multimodal Interaction (ICMI)"
    },
    {
      "citation_id": "4",
      "title": "Multi-Modal Audio, Video and Physiological Sensor Learning for Continuous Emotion Prediction",
      "authors": [
        "Kevin Brady",
        "Youngjune Gwon",
        "Pooya Khorrami",
        "Elizabeth Godoy",
        "William Campbell",
        "Charlie Dagli",
        "Thomas Huang"
      ],
      "year": "2016",
      "venue": "Multi-Modal Audio, Video and Physiological Sensor Learning for Continuous Emotion Prediction"
    },
    {
      "citation_id": "5",
      "title": "IEMOCAP: interactive emotional dyadic motion capture database",
      "authors": [
        "Carlos Busso",
        "Murtaza Bulut",
        "Chi-Chun Lee",
        "Abe Kazemzadeh",
        "Emily Mower",
        "Samuel Kim",
        "Jeannette Chang",
        "Sungbok Lee",
        "Shrikanth Narayanan"
      ],
      "year": "2008",
      "venue": "Language Resources and Evaluation"
    },
    {
      "citation_id": "6",
      "title": "Collective deep quantization for efficient cross-modal retrieval",
      "authors": [
        "Yue Cao",
        "Mingsheng Long",
        "Jianmin Wang",
        "Shichen Liu"
      ],
      "year": "2017",
      "venue": "Thirty-First AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "7",
      "title": "The AMI meeting corpus: a pre-announcement",
      "authors": [
        "Jean Carletta",
        "Simone Ashby",
        "Sebastien Bourban",
        "Mike Flynn",
        "Mael Guillemot",
        "Thomas Hain",
        "Jaroslav Kadlec",
        "Vasilis Karaiskos",
        "Wessel Kraaij",
        "Melissa Kronenthal"
      ],
      "year": "2006",
      "venue": "International Workshop on Machine Learning for Multimodal Interaction"
    },
    {
      "citation_id": "8",
      "title": "Learning representations of emotional speech with deep convolutional generative adversarial networks",
      "authors": [
        "Jonathan Chang",
        "Stefan Scherer"
      ],
      "year": "2017",
      "venue": "IEEE International Conference on Acoustics"
    },
    {
      "citation_id": "9",
      "title": "Multimodal Multitask Learning for Dimensional and Continuous Emotion Recognition",
      "authors": [
        "Shizhe Chen",
        "Qin Jin",
        "Jinming Zhao",
        "Shuai Wang"
      ],
      "year": "2017",
      "venue": "The Workshop on Audio/visual Emotion Challenge"
    },
    {
      "citation_id": "10",
      "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "Jacob Devlin",
        "Ming-Wei Chang",
        "Kenton Lee",
        "Kristina Toutanova"
      ],
      "year": "2018",
      "venue": "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "arxiv": "arXiv:1810.04805"
    },
    {
      "citation_id": "11",
      "title": "Semi-supervised deep generative modelling of incomplete multi-modality emotional data",
      "authors": [
        "Changde Du",
        "Changying Du",
        "Hao Wang",
        "Jinpeng Li",
        "Wei-Long Zheng",
        "Bao-Liang Lu",
        "Huiguang He"
      ],
      "year": "2018",
      "venue": "Proceedings of the 26th ACM international conference on Multimedia"
    },
    {
      "citation_id": "12",
      "title": "An argument for basic emotions",
      "authors": [
        "Paul Ekman"
      ],
      "year": "1992",
      "venue": "Cognition & emotion"
    },
    {
      "citation_id": "13",
      "title": "Opensmile:the munich versatile and fast open-source audio feature extractor",
      "authors": [
        "Florian Eyben"
      ],
      "year": "2010",
      "venue": "ACM International Conference on Multimedia"
    },
    {
      "citation_id": "14",
      "title": "AVEC 2018 Workshop and Challenge: Bipolar Disorder and Cross-Cultural Affect Recognition",
      "authors": [
        "Fabien Ringeval",
        "Björn Schuller",
        "Michel Valstar",
        "Roddy Cowie",
        "Heysem Kaya",
        "Maximilian Schmitt",
        "Shahin Amiriparian",
        "Nicholas Cummins",
        "Denis Lalanne",
        "Adrien Michaud",
        "Elvan Çiftçi",
        "Hüseyin Güleç",
        "Albert Ali",
        "Maja Pantic"
      ],
      "year": "2018",
      "venue": "Proceedings of the 8th International Workshop on Audio/Visual Emotion Challenge, AVEC'18, co-located with the 26th ACM International Conference on Multimedia, MM 2018"
    },
    {
      "citation_id": "15",
      "title": "Emotion recognition in human-computer interaction",
      "authors": [
        "J Fragopanagos",
        "Taylor"
      ],
      "year": "2002",
      "venue": "IEEE Signal Processing Magazine"
    },
    {
      "citation_id": "16",
      "title": "Tools, language and cognition in human evolution",
      "authors": [
        "Kathleen Kathleen R Gibson",
        "Tim Gibson",
        "Ingold"
      ],
      "year": "1993",
      "venue": "Tools, language and cognition in human evolution"
    },
    {
      "citation_id": "17",
      "title": "Iterative quantization: A procrustean approach to learning binary codes for large-scale image retrieval",
      "authors": [
        "Yunchao Gong",
        "Svetlana Lazebnik",
        "Albert Gordo",
        "Florent Perronnin"
      ],
      "year": "2012",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "18",
      "title": "Generative adversarial nets",
      "authors": [
        "Ian Goodfellow",
        "Jean Pouget-Abadie",
        "Mehdi Mirza",
        "Bing Xu",
        "David Warde-Farley",
        "Sherjil Ozair",
        "Aaron Courville",
        "Yoshua Bengio"
      ],
      "year": "2014",
      "venue": "International Conference on Neural Information Processing Systems"
    },
    {
      "citation_id": "19",
      "title": "Multimodal Affective Analysis Using Hierarchical Attention Strategy with Word-Level Alignment",
      "authors": [
        "Yue Gu",
        "Kangning Yang",
        "Shiyu Fu",
        "Shuhong Chen",
        "Xinyu Li",
        "Ivan Marsic"
      ],
      "year": "2018",
      "venue": "Proceedings of the conference"
    },
    {
      "citation_id": "20",
      "title": "ICON: interactive conversational memory network for multimodal emotion detection",
      "authors": [
        "Devamanyu Hazarika",
        "Soujanya Poria",
        "Rada Mihalcea",
        "Erik Cambria",
        "Roger Zimmermann"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "21",
      "title": "Conversational memory network for emotion recognition in dyadic dialogue videos",
      "authors": [
        "Devamanyu Hazarika",
        "Soujanya Poria",
        "Amir Zadeh",
        "Erik Cambria",
        "Louis-Philippe Morency",
        "Roger Zimmermann"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 Conference of the North American Chapter"
    },
    {
      "citation_id": "22",
      "title": "Densely Connected Convolutional Networks",
      "authors": [
        "Gao Huang",
        "Zhuang Liu",
        "Laurens Van Der Maaten",
        "Kilian Weinberger"
      ],
      "year": "2017",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "23",
      "title": "An Investigation of Annotation Delay Compensation and Output-Associative Fusion for Multimodal Continuous Emotion Prediction",
      "authors": [
        "Zhaocheng Huang",
        "Ting Dang",
        "Nicholas Cummins",
        "Brian Stasak",
        "Julien Epps"
      ],
      "year": "2015",
      "venue": "International Workshop on Audio/visual Emotion Challenge"
    },
    {
      "citation_id": "24",
      "title": "Product quantization for nearest neighbor search",
      "authors": [
        "Herve Jegou",
        "Matthijs Douze",
        "Cordelia Schmid"
      ],
      "year": "2010",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "25",
      "title": "Product quantization for nearest neighbor search",
      "authors": [
        "Herve Jegou",
        "Matthijs Douze",
        "Cordelia Schmid"
      ],
      "year": "2010",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "26",
      "title": "Machine literature searching VIII. Operational criteria for designing information retrieval systems",
      "authors": [
        "Allen Kent",
        "Madeline Berry",
        "Fred Luehrs",
        "James Perry"
      ],
      "year": "1955",
      "venue": "American documentation"
    },
    {
      "citation_id": "27",
      "title": "Towards discriminative representation learning for speech emotion recognition",
      "authors": [
        "Runnan Li",
        "Zhiyong Wu",
        "Jia Jia",
        "Yaohua Bu",
        "Sheng Zhao",
        "Helen Meng"
      ],
      "year": "2019",
      "venue": "Proceedings of the 28th International Joint Conference on Artificial Intelligence (IJCAI)"
    },
    {
      "citation_id": "28",
      "title": "Dialoguernn: An attentive rnn for emotion detection in conversations",
      "authors": [
        "Navonil Majumder",
        "Soujanya Poria",
        "Devamanyu Hazarika",
        "Rada Mihalcea",
        "Alexander Gelbukh",
        "Erik Cambria"
      ],
      "year": "2019",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "29",
      "title": "The Stanford CoreNLP natural language processing toolkit",
      "authors": [
        "Mihai Christopher D Manning",
        "John Surdeanu",
        "Jenny Bauer",
        "Steven Finkel",
        "David Bethard",
        "Mcclosky"
      ],
      "year": "2014",
      "venue": "Proceedings of 52nd annual meeting of the association for computational linguistics: system demonstrations"
    },
    {
      "citation_id": "30",
      "title": "Deep Learning of Segment-Level Feature Representation with Multiple Instance Learning for Utterance-Level Speech Emotion Recognition",
      "authors": [
        "Shuiyang Mao",
        "Tan Ching",
        "Lee"
      ],
      "year": "2019",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "31",
      "title": "Discovering emotion in classroom motivation research",
      "authors": [
        "K Debra",
        "Julianne Meyer",
        "Turner"
      ],
      "year": "2002",
      "venue": "Educational psychologist"
    },
    {
      "citation_id": "32",
      "title": "Found in translation: Learning robust joint representations by cyclic translations between modalities",
      "authors": [
        "Hai Pham",
        "Paul Liang",
        "Thomas Manzini",
        "Louis-Philippe Morency",
        "Barnabás Póczos"
      ],
      "year": "2019",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "33",
      "title": "Context-dependent sentiment analysis in user-generated videos",
      "authors": [
        "Soujanya Poria",
        "Erik Cambria",
        "Devamanyu Hazarika",
        "Navonil Majumder",
        "Amir Zadeh",
        "Louis-Philippe Morency"
      ],
      "year": "2017",
      "venue": "Proceedings of the 55th annual meeting of the association for computational linguistics"
    },
    {
      "citation_id": "34",
      "title": "Context-dependent sentiment analysis in user-generated videos",
      "authors": [
        "Soujanya Poria",
        "Erik Cambria",
        "Devamanyu Hazarika",
        "Navonil Majumder",
        "Amir Zadeh",
        "Louis-Philippe Morency"
      ],
      "year": "2017",
      "venue": "Proceedings of the 55th annual meeting of the association for computational linguistics"
    },
    {
      "citation_id": "35",
      "title": "MELD: A multimodal multi-party dataset for emotion recognition in conversations",
      "authors": [
        "Soujanya Poria",
        "Devamanyu Hazarika",
        "Navonil Majumder"
      ],
      "year": "2018",
      "venue": "MELD: A multimodal multi-party dataset for emotion recognition in conversations",
      "arxiv": "arXiv:1810.02508"
    },
    {
      "citation_id": "36",
      "title": "Emotion recognition using acoustic and lexical feature",
      "authors": [
        "Sankaranarayanan Viktor Rozgić",
        "Shiri Ananthakrishnan",
        "Rohi Saleem",
        "Kumar"
      ],
      "year": "2012",
      "venue": "INTERSPEECH 2012"
    },
    {
      "citation_id": "37",
      "title": "Evidence for a three-factor theory of emotions",
      "authors": [
        "A James",
        "Albert Russell",
        "Mehrabian"
      ],
      "year": "1977",
      "venue": "Journal of research in Personality"
    },
    {
      "citation_id": "38",
      "title": "Semantic hashing",
      "authors": [
        "Ruslan Salakhutdinov",
        "Geoffrey Hinton"
      ],
      "year": "2009",
      "venue": "International Journal of Approximate Reasoning"
    },
    {
      "citation_id": "39",
      "title": "Improved Techniques for Training GANs",
      "authors": [
        "Tim Salimans",
        "Ian Goodfellow",
        "Wojciech Zaremba",
        "Vicki Cheung",
        "Alec Radford",
        "Xi Chen"
      ],
      "year": "2016",
      "venue": "Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems"
    },
    {
      "citation_id": "40",
      "title": "The truth of the F-measure",
      "authors": [
        "Yutaka Sasaki"
      ],
      "year": "2007",
      "venue": "Teach Tutor mater"
    },
    {
      "citation_id": "41",
      "title": "Investigating fuzzy-input fuzzy-output support vector machines for robust voice quality classification",
      "authors": [
        "Stefan Scherer",
        "John Kane",
        "Christer Gobl",
        "Friedhelm Schwenker"
      ],
      "year": "2013",
      "venue": "Computer Speech & Language"
    },
    {
      "citation_id": "42",
      "title": "Recognising realistic emotions and affect in speech: State of the art and lessons learnt from the first challenge",
      "authors": [
        "Björn Schuller",
        "Anton Batliner",
        "Stefan Steidl",
        "Dino Seppi"
      ],
      "year": "2011",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "43",
      "title": "Equivalence of distance-based and RKHS-based statistics in hypothesis testing",
      "authors": [
        "Dino Sejdinovic",
        "Bharath Sriperumbudur",
        "Arthur Gretton",
        "Kenji Fukumizu"
      ],
      "year": "2013",
      "venue": "The Annals of Statistics"
    },
    {
      "citation_id": "44",
      "title": "A combined rulebased & machine learning audio-visual emotion recognition approach",
      "authors": [
        "Kah Phooi",
        "Li-Minn Ang",
        "Chien Shing Ooi"
      ],
      "year": "2016",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "45",
      "title": "Unsupervised learning of video representations using lstms",
      "authors": [
        "Nitish Srivastava",
        "Elman Mansimov",
        "Ruslan Salakhudinov"
      ],
      "year": "2015",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "46",
      "title": "Multimodal transformer for unaligned multimodal language sequences",
      "authors": [
        "Yao-Hung Hubert Tsai",
        "Shaojie Bai",
        "Paul Liang",
        "J Zico Kolter",
        "Louis-Philippe Morency",
        "Ruslan Salakhutdinov"
      ],
      "year": "2019",
      "venue": "Multimodal transformer for unaligned multimodal language sequences",
      "arxiv": "arXiv:1906.00295"
    },
    {
      "citation_id": "47",
      "title": "Attention is all you need",
      "authors": [
        "Ashish Vaswani",
        "Noam Shazeer",
        "Niki Parmar",
        "Jakob Uszkoreit",
        "Llion Jones",
        "Aidan Gomez",
        "Łukasz Kaiser",
        "Illia Polosukhin"
      ],
      "year": "2017",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "48",
      "title": "Semi-supervised Deep Quantization for Cross-modal Search",
      "authors": [
        "Xin Wang",
        "Wenwu Zhu",
        "Chenghao Liu"
      ],
      "year": "2019",
      "venue": "Proceedings of the 27th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "49",
      "title": "Funnel-Structured Cascade for Multi-View Face Detection with Alignment-Awareness",
      "authors": [
        "Shuzhe Wu",
        "Meina Kan",
        "Zhenliang He",
        "Shiguang Shan",
        "Xilin Chen"
      ],
      "year": "2017",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "50",
      "title": "Learning alignment for multimodal emotion recognition from speech",
      "authors": [
        "Haiyang Xu",
        "Hui Zhang",
        "Kun Han",
        "Yun Wang",
        "Yiping Peng",
        "Xiangang Li"
      ],
      "year": "2019",
      "venue": "Learning alignment for multimodal emotion recognition from speech",
      "arxiv": "arXiv:1909.05645"
    },
    {
      "citation_id": "51",
      "title": "Multimodal speech emotion recognition using audio and text",
      "authors": [
        "Seunghyun Yoon",
        "Seokhyun Byun",
        "Kyomin Jung"
      ],
      "year": "2018",
      "venue": "IEEE Spoken Language Technology Workshop"
    },
    {
      "citation_id": "52",
      "title": "Memory fusion network for multi-view sequential learning",
      "authors": [
        "Amir Zadeh",
        "Paul Liang",
        "Navonil Mazumder",
        "Soujanya Poria",
        "Erik Cambria",
        "Louis-Philippe Morency"
      ],
      "year": "2018",
      "venue": "Thirty-Second AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "53",
      "title": "Modeling both context-and speaker-sensitive dependence for emotion detection in multi-speaker conversations",
      "authors": [
        "Dong Zhang",
        "Liangqing Wu",
        "Changlong Sun",
        "Shoushan Li",
        "Qiaoming Zhu",
        "Guodong Zhou"
      ],
      "year": "2019",
      "venue": "See Proceedings of the twenty-eighth international joint conference on artificial intelligence, IJCAI"
    },
    {
      "citation_id": "54",
      "title": "The role of children's emotions during design-based learning activity: a case study at a Dutch high school",
      "authors": [
        "Feiran Zhang",
        "Panos Markopoulos",
        "Tilde Bekker"
      ],
      "year": "2018",
      "venue": "10th International Conference on Computer Supported Education"
    },
    {
      "citation_id": "55",
      "title": "Collaborative quantization for crossmodal similarity search",
      "authors": [
        "Ting Zhang",
        "Jingdong Wang"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    }
  ]
}