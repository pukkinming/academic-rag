{
  "paper_id": "2503.06463v1",
  "title": "Axai-Cdss : An Affective Explainable Ai-Driven Clinical Decision Support System For Cannabis Use",
  "published": "2025-03-09T05:40:44Z",
  "authors": [
    "Tongze Zhang",
    "Tammy Chung",
    "Anind Dey",
    "Sang Won Bae"
  ],
  "keywords": [
    "Explainable Artificial Intelligence (XAI)",
    "Passive Sensing",
    "Affective Computing",
    "Clinical Decision Support Systems (CDSS)",
    "Cannabis Use Disorder",
    "Cannabis Intoxication",
    "Cannabis-Intoxicated Behaviors",
    "Personalized Intervention",
    "Large Language Models (LLMs)",
    "Algorithmic Decisions",
    "Transparency",
    "Healthcare AI",
    "Trustworthy AI",
    "Facial Emotion Recognition",
    "Causal Inference"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "As cannabis use has increased in recent years, researchers have come to rely on sophisticated machine learning models to predict cannabis use behavior and its impact on health. However, many artificial intelligence (AI) models lack transparency and interpretability due to their opaque nature, limiting their trust and adoption in real-world medical applications, such as clinical decision support systems (CDSS). To address this issue, this paper enhances algorithm explainability underlying CDSS by integrating multiple Explainable Artificial Intelligence (XAI) methods and applying causal inference techniques to clarify the models' predictive decisions under various scenarios. By providing deeper interpretability of the XAI outputs using Large Language Models (LLMs), we provide users with more personalized and accessible insights to overcome the challenges posed by AI's \"black box\" nature. Our system dynamically adjusts feedback based on user queries and emotional states, combining text-based sentiment analysis with real-time facial emotion recognition to ensure responses are empathetic, contextadaptive, and user-centered. This approach bridges the gap between the learning demands of interpretability and the need for intuitive understanding, enabling non-technical users such as clinicians and clinical researchers to interact effectively with AI models. Ultimately, this approach improves usability, enhances perceived trustworthiness, and increases the impact of CDSS in healthcare applications.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "The prevalence of cannabis use has risen dramatically over the past decade, leading to an increasing number of people meeting criteria for Cannabis Use Disorder (CUD)  [1, 2] . This surge presents significant challenges for healthcare providers, who must develop accurate and reliable methods for diagnosis and treatment  [3] . To address these challenges, Artificial Intelligence (AI) and Machine Learning (ML) technologies have emerged as promising tools in healthcare, offering new ways to analyze complex data and providing insights that may not be readily apparent through traditional methods  [4] . AI has great potential to revolutionize clinical decision-making; however, the \"black box\" nature of many AI models hinders their application in clinical settings. This opacity in the AI decision-making process often leads to skepticism and mistrust among healthcare providers, who may be reluctant to rely on these systems to make critical clinical decisions  [3] .\n\nTraditionally, drug use prediction models in addiction research have relied on isolated data sources, either focusing on verbal cues, self-reports, or independently on sensor signals  [4] . However, the complexity of substance use behaviors requires a more holistic, multimodal approach to capturing patterns of substance use in an individual's daily life  [1]  . Motivated by  [5]  [6], we combine linguistic data collected through user interactions on a customized platform with continuous sensor data from smartphones and wearable devices, aiming to create an integrated system capable of real-time prediction and intervention.\n\nIn order to reduce skepticism due to the opacity of AI models, the concept of explainable AI (XAI) has received attention as it aims to make AI systems more transparent and understandable to end users  [7] . This transparency enhances trust and facilitates the integration of AI into clinical practice. The aim of XAI is to make AI systems more transparent and explainable  [7] , thereby enhancing trust and facilitating their integration into clinical practice. These insights enable clinicians to understand the reasoning behind AI recommendations, thereby increasing confidence and trust  [8]  in using these systems as part of the decision-making process. Additionally, integrating sentiment analysis into clinical decision support systems (CDSS) is a new way to enhance the interaction between healthcare providers and AI systems  [9] . Sentiment analysis enables the system to determine the user's emotional state and thus respond accordingly. This personalized interaction increases user engagement  [10] , by creating a more empathetic and supportive clinical environment  [11] .\n\nThe novelty of this study lies in the joint use of multiple models. The first model uses facial expression recognition technology to analyze the emotional expression of the person (e.g., healthcare provider) using the system, and text sentiment analysis to identify the emotional state of the user. A second large language model focuses on generating interpretable AI-based explanations and clinical recommendations. This innovative combination of emotional state analysis and generation of AI-based explanation and recommendations not only ensures clear delineation of tasks, but also improves the overall efficiency and effectiveness of the system. By integrating emotion recognition of the user (i.e., the clinician) into the CDSS, the system dynamically adapts tone and content to the clinician's emotional state, providing empathic responses that promote trust and reduce cognitive load in busy clinical environments. At the same time, the interpretable AI component ensures transparency in the decision-making 979-8-3503-7550-3/24/$31.00 Â©2024 IEEE arXiv:2503.06463v1 [cs.HC] 9 Mar 2025 process, addressing the long-standing \"black box\" problem of AI-based CDSS.\n\nThe main objective of this research is to construct an affective-adaptive CDSS to help users (i.e., clinicians) obtain more effective AI interpretive feedback. While the system is designed to perform well in high-stress situations commonly encountered in clinical settings, it is also tailored to adapt to varying levels of a user's emotional states, ensuring usability and effectiveness even when clinicians are not operating under high stress. This approach accounts for the potential variability in affect when clinicians interact with the system  [1] . We integrate bilingual models for affective analysis and interpretive feedback generation, respectively, and provide personalized affective-adaptive responses in user interactions  [12] . This research explores how to optimize the user experience through human-computer interaction by developing an application system that integrates interpretive AI and affectivedriven feedback  [1] . In this study, we will discuss the design of the user interaction platform and address ethical issues regarding data privacy and participant consent. Our findings contribute to the growing body of research on substance use intervention and highlight the potential of combining advanced AI techniques with mobile health and data from wearables to support clinical interventions. The system is intended to assist clinicians in decision-making, rather than requiring them to make diagnoses based on emotional states. Its main purpose is to provide explainable insights and emotional adaptation based on artificial intelligence to support clinicians working in high-stress environments.\n\nThis study answers several key research questions: How does the integration of two language models and sentiment analysis impact the user experience and clarity of AI explanations in CDSS?\n\nWhat are the key challenges and benefits of integrating real-time emotion recognition to improve user interaction and decision-making in AI-driven clinical systems? How does the variability in clinicians' emotional responses, particularly for those new to the system, affect their interpretation and adoption of AI-driven insights?",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Related Work",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "A. Advantages Of Language Models In Designing Cdss-Hci",
      "text": "Clinical Decision Support Systems (CDSS) developed using Artificial Intelligence (AI) and Machine Learning (ML) technologies have received much attention in recent years, especially in improving healthcare outcomes  [13] . The integration of AI into healthcare systems is driven by the need to improve the accuracy, efficiency, and personalization of patient care. However, the adoption of AI in clinical settings has been challenging, especially due to the opaque nature of many AI models, leading to concerns about their trustworthiness and interpretability. This section reviews related work in the field of explainable AI (XAI)  [14]  and sentiment analysis  [15]  and its application in healthcare, particularly in the context of cannabis use disorder (CUD)  [16, 17] .\n\nLanguage models have recently gained prominence as a powerful tool for designing CDSS within the Human-Computer Interaction (HCI) framework. The main advantages of Language Models include the ability to process and analyze large amounts of unstructured textual data to provide valuable insights and predictions. Approaches often include training these models on large datasets to support clinical decision making  [18] . The results of several studies have shown that language models can significantly improve diagnostic accuracy, personalize patient care, and provide decision support by identifying patterns and correlations in data that may have been missed by clinicians  [13, 19] . However, language models also have significant limitations, including the need for large amounts of high-quality training data, the possibility of inherent bias in models, and challenges related to the interpretability and transparency of the decision-making process. Addressing these limitations is critical to the reliable and ethical use of language models in clinical settings  [20, 21] .\n\nThis predictive capability is particularly beneficial for chronic disease management, as early detection and timely intervention are critical to preventing disease progression and improving patient prognosis.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "B. Xai In Cdss-Hci",
      "text": "Explainable Artificial Intelligence (XAI) has become an important area of research that addresses the \"black box\" problem associated with many machine learning models  [7] . The need for transparency in the AI decision-making process is particularly acute in healthcare, where clinicians need a clear understanding of the factors that drive AI-generated recommendations. Various XAI techniques have been proposed to improve the interpretability of AI models and provide clinicians with the insights needed to trust and effectively use these systems. In this study, although we explored a variety of XAI techniques including SHAP (Shapley Additive Explanations)  [22] , rule-based explanations, and counterfactual explanations  [23] , we also use causal learning models  [24]  to improve our analyses and interpretation of the data.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "C. Prompt Engineering For Ai Interactions",
      "text": "In recent years, in the field of Natural Language Processing (NLP), Prompt Engineering has emerged as a key technique for guiding language models to generate specific outputs. Some research has been done in few-shot learning and zeroshot learning modes, where the user can provide a small number of textual hints to guide the model in generating a solution to a complex task. Compared to traditional finetuning methods, prompt engineering reduces time and cost, allowing the model to adapt to different task requirements without further training  [25] . These studies show that with well-designed cues, language models can be guided to perform multiple tasks.\n\nAs dialog systems are more and more widely used in HCI scenarios, users expect AI-driven systems to not only generate content-correct responses, but also to understand and adapt to their emotional states. Existing research emphasizes that emotional states such as anger, disgust, fear, happiness, sadness, surprise, and neutrality are typically expressed during humancomputer interactions. Systems that are able to reliably detect and respond to these emotions show the potential to improve user satisfaction by increasing the perceived relevance and empathy of their responses. Therefore, prompt engineering has been gradually used as an effective means to build emotionally adaptive dialog systems  [26] . This emotion-adaptive prompt design not only enhances the naturalness of the dialog, but also improves user satisfaction in emotional interactions  [27] .",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "D. Facial Emotion Recognition In Cdss",
      "text": "Integrating facial emotion recognition into CDSS has emerged as a promising advancement for enhancing the interaction between healthcare providers and AI tools. Facial emotion recognition utilizes computer vision and machine learning techniques to detect and interpret human facial expressions, enabling systems to assess a user's emotional state in real time  [28] . This capability is particularly valuable in high-stress healthcare environments, where clinicians often experience a range of emotions that can affect their decisionmaking processes. While this ability may be valuable in medical settings where clinicians may face high workloads or manage critical patient cases, it can also be adapted to different levels of stress and emotional states. The use of facial emotion recognition in CDSS is in line with the principles of human-computer interaction (HCI) and affective design, which emphasize the importance of user-centered and emotionally intelligent interfaces  [29] . This approach improves system usability and promotes a stronger trusting relationship between clinicians and AI tools. By recognizing and responding to users' emotional states, CDSS can provide a more personalized and effective support experience  [30] .\n\nMany systems focus solely on providing evidence-based recommendations without considering the emotional state of the clinician, which can have an impact on the decisionmaking process, both in high-stress environments and in routine clinical settings. The system is designed to support clinicians by adapting to different levels of user stress, ensuring that the clinician is able to make optimal evidence-based recommendations in a variety of situations  [31] . Although some studies have incorporated text sentiment analysis to adapt system responses, these approaches often neglect non-verbal cues, which are critical to a full understanding of a user's emotions  [32] . In addition, existing emotion recognition systems often rely on a single modality-text or facial expressions-which may not capture the full range of emotional cues. This single modality approach may result in an incomplete or inaccurate assessment of a user's emotional state  [33] .",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "E. Gaps In Health Risk Monitoring, Prediction And Intervention",
      "text": "Despite significant progress, large gaps remain in health risk surveillance, prediction and intervention. Current systems often face challenges in integrating with various data sources, maintaining data accuracy and providing real-time updates. Many existing health monitoring systems are heavy on analyzing data from a single data source such as electronic health records (EHRs), wearable devices, or other healthrelated technologies. This fragmentation limits the effectiveness of predictive models and the ability to provide realtime health updates  [34, 35] . Current research on addiction exhibits several gaps, particularly in the integration of natural language processing (NLP) and machine learning (ML) to fully understand and predict addiction-related health risks.\n\nSeveral studies have shown the potential of NLP in processing clinical data to predict addiction relapse and other health outcomes (e.g.,  [36] ). Another study highlighted the role of ML in analyzing social media data to detect patterns related to substance use and mental health  [37] . However, the integration of these approaches remains underexplored.\n\nAlthough the potential of combining NLP and ML to enhance addiction research is well recognized, research that fully integrates these techniques remains scarce. Integrating NLP and ML in a clinical decision support system is critical. Machine learning models, combined with XAI and causal modeling, provide transparent, interpretable analytics for patient data, enabling healthcare providers to understand complex algorithms, thereby enhancing the decision-making process. Further the use of language models allows systems to understand and generate natural language, making it easier for clinicians to interpret results. Most of the existing research tends to focus on NLP or ML individually, rather than exploring the synergistic effects of using them together  [38, 39] . The lack of an integrated approach has resulted in a lack of fully utilizing these technologies to better understand, predict, and intervene in addiction-related health risks  [40] . Sentiment analysis is a tool in the field of NLP traditionally used to assess the sentiment tone of textual data. However, the use of sentiment analysis in healthcare, particularly in CDSS, introduces a new dimension to the interaction between patients and clinicians  [41] . In contrast to the traditional use of sentiment analysis for understanding patient emotions, in our CDSS, sentiment analysis aims to match and respond to the emotional state of clinicians. The reason for applying sentiment analysis to clinicians is that the emotional and psychological state of healthcare providers can greatly influence their decision-making process and overall effectiveness  [42] . Highstress environments, emotional exhaustion, and the complexity of clinical decision-making all affect how clinicians interact with AI systems. Therefore, a sentiment analysis module that detects and aligns with a clinician's emotional state plays a critical role in improving the usability and acceptance of a CDSS  [40] . This alignment can make the interaction more enjoyable for the clinician, and also help to reduce cognitive load, thus allowing the clinician to more effectively focus on patient care. Additionally, by providing emotionally intelligent feedback, the system fosters a collaborative relationship between the clinician and the AI, reducing user frustration and thus better integrating AI recommendations into the clinical workflow  [42] .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Iii. Method",
      "text": "In this research, we developed a Web-based interactive application to provide interpretable machine learning analytics, causal-inference and sentiment analysis services. The application allows users (e.g., clinicians) to interact with the system through a natural language interface to access information such as interpretation of model predictions, and results from causal-inference and counterfactual reasoning analyses. These features are integrated into a user-friendly chat interface that makes complex machine learning model decisions understandable to non-technical users. Figure  1  illustrates the system's architecture and data flow, integrating user input, emotion recognition, AI processing, and feedback generation.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "A. Data Collection",
      "text": "The study employed used the AWARE framework app  [43]  to collect data from individuals who reported cannabis use with smartphone sensors, such as GPS and accelerometer, alongside Fitbit Charge 2 data for heart rate and step count  [44] [45] [46] . The data were analyzed to identify patterns of cannabis use, with physiological and behavioral data segmented into 5-minute intervals  [6] . Key statistics, like heart rate and step count from Fitbit, were extracted to evaluate (i.e., detect) cannabis intoxication. Self-reported cannabis use events were classified into 'intoxicated' and 'not intoxicated' to test the system's ability to determine and interpret behavioral outcomes from sensor inputs  [6] . Participants were financially compensated for their involvement, receiving $75 upon completion of the baseline evaluation and $25 for participating in the debriefing interview. Furthermore, they earned $10 per day if they successfully gathered at least 75% of the required data, including Fitbit recordings and experience sampling method (ESM) inputs  [6] . Participants who completed the system testing were compensated with a $20 Amazon gift card.\n\nFor the chat data, user interactions and generated responses were logged in Google Sheets. The data are structured with fields for email address, timestamp, role (user or assistant), and content  [47] . This allows for the persistent storage of conversation history, which can be retrieved and used to maintain context in the ongoing session.\n\nWe also conducted a user feedback survey to gather qualitative insights about system performance. The survey evaluated various aspects such as clarity, system usability and personalized recommendations. We asked participants to rate the system based on their own experiences, which helped refine the feedback mechanism and interface design. This feedback was critical in identifying the system's strengths and areas for improvement, especially in terms of user experience and interpretability of AI-driven insights.Table  I  shows information about the participants.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "B. Model Generation",
      "text": "In this study, we employed a personalized model generation strategy, whereby we constructed a machine learning model that is unique to each cannabis user to ensure that the model accurately captured and reflected individual differences. This approach centers on generating a separate model for each cannabis user, rather than using a common model for all cannabis users' data. Each cannabis user's data is first subjected to initial processing after collection, including operations such as removing duplicate values, filling in missing values (e.g., using the average value as appropriate), and normalizing values to ensure data quality and consistency. In order to eliminate redundancy between features, we also calculated the correlation between features and removed highly correlated features to avoid model overfitting. At the same time, this approach can optimize feature selection, thereby reducing redundancy and improving computational efficiency. Specifically, after correlation analysis, step count statistics (median, maximum, Q1, Q3), sedentary behavior indicators (Q1 sedentary), and battery discharge data (time and power consumption standard deviation, maximum, minimum) were removed. This significantly reduced the complexity of the model without affecting the predictive performance. For each cannabis user, we extracted the full history of that user from the preprocessed data. This data was used to train a personalized model for each cannabis user  [5] . For example, in counterfactual analysis, we used decision tree regression models to train each cannabis user individually. This individual",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "C. Interpretability Analysis",
      "text": "Interpretability analysis aims to provide users of the system (e.g., clinicians) with an understanding of the behavior of the machine learning models. By integrating SHAP (Shapley Additive Explanations) Value Calculation  [48] , Causal Analysis  [24] , Counterfactual Analysis  [23] , and Rule Extraction  [49] , the system is able to explain the model's decisionmaking process from multiple perspectives, enhancing the transparency and interpretability of the model.\n\nThis system combines a variety of interpretability techniques to help users (e.g., clinicians) understand and scrutinize the behavior and decisions of machine learning models from multiple perspectives. Whether it is global feature importance analysis based on SHAP values, variable interaction impact analysis based on causal inference, hypothetical scenarios generated through counterfactuals, or decision rules extracted through SkopeRules, the system is committed to presenting complex model behaviors to the user in a clear and easy-tounderstand manner  [5] . This multi-level explanatory analysis not only improves the transparency of the model, but also enhances users' trust and understanding of the model results. All XAI-related images are converted to img64 format and transmitted to the GPT-4 model, so that AI can read the information in the image. Figure  3  shows the various forms that XAI takes in the system.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "D. Prompt Engineering",
      "text": "In this research, we designed and developed a Web-based interactive dialogue system that can dynamically respond to the emotional state of the clinician using the system. By leveraging prompt engineering  [50] , the system optimizes feedback in dialogue interactions and integrates two complementary components to achieve its core function. The first component uses facial expression recognition technology and text sentiment analysis to identify the emotional state of the user, including anger, disgust, fear, happiness, sadness, surprise, and neutrality. The second component uses text sentiment recognition to generate an emotional analysis of the user's input text. By combining the analysis of the two, we transmit the resulting emotion to the GPT-4 model.\n\nThe facial emotion recognition module is a key innovation of the system. It uses a camera-based setup to detect and record the user's facial expressions every two seconds, since the duration of a neutral expression ranges up to 4 seconds  [51] . The module then identifies the most intense emotional changes between consecutive readings and uses the changes to adjust the system's response. For example, when the system recognizes that the user (e.g., a clinician) is experiencing a negative emotion (e.g., anxiety, confusion, or anger), it adjusts its tone to convey empathy and support, avoiding overly complex terminology while maintaining a respectful and professional tone. Conversely, when the system detects that the user is in a positive emotional state, it responds with an uplifting and affirmative tone, delivered with sincerity and professionalism. For neutral emotional states, the system ensures a professional and clear tone, prioritizing clarity and ease of use. Users do not need to actively chat with the system to extract emotions; instead, the system passively analyzes their facial expressions and short text input in real time. This ensures minimal disruption to their workflow while still allowing the system to adjust its response based on their emotional state. At the same time, the \"chat\" icon allows users to quickly locate the system's response, thereby reducing confusion when using it.\n\nIn this study, we designed a prompt engineering approach that combines multimodal affective data to ensure that AI interactions are empathetic and context-aware. The system integrates facial expression recognition and text sentiment analysis, prioritizing facial expressions as the primary indicator of user emotions. This is because in many cases, users may suppress emotional cues in text and express true emotions more nonverbally  [52, 53] . However, text sentiment analysis is still used as a secondary function to improve the interpretation of the context. The system assesses the relative change in facial expression rather than the absolute value, to prevent sudden changes in tone due to minor fluctuations in emotion. For example, if a clinician's level of frustration decreases slightly from 100% to 95%, they are still frustrated, and the system maintains empathy rather than shifting to neutral. This approach ensures a stable adaptation of the response and avoids overreacting to insignificant changes. We chose GPT-4 primarily because of its unique multimodal capabilities, which enable it to seamlessly handle facial image input and text conversations. Unlike most language models that only process text, GPT-4 is able to analyze images, making it particularly suitable for combining facial emotion recognition with conversational AI. The comparison between our system and a common GPT-4 system is shown in Figure  2 .\n\nIn addition to emotionally responsive interactions, the system employs a strategy of simplifying technical details to make XAI explanations more interpretable. Instead of presenting specific numerical values or highly technical expressions, the system instead uses qualitative and relevant descriptions. For example, instead of reporting the precise quantitative weight for a feature to describe a feature's importance to model prediction, the system conveys feature importance by saying \"this feature plays a more important role in the model decision\". Similarly, model output is also expressed in terms of 'high probability' or 'medium impact', rather than precise numerical probabilities. This simplification makes it easier for non-technical users to understand the model's explanations, thereby lowering the cognitive threshold for understanding complex concepts  [54] . The integration of these strategies enables the system to provide more empathetic and contextual interactions, and to improve user engagement. The precise model values are available to the user of the clinical decision support system (CDSS), if requested in a prompt.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "E. Dialogue System",
      "text": "The sentiment detection layer independently processes text sentiment analysis and facial sentiment recognition. Facial sentiment is detected every two seconds, and the system calculates the change in sentiment between user interactions. In our dialogue system, all seven emotions (joy, sadness, anger, neutrality, fear, disgust, surprise) share a percentage distribution, and the one with the highest percentage determines the final classification. Since emotions usually last up to four seconds, but conversations last one to two minutes  [51] , neutral states are not very meaningful for capturing the overall emotional trend of the user. Therefore, neutral emotions are excluded, and the other emotions are ranked according to intensity and frequency. The priority sequence is determined based on the framework of psychological research on emotional arousal  [55] , and is arranged in descending order according to the activation intensity of emotional valence. Specifically, negative emotion categories (such as anger and fear) are assigned higher decision-making weights, which is consistent with the priority processing of threats in human emotional response mechanisms. This ensures that the system focuses on the most prominent emotions throughout the interaction. In addition, in order to prevent misclassification, if the user does not show positive or negative emotions, the system will default to a neutral, professional response, rather than force an incorrect emotional label. The sentiment with the highest frequency is then set as the user's current sentiment. Meanwhile, text input is analyzed separately using a sentiment detection language model to detect sentiment. These results are combined in the response generation layer and submitted to the GPT-4 model to generate empathetic and contextually appropriate responses. The XAI analysis layer integrates data from the cannabis user's smartphone and wearable devices such as Fitbit to generate explainable insights using SHAP, SkopeRules, and counterfactuals. These insights are presented in a userfriendly visual format within the interface, allowing clinicians to directly view the model results alongside the interaction with the GPT-4 model. These visual explanations are further transmitted as images to the GPT-4 model, enabling the system to provide detailed, context-aware, and interpretable responses to the clinician user's queries.\n\nThe web-based interface enables real-time interaction, where users can either enter custom questions or choose from predefined queries. The system responds to user queries in real time. The system's responses are presented in a natural and empathetic manner, and are dynamically adjusted to the emotional state of the user as detected by a facial emotion recognition module via the camera. This module continuously analyzes emotions in real time, capturing subtle facial cues every two seconds and adjusting the response accordingly. The interface also includes the ability to explore XAI outputs such as SHAP explanations, with the system's user able to toggle between different interpretability methods at the touch of a button. These system outputs are displayed as interactive charts or tables that can be zoomed and repositioned for easy inspection by the user. To make complex insights more understandable, the system avoids technical jargon and instead provides qualitative, user-friendly explanations, bridging the gap between technical insights and clinical practice decisions. By combining multimodal sentiment recognition with explainable AI, the system not only enhances trust and usability, but also provides personalized explanations based on clinicians' needs and context. Figure  3  shows the interface of our system.\n\nIV. RESULT Our analysis utilized multiple XAI techniques to reveal patterns of behavior associated with cannabis use, highlighting key differences in activity levels, environmental conditions, and physiological responses. Notably, some participants exhibited frequent motion fluctuations, indicating an increase in their level of agitation during self-reported cannabis use. This observation was in line with changes in accelerometer readings, where elevated motion changes were detected, particularly in the dominant axis. Noise level measurements indicated that a subset of users were frequently exposed to high background noise, further substantiating the likelihood of cannabis use in social or public settings. This model uses a dataset containing a large number of negative samples during both the training and testing stages to improve the stability of its performance in scenarios with imbalanced category distributions. The dataset more realistically simulates the imbalanced distribution characteristics of categories in real-world scenarios by making the number of negative samples (samples that do not use cannabis) significantly greater than that of positive samples.\n\nThe cannabis use prediction in this project specifically refers to the identification (i.e., detection) of acute cannabis intoxication events based on physiological and behavioral sensor data. Data were collected from 34 young adults (18-25 years old) recruited from the community who reported current cannabis use. Data included heart rate, step count, and activity information from a Fitbit, as well as contextual data such as GPS and phone usage from the AWARE mobile sensing app. The system's model-prediction framework involved training a separate machine learning model for each participant using their sensor data (smartphone and Fitbit). Algorithms such as XGBoost and decision trees and rule-based models such as SkopeRules were tested, and the best-performing model for each individual was selected based on metrics such as accuracy, precision, recall and F1 score. The average accuracy for all participants exceeded 0.85, demonstrating strong predictive performance.\n\nThese explanatory AI tools can help us understand how models make predictions in light of specific features. One of our main challenges in this process was to translate these complex model outputs into feedback that, for example, users of the system, such as clinicians, could understand and apply. During the evaluation process, we not only focused on the predictive accuracy of the model, but also delved into the impact of prompt design on model interpretability and system user experience. By comparing the results with and without the use of prompt design, we found that prompt design improves system performance significantly. Without hints (i.e., effective prompts), the system feedback is often general and lacks specificity, making it difficult for the system's users to quickly understand the meaning of complex model results. However, with effective prompt design, we were able to break down complex model logic into more understandable parts, thus providing more detailed, precise, and understandable feedback to the system's users.\n\nThis study evaluates the performance of two systems. The first is the optimization system introduced in this paper, which incorporates both facial emotion recognition and text-based sentiment analysis to provide personalized and emotionally intelligent responses. The second is not an optimization system, created for the purpose of comparison by removing the facial emotion recognition and text-based sentiment analysis components. Both systems share the same interface layout to ensure consistency, with the comparison focusing on usability, personalization, and clarity through user evaluations conducted under controlled conditions. The two system interfaces are shown in Figure  3 . To evaluate both systems, a comprehensive survey was conducted, see Table  II , which included five key dimensions: system usability, personalization and relevance of insights, clarity and understandability, system strengths, and overall system user satisfaction. The survey questions were designed to measure the ease of use of interactions with the system, the appropriateness of personalized feedback, and the clarity of system-generated insights.\n\nDuring participant recruitment, we posted recruitment announcements on online platforms and screened eligible participants, including medical practitioners, health care providers, and system users with certain needs for emotion recognition. All participants signed informed consent forms and were informed of the data usage and privacy protection measures. The participants' backgrounds covered related fields such as clinical decision-making and healthcare to ensure broad applicability of the experimental results. Participants logged in to the system and accessed the designated interface to complete interactive tasks with the system (e.g., inputting health-related questions in natural language). After each interaction, the system performed emotion recognition and generated interpretative feedback. Experimental data are automatically recorded via a predefined Google Sheets API, including participant input, system feedback, and sentiment classification results. Participants were also asked to rate various aspects of the system using a questionnaire after each test.\n\nIn addition, the comparative results of the systems on these dimensions are illustrated in Figure  4  to show how the optimized system significantly outperforms the unoptimized model in each of the dimensions of clarity, personalized feedback, and overall system usability. The light orange colour in Figure  4  represents our system language model and the light blue colour represents the base language model. Each bar corresponds to a specific question (e.g., 'Question 1', 'Question 2'), and the connecting line at the top of the bar and the labelled p-value indicate the significant difference between the two sets of scores. Significance markers (e.g. *, **, ***) are coloured in red to indicate the significance levels p < 0.05, p < 0.01 and p < 0.001, respectively. p-values are marked in black if they are not statistically significant. This figure allows visualising the difference in performance between the two models on different problems and their significance. These evaluations help us better understand the role of prompt engineering and sentiment analysis models in improving the interpretability of the system, as evidenced by clearer feedback and more relevant suggestions, as well as improving the overall user experience through more intuitive interactions and faster response times.\n\nIn terms of system usability (Figure  4a ), the role of prompt engineering is crucial in enabling the system to more accurately understand the user's natural language input and provide intelligent feedback accordingly. Sentiment analysis modeling also plays a role in this dimension by enabling the system to adjust the interaction based on the emotional context of the user's input, ensuring that the system's feedback is not only accurate but also emotionally connected to the user.\n\nThe optimization system's strengths in personalized recommendations were more effective than the base system. On Question 2 in Figure  4b , a paired t-test was performed to examine the difference between subjective reports for the optimized and basic systems. The analysis revealed a significant improvement, t(11) = 2.57, p = 0.026, with a medium effect size (Cohen's d = 0.74), suggesting a meaningful interven-tion effect. The optimized system scored 8.00 (SD = 1.35), compared to the basic system's 6.50 (SD = 1.93). These scores were based on a 10-point scale, where 1 represented \"not relevant at all\" and 10 represented \"highly relevant\". This suggests that the optimization system is better able to provide personalized recommendations with higher relevance based on the user's specific health condition. This is largely due to prompt engineering, which allows the system to be more flexible and more tailored to the participant's mood in adjusting the feedback content and generating analytics that are more relevant to the cannabis use.\n\nFor clarity and comprehensibility, Figure  4c  shows that the optimized language modeling system using prompt engineering and sentiment analysis outperformed the base language modeling system on all questions. The optimized system achieved a score of 8.58 out of 10 on question 3, \"Clarity of System Explanation,\" compared to 6.67 out of 10 for the base system. Statistical analysis revealed a substantial improvement, t(11) = 3.36, p = 0.063, with a large effect size (Cohen's d = 0.97). This difference indicates that the optimized system utilizes prompt engineering techniques to simplify the output of complex explanatory AI into easy-to-understand text. This allows users to more intuitively understand the causes and logic of the model's predictions, avoiding confusion caused by technical jargon or complex analysis results.\n\nThe system benefits dimension (Figure  4d ) evaluates whether the system had a positive impact on the cannabis user's health behaviors and decisions. Question 2 highlights how the optimization system enhances the clinician's capacity to improve patient health outcomes, such as better sleep or increased physical activity, through personalized recommendations. This result demonstrates the potential of the optimization system to support clinicians in promoting better health out- In Figure  4e  on overall satisfaction, the results indicated a notable improvement, t(11) = 3.35, p = 0.006, with a strong effect size (Cohen's d = 0.97), highlighting the enhanced impact of the optimized system on user satisfaction.. The optimization system using prompt engineering is better able to provide system users with personalized feedback and specific guidance, allowing them to have a higher level of trust and acceptance of the system-generated suggestions.\n\nThe optimization system consistently scored highly in terms of clarity and understandability, which indicates that the integration of a bilingual model and sentiment analysis effectively supported users' understanding of the insights generated by the AI. This clarity is further validated by overall user satisfaction, which indicates that most users are satisfied with the insights and recommendations provided by the system. Meanwhile, the integration of real-time sentiment recognition enables the system to dynamically adjust its response, thereby enhancing its personalization capabilities. The system is capable of detecting a range of emotions, including 'angry', 'fear', 'disgust', 'sad', 'surprise', 'happy', and 'neutral'. This wide range of detectable emotions ensures that the system remains useful and adaptive even in low-stress contexts, effectively utilizing emotion analysis to personalize interactions. Personalization and Relevance are perceived as an important advantage, with a median personalization score of 9 in terms of relevance to the individual cannabis user's health and wellness needs, specifically reflecting the system's ability to provide personalized feedback based on the clinician's prompts. However, the variability in personalization scores indicates differences in system user experience, which suggests challenges in maintaining consistency of case-specific insights. This variability in personalization scores highlights the need for better judgment mechanisms to further tailor responses based on real-time sentiment data. Sentiment-driven responses help to facilitate interactions between clinicians and AI systems, as evidenced by high usability scores, with median ratings for ease of use and intuitiveness between 8 and 9. The system's ability to accurately interpret reference data and adapt response tone based on user sentiment leads to a positive, empathetic user experience that helps to increase engagement and user satisfaction.\n\nV. DISCUSSION This study introduces a novel approach to integrating multiple models in a clinical decision support system (CDSS) customized for managing cannabis use disorder (CUD)  [13, 60] . The combination of sentiment analysis with interpretable AI explanations generates an emotion adaptive and more transparent CDSS to help guide evidence-based decision making. Traditionally, many healthcare providers may be skeptical of AI systems, and reluctant to rely on AI to support critical clinical decisions  [16] . By employing interpretable models such as SHAP and rule-based systems, the system provides clinicians with a clear and more understandable view of how specific clinical decisions are made based on multimodal patient data (e.g., self-report, sensor), thereby increasing the probability of trust and adoption of system recommendations  [61, 62] .\n\nAlso, in addition to improving decision transparency, the system incorporates sentiment analysis that adjusts system responses based on a healthcare provider's emotional state when using the system  [37] . In busy, high-stress environments, a clinician's immediate emotional state could negatively impact their ability to make sound decisions  [63] . By recognizing and responding to these emotional cues in real time, a decision support system could provide more empathetic feedback to the clinician, thereby reducing cognitive load and potentially preventing burn out. Our findings suggest that effective prompt engineering improves the interpretability of AI by guiding responses towards structured, relevant, and contextaware output. Although the multimodal capabilities of GPT-4 were used in this study, the principles of effective prompt engineering can be generalized to other language models. Models such as Gemini, Claude, and LLaVA could also benefit from structured input, although their performance varies in medical reasoning and multimodal integration. Future work will include evaluating these models using the same prompt design framework to assess their suitability and effectiveness in clinical XAI applications.\n\nIn high-pressure healthcare environments, health care providers often experience a range of emotions that can affect their decision-making processes. The addition of facial emotion recognition enables the system to detect signs of fatigue, stress or frustration that may not be apparent from just text exchanges with the system. For example, a clinician interacting with the system who exhibits subtle expressions of anxiety may receive more calming and reassuring feedback with the inclusion of facial expression recognition  [64] . This emotionally adaptive interaction not only helps reduce cognitive burden, but also could help alleviate burnout, a key issue in healthcare settings  [65] . The CDSS's enhanced emotional intelligence promotes a more empathetic human-machine interaction, which is critical for building trust and reliance in AI-assisted tools. This empathetic interaction can increase system adoption rates as clinicians feel more understood and supported in their clinical work.\n\nIn addition, facial expression recognition addresses some of the limitations of previous iterations of the system. While text sentiment analysis can provide valuable insights, it may fail to capture the full complexity of human emotions, particularly in environments where verbal communication is limited or stressful enough to inhibit the expression of emotive language. Facial expressions often convey nuanced emotional information, which can result in possible misclassification of emotions. If accurately detected and interpreted, emotion recognition can significantly improve the responsiveness of the system. However, facial expression recognition also presents challenges, particularly in terms of data privacy and security. The system processes sensitive biometric data, raising ethical questions about consent and the protection of personal information. Ensuring the confidentiality and integrity of facial data is critical. Future work should focus on implementing encryption techniques and exploring approaches such as federated learning, which processes data on local devices rather than transmitting it to a central server. This approach reduces the risk of data breaches and unauthorized access.\n\nThe addition of facial emotion recognition and text sentiment recognition significantly enriches the CDSS, enabling it to provide more empathetic and context-sensitive feedback. This advancement improves the user experience for clinicians, could support more effective evidence-based decision-making, and strengthen the trusting relationship between healthcare providers and AI systems. By addressing the challenges of increasing accuracy of recommendations, data privacy and ensuring diversity and representativeness in the training data, the system can further improve its adaptability and effectiveness in a variety of clinical settings.\n\nOne limitation of our system is that it requires a stable internet connection to use GPT-4 for real-time processing. In clinical settings, an unstable network may affect response time and overall usability. To address this issue, future work will explore integrating lightweight local language models to provide offline backup functionality to ensure that the system remains operational even in low connectivity environments. In addition, while our system currently prioritizes facial expression recognition, further research is needed to evaluate the impact of integrating other context-aware features to improve the accuracy of emotion extraction. Future iterations will also investigate other multimodal models to balance the performance and computational efficiency of clinical decision support.\n\nAt the same time, training a unique model for each user requires a significant amount of computation, which motivates us to explore other methods, such as embedding-based modeling. Future work will further investigate hybrid modeling strategies, such as transfer learning and adaptive finetuning, to optimize computational requirements and prediction accuracy. In addition, comparative analyses with other personalization techniques, including clustering-based methods or meta-learning, may provide further insights into improving the adaptability of models across users.\n\nVI. CONCLUSION This research proposes an innovative approach in a clinical decision support system (CDSS) tailored to manage cannabis use disorder (CUD). By combining facial emotion recognition and text sentiment analysis with explainable AI-driven explanations, the system addresses long-standing challenges of AI in healthcare applications. These challenges include the opaque \"black box\" nature of many AI models and the historical lack of emotional intelligence in AI interactions with clinicians. The addition of facial emotion recognition capabilities enhances the system's ability to detect and respond to the emotional state of healthcare workers in real time. This affective intelligence design is particularly important in clinical settings. By providing supportive and emotionallyappropriate feedback to health care providers using the system, the system could reduce stress and cognitive load  [63] , thereby improving the overall user experience and fostering trust between clinicians and AI tools.\n\nFurthermore, the use of interpretable AI models, such as SHAP and rule-based systems, can improve the transparency and comprehensibility of the system's recommendations. This transparency is essential for increasing clinicians' trust in and reliance on AI assistance. By revealing the decision-making process behind a particular decision, the system can help reduce concerns of healthcare professionals regarding the \"black box\" of AI-based recommendations and promote its adoption. By addressing the technical issue of decision transparency and the emotional dimension of clinician support, the system developed in this study shows promise for improving cannabis use disorder management.",
      "page_start": 6,
      "page_end": 14
    },
    {
      "section_name": "Vii. Acknowledgment",
      "text": "This study was supported by the National Institute On Drug Abuse of the National Institutes of Health under Award Numbers U01DA056472 and R21 DA043181.",
      "page_start": 12,
      "page_end": 12
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: illustrates the systemâs",
      "page": 3
    },
    {
      "caption": "Figure 1: Overview of the Affective Explainable AI-Driven Clinical Decision Support System (AXAI-CDSS)",
      "page": 4
    },
    {
      "caption": "Figure 3: shows the various forms",
      "page": 5
    },
    {
      "caption": "Figure 2: Comparison of Our Proposed Real-Time Emotion-Aware Conversational AI (Left) and Basic LLMs (Right)",
      "page": 6
    },
    {
      "caption": "Figure 2: In addition to emotionally responsive interactions, the sys-",
      "page": 6
    },
    {
      "caption": "Figure 3: System Demonstration (Top) and Example of Multi-AI Explainability with Causal Reasoning (Bottom)",
      "page": 7
    },
    {
      "caption": "Figure 3: shows the interface of our system.",
      "page": 8
    },
    {
      "caption": "Figure 3: To evaluate both systems, a comprehensive",
      "page": 8
    },
    {
      "caption": "Figure 4: to show how the",
      "page": 8
    },
    {
      "caption": "Figure 4: represents our system language model and the",
      "page": 9
    },
    {
      "caption": "Figure 4: a), the role of prompt",
      "page": 9
    },
    {
      "caption": "Figure 4: b, a paired t-test was performed to",
      "page": 9
    },
    {
      "caption": "Figure 4: c shows that the",
      "page": 9
    },
    {
      "caption": "Figure 4: d) evaluates",
      "page": 9
    },
    {
      "caption": "Figure 4: Comparative Results across Different Metrics",
      "page": 10
    },
    {
      "caption": "Figure 4: e on overall satisfaction, the results indicated a",
      "page": 10
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "PID": "P1",
          "Ethnicity": "Asian",
          "Gender": "M",
          "Occupation/Area (Years of Experience in the Medical Domain)": "Pharmaceutical Researcher\n(10)",
          "Age": "20-29",
          "Education": "Doctoral Degree"
        },
        {
          "PID": "P2",
          "Ethnicity": "Asian",
          "Gender": "M",
          "Occupation/Area (Years of Experience in the Medical Domain)": "Medical Expert\n(6)",
          "Age": "20-29",
          "Education": "Master Degree"
        },
        {
          "PID": "P3",
          "Ethnicity": "Asian",
          "Gender": "M",
          "Occupation/Area (Years of Experience in the Medical Domain)": "Computer Programmer\n(0)",
          "Age": "20-29",
          "Education": "Bachelor Degree"
        },
        {
          "PID": "P4",
          "Ethnicity": "Asian",
          "Gender": "M",
          "Occupation/Area (Years of Experience in the Medical Domain)": "Medical Product Manager\n(10)",
          "Age": "30-39",
          "Education": "Master Degree"
        },
        {
          "PID": "P5",
          "Ethnicity": "Asian",
          "Gender": "M",
          "Occupation/Area (Years of Experience in the Medical Domain)": "Researcher\nin Medical Field (6)",
          "Age": "30-39",
          "Education": "PhD Student"
        },
        {
          "PID": "P6",
          "Ethnicity": "Asian",
          "Gender": "M",
          "Occupation/Area (Years of Experience in the Medical Domain)": "Graduate Student\nin Psychology (10)",
          "Age": "20-29",
          "Education": "Master Student"
        },
        {
          "PID": "P7",
          "Ethnicity": "Asian",
          "Gender": "M",
          "Occupation/Area (Years of Experience in the Medical Domain)": "Software Development Engineer\n(0)",
          "Age": "20-29",
          "Education": "Masterâs degree"
        },
        {
          "PID": "P8",
          "Ethnicity": "Asian",
          "Gender": "F",
          "Occupation/Area (Years of Experience in the Medical Domain)": "Engineer\n(0)",
          "Age": "20-29",
          "Education": "Masterâs degree"
        },
        {
          "PID": "P9",
          "Ethnicity": "Asian",
          "Gender": "F",
          "Occupation/Area (Years of Experience in the Medical Domain)": "PhD student\nin Engineering (3)",
          "Age": "20-29",
          "Education": "Ph.D."
        },
        {
          "PID": "P10",
          "Ethnicity": "Prefer not\nto say",
          "Gender": "M",
          "Occupation/Area (Years of Experience in the Medical Domain)": "Business (0)",
          "Age": "30-40",
          "Education": "Masterâs degree"
        },
        {
          "PID": "P11",
          "Ethnicity": "Asian",
          "Gender": "M",
          "Occupation/Area (Years of Experience in the Medical Domain)": "Physician (25)",
          "Age": "50-60",
          "Education": "Postgraduate, MD, PhD"
        },
        {
          "PID": "P12",
          "Ethnicity": "White",
          "Gender": "M",
          "Occupation/Area (Years of Experience in the Medical Domain)": "PhD student\nin Engineering (2)",
          "Age": "20-30",
          "Education": "Bachelorâs degree"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Section": "System Usability [56]",
          "Question": "Q1: How easy was it\nto interact with the system using natural\nlanguage? (1 - Very Difficult, 10 - Very Easy)\nQ2: How intuitive was the user interface for navigating through your data and the systemâs insights? (1 - Not Intuitive,\n10 - Very Intuitive)\nQ3: How satisfied were you with the systemâs response time in delivering insights? (1 - Very Dissatisfied, 10 - Very\nSatisfied)\nQ4: Does the systemâs answer correctly interpret cannabis user data? (1 - Very Dissatisfied, 10 - Very Satisfied)"
        },
        {
          "Section": "Personalization and Relevance\n[57]",
          "Question": "Q1: How do system insights and recommendations correlate with improved personal health and wellness for patients?\n(1 - Not Relevant, 10 - Very Relevant)\nQ2: To what extent did you feel\nthe insights were personalized to cannabis patientâs individual data and behaviors? (1\n- Not Personalized, 10 - Highly Personalized)\nQ3: Did the\nsystem adequately consider patientâs\ncurrent\ncondition when providing insights\n(e.g., heart\nrate,\nsleep\npatterns)? (1 - Not at All, 10 - Completely)"
        },
        {
          "Section": "Clarity and Comprehensibility\n[58]",
          "Question": "Q1: How clear and understandable were the explanations provided by the system (e.g., SHAP values, causal diagrams)?\n(1 - Very Confusing, 10 - Very Clear)\nQ2: How helpful were the systemâs visualizations (e.g., SHAP plots, causal diagrams) in aiding your understanding of\nthe results? (1 - Not Helpful, 10 - Very Helpful)\nQ3: Did you find the response generated by the system easy to follow? (1 - Very Difficult, 10 - Very Easy)\nQ4: Does this response contain evidence of correct personalization, reference appropriate user data, or correctly refuse\nto answer when such data is missing? (1 - Very Difficult, 10 - Very Easy)\nQ5: Does\nthe systemâs\nresponse demonstrate correct personalization,\nreference relevant user data appropriately, and\ncorrectly refuse to answer when the required data is missing? (1 - Very Difficult, 10 - Very Easy)"
        },
        {
          "Section": "System Benefits [59]",
          "Question": "Q1: How beneficial do you believe the insights were in improving your understanding of patient health behaviors (e.g.,\nsleep, exercise)? (1 - Not Beneficial, 10 - Very Beneficial)\nQ2: To what extent do you believe the systemâs recommendations will help you improve patient health outcomes (e.g.,\nbetter sleep,\nincreased physical activity)? (1 - Not Likely to Help, 10 - Very Likely to Help)\nQ3: Would you recommend this system to others looking for patient health insights from wearable data? (1 - Definitely\nNot, 10 - Definitely Yes)\nQ4: Did the systemâs response avoid misleading you? (1 - Definitely Not, 10 - Definitely Yes)"
        },
        {
          "Section": "Overall Satisfaction [59]",
          "Question": "Q1: How satisfied are you with the overall quality of\nthe insights and recommendations provided by the system? (1 -\nVery Dissatisfied, 10 - Very Satisfied)\nQ2: How likely are you to continue using this system to monitor and improve patient health in the future? (1 - Not\nLikely, 10 - Very Likely)"
        }
      ],
      "page": 9
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Ai-driven clinical decision support systems: an ongoing pursuit of potential",
      "authors": [
        "M Elhaddad",
        "S Hamam"
      ],
      "year": "2024",
      "venue": "Cureus"
    },
    {
      "citation_id": "2",
      "title": "Leveraging machine learning for personalized wearable biomedical devices: A review",
      "authors": [
        "A Olyanasab",
        "M Annabestani"
      ],
      "year": "2024",
      "venue": "Journal of Personalized Medicine"
    },
    {
      "citation_id": "3",
      "title": "Applying artificial intelligence to clinical decision support in mental health: What have we learned",
      "authors": [
        "G Golden",
        "C Popescu",
        "S Israel",
        "K Perlman",
        "C Armstrong",
        "R Fratila",
        "M Tanguay-Sela",
        "D Benrimoh"
      ],
      "year": "2024",
      "venue": "Health Policy and Technology"
    },
    {
      "citation_id": "4",
      "title": "Clinical decision support systems and predictive analytics",
      "authors": [
        "R Lourdusamy",
        "X Mattam"
      ],
      "year": "2020",
      "venue": "Machine Learning with Health Care Perspective: Machine Learning and Healthcare"
    },
    {
      "citation_id": "5",
      "title": "Exploring algorithmic explainability: Generating explainable ai insights for personalized clinical decision support focused on cannabis intoxication in young adults",
      "authors": [
        "T Zhang",
        "T Chung",
        "A Dey",
        "S Bae"
      ],
      "year": "2024",
      "venue": "2024 International Conference on Activity and Behavior Computing (ABC)"
    },
    {
      "citation_id": "6",
      "title": "Enhancing interpretable, transparent, and unobtrusive detection of acute marijuana intoxication in natural environments: Harnessing smart devices and explainable ai to empower just-in-time adaptive interventions: Longitudinal observational study",
      "authors": [
        "S Bae",
        "T Chung",
        "T Zhang",
        "A Dey",
        "R Islam"
      ],
      "year": "2025",
      "venue": "JMIR AI"
    },
    {
      "citation_id": "7",
      "title": "Explainable artificial intelligence (xai): Concepts, taxonomies, opportunities and challenges toward responsible ai",
      "authors": [
        "A Arrieta",
        "N DÃ­az-RodrÃ­guez",
        "J Del",
        "A Ser",
        "S Bennetot",
        "A Tabik",
        "S Barbado",
        "S GarcÃ­a",
        "D Gil-LÃ³pez",
        "R Molina",
        "Benjamins"
      ],
      "year": "2020",
      "venue": "Explainable artificial intelligence (xai): Concepts, taxonomies, opportunities and challenges toward responsible ai"
    },
    {
      "citation_id": "8",
      "title": "What clinicians want: contextualizing explainable machine learning for clinical end use",
      "authors": [
        "S Tonekaboni",
        "S Joshi",
        "M Mccradden",
        "A Goldenberg"
      ],
      "year": "2019",
      "venue": "Machine learning for healthcare conference"
    },
    {
      "citation_id": "9",
      "title": "Sentiment analysis in medical settings: New opportunities and challenges",
      "authors": [
        "K Denecke",
        "Y Deng"
      ],
      "year": "2015",
      "venue": "Artificial intelligence in medicine"
    },
    {
      "citation_id": "10",
      "title": "Establishing and maintaining long-term human-computer relationships",
      "authors": [
        "T Bickmore",
        "R Picard"
      ],
      "year": "2005",
      "venue": "ACM Transactions on Computer-Human Interaction (TOCHI)"
    },
    {
      "citation_id": "11",
      "title": "The relationship between physician empathy and disease complications: an empirical study of primary care physicians and their diabetic patients in parma, italy",
      "authors": [
        "S Del Canale",
        "D Louis",
        "V Maio",
        "X Wang",
        "G Rossi",
        "M Hojat",
        "J Gonnella"
      ],
      "year": "2012",
      "venue": "Academic medicine"
    },
    {
      "citation_id": "12",
      "title": "Deciphering diagnoses: How large language models explanations influence clinical decision making",
      "authors": [
        "D Umerenkov",
        "G Zubkova",
        "A Nesterov"
      ],
      "year": "2023",
      "venue": "Deciphering diagnoses: How large language models explanations influence clinical decision making",
      "arxiv": "arXiv:2310.01708"
    },
    {
      "citation_id": "13",
      "title": "Human-algorithmic interaction using a large language model-augmented artificial intelligence clinical decision support system",
      "authors": [
        "N Rajashekar",
        "Y Shin",
        "Y Pu",
        "S Chung",
        "K You",
        "M Giuffre",
        "C Chan",
        "T Saarinen",
        "A Hsiao",
        "J Sekhon"
      ],
      "year": "2024",
      "venue": "Proceedings of the CHI Conference on Human Factors in Computing Systems"
    },
    {
      "citation_id": "14",
      "title": "Current challenges and future opportunities for xai in machine learning-based clinical decision support systems: a systematic review",
      "authors": [
        "A Antoniadi",
        "Y Du",
        "Y Guendouz",
        "L Wei",
        "C Mazo",
        "B Becker",
        "C Mooney"
      ],
      "year": "2021",
      "venue": "Applied Sciences"
    },
    {
      "citation_id": "15",
      "title": "Aspect-based sentiment analysis of drug reviews applying cross-domain and cross-data learning",
      "authors": [
        "F GrÃ¤Ãer",
        "S Kallumadi",
        "H Malberg",
        "S Zaunseder"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 international conference on digital health"
    },
    {
      "citation_id": "16",
      "title": "Cannabis legalization and the decline of cannabis use disorder (cud) treatment utilization in the us",
      "authors": [
        "J Mennis",
        "G Stahler",
        "M Mason"
      ],
      "year": "2023",
      "venue": "Current Addiction Reports"
    },
    {
      "citation_id": "17",
      "title": "Spatial craving patterns in marijuana users: Insights from fmri brain connectivity analysis with high-order graph attention neural networks",
      "authors": [
        "J.-E Ding",
        "S Yang",
        "A Zilverstand",
        "K Kulkarni",
        "X Gu",
        "F Liu"
      ],
      "year": "2024",
      "venue": "IEEE Journal of Biomedical and Health Informatics"
    },
    {
      "citation_id": "18",
      "title": "Enabling multi-modal conversational interface for clinical imaging",
      "authors": [
        "K Dayanandan",
        "B Lall"
      ],
      "year": "2024",
      "venue": "Extended Abstracts of the CHI Conference on Human Factors in Computing Systems"
    },
    {
      "citation_id": "19",
      "title": "Understanding mental health using ubiquitous sensors and machine learning: Challenges ahead",
      "authors": [
        "T Tazin",
        "T Hossain",
        "S Hossain",
        "S Inoue"
      ],
      "year": "2024",
      "venue": "Human Activity and Behavior Analysis"
    },
    {
      "citation_id": "20",
      "title": "Large language models for therapy recommendations across 3 clinical specialties: comparative study",
      "authors": [
        "T Wilhelm",
        "J Roos",
        "R Kaczmarczyk"
      ],
      "year": "2023",
      "venue": "Journal of Medical Internet Research"
    },
    {
      "citation_id": "21",
      "title": "The clinical value of chatgpt for epilepsy presurgical decision making: Systematic evaluation on seizure semiology interpretation",
      "authors": [
        "Y Luo",
        "M Jiao",
        "N Fotedar",
        "J.-E Ding",
        "I Karakis",
        "V Rao",
        "M Asmar",
        "X Xian",
        "O Aboud",
        "Y Wen"
      ],
      "year": "2024",
      "venue": "medRxiv"
    },
    {
      "citation_id": "22",
      "title": "From local explanations to global understanding with explainable ai for trees",
      "authors": [
        "S Lundberg",
        "G Erion",
        "H Chen",
        "A Degrave",
        "J Prutkin",
        "B Nair",
        "R Katz",
        "J Himmelfarb",
        "N Bansal",
        "S.-I Lee"
      ],
      "year": "2020",
      "venue": "Nature machine intelligence"
    },
    {
      "citation_id": "23",
      "title": "Counterfactual explanations without opening the black box: Automated decisions and the gdpr",
      "authors": [
        "S Wachter",
        "B Mittelstadt",
        "C Russell"
      ],
      "year": "2017",
      "venue": "Harv. JL & Tech"
    },
    {
      "citation_id": "24",
      "title": "Causallearn: Causal discovery in python",
      "authors": [
        "Y Zheng",
        "B Huang",
        "W Chen",
        "J Ramsey",
        "M Gong",
        "R Cai",
        "S Shimizu",
        "P Spirtes",
        "K Zhang"
      ],
      "year": "2024",
      "venue": "Journal of Machine Learning Research"
    },
    {
      "citation_id": "25",
      "title": "Language models are few-shot learners",
      "authors": [
        "T Brown"
      ],
      "year": "2020",
      "venue": "Language models are few-shot learners",
      "arxiv": "arXiv:2005.14165"
    },
    {
      "citation_id": "26",
      "title": "Cfn-esa: A crossmodal fusion network with emotion-shift awareness for dialogue emotion recognition",
      "authors": [
        "J Li",
        "X Wang",
        "Y Liu",
        "Z Zeng"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "27",
      "title": "Personality-affected emotion generation in dialog systems",
      "authors": [
        "Z Wen",
        "J Cao",
        "J Shen",
        "R Yang",
        "S Liu",
        "M Sun"
      ],
      "year": "2024",
      "venue": "ACM Transactions on Information Systems"
    },
    {
      "citation_id": "28",
      "title": "Affective computing for healthcare: Recent trends, applications, challenges, and beyond",
      "authors": [
        "Y Liu",
        "K Wang",
        "L Wei",
        "J Chen",
        "Y Zhan",
        "D Tao",
        "Z Chen"
      ],
      "year": "2024",
      "venue": "Affective computing for healthcare: Recent trends, applications, challenges, and beyond",
      "arxiv": "arXiv:2402.13589"
    },
    {
      "citation_id": "29",
      "title": "Emotion recognition in doctor-patient interactions from real-world clinical video database: Initial development of artificial empathy",
      "authors": [
        "C.-W Huang",
        "B Wu",
        "P Nguyen",
        "H.-H Wang",
        "C.-C Kao",
        "P.-C Lee",
        "A Rahmanti",
        "J Hsu",
        "H.-C Yang",
        "Y.-C Li"
      ],
      "year": "2023",
      "venue": "Computer Methods and Programs in Biomedicine"
    },
    {
      "citation_id": "30",
      "title": "Affect and emotion in humancomputer interaction: From theory to applications",
      "authors": [
        "C Peter",
        "R Beale"
      ],
      "year": "2008",
      "venue": "Affect and emotion in humancomputer interaction: From theory to applications"
    },
    {
      "citation_id": "31",
      "title": "Automatic emotion recognition in clinical scenario: a systematic review of methods",
      "authors": [
        "L Pepa",
        "L Spalazzi",
        "M Capecci",
        "M Ceravolo"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "32",
      "title": "Systematic review of emotion detection with computer vision and deep learning",
      "authors": [
        "R Pereira",
        "C Mendes",
        "J Ribeiro",
        "R Ribeiro",
        "R Miragaia",
        "N Rodrigues",
        "N Costa",
        "A Pereira"
      ],
      "year": "2024",
      "venue": "Sensors"
    },
    {
      "citation_id": "33",
      "title": "Emotion recognition using electrodermal activity signals and multiscale deep convolutional neural network",
      "authors": [
        "N Ganapathy",
        "Y Veeranki",
        "H Kumar",
        "R Swaminathan"
      ],
      "year": "2021",
      "venue": "Journal of Medical Systems"
    },
    {
      "citation_id": "34",
      "title": "An explainable artificial intelligence framework for risk prediction of copd in smokers",
      "authors": [
        "X Wang",
        "Y Qiao",
        "Y Cui",
        "H Ren",
        "Y Zhao",
        "L Linghu",
        "J Ren",
        "Z Zhao",
        "L Chen",
        "L Qiu"
      ],
      "year": "2023",
      "venue": "BMC Public Health"
    },
    {
      "citation_id": "35",
      "title": "Ehr-based mobile and web platform for chronic disease risk prediction using large language multimodal models",
      "authors": [
        "C.-C Liao",
        "W.-T Kuo",
        "I.-H Hu",
        "Y.-C Shih",
        "J.-E Ding",
        "F Liu",
        "F.-M Hung"
      ],
      "year": "2024",
      "venue": "Proceedings of the 33rd ACM International Conference on Information and Knowledge Management"
    },
    {
      "citation_id": "36",
      "title": "Analysis of addiction craving onset through natural language processing of the online forum reddit",
      "authors": [
        "T Kramer",
        "G Groh",
        "N StÃ¼ben",
        "M Soyka"
      ],
      "year": "2024",
      "venue": "Plos one"
    },
    {
      "citation_id": "37",
      "title": "How machine learning is used to study addiction in digital healthcare: A systematic review",
      "authors": [
        "B Chhetri",
        "L Goyal",
        "M Mittal"
      ],
      "year": "2023",
      "venue": "International Journal of Information Management Data Insights"
    },
    {
      "citation_id": "38",
      "title": "Natural language processing in mental health research and practice",
      "authors": [
        "S Henry",
        "M Yetisgen",
        "O Uzuner"
      ],
      "year": "2021",
      "venue": "Mental Health Informatics: Enabling a Learning Mental Healthcare System"
    },
    {
      "citation_id": "39",
      "title": "Machine learning in medicine: a practical introduction to natural language processing",
      "authors": [
        "C Harrison",
        "C Sidey-Gibbons"
      ],
      "year": "2021",
      "venue": "BMC medical research methodology"
    },
    {
      "citation_id": "40",
      "title": "An overview of clinical decision support systems: benefits, risks, and strategies for success",
      "authors": [
        "R Sutton",
        "D Pincock",
        "D Baumgart",
        "D Sadowski",
        "R Fedorak",
        "K Kroeker"
      ],
      "year": "2020",
      "venue": "NPJ digital medicine"
    },
    {
      "citation_id": "41",
      "title": "Use Cases of Medical Sentiment Analysis",
      "authors": [
        "K Denecke"
      ],
      "year": "2023",
      "venue": "Use Cases of Medical Sentiment Analysis",
      "doi": "10.1007/978-3-031-30187-2_2"
    },
    {
      "citation_id": "42",
      "title": "Requirements analysis for an ai-based clinical decision support system for general practitioners: a user-centered design process",
      "authors": [
        "D SchÃ¼tze",
        "S Holtz",
        "M Neff",
        "S KÃ¶hler",
        "J Schaaf",
        "L Frischen",
        "B Sedlmayr",
        "B MÃ¼ller"
      ],
      "year": "2023",
      "venue": "BMC Medical Informatics and Decision Making"
    },
    {
      "citation_id": "43",
      "title": "Aware: mobile context instrumentation framework",
      "authors": [
        "D Ferreira",
        "V Kostakos",
        "A Dey"
      ],
      "year": "2015",
      "venue": "Frontiers in ICT"
    },
    {
      "citation_id": "44",
      "title": "Mobile assessment of acute effects of marijuana on cognitive functioning in young adults: observational study",
      "authors": [
        "T Chung",
        "S Bae",
        "E.-Y Mun",
        "B Suffoletto",
        "Y Nishiyama",
        "S Jang",
        "A Dey"
      ],
      "year": "2020",
      "venue": "JMIR mHealth and uHealth"
    },
    {
      "citation_id": "45",
      "title": "Mobile phone sensor-based detection of subjective cannabis intoxication in young adults: A feasibility study in realworld settings",
      "authors": [
        "S Bae",
        "T Chung",
        "R Islam",
        "B Suffoletto",
        "J Du",
        "S Jang",
        "Y Nishiyama",
        "R Mulukutla",
        "A Dey"
      ],
      "year": "2021",
      "venue": "Drug and alcohol dependence"
    },
    {
      "citation_id": "46",
      "title": "Enhancing interpretable, transparent and unobtrusive detection of acute marijuana intoxication in natural environments: Harnessing smart devices and explainable ai to empower just-in-time adaptive interventions",
      "authors": [
        "S Bae",
        "T Chung",
        "T Zhang",
        "M Ozolcer",
        "A Dey"
      ],
      "venue": "Enhancing interpretable, transparent and unobtrusive detection of acute marijuana intoxication in natural environments: Harnessing smart devices and explainable ai to empower just-in-time adaptive interventions"
    },
    {
      "citation_id": "47",
      "title": "Keep your stats in the cloud! evaluating the use of google sheets to teach quantitative methods",
      "authors": [
        "Z Kunicki",
        "N Zambrotta",
        "M Tate",
        "A Surrusco",
        "M Risi",
        "L Harlow"
      ],
      "year": "2019",
      "venue": "Journal of Statistics Education"
    },
    {
      "citation_id": "48",
      "title": "A unified approach to interpreting model predictions",
      "authors": [
        "S Lundberg",
        "S.-I Lee"
      ],
      "year": "2017",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "49",
      "title": "skope-rules: machine learning with logical rules in python",
      "year": "2024",
      "venue": "skope-rules: machine learning with logical rules in python"
    },
    {
      "citation_id": "50",
      "title": "Affective design analysis of explainable artificial intelligence (xai): a user-centric perspective",
      "authors": [
        "E Bernardo",
        "R Seva"
      ],
      "year": "2023",
      "venue": "Informatics"
    },
    {
      "citation_id": "51",
      "title": "Emotions revealed",
      "authors": [
        "P Ekman"
      ],
      "year": "2004",
      "venue": "Bmj"
    },
    {
      "citation_id": "52",
      "title": "Nonverbal communication on the net: Mitigating misunderstanding through the manipulation of text and use of images in computer-mediated communication",
      "year": "2019",
      "venue": "Nonverbal communication on the net: Mitigating misunderstanding through the manipulation of text and use of images in computer-mediated communication"
    },
    {
      "citation_id": "53",
      "title": "Multimodal emotion recognition",
      "authors": [
        "N Sebe",
        "I Cohen",
        "T Huang"
      ],
      "year": "2005",
      "venue": "Handbook of pattern recognition and computer vision"
    },
    {
      "citation_id": "54",
      "title": "Human-centered explainable ai (xai): From algorithms to user experiences",
      "authors": [
        "Q Liao",
        "K Varshney"
      ],
      "year": "2021",
      "venue": "Human-centered explainable ai (xai): From algorithms to user experiences",
      "arxiv": "arXiv:2110.10790"
    },
    {
      "citation_id": "55",
      "title": "Patterns of cognitive appraisal in emotion",
      "authors": [
        "C Smith",
        "P Ellsworth"
      ],
      "year": "1985",
      "venue": "Journal of personality and social psychology"
    },
    {
      "citation_id": "56",
      "title": "Sus: A quick and dirty usability scale",
      "authors": [
        "J Brooke"
      ],
      "year": "1996",
      "venue": "Usability Evaluation in Industry"
    },
    {
      "citation_id": "57",
      "title": "Ux research standardized usability questionnaire",
      "authors": [
        "C University"
      ],
      "venue": "Ux research standardized usability questionnaire"
    },
    {
      "citation_id": "58",
      "title": "The chatbot usability scale: the design and pilot of a usability scale for interaction with ai-based conversational agents",
      "authors": [
        "S Borsci",
        "A Malizia",
        "M Schmettow",
        "F Van Der Velde",
        "G Tariverdiyeva",
        "D Balaji",
        "A Chamberlain"
      ],
      "year": "2022",
      "venue": "Personal and ubiquitous computing"
    },
    {
      "citation_id": "59",
      "title": "Factors and design features influencing the continued use of wearable devices",
      "authors": [
        "O El-Gayar",
        "A Elnoshokaty"
      ],
      "year": "2023",
      "venue": "Journal of Healthcare Informatics Research"
    },
    {
      "citation_id": "60",
      "title": "Supporting physical activity behavior change with llm-based conversational agents",
      "authors": [
        "M JÃ¶rke",
        "S Sapkota",
        "L Warkenthien",
        "N Vainio",
        "P Schmiedmayer",
        "E Brunskill",
        "J Landay"
      ],
      "year": "2024",
      "venue": "Supporting physical activity behavior change with llm-based conversational agents",
      "arxiv": "arXiv:2405.06061"
    },
    {
      "citation_id": "61",
      "title": "Chatgpt, bard, and large language models for biomedical research: opportunities and pitfalls",
      "authors": [
        "S Thapa",
        "S Adhikari"
      ],
      "year": "2023",
      "venue": "Annals of biomedical engineering"
    },
    {
      "citation_id": "62",
      "title": "Assessing the research landscape and clinical utility of large language models: A scoping review",
      "authors": [
        "Y.-J Park",
        "A Pillai",
        "J Deng",
        "E Guo",
        "M Gupta",
        "M Paget",
        "C Naugler"
      ],
      "year": "2024",
      "venue": "BMC Medical Informatics and Decision Making"
    },
    {
      "citation_id": "63",
      "title": "The role of emotion in clinical decision making: an integrative literature review",
      "authors": [
        "D Kozlowski",
        "M Hutchinson",
        "J Hurley",
        "J Rowley",
        "J Sutherland"
      ],
      "year": "2017",
      "venue": "BMC medical education"
    },
    {
      "citation_id": "64",
      "title": "Emma: An emotion-aware wellbeing chatbot",
      "authors": [
        "A Ghandeharioun",
        "D Mcduff",
        "M Czerwinski",
        "K Rowan"
      ],
      "year": "2019",
      "venue": "2019 8th International Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "65",
      "title": "Adaptive emotion regulation might prevent burnout in emergency healthcare professionals: an exploratory study",
      "authors": [
        "K PÃ¡lfi",
        "J Major",
        "A HorvÃ¡th-SarrÃ³di",
        "A DeÃ¡k",
        "G FehÃ©r",
        "B GÃ¡cs"
      ],
      "year": "2024",
      "venue": "BMC Public Health"
    }
  ]
}