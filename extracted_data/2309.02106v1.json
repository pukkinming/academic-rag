{
  "paper_id": "2309.02106v1",
  "title": "Leveraging Label Information For Multimodal Emotion Recognition",
  "published": "2023-09-05T10:26:32Z",
  "authors": [
    "Peiying Wang",
    "Sunlu Zeng",
    "Junqing Chen",
    "Lu Fan",
    "Meng Chen",
    "Youzheng Wu",
    "Xiaodong He"
  ],
  "keywords": [
    "Multimodal emotion recognition",
    "label embedding",
    "cross-attention Ok",
    "look",
    "it's a beautiful day Angry",
    "Crazy",
    "Frustrated Happy",
    "Sweet",
    "Smile Sad",
    "Broke",
    "Cemetery Neutral",
    "Peaceful",
    "Calm"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Multimodal emotion recognition (MER) aims to detect the emotional status of a given expression by combining the speech and text information. Intuitively, label information should be capable of helping the model locate the salient tokens/frames relevant to the specific emotion, which finally facilitates the MER task. Inspired by this, we propose a novel approach for MER by leveraging label information. Specifically, we first obtain the representative label embeddings for both text and speech modalities, then learn the label-enhanced text/speech representations for each utterance via label-token and label-frame interactions. Finally, we devise a novel label-guided attentive fusion module to fuse the label-aware text and speech representations for emotion classification. Extensive experiments were conducted on the public IEMOCAP dataset, and experimental results demonstrate that our proposed approach outperforms existing baselines and achieves new state-of-the-art performance.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Generally, the emotion of spoken language is beyond the linguistic content of the utterance itself, and it is also related to the speaker's voice characteristics. To completely understand the emotion of the speaker, the text-and speech-based multimodal emotion recognition (MER) task was proposed to identify the emotion within an utterance  [1] .\n\nRecently, MER has attracted more and more attention. In the early phase, most works explored rule-based and neural network-based methods  [2, 3] . With the rapid development of self-supervised learning and pre-training, researchers attempt to tackle this task based on pre-trained models, e.g. BERT  [4]  and wav2vec2.0  [5] . For instance, Li et al.  [6]  proposed a contextaware multimodal fusion framework for the MER task, which applied BERT and WavLM as encoders. Chen et al.  [7]  proposed a key-sparse Transformer based on the RoBERTa and Wav2vec, which focuses more on emotion-related information. Despite their success, most of them only take labels as supervised signals while neglecting their inherent semantic information. Intuitively, the label information should be capable of helping the model to better understand the utterance. As shown in Figure  1 , for the text input, the token \"mad\" is similar to the label angry in semantics. As to the speech input, some frame segments also have in common with the tonal label. Based on above observation, we argue that the model may be able to locate the task-oriented salient tokens/frames accurately under the guidance of the label information. Then, the model can pay  more attention to the key information and effectively ignore the interference of redundant information. Therefore, as a kind of prior knowledge, leveraging label information is essential for the MER task. Label embedding is to learn the embeddings of the labels in classification tasks and has been proven to be effective in computer vision and natural language processing  [8] [9] [10] [11] , which enjoys a built-in ability to leverage alternative sources of information related to labels, such as class hierarchies or textual descriptions. However, there are rare speech-related work devoted to this technology. Take MER for example, there exist at least two obstacles that need to resolve. Firstly, labels are usually in the form of text. Due to the inherent disparities between the speech and the text, they cannot be directly exploited in speechrelated tasks. How to obtain representative label embeddings for the speech modality becomes a big challenge. Secondly, when introducing the label information, it will increase the difficulty of multimodal fusion. How to project the text/speech representations into the same label embedding space and fuse the multimodal features seamlessly is also a critical issue.\n\nIn this work, we propose a novel framework for MER to tackle the above challenges. We first summarize representative tokens/frames from the training set for each class as their descriptions. For text, we extract salient words for each label based on the frequency of tokens. As for speech, we utilize wav2vec2.0 to discretize the whole dataset, and then extract salient frames from them. After obtaining that, we further devise a novel label-enhanced multimodal emotion recognition model (LE-MER). Formally, given an utterance and the extracted label information, we first adopt BERT and wav2vec2.0 to learn representations for the text and speech input. To locate the salient tokens/frames in the utterance, we conduct label-text/speech interactions by introducing a label-token at- tention mechanism for the text and a label-frame one for the speech, which encourages the model to pay more attention to the emotion-related tokens/frames. Based on the above two cross-attention maps, we further introduce a label-guided attention mechanism to fuse the text and the speech. Since the inputs of this mechanism involve emotion-related information, it is capable of aligning the text and the speech from the emotional perspective.\n\nOur main contributions are summarized as follows: 1) To the best of our knowledge, this is the first work exploring a label embedding enhanced model for speech emotion recognition. 2) We propose a novel label-guided cross-attention mechanism to fuse different modalities, which is capable of learning the alignment between speech and text from the perspective of emotional space. 3) We show the effectiveness of our method on the IEMOCAP dataset with significant improvements compared with the baseline methods.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Proposed Approach",
      "text": "The overview architecture of our proposed LE-MER is illustrated in Figure  2 . LE-MER consists of three modules: a semantic label enhanced text encoder, a tonal label enhanced speech encoder, and a multi-modal fusion module with label-guided cross attention.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Semantic-Label Enhanced Text Encoder",
      "text": "Inspired by the success of Pre-trained Language Model (PLM)  [9, 12, 13]  on numerous NLP tasks, we apply the BERT  [4]  as the text encoder without loss of generality. For each utterance ut, we feed it into the BERT and get the sequence representation Ht ∈ R l t ×d t , where lt is the length of utterance and dt is the dimension.\n\nIn order to get the emotion-aware text representation, we fuse the semantic label information into the text encoder. Firstly, we extract the keywords on the text corpus under one class in the training set to get representative textual label descriptions. Specifically, we adopt the commonly used TF-IDF  [14]  algorithm to extract the Top-K words, then we feed these label descriptions into BERT, and obtain the semantic label embedding Lt ∈ R c×d t by averaging the token embeddings of all label descriptions, where c denotes the number of classes.\n\nAfter embedding both the words and the labels into a joint space, we can obtain the label-token attention matrix Gt ∈ R l t ×c by computing the cosine similarity between the text representation and label embedding as follows:\n\nThen we introduced a new objective based on the label-token interaction to encourage the emotion relevant words to be weighted higher than the irrelevant ones. Specifically, we conduct mean-pooling on the attention matrix Gt along the axis of sequence length, which is used as the discriminator for each class to judge the emotional relevance. Finally, we obtain the predicted logits p t g and the loss L t g :\n\nwhere CE(•, •) refers to the cross entropy loss.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Tonal-Label Enhanced Speech Encoder",
      "text": "The recent success of large pre-trained models  [15] [16] [17] [18] [19] [20] [21] [22]  motivates us to adopt novel, high-level features from self-supervised learning models. For the audio modality, we use wav2vec2.0  [5]  as our speech encoder. For each waveform of utterance, we obtain a sequence of contextualized representations from the output of wav2vec2.0 Hs ∈ R ls×ds , where ls is the number of time frames, and ds is the feature dimension. Similarly, we leverage the label information to obtain the emotion-aware speech representation. In order to represent the tonal label, we adopt a unified method as in text label embedding. That is, the key audio frames which contain representative tonal information will be extracted to generate the speech label embeddings for each class. To this end, we need to obtain the discrete representations of speech first. The quantizer module of the pre-trained wav2vec2.0 discretizes the output of the CNN feature encoder into a finite set  [5] , enabling the application of key-frames extraction on the speech data. In the same way, the TF-IDF is adopted to select the Top-K emotion-relevant frames under the same class. The embeddings of these emotion-relevant frames extracted from (codebook of) wav2vec2.0 will be averaged to produce the final tonal label embeddings Ls ∈ R ds×c , where ds is the feature dimension identical to the dimension of Hs.\n\nThrough the above way, we have embedded both the speech and the tonal label into a shared latent space, and then the labelframe interaction matrix can be computed in the following way:\n\nwhere Gs ∈ R ls×c , and each element indicates the similarity between the frames and the emotion category. In order to figure out the emotion related frames with the guidance of tonal label, we introduced another objective based on label-frame interaction. Specifically, the mean-pooling is conducted on the interaction matrix Gs along the frame axis to aggregate utterance level emotional correlation score.\n\nThe label-frame interaction based objective can be written as:\n\nwhich directly encourages the speech encoder to pay more attention to the emotional frames.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Multi-Modal Fusion With Label-Guided Cross Attention",
      "text": "Multimodal fusion technology for speech emotion recognition has been widely studied in recent years, including crossattention fusion  [23] , co-attention fusion  [24] , score fusion  [25] , time synchronous and asynchronous fusion  [26] , multimodal transformer  [6] , and etc. However, all these fusion mechanisms just devoted to aggregate word and speech embeddings, while ignored the the rich prior information contained in the emotion labels. We argue that the emotion labels can serve as a guidance to integrate the two modalities more efficiently. To this end, we elaborate a novel label-guided cross-attention to fuse multimodal emotion-related information.\n\nThe cross-attention mechanism have been proposed to capture the fine-grained interactions between the hidden representations of tokens and frames  [23, 27] :\n\nwhere Ar ∈ R l t ×ls , and W ∈ R ds×d t . Next, we can obtain the aligned hidden audio representation H ′ s by weighting Hs with cross-attention Ar, and the multimodal features Hm can be obtained by concatenating the text representation and the aligned speech representation:\n\nConsidering that the text and speech have been projected into the target emotional space in sections 2.1 and 2.2, we directly multiply the label-token interaction Gt and the label-frame interaction Gs to obtain the label-guided cross-attention matrix:\n\nwhere A l ∈ R l t ×ls , and each element indicates the similarity between text tokens and speech frames from the perspective of emotional correlation. Compared with A l , Ar solely represents the inherent semantic relations between the text and the speech, instead of emotion specific. To bridge the gap and integrate the emotion-aware relations into Ar, we propose an Attention Constraint Module which adopts A l to guide Ar, and we implement it with the mean squared error as follows:\n\nFinally, we aggregate the emotion-aware multimodal features Hm into a fixed-length vector v via max-pooling operation, and then feed v into a linear projection to obtain the prediction result and optimize it with the cross-entropy loss:\n\nThe overall loss function of the LE-MER is summarized as follows:\n\nwhere µ1, µ2, µ3, and µ4 are hyperparameters.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Experiments",
      "text": "In this section, we present the dataset, the results compared with other state-of-the-art approaches and the related analysis.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Dataset",
      "text": "We evaluate our proposed model on the commonly used Interactive Emotional Dyadic Motion Capture (IEMOCAP) database  [1] , which contains approximately 12 hours of audiovisual data. Among them, the text transcriptions, along with the corresponding audio, consist of five dyadic sessions where actors perform improvisations or scripted scenarios. To be consist with previous works  [6, 7, 26, [28] [29] [30] , we conduct experiments on 5531 utterances from four categories: angry, happy (merged with excited), sad, and neutral. We evaluate the model by a leave-onesession-out (5-fold) cross-validation (CV) strategy and adopt the average weighted accuracy (WA) and unweighted accuracy (UA) as evaluation metrics.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Experimental Setup",
      "text": "Data preprocessing. For speech modality, the 80-dimensional Log Mel-spectrograms  [31]  of each speech waveform are extracted by a 25ms window size with a 10ms step size and then normalized to the standardized normal distribution in utterance level. SpecAugment  [32]  is also applied to the extracted acoustic feature to improve the generalization ability of the model.\n\nFor text modality, we use historical utterances to enhance performance, as they can provide contextual information as well as some additional clues to the current utterance  [33] . Specifically, no more than ten historical utterances are spliced for each utterance and the maximum token length is limited to 150.\n\nSettings. The pre-trained wav2vec2.0-conformer-BASE  [34]  and BERT-BASE  [4]  model are employed as our speech and text encoder, respectively. Following  [18, [35] [36] [37] , wav2vec2.0 is pre-trained on 960h LibriSpeech dataset. In addition, 2nd stage pretraining is applied to the pre-trained wav2vec2.0-conformer on the training set. We adopt Adam as our optimizer with a warm-up of 8000 steps and set the learning rate to 10 -6 for the wav2vec2.0 and 5×10 -6 for BERT model, while the batch size for training is 16. As for hyperparameters in Eq  (12) , we set µ1, µ2, µ3, and µ4 to 1, 0.5, 0.2, and 0.2 empirically 1  .\n\nBaselines. We compare our proposed LE-MER with several baselines: [6, 7, 28] adopted cross-attention mechanism to fuse multimodal information, where a modified key-sparse attention is proposed in  [7] . Hou et al.  [29]  proposed a self-guided modality calibration network to achieve alignment between audio and text modalities. Wu et al.  [26]  proposed a two-branch neural network to capture correlations between multimodalitly from both word level and utterance level. Santoso et al.  [30]  proposed to use the combination of a self-attention mechanism and a word-level confidence measure (CM) to mitigate the errors in MER produced by ASR system.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Main Results",
      "text": "Unimodal Results. As we can observe from Table  1 , the performance of the text encoder improves significantly after integrating historical utterances (A2), proving that historical utterances can provide additional cues to support the current utterances. In addition, we also investigate the effects of different initialization for text label embeddings (A3-A5). We can observe that A4 achieves superior performance than A3, while the best results are achieved by A5, which substantiates that our keyword initialization scheme yields a more effective and representative label embeddings than the others. For speech modality, 2nd stage pretraining before finetuning can improve WA and UA by at least absolute 1.8 percent (B2 vs. B1). Moreover, speech label embeddings with all three types of initialization (B3-B5) bring varying contributions compared to B2. Some potential prior information from BERT embedding (B4) boosts the performance compared with B3. By getting rid of the shackles of the modality gap and benefiting from the discretized tonal label generated from pre-trained quantizer of wav2vec2.0, B5 makes a further improvement (B5 vs. B4) and achieves the best result.\n\nMultimodal Results. In Table  2 , we compare our multimodal results on the IEMOCAP dataset with the existing state-of-theart methods, which share the same setting of data preprocessing with us, for a fair comparison. It shows that our proposed approach achieves state-of-the-art results compared to the others in terms of both WA and UA. Here we also present the result obtained by the score fusion scheme, which simply sums the logits produced by two unimodal models for predictions. This straightforward scheme can achieve favorable result that outperforms all the baselines, indicating that our label embeddings can facilitate unimodal encoders to locate the salient tokens/frames relevant to the specific emotion, thus yielding better result. Compared with methods based on attention mechanism, such as  [6, 7] , our model achieves superior performance, which proves that our proposed label-guided attentive fusion module can serve as a bridge to leverage cues from multimodality to integrate emotional information more effectively. Model WA(%) UA(%) Chen et al.  [7]  74.30 75.30 Chen et al.  [28]  74.92 76.64 Hou et al.  [29]  75.60 77.60 Wu et al.  [26]  77.57 78.41 Santoso et al.  [30]  78",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Discussion",
      "text": "Hyper-parameter Tuning of K. To explore the optimal number of frames for the tonal label, we conduct a grid search to obtain its value. As shown in Figure  3 , when K is larger than 100, both WA and UA decrease to some extent, implying that label embeddings with larger K contain some redundant information that is irrelevant with the corresponding type of emotion. Vice versa, label embeddings with smaller K lack enough emotion-related information, causing performance degradation. Therefore, we set K to 100 for the tonal label. As for the semantic label, we explore the optimal K with the same method, and the best K is set to 9. For sake of repetition, we omit the process here. Ablation study of Attention Constraint. In this section, we further explore how to utilize the label-guided attention matrix to improve the model performance in terms of modality fusion, and we present the corresponding results in Table  3 . A subtle decrease can be observed from C1 to C2 in terms of UA, revealing the superiority of the attention constraint scheme against simple summation of label-guided attention A l and vanilla attention Ar. We can attribute this performance gap to adopting attention constraint, as it provides a more delicate way to craft multimodal features Hm to be label-aware under the supervision of A l . Furthermore, we perform multimodal fusion with only A l (C3) or Ar (C4). Further performance degradation can be observed by comparing either C3 or C4 with C2, validating the necessity of the interaction between A l and Ar. Attention Visualization. To demonstrate the effectiveness of our unimodal label embeddings, we perform the visualizations of both Gs and Gt on one utterance in IEMOCAP and present them in Figure  4 . In this example, waveform is aligned with the tokenized text, and both Gs and Gt have been averaged over the class dimension to generate the vectors Gs and Gt. Results show that the Gs assigns larger attention weights to emotionrelated speech segments, while Gt has higher weights on some emotional words, such as \"cool\". This reveals that our unimodal label embeddings can effectively guide the encoders to focus on emotion relevant information from the input.\n\nThat ' s so cool .",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Conclusions",
      "text": "In this paper, we presented LE-MER, a novel multimodal fusion framework for speech emotion recognition, which takes advantage of the both the textual and speech label information to extract the emotional token and frames, respectively. By mapping the speech and text representations to a common emotional space, we can learn the alignment between the text words and speech frames and fuse the emotional information more efficiently. Experimental results on the public IEMOCAP dataset demonstrated the superior performance of LE-MER and the importance of each component. In the future, we will explore how to extend this method to other speech tasks.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: , for the text input, the token “mad” is similar to the",
      "page": 1
    },
    {
      "caption": "Figure 1: Visualization of labels. The semantic label presents",
      "page": 1
    },
    {
      "caption": "Figure 2: The architecture of our proposed model LE-MER.",
      "page": 2
    },
    {
      "caption": "Figure 2: LE-MER consists of three modules: a seman-",
      "page": 2
    },
    {
      "caption": "Figure 3: , when K is larger than",
      "page": 4
    },
    {
      "caption": "Figure 3: Effect of K for speech label embeddings initialization.",
      "page": 4
    },
    {
      "caption": "Figure 4: In this example, waveform is aligned with the",
      "page": 4
    },
    {
      "caption": "Figure 4: Visualization of eGs and eGt.",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table 1: Comparison of our unimodal results on IEMOCAP WA",
      "data": [
        {
          "Model": "Chen et al. [7]\nChen et al. [28]\nHou et al. [29]\nWu et al. [26]\nSantoso et al. [30]\nLi et al. [6]",
          "WA(%)\nUA(%)": "74.30\n75.30\n74.92\n76.64\n75.60\n77.60\n77.57\n78.41\n78.40\n78.60\n80.36\n81.70"
        },
        {
          "Model": "Our Score Fusion",
          "WA(%)\nUA(%)": "81.32\n82.18"
        },
        {
          "Model": "Ours",
          "WA(%)\nUA(%)": "82.40\n83.11"
        }
      ],
      "page": 4
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "3",
      "title": "Multimodal speech emotion recognition using audio and text",
      "authors": [
        "S Yoon",
        "S Byun",
        "K Jung"
      ],
      "year": "2018",
      "venue": "SLT 2018"
    },
    {
      "citation_id": "4",
      "title": "Disentanglement for audio-visual emotion recognition using multitask setup",
      "authors": [
        "R Peri",
        "S Parthasarathy",
        "C Bradshaw",
        "S Sundaram"
      ],
      "year": "2021",
      "venue": "ICASSP 2021"
    },
    {
      "citation_id": "5",
      "title": "Bert: Pretraining of deep bidirectional transformers for language understanding",
      "authors": [
        "J Devlin",
        "M.-W Chang",
        "K Lee",
        "K Toutanova"
      ],
      "year": "2018",
      "venue": "Bert: Pretraining of deep bidirectional transformers for language understanding",
      "arxiv": "arXiv:1810.04805"
    },
    {
      "citation_id": "6",
      "title": "wav2vec 2.0: A framework for self-supervised learning of speech representations",
      "authors": [
        "A Baevski",
        "Y Zhou",
        "A Mohamed",
        "M Auli"
      ],
      "year": "2020",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "7",
      "title": "Context-aware multimodal fusion for emotion recognition",
      "authors": [
        "J Li",
        "S Wang",
        "Y Chao",
        "X Liu",
        "H Meng"
      ],
      "year": "2022",
      "venue": "Proc. Interspeech 2022"
    },
    {
      "citation_id": "8",
      "title": "Key-sparse transformer for multimodal speech emotion recognition",
      "authors": [
        "W Chen",
        "X Xing",
        "X Xu",
        "J Yang",
        "J Pang"
      ],
      "year": "2022",
      "venue": "ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "9",
      "title": "Joint embedding of words and labels for text classification",
      "authors": [
        "G Wang",
        "C Li",
        "W Wang",
        "Y Zhang",
        "D Shen",
        "X Zhang",
        "R Henao",
        "L Carin"
      ],
      "year": "2018",
      "venue": "Joint embedding of words and labels for text classification",
      "arxiv": "arXiv:1805.04174"
    },
    {
      "citation_id": "10",
      "title": "Fusing label embedding into bert: An efficient improvement for text classification",
      "authors": [
        "Y Xiong",
        "Y Feng",
        "H Wu",
        "H Kamigaito",
        "M Okumura"
      ],
      "year": "2021",
      "venue": "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021"
    },
    {
      "citation_id": "11",
      "title": "Label anchored contrastive learning for language understanding",
      "authors": [
        "Z Zhang",
        "Y Zhao",
        "M Chen",
        "X He"
      ],
      "year": "2022",
      "venue": "Label anchored contrastive learning for language understanding",
      "arxiv": "arXiv:2205.10227"
    },
    {
      "citation_id": "12",
      "title": "Legal charge prediction via bilinear attention network",
      "authors": [
        "Y Le",
        "Y Zhao",
        "M Chen",
        "Z Quan",
        "X He",
        "K Li"
      ],
      "year": "2022",
      "venue": "Proceedings of the 31st ACM International Conference on Information & Knowledge Management"
    },
    {
      "citation_id": "13",
      "title": "Pre-trained models for natural language processing: A survey",
      "authors": [
        "X Qiu",
        "T Sun",
        "Y Xu",
        "Y Shao",
        "N Dai",
        "X Huang"
      ],
      "year": "2020",
      "venue": "Science China Technological Sciences"
    },
    {
      "citation_id": "14",
      "title": "Bimodal speech emotion recognition using pre-trained language models",
      "authors": [
        "V Heusser",
        "N Freymuth",
        "S Constantin",
        "A Waibel"
      ],
      "year": "2019",
      "venue": "Bimodal speech emotion recognition using pre-trained language models",
      "arxiv": "arXiv:1912.02610"
    },
    {
      "citation_id": "15",
      "title": "Keyword extraction: a review of methods and approaches",
      "authors": [
        "S Beliga"
      ],
      "year": "2014",
      "venue": "Keyword extraction: a review of methods and approaches"
    },
    {
      "citation_id": "16",
      "title": "Wavlm: Large-scale selfsupervised pre-training for full stack speech processing",
      "authors": [
        "S Chen",
        "C Wang",
        "Z Chen",
        "Y Wu",
        "S Liu",
        "Z Chen",
        "J Li",
        "N Kanda",
        "T Yoshioka",
        "X Xiao"
      ],
      "year": "2022",
      "venue": "IEEE Journal of Selected Topics in Signal Processing"
    },
    {
      "citation_id": "17",
      "title": "Hubert: Self-supervised speech representation learning by masked prediction of hidden units",
      "authors": [
        "W.-N Hsu",
        "B Bolte",
        "Y.-H Tsai",
        "K Lakhotia",
        "R Salakhutdinov",
        "A Mohamed"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "18",
      "title": "Ufo2: A unified pre-training framework for online and offline speech recognition",
      "authors": [
        "L Fu",
        "S Li",
        "Q Li",
        "L Deng",
        "F Li",
        "L Fan",
        "M Chen",
        "X He"
      ],
      "year": "2023",
      "venue": "Ufo2: A unified pre-training framework for online and offline speech recognition"
    },
    {
      "citation_id": "19",
      "title": "Self-supervised speech representation learning: A review",
      "authors": [
        "A Mohamed",
        "H -Y. Lee",
        "L Borgholt",
        "J Havtorn",
        "J Edin",
        "C Igel",
        "K Kirchhoff",
        "S.-W Li",
        "K Livescu",
        "L Maaløe"
      ],
      "year": "2022",
      "venue": "IEEE Journal of Selected Topics in Signal Processing"
    },
    {
      "citation_id": "20",
      "title": "Data2vec: A general framework for self-supervised learning in speech, vision and language",
      "authors": [
        "A Baevski",
        "W.-N Hsu",
        "Q Xu",
        "A Babu",
        "J Gu",
        "M Auli"
      ],
      "year": "2022",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "21",
      "title": "Tera: Self-supervised learning of transformer encoder representation for speech",
      "authors": [
        "A Liu",
        "S.-W Li",
        "H.-Y Lee"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "22",
      "title": "vq-wav2vec: Selfsupervised learning of discrete speech representations",
      "authors": [
        "A Baevski",
        "S Schneider",
        "M Auli"
      ],
      "venue": "International Conference on Learning Representations"
    },
    {
      "citation_id": "23",
      "title": "wav2vec: Unsupervised pre-training for speech recognition",
      "authors": [
        "S Schneider",
        "A Baevski",
        "R Collobert",
        "M Auli"
      ],
      "year": "2019",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "24",
      "title": "Learning alignment for multimodal emotion recognition from speech",
      "authors": [
        "H Xu",
        "H Zhang",
        "K Han",
        "Y Wang",
        "Y Peng",
        "X Li"
      ],
      "year": "2019",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "25",
      "title": "Jointly fine-tuning\" bert-like\" self supervised models to improve multimodal speech emotion recognition",
      "authors": [
        "S Siriwardhana",
        "A Reis",
        "R Weerasekera",
        "S Nanayakkara"
      ],
      "year": "2020",
      "venue": "Jointly fine-tuning\" bert-like\" self supervised models to improve multimodal speech emotion recognition",
      "arxiv": "arXiv:2008.06682"
    },
    {
      "citation_id": "26",
      "title": "Multimodal emotion recognition with high-level speech and text features,\" in (ASRU) 2021",
      "authors": [
        "M Makiuchi",
        "K Uto",
        "K Shinoda"
      ],
      "year": "2021",
      "venue": "Multimodal emotion recognition with high-level speech and text features,\" in (ASRU) 2021"
    },
    {
      "citation_id": "27",
      "title": "Emotion recognition by fusing time synchronous and time asynchronous representations",
      "authors": [
        "W Wu",
        "C Zhang",
        "P Woodland"
      ],
      "year": "2021",
      "venue": "ICASSP 2021"
    },
    {
      "citation_id": "28",
      "title": "Cross-modal transfer learning via multi-grained alignment for end-to-end spoken language understanding",
      "authors": [
        "Y Zhu",
        "Z Wang",
        "H Liu",
        "P Wang",
        "M Feng",
        "M Chen",
        "X He"
      ],
      "year": "2022",
      "venue": "Proc. Interspeech 2022"
    },
    {
      "citation_id": "29",
      "title": "Multimodal emotion recognition with temporal and semantic consistency",
      "authors": [
        "B Chen",
        "Q Cao",
        "M Hou",
        "Z Zhang",
        "G Lu",
        "D Zhang"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "30",
      "title": "Multi-modal emotion recognition with self-guided modality calibration",
      "authors": [
        "M Hou",
        "Z Zhang",
        "G Lu"
      ],
      "year": "2022",
      "venue": "ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "31",
      "title": "Speech emotion recognition based on attention weight correction using word-level confidence measure",
      "authors": [
        "J Santoso",
        "T Yamada",
        "S Makino",
        "K Ishizuka",
        "T Hiramura"
      ],
      "year": "2021",
      "venue": "Interspeech"
    },
    {
      "citation_id": "32",
      "title": "Incremental learning for end-to-end automatic speech recognition",
      "authors": [
        "L Fu",
        "X Li",
        "L Zi",
        "Z Zhang",
        "Y Wu",
        "X He",
        "B Zhou"
      ],
      "year": "2021",
      "venue": "2021 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)"
    },
    {
      "citation_id": "33",
      "title": "Specaugment: A simple data augmentation method for automatic speech recognition",
      "authors": [
        "D Park",
        "W Chan",
        "Y Zhang",
        "C.-C Chiu",
        "B Zoph",
        "E Cubuk",
        "Q Le"
      ],
      "year": "2019",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "34",
      "title": "Emotion recognition in conversation: Research challenges, datasets, and recent advances",
      "authors": [
        "S Poria",
        "N Majumder",
        "R Mihalcea",
        "E Hovy"
      ],
      "year": "2019",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "35",
      "title": "Pushing the limits of semi-supervised learning for automatic speech recognition",
      "authors": [
        "Y Zhang",
        "J Qin",
        "D Park",
        "W Han",
        "C.-C Chiu",
        "R Pang",
        "Q Le",
        "Y Wu"
      ],
      "year": "2020",
      "venue": "Pushing the limits of semi-supervised learning for automatic speech recognition",
      "arxiv": "arXiv:2010.10504"
    },
    {
      "citation_id": "36",
      "title": "Emotion recognition from speech using wav2vec 2.0 embeddings",
      "authors": [
        "L Pepino",
        "P Riera",
        "L Ferrer"
      ],
      "year": "2021",
      "venue": "Proc. Interspeech 2021"
    },
    {
      "citation_id": "37",
      "title": "Exploring wav2vec 2.0 finetuning for improved speech emotion recognition",
      "authors": [
        "L.-W Chen",
        "A Rudnicky"
      ],
      "year": "2021",
      "venue": "Exploring wav2vec 2.0 finetuning for improved speech emotion recognition",
      "arxiv": "arXiv:2110.06309"
    },
    {
      "citation_id": "38",
      "title": "A fine-tuned wav2vec 2.0/hubert benchmark for speech emotion recognition, speaker verification and spoken language understanding",
      "authors": [
        "Y Wang",
        "A Boumadane",
        "A Heba"
      ],
      "year": "2021",
      "venue": "A fine-tuned wav2vec 2.0/hubert benchmark for speech emotion recognition, speaker verification and spoken language understanding",
      "arxiv": "arXiv:2111.02735"
    }
  ]
}