{
  "paper_id": "2504.17974v2",
  "title": "Optimism, Expectation, Or Sarcasm? Multi-Class Hope Speech Detection In Spanish And English",
  "published": "2025-04-24T23:00:46Z",
  "authors": [
    "Sabur Butt",
    "Fazlourrahman Balouchzahi",
    "Ahmad Imam Amjad",
    "Maaz Amjad",
    "Hector G. Ceballos",
    "Salud Maria Jimenez-Zafra"
  ],
  "keywords": [
    "Hope Speech Detection",
    "Sarcasm Detection",
    "Multilingual NLP",
    "Emotion Recognition",
    "Fine-grained Sentiment Analysis"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Hope is a complex and underexplored emotional state that plays a significant role in education, mental health, and social interaction. Unlike basic emotions, hope manifests in nuanced forms ranging from grounded optimism to exaggerated wishfulness or sarcasm, making it difficult for Natural Language Processing systems to detect accurately. This study introduces PolyHope V2, a multilingual, fine-grained hope-speech dataset comprising over 30,000 annotated tweets in English and Spanish. This resource distinguishes between four hope subtypes-Generalized, Realistic, Unrealistic, and Sarcastic-and enhances existing datasets by explicitly labeling sarcastic instances. We benchmark multiple pretrained transformer models and compare them with large language models (LLMs) such as GPT-4 and Llama 3 under zero-shot and few-shot regimes.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Recent improvements in Natural Language Processing (NLP) have enhanced applications in sentiment analysis, mental health assessments, social media monitoring, and educational platforms  [1] [2] [3] [4] [5] . Despite recent progress, a persistent challenge in emotion recognition lies in identifying subtle and complex emotions, particularly hope, which is often overlooked in standard emotion taxonomies  [6] . Unlike overt emotions such as anger, fear, sadness, or joy, hope is an inherently ambiguous emotion that exhibits differently across various contexts, and carries multiple emotional dimensions  [7, 8] . Therefore, traditional NLP techniques struggle to capture subtle emotions and highlight the need for more refined, context-aware approaches to accurately detect nuanced emotional expressions, such as hope  [9] .\n\nAccurate detection of hope speech unlocks substantial benefits across real-world NLP applications, such as enhancing student engagement and success  [10] , understanding patients' mental states  [11, 12] , and Non-medical prescription drug use (NPMDU). For example, within online education, instructors can leverage these systems as personalized support tools to accurately identify expressions of hope to better understand students' motivations, interests, and emotional barriers-ultimately enabling targeted interventions  [10] . Similarly, mental health professionals can leverage the natural progression of hope speech to better understand patients' mental states, therapeutic outcomes, and concealed psychological factors over time. Research emphasizes that accurately interpreting psychological states requires identifying positive emotional expressions, particularly hope and optimism  [11] . As a result, NLP systems significantly benefit from incorporating hope detection technologies, since integrating positive emotional indicators alongside negative sentiments enables more comprehensive, nuanced, and effective analytic models.\n\nTraditional NLP techniques struggle to detect hope due to its complex, dual nature as defined by Lazarus  [13] . According to Lazarus, hope consists of two essential components: (i) a specific desire for positive outcomes, and (ii) an overall optimistic view of future events. The inherent contradictions within hope result in complicated textual interpretations, causing individual textual markers to be variably perceived as genuine goals, imaginary objectives, or sardonic remarks. Consequently, expressions intended to reflect hopeful thinking may inadvertently suggest agitation, depending on their textual context. For example, the sentence \"I hope things finally change around here\" reveals optimism but also hints at frustration, depending on how it is used in the text. Most emotion recognition systems trained on simplified affective categories (e.g., Ekman's six basic emotions) tend to fail when processing hope, as a generic positive sentiment or as non-committal discourse, failing to grasp its subtleties  [7, 14] . Thus, emotion recognition systems trained on basic emotion sets fail to correctly identify hope and often misclassify it either as a simple positive emotion or as a neutral, non-committal state.\n\nAdditionally, recognizing hope is further complicated by sarcasm, as sarcastic expressions frequently employ positive language to communicate negative sentiments. The mismatch between literal meanings and intended emotions makes it challenging for machine learning and deep learning classifiers to distinguish genuinely hopeful statements from their sarcastic counterparts. For instance, sarcasm occurs when someone says, \"Great, I really hope this mess gets worse,\" even though the words themselves convey hope, but the semantic context reveals sarcastic negativity  [15] .\n\nThe semantic incongruity between lexical content and emotional meaning makes emotion detection more challenging. The combination of figurative language and sarcasm complicates consistent emotion classification for computers since they primarily work with superficial word analysis instead of contextual interpretation  [1] . Accurate context-aware emotion detection primarily depends on extensive, diverse datasets and sophisticated models  [16] . However, there is a notable lack of annotated datasets explicitly related to hope, and existing datasets often either entirely omit hope or classify it merely as a subset of general positive emotions that cause incorrect predictions. Therefore, more comprehensive annotated datasets are necessary that explicitly capture hope through genuinely positive sentiments, unfounded expectations, and sarcastic expressions.\n\nIn this study, we bridge the gap between emotion recognition and pragmatic language understanding and present a bilingual annotated hope speech dataset designed for hope detection in both English and Spanish. We explicitly annotate sarcasm within hope expressions and create rich annotation guidelines for different hope categories. We extend  [9]  hope categories (i) generalized hope, (ii) realistic hope, (iii) unrealistic hope or wishful thinking, and (iv) sarcasm. We develop sarcasm-resilient emotion classification models as a baseline to detect and evaluate the performance of the hope detection system in English and Spanish. This dataset can be used for advancing cross-linguistic emotion recognition research.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Literature Review",
      "text": "NLP has advanced beyond basic sentiment classification (positive, negative, or neutral) toward identifying more nuanced emotional states, including the subtle and underexplored emotion of hope  [6, 16] . While emotions such as joy, fear, and regret have received substantial attention, hope remains comparatively underresearched, despite its significant role in mental health support  [11] , education  [10] , online communication, and social programs  [17] . Hope consists of two distinct components that contribute to its complexity: (i) a general optimism inherent in individuals, and (ii) a specific expectation for positive outcomes  [13] . Given its unique semantic, emotional, and contextual characteristics, recent research in hope speech detection has increasingly treated hope as an independent emotional category rather than merely grouping it under general positive emotions.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Methodologies For Hope Speech Detection",
      "text": "Different methodologies have been proposed for detecting hope speech and addressing challenges related to context, semantics, and multilingual content. Various studies have explored techniques to identify hope expressions within social media and textual documents  [18, 19] . For instance, Balouchzahi et al.  [9]  introduced \"PolyHope,\" an NLP system that operates at semantic and contextual levels to detect nuanced expressions of hope. Similarly, another study  [20]  leveraged transformer-based models to differentiate between expressions of hope and regret, and highlighted complexities in jointly modeling these emotionally similar pairs. Despite these advancements, current methodologies lack in distinguishing hope expressions from sarcastic or superficially hopeful messages, particularly on social media platforms (e.g., X (previously Twitter), and Reddit). Deep learning models provide poor results because they often misinterpret pragmatic signals and contextual discourse markers  [21] . These limitations underline the need to incorporate pragmatic and contextual awareness to improve model accuracy. Moreover, addressing these contextual challenges becomes even more critical when extending hope detection methodologies across multilingual and cross-cultural datasets.\n\nA growing body of work has explored multilingual and cross-lingual hope speech detection, recognizing its importance in linguistically diverse societies. Chakravarthi et al.  [22, 23]  introduced multilingual approaches to detect hope speech in English, Tamil, Malayalam, and Kannada. The authors explicitly emphasized equality and diversity and inclusion (EDI) to detect hope speech. Nevertheless, their models did not sufficiently account for distortions caused by sarcasm or unrealistic expectations embedded within hopeful expressions. Similarly, researchers such as Arunima et al.  [24]  have contributed to shared tasks like Hope Speech Detection for Equality, Diversity, and Inclusion (HopeEDI), which expanded datasets and evaluation benchmarks in multiple Indian languages. Additional efforts have targeted Urdu datasets, with studies exploring the transferability of hope speech models across languages and the need for language-specific semantic resources  [25, 26] . Chakravarti  [27]  addressed hope speech detection in Tamil, English, and Malayalam, highlighting classification challenges due to code-mixed data. Moreover, Malik et al.  [28]  explored a joint multilingual and translation-based approach for hope speech detection in English and Russian using a fine-tuned Russian-RoBERTa model, achieving 94% accuracy and 80.24% F1-score, though multiclass classification was not addressed. However, most of these models do not explicitly address the confounding effects of sarcasm, exaggeration, or unrealistic optimism expressed through hopeful language-factors that often distort detection accuracy in real-world applications. Moreover, research on low-resource and codemixed language scenarios remains limited, although recent work has begun to tackle these challenges through cross-lingual transfer learning and zero-shot classification approaches  [29] .\n\nMoreover, the integration of sarcasm detection and contextual awareness into current lexical and dictionary-based hope detection methods often underperformed in analyzing unstructured, informal textual data. Therefore, integrating pragmatic, contextual, and multilingual perspectives to ensure accurate and reliable emotion recognition outcomes is crucial.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Existing Datasets For Hope Speech Detection",
      "text": "Several datasets have been introduced to facilitate advancements in hope speech detection in various languages, such as Spanish  [30, 31] , Bengali  [32] . For example, the \"HopeEDI\" dataset  [22]  introduced a multilingual corpus specifically targeting hopeful expressions related to equality, diversity, and inclusion (EDI). Additionally, the \"IberLEF\" Hope datasets  [30, 31]  extended hope speech detection into Spanish, and emphasized annotations around social causes and community identities. Another dataset, \"BongHope\"  [32] , presented a Bengali corpus for hope speech detection. Furthermore, the HOPE shared tasks at IberLEF provided comprehensive benchmarks and detailed insights into effective methodologies for annotating hope datasets  [31, 33] . In parallel, the HASOC-Hope subtrack, part of the FIRE shared tasks, introduced additional multilingual datasets covering Hindi, Malayalam, and Tamil, further enriching the research landscape in this area  [34] ). Finally, the LREC Hope Speech dataset offers a multilingual resource tailored for cross-lingual and zero-shot hope speech detection, with a special focus on low-resource language scenarios  [35] .\n\nHowever, most existing datasets either categorize hope speech through a simplistic binary labeling scheme (hope vs. non-hope) or do not annotate nuanced subcategories such as generalized optimism, unrealistic hope, or sarcastic hope. Additionally, few datasets explicitly annotate sarcasm, which limits their usefulness in training or evaluating models capable of differentiating genuine hope from sarcastic expressions. Moreover, annotation schemas typically lack cross-linguistic alignment, which poses challenges for developing robust multilingual models. As a result, there remains a notable gap in available datasets that provide both fine-grained hope annotations and explicit sarcasm labeling.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Sarcasm In Hope Speech: A Complex Challenge",
      "text": "The detection of hope speech is highly impeded by the presence of sarcasm. Positive sentimental language through rhetorical expression functions as sarcasm to convey negative meanings while possessing a humorous or mocking quality. For example, \"This one needs to go well as it happened previously\". This hopeful-sounding statement contains semantic skepticism through ironic speech. The model detection of hopeful speech can fail due to sarcastic statements that resemble hopeful statements. The improper evaluation of sentiment features occurs when basic analysis methods, such as positive polarity measurements, operate independently from detailed contextual analysis.\n\nThe task of sarcasm detection is challenging because it requires contextual information, tonal cues, and background understanding  [16] . Research on sarcasm detection within NLP has approached this problem in four ways: (i) Contextual embeddings (e.g., BERT, RoBERTa)  [36] , (ii) Multitask learning frameworks (to jointly learn emotion and sarcasm)  [37] , (iii) Contrastive learning approaches  [38] , and (iv) Discourse-aware models  [39]  that incorporate surrounding sentences or conversational history. Yet sarcasm-aware hope detection remains an unexplored intersection, and no existing hope detection dataset provides labeled sarcastic examples, as emphasized by Balouchzahi et al.  [9]  in their analysis of NPMDU-related textual data. To address this challenge, we annotate hope expressions along with sarcasm indicators and provide a richer, more realistic representation of hope in social discourse. Such annotation is critical for building models that can distinguish between sincere emotional optimism and sarcastic cynicism, as such a distinction is particularly important in domains like education, counseling, and mental health.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Dataset",
      "text": "In this work, we present PolyHope V2, a multilingual and fine-grained hope speech dataset consisting of around 30,000 annotated tweets in both English and Spanish. PolyHope V2 brings together the original English-language PolyHope tweets  [9] , a newly harvested Spanish counterpart, and an additional sarcasm layer to create a multilingual, multi-label resource for modelling hope on social media. Altogether the corpus contains 9,515 English and 20,442 Spanish tweets, each message annotated at the tweet level for hope, sarcasm, and the related fine-grained categories described below. The next subsections outline how the English, Spanish, and sarcasm components were collected, cleaned, and aligned. The complete dataset is publicly available on the web page of a shared task that we have organized to evaluate it and make it available to the scientific community 1  .",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "English",
      "text": "We relied on the PolyHope corpus  [9]  for both binary and multi-class modeling. Balouchzahi et al.  [9]  used Twitter to collect hope-related instances, gathering roughly 100,000 English tweets posted between January and June 2022, a period dominated by discussions of women's reproductive rights, Black civil-rights issues, religion, and mainstream politics. After discarding incomplete or duplicate texts, the pool dropped to just under 70,000 tweets. Filtering out messages with fewer than ten tokens reduced the set to fewer than 50,000, and the subsequent removal of retweets left about 23,000 unique, original tweets. These were shuffled to avoid topical clustering, and a random sample of 10,000 tweets was selected for manual annotation. A small number were eliminated during quality control, so the final annotated subset is slightly smaller than the initial sample, but still provides a substantial and balanced foundation for modelling hope phenomena in English social-media discourse. The further details on annotation and pre-processing are provided in the dataset paper  [9] .",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Spanish",
      "text": "For Spanish, we rely on the corpus introduced by Sidorov et al.  [40]  for both binary and multi-class hope detection. The authors translated the PolyHope English trigger words into Spanish (e.g., hope → esperanza, wish → ojalá, believe → creo, dream → sueño) and asked native speakers to check that the list covered all common variants. Using these cues they first collected 82,725 keyword-matching tweets, then added the 50,000 most recent Spanish tweets available in March 2022, yielding 132,725 raw items. After standard preprocessing-de-identifying user mentions and URLs, removing duplicates and retweets, and discarding messages with fewer than five tokens or only emojis-the dataset was reduced to roughly 33,300 unique, content-rich tweets. After pre-processing, only 35,000 tweets were left, and the remaining were excluded during the quality control and annotation phase. The further details on annotation and pre-processing are provided in the dataset paper  [40] .",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Sarcasm For English And Spanish",
      "text": "We expanded the English part of the dataset using two benchmark datasets in sarcasm detection iSarcasmEval  [41]  and Sarcasm Corpus V2  [42] . The dataset is aligned with our task as they are also collected using Twitter (Now X). We combined these datasets and used our Hope classifier (the best transformer model described in  [9]  fine-tuned on the PolyHope dataset), filtered all instances of sarcasm that can be detected as Hope in any category. Similarly, we developed a deep learning classifier (the best transformer model described in  [9]  fine-tuned on the Sarcasm dataset) and filtered all instances of potential sarcasm in other classes of Hope, i.e., unrealistic hope.\n\nThe Spanish instances of Sarcasm were synthetically generated using GPT-4o, by translating the English instances and keeping the sarcastic tone in Spanish. The new labels were then verified using two expert annotators. In the binary setting we merged every tweet annotated as \"sarcasm\" with the broader \"not-hope\" category, producing a clean dichotomy between hopeful and non-hopeful discourse. This decision reflects the pragmatic observation that sarcastic posts rarely convey genuine hope; rather, they typically critique or ridicule the prospect of a positive outcome. The grouping of sarcasm with no hope, therefore, sharpens the semantic boundary the classifier must learn, reduces class imbalance, and minimizes the risk that ironic language will be misinterpreted as an expression of hope.\n\nFor the multi-class task, however, we preserved sarcasm as its own label alongside hope, not-hope, and the other original classes provided by PolyHope. Retaining sarcasm allows the model to capture the distinctive lexical and pragmatic cues of ironic speech, which differ markedly from both sincerely hopeful and straightforwardly pessimistic language. Keeping it separate also facilitates finer-grained downstream analyses-researchers can quantify how sarcastic framing interacts with sensitive topics such as reproductive rights or racial justice-while still leveraging the same annotated tweet set described in  [9]  and  [40] . The examples of the data, in English and Spanish, can be seen in Table  1  and 2 respectively, while Table  3  shows the statistics of the dataset. Generalized Hope #USER# Aquí en la Mora y Piedra Azul seguimos esperando la segunda fase de alumbrado público de sus calles #USER# #USER# Realistic Hope Todo el maldito día trabajando en una presentación. Ansío el día en que deje de ser una corporate slave.\n\nUnrealistic Hope Si el sonido de la lluvia afuera no fuera ya suficientemente malo. . . sería el fondo perfecto para mí mientras intento estudiar. ¡Maravilloso! Sarcasm",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Methodology",
      "text": "This section details the baselines employed for hope speech and sarcasm detection within the PolyHope v.2 dataset. We explore two primary approaches: fine-tuning pretrained Transformer models and leveraging Zero-Shot Learning (ZSL) and Few-Shot Learning (FSL) capabilities of Large Language Models (LLMs).",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Fine-Tuning Pre-Trained Transformer Models",
      "text": "To establish strong and reproducible baselines for the proposed task, we adopted a fine-tuning approach using pre-trained Transformer models from the Hugging Face Transformers library 2  . This approach leverages transfer learning, enabling us to adapt models pre-trained on large corpora to our task-specific dataset. We fine-tuned several Transformer architectures, including Roberta, Albert, Electra, and DistilBERT, to evaluate their performance on both binary (Hope vs. Not Hope) and multiclass (Not Hope, Generalized Hope, Realistic Hope, Unrealistic Hope, and Sarcasm) classification tasks.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Experimental Setup",
      "text": "We implemented a stratified 5-fold cross-validation strategy using the simpletransformers library 3  to ensure robust and generalizable evaluation. This is crucial to mitigate potential biases and provide reliable performance estimates.\n\nWithin each fold, we executed the following steps:\n\n1. Model Initialization: We instantiated a ClassificationModel from the simpletransformers library, specifying the pre-trained architecture and model variant. 2. Fine-tuning Procedure: The instantiated model was then fine-tuned on the training partition. We employed early stopping based on the development set performance to prevent overfitting. 3. Evaluation Metrics: After fine-tuning, the model's performance was assessed on the held-out test split. We report accuracy, weighted precision, weighted recall, weighted F1-score, macro precision, macro recall, and macro F1-score. We emphasize weighted and macro averages due to the imbalanced nature of the dataset, providing a comprehensive performance profile across all classes. 4. Qualitative Analysis: To gain deeper insights into model behavior, we generated classification reports and confusion matrices for each fold. The confusion matrices were visualized to enable identification of systematic misclassifications. These matrices were averaged across folds to provide an aggregate view of model confusions.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Model Selection And Training Regimen",
      "text": "We conducted experiments on both English and Spanish datasets. The specific models used for each language are detailed in Tables  4  and 5 . These models were selected based on their suitability and previous success in addressing similar NLP challenges as addressed in Section 2.1. All models were fine-tuned using the AdamW optimizer with a learning rate of 3e-5 and a weight decay of 0.01. We trained for a maximum of 15 epochs per fold, employing early stopping with a patience of 3 epochs based on the performance on a held-out development set. The maximum sequence length was set to 100 tokens. A batch size of 32 was used for all experiments. The choice of these hyperparameters was guided by a coarse grid search on a separate development set, aiming to optimize performance while preventing overfitting. Through this fine-tuning exercise, we aim to establish strong, reproducible baselines for the PolyHope V2. The results obtained will quantify the effectiveness of Transformer models for hope speech detection and, critically, sarcasm identification. This comparative analysis will provide a benchmark against which to evaluate the performance gains achieved by our proposed ZSL and FSL methodologies leveraging LLMs.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Zero-Shot And Few-Shot Learning With Large Language Models",
      "text": "We further investigated the potential of LLMs to perform hope speech and sarcasm detection in ZSL and FSL settings. This approach allows us to leverage the preexisting knowledge and reasoning capabilities of LLMs without extensive task-specific training. We employed GPT-4 and Llama 3, two state-of-the-art LLMs, to explore their effectiveness in this context. We selected GPT-4  [43, 44]  and Llama 3  [45]  for their superior performance in various natural language understanding and generation tasks. GPT-4, known for its advanced reasoning and generalization abilities, serves as a high-performing benchmark. Llama 3, a powerful open-source LLM, offers an alternative perspective and allows for greater control and customization.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Prompt Engineering",
      "text": "The performance of LLMs in ZSL and FSL settings is highly dependent on the design of effective prompts. We carefully crafted prompts to elicit the desired classification behavior from the LLMs.\n\nZero-Shot Prompts: For ZSL, we used a simple prompt that instructed the LLM to classify the input text into one of the predefined categories (Hope, Not Hope for binary classification; Generalized Hope, Realistic Hope, Unrealistic Hope, Not Hope, Sarcasm for multiclass classification). The prompt was formatted as follows:\n\nClassify the following text into one of the categories \\{categories\\}:\n\nText: \\{text\\} Label:\n\nFew-Shot Prompts: For FSL, we provided the LLMs with a set of labeled examples to guide their classification. We selected 5 balanced examples per class. The prompt was structured as follows:\n\nHere are some labeled examples:\n\nText: \\{example1_text\\} Label: \\{example1_label\\} Text: \\{example2_text\\} Label: \\{example2_label\\} ...",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Now Classify This Text:",
      "text": "Text: \\{text\\} Label:",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Implementation Details",
      "text": "GPT-4: We used the OpenAI API to access GPT-4. The classify text function sends a request to the OpenAI API with the appropriate prompt and receives the LLM's classification. The temperature parameter was set to 0 to encourage deterministic and consistent responses. response = openai.chat.completions.create( model=\"gpt-4\", messages=[{\"role\": \"system\", \"content\": \"You are an NLP model trained for text classification.\"}, {\"role\": \"user\", \"content\": prompt}], temperature=0 ) Llama 3: We accessed Llama 3 via the Together AI API. The format few shot prompt function constructs the prompt, including a system message defining the LLM's role as a text classification expert and providing example classifications. The Together AI API client then sends the prompt to the Llama 3 model. We set the temperature to 0.3 and the maximum number of tokens to 50. response = client.chat.completions.create( model=\"meta-llama/Llama-3-70b-chat-hf\", messages=messages, temperature=0.3, max_tokens=50, stop=[\"\\n\"], stream=True )\n\nTo evaluate the performance of both GPT-4 and Llama 3, we used the same evaluation metrics as in the fine-tuning experiments. These metrics allow for a direct comparison between the LLM-based approaches and the fine-tuned Transformer baselines.\n\nBy evaluating GPT-4 and Llama 3 in ZSL and FSL settings, we aim to assess the potential of LLMs for hope speech and sarcasm detection without task-specific training. For the FSL setting, we have taken a sample of 10 per label in our prompt draft and predicted on the rest of the datasets for both languages. The results will provide insights into the strengths and limitations of these models and inform future research directions in this area. We expect that FSL will outperform ZSL, and that GPT-4 and Llama 3 will exhibit different strengths and weaknesses due to their architectural differences and training data. A comparison with the fine-tuned Transformer baselines will reveal the trade-offs between these two approaches and highlight the potential for combining them to achieve even better performance.",
      "page_start": 11,
      "page_end": 12
    },
    {
      "section_name": "Results",
      "text": "To evaluate the performance of both fine-tuned and prompt-based models on the PolyHope V2 dataset, we conducted experiments across binary and multiclass hope speech detection tasks in English and Spanish. The following tables present detailed results from two modeling strategies: (i) supervised fine-tuning of Pre-trained Language Models (PLMs), and (ii) prompt-based classification using large generative models in zero-shot and few-shot settings. Tables6 and 8 report the outcomes of PLM fine-tuning, while Tables  7  and 9  summarize the performance of GPT-4 and Llama-3 under prompt-based configurations. Each table includes weighted and macro-averaged precision, recall, F1-score, and overall accuracy to provide a comprehensive comparison across approaches, tasks, and languages.\n\nFine-tuning domain-specific transformers consistently delivers the strongest and most balanced performance across both languages (Tables  6, 8 ). In the binary task, English RoBERTa reaches a macro F 1 of 86.5%, while its Spanish counterpart   large-language-model baselines (Tables  7, 9 ) narrow the margin in the binary setting-GPT-4 with ten in-context demonstrations yields weighted F 1 scores of 77.9% (English) and 75.5% (Spanish) but still lag the best fine-tuned transformer by 8-9% and exhibit macro scores 5-9% lower than their own weighted figures, signalling over-reliance on the dominant Not-Hope class. Zero-shot prompting widens these disparities further, bringing five-fold macro F 1 down to 36-40% in both languages.\n\nOverall, the empirical evidence shows that supervised transformer adaptation not only yields higher aggregate accuracy (≈79% English, ≈73% Spanish, five-fold) but also preserves class balance, whereas few-and zero-shot prompting-while informative as a reasoning baseline-remains insufficient for the fine-grained distinctions mandated by the PolyHope V2 dataset. Several interacting factors explain the systematic gap observed in Tables  7 8 9 . First, the transformers were discriminative models explicitly fine-tuned on PolyHope tweets; gradient updates aligned their internal representations with the lexical idiosyncrasies of X/Twitter, the pragmatic cues that mark sarcasm, and the subtle distinctions among our three hope sub-types. GPT-4 and Llama-3, by contrast, were deployed in a zero-or few-shot generative regime: without parameter adaptation they relied on priors learned from broad, largely formal corpora where the boundary between \"realistic\" and \"unrealistic\" hope is seldom annotated. Second, the demonstration pool was deliberately kept minimal (ten examples per class) to probe \"out-of-the-box\" reasoning. This is dwarfed by the stylistic diversity and label noise of social-media text, so the LLMs struggle to infer robust class-conditional cues, particularly for the minority labels that constitute < 10% of either language's corpus.\n\nFine-tuned Pre-trained Language Models (PLMs), in contrast, see every training instance and therefore learn token-level features even for sparse categories. Third, prompt-based classification is highly sensitive to surface form: small edits in wording or label order can shift accuracy by more than 5%  [46] . Our study used a single handcrafted template to ensure comparability, but that template may sit at a sub-optimal point in the LLM's likelihood landscape. Supervised Pre-trained Language Models (PLMs) avoid this volatility by emitting logits directly over a fixed label vocabulary. Moreover, the generative output must be parsed back into a discrete class, introducing an additional failure channel for example, returning wishful thinking instead of Unrealistic Hope or attaching extraneous punctuation which the evaluation counts as error even when the underlying reasoning is partially correct.\n\nFinally, sarcastic hope often relies on culture specific pragmatic signals that differ between English and Spanish; without fine-tuning, the LLMs lack exposure to these bilingual nuances, reducing macro scores in both languages. Hence, the observed deficiencies do not necessarily imply limited reasoning capacity in GPT-4 or Llama 3; rather, they reflect domain mismatch, extremely small in-context support, prompt sensitivity, and generative-to-discriminative conversion overheads-constraints we accepted so that the LLM runs would serve as a clean baseline of unadapted reasoning on this complex multilingual taxonomy.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Discussion And Analysis",
      "text": "Figure  1  and 2 shows the multi-class confusion matrix between the best performing transfomer models. Figure  1  (English) and Figure  2  (Spanish) show that the best transformer already separates the two polar classes extremely well-Sarcasm is identified in more than 94% of cases in both languages, and Not-Hope achieves recalls of 85.4% (3,448/4,081) in English and 85.8% (8,395/9,788) in Spanish-yet the model falters when deciding which variety of sincere hope is being expressed. In English almost one quarter of all Generalized Hope tweets (295/2,335, 12.6%) are downgraded to Not-Hope, while the three hope sub-types frequently trade places with one another (e.g., 20.9% of Realistic Hope is predicted as Generalized, 11.7% of Unrealistic Hope as Realistic). The Spanish matrix magnifies the same phenomenon: although Generalized Hope still attains a respectable 72.8% recall (3,646/5,007), the model misroutes nearly one fifth of those instances to Not-Hope and, most strikingly, recognises fewer than half of the Realistic Hope tweets correctly (49.5%, 1,001/2,024), dispersing the remainder almost evenly across the other hope labels. These error patterns suggest that the network has learned robust cues for broad polarity and for pragmatic irony, but lacks the contextual or world-knowledge signals required to gauge how grounded a hopeful statement is, a limitation that grows sharper under the stronger class imbalance of the Spanish data.\n\nTo understand why the model stumbles on the three fine-grained hope labels, we manually inspected 385 mis-classified tweets from one English fold (≈ 20% of its 1,903 test items). Three recurrent patterns emerged:\n\n• Lexical \"hope\" triggers without genuine optimism. Many Not-Hope posts contain aspirational verbs in a negative frame, e.g. We yearn for change, but nothing ever improves\" (true Not-Hope, predicted Generalized Hope). The vocabulary cue yearn\" activates the hope prototype even though the surrounding discourse is pessimistic. • Implicit stance overrides explicit markers. Political commentary that affirms a group yet attacks its opponents is often annotated as Generalized Hope but judged by the model as Not-Hope. Example: \"#Woke people accuse conservatives of narrow-mindedness; I still believe we can find common ground\" (true Generalized Hope, predicted Not-Hope). The optimism is subtle, buried inside an adversarial sentence.\n\nFig.  1  The figure shows the confusion matrix of the best-performing transformer model (RoBERTa) in English.\n\nFig.  2  The figure shows the confusion matrix of the best-performing transformer model (RoBERTa) in Spanish.\n\n• Boundary fuzziness between hope sub-types. Tweets that express a concrete but weakly grounded desire swing between Generalized, Realistic, and Unrealistic Hope. For instance, Hey #USER, hope you'll read this on air tomorrow!\" (true Generalized Hope, predicted Realistic Hope) lacks external evidence and was annotated as Generalized, yet the model judged the request sufficiently specific to be Realistic. Conversely, progress-update posts such as Still trying to finish Chapter 5 before July; wish me luck\" (true Realistic Hope, predicted Generalized Hope) are grounded in an attainable goal but resemble generic motivational tweets in form.\n\nThese observations align with the quantitative error matrix: the classifier already recognises overt polarity and sarcastic cues, but its token-level heuristics cannot reliably infer the speaker's degree of realism. Future work should therefore incorporate temporal or evidential features-e.g. futurity markers, probability adverbs, or external knowledge about plausibility-to sharpen the distinctions among Generalized, Realistic, and Unrealistic Hope.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Conclusion",
      "text": "This paper introduces PolyHope V2, a novel bilingual dataset and modeling framework aimed at detecting fine-grained categories of hope speech-including sarcasm-across English and Spanish tweets. Our results demonstrate that fine-tuned transformer models consistently outperform zero-shot and few-shot large language models, particularly in maintaining class balance and accurately distinguishing among nuanced subtypes of hope. While LLMs like GPT-4 and Llama 3 show promise in binary classification tasks, their performance declines sharply in more granular multi-class settings due to their lack of domain adaptation and sensitivity to prompt design. The inclusion of sarcasm as a separate category further sharpens the challenge, revealing the limitations of current models in interpreting pragmatic cues. Our analysis underscores the need for richer annotations, cultural grounding, and external knowledge integration to advance the state of hope-speech detection. PolyHope V2 thus provides a strong empirical and methodological basis for future multilingual emotion detection research, bridging the gap between surface-level sentiment and deeper, context-aware emotional understanding.",
      "page_start": 17,
      "page_end": 17
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: and 2 shows the multi-class confusion matrix between the best performing",
      "page": 15
    },
    {
      "caption": "Figure 1: (English) and Figure 2 (Spanish) show that the best",
      "page": 15
    },
    {
      "caption": "Figure 1: The figure shows the confusion matrix of the best-performing transformer model (RoBERTa)",
      "page": 16
    },
    {
      "caption": "Figure 2: The figure shows the confusion matrix of the best-performing transformer model (RoBERTa)",
      "page": 16
    }
  ],
  "tables": [
    {
      "caption": "Table 1: SampleEnglishtextsfromnewproposedversionofPolyHopeV2withSarcasm.",
      "data": [
        {
          "Text": "#USER# You just can’t get out of your own way, can you?",
          "Label": "Not Hope"
        },
        {
          "Text": "Congratulations #USER# bro for completing, my allah god bless u ahead\na good and bright future inshallah... #URL#",
          "Label": "Generalized Hope"
        },
        {
          "Text": "#USER# damn man i’m here just tryna be a nice friend and dis is how u treat me\ni hope u choke with cold spaghetti",
          "Label": "Unrealistic Hope"
        },
        {
          "Text": "It was really hard exam but fortunately I studied a little last night and\nI expect to pass the test",
          "Label": "Realistic Hope"
        },
        {
          "Text": "Oh, I’m sure this meeting will solve all our problems!",
          "Label": "Sarcasm"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table 1: SampleEnglishtextsfromnewproposedversionofPolyHopeV2withSarcasm.",
      "data": [
        {
          "Text": "#USER# Que gente de m. . . sino saben no hablen.\nTengo mel. . . haya dado la teta a la melliza equivocada, entr el sue˜no y cans",
          "Label": "Not Hope"
        },
        {
          "Text": "#USER# Uy! Ojal´a. Pero tiene que ser m´as como el\nFight\nfor NY porque el Icon era asquerosamente horrible.",
          "Label": "Generalized Hope"
        },
        {
          "Text": "#USER# Aqu´ı en la Mora y Piedra Azul seguimos esperando\nla segunda fase de alumbrado p´ublico de sus calles #USER# #USER#",
          "Label": "Realistic Hope"
        },
        {
          "Text": "Todo el maldito d´ıa trabajando en una presentaci´on.\nAns´ıo el d´ıa en que deje de ser una corporate slave.",
          "Label": "Unrealistic Hope"
        },
        {
          "Text": "Si el sonido de la lluvia afuera no fuera ya suficientemente\nmalo. . .\nser´ıa el\nfondo perfecto para m´ı mientras intento\nestudiar.\n¡Maravilloso!",
          "Label": "Sarcasm"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table 1: SampleEnglishtextsfromnewproposedversionofPolyHopeV2withSarcasm.",
      "data": [
        {
          "Task": "Binary",
          "Labels": "Hope\nNot Hope",
          "English\nSpanish": "4,434\n9,654\n5,081\n10,788"
        },
        {
          "Task": "Multiclass",
          "Labels": "Not Hope\nSarcasm\nGeneralized Hope\nRealistic Hope\nUnrealistic Hope",
          "English\nSpanish": "4,081\n9,788\n1,259\n1259\n2,335\n5,007\n982\n2024\n858\n2364"
        }
      ],
      "page": 8
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "A deeper look into sarcastic tweets using deep convolutional neural networks",
      "authors": [
        "S Poria",
        "E Cambria",
        "D Hazarika",
        "P Vij"
      ],
      "year": "2016",
      "venue": "A deeper look into sarcastic tweets using deep convolutional neural networks",
      "arxiv": "arXiv:1610.08815"
    },
    {
      "citation_id": "2",
      "title": "New avenues in opinion mining and sentiment analysis",
      "authors": [
        "E Cambria",
        "B Schuller",
        "Y Xia",
        "C Havasi"
      ],
      "year": "2013",
      "venue": "IEEE Intelligent systems"
    },
    {
      "citation_id": "3",
      "title": "Emotion detection from text in learning environments: a review",
      "authors": [
        "M Bustos-López",
        "N Cruz-Ramírez",
        "A Guerra-Hernández",
        "L Sánchez-Morales",
        "G Alor-Hernández"
      ],
      "year": "2021",
      "venue": "New Perspectives on Enterprise Decision-Making Applying Artificial Intelligence Techniques"
    },
    {
      "citation_id": "4",
      "title": "Emotion recognition on social media using natural language processing (nlp) techniques",
      "authors": [
        "L Gomez",
        "T Watt",
        "K Babaagba",
        "C Chrysoulas",
        "A Homay",
        "R Rangarajan",
        "X Liu"
      ],
      "year": "2023",
      "venue": "Proceedings of the 2023 6th International Conference on Information Science and Systems"
    },
    {
      "citation_id": "5",
      "title": "Unveiling emotions: Nlp-based mood classification and well-being tracking for enhanced mental health awareness",
      "authors": [
        "A Mishra",
        "A Rai",
        "D Nandan",
        "U Kshirsagar",
        "M Singh"
      ],
      "year": "2025",
      "venue": "Mathematical Modelling of Engineering Problems"
    },
    {
      "citation_id": "6",
      "title": "Sentiment analysis: Automatically detecting valence, emotions, and other affectual states from text",
      "authors": [
        "S Mohammad"
      ],
      "year": "2021",
      "venue": "Emotion Measurement"
    },
    {
      "citation_id": "7",
      "title": "Emobank: Studying the impact of annotation perspective and representation format on dimensional emotion analysis",
      "authors": [
        "S Buechel",
        "U Hahn"
      ],
      "year": "2022",
      "venue": "Emobank: Studying the impact of annotation perspective and representation format on dimensional emotion analysis",
      "arxiv": "arXiv:2205.01996"
    },
    {
      "citation_id": "8",
      "title": "Semeval-2019 task 3: Emocontext contextual emotion detection in text",
      "authors": [
        "A Chatterjee",
        "K Narahari",
        "M Joshi",
        "P Agrawal"
      ],
      "year": "2019",
      "venue": "Proceedings of the 13th International Workshop on Semantic Evaluation"
    },
    {
      "citation_id": "9",
      "title": "Polyhope: Two-level hope speech detection from tweets",
      "authors": [
        "F Balouchzahi",
        "G Sidorov",
        "A Gelbukh"
      ],
      "year": "2023",
      "venue": "Expert Systems with Applications"
    },
    {
      "citation_id": "10",
      "title": "Trends and development in technology-enhanced adaptive/personalized learning: A systematic review of journal publications from",
      "authors": [
        "H Xie",
        "H.-C Chu",
        "G.-J Hwang",
        "C.-C Wang"
      ],
      "year": "2007",
      "venue": "Computers & Education"
    },
    {
      "citation_id": "11",
      "title": "Beyond lda: exploring supervised topic modeling for depression-related language in twitter",
      "authors": [
        "P Resnik",
        "W Armstrong",
        "L Claudino",
        "T Nguyen",
        "V.-A Nguyen",
        "J Boyd-Graber"
      ],
      "year": "2015",
      "venue": "Proceedings of the 2nd Workshop on Computational Linguistics and Clinical Psychology: from Linguistic Signal to Clinical Reality"
    },
    {
      "citation_id": "12",
      "title": "Analysis of expressions of hope and regret associated with nonmedical prescription drug use in x chatter",
      "authors": [
        "F Balouchzahi",
        "S Butt",
        "A Sarker",
        "A.-G Ma",
        "G Sidorov",
        "A Gelbukh"
      ],
      "venue": "Analysis of expressions of hope and regret associated with nonmedical prescription drug use in x chatter"
    },
    {
      "citation_id": "13",
      "title": "Emotion and Adaptation",
      "authors": [
        "R Lazarus"
      ],
      "year": "1991",
      "venue": "Emotion and Adaptation"
    },
    {
      "citation_id": "14",
      "title": "An argument for basic emotions",
      "authors": [
        "P Ekman"
      ],
      "year": "1992",
      "venue": "Cognition & emotion"
    },
    {
      "citation_id": "15",
      "title": "Sarcasm as contrast between a positive sentiment and negative situation",
      "authors": [
        "E Riloff",
        "A Qadir",
        "P Surve",
        "L De Silva",
        "N Gilbert",
        "R Huang"
      ],
      "year": "2013",
      "venue": "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "16",
      "title": "Sentiment analysis is a big suitcase",
      "authors": [
        "E Cambria",
        "S Poria",
        "A Gelbukh",
        "M Thelwall"
      ],
      "year": "2017",
      "venue": "IEEE Intelligent Systems"
    },
    {
      "citation_id": "17",
      "title": "Hope: A multilingual approach to identifying positive communication in social media",
      "authors": [
        "F Ullah",
        "M Zamir",
        "M Ahmad",
        "G Sidorov",
        "A Gelbukh"
      ],
      "year": "2024",
      "venue": "Colocated with the 40th Conference of the Spanish Society for Natural Language Processing (SEPLN 2024)"
    },
    {
      "citation_id": "18",
      "title": "Learning to identify emotions in text",
      "authors": [
        "C Strapparava",
        "R Mihalcea"
      ],
      "year": "2008",
      "venue": "Proceedings of the 2008 ACM Symposium on Applied Computing"
    },
    {
      "citation_id": "19",
      "title": "Hope speech detection: A computational analysis of the voice of peace",
      "authors": [
        "S Palakodety",
        "A Khudabukhsh",
        "J Carbonell"
      ],
      "year": "2020",
      "venue": "ECAI 2020"
    },
    {
      "citation_id": "20",
      "title": "Regret and hope on transformers: An analysis of transformers on regret and hope speech detection datasets",
      "authors": [
        "G Sidorov",
        "F Balouchzahi",
        "S Butt",
        "A Gelbukh"
      ],
      "year": "2023",
      "venue": "Applied Sciences"
    },
    {
      "citation_id": "21",
      "title": "Sentiment analysis: An overview from linguistics",
      "authors": [
        "M Taboada"
      ],
      "year": "2016",
      "venue": "Annual Review of Linguistics"
    },
    {
      "citation_id": "22",
      "title": "Hopeedi: A multilingual hope speech detection dataset for equality, diversity, and inclusion",
      "authors": [
        "B Chakravarthi"
      ],
      "year": "2020",
      "venue": "Proceedings of the Third Workshop on Computational Modeling of People's Opinions, Personality, and Emotion's in Social Media"
    },
    {
      "citation_id": "23",
      "title": "Overview of the shared task on hope speech detection for equality, diversity, and inclusion",
      "authors": [
        "B Chakravarthi",
        "V Muralidaran",
        "R Priyadharshini",
        "S Cn",
        "J Mccrae",
        "M García",
        "S Jiménez-Zafra",
        "R Valencia-García",
        "P Kumaresan",
        "R Ponnusamy"
      ],
      "year": "2022",
      "venue": "Proceedings of the Second Workshop on Language Technology for Equality, Diversity and Inclusion"
    },
    {
      "citation_id": "24",
      "title": "ssn dibertsity@ lt-edi-eacl2021: hope speech detection on multilingual youtube comments via transformer based approach",
      "authors": [
        "S Arunima",
        "A Ramakrishnan",
        "A Balaji",
        "D Thenmozhi"
      ],
      "year": "2021",
      "venue": "Proceedings of the First Workshop on Language Technology for Equality, Diversity and Inclusion"
    },
    {
      "citation_id": "25",
      "title": "Urduhope: Analysis of hope and hopelessness in urdu texts",
      "authors": [
        "F Balouchzahi",
        "S Butt",
        "M Amjad",
        "G Sidorov",
        "A Gelbukh"
      ],
      "year": "2025",
      "venue": "Knowledge-Based Systems"
    },
    {
      "citation_id": "26",
      "title": "Vel@ iberlef 2024: Hope speech detection in spanish social media comments using bert pre-trained model",
      "authors": [
        "K Ponnusamy",
        "M Vegupatti",
        "P Kumaresan",
        "R Priyadharshini",
        "P Buitelaar",
        "B Chakravarthi"
      ],
      "year": "2024",
      "venue": "Proceedings of the Iberian Languages Evaluation Forum (IberLEF 2024), Co-located with the 40th Conference of the Spanish Society for Natural Language Processing (SEPLN 2024)"
    },
    {
      "citation_id": "27",
      "title": "Multilingual hope speech detection in english and dravidian languages",
      "authors": [
        "B Chakravarthi"
      ],
      "year": "2022",
      "venue": "International Journal of Data Science and Analytics"
    },
    {
      "citation_id": "28",
      "title": "Multilingual hope speech detection: A robust framework using transfer learning of fine-tuning roberta model",
      "authors": [
        "M Malik",
        "A Nazarova",
        "M Jamjoom",
        "D Ignatov"
      ],
      "year": "2023",
      "venue": "Journal of King Saud University-Computer and Information Sciences"
    },
    {
      "citation_id": "29",
      "title": "Cross-lingual transfer of multilingual models on low resource african languages",
      "authors": [
        "H Thangaraj",
        "A Chenat",
        "J Walia",
        "V Marivate"
      ],
      "year": "2024",
      "venue": "Cross-lingual transfer of multilingual models on low resource african languages",
      "arxiv": "arXiv:2409.10965"
    },
    {
      "citation_id": "30",
      "title": "Hope speech detection in spanish: The lgbt case",
      "authors": [
        "D García-Baena",
        "M García-Cumbreras",
        "S Jiménez-Zafra",
        "J García-Díaz",
        "R Valencia-García"
      ],
      "year": "2023",
      "venue": "Language Resources and Evaluation"
    },
    {
      "citation_id": "31",
      "title": "Overview of hope at iberlef 2024: Approaching hope speech detection in social media from two perspectives, for equality, diversity and inclusion and as expectations",
      "authors": [
        "D García-Baena",
        "F Balouchzahi",
        "S Butt",
        "M García-Cumbreras",
        "A Tonja",
        "J García-Díaz",
        "S Bozkurt",
        "B Chakravarthi",
        "H Ceballos",
        "R Valencia-García"
      ],
      "year": "2024",
      "venue": "Overview of hope at iberlef 2024: Approaching hope speech detection in social media from two perspectives, for equality, diversity and inclusion and as expectations"
    },
    {
      "citation_id": "32",
      "title": "Bonghope: An annotated corpus for bengali hope speech detection",
      "authors": [
        "T Nath",
        "V Singh",
        "V Gupta"
      ],
      "year": "2023",
      "venue": "Bonghope: An annotated corpus for bengali hope speech detection"
    },
    {
      "citation_id": "33",
      "title": "Overview of hope at iberlef 2023: Multilingual hope speech detection",
      "authors": [
        "S Jiménez-Zafra",
        "M Garcia-Cumbreras",
        "D García-Baena",
        "J Garcia-Díaz",
        "B Chakravarthi",
        "R Valencia-García",
        "L Ureña-López"
      ],
      "year": "2023",
      "venue": "Procesamiento del lenguaje natural"
    },
    {
      "citation_id": "34",
      "title": "Overview of the hasoc track at fire 2020: Hate speech and offensive language identification in tamil, malayalam, hindi, english and german",
      "authors": [
        "T Mandl",
        "S Modha",
        "M Kumar",
        "A Chakravarthi"
      ],
      "year": "2020",
      "venue": "Proceedings of the 12th Annual Meeting of the Forum for Information Retrieval Evaluation"
    },
    {
      "citation_id": "35",
      "title": "Zeroshot cross-lingual content filtering: Offensive language and hate speech detection",
      "authors": [
        "A Pelicon",
        "R Shekhar",
        "M Martinc",
        "B Škrlj",
        "M Purver",
        "S Pollak"
      ],
      "year": "2021",
      "venue": "Proceedings of the EACL Hackashop on News Media Content Analysis and Automated Report Generation"
    },
    {
      "citation_id": "36",
      "title": "A novel hierarchical bert architecture for sarcasm detection",
      "authors": [
        "H Srivastava",
        "V Varshney",
        "S Kumari",
        "S Srivastava"
      ],
      "year": "2020",
      "venue": "Proceedings of the Second Workshop on Figurative Language Processing"
    },
    {
      "citation_id": "37",
      "title": "Sentiment and emotion help sarcasm? a multi-task learning framework for multi-modal sarcasm, sentiment and emotion analysis",
      "authors": [
        "D Chauhan",
        "S Dhanush",
        "A Ekbal",
        "P Bhattacharyya"
      ],
      "year": "2020",
      "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "38",
      "title": "Debiasing multimodal sarcasm detection with contrastive learning",
      "authors": [
        "M Jia",
        "C Xie",
        "L Jing"
      ],
      "year": "2024",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "39",
      "title": "Transformer-based context-aware sarcasm detection in conversation threads from social media",
      "authors": [
        "X Dong",
        "C Li",
        "J Choi"
      ],
      "year": "2020",
      "venue": "Proceedings of the Second Workshop on Figurative Language Processing"
    },
    {
      "citation_id": "40",
      "title": "Mindhope: Multilingual identification of nuanced dimensions of hope",
      "authors": [
        "G Sidorov",
        "F Balouchzahi",
        "L Ramos",
        "H Gómez-Adorno",
        "A Gelbukh"
      ],
      "year": "2024",
      "venue": "Mindhope: Multilingual identification of nuanced dimensions of hope"
    },
    {
      "citation_id": "41",
      "title": "Semeval-2022 task 6: isarcasmeval, intended sarcasm detection in english and arabic",
      "authors": [
        "I Farha",
        "S Oprea",
        "S Wilson",
        "W Magdy"
      ],
      "year": "2022",
      "venue": "The 16th International Workshop on Semantic Evaluation"
    },
    {
      "citation_id": "42",
      "title": "Creating and characterizing a diverse corpus of sarcasm in dialogue",
      "authors": [
        "S Oraby",
        "V Harrison",
        "L Reed",
        "E Hernandez",
        "E Riloff",
        "M Walker"
      ],
      "year": "2016",
      "venue": "Proceedings of the 17th Annual Meeting of the Special Interest Group on Discourse and Dialogue"
    },
    {
      "citation_id": "43",
      "title": "Gpt-4 technical report",
      "authors": [
        "J Achiam",
        "S Adler",
        "S Agarwal",
        "L Ahmad",
        "I Akkaya",
        "F Aleman",
        "D Almeida",
        "J Altenschmidt",
        "S Altman",
        "S Anadkat"
      ],
      "year": "2023",
      "venue": "Gpt-4 technical report",
      "arxiv": "arXiv:2303.08774"
    },
    {
      "citation_id": "44",
      "title": "Sparks of artificial general intelligence: Early experiments with gpt",
      "authors": [
        "S Bubeck",
        "R Eldan",
        "J Gehrke",
        "E Horvitz",
        "E Kamar",
        "P Lee",
        "Y Lee",
        "Y Li",
        "S Lundberg"
      ],
      "year": "2023",
      "venue": "Sparks of artificial general intelligence: Early experiments with gpt"
    },
    {
      "citation_id": "45",
      "title": "The llama 3 herd of models",
      "authors": [
        "A Grattafiori",
        "A Dubey",
        "A Jauhri",
        "A Pandey",
        "A Kadian",
        "A Al-Dahle",
        "A Letman",
        "A Mathur",
        "A Schelten",
        "A Vaughan"
      ],
      "year": "2024",
      "venue": "The llama 3 herd of models",
      "arxiv": "arXiv:2407.21783"
    },
    {
      "citation_id": "46",
      "title": "Large language models are human-level prompt engineers",
      "authors": [
        "Y Zhou",
        "A Muresanu",
        "Z Han",
        "K Paster",
        "S Pitis",
        "H Chan",
        "J Ba"
      ],
      "year": "2022",
      "venue": "The Eleventh International Conference on Learning Representations"
    }
  ]
}