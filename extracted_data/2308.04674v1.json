{
  "paper_id": "2308.04674v1",
  "title": "Addressing Racial Bias In Facial Emotion Recognition",
  "published": "2023-08-09T03:03:35Z",
  "authors": [
    "Alex Fan",
    "Xingshuo Xiao",
    "Peter Washington"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Fairness in deep learning models trained with high-dimensional inputs and subjective labels remains a complex and understudied area. Facial emotion recognition, a domain where datasets are often racially imbalanced, can lead to models that yield disparate outcomes across racial groups. This study focuses on analyzing racial bias by subsampling training sets with varied racial distributions and assessing test performance across these simulations. Our findings indicate that smaller datasets with posed faces improve on both fairness and performance metrics as the simulations approach racial balance. Notably, the F1-score increases by 27.2% points, and demographic parity increases by 15.7% points on average across the simulations. However, in larger datasets with greater facial variation, fairness metrics generally remain constant, suggesting that racial balance by itself is insufficient to achieve parity in test performance across different racial groups.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Emotion recognition, commonly referred to as facial expression recognition (FER), encompasses the identification and analysis of facial expressions displayed by individuals in images or videos. This complex procedure consists of three key stages: face detection, feature extraction, and emotion classification.  (Ko, 2018) . FER finds extensive utility across various domains including human-computer interaction (HCI)  (Picard, 1999) , media analytics  (Zhao et al., 2019) , robotics  (Tao & Tan, 2005) , and health informatics  (Voss et al., 2019; Washington et al., 2022) .\n\nHistorically, automatic emotion recognition predominantly relied on the extraction of domain-specific features such as facial action units  (Hamm et al., 2011) . However, with the rapid progression of machine learning (ML) and deep learning (DL) techniques, deep neural networks (DNNs) have emerged as a prominent approach for developing facial emotion recognition models. Such models necessitate expansive datasets to ensure robustness and accuracy in their predictions. In recent years, researchers have proposed DL models which leverage more expansive datasets for training  (Li & Deng, 2018) . Despite the impressive achievements of deep learning methods in FER, a significant challenge arises from the presence of racial bias in such models. This issue, which is well documented in existing literature  (Chen & Joo, 2021b; Domnich & Anbarjafari, 2021; Sham et al., 2023b; Xu et al., 2020b) , necessitates immediate attention to mitigate discriminatory outcomes and to provide equitable opportunities for individuals of diverse ethnicities and skin colors. Addressing and rectifying the biases within DNNs is of paramount importance for real-world translation of such models.\n\nBiases within DNNs can primarily be attributed to two fundamental sources: the training data and the algorithms themselves  (Mehrabi et al., 2022b) . Given that models learn from input data, any biases present within the underlying datasets are inherently ingrained within the learning process of the algorithms. Furthermore, the design of feature extraction processes for these models may introduce biases that disproportionately affect different racial groups. An illustrative example is the consideration of skin color as a learned feature extracted during the deep learning process, which can ultimately lead to unfair predictions.\n\nTo address the issue of racial bias in FER datasets, we conduct a simulation study on AffectNet and CAFE datasets  (Mollahosseini et al., 2017; LoBue & Thrasher, 2015) . In each simulation, we select a specific race as the simulated race, and ensure other races equal representations. We train the FER model using sub-sampled data with varying proportions of the simulated race, measuring the accuracy, F1-scores, and fairness metrics. We find that the racial balance of the training set has some influence on the test race-specific F1-score, but mitigating balance alone is insufficient to address other types of bias such as annotator bias.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Related Works",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Deep-Learning-Based Emotion Recognition",
      "text": "Deep learning models serve as the foundation for each stage of FER  (Ko, 2018) . Training DNNs for FER requires the utilization of diverse datasets, encompassing varying numbers of labels and data types  (Mollahosseini et al., 2017; LoBue & Thrasher, 2015; Lucey et al., 2010; Lyons et al., 1998; Li et al., 2017; Goodfellow et al., 2013) . Li and Deng provided an overview of popular datasets designed specifically for deep emotion recognition (2018).\n\nIn the realm of deep-learning-based FER approaches, convolutional neural networks (CNNs) and recurrent neural networks (RNNs) are frequently employed.  Khorrami et al. demonstrated  that CNNs excel at accurately extracting facial action units (FAUs), thereby yielding promising classification performance on the CK+ dataset (2015). Lopes et al. proposed a combination of CNNs and image pre-processing techniques, such as cropping and normalization, to reduce the number of convolutional layers and alleviate the need for extensive training data. This approach resulted in improved overall accuracy and computational efficiency on the CK+, JAFFE, and BU3DFE datasets (2017). Agrawal and Mittal introduced a novel CNN model that investigated the influence of kernel size and the number of filters on the final classification accuracy, leading to further improvements in performance (2020). Recent studies have also explored the integration of generative adversarial networks (GANs) within CNNs for data augmentation and training purposes  (Peng et al., 2020; Porcu et al., 2020) .\n\nIncorporating CNNs with RNNs, Zhu et al. proposed a novel FER approach that integrates features learned from each layer of a CNN within a bidirectional  RNN architecture (2017) . RNNs have also proven effective in capturing dynamic facial actions within multimodal FER, as demonstrated by  Majumder et al. (Majumder et al., 2019; Hasani & Mahoor, 2017) .\n\nDespite achieving high overall accuracy, the performance across different racial groups has yet to be thoroughly and systematically studied. While multicultural FER models have demonstrated their effectiveness in improving accuracy, their impact on fairness has yet to be explicitly addressed  (Sohail et al., 2022; Ali et al., 2016) .",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Fairness In Machine Learning",
      "text": "Numerous research endeavors have been dedicated to addressing issues of unfairness within ML systems  (Du et al., 2020; Mehrabi et al., 2022b; Oneto & Chiappa, 2020; Mehrabi et al., 2022a) . The underlying causes of biases in ML have been explored in several publications  (Chouldechova & Roth, 2018; Martínez-Plumed et al., 2019) . Evaluation metrics, shaped by social contexts, are employed to gauge the fairness of these models  (Chouldechova & Roth, 2018; Castelnovo et al., 2021) .\n\nML fairness improvement methods are generally categorized into pre-processing, in-processing, and postprocessing techniques, contingent upon the stage at which the fairness correction method is applied  (Du et al., 2020; Mehrabi et al., 2022b; Oneto & Chiappa, 2020; Mehrabi et al., 2022a) . Additionally, AI fairness toolkits have been developed, harnessing these methods  (Bird et al., 2020; Bellamy et al., 2018; Wexler et al., 2020; Saleiro et al., 2018) .\n\nSeveral prior studies have delved into the examination and alleviation of racial bias in FER. Raina et al. utilized artificial facial images and observed racial bias in FER models (2022). Sham et al. conducted an investigation into racial bias in popular state-of-the-art FER methods and revealed that the presence of uneven or insufficient representation within the training data leads to biased performance outcomes (2023a).\n\nAdditionally, biases in expression labeling within datasets, influenced by the impact of races on emotion perceptions, was identified as contributors to unfairness  (Rhue, 2018; Chen & Joo, 2021a) . Conversely, Chen and Joo's study did not report any systematic labeling biases for races, attributing the absence to imbalanced racial representations in the dataset (2021a).\n\nAlthough some methods have demonstrated effectiveness in correcting FER racial bias  (Xu et al., 2020a) , the results highly depend on the datasets and models. Due to the involvement of highly subjective labels and high-dimensional inputs in emotion recognition, the field has yet to address the issue of fairness comprehensively in this domain and similar areas with complex and heterogeneous data streams.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Methods",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Datasets",
      "text": "We employ two datasets to investigate racial bias. The first dataset is the Child Affective Facial Expression (CAFE) dataset, a collection of images featuring children posing specific emotions  (LoBue & Thrasher, 2015) . The second dataset is AffectNet, a widely recognized large-scale dataset for general facial emotion recognition  (Mollahosseini et al., 2017) .\n\nTo align the datasets, we filter the examples within AffectNet to include only those with emotion labels matching those in the CAFE dataset, specifically neutral, sadness, happiness, surprise, anger, disgust, and fear. Additionally, we exclude grayscale images, which also contributes to more accurate race estimates. We calculate the per-pixel squared error (summed across the three channels) using the mean pixel value of each image. Images with an average per-pixel To separate the faces in CAFE from their white background, we utilize OpenCV bounding boxes, following a methodology that AffectNet uses during its data collection process  (Mollahosseini et al., 2017; Bradski, 2000) . We exclude images in which a face could not be adequately bounded, resulting in 1,178 usable images. Because the CAFE dataset involves participants posing for multiple emotions, we opt to split the data at the participant level when generating the training, validation, and test sets, resulting in sizes of N = 713, 227, and 222, respectively.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Race Estimates",
      "text": "We require race labels to analyze the bias. CAFE provides participants' self-reported ground truth race labels: European-American, African-American, (East) Asian, Latino, and South Asian. However, for the AffectNet dataset, race information is not available; we approach this problem by estimating race labels. We utilize models trained on labeled race datasets, specifically FairFace, which exhibits greater racial balance compared to similar datasets, and models trained on FairFace demonstrate improved per-formance on non-white faces relative to other models and datasets  (Karkkainen & Joo, 2021) . Our model uses the paper's original weights to predict the race labels, and the counts and proportions of the two datasets are presented in Tables  1  and 2 . As expected, European-American faces make up the majority of the training set distributions for both CAFE and AffectNet, comprising 40.4% and 67.3% of their respective datasets.\n\nThe FairFace-based model categorizes the AffectNet faces into seven categories: European-American, African-American, (East) Asian, Latino, South Asian, Middle Eastern, and Southeast Asian. It is possible to exclude the Middle Eastern and Southeast Asian groups, from the Affect-Net experiments to align the racial categories with those of CAFE. However, we choose to retain all races in the experiments assuming that there could be latent information from these additional race categories that affects the training process.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Simulating Racial Composition",
      "text": "To investigate the impact of racial representation in the training set on test performance, we select a specific race (henceforth referred to as the simulated race) and vary their proportion. The sampling process ensures an equal representation for non-simulated races by sampling N examples for each race. Then we set a ratio (R) of the simulated race to a non-simulated race and sample N * R examples of the simulated race. All sampling is done without replacement. We analyze the effects of under-representation and over-representation of the simulated race by varying the ratio.\n\nIn the simulations, we fine-tune a ResNet-50 model on the sub-sampled training set. The model's performance is evaluated on the validation set throughout the epochs, and the weights yielding the highest accuracy on the validation set are used to evaluate on the test set. While accuracy serves as one evaluation metric, we also consider the racespecific F1-score, which is calculated after filtering to the simulated race. Additionally, we explore fairness metrics and extend their applicability to multi-class classification problems.\n\nDemographic parity and equality of odds are two fairness principles that are commonly used in the algorithmic fairness literature  (Barocas et al., 2019) . Demographic parity ensures that the positive prediction rate remains consistent across sensitive attributes. One approach to quantify this principle is the use of the ratio of the smallest positive prediction rate to the largest. A value of one suggests that the model achieves demographic parity  (Bird et al., 2020) . To implement this metric, we transform the multi-class problem into multiple one-versus-rest sub-problems. For each emotion, we compute the demographic parity ratio across the different races and then average these ratios to obtain an overall measure of demographic parity.\n\nEquality of odds requires parity in both true-positive and false-positive rates across sensitive attributes. A similar procedure to that used for the demographic parity ratio can be employed to derive two separate ratios: the true-positive parity ratio and the false-positive parity ratio  (Bird et al., 2020) . The equalized odds ratio is determined by selecting the smaller of the two ratios. This indicates that equality of odds is achieved when both parity ratios closely approximate one.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Experiments",
      "text": "We conduct three sets of simulations in our study. The first simulation focuses on the CAFE dataset, which we sample at the participant level. Given the limited size of the training set, we set the number of participants N to be 5 and the ratio R ranges from 0 to 2.0 with increments of 0.2. This approach ensures that each simulation comprises a whole number of participants, and the sizes of each non-simulated races are approximately equal, with 40-50 observations per racial group.\n\nThe second simulation involves the AffectNet dataset with N set to 50 observations, and R follows the same range and increments as in the previous simulation. This simulation tests the consistency of results between AffectNet and CAFE in the context of a small dataset size regime.\n\nIn the third simulation, we use the AffectNet dataset with N set to 3500 observations, allowing for a larger training set. This simulation investigates whether the trends observed in the first and second simulations generalize to larger data regimes, which is relevant for the typical application of facial emotion recognition.\n\nThroughout the experiments, we maintain consistent training hyperparameters. We use a fixed learning rate of 1e -4 with an Adam optimizer (with β 1 = 0.9 and β 2 = 0.999) and L2-weight decay on the model parameters. Crossentropy loss is backpropagated at each batch step, and each simulation undergoes training for 5 epochs. To account for the emotion label imbalance within the sub-sampled dataset, we apply a weight to the loss function based on the number of ground truth labels. These experiments were conducted using an NVIDIA K80 GPU unit.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Results",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Cafe Simulation",
      "text": "We display metrics from various levels of R in four race simulations: African-American, East Asian, European-American, and Latino. We calculate weighted F1-score and accuracy at a race level by filtering the predictions to the simulated race. We calculate demographic parity ratio and equalized odds ratio without any filters. Our hypothesis anticipates that race-specific F1-score and fairness metrics would improve as the dataset becomes more racially balanced. However, as the simulations over-sample and the simulated race becomes overrepresented, we expect fairness metrics to plateau or decline, as the model's fairness performance for the non-simulated races is likely to deteriorate.\n\nThe simulations conducted on the CAFE dataset align with expectations on some metrics. Figure  2  shows that the F1score and the demographic parity ratio increase (+27.2% and +15.7% points respectively on average) as the dataset becomes balanced and stabilize when the dataset oversamples the simulated race. On the other hand, the equalized odds ratio exhibits greater inconsistency, with only the Latino simulations displaying a clear upward trend, while the other races exhibit random or downward trends.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "European-American Latino",
      "text": "African-American East Asian Additionally, in Figure  3  we present the disaggregated F1scores for each race and label before their aggregation. The disaggregated results reveal that a significant portion of the F1-score improvement stems from emotions such as neutral, sad, and fear. Interpreting changes in East Asian F1-scores proves challenging, possibly due to the limited presence of Asian participants in the test set. Moreover, surprise and disgust appear to be more challenging emotions to predict, which could explain the seemingly random or marginal trends observed.\n\nAffectNet Small Simulation The performance on the small sub-sample of AffectNet, achieving 15.2% racespecific F1-score and 0.286 demographic parity ratio on average, is noticeably inferior to the CAFE simulations.\n\nThe limited size of the training set and the substantially greater variation in emotion distribution from the \"wild\" images in AffectNet likely contribute to this discrepancy.\n\nDespite the potential for model overfitting, the overall trend shown in Figure  4  indicates that the model's performance does not significantly change as the dataset becomes more racially balanced, as evidenced by the nearly random trends observed in the F1-score and fairness metrics.\n\nAffectNet Large Simulation The performance on the larger AffectNet simulation, although overfitting less, does not exhibit increases in F1-score and fairness metrics as the dataset becomes racially balanced. Figure  5  shows the race-specific F1-score fluctuating around 55% on average for all race simulations with no visible trends. Both demographic parity ratio and equalized odds ratio also stay roughly constant or even trend downwards.\n\nFurthermore, the unaggregated F1-scores demonstrate minimal variation, with the exception of anger, which experiences a temporary improvement when the Asian simulations are over-sampled.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Discussion",
      "text": "The simulations conducted on CAFE indicate that racial balance of the training set has some influence on the test race-specific F1-scores. However, only certain fairness metric trends align with our expectations. Demographic parity ratio increases as the training set becomes balanced and then plateaus, but equalized odds ratio, a stricter metric, shows random trends through different racial distribution simulations.\n\nThe AffectNet simulations exhibit mostly random trends across scenarios. Nevertheless, even for CAFE, biases persist within the models since the fairness metrics fail to approach a value of 1. This suggests that there may be sources of bias that the simulations are unable to capture. One possible explanation could be biased race estimations obtained from the FairFace model. Upon examining a small sample of faces categorized by this model, we observe that White, Black, Latino, and East Asian faces are estimated accurately.\n\nOther race and ethnicity categories, however, appear to be more prone to misclassification, particularly Middle Eastern faces based on lighter skin tones and Indian faces based on darker skin tones. This bias could potentially impact the training process even though the simulations focus on races estimated with reasonable accuracy. However, since the race labels for AffectNet are not available, it is challenging to ascertain the extent of estimator bias. As part of future extensions, it may be worthwhile to exclude the less-consistent racial categories to explore whether the estimator bias contributes to the lack of trends in the AffectNet simulation.\n\nAnother source of bias that cannot be addressed through the simulation of racial compositions is annotation bias. There is evidence in the psychology literature suggesting that individuals are less accurate in determining facial expressions for races different from their own, resulting in potentially disproportionate labeling biases (Zhongqing  Jiang et al., 2023; Chen & Joo, 2021a) . Annotation bias could be particularly problematic for AffectNet since its annotations were derived from only 12 labelers, with most labels being annotated by a single individual 1    (Mollahosseini et al., 2017) .\n\nIt would be possible to quantify and analyze this bias by collecting data on each observation, the race of the labelers, and the annotation process. CAFE, in fact, provides information about the aggregate race distribution of the labelers  (LoBue & Thrasher, 2015) , and this information has been incorporated into the training process in prior work  (Washington et al., 2021) .\n\nGiven the persistence of these non-compositional biases within AffectNet, traditional bias mitigation techniques like loss re-weighting or fairness regularization may not yield strong results. This underscores the need for the fairness community to explore alternative methods of bias mitigation, particularly in settings involving high-dimensional image  inputs and subjective labels.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Conclusion",
      "text": "There is an ongoing need for addressing fairness in facial emotion recognition. By simulating different racial distributions within the training sets, we demonstrate the impact of compositional racial imbalance on test performance and disparities in the CAFE dataset. Moreover, we extend this analysis to AffectNet, which comprises non-posed, \"in the wild\" expressions. The results reveal the persistent presence of bias across simulations of racial composition in the training set, with no improvement observed in the performance of race-specific F1-scores even when enforcing racial balance within the simulation. To further advance research in this domain, we propose exploring additional avenues of inquiry, such as re-simulating the AffectNet experiments while excluding racial groups that are inaccurately estimated during the pre-processing stage.",
      "page_start": 7,
      "page_end": 7
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Example of the procedure for simulating East Asian representation in a dataset. This represents a contrived case where East",
      "page": 4
    },
    {
      "caption": "Figure 2: shows that the F1-",
      "page": 4
    },
    {
      "caption": "Figure 2: CAFE racial composition simulations with all test met-",
      "page": 5
    },
    {
      "caption": "Figure 3: CAFE racial composition simulations with unaggre-",
      "page": 5
    },
    {
      "caption": "Figure 4: AffectNet racial composition simulations using small",
      "page": 5
    },
    {
      "caption": "Figure 3: we present the disaggregated F1-",
      "page": 5
    },
    {
      "caption": "Figure 4: indicates that the model’s performance",
      "page": 5
    },
    {
      "caption": "Figure 5: shows the",
      "page": 5
    },
    {
      "caption": "Figure 5: AffectNet racial composition simulations using larger",
      "page": 6
    },
    {
      "caption": "Figure 6: The unaggregated F1-scores for the AffectNet simula-",
      "page": 6
    },
    {
      "caption": "Figure 7: A random sample of image/race examples from the Fair-",
      "page": 7
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Original Training Distri\nEuropean-American 3": "African American 2",
          "bution\n(37.5%)": "(25%)"
        },
        {
          "Original Training Distri\nEuropean-American 3": "East Asian 2",
          "bution\n(37.5%)": "(25%)"
        },
        {
          "Original Training Distri\nEuropean-American 3": "South Asian 1",
          "bution\n(37.5%)": "(12.5%)"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Original Training Distri\nEuropean-American 1": "African American 1",
          "bution\n(20%)": "(20%)"
        },
        {
          "Original Training Distri\nEuropean-American 1": "East Asian 2",
          "bution\n(20%)": "(40%)"
        },
        {
          "Original Training Distri\nEuropean-American 1": "South Asian 1",
          "bution\n(20%)": "(20%)"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Simulated Race (SR) E. Asian": "Samples per Non-SR (N)",
          "Column_2": "1"
        },
        {
          "Simulated Race (SR) E. Asian": "Ratio of SR to Non-SR (R)",
          "Column_2": "2"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "African−American": "European−American",
          "Column_2": "",
          "Column_3": "",
          "Column_4": "",
          "Column_5": "",
          "Column_6": "",
          "Column_7": "",
          "Column_8": "",
          "Column_9": "",
          "Column_10": "",
          "East Asian": "Latino",
          "Column_12": "",
          "Column_13": "",
          "Column_14": "",
          "Column_15": "",
          "Column_16": "",
          "Column_17": "",
          "Column_18": "",
          "Column_19": "",
          "Column_20": ""
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "African−American": "European−American",
          "Column_2": "",
          "Column_3": "",
          "Column_4": "",
          "Column_5": "",
          "Column_6": "",
          "Column_7": "",
          "East Asian": "Latino",
          "Column_9": "",
          "Column_10": "",
          "Column_11": "",
          "Column_12": "",
          "Column_13": "",
          "Column_14": "",
          "Column_15": ""
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Angry Disgust Fear\n1.00\n0.75\n0.50\n0.25\n0.00\nHappy Neutral Sad\n1.00 race\n0.75 erocS−1F African−American\n0.50 East Asian\n0.25 European−American\n0.00 Latino\nSurprise 0.0 0.5 1.0 1.5 2.00.0 0.5 1.0 1.5 2.0\n1.00\n0.75\n0.50\n0.25\n0.00\n0.0 0.5 1.0 1.5 2.0\nSimulation−Race to Non−Simulation−Race Ratio": "",
          "Column_2": "race\nAfrican−American\nEast Asian"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Angry": "Happy",
          "Column_2": "",
          "Column_3": "",
          "Column_4": "",
          "Column_5": "",
          "Column_6": "",
          "Column_7": "",
          "Disgust": "Neutral",
          "Column_9": "",
          "Column_10": "",
          "Column_11": "",
          "Column_12": "",
          "Column_13": "",
          "Column_14": "",
          "Column_15": "",
          "Column_16": "",
          "Column_17": "",
          "Column_18": ""
        },
        {
          "Angry": "Surprise",
          "Column_2": "",
          "Column_3": "",
          "Column_4": "",
          "Column_5": "",
          "Column_6": "",
          "Column_7": "",
          "Disgust": "",
          "Column_9": "",
          "Column_10": "",
          "Column_11": "",
          "Column_12": "",
          "Column_13": "",
          "Column_14": "",
          "Column_15": "",
          "Column_16": "",
          "Column_17": "",
          "Column_18": ""
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "African−American": "European−American",
          "Column_2": "",
          "Column_3": "",
          "Column_4": "",
          "Column_5": "",
          "Column_6": "",
          "Column_7": "",
          "Column_8": "",
          "Column_9": "",
          "Column_10": "",
          "East Asian": "Latino",
          "Column_12": "",
          "Column_13": "",
          "Column_14": "",
          "Column_15": "",
          "Column_16": "",
          "Column_17": "",
          "Column_18": "",
          "Column_19": "",
          "Column_20": ""
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Angry": "Happy",
          "Column_2": "",
          "Column_3": "",
          "Column_4": "",
          "Column_5": "",
          "Column_6": "",
          "Column_7": "",
          "Column_8": "",
          "Disgust": "Neutral",
          "Column_10": "",
          "Column_11": "",
          "Column_12": "",
          "Column_13": "",
          "Column_14": "",
          "Column_15": "",
          "Column_16": "",
          "Fear": "Sad",
          "Column_18": "",
          "Column_19": "",
          "Column_20": "",
          "Column_21": "",
          "Column_22": "",
          "Column_23": "",
          "Column_24": ""
        },
        {
          "Angry": "Surprise",
          "Column_2": "",
          "Column_3": "",
          "Column_4": "",
          "Column_5": "",
          "Column_6": "",
          "Column_7": "",
          "Column_8": "",
          "Disgust": "",
          "Column_10": "",
          "Column_11": "",
          "Column_12": "",
          "Column_13": "",
          "Column_14": "",
          "Column_15": "",
          "Column_16": "",
          "Fear": "",
          "Column_18": "",
          "Column_19": "",
          "Column_20": "",
          "Column_21": "",
          "Column_22": "",
          "Column_23": "",
          "Column_24": ""
        }
      ],
      "page": 6
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Using cnn for facial expression recognition: a study of the effects of kernel size and number of filters on accuracy",
      "authors": [
        "A Agrawal",
        "N Mittal"
      ],
      "year": "2020",
      "venue": "The Visual Computer"
    },
    {
      "citation_id": "2",
      "title": "Boosted nne collections for multicultural facial expression recognition",
      "authors": [
        "G Ali",
        "M Iqbal",
        "T.-S Choi"
      ],
      "year": "2016",
      "venue": "Pattern Recognition"
    },
    {
      "citation_id": "3",
      "title": "Limitations and Opportunities. fairmlbook.org",
      "authors": [
        "S Barocas",
        "M Hardt",
        "A Narayanan",
        "Machine Fairness",
        "Learning"
      ],
      "year": "2019",
      "venue": "Limitations and Opportunities. fairmlbook.org"
    },
    {
      "citation_id": "4",
      "title": "AI fairness 360: An extensible toolkit for detecting, understanding, and mitigating unwanted algorithmic bias",
      "authors": [
        "R Bellamy",
        "K Dey",
        "M Hind",
        "S Hoffman"
      ],
      "year": "2018",
      "venue": "AI fairness 360: An extensible toolkit for detecting, understanding, and mitigating unwanted algorithmic bias"
    },
    {
      "citation_id": "5",
      "title": "Fairlearn: A toolkit for assessing and improving fairness in AI",
      "authors": [
        "S Bird",
        "M Dudík",
        "R Edgar",
        "B Horn",
        "R Lutz",
        "V Milan",
        "M Sameki",
        "H Wallach",
        "K Walker"
      ],
      "year": "2020",
      "venue": "Fairlearn: A toolkit for assessing and improving fairness in AI"
    },
    {
      "citation_id": "6",
      "title": "The OpenCV Library",
      "authors": [
        "G Bradski"
      ],
      "year": "2000",
      "venue": "Dr. Dobb's Journal of Software Tools"
    },
    {
      "citation_id": "7",
      "title": "The zoo of fairness metrics in machine learning",
      "authors": [
        "A Castelnovo",
        "R Crupi",
        "G Greco",
        "D Regoli",
        "I Penco",
        "A Cosentini"
      ],
      "year": "2021",
      "venue": "The zoo of fairness metrics in machine learning"
    },
    {
      "citation_id": "8",
      "title": "Understanding and mitigating annotation bias in facial expression recognition",
      "authors": [
        "Y Chen",
        "J Joo"
      ],
      "year": "2021",
      "venue": "Understanding and mitigating annotation bias in facial expression recognition",
      "arxiv": "arXiv:2108.08504"
    },
    {
      "citation_id": "9",
      "title": "Understanding and mitigating annotation bias in facial expression recognition",
      "authors": [
        "Y Chen",
        "J Joo"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision"
    },
    {
      "citation_id": "10",
      "title": "The frontiers of fairness in machine learning",
      "authors": [
        "A Chouldechova",
        "A Roth"
      ],
      "year": "2018",
      "venue": "The frontiers of fairness in machine learning",
      "arxiv": "arXiv:1810.08810"
    },
    {
      "citation_id": "11",
      "title": "Gender bias assessment in emotion recognition",
      "authors": [
        "A Domnich",
        "G Anbarjafari",
        "Responsible",
        "Ai"
      ],
      "year": "2021",
      "venue": "Gender bias assessment in emotion recognition",
      "arxiv": "arXiv:2103.11436"
    },
    {
      "citation_id": "12",
      "title": "Fairness in deep learning: A computational perspective",
      "authors": [
        "M Du",
        "F Yang",
        "N Zou",
        "X Hu"
      ],
      "year": "2020",
      "venue": "Fairness in deep learning: A computational perspective",
      "arxiv": "arXiv:1908.08843"
    },
    {
      "citation_id": "13",
      "title": "Challenges in representation learning: A report on three machine learning contests",
      "authors": [
        "I Goodfellow",
        "D Erhan",
        "P Carrier",
        "A Courville",
        "M Mirza",
        "B Hamner",
        "W Cukierski",
        "Y Tang",
        "D Thaler",
        "D.-H Lee"
      ],
      "year": "2013",
      "venue": "Neural Information Processing: 20th International Conference"
    },
    {
      "citation_id": "14",
      "title": "Automated facial action coding system for dynamic analysis of facial expressions in neuropsychiatric disorders",
      "authors": [
        "J Hamm",
        "C Kohler",
        "R Gur",
        "R Verma"
      ],
      "year": "2011",
      "venue": "Journal of Neuroscience Methods",
      "doi": "10.1016/j.jneumeth.2011.06.023"
    },
    {
      "citation_id": "15",
      "title": "Facial expression recognition using enhanced deep 3d convolutional neural networks",
      "authors": [
        "B Hasani",
        "M Mahoor"
      ],
      "year": "2017",
      "venue": "2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)",
      "doi": "10.1109/cvprw.2017.282"
    },
    {
      "citation_id": "16",
      "title": "Fairface: Face attribute dataset for balanced race, gender, and age for bias measurement and mitigation",
      "authors": [
        "K Karkkainen",
        "J Joo"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision"
    },
    {
      "citation_id": "17",
      "title": "Do deep neural networks learn facial action units when doing expression recognition?",
      "authors": [
        "P Khorrami",
        "T Paine",
        "T Huang"
      ],
      "year": "2015",
      "venue": "Proceedings of the IEEE international conference on computer vision workshops"
    },
    {
      "citation_id": "18",
      "title": "A brief review of facial emotion recognition based on visual information",
      "authors": [
        "B Ko"
      ],
      "year": "2018",
      "venue": "Sensors"
    },
    {
      "citation_id": "19",
      "title": "Deep facial expression recognition: A survey",
      "authors": [
        "S Li",
        "W Deng"
      ],
      "year": "2018",
      "venue": "Deep facial expression recognition: A survey",
      "doi": "10.48550/ARXIV.1804.08348"
    },
    {
      "citation_id": "20",
      "title": "Reliable crowdsourcing and deep locality-preserving learning for expression recognition in the wild",
      "authors": [
        "S Li",
        "W Deng",
        "J Du"
      ],
      "year": "2017",
      "venue": "2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
      "doi": "10.1109/CVPR.2017.277"
    },
    {
      "citation_id": "21",
      "title": "The child affective facial expression (cafe) set: validity and reliability from untrained adults",
      "authors": [
        "V Lobue",
        "C Thrasher"
      ],
      "year": "2015",
      "venue": "Front. Psychol"
    },
    {
      "citation_id": "22",
      "title": "Facial expression recognition with convolutional neural networks: Coping with few data and the training sample order",
      "authors": [
        "A Lopes",
        "E De Aguiar",
        "A De Souza",
        "T Oliveira-Santos"
      ],
      "year": "2017",
      "venue": "Pattern Recognition",
      "doi": "10.1016/j.patcog.2016.07.026"
    },
    {
      "citation_id": "23",
      "title": "The extended cohn-kanade dataset (ck+): A complete dataset for action unit and emotionspecified expression",
      "authors": [
        "P Lucey",
        "J Cohn",
        "T Kanade",
        "J Saragih",
        "Z Ambadar",
        "I Matthews"
      ],
      "year": "2010",
      "venue": "2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition -Workshops",
      "doi": "10.1109/CVPRW.2010.5543262"
    },
    {
      "citation_id": "24",
      "title": "Coding facial expressions with gabor wavelets",
      "authors": [
        "M Lyons",
        "S Akamatsu",
        "M Kamachi",
        "J Gyoba"
      ],
      "year": "1998",
      "venue": "Proceedings Third IEEE International Conference on Automatic Face and Gesture Recognition",
      "doi": "10.1109/AFGR.1998.670949"
    },
    {
      "citation_id": "25",
      "title": "Dialoguernn: An attentive rnn for emotion detection in conversations",
      "authors": [
        "N Majumder",
        "S Poria",
        "D Hazarika",
        "R Mihalcea",
        "A Gelbukh",
        "E Cambria"
      ],
      "year": "2019",
      "venue": "Dialoguernn: An attentive rnn for emotion detection in conversations"
    },
    {
      "citation_id": "26",
      "title": "Fairness and missing values",
      "authors": [
        "F Martínez-Plumed",
        "C Ferri",
        "D Nieves",
        "J Hernández-Orallo"
      ],
      "year": "2019",
      "venue": "Fairness and missing values",
      "arxiv": "arXiv:1905.12728"
    },
    {
      "citation_id": "27",
      "title": "A survey on bias and fairness in machine learning",
      "authors": [
        "N Mehrabi",
        "F Morstatter",
        "N Saxena",
        "K Lerman",
        "A Galstyan"
      ],
      "year": "2022",
      "venue": "A survey on bias and fairness in machine learning",
      "arxiv": "arXiv:1908.09635"
    },
    {
      "citation_id": "28",
      "title": "A survey on bias and fairness in machine learning",
      "authors": [
        "N Mehrabi",
        "F Morstatter",
        "N Saxena",
        "K Lerman",
        "A Galstyan"
      ],
      "year": "2022",
      "venue": "A survey on bias and fairness in machine learning",
      "arxiv": "arXiv:1908.09635"
    },
    {
      "citation_id": "29",
      "title": "Affectnet: A database for facial expression, valence, and arousal computing in the wild",
      "authors": [
        "A Mollahosseini",
        "B Hasani",
        "M Mahoor"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "30",
      "title": "Fairness in Machine Learning",
      "authors": [
        "L Oneto",
        "S Chiappa"
      ],
      "year": "2020",
      "venue": "Fairness in Machine Learning",
      "doi": "10.1007/978-3-030-43883-87"
    },
    {
      "citation_id": "31",
      "title": "Emotion recognition using generative adversarial networks",
      "authors": [
        "Z Peng",
        "J Li",
        "Z Sun"
      ],
      "year": "2020",
      "venue": "2020 International Conference on Computer Engineering and Intelligent Control (ICCEIC)",
      "doi": "10.1109/ICCEIC51584.2020.00023"
    },
    {
      "citation_id": "32",
      "title": "Affective computing for hci",
      "authors": [
        "R Picard"
      ],
      "year": "1999",
      "venue": "Affective computing for hci"
    },
    {
      "citation_id": "33",
      "title": "Evaluation of data augmentation techniques for facial expression recognition systems",
      "authors": [
        "S Porcu",
        "A Floris",
        "L Atzori"
      ],
      "year": "2020",
      "venue": "Electronics",
      "doi": "10.3390/electronics9111892"
    },
    {
      "citation_id": "34",
      "title": "Exploring biases in facial expression analysis",
      "authors": [
        "R Raina",
        "M Monares",
        "M Xu",
        "S Fabi",
        "X Xu",
        "L Li",
        "W Sumerfield",
        "J Gan",
        "V De Sa"
      ],
      "year": "2022",
      "venue": "NeurIPS 2022 Workshop on Synthetic Data for Empowering ML Research"
    },
    {
      "citation_id": "35",
      "title": "Racial influence on automated perceptions of emotions",
      "authors": [
        "L Rhue"
      ],
      "year": "2018",
      "venue": "SSRN Electronic Journal",
      "doi": "10.2139/ssrn.3281765"
    },
    {
      "citation_id": "36",
      "title": "A bias and fairness audit toolkit",
      "authors": [
        "P Saleiro",
        "B Kuester",
        "A Stevens",
        "A Anisfeld",
        "L Hinkson",
        "J London",
        "R Ghani",
        "Aequitas"
      ],
      "year": "2018",
      "venue": "A bias and fairness audit toolkit",
      "arxiv": "arXiv:1811.05577"
    },
    {
      "citation_id": "37",
      "title": "Ethical ai in facial expression analysis: racial bias. Signal, Image and Video Processing",
      "authors": [
        "A Sham",
        "K Aktas",
        "D Rizhinashvili",
        "D Kuklianov",
        "F Alisinanoglu",
        "I Ofodile",
        "C Ozcinar",
        "G Anbarjafari"
      ],
      "year": "2023",
      "venue": "Ethical ai in facial expression analysis: racial bias. Signal, Image and Video Processing",
      "doi": "10.1007/s11760-022-02246-8"
    },
    {
      "citation_id": "38",
      "title": "Ethical ai in facial expression analysis: Racial bias. Signal, Image and Video Processing",
      "authors": [
        "A Sham",
        "K Aktas",
        "D Rizhinashvili",
        "D Kuklianov",
        "F Alisinanoglu",
        "I Ofodile",
        "C Ozcinar",
        "G Anbarjafari"
      ],
      "year": "2023",
      "venue": "Ethical ai in facial expression analysis: Racial bias. Signal, Image and Video Processing"
    },
    {
      "citation_id": "39",
      "title": "Racial identity-aware facial expression recognition using deep convolutional neural networks",
      "authors": [
        "M Sohail",
        "G Ali",
        "J Rashid",
        "I Ahmad",
        "S Almotiri",
        "M Alghamdi",
        "A Nagra",
        "K Masood"
      ],
      "year": "2022",
      "venue": "Applied Sciences",
      "doi": "10.3390/app12010088"
    },
    {
      "citation_id": "40",
      "title": "Affective computing: A review",
      "authors": [
        "J Tao",
        "T Tan"
      ],
      "year": "2005",
      "venue": "Affective Computing and Intelligent Interaction: First International Conference, ACII 2005"
    },
    {
      "citation_id": "41",
      "title": "Effect of wearable digital intervention for improving socialization in children with autism spectrum disorder: a randomized clinical trial",
      "authors": [
        "C Voss",
        "J Schwartz",
        "J Daniels",
        "A Kline",
        "N Haber",
        "P Washington",
        "Q Tariq",
        "T Robinson",
        "M Desai",
        "J Phillips"
      ],
      "year": "2019",
      "venue": "JAMA pediatrics"
    },
    {
      "citation_id": "42",
      "title": "Training affective computer vision models by crowdsourcing soft-target labels",
      "authors": [
        "P Washington",
        "H Kalantarian",
        "J Kent",
        "A Husic",
        "A Kline",
        "E Leblanc",
        "C Hou",
        "C Mutlu",
        "K Dunlap",
        "Y Penev"
      ],
      "year": "2021",
      "venue": "Cognitive computation"
    },
    {
      "citation_id": "43",
      "title": "Improved digital therapy for developmental pediatrics using domain-specific artificial intelligence: Machine learning study",
      "authors": [
        "P Washington",
        "H Kalantarian",
        "J Kent",
        "A Husic",
        "A Kline",
        "E Leblanc",
        "C Hou",
        "O Mutlu",
        "K Dunlap",
        "Y Penev"
      ],
      "year": "2022",
      "venue": "JMIR Pediatrics and Parenting"
    },
    {
      "citation_id": "44",
      "title": "The what-if tool: Interactive probing of machine learning models",
      "authors": [
        "J Wexler",
        "M Pushkarna",
        "T Bolukbasi",
        "M Wattenberg",
        "F Viegas",
        "J Wilson"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Visualization and Computer Graphics",
      "doi": "10.1109/TVCG.2019.2934619"
    },
    {
      "citation_id": "45",
      "title": "Investigating bias and fairness in facial expression recognition",
      "authors": [
        "T Xu",
        "J White",
        "S Kalkan",
        "H Gunes"
      ],
      "year": "2020",
      "venue": "Investigating bias and fairness in facial expression recognition",
      "arxiv": "arXiv:2007.10075"
    },
    {
      "citation_id": "46",
      "title": "Investigating bias and fairness in facial expression recognition",
      "authors": [
        "T Xu",
        "J White",
        "S Kalkan",
        "H Gunes"
      ],
      "year": "2020",
      "venue": "Computer Vision-ECCV 2020 Workshops: Glasgow, UK"
    },
    {
      "citation_id": "47",
      "title": "Affective computing for large-scale heterogeneous multimedia data: A survey",
      "authors": [
        "S Zhao",
        "S Wang",
        "M Soleymani",
        "D Joshi"
      ],
      "year": "2019",
      "venue": "ACM Transactions on Multimedia Computing, Communications, and Applications (TOMM)"
    },
    {
      "citation_id": "48",
      "title": "The other-race effect in facial expression processing: Behavioral and ERP evidence from a balanced cross-cultural study in women",
      "authors": [
        "Zhongqing Jiang",
        "Guillermo Recio",
        "Wenhui Li",
        "Peng Zhu",
        "Jiamei He",
        "Werner Sommer"
      ],
      "year": "2023",
      "venue": "Int J Psychophysiol"
    },
    {
      "citation_id": "49",
      "title": "Dependency exploitation: A unified cnn-rnn approach for visual emotion recognition",
      "authors": [
        "X Zhu",
        "L Li",
        "W Zhang",
        "T Rao",
        "M Xu",
        "Q Huang",
        "D Xu"
      ],
      "year": "2017",
      "venue": "Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence, IJCAI-17",
      "doi": "10.24963/ijcai.2017/503"
    }
  ]
}