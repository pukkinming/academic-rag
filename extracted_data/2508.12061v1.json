{
  "paper_id": "2508.12061v1",
  "title": "Varan: Variational Inference For Self-Supervised Speech Models Fine-Tuning On Downstream Tasks",
  "published": "2025-08-16T14:26:59Z",
  "authors": [
    "Daria Diatlova",
    "Nikita Balagansky",
    "Alexander Varlamov",
    "Egor Spirin"
  ],
  "keywords": [
    "automatic speech recognition",
    "emotion recognition",
    "fine-tuning",
    "SSL"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Conventional methods for aggregating layers in fine-tuned self-supervised speech models, such as using the final layer or weighted sum, suffer from information bottlenecks and static feature weighting for all dataset examples. We propose VARAN, a framework that dynamically tailors layer aggregation to individual inputs. By employing layer-specialized probing heads and data-dependent weighting, VARAN adaptively prioritizes layer's features based on input. Evaluations on automatic speech recognition (ASR) and speech emotion recognition (SER) tasks demonstrate VARAN's superior performance, particularly when using the LoRA fine-tuning technique. The framework resolves the trade-off between preserving layerspecific information and enabling flexible feature utilization, advancing efficient adaptation of self-supervised speech representations.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Self-supervised learning (SSL) has revolutionized speech processing by enabling models like WavLM  [1]  and Data2Vec  [2]  to learn rich, transferable representations from vast unlabeled audio corpora. These models excel in downstream tasks-such as ASR, SER, SV and others by leveraging hierarchical features across transformer encoder layers. However, effectively utilizing these features remains challenging. Conventional approaches aggregate layer-wise representations either by relying solely on the final layer or combining them via static, learnable weights. While simple, these methods suffer from two critical limitations: an information bottleneck, where compressing multi-layer features into a single vector discards nuanced hierarchical information (e.g., phonetic details in lower layers vs. semantic context in higher ones), and static aggregation, which applies fixed weights across all inputs, ignoring the variability in input complexity (e.g., ambiguous utterances requiring multi-layer analysis versus straightforward ones needing focused high-level features).\n\nTo address these issues, we propose VARAN (Variational Adaptive Layer Aggregation Network), a novel framework that dynamically tailors layer aggregation to individual inputs. VARAN introduces two key modifications: • Layer-Specialized Probing Heads: Lightweight task-specific models are applied to each encoder layer, preserving distinct hierarchical features without cross-layer interference. • Data-Dependent Weighting: A variational mechanism learns input-specific weights to prioritize layers, enabling the model to adaptively combine features based on input complexity.\n\nExtensive experiments on ASR and SER tasks demonstrate VARAN 's better or on-par results relative to static aggregation baselines, both in the standard fine-tuning setup and when applying LoRA.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "2. Prelimenaries",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "2.1. Ssl Models' Architecture",
      "text": "Modern SSL backbones share a similar architectural design that consists of two fundamental components: a feature extractor F, typically implemented using CNNs, and a transformer encoder E. The Transformer encoder is composed of n layers which are denoted as Li.\n\nDuring the forward pass on downstream tasks, a raw waveform x first passes through the feature extractor to produce an initial representation h0 = F(x). Subsequently, h0 is processed through a series of transformer encoder layers. Each layer computes its output as hi = Li(hi-1).\n\nAfter completing the forward pass, a sequence of hidden states H = (h0, . . . , hn) is obtained. This set of hidden states is then used for downstream tasks.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Layer Aggregation For Downstream Tasks",
      "text": "The formulation of downstream tasks usually involves predicting y (i.e., class label, text, or speaker embedding) from waveform x. This requires the SSL model, trained for the downstream task to have a probing head p θ (y|x) = ψ ĥ , where ψ denotes the downstream model, θ are the model parameters and ĥ are the hidden states of the model. In the simplest case ĥ = hn, which we refer to as the last layer. However, this approach does not utilize useful features from lower layers, leading to suboptimal performance. To address this issue, one can use learnable weights w = (w0, ..., wn) to sum the representations from different layers ĥ = n i=0 wi • hi .\n\n(1)",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Efficient Fine-Tuning Methods",
      "text": "The simplest approach to adapt a pre-trained model for a downstream task is full fine-tuning, where the model is initialized with pre-trained weights and all parameters are updated according to the training data. However, this approach often leads to challenges such as catastrophic forgetting  [3]  and representation collapse, particularly when the downstream task has limited training data. To address these issues, parameter-efficient fine-tuning methods have emerged as a compelling alternative. These methods not only reduce the number of trainable parameters but also help retain the valuable knowledge acquired during pre-training.\n\nOne prominent example of such methods is LoRA  [4] , which offers an elegant solution by freezing the original model parameters and introducing low-rank matrices. Specifically, instead of updating the entire weight matrix W , LoRA decomposes the update ∆W into the product of two smaller matrices:\n\nwhere A ∈ R d×r and B ∈ R r×k are trainable matrices, and r ≪ min(d, k) represents the rank of the decomposition. By constraining the updates to a low-rank subspace, LoRA significantly reduces the number of trainable parameters while preserving the pre-trained model's knowledge. This enables efficient adaptation to downstream tasks without compromising the quality of the learned representations.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Method",
      "text": "In this section, we propose VARAN, a Variational Inferencebased method for aggregating self-supervised speech model outputs. We begin by outlining the limitations of current approaches. We then explain how our method addresses them, provide its theoretical foundation, derive the training objective, and finally detail the resulting architectural changes to the model.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Limitations Of Current Aggregation Methods",
      "text": "A common approach for aggregating hidden states before applying the probing head for a any downstream task, as described in Section 2.2, introduces two key limitations: (1) an information bottleneck resulting from compressing layer-specific features into a single vector ĥ, and (2) a static layer prioritization, where a fixed set of weights w ignores the optimal combination of features H that depends on the input x.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Varan: Variational Inference Perspective",
      "text": "To address the limitation of the existing layer aggregation methods we suggest to view a downstream task from variational inference perspective. We assume that for any downstream task, our goal is to predict the target y using input data x. As mentioned in 2.2 one could use a simple linear classifier on top of the weighted sum of hidden layers, see Equation  1 , but we found this setup to be suboptimal, as it may require different layer weights w to achieve the best performance for different tasks and even for individual samples. Choosing the right layer or combination of layers is challenging, as the training data does not have the optimal layer distribution. However, we can learn it with the help of variational inference. In our setup, we have a classifier on top of each backbone's layer p θ (y | x, l = i) and layer classifier q θ (l | x), where l is a discrete random variable and i is a layer index. To maximize p(y | x), we can use the following objective:\n\nwhere p(l) is a prior distribution on layer index.\n\nYou can find more details on derivation in Appendix A.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Varan: Training",
      "text": "The training objective L(θ) derives from a variational upper bound, see Equation  3 . Inspired by β-VAE  [7] , we use a β scaling factor as the regularization term. With this modification, our objective can be written as:  [8] :\n\nwhere the first term is a common downstream-task loss (eg. Cross Entropy for classification) computed for each ψi(hi) and averaged with the predicted set of weighs {wi(hi)} n i=1 . Where ψi is a probing head for each encoder layer Li. Note that {wi(hi)} n i=1 obtained individually for each sample x. Crucially, unlike Gumbel-based methods  [9] , gradients flow directly through q θ (l|x), ensuring stable training of datadependent weights.\n\nThe second KL term regularizes the predicted distribution for each training sample toward a prior p(l) defined for each downstream task. In practice, we used discretized reversed χ 2 distribution and vary degrees of freedom during the hyperparameter search.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Varan: Inference",
      "text": "At inference, predictions combine layer outputs via learned weights:\n\nVARAN addreses both an information botteleneck and a static layer prioritization issues: the first component of Equation 5 ensures that the model learns a unique set of weights wi for each sample to enable flexible, dynamic feature integration. The second component overcomes the information bottleneck by introducing different probing heads ψi that extract specialized information from each layer, incorporating these rich representations into the final output.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Varan: Architectural Perspective",
      "text": "An overview of the proposed method can be found in Figure  1 . To compute q θ (l = i|x) the posterior distribution predictor is parameterized using Multi-Head Self-Attention (MHSA). The input to the MHSA consists of hidden states from the Transformer Encoder blocks H = (h1, . . . , hn), where h ∈ R b×s×d (with dimensions corresponding to batch size b, sequence length s, and hidden size d). We apply mean pooling along the sequence length dimension and concatenate the results into a single tensor. This tensor is fed into the standard MHSA mechanism, which acts as a feature mixer across layers. Finally, we apply a linear layer to the last dimension to obtain a tensor Hprojected ∈ R b×L×1 , perform a squeeze operation to remove the singleton dimension and apply softmax function along the layer dimension to produce weights w ∈ R b×L for each batch sample. To get log(p θ (y|x, l = i)) we employ separate lightweight models ψi for each output of the encoder layer hi which in our implementation is a simple linear layer. The final vector is obtained using the weighted sum as in Equation  1 . The detailed implementation of the proposed method can be found at https://github.com/deepvk/varan.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Experiments",
      "text": "In this section, we describe the experimental setup and the results obtained by comparing the introduced VARAN to existing methods and techniques for fine-tuning SSL models on downstream tasks.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Setup",
      "text": "To examine the VARAN , we selected the Automatic Speech Recognition (ASR) and Speech Emotion Recognition (SER) tasks. These tasks require an understanding of acoustic features at both the semantic and phonetic levels, as well as speakerdependent information. For the ASR task, we follow the experimental setup described in  [12] . Specifically, we use the XS subset of the GigaSpeech dataset  [6]  for training. In addition to the test portion of the GigaSpeech dataset, we report results on the \"test clean\" and \"test other\" sets from the LibriSpeech dataset  [11] . For the SER task, we conduct experiments using the RAVDESS  [5]  and IEMOCAP  [10]  datasets. For the RAVDESS dataset, we use the original train-validation-test split and merge the neutral emotion with calm, resulting in a total of 7 training labels. For the IEMOCAP dataset, we perform 10fold cross-validation by training on 9 speakers and testing on the remaining one.\n\nAs baselines for layer aggregation methods, we employ two approaches: (1) the straightforward last layer method, where no aggregation occurs and features from the final layer are directly used; and (2) the weighted sum method, where features are aggregated using learnable weights without conditioning on sample variation. Furthermore, we investigate different finetuning techniques. Initially, we keep the CNN feature extractor frozen and train the rest of the model's parameters on a downstream task, which can be effective but computationally expensive, we refer this setup as Fine-Tuning. We also apply parameter-efficient fine-tuning using LoRA  [4] , a method that is both fast and stable for training large SSL models on downstream tasks, particularly under limited computational resources. More specifically, we follow  [12]  by replacing the feed-forward layers that come after the self-attention mechanism with LoRA layers.\n\nFor the backbones, we use both WavLM  [1]  and Data2Vec  [2]    and SER tasks, we used 2 fully-connected layers with a hidden size of 256 as a probing head. ASR decoding was conducted at the character level using beam search (beam size=5, beam threshold=20), with no language model integration. Since VARAN 's training objective derives from a variational upper bound (see Section 3), one essential consideration is selecting an appropriate prior distribution. We adopt a discretized reversed χ 2 as the prior, with degrees of freedom treated as hyperparameters during search.\n\nFor each fine-tuning, we use grid search to find the optimal hyperparameters on the validation sets of the corresponding datasets. See Table  1  for hyperparameter grid. For the IEMO-CAP dataset, \"Session 5\" is used. Full grid of the hyperparameters can be found in Table  1 .",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Results",
      "text": "Experimental results are summarized in Table  2 . On speech emotion recognition, VARAN consistently outperforms other layer aggregation methods across configurations. The sole ex-ception occurs in the LoRA fine-tuning setup with WavLM Large, where VARAN achieves performance comparable to weighted-sum aggregation.\n\nFor the IEMOCAP dataset, VARAN surpasses conventional methods less frequently in Fine-Tuning setup. However, the strongest results on this benchmark are observed in the LoRA fine-tuning regime, where VARAN delivers substantial improvements over all baselines, achieving a relative accuracy gain of 2.87% compared to weighted-sum aggregation.\n\nAccording to evaluations on the GigaSpeech ASR corpus and LibiSpeech test-other subsets, VARAN outperforms other methods in the LoRA fine-tuning setup across all models except Data2Vec Large. The relative improvement in WER on the GigaSpeech test set is 7.7% compared to the last-layer baseline. In the Fine-Tuning setup, performance gains are less stable: the last layer predominantly outperforms other methods.\n\nThis phenomenon can be explained by two observations. First,  [13]  demonstrated that models trained to predict discrete units learn phonetic and word information concentrated in higher layers. Second, during LoRA fine-tuning, models retain more pretraining knowledge  [12] , making layer aggregation in a data-dependent setup beneficial. Conversely, in Fine-Tuning, knowledge distribution may collapse to the last layer, diminishing the utility of multi-layer aggregation.\n\nIn general, VARAN consistently outperforms other methods in most scenarios, particularly in the LoRA setup and the speech emotion recognition task, where low-level features and layer weighting play a crucial role in final performance.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Analysis",
      "text": "To investigate the differences between datasets and models we plot the PMF function of prior distributions q(l) found during hyperparameter search for VARAN models. The results are presented in Figure  2 . The first observation is that both Data2Vec  [2]  base and large models benefit from the weights skewed towards the last layers in contrast to WavLM  [1] , which indicates that features needed for the tasks are grouped there. The second observation is that ASR benefits most from the last hidden state.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Related Work",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Speech Self-Supervised Model Fine-Tuning",
      "text": "Recent work has explored strategies to optimize speech SSL models for downstream tasks through architectural and training adaptations.  [14]  demonstrated that scaling probing head size not only improves model-specific performance on SER and SV tasks but also reshuffles SSL model rankings in benchmark evaluations. Work by  [12]  investigated continual learning in SSL fine-tuning for ASR, revealing that full model unfreezing induces pre-training knowledge forgetting, whereas parameterefficient methods like LoRA and Elastic Weight Consolidation mitigate this issue and achieve superior word error rates (WER), particularly on out-of-distribution data. Complementary to these approaches,  [15]  proposed Multi-Head-Factorized Attentive Pooling (MHFA) for SV to hierarchically aggregate low-level speaker features with high-level phonetic information, thereby enhancing feature disentanglement. 5.2. Layer Analysis of Speech Self-Supervised Models  [1]  found that layers at the beginning and middle of the WavLM model had higher magnitudes of learned weights after finetuning on speaker-dependent tasks, indicating their importance for speaker verification (SV) and speaker identification (SI).  [13]  used CCA analysis to show that models trained to predict discrete units learn phonetic and word information in intermediate layers, with this information concentrated in higher layers. They also demonstrated that layer-wise phone and word content correlate well with downstream task performance for phoneme recognition (PR) and automatic speech recognition (ASR).  [16]  showed that weights learned via weighted sum are not correlated with layers' performance on tasks. Both  [16]  and  [13]  found that for some tasks and SSL models, the best individual layer outperforms the learned weighted sum. However, this improvement in single-layer benchmarking is specific to individual SSL models and is not consistent across all models  [13] .",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Variational Inference And Early Exit",
      "text": "From one perspective, our method could be seen as VAE  [17]  with layer index l as a latent variable. Therefore, we can successfully adapt some techniques for it, like  [7] .  [18]  used a similar approach, but for adaptive computational time  [19]  inference. This method was also adapted for early exiting during inference  [20] . While reducing inference time of the models is a promising direction, in our work, we concentrated solely on performance improvement.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Conclusion",
      "text": "In our work, we present a novel layer aggregation method called VARAN, which uses per-sample weighting of the predictions from probing heads to obtain final predictions. Through the extensive experiments, we show that VARAN method achieves better or comparable results on the SER and ASR tasks. We also study its performance across different models and setups, allowing as to analyze the importance of the layers for the different tasks. We believe our findings can amplify further research on feature aggregation methods.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "References",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "A. Elbo",
      "text": "Our goal is to maximize the likelihood p(y | x). As SSL models use layer-aggregation methods for hidden representations from multiple layers, we need to derive a lower bound for the prior downstream task layer distribution.\n\nFor simplicity, let us denote the approximated posterior as q θ (l | x), the true posterior which is a particular downstream task distribution as p(l | x, y), the prior layer distribution as p(l) and E l∼q θ (l|x) as Eq . Our goal is to approximate the posterior distribution so that it is maximally close to the true posterior. For that, let us derive a formula for KL-divergence: At this point, we still don't know how to access p(l, y | x). Let us rearrange the terms and write the log-likelihood log p(y|x) as: log p(y | x) = E log p(l, y | x) q θ (l | x) + KL (q θ (l | x) ∥ p(l | x, y)) .\n\nAs log-likelihood is the logarithm of the probability, it is ≥ 0. KL-divergence is a measure of distance which is also ≥ 0, then the lower bound for likelihood can be written as: From the definition of KL-divergence, KL [q θ (l | x) ∥ p(l)] = E log q θ (l|x) p(l) , we get that E log p(l) q θ (l|x) = -KL (q θ (l | x) ∥ p(l)). Which gives us equation 6 that is the same as equation 4:\n\nBy expanding E and selecting the downstream model, p θ , we get equation 7:\n\n+ KL (q θ (l | x) ∥ p(l)) .  (7)",
      "page_start": 6,
      "page_end": 6
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Overview of the VARAN layer aggregation method.",
      "page": 2
    },
    {
      "caption": "Figure 2: Prior distributions found during hyperparameters search. Models trained on RAVDESS[5] dataset tend to have distribution",
      "page": 3
    },
    {
      "caption": "Figure 1: To compute qθ(l = i|x) the posterior distribution predictor is",
      "page": 3
    },
    {
      "caption": "Figure 2: The first observation is that both Data2Vec",
      "page": 5
    }
  ],
  "tables": [
    {
      "caption": "Table 1: Hyperparameter search space.",
      "page": 3
    },
    {
      "caption": "Table 2: The results of VARAN are compared with other layer aggregation methods across various fine-tuning techniques. VARAN",
      "page": 4
    },
    {
      "caption": "Table 1: for hyperparameter grid. For the IEMO-",
      "page": 4
    },
    {
      "caption": "Table 1: 4.2. Results",
      "page": 4
    },
    {
      "caption": "Table 2: On speech",
      "page": 4
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Wavlm: Large-scale selfsupervised pre-training for full stack speech processing",
      "authors": [
        "S Chen",
        "C Wang",
        "Z Chen",
        "Y Wu",
        "S Liu",
        "Z Chen",
        "J Li",
        "N Kanda",
        "T Yoshioka",
        "X Xiao"
      ],
      "year": "2022",
      "venue": "IEEE Journal of Selected Topics in Signal Processing"
    },
    {
      "citation_id": "2",
      "title": "Data2vec: A general framework for self-supervised learning in speech, vision and language",
      "authors": [
        "A Baevski",
        "W.-N Hsu",
        "Q Xu",
        "A Babu",
        "J Gu",
        "M Auli"
      ],
      "year": "2022",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "3",
      "title": "Measuring catastrophic forgetting in neural networks",
      "authors": [
        "R Kemker",
        "A Abitino",
        "M Mcclure",
        "C Kanan"
      ],
      "year": "2017",
      "venue": "AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "4",
      "title": "Lora: Low-rank adaptation of large language models",
      "authors": [
        "J Hu",
        "Y Shen",
        "P Wallis",
        "Z Allen-Zhu",
        "Y Li",
        "S Wang",
        "W Chen"
      ],
      "year": "2021",
      "venue": "International Conference on Learning Representations"
    },
    {
      "citation_id": "5",
      "title": "The ryerson audio-visual database of emotional speech and song (ravdess): A dynamic, multimodal set of facial and vocal expressions in north american english",
      "authors": [
        "S Livingstone",
        "F Russo"
      ],
      "year": "2018",
      "venue": "PloS one"
    },
    {
      "citation_id": "6",
      "title": "Gigaspeech: An evolving, multi-domain ASR corpus with 10, 000 hours of transcribed audio",
      "authors": [
        "G Chen",
        "S Chai",
        "G Wang",
        "J Du",
        "W Zhang",
        "C Weng",
        "D Su",
        "D Povey",
        "J Trmal",
        "J Zhang",
        "M Jin",
        "S Khudanpur",
        "S Watanabe",
        "S Zhao",
        "W Zou",
        "X Li",
        "X Yao",
        "Y Wang",
        "Y Wang",
        "Z You",
        "Z Yan"
      ],
      "year": "2021",
      "venue": "CoRR"
    },
    {
      "citation_id": "7",
      "title": "beta-VAE: Learning basic visual concepts with a constrained variational framework",
      "authors": [
        "I Higgins",
        "L Matthey",
        "A Pal",
        "C Burgess",
        "X Glorot",
        "M Botvinick",
        "S Mohamed",
        "A Lerchner"
      ],
      "year": "2017",
      "venue": "International Conference on Learning Representations"
    },
    {
      "citation_id": "8",
      "title": "Auto-encoding variational bayes",
      "authors": [
        "D Kingma",
        "M Welling"
      ],
      "year": "2014",
      "venue": "2nd International Conference on Learning Representations, ICLR 2014"
    },
    {
      "citation_id": "9",
      "title": "Categorical reparameterization with gumbel-softmax",
      "authors": [
        "E Jang",
        "S Gu",
        "B Poole"
      ],
      "year": "2016",
      "venue": "International Conference on Learning Representations"
    },
    {
      "citation_id": "10",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "11",
      "title": "Librispeech: an asr corpus based on public domain audio books",
      "authors": [
        "V Panayotov",
        "G Chen",
        "D Povey",
        "S Khudanpur"
      ],
      "year": "2015",
      "venue": "2015 IEEE international conference on acoustics, speech and signal processing (ICASSP)"
    },
    {
      "citation_id": "12",
      "title": "Less forgetting for better generalization: Exploring continual-learning fine-tuning methods for speech self-supervised representations",
      "authors": [
        "S Zaiem",
        "T Parcollet",
        "S Essid"
      ],
      "year": "2024",
      "venue": "Less forgetting for better generalization: Exploring continual-learning fine-tuning methods for speech self-supervised representations",
      "arxiv": "arXiv:2407.00756"
    },
    {
      "citation_id": "13",
      "title": "Comparative layer-wise analysis of self-supervised speech models",
      "authors": [
        "A Pasad",
        "B Shi",
        "K Livescu"
      ],
      "year": "2023",
      "venue": "ICASSP 2023 -2023 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "14",
      "title": "Speech self-supervised representations benchmarking: a case for larger probing heads",
      "authors": [
        "S Zaiem",
        "Y Kemiche",
        "T Parcollet",
        "S Essid",
        "M Ravanelli"
      ],
      "year": "2024",
      "venue": "Computer Speech & Language"
    },
    {
      "citation_id": "15",
      "title": "An attention-based backend allowing efficient fine-tuning of transformer models for speaker verification",
      "authors": [
        "J Peng",
        "O Plchot",
        "T Stafylakis",
        "L Mosner",
        "L Burget",
        "J Cernocký"
      ],
      "year": "2022",
      "venue": "IEEE Spoken Language Technology Workshop, SLT 2022",
      "doi": "10.1109/SLT54892.2023.10022775"
    },
    {
      "citation_id": "16",
      "title": "A large-scale evaluation of speech foundation models",
      "authors": [
        "S Yang",
        "H Chang",
        "Z Huang",
        "A Liu",
        "C Lai",
        "H Wu",
        "J Shi",
        "X Chang",
        "H Tsai",
        "W Huang",
        "T Feng",
        "P Chi",
        "Y Lin",
        "Y Chuang",
        "T Huang",
        "W Tseng",
        "K Lakhotia",
        "S Li",
        "A Mohamed",
        "S Watanabe",
        "H Lee"
      ],
      "year": "2024",
      "venue": "IEEE ACM Trans. Audio Speech Lang. Process",
      "doi": "10.1109/TASLP.2024.3389631"
    },
    {
      "citation_id": "17",
      "title": "Auto-encoding variational bayes",
      "authors": [
        "D Kingma",
        "M Welling"
      ],
      "year": "2014",
      "venue": "2nd International Conference on Learning Representations, ICLR 2014"
    },
    {
      "citation_id": "18",
      "title": "Pondernet: Learning to ponder",
      "authors": [
        "A Banino",
        "J Balaguer",
        "C Blundell"
      ],
      "year": "2021",
      "venue": "ICML Workshop"
    },
    {
      "citation_id": "19",
      "title": "Adaptive computation time for recurrent neural networks",
      "authors": [
        "A Graves"
      ],
      "year": "2016",
      "venue": "Adaptive computation time for recurrent neural networks",
      "arxiv": "arXiv:1603.08983"
    },
    {
      "citation_id": "20",
      "title": "Palbert: Teaching albert to ponder",
      "authors": [
        "N Balagansky",
        "D Gavrilov"
      ],
      "venue": "Advances in Neural Information Processing Systems"
    }
  ]
}