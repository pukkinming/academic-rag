{
  "paper_id": "2208.09269v1",
  "title": "Feature Selection Enhancement And Feature Space Visualization For Speech-Based Emotion Recognition",
  "published": "2022-08-19T11:29:03Z",
  "authors": [
    "Sofia Kanwal",
    "Sohail Asghar",
    "Hazrat Ali"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Robust speech emotion recognition relies on the quality of the speech features. We present speech features enhancement strategy that improves speech emotion recognition. We used the INTERSPEECH 2010 challenge feature-set. We identified subsets from the features set and applied Principle Component Analysis to the subsets. Finally, the features are fused horizontally. The resulting feature set is analyzed using t-distributed neighbour embeddings (t-SNE) before the application of features for emotion recognition. The method is compared with the state-of-the-art methods used in the literature. The empirical evidence is drawn using two well-known datasets: Emotional Speech Dataset (EMO-DB) and Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS) for two languages, German and English, respectively. Our method achieved an average recognition gain of 11.5% for six out of seven emotions for the EMO-DB dataset, and 13.8% for seven out of eight emotions for the RAVDESS dataset as compared to the baseline study.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Speech is one of the most remarkable, natural, and immediate methods for individuals to impart their emotions Akc ¸ay and Oguz (2020). The human voice reflects the emotional and psychological states of a person. The speech emotion recognition (SER) domain has been explored for more than two decades  Schuller (2018) . It is helpful in numerous applications that require human-machine interaction. SER is gaining attention in many fields like automatic speech processing, pattern recognition, artificial intelligence, and domains involving human-machine interaction  Lee and Narayanan (2005) . By recognizing speech emotions using artificial intelligence-based methods, the interaction between humans and machines can be made more natural and effective. SER is advantageous in specific areas like automatic SER systems could help reduce accident ratio by recognizing aggressive drivers and alerting other road users  Ji et al. (2006) ;  Yang et al. (2017) . In the health care field, SER systems could help physicians understand the actual emotional state  Harati et al. (2018) . Automatic learning can be improved by recognizing learners' emotions, alerting tutors, and suggesting changes to reduce negative emotions of boredom, frustration, and anxiety  Jithendran et al. (2020) . The computer game industry may gain more natural interaction among game players and computers by exploiting audio emotion detection systems  Schuller (2016) . Hence, SER has significant value in improving lives in the digital era. One of the challenging tasks in the design of a speech-based emotion recognition framework is the identification and extraction of features that proficiently describe diverse emotions and perform reliably  Zheng et al. (2019) . Humans have the ability to recognize emotions using verbal and non-verbal cues. However, in the case of machines, a comprehensive set of features is required to perform emotion recognition tasks intelligently. Speech has a large range of features, however, not all of them play a significant arXiv:2208.09269v1 [eess.SP] 19 Aug 2022 role in distinguishing emotions  Özseven (2019)  and less important features need to be discarded by dimensionality reduction methods. One more thing which makes defining emotion boundaries even more difficult, is the presence of wide range of languages, accents, sentences, and speaking styles of the different speakers.\n\nFurther, there is no standardized feature set available in the literature and existing work in SER is based on the experimental studies so far Akc ¸ay and Oguz (2020) and also, many of the existing SER systems are not as much accurate to be fully relied upon  Xu et al. (2020) . Among two broad categories of speech features: linguistic and paralinguistic, the later is more advantageous. The reason is paralinguistic features are helpful to recognize speech emotions irrespective of the language being spoken, the text being spoken and the accent being used  Hook et al. (2019) . Their only dependence is on characteristics of speech signals such as tone, loudness, frequency, pitch, etc. In past, many researchers used paralinguistic attributes for SER, however, a universal set based on such cues has not established yet El  Ayadi et al. (2011) ;  Anagnostopoulos et al. (2015) . Therefore, it is required to make improvements regarding the effective use of paralinguistic attributes for the SER. Out of various paralinguistic features, the most commonly used are prosodic, spectral, voice quality and Teager energy operator (TEO) Akc ¸ay and Oguz (2020). Prosodic features portray the emotional value of speech  Tripathi and Rao (2018)  and are related to pitch, energy and, formant frequencies  Lee and Narayanan (2005) . Spectral features are based on the distribution of spectral energy throughout the speech range of frequency  Nwe et al. (2003) . Good examples of spectral features are Mel Frequency Cepstrum Coefficients (MFCC) and linear predictor features. Qualitative features are related to perceiving emotions by the quality of voice, e.g, voice level, voice pitch and temporal structure  Gobl and Chasaide (2003) . Teager Energy Operator (TEO) based features are related to vocal chord movement while speaking under stressful situations  Teager and Teager (1990) . Some studies reported emotion recognition based on speech features combined with other modalities, for example, text  Lee and Lee (2007) , facial expressions  Nemati et al. (2019) ;  Bellegarda (2013)  body signals, and outward appearances  Yu and Tapus (2019) ;  Ozkul et al. (2012)  to fabricate multi-modal frameworks for emotion analysis. However in this work, we limit ourselves to emotion recognition explicitly from speech features only. In this work, we use the features set of the INTERSPEECH 2010 challenge which is the combination of prosodic, spectral and energy-related features and have a large set of attributes, i.e. 1582. Such a huge feature-space requires a good dimensionality reduction technique. One such technique most effectively used in speech research is Principle Component Analysis (PCA)  Jolliffe and Cadima (2016) . The efficient use of PCA requires the removal of outliers and scaling the data  Sarkar et al. (2014) . To apply PCA appropriately, we normalized the chosen feature set and discovered 3 subsets based on the types of speech features. The PCA is applied separately to each subset. We analyzed the finalized feature set thoroughly, by drawing t-SNE graphs for the two datasets. For classification, Support Vector Machine (SVM) for One-Against-All is used. The results highlighted that appropriate selection of features and successful application of the feature selection method impacted the classification performance positively. The method achieved a decision-level correct classification rate of 71.08 % for eight emotions using the RAVDESS dataset  Livingstone and Russo (2018)  and 82.48 % for seven emotions using the EMO-DB dataset  Burkhardt et al. (2005) . Furthermore, the proposed method outperformed the results of  Yang et al. (2017) ;  Venkataramanan and Rajamohan (2019) ;  Tawari and Trivedi (2010) . Our contributions in this work are:\n\n• An improved speech emotion classification method using robust features.\n\n• An approach to apply features reduction technique on a subset of speech features.\n\n• An evaluation mechanism of the selected speech features using t-SNE visualization before applying a machine learning classifier.\n\nThe remainder of the paper is composed as follows. Section 2 provides a review of two key modules of speech-based emotion classification framework, i.e., speech features and classifiers. Section 3 introduces the speech emotion datasets and assessment criteria utilized in this work. Evaluation results of the framework utilizing various databases and various situations are presented in Section 4. Finally, Section 5 concludes the paper.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Literature Review",
      "text": "This section presents the previous work on speech-based emotion recognition with respect to selection of relevant speech features and machine learning methods used in SER systems.  Yang Yang et al. (2017)  opted for prosodic features mainly and used mutual information as a feature selection method to choose top-80 features for the recognition task. In  Venkataramanan and Rajamohan (2019) , few features such as MFCC, pitch, energy, and Mel-spectrogram were used, and the top-2 and top-3 accuracy of 84.13% and 91.38% respectively were achieved for the RAVDESS dataset. Fourier coefficients of the speech signal were extracted and applied on EMO-DB dataset in  Wang et al. (2015) , which came up with a recognition rate of 79.51%.  Lugger and Yang (2007)  used voice quality 85 and prosodic features in combination for emotion recognition. In  Tripathi and Rao (2017) ,a speech-based emotion classification system was developed for the Bengali speech corpus. The classifiers used were ANN, KNN, SVMs, and Naive Bayes. The maximum accuracy achieved was 89%. For robust emotion classification, an enhanced sparse representation classifier is proposed  Zhao et al. (2014) . The classifier performed better in both noisy and clean data in comparison to other existing classifiers. In Al Machot et al. (  2011 ), a probability-based classifier, Bayesian Quadratic Discriminate was used to classify emotions. Classification accuracy of 86.67% on EMO-DB dataset was reported by reducing the calculation cost and using a small number of features. Another study reported an ensemble classification method for speech emotion classification on Thai speech corpus  Prasomphan and Doungwichain (2018) . The ensembled algorithms were SVM, KNN, and ANN to improve the accuracy. To make a comprehensive feature set, capable of performing well in all situations and supporting multiple languages, there is a need to include all the important speech emotion attributes. One such set of features available in the literature is INTERSPEECH 2010 paralinguistic challenge feature-set.  Schuller et al. (2010) . This feature-set is an extension of the INTERSPEECH 2009 paralinguistic emotion challenge feature-set  Schuller et al. (2009) .",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Materials And Methods",
      "text": "This section discusses the material and methods used in this research.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Data And Preprocessing",
      "text": "The benchmark datasets used in this research are (EMO-DB)  Burkhardt et al. (2005)  and (RAVDESS)  Livingstone and Russo (2018) .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Emo-Db",
      "text": "The EMO-DB is an acted dataset of ten experts (five male and five female) kept in the German language. It comprises seven emotion classes: anger, boredom, fear, happiness, disgust, neutral, and sadness. There are numerous expressions of a similar speaker. Ten sentences, which are phonetically unbiased, are picked for dataset development. Out of these 10 sentences, 5 sentences are short (around 1.5 seconds length) and 5 are long sentences (roughly 4 seconds term). Every emotion class has an almost equivalent number of expressions to avoid the issue of under-sampling emotion class. There is a sum of 535 expressions in this dataset. The validity of the dataset is ensured by rejecting the samples having a recognition rate less than 80% in the subjective listening test. The metadata of the EMO-DB dataset is given in Table1.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Database",
      "text": "EMO-DB",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Preprocessing",
      "text": "The preprocessing step involved reading audio files, removing unvoiced segments, and framing the signal having 60ms length.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Feature Extraction",
      "text": "After preprocessing, we moved towards feature analysis which involved the extraction of useful features for speech emotion analysis. In this research, the OpenSMILE toolbox was used and the resulting feature-set was INTERSPEECH 2010 Challenge feature-set  Schuller et al. (2010) . The reason for opting INTERSPEECH 2010 feature-set is because, it covers most of the features (namely, prosodic, spectral, and energy) effective for emotion recognition. The choice of initial features set is in line with the findings of  Özseven and Dügenci (2018)  which reported the effectiveness of these features for emotion recognition. INTERSPEECH 2010 feature-set consists of a total of 1582 features. Upon a thorough analysis of the feature-set, we identified three subsets. The first subset consisted of 34 low-level descriptors (LLDs) with 34 corresponding delta co-efficients, having applied 21 functionals on each of it. This resulted in a subset of 1428 features. The second subset was of 19 functionals applied to 4 pitch based LLDs with their corresponding 4 delta coefficients resulting in a total of 152 features. The third feature-set consisted only two features, which were pitch onset and duration.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Feature Reduction Technique",
      "text": "As the feature-set was huge, consisting of 1582 features, we needed a good dimensionality reduction technique. For feature reduction in SER, some of the commonly used methods are Forward Feature Selection (FFS), Backward Feature Selection (BFS)  Pao et al. (2005) , Principle Component Analysis (PCA) and Linear Discriminant Analysis (LDA)  Haq et al. (2008) ,  Özseven et al. (2018) . Among these the most commonly used and applied in many studies is  PCA Chen et al. (2012) ;  Scherer et al. (2015) ;  Patel et al. (2011) . PCA includes finding the eigen values and eigen vectors of the available covariance matrix and choosing the necessary number of eigenvectors comparing to the biggest eigenvalues to create a transformed matrix. We have fed the openSMILE INTERSPEECH 2010 feature-set of 1582 features to the PCA and selected the top-100 features which we used for the classification task.\n\nIn Figure  1  and Figure  2 , although we can see some clusters, however, most of the emotions are evenly distributed.    2017 ), when we used INTERSPEECH 2010 challenge feature-set and applied PCA for features reduction, a clearer representation of clusters could be seen. In Figure  3  for the EMO-DB dataset, emotion categories of sad, neutral, and boredom were distributed in relatively compact clusters in the feature-space such that high classification accuracy was anticipated. It is also noticeable from Figure  3 , that the data points for emotions such as happiness and anger were overlapping with each other giving a chance of misclassifying, however, overall classification of happiness and anger was not going to degrade due to being clustered at one side of feature-space. Likewise, two-dimensional representation of t-SNE of RAVDESS dataset is shown in Figure  4 . After using INTERSPEECH 2010 feature-set, there was a compact representation of clusters of anger, sadness, calm, and fear. Based on these observations, it was expected that overall classification accuracy for emotion classes would greatly improve.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Computational Setup",
      "text": "To compare the performance of selected features, the process is divided into two phases: training and testing. The One-Against-All SVM classifier was trained and validated with its respective parameters to obtain the optimal model for 70% of total samples. In the testing phase, the remaining 30% of the samples were used to test the model. The entire framework covering preprocessing, feature selection, and",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Experimental Results And Discussion",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Performance Evaluation Scenarios",
      "text": "To evaluate the framework, along with metrics of accuracy and recall, two new metrics top-2 accuracy and top-3 accuracy are also introduced. Top-2 accuracy is the accuracy when the true class matches with any one of the two most probable classes predicted by the model and top-3 accuracy is the accuracy when the true class matches with any one of 3 most probable classes predicted by the model. The terminology used for evaluation is as follows:\n\nThe ratio of unclassified samples over all samples in the test set is known as the rejection rate. Average classification performance for all emotions after fusion is taken as decision-level-classification-rate. It is defined in equation 1. where DL(t p) m1 denotes the number of true positive utterances of emotion class m1 to mn where n is the number of emotion classes and N is the total number of utterances. To evaluate the emotion classification performance of each individual emotion, the metric we are using is decision level recall rate for emotion mi. Mathematically it is written as given in equation 2.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Dl -Classi F Ication",
      "text": "",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Dlemotion",
      "text": "where DL(t p) mi and DL(tn) mi denote the decision level true positive and true negative utterances for emotion mi, respectively.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Emotion Classification Performance",
      "text": "For analyzing the performance of our feature driven One-Against-All SVM based emotion classification method, the comparison with the state-of-the-art is performed. After that the performance improvement of the proposed method for general test and gender dependent test is presented. The baseline methods considered for comparison are  Yang et al. (2017) , and  Venkataramanan and Rajamohan (2019) . A summary of key features is shown in Table3. In the later sections, we provide decision level emotion classification recall, top-2 accuracy and top-3 accuracy depending on which metric is provided by the reference method.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Performance Comparison For General Test",
      "text": "In general test performance comparison, the data samples from EMO-DB and RAVDESS datasets were used through seven-fold cross validation mechanism. In each round, one-seventh of the samples are kept",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Comparison Of Average Recall",
      "text": "The decision level average recall for each emotion of the two datasets for our method is compared with  Yang et al. (2017)  in Figure  6  and Figure  7 . We can see from Figure  6  that our method outperforms the baseline method in  Yang et al. (2017)  for six out of seven emotions using EMO-DB dataset. In the same way performance gain for seven out of eight emotions in case of RAVDESS dataset is achieved shown in Figure  7 . The average recall values for data samples of EMO-DB and RAVDESS datasets through seven rounds of cross validation and five upsampling are also given in Table  4 . Here in Table4, the performance gain in terms of percentage is given. Our feature selection mechanism improved the recall rate for most of the emotions on both datasets.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Performance Comparison Using Thresholding Fusion",
      "text": "Here in this section, we are doing a comparison using a thresholding fusion mechanism. Thresholding fusion mechanism is an extension of the normal classification method  Vapnik (1998) . Here it is possible to output the class label only when it has a confidence score greater than certain threshold, otherwise it is rejected. In practice, such type of classification enhancement is needed when we are only concerned with samples having the high accuracy. For example, if a psychologist wanted to know when a patient, having some psychological disorder, at certain times feels emotionally high, thresholding fusion mechanism can be used.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "General Test Comparison",
      "text": "Figure  8  and Figure  9  show the decision-level correct classification rate when samples are rejected by gauging the confidence score.\n\nIt can be seen that by rejecting about 50% of the samples, we are able to gain above 90% accuracy rate. The performance gain as compared to the baseline study is given in Figure  8  and Figure  9  for EMO-DB and RAVDESS datasets respectively. This scheme of gaining a high accuracy rate at the cost of rejecting samples could be beneficial in situations where high accuracy for precise samples is required as compared to including all samples with low accuracy.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Gender-Dependent Comparison",
      "text": "Decision level correct classification rate for gender-dependent tests for male and female speakers are shown in Figure  10  and Figure  11  respectively. The features selected for gender-specific tests were based on male and female utterances and were not the same as the general tests. The performance gain of about 10% is achieved by rejecting 50% of in Table5. The gender-dependent accuracy score achieved on EMO-DB dataset is also compared with the results reported in  Tawari and Trivedi (2010) . The accuracy results averaged for both male and female speakers in our work are 85.47%, whereas 84% in the work by  Tawari and Trivedi (2010) . These results indicate the effectiveness of using appropriately tuned features along with classifier.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Dataset",
      "text": "Metric Venkataramanan and Rajamohan (2019)  Tawari and Trivedi (2010)",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Conclusion",
      "text": "In this paper, we presented a strategy to improve the performance of speech emotion recognition frameworks. We used a feature selection strategy based on INTERSPEECH 2010 challenge feature-set with PCA. Here we reanalyzed and extended the existing model of multiclass SVM with thresholding fusion for speech-based emotion classification. Our in-depth analysis of feature-space through t-SNE showed that selected features represent most of the emotion-related information. The datasets used for evaluation were EMO-DB and RAVDESS having emotions of anger, calm, disgust, fear, happiness, sad, surprise, and neutral. We used PCA, where subsets of features were fed separately and finally fused to reduce the features from 1582 to 100. We used the One-Against-All SVM model for emotion classification. The performance of feature-driven SER method was compared with the baseline methods. The experimental results demonstrate the effectiveness of the selected features as the method improved the performance of speech emotion recognition. In future, it is anticipated that further investigation of various features will bring additional improvement to the recognition accuracy.",
      "page_start": 12,
      "page_end": 12
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: and Figure 2, although we can see some clusters, however, most of the emotions are evenly",
      "page": 4
    },
    {
      "caption": "Figure 1: 2 Dimensional t-SNE graph of features used in Yang et al. (2017) for EMO-DB dataset",
      "page": 5
    },
    {
      "caption": "Figure 2: 2 Dimensional t-SNE graph of features used in Yang et al. (2017) for RAVDESS dataset",
      "page": 5
    },
    {
      "caption": "Figure 3: for the EMO-DB dataset, emotion categories of sad, neutral, and boredom were",
      "page": 5
    },
    {
      "caption": "Figure 3: , that the data points for emotions such as happiness and",
      "page": 5
    },
    {
      "caption": "Figure 3: Two-dimensional t-SNE graphs of INTERSPEECH 2010 feature-set after applying PCA for",
      "page": 6
    },
    {
      "caption": "Figure 4: 2 Dimensional t-SNE graphs of features after using INTERSPEECH 2010 feature-set and",
      "page": 6
    },
    {
      "caption": "Figure 5: EXPERIMENTAL RESULTS AND DISCUSSION",
      "page": 6
    },
    {
      "caption": "Figure 5: The computational model is divided into modules such as A: Preprocessing, B: Feature",
      "page": 7
    },
    {
      "caption": "Figure 6: and Figure 7.",
      "page": 8
    },
    {
      "caption": "Figure 6: that our method outperforms the baseline method in Yang et al. (2017) for",
      "page": 8
    },
    {
      "caption": "Figure 7: The average recall values",
      "page": 8
    },
    {
      "caption": "Figure 6: Decision-level emotion classiﬁcation recall (%) for each individual emotion for our method",
      "page": 8
    },
    {
      "caption": "Figure 7: Decision-level emotion classiﬁcation recall (%) for each individual emotion for our method",
      "page": 9
    },
    {
      "caption": "Figure 8: and Figure 9 show the decision-level correct classiﬁcation rate when samples are rejected by",
      "page": 9
    },
    {
      "caption": "Figure 8: Decision-level correct classiﬁcation rate versus rejection rate for general test of our method",
      "page": 10
    },
    {
      "caption": "Figure 8: and Figure 9 for EMO-DB",
      "page": 10
    },
    {
      "caption": "Figure 9: Decision-level correct classiﬁcation rate versus rejection rate for general test of our method",
      "page": 10
    },
    {
      "caption": "Figure 10: and Figure 11 respectively.",
      "page": 10
    },
    {
      "caption": "Figure 10: Comparison of decision-level correct classiﬁcation rate against rejection rate for gender",
      "page": 11
    },
    {
      "caption": "Figure 11: Comparison of decision-level correct classiﬁcation rate against rejection rate for gender",
      "page": 11
    },
    {
      "caption": "Figure 10: and Figure 11 that the emotion classiﬁcation",
      "page": 11
    }
  ],
  "tables": [
    {
      "caption": "Table 1: Emotional Speech Databases (EMO-DB)",
      "page": 3
    },
    {
      "caption": "Table 2: Emotional Speech Databases (RAVDESS)",
      "page": 4
    },
    {
      "caption": "Table 3: Comparison of the key characteristics of the proposed method with baseline methods of speech",
      "page": 8
    },
    {
      "caption": "Table 4: Figure 6. Decision-level emotion classiﬁcation recall (%) for each individual emotion for our method",
      "page": 8
    },
    {
      "caption": "Table 4: Performance gain of our method with Yang et al. (2017) in terms of average recall for each",
      "page": 9
    },
    {
      "caption": "Table 5: Performance comparison of our method with Venkataramanan and Rajamohan (2019) with",
      "page": 12
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Speech emotion recognition: Emotional models, databases, features, preprocessing methods, supporting modalities, and classifiers",
      "authors": [
        "M Akc ¸ay",
        "K Oguz"
      ],
      "year": "2020",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "2",
      "title": "A novel real-time emotion detection system from audio streams based on bayesian quadratic discriminate classifier for adas",
      "authors": [
        "F Al Machot",
        "A Mosa",
        "K Dabbour",
        "A Fasih",
        "C Schwarzlmuller",
        "M Ali",
        "K Kyamakya"
      ],
      "year": "2011",
      "venue": "Nonlinear Dynamics and Synchronization (INDS) & 16th Int'l Symposium on Theoretical Electrical Engineering (ISTET)"
    },
    {
      "citation_id": "3",
      "title": "Features and classifiers for emotion recognition from speech: a survey from 2000 to 2011",
      "authors": [
        "C.-N Anagnostopoulos",
        "T Iliou",
        "I Giannoukos"
      ],
      "year": "2015",
      "venue": "Artificial Intelligence Review"
    },
    {
      "citation_id": "4",
      "title": "Data-driven analysis of emotion in text using latent affective folding and embedding",
      "authors": [
        "J Bellegarda"
      ],
      "year": "2013",
      "venue": "Computational Intelligence"
    },
    {
      "citation_id": "5",
      "title": "A database of german emotional speech",
      "authors": [
        "F Burkhardt",
        "A Paeschke",
        "M Rolfes",
        "W Sendlmeier",
        "B Weiss"
      ],
      "year": "2005",
      "venue": "Ninth European Conference on Speech Communication and Technology"
    },
    {
      "citation_id": "6",
      "title": "Speech emotion recognition: Features and classification models",
      "authors": [
        "L Chen",
        "X Mao",
        "Y Xue",
        "L Cheng"
      ],
      "year": "2012",
      "venue": "Digital signal processing"
    },
    {
      "citation_id": "7",
      "title": "Survey on speech emotion recognition: Features, classification schemes, and databases",
      "authors": [
        "M El Ayadi",
        "M Kamel",
        "F Karray"
      ],
      "year": "2011",
      "venue": "Pattern Recognition"
    },
    {
      "citation_id": "8",
      "title": "The role of voice quality in communicating emotion, mood and attitude",
      "authors": [
        "C Gobl",
        "A Chasaide"
      ],
      "year": "2003",
      "venue": "Speech communication"
    },
    {
      "citation_id": "9",
      "title": "Audio-visual feature selection and reduction for emotion classification",
      "authors": [
        "S Haq",
        "P Jackson",
        "J Edge"
      ],
      "year": "2008",
      "venue": "Proc. Int. Conf. on Auditory-Visual Speech Processing (AVSP'08)"
    },
    {
      "citation_id": "10",
      "title": "Depression severity classification from speech emotion",
      "authors": [
        "S Harati",
        "A Crowell",
        "H Mayberg",
        "S Nemati"
      ],
      "year": "2018",
      "venue": "2018 40th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)"
    },
    {
      "citation_id": "11",
      "title": "Automatic speech based emotion recognition using paralinguistics features",
      "authors": [
        "J Hook",
        "F Noroozi",
        "O Toygar",
        "G Anbarjafari"
      ],
      "year": "2019",
      "venue": "Bulletin of the Polish Academy of Sciences. Technical Sciences"
    },
    {
      "citation_id": "12",
      "title": "A probabilistic framework for modeling and real-time monitoring human fatigue",
      "authors": [
        "Q Ji",
        "P Lan",
        "C Looney"
      ],
      "year": "2006",
      "venue": "IEEE Transactions on systems, man, and cybernetics-Part A: Systems and humans"
    },
    {
      "citation_id": "13",
      "title": "Emotion recognition on e-learning community to improve the learning outcomes using machine learning concepts: A pilot study",
      "authors": [
        "A Jithendran",
        "P Karthik",
        "S Santhosh",
        "J Naren"
      ],
      "year": "2020",
      "venue": "Smart Systems and IoT: Innovations in Computing"
    },
    {
      "citation_id": "14",
      "title": "Principal component analysis: a review and recent developments",
      "authors": [
        "I Jolliffe",
        "J Cadima"
      ],
      "year": "2016",
      "venue": "Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences"
    },
    {
      "citation_id": "15",
      "title": "Gender differences in the recognition of vocal emotions",
      "authors": [
        "A Lausen",
        "A Schacht"
      ],
      "year": "2018",
      "venue": "Frontiers in Psychology"
    },
    {
      "citation_id": "16",
      "title": "Emotion recognition for affective user interfaces using natural language dialogs",
      "authors": [
        "C Lee",
        "G Lee"
      ],
      "year": "2007",
      "venue": "RO-MAN 2007-The 16th IEEE International Symposium on Robot and Human Interactive Communication"
    },
    {
      "citation_id": "17",
      "title": "Toward detecting emotions in spoken dialogs",
      "authors": [
        "C Lee",
        "S Narayanan"
      ],
      "year": "2005",
      "venue": "IEEE transactions on speech and audio processing"
    },
    {
      "citation_id": "18",
      "title": "The ryerson audio-visual database of emotional speech and song (ravdess): A dynamic, multimodal set of facial and vocal expressions in north american english",
      "authors": [
        "S Livingstone",
        "F Russo"
      ],
      "year": "2018",
      "venue": "PloS one"
    },
    {
      "citation_id": "19",
      "title": "The relevance of voice quality features in speaker independent emotion recognition",
      "authors": [
        "M Lugger",
        "B Yang"
      ],
      "year": "2007",
      "venue": "2007 IEEE International Conference on Acoustics, Speech and Signal Processing-ICASSP'07"
    },
    {
      "citation_id": "20",
      "title": "A hybrid latent space data fusion method for multimodal emotion recognition",
      "authors": [
        "S Nemati",
        "R Rohani",
        "M Basiri",
        "M Abdar",
        "N Yen",
        "V Makarenkov"
      ],
      "year": "2019",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "21",
      "title": "Speech emotion recognition using hidden markov models",
      "authors": [
        "T Nwe",
        "S Foo",
        "De Silva"
      ],
      "year": "2003",
      "venue": "Speech communication"
    },
    {
      "citation_id": "22",
      "title": "Multimodal analysis of upper-body gestures, facial expressions and speech",
      "authors": [
        "S Ozkul",
        "E Bozkurt",
        "S Asta",
        "Y Yemez",
        "E Erzin"
      ],
      "year": "2012",
      "venue": "Procceedings of the 4th International Workshop on Corpora for Research on Emotion Sentiment and Social Signals"
    },
    {
      "citation_id": "23",
      "title": "A novel feature selection method for speech emotion recognition",
      "authors": [
        "T Özseven"
      ],
      "year": "2019",
      "venue": "Applied Acoustics"
    },
    {
      "citation_id": "24",
      "title": "Speech acoustic (spac): A novel tool for speech feature extraction and classification",
      "authors": [
        "T Özseven",
        "M Dügenci"
      ],
      "year": "2018",
      "venue": "Applied Acoustics"
    },
    {
      "citation_id": "25",
      "title": "A content analaysis of the research aapproaches in speech emotion recognition",
      "authors": [
        "T Özseven",
        "M Dügenci",
        "A Durmus ¸oglu"
      ],
      "year": "2018",
      "venue": "International Journal of Engineering Sciences & Research Technology"
    },
    {
      "citation_id": "26",
      "title": "Emotion recognition and evaluation of mandarin speech using weighted d-knn classification",
      "authors": [
        "T.-L Pao",
        "Y.-T Chen",
        "J.-H Yeh",
        "Y.-H Chang"
      ],
      "year": "2005",
      "venue": "Proceedings of the 17th conference on computational linguistics and speech processing"
    },
    {
      "citation_id": "27",
      "title": "Mapping emotions into acoustic space: The role of voice production",
      "authors": [
        "S Patel",
        "K Scherer",
        "E Björkner",
        "J Sundberg"
      ],
      "year": "2011",
      "venue": "Biological psychology"
    },
    {
      "citation_id": "28",
      "title": "Detecting human emotions in a large size of database by using ensemble classification model. Mobile Networks and Applications",
      "authors": [
        "S Prasomphan",
        "S Doungwichain"
      ],
      "year": "2018",
      "venue": "Detecting human emotions in a large size of database by using ensemble classification model. Mobile Networks and Applications"
    },
    {
      "citation_id": "29",
      "title": "An efficient use of principal component analysis in workload characterization-a study",
      "authors": [
        "J Sarkar",
        "S Saha",
        "S Agrawal"
      ],
      "year": "2014",
      "venue": "AASRI Procedia"
    },
    {
      "citation_id": "30",
      "title": "Comparing the acoustic expression of emotion in the speaking and the singing voice",
      "authors": [
        "K Scherer",
        "J Sundberg",
        "L Tamarit",
        "G Salomão"
      ],
      "year": "2015",
      "venue": "Computer Speech & Language"
    },
    {
      "citation_id": "31",
      "title": "Emotion modelling via speech content and prosody: in computer games and elsewhere",
      "authors": [
        "B Schuller"
      ],
      "year": "2016",
      "venue": "Emotion in Games"
    },
    {
      "citation_id": "32",
      "title": "The interspeech 2009 emotion challenge",
      "authors": [
        "B Schuller",
        "S Steidl",
        "A Batliner"
      ],
      "year": "2009",
      "venue": "Tenth Annual Conference of the International Speech Communication Association"
    },
    {
      "citation_id": "33",
      "title": "The interspeech 2010 paralinguistic challenge",
      "authors": [
        "B Schuller",
        "S Steidl",
        "A Batliner",
        "F Burkhardt",
        "L Devillers",
        "C Müller",
        "S Narayanan"
      ],
      "year": "2010",
      "venue": "Eleventh Annual Conference of the International Speech Communication Association"
    },
    {
      "citation_id": "34",
      "title": "Speech emotion recognition: Two decades in a nutshell, benchmarks, and ongoing trends",
      "authors": [
        "B Schuller"
      ],
      "year": "2018",
      "venue": "Communications of the ACM"
    },
    {
      "citation_id": "35",
      "title": "Speech emotion analysis: Exploring the role of context",
      "authors": [
        "A Tawari",
        "M Trivedi"
      ],
      "year": "2010",
      "venue": "IEEE Transactions on multimedia"
    },
    {
      "citation_id": "36",
      "title": "Evidence for nonlinear sound production mechanisms in the vocal tract",
      "authors": [
        "H Teager",
        "S Teager"
      ],
      "year": "1990",
      "venue": "Speech production and speech modelling"
    },
    {
      "citation_id": "37",
      "title": "Improvement of phone recognition accuracy using speech mode classification",
      "authors": [
        "K Tripathi",
        "K Rao"
      ],
      "year": "2017",
      "venue": "International Journal of Speech Technology"
    },
    {
      "citation_id": "38",
      "title": "Improvement of phone recognition accuracy using speech mode classification",
      "authors": [
        "K Tripathi",
        "K Rao"
      ],
      "year": "2018",
      "venue": "International Journal of Speech Technology"
    },
    {
      "citation_id": "39",
      "title": "Statistical learning theory john wiley & sons hobeken",
      "authors": [
        "V Vapnik"
      ],
      "year": "1998",
      "venue": "Statistical learning theory john wiley & sons hobeken"
    },
    {
      "citation_id": "40",
      "title": "Emotion recognition from speech",
      "authors": [
        "K Venkataramanan",
        "H Rajamohan"
      ],
      "year": "2019",
      "venue": "Emotion recognition from speech",
      "arxiv": "arXiv:1912.10458"
    },
    {
      "citation_id": "41",
      "title": "Speech emotion recognition using fourier parameters",
      "authors": [
        "K Wang",
        "N An",
        "B Li",
        "Y Zhang",
        "L Li"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on affective computing"
    },
    {
      "citation_id": "42",
      "title": "Improve accuracy of speech emotion recognition with attention head fusion",
      "authors": [
        "M Xu",
        "F Zhang",
        "S Khan"
      ],
      "year": "2020",
      "venue": "2020 10th annual computing and communication workshop and conference (CCWC)"
    },
    {
      "citation_id": "43",
      "title": "Enhanced multiclass svm with thresholding fusion for speech-based emotion classification",
      "authors": [
        "N Yang",
        "J Yuan",
        "Y Zhou",
        "I Demirkol",
        "Z Duan",
        "W Heinzelman",
        "M Sturge-Apple"
      ],
      "year": "2017",
      "venue": "International journal of speech technology"
    },
    {
      "citation_id": "44",
      "title": "Interactive robot learning for multimodal emotion recognition",
      "authors": [
        "C Yu",
        "A Tapus"
      ],
      "year": "2019",
      "venue": "International Conference on Social Robotics"
    },
    {
      "citation_id": "45",
      "title": "Robust emotion recognition in noisy speech via sparse representation",
      "authors": [
        "X Zhao",
        "S Zhang",
        "B Lei"
      ],
      "year": "2014",
      "venue": "Neural Computing and Applications"
    },
    {
      "citation_id": "46",
      "title": "Research on speech emotional feature extraction based on multidimensional feature fusion",
      "authors": [
        "C Zheng",
        "C Wang",
        "W Sun",
        "N Jia"
      ],
      "year": "2019",
      "venue": "International Conference on Advanced Data Mining and Applications"
    }
  ]
}