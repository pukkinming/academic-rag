{
  "paper_id": "2211.07307v1",
  "title": "Sentiment Recognition Of Italian Elderly Through Domain Adaptation On Cross-Corpus Speech Dataset",
  "published": "2022-11-14T12:39:41Z",
  "authors": [
    "Francesca Gasparini",
    "Alessandra Grossi"
  ],
  "keywords": [
    "Speech emotion recognition",
    "Sentiment recognition",
    "Domain adaptation",
    "cross-corpus SER",
    "crosslanguage SER"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "The aim of this work is to define a speech emotion recognition (SER) model able to recognize positive, neutral and negative emotions in natural conversations of Italian elderly people. Several datasets for SER are available in the literature. However most of them are in English or Chinese, have been recorded while actors and actresses pronounce short phrases and thus are not related to natural conversation. Moreover only few speeches among all the databases are related to elderly people. Therefore, in this work, a multi-language and multi-age corpus is considered merging a dataset in English, that includes also elderly people, with a dataset in Italian. A general model, trained on young and adult English actors and actresses is proposed, based on XGBoost. Then two strategies of domain adaptation are proposed to adapt the model either to elderly people and to Italian speakers. The results suggest that this approach increases the classification performance, underlining also that new datasets should be collected.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Emotions play a relevant role in defining individuals' behaviours and coordination in humanhuman interactions  [1] . In particular, humans find speech conversations more natural and effective than its written form as way to express themselves  [2] . During conversations, people try to convey their thought not only by words but also by bodily, vocal or facial expressions  [1, 3] . Specifically in vocal expressions the affective state of individuals is expressed both by the linguistic and acoustic information carried by the speech  [4] . For instance, the same sentence said with different intonations can express different emotions by the speaker and, thus, can lead to a different response from the listener  [5] . Therefore, in order to create a natural interaction between humans and computers, the machine must be able to understand emotions from the speaker's voice and consequently adapt. Speech Emotion Recognition (SER) consists of the task of processing and classifying speech signals in order to recognize the emotional state of the speaker  [5, 6] . Systems based on SER have different fields of application, such as health care  [7] , e-learning tutoring  [8] , automotive  [9]  or entertainment  [10, 11] . In particular, these kinds of systems can be employed for the definition of diagnostic tools able to help therapists in detecting psychological disorders  [12]  or for automatically recognising mental state alteration in drivers  [13] . Automatic emotion detection systems can also be used in the call center or mobile communications to detect the emotions of callers and to help agents improving the quality of service  [14, 15] , or in human-robot interactions to support a more natural and social communication between human and machine  [16, 17] . Several researches have been carried out in the field of Speech Emotion Recognition during the last three decades  [18] . In particular, many of these analysis are performed considering only one between linguistic or acoustic information of speech while in recent analysis a multi-modal approach is examined  [19] .\n\nIn our study, we focus only on acoustic information. In this field, both traditional machine learning and deep learning approaches have been taken into account in previous literature. In general, the traditional pipeline in a SER system consists of three steps: signal preprocessing, features extraction and classification  [20] . Concerning features extractions, different set of features have been tested: traditional features extracted by audio signals  [2] , including prosodic (such as pitch, energy and duration), spectral (such as fundamental frequency, Mel Frequency Cepstral Coefficients or Linear Prediction Cepstral Coefficients) and voice quality features (such as jitter or shimmer), as well as deep features extracted by pre-trained networks. In this latter, the audio signals are usually represented as Spectrogram or Scalogram and used as input to pre-trained network to extract features  [21, 22] . With reference to classifiers, in several research such as  [23, 24] , traditional classifiers have been employed. In particular, according to  [25] , the classical classification techniques preferred in SER system are Gaussian Mixture Model, Hidden Markov Model, Artificial Neutral Network, Decision Trees and Support Vector Machine. In few analysis  [26, 27]  also ensemble techniques combining several classifiers have been tested. Deep approaches have been also considered in the last years. In particular, framework using Convolutional Neutral Network (CNN)  [28] , Recurrent Neural Network (RNN)  [29]  and Long Short-Term memory network (LSTM)  [30]  have been evaluated, using both traditional features  [31]  and raw audio signals. In some cases, also mechanism of attention  [32, 33]  or auto encoding  [34]  have been added to classifiers in order to increase performance. The main SER approaches have been summarized in review manuscripts such as  [6]  or  [25] . Despite the huge number of analyzes carried out, there are still numerous issues that make difficult to recognize emotions in speech. In  [18]  some of these challenges and the approaches tested so far to solve them are summarized. In particular, speech emotion recognition algorithms struggle in recognize emotions when people of different language or age are considered. In literature there are many datasets collected for SER purpose. These corpora can be classified into three groups with reference to how emotional speech is generated  [35] : i) Acted datasets, where the data are collected from actors/actresses that try to simulate emotions; ii) Evoked or Elicited datasets, where the subjects are involved into situations especially created to evoke or induce certain emotions; and iii) Spontaneous or Natural datasets, which contain more authentic emotions as collected from real-world situations like call-centers or public places  [18] . Most of the datasets available in the literature are composed of recited speeches  [36] , while only few of them consider natural conversations  [37, 38, 39] . Moreover, the considered languages are mainly English and Chinese. It has been demonstrated that language has a strong influence in how emotions are expressed  [24] , and thus multi-language datasets have been proposed  [40, 41] . Age is another factor that influences the acoustic characteristics of the voice, especially in the case of elderly  [42, 43] . However, this is still an open field of research and few works face the problem of SER in case of elderly, or varying the age  [22, 33, 44, 45] , and old subjects are rarely present in available datasets  [46, 47, 48, 38] .\n\nIn this work we consider the problem of SER, considering elderly Italian people. Moreover we focus on positive, neutral and negative emotions. We propose to consider a multi-language, multi-aged approach, considering a cross-corpus dataset, described in Section 2. We start from a general model trained on an English dataset of young and adult subjects, and we refine this model to adapt either to elderly and Italian language, as described in Section 3, adopting two different domain adaptation techniques. In Section 4 preprocessing of raw data, feature extraction and data augmentation, needed to apply the proposed solutions are presented. The results, discussed in Section 5, underline the potentialities and the limits of the proposed approaches, while future perspective are drawn in the Conclusions.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Cross-Corpus Dataset",
      "text": "In this work, we consider two datasets available in the literature, labeled with emotions, and characterized by the presence of elderly subjects or by the presence of Italian sentences: the CRowd-sourced Emotional Multimodal Actors dataset CREMA-D  [47]  and EMOVO  [49] .  [47]  is a free audio-visual dataset collected to investigate facial and vocal expressions and perception of acted emotions. It consists of 7442 audio and video recordings of professional actors playing 12 utterances each one expressed in six emotional states (happy, sad, anger, fear, disgust and neutral) at different intensity levels. In the first utterance, the actors were directed to simulate each emotion in three levels of intensity (low, medium and high) while, for the other eleven sentences, they were free to express the emotion at their preferred intensity. The sentences selected for the experiment are in English and have a neutral semantic content. In total, 48 actors and 43 actresses of different ages and ethnicity were involved in the experiments, including 6 elderly with more than 60 years and 85 adults aged between 20 and 59 years. For the purpose of our analysis, the two groups of subjects are considered separately with a total of 492 signals for elderly, named hereinafter CREMA-D-ELD, and 6950 signals for adults (CREMA-D-ADULT ). For further details of CREMA-D dataset, please refer to the reference manuscript  [47] .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Crema-D",
      "text": "EMOVO  [49]  is an acted free audio speech emotional dataset based on the Italian language. The corpus was collected from six young Italian actors (3 male and 3 female) with a mean age of 27.1 (no elderly actors were involved). Similarly to CREMA-D, in the experimental protocol, 14 utterances had to be performed by the actors simulating different emotional states. In particular,  for each utterance, 7 affective states were considered: neutral, disgust, fear, anger, joy, surprise and sadness. The total number of utterances collected in the dataset is 588, with a mean of 98 signals per actor. More details about EMOVO can be found in  [49]  In Table  1  the main information about these two datasets are summarized.\n\nIn both the selected datasets, the signals are labeled using the six basic emotions defined by Ekman. In order to use these datasets in our analysis, each emotion has been converted into its respective sentiment according with the mapping defined in  [50] . In particular, we have considered anger, fear, disgust and sadness as negative sentiments, happy (or joy) as positive sentiment and neutral as neutral sentiment. All the EMOVO signals labeled as \"surprise\" has been instead excluded from the analysis as difficult to be mapped into a single sentiment class  [50] . The distribution of the utterances in the three sentiment classes is shown in Figure  1  for the two datasets considered.\n\nConcerning the sentiment analysis, two other datasets are usually adopted in Speech Sentiment Recognition researches: Multimodal EmotionLines Dataset (MELD)  [50]  and CMU Multimodal Opinion Sentiment and Emotion Intensity (CMU-MOSEI)  [51] . The first  [50]  is a data corpus composed by more then 13000 utterances from 1433 dialogues from the TV-series Friends and labeled with three sentiment class: negative, positive and neutral. CMU-MOSEI  [51] , instead, contains 23453 annotated video-clips from 250 different topics, gathered from online video sharing websites and labeled with sentiment in Likert scale. Despite both the datasets are directly labeled with sentiment, they were excluded from our analysis. In particular, concerning MELD, the dataset has been discarded due to the presence, in several audios, of laugh tracks or multiple voices overlapping the main actor's speech. This makes the audio signal very noisy and makes it difficult to identify which part of the audio is related to the labelled sentiment.\n\nWith reference to CMU-MOSEI, instead, the dataset has been excluded from the study because of the lack of the subject's age that makes impossible to separate signals collected from elderly from the one's collected from young or adults.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Data Adaptation Strategies",
      "text": "The proposed analysis considers two research hypotheses: In all the experiments performed, the gradient boosted decision trees algorithm implemented as XGBoost  [52]  has been selected as classification model while two different instance weighting domain adaptation strategies have been tested:\n\n• the Kullback-Leiber Important Estimation Procedure (KLIEP) strategy  [53]  that assigns a weight to the training instances during the classifier learning task in order to minimize the Kullback-Leibler divergence between train and target distributions. In our analysis we have considered the supervised implementation of this algorithm using \"rbf\" as Kernel with two different gamma: 0.1 and 1. • the Transfer AdaBoost for Classification (TrAdaBoost)  [54]  is a supervised domain adaptation strategy that extends boosting-based learning algorithms to the field of transfer learning. In particular, at each iteration, the algorithm trains a new weak classifier giving less importance to the training instances poorly predicted in previous iterations while emphasising the target samples correctly recognized. The final model is the combination of the last half computed estimators weighted according to their relevance. The number of iterations selected in our experiments is 10.\n\nThe application of these two strategies requires to split the data into three distinct sets: i) Training (or Source) set made up of a large amount of labeled data used to train the general model; ii) Target set consisting of few samples belonging to a new but related domain that are used to adapt the general model to this new data distribution and iii) Test set composed by data similar to Target set and used to evaluate the model performances. In our experiments, the definition of these three sets changes according to the research hypothesis considered. In particular, in multi-age analysis, the data of CREMA-D-ADULT have been used as Training set while Target and Test sets have been defined as subsets of CREMA-D-ELD. Instead, in multilanguage analysis, the training of the general model is performed using CREMA-D-ADULT data while Target and Test sets are both defined as partitions of EMOVO data.\n\nDifferent validation strategies have been tested to partition the data of CREMA-D-ELD and EMOVO into Target and Test set:\n\n• Leave One Subject Out (LOSO) Cross Validation strategy, where the folds are partitioned according to subject and thus, at each iteration, all the data of a single subject are used as Test set while the data of the remaining subjects are used as Target set. • Leave One Utterance Out (LOUO) Cross Validation strategy, where the folds are defined according to the pronounced utterances, thus at each iteration, all the data related to a single utterance are used to test the model while the data of the remaining utterances are used as Target set.\n\nTo test the performances of our classification models, several well-known evaluation metrics are computed  [55]  including the accuracy, single class F1-score, evaluated as the harmonic mean of single class precision and recall, and macro F1-score  [56]  computed as the unweighted mean of the single class F1-score.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Model Input Data",
      "text": "To apply the strategies of domain adaptation described in the previous section, preprocessing, feature extraction, and data augmentation to balance the classes have been performed on raw data. The whole process is depicted in Figure  2 .",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Preprocessing",
      "text": "The audio signals of each dataset are preprocessed to extract only the information concerning the target speaker's voice. In particular, the audio clips were first converted from stereo to mono by averaging samples across the two channels. Then, each signal was filtered using a pass-band Butterworth filter with lower cutoff frequency at 300 Hz and upper cutoff frequency at 3000 Hz to removes the spectral components out of the voice frequency range  [57] .",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Feature Extraction",
      "text": "From the pre-processed signals, the eGeMAPS acoustic feature set was extracted using the python library implementation of openSMILE toolkit  [58] . The eGeMAPS feature set (extended Geneva Minimalistic Acoustic Parameter Set)  [59]  is a set of audio features proposed for affective analysis in voice signals. It consists of 25 Low Level Descriptor (LLD) features including energy, frequency, cepstral, spectral and dynamic parameters. In order to summarize the variation of these parameters over the time windows, some high level functional features are extracted using statistical functions as arithmetic mean, standard deviation or percentile. Applying these statistics, a total of 88 features have been extracted for each considered signal. The extracted features have been normalized by z-scoring in order to reduce inter signals differences.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Data Augmentation",
      "text": "Only for Training (or Source) and Target dataset, the feature extraction step has been followed by data augmentation. In both the datasets, the cardinality of the negative sentiment class is four times greater then positive or neutral ones. This is due to an imbalance among the number of emotions mapped as negative (angry, fear, sadness, disgust) and the number of emotions mapped as positive (happy) and neutral (neutral) in the selected emotion-sentiment transformation. In order to create more balanced classes, a two steps procedure have been applied to training and target data according to the experiment considered . First the majority class have been under-sampled, discarding randomly half of the negative instances. In this process, the discarded elements have been selected trying to keep balanced the number of elements for each negative emotions. Then an oversampling strategy based on SMOTE algorithm  [60]  has been applied to increase the number of samples in the two minority classes (positive and neutral). SMOTE (Synthetic Minority Oversampling TEchnique)  [60]  is an oversampling method that random generates new synthetic data for the minority class starting from the original data points. In particular, at each iteration, the algorithm selects one of the k-nearest neighbors of a random minority class element and create new artificial elements linear interpolating the two instances using a random number between zero and one. The procedure is repeated until the cardinality of the classes is balanced.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Results And Discussion",
      "text": "The aim of this work is define a classification model able to automatically recognize three sentiment states (positive, neutral and negative) using acoustic features extracted from speech when different age and language are considered. In particular, two different experiments have been carried out to evaluate the research hypothesis described in Section 3: domain adaptation on elderly and domain adaptation on language.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Domain Adaptation On Elderly",
      "text": "In the first analysis, a multi-age corpus sentiment classification is considered. As described in Section 3, the two parts of CREMA-D dataset have been used respectively for Training set (CREMA-D-ADULT) and Target and Test set (CREMA-D-ELD). For each domain adaptation strategy, two different evaluation methods are tested: LOSO and LOUO. The results achieved in these experiments are compared with the performances reached by the XGBoost model when no domain adaptation strategy is applied. In this case, thus, the classifier is trained on CREMA-D-ADULT data and tested on the independent dataset CREMA-D-ELD. The classification settings considered in the analysis are summarized in Table  2 . For each of these analyses, Table  3  reported the classification performance achieved by the XGBoost classifiers in terms of accuracy, macro F1-score and single class F1-score. The results show how, in case of elderly, the use of domain adaptation techniques does not significantly increase the performances of the classification This difference can be due to the presence of a higher number of different instances in the negative class than in the other two classes where several instances were artificially created using SMOTE data augmentation strategy. From these preliminary results, it seems that data adaptation does not increase the performance of the proposed SER model. This is probably related to several aspects. The elderly here considered are actors or actresses, and thus they are not so significantly different from a population of young and adult persons. Moreover the elderly are only 6, of which only one is a female. A more realistic dataset should be consider to proper verify this research question.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "Domain Adaptation On Language",
      "text": "The second part of our study focused on speech sentiment recognition when multi-languagecorpus datasets are taken into account. The trials tested for this analysis are summarized in Table  4 . Two different datasets were used: the English dataset CREMA-D-ADULT, used to train the model, and the Italian dataset EMOVO, as Target and Test set. Furthermore, similarly to elderly, the results obtained varying the domain adaptation technique (KLIEP and TrAdaBoost) and evaluation strategy (LOSO, LOUO) were compared with the performance reached by the classification model trained without domain adaptation. The values of accuracy, macro-F1 score and per-class F1-scores achieved in the different experiments are reported in Table  5 .\n\nFrom the analysis of the results, it emerges how the best performances in both the validation strategies were obtained applying the TrAdaBoost domain adaptation method. In particular, In almost all the trials, the use of domain adaptation techniques allowed to better recognize the instances of Positive class, reaching often more balanced classification performances in identify the three sentiments. Nevertheless, the Negative sentiment is still the class better recognized from all the classification models examined, thus confirming what has already been observed on the elderly analysis.\n\nFinally, the last remark concerns the performance differences between the two validation strategies applied. In particular, the partition of Target and Test set using utterances allows to achieve better results than the one based on subjects. This can be explained by the fact that, in addition to language, the division by utterance also takes into account the difference between people with regard to personal vocal characteristics or how they express their emotions. Using this method, data from each of the analyzed subjects appear in each of the folds generated, allowing the classification model to better learn about vocal timbre differences or differences in the individuals' personalities. However, it is worth to underline that both the datasets analyzed are acted, making perhaps more similar how the same subject expresses the same emotion, also in different sentences. For this reason, in future analyzes, it may be necessary to validate the hypotheses here proposed on new natural datasets collected in real situations.",
      "page_start": 9,
      "page_end": 11
    },
    {
      "section_name": "Conclusion",
      "text": "The sentiment emotion recognition task is still an open field of research, especially when considering different languages and ages. In particular in the case of our interest, Italian elderly, no datasets are available in the literature. Domain adaptation techniques could partially solve this lack of data. However our preliminary results indicate that there is the urgency of a more realistic collection of data, that also faces the need of considering different ages. Domain adaptation techniques seem to better perform in case of cross-language datasets, paving the way for further researches in this direction. For what concerns the lack of performance increase applying domain adaptation models in the case of multi-age corpus, conclusions can not be drawn, due to the peculiarity of the datasets available (where the collected speeches were recorded by professional actors) and given the low presence of elderly people.",
      "page_start": 11,
      "page_end": 11
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Number of utterances mapped as negative, positive or neutral in the two datasets CREMA-D",
      "page": 4
    },
    {
      "caption": "Figure 2: 4.1. Preprocessing",
      "page": 6
    },
    {
      "caption": "Figure 2: Pipeline used, in the analysis, for extracting features from the signals of Training and Target",
      "page": 7
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "DA Strategy": "",
          "Training Set": "CREMA-D-ADULT",
          "Target set\nTest set": "CREMA-D-ELD",
          "Validation\nStrategy": ""
        },
        {
          "DA Strategy": "No\nDomain\nAdaptation",
          "Training Set": "all 85 young and\nadults subjects",
          "Target set\nTest set": "no target dataset",
          "Validation\nStrategy": "Training / Test\nindependent"
        },
        {
          "DA Strategy": "KLIEP",
          "Training Set": "all 85 young and\nadults subjects",
          "Target set\nTest set": "5 elderly x 12 utterances\n11 utterances x 6 elderly",
          "Validation\nStrategy": "LOSO\nLOUO"
        },
        {
          "DA Strategy": "TrAdaBoost",
          "Training Set": "all 85 young and\nadults subjects",
          "Target set\nTest set": "5 elderly x 12 utterances\n11 utterances x 6 elderly",
          "Validation\nStrategy": "LOSO\nLOUO"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table 5: Fromtheanalysisoftheresults,itemergeshowthebestperformancesinboththevalidation",
      "data": [
        {
          "Classifier": "XGBoostclassifier",
          "DA Strategy\nValidation\nStrategy": "No Domain\nTraining / Test\nAdaptation\nindependent",
          "Macro\nF1-score": "62%",
          "Negative\nNeutral\nPositive\nF1-score\nF1-score\nF1-score": "0,51\n0,79\n0,55",
          "Accuracy": "70%"
        },
        {
          "Classifier": "",
          "DA Strategy\nValidation\nStrategy": "LOSO\nKLIEP\nLOUO",
          "Macro\nF1-score": "60%\n60%",
          "Negative\nNeutral\nPositive\nF1-score\nF1-score\nF1-score": "0,76\n0,57\n0,49\n0,76\n0,56\n0,47",
          "Accuracy": "67%\n67%"
        },
        {
          "Classifier": "",
          "DA Strategy\nValidation\nStrategy": "LOSO\nTrAdaBoost\nLOUO",
          "Macro\nF1-score": "62%\n62%",
          "Negative\nNeutral\nPositive\nF1-score\nF1-score\nF1-score": "0,79\n0,56\n0,52\n0,78\n0,55\n0,52",
          "Accuracy": "70%\n69%"
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "DA Strategy": "",
          "Training Set": "CREMA-D-ADULT",
          "Target set\nTest set": "EMOVO",
          "Validation\nStrategy": ""
        },
        {
          "DA Strategy": "No\nDomain\nAdaptation",
          "Training Set": "all 85 young and\nadults subjects",
          "Target set\nTest set": "no target dataset",
          "Validation\nStrategy": "Training / Test\nindependent"
        },
        {
          "DA Strategy": "KLIEP",
          "Training Set": "all 85 young and\nadults subjects",
          "Target set\nTest set": "6 subjects x 14 utterances\n13 utterances x 6 subjects",
          "Validation\nStrategy": "LOSO\nLOUO"
        },
        {
          "DA Strategy": "TrAdaBoost",
          "Training Set": "all 85 young and\nadults subjects",
          "Target set\nTest set": "5 subjects x 14 utterances\n13 utterances x 6 subjects",
          "Validation\nStrategy": "LOSO\nLOUO"
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Classifier": "XGBoostclassifier",
          "DA Strategy\nValidation\nStrategy": "No Domain\nTraining / Test\nAdaptation\nindependent",
          "Macro\nF1-score": "35%",
          "Negative\nNeutral\nPositive\nF1-score\nF1-score\nF1-score": "0,07\n0,71\n0,28",
          "Accuracy": "57%"
        },
        {
          "Classifier": "",
          "DA Strategy\nValidation\nStrategy": "LOSO\nKLIEP\nLOUO",
          "Macro\nF1-score": "33%\n32%",
          "Negative\nNeutral\nPositive\nF1-score\nF1-score\nF1-score": "0,64\n0,19\n0,16\n0,68\n0,22\n0,06",
          "Accuracy": "48%\n51%"
        },
        {
          "Classifier": "",
          "DA Strategy\nValidation\nStrategy": "LOSO\nTrAdaBoost\nLOUO",
          "Macro\nF1-score": "44%\n85%",
          "Negative\nNeutral\nPositive\nF1-score\nF1-score\nF1-score": "0,68\n0,25\n0,39\n0,91\n0,90\n0,74",
          "Accuracy": "56%\n88%"
        }
      ],
      "page": 10
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Reading emotions, reading people: Emotion perception and inferences drawn from perceived emotions",
      "authors": [
        "J Lange",
        "M Heerdink",
        "G Van Kleef"
      ],
      "year": "2022",
      "venue": "Current Opinion in Psychology"
    },
    {
      "citation_id": "2",
      "title": "Survey on speech emotion recognition: Features, classification schemes, and databases",
      "authors": [
        "M Ayadi",
        "M Kamel",
        "F Karray"
      ],
      "year": "2011",
      "venue": "Pattern recognition"
    },
    {
      "citation_id": "3",
      "title": "Databases, features and classifiers for speech emotion recognition: a review",
      "authors": [
        "M Swain",
        "A Routray",
        "P Kabisatpathy"
      ],
      "year": "2018",
      "venue": "International Journal of Speech Technology"
    },
    {
      "citation_id": "4",
      "title": "Features and classifiers for emotion recognition from speech: a survey from 2000 to 2011",
      "authors": [
        "C.-N Anagnostopoulos",
        "T Iliou",
        "I Giannoukos"
      ],
      "year": "2015",
      "venue": "Artificial Intelligence Review"
    },
    {
      "citation_id": "5",
      "title": "Design of aging smart home products based on radial basis function speech emotion recognition",
      "authors": [
        "X Wu",
        "Q Zhang"
      ],
      "year": "2022",
      "venue": "Frontiers in Psychology"
    },
    {
      "citation_id": "6",
      "title": "Speech emotion recognition: Emotional models, databases, features, preprocessing methods, supporting modalities, and classifiers",
      "authors": [
        "M Akçay",
        "K Oğuz"
      ],
      "year": "2020",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "7",
      "title": "Acoustical properties of speech as indicators of depression and suicidal risk",
      "authors": [
        "D France",
        "R Shiavi",
        "S Silverman",
        "M Silverman",
        "M Wilkes"
      ],
      "year": "2000",
      "venue": "IEEE transactions on Biomedical Engineering"
    },
    {
      "citation_id": "8",
      "title": "Using system and user performance features to improve emotion detection in spoken tutoring dialogs",
      "authors": [
        "A Hua",
        "D Litman",
        "K Forbes-Riley",
        "M Rotaru",
        "J Tetreault",
        "A Purandare"
      ],
      "year": "2006",
      "venue": "Proceedings of the Annual Conference of the International Speech Communication Association"
    },
    {
      "citation_id": "9",
      "title": "Towards multimodal emotion recognition in german speech events in cars using transfer learning",
      "authors": [
        "D Cevher",
        "S Zepf",
        "R Klinger"
      ],
      "year": "2019",
      "venue": "Towards multimodal emotion recognition in german speech events in cars using transfer learning",
      "arxiv": "arXiv:1909.02764"
    },
    {
      "citation_id": "10",
      "title": "Emotion recognition and its application to computer agents with spontaneous interactive capabilities",
      "authors": [
        "R Nakatsu",
        "J Nicholson",
        "N Tosa"
      ],
      "year": "1999",
      "venue": "Proceedings of the seventh ACM international conference on Multimedia (Part 1)"
    },
    {
      "citation_id": "11",
      "title": "Multimodal affect recognition in an interactive gaming environment using eye tracking and speech signals",
      "authors": [
        "A Alhargan",
        "N Cooke",
        "T Binjammaz"
      ],
      "year": "2017",
      "venue": "Proceedings of the 19th ACM international conference on multimodal interaction"
    },
    {
      "citation_id": "12",
      "title": "Detection of clinical depression in adolescents' speech during family interactions",
      "authors": [
        "L.-S Low",
        "N Maddage",
        "M Lech",
        "L Sheeber",
        "N Allen"
      ],
      "year": "2010",
      "venue": "IEEE Transactions on Biomedical Engineering"
    },
    {
      "citation_id": "13",
      "title": "A novel real-time emotion detection system from audio streams based on bayesian quadratic discriminate classifier for adas",
      "authors": [
        "F Al Machot",
        "A Mosa",
        "K Dabbour",
        "A Fasih",
        "C Schwarzlmüller",
        "M Ali",
        "K Kyamakya"
      ],
      "year": "2011",
      "venue": "Proceedings of the Joint INDS'11 & ISTET'11"
    },
    {
      "citation_id": "14",
      "title": "Two-stream emotion recognition for call center monitoring",
      "authors": [
        "P Gupta",
        "N Rajput"
      ],
      "year": "2007",
      "venue": "Eighth Annual Conference of the International Speech Communication Association"
    },
    {
      "citation_id": "15",
      "title": "Negative emotions detection as an indicator of dialogs quality in call centers",
      "authors": [
        "C Vaudable",
        "L Devillers"
      ],
      "year": "2012",
      "venue": "2012 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "16",
      "title": "Playing a different imitation game: Interaction with an empathic android robot",
      "authors": [
        "F Hegel",
        "T Spexard",
        "B Wrede",
        "G Horstmann",
        "T Vogt"
      ],
      "year": "2006",
      "venue": "2006 6th IEEE-RAS International Conference on Humanoid Robots"
    },
    {
      "citation_id": "17",
      "title": "Affect and emotion in human-computer interaction",
      "authors": [
        "C Jones",
        "A Deeming"
      ],
      "year": "2008",
      "venue": "Affect and emotion in human-computer interaction"
    },
    {
      "citation_id": "18",
      "title": "A survey of speech emotion recognition in natural environment",
      "authors": [
        "M Fahad",
        "A Ranjan",
        "J Yadav",
        "A Deepak"
      ],
      "year": "2021",
      "venue": "Digital Signal Processing"
    },
    {
      "citation_id": "19",
      "title": "Survey on bimodal speech emotion recognition from acoustic and linguistic information fusion",
      "authors": [
        "B Atmaja",
        "A Sasou",
        "M Akagi"
      ],
      "year": "2022",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "20",
      "title": "Speech emotion recognition: A review",
      "authors": [
        "A Thakur",
        "S Dhull"
      ],
      "year": "2021",
      "venue": "Advances in Communication and Computational Technology"
    },
    {
      "citation_id": "21",
      "title": "Real time speech emotion recognition using rgb image classification and transfer learning",
      "authors": [
        "M Stolar",
        "M Lech",
        "R Bolia",
        "M Skinner"
      ],
      "year": "2017",
      "venue": "11th International Conference on Signal Processing and Communication Systems (ICSPCS)"
    },
    {
      "citation_id": "22",
      "title": "Speech emotion recognition among elderly individuals using multimodal fusion and transfer learning",
      "authors": [
        "G Boateng",
        "T Kowatsch"
      ],
      "venue": "Companion Publication of the 2020 International Conference on Multimodal 2020"
    },
    {
      "citation_id": "23",
      "title": "Study of feature combination using hmm and svm for multilingual odiya speech emotion recognition",
      "authors": [
        "M Swain",
        "S Sahoo",
        "A Routray",
        "P Kabisatpathy",
        "J Kundu"
      ],
      "year": "2015",
      "venue": "International Journal of Speech Technology"
    },
    {
      "citation_id": "24",
      "title": "Cross lingual speech emotion recognition: Urdu vs. western languages",
      "authors": [
        "S Latif",
        "A Qayyum",
        "M Usman",
        "J Qadir"
      ],
      "year": "2018",
      "venue": "2018 International Conference on Frontiers of Information Technology (FIT)"
    },
    {
      "citation_id": "25",
      "title": "A comprehensive review of speech emotion recognition systems",
      "authors": [
        "T Wani",
        "T Gunawan",
        "S Qadri",
        "M Kartiwi",
        "E Ambikairajah"
      ],
      "year": "2021",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "26",
      "title": "Combining classifiers with diverse feature sets for robust speaker independent emotion recognition",
      "authors": [
        "M Lugger",
        "M.-E Janoir",
        "B Yang"
      ],
      "year": "2009",
      "venue": "17th European Signal Processing Conference"
    },
    {
      "citation_id": "27",
      "title": "Robust acoustic speech emotion recognition by ensembles of classifiers",
      "authors": [
        "B Schuller",
        "M Lang",
        "G Rigoll"
      ],
      "year": "2005",
      "venue": "Tagungsband Fortschritte der Akustik-DAGA# 05"
    },
    {
      "citation_id": "28",
      "title": "Speech emotion recognition from spectrograms with deep convolutional neural network",
      "authors": [
        "A Badshah",
        "J Ahmad",
        "N Rahim",
        "S Baik"
      ],
      "year": "2017",
      "venue": "2017 international conference on platform technology and service (PlatCon)"
    },
    {
      "citation_id": "29",
      "title": "Speech emotion recognition using scalogram based deep structure",
      "authors": [
        "K Aghajani",
        "I Esmaili Paeen Afrakoti"
      ],
      "year": "2020",
      "venue": "International Journal of Engineering"
    },
    {
      "citation_id": "30",
      "title": "Deep neural networks for emotion recognition combining audio and transcripts",
      "authors": [
        "J Cho",
        "R Pappagari",
        "P Kulkarni",
        "J Villalba",
        "Y Carmiel",
        "N Dehak"
      ],
      "year": "2019",
      "venue": "Deep neural networks for emotion recognition combining audio and transcripts",
      "arxiv": "arXiv:1911.00432"
    },
    {
      "citation_id": "31",
      "title": "Speech emotion recognition using mfcc features and lstm network",
      "authors": [
        "H Kumbhar",
        "S Bhandari"
      ],
      "year": "2019",
      "venue": "2019 5th International Conference On Computing, Communication, Control And Automation (ICCUBEA)"
    },
    {
      "citation_id": "32",
      "title": "Speech emotion recognition based on speech segment using lstm with attention model",
      "authors": [
        "B Atmaja",
        "M Akagi"
      ],
      "year": "2019",
      "venue": "2019 IEEE International Conference on Signals and Systems (ICSigSys)"
    },
    {
      "citation_id": "33",
      "title": "A speech emotion recognition method for the elderly based on feature fusion and attention mechanism",
      "authors": [
        "Q Jian",
        "M Xiang",
        "W Huang"
      ],
      "year": "2022",
      "venue": "Third International Conference on Electronics and Communication; Network and Computer Technology (ECNCT 2021)"
    },
    {
      "citation_id": "34",
      "title": "Improving speech emotion recognition with unsupervised representation learning on unlabeled speech",
      "authors": [
        "M Neumann",
        "N Vu"
      ],
      "year": "2019",
      "venue": "ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "35",
      "title": "Emotion recognition from speech: a review",
      "authors": [
        "S Koolagudi",
        "K Rao"
      ],
      "year": "2012",
      "venue": "International journal of speech technology"
    },
    {
      "citation_id": "36",
      "title": "Introducing the recola multimodal corpus of remote collaborative and affective interactions",
      "authors": [
        "F Ringeval",
        "A Sonderegger",
        "J Sauer",
        "D Lalanne"
      ],
      "year": "2013",
      "venue": "2013 10th IEEE international conference and workshops on automatic face and gesture recognition (FG)"
    },
    {
      "citation_id": "37",
      "title": "classification of emotion related user states in spontaneous children's speech",
      "authors": [
        "S Steidl"
      ],
      "year": "2009",
      "venue": "classification of emotion related user states in spontaneous children's speech"
    },
    {
      "citation_id": "38",
      "title": "Lssed: a large-scale dataset and benchmark for speech emotion recognition",
      "authors": [
        "W Fan",
        "X Xu",
        "X Xing",
        "W Chen",
        "D Huang"
      ],
      "year": "2021",
      "venue": "ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "39",
      "title": "Ensemble methods for spoken emotion recognition in call-centres",
      "authors": [
        "D Morrison",
        "R Wang",
        "L Silva"
      ],
      "year": "2007",
      "venue": "Speech communication"
    },
    {
      "citation_id": "40",
      "title": "Categorical vs dimensional perception of italian emotional speech",
      "authors": [
        "E Parada-Cabaleiro",
        "G Costantini",
        "A Batliner",
        "A Baird",
        "B Schuller"
      ],
      "year": "2018",
      "venue": "Categorical vs dimensional perception of italian emotional speech"
    },
    {
      "citation_id": "41",
      "title": "Interface databases: Design and collection of a multilingual emotional speech database",
      "authors": [
        "V Hozjan",
        "Z Kacic",
        "A Moreno",
        "A Bonafonte",
        "A Nogueiras"
      ],
      "year": "2002",
      "venue": "Interface databases: Design and collection of a multilingual emotional speech database"
    },
    {
      "citation_id": "42",
      "title": "Steve An Xue, Effects of aging on selected acoustic voice parameters: Preliminary normative data and educational implications",
      "authors": [
        "D Deliyski"
      ],
      "year": "2001",
      "venue": "Educational gerontology"
    },
    {
      "citation_id": "43",
      "title": "Age and voice quality in professional singers",
      "authors": [
        "J Sundberg",
        "M Thörnvik",
        "A Söderström"
      ],
      "year": "1998",
      "venue": "Logopedics Phoniatrics Vocology"
    },
    {
      "citation_id": "44",
      "title": "Age driven automatic speech emotion recognition system",
      "authors": [
        "D Verma",
        "D Mukhopadhyay"
      ],
      "year": "2016",
      "venue": "2016 International Conference on Computing, Communication and Automation (ICCCA)"
    },
    {
      "citation_id": "45",
      "title": "Is everything fine, grandma? acoustic and linguistic modeling for robust elderly speech emotion recognition",
      "authors": [
        "G Soğancıoğlu",
        "O Verkholyak",
        "H Kaya",
        "D Fedotov",
        "T Cadée",
        "A Salah",
        "A Karpov"
      ],
      "year": "2020",
      "venue": "Is everything fine, grandma? acoustic and linguistic modeling for robust elderly speech emotion recognition",
      "arxiv": "arXiv:2009.03432"
    },
    {
      "citation_id": "46",
      "title": "The interspeech 2020 computational paralinguistics challenge: Elderly emotion, breathing & masks",
      "authors": [
        "B Schuller",
        "A Batliner",
        "C Bergler",
        "E.-M Messner",
        "A Hamilton",
        "S Amiriparian",
        "A Baird",
        "G Rizos",
        "M Schmitt",
        "L Stappen"
      ],
      "year": "2020",
      "venue": "The interspeech 2020 computational paralinguistics challenge: Elderly emotion, breathing & masks"
    },
    {
      "citation_id": "47",
      "title": "Crema-d: Crowdsourced emotional multimodal actors dataset",
      "authors": [
        "H Cao",
        "D Cooper",
        "M Keutmann",
        "R Gur",
        "A Nenkova",
        "R Verma"
      ],
      "year": "2014",
      "venue": "Crema-d: Crowdsourced emotional multimodal actors dataset"
    },
    {
      "citation_id": "48",
      "title": "Toronto emotional speech set (TESS)",
      "authors": [
        "M Pichora-Fuller",
        "K Dupuis"
      ],
      "year": "2020",
      "venue": "Toronto emotional speech set (TESS)",
      "doi": "10.5683/SP2/E8H2MF"
    },
    {
      "citation_id": "49",
      "title": "Emovo corpus: an italian emotional speech database",
      "authors": [
        "G Costantini",
        "I Iaderola",
        "A Paoloni",
        "M Todisco"
      ],
      "year": "2014",
      "venue": "International Conference on Language Resources and Evaluation (LREC 2014)"
    },
    {
      "citation_id": "50",
      "title": "Meld: A multimodal multi-party dataset for emotion recognition in conversations",
      "authors": [
        "S Poria",
        "D Hazarika",
        "N Majumder",
        "G Naik",
        "E Cambria",
        "R Mihalcea"
      ],
      "year": "2018",
      "venue": "Meld: A multimodal multi-party dataset for emotion recognition in conversations",
      "arxiv": "arXiv:1810.02508"
    },
    {
      "citation_id": "51",
      "title": "Multimodal language analysis in the wild: Cmu-mosei dataset and interpretable dynamic fusion graph",
      "authors": [
        "A Zadeh",
        "P Liang",
        "S Poria",
        "E Cambria",
        "L.-P Morency"
      ],
      "year": "2018",
      "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "52",
      "title": "Xgboost: A scalable tree boosting system",
      "authors": [
        "T Chen",
        "C Guestrin"
      ],
      "year": "2016",
      "venue": "Proceedings of the 22nd acm sigkdd international conference on knowledge discovery and data mining"
    },
    {
      "citation_id": "53",
      "title": "Direct importance estimation with model selection and its application to covariate shift adaptation",
      "authors": [
        "M Sugiyama",
        "S Nakajima",
        "H Buenau",
        "M Kawanabe"
      ],
      "year": "2007",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "54",
      "title": "Boosting for transfer learning",
      "authors": [
        "W Dai",
        "Q Yang",
        "G.-R Xue",
        "Y Yu"
      ],
      "year": "2007",
      "venue": "Boosting for transfer learning",
      "doi": "10.1145/1273496.1273521"
    },
    {
      "citation_id": "55",
      "title": "Metrics for multi-class classification: an overview",
      "authors": [
        "M Grandini",
        "E Bagli",
        "G Visani"
      ],
      "year": "2020",
      "venue": "Metrics for multi-class classification: an overview",
      "arxiv": "arXiv:2008.05756"
    },
    {
      "citation_id": "56",
      "title": "Optimal thresholding of classifiers to maximize f1 measure",
      "authors": [
        "Z Lipton",
        "C Elkan",
        "B Naryanaswamy"
      ],
      "year": "2014",
      "venue": "Joint European Conference on Machine Learning and Knowledge Discovery in Databases"
    },
    {
      "citation_id": "57",
      "title": "Environmental effects on reliability and accuracy of mfcc based voice recognition for industrial human-robot-interaction",
      "authors": [
        "B Birch",
        "C Griffiths",
        "A Morgan"
      ],
      "year": "2021",
      "venue": "Proceedings of the Institution of Mechanical Engineers"
    },
    {
      "citation_id": "58",
      "title": "Opensmile: the munich versatile and fast open-source audio feature extractor",
      "authors": [
        "F Eyben",
        "M Wöllmer",
        "B Schuller"
      ],
      "year": "2010",
      "venue": "Proceedings of the 18th ACM international conference on Multimedia"
    },
    {
      "citation_id": "59",
      "title": "The geneva minimalistic acoustic parameter set (gemaps) for voice research and affective computing",
      "authors": [
        "F Eyben",
        "K Scherer",
        "B Schuller",
        "J Sundberg",
        "E André",
        "C Busso",
        "L Devillers",
        "J Epps",
        "P Laukka",
        "S Narayanan"
      ],
      "year": "2015",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "60",
      "title": "Smote: synthetic minority over-sampling technique",
      "authors": [
        "N Chawla",
        "K Bowyer",
        "L Hall",
        "W Kegelmeyer"
      ],
      "year": "2002",
      "venue": "Journal of artificial intelligence research"
    }
  ]
}