{
  "paper_id": "2311.05567v1",
  "title": "Exploring Emotion Expression Recognition In Older Adults Interacting With A Virtual Coach",
  "published": "2023-11-09T18:22:32Z",
  "authors": [
    "Cristina Palmero",
    "Mikel deVelasco",
    "Mohamed Amine Hmani",
    "Aymen Mtibaa",
    "Leila Ben Letaifa",
    "Pau Buch-Cardona",
    "Raquel Justo",
    "Terry Amorese",
    "Eduardo González-Fraile",
    "Begoña Fernández-Ruanova",
    "Jofre Tenorio-Laranga",
    "Anna Torp Johansen",
    "Micaela Rodrigues da Silva",
    "Liva Jenny Martinussen",
    "Maria Stylianou Korsnes",
    "Gennaro Cordasco",
    "Anna Esposito",
    "Mounim A. El-Yacoubi",
    "Dijana Petrovska-Delacrétaz",
    "M. Inés Torres",
    "Sergio Escalera"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "The EMPATHIC project aimed to design an emotionally expressive virtual coach capable of engaging healthy seniors to improve well-being and promote independent aging. One of the core aspects of the system is its human sensing capabilities, allowing for the perception of emotional states to provide a personalized experience. This paper outlines the development of the emotion expression recognition module of the virtual coach, encompassing data collection, annotation design, and a first methodological approach, all tailored to the project requirements. With the latter, we investigate the role of various modalities, individually and combined, for discrete emotion expression recognition in this context: speech from audio, and facial expressions, gaze, and head dynamics from video. The collected corpus includes users from Spain, France, and Norway, and was annotated separately for the audio and video channels with distinct emotional labels, allowing for a performance comparison across cultures and label types. Results confirm the informative power of the modalities studied for the emotional categories considered, with multimodal methods generally outperforming others (around 68% accuracy with audio labels and 72-74% with video labels). The findings are expected to contribute to the limited literature on emotion recognition applied to older adults in conversational human-machine interaction.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "E MOTION recognition plays a pivotal role in conversa- tional human-machine interaction (HMI)  [1] ,  [2] , enabling systems to perceive and respond to users' emotional states  [3] ,  [4] . Research in affective computing has long proved the possibility of detecting emotion expressions with datadriven approaches using different input modalities, mainly linguistic, acoustic, and facial expressions  [5] ,  [6] . Multimodal approaches have also shown promising results in enhancing recognition accuracy and robustness  [7] . The literature has shifted from recognizing acted, contextless prototypical expressions to more spontaneous reactions and in-the-wild data, which presents numerous challenges for unimodal and multimodal models  [7] . In addition to the increased appearance and behavioral variability, spontaneous emotions are more subtle and difficult to disambiguate, significantly differing from acted emotions in surface representation  [8] . Consequently, the emotional space is smaller and more compact  [9] ,  [10] . In addition, natural contexts suffer from a high imbalance in emotional categories, negatively affecting the learning process of data-driven approaches. Conversational HMI users tend to exhibit less intense emotional responses than when interacting with other humans due to the limited emotional capacity of artificial agents, resulting in more neutral expressions  [10] . Visual-based recognition is further affected by the speaking effect, for which facial deformations caused by speaking can be mistaken for emotional expressions  [11] . Lastly, establishing a reliable gold standard remains difficult due to subjective perceptual annotation procedures, resulting in low agreement and potential discrepancies between emotions expressed and perceived  [12] .\n\nEmotion recognition research has traditionally focused on young adults. However, the global aging of the population is generating new socioeconomic challenges. Thus, older adults have been positioned as a clear beneficiary of technological development in HMI  [3] ,  [13] -  [15] . Understanding emotions in older adults is of special importance, as it can lead to the development of customized virtual coaching applications and companions, and healthcare technologies that foster active aging and independent living. This presents additional computational challenges, since aging changes facial features, voices, and speaking styles, among others  [16] ,  [17] . For example, a higher intensity in facial expressions and speech is associated with a higher emotion recognition accuracy; however, older subjects display less intense vocal and facial expressions compared to younger subjects  [18] ,  [19] . Therefore, models trained in other age groups do not perform optimally for recognition in older adults  [19] , and models trained on data from this age group tend to show lower performance than other groups  [20] ,  [21] . The lack of public databases including older adults hinders progress on this front. To our knowledge, only two datasets that focus on older adults provide non-acted data: the ComParE Elderly Emotion Sub-Challenge speech dataset  [22] , which includes personal narratives; and ElderReact  [19] , containing monologue videos of older adults reacting to specific items spontaneously, but possibly exaggerating their responses due to the nature of the videos. Very few non-acted, interaction-oriented datasets include older adults  [23] , and we are unaware of any HMI dataset featuring them.\n\nThis paper presents a comprehensive study on computational, non-verbal discrete emotion expression recognition in interactions between older adults and a simulated Virtual Coach (VC), as a specific case of HMI scenario. The work was developed as part of the European EMPATHIC project  [24] ,  [25] , which aimed to explore and validate new interaction paradigms for empathic, expressive, and advanced VCs to improve independent, healthy-life-years of this age group. As part of the project, 157 participants over 65 years old from three countries (Spain, France, and Norway) were recorded interacting with an initial version of the EMPATHIC-VC in a Wizard of Oz (WoZ) paradigm. This data aided in system development and in the study of the interaction between older adults and VCs  1  . Under this framework, we first describe the emotion annotation procedure and methodological choices tailored to the project requirements. Then, using a deep learningbased approach, we investigate the contribution of different modalities for emotion recognition, including speech, facial expressions, eye gaze, and head dynamics, individually and combined, in various evaluation scenarios.\n\nThis framework provides two unique features that we exploit in our work. First, it allows us to perform a comparative analysis across cultures (where culture influences not only emotional expression but also the annotation procedure) and languages as well as multi-country versus country-specific training, two aspects that have received limited attention  [23] . Second, the project involved a channel-specific annotation, providing two distinct sets of emotional labels, audio-and video-based (explained in Sec. III). Thus, we consider speech from audio and facial expressions from video as main modalities, while eye gaze and head movements act as additional modalities that can also be extracted from video. We assess the effectiveness of the main modalities in recognizing their associated labels, and the possible performance improvement when being combined with the remaining (auxiliary) modalities. Additionally, we conduct a cross-channel evaluation, wherein the main modality of one label type and the auxiliary modalities are individually employed to recognize the labels derived from the other channel. This offers insight into the transferability and adaptability of modalities across label types, possibly revealing shared emotional cues. Lastly, we analyze performance differences between training and evaluating on spoken and silent instances, to understand how the presence or absence of speech affects performance for this age group.\n\nThis paper is organized as follows. Sec. II reviews current trends in affective computing for emotion recognition with the modalities considered in this work. Sec. III describes the corpus and the annotation protocol. Sec. IV details our computational approach. Sec. V describes the evaluation protocol and results of all evaluation scenarios, which are discussed in Sec. VI. Finally, Sec. VII concludes the paper.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Related Work",
      "text": "In this section, we first summarize the two main models of emotion used in affective computing. Second, we review related computational approaches for the automatic recognition of emotional states from speech, facial expressions, gaze, and head cues. Finally, we discuss multimodal approaches using such cues, with an emphasis on works featuring older adults.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "A. Models Of Emotion",
      "text": "Expressions of emotion are generally represented by two main different models: a categorical or discrete model, and a dimensional or continuous model. The categorical model identifies a set of discrete emotional categories, ranging from the basic Ekman emotions  [26]  (happy, surprised, contempt, sad, fearful, disgusted, and angry), to a larger set with more specific and realistic affective states. Indeed, ordinary communication involves a variety of complex feelings that cannot be characterized by a reduced, fixed set of categories  [27] . Therefore, such categories are usually selected considering the task at hand. For instance, categories such as bored, frustrated, delighted, calm, satisfied, or excited are more applicable to HMI scenarios than most basic states  [28] .\n\nGiven the complexity of the emotional semantic space, a number of researchers  [27] ,  [29]  are more in favour of adopting a dimensional model such as the circumplex model of affect  [30] . In the dimensional model, each affective state is represented by a point in a two-dimensional space, where the valence dimension represents the polarity of the emotion, i.e., a positive or negative value along a continuum, and the arousal dimension represents the degree of emotional activation. i.e. values vary from low to high along a continuum. Other versions include a third dimension, dominance, which represents a sense of control over the situation while experiencing the emotion. The Valence, Arousal, and Dominance (VAD) model has been widely exploited for audio/video-based emotion recognition  [10] ,  [27] ,  [31] , allowing for the encoding of slight emotional changes over time  [31] .\n\nBoth models have their own advantages and drawbacks. For instance, emotional categories may not account for intensity and exhibit fuzzy boundaries. Conversely, dimensional models introduce more subjectivity in emotion scaling across raters. Ultimately, the choice depends on the objectives of the task. In our case, we are interested in detecting prespecified events of interest that are expected to occur during the interaction, for which the EMPATHIC-VC system can react and adapt to, in a practical and interpretable way. Thus, the categorical model better fits the needs of the system.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "B. Emotions From Speech",
      "text": "The speech signal captures the speaker's communicative intention, encompassing not only the words spoken but also the intonation, prosody, pauses, and other paralinguistic elements that contribute to the message. In the same way, speech provides a lot of information about the speaker, their accent, profile, speaking style, and current emotional state  [32] ,  [33] .\n\nThe most commonly used features for speech emotion recognition (SER) are based on Low-Level Descriptors (LLDs), such as zero crossing rates, pitch, formants, energy, jitter, shimmer, spectral centroids, Mel-Frequency Cepstral Coefficients (MFCC), flux, etc., as well as on their descriptive statistics or functionals (e.g., mean, SD, quartiles)  [10] ,  [32] ,  [34] . Some works have proposed the standardization of the feature sets. However, only GeMAPS, which contains a combination of the previously mentioned LLDs and functionals, and the feature sets proposed in the ComPaRE challenge series, which are variations of GeMAPS, have become a reference  [35] ,  [36] . Spectrograms have also been used as a sequence of features represented as an image, which has been demonstrated to be specifically useful to feed Convolutional Neural Networks (CNNs)  [33] . More recently, the first framework for self-learning rich representations of speech was published, called Wav2Vec  [37] , which was initially used for SER in English by  [38]  and in Spanish by  [10] . Shortly afterward, new frameworks were proposed, including Hubert  [39] , UniSpeech  [40] , and WavLM  [41] . A comparison of selfsupervised representations for SER can be found in  [33] .\n\nSimilarly to other domains, the rise of deep learning also caused a gradual transition from traditional classifiers to deep neural networks (DNNs) for SER  [42] ,  [43] . Current approaches feature Multilayer Perceptrons (MLP)  [33] , CNNs  [10] , Recurrent Neural Networks  [33] ,  [44] , Transformers  [45] ,  [46] , and combinations of different DNNs  [10] ,  [47] .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "C. Emotions From Facial Expressions",
      "text": "Facial expressions are considered one of the most significant means for humans to express their emotions and intentions in their daily communication  [26] . Facial expression recognition (FER) systems can be divided into two main categories according to the type of facial input they rely on: staticimage FER and dynamic-sequence FER  [48] . In static-based methods, the feature representation is based only on the spatial information associated with a single image, whereas dynamic-based methods consider the temporal relation among contiguous frames as well as the facial deformation dynamics.\n\nIn turn, and similarly to SER, FER approaches can also be divided into conventional and deep learning based approaches. The former is usually composed of three major steps: face and landmarks detection, feature extraction, and emotion classification. These conventional algorithms usually extract face-based handcrafted features such as pixel intensities  [49] , Gabor filters  [50] , local binary patterns  [51] , and histograms of oriented gradients  [52] . However, these often lack enough generalizability in in-the-wild settings. By contrast, deep learningbased approaches are used as a conjoint feature extraction tool and facial expression classifier, reducing the dependency on preprocessing techniques and human expertise-based feature extraction. The same neural network approaches discussed for SER have been applied to FER with similar results; hence, we avoid repeating them here. We refer the reader to the surveys of  [48] ,  [53] ,  [54]  for a comprehensive review of the state of the art. As an example of approach related to the one used in this paper, we highlight the work of  [55] , one of the first to demonstrate the capability of CNNs to recognize the Ekman emotions by outperforming traditional methods on popular posed and spontaneous expression datasets.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "D. Emotions From Eye Gaze And Head Pose",
      "text": "Extensive behavioral and neuroscience literature has confirmed a relationship between eye state, gaze direction, and facial expressions on the perception of emotions and mental states  [56] . Eye-related features that have been studied or used the most in affective computing are: pupil size, blinks, gaze direction, direct/averted gaze, extracted patterns of gaze events, and eye aperture/closure  [57] ,  [58] . Dedicated eye trackers are generally required to extract these features with high accuracy. However, this is impractical for many everyday scenarios or HMI settings, such as the EMPATHIC-VC, where a non-obtrusive or lower-cost approach is preferred. For such scenarios, regular cameras can now be used to estimate eye gaze and approximate the location of pupil and eye landmarks by means of appearance-or model-based methods  [59] . In particular, appearance-based gaze estimation has improved significantly during the past decade, boosted by deep learning advances  [60] . Therefore, a number of works compute statistical features, or functionals, from the raw or smoothed estimated gaze trajectories over a time window, or compute features (e.g., eye closure, pupil size) based on specific eye landmarks instead  [58] ,  [61] ,  [62] . Blinks can usually be detected via dedicated appearance-based methods  [63] , or by detecting the action unit (AU) #45.\n\nOn a related note, head rotation plays an important role in stabilizing gaze to fixate on objects of interest  [64] . There is evidence of the relationship between head pose dynamics and expression and perception of different emotional and mental states  [65] -  [67] , being particularly related to emotional intensity  [68] . Affect recognition works have relied on head pose categorizations such as head tilts, nods, and shakes  [65] ,  [69] , which usually require specific action detectors. More recent approaches directly use temporal 3D rotational angles (yaw, pitch, and roll) to describe head motion trajectories, as well as angular displacement, velocity, acceleration, and windowbased functionals computed from such trajectories  [61] ,  [70] ,  [71] , dynamic features based on the discrete Fourier transform  [72] , or clustered sequences of kinemes  [73] . Head orientation is generally extracted with appearance-based methods and model-based 3D head registration  [74] .\n\nDue to their relationship, a handful of works have combined head and gaze features together for emotion recognition  [75] . Although these features have been proven to be sufficient for specific affective states in some scenarios  [73] ,  [76] , they are usually added to facial or speech modalities to provide complementary rather than redundant information.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "E. Multimodal Emotion Recognition",
      "text": "With significant advancements in multimodal machine learning, multimodal emotion recognition has gained considerable momentum lately (see  [77] -  [79]  for exhaustive surveys on multimodal machine learning, and  [5] -  [7] ,  [12]  for multimodal emotion recognition). By leveraging the complementary information of multiple modalities, multimodal systems can achieve higher accuracy and reliability compared to unimodal systems. Multimodal fusion methods are broadly classified into featurebased, decision-based, and hybrid approaches. The former consists in combining the features extracted from different modalities, with methods that range from naive feature concatenation to attention-based approaches. Feature-based fusion allows learning from cross-modal correlations; however, an alignment among modalities is required since they may have different sampling rates or representations (e.g., video frames vs audio segments), and not all modalities may be available at all times. Instead, decision-based fusion combines the scores or predictions of unimodal models for a final multimodal prediction, thus alleviating the alignment and incomplete data problems but disregarding cross-modal correlations. Lastly, hybrid approaches combine feature-and decision-based fusion. Generally, the best fusion type is task-and dataset-dependent.\n\nMost multimodal emotion recognition works combine at least paralinguistic and facial expression features  [80] , or acoustic and linguistic  [47] . Gaze and head cues are usually combined with other features like facial information  [81] ,  [82]  and/or speech cues  [61] ,  [83] ,  [84] . Nonetheless, their use is less explored compared to audiovisual fusion. One of the few works that combine speech, facial expressions, and gaze features is that of  [62] , which uses audio GeMAPS features, and a subset of the gaze functionals proposed by  [76]  and facial features extracted from a pre-trained CNN from video.\n\nThe ComParE challenge recently drew attention to emotion recognition for older adults, in which challenge participants could use acoustic and linguistic features  [22] . However, the work of  [19]  is one of the few addressing discrete emotion recognition using the modalities considered in this work for such age group, based on ElderReact. More specifically, they extract gaze and head features, facial AUs (including blink), and facial landmarks from video, and voice quality, MFCCs, and prosody features from audio.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Iii. Empathic Woz Corpus",
      "text": "Here, we describe the subset of the EMPATHIC WoZ Corpus considered for this work, and the protocol followed for the annotation of audio and facial expressions from video.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "A. Data Collection",
      "text": "The target population of the EMPATHIC project was defined as healthy older adults based on the following inclusion criteria: 1) above the age of 65 or turning 65 in 2019; 2) good hearing and sight (with or without glasses/hearing aid); 3) living independently at home; and 4) read, write, and speak the testing language fluently. Recruitment  properly informed and signed an informed consent prior to enrolling on the study. Hereinafter, we refer to the Spanish subset as SP, the French as FR, the Norwegian as NO, and the complete data as WHOLE (or WH).\n\nWe used the WoZ paradigm for data acquisition, commonly used when building systems based on natural language and other artificial intelligence-driven applications  [85] . Its key principle is that study participants believe they are interacting with an autonomous system, while actually the actions of the system are controlled by a human (i.e., the wizard). This wizard is usually in a different room and connected to the study setting through a network connection. The interaction sessions combined different questionnaires and interaction with the EMPATHIC-VC, detailed in  [3] . The setup consisted of a computer equipped with a webcam, a microphone, and an Internet connection (see Fig.  1 ). At the start of the session, participants chose one of five available visual representations of agents for their VC session. During the interaction, participants were alone with the VC to avoid bias or undesired interactions with the supervisor. Two dialogues of 5-10 min each were completed. The first served as an introduction to the system and thus did not focus on any specific issues. The second focused on the user's nutrition habits and goals.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "B. Definition Of Labels",
      "text": "Our emotional labels correspond to the users' perceived expression of emotion. The procedure for selecting such emotion categories followed a three-step data-driven approach.\n\nFirst, we considered the 27 categories defined in  [86] , which are based on the self-reported emotional states elicited by around 200 short videos over a population of nearly 1000 people. The list defines a rich semantic space of emotions, which includes categories such as amusement that were found to capture well the subjective emotional experience.\n\nAs a second step, we removed the categories that were highly unlikely to be encountered during the interaction between the user and the EMPATHIC-VC, and added some labels that might potentially be perceived in these interactions. We worked with the target languages simultaneously, i.e., Spanish, French, and Norwegian, to provide accurate terms to express the same feelings in different languages, considering that cultural context can be accounted for by the translation that the native speakers of each language can provide relative to the Lingua Franca (which in our case was English). The selected 18 labels were: relieved, bored, excited, calm, sad, amused, puzzled, pleased, interested, tense, surprised, concerned, enthusiastic, skeptical, embarrassed, tired, delighted, and annoyed.\n\nFinally, we ran a set of pilot experiments on SP. Our goal was two-fold: 1) shorten the previous list by only considering the subset of emotions perceived during the interaction; 2) assess to what extent we could match totally or partially this list to the list of basic emotions defined by Ekman, which are typically featured in visual-based discrete emotion recognition datasets. This pilot, as well as the posterior results of the annotation procedures, defined the final labels to be considered for audio and video channels, which are presented next.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "C. Annotation Protocol",
      "text": "Few works are found in the literature aimed at establishing the amount of emotional information provided through the different audio and video channels. In particular, the study of such channels separately and their combination concludes that the latter does not always yield the best perception results, as might be otherwise expected (e.g.,  [23] ). Previous studies have established that the emotional information provided by each channel or combination strongly depends on the specific emotion, context, and language  [87] . For instance, there is vast evidence in the literature that, with respect to dimensional models, arousal can be better detected from the audio channel, while valence is much better estimated from the video channel  [88] . It has also been suggested that humans, when posed with the task of decoding emotional states, selectively attend to emotional cues that align closely with their personal and cultural experiences, thus minimizing the cognitive effort required for emotional processing during the task. Consequently, when annotating perceived emotional expressions from audio and video simultaneously, raters tend to explore the most familiar channel: if rater and rated person are culturally and/or language akin, the rater tends to exploit the auditory signal, whereas when they are culturally distant, they tend to rely more on visual cues  [89] .\n\nOne of the salient attributes considered in the EMPATHIC project is culture and cultural differences, so it was important that the annotation be carried out separately per country by native speakers to be able to capture subtle culture-specific emotional cues. Therefore, in order to avoid the annotators' reliance towards a single channel, we decided to separate channels at the annotation level, having different annotators for each channel. This, in turn, results in a richer variety of emotional information from different perceptual channels, which can be later leveraged by the EMPATHIC-VC system. We employed instructed annotators to be able to control the whole procedure and update it if necessary. Preliminary trials showed that annotators preferred having access to the entire video or speech file instead of annotating isolated snippets due to the presence of context, which helped them make more accurate estimations of the users' emotional state.\n\nThe annotation process consisted in determining the start and end times of all events associated to given emotions categories throughout a WoZ interaction. To ensure a high inter-rater agreement, we employed a sequential annotation process. Initially, each annotator received a set of files to annotate independently. Subsequently, the within-country inter-rater agreement was calculated with an ad-hoc measure based on event overlap. If the agreement score fell below a predefined threshold, annotators engaged in discussions and re-annotated the files. When the threshold was met, the annotators received the remaining files and continued the process of discussing and re-annotating until the desired level of agreement was attained.\n\n1) Audio annotations: These were carried out by listening to the audio signal in Transcriber 3  with nine native annotators (three per country). For the specific case of audio, the perceived emotions were labeled in terms of both, the categorical and the VAD models. The categorical labels were: calm/tired/bored, pleased/amused, puzzled, sad, and tense. The first two labels consist of a combination of similar categories, which was decided after the first annotation rounds as they were highly confused among annotators. For simplicity, we henceforth refer to them as calm and pleased. Parts of the audio signal with no annotated label are not categorized. The labels assigned to the dimensional VAD model were also discretized for simplicity, and defined as: 1) positive, neutral, and negative, for valence; 2) excited, slightly excited, and neutral, for arousal; and 3) dominant, neither dominant nor dominated, and defensive, for dominance.\n\nThe inter-annotator agreement for the categorical annotation was computed with Cohen's Kappa for each pair of annotators at the millisecond level. SP and NO scored an average coefficient of 0.792 and 0.692, respectively, which indicate substantial agreement  [90] , while FR scored 0.554, indicating moderate agreement. Once the entire corpus was labeled, we combined the raters annotations using 3-s segments with a 1 s stride (see Fig.  2 ). To assign an emotion to each segment (i.e., the gold standard, or ground truth), the majority emotion was assigned if that emotion spanned a specific percentage of the whole segment. Otherwise, the segment was left without annotation, referred to as discarded. Thus, each segment has four different annotations: one categorical and three for VAD.\n\n2) Video annotations: The annotation was carried out with an in-house software by six native annotators, two from each country. Annotators were instructed to watch muted videos, taking into account only facial expressions and head movements, thus disregarding out-of-face information such as body or hand movements. In addition, they were instructed to watch a short snippet of each user's video (up to 1 min) to familiarize with the user's baseline facial expression. For video annotations specifically, a cross-country calibration was performed after the first set of files was annotated for a small, random subset of videos. This was performed to ensure a common understanding of the instructions, and of the minimum intensity an expression should have to be categorized as such, which can be more objectively determined across countries than for audio-based annotations.\n\nThe categorical labels considered were: sad, annoyed/angry (henceforth referred to as angry), surprised, happy/amused (henceforth referred to as happy), pensive, and other. The first four are part of the Ekman's basic emotions  [26] . Pensive is a mental state rather than an emotional expression; however, it was included in our model as it was found to be a frequent facial expression during the conversation when users were preparing their response, as in previous HMI-oriented works  [65] ,  [91] . Similarly to audio-based annotations, some categories were combined into a single label due to being often confused by annotators. Raters were instructed to annotate as one of the first five categories those segments in which it was clear to them that the expression was present simultaneously. The label other was used to denote those segments in which an expression was occurring but was not included in our expression list, or when more than one expression from the list was present. Finally, all non-labeled instances were considered to be a neutral expression, denoting the baseline face as well as calmed, quiet, or very subtle emotional expressions which do not exceed the consensual expression thresholds.\n\nPost-hoc inter-rater reliability was computed at frame level by means of Cohen's kappa coefficient, achieving a value of 0.7 for SP and FR, and 0.68 for NO, which indicates substantial agreement  [90] . We used the intersection between the two annotators to create the final gold standard. Frames with no intersection were discarded for automatic processing, representing around 8% of the total amount of frames.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "D. Analysis Of Labels",
      "text": "A thorough analysis of corpus annotations is reported in  [92] . Here, we summarize the findings, with an emphasis on the categorical labels that will be used in our evaluation.\n\nThe number of final audio segments per emotion category is detailed in Table  I . As can be seen, calm is the most frequent emotion with around 95% of the samples, with respect to instances where the user is speaking and disregarding discarded, whereas sad and tense are quasi absent. Specifically for NO, users rarely showed a puzzled expression. With regards to the VAD model, we highlight the following differences: 1) around 30% of FR segments and only 3-4% of SP and NO segments are marked with slightly excited for the arousal dimension, while the rest is neutral; 2) SP segments are mostly divided between positive and neutral valence; 3) about 25% of FR segments have positive valence, while for NO they are mainly neutral; and 4) participants in the three datasets are often neither dominant nor dominated.\n\nTable  II  provides the distribution of emotion categories from video corresponding to spoken (top) and silence (bottom) instances separately. The reported quantities do not include the 0.3% of frames that are not matched to any audio segment, which mainly happened at the end of the video due to audiovideo length mismatch. Similarly to audio annotations, video annotations lead to highly imbalanced results. Pensive was the most frequent manually labeled expression, appearing 11% of the time, followed by happy, present in 2% of the total images. Despite these findings, the neutral category clearly dominates over all categories, appearing around 87% of the time.\n\nAs observed, the main challenges encountered in the EM-PATHIC WoZ corpus are: 1) the imbalance between the different emotion classes; and 2) the imbalanced number of subjects across countries and limited data samples, particularly for audio. The former indicates that the interaction with the VC did not lead the users to experience strong emotions like sad, angry and surprise, and is in line with what it is usually observed in real, spontaneous HMI interactions. In addition, many of the users may have an a priory positive attitude since they are volunteers to participate in the experiment. The reduced number of audio samples is partly caused by the amount of time that participants had to wait for the WoZ to respond. The high class imbalance is a problem for data-driven models to properly learn any discriminative information for the minority classes. Hence, for this study, we reduce the number of categories to the three most represented for each label type. That is, for audio, we maintain calm, pleased, and puzzled, whereas for video we keep neutral, happy, and pensive.\n\nTable  III  depicts the relationship among audio-video labels per country, using the audio segments as reference and computing the most repeated video category for the valid frames within the start-end times of an audio segment. We find that audio-based calm and video-based neutral coincide 76-79% of the time (66-70% if including all labels). However, there is no evident one-to-one correspondence for the remaining cases. Given that each channel contributes distinct information, we retain the two label types as independent entities, allowing the system to estimate both of them at each time step.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Iv. Methodology",
      "text": "In this section, we describe our methodology and training strategy for data-driven recognition of emotional states using different modalities. The methodological choices depend on the EMPATHIC-VC system requirements, which follow those of common multi-agent systems  [1] . The human sensing module, which includes emotion recognition, is one of the multiple modules that must communicate timely with a dialogue manager. The manager controls the conversation flow by integrating the information from human sensing and other modules to transfer the appropriate VC reactions to the natural language generation and avatar animation modules  [4] ,  [25] . In the final system, some modules would be located in remote servers and thus data transfers would be done via network. Thus, efficiency in the whole process is crucial to ensure a seamless and natural interaction. Therefore, we prioritize independent, lightweight computational submodules for each channel, which can operate asynchronously and produce estimates at the lowest granularity level for further processing.\n\nFig.  3  shows an overview of our methodological pipeline. In summary, we first extract features from the different modalities. More specifically, for the main modalities (i.e., speech from audio and facial expressions from video), we train individual models for their respective labels on all the available data to learn rich emotional features. In parallel, we extract additional features from video, namely looking-at-VC, head, 3D gaze, and eye movement information. Since the features of each modality are extracted at different time resolutions (i.e., audio features every 3 s, facial features at every frame, and additional features every 1.5 s), we apply fixed modality synchronization tailored to each label type, which allows us to perform the cross-channel and multimodal evaluation. Finally, the previously extracted features are combined and further evolved with an MLP to recognize the user's emotional state for the audio-and video-based labels separately.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "A. Speech Features From Audio",
      "text": "In this work, we only consider those audio segments with an associated emotion while the user is speaking and for which the avatar speaks less than one-third of the segment duration.\n\nFirst, we use the WavLM speech model  [41]  to extract the acoustic information of each segment using the raw signal waveform. WavLM was trained on 94k hours of Englishspoken audios extracted from three large-scale speech datasets, and can obtain high performance in SER, among other tasks. We extract the features of its last hidden states, which outputs a 1024D vector every 1/50 s. This results in 150 feature vector instances per segment. We compute the average for the time dimension to reduce the final feature length to 1024.\n\nThen, we feed such features to four two-layer MLPs: one for categorical emotional state recognition and three for the dimensional model. Although the main goal is to perform categorical recognition, we decided to include the dimensional model to leverage the available annotations and enrich the feature representation, given the relationship between the two models  [30] . We also chose to train them separately since each task may converge at a different rate; thus, a multi-task learning approach would not be optimal for all the outputs. The first layer reduces the 1024 extracted features to 64 with ReLU as the activation function, while the second one is in charge of extracting the logits for the prediction of the emotional states via softmax. Cross-entropy is used as a loss function, and the Adam optimizer is used to train the four networks with a learning rate of 0.001 over 5K iterations. To deal with the imbalance of the data, the sampling probability for the samples of the minority classes is four times higher than neutral.\n\nFinally, the logits of the four models are concatenated to the computed WavLM features in a hybrid fusion fashion, resulting in a 1031D feature vector. This way, we preserve the generic speech representation and augment it with a reduced set of domain-specific information. We refer to this feature set, and consequently to this modality, as A.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "B. Facial Expression Features From Video",
      "text": "For this work, we adopt a static-based approach, despite dynamic ones usually being better suited for this task  [12] . In our context, issues such as frame loss during data transfer or limitations in network capacity leading to low frame rates may impede the effective utilization of fine-grained dynamics.\n\nFor each video frame, we first detect faces using Face-Boxes  [93]  and estimate 68 facial landmarks in the image space by means of 3DDFA v2  [94] . Less than 1% of TABLE IV: Functionals computed for each element of the additional modalities (3D gaze vector, eye rotation, and head pose). x • and y • correspond to horizontal and vertical 3D gaze/eye rotation components, respectively. y, p, and r correspond to head yaw, pitch, and roll, respectively. the data is lost on these steps. Using these landmarks, the face is rotated, scaled, and cropped to obtain a normalized RGB image of 224x224 pixels. Then, we use the Xception CNN model  [95]  pretrained on ImageNet  [96]  to extract discriminative features from the face images, and add four fully connected dense layers to the top of the network, each followed by ReLU and dropout (with a rate of 0.5), in addition to a final softmax layer for FER. During optimization, we found that the best strategy was to freeze the first 70 layers of Xception and fine tuning the last 10. Consequently, we finetune such layers and train the added ones from scratch on both spoken and silent instances. According to this transfer learning scheme, we get a total of 23.6M parameters, where 16.5M are trainable, and the remaining 7M are non-trainable. Training is based on the Adam optimizer, with a learning rate of 0.001. To tackle the class imbalance issue, we use a weighted crossentropy loss function where the weight of each emotion class is associated with the inverse frequency in the training set.\n\nLastly, we extract the output 256D features from the last hidden layer. We refer to this feature set and modality as F.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "C. Additional Features From Video",
      "text": "We also use the video stream to compute a series of additional features based on the per-frame estimated 3D direction vector (represented as horizontal and vertical gaze angles) and head pose (yaw, pitch, and roll).\n\nTo do so, we first fit a 3D face morphable model  [97]  to the detected 2D landmarks from Sec. IV-B and apply perspectiven-point  [98]  to estimate the 3D head position and orientation, as well as the 3D eye position as the gaze origin. A normalized face image is fed to the 3D gaze estimation model ETH-XGAZE, trained on a dataset of homonymous name  [99] . Although none of the existing gaze estimation datasets include older adults, a qualitative examination of the estimated gaze direction using this model showed that performance was mostly impacted for users wearing colored lens or eyeglasses with substantial reflection caused by the computer screen. We did not detect blinks or pupil size due to their low reliability in our scenario. To reduce noise, the estimated head pose and gaze trajectories are postprocessed with a 5-frame median filter. Combining head pose h = (y, p, r) and gaze g = (x g , y g ) vectors, we further convert the 3D gaze direction into an eyein-head gaze direction vector, that is, mimicking eye rotation e = (x e , y e ). The three data sources are filtered to discard invalid data (e.g., frames with incorrectly detected faces, or for which head or eye movements are not anatomically plausible). Following other works on eye and head pose processing  [73] ,  [100] , all trajectories are processed using a sliding window of 1.5 s and stride of 1, and centered at every half second throughout the video. Windows smaller than 0.5 s or for which more than 50% of the frames are invalid are discarded, accounting for around 2% of the windows. We compute a 227D feature vector for each window, containing information from the three data sources represented as functionals of the trajectories (see Table  IV ), and a complementary attention measure, described below. Due to the effect of glasses on the resulting eye trajectories, we add a manually annotated ternary flag to denote whether the participant is wearing glasses, and if so, whether the eyes are clearly visible. We refer to the resulting 228D feature set and modality as G.\n\n1) Looking at VC: The EMPATHIC-VC system can use this measure to estimate whether the user is engaged with the VC. Here, we largely follow  [101]  to estimate the location of the VC, wherein it is assumed that the zone (cluster) in the 3D space with the highest density of gaze vectors intersecting the camera plane is where the VC is located. We create a 6D one-hot encoding vector denoting the looking-at-VC likelihood from lower to higher, based on the distance from the gaze point to the cluster. Per-valid-frame vectors are averaged over a time window, producing a 6D vector per window.\n\n2) 3D gaze direction: We compute g functionals of: percomponent (i.e., x g , y g ) gaze angles, per-component angle differences (e.g., ∆x g ) and their magnitude (e.g., abs ∆x g ) between any two consecutive frames, direction (∆g) and speed (∆g/t) of the gaze vector between any two consecutive frames, and per-component speed (e.g., ∆x g /t) between any two consecutive frames. This results in a 67D feature vector.\n\n3) Eye rotation: We compute the same functionals for e as for g, resulting in a 67D feature vector.\n\n4) Head rotation: Lastly, we compute h functionals for the following: per-component (i.e., y, p, r) head pose angle, percomponent angle differences (e.g., ∆y) and their magnitude (e.g., abs ∆y) between any two consecutive frames, and per-component speed (e.g., ∆y/t) with respect to any two consecutive frames. This results in a 87D feature vector.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "D. Temporal Synchronization Of Modalities",
      "text": "In order to effectively integrate and analyze the multimodal data captured from different sources, we employ a fixed modality synchronization approach per label type.\n\nFor the audio-based evaluation, for which the system would output an estimate every 3 s, we compute the average and SD of the available per-frame F features within an audio segment, resulting in a 512D vector. This provides a robust facial expression descriptor that is less susceptible to accidental fluctuations despite disregarding facial temporal dynamics. Preliminary experiments evaluated a second version, consisting in concatenating the features of the most central frame of each second of the audio segment, hence maintaining such dynamics. However, the former version outperformed the latter for the majority of settings. Regarding G, we use the window aligned to the center of the audio segment, thus discarding those windows at the extremes of the segment.\n\nConversely, for video-based evaluation, the temporal resolution is increased to frame level. Thus, each G window and A segment are used multiple times and matched to different frames. In particular, we associate each frame with a specific G window and A segment based on its closest proximity to the central timestamp of the respective window and segment.\n\nThis way, all F frames, G windows, and A segments receive audio and video labels. For each evaluation case, feature sets that do not have correspondence due to missing data of any of the modalities are omitted, resulting in around 86% of the original data for audio and 98% for video.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "E. Final Models",
      "text": "The extracted features from a given modality are normalized according to the training set range and fed to a 2-layer MLP with ReLU activation and dropout of 0.5, followed by a softmax dense layer for classification of a given label type. We evaluate three low-complexity MLP configurations, with number of hidden layers 100-20, 200-40, and 500-100. For multimodal evaluation, the feature sets of the different modalities are concatenated before being fed to the MLP. We evaluated other attention-based fusion approaches in preliminary experiments, such as self-and cross-modal attention  [102] . However, their performance was equivalent to concatenation, so we proceed with the latter for the experimental evaluation.\n\nWe tackle data imbalance by randomly sampling instances of each class with the same probability. Additionally, due to the small sample size of the audio-based evaluation, we employ an oversampling strategy such that each sample of the minority class (pleased for SP and FR, and puzzled for NO and WH) is utilized around three times per epoch. To maintain an approximate balance between classes, the other classes are sampled a similar number of times. The training samples per epoch are thus set to 5418 samples for SP, 2556 for FR, 234 for NO, and 10494 for WH. Conversely, since the sample size for video-based evaluation is considerably higher but also contains higher redundancy, we set the training sample size to 7500 for all countries. Samples are randomly selected; thus, at the end of the training stage, all samples from the minority classes are seen multiple times, while for neutral, only a fraction is seen.\n\nAll evaluated models are trained with cross-entropy loss, Adam optimizer, learning rate of 0.0001, and batch size of 64. We empirically set the number of training epochs to 100 for all countries and evaluations except for NO with audiobased labels, for which we train for 200 epochs.",
      "page_start": 9,
      "page_end": 10
    },
    {
      "section_name": "V. Experimental Evaluation",
      "text": "Here, we present a comprehensive experimental evaluation to assess the impact of different modalities on the recognition performance of emotional states for audio and video labels.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "A. Research Questions",
      "text": "The characteristics of the EMPATHIC WoZ scenario allow us to evaluate the contribution of the different modalities for the considered emotions in various contexts. First, we separately consider the evaluation scenario with audio-based labels and the one with video-based labels. We have a main modality for each label type: A for audio-based and F for video-based labels. We refer to the remaining modalities (e.g., F and G for audio-based evaluation) as auxiliaries for that evaluation scenario. Main and auxiliary modalities can be combined to improve performance. Each evaluation is performed in each country individually (SP, FR, and NO) and on WH. The latter allows us to evaluate trends of the complete set of data and quantify the effect of training with country-specific data in comparison to a larger multi-country set. The audio-based scenario only includes data where the user is speaking. By contrast, for the video-based scenario, we can compare the performance of evaluating spoken content to silent content. What is more, as for the country-oriented evaluation, we can assess the effect of training the final video-based model with speaking-status-specific data in comparison to with all data.\n\nOn this basis, we aim to answer the following research questions: RQ 1 ) Can the main modality for a given label type obtain the same discriminative power for all the classes considered?; RQ 2 ) Can the auxiliary modalities achieve similar performance to the main modality?; RQ 3 ) Is multimodality beneficial?; RQ 4 ) Are there noteworthy differences in performance among countries?; RQ 5 ) Does training with data from multiple countries prove beneficial with respect to countryspecific training?; RQ 6 ) For video-based evaluation, does training with spoken and silent instances prove beneficial with respect to spoken/silent-specific training?; RQ 7 ) Are there any performance differences between spoken and silence instances?; and RQ 8 ) Are there any performance differences between audio-and video-based evaluation?",
      "page_start": 9,
      "page_end": 10
    },
    {
      "section_name": "B. Evaluation Protocol",
      "text": "We build 10-fold subject-independent training and test splits for each country subset (SP, FR, NO) and a fourth one with the data of all the countries (WH) following approximately a 9:1 ratio. The folds for WH contain the same subjects as the per-country folds. Architecture selection (i.e., the number of MLP hidden units) and hyperparameter tuning are carried out independently per experiment based on random validation subpartitions of the training splits. For each experiment, the best configuration over all folds is then retrained on the whole per-fold training split and used for all folds. Best architectures per experiment are reported in Sec. A-C of the supplementary material. We perform 10-fold cross-validation three times following the same splits for all models to account for the stochasticity of the data sampling and whole learning process.\n\nPerformance is measured per fold by means of the unweighted average accuracy, also known as unweighted average recall, which gives the same weight to the accuracy of each class regardless of the number of samples for each class. Per-class accuracy is thus equivalent to per-class recall (i.e., the number of samples predicted correctly out of the total number of samples for a given class). Note that the test splits of some folds do not contain all classes, especially puzzled for NO. In such cases, the average accuracy is computed only for the classes that have at least one sample in the test split. We also perform multiple pairwise comparisons with the corrected repeated k-fold cross-validation t-test  [103]  to test for statistically significant differences (p<.05) among average accuracy results. We control for the false discovery rate using the BKY correction  [104] , grouped by country subset. Due to the large amount of experiments, in the following subsections we report the results for the WH dataset, trends observed across different countries, and noteworthy highlights specific to each country. For additional results on a per-country basis, please refer to Sec. B, C and D of the supplementary material for audio-based, video-based under speech, and videobased under silence results, respectively. Country sets used for training and testing are denoted as training country→testing country (e.g., WH→SP to denote training with WH and testing with SP). The WH models selected for the country-specific comparison are the ones that worked better for the WH validation sets, so the reported performance would be different if selecting the best models for each country independently. Likewise, models trained on WH with silence and speech data are also the ones that worked better for the WH validation sets.",
      "page_start": 9,
      "page_end": 10
    },
    {
      "section_name": "C. Audio-Based Emotion Expression Recognition Results",
      "text": "Table V summarizes the results for the different experiments with audio-based labels on the WH dataset. We report below the results with respect to each research question.\n\n1) Main modality: A alone obtains higher accuracy with calm, followed by pleased and then puzzled, correlated with the number of samples per class. Furthermore, puzzled gets more confused with calm than pleased. Country-wise, we see similar trends, except that, for SP, puzzled obtains higher accuracy than pleased. For FR, the accuracy for pleased is less stable than for puzzled. In general, the results are more stable across runs than across folds. The mean SD across folds for WH is 3.3%, while the mean SD across runs is around 0.8%. Thus, as a general note, changes in the standard error of the mean (SEM) mainly denote higher variability across folds.\n\n2) Auxiliary modalities: Despite the high accuracy obtained by F for pleased and puzzled, confusion patterns reveal that calm is mostly confused with puzzled, which does not occur for pleased. This implies that F provides information that is particularly discriminative and potentially correlated to the audio modality specifically for the latter class. By further analyzing the F predictions and their correlation to facial expression categories, we observe that 97% of the pleased predictions correspond to audio segments where the majority facial expression is happy, despite only 30% of the audio segments matching to a happy expression having the pleased annotation. Thus, the same features that correspond to the happy facial expression are related to the facial features corresponding to a pleased speech. On the contrary, G's results are slightly over random performance, indicating that G alone is not informative enough to recognize the classes considered. All unimodal (A, F, G) and unimodal vs bimodal (F vs A+F, G vs A+G) pairwise comparisons are significantly different (p<.0001). Trends are overall maintained country-wise, except that, for SP, calm benefits more from F and puzzled from G. Nevertheless, G and F results are generally less stable than A.\n\n3) Multimodality: Overall, incorporating F improves performance over A alone. By contrast, incorporating G seems detrimental on average. The best multimodal approach for WH, A+F, achieves a 2.5% relative performance increase over A. Class-wise, we observe that adding G increases performance and stability for calm, while adding F is beneficial for pleased and puzzled, despite the stability of the latter decreases. Statistical tests further confirm that A+F vs A+G (p=.038) and A+G vs A+F+G (p=.024) differ significantly. SP and NO follow similar trends class-wise, although for them, G is also beneficial for puzzled but to a lesser extent than A. For FR we observe an inverse trend, which we comment below.\n\n4) Comparison across countries: Fig.  4  depicts per-country average accuracy results. With respect to multimodality, for SP and NO, we observe a similar trend to that of Table  V , with A+F and A+F+G outperforming A alone. For FR, however, ac-curacy decreases as the number of features increases, obtaining the highest average accuracy overall with A (66.28%), which evidences variations among countries. First, FR obtained the lowest inter-agreement score (Sec. III-C1). Analyzing the FR dataset distribution, we find that the average SD of the FR audio features is slightly larger than that of the other countries (0.68 for FR, 0.65 for SP, and 0.63 for NO), indicating that the data is more dispersed in the feature space. Furthermore, the best models for FR are the ones with the lowest number of parameters, as opposed to NO, which requires the largest number of parameters despite having a similar sample size (Sec. B.C of the supplementary). These results indicate that adding more features to FR increases the risk of overfitting, thus decreasing performance. Class-wise, in contrast to WH results, both G and F aid in puzzled recognition for SP, while both aid in calm recognition for FR. Additionally, FR obtains the highest accuracy for pleased with A alone, while for the rest of the classes and countries, multimodal models outperform A. With respect to the auxiliary modalities, NO and FR appear to leverage F and G better than SP, respectively. Per-class accuracies are redistributed with respect to WH.\n\n5) Expanding training data including other countries: Fig.  4  also depicts the effect of training with the WH dataset instead of each country separately. As can be seen, adding more data from different countries consistently improves accuracy on average for SP and NO with A-based models, although stability decreases. NO trained on WH obtains the highest accuracy overall (70.13% with A+F). For FR, however, we find the opposite effect again, with a decrease in accuracy of up to 4.1% for A, where pleased is the most affected class. By contrast, the behavior of the auxiliary modalities is the opposite, with F obtaining the highest accuracy for this class, which additionally increases with respect to FR→FR. Considering previous findings, we hypothesize that the FR audio feature distribution of the pleased class is significantly different than that of SP and NO; thus, adding more data is detrimental. Another difference comes from the arousal distribution of FR, being the country with the highest number of annotated excited instances (61.75% compared to 12-18% for the other countries). Continuing with class-wise results, SP obtains performance increases for all classes in a similar proportion, benefiting from the increased data variability and sample size. By contrast, calm performance decreases for NO, which may be caused by the significant increase in the number of training instances for the minority classes (around 279% and 4369% increase, respectively). With respect to auxiliary modalities, the general trend shows that adding more data hurts performance. By further analyzing confusion patterns, we observe that these are mostly reversed when training with WH, especially affecting discrimination between calm and the other classes for G and calm and puzzled for F. For the latter modality, though, pleased is still recognized accurately.  (speech data); and 2) training on all samples irrespective of speaking status (speech+silence). We report the results below. 1) Main modality: F alone obtains similar accuracy for neutral and happy, while comparatively struggles with pensive, which is highly confused with neutral. This behavior is not proportional to the number of instances since pensive has more than happy. Nonetheless, happy performance is slightly less stable than that of pensive. Country-wise, however, the performance gap is found between neutral and the minority classes instead. The SD across folds is 6.2%, higher for this scenario than for the audio-based but more consistent. By contrast, the SD across runs is around 0.05%. This unveils the large variability across subjects.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "D. Video-Based Emotion Expression Recognition Under Speech",
      "text": "2) Auxiliary modalities: A and G obtain accuracy results closer to the main modality than for the audio-based scenario, with G slightly outperforming F on average. Statistical tests show significant differences for F vs A and F vs G (p=.015), and F vs F+G/A (p<.001) when training with speech data, and for all comparisons when training with all data (p<.001). Remarkably, G achieves the highest accuracy overall for pensive. A appears to be informative for pensive as well but to a lesser extent, also outperforming F, and is more informative than G for happy. Nonetheless, G is less stable than A class-wise, although on average, they are more stable than F. Trends are maintained across countries class-wise. However, on average, A is more informative than G for them, mostly caused by the extremely low performance of G for happy, which gets confused with neutral. Analyzing the confusion patterns for all datasets, we confirm that gaze cues are highly discriminative for pensive and audio cues are highly discriminative for happy.\n\n3) Multimodality: When training on speech data, we observe that adding A or G to F increases accuracy, and the highest is achieved by combining the three modalities, showing a 6.9% relative improvement over F alone. Class-wise, adding G substantially improves performance for pensive followed by neutral, while adding F has a more subtle effect. By contrast, happy appears to benefit from A and not G, but does so when combining the three modalities. We observe similar trends when training on all data. Statistical tests confirm significant differences for the following cases when training on speech data: F vs F+G (p=.031), F vs F+A+G (p=.018), F+G vs F+A (p=.044), and F+A vs F+A+G (p=.015); and when training on all data: F vs F+G (p=.015). Trends are overall maintained across countries, with some differences highlighted below.\n\n4) Comparison across countries: Fig.  5a  illustrates percountry average results for all modalities and the two training regimes. In general, SP achieves the highest accuracy for all modality combinations, greatly benefiting from G followed by A when added to F. It obtains the highest accuracy overall with F+A+G (69.34%) when training with speech, and with F+G (69.33%) when training with all data. NO achieves a similar accuracy with the trimodal model, although it benefits more from A than from G. By contrast, FR barely benefits from adding A, and repeatedly scores the lowest, despite having slightly more data than NO and almost equal performance on average with F. This might be partly caused by the difference in class proportions across countries, with happy being the most variant class (4.7% of the total data for FR, 0.8% for SP, and 2.7% for NO). The difference in the distribution of audio features for FR (discussed in Sec. V-C) is also noticeable here, with A alone obtaining the lowest accuracy for FR, and with a substantial difference compared to the other countries. Adding A to F hurts pensive recognition for FR due to a high confusion between neutral and pensive, although their performance alone is better, and the highest with A. With respect to G, we observe the highest discriminative power for pensive with SP, with more elevated levels of neutral-happy confusion for the other two countries, thus scoring lower in the comparison.\n\n5) Expanding training data including other countries: Fig.  5a  also illustrates the effect of training with WH instead of each country separately for the two training regimes. In general, adding more data increases accuracy on average for all cases except for A, for which accuracy is mostly maintained or slightly reduced. As can be seen, FR obtains the highest performance increase overall despite still scoring the lowest, and NO obtains the highest accuracy results overall, with F+A+G being the top performer (74.24%). SP and FR also achieve the highest accuracy with F+A+G. Models are slightly less stable when training on WH for F-based models, except for NO. SP obtains the lowest gain, highly likely due to being the country with the most instances in WH for all classes except for happy. However, when we investigate class-wise trends, we observe an interesting difference. For NO and FR, the minority classes have their accuracy significantly increased, and the neutral accuracy decreased for all modality combinations, proving that the increase in variability and effective training data is beneficial for them to decrease confusion with neutral. By contrast, SP sees the neutral accuracy increase with all F-based models for all cases and with G only when training with speech data, while F and F+G maintain accuracy for pensive and decrease it for happy. However, performance increases when adding A for the minority classes, although A alone gets happy accuracy reduced, while with G alone they both improve. For silence-based evaluation (Sec. V-E), happy accuracy does increase with F-based models when training on WH. Thus, we believe this difference in happy performance is due to the facial deformations caused by speaking being different across countries, greatly increasing variability. Although this is true for all countries, due to the small number of happy instances for SP, this distribution might be narrower than that of FR and NO. Therefore, increasing variability could be detrimental. Then, when adding A to F, A helps discriminate better among classes that visually might be more similar. Regarding A alone, we mostly observe a redistribution of accuracies among classes, with an increase in happy-neutral confusion.",
      "page_start": 11,
      "page_end": 12
    },
    {
      "section_name": "6) Expanding Training Data Including Silence Instances:",
      "text": "As can be seen in Table  VI  for WH, training on all data marginally but consistently improves performance on average, with the highest improvement obtained with G. Class-wise, confusion patterns reveal that the minority classes are less predicted as neutral, with a slight increase in confusion in the other direction in some cases. Nonetheless, the change is positive for pensive and happy. For G specifically, the neutral-happy confusion patterns are inverted. The consistent improvement is also observed across countries, as depicted in Fig.  5a , both when training per country and when training on WH, and the class-wise trends are generally maintained. As a matter of fact, per-class accuracies tend to be more balanced in this setting. This indicates that, by including training instances with no facial deformations caused by speaking, the models can pick up other cues that are consistent regardless of speaking status, which helps detect more actual happy and pensive instances. Specifically, pensive always obtains the highest accuracy overall, with a slight performance increase when training on WH with models including G. The other classes also increase their accuracy when training with all data, although the highest accuracy is obtained from including A, which can only be accomplished when training with speech instances.  1) Main modality: For WH, results match the video-underspeech scenario on average and per class, with a decrease in stability. The neutral-pensive confusion also decreases. Country-wise, trends are similar to the video-under-speech scenario but differ from WH. For SP, pensive is the top performer (73.3% accuracy) followed closely by neutral, while for FR and NO, the top performer is neutral by a great margin (around 78-80% accuracy). For NO specifically, pensive obtains extremely low accuracy (38.3%). As usual, the latter is mostly misclassified with neutral, but also with happy to a lesser extent, a behavior we had not observed until now. Compared to the video-under-speech scenario, the proportion of neutral instances under silence is higher (around 93-98% depending on the country), which might explain the accuracy bias. For the other classes, however, accuracy results do not generally match class proportions, with happy corresponding only to 1.4% of the total sample size for WH but scoring very close to pensive. The SD across folds is 8.5%, slightly higher than the speech scenario. The SD across runs has a fairly larger range, with an approximate mean of 0.1%.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "E. Video-Based Emotion Expression Recognition Under Silence",
      "text": "2) Auxiliary modality: Following previous trends, G alone achieves the highest accuracy and discriminative power for pensive. The average accuracy for G gets the closest to F compared to audio and video-under-speech scenarios. G is more stable than F on average for neutral and pensive, while for happy is not. No significant differences are found between F and G on average. These trends are also found across countries. Interestingly, for NO, pensive is misclassified as happy at a similar proportion compared to the main modality.\n\n3) Multimodality: Adding G to F consistently increases accuracy on average, following previous trends class-wise, i.e., increasing accuracy and stability for neutral and pensive, while not contributing for happy, for which F alone is usually the top performer. As a matter of fact, pensive is the most benefited class for WH and across countries, and neutral obtains the highest accuracy overall across training regimes.\n\nNo statistically significant differences are found, likely due to the high SEM of F alone, which decreases with multimodality.\n\n4) Comparison across countries: Fig.  5b  depicts percountry average accuracy results. As can be seen, all countries benefit from multimodality, with FR benefiting the most when training on silence data. SP achieves the highest accuracy for all settings by a large margin, obtaining the best accuracy overall with F+G (74.35%) when training with silence data. Contrary to the video-under-speech evaluation, NO scores the lowest. With respect to the auxiliary modality G, its difference with respect to F is small for FR for both training regimes and for NO only when training when silence, as the performance of G decreases when training with all data. Class-wise, trends vary compared to WH depending on the modality, as already discussed above. Again, one highlight is the low accuracy and stability for pensive with either modality for NO compared to the other countries and classes, as well as its confusion with happy. In fact, NO is the country with the highest number of pensive instances (around 5.4% of the NO sample size), which might indicate a difference in the annotation procedure.\n\n5) Expanding training data including other countries: Fig.  5  also shows the effect of training with WH. Similarly to the video-under-speech evaluation scenario, SP barely benefits from adding cross-country data compared to the other countries. Still, the highest accuracy is obtained by SP with F+G (75.4%) when training with silence. For FR, the highest performance increase is obtained by F while, for NO, G is the most benefited modality. Class-wise, we again observe differences between SP and the other two countries. For NO and FR, F-based models increase performance for happy and pensive and reduce it for neutral, while for SP, neutral and happy performance increases while pensive performance decreases. We conclude that happy is easier to recognize for F-based models in all countries when no facial deformations caused by speaking occur. However, for SP, we observe an increase in confusion between neutral and pensive when training with WH, which might be explained by a difference in user behavior or annotation procedure between SP and the other countries when users do not speak. This, in turn, might explain their difference in neutral-pensive ratios (61:1 for SP, 33:1 for FR, and 17:1 for NO), compared to their similarity when users speak (around 5:1 for all countries). By contrast, NO shows a different trend compared to the other countries for pensive with G when training with WH: for SP and FR, pensive accuracy decreases when training with silence and it is maintained when training with speech and silence, while for NO, its accuracy increases for both cases. However, we observe again that this accuracy increase also comes with a slight increase in confusion with happy. The performance decrease for pensive for FR and SP is opposed to the general increase observed for this class on video-under-speech.",
      "page_start": 13,
      "page_end": 14
    },
    {
      "section_name": "6) Expanding Training Data Including Speech Instances:",
      "text": "As can be seen in Table  VII , this training regime slightly reduces the performance of all models on average when applied on WH except for G, for which accuracy is maintained but stability decreases. For F-based models, we observe an increase in confusion between neutral and the other two classes since the facial deformations added during training decrease discriminative power, while for G, confusion with neutral mostly increases for happy. Fig.  5b  allows us to compare cross-country performance. We observe similar trends as for WH on average and class-wise, except for SP→SP with happy and NO→NO. The difference in overall behavior for NO is unclear. We observe a general performance decrease for pensive, which could be caused by the high difference in the number of instances between speech and silence sets (808% increase for WH when including speech instances, and similarly high per country), causing the models to learn patterns more tailored to pensive episodes while speaking. This issue may also be causing part of the performance deterioration for happy, and it is the opposite effect observed for the videounder-speech scenario when training on all data. In general, and in contrast to video-under-speech, per-class accuracies tend to be more balanced when trained on silence only for Fbased models, but also when trained on WH. For G, it is harder to categorize due to pensive being the main discriminated class. Nonetheless, this auxiliary modality tends to perform best on average when training on all data, indicating that it benefits from the added variability from different countries and speaking status, regardless of the evaluation scenario.\n\n7) Effect of speaking status on video-based evaluation: For WH, the users' emotional expressions can be better recognized when they are not speaking with G and F-based models on average. Indeed, evaluating on silence instances with no facial deformations that add noise should increase the discriminative power for happy and pensive. However, G should not be directly affected by facial deformations, so this leads us to believe that gaze and head patterns may be correlated to speaking status beyond facial deformations, and that such patterns are more discriminative for silence instances. Country-wise, G also tends to work better when the user is silent. By contrast, trends differ for F-based models depending on the country and the training regime. Class-wise, neutral always obtains the highest accuracy for silence samples trained on silence and speech per country, along with a higher number of false positives. Happy is easier to recognize during silence instances for SP and FR with F when training on WH with silent instances. For NO, happy is easier to recognize during speech instances with models that combine F and A, also when training with WH, highlighting again the importance of A for this country. For WH, both scenarios are equally performant. Finally, pensive is easier to discriminate when evaluating on silence instances for FR, SP, and WH, although it is difficult to determine which training regime is best due to fluctuating confusion patterns. By contrast, for NO, this class is easier to categorize when the user is speaking with F+G when training on WH with speech instances. Note that, with respect to sample size, both speech and silence subsets are given in around 1:1 ratio for neutral and 2:1 for happy, while for pensive, ratios range from 14:1 for SP to 4:1 for NO.",
      "page_start": 15,
      "page_end": 15
    },
    {
      "section_name": "Vi. Discussion",
      "text": "Here, we summarize the findings for each research question (Sec. V-A), and discuss limitations and potential future work.\n\n1) Discriminative power of the main modality: We observe varying levels of confusion between the minority classes and the majority class for both label types, with the highest confusion being observed with pensive. We strongly believe that this is mostly caused by the FER model considering only spatial information, since this class is mostly characterized by specific dynamics. Consequently, it is the most benefited when F is combined with G, which does provide salient dynamics.\n\nNevertheless, the minority classes tend to be well discriminated against each other. Despite employing a balancing strategy, per-class accuracies seem to be associated with their corresponding sample sizes. It is important to note, however, that the majority class in both label types (neutral and calm) is characterized by a relatively lower intensity or absence of emotional expression and can be encountered in transitional phases between other emotions. This poses a challenge in establishing clear category boundaries. All things considered, average accuracies are around 60-70%, which is a decent rate for a three-class unimodal classifier given the nature of our scenario. This could be further improved by leveraging temporal dependencies across time steps  [105] ,  [106] .\n\n2) Performance of auxiliary modalities compared to the main modality: This depends on the label type. For the audiobased scenario, G struggles to achieve sufficient accuracy. By contrast, F does a better job, usually recognizing pleased with similar or higher accuracy and discrimination power than A alone, particularly due to the similarity between happy and pleased facial features. Conversely, for the video-based scenario, both A and G achieve accuracies closer to the main modality, especially when evaluating on silence instances. In particular, G alone is able to recognize pensive with extremely high accuracy, always better than F alone. In addition, in some cases, A is capable of obtaining higher accuracies than F for pensive and happy, being especially informative for the latter.\n\nAchieving such recognition rates using only the auxiliary modalities is advantageous in scenarios where network or sensor failures may produce potential asynchronies between audio and video streams or even data loss. Having channel-specific labels already allows for individual processing. Therefore, if one stream is affected or even deactivated to reduce processing times, the system can still maintain functionality by utilizing the auxiliary modalities. In addition, the video modality may be intentionally altered or disabled for privacy purposes. In the case of alteration, G could still be extracted; however, in the event of video deactivation, only A would remain useful, from which the important video-based events could still be recognized. Nonetheless, as a prospective direction, it is worth considering crossmodal training techniques  [62] , which learn from multiple modalities at training time to improve singlemodality recognition during inference.\n\n3) Multimodality: It is beneficial, provided that the modality that is added to the main modality provides discriminative information for a given class. This is the case when adding F to A for the audio-based scenario for the minority classes, and when adding A or G to F for the video-based scenario for all classes, except happy for G. The result also depends on the distribution of features: the number of network parameters usually increases with the number of input features, which may lead to overfitting if not accounted for.\n\nThe increase in accuracy when combining A and F is in line with the large audiovisual emotion recognition literature  [5]  and with the few works addressing our target population  [19] . It is difficult to contextualize the A/F+G results within the literature, as the conclusions depend on the scenario and features used, and most speech-video-gaze/head works employ the VAD model instead  [58] . As an example of discrete emotion recognition, our results resemble the findings of the aforementioned crossmodal work  [62] , for which including gaze features also improved performance for video-test, while for audio-test only one of their gaze alternatives outperforms the no-gaze option. Class-wise, they observed a similar gaze spatial distribution between neutral and happy.\n\nIdeally, however, a multimodal system should effectively disregard irrelevant information from individual modalities, thereby preventing any detrimental impact on performance. In that sense, simple feature concatenation may not be a solution to this problem, as it may fail to capture the interactions between features adequately. By contrast, attention-based methods are known to adaptively balance the contributions of different modalities  [79] . Nonetheless, our preliminary experiments found no differences among fusion types. Although it is possible that both approaches do yield similar results in our scenario, 1) the stage-wise training, and 2) the fixed-modality synchronization applied are factors that may have influenced the outcome. For the former, end-to-end training would allow modality features to jointly evolve from early network layers  [105] . For the latter, a flexible temporal synchronization able to capture long-term cross-modal dependencies would compensate for the unaligned nature of communication  [107] .\n\n4) Cross-country differences: There are numerous differences that have been previously discussed and will not be reiterated here, which have exposed disparities in how emotions are expressed and perceived across countries. Such differences in behavior and annotation are also reflected in the divergence of the proportions of per-class sample sizes and distribution in the feature space. Nonetheless, results are subject to the imbalance in the number of samples among countries. This causes NO, the country with the lowest sample size, to obtain lower performances in general.",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "5) Multi-Country Training:",
      "text": "In general, main modalities and multimodal combinations benefit from this training regime. With respect to auxiliary modalities, however, only G benefits from it for the video-based scenario, indicating that the additional features are not as transferable cross-country as the main modalities. Performance gains mainly come from the increased variability, which especially benefits minority classes. However, the extent of such gains does not correlate with the sample size increase, since such increase is proportionally similar for the audio-based scenario than for the video-based scenario, but gains are higher for the latter. This can be attributed to: 1) the audio-based scenario having fewer samples than video; 2) A features already being more generic since they come from a large-scale model, thus more variability may not affect results; 3) some acoustic features of emotional expressions being more language-or culturespecific and hence less transferable across countries than facial expressions, which is in line with previous studies  [89] . Note that no sampling strategy has been applied to balance the number of data samples across countries, hence having a bias towards SP, the country with the highest number of samples. Nonetheless, as found in our experiments, the effect of multicountry training will always depend on the feature distribution and the cultural similarities across countries.",
      "page_start": 15,
      "page_end": 15
    },
    {
      "section_name": "6) Training With Spoken And Silent Instances:",
      "text": "Training on all data consistently but marginally improves performance when evaluating on spoken instances, increasing discriminative power for the minority classes. This is due to the fact that the added variability, without the noise produced by speaking, helps the models learn discriminative features that are less influenced by the speaking effect. Interestingly though, the performance obtained with this training regime is on par with that obtained when combining F and A trained on spoken instances only. By contrast, when evaluating on silent instances, training with all data produces the opposite effect, as the variability added makes the recognition task harder. Note that there is no sampling strategy to balance the number of spoken/silent data, hence having a slight bias towards spoken samples, primarily for the minority classes. Nonetheless, these findings indicate that speaking status significantly influences the learning process and should be taken into account when devising similar solutions to ours.\n\n7) Speech vs silence performance: This is highly countryspecific. In general, and related to RQ 6 , both speaking statuses achieve similar accuracy on average, but the minority classes tend to be better discriminated against when the user is not speaking, provided the model was trained on silent instances only (note that the original F features were trained on both instance types, so differences in performance will come exclusively from the final models). This conclusion is intuitive for F-based models since facial deformations affect discriminative power. However, since this difference in performance is also observed for G, this indicates that gaze and head patterns are correlated with speaking status beyond facial deformations, and such patterns are more discriminative for silence instances. Nonetheless, if A is combined with F for spoken instances, both spoken and silent instances can obtain comparable results.\n\n8) Audio vs video performance: Audio-based results are constrained by the limited amount of data. While this effect is balanced for A by the use of a pre-trained large-scale model, which may also aid in generalization ability, auxiliary modalities are indeed impacted. By contrast, the video-based evaluation can leverage sample sizes two orders of magnitude higher, but with higher redundancy among samples. Overall, we observe a higher performance for video-based evaluation than for audio. This can be attributed to the differences in the number of samples. Another reason could be that WavLM was trained on English speech; therefore, even if the extracted features are generic enough due to the large pre-training, they are not specialized to the languages considered in this work and hence important subtleties might have been lost.",
      "page_start": 16,
      "page_end": 16
    },
    {
      "section_name": "Vii. Conclusion",
      "text": "In this paper, we presented a first comprehensive study on non-verbal emotion expression recognition in interactions between older adults and a simulated VC within the context of the EMPATHIC project. We also described the rationale for data collection and annotation procedure aimed at developing a computational approach that could leverage cues from audio and video channels separately. By analyzing the influence of different modalities, training approaches, and communication modes, this research aimed to shed light on some of the factors that affect the effectiveness of emotion recognition in this scenario. Our findings demonstrate that facial, speech, head, and gaze cues can contribute to the accurate recognition of the channel-specific emotional expressions considered with varying levels of discriminative power. As the evaluation was conducted in a subject-independent manner, these cues would prove even more valuable for a personalized online setup, in which the model could continuously learn from the user's behavior during the interaction. Furthermore, we determined that multi-country training can generally compensate for limited data of a particular country, thereby enhancing overall performance despite country-specific differences.\n\nThe insights gained are expected to contribute to the development of more accurate emotion recognition systems, and pave the way for improved VC experiences and personalized technologies catering to the well-being of this age group.",
      "page_start": 16,
      "page_end": 16
    },
    {
      "section_name": "-Supplementary Material -",
      "text": "In this supplementary material, we include additional methodological and evaluation details to those provided in Sec. IV and Sec. V of the main paper. First, we complete the description of the gaze and head trajectory filtering, and the computation of the Looking-at-VC gaze feature (Sec. A-A and Sec. A-B, respectively, complementing Sec. IV-C of the main paper). Second, we provide the sample size used to train and test the final models with the different audio-and video-based labels (Sec. A-C, complementing Sec. IV-D). Third, we also provide the complexity of the best models on validation for each evaluated final model (Sec. A-D, complementing Sec. V-B). Finally, we report the per-country emotion expression recognition results for the audio-based (Sec. B, complementing Sec. V-C), video-based under speech (Sec. C, complementing Sec. V-D), and video-based under silence (Sec. D, complementing Sec. V-E) evaluations.",
      "page_start": 19,
      "page_end": 19
    },
    {
      "section_name": "A. Methodology And Evaluation Details",
      "text": "",
      "page_start": 19,
      "page_end": 19
    },
    {
      "section_name": "A.A. Filtering Invalid Data From Estimated Head And Eye Gaze",
      "text": "When filtering invalid head and eye gaze trajectories, anatomically implausible movements refer to: eye rotation larger than 40 •  [108] , or faster than 860 • between consecutive frames, which is the highest peak angular speed that saccades have been reported to reach  [109] ; and head movements faster than 700 • /s between consecutive frames, based on existing research on maximum rotation speed for voluntary motion  [110] .",
      "page_start": 19,
      "page_end": 19
    },
    {
      "section_name": "A.B. Looking-At-Virtual-Coach Computation",
      "text": "We adapt the work of  [101]  to find the zone with the highest density of gaze points. More specifically, we find gaze point clusters near the center of the plane using the Mean Shift clustering algorithm  [111]  with a bandwidth value estimated per video, and select the cluster with the highest number of points. To account for possible noisy estimates of the line of gaze, head pose, and VC's position, we assign weights to each gaze point based on its Mahalanobis distance to the cluster's distribution weighted by the cluster's inverse covariance. The weights are assigned to each gaze point p i such that:\n\notherwise,   where d(p i , c) is the Mahalanobis distance between the gaze point p i and the cluster c, and the thresholds are set to thr 1 = 1 and thr 2 = 4 standard deviations. For the second case of the piecewise function, points that belong to the cluster are transformed to be in the range [0.7, 1), whereas points that do not belong to it are transformed to be in the range (0, 0.7). These values were found empirically. Per-point weights are further binned and converted into a 6D one-hot encoding vector denoting the likelihood of looking at VC from lower to higher. Per-valid-frame vectors are averaged over a time window, producing a 6D feature vector per window.",
      "page_start": 20,
      "page_end": 20
    },
    {
      "section_name": "A.C. Sample Size Used For Evaluated Models",
      "text": "Table  II  includes the number of data samples per class and country used for the final evaluated models with audio-based labels. Similarly, Table  III  includes the number of data samples per class and country used for the final evaluated models with video-based labels when training with speech instances (left), and silence instances (right). As can be seen, the class ratios with respect to the original sample size are generally maintained.",
      "page_start": 21,
      "page_end": 21
    },
    {
      "section_name": "A.D. Complexity Of Best Evaluated Models",
      "text": "As explained in Sec. IV-E of the main paper, we evaluate three low-complexity MLP configurations, from lowest to highest: 1) 100 hidden units for the first MLP layer, and 20 for the second, referred to as low (L); 2) 200 and 40, referred to as mid (M); and 3) 500 and 100, referred to as high (H). For each evaluated model, with specific modalities, country and",
      "page_start": 19,
      "page_end": 19
    },
    {
      "section_name": "Modality Spain France Norway Whole",
      "text": "",
      "page_start": 19,
      "page_end": 19
    },
    {
      "section_name": "Modality Spain France Norway Whole",
      "text": "Training on speech data:\n\nTraining on all data (speech+silence):",
      "page_start": 20,
      "page_end": 20
    },
    {
      "section_name": "Modality Spain France Norway Whole",
      "text": "Training on silence data:\n\nTraining on all data (speech+silence):\n\nspeaking status used for training, and label type, we select the best configuration in a validation subset and apply it to the test splits of each fold. Tables IV, V and VI report the best configuration for each model of audio, video-under-speech, and video-under-silence evaluations, respectively.",
      "page_start": 21,
      "page_end": 21
    },
    {
      "section_name": "B. Audio-Based Emotion Recognition Results",
      "text": "In this section, we report the results for audio-based emotion recognition with respect to each country separately.",
      "page_start": 20,
      "page_end": 20
    },
    {
      "section_name": "B.A. Spain Evaluation",
      "text": "Table VII summarizes the Spanish results. We discuss them below.\n\n1) Main modality: Contrary to WHOLE, pleased obtains lower accuracy than puzzled. Confusion matrices reveal that these two classes get confused with calm at a similar proportion.\n\n2) Auxiliary modalities: Average trends are similar to those for WHOLE, although the standard error of the mean (SEM) is higher for F than for G on average and per class. Pleased obtains higher accuracy with F than with A for SP→SP. Confusion matrices reveal that, for G, puzzled and pleased are mostly confused with calm, while for F, puzzled is more confused with calm. Accuracy remarkably increases for puzzled when training on WHOLE while decreasing at a similar degree for calm. As a matter of fact, puzzled obtains higher accuracy with F than A for WH→SP, while pleased decreases with respect to SP→SP. Statistical tests confirm significant differences (p<.0001) for the following pairwise comparisons: for SP→SP, A vs F, A vs G, F vs A+F, and G vs A+G; for WH→SP, all unimodal and unimodal vs bimodal comparisons.\n\n3) Multimodality: We observe that adding F to A moderately improves performance overall, and adding G to F+A increases it further, being the top performer. Statistical tests confirm significant differences for A+F vs A+G (p=.008). Class-wise, adding G is beneficial for calm, with F+G obtaining the highest accuracy overall. Additionally, adding F is beneficial for pleased, and both modalities are beneficial for puzzled.",
      "page_start": 20,
      "page_end": 20
    },
    {
      "section_name": "4) Expanding Training Data:",
      "text": "Training on WHOLE consistently improves performance for SP, with the highest accuracy overall obtained with A+F+G for WH→SP, although the SEM also increases for all models except for F. Most A-based models improve their performance for all classes. Auxiliary modalities see a substantial decrease in performance for calm, while puzzled recognition improves. Confusion matrices reveal that calm is highly confused with the two minority classes for both auxiliary modalities, although pleased is better recognized for F. For all models, the SEM decreases for calm, but increases for the other classes, except for the auxiliary modalities, which see a decrease in SEM for puzzled. There are no statistically significant differences.",
      "page_start": 21,
      "page_end": 21
    },
    {
      "section_name": "B.B. France Evaluation",
      "text": "Table VIII summarizes the French results. We discuss them below.\n\n1) Main modality: Trends are similar to those for WHOLE, although the SEM for pleased is higher than that of puzzled for FR. There is less confusion among classes than for SP.\n\n2) Auxiliary modalities: Trends are equivalent to those for WHOLE, with the SEM of F consistently lower than that of G. With F, puzzled obtains the highest accuracy among all models with FR→FR, contrary to SP and NO. Pleased accuracy notably increases when training on WHOLE. The confusion patterns of G are similar to those for SP; however, for F, calm is greatly confused with puzzled. Statistical tests confirm significant differences (p<.0001) for the following  pairwise comparisons: for FR→FR, A vs F, A vs G, F vs A+F, and G vs A+G; for WH→FR, all unimodal (F vs G obtaining p=.037) and unimodal vs bimodal comparisons.\n\n3) Multimodality: Contrary to SP and NO, for FR adding more modalities is detrimental to accuracy, although there are no statistically significant differences. Class-wise, for FR→FR, calm obtaining the highest accuracy with A+F+G, while for WH→FR, puzzled obtains the highest accuracy with A+F. However, these class-wise performance increases seem to come from pure accuracy redistribution, not increased discriminative power. See the discussion on the main paper (Sec. V-C.3) for more details.",
      "page_start": 20,
      "page_end": 20
    },
    {
      "section_name": "4) Expanding Training Data:",
      "text": "Training with WHOLE is not beneficial for FR, with all models consistently decreasing in accuracy on average and increasing their SEM. Class-wise, we observe that, for A-based models, calm does benefit largely from this training regime, usually at the expense of a decrease in accuracy for the other classes. Puzzled does improve for some multimodal models. By contrast, pleased accuracy decreases substantially. We hypothesize that the distribution of audio features for pleased instances is different for FR than for the other two countries, thus the additional variability is particularly detrimental. Auxiliary modalities follow an inverse trend compared to WHOLE, but the same as for SP, decreasing performance for calm while increasing it greatly for pleased. Following SP, the SEM for calm decreases for all models, while for the other classes, it tends to increase for all models except for G.",
      "page_start": 21,
      "page_end": 21
    },
    {
      "section_name": "B.C. Norway Evaluation",
      "text": "Table  IX  summarizes the Norwegian results. We discuss them below.\n\n1) Main modality: For NO, there is a substantial difference across class performance due to data imbalance. More concretely, only 0.23% of the NO dataset belongs to puzzled instances, and some folds do not contain any test instance of this class, which makes this subset harder to evaluate. For this country, pleased is more often confused with calm than for the other countries.\n\n2) Auxiliary modalities: Accuracy trends are similar to those for WHOLE. For NO→NO, pleased gets greatly confused with neutral with G. By contrast, with F, pleased obtains the highest accuracy among all models both when training with NO and with WHOLE. Pleased accuracy also increases notably with G when adding more training data. Statistical tests confirm statistically significant differences (p<.0001) for the following pairwise comparisons: for NO→NO, A vs G, G vs A+G; for WH→NO, A vs F, A vs G, F vs A+F, and G vs A+G.\n\n3) Multimodality: NO shows similar trends with respect to WHOLE and SP, improving performance with multimodal models but with no statistically significant differences. More concretely, A+F+G obtains the highest performance. This is the highest performance increase caused by multimodal fusion across all countries.\n\n4) Expanding training data: Training with WHOLE is beneficial for A-based models, showing a higher improvement In this section, we report the results for emotion expression recognition with video-based labels, for those instances in which the user is speaking.",
      "page_start": 22,
      "page_end": 22
    },
    {
      "section_name": "C.A. Spain Evaluation",
      "text": "Table X summarizes the Spanish results. We detail them below.\n\n1) Main modality: Contrary to WHOLE, for SP (and for the other countries), neutral obtains the highest accuracy, while the accuracy for happy and pensive is reduced and similar. This time, the accuracy distribution does follow the per-class sample size. The two minority classes are confused with neutral at a similar rate.\n\n2) Auxiliary modalities: For SP→SP, A outperforms F on average, while class-wise trends are the same as for WHOLE. For WH→SP, trends are the same as for WHOLE. By contrast, A does not outperform F for happy. The SEM is reduced with both modalities on average and for all classes except for neutral with G for WH→SP, and for happy with A. Statistical results confirm significant differences for all cases of G vs F+G (p<.01), and for A vs F+A (p<.01) only for WH→SP training with speech data.\n\n3) Multimodality: Similar trends to WHOLE, with F+A+G obtaining the highest accuracy on average. Class-wise, SEM increases for happy with multimodal models, but it decreases for the other classes. Pensive is the class that most benefits from multimodality. Statistical significance results when training on speech data: for SP→SP, F+A vs F+A+G (p=.009); for WH→SP, F vs F+G (p=.004), F vs F+A (p=.015), F vs F+A+G (p<.001), and F+A vs F+A+G (p=.003). Significance results when training on speech and silence data: for WH→SP, F vs F+G (p=.001).\n\n4) Expanding training data including other countries: On average, we observe that training with the WHOLE dataset marginally increases accuracy, with the largest increase obtained for G when trained on speech data and no statistically significant differences. Class-wise, and in contrast to the other countries, we observe that, for F-based models, training with WHOLE causes the accuracy for neutral to increase, especially for F alone, while for the auxiliary modalities, it only increases when training with speech and silence data. The accuracy for happy and pensive depends on the modality used: for F-based models, their accuracy decreases for all except for F+A, for which it increases, and F alone, for which pensive stays the same; for A alone, happy decreases and pensive increases; and for G alone, they both increase. Furthermore, for F-based models, the SEM for neutral is decreased, while for the other classes it is increased. By contrast, for auxiliary modalities, the SEM for neutral and pensive is increased, while for happy, it is maintained. It is important to highlight that SP is the most unbalanced dataset over the three countries for both training regimes and, while its minority class, happy, is increased the most with respect to the other countries (473% when training with speech and 635% when training with speech and silence), this does not translate into a higher accuracy for such class, suggesting that the higher number of samples does not provide the necessary variability to increase discriminative power. What is more, based on the findings from the silence-based scenario, for which the behavior for happy and pensive is reversed, we also hypothesize that the facial deformations caused by speaking are different across countries, hence adding variability that might not be necessary for a possibly narrow distribution of the minority classes for SP. However, this effect is compensated for when adding A, which helps increase discriminative power.\n\n5) Expanding training data including silence instances: Adding silence instances during training marginally but consistently improves performance on average for both SP→SP and WH→SP, with no statistically significant differences. Class-",
      "page_start": 22,
      "page_end": 22
    },
    {
      "section_name": "C.B. France Evaluation",
      "text": "Table XI summarizes the French results. We discuss them below.\n\n1) Main modality: The accuracy gap between the minority classes and neutral is higher for FR than for SP due to an increase in confusion. Nonetheless, it is worth mentioning that, when training with WHOLE, happy performance almost equals that of neutral.\n\n2) Auxiliary modalities: FR maintains the SP trends. One interesting difference is that, for FR, A moderately surpasses F when recognizing happy, in addition to pensive as in previous experiments, which is the behavior observed for WHOLE. The SEM increases with A on average, while it decreases with G. Statistical significance results: when training on speech only, for FR→FR, all F vs A and F vs G comparisons (p=.024 for FR→FR and p=.005 for WH→FR) and all G/A vs F+G/A comparisons (all p<.001 except A vs F+A for FR→FR, p=.024); when training on speech and silence, F vs G (p=.013 for FR→FR, and p=.003 for WH→FR) and G vs F+G (p=.005 for FR→FR and p<.0001 for WH→FR).\n\n3) Multimodality: Similar trends to WHOLE, with F+A+G obtaining the highest accuracy on average again, and with pensive being the most benefited class. However, contrary to the other countries, we find that adding A hinders the recognition of pensive for FR→FR, and the accuracy increase on average is also minimal. Nonetheless, the pensive anomaly gets corrected when training on WHOLE. The SEM increases on average, and for pensive and happy, except for F+G for the latter, while decreasing for neutral. Statistical significance results when training on speech data: for FR→FR, F vs F+G (p=.02), F+A vs F+A+G (p=.021); for WH→FR, F vs F+G (p=.024), F vs F+A+G (p=.007), F+G vs F+A+G (p=.044), F+A vs F+A+G (p=.005). Statistical significance results when training on speech and silence data: F vs F+G (p=.009 for FR→FR, and p=.018 for WH→FR).\n\n4) Expanding training data including other countries: On average, we find that training on WHOLE consistently improves performance, with statistically significant differences for the following cases: when training with speech data, F+G (p=.027), F+A (p=.048), and F+A+G (p=.028); and when training with speech and silence data, F+G (p=.032). Contrary to SP, for FR neutral performance greatly decreases while happy and pensive increases. It is not only an accuracy redistribution across classes since the average accuracy also increases. Indeed, training with WHOLE increases the effective number of instances and variability for happy and pensive (69% and 270%, respectively), and the variability for neutral (250%), for which only a 15% of the data is used. By analyzing confusion patterns, we find that training with WHOLE substantially increases the discriminative power of happy and pensive for F-based models. For G, only pensive discriminative power increases, while for A we mainly observe a redistribution in confusion patterns.",
      "page_start": 23,
      "page_end": 24
    },
    {
      "section_name": "5) Expanding Training Data Including Silence Instances:",
      "text": "This scenario also consistently improves performance with respect to training on speech data only, but to a lesser extent than when including other countries. This performance improvement is statistically significant in the following cases at various p-value levels (p<.05): for FR→FR, F, and F+G; for WH→FR, F, and F+G. Class-wise, we observe a similar behavior to including other countries but to a lower extent, with the accuracy for neutral decreasing while the accuracy for the other classes increases. Additionally, training with speech and silence tends to maintain overall confusion patterns with respect to training with speech only for each training regime separately (i.e., FR→FR and WH→FR). For F-based models, we obtain the best class accuracy balance on the WH→FR scenario. As for SP, G trained with speech and silence instances achieves the highest performance for pensive across all evaluated models for both FR→FR and WH→FR, at the expense of an increase of neutral samples being misclassified as pensive. Similar to SP as well, and continuing with G, happy accuracy increases due to a redistribution of confusion with neutral, despite WH→FR obtaining the highest accuracy for happy among the four scenarios. Thus, the most balanced results is obtained with WH→FR trained with speech only.",
      "page_start": 24,
      "page_end": 24
    },
    {
      "section_name": "C.C. Norway Evaluation",
      "text": "Table XII summarizes the Norwegian results, which we discuss below.\n\n1) Main modality: Trends are similar to FR, although the gap between neutral and the minority classes is even higher, highly likely due to the decrease in sample size.\n\n2) Auxiliary modalities: We observe similar trends as for other countries; however, for NO→NO, A obtains the highest accuracy among all models for pensive, instead of G. Actually, for the latter, happy instances are mostly detected as neutral, showing the highest confusion among countries. Statistically significant results: for NO→NO, F vs G/A (p=.046 and p=.048, respectively) and G vs F+G (p=.008) when training with speech data, and G vs F+G (p=.011) when training with speech and silence; for WH→NO, F vs G and G vs F+G for both training types (p<.0001), F vs G (p<.0001), A vs G (p=.007) and A vs F+A (p=.001) when training with speech data.\n\n3) Multimodality: Contrary to the other countries, adding G and A to F obtain similar performance on average, with F+A slightly outperforming F+G. However, it is with the combination of the three modalities that the highest performance is achieved. Also, on average, the SEM decreases for F+A while it increases for F+G. Class-wise trends are similar, with pensive again benefiting the most from multimodality. For F+A, the SEM decreases for neutral and happy, and increases for pensive, with the latter especially for F+G. Statistical significance results when training on speech data: for NO→NO, all pairwise comparisons (all p<.001 except F+G vs F+A+G, with p=.003); for WH→NO, all pairwise comparisons (p<.001 except F vs F+A and F vs F+G vs F+A+G with p <.01). Statistical significance results when training on speech and silence data: for all cases, F vs F+G (p<.0001).\n\n4) Expanding training data including other countries: As for FR, training with WHOLE substantially improves performance, with statistically significant differences for all models at different p-value levels (p<.01) except for A and G. The largest performance increase is obtained by F+A+G. Classwise, we also observe the same trend as for FR for F-based models, with neutral accuracy decreasing while happy and pensive accuracy increases. For G, neutral-happy confusion is redistributed, while pensive maintains its discriminative power. Finally, for A, neutral gets more predicted as happy than when training on NO, although the overall change is positive for the latter.",
      "page_start": 24,
      "page_end": 25
    },
    {
      "section_name": "5) Expanding Training Data Including Silence Instances:",
      "text": "This training scenario also improves performance consistently but to a lesser extent than when adding instances from other countries. Statistical tests show significant differences when training with speech only compared to speech and silence with F+G (p=.035) for NO→NO. We also see the same trends class-wise as for FR. Unlike other countries, NO obtains the highest accuracy for pensive with F+G instead of with G for WH→NO. For F-based models, WH→NO training on speech and silence achieves the highest balance among class accuracies, while for G alone it is better when training on speech only, despite happy achieving lower accuracy. Nonetheless, if we are interested in being better at detecting the minority classes even with a slight increase in false positives, then training on speech and silence would be recommended.",
      "page_start": 25,
      "page_end": 25
    },
    {
      "section_name": "D. Video-Based Recognition Results Under Silence",
      "text": "This section presents the emotion recognition results with video-based labels for instances where the user is not speaking.",
      "page_start": 25,
      "page_end": 25
    },
    {
      "section_name": "D.A. Spain Evaluation",
      "text": "Table XIII summarizes the Spanish results, which we discuss below.\n\n1) Main modality: Contrary to WHOLE, pensive obtains the highest accuracy followed by neutral for SP→SP, and pensive is the most stable. Confusion patterns reveal a neutral-pensive confusion with the same proportion both ways, whereas happy is mostly confused with neutral in one direction.\n\n2) Auxiliary modality: We observe the same trends as for WHOLE, with G outperforming F for pensive and being the top performer for such class for all training configurations except for SP→SP when training with silence data, for which F+G outperforms G for this class. By contrast, happy performance is quite low. In addition, the SEM decreases on average and for neutral, but increases for happy and pensive. We find significant differences for F vs F+G for SP→SP (p=.041 and p=.046 when training on silence, and speech and silence, respectively).\n\n3) Multimodality: Similar trends on average and class-wise, with F vs F+G statistically different when WH→SP (p=.041). In addition, the SEM for happy increases with multimodality.",
      "page_start": 25,
      "page_end": 25
    },
    {
      "section_name": "4) Expanding Training Data Including Other Countries:",
      "text": "Training with WHOLE increases accuracy mostly when training on silence data only, except for G. By contrast, when training on speech and silence data, only F and G obtain an accuracy increase. Class-wise, neutral and happy increase performance, while pensive decreases. G has a different behavior, decreasing accuracy for happy when training with WHOLE. This class-wise behavior is partly opposed to what is found for SP on the video-under-speech evaluation scenario, for which neutral increased performance, happy decreased for Fbased models and increased for G alone, and pensive was maintained for F-based models and increased for G. It also contrasts with the behavior found for FR and NO across speech and silence scenarios, for which neutral accuracy decreases while for happy and pensive accuracy increases (except for FR with G for the latter, which follows the same behavior as SP) when training with WHOLE. Compared to the speechbased scenario, happy is easier to distinguish from neutral with F alone when there are no facial deformations caused by speaking, while for G we notice an increase in confusion with neutral. For pensive, F-based models experience an increase in confusion with neutral when training with WHOLE, which might be explained by a difference in the annotation procedure or user behavior for pensive between SP and the other countries. The SEM increases on average and for pensive when training with WHOLE, while for neutral it decreases for Fbased models and increases for G. Finally, for happy, SEM decreases for G, while for F-based models it decreases when training on silence and increases when training on silence and speech. 5) Expanding training data including speech instances: Contrary to the speech evaluation scenario, training on speech and silence data is detrimental when evaluating on silence only. On average, we only observe an increase in accuracy for G alone. Class-wise, for SP→SP we observe an increase in accuracy for neutral and happy, and a decrease for pensive. By contrast, for WH→SP the accuracy for happy decreases for F-based models instead, which concurs with our previous hypotheses, and increases for G. The SEM slightly increase on average and decrease for happy for SP→SP, while for both training regimes the SEM of neutral decreases.",
      "page_start": 25,
      "page_end": 26
    },
    {
      "section_name": "D.B. France Evaluation",
      "text": "Table XIV summarizes the French results. We report them below.\n\n1) Main modality: For FR, trends are similar with respect to the speech evaluation, with a decrease in accuracy and stability for happy.\n\n2) Auxiliary modality: In line with previous per-country findings, G alone obtains the highest accuracy overall for pensive. The average performance difference between F and G is even smaller than for SP. The following statistically significant differences are found: for FR→FR, G vs F+G when training on silence (p=.009); for WH→FR, F vs G (p=.012) and G vs F+G (p<.001) when training with silence data, and G vs F+G (p=.008) when training on silence and speech.\n\n3) Multimodality: F+G surpasses F on average significantly (p=.001 for FR→FR, and p<.001 for WH→FR). We observe the same trends class-wise, with F+G obtaining the highest performance overall for neutral, while for happy adding G is not beneficial. With respect to SEM changes, however, we see an slight increase on average, for happy it tends to decrease, while for pensive it increases for all cases except for FR→FR when training with silence, when it decreases instead.",
      "page_start": 26,
      "page_end": 26
    },
    {
      "section_name": "4) Expanding Training Data Including Other Countries:",
      "text": "Training with WHOLE substantially increases performance on average. Statistical tests confirm significant differences for all models (p=[.01,.03] when training with silence and p<.01 when training with silence and speech) except G. Class-wise trends for F-based models are the same as for the evaluation with speech data, with neutral accuracy decreasing, while the accuracy and discriminative power for happy and pensive increases. By contrast, for G, neutral and happy accuracy increase while pensive accuracy decreases when training with silence, and it is maintained when trained on speech and silence. Standard errors for F increase on average and for neutral, but decrease for happy and pensive, whereas for G, they decrease on average and for pensive, but increase for neutral and happy.\n\n5) Expanding training data including speech instances: This scenario causes the accuracy to decrease on average for F-based models, while the opposite is observed for G. We obtain significant differences for all comparisons at different pvalue levels (p<.05) except for G. Class-wise, the accuracy for neutral increases while for happy and pensive decreases, which is the opposite than when adding data from other countries, and which follows WHOLE trends. Note that, for G, the accuracy of happy is maintained for FR→FR and decreased for WH→FR. Confusion patterns are similar to those for SP. Standard errors also follow the same trends in general.",
      "page_start": 26,
      "page_end": 26
    },
    {
      "section_name": "D.C. Norway Evaluation",
      "text": "Table XV summarizes Norwegian results. We detail them below.\n\n1) Main modality: Again, trends do not follow those of WHOLE, with pensive scoring the lowest followed by happy.  Compared to the speech scenario, evaluating with silence instances substantially decreases performance for pensive. The confusion between neutral and the minority classes is higher than for SP but lower than for FR. In the unique case of NO, however, a proportion of pensive instances are misclasified as happy.\n\n2) Auxiliary modality: G alone obtains the highest accuracy for pensive for all scenarios except for WH→NO training on silence, for which F+G slightly outperforms it. The difference between F and G on average is moderately larger than for FR but smaller than for SP. No statistically significant differences are found. Compared to the other countries, pensive gets more misclasified with the other two classes.\n\n3) Multimodality: Similar trends, with statistically significant differences for all pairwise comparisons (p<.01) except on NO→NO training on silence data. Similar trends class-wise as well. The SEM increases on average and for happy, while it decreases for neutral.",
      "page_start": 26,
      "page_end": 26
    },
    {
      "section_name": "4) Expanding Training Data Including Other Countries:",
      "text": "Training on WHOLE consistently increases performance on average, although only the increase for G when training on speech and silence is statistically significant (p=.03). Classwise trends are maintained from FR for F-based models, while G follows F trends instead, although the increase for pensive is smaller. Despite the fact that training with WHOLE increases pensive performance with G, it does not necessarily increase its discriminative power, since confusion with happy increases. Standard errors increase for all classes with F-based models while decreasing on average, whereas with G they increase for all except for neutral.\n\n5) Expanding training data including speech instances: Unlike for SP and FR, the effect of this training regime for NO is unclear, with extremely similar results on average. Only for G we observe larger differences. Class-wise trends are maintained, with neutral performance increasing while happy and pensive decreases. For the latter, we find the exceptions of G and F+G for NO→NO, for which pensive accuracy increases. For F-based models, we find that confusion increases between neutral and the minority classes at a similar rate, as opposed to SP and FR, for which confusion happens mostly between neutral and pensive.",
      "page_start": 27,
      "page_end": 27
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Setup with a participant during an interaction session.",
      "page": 4
    },
    {
      "caption": "Figure 1: ). At the start of the session,",
      "page": 4
    },
    {
      "caption": "Figure 2: Representation of the segmentation of annotated emo-",
      "page": 5
    },
    {
      "caption": "Figure 2: ). To assign an emotion to each segment",
      "page": 5
    },
    {
      "caption": "Figure 3: shows an overview of our methodological pipeline.",
      "page": 7
    },
    {
      "caption": "Figure 3: Overview of the methodological pipeline.",
      "page": 7
    },
    {
      "caption": "Figure 4: Per-country audio-based results, training on either",
      "page": 10
    },
    {
      "caption": "Figure 4: depicts per-country",
      "page": 10
    },
    {
      "caption": "Figure 4: also depicts the effect of training with the WH dataset",
      "page": 11
    },
    {
      "caption": "Figure 5: a illustrates per-",
      "page": 12
    },
    {
      "caption": "Figure 5: a also illustrates the effect of training with WH instead",
      "page": 12
    },
    {
      "caption": "Figure 5: Per-country video-based results under (a) speech or (b) silence, trained on SP, FR, NO, and WH training sets, and",
      "page": 13
    },
    {
      "caption": "Figure 5: b depicts per-",
      "page": 13
    },
    {
      "caption": "Figure 5: also shows the effect of training with WH. Similarly",
      "page": 13
    },
    {
      "caption": "Figure 5: b allows us to",
      "page": 14
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "",
          "Column_2": "",
          "Column_3": "",
          "Column_4": "SP→SP",
          "Column_5": ""
        },
        {
          "Column_1": "",
          "Column_2": "",
          "Column_3": "",
          "Column_4": "FR→FR\nNO→NO",
          "Column_5": ""
        },
        {
          "Column_1": "",
          "Column_2": "",
          "Column_3": "",
          "Column_4": "WH→SP",
          "Column_5": ""
        },
        {
          "Column_1": "",
          "Column_2": "",
          "Column_3": "",
          "Column_4": "WH→FR\nWH→NO",
          "Column_5": ""
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "SP→SP\nFR→FR\nNO→NO\nWH→SP": "WH→FR\nWH→NO",
          "Column_2": "",
          "Column_3": "",
          "Column_4": "",
          "Column_5": "",
          "Column_6": "",
          "Column_7": ""
        }
      ],
      "page": 13
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Multimodal human-computer interaction: A survey",
      "authors": [
        "A Jaimes",
        "N Sebe"
      ],
      "year": "2007",
      "venue": "Computer vision and image understanding"
    },
    {
      "citation_id": "2",
      "title": "The semaine database: Annotated multimodal records of emotionally colored conversations between a person and a limited agent",
      "authors": [
        "G Mckeown",
        "M Valstar",
        "R Cowie",
        "M Pantic",
        "M Schroder"
      ],
      "year": "2011",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "3",
      "title": "Analysis of the interaction between elderly people and a simulated virtual coach",
      "authors": [
        "R Justo",
        "L Ben Letaifa",
        "C Palmero",
        "E Gonzalez-Fraile",
        "A Torp Johansen",
        "A Vázquez",
        "G Cordasco",
        "S Schlögl",
        "B Fernández-Ruanova",
        "M Silva",
        "S Escalera",
        "M Develasco",
        "J Tenorio-Laranga",
        "A Esposito",
        "M Korsnes",
        "M Torres"
      ],
      "year": "2020",
      "venue": "Journal of Ambient Intelligence and Humanized Computing"
    },
    {
      "citation_id": "4",
      "title": "Dialogue management and language generation for a robust conversational virtual coach: Validation and user study",
      "authors": [
        "A Vázquez",
        "A López Zorrilla",
        "J Olaso",
        "M Torres"
      ],
      "year": "2023",
      "venue": "Sensors"
    },
    {
      "citation_id": "5",
      "title": "A review of affective computing: From unimodal analysis to multimodal fusion",
      "authors": [
        "S Poria",
        "E Cambria",
        "R Bajpai",
        "A Hussain"
      ],
      "year": "2017",
      "venue": "Information fusion"
    },
    {
      "citation_id": "6",
      "title": "Deep learning for human affect recognition: Insights and new developments",
      "authors": [
        "P Rouast",
        "M Adam",
        "R Chiong"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "7",
      "title": "A review and meta-analysis of multimodal affect detection systems",
      "authors": [
        "J Kory"
      ],
      "year": "2015",
      "venue": "ACM computing surveys"
    },
    {
      "citation_id": "8",
      "title": "Affective and behavioural computing: Lessons learnt from the first computational paralinguistics challenge",
      "authors": [
        "B Schuller",
        "F Weninger",
        "Y Zhang",
        "F Ringeval",
        "A Batliner",
        "S Steidl",
        "F Eyben",
        "E Marchi",
        "A Vinciarelli",
        "K Scherer",
        "M Chetouani",
        "M Mortillaro"
      ],
      "year": "2019",
      "venue": "Computer Speech & Language"
    },
    {
      "citation_id": "9",
      "title": "Analyzing emotion in spontaneous speech",
      "authors": [
        "R Chakraborty",
        "M Pandharipande",
        "S Kopparapu"
      ],
      "year": "2017",
      "venue": "Analyzing emotion in spontaneous speech"
    },
    {
      "citation_id": "10",
      "title": "Automatic identification of emotional information in spanish tv debates and human-machine interactions",
      "authors": [
        "M Develasco",
        "R Justo",
        "M Torres"
      ],
      "year": "2022",
      "venue": "Applied Sciences"
    },
    {
      "citation_id": "11",
      "title": "Facial expression recognition in the presence of speech using blind lexical compensation",
      "authors": [
        "S Mariooryad",
        "C Busso"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "12",
      "title": "A survey of affect recognition methods: Audio, visual, and spontaneous expressions",
      "authors": [
        "Z Zeng",
        "M Pantic",
        "G Roisman",
        "T Huang"
      ],
      "year": "2009",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "13",
      "title": "The cita go-on dialogue system: Mid-term achievements",
      "authors": [
        "J Olaso",
        "M García-Sebastián",
        "A López Zorrilla",
        "M Tainta",
        "M Ecay-Torres",
        "M Torres",
        "P Martínez-Lage"
      ],
      "year": "2023",
      "venue": "Proc. ACM Int. Conf. on PErvasive Technologies Related to Assistive Environments"
    },
    {
      "citation_id": "14",
      "title": "Interaction with a virtual coach for active and healthy ageing",
      "authors": [
        "M Mctear",
        "K Jokinen",
        "M Alam",
        "Q Saleem",
        "G Napolitano",
        "F Szczepaniak",
        "M Hariz",
        "G Chollet",
        "C Lohr",
        "J Boudy",
        "Z Azimi",
        "S Roelen",
        "R Wieching"
      ],
      "year": "2023",
      "venue": "Sensors"
    },
    {
      "citation_id": "15",
      "title": "Multimodal emotion analysis of robotic assistance in elderly care",
      "authors": [
        "A Demaeght",
        "C Miclau",
        "J Hartmann",
        "J Markwardt",
        "O Korn"
      ],
      "year": "2022",
      "venue": "Proceedings of the 15th International Conference on PErvasive Technologies Related to Assistive Environments"
    },
    {
      "citation_id": "16",
      "title": "Emotion experience and expression across the adult life span: insights from a multimodal assessment study",
      "authors": [
        "C Magai",
        "N Consedine",
        "Y Krivoshekova",
        "E Kudadjie-Gyamfi",
        "R Mcpherson"
      ],
      "year": "2006",
      "venue": "Psychology and aging"
    },
    {
      "citation_id": "17",
      "title": "Facial age affects emotional expression decoding",
      "authors": [
        "M Fölster",
        "U Hess",
        "K Werheid"
      ],
      "year": "2014",
      "venue": "Frontiers in Psychology"
    },
    {
      "citation_id": "18",
      "title": "Emotion, physiology, and expression in old age",
      "authors": [
        "R Levenson",
        "L Carstensen",
        "W Friesen",
        "P Ekman"
      ],
      "year": "1991",
      "venue": "Psychology and aging"
    },
    {
      "citation_id": "19",
      "title": "Elderreact: a multimodal dataset for recognizing emotional response in aging adults",
      "authors": [
        "K Ma",
        "X Wang",
        "X Yang",
        "M Zhang",
        "J Girard",
        "L.-P Morency"
      ],
      "year": "2019",
      "venue": "International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "20",
      "title": "Speech emotion recognition using fourier parameters",
      "authors": [
        "K Wang",
        "N An",
        "B Li",
        "Y Zhang",
        "L Li"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on affective computing"
    },
    {
      "citation_id": "21",
      "title": "Facial emotion recognition in the elderly using a svm classifier",
      "authors": [
        "N Lopes",
        "A Silva",
        "S Khanal",
        "A Reis",
        "J Barroso",
        "V Filipe",
        "J Sampaio"
      ],
      "year": "2018",
      "venue": "2nd International Conference on Technology and Innovation in Sports, Health and Wellbeing"
    },
    {
      "citation_id": "22",
      "title": "The interspeech 2020 computational paralinguistics challenge: Elderly emotion, breathing & masks",
      "authors": [
        "B Schuller",
        "A Batliner",
        "C Bergler",
        "E.-M Messner",
        "A Hamilton",
        "S Amiriparian",
        "A Baird",
        "G Rizos",
        "M Schmitt",
        "L Stappen",
        "H Baumeister",
        "A Deighton Macintyre",
        "S Hantke"
      ],
      "year": "2020",
      "venue": "The interspeech 2020 computational paralinguistics challenge: Elderly emotion, breathing & masks"
    },
    {
      "citation_id": "23",
      "title": "Sewa db: A rich database for audio-visual emotion and sentiment research in the wild",
      "authors": [
        "J Kossaifi",
        "R Walecki",
        "Y Panagakis",
        "J Shen",
        "M Schmitt",
        "F Ringeval",
        "J Han",
        "V Pandit",
        "A Toisoul",
        "B Schuller",
        "K Star",
        "E Hajiyev",
        "M Pantic"
      ],
      "year": "2019",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "24",
      "title": "Proceedings of the 12th ACM International Conference on PErvasive Technologies Related to Assistive Environments",
      "authors": [
        "M Torres",
        "J Olaso",
        "C Montenegro",
        "R Santana",
        "A Vázquez",
        "R Justo",
        "J Lozano",
        "S Schlögl",
        "G Chollet",
        "N Dugan",
        "M Irvine",
        "N Glackin",
        "C Pickard",
        "A Esposito",
        "G Cordasco",
        "A Troncone",
        "D Petrovska-Delacretaz",
        "A Mtibaa",
        "M Hmani",
        "M Korsnes",
        "L Martinussen",
        "S Escalera",
        "C Cantariño",
        "O Deroo",
        "O Gordeeva",
        "J Tenorio-Laranga",
        "E Gonzalez-Fraile",
        "B Fernandez-Ruanova",
        "A Gonzalez-Pinto"
      ],
      "year": "2019",
      "venue": "Proceedings of the 12th ACM International Conference on PErvasive Technologies Related to Assistive Environments"
    },
    {
      "citation_id": "25",
      "title": "The empathic virtual coach: A demo",
      "authors": [
        "J Olaso",
        "A Vázquez",
        "L Ben Letaifa",
        "M Velasco",
        "A Mtibaa",
        "M Hmani",
        "D Petrovska-Delacrétaz",
        "G Chollet",
        "C Montenegro",
        "A López-Zorrilla",
        "R Justo",
        "R Santana",
        "J Tenorio-Laranga",
        "E González-Fraile",
        "B Fernández-Ruanova",
        "G Cordasco",
        "A Esposito",
        "K Gjellesvik",
        "A Johansen",
        "M Kornes",
        "C Pickard",
        "C Glackin",
        "G Cahalane",
        "P Buch",
        "C Palmero",
        "S Escalera",
        "O Gordeeva",
        "O Deroo",
        "A Fernández",
        "D Kyslitska",
        "J Lozano",
        "M Torres",
        "S Schlögl"
      ],
      "year": "2021",
      "venue": "International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "26",
      "title": "Handbook of cognition and emotion",
      "authors": [
        "P Ekman"
      ],
      "year": "1999",
      "venue": "Handbook of cognition and emotion"
    },
    {
      "citation_id": "27",
      "title": "Automatic, dimensional and continuous emotion recognition",
      "authors": [
        "H Gunes",
        "M Pantic"
      ],
      "year": "2010",
      "venue": "International Journal of Synthetic Emotions"
    },
    {
      "citation_id": "28",
      "title": "Affect detection: An interdisciplinary review of models, methods, and their applications",
      "authors": [
        "R Calvo",
        "S Mello"
      ],
      "year": "2010",
      "venue": "IEEE Transactions on affective computing"
    },
    {
      "citation_id": "29",
      "title": "Recognising realistic emotions and affect in speech: State of the art and lessons learnt from the first challenge",
      "authors": [
        "B Schuller",
        "A Batliner",
        "S Steidl",
        "D Seppi"
      ],
      "year": "2011",
      "venue": "Speech communication"
    },
    {
      "citation_id": "30",
      "title": "A circumplex model of affect",
      "authors": [
        "J Russell"
      ],
      "year": "1980",
      "venue": "Journal of personality and social psychology"
    },
    {
      "citation_id": "31",
      "title": "Avec 2014: 3d dimensional affect and depression recognition challenge",
      "authors": [
        "M Valstar",
        "B Schuller",
        "K Smith",
        "T Almaev",
        "F Eyben",
        "J Krajewski",
        "R Cowie",
        "M Pantic"
      ],
      "year": "2014",
      "venue": "Proceedings of the 4th International Workshop on Audio/Visual Emotion Challenge"
    },
    {
      "citation_id": "32",
      "title": "Speech emotion recognition using deep neural network considering verbal and nonverbal speech sounds",
      "authors": [
        "K.-Y Huang",
        "C.-H Wu",
        "Q.-B Hong",
        "M.-H Su",
        "Y.-H Chen"
      ],
      "year": "2019",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "33",
      "title": "Analysis and Automatic Identification of Spontaneous Emotions in Speech from Human-Human and Human-Machine Communication",
      "authors": [
        "M Develasco"
      ],
      "year": "2023",
      "venue": "Departamento de Electricidad y Electrónica"
    },
    {
      "citation_id": "34",
      "title": "Audio features for music emotion recognition: a survey",
      "authors": [
        "R Panda",
        "R Malheiro",
        "R Paiva"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "35",
      "title": "The interspeech 2013 computational paralinguistics challenge: Social signals, conflict, emotion, autism",
      "authors": [
        "B Schuller",
        "S Steidl",
        "A Batliner",
        "A Vinciarelli",
        "K Scherer",
        "F Ringeval",
        "M Chetouani",
        "F Weninger",
        "F Eyben",
        "E Marchi",
        "M Mortillaro",
        "H Salamin",
        "A Polychroniou",
        "F Valente",
        "S Kim"
      ],
      "year": "2013",
      "venue": "Proceedings INTERSPEECH 2013, 14th Annual Conference of the International Speech Communication Association"
    },
    {
      "citation_id": "36",
      "title": "The geneva minimalistic acoustic parameter set (gemaps) for voice research and affective computing",
      "authors": [
        "F Eyben",
        "K Scherer",
        "B Schuller",
        "J Sundberg",
        "E André",
        "C Busso",
        "L Devillers",
        "J Epps",
        "P Laukka",
        "S Narayanan",
        "K Truong"
      ],
      "year": "2015",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "37",
      "title": "wav2vec 2.0: A framework for self-supervised learning of speech representations",
      "authors": [
        "A Baevski",
        "Y Zhou",
        "A Mohamed",
        "M Auli"
      ],
      "year": "2020",
      "venue": "wav2vec 2.0: A framework for self-supervised learning of speech representations"
    },
    {
      "citation_id": "38",
      "title": "A proposal for multimodal emotion recognition using aural transformers and action units on ravdess dataset",
      "authors": [
        "C Luna-Jiménez",
        "R Kleinlein",
        "D Griol",
        "Z Callejas",
        "J Montero",
        "F Fernández-Martínez"
      ],
      "year": "2022",
      "venue": "Applied Sciences"
    },
    {
      "citation_id": "39",
      "title": "Hubert: Self-supervised speech representation learning by masked prediction of hidden units",
      "authors": [
        "W.-N Hsu",
        "B Bolte",
        "Y.-H Tsai",
        "K Lakhotia",
        "R Salakhutdinov",
        "A Mohamed"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Trans. on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "40",
      "title": "Unispeech: Unified speech representation learning with labeled and unlabeled data",
      "authors": [
        "C Wang",
        "Y Wu",
        "Y Qian",
        "K Kumatani",
        "S Liu",
        "F Wei",
        "M Zeng",
        "X Huang"
      ],
      "year": "2021",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "41",
      "title": "Wavlm: Large-scale self-supervised pretraining for full stack speech processing",
      "authors": [
        "S Chen",
        "C Wang",
        "Z Chen",
        "Y Wu",
        "S Liu",
        "Z Chen",
        "J Li",
        "N Kanda",
        "T Yoshioka",
        "X Xiao",
        "J Wu",
        "L Zhou",
        "S Ren",
        "Y Qian",
        "Y Qian",
        "J Wu",
        "M Zeng",
        "X Yu",
        "F Wei"
      ],
      "venue": "IEEE Journal of Selected Topics in Signal Processing"
    },
    {
      "citation_id": "42",
      "title": "A systematic literature review of speech emotion recognition approaches",
      "authors": [
        "Y Singh",
        "S Goel"
      ],
      "year": "2022",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "43",
      "title": "An ongoing review of speech emotion recognition",
      "authors": [
        "J De Lope",
        "M Graña"
      ],
      "year": "2023",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "44",
      "title": "Speech emotion recognition with dual-sequence lstm architecture",
      "authors": [
        "J Wang",
        "M Xue",
        "R Culhane",
        "E Diao",
        "J Ding",
        "V Tarokh"
      ],
      "year": "2020",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "45",
      "title": "Speech emotion recognition using self-supervised features",
      "authors": [
        "E Morais",
        "R Hoory",
        "W Zhu",
        "I Gat",
        "M Damasceno",
        "H Aronowitz"
      ],
      "year": "2022",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "46",
      "title": "Dawn of the transformer era in speech emotion recognition: closing the valence gap",
      "authors": [
        "J Wagner",
        "A Triantafyllopoulos",
        "H Wierstorf",
        "M Schmitt",
        "F Burkhardt",
        "F Eyben",
        "B Schuller"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "47",
      "title": "Survey on bimodal speech emotion recognition from acoustic and linguistic information fusion",
      "authors": [
        "B Atmaja",
        "A Sasou",
        "M Akagi"
      ],
      "year": "2022",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "48",
      "title": "Deep facial expression recognition: A survey",
      "authors": [
        "S Li",
        "W Deng"
      ],
      "year": "2020",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "49",
      "title": "PCA-based dictionary building for accurate facial expression recognition via sparse representation",
      "authors": [
        "M Mohammadi",
        "E Fatemizadeh",
        "M Mahoor"
      ],
      "year": "2014",
      "venue": "Journal of Visual Communication and Image Representation"
    },
    {
      "citation_id": "50",
      "title": "Gabor feature based classification using the enhanced fisher linear discriminant model for face recognition",
      "authors": [
        "C Liu",
        "H Wechsler"
      ],
      "year": "2002",
      "venue": "IEEE Transactions on Image Processing"
    },
    {
      "citation_id": "51",
      "title": "Facial expression recognition based on local binary patterns: A comprehensive study",
      "authors": [
        "C Shan",
        "S Gong",
        "P Mcowan"
      ],
      "year": "2009",
      "venue": "Image and Vision Computing"
    },
    {
      "citation_id": "52",
      "title": "DISFA: A spontaneous facial action intensity database",
      "authors": [
        "S Mavadati",
        "M Mahoor",
        "K Bartlett",
        "P Trinh",
        "J Cohn"
      ],
      "year": "2013",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "53",
      "title": "Survey on rgb, 3d, thermal, and multimodal approaches for facial expression recognition: History, trends, and affect-related applications",
      "authors": [
        "C Corneanu",
        "M Simón",
        "J Cohn",
        "S Guerrero"
      ],
      "year": "2016",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "54",
      "title": "A brief review of facial emotion recognition based on visual information",
      "authors": [
        "B Ko"
      ],
      "year": "2018",
      "venue": "sensors"
    },
    {
      "citation_id": "55",
      "title": "Going deeper in facial expression recognition using deep neural networks",
      "authors": [
        "A Mollahosseini",
        "D Chan",
        "M Mahoor"
      ],
      "year": "2016",
      "venue": "Going deeper in facial expression recognition using deep neural networks"
    },
    {
      "citation_id": "56",
      "title": "Neurocognitive mechanisms of gazeexpression interactions in face processing and social attention",
      "authors": [
        "R Graham",
        "K Labar"
      ],
      "year": "2012",
      "venue": "Neuropsychologia"
    },
    {
      "citation_id": "57",
      "title": "Emotion recognition using eye-tracking: taxonomy, review and current challenges",
      "authors": [
        "J Lim",
        "J Mountstephens",
        "J Teo"
      ],
      "year": "2020",
      "venue": "Sensors"
    },
    {
      "citation_id": "58",
      "title": "Affective computing using speech and eye gaze: a review and bimodal system proposal for continuous affect prediction",
      "authors": [
        "J O'dwyer",
        "N Murray",
        "R Flynn"
      ],
      "year": "2018",
      "venue": "Affective computing using speech and eye gaze: a review and bimodal system proposal for continuous affect prediction",
      "arxiv": "arXiv:1805.06652"
    },
    {
      "citation_id": "59",
      "title": "Evaluation of appearance-based methods and implications for gaze-based applications",
      "authors": [
        "X Zhang",
        "Y Sugano",
        "A Bulling"
      ],
      "year": "2019",
      "venue": "Proc. CHI conference on human factors in computing systems"
    },
    {
      "citation_id": "60",
      "title": "Automatic gaze analysis: A survey of deep learning based approaches",
      "authors": [
        "S Ghosh",
        "A Dhall",
        "M Hayat",
        "J Knibbe",
        "Q Ji"
      ],
      "year": "2021",
      "venue": "Automatic gaze analysis: A survey of deep learning based approaches",
      "arxiv": "arXiv:2108.05479"
    },
    {
      "citation_id": "61",
      "title": "Multimodal depression detection: fusion analysis of paralinguistic, head pose and eye gaze behaviors",
      "authors": [
        "S Alghowinem",
        "R Goecke",
        "M Wagner",
        "J Epps",
        "M Hyett",
        "G Parker",
        "M Breakspear"
      ],
      "year": "2016",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "62",
      "title": "Gaze-enhanced crossmodal embeddings for emotion recognition",
      "authors": [
        "A Abdou",
        "E Sood",
        "P Müller",
        "A Bulling"
      ],
      "year": "2022",
      "venue": "Proc. ACM on Human-Computer Interaction"
    },
    {
      "citation_id": "63",
      "title": "Rt-bene: A dataset and baselines for real-time blink estimation in natural environments",
      "authors": [
        "K Cortacero",
        "T Fischer",
        "Y Demiris"
      ],
      "year": "2019",
      "venue": "Proc. International Conference on Computer Vision Workshops"
    },
    {
      "citation_id": "64",
      "title": "Gaze control in humans: eye-head coordination during orienting movements to targets within and beyond the oculomotor range",
      "authors": [
        "D Guitton",
        "M Volle"
      ],
      "year": "1987",
      "venue": "Journal of neurophysiology"
    },
    {
      "citation_id": "65",
      "title": "Real-time inference of complex mental states from facial expressions and head gestures",
      "authors": [
        "R Kaliouby",
        "P Robinson"
      ],
      "year": "2005",
      "venue": "Real-time vision for human-computer interaction"
    },
    {
      "citation_id": "66",
      "title": "Looking at you or looking elsewhere: The influence of head orientation on the signal value of emotional facial expressions",
      "authors": [
        "U Hess",
        "R Adams",
        "R Kleck"
      ],
      "year": "2007",
      "venue": "Motivation and Emotion"
    },
    {
      "citation_id": "67",
      "title": "Rigid head motion in expressive speech animation: Analysis and synthesis",
      "authors": [
        "C Busso",
        "Z Deng",
        "M Grimm",
        "U Neumann",
        "S Narayanan"
      ],
      "year": "2007",
      "venue": "IEEE transactions on audio, speech, and language processing"
    },
    {
      "citation_id": "68",
      "title": "Body movements for affective expression: A survey of automatic recognition and generation",
      "authors": [
        "M Karg",
        "A.-A Samadani",
        "R Gorbet",
        "K Kühnlenz",
        "J Hoey",
        "D Kulić"
      ],
      "year": "2013",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "69",
      "title": "Dimensional emotion prediction from spontaneous head gestures for interaction with sensitive artificial listeners",
      "authors": [
        "H Gunes",
        "M Pantic"
      ],
      "year": "2010",
      "venue": "Intelligent Virtual Agents: 10th International Conference"
    },
    {
      "citation_id": "70",
      "title": "Decoupling facial expressions and head motions in complex emotions",
      "authors": [
        "A Adams",
        "M Mahmoud",
        "T Baltrušaitis",
        "P Robinson"
      ],
      "year": "2015",
      "venue": "2015 International conference on affective computing and intelligent interaction"
    },
    {
      "citation_id": "71",
      "title": "On the role of head motion in affective expression",
      "authors": [
        "A Samanta",
        "T Guha"
      ],
      "year": "2017",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "72",
      "title": "Low-level characterization of expressive head motion through frequency domain analysis",
      "authors": [
        "Y Ding",
        "L Shi",
        "Z Deng"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "73",
      "title": "Emotion sensing from head motion capture",
      "authors": [
        "A Samanta",
        "T Guha"
      ],
      "year": "2020",
      "venue": "IEEE Sensors Journal"
    },
    {
      "citation_id": "74",
      "title": "Head pose estimation: A survey of the last ten years",
      "authors": [
        "K Khan",
        "R Khan",
        "R Leonardi",
        "P Migliorati",
        "S Benini"
      ],
      "year": "2021",
      "venue": "Signal Processing: Image Communication"
    },
    {
      "citation_id": "75",
      "title": "Investigating the relationship between momentary emotion self-reports and head and eye movements in hmd-based 360 vr video watching",
      "authors": [
        "T Xue",
        "A Ali",
        "G Ding",
        "P Cesar"
      ],
      "year": "2021",
      "venue": "Extended abstracts of the 2021 conference on human factors in computing systems"
    },
    {
      "citation_id": "76",
      "title": "Eye-based continuous affect prediction",
      "authors": [
        "J O'dwyer",
        "N Murray",
        "R Flynn"
      ],
      "year": "2019",
      "venue": "2019 8th International Conference on Affective Computing and Intelligent Interaction"
    },
    {
      "citation_id": "77",
      "title": "Deep multimodal learning: A survey on recent advances and trends",
      "authors": [
        "D Ramachandram",
        "G Taylor"
      ],
      "year": "2017",
      "venue": "IEEE signal processing magazine"
    },
    {
      "citation_id": "78",
      "title": "Multimodal machine learning: A survey and taxonomy",
      "authors": [
        "T Baltrušaitis",
        "C Ahuja",
        "L.-P Morency"
      ],
      "year": "2018",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "79",
      "title": "Deep multimodal representation learning: A survey",
      "authors": [
        "W Guo",
        "J Wang",
        "S Wang"
      ],
      "year": "2019",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "80",
      "title": "Survey on audiovisual emotion recognition: databases, features, and data fusion strategies",
      "authors": [
        "C.-H Wu",
        "J.-C Lin",
        "W.-L Wei"
      ],
      "year": "2014",
      "venue": "APSIPA transactions on signal and information processing"
    },
    {
      "citation_id": "81",
      "title": "Multimodal coordination of facial action, head rotation, and eye motion during spontaneous smiles",
      "authors": [
        "J Cohn",
        "L Reed",
        "T Moriyama",
        "J Xiao",
        "K Schmidt",
        "Z Ambadar"
      ],
      "year": "2004",
      "venue": "IEEE Int. Conf. on Automatic Face and Gesture Recognition"
    },
    {
      "citation_id": "82",
      "title": "Continuous emotion recognition in videos by fusing facial expression, head pose and eye gaze",
      "authors": [
        "S Wu",
        "Z Du",
        "W Li",
        "D Huang",
        "Y Wang"
      ],
      "year": "2019",
      "venue": "Int. Conf. on Multimodal Interaction"
    },
    {
      "citation_id": "83",
      "title": "Multimodal affect recognition in an interactive gaming environment using eye tracking and speech signals",
      "authors": [
        "A Alhargan",
        "N Cooke",
        "T Binjammaz"
      ],
      "year": "2017",
      "venue": "Proceedings of the 19th ACM international conference on multimodal interaction"
    },
    {
      "citation_id": "84",
      "title": "Speech, head, and eye-based cues for continuous affect prediction",
      "authors": [
        "J O'dwyer"
      ],
      "year": "2019",
      "venue": "8th International Conference on Affective Computing and Intelligent Interaction Workshops and Demos"
    },
    {
      "citation_id": "85",
      "title": "Wizard of oz experimentation for language technology applications: challenges and tools",
      "authors": [
        "S Schlögl",
        "G Doherty",
        "S Luz"
      ],
      "year": "2015",
      "venue": "Interacting with Computers"
    },
    {
      "citation_id": "86",
      "title": "Self-report captures 27 distinct categories of emotion bridged by continuous gradients",
      "authors": [
        "A Cowen",
        "D Keltner"
      ],
      "year": "2017",
      "venue": "Proc. of the national academy of sciences"
    },
    {
      "citation_id": "87",
      "title": "The perceptual and cognitive role of visual and auditory channels in conveying emotional information",
      "authors": [
        "A Esposito"
      ],
      "year": "2009",
      "venue": "Cognitive Computation"
    },
    {
      "citation_id": "88",
      "title": "Facial and vocal expressions of emotion",
      "authors": [
        "J Russell",
        "J.-A Bachorowski",
        "J.-M Fernández-Dols"
      ],
      "year": "2003",
      "venue": "Annual review of psychology"
    },
    {
      "citation_id": "89",
      "title": "A cross-cultural study on the effectiveness of visual and vocal channels in transmitting dynamic emotional information",
      "authors": [
        "M Riviello",
        "A Esposito"
      ],
      "year": "2012",
      "venue": "Acta Polytechnica Hungarica"
    },
    {
      "citation_id": "90",
      "title": "Interrater reliability: the kappa statistic",
      "authors": [
        "M Mchugh"
      ],
      "year": "2012",
      "venue": "Biochemia medica"
    },
    {
      "citation_id": "91",
      "title": "Development of user-state conventions for the multimodal corpus in smartkom",
      "authors": [
        "S Steininger",
        "F Schiel",
        "O Dioubina",
        "S Raubold"
      ],
      "year": "2002",
      "venue": "Proc. Workshop on Multimodal Resources and Multimodal Systems Evaluation"
    },
    {
      "citation_id": "92",
      "title": "Emotional features of interactions with empathic agents",
      "authors": [
        "C Greco",
        "C Buono",
        "P Buch-Cardona",
        "G Cordasco",
        "S Escalera",
        "A Esposito",
        "A Fernandez",
        "D Kyslitska",
        "M Kornes",
        "C Palmero",
        "J Tenorio-Laranga",
        "A Johansen",
        "M Torres"
      ],
      "year": "2021",
      "venue": "Proc. IEEE/CVF International Conference on Computer Vision"
    },
    {
      "citation_id": "93",
      "title": "Faceboxes: A cpu real-time face detector with high accuracy",
      "authors": [
        "S Zhang",
        "X Zhu",
        "Z Lei",
        "H Shi",
        "X Wang",
        "S Li"
      ],
      "year": "2017",
      "venue": "Faceboxes: A cpu real-time face detector with high accuracy"
    },
    {
      "citation_id": "94",
      "title": "Towards fast, accurate and stable 3d dense face alignment",
      "authors": [
        "J Guo",
        "X Zhu",
        "Y Yang",
        "F Yang",
        "Z Lei",
        "S Li"
      ],
      "year": "2020",
      "venue": "European Conference on Computer Vision"
    },
    {
      "citation_id": "95",
      "title": "Xception: Deep learning with depthwise separable convolutions",
      "authors": [
        "F Chollet"
      ],
      "year": "2017",
      "venue": "Proc. IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "96",
      "title": "Ima-geNet: A large-scale hierarchical image database",
      "authors": [
        "J Deng",
        "W Dong",
        "R Socher",
        "L.-J Li",
        "K Li",
        "L Fei-Fei"
      ],
      "year": "2009",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "97",
      "title": "A multiresolution 3d morphable face model and fitting framework",
      "authors": [
        "P Huber",
        "G Hu",
        "R Tena",
        "P Mortazavian",
        "P Koppen",
        "W Christmas",
        "M Ratsch",
        "J Kittler"
      ],
      "year": "2016",
      "venue": "Proceedings of the 11th international joint conference on computer vision, imaging and computer graphics theory and applications"
    },
    {
      "citation_id": "98",
      "title": "Epnp: An accurate o(n) solution to the pnp problem",
      "authors": [
        "V Lepetit",
        "F Moreno-Noguer",
        "P Fua"
      ],
      "year": "2009",
      "venue": "International Journal Of Computer Vision"
    },
    {
      "citation_id": "99",
      "title": "Eth-xgaze: A large scale dataset for gaze estimation under extreme head pose and gaze variation",
      "authors": [
        "X Zhang",
        "S Park",
        "T Beeler",
        "D Bradley",
        "S Tang",
        "O Hilliges"
      ],
      "year": "2020",
      "venue": "European Conference on Computer Vision"
    },
    {
      "citation_id": "100",
      "title": "Eye movement analysis for activity recognition using electrooculography",
      "authors": [
        "A Bulling",
        "J Ward",
        "H Gellersen",
        "G Tröster"
      ],
      "year": "2010",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "101",
      "title": "Using eye tracking to investigate interaction between humans and virtual agents",
      "authors": [
        "T Amorese",
        "C Greco",
        "M Cuciniello",
        "C Buono",
        "C Palmero",
        "P Buch-Cardona",
        "S Escalera",
        "M Torres",
        "G Cordasco",
        "A Esposito"
      ],
      "year": "2022",
      "venue": "IEEE Conference on Cognitive and Computational Aspects of Situation Management (CogSIMA)"
    },
    {
      "citation_id": "102",
      "title": "Is cross-attention preferable to self-attention for multi-modal emotion recognition?",
      "authors": [
        "V Rajan",
        "A Brutti",
        "A Cavallaro"
      ],
      "year": "2022",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "103",
      "title": "Evaluating the replicability of significance tests for comparing learning algorithms",
      "authors": [
        "R Bouckaert",
        "E Frank"
      ],
      "year": "2004",
      "venue": "Pacific-Asia Conference on Knowledge Discovery and Data Mining"
    },
    {
      "citation_id": "104",
      "title": "Adaptive linear step-up procedures that control the false discovery rate",
      "authors": [
        "Y Benjamini",
        "A Krieger",
        "D Yekutieli"
      ],
      "year": "2006",
      "venue": "Biometrika"
    },
    {
      "citation_id": "105",
      "title": "End-to-end multimodal emotion recognition using deep neural networks",
      "authors": [
        "P Tzirakis",
        "G Trigeorgis",
        "M Nicolaou",
        "B Schuller",
        "S Zafeiriou"
      ],
      "year": "2017",
      "venue": "IEEE Journal of selected topics in signal processing"
    },
    {
      "citation_id": "106",
      "title": "Earlystage parkinson's disease detection based on action unit derivatives",
      "authors": [
        "A Filali Razzouki",
        "L Jeancolas",
        "G Mangone",
        "S Sambin",
        "A Chalanc ¸on",
        "M Gomes",
        "S Lehéricy",
        "J.-C Corvol",
        "M Vidailhet",
        "I Arnulf",
        "M El-Yacoubi",
        "D Petrovska-Delacrétaz"
      ],
      "venue": "Dispositifs biomédicaux et technologies numériques en santé ; des besoins aux usages"
    },
    {
      "citation_id": "107",
      "title": "Multimodal transformer for unaligned multimodal language sequences",
      "authors": [
        "Y.-H Tsai",
        "S Bai",
        "P Liang",
        "J Kolter",
        "L.-P Morency",
        "R Salakhutdinov"
      ],
      "year": "2019",
      "venue": "Proceedings of the conference. Association for Computational Linguistics. Meeting"
    },
    {
      "citation_id": "108",
      "title": "Normal range of eye movement and its relationship to age",
      "authors": [
        "Y Shin",
        "H Lim",
        "M Kang",
        "M Seong",
        "H Cho",
        "J Kim"
      ],
      "year": "2016",
      "venue": "Acta Ophthalmologica"
    },
    {
      "citation_id": "109",
      "title": "The main sequence, a tool for studying human eye movements",
      "authors": [
        "A Bahill",
        "M Clark",
        "L Stark"
      ],
      "year": "1975",
      "venue": "Mathematical biosciences"
    },
    {
      "citation_id": "110",
      "title": "Frequency and velocity of rotational head perturbations during locomotion",
      "authors": [
        "G Grossman",
        "R Leigh",
        "L Abel",
        "D Lanska",
        "S Thurston"
      ],
      "year": "1988",
      "venue": "Experimental brain research"
    },
    {
      "citation_id": "111",
      "title": "Mean shift: A robust approach toward feature space analysis",
      "authors": [
        "D Comaniciu",
        "P Meer"
      ],
      "year": "2002",
      "venue": "IEEE Transactions on pattern analysis and machine intelligence"
    }
  ]
}