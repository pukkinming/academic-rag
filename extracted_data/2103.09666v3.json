{
  "paper_id": "2103.09666v3",
  "title": "Multimodal End-To-End Sparse Model For Emotion Recognition",
  "published": "2021-03-17T14:05:05Z",
  "authors": [
    "Wenliang Dai",
    "Samuel Cahyawijaya",
    "Zihan Liu",
    "Pascale Fung"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Existing works on multimodal affective computing tasks, such as emotion recognition, generally adopt a two-phase pipeline, first extracting feature representations for each single modality with hand-crafted algorithms and then performing end-to-end learning with the extracted features. However, the extracted features are fixed and cannot be further fine-tuned on different target tasks, and manually finding feature extraction algorithms does not generalize or scale well to different tasks, which can lead to sub-optimal performance. In this paper, we develop a fully end-to-end model that connects the two phases and optimizes them jointly. In addition, we restructure the current datasets to enable the fully end-to-end training. Furthermore, to reduce the computational overhead brought by the end-to-end model, we introduce a sparse cross-modal attention mechanism for the feature extraction. Experimental results show that our fully end-to-end model significantly surpasses the current state-of-theart models based on the two-phase pipeline. Moreover, by adding the sparse cross-modal attention, our model can maintain performance with around half the computation in the feature extraction part.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Humans show their characteristics through not only the words they use, but also the way they speak and their facial expressions. Therefore, in multimodal affective computing tasks, such as emotion recognition, there are usually three modalities: textual, acoustic, and visual. One of the main challenges in these tasks is how to model the interactions between different modalities, as they contain both supplementary and complementary information  (Baltrušaitis et al., 2018) .  In the existing works, we discover that a twophase pipeline is generally used  (Zadeh et al., 2018a,b; Tsai et al., 2018 Tsai et al., , 2019;; Rahman et al., 2020) . In the first phase, given raw input data, feature representations are extracted with hand-crafted algorithms for each modality separately, while in the second phase, end-to-end multimodal learning is performed using extracted features. However, there are three major defects of this two-phase pipeline: 1) the features are fixed after extraction and cannot be further fine-tuned on target tasks; 2) manually searching for appropriate feature extraction algorithms is needed for different target tasks; and 3) the hand-crafted model considers very few data points to represent higher-level feature, which might not capture all the useful information. These defects can result in sub-optimal performance.\n\nIn this paper, we propose a fully end-to-end model that connects the two phases together and optimizes them jointly. In other words, the model receives raw input data and produces the output pre-dictions, which allows the features to be learned automatically through the end-to-end training. However, the current datasets for multimodal emotion recognition cannot be directly used for the fully end-to-end training, and we thus conduct a data restructuring to make this training possible. The benefits from the end-to-end training are that the features are optimized on specific target tasks, and there is no need to manually select feature extraction algorithms. Despite the advantages of the endto-end training, it does bring more computational overhead compared to the two-phase pipeline, and exhaustively processing all the data points makes it computationally expensive and prone to overfitting. Thus, to mitigate these side-effects, we also propose a multimodal end-to-end sparse model, a combination of a sparse cross-modal attention mechanism and sparse Convolutional Neural Network (CNN)  (Graham and van der Maaten, 2017) , to select the most relevant features for the task and reduce the redundant information and noise in the video and audio.\n\nExperimental results show that the simply endto-end training model is able to consistently outperform the existing state-of-the-art models which are based on the two-phase pipeline. Moreover, the incorporation of the sparse cross-modal attention and sparse CNN is able to greatly reduce the computational cost and maintain the performance.\n\nWe summarize our contributions as follows.\n\n• To the best of our knowledge, we are the first to apply a fully end-to-end trainable model for the multimodal emotion recognition task.\n\n• We restructure the existing multimodal emotion recognition datasets to enable the end-toend training and cross-modal attention based on the raw data.\n\n• We show that the fully end-to-end training significantly outperforms the current state-of-theart two-phase models, and the proposed sparse model can greatly reduce the computational overhead while maintaining the performance of the end-to-end training. We also conduct a thorough analysis and case study to improve the interpretability of our method.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Works",
      "text": "Human affect recognition is a popular and widely studied research topic  (Mirsamadi et al., 2017; Zhang and Liu, 2017; Xu et al., 2020; Dai et al., 2020b) . In recent years, there is a trend to leverage multimodal information to tackle these research tasks, such as emotion recognition  (Busso et al., 2008) , sentiment analysis  (Zadeh et al., 2016 (Zadeh et al., , 2018b)) , personality trait recognition  (Nojavanasghari et al., 2016) , etc, have drawn more and more attention. Different methods have been proposed to improve the performance and crossmodal interactions. In earlier works, early fusion  (Morency et al., 2011; Pérez-Rosas et al., 2013)  and late fusion  (Zadeh et al., 2016; Wang et al., 2017)  of modalities were widely adopted.\n\nLater, more complex approaches were proposed. For example,  Zadeh et al. (2017)  introduced the Tensor Fusion Network to model the interactions of the three modalities by performing the Cartesian product, while  (Wang et al., 2019)  used an attention gate to shift the words using the visual and acoustic features. In addition, based on the Transformer  (Vaswani et al., 2017) ,  Tsai et al. (2019)  introduced the Multimodal Transformer to improve the performance given unaligned multimodal data, and  Rahman et al. (2020)  introduced a multimodal adaptation gate to integrate visual and acoustic information into a large pre-trained language model. However, unlike some other multimodal tasks  (Chen et al., 2017; Yu et al., 2019; Li et al., 2019)  using fully end-to-end learning, all of these methods require a feature extraction phase using hand-crafted algorithms (details in Section 5.2), which makes the whole approach a twophase pipeline.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Dataset Reorganization",
      "text": "The fully end-to-end multimodal model requires the inputs to be raw data for the three modalities (visual, textual and acoustic). The existing multimodal emotion recognition datasets cannot be directly applied for the fully end-to-end training for two main reasons. First, the datasets provide split of training, validation and test data for the hand-crafted features as the input of the model and emotion or sentiment labels as the output of the model. However, this dataset split cannot be directly mapped to the raw data since the split indices cannot be matched back to the raw data. Second, the labels of the data samples are aligned with the text modality. However, the visual and acoustic modalities are not aligned with the textual modality in the raw data, which disables the fully end-to-end training. To make the existing datasets usable for the fully end-to-end training and evaluation, we need to reorganize them according to two steps: 1) align the text, visual and acoustic modalities; 2) split the aligned data into training, validation and test sets.\n\nIn this work we reorganize two emotion recognition datasets: Interactive Emotional Dyadic Motion Capture (IEMOCAP) and CMU Multimodal Opinion Sentiment and Emotion Intensity (CMU-MOSEI). Both have multi-class and multi-labelled data for multimodal emotion recognition obtained by generating raw utterance-level data, aligning the three modalities, and creating a new split over the aligned data. In the following section, we will first introduce the existing datasets, and then we will give a detailed description of how we reorganize them.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Iemocap",
      "text": "IEMOCAP  (Busso et al., 2008 ) is a multimodal emotion recognition dataset containing 151 videos. In each video, two professional actors conduct dyadic conversations in English. The dataset is labelled by nine emotion categories, but due to the data imbalance issue, we take the six main categories: angry, happy, excited, sad, frustrated, and neutral. As the dialogues are annotated at the utterance level, we clip the data per utterance from the provided text transcription time, which results in 7,380 data samples in total. Each data sample consists of three modalities: audio data with a sampling rate of 16 kHz, a text transcript, and image frames sampled from the video at 30 Hz. The provided pre-processed data from the existing work  (Busso et al., 2008)   1  doesn't provide an identifier for each data sample, which makes it impossible to reproduce it from the raw data. To cope with this problem, we create a new split for the dataset by randomly allocating 70%, 10%, and 20% of data into the training, validation, and testing sets, respectively. The statistics of our dataset split are shown in Table  1 .",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Cmu-Mosei",
      "text": "CMU-MOSEI  (Zadeh et al., 2018b)   sample in CMU-MOSEI consists of three modalities: audio data with a sampling rate of 44.1 kHz, a text transcript, and image frames sampled from the video at 30 Hz. We generate the utterance-level data from the publicly accesible raw CMU-MOSEI dataset.  2  The generated utterances are perfectly matched with the preprocessed data from the existing work  (Zadeh et al., 2018b) , but there are two issues with the existing dataset: 1) in includes many misaligned data samples; and 2) many of the samples do not exist in the generated data, and vice versa, in the provided standard split from the CMU MultiModal SDK.  3  To cope with the first issue, we perform data cleaning to remove the misaligned samples, which results in 20,477 clips in total. We then create a new dataset split following the CMU-MOSEI split for the sentiment classification task.  4 The statistics of the new dataset split setting are shown in Table  2 .",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Methodology",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Problem Definition",
      "text": "We define I multimodal data samples as",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Fully End-To-End Multimodal Modeling",
      "text": "We build a fully end-to-end model which jointly optimizes the two separate phases (feature extraction and multimodal modelling).\n\nFor each spectrogram chunk and image frame in the visual and acoustic modalities, we first use a pre-trained CNN model (an 11-layer VGG (Simonyan and Zisserman, 2014) model) to extract the input features, which are then flattened to vector representations using a linear transformation. After that, we can obtain a sequence of representations for both visual and acoustic modalities. Then, we use a Transformer  (Vaswani et al., 2017)  model to encode the sequential representations since it contains positional embeddings to model the temporal information. Finally, we take the output vector at the \"CLS\" token and apply a feed-forward network (FFN) to get the classification scores.\n\nIn addition, to reduce GPU memory and align with the two-phase baselines which extract visual features from human faces, we use a MTCNN  (Zhang et al., 2016)  model to get the location of faces for the image frames before feeding them into the VGG. For the textual modality, the Transformer model is directly used to process the sequence of words. Similar to the visual and acoustic modalities, we consider the feature at the \"CLS\" token as the output feature and feed it into a FFN to generate the classification scores. We take a weighted sum of the classification scores from each modality to make the final prediction score.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Multimodal End-To-End Sparse Model",
      "text": "Although the fully end-to-end model has many advantages over the two-phase pipeline, it also brings much computational overhead. To reduce this overhead without downgrading the performance, we introduce our Multimodal End-to-end Sparse Model (MESM). Figure  2  shows the overall architecture of MESM. In contrast to the fully end-to-end model, we replace the original CNN layers (except the first one for low-level feature capturing) with N cross-modal sparse CNN blocks. A cross-modal sparse CNN block consists of two parts, a crossmodal attention layer and a sparse CNN model that contains two sparse VGG layers and one sparse max-pooling layer.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Cross-Modal Attention Layer",
      "text": "The cross-modal attention layer accepts two inputs: a query vector q ∈ R d and a stack of feature maps M ∈ R C×S×H×W , where C, S, H, and W are the number of channels, sequence length, height, and width, respectively. Then, the cross-modal spatial attention is performed over the feature maps using the query vector. The cross-modal spatial attention can be formularized in the following steps:\n\nin which W m ∈ R k×C , W q ∈ R k×d , and W i ∈ R k are linear transformation weights, and b m ∈ R k and b i ∈ R 1 are biases, where k is a pre-defined hyper-parameter, and ⊕ represents the broadcast addition operation of a tensor and a vector. In Eq.2, the softmax function is applied to the (H × W ) dimensions, and M i ∈ R S×H×W is the tensor of the spatial attention scores corresponding to each feature map. Finally, to make the input feature maps M sparse while reserving important information, firstly, we perform Nucleus Sampling  (Holtzman et al., 2019)  on M i to get the top-p portion of the probability mass in each attention score map (p is a pre-defined hyper-parameter in the range of (0, 1]). In M ns , the points selected by the Nucleus Sampling are set to one and the others are set to zero. Then, we do broadcast point-wise multiplication between M ns and M to generate the output M o . Therefore, M o is a sparse tensor with some positions being zero, and the degree of sparsity is controlled by p.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Sparse Cnn",
      "text": "We use the submanifold sparse CNN (Graham and van der Maaten, 2017) after the cross-modal attention layer. It is leveraged for processing lowdimensional data which lies in a space of higher dimensionality. In the multimodal emotion recognition task, we assume that only part of the data is related to the recognition of emotions (an intuitive example is given in Figure  1 ), which makes it align with the sparse setting. In our model, the sparse CNN layer accepts the output from the crossmodal attention layer, and does convolution computation only at the active positions. Theoretically, in terms of the amount of computation (FLOPs) at a single location, a standard convolution costs z 2 mn FLOPs, and a sparse convolution costs amn FLOPs, where z is the kernel size, m is the number of input channels, n is the number of output channels, and a is the number of active points at this location. Therefore, considering all locations and all layers, the sparse CNN can help to significantly reduce computation.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Experiments",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Evaluation Metrics",
      "text": "Following prior works  (Tsai et al., 2018; Wang et al., 2019; Tsai et al., 2019; Dai et al., 2020a) , we use the accuracy and F1-score to evaluate the models on the IEMOCAP dataset. On the CMU-MOSEI dataset, we use the weighted accuracy instead of the standard accuracy. Additionally, according to  Dai et al. (2020a) , we use the standard binary F1 rather than the weighted version.\n\nWeighted Accuracy Similar to existing works  (Zadeh et al., 2018b; Akhtar et al., 2019) , we use the weighted accuracy (WAcc)  (Tong et al., 2017)  to evaluate the CMU-MOSEI dataset, which contains many more negative samples than positive ones on each emotion category. If normal accuracy is used, a model will still get a fine score when predicting all samples to be negative. The formula of the weighted accuracy is\n\nin which P means total positive, TP true positive, N total negative, and TN true negative.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Baselines",
      "text": "For our baselines, we use a two-phase pipeline, which consists of a feature extraction step and an end-to-end learning step.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Feature Extraction",
      "text": "We follow the feature extraction procedure in the previous works  (Zadeh et al., 2018b; Tsai et al., 2018 Tsai et al., , 2019;; Rahman et al., 2020) . For the visual data, we extract 35 facial action units (FAUs) using the OpenFace library 5    (Baltrušaitis et al., 2015; Baltrusaitis et al., 2018)  for the image frames in the video, which capture the movement of facial muscles  (Ekman et al., 1980) . For the acoustic data, we extract a total of 142 dimension features consisting of 22 dimension bark band energy (BBE) features, 12 dimension mel-frequency cepstral coefficient (MFCC) features, and 108 statistical features from 18 phonological classes. We extract the features per 400 ms time frame using the DisVoice library 6    (Vásquez-Correa et al., 2018 , 2019) . For textual data, we use the pre-trained\n\nTable  4 : The results on the CMU-MOSEI dataset. WAcc stands for weighted accuracy. We report the accuracy and the F1-score on six emotion categories: angry, disgusted, fear, happy, sad and surprised. We re-run the models marked by † , as the data we use is unaligned along the sequence length dimension and the split is different.\n\nGloVe  (Pennington et al., 2014)  word embeddings (glove.840B.300d 7  ).\n\nMultimodal Learning As different modalities are unaligned in the data, we cannot compare our method with existing works that can only handle aligned input data. We use four multimodal learning models as baselines: the late fusion LSTM (LF-LSTM) model, the late fusion Transformer (LF-TRANS) model, the Emotion Embeddings (EmoEmbs) model  (Dai et al., 2020a) , and the Multimodal Transformer (MulT) model  (Tsai et al., 2019) . They receive the hand-crafted features extracted from the first step as input and give the classification decisions.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Training Details",
      "text": "We use the Adam optimizer (Kingma and Ba, 2014) for the training of every model we use. For the loss function, we use the binary cross-entropy loss as both of the datasets are multi-class and multilabelled. In addition, the loss for the positive samples is weighted by the ratio of the number of positive and negative samples to mitigate the imbalance problem. For all of the models, we perform an exhaustive hyper-parameter search to ensure we have solid comparisons. The best hyper-parameters are reported in Appendix A. Our experiments are run on an Nvidia 1080Ti GPU, and our code is implemented in the PyTorch  (Paszke et al., 2019)  framework v1.6.0. We perform preprocessing for the text and audio modalities. For the text modality, we perform word tokenization for our baseline and subword tokenization for our end-to-end model. We limit the length of the text to up to 50 tokens.\n\nFor the audio modality, we use mel-spectrograms with a window size of 25 ms and stride of 12.5 ms and then chunk the spectrograms per 400 ms time window.\n\n6 Analysis",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Results Analysis",
      "text": "In Table  3 , we show the results on the IEMOCAP dataset. Compared to the baselines, the fully endto-end (FE2E) model surpasses them by a large margin on all the evaluation metrics. Empirically, this shows the superiority of the FE2E model over the two-phase pipeline. Furthermore, our MESM achieves comparable results with the FE2E model, while requiring much less computation in the feature extraction. Here, we only show the results of MESM with the best p value of the Nucleus Sampling. In Section 6.3, we conduct a more detailed discussion of the effects of the top-p values. We further evaluate the methods on the CMU-MOSEI dataset and the results are shown in Table  4 . We observe similar trends on this dataset.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Case Study",
      "text": "To improve the interpretability and gain more insights from our model, we visualize the attention maps of our sparse cross-modal attention mechanism on the six basic emotions: happy, sad, angry, surprised, fear, and disgusted. As shown in Figure  3 , in general, the models attend to several regions of interest such as the mouth, eyes, eyebrows, and facial muscles between the mouth and the eyes. We verify our method by comparing the regions that our model captures based on the facial action coding system (FACS)  (Ekman, 1997) . Following the mapping of FACS to human emotion categories  (Basori, 2016; Ahn and Chung, 2017) , we conduct empirical analysis to validate the sparse cross-modal attention on each emotion category. For example, the emotion happy is highly influenced by raising of the lip on both ends, while sad is related to a lowered lip on both ends and downward movement of the eyelids. Angry is determined from a narrowed gap between the eyes and thinned lips, while surprised is expressed with an open mouth and raising of the eyebrows and eyelids. Fear is indicated by a rise of the eyebrows and upper eyelids, and also an open mouth with the ends of the lips slightly moving toward the cheeks.\n\nFor the emotion disgusted, wrinkles near the nose area and movement of the upper lip region are the determinants.\n\nBased on the visualization of the attention maps on the visual data in Figure  3 , the MESM can capture most of the specified regions of interest for the six emotion categories. For the emotion angry, the sparse cross-modal attention can retrieve the features from the lip region quite well, but it sometimes fails to capture the gap between the eyes. For surprised, the eyelids and mouth regions can be successfully captured by MESM, but sometimes the model fails to consider the eyebrow regions. For the acoustic modality, it is hard to analyse the attention in terms of emotion labels. We show a general visualization of the attention maps over the audio data in Figure  4 . The model attends to",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Effects Of Nucleus Sampling",
      "text": "To have an in-depth understanding of the effects of Nucleus Sampling on the MESM, we perform more experiments with different top-p values ranging from 0 to 1, with a step of 0.1. As shown in Figure  5 , empirically, the amount of computation is reduced consistently with the decrease of the top-p values. In terms of performance, with a top-p value from 0.9 to 0.5, there is no significant drop in the evaluation performance. Starting from 0.5 to 0.1, we can see a clear downgrade in the performance, which means some of the useful information for recognizing the emotion is excluded. The inflection point of this elbow shaped trend line can be an indicator to help us make a decision on the value of the top-p. Specifically, with a top-p of 0.5, the MESM can achieve comparable performance to the FE2E model with around half of the FLOPs in the feature extraction.   FE2E ) and multimodal end-to-end sparse model (MESM) on the IEMOCAP dataset. In the Mods. (modalities) column, the T/A/V indicates the existence of the textual (T), acoustic (A), and visual (V) modalities.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Ablation Study",
      "text": "We conduct a comprehensive ablation study to further investigate how the models perform when one or more modalities are absent. The results are shown in Table  5 . Firstly, we observe that the more modalities the more improvement in the performance. TAV, representing the presence of all three modalities, results in the best performance for both models, which shows the effectiveness of having more modalities. Secondly, with only a single modality, the textual modality results in better performance than the other two, which is similar to the results of previous multimodal works. This phenomenon further validates that using textual (T) to attend to acoustic (A) and visual (V) in our crossmodal attention mechanism is a reasonable choice. Finally, with two modalities, the MESM can still achieve a performance that is on par with the FE2E model or is even slightly better.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Conclusion And Future Work",
      "text": "In this paper, we first compare and contrast the two-phase pipeline and the fully end-to-end (FE2E) modelling of the multimodal emotion recognition task. Then, we propose our novel multimodal end-to-end sparse model (MESM) to reduce the computational overhead brought by the fully endto-end model. Additionally, we reorganize two existing datasets to enable fully end-to-end training. The empirical results demonstrate that the FE2E model has an advantage in feature learning and surpasses the current state-of-the-art models that are based on the two-phase pipeline. Furthermore, MESM is able to halve the amount of computation in the feature extraction part compared to FE2E, while maintaining its performance. In our case study, we provide a visualization of the crossmodal attention maps on both visual and acoustic data. It shows that our method can be interpretable, and the cross-modal attention can successfully select important feature points based on different emotion categories. For future work, we believe that incorporating more modalities into the sparse cross-modal attention mechanism is worth exploring since it could potentially enhance the robustness of the sparsity (selection of features).",
      "page_start": 12,
      "page_end": 12
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: An illustration of feature extraction from",
      "page": 1
    },
    {
      "caption": "Figure 2: Architecture of our Multimodal End-to-end Sparse Model (MESM). On the left, we show the general",
      "page": 4
    },
    {
      "caption": "Figure 2: shows the overall architecture of",
      "page": 4
    },
    {
      "caption": "Figure 1: ), which makes",
      "page": 5
    },
    {
      "caption": "Figure 3: Case study of MESM on six basic emotion categories (happy, sad, angry, surprised, fear, disgusted).",
      "page": 7
    },
    {
      "caption": "Figure 4: Visualization of cross-modal attention of the",
      "page": 7
    },
    {
      "caption": "Figure 3: , in general, the models attend to several",
      "page": 7
    },
    {
      "caption": "Figure 3: , the MESM can cap-",
      "page": 7
    },
    {
      "caption": "Figure 4: The model attends to",
      "page": 7
    },
    {
      "caption": "Figure 5: The trend line of the Top: Weighted Accu-",
      "page": 8
    },
    {
      "caption": "Figure 5: , empirically, the amount of computation is",
      "page": 8
    },
    {
      "caption": "Figure 6: Case study of the sparse cross-modal attention maps on six basic emotion categories (happy, sad, angry,",
      "page": 12
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "MFCC": "Prosody"
        },
        {
          "MFCC": "Glottal Source"
        },
        {
          "MFCC": "..."
        }
      ],
      "page": 1
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "A study on character's emotional appearance in distinction focused on 3d animation \"inside out",
      "authors": [
        "Duck-Ki Ahn",
        "Jean-Hun Chung"
      ],
      "year": "2017",
      "venue": "Journal of Digital Convergence",
      "doi": "10.14400/JDC.2017.15.2.361"
    },
    {
      "citation_id": "2",
      "title": "Multi-task learning for multi-modal emotion recognition and sentiment analysis",
      "year": "2019",
      "venue": "Multi-task learning for multi-modal emotion recognition and sentiment analysis",
      "arxiv": "arXiv:1905.05812"
    },
    {
      "citation_id": "3",
      "title": "Multimodal machine learning: A survey and taxonomy",
      "authors": [
        "Tadas Baltrušaitis",
        "Chaitanya Ahuja",
        "Louis-Philippe Morency"
      ],
      "year": "2018",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "4",
      "title": "Openface 2.0: Facial behavior analysis toolkit",
      "authors": [
        "Tadas Baltrusaitis",
        "Amir Zadeh",
        "Chong Lim",
        "Louis-Philippe Morency"
      ],
      "year": "2018",
      "venue": "2018 13th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2018)"
    },
    {
      "citation_id": "5",
      "title": "Cross-dataset learning and person-specific normalisation for automatic action unit detection",
      "authors": [
        "T Baltrušaitis",
        "M Mahmoud",
        "P Robinson"
      ],
      "year": "2015",
      "venue": "2015 11th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG)",
      "doi": "10.1109/FG.2015.7284869"
    },
    {
      "citation_id": "6",
      "title": "Emotional facial expression based on action units and facial muscle",
      "authors": [
        "Ahmad Hoirul"
      ],
      "year": "2016",
      "venue": "International Journal of Electrical and Computer Engineering (IJECE)",
      "doi": "10.11591/ijece.v6i5.12135"
    },
    {
      "citation_id": "7",
      "title": "Iemocap: Interactive emotional dyadic motion capture database. Language resources and evaluation",
      "authors": [
        "Carlos Busso",
        "Murtaza Bulut",
        "Chi-Chun Lee",
        "Abe Kazemzadeh",
        "Emily Mower",
        "Samuel Kim",
        "Jeannette Chang",
        "Sungbok Lee",
        "Shrikanth S Narayanan"
      ],
      "year": "2008",
      "venue": "Iemocap: Interactive emotional dyadic motion capture database. Language resources and evaluation"
    },
    {
      "citation_id": "8",
      "title": "Sca-cnn: Spatial and channel-wise attention in convolutional networks for image captioning",
      "authors": [
        "Long Chen",
        "Hanwang Zhang",
        "Jun Xiao",
        "L Nie",
        "Jian Shao",
        "Wei Liu",
        "Tat-Seng Chua"
      ],
      "year": "2017",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "9",
      "title": "Modality-transferable emotion embeddings for low-resource multimodal emotion recognition",
      "authors": [
        "Wenliang Dai",
        "Zihan Liu",
        "Tiezheng Yu",
        "Pascale Fung"
      ],
      "year": "2020",
      "venue": "Modality-transferable emotion embeddings for low-resource multimodal emotion recognition"
    },
    {
      "citation_id": "10",
      "title": "Kungfupanda at semeval-2020 task 12: Bert-based multi-task learning for offensive language detection",
      "authors": [
        "Wenliang Dai",
        "Tiezheng Yu",
        "Zihan Liu",
        "Pascale Fung"
      ],
      "year": "2020",
      "venue": "Kungfupanda at semeval-2020 task 12: Bert-based multi-task learning for offensive language detection"
    },
    {
      "citation_id": "11",
      "title": "Facial signs of emotional experience",
      "authors": [
        "Paul Ekman",
        "Sonia Wallace V Freisen",
        "Ancoli"
      ],
      "year": "1980",
      "venue": "Journal of personality and social psychology"
    },
    {
      "citation_id": "12",
      "title": "What the face reveals: Basic and applied studies of spontaneous expression using the Facial Action Coding System (FACS)",
      "authors": [
        "Rosenberg Ekman"
      ],
      "year": "1997",
      "venue": "What the face reveals: Basic and applied studies of spontaneous expression using the Facial Action Coding System (FACS)"
    },
    {
      "citation_id": "13",
      "title": "Submanifold sparse convolutional networks",
      "authors": [
        "Benjamin Graham",
        "Laurens Van Der Maaten"
      ],
      "year": "2017",
      "venue": "Submanifold sparse convolutional networks",
      "arxiv": "arXiv:1706.01307"
    },
    {
      "citation_id": "14",
      "title": "The curious case of neural text degeneration",
      "authors": [
        "Ari Holtzman",
        "Jan Buys",
        "Li Du",
        "Maxwell Forbes",
        "Yejin Choi"
      ],
      "year": "2019",
      "venue": "The curious case of neural text degeneration",
      "arxiv": "arXiv:1904.09751"
    },
    {
      "citation_id": "15",
      "title": "Adam: A method for stochastic optimization",
      "authors": [
        "P Diederik",
        "Jimmy Kingma",
        "Ba"
      ],
      "year": "2014",
      "venue": "Adam: A method for stochastic optimization",
      "arxiv": "arXiv:1412.6980"
    },
    {
      "citation_id": "16",
      "title": "Visualbert: A simple and performant baseline for vision and language",
      "authors": [
        "Liunian Harold",
        "Mark Yatskar",
        "C Da Yin",
        "Kai-Wei Hsieh",
        "Chang"
      ],
      "year": "2019",
      "venue": "Visualbert: A simple and performant baseline for vision and language"
    },
    {
      "citation_id": "17",
      "title": "Automatic speech emotion recognition using recurrent neural networks with local attention",
      "authors": [
        "E Seyedmahdad Mirsamadi",
        "C Barsoum",
        "Zhang"
      ],
      "year": "2017",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "18",
      "title": "Towards multimodal sentiment analysis: Harvesting opinions from the web",
      "authors": [
        "Louis-Philippe Morency",
        "Rada Mihalcea",
        "Payal Doshi"
      ],
      "year": "2011",
      "venue": "Proceedings of the 13th international conference on multimodal interfaces"
    },
    {
      "citation_id": "19",
      "title": "Deep multimodal fusion for persuasiveness prediction",
      "authors": [
        "Behnaz Nojavanasghari",
        "Deepak Gopinath",
        "Jayanth Koushik",
        "Tadas Baltrušaitis",
        "Louis-Philippe Morency"
      ],
      "year": "2016",
      "venue": "Proceedings of the 18th ACM International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "20",
      "title": "Pytorch: An imperative style, high-performance deep learning library",
      "authors": [
        "Adam Paszke",
        "Sam Gross",
        "Francisco Massa",
        "Adam Lerer",
        "James Bradbury",
        "Gregory Chanan",
        "Trevor Killeen",
        "Zeming Lin",
        "Natalia Gimelshein",
        "Luca Antiga"
      ],
      "year": "2019",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "21",
      "title": "Glove: Global vectors for word representation",
      "authors": [
        "Jeffrey Pennington",
        "Richard Socher",
        "Christopher Manning"
      ],
      "year": "2014",
      "venue": "Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)"
    },
    {
      "citation_id": "22",
      "title": "Utterance-level multimodal sentiment analysis",
      "authors": [
        "Verónica Pérez-Rosas",
        "Rada Mihalcea",
        "Louis-Philippe Morency"
      ],
      "year": "2013",
      "venue": "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "23",
      "title": "Integrating multimodal information in large pretrained transformers",
      "authors": [
        "Wasifur Rahman",
        "Md Kamrul Hasan",
        "Sangwu Lee",
        "Ali Zadeh",
        "Chengfeng Amir",
        "Louis-Philippe Mao",
        "Ehsan Morency",
        "Hoque"
      ],
      "year": "2020",
      "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "24",
      "title": "Very deep convolutional networks for large-scale image recognition",
      "authors": [
        "Karen Simonyan",
        "Andrew Zisserman"
      ],
      "year": "2014",
      "venue": "Very deep convolutional networks for large-scale image recognition",
      "arxiv": "arXiv:1409.1556"
    },
    {
      "citation_id": "25",
      "title": "Combating human trafficking with multimodal deep models",
      "authors": [
        "Edmund Tong",
        "Amir Zadeh",
        "Cara Jones",
        "Louis-Philippe Morency"
      ],
      "year": "2017",
      "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "26",
      "title": "Multimodal transformer for unaligned multimodal language sequences",
      "authors": [
        "Yao-Hung Hubert Tsai",
        "Shaojie Bai",
        "Paul Liang",
        "J Zico Kolter",
        "Louis-Philippe Morency",
        "Ruslan Salakhutdinov"
      ],
      "year": "2019",
      "venue": "Proceedings of the conference. Association for Computational Linguistics. Meeting"
    },
    {
      "citation_id": "27",
      "title": "Learning factorized multimodal representations",
      "authors": [
        "Yao-Hung Hubert Tsai",
        "Paul Liang",
        "Amir Zadeh",
        "Louis-Philippe Morency",
        "Ruslan Salakhutdinov"
      ],
      "year": "2018",
      "venue": "Learning factorized multimodal representations",
      "arxiv": "arXiv:1806.06176"
    },
    {
      "citation_id": "28",
      "title": "Phonet: A tool based on gated recurrent neural networks to extract phonological posteriors from speech",
      "authors": [
        "J Vásquez-Correa",
        "P Klumpp",
        "J Orozco-Arroyave",
        "E Nöth"
      ],
      "year": "2019",
      "venue": "Phonet: A tool based on gated recurrent neural networks to extract phonological posteriors from speech"
    },
    {
      "citation_id": "29",
      "title": "Towards an automatic evaluation of the dysarthria level of patients with parkinson's disease",
      "authors": [
        "Juan Camilo Vásquez-Correa",
        "J Orozco-Arroyave",
        "T Bocklet",
        "E Nöth"
      ],
      "year": "2018",
      "venue": "Journal of communication disorders"
    },
    {
      "citation_id": "30",
      "title": "Attention is all you need",
      "authors": [
        "Ashish Vaswani",
        "Noam Shazeer",
        "Niki Parmar",
        "Jakob Uszkoreit",
        "Llion Jones",
        "Aidan Gomez",
        "Łukasz Kaiser",
        "Illia Polosukhin"
      ],
      "year": "2017",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "31",
      "title": "Select-additive learning: Improving generalization in multimodal sentiment analysis",
      "authors": [
        "Haohan Wang",
        "Aaksha Meghawat",
        "Louis-Philippe Morency",
        "Eric Xing"
      ],
      "year": "2017",
      "venue": "2017 IEEE International Conference on Multimedia and Expo (ICME)"
    },
    {
      "citation_id": "32",
      "title": "Words can shift: Dynamically adjusting word representations using nonverbal behaviors",
      "authors": [
        "Yansen Wang",
        "Ying Shen",
        "Zhun Liu",
        "Paul Liang",
        "Amir Zadeh",
        "Louis-Philippe Morency"
      ],
      "year": "2019",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "33",
      "title": "Emograph: Capturing emotion correlations using graph networks",
      "authors": [
        "Peng Xu",
        "Zihan Liu",
        "Genta Indra Winata",
        "Zhaojiang Lin",
        "Pascale Fung"
      ],
      "year": "2020",
      "venue": "Emograph: Capturing emotion correlations using graph networks"
    },
    {
      "citation_id": "34",
      "title": "Deep modular co-attention networks for visual question answering",
      "authors": [
        "J Zhou Yu",
        "Yuhao Yu",
        "D Cui",
        "Qi Tao",
        "Tian"
      ],
      "year": "2019",
      "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "35",
      "title": "Tensor fusion network for multimodal sentiment analysis",
      "authors": [
        "Amir Zadeh",
        "Minghai Chen",
        "Soujanya Poria",
        "Erik Cambria",
        "Louis-Philippe Morency"
      ],
      "year": "2017",
      "venue": "Tensor fusion network for multimodal sentiment analysis",
      "arxiv": "arXiv:1707.07250"
    },
    {
      "citation_id": "36",
      "title": "Soujanya Poria, Erik Cambria, and Louis-Philippe Morency. 2018a. Memory fusion network for multi-view sequential learning",
      "authors": [
        "Amir Zadeh",
        "Paul Liang",
        "Navonil Mazumder"
      ],
      "venue": "Soujanya Poria, Erik Cambria, and Louis-Philippe Morency. 2018a. Memory fusion network for multi-view sequential learning",
      "arxiv": "arXiv:1802.00927"
    },
    {
      "citation_id": "37",
      "title": "Mosi: multimodal corpus of sentiment intensity and subjectivity analysis in online opinion videos",
      "authors": [
        "Amir Zadeh",
        "Rowan Zellers",
        "Eli Pincus",
        "Louis-Philippe Morency"
      ],
      "year": "2016",
      "venue": "Mosi: multimodal corpus of sentiment intensity and subjectivity analysis in online opinion videos",
      "arxiv": "arXiv:1606.06259"
    },
    {
      "citation_id": "38",
      "title": "Multimodal language analysis in the wild: Cmu-mosei dataset and interpretable dynamic fusion graph",
      "authors": [
        "Amirali Bagher Zadeh",
        "Paul Liang",
        "Soujanya Poria",
        "Erik Cambria",
        "Louis-Philippe Morency"
      ],
      "year": "2018",
      "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "39",
      "title": "Joint face detection and alignment using multitask cascaded convolutional networks",
      "authors": [
        "Kaipeng Zhang",
        "Zhanpeng Zhang",
        "Zhifeng Li",
        "Yu Qiao"
      ],
      "year": "2016",
      "venue": "IEEE Signal Processing Letters"
    },
    {
      "citation_id": "40",
      "title": "Sentiment analysis and opinion mining",
      "authors": [
        "L Zhang",
        "B Liu"
      ],
      "year": "2017",
      "venue": "Encyclopedia of Machine Learning and Data Mining"
    }
  ]
}