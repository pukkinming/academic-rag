{
  "paper_id": "2402.10921v1",
  "title": "Am 2 -Emoje:Adaptive Missing-Modality Emotion Recognition In Conversation Via Joint Embedding Learning",
  "published": "2024-01-26T19:57:26Z",
  "authors": [
    "Naresh Kumar Devulapally",
    "Sidharth Anand",
    "Sreyasee Das Bhattacharjee",
    "Junsong Yuan"
  ],
  "keywords": [
    "multimodal emotion recognition",
    "joint embedding learning",
    "adaptive fusion",
    "missing modality"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Human emotion can be presented in different modes i.e., audio, video, and text. However, the contribution of each mode in exhibiting each emotion is not uniform. Furthermore, the availability of complete mode-specific details may not always be guaranteed in the test time. In this work, we propose AM 2 -EmoJE, a model for Adaptive Missing-Modality Emotion Recognition in Conversation via Joint Embedding Learning model that is grounded on two-fold contributions: First, a query adaptive fusion that can automatically learn the relative importance of its mode-specific representations in a query-specific manner. By this the model aims to prioritize the mode-invariant spatial (within utterance) query details of the emotion patterns, while also retaining its mode-exclusive aspects within the learned multimodal query descriptor. Second the multimodal joint embedding learning module that explicitly addresses various missing modality scenarios in test-time. By this, the model learns to emphasize on the correlated patterns across modalities, which may help align the cross-attended mode-specific descriptors pairwise within a joint-embedding space and thereby compensate for missing modalities during inference. By leveraging the spatiotemporal details at the dialogue level, the proposed AM 2 -EmoJE not only demonstrates superior performance (around 2 -4% improvement in Weighted-F1 scores) compared to the best-performing state-of-the-art multimodal methods, by effectively leveraging body language in place of face expression, it also exhibits an enhanced privacy feature. By reporting around 2-5% improvement in the weighted-F1 score, the proposed multimodal joint embedding module facilitates an impressive performance gain in a variety of missing-modality query scenarios during test time.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Introduction",
      "text": "Emotion analysis and its dynamic evolution in humans during conversations is pivotal to various critical tasks ranging from sentiment analysis in social media to affect-aware humanrobot interactions. The problem is particularly complex due to the involvement of multi-party stakeholders, their mutual interactions described via different modalities like text, video, and audio, as well as their body language, which influence Fig.  1 . Proposed AM 2 -EmoJE framework the speaker's emotions and its spatio-temporal progression expressed in an utterance.\n\nIn particular, a speaker's emotion depends on various intra (e.g. personal details, behavioral patterns, and habits) and inter (e.g. audience behavior, their interpreting conducts) personal contexts and other environmental circumstances. At one end, most multimodal methods assume data completeness, which may not hold in practice due to privacy, device, or security constraints. We note that the mode-specific descriptor components exhibiting an individual's emotion have some correlation, which may be instrumental to handling the missing modality information. However, this aspect has not yet been sufficiently explored by the existing literature. At the other end, the significance of this variety of heterogeneous mode information may not be uniform in evaluating emotions for all speakers. For example, a speaker's facial expression may have only limited information about their true emotion, while their body language may still be more semantically rich. Thus the challenges related to the presence of strong heterogeneity both within and across multiple modalities and complex interplay and the influence of a missing modality information toward learning comprehensive multimodal interaction patterns in a real-life conversation setting are critical yet under-studied.\n\nToward these, we propose AM 2 -EmoJE, an Adaptive",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Missing-Modality Emotion Recognition In Conversation Via",
      "text": "Joint Embedding Learning model that is grounded on the following contributions. 1. Query Adaptive Fusion (QAF) that derives the relative importance of the cross-attended mode descriptors on the fly to deliver a robust multimodal query descriptor preserving both instance-specific and category-specific utterance-level spatial patterns in parallel. 2. Multimodal joint embedding that may leverage an auxiliary boolean mask vector as user input to turn on an effective mode switching mechanism, which proves instrumental in delivering a competitive decision ability for queries with incomplete mode information. 3. Extensive Evaluation Analysis using publicly available (MELD  [1] , IEMOCAP  [2] ) datasets not only demonstrate an impressive classification performance (2 -4% improvement in Weighted-F1 of AM 2 -EmoJE in complete multimodal test scenarios, but also exhibit a competitive decision making for queries with missing modality information.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Related Work",
      "text": "Multimodal solutions toward motion Recognition in Conversations (ERC) have recently demonstrated significant performance improvements compared to their unimodal counterparts. While natural language transcriptions serve as strong emotion indicators for unimodal ER pipelines, leveraging the advantages of cross-modal interactions demonstrates significant performance gain  [3, 4, 5, 6] .\n\nWhile exhibiting promising performances, traditional multimodal methods assume data completeness, which may not be a feasible constraint for various practice issues related to privacy, device, or security constraints and missing modality in a test environment appears to be challenging to these models. In a recent work, Ma et al.  [7]  investigate the behavior of Transformer-based multimodal frameworks in the presence of modal-incomplete data. Lee et al.  [8]  propose a prompt learning framework to address the issues of missing modalities during training or testing. However, the limitation of computational resources still persists as a bottleneck to such training-heavy transformer models.\n\nWhile a few recent works  [9, 10, 11, 12]  attempt to address the missing modality challenge in specific application settings, a straightforward extension to these preliminary models for the complex task of ERC, may not be feasible. Evaluating the relative contribution of multiple modespecific components describing a speaker's emotion dynamics on the fly, while estimating their missing-mode description in parallel, is challenging. Toward these, we propose AM 2 -EmoJE that is motivated by two-fold research objectives:First, a query adaptive fusion that can automatically learn the relative importance of its mode-specific representations in a query-specific manner; Second multimodal joint embedding learning framework that learns a mode-switching mechanism to align the cross-attended mode-specific descriptors pairwise within a joint-embedding space and thereby allowing the model to compensate for missing modalities during inference.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Proposed Method",
      "text": "Problem Definition: Given a multi-party dialogue d := {u j } j ∈ D represented as a sequence of utterances {u j } j , the objective is to evaluate the dominant emotional states of the speaker as expressed in each utterance u j . For brevity, now onward we will omit the suffix j, and an arbitrary utterance u j will be represented as u unless the suffix is specifically required. Each u ∈ D contains (u v , u a , u t ), where u v is the video, u a is the audio, and u t is the text transcription component of the utterance. In this work, we propose a continual emotion evaluation model that can evaluate the series of dialogues by single or multiple speakers in a conversational environment. Unlike existing multimodal literature, which primarily relies on the availability of complete modespecific components representing a multimode utterance, in this work we allow incomplete query videos, i.e., some of the mode-specific information related to the query may be missing. Therefore, the availability information of each modespecific query component is passed by a boolean mask vector w := (w v , w a , w t ). For example, a query that only has an audio and transcript will is passed with a mask vector w with w v = 0. In another query environment, if a user is only interested in analyzing the visual component of a multimode query, then the mask vector w with w a = 0 and w t = 0 is passed with the query that may enable the system to perform only the visual analysis.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Mode-Specific Feature Representation",
      "text": "To capture the spatio-temporal evolution of information within each utterance, the first-level mode-specific feature representation scheme as described below:",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Text Representation",
      "text": "To derive a compact descriptor for the text component u t represented as a sequence of w words, i.e. u t = {ω 1 , ω 2 , ..., ω w }, we employ the pretrained model SBERT  [13]  to obtain the fixed language embedding f T ∈ R w×dt for the text component u t .",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Video Representation",
      "text": "For the visual component u v of each utterance u ∈ d, FFmpeg is used to identify n key frames and MTCNN [?] is applied to extract the aligned faces from each key frame. To deliver a comprehensive analysis of the subjects' emotional evolution to appear in an utterance, we analyze their facial expression in contrast to their body language, each u v is split into two sequences, which cover the visuals of distinct regions in the utterance: 'face sequence' that contains the sequence of derived frames containing only the subjects' face regions; 'body sequence' that exclusively contains the subjects' body segment in the utterance. YOLOv7  [14]  fine-tuned for human detection, is employed to localize the human body segment in a frame, To analyze the body language in exclusion, we subtract the face region from the identified body segment in a frame to design the derived frames in the 'body sequence'. Thus, the visual content u v of u is represented in terms of two equalsized derived frame sequences: v f ace = {au 1 , au 2 , ..., au n } and v body = {b 1 , b 2 , ..., b n }, where each au j and b j represent a learned descriptor describing the j th element in v f ace and v body respectively. Two identical Bi-LSTM-based sequence representation modules with a hidden embedding size of d v , which take v f ace or v body as inputs, are employed to obtain the initial regional descriptors f f ace ∈ R n×dv or f body ∈ R n×dv . These two independent LSTM networks are then merged via a stacked self-attention layer that attends to the pair of these network-specific inputs to derive a selfattended visual descriptor f v ∈ R 2n×dv for u v .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Audio Representation",
      "text": "We use the Pathout Fast 2D Spectrogram model (PASST)  [15] , which is initialized from a vision transformer trained on ImageNet, to fine-tuned on 10s audio clips from Au-dioSet. To derive a fine-grained understanding of the audio signal evolution, the audio component of each utterance is divided into smaller e overlapped segments and each segment is represented using their corresponding PASST descriptor a i ∈ R dpasst such that u a :== {a 1 , a 2 , ..., a e }. Similar to the video representation, we use a BiLSTM to derive an overall audio feature for the utterance f a ∈ R da .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Weighted Multimodal Attention",
      "text": "Toward integrating the mode-specific contents across modalities, we note that the expression captured by these uni-mode components may not solely reflect their intra-mode data pattern. They may also represent the influence of information depicted by other modalities. For example, a speaker's audio may reflect certain emotions, which may not be sufficiently supported by their body language or an utterance transcript may be continually modulated by the expressions shown by the audience in the previous utterance segments. Furthermore, the availability of a quality mode-specific representation of an utterance may also not be guaranteed. For instance, the availability of visual cues may not be assured in every query scenario. Otherwise, noisy audio may also deteriorate the overall data quality occasionally.\n\nWhile existing multimodal emotion analysis models do not allow such incomplete query information, we design an Adaptive Cross-attended Network (ACN) that enables a flexible yet robust feature representation technique to capture the effects of same-and different-modality cues at various levels of details (including missing modality cues). As observed in Figure  1 , each layer of ACN is administered by multiple cross-modal cues from multiple outer networks into a single mode-specific inner network. The merging layer of ACN incorporates a simple yet effective masking mechanism toward facilitating the learning of several variants of mode combinations, wherein the model is explicitly trained to control the contribution of each 'outer' network including a variety of missing-modality scenarios. The keys and values are generated as K l mi = linear(w mi (W l,K mi ) T e mi ), V l mi = linear(w mi (W l,V mi ) T e mi ), where W l,K ∈ R dm i ×d l and W l,V ∈ R dm i ×d l are key and value weight matrices in the l th layer of the attention network. The output of each attention head in the l th layer is then computed as:\n\nThus, for each modality m in M, the set of all modalities leveraged in the model, the learned descriptor g m from multiple heads are mean-pooled to capture the weighted aggregated details within the final output of the Central network.\n\nAs shown in Figure  1 , each mode-specific Central query network for each m ∈ M of ACN thus produces an average pooled cross-attended mode-specific descriptor f m ACN ∈ R d for the uni-mode components t, v, and a for u. We will discuss the learning algorithm later in Section 3.5.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Query Adaptive Fusion",
      "text": "In contrast to the existing fusion techniques, which assume that the mode-specific contributions in the resulting multimodal descriptor for a query input should be uniform, in reality, the quality and availability of the complete mode-specific components may not be feasible in general. Therefore, we propose a Query Adaptive Fusion (QAF) mechanism that designs a linear combination of the learned cross-attended mode descriptors g m to define a comprehensive multimode utterance-level descriptor as follows:\n\nwhere 0 ≤ α m mi ∀m, m i ∈ M ≤ 1 are learnable parameters. Thus, the proposed fusion function A provides a flexible multimodal representation mechanism, by which the resulting multimodal descriptor A(u) for an utterance u can retain category-specific discriminative data patterns, however not completely disregarding the unique instance-specific data patterns observed in the utterance. Furthermore, given an utterance u, in the absence of its complete mode-specific details (as provided by the user-specified mask vector w) related to m, the model automatically learns to adopt its learnable parameters {α m mi } mi to derive an adjusted multimodal descriptor A(u). We adopt the optimization approach of  [16]  to learn the interpolation parameters α m mi .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Classification",
      "text": "Given a conversational dialogue represented using a sequence of n utterances {u j } n j ∈ D, the emotion of the speaker s is estimated by leveraging two parallel utterance sequences: Dialogue Context that describes the sequence {A(u j )} j ; Speaker Context that describes a sub-sequence {A(u sj )} j , where the sub-sequence {u sj } sj ∈  [1,n]  is generated from the dialogue and includes only those utterances, in which s vocally contributes to the conversation. Two parallel Bi-LSTMs are trained to capture the spatio-temporal contexts independently of these contexts' perspectives: s l ∈ R s representing the Speaker Context and d l ∈ R s representing the Dialogue Context. These representations are passed through a classification head to arrive at the classification decision for the emotion for the utterance u.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Training Objectives",
      "text": "The learning of AM 2 -EmoJE includes two independent learning objectives: a multi-component loss objective (L) that combinedly optimizes the guided NCE loss (L ACE ) and the focal loss (L f l ) for the classifier; a CLIP-like loss function L joint to learn the pairwise joint embedding spaces. We discuss them below.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Guided Nce",
      "text": "The proposed Adaptive Cross-attended Network (ACN) jointly learns the cross-attended representations f t ACN,j , f v ACN,j , and f a ACN,j with twofold contributions: 1) preserving instance specific discriminability by identifying more reliable modes on the fly. We intuitively expect a speaker's face and body language to display similar emotions, and a significant deviation from this would require us to rely more on the other modalities like audio or transcript to accurately estimate their true emotions. Guided by this observation, we leverage an aggregated noise contrastive estimation (L ACE ) as below:\n\nthat computes the probability of both features f m ACN,j and f m i ACN,j representing the same instance u j compared to other elements in a uniformly sampled negative set N j . N is the sample batch. The averaged Focal Loss L f l  [17] , specifically effective for an imbalanced dataset like ours, is employed to preserve the category details and a combined loss function L = L ACE +L f l to jointly learn its mode-specific Central query networks.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Multimodal Joint Embedding",
      "text": "To address the potential missing modality scenarios in a test environment, we introduce an effective joint latent representation learning model that follows a similar approach as proposed by Radford et al.  [18] : the latent descriptor for a positive pair (i.e. a pair of mode-specific representatives describing the same sample using two different modes) are mapped closely, while a negative pair (i.e. a pair of mode-specific representatives describing the two different samples using two different modes) samples will be mapped father from each other in the joint embedding space. Formally, given a minibatch of N of samples represented using their cross-attended descriptors in a twomode space (i.e. N := {(f\n\n, we use Jensen-Shannon Divergence, which is a symmetric version of the KL-Divergence to learn the pair-wise invertible linear mappings Sm k →m joint : R dm k → R d joint , Table  1 . Performance Comparison of different methods using the weighted average F1 measure (W-Avg F1) on the MELD dataset with uni modal (T-Text, A-Audio, and V-Video) and multi-modal representation. Due to the imbalanced class distribution of the dataset, the 'Fear' and 'disgust' classes are represented as the minority classes, the proposed method was also compared against other 5 majority classes ('Neutral', 'Surprise', 'Sadness', 'Joy', and 'Anger') in the dataset and the results are reported in column 'w-avg F1 5 CLS'. More details on emotion-specific comparison are provided in the supplementary material. T + A 0.6073 -EmoCaps  [24]  T + A + V 0.6400 -M2FNet  [3]  T + A + V 0.6785 -Cross-Modal Distribution Matching  [25]  T + A 0.571 -Transformer Based Crossmodality Fusion  [26]  T + A + V 0.64 -Hierarchical Uncertainty  [27]  T + A + V 0.59 -Shape of Emotion  [28]  T + A + V 0.63 -UniMSE  [29]  T + A + V 0.66 -EmotionCLIP  [30]  T + A + V 0.3459 - ∀m k ∈ M to a single joint embedding space D joint ⊂ R d joint and the corresponding loss function is defined as:\n\nwhere x := (f\n\n. The model jointly optimizes a single loss function\n\nLm k →m joint to learn the collection of invertible mapping functions: {Sm k →m joint } m,k∈M .",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Experiments 4.1. Datasets",
      "text": "MELD  [1]  is a multimodal group conversational dataset that sources clips from the TV Series FRIENDS and consists of the 6 basic emotions. IEMO-CAP  [24]  is a dual-party conversational dataset acted out by professional actors and features conversations that are far longer on average than those in MELD.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Results",
      "text": "Tables  1  and 2  report the comparative performance of the proposed method against several state-of-the-art methods  [20, 19, 21, 23, 24, 3, 25, 31, 26, 27, 28, 29]  in the MELD and IEMOCAP datasets, respectively. While text appears as the most reliable unimodal feature, combining multiple features has been undoubtedly helpful to deliver a competitive 71.98% weighted F1score (w-avg F1), which is an improvement of ≈ 4% across all classes, as compared to the best-performing baseline M2FNet  [3] . MELD is an unbalanced dataset that has 'fear' and 'disgust' identified as the minority classes, with less than 50 samples as representatives. AM 2 -EmoJE reports 82.05% w-avg F1, which is a significant 20% improvement in the w-avg F1 score compared to the best-performing baseline using the five majority classes. Compared to the existing works, which offer equal emphasis on all modes, adaptively identifying the more reliable modes to design the relative weight assignments in a query-specific manner has been proven to be effective for delivering a more accurate estimate of a speaker's emotional state. Furthermore, from the performance reported in the sub-row described using the mode 'T+A+V(no face)' (that uses cues from audio, transcript, and visual capturing subjects' body regions only), we observe that the proposed AM 2 -EmoJE still remains competitive (i.e., reporting ≈ 2% improved w-avg F1 score) against the best-performing baseline. In fact, as we compare the result reported in another sub-row described using mode 'T+A+V(no body)' (that uses cues from audio, transcript, and visual capturing subjects' face regions only), we observe that AM 2 -EmoJE's performance remains nearly equivalent as in two different multi-mode data scenarios. Finally, to evaluate the contribution of the proposed multimodal joint learning (Section 3.5.2), in the table we also report the performance of the proposed AM 2 -EmoJE in multiple missing modality scenarios. For example, comparing the performance between the sub-row described using mode 'T+A' (that uses only text and audio) and 'T+A(JE)' (that uses only text and audio and uses multimodal joint compensate for the visual modality) we note that the proposed multimodal joint learning module enables around 5% (and 2%) gain in the w-avg F1 score overall (and five majority) classes. A similar observation can also be made by comparing the subrows 'A+V' and 'A+V(JE)' (and subrows 'T+V' and 'T+V(JE)' ) which reports around 8% (and 7%) performance gain in the w-avg F1 score. Augmented with the proposed multimodal joint learning module, AM 2 -EmoJE attains a state-of-theart performance in two-mode data scenarios, where the queries are presented with a variety of missing modality scenarios (e.g. 'video' or 'audio').\n\nA similar performance was also observed in Table  2  which reports the comparative analysis using the IEMOCAP dataset. The proposed AM 2 -EmoJE reports 74.91% w-avg F1, which is around 4.9% improvement compared to the best-performing baseline LIGHT-SERNET  [32] . Being equipped with the proposed multimodal joint learning module, the model again demonstrates a competitive performance in two-mode data scenarios, where the queries are represented using 'text and audio' or 'text and video'.\n\nTable  3  reports an ablation study. The first two rows in the Table demonstrate the superiority of the weighted multimodal attention (Section 3.2) and weighted fusion techniques (Section 3.3) compared to vanilla cross-attention and static weights (where we use equal weight for all modes). The subrows in the third row report the performance of the proposed AM 2 -EmoJE where only classification focal loss L f l is used, but we did not use L ACE in Eqn (3). As observed in the table, while the introduction of the Guided NCE (Section 3.5.1) with focal loss delivers around 1 -2% improvement in w-F1 score, by enabling a query adaptive multimodal fusion scheme (Section 3.3) AM 2 -EmoJE delivers a robust (2 -4% improved) the w-F1 score.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Conclusion",
      "text": "We present AM 2 -EMOJE with our weighted multimodal attention and query adaptive fusion, allowing us to effectively combine the information from various modes to make better decision about the emotional state of subjects in a group conversation. The model is also trained with our proposed Guided NCE loss that allows the model to learn representation of the subjects with only their facial features or body language, allowing us to better preserve the privacy of participants and still achieve performance that is very close to the state-of-the-art. Furthermore, we also propose an effective Multimodal Joint-Embedding scheme that allows the model to effectively compensate for missing modalities during inference, allowing its performance on a subset of modalities to be close to its performance on the full set of modalities as shown in our comparison with other state-of-the-art models and in the ablation studies in the absence of Joint-Embedding.\n\nTable  2 . Performance comparison of difference methods using the weighted average F1 measure (W-Avg F1) on the IEMOCAP dataset with uni (T:=Text, A:=Audio, and V:= Video) and multi-modal Data Representations. 'Feature Concat' in row 13 and row 14 describe the concatenation of multiple uni-mode descriptors to define a multimodal descriptor. More details on emotion-specific comparison are provided in the supplementary material",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Method",
      "text": "Mode w-Avg F1 MFN  [19]  T + A 0.3490 ICON  [20]  T + A + V 0.6350 DialogueRNN  [21]  T + A + V 0.6275 MMGCN  [33]  T + A + V 0.6622 DialogueCRN  [23]  T + A 0.6620 Hierarchical Uncertainty  [27]  T + A + V 0.6598 DAG-ERC+HCL  [31]  T 0.6803 M2FNet  [3]  T + A + V 0.6986 LIGHT-SERNET  [32]  T + A + V 0.7020",
      "page_start": 6,
      "page_end": 6
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Proposed AM2-EmoJE framework",
      "page": 1
    },
    {
      "caption": "Figure 1: , each layer of ACN is administered by",
      "page": 3
    },
    {
      "caption": "Figure 1: , each mode-specific Central query",
      "page": 3
    }
  ],
  "tables": [
    {
      "caption": "Table 1: PerformanceComparisonofdifferentmethodsusingthe",
      "data": [
        {
          "Method": "MFN [19]",
          "Mode": "T + A",
          "w-Avg F1": "0.547",
          "w-avg F1\n5-CLS": "0.5732"
        },
        {
          "Method": "ICON [20]",
          "Mode": "T\nA\nT + A",
          "w-Avg F1": "0.546\n0.377\n0.563",
          "w-avg F1\n5-CLS": "0.5718\n0.3947\n0.5897"
        },
        {
          "Method": "DialogueRNN [21]",
          "Mode": "T\nA\nT + A",
          "w-Avg F1": "0.551\n0.34\n0.57",
          "w-avg F1\n5-CLS": "0.5759\n0.3542\n0.5971"
        },
        {
          "Method": "ConGCN [22]",
          "Mode": "T\nA\nT + A",
          "w-Avg F1": "0.574\n0.422\n0.594",
          "w-avg F1\n5-CLS": "0.5969\n0.44\n0.6175"
        },
        {
          "Method": "DialogueCRN [23]",
          "Mode": "T + A",
          "w-Avg F1": "0.6073",
          "w-avg F1\n5-CLS": "-"
        },
        {
          "Method": "EmoCaps [24]",
          "Mode": "T + A + V",
          "w-Avg F1": "0.6400",
          "w-avg F1\n5-CLS": "-"
        },
        {
          "Method": "M2FNet [3]",
          "Mode": "T + A + V",
          "w-Avg F1": "0.6785",
          "w-avg F1\n5-CLS": "-"
        },
        {
          "Method": "Cross-Modal\nDistribution\nMatching [25]",
          "Mode": "T + A",
          "w-Avg F1": "0.571",
          "w-avg F1\n5-CLS": "-"
        },
        {
          "Method": "Transformer\nBased\nCross-\nmodality Fusion [26]",
          "Mode": "T + A + V",
          "w-Avg F1": "0.64",
          "w-avg F1\n5-CLS": "-"
        },
        {
          "Method": "Hierarchical Uncertainty [27]",
          "Mode": "T + A + V",
          "w-Avg F1": "0.59",
          "w-avg F1\n5-CLS": "-"
        },
        {
          "Method": "Shape of Emotion [28]",
          "Mode": "T + A + V",
          "w-Avg F1": "0.63",
          "w-avg F1\n5-CLS": "-"
        },
        {
          "Method": "UniMSE [29]",
          "Mode": "T + A + V",
          "w-Avg F1": "0.66",
          "w-avg F1\n5-CLS": "-"
        },
        {
          "Method": "EmotionCLIP [30]",
          "Mode": "T + A + V",
          "w-Avg F1": "0.3459",
          "w-avg F1\n5-CLS": "-"
        },
        {
          "Method": "AM2-EmoJE",
          "Mode": "T + A\nT + V\nA + V\nT + A (JE)\nT + V (JE)\nA + V (JE)\nNo Face\nNo Body\nT + A + V",
          "w-Avg F1": "0.6263\n0.6196\n0.5285\n0.6836\n0.6897\n0.6085\n0.6914\n0.7089\n0.7198",
          "w-avg F1\n5-CLS": "0.6845\n0.6782\n0.5825\n0.7051\n0.7106\n0.6572\n0.7944\n0.8117\n0.8205"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Method": "MFN[19]",
          "Mode": "T + A",
          "w-Avg F1": "0.3490"
        },
        {
          "Method": "ICON[20]",
          "Mode": "T + A + V",
          "w-Avg F1": "0.6350"
        },
        {
          "Method": "DialogueRNN[21]",
          "Mode": "T + A + V",
          "w-Avg F1": "0.6275"
        },
        {
          "Method": "MMGCN[33]",
          "Mode": "T + A + V",
          "w-Avg F1": "0.6622"
        },
        {
          "Method": "DialogueCRN[23]",
          "Mode": "T + A",
          "w-Avg F1": "0.6620"
        },
        {
          "Method": "Hierarchical Uncertainty\n[27]",
          "Mode": "T + A + V",
          "w-Avg F1": "0.6598"
        },
        {
          "Method": "DAG-ERC+HCL[31]",
          "Mode": "T",
          "w-Avg F1": "0.6803"
        },
        {
          "Method": "M2FNet[3]",
          "Mode": "T + A + V",
          "w-Avg F1": "0.6986"
        },
        {
          "Method": "LIGHT-SERNET[32]",
          "Mode": "T + A + V",
          "w-Avg F1": "0.7020"
        },
        {
          "Method": "AM2-EmoJE",
          "Mode": "T + A",
          "w-Avg F1": "0.6162"
        },
        {
          "Method": "",
          "Mode": "T + V",
          "w-Avg F1": "0.6343"
        },
        {
          "Method": "",
          "Mode": "A + V",
          "w-Avg F1": "0.5379"
        },
        {
          "Method": "",
          "Mode": "T + A (JE)",
          "w-Avg F1": "0.6919"
        },
        {
          "Method": "",
          "Mode": "T + V (JE)",
          "w-Avg F1": "0.7094"
        },
        {
          "Method": "",
          "Mode": "A + V (JE)",
          "w-Avg F1": "0.6580"
        },
        {
          "Method": "",
          "Mode": "No Face",
          "w-Avg F1": "0.7175"
        },
        {
          "Method": "",
          "Mode": "No Body",
          "w-Avg F1": "0.7286"
        },
        {
          "Method": "",
          "Mode": "T + A + V",
          "w-Avg F1": "0.7491"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Model": "No Multimodal Attention",
          "Modalities": "T+A+V",
          "MELD": "0.6160",
          "IEMOCAP": "0.7527"
        },
        {
          "Model": "Equal Fusion Weights",
          "Modalities": "T+A+V",
          "MELD": "0.6934",
          "IEMOCAP": "0.7165"
        },
        {
          "Model": "No Guided NCE",
          "Modalities": "No Body",
          "MELD": "0.6343",
          "IEMOCAP": "0.6491"
        },
        {
          "Model": "",
          "Modalities": "No Face",
          "MELD": "0.6021",
          "IEMOCAP": "0.6182"
        },
        {
          "Model": "",
          "Modalities": "T+A+V",
          "MELD": "0.7041",
          "IEMOCAP": "0.7286"
        },
        {
          "Model": "AM2-EmoJE",
          "Modalities": "T+A+V",
          "MELD": "0.7198",
          "IEMOCAP": "0.7491"
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Meld: A multimodal multi-party dataset for emotion recognition in conversations",
      "authors": [
        "Soujanya Poria",
        "Devamanyu Hazarika",
        "Navonil Majumder",
        "Gautam Naik",
        "Erik Cambria",
        "Rada Mihalcea"
      ],
      "year": "2019",
      "venue": "Meld: A multimodal multi-party dataset for emotion recognition in conversations"
    },
    {
      "citation_id": "3",
      "title": "Iemocap: interactive emotional dyadic motion capture database",
      "authors": [
        "Carlos Busso",
        "Murtaza Bulut",
        "Chi-Chun Lee",
        "Abe Kazemzadeh",
        "Emily Mower",
        "Samuel Kim",
        "Jeannette Chang",
        "Sungbok Lee",
        "Shrikanth Narayanan"
      ],
      "year": "2008",
      "venue": "Language Resources and Evaluation"
    },
    {
      "citation_id": "4",
      "title": "M2fnet: Multi-modal fusion network for emotion recognition in conversation",
      "authors": [
        "Purbayan Vishal Chudasama",
        "Ashish Kar",
        "Nirmesh Gudmalwar",
        "Pankaj Shah",
        "Naoyuki Wasnik",
        "Onoe"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "5",
      "title": "EmoCaps: Emotion capsule based model for conversational emotion recognition",
      "authors": [
        "Zaijing Li",
        "Fengxiao Tang",
        "Ming Zhao",
        "Yusen Zhu"
      ],
      "year": "2022",
      "venue": "Findings of the Association for Computational Linguistics: ACL 2022"
    },
    {
      "citation_id": "6",
      "title": "Efficient large-scale multi-modal classification",
      "authors": [
        "Douwe Kiela",
        "Edouard Grave",
        "Armand Joulin",
        "Tomas Mikolov"
      ],
      "year": "2018",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "7",
      "title": "Learning modality-fused representation based on transformer for emotion analysis",
      "authors": [
        "Piao Shi",
        "Min Hu",
        "Fuji Ren",
        "Xuefeng Shi",
        "Liangfeng Xu"
      ],
      "year": "2022",
      "venue": "Journal of Electronic Imaging"
    },
    {
      "citation_id": "8",
      "title": "Are multimodal transformers robust to missing modality?",
      "authors": [
        "Mengmeng Ma",
        "Jian Ren",
        "Long Zhao",
        "Davide Testuggine",
        "Xi Peng"
      ],
      "year": "2022",
      "venue": "2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "9",
      "title": "Multimodal prompting with missing modalities for visual recognition",
      "authors": [
        "Yi-Lun Lee",
        "Yi-Hsuan Tsai",
        "Wei-Chen Chiu",
        "Chen-Yu Lee"
      ],
      "year": "2023",
      "venue": "2023 IEEE/CVF Con-ference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "10",
      "title": "Modality translation-based multimodal sentiment analysis under uncertain missing modalities",
      "authors": [
        "Zhizhong Liu",
        "Bin Zhou",
        "Dianhui Chu",
        "Yuhang Sun",
        "Lingqiang Meng"
      ],
      "year": "2024",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "11",
      "title": "Tag-assisted multimodal sentiment analysis under uncertain missing modalities",
      "authors": [
        "Jiandian Zeng",
        "Tianyi Liu",
        "Jiantao Zhou"
      ],
      "year": "2022",
      "venue": "Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval"
    },
    {
      "citation_id": "12",
      "title": "M3ae: Multimodal representation learning for brain tumor segmentation with missing modalities",
      "authors": [
        "Hong Liu",
        "Dong Wei",
        "Donghuan Lu",
        "Jinghan Sun",
        "Liansheng Wang",
        "Yefeng Zheng"
      ],
      "year": "2023",
      "venue": "Proceedings of the Thirty-Seventh AAAI Conference on Artificial Intelligence and Thirty-Fifth Conference on Innovative Applications of Artificial Intelligence and Thirteenth Symposium on Educational Advances in Artificial Intelligence"
    },
    {
      "citation_id": "13",
      "title": "M3care: Learning with missing modalities in multimodal healthcare data",
      "authors": [
        "Chaohe Zhang",
        "Xu Chu",
        "Liantao Ma",
        "Yinghao Zhu",
        "Yasha Wang",
        "Jiangtao Wang",
        "Junfeng Zhao"
      ],
      "year": "2022",
      "venue": "Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining"
    },
    {
      "citation_id": "14",
      "title": "Sentence-bert: Sentence embeddings using siamese bert-networks",
      "authors": [
        "Nils Reimers",
        "Iryna Gurevych"
      ],
      "year": "1908",
      "venue": "CoRR"
    },
    {
      "citation_id": "15",
      "title": "Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors",
      "authors": [
        "Chien-Yao Wang",
        "Alexey Bochkovskiy",
        "Hong-Yuan Mark Liao"
      ],
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "16",
      "title": "Efficient training of audio transformers with patchout",
      "authors": [
        "Khaled Koutini",
        "Jan Schlüter",
        "Hamid Eghbal-Zadeh",
        "Gerhard Widmer"
      ],
      "year": "2022",
      "venue": "Interspeech 2022, 23rd Annual Conference of the International Speech Communication Association, Incheon"
    },
    {
      "citation_id": "17",
      "title": "Anton van den Hengel, and Javen Qinfeng Shi",
      "authors": [
        "Amin Parvaneh",
        "Ehsan Abbasnejad",
        "Damien Teney",
        "Reza Haffari"
      ],
      "year": "2022",
      "venue": "Anton van den Hengel, and Javen Qinfeng Shi"
    },
    {
      "citation_id": "18",
      "title": "Focal loss for dense object detection",
      "authors": [
        "Tsung-Yi Lin",
        "Priya Goyal",
        "Ross Girshick",
        "Kaiming He",
        "Piotr Dollár"
      ],
      "year": "2017",
      "venue": "CoRR"
    },
    {
      "citation_id": "19",
      "title": "Learning transferable visual models from natural language supervision",
      "authors": [
        "Alec Radford",
        "Jong Kim",
        "Chris Hallacy",
        "Aditya Ramesh",
        "Gabriel Goh",
        "Sandhini Agarwal",
        "Girish Sastry",
        "Amanda Askell",
        "Pamela Mishkin",
        "Jack Clark"
      ],
      "year": "2021",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "20",
      "title": "Memory fusion network for multi-view sequential learning",
      "authors": [
        "Amir Zadeh",
        "Paul Liang",
        "Navonil Mazumder",
        "Soujanya Poria",
        "Erik Cambria",
        "Louis-Philippe Morency"
      ],
      "year": "2018",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "21",
      "title": "ICON: Interactive conversational memory network for multimodal emotion detection",
      "authors": [
        "Devamanyu Hazarika",
        "Soujanya Poria",
        "Rada Mihalcea",
        "Erik Cambria",
        "Roger Zimmermann"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "22",
      "title": "Dialoguernn: An attentive rnn for emotion detection in conversations",
      "authors": [
        "Navonil Majumder",
        "Soujanya Poria",
        "Devamanyu Hazarika",
        "Rada Mihalcea",
        "Alexander Gelbukh",
        "Erik Cambria"
      ],
      "year": "2019",
      "venue": "Dialoguernn: An attentive rnn for emotion detection in conversations"
    },
    {
      "citation_id": "23",
      "title": "Modeling both context-and speaker-sensitive dependence for emotion detection in multi-speaker conversations",
      "authors": [
        "Dong Zhang",
        "Liangqing Wu",
        "Changlong Sun",
        "Shoushan Li",
        "Qiaoming Zhu",
        "Guodong Zhou"
      ],
      "year": "2019",
      "venue": "Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI-19"
    },
    {
      "citation_id": "24",
      "title": "Dialoguecrn: Contextual reasoning networks for emotion recognition in conversations",
      "authors": [
        "Dou Hu",
        "Lingwei Wei",
        "Xiaoyong Huai"
      ],
      "year": "2021",
      "venue": "Dialoguecrn: Contextual reasoning networks for emotion recognition in conversations",
      "arxiv": "arXiv:2106.01978"
    },
    {
      "citation_id": "25",
      "title": "Emocaps: Emotion capsule based model for conversational emotion recognition",
      "authors": [
        "Zaijing Li",
        "Fengxiao Tang",
        "Ming Zhao",
        "Yusen Zhu"
      ],
      "year": "2022",
      "venue": "Emocaps: Emotion capsule based model for conversational emotion recognition",
      "arxiv": "arXiv:2203.13504"
    },
    {
      "citation_id": "26",
      "title": "Semi-supervised multi-modal emotion recognition with cross-modal distribution matching",
      "authors": [
        "Jingjun Liang",
        "Ruichen Li",
        "Qin Jin"
      ],
      "year": "2020",
      "venue": "Proceedings of the 28th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "27",
      "title": "Robust multimodal emotion recognition from conversation with transformer-based crossmodality fusion",
      "authors": [
        "Baijun Xie",
        "Mariia Sidulova",
        "Chung Hyuk"
      ],
      "year": "2021",
      "venue": "Sensors"
    },
    {
      "citation_id": "28",
      "title": "Modeling hierarchical uncertainty for multimodal emotion recognition in conversation",
      "authors": [
        "Feiyu Chen",
        "Jie Shao",
        "Anjie Zhu",
        "Deqiang Ouyang",
        "Xueliang Liu",
        "Heng Tao Shen"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Cybernetics"
    },
    {
      "citation_id": "29",
      "title": "Shapes of emotions: Multimodal emotion recognition in conversations via emotion shifts",
      "authors": [
        "Harsh Agarwal",
        "Keshav Bansal",
        "Abhinav Joshi",
        "Ashutosh Modi"
      ],
      "year": "2021",
      "venue": "Shapes of emotions: Multimodal emotion recognition in conversations via emotion shifts",
      "arxiv": "arXiv:2112.01938"
    },
    {
      "citation_id": "30",
      "title": "Unimse: Towards unified multimodal sentiment analysis and emotion recognition",
      "authors": [
        "Guimin Hu",
        "Ting-En Lin",
        "Yi Zhao",
        "Guangming Lu",
        "Yuchuan Wu",
        "Yongbin Li"
      ],
      "year": "2022",
      "venue": "Unimse: Towards unified multimodal sentiment analysis and emotion recognition",
      "arxiv": "arXiv:2211.11256"
    },
    {
      "citation_id": "31",
      "title": "Learning emotion representations from verbal and nonverbal communication",
      "authors": [
        "Sitao Zhang",
        "Yimu Pan",
        "James Wang"
      ],
      "year": "2023",
      "venue": "Learning emotion representations from verbal and nonverbal communication"
    },
    {
      "citation_id": "32",
      "title": "Hybrid curriculum learning for emotion recognition in conversation",
      "authors": [
        "Lin Yang",
        "Yi Shen",
        "Yue Mao",
        "Longjun Cai"
      ],
      "year": "2021",
      "venue": "AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "33",
      "title": "Light-sernet: A lightweight fully convolutional neural network for speech emotion recognition",
      "authors": [
        "Arya Aftab",
        "Alireza Morsali",
        "Shahrokh Ghaemmaghami",
        "Benoit Champagne"
      ],
      "year": "2022",
      "venue": "ICASSP 2022 -2022 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "34",
      "title": "Mmgcn: Multi-modal graph convolution network for personalized recommendation of micro-video",
      "authors": [
        "Yinwei Wei",
        "Xiang Wang",
        "Liqiang Nie",
        "Xiangnan He",
        "Richang Hong",
        "Tat-Seng Chua"
      ],
      "year": "2019",
      "venue": "Proceedings of the 27th ACM International Conference on Multimedia"
    }
  ]
}