{
  "paper_id": "2308.13911v1",
  "title": "A Wide Evaluation Of Chatgpt On Affective Computing Tasks",
  "published": "2023-08-26T16:10:30Z",
  "authors": [
    "Mostafa M. Amin",
    "Rui Mao",
    "Erik Cambria",
    "Björn W. Schuller"
  ],
  "keywords": [
    "ChatGPT",
    "GPT-4",
    "Foundation Models",
    "Affective Computing",
    "Aspect-Based Sentiment Analysis",
    "Sentiment Analysis",
    "Sentiment Intensity Ranking",
    "Emotions Intensity Ranking",
    "Suicide Tendency Detection",
    "Toxicity Detection",
    "Well-being Assessment",
    "Engagement Measurement",
    "Personality Assessment",
    "Sarcasm Detection",
    "and Subjectivity Detection 3.2.2 Sentence-level labels prompts"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "With the rise of foundation models, a new artificial intelligence paradigm has emerged, by simply using general purpose foundation models with prompting to solve problems instead of training a separate machine learning model for each problem. Such models have been shown to have emergent properties of solving problems that they were not initially trained on. The studies for the effectiveness of such models are still quite limited. In this work, we widely study the capabilities of the ChatGPT models, namely GPT-4 and GPT-3.5, on 13 affective computing problems, namely aspect extraction, aspect polarity classification, opinion extraction, sentiment analysis, sentiment intensity ranking, emotions intensity ranking, suicide tendency detection, toxicity detection, well-being assessment, engagement measurement, personality assessment, sarcasm detection, and subjectivity detection. We introduce a framework to evaluate the ChatGPT models on regression-based problems, such as intensity ranking problems, by modelling them as pairwise ranking classification. We compare ChatGPT against more traditional NLP methods, such as end-to-end recurrent neural networks and transformers. The results demonstrate the emergent abilities of the ChatGPT models on a wide range of affective computing problems, where GPT-3.5 and especially GPT-4 have shown strong performance on many problems, particularly the ones related to sentiment, emotions, or toxicity. The ChatGPT models fell short for problems with implicit signals, such as engagement measurement and subjectivity detection.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Introduction",
      "text": "With the introduction of foundation models  [1] ,  [2] , a new paradigm to utilise machine learning models was introduced in Natural Language Processing (NLP). The paradigm relies on the emerging capabilities of Large Language Models (LLMs)  [3]  to perform more complex tasks with scaling. Instead of training specialised models for specific problems, a large general foundation model would be trained once with general knowledge and this would be utilised (via prompting) in many other problems later on. This paradigm has been introduced by language models as few-shot learners  [4] , and popularised with the launch of the groundbreaking ChatGPT foundation models, namely GPT-3.5  [5]  and its superior GPT-4  [6] , which utilised techniques like Reinforcement Learning with Human Feedback (RLHF)  [5] . The emerging capabilities of ChatGPT are being studied in several domains, mostly either related to general artificial intelligence  [7] , or traditional NLP problems, namely neural machine translation  [8] , and named entity recognition (NER)  [9] .\n\nWe examined the emerging capabilities of ChatGPT in affective computing in an early evaluation in a previous work  [10] , where we evaluated the performance of ChatGPT on three affective computing problems, namely suicide tendency detection, bigfive personality assessment, and sentiment analysis. The study has shown interesting initial results confirming the emerging capabilities of ChatGPT, with results comparable to classical NLP methods such as Bag-of-Words (BoW)  [11]  and Word2Vec  [12] , whereas worse than fine-tuned language models like RoBERTa  [13] . This study was performed on an early stage of ChatGPT, when the API for it was not yet released. Consequently, it was done manually on a small evaluation set. Additionally, given that prompt engineering is an emerging field, the prompting was less developed. Last but not least, the former study of ChatGPT evaluations focused on classification-based affective computing tasks, where regressionbased tasks were not evaluated.\n\nIn this work, we aim to extend these evaluations beyond the limitations of the previous work  [10] , namely by exploring better prompting, bigger evaluation, handling of different learning tasks, and the wider scope of affective computing problems  [14] ,  [15] . We examine a total of 34 setups for 13 affective computing problems. The problems are aspect extraction, aspect polarity classification, opinion extraction, sentiment analysis, sentiment intensity ranking, emotions intensity ranking, suicide tendency detection, toxicity detection, well-being assessment, engagement measurement, personality assessment, sarcasm detection, and subjectivity detection. The evaluation of these tasks is conducted due to their recognised prominence in the field of affective computing. These tasks collectively encompass various dimensions of understanding affective language and are widely acknowledged as pivotal in this domain.\n\nOur evaluation yields the following findings: ChatGPT models excel in sentiment-related tasks, outperforming supervised baselines in opinion extraction, emotions, and sentiment intensity ranking. They particularly excel in identifying extremely negative emotions, notably in well-being assessment and toxicity detection, likely due to safety-focused training. However, they struggle with implicit signal tasks such as engagement measurement, personality assessment, sarcasm detection, and subjectivity detection.\n\nThe contributions of this paper are as follows:\n\n1) Executing a wide evaluation of the performance of the ChatGPT models, namely GPT-3.5 and GPT-4, on a wide range of affective computing problems. 2) Introducing a prompting framework that can be applied to LLMs for a wide range of affective computing problems. 3) Introducing a framework for the conversion of a regression task into a pairwise ranking classification task for the purpose of evaluating ChatGPT models.\n\nThe paper is organised as follows: we discuss related work in the next section; afterwards, we introduce our method; then, we present and discuss the experimental results; finally, we provide concluding remarks.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Related Work",
      "text": "We introduce related works with a primary focus on the evaluation of foundation models in affective computing or NLP tasks. Affective computing is an important research domain in NLP, including diverse tasks, such as sentiment analysis  [16] ,  [17] , emotion detection  [18] ,  [19] , sarcasm detection  [20] ,  [21] , personality analysis  [22] , mental health analysis  [23] ,  [24] , figurative language processing  [25] ,  [26] , and more.  [27]  evaluates ChatGPT on various sentiment analysis-related tasks, including aspect extraction.  [28]  evaluates ChatGPT on sentiment analysis, while  [29]  evaluates ChatGPT's ability to predict personality. The works of  [30] ,  [31] ,  [32]  explore the capabilities of ChatGPT on a wide range of NLP problems including sentiment analysis and emotion recognition, and other NLP tasks like NER and text summarising. Furthermore, we evaluated the fusion capabilities of ChatGPT with traditional NLP methods in  [33] .\n\nThe aforementioned ChatGPT models assessments concentrated on a confined subset of tasks within the domain of affective computing, notably sentiment analysis and emotion detection. Nonetheless, considering the expansive scope of the affective computing field, there exists merit in conducting a comprehensive evaluation of the proficiency exhibited by the ChatGPT models within this domain. This is motivated by the recognition that distinct affective computing tasks encapsulate intricate emotional nuances, demanding ChatGPT models to adeptly discern them across diverse application contexts. A comprehensive evaluation encompassing a wider array of tasks also serves as a means to uncover any inherent biases that might manifest in the performance of the ChatGPT models.\n\nOn the other hand, the latest survey on the assessments of the ChatGPT models has unveiled a notable discrepancy in findings, stemming from the employment of diverse prompting strategies  [34] . Moreover, the evaluation process has scarcely encompassed regression tasks, due to the difficulty of prompting objective scores from LLMs in different task setups. This highlights the imperative of adopting a unified prompting framework to counteract the biases introduced by distinct affective computing evaluation tasks. Furthermore, the establishment of an efficacious evaluation methodology for assessing LLMs on regression tasks holds informative value for forthcoming research endeavours.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Methodology",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Utilising Chatgpt",
      "text": "We utilise the API provided by OpenAI to query ChatGPT 1 , using 'gpt-3.5-turbo-0301' for GPT-3.5 and 'gpt-4-0314' for GPT-4. For each problem and target label combination, we construct a prompt for it, which is used as the system prompt. For each given example in the evaluation data, we send two messages to ChatGPT, namely, the system prompt of the problem and a user message that includes only the input text of the example. The assistant response acquired by ChatGPT is then the answer for the example. We exclude the examples that did not have simple parsing criteria. Unlike the previous work  [10] , the extra specificity in the system prompt made the vast majority of examples easy to parse.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Prompting",
      "text": "The prompt design follows a general pattern:\n\n1) Define the role of the assistant as an expert in the given problem, and add a problem description. 2) Define the prediction task, namely, by specifying the type of labels and stating that the user will give an input and the assistant should reply to that by predicting the label for that example. 3) Emphasise the answer format in bullet points of the exact requested format, and also the not allowed formats. We do not allow it to explain the reasoning behind the decision, or mentioning statements like My guess is",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "2) Opinion Extraction Prompt",
      "text": "You are an aspect-based sentiment analysis expert, you will be given a sentence by the user that contains aspect words objects. Your task is to list all the sentiment opinionated words / expressions, that are corresponding to the aspect in the text (if any). You just need to list the words/expression in bullet points without classifying them. There will be many words without sentiment, these should not be listed. Use the following format:\n\nThe following text is appended at the end of each of the following prompts.\n\nUse",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "2) Sentiment Intensity Ranking Prompt",
      "text": "You are an expert at sentiment analysis. Given a pair of texts A and B from the user, you will output which text expresses more positive sentiment.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "3) Emotion Intensity Ranking Prompt",
      "text": "You are an expert at emotion analysis. Given a pair of texts A and B from the user, you will output which text expresses higher intensity of the {emotion} emotion.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "4) Suicide Detection Prompt",
      "text": "You are an expert at psyche analysis. Given a text by the user, solve the binary classification of analysing if the text expresses a tendency for suicide.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "5) Toxicity Detection Prompt",
      "text": "You are an expert at toxicity analysis. Assume that we have the capability of analysing 6 toxicity traits. \"toxic\", \"severe toxic\", \"obscene\", \"threat\", \"insult\", \"identity hate\". Your task is to make binary classification for the trait {trait}, and not the remaining traits. Given a text by the user, estimate if the given text displays the trait {trait} or not.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "6) Well-Being Assessment Prompt",
      "text": "You are an expert at psyche analysis. Given a text by the user, estimate if the given text talks about a stress-related topic, or expresses emotional stress be it implicit or explicit.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "7) Engagement Measurement Prompt",
      "text": "You are an expert at social media analysis. Given a pair of texts A and B representing tweets, estimate which text is more engaging. You will achieve this by estimating which text is more viral, by estimating which one has a higher number of retweets.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "8) Personality Assessment Prompt",
      "text": "You are an expert at the big-five personality traits assessment. Given a pair of texts A and B from the user, you will output which text expresses higher intensity of the {trait} trait, from the big-five OCEAN personality traits. 9) Sarcasm detection prompt You are an expert at sarcasm analysis. Given a text by the user, estimate if the given text is sarcastic or not.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "10) Subjectivity Detection Prompt",
      "text": "You are an expert at language and sentiment analysis. The user will give you a text, your task is to make a binary classification on the text, if the given text is opinionated / subjective / biased, or if it is non-opinionated / objective / descriptive / factual. Please note that this is about \"how\" the text is described and not \"what\" it describes, so the text can still \"objectively\" describe a fictional story with some emotional terms.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Pairwise Comparison To Solve Regression Tasks",
      "text": "Querying and prompting ChatGPT models for classification tasks can be readily accomplished by instructing them to select a suitable label from the predefined label set of classification tasks. Nonetheless, the challenge arises when transitioning to regression tasks, as querying an objective score from the ChatGPT models becomes intricate due to the variations in scales and the inherent subjectivity associated with dataset annotation criteria. Thus, we evaluate ChatGPT on regression problems by modelling regression problems as a pairwise ranking classification problem. In other words, given N regression labels in the evaluation data, we can remodel the problem into a classification problem by sampling M pairs (a, b), and solve the binary classification problem 'is y a > y b ?'. A crucial aspect of this framework is how to sample the M pairs to be as few as possible whilst being representative. We employ the small-world graph generation algorithm  [35] , which is a densely connected graph without having a very high number of connections. In our setup we sample M = 4N pairs. This modelling can be further applied for predictions not just evaluations. A prediction procedure would utilise successive halving of a range of answers by comparisons with the median item within the remaining range, until the range has a small width, similar to the binary search algorithm. However, we do not use this prediction mechanism in this work.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Compared Model",
      "text": "To evaluate the performance of the ChatGPT models, we compare them with a Recurrent Neural Network (RNN)-based framework that consists of a single bidirectional LSTM layer (with L units in each direction) followed by N fully-connected layers with U units (with ReLU activations), then a final prediction layer. The final layer uses sigmoid activation in most setups, except for aspect-based problems (softmax for multi-class classification), sentiment ranking (tanh), and engagement measurement (ReLU). This framework leverages different features that will be introduced in the following section.\n\nAdam  [36]  is employed as an optimisation algorithm, with a learning rate α. The loss function is crossentropy for classification tasks  [11] , and Mean Absolute Error (MAE) for regression tasks  [11] . For problems with imbalanced datasets, namely toxicity detection and aspect extraction, we make use of a weighting parameter λ that discounts the weight of the '0/negative' class (typically the over-represented class). In order to tune the hyperparameters λ, α, N, U, L, we opt to utilise the hyperparameter optimisation toolkit SMAC  [37]  to select the best hyperparameters for each problem. The hyperparameter space is\n\n(log-sampled), and λ ∈ [10 -3 , 1] (log-sampled). We sample 25 combinations for each problem. In each training run, we train for a total of 300 epochs with early stopping (30 epochs). The model with the best validation score is the one used for testing.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Text Features",
      "text": "We utilise two different textual representations of the input texts and training an RNN on top of that. The representations are the raw text as sequence of word IDs (we call that the end-to-end (E2E) approach), and RoBERTa features  [13]  In the E2E approach, the IDs are limited to the most common 2,000 words in the training set of a given problem, and we train embeddings of dimension 128 for that, jointly with the rest of the model.\n\nRoBERTa Language Model We employ the RoBERTa language model to extract features for a given sentence, by basically running the RoBERTa-base model 2 on a given sentence to acquire a sequence of features, corresponding to features of the subwords. Each subword is represented by a feature vector of size 768. The RoBERTa-base model  [13]  is the smaller variant of the RoBERTa architecture, which is based on the BERT transformer architecture  [38] . For word-based labelling in the aspect-related problems, we give all subwords the same label as the corresponding word at training, while using the prediction of the first subword of a word as the corresponding word prediction at evaluation similar to  [38] .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Datasets",
      "text": "We employ 13 datasets for the 13 evaluation tasks in the affective computing domain. The summary of the statistics of our employed datasets is given in Table  1 . For training and validation, we make use of the original splits provided with the datasets, or we split them otherwise 3 . For benchmarking, we always downsample the testing set if the original testing set is not small enough, due to the very tight restrictions on scaled usage of GPT-4. The Aspect-based Sentiment Analysis Datasets are from the SemEval 2014  [39]  and SemEval 2015  [40]  shared tasks. The shared tasks sourced data from laptop and restaurant reviews, termed lap14, res14, and res15 in the later result table. We employ the split of training, validation, and testing sets from the work of  [41] . The dataset contains three sets of word annotations labels for aspect extraction, aspect polarity prediction, and opinion extraction tasks. The Sentiment Analysis Dataset is the Twitter140 dataset  [42]  which consists of tweets and their corresponding sentiment negative or positive labels. Given the extremely small size of the provided testing set, we downsample and split the large training set into the Train/Dev/Test sets we use. The Sentiment Intensity Ranking Dataset is from the SemEval-2017 Shared Task 5  [43] . The dataset sourced data from two domains, namely, microblog messages and news headlines. The sentiment intensity scores are within [-1, 1]. Our experiments are conducted on the microblog data. The Emotion Intensity Dataset is from the WASSA-2017 Shared Task on Emotion Intensity (EmoInt)  [44] . The dataset includes four emotions, namely joy, sadness, fear, and anger and emotion intensity scores, ranging within [0, 1].  The Well-being Assessment Dataset is from the work of  [23] , where the data were from Reddit and Twitter, with two benchmarks on each. For the (Combi) Reddit benchmark, we make use of the bodies of the posts, instead of the titles. Binary labels are used in this dataset, indicating stress-negative and stress-positive text.\n\nThe Engagement Measurement Dataset is from Kaggle TEDTalks Tweets 5 . The dataset was obtained from Twitter and pertains to discussions associated with TEDTalks. The dataset contains tweet content, and the number of likes. This allows us to conduct a comparative analysis, discerning the relative favourability of different content. We use the log 10 (number of retweets + 1) as the target label, since it represents the labels distribution effectively.\n\nThe Personality Assessment Dataset is the First Impressions dataset  [46] , from which we use the transcripts of personality annotated videos. The labels are expressed by the big-five personality model (Openness, Conscientiousness, extraversion, agreeableness, and neuroticism), with labels within [0, 1].\n\nThe Sarcasm Detection Dataset is from the work of  [47]  (Version 1). The data were sourced from TheOnion and HuffPost news headlines, associated with binary labels, indicating if a news headline is sarcastic or not.\n\nThe Subjectivity Detection Dataset is from the work of  [48] . The data were sourced from movie reviews from Rotten Tomatoes,",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Results",
      "text": "The main results of the experiments are shown in Table  2 , where we evaluate classification accuracy and Unweighted Average Recall (UAR), which is the unweighted average of the accuracy of classifying each class separately  [49] . For the time-series labels, we use micro-averaging for the measuring the performance. For all results, we utilise a two-tailed randomised permutation test to check for the statistical significance of the differences in performance compared to GPT-3.5  [50] .\n\nThe results show that the RNN trained with RoBERTa features has the best performance in the majority of problems, for both metrics; in most of these, it is even significantly better than GPT-3.5. Furthermore, GPT-4 comes at second best overall, with the best performance on some of the setups. In the instances where GPT-3.5 is better than RoBERTa, it is with a difference that is not regarded as statistically significant, and it is often due to the fact that the RoBERTa-based approach prioritised improving UAR instead of accuracy. On the other hand, comparing GPT-3.5 to the simpler baseline E2E presents a different picture; E2E is sometimes significantly worse and sometimes significantly better than GPT-3.5.\n\nFor aspect extraction and aspect target predictions, RoBERTa has the best performance overall especially when considering UAR, followed by GPT-3.5. However, E2E is better than GPT-3.5 (considering UAR only) to identify the aspect but not its polarity. Surprisingly, GPT-4 has the worst performance only on this problem; upon inspection of few of the GPT-4 results, we found that despite its right identification of the aspect expressions, still it tends to include more surroundings words, which is probably a major reason behind this deterioration in performance. For the opinion extraction problem, GPT-4 has the best performance, especially considering UAR, followed by RoBERTa, then GPT-3.5.\n\nIn sentiment related problems, RoBERTa and GPT-4 are far better than both GPT-3.5 and E2E. GPT-3.5 and E2E have relatively close performance on sentiment-based problems. The results of GPT-3.5 on sentiment analysis are similar to previous work  [10] ,  [33] .\n\nIn emotions intensity ranking, GPT-4 is the best model (significantly better than GPT-3.5 and E2E), followed by RoBERTa, then GPT-3.5, then E2E (significantly worse than GPT-3.5). An interesting observation is the strong performance of GPT-3.5 in identifying the emotion sadness; since ChatGPT models seem to be very competent in problems related to identifying negative emotions as we will elaborate.\n\nOn psychology-related problems with extreme negative emotions, namely the detection of suicide tendency, well-being, and toxicity, the ChatGPT models have a strong performance in many cases that significantly outperforms E2E. The results of the suicidedetection are consistent with previous work  [10] ,  [33] , where RoBERTa is significantly better than GPT-3.5. Moreover, ChatGPT models seem to thrive with longer texts, as seen in the results of the well-being problem part Reddit bodies, which is the only part consisting of long texts of the full body of Reddit posts, where both GPT-4 and GPT-3.5 are achieving the best results; on the shorter texts, RoBERTa is the best model by a wide margin. Furthermore, the ChatGPT models are showing the best results for the toxicity problem. This might come as non-surprising given the fact that the ChatGPT models from OpenAI generally are tuned to identify toxicity as part of applying safety policies 6 . However, they are not as competent at specifying further the reason behind toxicity, given their inferiority compared to RoBERTa about the more specific toxicity labels.\n\nFor the tasks with more implicit or latent social signals, namely engagement measurement, personality assessment, sarcasm detection, and subjectivity detection, GPT-3.5 has very poor performance that is significantly worse than E2E in most cases and significantly worse than RoBERTa in all cases. GPT-4 shows minor improvement over GPT-3.5 on these problems, where it slightly surpasses E2E only in the sarcasm and subjectivity detection problems. Similar to previous work  [10] ,  [33] , the results of the personality from GPT-3.5 are the worst, even compared to the simple baseline BoW. The results of the ChatGPT models on the engagement measurement problem are extremely poor, close to a random predictor (UAR 50 %). In earlier experiments, we attempted to make GPT-3.5 solve different formulations of the engagement measurement problem; the first was binary classifying if the number of retweets is < 10, the second was classifying if the number of retweets is < 100, and the third is 3-class classifying the number of retweets [0, 10),  [10, 100) , and [100, ∞). It still yielded the same poor results in all of them.\n\nThe effectiveness of the introduced regression evaluation technique in section 3.3 is demonstrated by the results of the personality assessment problem. In the previous work  [10] ,  [33] , personality assessment was evaluated as a binary classification problem, while the results of GPT-3.5 here are evaluated using the newly introduced pairwise ranking evaluation framework. The consistency of the results and their relative order across the five independent labels indicate the effectiveness of the technique. The same applies to the engagement measurement problem as mentioned earlier.\n\nThe issues of parsing results mentioned in the early evaluation  [10]  are mostly resolved within this study. This is due to the system prompt (which enforces instructions) introduced in the API version of ChatGPT models (which was not available at the early evaluation  [10] ), and our more precise formulation of the prompts (see section 3.1). A crucial aspect is highlighting the possible answering formats while disallowing improper formats. The redundancy is also helpful in this regard, since we specify the formatting in a general form once on the problem description (like  [10] ), then, we specify it a second time in a precise manner in the notes bullet points (unlike  [10] ). This led to the behaviour that very few examples are not following the described format, unlike  [10] . However, the issue still stands for the compound predictions, in particular, we faced the same parsing issues for the aspect-related problems because the predictions contained multiple labels, where ChatGPT models were improvising in the response format or modifying some of the mentioned input words. As a result, the responses of ChatGPT models are not fully reliable 6. https://openai.com/policies/usage-policies for properly formatting complex predictions, only straight-forward predictions with a short answer.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Conclusion",
      "text": "In this paper, we evaluated ChatGPT models, namely GPT-3.5 and GPT-4, on 13 affective computing problems. We prompted the ChatGPT models methodically for each problem, then, we used the API to retrieve predictions. We compared the ChatGPT models against two traditional natural language processing models, namely an end-to-end (E2E) Recurrent Neural Network (RNN), or extending the RNN by using the input features from the RoBERTa model. The results have shown that the general performance order of the models is championed by RoBERTa, followed by GPT-4, then, E2E and GPT-3.5 are the following rivals with no clear winner for all problems. GPT-4 is mostly better than GPT-3.5, except for the aspect extraction problem, with a statistically significant difference in most cases.\n\nThe ChatGPT models showed strong performance in sentimentrelated problems, where GPT-3.5 is usually better than E2E, and GPT-4 tends to be often better than RoBERTa, e. g. , in the opinion extraction, and emotions and sentiment intensity ranking problems. Both ChatGPT models have shown their strongest performance in problems identifying extremely negative emotions, especially for well-being assessment over long texts and toxicity detection; this is probably due to the extra training by OpenAI in an attempt to enforce safety policies. The problems that the ChatGPT models fell very short were problems with implicit signals, like engagement measurement, personality assessment, sarcasm detection, and subjectivity detection. GPT-3.5 has shown significantly worse performance than E2E for most of these, and GPT-4 was at best slightly better than E2E if not significantly worse for two of these problems. Future efforts can focus on reinforced prompt design, and a synergistic combination of the compared approaches.\n\nRui Mao is a research fellow and lead investigator at Nanyang Technological University. He obtained his Ph.D. degree in Computing Science from the University of Aberdeen. His research interest lies at NLP, affective computing, and their applications in finance and cognitive science. He and his funded company (Ruimao Tech) have developed an end-to-end system (MetaPro) for computational metaphor processing and a neural search engine (wensousou.com) for searching Chinese ancient poems with modern language. Contact him at rui.mao@ntu.edu.sg.\n\nErik Cambria is a professor of Computer Science and Engineering at Nanyang Technological University, Singapore. His research focuses on neurosymbolic AI for explainable natural language processing in domains like sentiment analysis, dialogue systems, and financial forecasting. He is an IEEE Fellow and a recipient of several awards, e. g., IEEE Outstanding Career Award, was listed among the AI's 10 to Watch, and was featured in Forbes as one of the 5 People Building Our AI Future. Contact him at cambria@ntu.edu.sg.",
      "page_start": 6,
      "page_end": 7
    }
  ],
  "figures": [],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Problem": "ABSA\nres14\nlap14\nres15",
          "Train": "2,436\n2,439\n1,052",
          "Dev": "608\n609\n263",
          "Test": "800\n800\n685"
        },
        {
          "Problem": "Sentiment Analysis",
          "Train": "100,000",
          "Dev": "10,000",
          "Test": "2,500"
        },
        {
          "Problem": "Sentiment Ranking",
          "Train": "1,000",
          "Dev": "300",
          "Test": "365"
        },
        {
          "Problem": "Emotion\nSadness\nJoy\nfear\nAnger",
          "Train": "786\n823\n1,147\n857",
          "Dev": "74\n79\n110\n84",
          "Test": "673\n714\n995\n760"
        },
        {
          "Problem": "Suicide",
          "Train": "23,398",
          "Dev": "5,611",
          "Test": "2,345"
        },
        {
          "Problem": "Toxicity",
          "Train": "30,000",
          "Dev": "6,864",
          "Test": "959"
        },
        {
          "Problem": "Well-be.\nReddit bodies\nReddit\ntitles\nTwitter denoised\nTwitter full",
          "Train": "1,511\n3,538\n851\n5,900",
          "Dev": "458\n996\n400\n1,500",
          "Test": "935\n998\n800\n1,500"
        },
        {
          "Problem": "Engagement",
          "Train": "30,037",
          "Dev": "5,000",
          "Test": "4,000"
        },
        {
          "Problem": "Personality",
          "Train": "5,992",
          "Dev": "2,000",
          "Test": "1,996"
        },
        {
          "Problem": "Sarcasm",
          "Train": "18,709",
          "Dev": "4,000",
          "Test": "4,000"
        },
        {
          "Problem": "Subjectivity",
          "Train": "6,000",
          "Dev": "2,000",
          "Test": "2,000"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Dataset": "",
          "Accuracy [%]": "GPT-3.5",
          "UAR [%]": "GPT-3.5"
        },
        {
          "Dataset": "Aspect\nExtraction",
          "Accuracy [%]": "86.95\n84.60\n84.57",
          "UAR [%]": "76.18\n77.62\n78.56"
        },
        {
          "Dataset": "Aspect\nPolarity",
          "Accuracy [%]": "85.13\n82.23\n82.38",
          "UAR [%]": "49.96\n47.37\n47.79"
        },
        {
          "Dataset": "Opinion\nExtraction",
          "Accuracy [%]": "91.04\n89.43\n89.32",
          "UAR [%]": "77.28\n73.51\n76.90"
        },
        {
          "Dataset": "Sentiment Analysis",
          "Accuracy [%]": "80.54",
          "UAR [%]": "79.93"
        },
        {
          "Dataset": "Sentiment Ranking",
          "Accuracy [%]": "69.30",
          "UAR [%]": "68.69"
        },
        {
          "Dataset": "Joy\nFear\nEmotion\nAnger\nRanking\nSadness",
          "Accuracy [%]": "74.07\n72.76\n72.12\n78.19",
          "UAR [%]": "74.29\n72.86\n72.09\n78.22"
        },
        {
          "Dataset": "Suicide Detection",
          "Accuracy [%]": "89.46",
          "UAR [%]": "89.44"
        },
        {
          "Dataset": "Toxicity",
          "Accuracy [%]": "87.37\n66.55\n83.45\n70.59\n80.14\n66.82",
          "UAR [%]": "87.19\n80.65\n83.48\n80.12\n83.21\n78.61"
        },
        {
          "Dataset": "Well-being",
          "Accuracy [%]": "91.93\n80.61\n60.53\n66.24",
          "UAR [%]": "84.41\n80.05\n65.05\n66.26"
        },
        {
          "Dataset": "Engagement",
          "Accuracy [%]": "51.92",
          "UAR [%]": "51.85"
        },
        {
          "Dataset": "Openness\nConscient.\nPersonality\nExtraversion\nAgreeable.\nNeuroticism",
          "Accuracy [%]": "50.11\n55.54\n53.55\n51.67\n48.94",
          "UAR [%]": "50.60\n55.84\n53.38\n52.10\n49.04"
        },
        {
          "Dataset": "Sarcasm",
          "Accuracy [%]": "59.13",
          "UAR [%]": "56.82"
        },
        {
          "Dataset": "Subjectivity",
          "Accuracy [%]": "59.56",
          "UAR [%]": "58.59"
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "On the Opportunities and Risks of Foundation Models",
      "authors": [
        "R Bommasani",
        "D Hudson",
        "E Adeli",
        "R Altman",
        "S Arora",
        "S Arx",
        "M Bernstein",
        "J Bohg",
        "A Bosselut",
        "E Brunskill",
        "E Brynjolfsson"
      ],
      "year": "2021",
      "venue": "On the Opportunities and Risks of Foundation Models",
      "arxiv": "arXiv:2108.07258"
    },
    {
      "citation_id": "2",
      "title": "A Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT",
      "authors": [
        "C Zhou",
        "Q Li",
        "C Li",
        "J Yu",
        "Y Liu",
        "G Wang",
        "K Zhang",
        "C Ji",
        "Q Yan",
        "L He",
        "H Peng",
        "J Li",
        "J Wu",
        "Z Liu",
        "P Xie",
        "C Xiong",
        "J Pei",
        "P Yu",
        "L Sun"
      ],
      "year": "2023",
      "venue": "A Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT",
      "arxiv": "arXiv:2302.09419"
    },
    {
      "citation_id": "3",
      "title": "Emergent Abilities of Large Language Models",
      "authors": [
        "J Wei",
        "Y Tay",
        "R Bommasani",
        "C Raffel",
        "B Zoph",
        "S Borgeaud",
        "D Yogatama",
        "M Bosma",
        "D Zhou",
        "D Metzler",
        "E Chi",
        "T Hashimoto",
        "O Vinyals",
        "P Liang",
        "J Dean",
        "W Fedus"
      ],
      "year": "2022",
      "venue": "Emergent Abilities of Large Language Models",
      "arxiv": "arXiv:2206.07682"
    },
    {
      "citation_id": "4",
      "title": "Language Models are Few-Shot Learners",
      "authors": [
        "T Brown",
        "B Mann",
        "N Ryder",
        "M Subbiah",
        "J Kaplan",
        "P Dhariwal",
        "A Neelakantan",
        "P Shyam",
        "G Sastry",
        "A Askell",
        "S Agarwal",
        "A Herbert-Voss",
        "G Krueger",
        "T Henighan",
        "R Child"
      ],
      "venue": "Language Models are Few-Shot Learners"
    },
    {
      "citation_id": "5",
      "title": "Training language models to follow instructions with human feedback",
      "authors": [
        "L Ouyang",
        "J Wu",
        "X Jiang",
        "D Almeida",
        "C Wainwright",
        "P Mishkin",
        "C Zhang",
        "S Agarwal",
        "K Slama",
        "A Ray",
        "J Schulman",
        "J Hilton",
        "F Kelton",
        "L Miller",
        "M Simens",
        "A Askell",
        "P Welinder",
        "P Christiano",
        "J Leike",
        "R Lowe"
      ],
      "year": "2022",
      "venue": "NeurIPS"
    },
    {
      "citation_id": "6",
      "title": "GPT-4",
      "authors": [
        "Openai"
      ],
      "year": "2023",
      "venue": "GPT-4",
      "arxiv": "arXiv:2303.08774"
    },
    {
      "citation_id": "7",
      "title": "Sparks of Artificial General Intelligence: Early experiments with GPT-4",
      "authors": [
        "S Bubeck",
        "V Chandrasekaran",
        "R Eldan",
        "J Gehrke",
        "E Horvitz",
        "E Kamar",
        "P Lee",
        "Y Lee",
        "Y Li",
        "S Lundberg",
        "H Nori",
        "H Palangi",
        "M Ribeiro",
        "Y Zhang"
      ],
      "year": "2023",
      "venue": "Sparks of Artificial General Intelligence: Early experiments with GPT-4",
      "arxiv": "arXiv:2303.12712"
    },
    {
      "citation_id": "8",
      "title": "How Good Are GPT Models at Machine Translation? A Comprehensive Evaluation",
      "authors": [
        "A Hendy",
        "M Abdelrehim",
        "A Sharaf",
        "V Raunak",
        "M Gabr",
        "H Matsushita",
        "Y Kim",
        "M Afify",
        "H Awadalla"
      ],
      "year": "2023",
      "venue": "How Good Are GPT Models at Machine Translation? A Comprehensive Evaluation",
      "arxiv": "arXiv:2302.09210"
    },
    {
      "citation_id": "9",
      "title": "Prompt ChatGPT In MNER: Improved multimodal named entity recognition method based on auxiliary refining knowledge from ChatGPT",
      "authors": [
        "J Li",
        "H Li",
        "Z Pan",
        "G Pan"
      ],
      "year": "2023",
      "venue": "Prompt ChatGPT In MNER: Improved multimodal named entity recognition method based on auxiliary refining knowledge from ChatGPT",
      "arxiv": "arXiv:2305.12212"
    },
    {
      "citation_id": "10",
      "title": "Will Affective Computing Emerge from Foundation Models and General AI? A First Evaluation on ChatGPT",
      "authors": [
        "M Amin",
        "E Cambria",
        "B Schuller"
      ],
      "year": "2023",
      "venue": "IEEE Intelligent Systems"
    },
    {
      "citation_id": "11",
      "title": "Pattern Recognition and Machine Learning",
      "authors": [
        "C Bishop"
      ],
      "year": "2006",
      "venue": "Pattern Recognition and Machine Learning"
    },
    {
      "citation_id": "12",
      "title": "Distributed Representations of Words and Phrases and their Compositionality",
      "authors": [
        "T Mikolov",
        "I Sutskever",
        "K Chen",
        "G Corrado",
        "J Dean"
      ],
      "year": "2013",
      "venue": "Distributed Representations of Words and Phrases and their Compositionality"
    },
    {
      "citation_id": "13",
      "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach",
      "authors": [
        "Y Liu",
        "M Ott",
        "N Goyal",
        "J Du",
        "M Joshi",
        "D Chen",
        "O Levy",
        "M Lewis",
        "L Zettlemoyer",
        "V Stoyanov"
      ],
      "year": "2019",
      "venue": "RoBERTa: A Robustly Optimized BERT Pretraining Approach",
      "arxiv": "arXiv:1907.11692"
    },
    {
      "citation_id": "14",
      "title": "Affective Computing and Sentiment Analysis",
      "authors": [
        "E Cambria"
      ],
      "year": "2016",
      "venue": "IEEE Intelligent Systems"
    },
    {
      "citation_id": "15",
      "title": "Computational Paralinguistics: Emotion, Affect and Personality in Speech and Language Processing",
      "authors": [
        "B Schuller",
        "A Batliner"
      ],
      "year": "2013",
      "venue": "Computational Paralinguistics: Emotion, Affect and Personality in Speech and Language Processing"
    },
    {
      "citation_id": "16",
      "title": "Bridging Towers of Multi-task Learning with a Gating Mechanism for Aspect-based Sentiment Analysis and Sequential Metaphor Identification",
      "authors": [
        "R Mao",
        "X Li"
      ],
      "year": "2021",
      "venue": "Proceedings of AAAI"
    },
    {
      "citation_id": "17",
      "title": "SenticNet 7: A commonsense-based neurosymbolic AI framework for explainable sentiment analysis",
      "authors": [
        "E Cambria",
        "Q Liu",
        "S Decherchi",
        "F Xing",
        "K Kwok"
      ],
      "year": "2022",
      "venue": "Proceedings of LREC"
    },
    {
      "citation_id": "18",
      "title": "SKIER: A Symbolic Knowledge Integrated Model for Conversational Emotion Recognition",
      "authors": [
        "W Li",
        "L Zhu",
        "R Mao",
        "E Cambria"
      ],
      "year": "2023",
      "venue": "Proceedings of AAAI"
    },
    {
      "citation_id": "19",
      "title": "The Biases of Pre-Trained Language Models: An Empirical Study on Prompt-Based Sentiment Analysis and Emotion Detection",
      "authors": [
        "R Mao",
        "Q Liu",
        "K He",
        "W Li",
        "E Cambria"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "20",
      "title": "Sarcasm Detection in News Headlines using Supervised Learning",
      "authors": [
        "A Jayaraman",
        "T Trueman",
        "G Ananthakrishnan",
        "S Mitra",
        "Q Liu",
        "E Cambria"
      ],
      "year": "2022",
      "venue": "International Conference on Artificial Intelligence and Data Engineering (AIDE)"
    },
    {
      "citation_id": "21",
      "title": "KnowleNet: Knowledge fusion network for multimodal sarcasm detection",
      "authors": [
        "T Yue",
        "R Mao",
        "H Wang",
        "Z Hu",
        "E Cambria"
      ],
      "year": "2023",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "22",
      "title": "PAED: Zero-Shot Persona Attribute Extraction in Dialogues",
      "authors": [
        "L Zhu",
        "W Li",
        "R Mao",
        "V Pandelea",
        "E Cambria"
      ],
      "year": "2023",
      "venue": "Proceedings of ACL"
    },
    {
      "citation_id": "23",
      "title": "Stress Detection from Social Media Articles: New Dataset Benchmark and Analytical Study",
      "authors": [
        "A Rastogi",
        "Q Liu",
        "E Cambria"
      ],
      "year": "2022",
      "venue": "IJCNN"
    },
    {
      "citation_id": "24",
      "title": "Hierarchical Attention Network for Explainable Depression Detection on Twitter Aided by Metaphor Concept Mappings",
      "authors": [
        "S Han",
        "R Mao",
        "E Cambria"
      ],
      "year": "2022",
      "venue": "Proceedings of COLING"
    },
    {
      "citation_id": "25",
      "title": "Word Embedding and WordNet Based Metaphor Identification and Interpretation",
      "authors": [
        "R Mao",
        "C Lin",
        "F Guerin"
      ],
      "year": "2018",
      "venue": "Proceedings of ACL"
    },
    {
      "citation_id": "26",
      "title": "MetaPro Online: A Computational Metaphor Processing Online System",
      "authors": [
        "R Mao",
        "X Li",
        "K He",
        "M Ge",
        "E Cambria"
      ],
      "year": "2023",
      "venue": "Proceedings of ACL (System Demonstrations)"
    },
    {
      "citation_id": "27",
      "title": "Sentiment Analysis in the Era of Large Language Models: A Reality Check",
      "authors": [
        "W Zhang",
        "Y Deng",
        "B Liu",
        "S Pan",
        "L Bing"
      ],
      "year": "2023",
      "venue": "Sentiment Analysis in the Era of Large Language Models: A Reality Check",
      "arxiv": "arXiv:2305.15005"
    },
    {
      "citation_id": "28",
      "title": "Is ChatGPT a Good Sentiment Analyzer? A Preliminary Study",
      "authors": [
        "Z Wang",
        "Q Xie",
        "Z Ding",
        "Y Feng",
        "R Xia"
      ],
      "year": "2023",
      "venue": "Is ChatGPT a Good Sentiment Analyzer? A Preliminary Study",
      "arxiv": "arXiv:2304.04339"
    },
    {
      "citation_id": "29",
      "title": "Is ChatGPT a Good Personality Recognizer? A Preliminary Study",
      "authors": [
        "Y Ji",
        "W Wu",
        "H Zheng",
        "Y Hu",
        "X Chen",
        "L He"
      ],
      "year": "2023",
      "venue": "Is ChatGPT a Good Personality Recognizer? A Preliminary Study",
      "arxiv": "arXiv:2307.03952"
    },
    {
      "citation_id": "30",
      "title": "ChatGPT: Jack of all trades, master of none",
      "authors": [
        "J Kocoń",
        "I Cichecki",
        "O Kaszyca",
        "M Kochanek",
        "D Szydło",
        "J Baran",
        "J Bielaniewicz",
        "M Gruza",
        "A Janz",
        "K Kanclerz"
      ],
      "year": "2023",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "31",
      "title": "Can ChatGPT Understand Too? A Comparative Study on ChatGPT and Fine-tuned BERT",
      "authors": [
        "Q Zhong",
        "L Ding",
        "J Liu",
        "B Du",
        "D Tao"
      ],
      "year": "2023",
      "venue": "Can ChatGPT Understand Too? A Comparative Study on ChatGPT and Fine-tuned BERT",
      "arxiv": "arXiv:2302.10198"
    },
    {
      "citation_id": "32",
      "title": "Is ChatGPT a General-Purpose Natural Language Processing Task Solver",
      "authors": [
        "C Qin",
        "A Zhang",
        "Z Zhang",
        "J Chen",
        "M Yasunaga",
        "D Yang"
      ],
      "year": "2023",
      "venue": "Is ChatGPT a General-Purpose Natural Language Processing Task Solver",
      "arxiv": "arXiv:2302.06476"
    },
    {
      "citation_id": "33",
      "title": "Can ChatGPT's Responses Boost Traditional Natural Language Processing",
      "authors": [
        "M Amin",
        "E Cambria",
        "B Schuller"
      ],
      "year": "2023",
      "venue": "IEEE Intelligent Systems"
    },
    {
      "citation_id": "34",
      "title": "GPTEval: A survey on assessments of ChatGPT and GPT-4",
      "authors": [
        "R Mao",
        "G Chen",
        "X Zhang",
        "F Guerin",
        "E Cambria"
      ],
      "year": "2023",
      "venue": "GPTEval: A survey on assessments of ChatGPT and GPT-4",
      "arxiv": "arXiv:2308.12488"
    },
    {
      "citation_id": "35",
      "title": "Overcoming Calibration Problems in Pattern Labeling with Pairwise Ratings: Application to Personality Traits",
      "authors": [
        "B Chen",
        "S Escalera",
        "I Guyon",
        "V Ponce-López",
        "N Shah",
        "M Oliu Simón"
      ],
      "year": "2016",
      "venue": "Computer Vision -ECCV"
    },
    {
      "citation_id": "36",
      "title": "",
      "authors": [
        "Workshops"
      ],
      "year": "2016",
      "venue": ""
    },
    {
      "citation_id": "37",
      "title": "Adam: A Method for Stochastic Optimization",
      "authors": [
        "D Kingma",
        "J Ba"
      ],
      "year": "2015",
      "venue": "ICLR"
    },
    {
      "citation_id": "38",
      "title": "SMAC3: A Versatile Bayesian Optimization Package for Hyperparameter Optimization",
      "authors": [
        "M Lindauer",
        "K Eggensperger",
        "M Feurer",
        "A Biedenkapp",
        "D Deng",
        "C Benjamins",
        "T Ruhkopf",
        "R Sass",
        "F Hutter"
      ],
      "year": "2022",
      "venue": "Journal of Machine Learning Research"
    },
    {
      "citation_id": "39",
      "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
      "authors": [
        "J Devlin",
        "M.-W Chang",
        "K Lee",
        "K Toutanova"
      ],
      "year": "2019",
      "venue": "Proceedings of NAACL"
    },
    {
      "citation_id": "40",
      "title": "SemEval-2014 Task 4: Aspect Based Sentiment Analysis",
      "authors": [
        "M Pontiki",
        "D Galanis",
        "J Pavlopoulos",
        "H Papageorgiou",
        "I Androutsopoulos",
        "S Manandhar"
      ],
      "year": "2014",
      "venue": "Proceedings of SemEval 2014"
    },
    {
      "citation_id": "41",
      "title": "SemEval-2015 Task 12: Aspect Based Sentiment Analysis",
      "authors": [
        "M Pontiki",
        "D Galanis",
        "H Papageorgiou",
        "S Manandhar",
        "I Androutsopoulos"
      ],
      "year": "2015",
      "venue": "Proceedings of SemEval 2015"
    },
    {
      "citation_id": "42",
      "title": "Relation-Aware Collaborative Learning for Unified Aspect-Based Sentiment Analysis",
      "authors": [
        "Z Chen",
        "T Qian"
      ],
      "year": "2020",
      "venue": "Proceedings of ACL, Online"
    },
    {
      "citation_id": "43",
      "title": "Twitter Sentiment Classification using Distant Supervision",
      "authors": [
        "A Go",
        "R Bhayani",
        "L Huang"
      ],
      "year": "2009",
      "venue": "CS224N project report"
    },
    {
      "citation_id": "44",
      "title": "SemEval-2017 Task 5: Fine-Grained Sentiment Analysis on Financial Microblogs and News",
      "authors": [
        "K Cortis",
        "A Freitas",
        "T Daudert",
        "M Huerlimann",
        "M Zarrouk",
        "S Handschuh",
        "B Davis"
      ],
      "year": "2017",
      "venue": "Proceedings of SemEval-2017"
    },
    {
      "citation_id": "45",
      "title": "WASSA-2017 Shared Task on Emotion Intensity",
      "authors": [
        "S Mohammad",
        "F Bravo-Marquez"
      ],
      "year": "2017",
      "venue": "Proceedings of WASSA 2017"
    },
    {
      "citation_id": "46",
      "title": "Suicide and Depression Detection in Social Media Forums",
      "authors": [
        "V Desu",
        "N Komati",
        "S Lingamaneni",
        "F Shaik"
      ],
      "year": "2022",
      "venue": "Smart Intelligent Computing and Applications"
    },
    {
      "citation_id": "47",
      "title": "Chalearn lap 2016: First Round Challenge on First Impressions -Dataset and Results",
      "authors": [
        "V Ponce-López",
        "B Chen",
        "M Oliu",
        "C Corneanu",
        "A Clapés",
        "I Guyon",
        "X Baró",
        "H Escalante",
        "S Escalera"
      ],
      "year": "2016",
      "venue": "ECCV"
    },
    {
      "citation_id": "48",
      "title": "Sarcasm Detection using News Headlines Dataset",
      "authors": [
        "R Misra",
        "P Arora"
      ],
      "year": "2023",
      "venue": "AI Open"
    },
    {
      "citation_id": "49",
      "title": "A Sentimental Education: Sentiment Analysis Using Subjectivity Summarization Based on Minimum Cuts",
      "authors": [
        "B Pang",
        "L Lee"
      ],
      "year": "2004",
      "venue": "Proceedings of ACL"
    },
    {
      "citation_id": "50",
      "title": "The INTER-SPEECH 2013 Computational Paralinguistics Challenge: Social Signals, Conflict, Emotion, Autism",
      "authors": [
        "B Schuller",
        "S Steidl",
        "A Batliner",
        "A Vinciarelli",
        "K Scherer",
        "F Ringeval",
        "M Chetouani",
        "F Weninger",
        "F Eyben",
        "E Marchi",
        "M Mortillaro",
        "H Salamin",
        "A Polychroniou",
        "F Valente",
        "S Kim"
      ],
      "year": "2013",
      "venue": "The INTER-SPEECH 2013 Computational Paralinguistics Challenge: Social Signals, Conflict, Emotion, Autism"
    },
    {
      "citation_id": "51",
      "title": "Mostafa M. Amin is currently working toward the Ph.D. degree with the Chair of Embedded Intelligence for Health Care and Wellbeing with University of Augsburg, while working as Senior Research Data Scientist at SyncPilot GmbH in Augsburg",
      "authors": [
        "P Good"
      ],
      "year": "1994",
      "venue": "His research interests include Affective Computing, Audio and Text Analytics"
    }
  ]
}