{
  "paper_id": "2510.08606v1",
  "title": "Centering Emotion Hotspots: Multimodal Local-Global Fusion And Cross-Modal Alignment For Emotion Recognition In Conversations",
  "published": "2025-10-07T10:11:50Z",
  "authors": [
    "Yu Liu",
    "Hanlei Shi",
    "Haoxun Li",
    "Yuqing Sun",
    "Yuxuan Ding",
    "Linlin Gong",
    "Leyuan Qu",
    "Taihao Li"
  ],
  "keywords": [
    "Emotion recognition in conversations",
    "multimodal affective computing",
    "cross-modal alignment"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Emotion Recognition in Conversations (ERC) is hard because discriminative evidence is sparse, localized, and often asynchronous across modalities. We center ERC on emotion hotspots and present a unified model that detects perutterance hotspots in text, audio, and video, fuses them with global features via Hotspot-Gated Fusion, and aligns modalities using a routed Mixture-of-Aligners; a cross-modal graph encodes conversational structure. This design focuses modeling on salient spans, mitigates misalignment, and preserves context. Experiments on standard ERC benchmarks show consistent gains over strong baselines, with ablations confirming the contributions of HGF and MoA. Our results point to a hotspot-centric view that can inform future multimodal learning, offering a new perspective on modality fusion in ERC.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Emotion Recognition in Conversations (ERC)  [1, 2]  aims to identify the emotion of each utterance in a dialogue from a predefined set of categories. Recent research has advanced ERC in multiple directions, including architectural improvements and multimodal modeling. On the architecture side, early approaches predominantly used recurrent networks or Transformers to model conversational sequences  [3] . Graphbased models  [4]  further captured long-range dependencies by representing utterances and speakers as nodes. On the multimodal side, GBAN  [5]  employed gating to balance aligned and global features, while MMoE  [6]  trained independent expert modules for different interaction patterns and integrated their outputs. Across these approaches, a common practice is to rely on time-aggregated, per-modality representations at the utterance level-textual embeddings and pooled acoustic or visual descriptors-which we refer to as global features.\n\nYet emotions often surface in short, high-intensity bursts: an affect-laden word in text, a brief pitch or energy spike or a laugh in audio, a fleeting eyebrow raise or wry smile in video; we identify these as emotion hotspots. However, when tokens, clips, and frames are treated uniformly, these hotspots are often overlooked as the abundance of neutral or weakly affective content masks scarce emotional cues. Unimodal results support the same insight: focusing on high-information hotspots improves recognition  [7, 8] . In multimodal ERC, however, these hotspots are typically asynchronous rather than co-temporal-prosody may crest before the face reacts, and both may lead or lag the decisive word.\n\nThis asynchrony explains why global pooling or naïve alignment (uniform frame/word matching or early fusion without handling unaligned sequences) tends to dilute local evidence and underperform on misaligned data  [9] . To address it, we detect per-modality emotion hotspots and softly align them around their local maxima rather than enforcing rigid time matches. We realize this with an adaptive hotspot-global fusion, a routed mixture-of-experts crossattention for cross-modal alignment  [10] , and a graph pathway that supplies conversational structure. The two streams are combined for utterance-level prediction. Figure  1  provides an overview of the model architecture, architectural details appear in Section 2. Our main contributions are summarized as follows:\n\n• We introduce Hotspot-Gated Fusion (HGF), an adaptive, modality-agnostic mechanism that identifies and weights localized high-intensity segments (\"emotion hotspots\") within each modality and fuses them with global context. • We employ HGF with Mixture-of-Aligners (MoA) for cross-modal alignment under temporal offsets and a conversational graph branch for dialogue structure; together they align modalities while preserving contextual dependencies. • On standard ERC benchmarks, the model yields consistent performance gains over strong baselines; ablations attribute improvements primarily to HGF and MoA.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Method",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Overview",
      "text": "The complete architecture is illustrated in Figure  2 . Given text, audio, and video for each utterance (with conversational context), our approach centers learning on emotion hotspots-short, high-intensity segments-while preserving helpful global context. We first apply HGF to integrate modality-wise hotspots with utterance-level global features, producing enhanced unimodal sequences. On top of the encoded unimodal representations (denoted later as {X m }), we use MoA to address cross-modal temporal offsets and a cross-modal graph to inject relation-aware dialogue structure. The two streams are concatenated and fed to a token-level classifier. This separation lets HGF surface the most informative local evidence, while MoA and the graph branch handle alignment and conversational dependencies.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Inputs And Hotspot-Gated Fusion",
      "text": "We use three modalities m ∈ {T, A, V}. For each utterance u and modality m, we prepare a global content sequence C m and a hotspot sequence H m that highlights localized emotional salience. For vision, we extract motion-sensitive regions using optical-flow-based features; for audio, we obtain prosodic bursts via emphaclass  [11] ; for text, we concatenate the conversational context and leverage a strong Large Language Model  [12]  to select salient spans.\n\nHotspot-Gated Fusion. HGF performs token-wise gated interpolation between C m and H m :\n\nwhere L is sequence length and ⊙ denotes broadcasted element-wise multiplication. A per-modality encoder Enc m then maps Z m to a shared hidden space: X m = Enc m (Z m ).",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Moa For Cross-Modal Alignment",
      "text": "We introduce MoA for each ordered modality pair (j ← k) to flexibly select and fuse source information from modality k for aligning the target modality j. For each (j ← k) we instantiate E lightweight aligner experts (a single crossattention layer with a small feed-forward):\n\nWe perform utterance-level routing: for each target utterance t, concatenate the target embedding with a mean-pooled source context and produce expert logits,\n\nTopK is implemented as a masked softmax; experts are densely evaluated and linearly mixed (gated sparsification). The expert mixture is fed into a shallow cross-attention block to restore capacity, yielding\n\n(4) For each target modality j, we concatenate aligned representations from all sources and apply self-attention to build a modality memory, then concatenate across all targets to form the final cross-modal representation:\n\nThis retains utterance granularity and composes cross-modal context without length pooling.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Cross-Modal Graph Pathway",
      "text": "We build an utterance-level multi-relational graph whose nodes are (modality m, utterance t). Edges consist of intramodal temporal links within a window and optional sametime cross-modal links; each edge is typed by its temporal direction s ∈ {-1, 0, 1} and the source/target modality pair. A relation-aware GNN produces node embeddings that are then concatenated per utterance across modalities:\n\nHere X graph packs the encoded unimodal sequences as node features, E is the set of typed edges, |M| is the number of modalities, and MultiConcat(•) restores utterance-wise, modality-wise concatenation. This pathway supplies structured conversational context complementary to MoA.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Loss Function",
      "text": "We optimize a sum of the utterance-level task loss and an optional MoA load-balancing regularizer:\n\nHere λ ≥ 0 is a scalar weight (we use λ > 0 by default). The task loss is the negative log-likelihood over utterances:\n\nwhere (b, t) indexes batch b and utterance t, H b,t is the model's utterance representation (concatenating the MoA and graph pathways), and p(• | H b,t ) is produced by the classifier; class-balancing weights can be applied when needed. The load-balancing term encourages uniform expert usage in MoA:\n\nHere E is the number of experts, π t,e is the TopK-masked softmax routing weight, and u e is the average expert usage.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Experiments",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Experimental Setting",
      "text": "Dataset. We evaluate our method on two public multimodal ERC datasets: IEMOCAP  [23]  and CMU-MOSEI  [24] . IEMOCAP contains 151 dialogues (7433 utterances, 12 hours in total) annotated with six emotions: happy, sad, neutral, angry, excited, and frustrated. To reduce label ambiguity, (happy, excited) and (sad, frustrated) are merged into a four-class setting. CMU-MOSEI provides annotations for six basic emotions: happiness, sadness, anger, fear, surprise, and disgust.\n\nEvaluation Metrics. The primary metrics for evaluation are the weighted F1-score (w-F1) and Accuracy (Acc.). The w-F1 metric is computed as the sum K k=1 freq k ×F1 k , where freq k is the relative frequency associated with class k. Accuracy, conversely, measures the percentage of correct predictions over the entire test set.\n\nBaselines and Implementation. Our model extends the CORECT framework  [16]  with modifications while ensuring fair comparison with baseline methods. On IEMOCAP, we conduct both six-class and four-class experiments, while on CMU-MOSEI, we perform binary classification for each emotion category. The implementation is based on PyTorch 2.1.1 and torch-geometric, trained on 4×L20 GPUs. We fix the batch size and random seed across all experiments. Adam optimizer is used, with a ReduceLR scheduler to decrease the learning rate when validation accuracy plateaus.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Overall Results",
      "text": "We compare our model against dataset-specific SOTA baselines, which include RNN-based models and graph-based methods. Tables  1  and 2  show that augmenting the baseline graph pathway with HGF and the MoA yields consistent, architecture-driven gains. HGF injects an emotion hotspot prior at the unimodal level, strengthening emotion-sensitive Running HGF and MoA in parallel with the graph pathway and concatenating their outputs at the utterance level leads to state-of-the-art results, including 72.52 Acc./w-F1 on IEMOCAP (6-way) and 85.90/85.89 on IEMOCAP (4-way), with leading scores on most categories in CMU-MOSEI; the tables provide the detailed per-class trends that underpin these gains.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Ablation Study",
      "text": "As Tabel 1 shows, we ablate modules incrementally atop the original baseline, which already includes the cross-modal graph pathway. We keep the graph pathway intact for fairness to baseline  [16]  and because our design complements-rather than replaces-its relational/temporal modeling.\n\nAdding HGF provides the first substantial improvement by amplifying emotion hotspot-guided local cues and reducing reliance on aggressive pooling, with table evidence of stronger Neutral/Excited performance and overall stability. Adding MoA on top delivers further, steady gains by selectively routing cross-modal evidence to the target utterance and sharpening alignment where it is most discriminative, reflected in the consistent transition from baseline to +HGF to +HGF+MoA in both aggregate and class-wise results.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Conclusion",
      "text": "We define emotion hotspots-short, high-intensity bursts within each modality-and place them at the center of ERC. Yet hotspots across modalities seldom peak simultaneously, making alignment essential. To resolve cross-modal misalignment while preserving context, we couple an adaptive Hotspot-Gated Fusion (HGF) for hotspot-global integration with a routed Mixture-of-Aligners (MoA) cross-attention for alignment, and add a conversational graph pathway to encode dialogue structure. The resulting model attains state-of-theart results on IEMOCAP (6-way and 4-way) and ranks best on most emotions in CMU-MOSEI while remaining competitive elsewhere. Ablations show that HGF yields substantial gains that are further amplified by MoA-with the largest improvements on Excited and Neutral-and expert routing offers interpretability, underscoring a robust, general approach to multimodal ERC.",
      "page_start": 5,
      "page_end": 5
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The model locates multimodal emotion hotspots and",
      "page": 1
    },
    {
      "caption": "Figure 2: Per-utterance inputs from text (T), audio (A), and video (V) are first combined by HGF and then encoded into a shared",
      "page": 3
    }
  ],
  "tables": [
    {
      "caption": "Table 1: Comparison with previous state-of-the-art methods on IEMOCAP (6-way). The bolded results indicate the best",
      "data": [
        {
          "Methods": "",
          "IEMOCAP(6-way)": "Happy\nSad\nNeutral\nAngry\nExcited\nFrustrated"
        },
        {
          "Methods": "DialogueRNN [3]\nDialogueGCN [4]\nDialogueCRN [13]\nMMGCN [14]\nCOGMEN [15]\nCORECT [16]\nM3NET [17]\nCMERC [18]",
          "IEMOCAP(6-way)": "33.18\n78.80\n59.21\n65.28\n71.86\n58.91\n47.10\n80.88\n58.71\n66.08\n70.97\n61.21\n51.59\n74.54\n62.38\n67.25\n73.96\n59.97\n45.45\n77.53\n61.99\n66.70\n72.04\n64.12\n55.76\n80.17\n63.21\n61.69\n74.91\n63.90\n56.19\n82.11\n63.97\n66.49\n69.12\n61.91\n56.77\n82.26\n68.94\n66.67\n78.06\n60.32\n60.73\n69.51\n67.02\n81.89\n71.65\n77.45"
        },
        {
          "Methods": "baseline\nbaseline + HGF\nbaseline + HGF + MoA(Ours)",
          "IEMOCAP(6-way)": "56.88\n79.19\n64.34\n64.62\n70.59\n63.23\n56.69\n80.33\n70.48\n68.91\n78.25\n63.45\n83.67\n72.35\n79.13\n58.97\n69.23\n66.94"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 1: Comparison with previous state-of-the-art methods on IEMOCAP (6-way). The bolded results indicate the best",
      "data": [
        {
          "Methods": "",
          "IEMOCAP(4-way)": "Acc.(%)\nw-F1(%)",
          "CMU-MOSEI (w-F1(%))": "Happiness\nSadness\nAngry\nFear\nDisgust\nSurprise"
        },
        {
          "Methods": "bc-LSTM [19]\nCHFusion [20]\nMultilouge-Net [21]\nTBJE [22]\nCOGMEN [15]\nCORECT [16]",
          "IEMOCAP(4-way)": "75.20\n75.13\n76.59\n76.80\n-\n-\n-\n-\n82.29\n82.15\n82.93\n82.92",
          "CMU-MOSEI (w-F1(%))": "-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n67.84\n65.34\n67.03\n84.47∗\n74.91\n85.66∗\n65.91\n68.54\n70.78\n84.47∗\n80.23\n85.66∗\n68.73\n69.11\n74.20\n84.47∗\n80.23\n85.66∗\n86.00\n69.24\n70.67\n76.36\n84.47∗\n82.84"
        },
        {
          "Methods": "Ours",
          "IEMOCAP(4-way)": "85.90\n85.89",
          "CMU-MOSEI (w-F1(%))": "71.04\n72.34\n76.43\n84.81\n85.21\n85.84"
        }
      ],
      "page": 4
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Emotion recognition in conversations: A survey focusing on context, speaker dependencies, and fusion methods",
      "authors": [
        "Yao Fu",
        "Shaoyang Yuan",
        "Chi Zhang"
      ],
      "year": "2023",
      "venue": "Emotion recognition in conversations: A survey focusing on context, speaker dependencies, and fusion methods"
    },
    {
      "citation_id": "3",
      "title": "A systematic review on affective computing: emotion models, databases, and recent advances",
      "authors": [
        "Yan Wang",
        "Wei Song",
        "Wei Tao"
      ],
      "year": "2022",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "4",
      "title": "Dialoguernn: An attentive rnn for emotion detection in conversations",
      "authors": [
        "Navonil Majumder",
        "Soujanya Poria",
        "Devamanyu Hazarika"
      ],
      "year": "2019",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "5",
      "title": "DialogueGCN: A graph convolutional neural network for emotion recognition in conversation",
      "authors": [
        "Deepanway Ghosal",
        "Navonil Majumder",
        "Soujanya Poria"
      ],
      "year": "2019",
      "venue": "Proceedings of the EMNLP-IJCNLP"
    },
    {
      "citation_id": "6",
      "title": "Group gated fusion on attention-based bidirectional alignment for multimodal emotion recognition",
      "authors": [
        "Pengfei Liu",
        "Kun Li",
        "Helen Meng"
      ],
      "year": "2020",
      "venue": "Group gated fusion on attention-based bidirectional alignment for multimodal emotion recognition"
    },
    {
      "citation_id": "7",
      "title": "MMoE: Enhancing multimodal models with mixtures of multimodal interaction experts",
      "authors": [
        "Haofei Yu",
        "Zhengyang Qi",
        "Lawrence Jang"
      ],
      "year": "2024",
      "venue": "Proceedings of the EMNLP"
    },
    {
      "citation_id": "8",
      "title": "Multimodal emotion recognition with automatic peak frame selection",
      "authors": [
        "Sara Zhalehpour",
        "Zahid Akhtar",
        "Cigdem Erdem"
      ],
      "year": "2014",
      "venue": "2014 IEEE International Symposium on Innovations in INISTA Proceedings"
    },
    {
      "citation_id": "9",
      "title": "Speech emotion recognition using machine learning -a systematic review",
      "authors": [
        "Talen Samaneh Madanian",
        "Olayinka Chen",
        "Adeleye"
      ],
      "year": "2023",
      "venue": "Intelligent Systems with Applications"
    },
    {
      "citation_id": "10",
      "title": "Multimodal transformer for unaligned multimodal language sequences",
      "authors": [
        "Yao-Hung Hubert Tsai",
        "Shaojie Bai",
        "Paul Liang"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Annual Meeting of the ACL"
    },
    {
      "citation_id": "11",
      "title": "Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity",
      "authors": [
        "William Fedus",
        "Barret Zoph",
        "Noam Shazeer"
      ],
      "year": "2022",
      "venue": "Journal of Machine Learning Research"
    },
    {
      "citation_id": "12",
      "title": "Emphassess: a prosodic benchmark on assessing emphasis transfer in speech-to-speech models",
      "authors": [
        "Maureen De Seyssel",
        "Antony D' Avirro",
        "Adina Williams"
      ],
      "year": "2023",
      "venue": "Emphassess: a prosodic benchmark on assessing emphasis transfer in speech-to-speech models",
      "arxiv": "arXiv:2312.14069"
    },
    {
      "citation_id": "13",
      "title": "Deepseek-v3 technical report",
      "authors": [
        "Aixin Liu",
        "Bei Feng",
        "Bing Xue"
      ],
      "year": "2024",
      "venue": "Deepseek-v3 technical report",
      "arxiv": "arXiv:2412.19437"
    },
    {
      "citation_id": "14",
      "title": "DialogueCRN: Contextual reasoning networks for emotion recognition in conversations",
      "authors": [
        "Dou Hu",
        "Lingwei Wei",
        "Xiaoyong Huai"
      ],
      "year": "2021",
      "venue": "Proceedings of the ACL"
    },
    {
      "citation_id": "15",
      "title": "Mmgcn: Multimodal graph convolution network for personalized recommendation of micro-video",
      "authors": [
        "Yinwei Wei",
        "Xiang Wang",
        "Liqiang Nie"
      ],
      "year": "2019",
      "venue": "Proceedings of the 27th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "16",
      "title": "COGMEN: COntextualized GNN based multimodal emotion recognitioN",
      "authors": [
        "Abhinav Joshi",
        "Ashwani Bhat",
        "Ayush Jain"
      ],
      "year": "2022",
      "venue": "Proceedings of the ACL"
    },
    {
      "citation_id": "17",
      "title": "Conversation understanding using relational temporal graph neural networks with auxiliary cross-modality interaction",
      "authors": [
        "Cam-Van Thi Nguyen",
        "Anh-Tuan Mai",
        "The-Son Le"
      ],
      "year": "2023",
      "venue": "Proceedings of the EMNLP"
    },
    {
      "citation_id": "18",
      "title": "Multivariate, multi-frequency and multimodal: Rethinking graph neural networks for emotion recognition in conversation",
      "authors": [
        "Feiyu Chen",
        "Jie Shao",
        "Shuyuan Zhu"
      ],
      "year": "2023",
      "venue": "2023 IEEE/CVF CVPR"
    },
    {
      "citation_id": "19",
      "title": "Multimodal emotion recognition calibration in conversations",
      "authors": [
        "Geng Tu",
        "Feng Xiong",
        "Bin Liang"
      ],
      "year": "2024",
      "venue": "Proceedings of the 32nd ACM International Conference on Multimedia"
    },
    {
      "citation_id": "20",
      "title": "Context-dependent sentiment analysis in user-generated videos",
      "authors": [
        "Soujanya Poria",
        "Erik Cambria",
        "Devamanyu Hazarika"
      ],
      "year": "2017",
      "venue": "Proceedings of the ACL"
    },
    {
      "citation_id": "21",
      "title": "Multimodal sentiment analysis using hierarchical fusion with context modeling",
      "authors": [
        "Navonil Majumder",
        "Devamanyu Hazarika",
        "Alexander Gelbukh"
      ],
      "year": "2018",
      "venue": "Knowledge-based systems"
    },
    {
      "citation_id": "22",
      "title": "Multilogue-net: A context-aware RNN for multi-modal emotion detection and sentiment analysis in conversation",
      "authors": [
        "Aman Shenoy",
        "Ashish Sardana"
      ],
      "year": "2020",
      "venue": "Second Grand-Challenge and Workshop on Multimodal Language"
    },
    {
      "citation_id": "23",
      "title": "A transformer-based joint-encoding for emotion recognition and sentiment analysis",
      "authors": [
        "Jean-Benoit Delbrouck",
        "Noé Tits",
        "Mathilde Brousmiche"
      ],
      "year": "2020",
      "venue": "Second Grand-Challenge and Workshop on Multimodal Language"
    },
    {
      "citation_id": "24",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "Carlos Busso",
        "Murtaza Bulut",
        "Chi-Chun Lee"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "25",
      "title": "Multimodal language analysis in the wild: CMU-MOSEI dataset and interpretable dynamic fusion graph",
      "authors": [
        "Amirali Bagher Zadeh",
        "Paul Liang",
        "Soujanya Poria"
      ],
      "year": "2018",
      "venue": "Proceedings of the ACL"
    }
  ]
}