{
  "paper_id": "2212.08744v3",
  "title": "Toward Cross-Subject And Cross-Session Generalization In Eeg-Based Emotion Recognition: Systematic Review, Taxonomy, And Methods",
  "published": "2022-12-16T22:48:37Z",
  "authors": [
    "Andrea Apicella",
    "Pasquale Arpaia",
    "Giovanni D'Errico",
    "Davide Marocco",
    "Giovanna Mastrati",
    "Nicola Moccaldi",
    "Roberto Prevete"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "A systematic review on machine-learning strategies for improving generalizability (cross-subjects and cross-sessions) electroencephalography (EEG) based in emotion classification was realized. In this context, the nonstationarity of EEG signals is a critical issue and can lead to the Dataset Shift problem. Several architectures and methods have been proposed to address this issue, mainly based on transfer learning methods. 418 papers were retrieved from the Scopus, IEEE Xplore and PubMed databases through a search query focusing on modern machine learning techniques for generalization in EEG-based emotion assessment. Among these papers, 75 were found eligible based on their relevance to the problem. Studies lacking a specific cross-subject and cross-session validation strategy and making use of other biosignals as support were excluded. On the basis of the selected papers' analysis, a taxonomy of the studies employing Machine Learning (ML) methods was proposed, together with a brief discussion on the different ML approaches involved. The studies with the best results in terms of average classification accuracy were identified, supporting that transfer learning methods seem to perform better than other approaches. A discussion is proposed on the impact of (i) the emotion theoretical models and (ii) psychological screening of the experimental sample on the classifier performances.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Emotions are our internal compass and play a primary role in learning, reasoning, decision-making processes, and communication between individuals. The Information and Communication Technology (ICT) sector's interest in emotions has grown tremendously in recent years, shaping the concept of affective computing, an emerging field aimed at monitoring and predicting emotions in order to improve human-computer interaction  Cambria et al. (2017) ; for instance, the introduction of affective loops makes it possible to implement increasingly adaptive human-machine interfaces and virtual assistants tailored to users  Saganowski et al. (2020) , or the outputs of emotion monitoring systems, in the healthcare context, can be useful in the treatment of psychological disorders based on emotional deficits, in autism  Feng et al. (2018) , in the improvement of wellbeing  Healy et al. (2018) , and in stress containment  Saganowski (2022) .\n\nIn particular, in this context, there is a growing interest in the literature for Brain-Computer Interface (BCI) systems based on EEG signals  Torres et al. (2020) . In fact, the number of annual scientific publications indexed on Scopus database on the topic of EEG-based emotion recognition shows an exponential growth trend (see Fig.  1 ).\n\nA critical issue underlying the processing and classification of EEG signals is their inherent variability among different subjects or different acquisition times (i.e. sessions) of the same subject, since the EEG signal is usually stochastic and stationary only for short intervals (generally ranging from a few seconds to minutes)  Inouye et al. (1995) ;  Im (2018) ;  Sörnmo and Laguna (2005) . More in detail, the EEG signal is not a Wide Sense Stationary signal  Cao and Slobounov (2011) . This characteristic of non-stationarity implies a variation in the temporal and spectral characteristics of the EEG signal over time. This is an open issue in the literature leading to a loss of generalizability for classification systems across subjects (inter-subject task) and, for the same subject, across different sessions (intra-subject task)  Nasiri and Clifford (2020) .\n\nData-driven approaches using Machine Learning (ML) are often employed at multiple levels in the EEG signal processing pipeline to pursue the classification of emotional states and their generalization across subjects and sessions.\n\nCurrently, the literature shows increasing use of modern machine learning strategies, adopting deep neural networks and transfer learning-based approaches, such as domain adaptation, domain generalization and/or hybrid methods  Zhao et al. (2021) . This paper proposes a systematic review on the use of machine learning to improve generalizability capabilities in EEG-based emotion recognition systems across different subjects and sessions.\n\nAs will be discussed in detail in the next section, several surveys have been proposed in recent years, gathering and discussing the main directions of the literature on this research topic. However, to the best of our knowledge, a focus on the application of ML methods to improve the inter/intra-subjective generalization performance of EEG-based emotion recognition is missing in the literature.\n\nThe rest of the paper is organized as follows: Section 2 reviews related works, with reference to recent surveys carried out on this specific topic. Section 3 presents a theoretical background on EEG, with a first part focused on BCIs for emotion recognition and a second part on ML for emotion recognition. Section 4 presents the used search queries and the paper selection process according to the PRISMA method  Liberati et al. (2009) . Section 5 presents the results of the review, proposing a taxonomy of the ML methods currently proposed in the selected papers, discussing the ML methods with respect the proposed taxonomy. A statistical analyses of the results was reported. Section 6 aims to discuss the results obtained, reporting the most promising lines of research and approaches that have emerged and highlighting possible future directions in this area. Finally, Section 7 draws conclusions.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Related Works",
      "text": "Several reviews have been conducted in recent years on this line of research.  Alarcao and Fonseca Alarcao and Fonseca (2017)  focus on the generic topic of EEG-Emotion Recognition, presenting a review of papers published in the period from 2009 to 2016. The survey appears interesting in focusing on the different stages of the emotion recognition process from EEG signals and proposing a criterion for assessing the quality of the papers by applying a set of well-known guidelines  (Brouwer's recommendations Brouwer et al. (2015) ). However, there is no in-depth analysis on the issue of inter/intra-subject generalisation, nor the EEG-nonstationarity problem is addressed. Other reviews  Lotte et al. (2018) ;  Wu et al. (2020)  analyse works about the EEG-based classification methods, but without focusing on the emotion domain.  Wu et al. Wu et al. (2020) , offer a non-systematic review focusing on the affective BCIs (aBCIs), but without an in-depth analysis on the emotion recognition problem. The study proposed in  Suhaimi et al. (2020)  deals with the EEG-inter/intra-subject variability problem as a specific topic on which to focus future research efforts, without exploring the problem in-depth. Recently, Li and colleagues  Li et al. (2021b)  published a review focusing on the topic of EEG-based emotion recognition and discussing the importance of transfer learning. While offering some interesting results, it does not present itself as a systematic review (only 18 studies were reported without PRISMA methodology to collect them). Thus, differently from the cited works, a systematic literature review focused on the inter/intra-subject generalization on EEG-based emotion recognition systems and the use of modern ML-based methods as a possible solution is proposed in this paper.\n\n3 Theoretical Background",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Bci For Emotion Recognition",
      "text": "The emotional states can be recognized through several biosignals. In particular, brain signals have received increasing attention from the scientific community. In fact, the EEG signal resulted particularly effective for emotion recognition due to the high temporal resolution and its non-invasiveness. The EEG signal has a frequency range between [0.01, 100.00] Hz, an amplitude varying typically within the range  [-100, 100]  µV , and a power spectral density higher at lower frequencies  Daly et al. (2012) . Five background rhythms are present in the EEG and can be classified into different frequency bands: delta [0.5, 4.0] Hz, theta  [4, 7]  Hz, alpha  [8, 13]  Hz, beta  [14, 30]  Hz, and gamma  [30, 100]  Hz.\n\nThe 10-20 International Positioning System is an internationally recognized method to place the electrodes on the scalp  Acharya et al. (2016)  for the EEG signal recording. The method allows to maintain a standardized EEG electrodes placement proportional to the scalp size and shape in order to preserve the relationship between each location and the underlying brain area. In order to obtain a high-quality EEG, a substantial requirement is the use of high performance electrodes. The electrodes need to ensure a good and constant electrical contact with the skin and therefore need low impedance properties  Casson (2019) . The electrode-skin contact can either be ensured by adding a conductive gel between the electrode and the skin or by increasing the contact surface that ensures electrical contact. Recently, besides wet electrodes, dry electrodes are employed for the EEG signal recording. A good signal quality and comparable performances with respect to wet electrodes are achieved using dry electrodes  Lopez-Gordo et al. (2014) .\n\nBesides the quality of the EEG signal, the emotion induction methods and the eliciting stimuli employed represent a crucial point for the effectiveness of the emotional elicitation. Facial and body movements, recall of past events, odors, images, film clips, and music are techniques currently used in laboratories for inducing emotions. Current literature reports that film clips, images, and music are particularly effective to elicit emotions  Westermann et al. (1996) ;  Lang (2005) . The use of images over other kind of stimuli represents a great advantage insofar as the images are standardized stimuli. Datasets of images were exper-imentally validated (e.g., International Affective Picture System -IAPS  Lang (2005) , Open Affective Standardized Image Set -OASIS  Kurdi et al. (2017) , and Geneva Affective Picture Database -GAPED Dan-Glauser and Scherer (  2011 )). There are several publicly-available databases for EEG-based emotion recognition (e.g.,  DEAP Koelstra et al. (2011) ,  SEED Zheng and Lu (2015) , and DREAMER  Katsigiannis and Ramzan (2017) ). Each dataset is characterized by different physiological signals and a well-established experimental setup (in terms of stimulus sources, emotional theory adopted, number of subjects, and psychometric metrological references). For a comprehensive description of the various available datasets see  Li et al. (2021b) .\n\nIn case of self-produced EEG data, the preprocessing stage is fundamental to filter out the noise from the brain activity signal. Some steps are often helpful to achieve a successful EEG signal preprocessing: (i) line noise removal, (ii) referencing, (iii) bad channels removal, and (iv) artifacts removal. They can be summarized as:\n\n• Line noise removal: line noise is an artifact which contaminates the gamma band of the EEG signal, specifically at 50 Hz (60 Hz in the USA)  Im (2018) . Conventional filters such as lowpass filters with cutoff frequencies between 50 Hz and 70 Hz can be used to remove this artifact and reduce the noise at higher frequencies, notch filters must be employed to reject the 50 Hz power supply.\n\n• Referencing: when acquiring the EEG signals, the voltage in a specific scalp area is measured with respect to a reference electrode. If the reference electrode is subjected to artifacts, all electrodes are also affected.\n\nThe re-referencing allows to minimize the impact of the reference electrode by subtracting a reference channel from the original EEG channels. Most used reference electrodes are the mastoid channel, the EEG signal at a specific channel, the average of the mastoid channels, or the average of all EEG channels  Lepage et al. (2014) . To avoid lateralization effects  Lepage et al. (2014) , also Cz and FCz electrodes are often used as references.\n\n• Bad channels removal: EEG signal of one or more channels can be also contaminated by the noise due to a poor contact between the electrode and the scalp. Therefore, it is necessary to detect the noisy or bad channels and remove them. Methods to find out bad channels are the visual inspection of each single channel, the dispersion and the correlation criteria da Cruz et al. (  2018 ). Once the EEG signal has been preprocessed, it is usually divided into epochs, and a feature extraction process is then applied. EEG features can be categorized into three domains, namely time, frequency and time-frequency.   (2014) . Good results in the recognition of emotional states can be achieved by using entropy-based features, i.e., approximate, sample, differential, and wavelet entropy  Phadikar et al. (2019) . Higher-order crossing (HOC), the fractal dimension, and the Non-Stationary Index (NSI)  Patil et al. (2016) ;  Nan and Jinghua (1988) ;  Kroupi et al. (2011)  are further time domain feature often used for the EEG analysis.\n\n• Frequency domain: the most used feature is the power spectral density (PSD). PSD is the signal power in the unit frequency band  Wang et al. (2011) . Other representative features of different emotional states involving the PSD are: (i) logarithm, (ii) maximum, (iii) minimum and, (iv) standard deviation of the power spectrum.\n\n• Time-frequency domain: the time-frequency analysis (TFA) allows to observe spectrum changes with timeZhang (2019). The short-time Fourier transform (STFT),the continuous wavelet transform (CWT), the discrete wavelet transform (DWT)  Hernández et al. (2018) , matching pursuit and empirical mode decomposition are the most used methods to extract timefrequency features.\n\nOften, the number of EEG features is very high, therefore a feature selection strategy is required  Jenke et al. (2014) . Another critical point is represented by the large number of EEG channels often used for the signal acquisitions. A high number of channels can lead to high computational complexity. Therefore, the selection of the most informative EEG channels can be crucial  Apicella et al. (2022) .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Machine Learning For Emotion Recognition",
      "text": "After that the EEG have been properly preprocessed and a proper set of features has been extracted, the data are ready to be fed to a supervised ML system. The typical pipeline of a ML framework applied to an EEG emotion recognition task is reported in Fig.  2 .\n\nIn the current literature, a large part of research works proposed methods framed into Transfer Learning approach to tackle emotion recognition tasks. The motivation of this trend can be summarized as follows: in classical supervised ML a set of already labeled data has to be available. This implies that in the EEG emotion recognition tasks a set of EEG signals recorded from one or more subjects has to be mapped with the emotion felt during the acquisition. Labelled data can be then used to train the ML system, generating a ML model able to classify the input data. Once the ML model has been obtained, new unlabelled data can be fed to the ML model to obtain the predicted emotion/class. To evaluate the trained model, it is a good practice to reserve a part of the labelled data outside from the training stage. These data can then be used to evaluate the final model predictions using suitable performance metrics (e.g. accuracy). However, a standard hypothesis of the traditional ML methods states that all the available data, no matter if involved in the training process or not, come from the same probability distribution. Due to the characteristics of the EEG data, this assumption results not always verified in the EEG signal. Indeed, the EEG signals acquired from a subject can be strongly different from the one acquired from another subject, even under the same conditions  Im (2018) . This can also happen for EEG signals acquired from the same subject in different times/sessions, leading to loss generalisation performances in cross-subject/session problems. In the current literature, this problem was initially addressed considering the availability of further unlabelled data belonging to the target subject/session which can help the training of the ML model (transductive learning approaches). However, these methods do not make any consideration about the data distributions. In fact, the training EEG data can be sensibly different in terms of probability distribution(s) from the data used outside the training stage. In ML literature, this can be viewed an instance of the Dataset Shift problem  Quinonero-Candela et al. (2008) . In a nutshell, Dataset Shift arises when the standard ML assumption is not verified, so the distribution of the training data differs from the data distribution used outside of the training stage. The idea that the training data come from different probability distribution(s) respect to the data used outside of the training stage is the main hypothesis of the transfer learning approaches.\n\nIn the last years, several architectures and methods have been proposed to address the dataset shift problem following the base assumptions of transfer learning, and different categorizations of the proposed methods have been reported  Pan and Yang (2009) ;  Ganin and Lempitsky (2015) . One of the first and most important review on Transfer Learning methods was proposed in  Pan and Yang (2009) , however several new strategies (e.g., Domain Generalization-based works) have been proposed in the following years.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Papers Selection Method",
      "text": "The present literature review took into account the guidelines for systematic literature reviews presented by  Kitchenham (Kitchenham (2004) ), covering also the use of PRISMA (Preferred Reporting Items for Systematic Reviews and Meta-Analyses) recommendations in order to transparently report the document extraction process  (Liberati et al. (2009) ). The survey was conducted covering the period between January 2010 to March 2022, using the following databases: Scopus, IEEE (Institute of Electrical and Electronics Engineers), Xplore, and PubMed.\n\nIn accordance with PRISMA's recommendations, the pipeline consists in four successive steps: 'Identification', 'Screening', 'Eligibility' and, finally, 'Inclusion', which considerably narrowed down the amount of surveyed work. For the initial identification of the articles, the following query was used in all pre-dicted data sources, taking into account titles and abstracts: EEG AND (Emotion OR Preference) AND (\"Domain Adaptation\" OR \"Domain Generalisation\" OR \"Transfer Learning\" OR \"Adversarial\" OR \"Transfer\" OR \"Cross Session\" OR \"Cross Subject\" OR \"Cross Gender\" OR \"Non-stationary EEG\").\n\nFrom the first phase, 418 articles were collected. Then, as a first prescreening process, the following criteria are used to exclude papers from the review: duplicated articles, not peer-reviewed articles, and not written in English articles. For all articles that survived the screening stage, a careful examination of the full text was carried out. In a conclusive screening, the following papers were excluded: (a) all articles in which the problem of generalisability was not explicitly stated as a peculiar topic, (b) studies in which a cross-subject/session validation strategy was not clearly envisaged, (c) studies oriented towards a 'multimodal fusion', i.e. aimed at corroborating the EEG signal-based classification with other biosignals, (d) studies not specifically focusing on emotion recognition. Thus, 75 papers survived and were included in the review analysis. The complete flow diagram of the systematic review process using the PRISMA approach is presented in Fig.  3 .",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "Results",
      "text": "In this paper, a categorisation of the reviewed articles with respect ML methods which do not satisfy the standard ML hypothesis is proposed (see Figure  4 ). Among all the reviewed papers, a subset of them leveraging on the classical transductive learning approaches, i.e. considering a set of unlabelled data coming from the target subject/session in the training stage, is isolated. On the other side, the papers dealing with the cross-subject/session problem as an instance of the dataset shift problem were considered. These methods are known in literature as transfer learning methods, which can be further divided in:\n\n• Unsupervised and Semisupervised Domain Adaptation methods: they consider both the data coming from different subjects/sessions and unlabelled data coming from a target subject/session during the training stage. Therefore, they can be considered as an intersection between the transductive and transfer Learning methods;\n\n• Supervised Domain Adaptation methods: these methods rely on the hypothesis that both labelled and unlabelled data from the target subject/session are available during the training stage;\n\n• Domain Generalization methods: in these methods data from several probability distributions are available and can be used during the training, but no data from the target subject/session is used during the training stage.\n\nOn the basis of the above categorization shown in Fig.  4 , in Tab. 1 all papers included in the review were reported, indicating if belonging to the proposed taxonomy or to classical ML methods. Moreover, for each research study several information is presented, such as the type of generalisation (cross-subject, cross-session, or cross-device), the EEG dataset, the adopted classifier (whether proposed as a personal contribution or adopted from the literature), and the validation strategy. Studies in which the description of the experimental setup was not sufficiently clear, especially in terms of validation strategies, were voluntarily omitted from the table for reproducibility issues.\n\nIn the following of this section, the reviewed papers are discussed in according with the Tab. 1.",
      "page_start": 9,
      "page_end": 10
    },
    {
      "section_name": "Classical Machine Learning Approaches",
      "text": "A model trained on a set of EEG data acquired from a given subject at a specific time (or during a specific session) could not work as expected in classifying EEG signal acquired from a different subject or from the same subject at different times. In other words, the model has poor generalization performance. To deal with this problem, several solution based on Machine Learning approach have been proposed over the years. The most common approaches proposed proper strategy of feature transformations or feature selection. The former wants to be transformations of the data able to hold only the most useful information with the hope that these are shared between all the subjects, while the latter are methods to select only useful information from the input signals without changing it. The feature extraction/selection is usually one of the first step of a machine learning pipeline, where the training data are transformed before being fed to the training stage of a machine learning model. These methods adopted the classical machine learning framework, that is that only the training data are available during the training stage, without any knowledge of the effective data on which the model will be effectively used. This can be viewed as a consequence of the the starting hypothesis of the traditional ML methods stating that all the available data, no matter if used in the training process or not, come from the same probability distribution. Under this hypothesis, the training data are enough to generalise over all the possible data.\n\nThe underlying hypothesis is that a proper EEG data transformation is enough to allow a ML model to generalise well, independently from the fact the data belongs to the same subject/session used during the training or not. Going deeper, in a ML problem on EEG data the feature extraction and selection process can be made considering two different aspects: i) the EEG signal or ii) the electrodes. In the first case, a proper transformation or selection strategy for the EEG signal is made. The reviewed literature proposed different works that analyse if several known feature extraction methods are suitable to generalise across several datasets Rayatdoost and Soleymani (2018);  Li et al. (2018b) . In particular, in  Rayatdoost and Soleymani (2018)  the authors investigated the robustness of emotion recognition methods across different experimental conditions, subjects and datasets.\n\nIn  Yang et al. (2019) ;  Jiang et al. (2019a)  Sequential Backward Selection (SBS) was applied to find a good set of features that generalise across different subjects. To find the best subset of features, SBS decreases the number of features in an iterative way measuring, at each step, the obtained accuracy on a given classifier (SVM in  Yang et al. (2019) , Decision Trees in  Jiang et al. (2019a) ). SBS method is adopted to exploit the significant differences between the classes. A leave-one-subject-out verification strategy was employed on DEAP and SEED datasets in  Yang et al. (2019) , while  Jiang et al. (2019a)  validates its results on DEAP and self-produced data.\n\nIn  Cai et al. (2019) ;  Yin et al. (2017)  a family of Transferable Recursive Feature Elimination (TRFE) methods are used to make a set of EEG features steadily distributed among all the training subjects, therefore removing the EEG features resulting not generic for all users. The proposed feature selector is validated using SVMs as classifiers on DEAP dataset both in within-subject and cross-subject ways. In  Zhang and Yin (2020)  Cross-subject Recursive Feature Elimination (C-RFE) is exploited to rank the features in order of importance with the aim of removing the features giving a low contribution to the classification. The method is validated on EEG data fed to SVMs.\n\nIn  Liu et al. (2020)  an evolution of the well-known Differential Entropy features is proposed. The Dynamic Differential Entropy (DDE) features take into account also the time-domain instead of only the frequency domain extracted by the classical DE. The goal is to maximise the difference between classes minimising at the same time the difference within classes, learning a set of common characteristics across different subjects.\n\nIn  Li et al. (2019c)  a latent representation of the EEG data from SEED and DEAP is learned through a Variational Auto Encoder (VAE) and then classified using a LSTM. VAEs start from the hypothesis that all the data are generated by a random process involving latent variables. A VAE is usually trained to encode the input data into a latent representation and then mapping it to a reconstructed version of the data.  Li et al. (2019c)  hypothesises that there exists learnable intrinsic features shared across several subjects EEG signals taking part in emotional processes. These intrinsic features can be encoded by the VAE latent representations. The power of VAE to represent latent EEG factors is also investigated in  Li et al. (2020c) , together with classical Auto-Encoders (AEs) and Restricted Boltzmann Machines (RBMs). Final classification is made with an LSTM and the generalisation performances are evaluated in LOSO mode on DEAP and SEED dataset.\n\nIn  Pandey and Seeja (2019)  the cross-subject problem is tackled using Variational Mode Decomposition (VMD) as feature extraction technique. The system is validated in an hold-out way without any intersections between subjects' data in the training and the test set using a DNN as emotions classifier. Despite the encouraging results reported, no reason about why the proposed system works well in a cross-subject approach seems to be provided. In  Chen et al. (2021c) ;  Fernandez et al. (2021) ; Arevalillo-Herráez et al. (  2019 ) is shown that the normalisation scheme used to preprocess the EEG data can affect the cross-subject performances.\n\nIn  Chen et al. (2021c)  several normalisation methods were applied following two different schemes: i) All-subjects, where the whole dataset was normalised, ii) Single-subject, where the normalisation is made individually for each subject. The All-subject schema is the most common method used to mitigate the impact of each data values on the entire dataset. Single-subject, instead, consider each subject individually, applying normalisation on each subject. The authors empirically shown on SEED dataset that Single-subject Z-score performs better in a EEG emotion recognition problem respect to other normalisation schemes as min-max normalisation. On the same data, in  Fernandez et al. (2021)  the authors apply single-subject Z -score normalisation after each neural network layer (Stratified Normalisation).\n\nIn Arevalillo-Herráez et al. (  2019 ) a simple transformation of the original data is proposed. It consists in using binary features having 0 and 1 as components values based on the fact that the feature is lower or higher than the median feature value. This leads to a more effective reduction of the subjectdependent part of the EEG signal.\n\nInstead, considering the electrodes in place of the EEG signals, different channels selection strategies searching for a robust set of channels across the subjects was proposed in  Gupta et al. (2018) ;  Zhang et al. (2016) . To achieve EEG-based cross-session emotion recognition, in  Peng et al. (2021)  the author propose a way to learn the importance of the EEG channels and features to separate discriminative features from the noisy and redundant ones. The proposed strategy is evaluated on pairs of a-priori chosen sessions.\n\nIn  Tian et al. (2021)  a neural network to classify emotion by EEG signals is proposed. The proposed model introduces a channel-attention layer to select the most important channels for a set of emotions. Notably, the different personalities across the subjects are taken into account, grouping together the subjects having similar personalities. Indeed, a different model for each group was trained. Validation is made on the ASCERTAIN dataset. This dataset results particularly suited for this task, since it links personality and emotional state with physiological reactions.\n\nIn other works, the structure of the electrodes is taken into account and modelled as a graph. Graph representation methodology resulted effective to model structured data achieving significant performance in many applications, included EEG emotion signal processing  Song et al. (2018) . GNNs are useful to retain the spatial structure of the electrodes disposition. Usually, the graph structure is fixed and given a priori following the spatial disposition of the electrodes on the scalp. Instead, Dynamical Graph Convolutional Neural Networks (DGCNN, Song et al. (  2018 )) and Self-Organized Graph Neural Network (SOGNN,  Li et al. (2021a) ) organises the graph structure leveraging on the input brain signals rather than on a predefined graph structure. The resulting graph can be processed by the graph convolutional layers to extract the more suitable features for emotion recognition. The features obtained are also tested in cross-subject scenarios.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Transductive Methods",
      "text": "Transductive methods  Vapnik (2006)  start from the hypothesis that the unlabeled test data are available in the training stage (no assumption about the distribution of the data, differently from the DA methods). The idea is that in several problems there is only a specific set of data (usually corresponding to the test set) to classify, and it is available at training time. Note that in standard ML approach the goal is to generalize on new unseen data, by contrast in transductive learning the goal is to correctly classify the test set only. The Transductive SVM (TSVM) is an example of a transductive method. Described in  Joachims et al. (1999) , differently from classical SVMs that leverages only on labelled data, TSVMs exploit also unlabelled test data to find the best decision boundary between the classes. In other words, the target data is viewed as an additional set of information about the data. One of the main drawback of TSVM is that an estimation of the number of elements for each class in the test set is needed. Progressive TSVM (PTSVM,  Chen et al. (2003) ) tries to resolve this problem progressively labeling the unlabeled data during the training instead of classifying it as a whole at the same time.\n\nThe only study collected in this review using explicitly transdutive methods is  Yang et al. (2020) , where the PTSVM generalisation power across several sessions of EEG data acquired from different subjects is validated.",
      "page_start": 11,
      "page_end": 13
    },
    {
      "section_name": "Transfer Learning Methods",
      "text": "Transfer Learning methods are based on the concepts of Domain and Task. Following the survey of Pan et al.  Pan and Yang (2009) , a Domain can be defined as a set D = {F, P (X)} where F is a feature space and P (X) is the marginal probability distribution of a specific dataset X = {x 1 , x 2 , . . . , x n } ∈ F . Instead, a Task is a set T = {L, f } where L is a label space and f is a predictive function f usually learned by the data. For instance, f (x i ) can be used to assign the predicted label to x i ∈ X. Therefore, f can be equivalently viewed as the probability of a label y given a data x, i.e. p(y ∈ L|x ∈ X).\n\nA Dataset of n points can be defined as a set S = {(x i ∈ X, y i ∈ L)} n i=1 . Transfer learning wants to exploit the knowledge of a domain D A on a task T A to resolve the same or another task T B on another domain D B . By the definition of domain, it is straightforward that two domains D A = {F A , P (X A )} and D B = {F B , P (X B )} can be considered different if they differ in the feature spaces or in the marginal probability distributions. Obviously, the same holds for two Tasks T A = {L A , f A } and T B = {L B , f B }. More in details, the following cases can happen:\n\n1. D A = D B and T A = T B : since the Tasks and the Domains are the same, this can be considered a classical Machine Learning Problem.",
      "page_start": 13,
      "page_end": 14
    },
    {
      "section_name": "D",
      "text": "The non-stationarity of the EEG signals between different subjects in an emotion classification problem can be viewed as a multi-domain problem where the data belonging to each subject are sampled from different Domains. Usually, given a pair of two different subjects A and B, a common features space is assumed to be shared by the two domains (the EEG data representation), the conditional data distributions P (L A |X A ) = P (L B |X B ) are assumed to be the same while the marginal probability distributions are different on the available data, i.e. P (X A ) = P (X B ). Therefore, minimising the non-stationarity of EEG signal should be viewed as reducing a discrepancy measure between several Domains.\n\nIn the current literature, transfer learning strategies can be divided in three families:\n\n• Unsupervised/semisupervised Domain Adaptation (DA) methods,\n\n• Supervised DA Methods, also known as PreTraining, (PT) methods,\n\n• Domain Generalization (DG) methods.\n\nThese families differ mainly in which data are processed during the learning stage. DA methods start from the hypothesis that data sampled from two different domains are available, called Source Domain and Target Domain respectively. The main difference between them is that, while a complete dataset S Source = {(x i , y i )} n i=1 can be sampled from the Source domain, only feature data points X T arget = {x j } m j=1 ∈ F T arget can be sampled from the Target one, without knowledge (unsupervised DA) or minimal knowledge (semi-supervised DA) of their real labels. Unsupervised DA methods can be viewed as transductive machine learning methods with the further hypothesis that the data come from two different distributions. Instead, PT methods work adapting a model already trained on a known (Source) Domain to go toward a new (Target) domain. Since both features X and labels y of the new domain are known during the adapting stage, PT strategies are also known as supervised DA methods. In contrast, DG methods rely on the hypothesis that d ≥ 2 source domains together with their labeled samples are available, while any data from the Target domain is unknown.\n\nDA and DG methods are getting a great deal of attention in the scientific literature in different contexts (e.g. image classification and voice recognition), and several proposals have been made until now. One trend of the literature is to adapt DA/DG methods originally proposed for a context to another one. For example, in  Zhou et al. (2020b)  methods to adapt DA strategies for image classification to EEG emotion classification are proposed. However, each context has its characteristics and peculiarities, making the transfer of a DA method from a task to another task not immediate. Several attempts were made by the scientific community to adapt well-established DA/DG methods in tasks involving the processing of EEG signals in the emotion recognition field.",
      "page_start": 14,
      "page_end": 15
    },
    {
      "section_name": "Domain Adaptation (Da) Methods",
      "text": "Several DA methods relied on minimising the discrepance measures between the Source and the Target domain. In  Ganin and Lempitsky (2015)  these methods are categorised into shallow and deep DA. As will be explained in the section (Proposed Taxonomy), an extension of this taxonomy is proposed in this paper, by adding the Source selection methods:\n\n• Source selection: a subset of DA methods take into account that not all the training data can effectively be useful for the target space. Therefore, a selection of the training data is made, in order to avoid negative transfer. Several of these methods are also known as Instance weighting  Jiang (2008) , since they assign weights to the data; they can be made as a kind of preprocessing for the other methods;\n\n• Shallow DA: the data representation is given a-priori. Only a mapping between the Source and Target representations is learned, without affecting the starting data representation;\n\n• Deep DA: the data representation is learned as part of the DA strategy.\n\nIn the following part of this section, the studies according the above discussed ML approaches are reported.",
      "page_start": 15,
      "page_end": 15
    },
    {
      "section_name": "Source Selection",
      "text": "Source selection methods take into account that not all the training data can effectively be useful for the target space. In  Zhang et al. (2019b)  TrAdaBoost  Dai et al. (2007)  is used to score the source EEG data so that they does not negatively influence the training process. In other words, a small amount of labeled target data helps to vote on the usefulness of each of the available source data instance. As initial step, only the source subjects closest to the target one are selected according to the MMD similarity and fed as auxiliary data to TrAdaBoost. This family of methods can be also used as initial step of another DA method, for example in  Lin et al. (2017) ;  Zhou et al. (2020a)  the similarity between source and target EEG data is measured using the Pearson Correlation Coefficient and the Average Frechet Distance respectively. In particular, in  Zhou et al. (2020a)  only the closer EEG source data to the target data are fed to TCA together with the target one. Finally, the classification step is made by an Echo State Network (ESN,  Ozturk et al. (2007) ). In  Hua et al. (2021) , Neighborhood Component Analysis (NCA,  Kenneth et al. (2020) ) is employed to learn the Mahalanobis distance between data and linearly project them into a subspace such that the classification accuracy is maximized and to reduce the dimensionality of the EEG features. The obtained features are then used with Geodesic flow kernel for Unsupervised Domain Adaptation  Gong et al. (2012) . In  Wang et al. (2021a)  (DMATN) data belonging to the existing subjects are divided into several sub-source domains. Then, a set of sub-source are chosen as the most relevant with the target data. The proposed architecture combines together DAN and DANN to learn representation domain invariant representation.",
      "page_start": 15,
      "page_end": 16
    },
    {
      "section_name": "Shallow Da Methods",
      "text": "Different strategies were proposed in literature, usually relied on one of following alternatives:\n\n• Target Space-Based (TSB): searching for a good transformation which directly maps Source data S to the Target data T space (S → T );\n\n• Shared Space-Based (SSB): searching for a good transformation which maps Source S and Target T data in a new shared space where the discrepancy between S and T is minimal (S, T → C).\n\nOnce all the data are projected in a common space having the marginal distributions of the Domains close enough each others, common classification methods can be used to emotion recognition. In Target space-based papers,  Fernando et al. (2013)  tried to align the source space toward the target one. Rather than using the data in their original feature spaces, the authors used PCA for a more robust and compact data representation. More specifically, two PCA projection matrices Z S and Z T are computed for the Source and the Target domain respectively. Therefore, a transformation matrix M able to align the source space to the target one is searched by an optimisation problem, i.e.",
      "page_start": 16,
      "page_end": 16
    },
    {
      "section_name": "Arg Min",
      "text": "This problem has a closed form solution in the form M = Z T S Z T . Indeed, a similarity between the projected data can be computed.\n\nIn  Chai et al. (2017)  ASFM is adopted for EEG-based Emotion Recognition. However, as a large part of the domain adaptation strategies,  Chai et al. (2017)  uses all the source subject as a whole as they belong to the same domain. In other words, the data belonging to different training subjects are viewed as an unique subject. Differently, in  Chai et al. (2018)  (Multi-Subject Subspace Alignment, MSSA) the ASFM strategy is applied to each source subject individually, then the projected data are fed to different for-subject classifiers.\n\nOther data transformations have been investigated in the DA scenario for EEG emotion recognition, such as Robust Principal Component Analysis  Wright et al. (2009)  in  Lin (2019) . RCA decomposes a set X of data as X = L + S, where L and S are two superimposed matrices: the former is a low-rank matrix, the latter a sparse matrix. These matrices are computed resolving the following optimisation problem: min   2012 )) is adapted for EEG emotion recognition task to generalise across different subjects. In a nutshell, STM maps source data to target data by an affine transformation. The solution of the proposed problem is in closed form, so it can be easily computed. Few labeled target data are used to make a source data selection, so starting from the hypothesis that a small amount of labeled data are available. On the other side, ins Shared space-based the Maximum Mean Discrepancy (MMD,  Gretton et al. (2006) ) is one of the currectly most used discrepancy measure in DA/DG strategies. In the original study, MMD is proposed to test if two probability distributions p and q are different or not. Formally, the authors show that in a Reduced Kernel Hilbert Space (RKHS) a discrepancy measure between the two distributions can be defined as\n\nwhere ϕ(•) is an appropriate feature mapping. In  Gretton et al. (2006)  is proven that, in a RKHS, M M D(p, q) is 0 if and only if the two distributions p and q are the same. MMD can be empirical estimated as the difference between the averages of two data sampled from the two distributions projected in a RKHS. Therefore, considering X S and X T as two sets sampled from the Source and the Target domain respectively, empirical M M D(X S , X T ) can be expressed as:\n\nT are elements of X S and X T respectively. In other words, having two samples from two different distributions, the distance between the two distributions can be estimate through the distance between two means of the samples projected in a RKHS.\n\nTransfer Component Analysis (TCA,  Pan et al. (2010) ) is one of the most used MMD-based DA method. In the original work, two different TCA versions were proposed: i) an unsupervised version, where a transformation of the data is found such that the data variance is maximally preserved reducing, at the same time, the MMD distance of the domains distributions, and ii) a supervised one, where the data dependence with the training labels is taken into account.\n\nAn evaluation of the unsupervised TCA on EEG data for emotion recognition was made in  Zheng et al. (2015) . Instead of using all the available EEG data, a random selection of a subset of samples from Source domain data was made during the evaluation strategy, letting out a subject as Target domain. In  Xue et al. (2020)  TCA is tested on SEED dataset trying several desired dimensions for the feature space. Instead, in  He et al. (2022b)  TCA is tested on self-made EEG data.\n\nIn  Long et al. (2013)  Transfer Sparse Coding (TSC) the MMD was exploited to find a sparse representation of image data sampled from different distribution. Sparse code representations are well-known data approximation obtained as linear combinations of elements in a set of basis functions. In a nutshell, a sparse coding method searches for a representative over-complete set of basis functions (a dictionary) together with an encoding that best represent the data. In its simplest form, the sparse coding problem can be expressed as\n\nwhere X ∈ R m×n is a matrix containing n data points to approximate and B ∈ R m×k and S ∈ R k×n are the dictionary matrix and the encoding matrix respectively, where k > m to ensure the over-completeness. The sparsity is induced by the second equation term on the coefficient matrix columns s i and regulated through the hyperparameter λ ∈ R. However, if X is composed of data sampled from two different Domains (e.g., X = [X S |X T ]) the above formalisation does not take into account the differences between the marginal distributions. To deal with this problem,  Long et al. (2013)  proposed to add a further regularisation term to the objective function that takes into account the MMD distance between the different Domains of the input data.\n\nIn  Ni et al. (2021) , similarly, a common dictionary between source and target domain is computed, but preserving the local information between samples and the discriminative knowledge between the domains exploiting the PCA and Fisher criteria  Fisher (1992) . This work required a little set of labelled data from the target domain, falling in the semi-supervised DA approaches.\n\nWhile it is not specifically designed for Domain Adaptation, Kernel-PCA (KPCA,  Schölkopf et al. (1997) ) is often used in comparisons with several DA methods. In a nutshell, KPCA uses the kernel trick to project the data into a kernel space and then applying the PCA on the projected data. A comparison between Kernel-PCA and TCA for EEG emotion recognition is reported in  Zheng et al. (2015) . In  Chai et al. (2016)  (SAAE), a features transformation ϕ(•) is computed through Kernel-PCA maximising the embedded data variance. Before the transformation, an autoencoder trained on data from both the Source and the Target Domains was employed to preprocess the data. In  Zheng and Lu (2016)  TCA, KPCA, TSVM, and TPT are evaluated on the EEG-based emotional SEED dataset in a Leave-On-Subject-Out approach, while in Lan et al. (  2018 ) similar methods are tested on SEED and DEAP also for Cross-Dataset generalisation.",
      "page_start": 16,
      "page_end": 16
    },
    {
      "section_name": "Deep Da Methods",
      "text": "In deep DA approaches, a feature data representation learning is embedded in the DA method. Instead of searching for a transformation of features given a priori, this is done by changing the feature space representation.\n\nDeep DA methods can be further divided in:\n\n• Common Shared Space (CSS) methods: Source and Target are projected in a new shared space\n\n• Shared+Specific Spaces (SSS) methods: Source and Target are first projected in a unique shared space, then auxiliary more specific spaces are used.\n\nAs can be seen in Table  2 , CSS represents the category of studies exhibiting the best performance in terms of accuracy, at least in four of the cases considered. In  Tzeng et al. (2014)  (Deep Domain Confusion, DDC) two identical networks are trained together, the former classifying data from the Source Domain, the latter adapting the distance between Source and Target domains using Target feature data. The combination of both the classification performance and the MMD is used as final loss.  Zhang et al. (2019a)  uses DDC for cross-subject EEG emotion recognition. The networks' architectures used are of type residual CNNs  He et al. (2016) . To be fed to the CNNs, the EEG inputs are firstly transformed into Electrodefrequency Distribution Maps (EFDMs,  Wang et al. (2020) ). Results are validated with a Leave-One-Subject out CV approach. The authors of  Long et al. (2015)  proposed a DA framework considering the general structure of a Convolutional Neural Network (CNN), that is usually composed by a sequence of convolutional layers followed by a fully-connected ones. The authors hypothesise that in a deep neural network the transition from general to the specific task features grows with the increasing of the network depth. Indeed, in a CNN, while the initial convolutional layers learn general features, the final fully-connected ones learn domain specific features that are not transferable. Their proposed model (Deep Adaptation Network, DAN) deeply adapt the final fully connected layers minimising the Multi-Kernel Maximum Mean Discrepancies (MK-MMD,  Zhu et al. (2017) ), a multiple kernel variant of MMD used as distribution discrepancy measurement. DAN was evaluated in EEG emotion recognition on SEED and SEED-IV in  Li et al. (2018a) . In  Kuang et al. (2021)  the proposed Multi-Spatial Domain Adaptation Network (MSDAN) aligns source and target domain considering the spatial relationships between the electrodes. This is done by using Graph Convolutional Layers and exploiting MMD distance in the resulting graph space. Differently from other works,  Kuang et al. (2021)  uses data acquired in a Virtual Reality (VR) environment to generate stimuli, and the cross device problem is taken into account. One of the most used deep DA strategies is the Domain Adversarial Learning, proposed in  Ganin and Lempitsky (2015) ;  Ajakan et al. (2014) ;  Ganin et al. (2016) . The authors proposed an embedded problem formulation considering both the desired task and the discrepancy between the Source and the Target domain. The basic idea is to make the data distributions indistinguishable for an ad-hoc domain classifier. This can be made by a deep neural network model (Domain Adversarial Neural Network, DANN) that, for each input, predicts both the corresponding class and the belonging domain. In a nutshell, DANN is composed of three main components: a feature extractor, a label predictor, and a domain classifier. Therefore, a learning process searches for a feature mapping maximising the class prediction performances and, at the same time, maximising the domain classification loss to make the feature distributions as similar as possible. DANN is evaluated in EEG emotion recognition task in  Jin et al. (2017)  on SEED. In  Li et al. (2018d)  BiDANN, a DANN variation is adopted for EEG emotion recognition, but considering the differences between the brain hemispheres, is proposed. In a nutshell, EEG data from the two hemispheres are processed separately: two different features mapping together with a domain discriminator are learned for the brain hemispheres, instead of only one feature mapping as in the original DANN formulation. Difference between the hemispheres in a DA approach is not dealt only by BiDANN; for instance, BiHDM  Li et al. (2018e, 2020d  uses two different RNN to code the data belonging to the two hemispheres, and also in this case a domain discriminator is used to mix up the features of the Source and the Target domain. In  He et al. (2022a)  the authors propose a new DA method which is framed in the context of deep adversarial learning approaches. In particular a temporal convolutional network is used as encoder. Interestingly, the method is successfully evaluated in both cross-subject and cross-dataset. In  Ye et al. (2021) ;  Zhong et al. (2020)  domain adversarial approaches are used together with Graph Neural Networks (GNN,  Zhou et al. (2020b) ) as feature extractor. In particular,  Ye et al. (2021)  leverages on an attention mechanism  Niu et al. (2021)  focusing the learning stage on the alignment of the more changeling areas of the feature space. Performances are evaluated on SEED dataset. Instead,  Zhong et al. (2020)  proposes a node-wise domain adversarial training (NodeDAT) method to regularise the GNN output for better subject-independent performances. In EEG literature, Domain adversarial learning is widely used in several other studies for EEG data recognition, for example in  Tzeng et al. (2017) ;  Bao et al. (2020) ;  Li et al. (2019d,a) ;  Furukawa et al. (2021) ;  Hwang et al. (2020b) . In particular, in  Li et al. (2019d)  possible differences between several brain regions are also taken into account with a proposed attention module. In  Du et al. (2020)  (ATtention-based LSTM with Domain Discriminator, ATDD-LSTM) a domain discriminator in terms of LSTMs is presented to reduce the discrepance between the distributions. An attention-based encoder-decoder focuses on emotion-related helping the final classification probability estimation. An interesting adversarial approach was also investigated in  Wang et al. (2021c) . The proposed work exploits the Covariance Matrices between EEG data and Riemannian distances  Barbaresco (2008) . The work proposed a new kind of Neural Network (daSPDnet) able to retain the intrinsic geometry information of the data. However, differently from the typical DA approach, a little set of labelled data belonging to the Target domain are required during the training process, resulting as semi-supervised DA method. A similar approach, also requiring a few of labelled target data, was proposed in  Li et al. (2021c) . Multi-source Domain Transfer Discriminative Dictionary Learning modeling (MDTDDL) is developed in  Gu et al. (2022) ; the aim is to learn a joint subspace between source and target domains exploiting dictionary learning  Mairal et al. (2008)  methods. DEEP and SEED are evaluated both in Cross-Subject and Cross-Session mode. In  Tzeng et al. (2017)  Adversarial Discriminative Domain Adaptation (ADDA), a strategy to tackle the DA on an image classification task, was proposed. Differently from DANN, the ADDA basic idea consisted in building two different functions for the Source and the Target domains represented with two different encoders E S and E T , respectively. E S is trained together with a classifier C using labelled data from the Source domain. Then, through an adversarial learning procedure, E T is trained to map the Target domain data in the space of E S outputs. In this space, target data can be classified by C. A similar idea was adapted in EEG emotion classification domain in  Luo et al. (2018)  (Wasserstein GAN Domain Adaptation, WGANDA), mixing together a pre-training stage and an adversarial training stage. More in detail, two generators for the source and target domain respectively are pre-trained to output two feature vectors of the same size. These vectors are considered as belonging to a shared feature space. An adversarial training step based on minimising the Wasserstein distance tunes the parameters of the generators such that the outputs match more closely as possible each other. The combined outputs are then used as input for an output classifier network. Inspired by the MMD optimization made in  Chai et al. (2016) , in  Bao et al. (2020) (TDANN) a two stage DA method is proposed. In the first stage, MMD is minimized training a CNN equipped with adaBN  Li et al. (2018c) . To be fed to the CNN and to preserve spatial information, the EEG input signals are transformed into images  Bashivan et al. (2015) ;  Hwang et al. (2020a) . In the second stage, a domain discriminator is used to further reduce the distance between the source and the target distributions. The method was evaluated in a leave-one-subject out cross validation framework. One of the main issue of the DANN networks is that no label is considered during the adversarial learning process, therefore the relationship between target data and the task-specific decision boundary during the distributions alignment is not taken into account. A DA method can confuse the distributions of the two domains by reducing the distance between them, resulting in a simple mixing of the samples of the two domains, leading the categories within each domain to not be distinguishable. Indeed, in DANN the decision boundary inside each domain is ignored.  In Saito et al. (2018)  (Maximum Classifier Discrepancy, MCD), instead, the labels of the Source domain data are considered, helping to search a good task-specific decision boundaries between the classes. This is achieved by using different classifiers fed with the same inputs and evaluating the discrepancy. More in detail, two classifier C 1 and C 2 with the same characteristics are fed with input of feature generator G. G can be fed with data x coming from the source or the target domain. The output of C 1 and C 2 are the labels of the input x returned by G. Before the training step, C 1 and C 2 start from different initial state, rising two different classifiers after the training. How much the two classifiers disagree on their predictions on the same input is defined discrepancy by the authors. Indeed, the generator G is trained to minimise the discrepancy (that is, project source and target data in the same space), while C 1 and C 2 are trained to maximise the discrepancy (so that the two classification boundaries are far from each other). The learned generator G will be able to relocate the target domain data in the source space, but taking into account its most probable belonging class. Task-Specific Domain Adversarial Neural Network (T-DANN,  Ding et al. (2021) ) is an MCD similar model proposed for EEG emotion recognition. T-DANN adapts the conditional distribution between domains and, at the same time, adapts classification boundaries between classes exploiting MCD in conjunction with a domain discriminator. Instead,  Ning et al. (2021)  deal with the excessive allignment problem exploiting a few-shot learning and attention mechanism approach. From a different point of view,  Wang et al. (2021b)  used Siamese Networks  Koch et al. (2015)  for evaluating the similarity between samples belonging to different domains. Siamese networks were originally proposed to to determine whether two inputs belong to the same category or not. In  Wang et al. (2021b)  the Siamese framework is converted to handle different domains. However, this method require a few of labelled data belonging to the Target domain. In  Tao and Dan (2021)  the authors propose a DA approach for EEGbased emotion recognition based on a multi-source co-adaptation framework (MACI). The proposed framework mainly takes advantage of correlation knowledge among several sources and features to build a proper objective function. The proposed method is compared with both standard (shallow) DA approaches and deep (CNN-based) approaches. Cross-subjects and cross-datasets evaluations are performed. Computational costs is a critical point of the proposed framework. The authors of  Luo et al. (2021)  propose a novel approach which attempts to unify in an unique optimisation problem two standard DA approaches, instance reweighting (that we refer as source selection) and feature matching. This novel approach is named Progressive Low-Rank Subspace Alignment (PLRSA). In particular, instance reweighting is implemented by minimizing the Maximum Mean Discrepancy (MMD) distance and the TrAdaBoost algorithm, and feature matching by the Transfer Component Analysis (TCA). Importantly, a tiny amount of labeled target data is used to better exploit the source auxiliary data. The proposed method is evaluated in a both cross-subjects and cross-sessions scenario. The method is compared with five state-of-the-art DA methods. The results seem promising, however the time complexity is a little more expensive than related state-of-the-art methods.\n\nAlthough several studies start from the hypothesis that a shared feature space is enough for DA, Shared+Specific Space (SSS) methods go in different direction, believing that a single shared classifier built in a shared space still has poor performance for the never seen sessions/subjects. Notably, in these studies each subject/session available is considered as a single domain, and not as a whole. Hypothetically, EEG data representations can be splitted into shared emotional components, universal to all the subjects, and private components, specific to each subject. Leveraging on this hypothesis,  Zhao et al. (2021)  builds a shared encoder and private encoders for each source subject data to capture the subject-invariant emotional representations and private components, respectively. The obtained encoders are then used to build several emotion classifiers. Finally, a new subject classifier using few data is built. All these classifier are built exploiting the shared encoder. A classifier fusion strategy is then applied to obtain the final classification result. However, the proposed framework requires few labelled target data, falling in the unsupervised DA category. MEERNet  Chen et al. (2021b)  considers different classifiers for each different domain (subject or session), preceded by a feature extractor shared by all the domains. Final classification is made averaging between domain-specific classifiers. Similarly,  Luo and Lu (2021)  proposed a framework composed of a common feature extractor to map all the domains in a common subspace, a main task classifier or regressor, and private discriminators for each domain. The training is made reducing the Wasserstein distance between the marginal distribution of each source domain and target one in an adversarial way. In  Chen et al. (2021a)  the authors propose a Multi Source-Marginal Distribution Adaptation (MS-MDA) algorithm for EEG emotion recognition. Also in this case, the key idea is that the final response is obtained by the average of the responses of target-source specific classifiers, preceded by a common feature extractor. Notably, the authors explore the impact of different types of data normalisation on the performance of the proposed model. MS-MDA is compared with several standard DA methods and it has very promising results. Similarly, the authors of  Cao et al. (2021)  propose Multi-Dource and Multi-Representation Adaptation (MSMRA), an approach with many similarities with respect MS-MDA algorithm  Chen et al. (2021a) . Both cross-subjects and cross-sessions evaluations are performed.",
      "page_start": 19,
      "page_end": 23
    },
    {
      "section_name": "Supervised Da (Pretraining) Methods",
      "text": "In the supervised Domain Adaptation category, four studies were included in the reviews. In  Cimtay and Ekmekcioglu (2020)   In  Wang et al. (2020)  a CNN model trained on different subjects and sessions taken from the SEED dataset is then re-trained on a small amount of data of a target subject taken from DEAP dataset to evaluate the cross-dataset emotion recognition performances. In  Li et al. (2020a)  several classifiers trained on different data belonging to different subjects and sessions are ensembled together obtaining a final classifier suitable both for cross-sessions and cross-subjects EEG emotion recognition.",
      "page_start": 23,
      "page_end": 23
    },
    {
      "section_name": "Domain Generalization (Dg) Methods",
      "text": "Finally, differently from classical domain adaptation methods, in Domain Generalization data from several domains are available, but no data from the test domain is observed during the training stage. Differently from classical domain adaptation methods, data from several domains are available, but no data from the test domain is observed during the training stage  Muandet et al. (2013) . Methods can be divided as:\n\n• shallow DG: a data transformation is given a priori;\n\n• deep DG: the data rapresentation is learned as part of the DG strategy.\n\nShallow DG methods share the same principles of shollow DA ones, building a shared space between domains letting the input data representation unchanged. Domain Invariant Component Analysis (DICA)  Muandet et al. (2013)  searches for common features across several domains. Features data are transformed by a learned orthogonal transformation which minimizes the dissimilarity between a set of known domains preserving, at the same time, the relations between data features and their real labels. The authors also provided an unsupervised DICA version which did not take care of the class labels.\n\nIn  Ghifary et al. (2016)  Scatter Component Analysis (SCA) is proposed. The aim of the authors is to propose a method adapt both DG and DA requirements. SCA search for a data transformation where at the same time, i) the source and the target Domains are similar, ii) elements of the same class are similar, iii) elements of different classes are well separated and, iv) the variance of the whole data is maximised. This is made introducing Scatter, a measure closely related to MMD. In  Ma et al. (2019) , SCA and DICA are applied and evaluated on SEED dataset.\n\nOn the other side, in deep DG methods include the data representation as part of the generalization strategy. In  Gonzalez et al. (2019)  data from similar subjects are used to train the same classifier. The similarity between data is computed through a clustering algorithm. This subset of similar subjects is used to train a final CNN classifier. In  Liu et al. (2021)  a similar strategy is adopted, but for Domain Adaptation context.  Hagad et al. (2021)  join together BiDANN and Variational Autoencoder (VAE) obtaining a subject-invariant  Bi-lateral Variational Domain Adversarial Neural Network (BiVDANN) . VAEs are generative neural networks able to learn embedding of data constrained to a Gaussian distribution. As any classical autoencoder, a VAE is composed of an encoder network able to transforms data to an embedding space while a decoder network is able to reconstruct the original input from the embedding. In the proposed work, the learned features are further refined by domain adversarial training across different subjects to learn subject-independent features. Furthermore, to maximize dataset intercompatibility spectral topography data of the EEG signal are used as input.\n\nThe pie charts in Figure  5  show some statistics about the papers included in the survey. First of all, it is evident that almost three quarters of the studies surveyed (73.4 %) focus on a cross-subject mode of generalisation, while crosssession studies account for only 17 % and only 9 % operate a cross-dataset mode of generalisation. Graph b) shows the percentage distribution according to the proposed taxonomy. Looking at this graph, it is evident that the majority of generalisation studies are moving towards the use of Deep Domain Adaptation (CSS) (33.33 %), at the expense of more traditional approaches, which still retain 26.67 %. This is followed by Shallow DA (SSB) approaches (9.33 %), Deap DA (SSS) (8 %), Source Selection DA (6.67 %), Shallow DA (TSB) and Supervised DA (5.33 %), Deep DG (4 %) and finally Transductive (1.33 %). The pie chart in Figure  5 .c) shows the number of times each EEG dataset is exploited in the reviewed literature. The mainly used datasets are SEED (45.8 %) and DEAP (27.5 %). This is followed by 8.3 % of studies that propose their own self-produced dataset, while of the other datasets available in the literature only SEED IV stands out (at 7.5 %), which is interesting in that it adopts a discrete space of four emotions for classification (happy, sad, fear and neutral). Each of the other datasets do not exceed 5 % (MAHNOB  Soleymani et al. (2011)  and DREAMER  Katsigiannis and Ramzan (2017)  (3.33 %),  CMEED Zhao et al. (2018)  (1.7 %),  ASCERTAIN Subramanian et al. (2016) ,  MDME Shenoy et al. (2006)  and  SDMN Lin et al. (2010)  (0.8 %)).\n\nFinally, Figure  5 .d) offers an interesting statistic about the interest of the authors of the studies examined in the various perspectives of emotion representation. As already mentioned in section 1.3, the two dominant perspectives, and the only ones considered in the literature examined, are those based on categorical and dimensional models. More than 80 per cent of the works are based on a representation of emotions, and then their subsequent classification in terms of valence and arousal (and only in one case also dominance  Wang et al. (2021c) ).\n\nStudies identified as best performers in terms of mean classification accuracy are proposed in Table  2 . Only cross-subject studies were considered in this selection, being the statistically most significant percentage among all the generalisation studies surveyed. The ten best results were identified considering as many classification issues. Each issue is defined by considering the number and type of classes identified (binary and ternary on valence and arousal, quaternary on the two-dimensional valence-arousal plane, binary and quaternary on discrete dimensions) and the dataset adopted (considering three possibilities: DEAP, SEED, other). Only studies reporting both mean accuracy and standard deviation were included in the performance assessment.",
      "page_start": 24,
      "page_end": 26
    },
    {
      "section_name": "Discussion",
      "text": "To date, no robust electroencephalographic patterns are recognized in scientific literature for correlating with emotional states. Some studies base their results on the asymmetry of scalp activations, but several theories based on statistical samples that are not yet particularly large still coexist  Demaree et al. (2005) ;  Coan and Allen (2003) ;  Davidson (1984) .\n\nWhen aiming for a generalization goal in EEG-based Emotion Recognition, Transfer Learning methods are becoming more and more established in the literature. Domain Adaptation methods (Deep DA (CSB), Shallow DA (SSB), Source Selection DA, Deep DA (SSS), Shallow DA (TSB) and Supervised DA) exceed 60 % of the total surveyed studies and exhibit very high accuracy performances in the table of best performers (see Table  2 ). In particular, Deep DA (Common Shared Space) is used by five best performers studies. This could be also due to the current massive use of Deep DA (Common Shared Space) in the literature. Indeed, one third of the surveyed studies (Figure  5 .d) belongs to this category.\n\nHowever, a still substantial percentage of works (27.4 %) belongs to the Classical ML category. An emblematic case in this context is  Li et al. (2021a) , namely the best performer in the classification issue on SEED IV with four discrete classes. This is an interesting study based on a self-organized graph construction module. This solution can be considered as a peculiar implementation of the well established adaptive filters strategy, when the generalization goal is pursued by customising the network to the current input. Conversely, the DA strategies make the data from different domains more homogeneous by means of appropriate transformations. The different impact between DA and adaptive filters approaches can be better appreciated by making a comparison between the previous study and  Zhong et al. (2020) . Both studies address the problem of four-class classification on the same dataset by using a pipeline based on graphs and deep networks. In the first case, an adaptive graph is used without any DA methods, while the second study makes use of a (nonadaptive) graph approach  in combination with Domain Adaptation techniques. Even though they use different approaches, the reported accuracy performances are comparable. This suggests how the dynamic search for feature extraction procedures represents an interesting frontier for future studies in this area, not excluding the potential of using this approach in combination with DA/DG techniques.\n\nAnother point to take into account is that the proliferation of EEG acquisition devices on the market is not always coupled with consistency in terms of quality between the various devices (considering electrode type and positioning, interference shielding and signal-to-noise ratio, amplification strategies, etc). A comparison among different studies must take into account the quality of EEG instrumentation used. The IEC 60601-2-26 standard applies to basic safety and essential performance of electroencephalographs used in a clinical environment.\n\nAmong the requirements, the minimum overall signal quality for an electroencephalographic device to be considered acceptable is defined  Goldsack et al. (2020) . Even if IEC 60601-2-26 is a standard specifically developed for clinical purposes, it is nowadays the only available standard for EEG instrumentation quality certification. In the future, it is desirable for research to be increasingly based on certified instruments. However, an encouraging trend emerges from the most recent public datasets. They are all based on standardized equipment: (i) Neuroelectrics Enobio 8 in the case of LUMED  Cimtay and Ekmekcioglu (2020) , (ii) NuAmp Neuroscan in the case of  CMEED Du et al. (2020) , and (iii) gtec.HIamp in the case of the dataset produced by  Bao et al. (2020)) .\n\nA further concern in the use of public datasets is its underlying theoretical background, often acritically accepted by the scientists. Many studies validate the same machine learning algorithm on different datasets although the targeted psychic phenomena are radically different. Indeed, each dataset leverages on a specific theory of emotions and related experimental setup of emotion elicitation. For instance, DEAP is based on a dimensional approach and SEED IV on discrete one. The discrete theory is based on the assumption that there are basic emotions that have evolved through natural selection TenHouten (2017). In this vein, close to the Darwinian tradition, Ekman's theory identifies six basic emotions that would be universal and innate: anger, disgust, fear, happiness, sadness and surprise  Ekman (1999) . After him, Plutchik, identifies eight basic emotions (anger, anticipation, joy, trust, fear, surprise, sadness and disgust) and arranges them on a wheel model  Suhaimi et al. (2020) . In contrast, the dimensional theory expresses emotions in a continuous two-dimensional (valence-arousal) or three-dimensional (valence-arousal-dominance) space. While valence measures levels of pleasantness (happy vs. sad) of an emotion, arousal identifies degrees of excitement or motivational activation. In the three-dimensional model, the dominance dimension is added to valence and arousal, where the dominance evaluates emotions on a scale between submission and empowerment  Torres et al. (2020) . The underlying assumption of discrete approach is that few fundamental emotions are mediated by associated dedicated neural circuits, with a large innate (hardwired) component. Only two main brain networks are recognized by the dimensional approach  Posner et al. (2009) . The two theoretical approaches identify two different phenomena also at a neurophysiological level, with peculiar spatial signal features.\n\nFinally, at present, the available public datasets do not adopt an established practice of psychological screening of the subjects involved. In general, studies on EEG-based emotion assessment could benefit from administering psychometric questionnaires to participants. Indeed, psychological data could help to understand individual differences in emotional response, leading to clustering of subjects  Liu et al. (2021) . Recently, unsupervised clustering based on large datasets is emerging as a promising strategy for empirical identification of personality types  Gerlach et al. (2018) . Meanwhile, correlations have been found between personality types and EEG patterns  Li et al. (2020b) . Moreover, prior psychological assessments allow to manage bias due to individual traits or states. The introduction of psycho-metric tests and assessments during the production of upcoming datasets could lead to a much more fruitful use of data in support of generalization.",
      "page_start": 26,
      "page_end": 26
    },
    {
      "section_name": "Conclusion",
      "text": "A systematic literature review collecting papers on machine learning strategies to pursue (cross-subjects and cross-sessions) generalizability in EEG-based emotion recognition was carried out. Among the 418 articles retrieved from Scopus, IEEE (Institute of Electrical and Electronics Engineers) Xplore, and PubMed databases, 75 papers resulted eligible. A taxonomy of the studies employing ML method was proposed.\n\nThe studies with the best results in terms of average classification accuracy were identified, and the ten best results considering as many classification problems were highlighted. An interesting perspective based on self-organized graph construction modules emerged as peculiar strategy. This suggests how the adaptive feature extraction procedures represent an interesting frontier for future studies in this area, not excluding the potential of using this approach in combination with DA/DG techniques.\n\nFuture research on EEG-based emotion assessment could also benefit from administering psychometric questionnaires to participants in order to conduct a psychological screening of the experimental sample. This could help to understand individual differences in emotional responses, leading to clustering of subjects also taking into account the different subjects' personality.",
      "page_start": 30,
      "page_end": 30
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Scopus trend for EEG-based Emotion Recognition studies.",
      "page": 2
    },
    {
      "caption": "Figure 2: A pipeline of a classical ML process involving EEG signals.",
      "page": 7
    },
    {
      "caption": "Figure 2: In the current literature, a large part of research works proposed methods",
      "page": 7
    },
    {
      "caption": "Figure 3: PRISMA flow diagram of the systematic review process.",
      "page": 8
    },
    {
      "caption": "Figure 4: ). Among all the reviewed papers, a subset of them leveraging on the classi-",
      "page": 9
    },
    {
      "caption": "Figure 4: , in Tab. 1 all papers",
      "page": 9
    },
    {
      "caption": "Figure 4: Proposed Taxonomy.",
      "page": 10
    },
    {
      "caption": "Figure 5: Pie charts for distribution of papers occurrences according to: a) gen-",
      "page": 25
    },
    {
      "caption": "Figure 5: show some statistics about the papers included",
      "page": 25
    },
    {
      "caption": "Figure 5: c) shows the number of times each EEG dataset is",
      "page": 25
    },
    {
      "caption": "Figure 5: d) offers an interesting statistic about the interest of the",
      "page": 25
    },
    {
      "caption": "Figure 5: d) belongs to this",
      "page": 26
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Abstract": ""
        },
        {
          "Abstract": ""
        },
        {
          "Abstract": "based in emotion classification was\nrealized."
        },
        {
          "Abstract": "stationarity of EEG signals is a critical"
        },
        {
          "Abstract": ""
        },
        {
          "Abstract": "address\nthis\nissue, mainly based on transfer"
        },
        {
          "Abstract": ""
        },
        {
          "Abstract": ""
        },
        {
          "Abstract": ""
        },
        {
          "Abstract": "pers,\n75 were\nfound eligible based on their"
        },
        {
          "Abstract": ""
        },
        {
          "Abstract": ""
        },
        {
          "Abstract": ""
        },
        {
          "Abstract": ""
        },
        {
          "Abstract": ""
        },
        {
          "Abstract": ""
        },
        {
          "Abstract": ""
        },
        {
          "Abstract": "approaches. A discussion is proposed on the impact of"
        },
        {
          "Abstract": "theoretical models and (ii) psychological"
        },
        {
          "Abstract": "sample on the classifier performances."
        },
        {
          "Abstract": "This work has been published on Neurocomputing journal. Please refer to the final ver-"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "1\nIntroduction": "Emotions are our internal compass and play a primary role in learning, reason-"
        },
        {
          "1\nIntroduction": "ing, decision-making processes, and communication between individuals. The"
        },
        {
          "1\nIntroduction": "Information and Communication Technology (ICT)\nsector’s\ninterest\nin emo-"
        },
        {
          "1\nIntroduction": "tions has grown tremendously in recent years, shaping the concept of affective"
        },
        {
          "1\nIntroduction": "computing, an emerging field aimed at monitoring and predicting emotions in"
        },
        {
          "1\nIntroduction": "order\nto improve human-computer\ninteraction Cambria et al.\n(2017);\nfor\nin-"
        },
        {
          "1\nIntroduction": "stance,\nit possible to implement\nin-\nthe introduction of affective loops makes"
        },
        {
          "1\nIntroduction": "creasingly adaptive human-machine interfaces and virtual assistants tailored to"
        },
        {
          "1\nIntroduction": "users Saganowski et al. (2020), or the outputs of emotion monitoring systems, in"
        },
        {
          "1\nIntroduction": "the healthcare context, can be useful\nin the treatment of psychological disorders"
        },
        {
          "1\nIntroduction": "based on emotional deficits,\nin autism Feng et al. (2018),\nin the improvement of"
        },
        {
          "1\nIntroduction": "wellbeing Healy et al. (2018), and in stress containment Saganowski (2022)."
        },
        {
          "1\nIntroduction": "In particular,\nin this context,\nthere is a growing interest\nin the literature"
        },
        {
          "1\nIntroduction": "for Brain-Computer Interface (BCI) systems based on EEG signals Torres et al."
        },
        {
          "1\nIntroduction": "(2020).\nIn fact, the number of annual scientific publications indexed on Scopus"
        },
        {
          "1\nIntroduction": "database on the topic of EEG-based emotion recognition shows an exponential"
        },
        {
          "1\nIntroduction": "growth trend (see Fig. 1)."
        },
        {
          "1\nIntroduction": "A critical issue underlying the processing and classification of EEG signals is"
        },
        {
          "1\nIntroduction": "their inherent variability among different subjects or different acquisition times"
        },
        {
          "1\nIntroduction": "(i.e.\nsessions) of\nthe same subject,\nsince the EEG signal\nis usually stochastic"
        },
        {
          "1\nIntroduction": "and stationary only for short intervals (generally ranging from a few seconds to"
        },
        {
          "1\nIntroduction": "minutes) Inouye et al. (1995); Im (2018); Sörnmo and Laguna (2005). More in"
        },
        {
          "1\nIntroduction": "detail, the EEG signal is not a Wide Sense Stationary signal Cao and Slobounov"
        },
        {
          "1\nIntroduction": "(2011). This characteristic of non-stationarity implies a variation in the tempo-"
        },
        {
          "1\nIntroduction": "ral and spectral characteristics of the EEG signal over time. This is an open is-"
        },
        {
          "1\nIntroduction": "sue in the literature leading to a loss of generalizability for classification systems"
        },
        {
          "1\nIntroduction": "across\nsubjects\ntask) and,\nfor\nthe same subject, across different\n(inter-subject"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "task) Nasiri and Clifford (2020).\nsessions (intra-subject": "Data-driven approaches using Machine Learning (ML) are often employed at"
        },
        {
          "task) Nasiri and Clifford (2020).\nsessions (intra-subject": "multiple levels in the EEG signal processing pipeline to pursue the classification"
        },
        {
          "task) Nasiri and Clifford (2020).\nsessions (intra-subject": "of emotional states and their generalization across subjects and sessions."
        },
        {
          "task) Nasiri and Clifford (2020).\nsessions (intra-subject": "Currently,\nthe\nliterature\nshows\nincreasing use\nof modern machine\nlearn-"
        },
        {
          "task) Nasiri and Clifford (2020).\nsessions (intra-subject": "ing strategies, adopting deep neural networks and transfer\nlearning-based ap-"
        },
        {
          "task) Nasiri and Clifford (2020).\nsessions (intra-subject": "proaches,\nsuch as domain adaptation,\ndomain generalization and/or hybrid"
        },
        {
          "task) Nasiri and Clifford (2020).\nsessions (intra-subject": "methods Zhao et al.\n(2021). This paper proposes a systematic review on the"
        },
        {
          "task) Nasiri and Clifford (2020).\nsessions (intra-subject": "use of machine learning to improve generalizability capabilities\nin EEG-based"
        },
        {
          "task) Nasiri and Clifford (2020).\nsessions (intra-subject": "emotion recognition systems across different subjects and sessions."
        },
        {
          "task) Nasiri and Clifford (2020).\nsessions (intra-subject": "As will be discussed in detail\nin the next section, several surveys have been"
        },
        {
          "task) Nasiri and Clifford (2020).\nsessions (intra-subject": "proposed in recent years, gathering and discussing the main directions of\nthe"
        },
        {
          "task) Nasiri and Clifford (2020).\nsessions (intra-subject": "literature on this\nresearch topic.\nHowever,\nto the best of our knowledge,\na"
        },
        {
          "task) Nasiri and Clifford (2020).\nsessions (intra-subject": "focus on the application of ML methods to improve the inter/intra-subjective"
        },
        {
          "task) Nasiri and Clifford (2020).\nsessions (intra-subject": "generalization performance of EEG-based emotion recognition is missing in the"
        },
        {
          "task) Nasiri and Clifford (2020).\nsessions (intra-subject": "literature."
        },
        {
          "task) Nasiri and Clifford (2020).\nsessions (intra-subject": "The rest of the paper is organized as follows: Section 2 reviews related works,"
        },
        {
          "task) Nasiri and Clifford (2020).\nsessions (intra-subject": "with reference to recent\nsurveys carried out on this\nspecific topic.\nSection 3"
        },
        {
          "task) Nasiri and Clifford (2020).\nsessions (intra-subject": "presents a theoretical background on EEG, with a first part focused on BCIs for"
        },
        {
          "task) Nasiri and Clifford (2020).\nsessions (intra-subject": "emotion recognition and a second part on ML for emotion recognition. Section"
        },
        {
          "task) Nasiri and Clifford (2020).\nsessions (intra-subject": "4 presents\nthe used search queries and the paper\nselection process according"
        },
        {
          "task) Nasiri and Clifford (2020).\nsessions (intra-subject": "to the PRISMA method Liberati et al.\n(2009).\nSection 5 presents\nthe results"
        },
        {
          "task) Nasiri and Clifford (2020).\nsessions (intra-subject": "of\nthe review, proposing a taxonomy of\nthe ML methods currently proposed"
        },
        {
          "task) Nasiri and Clifford (2020).\nsessions (intra-subject": "in the selected papers, discussing the ML methods with respect\nthe proposed"
        },
        {
          "task) Nasiri and Clifford (2020).\nsessions (intra-subject": "taxonomy. A statistical analyses of\nthe results was\nreported.\nSection 6 aims"
        },
        {
          "task) Nasiri and Clifford (2020).\nsessions (intra-subject": "to discuss the results obtained,\nreporting the most promising lines of research"
        },
        {
          "task) Nasiri and Clifford (2020).\nsessions (intra-subject": "and approaches\nthat have emerged and highlighting possible future directions"
        },
        {
          "task) Nasiri and Clifford (2020).\nsessions (intra-subject": "in this area. Finally, Section 7 draws conclusions."
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "as a specific topic on which to focus future research efforts, without exploring the": "problem in-depth. Recently, Li and colleagues Li et al. (2021b) published a re-"
        },
        {
          "as a specific topic on which to focus future research efforts, without exploring the": "view focusing on the topic of EEG-based emotion recognition and discussing the"
        },
        {
          "as a specific topic on which to focus future research efforts, without exploring the": "importance of transfer learning. While offering some interesting results,"
        },
        {
          "as a specific topic on which to focus future research efforts, without exploring the": "not present itself as a systematic review (only 18 studies were reported without"
        },
        {
          "as a specific topic on which to focus future research efforts, without exploring the": "PRISMA methodology to collect them). Thus, differently from the cited works,"
        },
        {
          "as a specific topic on which to focus future research efforts, without exploring the": "a systematic literature review focused on the inter/intra-subject generalization"
        },
        {
          "as a specific topic on which to focus future research efforts, without exploring the": "on EEG-based emotion recognition systems and the use of modern ML-based"
        },
        {
          "as a specific topic on which to focus future research efforts, without exploring the": "methods as a possible solution is proposed in this paper."
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "imentally validated (e.g.,\nInternational Affective Picture System -\nIAPS Lang": "(2005), Open Affective Standardized Image Set\n- OASIS Kurdi et al.\n(2017),"
        },
        {
          "imentally validated (e.g.,\nInternational Affective Picture System -\nIAPS Lang": "and Geneva Affective Picture Database\n- GAPED Dan-Glauser and Scherer"
        },
        {
          "imentally validated (e.g.,\nInternational Affective Picture System -\nIAPS Lang": "(2011)). There are several publicly-available databases for EEG-based emotion"
        },
        {
          "imentally validated (e.g.,\nInternational Affective Picture System -\nIAPS Lang": "recognition (e.g., DEAP Koelstra et al. (2011), SEED Zheng and Lu (2015), and"
        },
        {
          "imentally validated (e.g.,\nInternational Affective Picture System -\nIAPS Lang": "DREAMER Katsigiannis and Ramzan (2017)). Each dataset\nis characterized"
        },
        {
          "imentally validated (e.g.,\nInternational Affective Picture System -\nIAPS Lang": "by different physiological signals and a well-established experimental setup (in"
        },
        {
          "imentally validated (e.g.,\nInternational Affective Picture System -\nIAPS Lang": "terms of stimulus sources, emotional theory adopted, number of subjects, and"
        },
        {
          "imentally validated (e.g.,\nInternational Affective Picture System -\nIAPS Lang": "psychometric metrological references). For a comprehensive description of the"
        },
        {
          "imentally validated (e.g.,\nInternational Affective Picture System -\nIAPS Lang": "various available datasets see Li et al. (2021b)."
        },
        {
          "imentally validated (e.g.,\nInternational Affective Picture System -\nIAPS Lang": "In case of self-produced EEG data, the preprocessing stage is fundamental to"
        },
        {
          "imentally validated (e.g.,\nInternational Affective Picture System -\nIAPS Lang": "filter out the noise from the brain activity signal. Some steps are often helpful"
        },
        {
          "imentally validated (e.g.,\nInternational Affective Picture System -\nIAPS Lang": "to achieve a successful EEG signal preprocessing:\n(i)\nline noise\nremoval,\n(ii)"
        },
        {
          "imentally validated (e.g.,\nInternational Affective Picture System -\nIAPS Lang": "referencing, (iii) bad channels removal, and (iv) artifacts removal. They can be"
        },
        {
          "imentally validated (e.g.,\nInternational Affective Picture System -\nIAPS Lang": "summarized as:"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "to achieve a successful EEG signal preprocessing:\n(i)\nline noise\nremoval,\n(ii)": "referencing, (iii) bad channels removal, and (iv) artifacts removal. They can be"
        },
        {
          "to achieve a successful EEG signal preprocessing:\n(i)\nline noise\nremoval,\n(ii)": "summarized as:"
        },
        {
          "to achieve a successful EEG signal preprocessing:\n(i)\nline noise\nremoval,\n(ii)": "• Line noise removal:\nline noise is an artifact which contaminates the gamma"
        },
        {
          "to achieve a successful EEG signal preprocessing:\n(i)\nline noise\nremoval,\n(ii)": "band of\nthe EEG signal,\nin the USA)\nIm\nspecifically at 50 Hz\n(60 Hz"
        },
        {
          "to achieve a successful EEG signal preprocessing:\n(i)\nline noise\nremoval,\n(ii)": "(2018). Conventional filters such as lowpass filters with cutoff frequencies"
        },
        {
          "to achieve a successful EEG signal preprocessing:\n(i)\nline noise\nremoval,\n(ii)": "between 50 Hz and 70 Hz can be used to remove this artifact and reduce"
        },
        {
          "to achieve a successful EEG signal preprocessing:\n(i)\nline noise\nremoval,\n(ii)": "the noise at higher\nfrequencies, notch filters must be employed to reject"
        },
        {
          "to achieve a successful EEG signal preprocessing:\n(i)\nline noise\nremoval,\n(ii)": "the 50 Hz power supply."
        },
        {
          "to achieve a successful EEG signal preprocessing:\n(i)\nline noise\nremoval,\n(ii)": "• Referencing: when acquiring the EEG signals,\nthe voltage in a specific"
        },
        {
          "to achieve a successful EEG signal preprocessing:\n(i)\nline noise\nremoval,\n(ii)": "scalp area is measured with respect\nto a reference electrode.\nIf\nthe ref-"
        },
        {
          "to achieve a successful EEG signal preprocessing:\n(i)\nline noise\nremoval,\n(ii)": "erence electrode is subjected to artifacts, all electrodes are also affected."
        },
        {
          "to achieve a successful EEG signal preprocessing:\n(i)\nline noise\nremoval,\n(ii)": "The re-referencing allows to minimize the impact of the reference electrode"
        },
        {
          "to achieve a successful EEG signal preprocessing:\n(i)\nline noise\nremoval,\n(ii)": "by subtracting a reference channel\nfrom the original EEG channels. Most"
        },
        {
          "to achieve a successful EEG signal preprocessing:\n(i)\nline noise\nremoval,\n(ii)": "used reference electrodes are the mastoid channel,\nthe EEG signal at a"
        },
        {
          "to achieve a successful EEG signal preprocessing:\n(i)\nline noise\nremoval,\n(ii)": "specific channel, the average of the mastoid channels, or the average of all"
        },
        {
          "to achieve a successful EEG signal preprocessing:\n(i)\nline noise\nremoval,\n(ii)": "EEG channels Lepage et al. (2014). To avoid lateralization effects Lepage"
        },
        {
          "to achieve a successful EEG signal preprocessing:\n(i)\nline noise\nremoval,\n(ii)": "et al. (2014), also Cz and FCz electrodes are often used as references."
        },
        {
          "to achieve a successful EEG signal preprocessing:\n(i)\nline noise\nremoval,\n(ii)": "• Bad channels\nremoval: EEG signal of one or more channels can be also"
        },
        {
          "to achieve a successful EEG signal preprocessing:\n(i)\nline noise\nremoval,\n(ii)": "contaminated by the noise due to a poor contact between the electrode and"
        },
        {
          "to achieve a successful EEG signal preprocessing:\n(i)\nline noise\nremoval,\n(ii)": "the scalp. Therefore, it is necessary to detect the noisy or bad channels and"
        },
        {
          "to achieve a successful EEG signal preprocessing:\n(i)\nline noise\nremoval,\n(ii)": "remove them. Methods to find out bad channels are the visual\ninspection"
        },
        {
          "to achieve a successful EEG signal preprocessing:\n(i)\nline noise\nremoval,\n(ii)": "of each single channel, the dispersion and the correlation criteria da Cruz"
        },
        {
          "to achieve a successful EEG signal preprocessing:\n(i)\nline noise\nremoval,\n(ii)": "et al. (2018)."
        },
        {
          "to achieve a successful EEG signal preprocessing:\n(i)\nline noise\nremoval,\n(ii)": "• Artifacts removal:\nartifacts are all the unwanted signals that may affect"
        },
        {
          "to achieve a successful EEG signal preprocessing:\n(i)\nline noise\nremoval,\n(ii)": "the measurement and corrupt the EEG signal. They can be due to all the"
        },
        {
          "to achieve a successful EEG signal preprocessing:\n(i)\nline noise\nremoval,\n(ii)": "physiological systems different\nfrom the brain,\nsuch as heart, eyes, mus-"
        },
        {
          "to achieve a successful EEG signal preprocessing:\n(i)\nline noise\nremoval,\n(ii)": "cle, etc., or all the environmental noise, such as wireless signals, electrode"
        },
        {
          "to achieve a successful EEG signal preprocessing:\n(i)\nline noise\nremoval,\n(ii)": "adhesion, cable movements, etc.\nIn the EEG signals, artifacts are present"
        },
        {
          "to achieve a successful EEG signal preprocessing:\n(i)\nline noise\nremoval,\n(ii)": "in specific frequency bands: ocular artifacts and cardiac activity are dom-"
        },
        {
          "to achieve a successful EEG signal preprocessing:\n(i)\nline noise\nremoval,\n(ii)": "inant below 4 Hz, muscle movements above 30 Hz. Other physiological"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "artifacts are due to skin perspiration, sweating, movements of the tongue,": "chest movements, etc. Removing artifacts from the EEG signal means to"
        },
        {
          "artifacts are due to skin perspiration, sweating, movements of the tongue,": "correct (or eliminate) the fluctuations introduced by the artifacts without"
        },
        {
          "artifacts are due to skin perspiration, sweating, movements of the tongue,": "causing distortions in the brain signal. Part of the artifacts can be removed"
        },
        {
          "artifacts are due to skin perspiration, sweating, movements of the tongue,": "with the filtering process (e.g.,\nline noise) but, for a more effective removal"
        },
        {
          "artifacts are due to skin perspiration, sweating, movements of the tongue,": "of artifacts,\nfurther processing is required Jiang et al. (2019b). Main arti-"
        },
        {
          "artifacts are due to skin perspiration, sweating, movements of the tongue,": "facts removal methods are:\nlinear regression, filtering, wavelet transform,"
        },
        {
          "artifacts are due to skin perspiration, sweating, movements of the tongue,": "empirical mode decomposition (EMD),\nIndependent Component Analy-"
        },
        {
          "artifacts are due to skin perspiration, sweating, movements of the tongue,": "sis\n(ICA), Principal Component Analysis\n(PCA), Canonical Correlation"
        },
        {
          "artifacts are due to skin perspiration, sweating, movements of the tongue,": "Analysis (CCA), and Artefact Subspace Reconstruction (ASR)."
        },
        {
          "artifacts are due to skin perspiration, sweating, movements of the tongue,": "Once the EEG signal has been preprocessed, it is usually divided into epochs,"
        },
        {
          "artifacts are due to skin perspiration, sweating, movements of the tongue,": "and a feature extraction process is then applied. EEG features can be catego-"
        },
        {
          "artifacts are due to skin perspiration, sweating, movements of the tongue,": "rized into three domains, namely time,\nfrequency and time-frequency."
        },
        {
          "artifacts are due to skin perspiration, sweating, movements of the tongue,": "• Time domain:\nthe main features are the statistics of\nthe signal,\nsuch as"
        },
        {
          "artifacts are due to skin perspiration, sweating, movements of the tongue,": "mean, variance, skewness, kurtosis, etc Geethanjali et al. (2012); Vidaurre"
        },
        {
          "artifacts are due to skin perspiration, sweating, movements of the tongue,": "et al.\n(2009); Yuen et al.\n(2009).\nOther\ntime-domain features are\nthe"
        },
        {
          "artifacts are due to skin perspiration, sweating, movements of the tongue,": "Hjorth parameters, namely Activity, Mobility, and Complexity Oh et al."
        },
        {
          "artifacts are due to skin perspiration, sweating, movements of the tongue,": "(2014). Good results in the recognition of emotional states can be achieved"
        },
        {
          "artifacts are due to skin perspiration, sweating, movements of the tongue,": "by using entropy-based features,\ni.e., approximate,\nsample, differential,"
        },
        {
          "artifacts are due to skin perspiration, sweating, movements of the tongue,": "and wavelet entropy Phadikar et al. (2019). Higher-order crossing (HOC),"
        },
        {
          "artifacts are due to skin perspiration, sweating, movements of the tongue,": "the\nfractal dimension, and the Non-Stationary Index (NSI) Patil\net al."
        },
        {
          "artifacts are due to skin perspiration, sweating, movements of the tongue,": "(2016); Nan and Jinghua (1988); Kroupi\net al.\n(2011) are\nfurther\ntime"
        },
        {
          "artifacts are due to skin perspiration, sweating, movements of the tongue,": "domain feature often used for the EEG analysis."
        },
        {
          "artifacts are due to skin perspiration, sweating, movements of the tongue,": "• Frequency domain:\nthe most used feature is\nthe power\nspectral density"
        },
        {
          "artifacts are due to skin perspiration, sweating, movements of the tongue,": "(PSD). PSD is the signal power in the unit frequency band Wang et al."
        },
        {
          "artifacts are due to skin perspiration, sweating, movements of the tongue,": "(2011). Other representative features of different emotional states involv-"
        },
        {
          "artifacts are due to skin perspiration, sweating, movements of the tongue,": "ing the PSD are:\n(i)\nlogarithm,\n(ii) maximum,\n(iii) minimum and,\n(iv)"
        },
        {
          "artifacts are due to skin perspiration, sweating, movements of the tongue,": "standard deviation of the power spectrum."
        },
        {
          "artifacts are due to skin perspiration, sweating, movements of the tongue,": "• Time-frequency domain:\nthe time-frequency analysis (TFA) allows to ob-"
        },
        {
          "artifacts are due to skin perspiration, sweating, movements of the tongue,": "serve spectrum changes with timeZhang (2019). The short-time Fourier"
        },
        {
          "artifacts are due to skin perspiration, sweating, movements of the tongue,": "transform (STFT),the continuous wavelet transform (CWT), the discrete"
        },
        {
          "artifacts are due to skin perspiration, sweating, movements of the tongue,": "wavelet transform (DWT) Hernández et al. (2018), matching pursuit and"
        },
        {
          "artifacts are due to skin perspiration, sweating, movements of the tongue,": "empirical mode decomposition are the most used methods to extract time-"
        },
        {
          "artifacts are due to skin perspiration, sweating, movements of the tongue,": "frequency features."
        },
        {
          "artifacts are due to skin perspiration, sweating, movements of the tongue,": "Often, the number of EEG features is very high, therefore a feature selection"
        },
        {
          "artifacts are due to skin perspiration, sweating, movements of the tongue,": "strategy is required Jenke et al. (2014). Another critical point is represented by"
        },
        {
          "artifacts are due to skin perspiration, sweating, movements of the tongue,": "the large number of EEG channels often used for the signal acquisitions. A high"
        },
        {
          "artifacts are due to skin perspiration, sweating, movements of the tongue,": "number of channels can lead to high computational complexity. Therefore, the"
        },
        {
          "artifacts are due to skin perspiration, sweating, movements of the tongue,": "selection of\nthe most\ninformative EEG channels can be crucial Apicella et al."
        },
        {
          "artifacts are due to skin perspiration, sweating, movements of the tongue,": "(2022)."
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "EEG acquisitions": "Figure 2: A pipeline of a classical ML process involving EEG signals."
        },
        {
          "EEG acquisitions": "3.2\nMachine Learning for Emotion Recognition"
        },
        {
          "EEG acquisitions": "After that the EEG have been properly preprocessed and a proper set of features"
        },
        {
          "EEG acquisitions": "has been extracted,\nthe data are ready to be fed to a supervised ML system."
        },
        {
          "EEG acquisitions": "The typical pipeline of a ML framework applied to an EEG emotion recognition"
        },
        {
          "EEG acquisitions": "task is reported in Fig. 2."
        },
        {
          "EEG acquisitions": "In the current\nliterature, a large part of research works proposed methods"
        },
        {
          "EEG acquisitions": "framed into Transfer Learning approach to tackle\nemotion recognition tasks."
        },
        {
          "EEG acquisitions": "The motivation of this trend can be summarized as follows:\nin classical super-"
        },
        {
          "EEG acquisitions": "vised ML a set of already labeled data has to be available. This implies that"
        },
        {
          "EEG acquisitions": "in the EEG emotion recognition tasks a set of EEG signals recorded from one"
        },
        {
          "EEG acquisitions": "or more subjects has\nto be mapped with the emotion felt during the acquisi-"
        },
        {
          "EEG acquisitions": "tion.\nLabelled data can be\nthen used to train the ML system, generating a"
        },
        {
          "EEG acquisitions": "ML model able to classify the input data. Once the ML model has been ob-"
        },
        {
          "EEG acquisitions": "tained, new unlabelled data can be fed to the ML model to obtain the predicted"
        },
        {
          "EEG acquisitions": "emotion/class. To evaluate the trained model,\nit is a good practice to reserve"
        },
        {
          "EEG acquisitions": "a part of\nthe\nlabelled data outside\nfrom the\ntraining stage.\nThese data can"
        },
        {
          "EEG acquisitions": "then be used to evaluate the final model predictions using suitable performance"
        },
        {
          "EEG acquisitions": "metrics (e.g. accuracy). However, a standard hypothesis of the traditional ML"
        },
        {
          "EEG acquisitions": "methods states that all the available data, no matter if\ninvolved in the training"
        },
        {
          "EEG acquisitions": "process or not, come from the same probability distribution. Due to the char-"
        },
        {
          "EEG acquisitions": "acteristics of the EEG data, this assumption results not always verified in the"
        },
        {
          "EEG acquisitions": "EEG signal.\nIndeed, the EEG signals acquired from a subject can be strongly"
        },
        {
          "EEG acquisitions": "different from the one acquired from another subject, even under the same con-"
        },
        {
          "EEG acquisitions": "ditions Im (2018). This can also happen for EEG signals acquired from the same"
        },
        {
          "EEG acquisitions": "subject in different times/sessions,\nleading to loss generalisation performances"
        },
        {
          "EEG acquisitions": "in cross-subject/session problems.\nIn the current\nliterature,\nthis problem was"
        },
        {
          "EEG acquisitions": "initially addressed considering the availability of further unlabelled data belong-"
        },
        {
          "EEG acquisitions": "ing to the target subject/session which can help the training of the ML model"
        },
        {
          "EEG acquisitions": "(transductive learning approaches). However, these methods do not make any"
        },
        {
          "EEG acquisitions": "consideration about the data distributions.\nIn fact, the training EEG data can"
        },
        {
          "EEG acquisitions": "be sensibly different in terms of probability distribution(s) from the data used"
        },
        {
          "EEG acquisitions": "outside the training stage.\nIn ML literature,\nthis can be viewed an instance"
        },
        {
          "EEG acquisitions": "of\nthe Dataset Shift problem Quinonero-Candela et al.\n(2008).\nIn a nutshell,"
        },
        {
          "EEG acquisitions": "Dataset Shift arises when the standard ML assumption is not verified,\nso the"
        },
        {
          "EEG acquisitions": "distribution of the training data differs from the data distribution used outside"
        },
        {
          "EEG acquisitions": "of the training stage. The idea that the training data come from different prob-"
        },
        {
          "EEG acquisitions": "ability distribution(s) respect to the data used outside of the training stage is"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Figure 3: PRISMA flow diagram of the systematic review process.": "the main hypothesis of the transfer learning approaches."
        },
        {
          "Figure 3: PRISMA flow diagram of the systematic review process.": ""
        },
        {
          "Figure 3: PRISMA flow diagram of the systematic review process.": "address\nthe dataset"
        },
        {
          "Figure 3: PRISMA flow diagram of the systematic review process.": "learning, and different categorizations of"
        },
        {
          "Figure 3: PRISMA flow diagram of the systematic review process.": "ported Pan and Yang (2009); Ganin and Lempitsky (2015). One of the first and"
        },
        {
          "Figure 3: PRISMA flow diagram of the systematic review process.": "most important review on Transfer Learning methods was proposed in Pan and"
        },
        {
          "Figure 3: PRISMA flow diagram of the systematic review process.": "Yang (2009), however several new strategies (e.g., Domain Generalization-based"
        },
        {
          "Figure 3: PRISMA flow diagram of the systematic review process.": "works) have been proposed in the following years."
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "dicted data sources, taking into account titles and abstracts: EEG AND (Emo-": "tion OR Preference) AND (\"Domain Adaptation\" OR \"Domain Generalisation\""
        },
        {
          "dicted data sources, taking into account titles and abstracts: EEG AND (Emo-": "OR \"Transfer Learning\" OR \"Adversarial\" OR \"Transfer\" OR \"Cross Session\""
        },
        {
          "dicted data sources, taking into account titles and abstracts: EEG AND (Emo-": "OR \"Cross Subject\" OR \"Cross Gender\" OR \"Non-stationary EEG\")."
        },
        {
          "dicted data sources, taking into account titles and abstracts: EEG AND (Emo-": "From the first phase,\n418\narticles were\ncollected.\nThen,\nas\na first pre-"
        },
        {
          "dicted data sources, taking into account titles and abstracts: EEG AND (Emo-": "screening process, the following criteria are used to exclude papers from the re-"
        },
        {
          "dicted data sources, taking into account titles and abstracts: EEG AND (Emo-": "view: duplicated articles, not peer-reviewed articles, and not written in English"
        },
        {
          "dicted data sources, taking into account titles and abstracts: EEG AND (Emo-": "articles. For all articles that survived the screening stage, a careful examination"
        },
        {
          "dicted data sources, taking into account titles and abstracts: EEG AND (Emo-": "of the full text was carried out.\nIn a conclusive screening, the following papers"
        },
        {
          "dicted data sources, taking into account titles and abstracts: EEG AND (Emo-": "were excluded:\n(a) all articles in which the problem of generalisability was not"
        },
        {
          "dicted data sources, taking into account titles and abstracts: EEG AND (Emo-": "explicitly stated as a peculiar topic, (b) studies in which a cross-subject/session"
        },
        {
          "dicted data sources, taking into account titles and abstracts: EEG AND (Emo-": "validation strategy was not\nclearly envisaged,\n(c)\nstudies oriented towards a"
        },
        {
          "dicted data sources, taking into account titles and abstracts: EEG AND (Emo-": "’multimodal\nfusion’,\ni.e.\naimed at corroborating the EEG signal-based classi-"
        },
        {
          "dicted data sources, taking into account titles and abstracts: EEG AND (Emo-": "fication with other biosignals,\n(d) studies not specifically focusing on emotion"
        },
        {
          "dicted data sources, taking into account titles and abstracts: EEG AND (Emo-": "recognition. Thus, 75 papers survived and were included in the review analysis."
        },
        {
          "dicted data sources, taking into account titles and abstracts: EEG AND (Emo-": "The complete flow diagram of the systematic review process using the PRISMA"
        },
        {
          "dicted data sources, taking into account titles and abstracts: EEG AND (Emo-": "approach is presented in Fig. 3."
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "other side, the papers dealing with the cross-subject/session problem as an in-": "stance of the dataset shift problem were considered. These methods are known"
        },
        {
          "other side, the papers dealing with the cross-subject/session problem as an in-": "in literature as transfer learning methods, which can be further divided in:"
        },
        {
          "other side, the papers dealing with the cross-subject/session problem as an in-": "• Unsupervised and Semisupervised Domain Adaptation methods:"
        },
        {
          "other side, the papers dealing with the cross-subject/session problem as an in-": "they consider both the data coming from different subjects/sessions and"
        },
        {
          "other side, the papers dealing with the cross-subject/session problem as an in-": "unlabelled data coming from a target subject/session during the training"
        },
        {
          "other side, the papers dealing with the cross-subject/session problem as an in-": "stage. Therefore,\nthey can be considered as an intersection between the"
        },
        {
          "other side, the papers dealing with the cross-subject/session problem as an in-": "transductive and transfer Learning methods;"
        },
        {
          "other side, the papers dealing with the cross-subject/session problem as an in-": "these methods rely on the\n• Supervised Domain Adaptation methods:"
        },
        {
          "other side, the papers dealing with the cross-subject/session problem as an in-": "hypothesis\nthat both labelled and unlabelled data from the target\nsub-"
        },
        {
          "other side, the papers dealing with the cross-subject/session problem as an in-": "ject/session are available during the training stage;"
        },
        {
          "other side, the papers dealing with the cross-subject/session problem as an in-": "in these methods data from several\n• Domain Generalization methods:"
        },
        {
          "other side, the papers dealing with the cross-subject/session problem as an in-": "probability distributions are available and can be used during the training,"
        },
        {
          "other side, the papers dealing with the cross-subject/session problem as an in-": "but no data from the target\nsubject/session is used during the training"
        },
        {
          "other side, the papers dealing with the cross-subject/session problem as an in-": "stage."
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Figure 4: Proposed Taxonomy.": "taxonomy or to classical ML methods. Moreover,\nfor each research study sev-"
        },
        {
          "Figure 4: Proposed Taxonomy.": "eral\ninformation is presented, such as the type of generalisation (cross-subject,"
        },
        {
          "Figure 4: Proposed Taxonomy.": "cross-session, or cross-device), the EEG dataset, the adopted classifier (whether"
        },
        {
          "Figure 4: Proposed Taxonomy.": "proposed as a personal contribution or adopted from the literature), and the"
        },
        {
          "Figure 4: Proposed Taxonomy.": "validation strategy. Studies in which the description of the experimental setup"
        },
        {
          "Figure 4: Proposed Taxonomy.": "was not sufficiently clear, especially in terms of validation strategies, were vol-"
        },
        {
          "Figure 4: Proposed Taxonomy.": "untarily omitted from the table for reproducibility issues."
        },
        {
          "Figure 4: Proposed Taxonomy.": "In the following of this section, the reviewed papers are discussed in according"
        },
        {
          "Figure 4: Proposed Taxonomy.": "with the Tab. 1."
        },
        {
          "Figure 4: Proposed Taxonomy.": "5.1\nClassical machine learning approaches"
        },
        {
          "Figure 4: Proposed Taxonomy.": "A model trained on a set of EEG data acquired from a given subject at a specific"
        },
        {
          "Figure 4: Proposed Taxonomy.": "time (or during a specific session) could not work as expected in classifying EEG"
        },
        {
          "Figure 4: Proposed Taxonomy.": "signal acquired from a different\nsubject or\nfrom the same subject at different"
        },
        {
          "Figure 4: Proposed Taxonomy.": "times.\nIn other words, the model has poor generalization performance. To deal"
        },
        {
          "Figure 4: Proposed Taxonomy.": "with this problem, several solution based on Machine Learning approach have"
        },
        {
          "Figure 4: Proposed Taxonomy.": "been proposed over the years. The most common approaches proposed proper"
        },
        {
          "Figure 4: Proposed Taxonomy.": "strategy of\nfeature transformations or\nfeature selection. The former wants\nto"
        },
        {
          "Figure 4: Proposed Taxonomy.": "be transformations of\nthe data able to hold only the most useful\ninformation"
        },
        {
          "Figure 4: Proposed Taxonomy.": "with the hope that these are shared between all the subjects, while the latter"
        },
        {
          "Figure 4: Proposed Taxonomy.": "are methods\nto select only useful\ninformation from the input\nsignals without"
        },
        {
          "Figure 4: Proposed Taxonomy.": "changing it. The feature extraction/selection is usually one of the first step of a"
        },
        {
          "Figure 4: Proposed Taxonomy.": "machine learning pipeline, where the training data are transformed before being"
        },
        {
          "Figure 4: Proposed Taxonomy.": "fed to the training stage of a machine learning model. These methods adopted"
        },
        {
          "Figure 4: Proposed Taxonomy.": "the classical machine learning framework, that is that only the training data are"
        },
        {
          "Figure 4: Proposed Taxonomy.": "available during the training stage, without any knowledge of the effective data"
        },
        {
          "Figure 4: Proposed Taxonomy.": "on which the model will be effectively used. This can be viewed as a consequence"
        },
        {
          "Figure 4: Proposed Taxonomy.": "of\nthe the starting hypothesis of\nthe traditional ML methods\nstating that all"
        },
        {
          "Figure 4: Proposed Taxonomy.": "the available data, no matter if used in the training process or not, come from"
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "the same probability distribution. Under this hypothesis, the training data are": "enough to generalise over all the possible data."
        },
        {
          "the same probability distribution. Under this hypothesis, the training data are": "The underlying hypothesis\nis\nthat a proper EEG data transformation is"
        },
        {
          "the same probability distribution. Under this hypothesis, the training data are": "enough to allow a ML model to generalise well,\nindependently from the fact the"
        },
        {
          "the same probability distribution. Under this hypothesis, the training data are": "data belongs to the same subject/session used during the training or not. Go-"
        },
        {
          "the same probability distribution. Under this hypothesis, the training data are": "ing deeper,\nin a ML problem on EEG data the feature extraction and selection"
        },
        {
          "the same probability distribution. Under this hypothesis, the training data are": "process can be made considering two different aspects:\ni) the EEG signal or ii)"
        },
        {
          "the same probability distribution. Under this hypothesis, the training data are": "the electrodes.\nIn the first case, a proper transformation or selection strategy"
        },
        {
          "the same probability distribution. Under this hypothesis, the training data are": "for the EEG signal\nis made."
        },
        {
          "the same probability distribution. Under this hypothesis, the training data are": "The reviewed literature proposed different works that analyse if several known"
        },
        {
          "the same probability distribution. Under this hypothesis, the training data are": "feature extraction methods are suitable to generalise across several datasets Ray-"
        },
        {
          "the same probability distribution. Under this hypothesis, the training data are": "atdoost and Soleymani (2018); Li et al. (2018b).\nIn particular,\nin Rayatdoost"
        },
        {
          "the same probability distribution. Under this hypothesis, the training data are": "and Soleymani (2018) the authors investigated the robustness of emotion recog-"
        },
        {
          "the same probability distribution. Under this hypothesis, the training data are": "nition methods across different experimental conditions, subjects and datasets."
        },
        {
          "the same probability distribution. Under this hypothesis, the training data are": "In Yang et al.\n(2019); Jiang et al.\n(2019a) Sequential Backward Selection"
        },
        {
          "the same probability distribution. Under this hypothesis, the training data are": "(SBS) was applied to find a good set of\nfeatures\nthat generalise across differ-"
        },
        {
          "the same probability distribution. Under this hypothesis, the training data are": "ent\nsubjects.\nTo find the best\nsubset of\nfeatures, SBS decreases\nthe number"
        },
        {
          "the same probability distribution. Under this hypothesis, the training data are": "of\nfeatures\nin an iterative way measuring,\nat\neach step,\nthe obtained accu-"
        },
        {
          "the same probability distribution. Under this hypothesis, the training data are": "racy on a given classifier (SVM in Yang et al. (2019), Decision Trees in Jiang"
        },
        {
          "the same probability distribution. Under this hypothesis, the training data are": "et al.\n(2019a)).\nSBS method is adopted to exploit\nthe significant differences"
        },
        {
          "the same probability distribution. Under this hypothesis, the training data are": "between the classes. A leave-one-subject-out verification strategy was employed"
        },
        {
          "the same probability distribution. Under this hypothesis, the training data are": "on DEAP and SEED datasets in Yang et al. (2019), while Jiang et al. (2019a)"
        },
        {
          "the same probability distribution. Under this hypothesis, the training data are": "validates its results on DEAP and self-produced data."
        },
        {
          "the same probability distribution. Under this hypothesis, the training data are": "In Cai et al.\n(2019); Yin et al.\n(2017) a family of Transferable Recursive"
        },
        {
          "the same probability distribution. Under this hypothesis, the training data are": "Feature Elimination (TRFE) methods are used to make a set of EEG features"
        },
        {
          "the same probability distribution. Under this hypothesis, the training data are": "steadily distributed among all the training subjects, therefore removing the EEG"
        },
        {
          "the same probability distribution. Under this hypothesis, the training data are": "features resulting not generic for all users. The proposed feature selector is val-"
        },
        {
          "the same probability distribution. Under this hypothesis, the training data are": "idated using SVMs as classifiers on DEAP dataset both in within-subject and"
        },
        {
          "the same probability distribution. Under this hypothesis, the training data are": "cross-subject ways.\nIn Zhang and Yin (2020) Cross-subject Recursive Feature"
        },
        {
          "the same probability distribution. Under this hypothesis, the training data are": "Elimination (C-RFE)\nis exploited to rank the features in order of\nimportance"
        },
        {
          "the same probability distribution. Under this hypothesis, the training data are": "with the aim of removing the features giving a low contribution to the classifi-"
        },
        {
          "the same probability distribution. Under this hypothesis, the training data are": "cation. The method is validated on EEG data fed to SVMs."
        },
        {
          "the same probability distribution. Under this hypothesis, the training data are": "In Liu et al. (2020) an evolution of the well-known Differential Entropy fea-"
        },
        {
          "the same probability distribution. Under this hypothesis, the training data are": "tures is proposed. The Dynamic Differential Entropy (DDE) features take into"
        },
        {
          "the same probability distribution. Under this hypothesis, the training data are": "account also the time-domain instead of only the frequency domain extracted"
        },
        {
          "the same probability distribution. Under this hypothesis, the training data are": "by the classical DE. The goal\nis to maximise the difference between classes min-"
        },
        {
          "the same probability distribution. Under this hypothesis, the training data are": "imising at the same time the difference within classes,\nlearning a set of common"
        },
        {
          "the same probability distribution. Under this hypothesis, the training data are": "characteristics across different subjects."
        },
        {
          "the same probability distribution. Under this hypothesis, the training data are": "In Li et al. (2019c) a latent representation of the EEG data from SEED and"
        },
        {
          "the same probability distribution. Under this hypothesis, the training data are": "DEAP is learned through a Variational Auto Encoder (VAE) and then classified"
        },
        {
          "the same probability distribution. Under this hypothesis, the training data are": "using a LSTM. VAEs start from the hypothesis that all the data are generated"
        },
        {
          "the same probability distribution. Under this hypothesis, the training data are": "by a random process\ninvolving latent variables. A VAE is usually trained to"
        },
        {
          "the same probability distribution. Under this hypothesis, the training data are": "encode the input data into a latent\nrepresentation and then mapping it\nto a"
        },
        {
          "the same probability distribution. Under this hypothesis, the training data are": "reconstructed version of the data. Li et al. (2019c) hypothesises that there exists"
        },
        {
          "the same probability distribution. Under this hypothesis, the training data are": "learnable intrinsic features\nshared across\nseveral\nsubjects EEG signals\ntaking"
        },
        {
          "the same probability distribution. Under this hypothesis, the training data are": "part\nin emotional processes.\nThese\nintrinsic\nfeatures\ncan be\nencoded by the"
        }
      ],
      "page": 11
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "VAE latent representations. The power of VAE to represent latent EEG factors": "is also investigated in Li et al. (2020c),\ntogether with classical Auto-Encoders"
        },
        {
          "VAE latent representations. The power of VAE to represent latent EEG factors": "(AEs) and Restricted Boltzmann Machines (RBMs). Final classification is made"
        },
        {
          "VAE latent representations. The power of VAE to represent latent EEG factors": "with an LSTM and the generalisation performances are evaluated in LOSO mode"
        },
        {
          "VAE latent representations. The power of VAE to represent latent EEG factors": "on DEAP and SEED dataset."
        },
        {
          "VAE latent representations. The power of VAE to represent latent EEG factors": "In Pandey and Seeja (2019) the cross-subject problem is tackled using Varia-"
        },
        {
          "VAE latent representations. The power of VAE to represent latent EEG factors": "tional Mode Decomposition (VMD) as feature extraction technique. The system"
        },
        {
          "VAE latent representations. The power of VAE to represent latent EEG factors": "is validated in an hold-out way without any intersections between subjects’ data"
        },
        {
          "VAE latent representations. The power of VAE to represent latent EEG factors": "in the training and the test set using a DNN as emotions classifier. Despite the"
        },
        {
          "VAE latent representations. The power of VAE to represent latent EEG factors": "encouraging results reported, no reason about why the proposed system works"
        },
        {
          "VAE latent representations. The power of VAE to represent latent EEG factors": "well\nin a cross-subject approach seems to be provided.\nIn Chen et al. (2021c);"
        },
        {
          "VAE latent representations. The power of VAE to represent latent EEG factors": "Fernandez et al. (2021); Arevalillo-Herráez et al. (2019) is shown that the nor-"
        },
        {
          "VAE latent representations. The power of VAE to represent latent EEG factors": "malisation scheme used to preprocess the EEG data can affect the cross-subject"
        },
        {
          "VAE latent representations. The power of VAE to represent latent EEG factors": "performances."
        },
        {
          "VAE latent representations. The power of VAE to represent latent EEG factors": "In Chen et al. (2021c) several normalisation methods were applied following"
        },
        {
          "VAE latent representations. The power of VAE to represent latent EEG factors": "two different schemes:\ni) All-subjects, where the whole dataset was normalised,"
        },
        {
          "VAE latent representations. The power of VAE to represent latent EEG factors": "ii) Single-subject, where the normalisation is made individually for each subject."
        },
        {
          "VAE latent representations. The power of VAE to represent latent EEG factors": "The All-subject schema is the most common method used to mitigate the impact"
        },
        {
          "VAE latent representations. The power of VAE to represent latent EEG factors": "of\neach data values on the\nentire dataset.\nSingle-subject,\ninstead,\nconsider"
        },
        {
          "VAE latent representations. The power of VAE to represent latent EEG factors": "each subject individually, applying normalisation on each subject. The authors"
        },
        {
          "VAE latent representations. The power of VAE to represent latent EEG factors": "empirically shown on SEED dataset that Single-subject Z-score performs better"
        },
        {
          "VAE latent representations. The power of VAE to represent latent EEG factors": "in a EEG emotion recognition problem respect to other normalisation schemes"
        },
        {
          "VAE latent representations. The power of VAE to represent latent EEG factors": "as min-max normalisation. On the same data,\nin Fernandez et al.\n(2021)\nthe"
        },
        {
          "VAE latent representations. The power of VAE to represent latent EEG factors": "authors apply single-subject Z − score normalisation after each neural network"
        },
        {
          "VAE latent representations. The power of VAE to represent latent EEG factors": "layer (Stratified Normalisation)."
        },
        {
          "VAE latent representations. The power of VAE to represent latent EEG factors": "In Arevalillo-Herráez et al.\n(2019) a simple transformation of\nthe original"
        },
        {
          "VAE latent representations. The power of VAE to represent latent EEG factors": "data is proposed.\nIt consists in using binary features having 0 and 1 as com-"
        },
        {
          "VAE latent representations. The power of VAE to represent latent EEG factors": "ponents values based on the fact\nthat\nthe feature is\nlower or higher\nthan the"
        },
        {
          "VAE latent representations. The power of VAE to represent latent EEG factors": "median feature value. This leads to a more effective reduction of the subject-"
        },
        {
          "VAE latent representations. The power of VAE to represent latent EEG factors": "dependent part of the EEG signal."
        },
        {
          "VAE latent representations. The power of VAE to represent latent EEG factors": "Instead,\nconsidering the\nelectrodes\nin place of\nthe EEG signals, different"
        },
        {
          "VAE latent representations. The power of VAE to represent latent EEG factors": "channels\nselection strategies\nsearching for a robust\nset of channels across\nthe"
        },
        {
          "VAE latent representations. The power of VAE to represent latent EEG factors": "subjects was proposed in Gupta et al. (2018); Zhang et al. (2016). To achieve"
        },
        {
          "VAE latent representations. The power of VAE to represent latent EEG factors": "EEG-based cross-session emotion recognition,\nin Peng et al. (2021) the author"
        },
        {
          "VAE latent representations. The power of VAE to represent latent EEG factors": "propose a way to learn the importance of the EEG channels and features to sep-"
        },
        {
          "VAE latent representations. The power of VAE to represent latent EEG factors": "arate discriminative features from the noisy and redundant ones. The proposed"
        },
        {
          "VAE latent representations. The power of VAE to represent latent EEG factors": "strategy is evaluated on pairs of a-priori chosen sessions."
        },
        {
          "VAE latent representations. The power of VAE to represent latent EEG factors": "In Tian et al.\n(2021) a neural network to classify emotion by EEG signals"
        },
        {
          "VAE latent representations. The power of VAE to represent latent EEG factors": "is proposed.\nThe proposed model\nintroduces a channel-attention layer\nto se-"
        },
        {
          "VAE latent representations. The power of VAE to represent latent EEG factors": "lect the most important channels for a set of emotions. Notably,\nthe different"
        },
        {
          "VAE latent representations. The power of VAE to represent latent EEG factors": "personalities across the subjects are taken into account, grouping together the"
        },
        {
          "VAE latent representations. The power of VAE to represent latent EEG factors": "subjects having similar personalities.\nIndeed, a different model\nfor each group"
        },
        {
          "VAE latent representations. The power of VAE to represent latent EEG factors": "was\ntrained.\nValidation is made on the ASCERTAIN dataset.\nThis dataset"
        },
        {
          "VAE latent representations. The power of VAE to represent latent EEG factors": "results particularly suited for this task, since it links personality and emotional"
        },
        {
          "VAE latent representations. The power of VAE to represent latent EEG factors": "state with physiological reactions."
        }
      ],
      "page": 12
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "In other works,\nthe\nstructure of\nthe\nelectrodes\nis\ntaken into account and": "modelled as a graph. Graph representation methodology resulted effective to"
        },
        {
          "In other works,\nthe\nstructure of\nthe\nelectrodes\nis\ntaken into account and": "model structured data achieving significant performance in many applications,"
        },
        {
          "In other works,\nthe\nstructure of\nthe\nelectrodes\nis\ntaken into account and": "included EEG emotion signal processing Song et al.\n(2018).\nGNNs are use-"
        },
        {
          "In other works,\nthe\nstructure of\nthe\nelectrodes\nis\ntaken into account and": "ful\nto retain the\nspatial\nstructure of\nthe\nelectrodes disposition.\nUsually,\nthe"
        },
        {
          "In other works,\nthe\nstructure of\nthe\nelectrodes\nis\ntaken into account and": "graph structure is fixed and given a priori\nfollowing the spatial disposition of"
        },
        {
          "In other works,\nthe\nstructure of\nthe\nelectrodes\nis\ntaken into account and": "the electrodes on the scalp.\nInstead, Dynamical Graph Convolutional Neural"
        },
        {
          "In other works,\nthe\nstructure of\nthe\nelectrodes\nis\ntaken into account and": "Networks (DGCNN, Song et al. (2018)) and Self-Organized Graph Neural Net-"
        },
        {
          "In other works,\nthe\nstructure of\nthe\nelectrodes\nis\ntaken into account and": "work (SOGNN, Li et al. (2021a)) organises the graph structure leveraging on the"
        },
        {
          "In other works,\nthe\nstructure of\nthe\nelectrodes\nis\ntaken into account and": "input brain signals rather than on a predefined graph structure. The resulting"
        },
        {
          "In other works,\nthe\nstructure of\nthe\nelectrodes\nis\ntaken into account and": "graph can be processed by the graph convolutional\nlayers to extract the more"
        },
        {
          "In other works,\nthe\nstructure of\nthe\nelectrodes\nis\ntaken into account and": "suitable features for emotion recognition. The features obtained are also tested"
        },
        {
          "In other works,\nthe\nstructure of\nthe\nelectrodes\nis\ntaken into account and": "in cross-subject scenarios."
        },
        {
          "In other works,\nthe\nstructure of\nthe\nelectrodes\nis\ntaken into account and": "5.2\nTransductive methods"
        },
        {
          "In other works,\nthe\nstructure of\nthe\nelectrodes\nis\ntaken into account and": "Transductive methods Vapnik (2006) start from the hypothesis that the unla-"
        },
        {
          "In other works,\nthe\nstructure of\nthe\nelectrodes\nis\ntaken into account and": "beled test data are available in the training stage (no assumption about\nthe"
        },
        {
          "In other works,\nthe\nstructure of\nthe\nelectrodes\nis\ntaken into account and": "distribution of the data, differently from the DA methods). The idea is that in"
        },
        {
          "In other works,\nthe\nstructure of\nthe\nelectrodes\nis\ntaken into account and": "several problems\nthere is only a specific set of data (usually corresponding to"
        },
        {
          "In other works,\nthe\nstructure of\nthe\nelectrodes\nis\ntaken into account and": "the test set) to classify, and it is available at training time. Note that in stan-"
        },
        {
          "In other works,\nthe\nstructure of\nthe\nelectrodes\nis\ntaken into account and": "dard ML approach the goal\nis\nto generalize on new unseen data, by contrast"
        },
        {
          "In other works,\nthe\nstructure of\nthe\nelectrodes\nis\ntaken into account and": "in transductive learning the goal\nis to correctly classify the test set only. The"
        },
        {
          "In other works,\nthe\nstructure of\nthe\nelectrodes\nis\ntaken into account and": "Transductive SVM (TSVM) is an example of a transductive method. Described"
        },
        {
          "In other works,\nthe\nstructure of\nthe\nelectrodes\nis\ntaken into account and": "in Joachims et al.\n(1999), differently from classical SVMs\nthat\nleverages only"
        },
        {
          "In other works,\nthe\nstructure of\nthe\nelectrodes\nis\ntaken into account and": "on labelled data, TSVMs exploit also unlabelled test data to find the best de-"
        },
        {
          "In other works,\nthe\nstructure of\nthe\nelectrodes\nis\ntaken into account and": "cision boundary between the classes.\nIn other words, the target data is viewed"
        },
        {
          "In other works,\nthe\nstructure of\nthe\nelectrodes\nis\ntaken into account and": "as an additional set of\ninformation about the data. One of the main drawback"
        },
        {
          "In other works,\nthe\nstructure of\nthe\nelectrodes\nis\ntaken into account and": "of TSVM is that an estimation of the number of elements for each class in the"
        },
        {
          "In other works,\nthe\nstructure of\nthe\nelectrodes\nis\ntaken into account and": "test set is needed. Progressive TSVM (PTSVM, Chen et al. (2003)) tries to re-"
        },
        {
          "In other works,\nthe\nstructure of\nthe\nelectrodes\nis\ntaken into account and": "solve this problem progressively labeling the unlabeled data during the training"
        },
        {
          "In other works,\nthe\nstructure of\nthe\nelectrodes\nis\ntaken into account and": "instead of classifying it as a whole at the same time."
        },
        {
          "In other works,\nthe\nstructure of\nthe\nelectrodes\nis\ntaken into account and": "The only study collected in this review using explicitly transdutive methods"
        },
        {
          "In other works,\nthe\nstructure of\nthe\nelectrodes\nis\ntaken into account and": "is Yang et al.\n(2020), where\nthe PTSVM generalisation power across\nseveral"
        },
        {
          "In other works,\nthe\nstructure of\nthe\nelectrodes\nis\ntaken into account and": "sessions of EEG data acquired from different subjects is validated."
        },
        {
          "In other works,\nthe\nstructure of\nthe\nelectrodes\nis\ntaken into account and": "5.3\nTransfer Learning methods"
        },
        {
          "In other works,\nthe\nstructure of\nthe\nelectrodes\nis\ntaken into account and": "Transfer Learning methods are based on the\nconcepts of Domain and Task."
        },
        {
          "In other works,\nthe\nstructure of\nthe\nelectrodes\nis\ntaken into account and": "Following the\nsurvey of Pan et al.\nPan and Yang (2009), a Domain can be"
        },
        {
          "In other works,\nthe\nstructure of\nthe\nelectrodes\nis\ntaken into account and": "is\nthe\ndefined as a set D = {F, P (X)} where F is a feature space and P (X)"
        },
        {
          "In other works,\nthe\nstructure of\nthe\nelectrodes\nis\ntaken into account and": "marginal probability distribution of a specific dataset X = {x1, x2, . . . , xn} ∈ F ."
        },
        {
          "In other works,\nthe\nstructure of\nthe\nelectrodes\nis\ntaken into account and": "Instead, a Task is a set T = {L, f } where L is a label space and f is a predictive"
        },
        {
          "In other works,\nthe\nstructure of\nthe\nelectrodes\nis\ntaken into account and": "function f usually learned by the data. For instance, f (xi) can be used to assign"
        },
        {
          "In other works,\nthe\nstructure of\nthe\nelectrodes\nis\ntaken into account and": "the predicted label to xi ∈ X. Therefore, f can be equivalently viewed as the"
        },
        {
          "In other works,\nthe\nstructure of\nthe\nelectrodes\nis\ntaken into account and": "probability of a label y given a data x,\ni.e. p(y ∈ L|x ∈ X)."
        }
      ],
      "page": 13
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "the adapting stage, PT strategies are also known as\nsupervised DA methods.": "In contrast, DG methods rely on the hypothesis that d ≥ 2 source domains to-"
        },
        {
          "the adapting stage, PT strategies are also known as\nsupervised DA methods.": "gether with their labeled samples are available, while any data from the Target"
        },
        {
          "the adapting stage, PT strategies are also known as\nsupervised DA methods.": "domain is unknown."
        },
        {
          "the adapting stage, PT strategies are also known as\nsupervised DA methods.": "DA and DG methods are getting a great deal of attention in the scientific"
        },
        {
          "the adapting stage, PT strategies are also known as\nsupervised DA methods.": "literature in different contexts (e.g.\nimage classification and voice recognition),"
        },
        {
          "the adapting stage, PT strategies are also known as\nsupervised DA methods.": "and several proposals have been made until now. One trend of\nthe literature"
        },
        {
          "the adapting stage, PT strategies are also known as\nsupervised DA methods.": "is to adapt DA/DG methods originally proposed for a context to another one."
        },
        {
          "the adapting stage, PT strategies are also known as\nsupervised DA methods.": "For example,\nin Zhou et al. (2020b) methods to adapt DA strategies for image"
        },
        {
          "the adapting stage, PT strategies are also known as\nsupervised DA methods.": "classification to EEG emotion classification are proposed. However, each context"
        },
        {
          "the adapting stage, PT strategies are also known as\nsupervised DA methods.": "has\nits characteristics and peculiarities, making the transfer of a DA method"
        },
        {
          "the adapting stage, PT strategies are also known as\nsupervised DA methods.": "from a task to another\ntask not\nimmediate.\nSeveral attempts were made by"
        },
        {
          "the adapting stage, PT strategies are also known as\nsupervised DA methods.": "the\nscientific\ncommunity to adapt well-established DA/DG methods\nin tasks"
        },
        {
          "the adapting stage, PT strategies are also known as\nsupervised DA methods.": "involving the processing of EEG signals in the emotion recognition field."
        },
        {
          "the adapting stage, PT strategies are also known as\nsupervised DA methods.": "Domain Adaptation (DA) methods"
        },
        {
          "the adapting stage, PT strategies are also known as\nsupervised DA methods.": "Several DA methods relied on minimising the discrepance measures between the"
        },
        {
          "the adapting stage, PT strategies are also known as\nsupervised DA methods.": "Source and the Target domain.\nIn Ganin and Lempitsky (2015) these methods"
        },
        {
          "the adapting stage, PT strategies are also known as\nsupervised DA methods.": "are categorised into shallow and deep DA. As will be explained in the section"
        },
        {
          "the adapting stage, PT strategies are also known as\nsupervised DA methods.": "(Proposed Taxonomy), an extension of this taxonomy is proposed in this paper,"
        },
        {
          "the adapting stage, PT strategies are also known as\nsupervised DA methods.": "by adding the Source selection methods:"
        }
      ],
      "page": 15
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "are categorised into shallow and deep DA. As will be explained in the section": "(Proposed Taxonomy), an extension of this taxonomy is proposed in this paper,"
        },
        {
          "are categorised into shallow and deep DA. As will be explained in the section": "by adding the Source selection methods:"
        },
        {
          "are categorised into shallow and deep DA. As will be explained in the section": "a subset of DA methods take into account that not all\n• Source selection:"
        },
        {
          "are categorised into shallow and deep DA. As will be explained in the section": "the training data can effectively be useful\nfor the target space. Therefore,"
        },
        {
          "are categorised into shallow and deep DA. As will be explained in the section": "a selection of the training data is made,\nin order to avoid negative trans-"
        },
        {
          "are categorised into shallow and deep DA. As will be explained in the section": "fer. Several of these methods are also known as Instance weighting Jiang"
        },
        {
          "are categorised into shallow and deep DA. As will be explained in the section": "(2008), since they assign weights to the data; they can be made as a kind"
        },
        {
          "are categorised into shallow and deep DA. As will be explained in the section": "of preprocessing for the other methods;"
        },
        {
          "are categorised into shallow and deep DA. As will be explained in the section": "• Shallow DA: the data representation is given a-priori. Only a mapping be-"
        },
        {
          "are categorised into shallow and deep DA. As will be explained in the section": "tween the Source and Target representations is learned, without affecting"
        },
        {
          "are categorised into shallow and deep DA. As will be explained in the section": "the starting data representation;"
        }
      ],
      "page": 15
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "TrAdaBoost. This family of methods can be also used as initial step of another": "DA method,\nfor example in Lin et al. (2017); Zhou et al. (2020a) the similarity"
        },
        {
          "TrAdaBoost. This family of methods can be also used as initial step of another": "between source and target EEG data is measured using the Pearson Correlation"
        },
        {
          "TrAdaBoost. This family of methods can be also used as initial step of another": "Coefficient and the Average Frechet Distance\nrespectively.\nIn particular,\nin"
        },
        {
          "TrAdaBoost. This family of methods can be also used as initial step of another": "Zhou et al.\n(2020a) only the\ncloser EEG source data to the\ntarget data are"
        },
        {
          "TrAdaBoost. This family of methods can be also used as initial step of another": "fed to TCA together with the\ntarget one.\nFinally,\nthe\nclassification step is"
        },
        {
          "TrAdaBoost. This family of methods can be also used as initial step of another": "made by an Echo State Network (ESN, Ozturk et al.\n(2007)).\nIn Hua et al."
        },
        {
          "TrAdaBoost. This family of methods can be also used as initial step of another": "(2021), Neighborhood Component Analysis\n(NCA, Kenneth et al.\n(2020))\nis"
        },
        {
          "TrAdaBoost. This family of methods can be also used as initial step of another": "employed to learn the Mahalanobis distance between data and linearly project"
        },
        {
          "TrAdaBoost. This family of methods can be also used as initial step of another": "them into a subspace such that\nthe classification accuracy is maximized and"
        },
        {
          "TrAdaBoost. This family of methods can be also used as initial step of another": "to reduce the dimensionality of\nthe EEG features. The obtained features are"
        },
        {
          "TrAdaBoost. This family of methods can be also used as initial step of another": "then used with Geodesic flow kernel for Unsupervised Domain Adaptation Gong"
        },
        {
          "TrAdaBoost. This family of methods can be also used as initial step of another": "et al. (2012).\nIn Wang et al. (2021a) (DMATN) data belonging to the existing"
        },
        {
          "TrAdaBoost. This family of methods can be also used as initial step of another": "subjects are divided into several sub-source domains. Then, a set of sub-source"
        },
        {
          "TrAdaBoost. This family of methods can be also used as initial step of another": "are chosen as the most relevant with the target data. The proposed architecture"
        },
        {
          "TrAdaBoost. This family of methods can be also used as initial step of another": "combines\ntogether DAN and DANN to learn representation domain invariant"
        },
        {
          "TrAdaBoost. This family of methods can be also used as initial step of another": "representation."
        },
        {
          "TrAdaBoost. This family of methods can be also used as initial step of another": "Shallow DA methods"
        },
        {
          "TrAdaBoost. This family of methods can be also used as initial step of another": "Different strategies were proposed in literature, usually relied on one of following"
        },
        {
          "TrAdaBoost. This family of methods can be also used as initial step of another": "alternatives:"
        }
      ],
      "page": 16
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "uses all the source subject as a whole as they belong to the same domain.\nIn": "other words, the data belonging to different training subjects are viewed as an"
        },
        {
          "uses all the source subject as a whole as they belong to the same domain.\nIn": "unique subject. Differently, in Chai et al. (2018) (Multi-Subject Subspace Align-"
        },
        {
          "uses all the source subject as a whole as they belong to the same domain.\nIn": "ment, MSSA) the ASFM strategy is applied to each source subject individually,"
        },
        {
          "uses all the source subject as a whole as they belong to the same domain.\nIn": "then the projected data are fed to different for-subject classifiers."
        },
        {
          "uses all the source subject as a whole as they belong to the same domain.\nIn": "Other data transformations have been investigated in the DA scenario for"
        },
        {
          "uses all the source subject as a whole as they belong to the same domain.\nIn": "EEG emotion recognition, such as Robust Principal Component Analysis Wright"
        },
        {
          "uses all the source subject as a whole as they belong to the same domain.\nIn": "et al. (2009) in Lin (2019). RCA decomposes a set X of data as X = L + S,"
        },
        {
          "uses all the source subject as a whole as they belong to the same domain.\nIn": "the former is a low-rank matrix,\nwhere L and S are two superimposed matrices:"
        },
        {
          "uses all the source subject as a whole as they belong to the same domain.\nIn": "the latter a sparse matrix. These matrices are computed resolving the following"
        },
        {
          "uses all the source subject as a whole as they belong to the same domain.\nIn": "optimisation problem:"
        },
        {
          "uses all the source subject as a whole as they belong to the same domain.\nIn": "min\n||L||∗ + λ||S||1"
        },
        {
          "uses all the source subject as a whole as they belong to the same domain.\nIn": "L,S"
        },
        {
          "uses all the source subject as a whole as they belong to the same domain.\nIn": "where || · ||∗\n|| · ||1\nthe l1 norm and λ a weighting"
        },
        {
          "uses all the source subject as a whole as they belong to the same domain.\nIn": "parameter.\nIn Lin (2019) a proposal to use RPCA to build a Cross-Day emotion"
        },
        {
          "uses all the source subject as a whole as they belong to the same domain.\nIn": "recognition model\nis made."
        },
        {
          "uses all the source subject as a whole as they belong to the same domain.\nIn": "In Li et al. (2019b) a method for personalised handwriting recognition (Style"
        },
        {
          "uses all the source subject as a whole as they belong to the same domain.\nIn": "Transfer Mapping, STM Zhang and Liu (2012))\nis adapted for EEG emotion"
        },
        {
          "uses all the source subject as a whole as they belong to the same domain.\nIn": "recognition task to generalise across different\nsubjects.\nIn a nutshell, STM"
        },
        {
          "uses all the source subject as a whole as they belong to the same domain.\nIn": "maps source data to target data by an affine transformation. The solution of"
        },
        {
          "uses all the source subject as a whole as they belong to the same domain.\nIn": "the proposed problem is\nin closed form,\nso it\ncan be\neasily computed.\nFew"
        },
        {
          "uses all the source subject as a whole as they belong to the same domain.\nIn": "labeled target data are used to make a source data selection, so starting from"
        },
        {
          "uses all the source subject as a whole as they belong to the same domain.\nIn": "the hypothesis that a small amount of\nlabeled data are available. On the other"
        },
        {
          "uses all the source subject as a whole as they belong to the same domain.\nIn": "side,\nins Shared space-based the Maximum Mean Discrepancy (MMD,Gretton"
        },
        {
          "uses all the source subject as a whole as they belong to the same domain.\nIn": "et al. (2006)) is one of the currectly most used discrepancy measure in DA/DG"
        },
        {
          "uses all the source subject as a whole as they belong to the same domain.\nIn": "strategies.\nIn the original\nstudy, MMD is proposed to test\nif\ntwo probability"
        },
        {
          "uses all the source subject as a whole as they belong to the same domain.\nIn": "distributions p and q are different or not. Formally, the authors show that in a"
        },
        {
          "uses all the source subject as a whole as they belong to the same domain.\nIn": "Reduced Kernel Hilbert Space (RKHS) a discrepancy measure between the two"
        },
        {
          "uses all the source subject as a whole as they belong to the same domain.\nIn": "distributions can be defined as"
        },
        {
          "uses all the source subject as a whole as they belong to the same domain.\nIn": "M M D(p, q) = ||EXS ∼p(ϕ(XS)) − EXT ∼q(ϕ(XT ))||2\nH"
        },
        {
          "uses all the source subject as a whole as they belong to the same domain.\nIn": "In Gretton et al. (2006) is proven\nwhere ϕ(·) is an appropriate feature mapping."
        },
        {
          "uses all the source subject as a whole as they belong to the same domain.\nIn": "that,\nin a RKHS, M M D(p, q) is 0 if and only if the two distributions p and q"
        },
        {
          "uses all the source subject as a whole as they belong to the same domain.\nIn": "are the same."
        },
        {
          "uses all the source subject as a whole as they belong to the same domain.\nIn": "MMD can be\nempirical\nestimated as\nthe difference between the averages of"
        },
        {
          "uses all the source subject as a whole as they belong to the same domain.\nIn": "two data sampled from the two distributions projected in a RKHS. Therefore,"
        },
        {
          "uses all the source subject as a whole as they belong to the same domain.\nIn": "considering XS and XT"
        },
        {
          "uses all the source subject as a whole as they belong to the same domain.\nIn": "domain respectively, empirical M M D(XS, XT ) can be expressed as:"
        }
      ],
      "page": 17
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "two distributions can be estimate through the distance between two means of": "the samples projected in a RKHS."
        },
        {
          "two distributions can be estimate through the distance between two means of": "Transfer Component Analysis (TCA, Pan et al. (2010)) is one of the most"
        },
        {
          "two distributions can be estimate through the distance between two means of": "used MMD-based DA method.\nIn the original work, two different TCA versions"
        },
        {
          "two distributions can be estimate through the distance between two means of": "were proposed:\ni) an unsupervised version, where a transformation of the data"
        },
        {
          "two distributions can be estimate through the distance between two means of": "is\nfound such that\nthe data variance is maximally preserved reducing, at\nthe"
        },
        {
          "two distributions can be estimate through the distance between two means of": "same time, the MMD distance of the domains distributions, and ii) a supervised"
        },
        {
          "two distributions can be estimate through the distance between two means of": "one, where the data dependence with the training labels is taken into account."
        },
        {
          "two distributions can be estimate through the distance between two means of": "An evaluation of the unsupervised TCA on EEG data for emotion recognition"
        },
        {
          "two distributions can be estimate through the distance between two means of": "was made in Zheng et al. (2015).\nInstead of using all the available EEG data,"
        },
        {
          "two distributions can be estimate through the distance between two means of": "a random selection of a subset of samples from Source domain data was made"
        },
        {
          "two distributions can be estimate through the distance between two means of": "during the evaluation strategy,\nletting out a subject as Target domain.\nIn Xue"
        },
        {
          "two distributions can be estimate through the distance between two means of": "et al. (2020) TCA is tested on SEED dataset trying several desired dimensions"
        },
        {
          "two distributions can be estimate through the distance between two means of": "for the feature space.\nInstead,\nin He et al. (2022b) TCA is tested on self-made"
        },
        {
          "two distributions can be estimate through the distance between two means of": "EEG data."
        },
        {
          "two distributions can be estimate through the distance between two means of": "In Long et al. (2013) Transfer Sparse Coding (TSC) the MMD was exploited"
        },
        {
          "two distributions can be estimate through the distance between two means of": "to find a sparse representation of image data sampled from different distribution."
        },
        {
          "two distributions can be estimate through the distance between two means of": "Sparse\ncode\nrepresentations are well-known data approximation obtained as"
        },
        {
          "two distributions can be estimate through the distance between two means of": "linear\ncombinations of\nelements\nin a set of basis\nfunctions.\nIn a nutshell, a"
        },
        {
          "two distributions can be estimate through the distance between two means of": "sparse coding method searches\nfor a representative over-complete set of basis"
        },
        {
          "two distributions can be estimate through the distance between two means of": "functions (a dictionary) together with an encoding that best represent the data."
        },
        {
          "two distributions can be estimate through the distance between two means of": "In its simplest form, the sparse coding problem can be expressed as"
        }
      ],
      "page": 18
    },
    {
      "caption": "Table 2: , CSS represents the category of studies exhibit-",
      "data": [
        {
          "Zheng et al.\n(2015).\nIn Chai et al.\n(2016)\n(SAAE), a features\ntransformation": "ϕ(·) is computed through Kernel-PCA maximising the embedded data variance."
        },
        {
          "Zheng et al.\n(2015).\nIn Chai et al.\n(2016)\n(SAAE), a features\ntransformation": "Before the transformation, an autoencoder trained on data from both the Source"
        },
        {
          "Zheng et al.\n(2015).\nIn Chai et al.\n(2016)\n(SAAE), a features\ntransformation": "and the Target Domains was employed to preprocess the data.\nIn Zheng and"
        },
        {
          "Zheng et al.\n(2015).\nIn Chai et al.\n(2016)\n(SAAE), a features\ntransformation": "Lu (2016) TCA, KPCA, TSVM, and TPT are\nevaluated on the EEG-based"
        },
        {
          "Zheng et al.\n(2015).\nIn Chai et al.\n(2016)\n(SAAE), a features\ntransformation": "emotional SEED dataset\nin a Leave-On-Subject-Out approach, while\nin Lan"
        },
        {
          "Zheng et al.\n(2015).\nIn Chai et al.\n(2016)\n(SAAE), a features\ntransformation": "et al.\n(2018)\nsimilar methods are tested on SEED and DEAP also for Cross-"
        },
        {
          "Zheng et al.\n(2015).\nIn Chai et al.\n(2016)\n(SAAE), a features\ntransformation": "Dataset generalisation."
        },
        {
          "Zheng et al.\n(2015).\nIn Chai et al.\n(2016)\n(SAAE), a features\ntransformation": "Deep DA methods"
        },
        {
          "Zheng et al.\n(2015).\nIn Chai et al.\n(2016)\n(SAAE), a features\ntransformation": "In deep DA approaches, a feature data representation learning is embedded in"
        },
        {
          "Zheng et al.\n(2015).\nIn Chai et al.\n(2016)\n(SAAE), a features\ntransformation": "the DA method.\nInstead of searching for a transformation of\nfeatures given a"
        },
        {
          "Zheng et al.\n(2015).\nIn Chai et al.\n(2016)\n(SAAE), a features\ntransformation": "priori, this is done by changing the feature space representation."
        },
        {
          "Zheng et al.\n(2015).\nIn Chai et al.\n(2016)\n(SAAE), a features\ntransformation": "Deep DA methods can be further divided in:"
        }
      ],
      "page": 19
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Multi-Spatial Domain Adaptation Network (MSDAN) aligns source and target": "domain considering the\nspatial\nrelationships between the\nelectrodes.\nThis\nis"
        },
        {
          "Multi-Spatial Domain Adaptation Network (MSDAN) aligns source and target": "done by using Graph Convolutional Layers and exploiting MMD distance\nin"
        },
        {
          "Multi-Spatial Domain Adaptation Network (MSDAN) aligns source and target": "the resulting graph space. Differently from other works, Kuang et al.\n(2021)"
        },
        {
          "Multi-Spatial Domain Adaptation Network (MSDAN) aligns source and target": "uses data acquired in a Virtual Reality (VR)\nenvironment\nto generate\nstim-"
        },
        {
          "Multi-Spatial Domain Adaptation Network (MSDAN) aligns source and target": "uli, and the cross device problem is taken into account. One of the most used"
        },
        {
          "Multi-Spatial Domain Adaptation Network (MSDAN) aligns source and target": "deep DA strategies is the Domain Adversarial Learning, proposed in Ganin and"
        },
        {
          "Multi-Spatial Domain Adaptation Network (MSDAN) aligns source and target": "Lempitsky (2015); Ajakan et al. (2014); Ganin et al. (2016). The authors pro-"
        },
        {
          "Multi-Spatial Domain Adaptation Network (MSDAN) aligns source and target": "posed an embedded problem formulation considering both the desired task and"
        },
        {
          "Multi-Spatial Domain Adaptation Network (MSDAN) aligns source and target": "the discrepancy between the Source and the Target domain. The basic idea is"
        },
        {
          "Multi-Spatial Domain Adaptation Network (MSDAN) aligns source and target": "to make the data distributions\nindistinguishable for an ad-hoc domain classi-"
        },
        {
          "Multi-Spatial Domain Adaptation Network (MSDAN) aligns source and target": "fier. This can be made by a deep neural network model (Domain Adversarial"
        },
        {
          "Multi-Spatial Domain Adaptation Network (MSDAN) aligns source and target": "Neural Network, DANN) that,\nfor each input, predicts both the corresponding"
        },
        {
          "Multi-Spatial Domain Adaptation Network (MSDAN) aligns source and target": "class and the belonging domain.\nIn a nutshell, DANN is\ncomposed of\nthree"
        },
        {
          "Multi-Spatial Domain Adaptation Network (MSDAN) aligns source and target": "main components:\na feature extractor, a label predictor, and a domain classi-"
        },
        {
          "Multi-Spatial Domain Adaptation Network (MSDAN) aligns source and target": "fier. Therefore, a learning process searches for a feature mapping maximising"
        },
        {
          "Multi-Spatial Domain Adaptation Network (MSDAN) aligns source and target": "the class prediction performances and, at\nthe same time, maximising the do-"
        },
        {
          "Multi-Spatial Domain Adaptation Network (MSDAN) aligns source and target": "main classification loss to make the feature distributions as similar as possible."
        },
        {
          "Multi-Spatial Domain Adaptation Network (MSDAN) aligns source and target": "DANN is evaluated in EEG emotion recognition task in Jin et al.\n(2017) on"
        },
        {
          "Multi-Spatial Domain Adaptation Network (MSDAN) aligns source and target": "SEED.\nIn Li et al.\n(2018d) BiDANN, a DANN variation is adopted for EEG"
        },
        {
          "Multi-Spatial Domain Adaptation Network (MSDAN) aligns source and target": "emotion recognition, but considering the differences between the brain hemi-"
        },
        {
          "Multi-Spatial Domain Adaptation Network (MSDAN) aligns source and target": "spheres,\nis proposed.\nIn a nutshell, EEG data from the two hemispheres are"
        },
        {
          "Multi-Spatial Domain Adaptation Network (MSDAN) aligns source and target": "processed separately:\ntwo different\nfeatures mapping together with a domain"
        },
        {
          "Multi-Spatial Domain Adaptation Network (MSDAN) aligns source and target": "discriminator are learned for the brain hemispheres,\ninstead of only one feature"
        },
        {
          "Multi-Spatial Domain Adaptation Network (MSDAN) aligns source and target": "mapping as\nin the original DANN formulation. Difference between the hemi-"
        },
        {
          "Multi-Spatial Domain Adaptation Network (MSDAN) aligns source and target": "spheres in a DA approach is not dealt only by BiDANN;\nfor instance, BiHDM"
        },
        {
          "Multi-Spatial Domain Adaptation Network (MSDAN) aligns source and target": "Li et al. (2018e, 2020d) uses two different RNN to code the data belonging to"
        },
        {
          "Multi-Spatial Domain Adaptation Network (MSDAN) aligns source and target": "the two hemispheres, and also in this case a domain discriminator\nis used to"
        },
        {
          "Multi-Spatial Domain Adaptation Network (MSDAN) aligns source and target": "mix up the features of the Source and the Target domain.\nIn He et al. (2022a)"
        },
        {
          "Multi-Spatial Domain Adaptation Network (MSDAN) aligns source and target": "the authors propose a new DA method which is framed in the context of deep"
        },
        {
          "Multi-Spatial Domain Adaptation Network (MSDAN) aligns source and target": "adversarial learning approaches.\nIn particular a temporal convolutional network"
        },
        {
          "Multi-Spatial Domain Adaptation Network (MSDAN) aligns source and target": "is used as encoder.\nInterestingly, the method is successfully evaluated in both"
        },
        {
          "Multi-Spatial Domain Adaptation Network (MSDAN) aligns source and target": "cross-subject and cross-dataset.\nIn Ye et al. (2021); Zhong et al. (2020) domain"
        },
        {
          "Multi-Spatial Domain Adaptation Network (MSDAN) aligns source and target": "adversarial approaches are used together with Graph Neural Networks (GNN,"
        },
        {
          "Multi-Spatial Domain Adaptation Network (MSDAN) aligns source and target": "Zhou et al. (2020b)) as feature extractor.\nIn particular, Ye et al. (2021) leverages"
        },
        {
          "Multi-Spatial Domain Adaptation Network (MSDAN) aligns source and target": "on an attention mechanism Niu et al. (2021) focusing the learning stage on the"
        },
        {
          "Multi-Spatial Domain Adaptation Network (MSDAN) aligns source and target": "alignment of the more changeling areas of the feature space. Performances are"
        },
        {
          "Multi-Spatial Domain Adaptation Network (MSDAN) aligns source and target": "evaluated on SEED dataset.\nInstead, Zhong et al. (2020) proposes a node-wise"
        },
        {
          "Multi-Spatial Domain Adaptation Network (MSDAN) aligns source and target": "domain adversarial training (NodeDAT) method to regularise the GNN output"
        },
        {
          "Multi-Spatial Domain Adaptation Network (MSDAN) aligns source and target": "for better subject-independent performances.\nIn EEG literature, Domain adver-"
        },
        {
          "Multi-Spatial Domain Adaptation Network (MSDAN) aligns source and target": "sarial\nlearning is widely used in several other studies for EEG data recognition,"
        },
        {
          "Multi-Spatial Domain Adaptation Network (MSDAN) aligns source and target": "for example in Tzeng et al.\n(2017); Bao et al.\n(2020); Li et al.\n(2019d,a); Fu-"
        },
        {
          "Multi-Spatial Domain Adaptation Network (MSDAN) aligns source and target": "rukawa et al. (2021); Hwang et al. (2020b).\nIn particular,\nin Li et al. (2019d)"
        },
        {
          "Multi-Spatial Domain Adaptation Network (MSDAN) aligns source and target": "possible differences between several brain regions are also taken into account"
        },
        {
          "Multi-Spatial Domain Adaptation Network (MSDAN) aligns source and target": "with a proposed attention module.\nIn Du et al. (2020) (ATtention-based LSTM"
        },
        {
          "Multi-Spatial Domain Adaptation Network (MSDAN) aligns source and target": "with Domain Discriminator, ATDD-LSTM) a domain discriminator in terms of"
        }
      ],
      "page": 20
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "LSTMs\nis presented to reduce the discrepance between the distributions. An": "attention-based encoder-decoder\nfocuses on emotion-related helping the final"
        },
        {
          "LSTMs\nis presented to reduce the discrepance between the distributions. An": "classification probability estimation. An interesting adversarial approach was"
        },
        {
          "LSTMs\nis presented to reduce the discrepance between the distributions. An": "also investigated in Wang et al. (2021c). The proposed work exploits the Covari-"
        },
        {
          "LSTMs\nis presented to reduce the discrepance between the distributions. An": "ance Matrices between EEG data and Riemannian distances Barbaresco (2008)."
        },
        {
          "LSTMs\nis presented to reduce the discrepance between the distributions. An": "The work proposed a new kind of Neural Network (daSPDnet) able to retain the"
        },
        {
          "LSTMs\nis presented to reduce the discrepance between the distributions. An": "intrinsic geometry information of the data. However, differently from the typical"
        },
        {
          "LSTMs\nis presented to reduce the discrepance between the distributions. An": "DA approach, a little set of\nlabelled data belonging to the Target domain are"
        },
        {
          "LSTMs\nis presented to reduce the discrepance between the distributions. An": "required during the training process, resulting as semi-supervised DA method."
        },
        {
          "LSTMs\nis presented to reduce the discrepance between the distributions. An": "A similar approach, also requiring a few of\nlabelled target data, was proposed"
        },
        {
          "LSTMs\nis presented to reduce the discrepance between the distributions. An": "in Li et al.\n(2021c). Multi-source Domain Transfer Discriminative Dictionary"
        },
        {
          "LSTMs\nis presented to reduce the discrepance between the distributions. An": "Learning modeling (MDTDDL) is developed in Gu et al. (2022); the aim is to"
        },
        {
          "LSTMs\nis presented to reduce the discrepance between the distributions. An": "learn a joint subspace between source and target domains exploiting dictionary"
        },
        {
          "LSTMs\nis presented to reduce the discrepance between the distributions. An": "learning Mairal et al.\n(2008) methods. DEEP and SEED are evaluated both"
        },
        {
          "LSTMs\nis presented to reduce the discrepance between the distributions. An": "in Cross-Subject and Cross-Session mode.\nIn Tzeng et al.\n(2017) Adversarial"
        },
        {
          "LSTMs\nis presented to reduce the discrepance between the distributions. An": "Discriminative Domain Adaptation (ADDA), a strategy to tackle the DA on"
        },
        {
          "LSTMs\nis presented to reduce the discrepance between the distributions. An": "an image classification task, was proposed. Differently from DANN, the ADDA"
        },
        {
          "LSTMs\nis presented to reduce the discrepance between the distributions. An": "basic idea consisted in building two different functions for the Source and the"
        },
        {
          "LSTMs\nis presented to reduce the discrepance between the distributions. An": "encoders ES\nand ET ,"
        },
        {
          "LSTMs\nis presented to reduce the discrepance between the distributions. An": "tively. ES is trained together with a classifier C using labelled data from the"
        },
        {
          "LSTMs\nis presented to reduce the discrepance between the distributions. An": "learning procedure, ET is trained"
        },
        {
          "LSTMs\nis presented to reduce the discrepance between the distributions. An": "to map the Target domain data in the space of ES outputs."
        },
        {
          "LSTMs\nis presented to reduce the discrepance between the distributions. An": "data can be classified by C. A similar idea was adapted in EEG emotion clas-"
        },
        {
          "LSTMs\nis presented to reduce the discrepance between the distributions. An": "sification domain in Luo et al. (2018) (Wasserstein GAN Domain Adaptation,"
        },
        {
          "LSTMs\nis presented to reduce the discrepance between the distributions. An": "WGANDA), mixing together a pre-training stage and an adversarial\ntraining"
        },
        {
          "LSTMs\nis presented to reduce the discrepance between the distributions. An": "stage. More in detail, two generators for the source and target domain respec-"
        },
        {
          "LSTMs\nis presented to reduce the discrepance between the distributions. An": "tively are pre-trained to output\ntwo feature vectors of\nthe same size.\nThese"
        },
        {
          "LSTMs\nis presented to reduce the discrepance between the distributions. An": "vectors are considered as belonging to a shared feature space. An adversarial"
        },
        {
          "LSTMs\nis presented to reduce the discrepance between the distributions. An": "training step based on minimising the Wasserstein distance tunes the parame-"
        },
        {
          "LSTMs\nis presented to reduce the discrepance between the distributions. An": "ters of the generators such that the outputs match more closely as possible each"
        },
        {
          "LSTMs\nis presented to reduce the discrepance between the distributions. An": "other.\nThe combined outputs are then used as\ninput\nfor an output classifier"
        },
        {
          "LSTMs\nis presented to reduce the discrepance between the distributions. An": "network.\nInspired by the MMD optimization made in Chai et al. (2016),\nin Bao"
        },
        {
          "LSTMs\nis presented to reduce the discrepance between the distributions. An": "et al. (2020)(TDANN) a two stage DA method is proposed.\nIn the first stage,"
        },
        {
          "LSTMs\nis presented to reduce the discrepance between the distributions. An": "MMD is minimized training a CNN equipped with adaBN Li et al. (2018c). To"
        },
        {
          "LSTMs\nis presented to reduce the discrepance between the distributions. An": "be fed to the CNN and to preserve spatial\ninformation, the EEG input signals"
        },
        {
          "LSTMs\nis presented to reduce the discrepance between the distributions. An": "are transformed into images Bashivan et al.\n(2015); Hwang et al.\n(2020a).\nIn"
        },
        {
          "LSTMs\nis presented to reduce the discrepance between the distributions. An": "the second stage, a domain discriminator is used to further reduce the distance"
        },
        {
          "LSTMs\nis presented to reduce the discrepance between the distributions. An": "between the source and the target distributions. The method was evaluated in"
        },
        {
          "LSTMs\nis presented to reduce the discrepance between the distributions. An": "a leave-one-subject out cross validation framework. One of\nthe main issue of"
        },
        {
          "LSTMs\nis presented to reduce the discrepance between the distributions. An": "the DANN networks is that no label\nis considered during the adversarial\nlearn-"
        },
        {
          "LSTMs\nis presented to reduce the discrepance between the distributions. An": "ing process, therefore the relationship between target data and the task-specific"
        },
        {
          "LSTMs\nis presented to reduce the discrepance between the distributions. An": "decision boundary during the distributions alignment is not taken into account."
        },
        {
          "LSTMs\nis presented to reduce the discrepance between the distributions. An": "A DA method can confuse the distributions of the two domains by reducing the"
        },
        {
          "LSTMs\nis presented to reduce the discrepance between the distributions. An": "distance between them, resulting in a simple mixing of the samples of the two"
        },
        {
          "LSTMs\nis presented to reduce the discrepance between the distributions. An": "domains,\nleading the categories within each domain to not be distinguishable."
        },
        {
          "LSTMs\nis presented to reduce the discrepance between the distributions. An": "Indeed,\nin DANN the decision boundary inside each domain is ignored.\nIn Saito"
        }
      ],
      "page": 21
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "et al.\n(2018)\n(Maximum Classifier Discrepancy, MCD),\ninstead,\nthe labels of": "the Source domain data are considered, helping to search a good task-specific"
        },
        {
          "et al.\n(2018)\n(Maximum Classifier Discrepancy, MCD),\ninstead,\nthe labels of": "decision boundaries between the\nclasses.\nThis\nis achieved by using different"
        },
        {
          "et al.\n(2018)\n(Maximum Classifier Discrepancy, MCD),\ninstead,\nthe labels of": "classifiers\nfed with the same inputs and evaluating the discrepancy. More in"
        },
        {
          "et al.\n(2018)\n(Maximum Classifier Discrepancy, MCD),\ninstead,\nthe labels of": "detail, two classifier C1 and C2 with the same characteristics are fed with input"
        },
        {
          "et al.\n(2018)\n(Maximum Classifier Discrepancy, MCD),\ninstead,\nthe labels of": "of feature generator G. G can be fed with data x coming from the source or the"
        },
        {
          "et al.\n(2018)\n(Maximum Classifier Discrepancy, MCD),\ninstead,\nthe labels of": "target domain. The output of C1 and C2 are the labels of the input x returned"
        },
        {
          "et al.\n(2018)\n(Maximum Classifier Discrepancy, MCD),\ninstead,\nthe labels of": "start\nfrom different\ninitial\nstate,"
        },
        {
          "et al.\n(2018)\n(Maximum Classifier Discrepancy, MCD),\ninstead,\nthe labels of": "Before the training step, C1 and C2"
        },
        {
          "et al.\n(2018)\n(Maximum Classifier Discrepancy, MCD),\ninstead,\nthe labels of": "rising two different classifiers after the training. How much the two classifiers"
        },
        {
          "et al.\n(2018)\n(Maximum Classifier Discrepancy, MCD),\ninstead,\nthe labels of": "disagree on their predictions on the same input\nis defined discrepancy by the"
        },
        {
          "et al.\n(2018)\n(Maximum Classifier Discrepancy, MCD),\ninstead,\nthe labels of": "authors.\nIndeed, the generator G is trained to minimise the discrepancy (that is,"
        },
        {
          "et al.\n(2018)\n(Maximum Classifier Discrepancy, MCD),\ninstead,\nthe labels of": "project source and target data in the same space), while C1 and C2 are trained"
        },
        {
          "et al.\n(2018)\n(Maximum Classifier Discrepancy, MCD),\ninstead,\nthe labels of": "to maximise the discrepancy (so that the two classification boundaries are far"
        },
        {
          "et al.\n(2018)\n(Maximum Classifier Discrepancy, MCD),\ninstead,\nthe labels of": "from each other). The learned generator G will be able to relocate the target"
        },
        {
          "et al.\n(2018)\n(Maximum Classifier Discrepancy, MCD),\ninstead,\nthe labels of": "domain data in the\nsource\nspace, but\ntaking into account\nits most probable"
        },
        {
          "et al.\n(2018)\n(Maximum Classifier Discrepancy, MCD),\ninstead,\nthe labels of": "belonging class. Task-Specific Domain Adversarial Neural Network (T-DANN,"
        },
        {
          "et al.\n(2018)\n(Maximum Classifier Discrepancy, MCD),\ninstead,\nthe labels of": "Ding et al. (2021)) is an MCD similar model proposed for EEG emotion recogni-"
        },
        {
          "et al.\n(2018)\n(Maximum Classifier Discrepancy, MCD),\ninstead,\nthe labels of": "tion. T-DANN adapts the conditional distribution between domains and, at the"
        },
        {
          "et al.\n(2018)\n(Maximum Classifier Discrepancy, MCD),\ninstead,\nthe labels of": "same time, adapts classification boundaries between classes exploiting MCD in"
        },
        {
          "et al.\n(2018)\n(Maximum Classifier Discrepancy, MCD),\ninstead,\nthe labels of": "conjunction with a domain discriminator.\nInstead, Ning et al. (2021) deal with"
        },
        {
          "et al.\n(2018)\n(Maximum Classifier Discrepancy, MCD),\ninstead,\nthe labels of": "the excessive allignment problem exploiting a few-shot\nlearning and attention"
        },
        {
          "et al.\n(2018)\n(Maximum Classifier Discrepancy, MCD),\ninstead,\nthe labels of": "mechanism approach. From a different point of view, Wang et al. (2021b) used"
        },
        {
          "et al.\n(2018)\n(Maximum Classifier Discrepancy, MCD),\ninstead,\nthe labels of": "Siamese Networks Koch et al. (2015) for evaluating the similarity between sam-"
        },
        {
          "et al.\n(2018)\n(Maximum Classifier Discrepancy, MCD),\ninstead,\nthe labels of": "ples belonging to different domains. Siamese networks were originally proposed"
        },
        {
          "et al.\n(2018)\n(Maximum Classifier Discrepancy, MCD),\ninstead,\nthe labels of": "to to determine whether two inputs belong to the same category or not.\nIn Wang"
        },
        {
          "et al.\n(2018)\n(Maximum Classifier Discrepancy, MCD),\ninstead,\nthe labels of": "et al. (2021b) the Siamese framework is converted to handle different domains."
        },
        {
          "et al.\n(2018)\n(Maximum Classifier Discrepancy, MCD),\ninstead,\nthe labels of": "However,\nthis method require a few of\nlabelled data belonging to the Target"
        },
        {
          "et al.\n(2018)\n(Maximum Classifier Discrepancy, MCD),\ninstead,\nthe labels of": "domain.\nIn Tao and Dan (2021) the authors propose a DA approach for EEG-"
        },
        {
          "et al.\n(2018)\n(Maximum Classifier Discrepancy, MCD),\ninstead,\nthe labels of": "based emotion recognition based on a multi-source\nco-adaptation framework"
        },
        {
          "et al.\n(2018)\n(Maximum Classifier Discrepancy, MCD),\ninstead,\nthe labels of": "(MACI). The proposed framework mainly takes advantage of correlation knowl-"
        },
        {
          "et al.\n(2018)\n(Maximum Classifier Discrepancy, MCD),\ninstead,\nthe labels of": "edge among several sources and features to build a proper objective function."
        },
        {
          "et al.\n(2018)\n(Maximum Classifier Discrepancy, MCD),\ninstead,\nthe labels of": "The proposed method is compared with both standard (shallow) DA approaches"
        },
        {
          "et al.\n(2018)\n(Maximum Classifier Discrepancy, MCD),\ninstead,\nthe labels of": "and deep (CNN-based) approaches. Cross-subjects and cross-datasets evalua-"
        },
        {
          "et al.\n(2018)\n(Maximum Classifier Discrepancy, MCD),\ninstead,\nthe labels of": "tions are performed.\nComputational costs\nis a critical point of\nthe proposed"
        },
        {
          "et al.\n(2018)\n(Maximum Classifier Discrepancy, MCD),\ninstead,\nthe labels of": "framework."
        },
        {
          "et al.\n(2018)\n(Maximum Classifier Discrepancy, MCD),\ninstead,\nthe labels of": "The authors of Luo et al. (2021) propose a novel approach which attempts to"
        },
        {
          "et al.\n(2018)\n(Maximum Classifier Discrepancy, MCD),\ninstead,\nthe labels of": "unify in an unique optimisation problem two standard DA approaches,\ninstance"
        },
        {
          "et al.\n(2018)\n(Maximum Classifier Discrepancy, MCD),\ninstead,\nthe labels of": "reweighting (that we refer as source selection) and feature matching. This novel"
        },
        {
          "et al.\n(2018)\n(Maximum Classifier Discrepancy, MCD),\ninstead,\nthe labels of": "approach is named Progressive Low-Rank Subspace Alignment\n(PLRSA).\nIn"
        },
        {
          "et al.\n(2018)\n(Maximum Classifier Discrepancy, MCD),\ninstead,\nthe labels of": "particular,\ninstance reweighting is\nimplemented by minimizing the Maximum"
        },
        {
          "et al.\n(2018)\n(Maximum Classifier Discrepancy, MCD),\ninstead,\nthe labels of": "Mean Discrepancy (MMD) distance and the TrAdaBoost algorithm, and fea-"
        },
        {
          "et al.\n(2018)\n(Maximum Classifier Discrepancy, MCD),\ninstead,\nthe labels of": "ture matching by the Transfer Component Analysis (TCA). Importantly, a tiny"
        },
        {
          "et al.\n(2018)\n(Maximum Classifier Discrepancy, MCD),\ninstead,\nthe labels of": "amount of labeled target data is used to better exploit the source auxiliary data."
        },
        {
          "et al.\n(2018)\n(Maximum Classifier Discrepancy, MCD),\ninstead,\nthe labels of": "The proposed method is evaluated in a both cross-subjects and cross-sessions"
        },
        {
          "et al.\n(2018)\n(Maximum Classifier Discrepancy, MCD),\ninstead,\nthe labels of": "scenario. The method is compared with five state-of-the-art DA methods. The"
        },
        {
          "et al.\n(2018)\n(Maximum Classifier Discrepancy, MCD),\ninstead,\nthe labels of": "results seem promising, however the time complexity is a little more expensive"
        }
      ],
      "page": 22
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "than related state-of-the-art methods.": "Although several\nstudies\nstart\nfrom the hypothesis\nthat a shared feature"
        },
        {
          "than related state-of-the-art methods.": "space is enough for DA, Shared+Specific Space (SSS) methods go in different"
        },
        {
          "than related state-of-the-art methods.": "direction, believing that a single shared classifier built in a shared space still has"
        },
        {
          "than related state-of-the-art methods.": "poor performance for the never seen sessions/subjects. Notably,\nin these stud-"
        },
        {
          "than related state-of-the-art methods.": "ies each subject/session available is considered as a single domain, and not as"
        },
        {
          "than related state-of-the-art methods.": "a whole. Hypothetically, EEG data representations can be splitted into shared"
        },
        {
          "than related state-of-the-art methods.": "emotional components, universal\nto all\nthe subjects, and private components,"
        },
        {
          "than related state-of-the-art methods.": "specific to each subject."
        },
        {
          "than related state-of-the-art methods.": "Leveraging on this hypothesis, Zhao et al. (2021) builds a shared encoder and"
        },
        {
          "than related state-of-the-art methods.": "private encoders for each source subject data to capture the subject-invariant"
        },
        {
          "than related state-of-the-art methods.": "emotional representations and private components, respectively. The obtained"
        },
        {
          "than related state-of-the-art methods.": "encoders are then used to build several emotion classifiers. Finally, a new sub-"
        },
        {
          "than related state-of-the-art methods.": "ject classifier using few data is built.\nAll\nthese classifier are built exploiting"
        },
        {
          "than related state-of-the-art methods.": "the shared encoder. A classifier\nfusion strategy is\nthen applied to obtain the"
        },
        {
          "than related state-of-the-art methods.": "final\nclassification result.\nHowever,\nthe proposed framework requires\nfew la-"
        },
        {
          "than related state-of-the-art methods.": "belled target data,\nfalling in the unsupervised DA category. MEERNet Chen"
        },
        {
          "than related state-of-the-art methods.": "et al.\n(2021b) considers different classifiers\nfor each different domain (subject"
        },
        {
          "than related state-of-the-art methods.": "or\nsession), preceded by a feature extractor\nshared by all\nthe domains. Final"
        },
        {
          "than related state-of-the-art methods.": "classification is made averaging between domain-specific classifiers.\nSimilarly,"
        },
        {
          "than related state-of-the-art methods.": "Luo and Lu (2021) proposed a framework composed of a common feature ex-"
        },
        {
          "than related state-of-the-art methods.": "tractor\nto map all\nthe domains\nin a common subspace, a main task classifier"
        },
        {
          "than related state-of-the-art methods.": "or regressor, and private discriminators for each domain. The training is made"
        },
        {
          "than related state-of-the-art methods.": "reducing the Wasserstein distance between the marginal distribution of\neach"
        },
        {
          "than related state-of-the-art methods.": "source domain and target one in an adversarial way.\nIn Chen et al. (2021a) the"
        },
        {
          "than related state-of-the-art methods.": "authors propose a Multi Source-Marginal Distribution Adaptation (MS-MDA)"
        },
        {
          "than related state-of-the-art methods.": "algorithm for EEG emotion recognition. Also in this case, the key idea is that"
        },
        {
          "than related state-of-the-art methods.": "the final response is obtained by the average of the responses of target-source"
        },
        {
          "than related state-of-the-art methods.": "specific classifiers, preceded by a common feature extractor. Notably,\nthe au-"
        },
        {
          "than related state-of-the-art methods.": "thors explore the impact of different types of data normalisation on the perfor-"
        },
        {
          "than related state-of-the-art methods.": "mance of the proposed model. MS-MDA is compared with several standard DA"
        },
        {
          "than related state-of-the-art methods.": "methods and it has very promising results. Similarly, the authors of Cao et al."
        },
        {
          "than related state-of-the-art methods.": "(2021) propose Multi-Dource and Multi-Representation Adaptation (MSMRA),"
        },
        {
          "than related state-of-the-art methods.": "an approach with many similarities with respect MS-MDA algorithm Chen et al."
        },
        {
          "than related state-of-the-art methods.": "(2021a). Both cross-subjects and cross-sessions evaluations are performed."
        },
        {
          "than related state-of-the-art methods.": "Supervised DA (PreTraining) methods"
        },
        {
          "than related state-of-the-art methods.": "In the supervised Domain Adaptation category,\nfour\nstudies were included in"
        },
        {
          "than related state-of-the-art methods.": "the reviews.\nIn Cimtay and Ekmekcioglu (2020) a pretrained version of Incep-"
        },
        {
          "than related state-of-the-art methods.": "tionResnetV2 Szegedy et al. (2017) is used as feature extractor for EEG data."
        },
        {
          "than related state-of-the-art methods.": "The classification is made by a final network layer added to the InceptionRes-"
        },
        {
          "than related state-of-the-art methods.": "netV2 module.\nInstead, Pusarla et al.\n(2022)\nexploited DenseNet121 Huang"
        },
        {
          "than related state-of-the-art methods.": "et al.\n(2017) as pre-trained model\nto build a new architecture fed with EEG"
        },
        {
          "than related state-of-the-art methods.": "data transformed in spectrogram images."
        },
        {
          "than related state-of-the-art methods.": "In Wang et al. (2020) a CNN model trained on different subjects and sessions"
        }
      ],
      "page": 23
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "taken from the SEED dataset is then re-trained on a small amount of data of a": "target subject taken from DEAP dataset to evaluate the cross-dataset emotion"
        },
        {
          "taken from the SEED dataset is then re-trained on a small amount of data of a": "recognition performances.\nIn Li et al. (2020a) several classifiers trained on dif-"
        },
        {
          "taken from the SEED dataset is then re-trained on a small amount of data of a": "ferent data belonging to different subjects and sessions are ensembled together"
        },
        {
          "taken from the SEED dataset is then re-trained on a small amount of data of a": "obtaining a final\nclassifier\nsuitable both for\ncross-sessions and cross-subjects"
        },
        {
          "taken from the SEED dataset is then re-trained on a small amount of data of a": "EEG emotion recognition."
        },
        {
          "taken from the SEED dataset is then re-trained on a small amount of data of a": "Domain Generalization (DG) methods"
        },
        {
          "taken from the SEED dataset is then re-trained on a small amount of data of a": "Finally, differently from classical domain adaptation methods,\nin Domain Gen-"
        },
        {
          "taken from the SEED dataset is then re-trained on a small amount of data of a": "eralization data from several domains are available, but no data from the test"
        },
        {
          "taken from the SEED dataset is then re-trained on a small amount of data of a": "domain is observed during the training stage. Differently from classical domain"
        },
        {
          "taken from the SEED dataset is then re-trained on a small amount of data of a": "adaptation methods, data from several domains are available, but no data from"
        },
        {
          "taken from the SEED dataset is then re-trained on a small amount of data of a": "the test domain is observed during the training stage Muandet et al.\n(2013)."
        },
        {
          "taken from the SEED dataset is then re-trained on a small amount of data of a": "Methods can be divided as:"
        }
      ],
      "page": 24
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Figure 5: Pie charts for distribution of papers occurrences according to: a) gen-": "eralization types, b) categories of the taxonomy, c) used datasets, d) emotional"
        },
        {
          "Figure 5: Pie charts for distribution of papers occurrences according to: a) gen-": "theories."
        },
        {
          "Figure 5: Pie charts for distribution of papers occurrences according to: a) gen-": "to an embedding space while a decoder network is able to reconstruct the orig-"
        },
        {
          "Figure 5: Pie charts for distribution of papers occurrences according to: a) gen-": "inal\ninput from the embedding.\nIn the proposed work, the learned features are"
        },
        {
          "Figure 5: Pie charts for distribution of papers occurrences according to: a) gen-": "further refined by domain adversarial training across different subjects to learn"
        },
        {
          "Figure 5: Pie charts for distribution of papers occurrences according to: a) gen-": "subject-independent features. Furthermore, to maximize dataset intercompati-"
        },
        {
          "Figure 5: Pie charts for distribution of papers occurrences according to: a) gen-": "bility spectral topography data of the EEG signal are used as input."
        },
        {
          "Figure 5: Pie charts for distribution of papers occurrences according to: a) gen-": "The pie charts in Figure 5 show some statistics about the papers included"
        },
        {
          "Figure 5: Pie charts for distribution of papers occurrences according to: a) gen-": "in the survey. First of all,\nit is evident that almost three quarters of the studies"
        },
        {
          "Figure 5: Pie charts for distribution of papers occurrences according to: a) gen-": "surveyed (73.4 %) focus on a cross-subject mode of generalisation, while cross-"
        },
        {
          "Figure 5: Pie charts for distribution of papers occurrences according to: a) gen-": "session studies account for only 17 % and only 9 % operate a cross-dataset mode"
        },
        {
          "Figure 5: Pie charts for distribution of papers occurrences according to: a) gen-": "of generalisation. Graph b) shows the percentage distribution according to the"
        },
        {
          "Figure 5: Pie charts for distribution of papers occurrences according to: a) gen-": "proposed taxonomy.\nLooking at\nthis graph,\nit\nis evident\nthat\nthe majority of"
        },
        {
          "Figure 5: Pie charts for distribution of papers occurrences according to: a) gen-": "generalisation studies are moving towards the use of Deep Domain Adaptation"
        },
        {
          "Figure 5: Pie charts for distribution of papers occurrences according to: a) gen-": "(CSS)\n(33.33 %),\nat\nthe\nexpense of more\ntraditional approaches, which still"
        },
        {
          "Figure 5: Pie charts for distribution of papers occurrences according to: a) gen-": "retain 26.67 %.\nThis\nis\nfollowed by Shallow DA (SSB) approaches\n(9.33 %),"
        },
        {
          "Figure 5: Pie charts for distribution of papers occurrences according to: a) gen-": "Deap DA (SSS) (8 %), Source Selection DA (6.67 %), Shallow DA (TSB) and"
        },
        {
          "Figure 5: Pie charts for distribution of papers occurrences according to: a) gen-": "Supervised DA (5.33 %), Deep DG (4 %) and finally Transductive (1.33 %)."
        },
        {
          "Figure 5: Pie charts for distribution of papers occurrences according to: a) gen-": "The pie chart\nin Figure 5.c)\nshows\nthe number of\ntimes each EEG dataset\nis"
        },
        {
          "Figure 5: Pie charts for distribution of papers occurrences according to: a) gen-": "exploited in the reviewed literature. The mainly used datasets are SEED (45.8"
        },
        {
          "Figure 5: Pie charts for distribution of papers occurrences according to: a) gen-": "%) and DEAP (27.5 %). This is followed by 8.3 % of studies that propose their"
        },
        {
          "Figure 5: Pie charts for distribution of papers occurrences according to: a) gen-": "own self-produced dataset, while of the other datasets available in the literature"
        },
        {
          "Figure 5: Pie charts for distribution of papers occurrences according to: a) gen-": "only SEED IV stands out\n(at 7.5 %), which is\ninteresting in that\nit adopts a"
        },
        {
          "Figure 5: Pie charts for distribution of papers occurrences according to: a) gen-": "discrete space of\nfour emotions for classification (happy, sad,\nfear and neutral)."
        },
        {
          "Figure 5: Pie charts for distribution of papers occurrences according to: a) gen-": "Each of the other datasets do not exceed 5 % (MAHNOB Soleymani et al. (2011)"
        },
        {
          "Figure 5: Pie charts for distribution of papers occurrences according to: a) gen-": "and DREAMER Katsigiannis and Ramzan (2017) (3.33 %), CMEED Zhao et al."
        },
        {
          "Figure 5: Pie charts for distribution of papers occurrences according to: a) gen-": "(2018) (1.7 %), ASCERTAIN Subramanian et al. (2016), MDME Shenoy et al."
        },
        {
          "Figure 5: Pie charts for distribution of papers occurrences according to: a) gen-": "(2006) and SDMN Lin et al. (2010) (0.8 %))."
        },
        {
          "Figure 5: Pie charts for distribution of papers occurrences according to: a) gen-": "Finally, Figure 5.d) offers an interesting statistic about\nthe interest of\nthe"
        },
        {
          "Figure 5: Pie charts for distribution of papers occurrences according to: a) gen-": "authors of\nthe studies examined in the various perspectives of emotion repre-"
        },
        {
          "Figure 5: Pie charts for distribution of papers occurrences according to: a) gen-": "sentation. As already mentioned in section 1.3, the two dominant perspectives,"
        },
        {
          "Figure 5: Pie charts for distribution of papers occurrences according to: a) gen-": "and the only ones considered in the literature\nexamined, are\nthose based on"
        },
        {
          "Figure 5: Pie charts for distribution of papers occurrences according to: a) gen-": "categorical and dimensional models. More than 80 per cent of\nthe works are"
        }
      ],
      "page": 25
    },
    {
      "caption": "Table 2: Only cross-subject studies were considered in",
      "data": [
        {
          "based on a representation of emotions, and then their subsequent classification": "in terms of valence and arousal\n(and only in one case also dominance Wang"
        },
        {
          "based on a representation of emotions, and then their subsequent classification": "et al. (2021c))."
        },
        {
          "based on a representation of emotions, and then their subsequent classification": "Studies\nidentified as best performers\nin terms of mean classification accu-"
        },
        {
          "based on a representation of emotions, and then their subsequent classification": "racy are proposed in Table 2.\nOnly cross-subject\nstudies were\nconsidered in"
        },
        {
          "based on a representation of emotions, and then their subsequent classification": "this selection, being the statistically most significant percentage among all the"
        },
        {
          "based on a representation of emotions, and then their subsequent classification": "generalisation studies surveyed. The ten best results were identified considering"
        },
        {
          "based on a representation of emotions, and then their subsequent classification": "as many classification issues. Each issue is defined by considering the number"
        },
        {
          "based on a representation of emotions, and then their subsequent classification": "and type of classes identified (binary and ternary on valence and arousal, qua-"
        },
        {
          "based on a representation of emotions, and then their subsequent classification": "ternary on the two-dimensional valence-arousal plane, binary and quaternary"
        },
        {
          "based on a representation of emotions, and then their subsequent classification": "on discrete dimensions) and the dataset adopted (considering three possibili-"
        },
        {
          "based on a representation of emotions, and then their subsequent classification": "ties: DEAP, SEED, other). Only studies\nreporting both mean accuracy and"
        },
        {
          "based on a representation of emotions, and then their subsequent classification": "standard deviation were included in the performance assessment."
        }
      ],
      "page": 26
    },
    {
      "caption": "Table 1: Reviewed studies on generalization strategies for emotion recognition.",
      "data": [
        {
          "see section 7).": "Classifier Category"
        },
        {
          "see section 7).": ""
        },
        {
          "see section 7).": ""
        },
        {
          "see section 7).": ""
        },
        {
          "see section 7).": ""
        },
        {
          "see section 7).": ""
        },
        {
          "see section 7).": ""
        },
        {
          "see section 7).": ""
        },
        {
          "see section 7).": "CLASSICAL ML"
        },
        {
          "see section 7).": ""
        },
        {
          "see section 7).": ""
        },
        {
          "see section 7).": ""
        },
        {
          "see section 7).": ""
        },
        {
          "see section 7).": ""
        },
        {
          "see section 7).": ""
        },
        {
          "see section 7).": ""
        },
        {
          "see section 7).": ""
        },
        {
          "see section 7).": ""
        },
        {
          "see section 7).": "TRANSDUCTIVE"
        },
        {
          "see section 7).": ""
        },
        {
          "see section 7).": ""
        },
        {
          "see section 7).": "DOMAIN ADAPTATION"
        },
        {
          "see section 7).": "(SOURCE SELECTION)"
        },
        {
          "see section 7).": ""
        },
        {
          "see section 7).": ""
        },
        {
          "see section 7).": "SHALLOW DA (TSB)"
        },
        {
          "see section 7).": ""
        },
        {
          "see section 7).": ""
        },
        {
          "see section 7).": ""
        },
        {
          "see section 7).": ""
        },
        {
          "see section 7).": ""
        },
        {
          "see section 7).": "SHALLOW DA (SSB)"
        },
        {
          "see section 7).": ""
        },
        {
          "see section 7).": ""
        },
        {
          "see section 7).": ""
        },
        {
          "see section 7).": ""
        },
        {
          "see section 7).": ""
        },
        {
          "see section 7).": ""
        },
        {
          "see section 7).": ""
        },
        {
          "see section 7).": ""
        },
        {
          "see section 7).": ""
        },
        {
          "see section 7).": ""
        },
        {
          "see section 7).": ""
        },
        {
          "see section 7).": ""
        },
        {
          "see section 7).": ""
        },
        {
          "see section 7).": "DEEP DA (CSS)"
        },
        {
          "see section 7).": ""
        },
        {
          "see section 7).": ""
        },
        {
          "see section 7).": ""
        },
        {
          "see section 7).": ""
        },
        {
          "see section 7).": ""
        },
        {
          "see section 7).": ""
        },
        {
          "see section 7).": ""
        },
        {
          "see section 7).": ""
        },
        {
          "see section 7).": ""
        },
        {
          "see section 7).": ""
        },
        {
          "see section 7).": ""
        }
      ],
      "page": 27
    },
    {
      "caption": "Table 2: The most representative studies according to their classification ac-",
      "data": [
        {
          "Classifier Category": "",
          "Study": "Zhao et al. (2021)",
          "Dataset": "SEED",
          "Classifier": "PPDA",
          "Evaluation Strategy": "LOO",
          "Cross Subject": "X",
          "Cross Session": "",
          "Cross Dataset": ""
        },
        {
          "Classifier Category": "",
          "Study": "Chen et al. (2021b)",
          "Dataset": "SEED, SEED IV",
          "Classifier": "MEERNet",
          "Evaluation Strategy": "LOO",
          "Cross Subject": "X",
          "Cross Session": "X",
          "Cross Dataset": ""
        },
        {
          "Classifier Category": "DEEP DA (SSS)",
          "Study": "Liu et al. (2021)",
          "Dataset": "DEAP, sp",
          "Classifier": "DASC",
          "Evaluation Strategy": "LOO",
          "Cross Subject": "X",
          "Cross Session": "",
          "Cross Dataset": ""
        },
        {
          "Classifier Category": "",
          "Study": "Chen et al. (2021a)",
          "Dataset": "SEED",
          "Classifier": "MS-MDA",
          "Evaluation Strategy": "LOO",
          "Cross Subject": "X",
          "Cross Session": "X",
          "Cross Dataset": ""
        },
        {
          "Classifier Category": "",
          "Study": "Luo and Lu (2021)",
          "Dataset": "SEED",
          "Classifier": "wMADA",
          "Evaluation Strategy": "LOO",
          "Cross Subject": "X",
          "Cross Session": "",
          "Cross Dataset": ""
        },
        {
          "Classifier Category": "",
          "Study": "Cimtay and Ekmekcioglu (2020)",
          "Dataset": "DEAP, SEED, sp",
          "Classifier": "nl",
          "Evaluation Strategy": "LOO",
          "Cross Subject": "X",
          "Cross Session": "",
          "Cross Dataset": "X"
        },
        {
          "Classifier Category": "SUPERVISED DA",
          "Study": "Wang et al. (2020)",
          "Dataset": "DEAP, SEED",
          "Classifier": "RCNN",
          "Evaluation Strategy": "LOO",
          "Cross Subject": "X",
          "Cross Session": "",
          "Cross Dataset": "X"
        },
        {
          "Classifier Category": "",
          "Study": "Pusarla et al. (2022)",
          "Dataset": "DEAP, SEED",
          "Classifier": "Densenet",
          "Evaluation Strategy": "LOO",
          "Cross Subject": "X",
          "Cross Session": "",
          "Cross Dataset": ""
        },
        {
          "Classifier Category": "DEEP DG",
          "Study": "Ma et al. (2019)",
          "Dataset": "SEED",
          "Classifier": "DG-DANN, DResNet",
          "Evaluation Strategy": "LOO",
          "Cross Subject": "X",
          "Cross Session": "",
          "Cross Dataset": ""
        },
        {
          "Classifier Category": "",
          "Study": "Hagad et al. (2021)",
          "Dataset": "DEAP, SEED",
          "Classifier": "BiVDANN",
          "Evaluation Strategy": "LOO",
          "Cross Subject": "X",
          "Cross Session": "",
          "Cross Dataset": ""
        }
      ],
      "page": 28
    },
    {
      "caption": "Table 2: The most representative studies according to their classification ac-",
      "data": [
        {
          "self-produced.": "Proposed"
        },
        {
          "self-produced.": "category"
        },
        {
          "self-produced.": "SUPERVISED DA"
        },
        {
          "self-produced.": ""
        },
        {
          "self-produced.": "DA - SOURCE SELECTION"
        },
        {
          "self-produced.": "DEEP DA (SSS)"
        },
        {
          "self-produced.": ""
        },
        {
          "self-produced.": "DEEP DA (CSS)"
        },
        {
          "self-produced.": ""
        },
        {
          "self-produced.": "DEEP DA (CSS)"
        },
        {
          "self-produced.": "DEEP DA (CSS)"
        },
        {
          "self-produced.": ""
        },
        {
          "self-produced.": ""
        },
        {
          "self-produced.": "DEEP DA (CSS)"
        },
        {
          "self-produced.": ""
        },
        {
          "self-produced.": ""
        },
        {
          "self-produced.": "CLASSICAL ML"
        },
        {
          "self-produced.": ""
        }
      ],
      "page": 28
    },
    {
      "caption": "Table 2: The most representative studies according to their classification ac-",
      "data": [
        {
          "in combination with Domain Adaptation techniques. Even though they use dif-": "ferent approaches,"
        },
        {
          "in combination with Domain Adaptation techniques. Even though they use dif-": "suggests how the dynamic search for"
        },
        {
          "in combination with Domain Adaptation techniques. Even though they use dif-": "an interesting frontier for future studies in this area, not excluding the potential"
        },
        {
          "in combination with Domain Adaptation techniques. Even though they use dif-": "of using this approach in combination with DA/DG techniques."
        },
        {
          "in combination with Domain Adaptation techniques. Even though they use dif-": ""
        },
        {
          "in combination with Domain Adaptation techniques. Even though they use dif-": "tion devices on the market is not always coupled with consistency in terms of"
        },
        {
          "in combination with Domain Adaptation techniques. Even though they use dif-": "quality between the various devices (considering electrode type and positioning,"
        },
        {
          "in combination with Domain Adaptation techniques. Even though they use dif-": "interference shielding and signal-to-noise ratio, amplification strategies, etc). A"
        },
        {
          "in combination with Domain Adaptation techniques. Even though they use dif-": "comparison among different studies must take into account the quality of EEG"
        },
        {
          "in combination with Domain Adaptation techniques. Even though they use dif-": "instrumentation used. The IEC 60601-2-26 standard applies to basic safety and"
        },
        {
          "in combination with Domain Adaptation techniques. Even though they use dif-": "essential performance of electroencephalographs used in a clinical environment."
        },
        {
          "in combination with Domain Adaptation techniques. Even though they use dif-": ""
        }
      ],
      "page": 28
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Among the requirements, the minimum overall signal quality for an electroen-": "cephalographic device\nto be\nconsidered acceptable\nis defined Goldsack et al."
        },
        {
          "Among the requirements, the minimum overall signal quality for an electroen-": "(2020). Even if IEC 60601-2-26 is a standard specifically developed for clinical"
        },
        {
          "Among the requirements, the minimum overall signal quality for an electroen-": "purposes,\nit is nowadays the only available standard for EEG instrumentation"
        },
        {
          "Among the requirements, the minimum overall signal quality for an electroen-": "quality certification.\nIn the future,\nit is desirable for research to be increasingly"
        },
        {
          "Among the requirements, the minimum overall signal quality for an electroen-": "based on certified instruments. However, an encouraging trend emerges\nfrom"
        },
        {
          "Among the requirements, the minimum overall signal quality for an electroen-": "the most recent public datasets. They are all based on standardized equipment:"
        },
        {
          "Among the requirements, the minimum overall signal quality for an electroen-": "(i) Neuroelectrics Enobio 8 in the case of LUMED Cimtay and Ekmekcioglu"
        },
        {
          "Among the requirements, the minimum overall signal quality for an electroen-": "(2020), (ii) NuAmp Neuroscan in the case of CMEED Du et al. (2020), and (iii)"
        },
        {
          "Among the requirements, the minimum overall signal quality for an electroen-": "gtec.HIamp in the case of the dataset produced by Bao et al. (2020))."
        },
        {
          "Among the requirements, the minimum overall signal quality for an electroen-": "A further concern in the use of public datasets is its underlying theoretical"
        },
        {
          "Among the requirements, the minimum overall signal quality for an electroen-": "background, often acritically accepted by the scientists. Many studies validate"
        },
        {
          "Among the requirements, the minimum overall signal quality for an electroen-": "the same machine learning algorithm on different datasets although the targeted"
        },
        {
          "Among the requirements, the minimum overall signal quality for an electroen-": "psychic phenomena are radically different.\nIndeed, each dataset leverages on a"
        },
        {
          "Among the requirements, the minimum overall signal quality for an electroen-": "specific theory of emotions and related experimental\nsetup of emotion elicita-"
        },
        {
          "Among the requirements, the minimum overall signal quality for an electroen-": "tion. For instance, DEAP is based on a dimensional approach and SEED IV on"
        },
        {
          "Among the requirements, the minimum overall signal quality for an electroen-": "discrete one. The discrete theory is based on the assumption that there are basic"
        },
        {
          "Among the requirements, the minimum overall signal quality for an electroen-": "emotions that have evolved through natural selection TenHouten (2017).\nIn this"
        },
        {
          "Among the requirements, the minimum overall signal quality for an electroen-": "vein, close to the Darwinian tradition, Ekman’s theory identifies six basic emo-"
        },
        {
          "Among the requirements, the minimum overall signal quality for an electroen-": "tions that would be universal and innate: anger, disgust, fear, happiness, sadness"
        },
        {
          "Among the requirements, the minimum overall signal quality for an electroen-": "and surprise Ekman (1999). After him, Plutchik,\nidentifies eight basic emotions"
        },
        {
          "Among the requirements, the minimum overall signal quality for an electroen-": "(anger, anticipation, joy, trust, fear, surprise, sadness and disgust) and arranges"
        },
        {
          "Among the requirements, the minimum overall signal quality for an electroen-": "them on a wheel model Suhaimi\net al.\n(2020).\nIn contrast,\nthe dimensional"
        },
        {
          "Among the requirements, the minimum overall signal quality for an electroen-": "theory expresses emotions in a continuous two-dimensional (valence-arousal) or"
        },
        {
          "Among the requirements, the minimum overall signal quality for an electroen-": "three-dimensional (valence-arousal-dominance) space. While valence measures"
        },
        {
          "Among the requirements, the minimum overall signal quality for an electroen-": "levels of pleasantness (happy vs.\nsad) of an emotion, arousal\nidentifies degrees"
        },
        {
          "Among the requirements, the minimum overall signal quality for an electroen-": "of excitement or motivational activation.\nIn the three-dimensional model,\nthe"
        },
        {
          "Among the requirements, the minimum overall signal quality for an electroen-": "dominance dimension is added to valence and arousal, where\nthe dominance"
        },
        {
          "Among the requirements, the minimum overall signal quality for an electroen-": "evaluates\nemotions on a scale between submission and empowerment Torres"
        },
        {
          "Among the requirements, the minimum overall signal quality for an electroen-": "et al. (2020). The underlying assumption of discrete approach is that few fun-"
        },
        {
          "Among the requirements, the minimum overall signal quality for an electroen-": "damental emotions are mediated by associated dedicated neural circuits, with"
        },
        {
          "Among the requirements, the minimum overall signal quality for an electroen-": "a large innate (hardwired) component. Only two main brain networks are rec-"
        },
        {
          "Among the requirements, the minimum overall signal quality for an electroen-": "ognized by the dimensional approach Posner et al. (2009). The two theoretical"
        },
        {
          "Among the requirements, the minimum overall signal quality for an electroen-": "approaches identify two different phenomena also at a neurophysiological\nlevel,"
        },
        {
          "Among the requirements, the minimum overall signal quality for an electroen-": "with peculiar spatial signal\nfeatures."
        },
        {
          "Among the requirements, the minimum overall signal quality for an electroen-": "Finally, at present, the available public datasets do not adopt an established"
        },
        {
          "Among the requirements, the minimum overall signal quality for an electroen-": "practice of psychological screening of the subjects involved.\nIn general, studies"
        },
        {
          "Among the requirements, the minimum overall signal quality for an electroen-": "on EEG-based emotion assessment could benefit from administering psychome-"
        },
        {
          "Among the requirements, the minimum overall signal quality for an electroen-": "tric questionnaires\nto participants.\nIndeed, psychological data could help to"
        },
        {
          "Among the requirements, the minimum overall signal quality for an electroen-": "understand individual differences\nin emotional\nresponse,\nleading to clustering"
        },
        {
          "Among the requirements, the minimum overall signal quality for an electroen-": "of subjects Liu et al. (2021). Recently, unsupervised clustering based on large"
        },
        {
          "Among the requirements, the minimum overall signal quality for an electroen-": "datasets is emerging as a promising strategy for empirical\nidentification of per-"
        },
        {
          "Among the requirements, the minimum overall signal quality for an electroen-": "sonality types Gerlach et al. (2018). Meanwhile, correlations have been found"
        },
        {
          "Among the requirements, the minimum overall signal quality for an electroen-": "between personality types and EEG patterns Li et al. (2020b). Moreover, prior"
        },
        {
          "Among the requirements, the minimum overall signal quality for an electroen-": "psychological assessments allow to manage bias due to individual traits or states."
        },
        {
          "Among the requirements, the minimum overall signal quality for an electroen-": "The introduction of psycho-metric tests and assessments during the production"
        }
      ],
      "page": 29
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "of upcoming datasets could lead to a much more fruitful use of data in support": "of generalization."
        },
        {
          "of upcoming datasets could lead to a much more fruitful use of data in support": "7\nConclusion"
        },
        {
          "of upcoming datasets could lead to a much more fruitful use of data in support": "A systematic literature review collecting papers on machine learning strategies"
        },
        {
          "of upcoming datasets could lead to a much more fruitful use of data in support": "to pursue (cross-subjects and cross-sessions) generalizability in EEG-based emo-"
        },
        {
          "of upcoming datasets could lead to a much more fruitful use of data in support": ""
        },
        {
          "of upcoming datasets could lead to a much more fruitful use of data in support": "IEEE (Institute of Electrical and Electronics Engineers) Xplore, and PubMed"
        },
        {
          "of upcoming datasets could lead to a much more fruitful use of data in support": "databases, 75 papers\nresulted eligible. A taxonomy of"
        },
        {
          "of upcoming datasets could lead to a much more fruitful use of data in support": "ML method was proposed."
        },
        {
          "of upcoming datasets could lead to a much more fruitful use of data in support": "The\nstudies with the best\nresults\nin terms of average"
        },
        {
          "of upcoming datasets could lead to a much more fruitful use of data in support": "racy were identified, and the ten best results considering as many classification"
        },
        {
          "of upcoming datasets could lead to a much more fruitful use of data in support": "problems were highlighted. An interesting perspective based on self-organized"
        },
        {
          "of upcoming datasets could lead to a much more fruitful use of data in support": "graph construction modules emerged as peculiar"
        },
        {
          "of upcoming datasets could lead to a much more fruitful use of data in support": ""
        },
        {
          "of upcoming datasets could lead to a much more fruitful use of data in support": "future studies in this area, not excluding the potential of using this approach in"
        },
        {
          "of upcoming datasets could lead to a much more fruitful use of data in support": "combination with DA/DG techniques."
        },
        {
          "of upcoming datasets could lead to a much more fruitful use of data in support": ""
        },
        {
          "of upcoming datasets could lead to a much more fruitful use of data in support": "administering psychometric questionnaires to participants in order to conduct"
        },
        {
          "of upcoming datasets could lead to a much more fruitful use of data in support": "a psychological\nscreening of\nthe experimental"
        },
        {
          "of upcoming datasets could lead to a much more fruitful use of data in support": "derstand individual differences in emotional responses,"
        },
        {
          "of upcoming datasets could lead to a much more fruitful use of data in support": "subjects also taking into account the different subjects’ personality."
        },
        {
          "of upcoming datasets could lead to a much more fruitful use of data in support": "Acronyms"
        },
        {
          "of upcoming datasets could lead to a much more fruitful use of data in support": "A-DNN - Adversarial Deep Neural Network"
        },
        {
          "of upcoming datasets could lead to a much more fruitful use of data in support": "AD-TCN - Adversarial Discriminative Temporal Convolutional Network"
        },
        {
          "of upcoming datasets could lead to a much more fruitful use of data in support": "ASFM - Adaptive Subspace Feature Matching"
        },
        {
          "of upcoming datasets could lead to a much more fruitful use of data in support": "ASI - Add-Session-In"
        },
        {
          "of upcoming datasets could lead to a much more fruitful use of data in support": "ATDD-LSTM - Attention-based LSTM"
        },
        {
          "of upcoming datasets could lead to a much more fruitful use of data in support": "BiDANN - Bi-hemispheres DANN"
        },
        {
          "of upcoming datasets could lead to a much more fruitful use of data in support": "BiHDM - Bi-Hemispheric Discrepancy Model"
        },
        {
          "of upcoming datasets could lead to a much more fruitful use of data in support": "BiLSTM - Bidirectional LSTM"
        },
        {
          "of upcoming datasets could lead to a much more fruitful use of data in support": "BiVDANN - Bi-lateral Variational Domain Adversarial Neural Network"
        },
        {
          "of upcoming datasets could lead to a much more fruitful use of data in support": "CSS - Common Shared Space"
        },
        {
          "of upcoming datasets could lead to a much more fruitful use of data in support": "DAN - Deep Adaptation Network"
        },
        {
          "of upcoming datasets could lead to a much more fruitful use of data in support": "DANN - Domain Adversarial Neural Network"
        },
        {
          "of upcoming datasets could lead to a much more fruitful use of data in support": "DASC - Domain Adaptation Subject Clustering"
        },
        {
          "of upcoming datasets could lead to a much more fruitful use of data in support": "DASRC - Domain Adaptation Sparse Representation Classifier"
        },
        {
          "of upcoming datasets could lead to a much more fruitful use of data in support": "DDC - Deep Domain Confusion"
        },
        {
          "of upcoming datasets could lead to a much more fruitful use of data in support": "DECNN - Dynamic Empirical Convolutional Neural network"
        },
        {
          "of upcoming datasets could lead to a much more fruitful use of data in support": "DGCNN - Dynamical Graph Convolutional Neural Networks"
        },
        {
          "of upcoming datasets could lead to a much more fruitful use of data in support": "DG-DANN - Domain Generalization DANN"
        },
        {
          "of upcoming datasets could lead to a much more fruitful use of data in support": "30"
        }
      ],
      "page": 30
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "DResNet - Domain Residual Network": "ESN - Echo State Network"
        },
        {
          "DResNet - Domain Residual Network": "GNB - Gaussian Naïve Bayes"
        },
        {
          "DResNet - Domain Residual Network": "HO - Hold Out"
        },
        {
          "DResNet - Domain Residual Network": "LOO - Leave One Out"
        },
        {
          "DResNet - Domain Residual Network": "LSTM - Long short-term memory"
        },
        {
          "DResNet - Domain Residual Network": "MACI - Multi-Source Co-adaptation Correlation Information"
        },
        {
          "DResNet - Domain Residual Network": "MDTDDL - Multi-source Domain Transfer Discriminative Dictionary Learn-"
        },
        {
          "DResNet - Domain Residual Network": "ing modelling"
        },
        {
          "DResNet - Domain Residual Network": "MEERNet - Multi-Source EEG-based Emotion Recognition Network"
        },
        {
          "DResNet - Domain Residual Network": "MIDA - Maximum Independence Domain Adaptation"
        },
        {
          "DResNet - Domain Residual Network": "MSDAN - Multi-Spatial Domain Adaptation Network"
        },
        {
          "DResNet - Domain Residual Network": "MS-MDA - Multi Source-Marginal Distribution Adaptation"
        },
        {
          "DResNet - Domain Residual Network": "MSSA - Multi-Subject Subspace Alignment"
        },
        {
          "DResNet - Domain Residual Network": "Na - not available"
        },
        {
          "DResNet - Domain Residual Network": "NCA = Neighborhood Component Analysis"
        },
        {
          "DResNet - Domain Residual Network": "O2OSE - ONE-TO-ONE-SESSION"
        },
        {
          "DResNet - Domain Residual Network": "PLRSA - Progressive Low-Rank Subspace Alignment"
        },
        {
          "DResNet - Domain Residual Network": "PPDA - Plug-and-Play Domain Adaptation"
        },
        {
          "DResNet - Domain Residual Network": "R2G-STNN - Regional To Global Spatial-Temporal Neural Network"
        },
        {
          "DResNet - Domain Residual Network": "RCNN - Residual CNN"
        },
        {
          "DResNet - Domain Residual Network": "RF - Random Forest"
        },
        {
          "DResNet - Domain Residual Network": "RFE - Recursive Feature Elimination"
        },
        {
          "DResNet - Domain Residual Network": "RGNN - Regularized Graph Neural Network"
        },
        {
          "DResNet - Domain Residual Network": "RPCA - Robust Principal Component Analysis"
        },
        {
          "DResNet - Domain Residual Network": "SAAE - Subspace Alignment Auto Encoder"
        },
        {
          "DResNet - Domain Residual Network": "SBS - Sequential Backward Selection"
        },
        {
          "DResNet - Domain Residual Network": "SDA-FSL - Single-Source Domain Adaptive Few-Shot Learning Network"
        },
        {
          "DResNet - Domain Residual Network": "SE2SE - session-to-session"
        },
        {
          "DResNet - Domain Residual Network": "SOGNN - Self-Organized Graph Neural Network"
        },
        {
          "DResNet - Domain Residual Network": "sp - self-produced"
        },
        {
          "DResNet - Domain Residual Network": "SSB - Shared Space-Based"
        },
        {
          "DResNet - Domain Residual Network": "SSS - Shared+Specific Spaces"
        },
        {
          "DResNet - Domain Residual Network": "STM - Style Transfer Mapping"
        },
        {
          "DResNet - Domain Residual Network": "SU2SU - subject-to-subject"
        },
        {
          "DResNet - Domain Residual Network": "SVM - Support Vector Machine"
        },
        {
          "DResNet - Domain Residual Network": "TCA - Transfer Component Analysis"
        },
        {
          "DResNet - Domain Residual Network": "TDANN - Two-Level Domain Adaptation Neural Network"
        },
        {
          "DResNet - Domain Residual Network": "TPT - Transductive Parameter Transfer"
        },
        {
          "DResNet - Domain Residual Network": "TRFE - Transferable Recursive Feature Elimination"
        },
        {
          "DResNet - Domain Residual Network": "TSB - Target Space-Based"
        },
        {
          "DResNet - Domain Residual Network": "VAE - Variational Auto Encoder"
        },
        {
          "DResNet - Domain Residual Network": "WGANDA - Wasserstein Generative Adversarial Network Domain Adaptation"
        },
        {
          "DResNet - Domain Residual Network": "wMADA - Wasserstein-Distance-based Multi-Source Adversarial Domain Adap-"
        },
        {
          "DResNet - Domain Residual Network": "tation"
        }
      ],
      "page": 31
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "References": "Acharya, J. N., Hani, A. J., Cheek, J., Thirumala, P., and Tsuchida, T. N."
        },
        {
          "References": "(2016).\nAmerican clinical neurophysiology society guideline\n2:\nguidelines"
        },
        {
          "References": "for standard electrode position nomenclature. The Neurodiagnostic Journal,"
        },
        {
          "References": "56(4):245–252."
        },
        {
          "References": "Ajakan, H., Germain, P., Larochelle, H., Laviolette, F.,\nand Marchand, M."
        },
        {
          "References": "(2014). Domain-adversarial neural networks. arXiv preprint arXiv:1412.4446."
        },
        {
          "References": "Alarcao, S. M. and Fonseca, M. J.\n(2017).\nEmotions\nrecognition using eeg"
        },
        {
          "References": "signals: A survey.\nIEEE Transactions on Affective Computing, 10(3):374–"
        },
        {
          "References": "393."
        },
        {
          "References": "Apicella, A., Arpaia, P.,\nIsgrò, F., Mastrati, G., and Moccaldi, N.\n(2022). A"
        },
        {
          "References": "survey on eeg-based solutions for emotion recognition with a low number of"
        },
        {
          "References": "channels.\nIEEE Access, pages 1–1."
        },
        {
          "References": "Arevalillo-Herráez, M., Cobos, M., Roger, S., and García-Pineda, M.\n(2019)."
        },
        {
          "References": "Combining inter-subject modeling with a subject-based data transformation"
        },
        {
          "References": "to improve affect recognition from eeg signals. Sensors, 19(13):2999."
        },
        {
          "References": "Bao, G., Zhuang, N., Tong, L., Yan, B., Shu, J., Wang, L., Zeng, Y., and Shen,"
        },
        {
          "References": "Z. (2020). Two-level domain adaptation neural network for eeg-based emotion"
        },
        {
          "References": "recognition. Frontiers in Human Neuroscience, 14."
        },
        {
          "References": "Barbaresco, F.\n(2008).\nInnovative tools\nfor\nradar\nsignal processing based on"
        },
        {
          "References": "cartan’s geometry of\nspd matrices & information geometry.\nIn 2008 IEEE"
        },
        {
          "References": "Radar Conference, pages 1–6. IEEE."
        },
        {
          "References": "Bashivan, P., Rish,\nI., Yeasin, M., and Codella, N. (2015). Learning represen-"
        },
        {
          "References": "tations\nfrom eeg with deep recurrent-convolutional neural networks.\narXiv"
        },
        {
          "References": "preprint arXiv:1511.06448."
        },
        {
          "References": "Brouwer, A.-M., Zander, T. O., Van Erp, J. B., Korteling, J. E., and Bronkhorst,"
        },
        {
          "References": "A. W.\n(2015). Using neurophysiological\nsignals\nthat\nreflect cognitive or af-"
        },
        {
          "References": "fective state:\nsix recommendations\nto avoid common pitfalls.\nFrontiers\nin"
        },
        {
          "References": "neuroscience, 9:136."
        },
        {
          "References": "Cai, J., Chen, W., and Yin, Z. (2019). Multiple transferable recursive feature"
        },
        {
          "References": "elimination technique for emotion recognition based on eeg signals. Symmetry,"
        },
        {
          "References": "11(5):683."
        },
        {
          "References": "Cambria, E., Das, D., Bandyopadhyay, S., and Feraco, A.\n(2017).\nAffective"
        },
        {
          "References": "computing and sentiment analysis. In A practical guide to sentiment analysis,"
        },
        {
          "References": "pages 1–10. Springer."
        },
        {
          "References": "Cao, C. and Slobounov, S. (2011). Application of a novel measure of eeg non-"
        },
        {
          "References": "stationarity as ‘shannon-entropy of the peak frequency shifting’for detecting"
        },
        {
          "References": "residual abnormalities\nin concussed individuals.\nClinical Neurophysiology,"
        },
        {
          "References": "122(7):1314–1321."
        }
      ],
      "page": 32
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Cao, J., He, X., Yang, C., Chen, S., Li, Z., and Wang, Z. (2021). Multi-source": "and multi-representation adaptation for cross-domain electroencephalography"
        },
        {
          "Cao, J., He, X., Yang, C., Chen, S., Li, Z., and Wang, Z. (2021). Multi-source": "emotion recognition. Frontiers in Psychology, 12:809459–809459."
        },
        {
          "Cao, J., He, X., Yang, C., Chen, S., Li, Z., and Wang, Z. (2021). Multi-source": "Casson, A. J. (2019). Wearable eeg and beyond. Biomedical engineering letters,"
        },
        {
          "Cao, J., He, X., Yang, C., Chen, S., Li, Z., and Wang, Z. (2021). Multi-source": "9(1):53–71."
        },
        {
          "Cao, J., He, X., Yang, C., Chen, S., Li, Z., and Wang, Z. (2021). Multi-source": "Chai, X., Wang, Q., Zhao, Y., Li, Y., Liu, D., Liu, X., and Bai, O. (2017). A fast,"
        },
        {
          "Cao, J., He, X., Yang, C., Chen, S., Li, Z., and Wang, Z. (2021). Multi-source": "efficient domain adaptation technique for cross-domain electroencephalogra-"
        },
        {
          "Cao, J., He, X., Yang, C., Chen, S., Li, Z., and Wang, Z. (2021). Multi-source": "phy (eeg)-based emotion recognition. Sensors, 17(5):1014."
        },
        {
          "Cao, J., He, X., Yang, C., Chen, S., Li, Z., and Wang, Z. (2021). Multi-source": "Chai, X., Wang, Q., Zhao, Y., Liu, X., Bai, O., and Li, Y. (2016). Unsupervised"
        },
        {
          "Cao, J., He, X., Yang, C., Chen, S., Li, Z., and Wang, Z. (2021). Multi-source": "domain adaptation techniques based on auto-encoder for non-stationary eeg-"
        },
        {
          "Cao, J., He, X., Yang, C., Chen, S., Li, Z., and Wang, Z. (2021). Multi-source": "based emotion recognition. Computers in biology and medicine, 79:205–214."
        },
        {
          "Cao, J., He, X., Yang, C., Chen, S., Li, Z., and Wang, Z. (2021). Multi-source": "Chai, X., Wang, Q., Zhao, Y., Liu, X., Liu, D., and Bai, O.\n(2018). Multi-"
        },
        {
          "Cao, J., He, X., Yang, C., Chen, S., Li, Z., and Wang, Z. (2021). Multi-source": "subject subspace alignment for non-stationary eeg-based emotion recognition."
        },
        {
          "Cao, J., He, X., Yang, C., Chen, S., Li, Z., and Wang, Z. (2021). Multi-source": "Technology and Health Care, 26(S1):327–335."
        },
        {
          "Cao, J., He, X., Yang, C., Chen, S., Li, Z., and Wang, Z. (2021). Multi-source": "Chen, H., Jin, M., Li, Z., Fan, C., Li, J., and He, H. (2021a). Ms-mda: Mul-"
        },
        {
          "Cao, J., He, X., Yang, C., Chen, S., Li, Z., and Wang, Z. (2021). Multi-source": "tisource marginal distribution adaptation for cross-subject and cross-session"
        },
        {
          "Cao, J., He, X., Yang, C., Chen, S., Li, Z., and Wang, Z. (2021). Multi-source": "eeg emotion recognition. Frontiers in Neuroscience, 15."
        },
        {
          "Cao, J., He, X., Yang, C., Chen, S., Li, Z., and Wang, Z. (2021). Multi-source": "Chen, H., Li, Z., Jin, M., and Li, J. (2021b). Meernet: Multi-source eeg-based"
        },
        {
          "Cao, J., He, X., Yang, C., Chen, S., Li, Z., and Wang, Z. (2021). Multi-source": "emotion recognition network for generalization across subjects and sessions."
        },
        {
          "Cao, J., He, X., Yang, C., Chen, S., Li, Z., and Wang, Z. (2021). Multi-source": "In 2021 43rd Annual\nInternational Conference of\nthe IEEE Engineering in"
        },
        {
          "Cao, J., He, X., Yang, C., Chen, S., Li, Z., and Wang, Z. (2021). Multi-source": "Medicine & Biology Society (EMBC), pages 6094–6097. IEEE."
        },
        {
          "Cao, J., He, X., Yang, C., Chen, S., Li, Z., and Wang, Z. (2021). Multi-source": "Chen, H., Sun, S., Li, J., Yu, R., Li, N., Li, X., and Hu, B. (2021c). Personal-"
        },
        {
          "Cao, J., He, X., Yang, C., Chen, S., Li, Z., and Wang, Z. (2021). Multi-source": "zscore: Eliminating individual difference for eeg-based cross-subject emotion"
        },
        {
          "Cao, J., He, X., Yang, C., Chen, S., Li, Z., and Wang, Z. (2021). Multi-source": "recognition.\nIEEE Transactions on Affective Computing."
        },
        {
          "Cao, J., He, X., Yang, C., Chen, S., Li, Z., and Wang, Z. (2021). Multi-source": "Chen, Y., Wang, G., and Dong, S. (2003). Learning with progressive transduc-"
        },
        {
          "Cao, J., He, X., Yang, C., Chen, S., Li, Z., and Wang, Z. (2021). Multi-source": "tive support vector machine. Pattern Recognition Letters, 24(12):1845–1855."
        },
        {
          "Cao, J., He, X., Yang, C., Chen, S., Li, Z., and Wang, Z. (2021). Multi-source": "Cimtay, Y. and Ekmekcioglu, E.\n(2020).\nInvestigating the use of pretrained"
        },
        {
          "Cao, J., He, X., Yang, C., Chen, S., Li, Z., and Wang, Z. (2021). Multi-source": "convolutional neural network on cross-subject and cross-dataset eeg emotion"
        },
        {
          "Cao, J., He, X., Yang, C., Chen, S., Li, Z., and Wang, Z. (2021). Multi-source": "recognition. Sensors, 20(7):2034."
        },
        {
          "Cao, J., He, X., Yang, C., Chen, S., Li, Z., and Wang, Z. (2021). Multi-source": "Coan, J. A. and Allen, J. J. (2003). The state and trait nature of\nfrontal eeg"
        },
        {
          "Cao, J., He, X., Yang, C., Chen, S., Li, Z., and Wang, Z. (2021). Multi-source": "asymmetry in emotion."
        },
        {
          "Cao, J., He, X., Yang, C., Chen, S., Li, Z., and Wang, Z. (2021). Multi-source": "da Cruz, J. R., Chicherov, V., Herzog, M. H., and Figueiredo, P. (2018). An au-"
        },
        {
          "Cao, J., He, X., Yang, C., Chen, S., Li, Z., and Wang, Z. (2021). Multi-source": "tomatic pre-processing pipeline for eeg analysis (app) based on robust statis-"
        },
        {
          "Cao, J., He, X., Yang, C., Chen, S., Li, Z., and Wang, Z. (2021). Multi-source": "tics. Clinical Neurophysiology, 129(7):1427–1437."
        },
        {
          "Cao, J., He, X., Yang, C., Chen, S., Li, Z., and Wang, Z. (2021). Multi-source": "Dai, W., Yang, Q., Xue, G.-R., and Yu, Y. (2007). Boosting for transfer learning."
        },
        {
          "Cao, J., He, X., Yang, C., Chen, S., Li, Z., and Wang, Z. (2021). Multi-source": "In Proceedings of\nthe 24th International Conference on Machine Learning,"
        },
        {
          "Cao, J., He, X., Yang, C., Chen, S., Li, Z., and Wang, Z. (2021). Multi-source": "ICML ’07, page 193–200, New York, NY, USA. Association for Computing"
        },
        {
          "Cao, J., He, X., Yang, C., Chen, S., Li, Z., and Wang, Z. (2021). Multi-source": "Machinery."
        }
      ],
      "page": 33
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Daly,\nI., Pichiorri, F., Faller, J., Kaiser, V., Kreilinger, A., Scherer, R., and": "Müller-Putz, G.\n(2012). What does clean eeg look like?\nIn 2012 Annual"
        },
        {
          "Daly,\nI., Pichiorri, F., Faller, J., Kaiser, V., Kreilinger, A., Scherer, R., and": "International Conference of\nthe IEEE Engineering in Medicine and Biology"
        },
        {
          "Daly,\nI., Pichiorri, F., Faller, J., Kaiser, V., Kreilinger, A., Scherer, R., and": "Society, pages 3963–3966. IEEE."
        },
        {
          "Daly,\nI., Pichiorri, F., Faller, J., Kaiser, V., Kreilinger, A., Scherer, R., and": "Dan-Glauser, E. S. and Scherer, K. R.\n(2011).\nThe geneva affective picture"
        },
        {
          "Daly,\nI., Pichiorri, F., Faller, J., Kaiser, V., Kreilinger, A., Scherer, R., and": "database (gaped):\na new 730-picture database focusing on valence and nor-"
        },
        {
          "Daly,\nI., Pichiorri, F., Faller, J., Kaiser, V., Kreilinger, A., Scherer, R., and": "mative significance. Behavior research methods, 43(2):468."
        },
        {
          "Daly,\nI., Pichiorri, F., Faller, J., Kaiser, V., Kreilinger, A., Scherer, R., and": "Davidson, R. J. (1984). Hemispheric asymmetry and emotion. Approaches to"
        },
        {
          "Daly,\nI., Pichiorri, F., Faller, J., Kaiser, V., Kreilinger, A., Scherer, R., and": "emotion, 2:39–57."
        },
        {
          "Daly,\nI., Pichiorri, F., Faller, J., Kaiser, V., Kreilinger, A., Scherer, R., and": "Demaree, H. A., Everhart, D. E., Youngstrom, E. A.,\nand Harrison, D. W."
        },
        {
          "Daly,\nI., Pichiorri, F., Faller, J., Kaiser, V., Kreilinger, A., Scherer, R., and": "(2005).\nBrain lateralization of\nemotional processing:\nhistorical\nroots and"
        },
        {
          "Daly,\nI., Pichiorri, F., Faller, J., Kaiser, V., Kreilinger, A., Scherer, R., and": "a future incorporating “dominance”.\nBehavioral and cognitive neuroscience"
        },
        {
          "Daly,\nI., Pichiorri, F., Faller, J., Kaiser, V., Kreilinger, A., Scherer, R., and": "reviews, 4(1):3–20."
        },
        {
          "Daly,\nI., Pichiorri, F., Faller, J., Kaiser, V., Kreilinger, A., Scherer, R., and": "Ding, K.-M., Kimura, T., Fukui, K.-i., and Numao, M. (2021). Eeg emotion en-"
        },
        {
          "Daly,\nI., Pichiorri, F., Faller, J., Kaiser, V., Kreilinger, A., Scherer, R., and": "hancement using task-specific domain adversarial neural network. In 2021 In-"
        },
        {
          "Daly,\nI., Pichiorri, F., Faller, J., Kaiser, V., Kreilinger, A., Scherer, R., and": "ternational Joint Conference on Neural Networks (IJCNN), pages 1–8. IEEE."
        },
        {
          "Daly,\nI., Pichiorri, F., Faller, J., Kaiser, V., Kreilinger, A., Scherer, R., and": "Du, X., Ma, C., Zhang, G., Li, J., Lai, Y.-K., Zhao, G., Deng, X., Liu, Y.-J.,"
        },
        {
          "Daly,\nI., Pichiorri, F., Faller, J., Kaiser, V., Kreilinger, A., Scherer, R., and": "and Wang, H. (2020). An efficient lstm network for emotion recognition from"
        },
        {
          "Daly,\nI., Pichiorri, F., Faller, J., Kaiser, V., Kreilinger, A., Scherer, R., and": "multichannel eeg signals.\nIEEE Transactions on Affective Computing."
        },
        {
          "Daly,\nI., Pichiorri, F., Faller, J., Kaiser, V., Kreilinger, A., Scherer, R., and": "Ekman, P. (1999). Basic emotions. Handbook of cognition and emotion, 98(45-"
        },
        {
          "Daly,\nI., Pichiorri, F., Faller, J., Kaiser, V., Kreilinger, A., Scherer, R., and": "60):16."
        },
        {
          "Daly,\nI., Pichiorri, F., Faller, J., Kaiser, V., Kreilinger, A., Scherer, R., and": "Feng, H., Golshan, H. M., and Mahoor, M. H. (2018). A wavelet-based approach"
        },
        {
          "Daly,\nI., Pichiorri, F., Faller, J., Kaiser, V., Kreilinger, A., Scherer, R., and": "to emotion classification using eda signals. Expert Systems with Applications,"
        },
        {
          "Daly,\nI., Pichiorri, F., Faller, J., Kaiser, V., Kreilinger, A., Scherer, R., and": "112:77–86."
        },
        {
          "Daly,\nI., Pichiorri, F., Faller, J., Kaiser, V., Kreilinger, A., Scherer, R., and": "Fernandez, J., Guttenberg, N., Witkowski, O., and Pasquali, A. (2021). Cross-"
        },
        {
          "Daly,\nI., Pichiorri, F., Faller, J., Kaiser, V., Kreilinger, A., Scherer, R., and": "subject eeg-based emotion recognition through neural networks with stratified"
        },
        {
          "Daly,\nI., Pichiorri, F., Faller, J., Kaiser, V., Kreilinger, A., Scherer, R., and": "normalization. Frontiers in neuroscience, 15:11."
        },
        {
          "Daly,\nI., Pichiorri, F., Faller, J., Kaiser, V., Kreilinger, A., Scherer, R., and": "Fernando, B., Habrard, A., Sebban, M., and Tuytelaars, T. (2013). Unsuper-"
        },
        {
          "Daly,\nI., Pichiorri, F., Faller, J., Kaiser, V., Kreilinger, A., Scherer, R., and": "vised visual domain adaptation using subspace alignment.\nIn Proceedings of"
        },
        {
          "Daly,\nI., Pichiorri, F., Faller, J., Kaiser, V., Kreilinger, A., Scherer, R., and": "the IEEE international conference on computer vision, pages 2960–2967."
        },
        {
          "Daly,\nI., Pichiorri, F., Faller, J., Kaiser, V., Kreilinger, A., Scherer, R., and": "Fisher, R. A. (1992). Statistical methods for research workers. In Breakthroughs"
        },
        {
          "Daly,\nI., Pichiorri, F., Faller, J., Kaiser, V., Kreilinger, A., Scherer, R., and": "in statistics, pages 66–70. Springer."
        },
        {
          "Daly,\nI., Pichiorri, F., Faller, J., Kaiser, V., Kreilinger, A., Scherer, R., and": "Furukawa, S., Sakuma, T.,\nand Kato, S.\n(2021).\nEmotion recognition with"
        },
        {
          "Daly,\nI., Pichiorri, F., Faller, J., Kaiser, V., Kreilinger, A., Scherer, R., and": "domain adaptation based on few-shot eeg learning.\nIn 2021 IEEE 10th Global"
        },
        {
          "Daly,\nI., Pichiorri, F., Faller, J., Kaiser, V., Kreilinger, A., Scherer, R., and": "Conference on Consumer Electronics (GCCE), pages 1–2. IEEE."
        },
        {
          "Daly,\nI., Pichiorri, F., Faller, J., Kaiser, V., Kreilinger, A., Scherer, R., and": "Ganin, Y. and Lempitsky, V. (2015). Unsupervised domain adaptation by back-"
        },
        {
          "Daly,\nI., Pichiorri, F., Faller, J., Kaiser, V., Kreilinger, A., Scherer, R., and": "propagation.\nIn International conference on machine learning, pages 1180–"
        },
        {
          "Daly,\nI., Pichiorri, F., Faller, J., Kaiser, V., Kreilinger, A., Scherer, R., and": "1189. PMLR."
        }
      ],
      "page": 34
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Ganin, Y., Ustinova, E., Ajakan, H., Germain, P., Larochelle, H., Laviolette,": "F., Marchand, M., and Lempitsky, V. (2016). Domain-adversarial training of"
        },
        {
          "Ganin, Y., Ustinova, E., Ajakan, H., Germain, P., Larochelle, H., Laviolette,": "neural networks. The journal of machine learning research, 17(1):2096–2030."
        },
        {
          "Ganin, Y., Ustinova, E., Ajakan, H., Germain, P., Larochelle, H., Laviolette,": "Geethanjali, P., Mohan, Y. K., and Sen, J. (2012). Time domain feature extrac-"
        },
        {
          "Ganin, Y., Ustinova, E., Ajakan, H., Germain, P., Larochelle, H., Laviolette,": "tion and classification of eeg data for brain computer interface.\nIn 2012 9th"
        },
        {
          "Ganin, Y., Ustinova, E., Ajakan, H., Germain, P., Larochelle, H., Laviolette,": "International Conference on Fuzzy Systems and Knowledge Discovery, pages"
        },
        {
          "Ganin, Y., Ustinova, E., Ajakan, H., Germain, P., Larochelle, H., Laviolette,": "1136–1139. IEEE."
        },
        {
          "Ganin, Y., Ustinova, E., Ajakan, H., Germain, P., Larochelle, H., Laviolette,": "Gerlach, M., Farb, B., Revelle, W., and Nunes Amaral, L. A. (2018). A robust"
        },
        {
          "Ganin, Y., Ustinova, E., Ajakan, H., Germain, P., Larochelle, H., Laviolette,": "data-driven approach identifies four personality types across four large data"
        },
        {
          "Ganin, Y., Ustinova, E., Ajakan, H., Germain, P., Larochelle, H., Laviolette,": "sets. Nature human behaviour, 2(10):735–742."
        },
        {
          "Ganin, Y., Ustinova, E., Ajakan, H., Germain, P., Larochelle, H., Laviolette,": "Ghifary, M., Balduzzi, D., Kleijn, W. B., and Zhang, M. (2016). Scatter compo-"
        },
        {
          "Ganin, Y., Ustinova, E., Ajakan, H., Germain, P., Larochelle, H., Laviolette,": "nent analysis: A unified framework for domain adaptation and domain gen-"
        },
        {
          "Ganin, Y., Ustinova, E., Ajakan, H., Germain, P., Larochelle, H., Laviolette,": "eralization.\nIEEE transactions on pattern analysis and machine intelligence,"
        },
        {
          "Ganin, Y., Ustinova, E., Ajakan, H., Germain, P., Larochelle, H., Laviolette,": "39(7):1414–1430."
        },
        {
          "Ganin, Y., Ustinova, E., Ajakan, H., Germain, P., Larochelle, H., Laviolette,": "Goldsack, J. C., Coravos, A., Bakker, J. P., Bent, B., Dowling, A. V., Fitzer-"
        },
        {
          "Ganin, Y., Ustinova, E., Ajakan, H., Germain, P., Larochelle, H., Laviolette,": "Attas, C., Godfrey, A., Godino, J. G., Gujar, N., Izmailova, E., et al. (2020)."
        },
        {
          "Ganin, Y., Ustinova, E., Ajakan, H., Germain, P., Larochelle, H., Laviolette,": "Verification,\nanalytical\nvalidation,\nand clinical\nvalidation (v3):\nthe\nfoun-"
        },
        {
          "Ganin, Y., Ustinova, E., Ajakan, H., Germain, P., Larochelle, H., Laviolette,": "dation of determining fit-for-purpose\nfor biometric monitoring technologies"
        },
        {
          "Ganin, Y., Ustinova, E., Ajakan, H., Germain, P., Larochelle, H., Laviolette,": "(biomets). npj digital Medicine, 3(1):1–15."
        },
        {
          "Ganin, Y., Ustinova, E., Ajakan, H., Germain, P., Larochelle, H., Laviolette,": "Gong, B., Shi, Y., Sha, F., and Grauman, K.\n(2012).\nGeodesic flow kernel"
        },
        {
          "Ganin, Y., Ustinova, E., Ajakan, H., Germain, P., Larochelle, H., Laviolette,": "for unsupervised domain adaptation.\nIn 2012 IEEE conference on computer"
        },
        {
          "Ganin, Y., Ustinova, E., Ajakan, H., Germain, P., Larochelle, H., Laviolette,": "vision and pattern recognition, pages 2066–2073. IEEE."
        },
        {
          "Ganin, Y., Ustinova, E., Ajakan, H., Germain, P., Larochelle, H., Laviolette,": "Gonzalez, H. A., Yoo, J., and Elfadel, I. M. (2019). Eeg-based emotion detection"
        },
        {
          "Ganin, Y., Ustinova, E., Ajakan, H., Germain, P., Larochelle, H., Laviolette,": "using unsupervised transfer learning. In 2019 41st Annual International Con-"
        },
        {
          "Ganin, Y., Ustinova, E., Ajakan, H., Germain, P., Larochelle, H., Laviolette,": "ference of\nthe IEEE Engineering in Medicine and Biology Society (EMBC),"
        },
        {
          "Ganin, Y., Ustinova, E., Ajakan, H., Germain, P., Larochelle, H., Laviolette,": "pages 694–697. IEEE."
        },
        {
          "Ganin, Y., Ustinova, E., Ajakan, H., Germain, P., Larochelle, H., Laviolette,": "Gretton, A., Borgwardt, K., Rasch, M., Schölkopf, B., and Smola, A. (2006). A"
        },
        {
          "Ganin, Y., Ustinova, E., Ajakan, H., Germain, P., Larochelle, H., Laviolette,": "kernel method for the two-sample-problem. Advances in neural\ninformation"
        },
        {
          "Ganin, Y., Ustinova, E., Ajakan, H., Germain, P., Larochelle, H., Laviolette,": "processing systems, 19."
        },
        {
          "Ganin, Y., Ustinova, E., Ajakan, H., Germain, P., Larochelle, H., Laviolette,": "Gu, X., Cai, W., Gao, M.,\nJiang, Y., Ning, X.,\nand Qian,\nP.\n(2022)."
        },
        {
          "Ganin, Y., Ustinova, E., Ajakan, H., Germain, P., Larochelle, H., Laviolette,": "Multi-source\ndomain\ntransfer\ndiscriminative\ndictionary\nlearning modeling"
        },
        {
          "Ganin, Y., Ustinova, E., Ajakan, H., Germain, P., Larochelle, H., Laviolette,": "for electroencephalogram-based emotion recognition.\nIEEE Transactions on"
        },
        {
          "Ganin, Y., Ustinova, E., Ajakan, H., Germain, P., Larochelle, H., Laviolette,": "Computational Social Systems."
        },
        {
          "Ganin, Y., Ustinova, E., Ajakan, H., Germain, P., Larochelle, H., Laviolette,": "Gupta, V., Chopda, M. D., and Pachori, R. B. (2018). Cross-subject emotion"
        },
        {
          "Ganin, Y., Ustinova, E., Ajakan, H., Germain, P., Larochelle, H., Laviolette,": "recognition using flexible analytic wavelet transform from eeg signals.\nIEEE"
        },
        {
          "Ganin, Y., Ustinova, E., Ajakan, H., Germain, P., Larochelle, H., Laviolette,": "Sensors Journal, 19(6):2266–2274."
        },
        {
          "Ganin, Y., Ustinova, E., Ajakan, H., Germain, P., Larochelle, H., Laviolette,": "Hagad,\nJ. L., Kimura, T., Fukui, K.-i.,\nand Numao, M.\n(2021).\nLearning"
        },
        {
          "Ganin, Y., Ustinova, E., Ajakan, H., Germain, P., Larochelle, H., Laviolette,": "subject-generalized topographical eeg embeddings using deep variational au-"
        },
        {
          "Ganin, Y., Ustinova, E., Ajakan, H., Germain, P., Larochelle, H., Laviolette,": "toencoders and domain-adversarial regularization. Sensors, 21(5):1792."
        }
      ],
      "page": 35
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "He, K., Zhang, X., Ren, S., and Sun, J. (2016). Deep residual learning for image": "recognition.\nIn Proceedings of\nthe IEEE conference on computer vision and"
        },
        {
          "He, K., Zhang, X., Ren, S., and Sun, J. (2016). Deep residual learning for image": "pattern recognition, pages 770–778."
        },
        {
          "He, K., Zhang, X., Ren, S., and Sun, J. (2016). Deep residual learning for image": "He, Z., Zhong, Y., and Pan, J.\n(2022a).\nAn adversarial discriminative\ntem-"
        },
        {
          "He, K., Zhang, X., Ren, S., and Sun, J. (2016). Deep residual learning for image": "poral convolutional network for eeg-based cross-domain emotion recognition."
        },
        {
          "He, K., Zhang, X., Ren, S., and Sun, J. (2016). Deep residual learning for image": "Computers in biology and medicine, 141:105048."
        },
        {
          "He, K., Zhang, X., Ren, S., and Sun, J. (2016). Deep residual learning for image": "He, Z., Zhuang, N., Bao, G., Zeng, Y., and Yan, B.\n(2022b). Cross-day eeg-"
        },
        {
          "He, K., Zhang, X., Ren, S., and Sun, J. (2016). Deep residual learning for image": "based emotion recognition using transfer\ncomponent analysis.\nElectronics,"
        },
        {
          "He, K., Zhang, X., Ren, S., and Sun, J. (2016). Deep residual learning for image": "11(4):651."
        },
        {
          "He, K., Zhang, X., Ren, S., and Sun, J. (2016). Deep residual learning for image": "Healy, M., Donovan, R., Walsh, P., and Zheng, H. (2018). A machine learning"
        },
        {
          "He, K., Zhang, X., Ren, S., and Sun, J. (2016). Deep residual learning for image": "emotion detection platform to support affective well being.\nIn 2018 IEEE"
        },
        {
          "He, K., Zhang, X., Ren, S., and Sun, J. (2016). Deep residual learning for image": "International Conference on Bioinformatics and Biomedicine (BIBM), pages"
        },
        {
          "He, K., Zhang, X., Ren, S., and Sun, J. (2016). Deep residual learning for image": "2694–2700. IEEE."
        },
        {
          "He, K., Zhang, X., Ren, S., and Sun, J. (2016). Deep residual learning for image": "Hernández, D., Trujillo, L., Villanueva, O., Romo-Fewell, O.,\net al.\n(2018)."
        },
        {
          "He, K., Zhang, X., Ren, S., and Sun, J. (2016). Deep residual learning for image": "Detecting epilepsy in eeg signals using time,\nfrequency and time-frequency"
        },
        {
          "He, K., Zhang, X., Ren, S., and Sun, J. (2016). Deep residual learning for image": "domain features.\nIn Computer science and engineering—theory and applica-"
        },
        {
          "He, K., Zhang, X., Ren, S., and Sun, J. (2016). Deep residual learning for image": "tions, pages 167–182. Springer."
        },
        {
          "He, K., Zhang, X., Ren, S., and Sun, J. (2016). Deep residual learning for image": "Hua, Y., Zhong, X., Zhang, B., Yin, Z., and Zhang, J. (2021). Manifold feature"
        },
        {
          "He, K., Zhang, X., Ren, S., and Sun, J. (2016). Deep residual learning for image": "fusion with dynamical feature selection for cross-subject emotion recognition."
        },
        {
          "He, K., Zhang, X., Ren, S., and Sun, J. (2016). Deep residual learning for image": "Brain Sciences, 11(11):1392."
        },
        {
          "He, K., Zhang, X., Ren, S., and Sun, J. (2016). Deep residual learning for image": "Huang, G., Liu, Z., Van Der Maaten, L., and Weinberger, K. Q. (2017). Densely"
        },
        {
          "He, K., Zhang, X., Ren, S., and Sun, J. (2016). Deep residual learning for image": "connected convolutional networks.\nIn Proceedings of\nthe IEEE conference on"
        },
        {
          "He, K., Zhang, X., Ren, S., and Sun, J. (2016). Deep residual learning for image": "computer vision and pattern recognition, pages 4700–4708."
        },
        {
          "He, K., Zhang, X., Ren, S., and Sun, J. (2016). Deep residual learning for image": "Hwang, S., Hong, K., Son, G., and Byun, H.\n(2020a).\nLearning cnn features"
        },
        {
          "He, K., Zhang, X., Ren, S., and Sun, J. (2016). Deep residual learning for image": "from de\nfeatures\nfor\neeg-based emotion recognition.\nPattern Analysis and"
        },
        {
          "He, K., Zhang, X., Ren, S., and Sun, J. (2016). Deep residual learning for image": "Applications, 23(3):1323–1335."
        },
        {
          "He, K., Zhang, X., Ren, S., and Sun, J. (2016). Deep residual learning for image": "Hwang, S., Ki, M., Hong, K.,\nand Byun, H.\n(2020b).\nSubject-independent"
        },
        {
          "He, K., Zhang, X., Ren, S., and Sun, J. (2016). Deep residual learning for image": "eeg-based emotion recognition using adversarial\nlearning.\nIn 2020 8th Inter-"
        },
        {
          "He, K., Zhang, X., Ren, S., and Sun, J. (2016). Deep residual learning for image": "national Winter Conference on Brain-Computer Interface (BCI), pages 1–4."
        },
        {
          "He, K., Zhang, X., Ren, S., and Sun, J. (2016). Deep residual learning for image": "IEEE."
        },
        {
          "He, K., Zhang, X., Ren, S., and Sun, J. (2016). Deep residual learning for image": "Im, C.-H. (2018). Computational eeg analysis. Springer Singapore, Singapore,"
        },
        {
          "He, K., Zhang, X., Ren, S., and Sun, J. (2016). Deep residual learning for image": "10:978–981."
        },
        {
          "He, K., Zhang, X., Ren, S., and Sun, J. (2016). Deep residual learning for image": "Inouye, T., Toi, S., and Matsumoto, Y.\n(1995). A new segmentation method"
        },
        {
          "He, K., Zhang, X., Ren, S., and Sun, J. (2016). Deep residual learning for image": "of electroencephalograms by use of akaike’s information criterion. Cognitive"
        },
        {
          "He, K., Zhang, X., Ren, S., and Sun, J. (2016). Deep residual learning for image": "brain research, 3(1):33–40."
        },
        {
          "He, K., Zhang, X., Ren, S., and Sun, J. (2016). Deep residual learning for image": "Jenke, R., Peer, A., and Buss, M. (2014). Feature extraction and selection for"
        },
        {
          "He, K., Zhang, X., Ren, S., and Sun, J. (2016). Deep residual learning for image": "emotion recognition from eeg.\nIEEE Transactions on Affective computing,"
        },
        {
          "He, K., Zhang, X., Ren, S., and Sun, J. (2016). Deep residual learning for image": "5(3):327–339."
        }
      ],
      "page": 36
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Jiang, J. (2008). A literature survey on domain adaptation of statistical clas-": "sifiers.\nURL: http://sifaka.\ncs. uiuc.\nedu/jiang4/domainadaptation/survey,"
        },
        {
          "Jiang, J. (2008). A literature survey on domain adaptation of statistical clas-": "3(1-12):3."
        },
        {
          "Jiang, J. (2008). A literature survey on domain adaptation of statistical clas-": "Jiang, W., Liu, G., Zhao, X., and Yang, F.\n(2019a).\nCross-subject\nemotion"
        },
        {
          "Jiang, J. (2008). A literature survey on domain adaptation of statistical clas-": "recognition with a decision tree classifier based on sequential backward selec-"
        },
        {
          "Jiang, J. (2008). A literature survey on domain adaptation of statistical clas-": "tion.\nIn 2019 11th International Conference on Intelligent Human-Machine"
        },
        {
          "Jiang, J. (2008). A literature survey on domain adaptation of statistical clas-": "Systems and Cybernetics (IHMSC), volume 1, pages 309–313. IEEE."
        },
        {
          "Jiang, J. (2008). A literature survey on domain adaptation of statistical clas-": "Jiang, X., Bian, G.-B., and Tian, Z.\n(2019b). Removal of artifacts\nfrom eeg"
        },
        {
          "Jiang, J. (2008). A literature survey on domain adaptation of statistical clas-": "signals: a review. Sensors, 19(5):987."
        },
        {
          "Jiang, J. (2008). A literature survey on domain adaptation of statistical clas-": "Jin, Y.-M., Luo, Y.-D., Zheng, W.-L., and Lu, B.-L. (2017). Eeg-based emotion"
        },
        {
          "Jiang, J. (2008). A literature survey on domain adaptation of statistical clas-": "recognition using domain adaptation network.\nIn 2017 international confer-"
        },
        {
          "Jiang, J. (2008). A literature survey on domain adaptation of statistical clas-": "ence on orange technologies (ICOT), pages 222–225. IEEE."
        },
        {
          "Jiang, J. (2008). A literature survey on domain adaptation of statistical clas-": "Joachims, T. et al. (1999). Transductive inference for text classification using"
        },
        {
          "Jiang, J. (2008). A literature survey on domain adaptation of statistical clas-": "support vector machines.\nIn Icml, volume 99, pages 200–209."
        },
        {
          "Jiang, J. (2008). A literature survey on domain adaptation of statistical clas-": "Katsigiannis, S. and Ramzan, N.\n(2017).\nDreamer: A database\nfor\nemotion"
        },
        {
          "Jiang, J. (2008). A literature survey on domain adaptation of statistical clas-": "recognition through eeg and ecg signals\nfrom wireless\nlow-cost off-the-shelf"
        },
        {
          "Jiang, J. (2008). A literature survey on domain adaptation of statistical clas-": "devices.\nIEEE journal of biomedical and health informatics, 22(1):98–107."
        },
        {
          "Jiang, J. (2008). A literature survey on domain adaptation of statistical clas-": "Kenneth, O. M., Bashir, S. A., Abisoye, O. A., and Mohammed, A. D. (2020)."
        },
        {
          "Jiang, J. (2008). A literature survey on domain adaptation of statistical clas-": "Face morphing\nattack\ndetection\nin\nthe\npresence\nof\npost-processed\nimage"
        },
        {
          "Jiang, J. (2008). A literature survey on domain adaptation of statistical clas-": "sources using neighborhood component analysis and decision tree classifier."
        },
        {
          "Jiang, J. (2008). A literature survey on domain adaptation of statistical clas-": "In International Conference on Information and Communication Technology"
        },
        {
          "Jiang, J. (2008). A literature survey on domain adaptation of statistical clas-": "and Applications, pages 340–354. Springer."
        },
        {
          "Jiang, J. (2008). A literature survey on domain adaptation of statistical clas-": "Kitchenham, B. (2004). Procedures for performing systematic reviews. Keele,"
        },
        {
          "Jiang, J. (2008). A literature survey on domain adaptation of statistical clas-": "UK, Keele University, 33(2004):1–26."
        },
        {
          "Jiang, J. (2008). A literature survey on domain adaptation of statistical clas-": "Koch, G., Zemel, R., Salakhutdinov, R., et al. (2015). Siamese neural networks"
        },
        {
          "Jiang, J. (2008). A literature survey on domain adaptation of statistical clas-": "for one-shot image recognition.\nIn ICML deep learning workshop, volume 2,"
        },
        {
          "Jiang, J. (2008). A literature survey on domain adaptation of statistical clas-": "page 0. Lille."
        },
        {
          "Jiang, J. (2008). A literature survey on domain adaptation of statistical clas-": "Koelstra, S., Muhl, C., Soleymani, M., Lee, J.-S., Yazdani, A., Ebrahimi, T.,"
        },
        {
          "Jiang, J. (2008). A literature survey on domain adaptation of statistical clas-": "Pun, T., Nijholt, A., and Patras,\nI.\n(2011). Deap: A database for emotion"
        },
        {
          "Jiang, J. (2008). A literature survey on domain adaptation of statistical clas-": "analysis; using physiological signals.\nIEEE transactions on affective comput-"
        },
        {
          "Jiang, J. (2008). A literature survey on domain adaptation of statistical clas-": "ing, 3(1):18–31."
        },
        {
          "Jiang, J. (2008). A literature survey on domain adaptation of statistical clas-": "Kroupi, E., Yazdani, A., and Ebrahimi, T.\n(2011). Eeg correlates of different"
        },
        {
          "Jiang, J. (2008). A literature survey on domain adaptation of statistical clas-": "emotional states elicited during watching music videos. In International Con-"
        },
        {
          "Jiang, J. (2008). A literature survey on domain adaptation of statistical clas-": "ference on Affective Computing and Intelligent\nInteraction, pages 457–466."
        },
        {
          "Jiang, J. (2008). A literature survey on domain adaptation of statistical clas-": "Springer."
        },
        {
          "Jiang, J. (2008). A literature survey on domain adaptation of statistical clas-": "Kuang, F., Shu, L., Hua, H., Wu, S., Zhang, L., Xu, X., Liu, Y., and Jiang,"
        },
        {
          "Jiang, J. (2008). A literature survey on domain adaptation of statistical clas-": "M. (2021). Cross-subject and cross-device wearable eeg emotion recognition"
        },
        {
          "Jiang, J. (2008). A literature survey on domain adaptation of statistical clas-": "using frontal eeg under virtual\nreality scenes.\nIn 2021 IEEE International"
        },
        {
          "Jiang, J. (2008). A literature survey on domain adaptation of statistical clas-": "Conference on Bioinformatics and Biomedicine\n(BIBM), pages 3630–3637."
        },
        {
          "Jiang, J. (2008). A literature survey on domain adaptation of statistical clas-": "IEEE."
        }
      ],
      "page": 37
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Kurdi, B., Lozano, S., and Banaji, M. R. (2017).\nIntroducing the open affective": "standardized image set (oasis). Behavior research methods, 49(2):457–470."
        },
        {
          "Kurdi, B., Lozano, S., and Banaji, M. R. (2017).\nIntroducing the open affective": "Lan, Z., Sourina, O., Wang, L., Scherer, R., and Müller-Putz, G. R.\n(2018)."
        },
        {
          "Kurdi, B., Lozano, S., and Banaji, M. R. (2017).\nIntroducing the open affective": "Domain adaptation techniques for eeg-based emotion recognition: a compar-"
        },
        {
          "Kurdi, B., Lozano, S., and Banaji, M. R. (2017).\nIntroducing the open affective": "ative\nstudy on two public datasets.\nIEEE Transactions on Cognitive and"
        },
        {
          "Kurdi, B., Lozano, S., and Banaji, M. R. (2017).\nIntroducing the open affective": "Developmental Systems, 11(1):85–94."
        },
        {
          "Kurdi, B., Lozano, S., and Banaji, M. R. (2017).\nIntroducing the open affective": "Lang, P. J. (2005).\nInternational affective picture system (iaps): Affective rat-"
        },
        {
          "Kurdi, B., Lozano, S., and Banaji, M. R. (2017).\nIntroducing the open affective": "ings of pictures and instruction manual. Technical report."
        },
        {
          "Kurdi, B., Lozano, S., and Banaji, M. R. (2017).\nIntroducing the open affective": "Lepage, K. Q., Kramer, M. A., and Chu, C. J. (2014). A statistically robust eeg"
        },
        {
          "Kurdi, B., Lozano, S., and Banaji, M. R. (2017).\nIntroducing the open affective": "re-referencing procedure to mitigate reference effect. Journal of neuroscience"
        },
        {
          "Kurdi, B., Lozano, S., and Banaji, M. R. (2017).\nIntroducing the open affective": "methods, 235:101–116."
        },
        {
          "Kurdi, B., Lozano, S., and Banaji, M. R. (2017).\nIntroducing the open affective": "Li, H., Jin, Y.-M., Zheng, W.-L., and Lu, B.-L. (2018a). Cross-subject emotion"
        },
        {
          "Kurdi, B., Lozano, S., and Banaji, M. R. (2017).\nIntroducing the open affective": "recognition using deep adaptation networks.\nIn International conference on"
        },
        {
          "Kurdi, B., Lozano, S., and Banaji, M. R. (2017).\nIntroducing the open affective": "neural\ninformation processing, pages 403–413. Springer."
        },
        {
          "Kurdi, B., Lozano, S., and Banaji, M. R. (2017).\nIntroducing the open affective": "Li, J., Chen, H., and Cai, T.\n(2020a).\nFoit: Fast online instance transfer\nfor"
        },
        {
          "Kurdi, B., Lozano, S., and Banaji, M. R. (2017).\nIntroducing the open affective": "improved eeg emotion recognition.\nIn 2020 IEEE International Conference"
        },
        {
          "Kurdi, B., Lozano, S., and Banaji, M. R. (2017).\nIntroducing the open affective": "on Bioinformatics and Biomedicine (BIBM), pages 2618–2625. IEEE."
        },
        {
          "Kurdi, B., Lozano, S., and Banaji, M. R. (2017).\nIntroducing the open affective": "Li, J., Li, S., Pan, J., and Wang, F. (2021a). Cross-subject eeg emotion recog-"
        },
        {
          "Kurdi, B., Lozano, S., and Banaji, M. R. (2017).\nIntroducing the open affective": "nition with self-organized graph neural network. Frontiers in Neuroscience,"
        },
        {
          "Kurdi, B., Lozano, S., and Banaji, M. R. (2017).\nIntroducing the open affective": "page 689."
        },
        {
          "Kurdi, B., Lozano, S., and Banaji, M. R. (2017).\nIntroducing the open affective": "Li, J., Qiu, S., Du, C., Wang, Y., and He, H.\n(2019a).\nDomain adaptation"
        },
        {
          "Kurdi, B., Lozano, S., and Banaji, M. R. (2017).\nIntroducing the open affective": "for eeg emotion recognition based on latent representation similarity.\nIEEE"
        },
        {
          "Kurdi, B., Lozano, S., and Banaji, M. R. (2017).\nIntroducing the open affective": "Transactions on Cognitive and Developmental Systems, 12(2):344–353."
        },
        {
          "Kurdi, B., Lozano, S., and Banaji, M. R. (2017).\nIntroducing the open affective": "Li, J., Qiu, S., Shen, Y.-Y., Liu, C.-L., and He, H. (2019b). Multisource transfer"
        },
        {
          "Kurdi, B., Lozano, S., and Banaji, M. R. (2017).\nIntroducing the open affective": "learning for\ncross-subject\neeg emotion recognition.\nIEEE transactions on"
        },
        {
          "Kurdi, B., Lozano, S., and Banaji, M. R. (2017).\nIntroducing the open affective": "cybernetics, 50(7):3281–3293."
        },
        {
          "Kurdi, B., Lozano, S., and Banaji, M. R. (2017).\nIntroducing the open affective": "Li, W., Hu, X., Long, X., Tang, L., Chen, J., Wang, F., and Zhang, D. (2020b)."
        },
        {
          "Kurdi, B., Lozano, S., and Banaji, M. R. (2017).\nIntroducing the open affective": "Eeg responses to emotional videos can quantitatively predict big-five person-"
        },
        {
          "Kurdi, B., Lozano, S., and Banaji, M. R. (2017).\nIntroducing the open affective": "ality traits. Neurocomputing, 415:368–381."
        },
        {
          "Kurdi, B., Lozano, S., and Banaji, M. R. (2017).\nIntroducing the open affective": "Li, W., Huan, W., Hou, B., Tian, Y., Zhang, Z., and Song, A.\n(2021b). Can"
        },
        {
          "Kurdi, B., Lozano, S., and Banaji, M. R. (2017).\nIntroducing the open affective": "emotion be transferred?–a review on transfer learning for eeg-based emotion"
        },
        {
          "Kurdi, B., Lozano, S., and Banaji, M. R. (2017).\nIntroducing the open affective": "recognition.\nIEEE Transactions on Cognitive and Developmental Systems."
        },
        {
          "Kurdi, B., Lozano, S., and Banaji, M. R. (2017).\nIntroducing the open affective": "Li, X., Song, D., Zhang, P., Zhang, Y., Hou, Y., and Hu, B. (2018b). Exploring"
        },
        {
          "Kurdi, B., Lozano, S., and Banaji, M. R. (2017).\nIntroducing the open affective": "eeg features in cross-subject emotion recognition. Frontiers in neuroscience,"
        },
        {
          "Kurdi, B., Lozano, S., and Banaji, M. R. (2017).\nIntroducing the open affective": "12:162."
        },
        {
          "Kurdi, B., Lozano, S., and Banaji, M. R. (2017).\nIntroducing the open affective": "Li, X., Zhao, Z., Song, D., Zhang, Y., Niu, C., Zhang, J., Huo, J., and Li, J."
        },
        {
          "Kurdi, B., Lozano, S., and Banaji, M. R. (2017).\nIntroducing the open affective": "(2019c). Variational autoencoder based latent factor decoding of multichan-"
        },
        {
          "Kurdi, B., Lozano, S., and Banaji, M. R. (2017).\nIntroducing the open affective": "nel eeg for emotion recognition.\nIn 2019 IEEE International Conference on"
        },
        {
          "Kurdi, B., Lozano, S., and Banaji, M. R. (2017).\nIntroducing the open affective": "Bioinformatics and Biomedicine (BIBM), pages 684–687. IEEE."
        }
      ],
      "page": 38
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Li, X., Zhao, Z., Song, D., Zhang, Y., Pan, J., Wu, L., Huo, J., Niu, C., and": "Wang, D. (2020c). Latent factor decoding of multi-channel eeg for emotion"
        },
        {
          "Li, X., Zhao, Z., Song, D., Zhang, Y., Pan, J., Wu, L., Huo, J., Niu, C., and": "recognition through autoencoder-like neural networks.\nFrontiers\nin neuro-"
        },
        {
          "Li, X., Zhao, Z., Song, D., Zhang, Y., Pan, J., Wu, L., Huo, J., Niu, C., and": "science, 14:87."
        },
        {
          "Li, X., Zhao, Z., Song, D., Zhang, Y., Pan, J., Wu, L., Huo, J., Niu, C., and": "Li, Y., Wang, L., Zheng, W., Zong, Y., Qi, L., Cui, Z., Zhang, T., and Song, T."
        },
        {
          "Li, X., Zhao, Z., Song, D., Zhang, Y., Pan, J., Wu, L., Huo, J., Niu, C., and": "(2020d). A novel bi-hemispheric discrepancy model\nfor eeg emotion recogni-"
        },
        {
          "Li, X., Zhao, Z., Song, D., Zhang, Y., Pan, J., Wu, L., Huo, J., Niu, C., and": "tion. IEEE Transactions on Cognitive and Developmental Systems, 13(2):354–"
        },
        {
          "Li, X., Zhao, Z., Song, D., Zhang, Y., Pan, J., Wu, L., Huo, J., Niu, C., and": "367."
        },
        {
          "Li, X., Zhao, Z., Song, D., Zhang, Y., Pan, J., Wu, L., Huo, J., Niu, C., and": "Li, Y., Wang, N., Shi, J., Hou, X., and Liu, J. (2018c). Adaptive batch normal-"
        },
        {
          "Li, X., Zhao, Z., Song, D., Zhang, Y., Pan, J., Wu, L., Huo, J., Niu, C., and": "ization for practical domain adaptation. Pattern Recognition, 80:109–117."
        },
        {
          "Li, X., Zhao, Z., Song, D., Zhang, Y., Pan, J., Wu, L., Huo, J., Niu, C., and": "Li, Y., Zheng, W., Cui, Z., Zhang, T., and Zong, Y.\n(2018d). A novel neural"
        },
        {
          "Li, X., Zhao, Z., Song, D., Zhang, Y., Pan, J., Wu, L., Huo, J., Niu, C., and": "network model based on cerebral hemispheric asymmetry for\neeg emotion"
        },
        {
          "Li, X., Zhao, Z., Song, D., Zhang, Y., Pan, J., Wu, L., Huo, J., Niu, C., and": "recognition.\nIn IJCAI, pages 1561–1567."
        },
        {
          "Li, X., Zhao, Z., Song, D., Zhang, Y., Pan, J., Wu, L., Huo, J., Niu, C., and": "Li, Y., Zheng, W., Wang, L., Zong, Y., and Cui, Z. (2019d). From regional to"
        },
        {
          "Li, X., Zhao, Z., Song, D., Zhang, Y., Pan, J., Wu, L., Huo, J., Niu, C., and": "global brain: A novel hierarchical spatial-temporal neural network model\nfor"
        },
        {
          "Li, X., Zhao, Z., Song, D., Zhang, Y., Pan, J., Wu, L., Huo, J., Niu, C., and": "eeg emotion recognition.\nIEEE Transactions on Affective Computing."
        },
        {
          "Li, X., Zhao, Z., Song, D., Zhang, Y., Pan, J., Wu, L., Huo, J., Niu, C., and": "Li, Y., Zheng, W., Zong, Y., Cui, Z., Zhang, T., and Zhou, X. (2018e). A bi-"
        },
        {
          "Li, X., Zhao, Z., Song, D., Zhang, Y., Pan, J., Wu, L., Huo, J., Niu, C., and": "hemisphere domain adversarial neural network model\nfor eeg emotion recog-"
        },
        {
          "Li, X., Zhao, Z., Song, D., Zhang, Y., Pan, J., Wu, L., Huo, J., Niu, C., and": "nition.\nIEEE Transactions on Affective Computing."
        },
        {
          "Li, X., Zhao, Z., Song, D., Zhang, Y., Pan, J., Wu, L., Huo, J., Niu, C., and": "Li, Z., Chen, H., Jin, M., and Li, J. (2021c). Reducing the calibration effort of"
        },
        {
          "Li, X., Zhao, Z., Song, D., Zhang, Y., Pan, J., Wu, L., Huo, J., Niu, C., and": "eeg emotion recognition using domain adaptation with soft\nlabels.\nIn 2021"
        },
        {
          "Li, X., Zhao, Z., Song, D., Zhang, Y., Pan, J., Wu, L., Huo, J., Niu, C., and": "43rd Annual International Conference of\nthe IEEE Engineering in Medicine"
        },
        {
          "Li, X., Zhao, Z., Song, D., Zhang, Y., Pan, J., Wu, L., Huo, J., Niu, C., and": "& Biology Society (EMBC), pages 5962–5965. IEEE."
        },
        {
          "Li, X., Zhao, Z., Song, D., Zhang, Y., Pan, J., Wu, L., Huo, J., Niu, C., and": "Liberati, A., Altman, D. G., Tetzlaff, J., Mulrow, C., Gøtzsche, P. C.,\nIoanni-"
        },
        {
          "Li, X., Zhao, Z., Song, D., Zhang, Y., Pan, J., Wu, L., Huo, J., Niu, C., and": "dis, J. P., Clarke, M., Devereaux, P. J., Kleijnen, J., and Moher, D. (2009)."
        },
        {
          "Li, X., Zhao, Z., Song, D., Zhang, Y., Pan, J., Wu, L., Huo, J., Niu, C., and": "The prisma statement for reporting systematic reviews and meta-analyses of"
        },
        {
          "Li, X., Zhao, Z., Song, D., Zhang, Y., Pan, J., Wu, L., Huo, J., Niu, C., and": "studies that evaluate health care interventions:\nexplanation and elaboration."
        },
        {
          "Li, X., Zhao, Z., Song, D., Zhang, Y., Pan, J., Wu, L., Huo, J., Niu, C., and": "Journal of clinical epidemiology, 62(10):e1–e34."
        },
        {
          "Li, X., Zhao, Z., Song, D., Zhang, Y., Pan, J., Wu, L., Huo, J., Niu, C., and": "Lin, Y.-P.\n(2019).\nConstructing a personalized cross-day eeg-based emotion-"
        },
        {
          "Li, X., Zhao, Z., Song, D., Zhang, Y., Pan, J., Wu, L., Huo, J., Niu, C., and": "classification model using transfer learning.\nIEEE journal of biomedical and"
        },
        {
          "Li, X., Zhao, Z., Song, D., Zhang, Y., Pan, J., Wu, L., Huo, J., Niu, C., and": "health informatics, 24(5):1255–1264."
        },
        {
          "Li, X., Zhao, Z., Song, D., Zhang, Y., Pan, J., Wu, L., Huo, J., Niu, C., and": "Lin, Y.-P., Jao, P.-K., and Yang, Y.-H. (2017).\nImproving cross-day eeg-based"
        },
        {
          "Li, X., Zhao, Z., Song, D., Zhang, Y., Pan, J., Wu, L., Huo, J., Niu, C., and": "emotion classification using robust principal component analysis.\nFrontiers"
        },
        {
          "Li, X., Zhao, Z., Song, D., Zhang, Y., Pan, J., Wu, L., Huo, J., Niu, C., and": "in computational neuroscience, 11:64."
        },
        {
          "Li, X., Zhao, Z., Song, D., Zhang, Y., Pan, J., Wu, L., Huo, J., Niu, C., and": "Lin, Y.-P., Wang, C.-H., Jung, T.-P., Wu, T.-L., Jeng, S.-K., Duann, J.-R., and"
        },
        {
          "Li, X., Zhao, Z., Song, D., Zhang, Y., Pan, J., Wu, L., Huo, J., Niu, C., and": "Chen, J.-H. (2010). Eeg-based emotion recognition in music listening.\nIEEE"
        },
        {
          "Li, X., Zhao, Z., Song, D., Zhang, Y., Pan, J., Wu, L., Huo, J., Niu, C., and": "Transactions on Biomedical Engineering, 57(7):1798–1806."
        }
      ],
      "page": 39
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Liu,\nJ., Shen, X., Song, S.,\nand Zhang, D.\n(2021).\nDomain adaptation for": "cross-subject\nemotion recognition by subject\nclustering.\nIn 2021 10th In-"
        },
        {
          "Liu,\nJ., Shen, X., Song, S.,\nand Zhang, D.\n(2021).\nDomain adaptation for": "ternational\nIEEE/EMBS Conference on Neural Engineering\n(NER), pages"
        },
        {
          "Liu,\nJ., Shen, X., Song, S.,\nand Zhang, D.\n(2021).\nDomain adaptation for": "904–908. IEEE."
        },
        {
          "Liu,\nJ., Shen, X., Song, S.,\nand Zhang, D.\n(2021).\nDomain adaptation for": "Liu, S., Wang, X., Zhao, L., Zhao, J., Xin, Q., and Wang, S. (2020). Subject-"
        },
        {
          "Liu,\nJ., Shen, X., Song, S.,\nand Zhang, D.\n(2021).\nDomain adaptation for": "independent emotion recognition of eeg signals based on dynamic empirical"
        },
        {
          "Liu,\nJ., Shen, X., Song, S.,\nand Zhang, D.\n(2021).\nDomain adaptation for": "convolutional neural network.\nIEEE/ACM Transactions on Computational"
        },
        {
          "Liu,\nJ., Shen, X., Song, S.,\nand Zhang, D.\n(2021).\nDomain adaptation for": "Biology and Bioinformatics."
        },
        {
          "Liu,\nJ., Shen, X., Song, S.,\nand Zhang, D.\n(2021).\nDomain adaptation for": "Long, M., Cao, Y., Wang, J., and Jordan, M. (2015). Learning transferable fea-"
        },
        {
          "Liu,\nJ., Shen, X., Song, S.,\nand Zhang, D.\n(2021).\nDomain adaptation for": "tures with deep adaptation networks. In International conference on machine"
        },
        {
          "Liu,\nJ., Shen, X., Song, S.,\nand Zhang, D.\n(2021).\nDomain adaptation for": "learning, pages 97–105. PMLR."
        },
        {
          "Liu,\nJ., Shen, X., Song, S.,\nand Zhang, D.\n(2021).\nDomain adaptation for": "Long, M., Ding, G., Wang, J., Sun, J., Guo, Y., and Yu, P. S. (2013). Transfer"
        },
        {
          "Liu,\nJ., Shen, X., Song, S.,\nand Zhang, D.\n(2021).\nDomain adaptation for": "sparse coding for\nrobust\nimage representation.\nIn Proceedings of\nthe IEEE"
        },
        {
          "Liu,\nJ., Shen, X., Song, S.,\nand Zhang, D.\n(2021).\nDomain adaptation for": "conference on computer vision and pattern recognition, pages 407–414."
        },
        {
          "Liu,\nJ., Shen, X., Song, S.,\nand Zhang, D.\n(2021).\nDomain adaptation for": "Lopez-Gordo, M. A., Sanchez-Morillo, D., and Valle, F. P.\n(2014).\nDry eeg"
        },
        {
          "Liu,\nJ., Shen, X., Song, S.,\nand Zhang, D.\n(2021).\nDomain adaptation for": "electrodes. Sensors, 14(7):12847–12870."
        },
        {
          "Liu,\nJ., Shen, X., Song, S.,\nand Zhang, D.\n(2021).\nDomain adaptation for": "Lotte, F., Bougrain, L., Cichocki, A., Clerc, M., Congedo, M., Rakotomamonjy,"
        },
        {
          "Liu,\nJ., Shen, X., Song, S.,\nand Zhang, D.\n(2021).\nDomain adaptation for": "A., and Yger, F. (2018). A review of classification algorithms for eeg-based"
        },
        {
          "Liu,\nJ., Shen, X., Song, S.,\nand Zhang, D.\n(2021).\nDomain adaptation for": "brain–computer interfaces: a 10 year update. Journal of neural engineering,"
        },
        {
          "Liu,\nJ., Shen, X., Song, S.,\nand Zhang, D.\n(2021).\nDomain adaptation for": "15(3):031005."
        },
        {
          "Liu,\nJ., Shen, X., Song, S.,\nand Zhang, D.\n(2021).\nDomain adaptation for": "Luo, J., Wu, M., Wang, Z., Chen, Y., and Yang, Y.\n(2021). Progressive low-"
        },
        {
          "Liu,\nJ., Shen, X., Song, S.,\nand Zhang, D.\n(2021).\nDomain adaptation for": "rank subspace alignment based on semi-supervised joint domain adaption for"
        },
        {
          "Liu,\nJ., Shen, X., Song, S.,\nand Zhang, D.\n(2021).\nDomain adaptation for": "personalized emotion recognition. Neurocomputing, 456:312–326."
        },
        {
          "Liu,\nJ., Shen, X., Song, S.,\nand Zhang, D.\n(2021).\nDomain adaptation for": "Luo, Y. and Lu, B.-L. (2021). Wasserstein-distance-based multi-source adver-"
        },
        {
          "Liu,\nJ., Shen, X., Song, S.,\nand Zhang, D.\n(2021).\nDomain adaptation for": "sarial domain adaptation for emotion recognition and vigilance estimation."
        },
        {
          "Liu,\nJ., Shen, X., Song, S.,\nand Zhang, D.\n(2021).\nDomain adaptation for": "In 2021 IEEE International Conference on Bioinformatics and Biomedicine"
        },
        {
          "Liu,\nJ., Shen, X., Song, S.,\nand Zhang, D.\n(2021).\nDomain adaptation for": "(BIBM), pages 1424–1428. IEEE."
        },
        {
          "Liu,\nJ., Shen, X., Song, S.,\nand Zhang, D.\n(2021).\nDomain adaptation for": "Luo, Y., Zhang, S.-Y., Zheng, W.-L.,\nand Lu, B.-L.\n(2018). Wgan domain"
        },
        {
          "Liu,\nJ., Shen, X., Song, S.,\nand Zhang, D.\n(2021).\nDomain adaptation for": "adaptation for eeg-based emotion recognition.\nIn International Conference"
        },
        {
          "Liu,\nJ., Shen, X., Song, S.,\nand Zhang, D.\n(2021).\nDomain adaptation for": "on Neural Information Processing, pages 275–286. Springer."
        },
        {
          "Liu,\nJ., Shen, X., Song, S.,\nand Zhang, D.\n(2021).\nDomain adaptation for": "Ma, B.-Q., Li, H., Zheng, W.-L., and Lu, B.-L. (2019). Reducing the subject"
        },
        {
          "Liu,\nJ., Shen, X., Song, S.,\nand Zhang, D.\n(2021).\nDomain adaptation for": "variability of eeg signals with adversarial domain generalization.\nIn Interna-"
        },
        {
          "Liu,\nJ., Shen, X., Song, S.,\nand Zhang, D.\n(2021).\nDomain adaptation for": "tional Conference on Neural Information Processing, pages 30–42. Springer."
        },
        {
          "Liu,\nJ., Shen, X., Song, S.,\nand Zhang, D.\n(2021).\nDomain adaptation for": "Mairal, J., Ponce, J., Sapiro, G., Zisserman, A., and Bach, F. (2008). Supervised"
        },
        {
          "Liu,\nJ., Shen, X., Song, S.,\nand Zhang, D.\n(2021).\nDomain adaptation for": "dictionary learning. Advances in neural\ninformation processing systems, 21."
        },
        {
          "Liu,\nJ., Shen, X., Song, S.,\nand Zhang, D.\n(2021).\nDomain adaptation for": "Muandet, K., Balduzzi, D., and Schölkopf, B.\n(2013). Domain generalization"
        },
        {
          "Liu,\nJ., Shen, X., Song, S.,\nand Zhang, D.\n(2021).\nDomain adaptation for": "via invariant feature representation.\nIn International Conference on Machine"
        },
        {
          "Liu,\nJ., Shen, X., Song, S.,\nand Zhang, D.\n(2021).\nDomain adaptation for": "Learning, pages 10–18. PMLR."
        }
      ],
      "page": 40
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Nan, X. and Jinghua, X. (1988). The fractal dimension of eeg as a physical mea-": "sure of conscious human brain activities. Bulletin of Mathematical Biology,"
        },
        {
          "Nan, X. and Jinghua, X. (1988). The fractal dimension of eeg as a physical mea-": "50(5):559–565."
        },
        {
          "Nan, X. and Jinghua, X. (1988). The fractal dimension of eeg as a physical mea-": "Nasiri, S. and Clifford, G. D. (2020). Attentive adversarial network for large-"
        },
        {
          "Nan, X. and Jinghua, X. (1988). The fractal dimension of eeg as a physical mea-": "scale sleep staging.\nIn Machine Learning for Healthcare Conference, pages"
        },
        {
          "Nan, X. and Jinghua, X. (1988). The fractal dimension of eeg as a physical mea-": "457–478. PMLR."
        },
        {
          "Nan, X. and Jinghua, X. (1988). The fractal dimension of eeg as a physical mea-": "Ni, T., Ni, Y., Xue, J., and Wang, S. (2021). A domain adaptation sparse rep-"
        },
        {
          "Nan, X. and Jinghua, X. (1988). The fractal dimension of eeg as a physical mea-": "resentation classifier\nfor\ncross-domain electroencephalogram-based emotion"
        },
        {
          "Nan, X. and Jinghua, X. (1988). The fractal dimension of eeg as a physical mea-": "classification. Frontiers in Psychology, page 3015."
        },
        {
          "Nan, X. and Jinghua, X. (1988). The fractal dimension of eeg as a physical mea-": "Ning, R., Chen, C. P., and Zhang, T. (2021). Cross-subject eeg emotion recog-"
        },
        {
          "Nan, X. and Jinghua, X. (1988). The fractal dimension of eeg as a physical mea-": "nition using domain adaptive few-shot learning networks.\nIn 2021 IEEE In-"
        },
        {
          "Nan, X. and Jinghua, X. (1988). The fractal dimension of eeg as a physical mea-": "ternational Conference on Bioinformatics and Biomedicine\n(BIBM), pages"
        },
        {
          "Nan, X. and Jinghua, X. (1988). The fractal dimension of eeg as a physical mea-": "1468–1472. IEEE."
        },
        {
          "Nan, X. and Jinghua, X. (1988). The fractal dimension of eeg as a physical mea-": "Niu, Z., Zhong, G., and Yu, H. (2021). A review on the attention mechanism of"
        },
        {
          "Nan, X. and Jinghua, X. (1988). The fractal dimension of eeg as a physical mea-": "deep learning. Neurocomputing, 452:48–62."
        },
        {
          "Nan, X. and Jinghua, X. (1988). The fractal dimension of eeg as a physical mea-": "Oh, S.-H., Lee, Y.-R., and Kim, H.-N. (2014). A novel eeg feature extraction"
        },
        {
          "Nan, X. and Jinghua, X. (1988). The fractal dimension of eeg as a physical mea-": "method using hjorth parameter.\nInternational Journal of Electronics and"
        },
        {
          "Nan, X. and Jinghua, X. (1988). The fractal dimension of eeg as a physical mea-": "Electrical Engineering, 2(2):106–110."
        },
        {
          "Nan, X. and Jinghua, X. (1988). The fractal dimension of eeg as a physical mea-": "Ozturk, M. C., Xu, D., and Principe, J. C. (2007). Analysis and design of echo"
        },
        {
          "Nan, X. and Jinghua, X. (1988). The fractal dimension of eeg as a physical mea-": "state networks. Neural computation, 19(1):111–138."
        },
        {
          "Nan, X. and Jinghua, X. (1988). The fractal dimension of eeg as a physical mea-": "Pan, S. J., Tsang,\nI. W., Kwok, J. T., and Yang, Q. (2010). Domain adapta-"
        },
        {
          "Nan, X. and Jinghua, X. (1988). The fractal dimension of eeg as a physical mea-": "tion via transfer component analysis.\nIEEE transactions on neural networks,"
        },
        {
          "Nan, X. and Jinghua, X. (1988). The fractal dimension of eeg as a physical mea-": "22(2):199–210."
        },
        {
          "Nan, X. and Jinghua, X. (1988). The fractal dimension of eeg as a physical mea-": "Pan, S. J. and Yang, Q. (2009). A survey on transfer learning.\nIEEE Transac-"
        },
        {
          "Nan, X. and Jinghua, X. (1988). The fractal dimension of eeg as a physical mea-": "tions on knowledge and data engineering, 22(10):1345–1359."
        },
        {
          "Nan, X. and Jinghua, X. (1988). The fractal dimension of eeg as a physical mea-": "Pandey, P. and Seeja, K. (2019). Subject independent emotion recognition from"
        },
        {
          "Nan, X. and Jinghua, X. (1988). The fractal dimension of eeg as a physical mea-": "eeg using vmd and deep learning. Journal of King Saud University-Computer"
        },
        {
          "Nan, X. and Jinghua, X. (1988). The fractal dimension of eeg as a physical mea-": "and Information Sciences."
        },
        {
          "Nan, X. and Jinghua, X. (1988). The fractal dimension of eeg as a physical mea-": "Patil, A., Deshmukh, C., and Panat, A.\n(2016). Feature extraction of eeg for"
        },
        {
          "Nan, X. and Jinghua, X. (1988). The fractal dimension of eeg as a physical mea-": "emotion recognition using hjorth features and higher order crossings.\nIn 2016"
        },
        {
          "Nan, X. and Jinghua, X. (1988). The fractal dimension of eeg as a physical mea-": "Conference on Advances in Signal Processing (CASP), pages 429–434. IEEE."
        },
        {
          "Nan, X. and Jinghua, X. (1988). The fractal dimension of eeg as a physical mea-": "Peng, Y., Kong, W., Qin, F., Nie, F., Fang, J., Lu, B.-L., and Cichocki, A."
        },
        {
          "Nan, X. and Jinghua, X. (1988). The fractal dimension of eeg as a physical mea-": "(2021).\nSelf-weighted semi-supervised classification for joint eeg-based emo-"
        },
        {
          "Nan, X. and Jinghua, X. (1988). The fractal dimension of eeg as a physical mea-": "tion recognition and affective activation patterns mining.\nIEEE Transactions"
        },
        {
          "Nan, X. and Jinghua, X. (1988). The fractal dimension of eeg as a physical mea-": "on Instrumentation and Measurement, 70:1–11."
        },
        {
          "Nan, X. and Jinghua, X. (1988). The fractal dimension of eeg as a physical mea-": "Phadikar, S., Sinha, N., and Ghosh, R. (2019). A survey on feature extraction"
        },
        {
          "Nan, X. and Jinghua, X. (1988). The fractal dimension of eeg as a physical mea-": "methods for eeg based emotion recognition.\nIn International Conference on"
        },
        {
          "Nan, X. and Jinghua, X. (1988). The fractal dimension of eeg as a physical mea-": "Innovation in Modern Science and Technology, pages 31–45. Springer."
        }
      ],
      "page": 41
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Posner, J., Russell, J. A., Gerber, A., Gorman, D., Colibazzi, T., Yu, S., Wang,": "Z., Kangarlu, A., Zhu, H., and Peterson, B. S. (2009). The neurophysiological"
        },
        {
          "Posner, J., Russell, J. A., Gerber, A., Gorman, D., Colibazzi, T., Yu, S., Wang,": "bases of emotion: An fmri study of\nthe affective circumplex using emotion-"
        },
        {
          "Posner, J., Russell, J. A., Gerber, A., Gorman, D., Colibazzi, T., Yu, S., Wang,": "denoting words. Human brain mapping, 30(3):883–895."
        },
        {
          "Posner, J., Russell, J. A., Gerber, A., Gorman, D., Colibazzi, T., Yu, S., Wang,": "Pusarla, A. N., Singh, B. A., and Tripathi, C. S.\n(2022).\nLearning densenet"
        },
        {
          "Posner, J., Russell, J. A., Gerber, A., Gorman, D., Colibazzi, T., Yu, S., Wang,": "features from eeg based spectrograms for subject independent emotion recog-"
        },
        {
          "Posner, J., Russell, J. A., Gerber, A., Gorman, D., Colibazzi, T., Yu, S., Wang,": "nition. Biomedical Signal Processing and Control, 74:103485."
        },
        {
          "Posner, J., Russell, J. A., Gerber, A., Gorman, D., Colibazzi, T., Yu, S., Wang,": "Quinonero-Candela, J., Sugiyama, M., Schwaighofer, A., and Lawrence, N. D."
        },
        {
          "Posner, J., Russell, J. A., Gerber, A., Gorman, D., Colibazzi, T., Yu, S., Wang,": "(2008). Dataset shift\nin machine learning. Mit Press."
        },
        {
          "Posner, J., Russell, J. A., Gerber, A., Gorman, D., Colibazzi, T., Yu, S., Wang,": "Rayatdoost, S.\nand Soleymani, M.\n(2018).\nCross-corpus\neeg-based emotion"
        },
        {
          "Posner, J., Russell, J. A., Gerber, A., Gorman, D., Colibazzi, T., Yu, S., Wang,": "recognition. In 2018 IEEE 28th International Workshop on Machine Learning"
        },
        {
          "Posner, J., Russell, J. A., Gerber, A., Gorman, D., Colibazzi, T., Yu, S., Wang,": "for Signal Processing (MLSP), pages 1–6. IEEE."
        },
        {
          "Posner, J., Russell, J. A., Gerber, A., Gorman, D., Colibazzi, T., Yu, S., Wang,": "Saganowski, S. (2022). Bringing emotion recognition out of the lab into real life:"
        },
        {
          "Posner, J., Russell, J. A., Gerber, A., Gorman, D., Colibazzi, T., Yu, S., Wang,": "Recent advances in sensors and machine learning. Electronics, 11(3):496."
        },
        {
          "Posner, J., Russell, J. A., Gerber, A., Gorman, D., Colibazzi, T., Yu, S., Wang,": "Saganowski, S., Dutkowiak, A., Dziadek, A., Dzieżyc, M., Komoszyńska, J.,"
        },
        {
          "Posner, J., Russell, J. A., Gerber, A., Gorman, D., Colibazzi, T., Yu, S., Wang,": "Michalska, W., Polak, A., Ujma, M.,\nand Kazienko, P.\n(2020).\nEmotion"
        },
        {
          "Posner, J., Russell, J. A., Gerber, A., Gorman, D., Colibazzi, T., Yu, S., Wang,": "recognition using wearables: A systematic literature review-work-in-progress."
        },
        {
          "Posner, J., Russell, J. A., Gerber, A., Gorman, D., Colibazzi, T., Yu, S., Wang,": "In 2020 IEEE International Conference on Pervasive Computing and Com-"
        },
        {
          "Posner, J., Russell, J. A., Gerber, A., Gorman, D., Colibazzi, T., Yu, S., Wang,": "munications Workshops (PerCom Workshops), pages 1–6. IEEE."
        },
        {
          "Posner, J., Russell, J. A., Gerber, A., Gorman, D., Colibazzi, T., Yu, S., Wang,": "Saito, K., Watanabe, K., Ushiku, Y., and Harada, T. (2018). Maximum classifier"
        },
        {
          "Posner, J., Russell, J. A., Gerber, A., Gorman, D., Colibazzi, T., Yu, S., Wang,": "discrepancy for unsupervised domain adaptation.\nIn Proceedings of the IEEE"
        },
        {
          "Posner, J., Russell, J. A., Gerber, A., Gorman, D., Colibazzi, T., Yu, S., Wang,": "conference on computer vision and pattern recognition, pages 3723–3732."
        },
        {
          "Posner, J., Russell, J. A., Gerber, A., Gorman, D., Colibazzi, T., Yu, S., Wang,": "Schölkopf, B., Smola, A., and Müller, K.-R. (1997). Kernel principal component"
        },
        {
          "Posner, J., Russell, J. A., Gerber, A., Gorman, D., Colibazzi, T., Yu, S., Wang,": "analysis. In International conference on artificial neural networks, pages 583–"
        },
        {
          "Posner, J., Russell, J. A., Gerber, A., Gorman, D., Colibazzi, T., Yu, S., Wang,": "588. Springer."
        },
        {
          "Posner, J., Russell, J. A., Gerber, A., Gorman, D., Colibazzi, T., Yu, S., Wang,": "Shenoy, P., Krauledat, M., Blankertz, B., Rao, R. P.,\nand Müller, K.-R."
        },
        {
          "Posner, J., Russell, J. A., Gerber, A., Gorman, D., Colibazzi, T., Yu, S., Wang,": "(2006). Towards adaptive classification for bci. Journal of neural engineering,"
        },
        {
          "Posner, J., Russell, J. A., Gerber, A., Gorman, D., Colibazzi, T., Yu, S., Wang,": "3(1):R13."
        },
        {
          "Posner, J., Russell, J. A., Gerber, A., Gorman, D., Colibazzi, T., Yu, S., Wang,": "Soleymani, M., Pantic, M., and Pun, T. (2011). Multimodal emotion recognition"
        },
        {
          "Posner, J., Russell, J. A., Gerber, A., Gorman, D., Colibazzi, T., Yu, S., Wang,": "in response to videos.\nIEEE transactions on affective computing, 3(2):211–"
        },
        {
          "Posner, J., Russell, J. A., Gerber, A., Gorman, D., Colibazzi, T., Yu, S., Wang,": "223."
        },
        {
          "Posner, J., Russell, J. A., Gerber, A., Gorman, D., Colibazzi, T., Yu, S., Wang,": "Song, T., Zheng, W., Song, P., and Cui, Z.\n(2018).\nEeg emotion recognition"
        },
        {
          "Posner, J., Russell, J. A., Gerber, A., Gorman, D., Colibazzi, T., Yu, S., Wang,": "using dynamical graph convolutional neural networks. IEEE Transactions on"
        },
        {
          "Posner, J., Russell, J. A., Gerber, A., Gorman, D., Colibazzi, T., Yu, S., Wang,": "Affective Computing, 11(3):532–541."
        },
        {
          "Posner, J., Russell, J. A., Gerber, A., Gorman, D., Colibazzi, T., Yu, S., Wang,": "Sörnmo, L. and Laguna, P. (2005). Bioelectrical signal processing in cardiac and"
        },
        {
          "Posner, J., Russell, J. A., Gerber, A., Gorman, D., Colibazzi, T., Yu, S., Wang,": "neurological applications, volume 8. Academic press."
        },
        {
          "Posner, J., Russell, J. A., Gerber, A., Gorman, D., Colibazzi, T., Yu, S., Wang,": "Subramanian, R., Wache, J., Abadi, M. K., Vieriu, R. L., Winkler, S., and Sebe,"
        },
        {
          "Posner, J., Russell, J. A., Gerber, A., Gorman, D., Colibazzi, T., Yu, S., Wang,": "N. (2016). Ascertain: Emotion and personality recognition using commercial"
        },
        {
          "Posner, J., Russell, J. A., Gerber, A., Gorman, D., Colibazzi, T., Yu, S., Wang,": "sensors.\nIEEE Transactions on Affective Computing, 9(2):147–160."
        }
      ],
      "page": 42
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Suhaimi, N. S., Mountstephens, J.,\nand Teo, J.\n(2020).\nEeg-based emotion": "recognition: A state-of-the-art\nreview of\ncurrent\ntrends and opportunities."
        },
        {
          "Suhaimi, N. S., Mountstephens, J.,\nand Teo, J.\n(2020).\nEeg-based emotion": "Computational\nintelligence and neuroscience, 2020."
        },
        {
          "Suhaimi, N. S., Mountstephens, J.,\nand Teo, J.\n(2020).\nEeg-based emotion": "Szegedy, C.,\nIoffe, S., Vanhoucke, V.,\nand Alemi, A. A.\n(2017).\nInception-"
        },
        {
          "Suhaimi, N. S., Mountstephens, J.,\nand Teo, J.\n(2020).\nEeg-based emotion": "v4,\ninception-resnet and the impact of\nresidual connections on learning.\nIn"
        },
        {
          "Suhaimi, N. S., Mountstephens, J.,\nand Teo, J.\n(2020).\nEeg-based emotion": "Thirty-first AAAI conference on artificial\nintelligence."
        },
        {
          "Suhaimi, N. S., Mountstephens, J.,\nand Teo, J.\n(2020).\nEeg-based emotion": "Tao, J. and Dan, Y. (2021). Multi-source co-adaptation for eeg-based emotion"
        },
        {
          "Suhaimi, N. S., Mountstephens, J.,\nand Teo, J.\n(2020).\nEeg-based emotion": "recognition by mining correlation information.\nFrontiers\nin Neuroscience,"
        },
        {
          "Suhaimi, N. S., Mountstephens, J.,\nand Teo, J.\n(2020).\nEeg-based emotion": "15:401."
        },
        {
          "Suhaimi, N. S., Mountstephens, J.,\nand Teo, J.\n(2020).\nEeg-based emotion": "TenHouten, W. D.\n(2017). From primary emotions to the spectrum of affect:"
        },
        {
          "Suhaimi, N. S., Mountstephens, J.,\nand Teo, J.\n(2020).\nEeg-based emotion": "An evolutionary neurosociology of the emotions.\nIn Neuroscience and social"
        },
        {
          "Suhaimi, N. S., Mountstephens, J.,\nand Teo, J.\n(2020).\nEeg-based emotion": "science, pages 141–167. Springer."
        },
        {
          "Suhaimi, N. S., Mountstephens, J.,\nand Teo, J.\n(2020).\nEeg-based emotion": "Tian, Z., Huang, D., Zhou, S., Zhao, Z., and Jiang, D. (2021). Personality first"
        },
        {
          "Suhaimi, N. S., Mountstephens, J.,\nand Teo, J.\n(2020).\nEeg-based emotion": "in emotion:\na deep neural network based on electroencephalogram channel"
        },
        {
          "Suhaimi, N. S., Mountstephens, J.,\nand Teo, J.\n(2020).\nEeg-based emotion": "attention for cross-subject emotion recognition. Royal Society open science,"
        },
        {
          "Suhaimi, N. S., Mountstephens, J.,\nand Teo, J.\n(2020).\nEeg-based emotion": "8(8):201976."
        },
        {
          "Suhaimi, N. S., Mountstephens, J.,\nand Teo, J.\n(2020).\nEeg-based emotion": "Torres, E. P., Torres, E. A., Hernández-Álvarez, M., and Yoo, S. G.\n(2020)."
        },
        {
          "Suhaimi, N. S., Mountstephens, J.,\nand Teo, J.\n(2020).\nEeg-based emotion": "Eeg-based bci emotion recognition: A survey. Sensors, 20(18):5083."
        },
        {
          "Suhaimi, N. S., Mountstephens, J.,\nand Teo, J.\n(2020).\nEeg-based emotion": "Tzeng, E., Hoffman, J., Saenko, K., and Darrell, T.\n(2017). Adversarial dis-"
        },
        {
          "Suhaimi, N. S., Mountstephens, J.,\nand Teo, J.\n(2020).\nEeg-based emotion": "criminative domain adaptation.\nIn Proceedings of\nthe IEEE conference on"
        },
        {
          "Suhaimi, N. S., Mountstephens, J.,\nand Teo, J.\n(2020).\nEeg-based emotion": "computer vision and pattern recognition, pages 7167–7176."
        },
        {
          "Suhaimi, N. S., Mountstephens, J.,\nand Teo, J.\n(2020).\nEeg-based emotion": "Tzeng,\nE., Hoffman,\nJ.,\nZhang, N.,\nSaenko, K.,\nand Darrell, T.\n(2014)."
        },
        {
          "Suhaimi, N. S., Mountstephens, J.,\nand Teo, J.\n(2020).\nEeg-based emotion": "Deep domain confusion: Maximizing for domain invariance.\narXiv preprint"
        },
        {
          "Suhaimi, N. S., Mountstephens, J.,\nand Teo, J.\n(2020).\nEeg-based emotion": "arXiv:1412.3474."
        },
        {
          "Suhaimi, N. S., Mountstephens, J.,\nand Teo, J.\n(2020).\nEeg-based emotion": "Vapnik, V. (2006). 24 transductive inference and semi-supervised learning."
        },
        {
          "Suhaimi, N. S., Mountstephens, J.,\nand Teo, J.\n(2020).\nEeg-based emotion": "Vidaurre, C., Krämer, N., Blankertz, B., and Schlögl, A. (2009). Time domain"
        },
        {
          "Suhaimi, N. S., Mountstephens, J.,\nand Teo, J.\n(2020).\nEeg-based emotion": "parameters as a feature for eeg-based brain–computer interfaces. Neural Net-"
        },
        {
          "Suhaimi, N. S., Mountstephens, J.,\nand Teo, J.\n(2020).\nEeg-based emotion": "works, 22(9):1313–1319."
        },
        {
          "Suhaimi, N. S., Mountstephens, J.,\nand Teo, J.\n(2020).\nEeg-based emotion": "Wang, F., Wu, S., Zhang, W., Xu, Z., Zhang, Y., Wu, C., and Coleman, S."
        },
        {
          "Suhaimi, N. S., Mountstephens, J.,\nand Teo, J.\n(2020).\nEeg-based emotion": "(2020). Emotion recognition with convolutional neural network and eeg-based"
        },
        {
          "Suhaimi, N. S., Mountstephens, J.,\nand Teo, J.\n(2020).\nEeg-based emotion": "efdms. Neuropsychologia, 146:107506."
        },
        {
          "Suhaimi, N. S., Mountstephens, J.,\nand Teo, J.\n(2020).\nEeg-based emotion": "Wang, F., Zhang, W., Xu, Z., Ping, J., and Chu, H. (2021a). A deep multi-source"
        },
        {
          "Suhaimi, N. S., Mountstephens, J.,\nand Teo, J.\n(2020).\nEeg-based emotion": "adaptation transfer network for cross-subject electroencephalogram emotion"
        },
        {
          "Suhaimi, N. S., Mountstephens, J.,\nand Teo, J.\n(2020).\nEeg-based emotion": "recognition. Neural Computing and Applications, pages 1–13."
        },
        {
          "Suhaimi, N. S., Mountstephens, J.,\nand Teo, J.\n(2020).\nEeg-based emotion": "Wang, X.-W., Nie, D., and Lu, B.-L. (2011). Eeg-based emotion recognition us-"
        },
        {
          "Suhaimi, N. S., Mountstephens, J.,\nand Teo, J.\n(2020).\nEeg-based emotion": "ing frequency domain features and support vector machines.\nIn International"
        },
        {
          "Suhaimi, N. S., Mountstephens, J.,\nand Teo, J.\n(2020).\nEeg-based emotion": "conference on neural\ninformation processing, pages 734–743. Springer."
        }
      ],
      "page": 43
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Wang, Y., Liu, J., Ruan, Q., Wang, S., and Wang, C. (2021b). Cross-subject": "eeg emotion classification based on few-label adversarial domain adaption."
        },
        {
          "Wang, Y., Liu, J., Ruan, Q., Wang, S., and Wang, C. (2021b). Cross-subject": "Expert Systems with Applications, 185:115581."
        },
        {
          "Wang, Y., Liu, J., Ruan, Q., Wang, S., and Wang, C. (2021b). Cross-subject": "Wang, Y., Qiu, S., Ma, X., and He, H. (2021c). A prototype-based spd matrix"
        },
        {
          "Wang, Y., Liu, J., Ruan, Q., Wang, S., and Wang, C. (2021b). Cross-subject": "network for domain adaptation eeg emotion recognition. Pattern Recognition,"
        },
        {
          "Wang, Y., Liu, J., Ruan, Q., Wang, S., and Wang, C. (2021b). Cross-subject": "110:107626."
        },
        {
          "Wang, Y., Liu, J., Ruan, Q., Wang, S., and Wang, C. (2021b). Cross-subject": "Westermann, R., Spies, K., Stahl, G., and Hesse, F. W. (1996). Relative effec-"
        },
        {
          "Wang, Y., Liu, J., Ruan, Q., Wang, S., and Wang, C. (2021b). Cross-subject": "tiveness and validity of mood induction procedures: A meta-analysis. Euro-"
        },
        {
          "Wang, Y., Liu, J., Ruan, Q., Wang, S., and Wang, C. (2021b). Cross-subject": "pean Journal of social psychology, 26(4):557–580."
        },
        {
          "Wang, Y., Liu, J., Ruan, Q., Wang, S., and Wang, C. (2021b). Cross-subject": "Wright, J., Ganesh, A., Rao, S., Peng, Y., and Ma, Y. (2009). Robust princi-"
        },
        {
          "Wang, Y., Liu, J., Ruan, Q., Wang, S., and Wang, C. (2021b). Cross-subject": "pal component analysis: Exact\nrecovery of corrupted low-rank matrices via"
        },
        {
          "Wang, Y., Liu, J., Ruan, Q., Wang, S., and Wang, C. (2021b). Cross-subject": "convex optimization. Advances in neural\ninformation processing systems, 22."
        },
        {
          "Wang, Y., Liu, J., Ruan, Q., Wang, S., and Wang, C. (2021b). Cross-subject": "Wu, D., Xu, Y., and Lu, B.-L. (2020). Transfer learning for eeg-based brain–"
        },
        {
          "Wang, Y., Liu, J., Ruan, Q., Wang, S., and Wang, C. (2021b). Cross-subject": "computer interfaces: A review of progress made since 2016.\nIEEE Transac-"
        },
        {
          "Wang, Y., Liu, J., Ruan, Q., Wang, S., and Wang, C. (2021b). Cross-subject": "tions on Cognitive and Developmental Systems, 14(1):4–19."
        },
        {
          "Wang, Y., Liu, J., Ruan, Q., Wang, S., and Wang, C. (2021b). Cross-subject": "Xue, B., Lv, Z., and Xue, J.\n(2020).\nFeature\ntransfer\nlearning in eeg-based"
        },
        {
          "Wang, Y., Liu, J., Ruan, Q., Wang, S., and Wang, C. (2021b). Cross-subject": "emotion recognition.\nIn 2020 Chinese Automation Congress\n(CAC), pages"
        },
        {
          "Wang, Y., Liu, J., Ruan, Q., Wang, S., and Wang, C. (2021b). Cross-subject": "3608–3611. IEEE."
        },
        {
          "Wang, Y., Liu, J., Ruan, Q., Wang, S., and Wang, C. (2021b). Cross-subject": "Yang, F., Zhao, X., Jiang, W., Gao, P., and Liu, G. (2019). Multi-method fusion"
        },
        {
          "Wang, Y., Liu, J., Ruan, Q., Wang, S., and Wang, C. (2021b). Cross-subject": "of cross-subject emotion recognition based on high-dimensional eeg features."
        },
        {
          "Wang, Y., Liu, J., Ruan, Q., Wang, S., and Wang, C. (2021b). Cross-subject": "Frontiers in computational neuroscience, 13:53."
        },
        {
          "Wang, Y., Liu, J., Ruan, Q., Wang, S., and Wang, C. (2021b). Cross-subject": "Yang, K., Bao, G., Zeng, Y., Tong, L., Shu, J., and Yan, B. (2020).\nImproving"
        },
        {
          "Wang, Y., Liu, J., Ruan, Q., Wang, S., and Wang, C. (2021b). Cross-subject": "session-to-session transfer performance of emotion recognition using adaptive"
        },
        {
          "Wang, Y., Liu, J., Ruan, Q., Wang, S., and Wang, C. (2021b). Cross-subject": "support vector machine.\nIn Journal of Physics: Conference Series, volume"
        },
        {
          "Wang, Y., Liu, J., Ruan, Q., Wang, S., and Wang, C. (2021b). Cross-subject": "1601, page 042028. IOP Publishing."
        },
        {
          "Wang, Y., Liu, J., Ruan, Q., Wang, S., and Wang, C. (2021b). Cross-subject": "Ye, Y., Zhu, X., Li, Y., Pan, T., and He, W. (2021). Cross-subject eeg-based"
        },
        {
          "Wang, Y., Liu, J., Ruan, Q., Wang, S., and Wang, C. (2021b). Cross-subject": "emotion recognition using adversarial domain adaption with attention mecha-"
        },
        {
          "Wang, Y., Liu, J., Ruan, Q., Wang, S., and Wang, C. (2021b). Cross-subject": "nism. In 2021 43rd Annual International Conference of the IEEE Engineering"
        },
        {
          "Wang, Y., Liu, J., Ruan, Q., Wang, S., and Wang, C. (2021b). Cross-subject": "in Medicine & Biology Society (EMBC), pages 1140–1144. IEEE."
        },
        {
          "Wang, Y., Liu, J., Ruan, Q., Wang, S., and Wang, C. (2021b). Cross-subject": "Yin, Z., Wang, Y., Liu, L., Zhang, W., and Zhang, J.\n(2017). Cross-subject"
        },
        {
          "Wang, Y., Liu, J., Ruan, Q., Wang, S., and Wang, C. (2021b). Cross-subject": "eeg feature selection for emotion recognition using transfer recursive feature"
        },
        {
          "Wang, Y., Liu, J., Ruan, Q., Wang, S., and Wang, C. (2021b). Cross-subject": "elimination. Frontiers in neurorobotics, 11:19."
        },
        {
          "Wang, Y., Liu, J., Ruan, Q., Wang, S., and Wang, C. (2021b). Cross-subject": "Yuen, C. T., San San, W., Seong, T. C., and Rizon, M. (2009). Classification of"
        },
        {
          "Wang, Y., Liu, J., Ruan, Q., Wang, S., and Wang, C. (2021b). Cross-subject": "human emotions from eeg signals using statistical features and neural network."
        },
        {
          "Wang, Y., Liu, J., Ruan, Q., Wang, S., and Wang, C. (2021b). Cross-subject": "International Journal of Integrated Engineering, 1(3)."
        },
        {
          "Wang, Y., Liu, J., Ruan, Q., Wang, S., and Wang, C. (2021b). Cross-subject": "Zhang, J., Chen, M., Zhao, S., Hu, S., Shi, Z., and Cao, Y. (2016). Relieff-based"
        },
        {
          "Wang, Y., Liu, J., Ruan, Q., Wang, S., and Wang, C. (2021b). Cross-subject": "eeg sensor selection methods for emotion recognition. Sensors, 16(10):1558."
        }
      ],
      "page": 44
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Zhang, W., Wang, F., Jiang, Y., Xu, Z., Wu, S., and Zhang, Y. (2019a). Cross-": "subject eeg-based emotion recognition with deep domain confusion.\nIn Inter-"
        },
        {
          "Zhang, W., Wang, F., Jiang, Y., Xu, Z., Wu, S., and Zhang, Y. (2019a). Cross-": "national conference on intelligent\nrobotics and applications, pages 558–570."
        },
        {
          "Zhang, W., Wang, F., Jiang, Y., Xu, Z., Wu, S., and Zhang, Y. (2019a). Cross-": "Springer."
        },
        {
          "Zhang, W., Wang, F., Jiang, Y., Xu, Z., Wu, S., and Zhang, Y. (2019a). Cross-": "Zhang, W. and Yin, Z.\n(2020).\nEeg feature selection for emotion recognition"
        },
        {
          "Zhang, W., Wang, F., Jiang, Y., Xu, Z., Wu, S., and Zhang, Y. (2019a). Cross-": "based on cross-subject\nrecursive feature elimination.\nIn 2020 39th Chinese"
        },
        {
          "Zhang, W., Wang, F., Jiang, Y., Xu, Z., Wu, S., and Zhang, Y. (2019a). Cross-": "Control Conference (CCC), pages 6256–6261. IEEE."
        },
        {
          "Zhang, W., Wang, F., Jiang, Y., Xu, Z., Wu, S., and Zhang, Y. (2019a). Cross-": "Zhang, X., Liang, W., Ding, T., Pan, J., Shen, J., Huang, X., and Gao, J."
        },
        {
          "Zhang, W., Wang, F., Jiang, Y., Xu, Z., Wu, S., and Zhang, Y. (2019a). Cross-": "(2019b). Individual similarity guided transfer modeling for eeg-based emotion"
        },
        {
          "Zhang, W., Wang, F., Jiang, Y., Xu, Z., Wu, S., and Zhang, Y. (2019a). Cross-": "recognition.\nIn 2019 IEEE International Conference on Bioinformatics and"
        },
        {
          "Zhang, W., Wang, F., Jiang, Y., Xu, Z., Wu, S., and Zhang, Y. (2019a). Cross-": "Biomedicine (BIBM), pages 1156–1161. IEEE."
        },
        {
          "Zhang, W., Wang, F., Jiang, Y., Xu, Z., Wu, S., and Zhang, Y. (2019a). Cross-": "Zhang, X.-Y.\nand Liu, C.-L.\n(2012). Writer\nadaptation with style\ntransfer"
        },
        {
          "Zhang, W., Wang, F., Jiang, Y., Xu, Z., Wu, S., and Zhang, Y. (2019a). Cross-": "mapping.\nIEEE transactions on pattern analysis and machine intelligence,"
        },
        {
          "Zhang, W., Wang, F., Jiang, Y., Xu, Z., Wu, S., and Zhang, Y. (2019a). Cross-": "35(7):1773–1787."
        },
        {
          "Zhang, W., Wang, F., Jiang, Y., Xu, Z., Wu, S., and Zhang, Y. (2019a). Cross-": "Zhang, Z.\n(2019).\nSpectral and time-frequency analysis.\nIn EEG Signal Pro-"
        },
        {
          "Zhang, W., Wang, F., Jiang, Y., Xu, Z., Wu, S., and Zhang, Y. (2019a). Cross-": "cessing and feature extraction, pages 89–116. Springer."
        },
        {
          "Zhang, W., Wang, F., Jiang, Y., Xu, Z., Wu, S., and Zhang, Y. (2019a). Cross-": "Zhao, G., Zhang, Y., and Ge, Y. (2018). Frontal eeg asymmetry and middle line"
        },
        {
          "Zhang, W., Wang, F., Jiang, Y., Xu, Z., Wu, S., and Zhang, Y. (2019a). Cross-": "power difference in discrete emotions. Frontiers\nin behavioral neuroscience,"
        },
        {
          "Zhang, W., Wang, F., Jiang, Y., Xu, Z., Wu, S., and Zhang, Y. (2019a). Cross-": "12:225."
        },
        {
          "Zhang, W., Wang, F., Jiang, Y., Xu, Z., Wu, S., and Zhang, Y. (2019a). Cross-": "Zhao, L.-M., Yan, X., and Lu, B.-L. (2021). Plug-and-play domain adaptation"
        },
        {
          "Zhang, W., Wang, F., Jiang, Y., Xu, Z., Wu, S., and Zhang, Y. (2019a). Cross-": "for cross-subject eeg-based emotion recognition.\nIn Proceedings of\nthe 35th"
        },
        {
          "Zhang, W., Wang, F., Jiang, Y., Xu, Z., Wu, S., and Zhang, Y. (2019a). Cross-": "AAAI Conference on Artificial Intelligence. sn."
        },
        {
          "Zhang, W., Wang, F., Jiang, Y., Xu, Z., Wu, S., and Zhang, Y. (2019a). Cross-": "Zheng, W.-L. and Lu, B.-L. (2015).\nInvestigating critical\nfrequency bands and"
        },
        {
          "Zhang, W., Wang, F., Jiang, Y., Xu, Z., Wu, S., and Zhang, Y. (2019a). Cross-": "channels for eeg-based emotion recognition with deep neural networks.\nIEEE"
        },
        {
          "Zhang, W., Wang, F., Jiang, Y., Xu, Z., Wu, S., and Zhang, Y. (2019a). Cross-": "Transactions on autonomous mental development, 7(3):162–175."
        },
        {
          "Zhang, W., Wang, F., Jiang, Y., Xu, Z., Wu, S., and Zhang, Y. (2019a). Cross-": "Zheng, W.-L. and Lu, B.-L.\n(2016).\nPersonalizing eeg-based affective models"
        },
        {
          "Zhang, W., Wang, F., Jiang, Y., Xu, Z., Wu, S., and Zhang, Y. (2019a). Cross-": "with transfer learning.\nIn Proceedings of\nthe twenty-fifth international\njoint"
        },
        {
          "Zhang, W., Wang, F., Jiang, Y., Xu, Z., Wu, S., and Zhang, Y. (2019a). Cross-": "conference on artificial\nintelligence, pages 2732–2738."
        },
        {
          "Zhang, W., Wang, F., Jiang, Y., Xu, Z., Wu, S., and Zhang, Y. (2019a). Cross-": "Zheng, W.-L., Zhang, Y.-Q., Zhu, J.-Y., and Lu, B.-L. (2015). Transfer compo-"
        },
        {
          "Zhang, W., Wang, F., Jiang, Y., Xu, Z., Wu, S., and Zhang, Y. (2019a). Cross-": "nents between subjects for eeg-based emotion recognition.\nIn 2015 interna-"
        },
        {
          "Zhang, W., Wang, F., Jiang, Y., Xu, Z., Wu, S., and Zhang, Y. (2019a). Cross-": "tional conference on affective computing and intelligent\ninteraction (ACII),"
        },
        {
          "Zhang, W., Wang, F., Jiang, Y., Xu, Z., Wu, S., and Zhang, Y. (2019a). Cross-": "pages 917–922. IEEE."
        },
        {
          "Zhang, W., Wang, F., Jiang, Y., Xu, Z., Wu, S., and Zhang, Y. (2019a). Cross-": "Zhong, P., Wang, D.,\nand Miao, C.\n(2020).\nEeg-based emotion recognition"
        },
        {
          "Zhang, W., Wang, F., Jiang, Y., Xu, Z., Wu, S., and Zhang, Y. (2019a). Cross-": "using regularized graph neural networks.\nIEEE Transactions on Affective"
        },
        {
          "Zhang, W., Wang, F., Jiang, Y., Xu, Z., Wu, S., and Zhang, Y. (2019a). Cross-": "Computing."
        },
        {
          "Zhang, W., Wang, F., Jiang, Y., Xu, Z., Wu, S., and Zhang, Y. (2019a). Cross-": "Zhou,\nJ., Chu, S., Li, X., Xiao, F.,\nand Sun, L.\n(2020a).\nAn eeg emotion"
        },
        {
          "Zhang, W., Wang, F., Jiang, Y., Xu, Z., Wu, S., and Zhang, Y. (2019a). Cross-": "recognition method based on transfer\nlearning and echo state network for"
        },
        {
          "Zhang, W., Wang, F., Jiang, Y., Xu, Z., Wu, S., and Zhang, Y. (2019a). Cross-": "hilcps. Microprocessors and Microsystems, page 103381."
        }
      ],
      "page": 45
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Zhou, J., Cui, G., Hu, S., Zhang, Z., Yang, C., Liu, Z., Wang, L., Li, C., and Sun,": "M. (2020b). Graph neural networks: A review of methods and applications."
        },
        {
          "Zhou, J., Cui, G., Hu, S., Zhang, Z., Yang, C., Liu, Z., Wang, L., Li, C., and Sun,": "AI Open, 1:57–81."
        },
        {
          "Zhou, J., Cui, G., Hu, S., Zhang, Z., Yang, C., Liu, Z., Wang, L., Li, C., and Sun,": "Zhu, X., Thung, K.-H., Adeli, E., Zhang, Y., and Shen, D. (2017). Maximum"
        },
        {
          "Zhou, J., Cui, G., Hu, S., Zhang, Z., Yang, C., Liu, Z., Wang, L., Li, C., and Sun,": "mean discrepancy based multiple kernel learning for incomplete multimodality"
        },
        {
          "Zhou, J., Cui, G., Hu, S., Zhang, Z., Yang, C., Liu, Z., Wang, L., Li, C., and Sun,": "neuroimaging data. In International Conference on Medical Image Computing"
        },
        {
          "Zhou, J., Cui, G., Hu, S., Zhang, Z., Yang, C., Liu, Z., Wang, L., Li, C., and Sun,": "and Computer-Assisted Intervention, pages 72–80. Springer."
        }
      ],
      "page": 46
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "American clinical neurophysiology society guideline 2: guidelines for standard electrode position nomenclature",
      "authors": [
        "J Acharya",
        "A Hani",
        "J Cheek",
        "P Thirumala",
        "T Tsuchida"
      ],
      "year": "2016",
      "venue": "The Neurodiagnostic Journal"
    },
    {
      "citation_id": "2",
      "title": "Domain-adversarial neural networks",
      "authors": [
        "H Ajakan",
        "P Germain",
        "H Larochelle",
        "F Laviolette",
        "M Marchand"
      ],
      "year": "2014",
      "venue": "Domain-adversarial neural networks",
      "arxiv": "arXiv:1412.4446"
    },
    {
      "citation_id": "3",
      "title": "Emotions recognition using eeg signals: A survey",
      "authors": [
        "S Alarcao",
        "M Fonseca"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "4",
      "title": "A survey on eeg-based solutions for emotion recognition with a low number of channels",
      "authors": [
        "A Apicella",
        "P Arpaia",
        "F Isgrò",
        "G Mastrati",
        "N Moccaldi"
      ],
      "year": "2022",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "5",
      "title": "Combining inter-subject modeling with a subject-based data transformation to improve affect recognition from eeg signals",
      "authors": [
        "M Arevalillo-Herráez",
        "M Cobos",
        "S Roger",
        "M García-Pineda"
      ],
      "year": "2019",
      "venue": "Sensors"
    },
    {
      "citation_id": "6",
      "title": "Two-level domain adaptation neural network for eeg-based emotion recognition",
      "authors": [
        "G Bao",
        "N Zhuang",
        "L Tong",
        "B Yan",
        "J Shu",
        "L Wang",
        "Y Zeng",
        "Z Shen"
      ],
      "year": "2020",
      "venue": "Frontiers in Human Neuroscience"
    },
    {
      "citation_id": "7",
      "title": "Innovative tools for radar signal processing based on cartan's geometry of spd matrices & information geometry",
      "authors": [
        "F Barbaresco"
      ],
      "year": "2008",
      "venue": "2008 IEEE Radar Conference"
    },
    {
      "citation_id": "8",
      "title": "Learning representations from eeg with deep recurrent-convolutional neural networks",
      "authors": [
        "P Bashivan",
        "I Rish",
        "M Yeasin",
        "N Codella"
      ],
      "year": "2015",
      "venue": "Learning representations from eeg with deep recurrent-convolutional neural networks",
      "arxiv": "arXiv:1511.06448"
    },
    {
      "citation_id": "9",
      "title": "Using neurophysiological signals that reflect cognitive or affective state: six recommendations to avoid common pitfalls",
      "authors": [
        "A.-M Brouwer",
        "T Zander",
        "J Van Erp",
        "J Korteling",
        "A Bronkhorst"
      ],
      "year": "2015",
      "venue": "Frontiers in neuroscience"
    },
    {
      "citation_id": "10",
      "title": "Multiple transferable recursive feature elimination technique for emotion recognition based on eeg signals",
      "authors": [
        "J Cai",
        "W Chen",
        "Z Yin"
      ],
      "year": "2019",
      "venue": "Symmetry"
    },
    {
      "citation_id": "11",
      "title": "Affective computing and sentiment analysis",
      "authors": [
        "E Cambria",
        "D Das",
        "S Bandyopadhyay",
        "A Feraco"
      ],
      "year": "2017",
      "venue": "A practical guide to sentiment analysis"
    },
    {
      "citation_id": "12",
      "title": "Application of a novel measure of eeg nonstationarity as 'shannon-entropy of the peak frequency shifting'for detecting residual abnormalities in concussed individuals",
      "authors": [
        "C Cao",
        "S Slobounov"
      ],
      "year": "2011",
      "venue": "Clinical Neurophysiology"
    },
    {
      "citation_id": "13",
      "title": "Multi-source and multi-representation adaptation for cross-domain electroencephalography emotion recognition",
      "authors": [
        "J Cao",
        "X He",
        "C Yang",
        "S Chen",
        "Z Li",
        "Z Wang"
      ],
      "year": "2021",
      "venue": "Frontiers in Psychology"
    },
    {
      "citation_id": "14",
      "title": "Wearable eeg and beyond",
      "authors": [
        "A Casson"
      ],
      "year": "2019",
      "venue": "Biomedical engineering letters"
    },
    {
      "citation_id": "15",
      "title": "A fast, efficient domain adaptation technique for cross-domain electroencephalography (eeg)-based emotion recognition",
      "authors": [
        "X Chai",
        "Q Wang",
        "Y Zhao",
        "Y Li",
        "D Liu",
        "X Liu",
        "O Bai"
      ],
      "year": "2017",
      "venue": "Sensors"
    },
    {
      "citation_id": "16",
      "title": "Unsupervised domain adaptation techniques based on auto-encoder for non-stationary eegbased emotion recognition",
      "authors": [
        "X Chai",
        "Q Wang",
        "Y Zhao",
        "X Liu",
        "O Bai",
        "Y Li"
      ],
      "year": "2016",
      "venue": "Computers in biology and medicine"
    },
    {
      "citation_id": "17",
      "title": "Multisubject subspace alignment for non-stationary eeg-based emotion recognition",
      "authors": [
        "X Chai",
        "Q Wang",
        "Y Zhao",
        "X Liu",
        "D Liu",
        "O Bai"
      ],
      "year": "2018",
      "venue": "Technology and Health Care"
    },
    {
      "citation_id": "18",
      "title": "Ms-mda: Multisource marginal distribution adaptation for cross-subject and cross-session eeg emotion recognition",
      "authors": [
        "H Chen",
        "M Jin",
        "Z Li",
        "C Fan",
        "J Li",
        "H He"
      ],
      "year": "2021",
      "venue": "Frontiers in Neuroscience"
    },
    {
      "citation_id": "19",
      "title": "Meernet: Multi-source eeg-based emotion recognition network for generalization across subjects and sessions",
      "authors": [
        "H Chen",
        "Z Li",
        "M Jin",
        "J Li"
      ],
      "year": "2021",
      "venue": "2021 43rd Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC)"
    },
    {
      "citation_id": "20",
      "title": "Personalzscore: Eliminating individual difference for eeg-based cross-subject emotion recognition",
      "authors": [
        "H Chen",
        "S Sun",
        "J Li",
        "R Yu",
        "N Li",
        "X Li",
        "B Hu"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "21",
      "title": "Learning with progressive transductive support vector machine",
      "authors": [
        "Y Chen",
        "G Wang",
        "S Dong"
      ],
      "year": "2003",
      "venue": "Pattern Recognition Letters"
    },
    {
      "citation_id": "22",
      "title": "Investigating the use of pretrained convolutional neural network on cross-subject and cross-dataset eeg emotion recognition",
      "authors": [
        "Y Cimtay",
        "E Ekmekcioglu"
      ],
      "year": "2020",
      "venue": "Sensors"
    },
    {
      "citation_id": "23",
      "title": "The state and trait nature of frontal eeg asymmetry in emotion",
      "authors": [
        "J Coan",
        "J Allen"
      ],
      "year": "2003",
      "venue": "The state and trait nature of frontal eeg asymmetry in emotion"
    },
    {
      "citation_id": "24",
      "title": "An automatic pre-processing pipeline for eeg analysis (app) based on robust statistics",
      "authors": [
        "J Da Cruz",
        "V Chicherov",
        "M Herzog",
        "P Figueiredo"
      ],
      "year": "2018",
      "venue": "Clinical Neurophysiology"
    },
    {
      "citation_id": "25",
      "title": "Boosting for transfer learning",
      "authors": [
        "W Dai",
        "Q Yang",
        "G.-R Xue",
        "Y Yu"
      ],
      "year": "2007",
      "venue": "Proceedings of the 24th International Conference on Machine Learning, ICML '07"
    },
    {
      "citation_id": "26",
      "title": "What does clean eeg look like?",
      "authors": [
        "I Daly",
        "F Pichiorri",
        "J Faller",
        "V Kaiser",
        "A Kreilinger",
        "R Scherer",
        "G Müller-Putz"
      ],
      "year": "2012",
      "venue": "2012 Annual International Conference of the IEEE Engineering in Medicine and Biology Society"
    },
    {
      "citation_id": "27",
      "title": "The geneva affective picture database (gaped): a new 730-picture database focusing on valence and normative significance",
      "authors": [
        "E Dan-Glauser",
        "K Scherer"
      ],
      "year": "2011",
      "venue": "Behavior research methods"
    },
    {
      "citation_id": "28",
      "title": "Hemispheric asymmetry and emotion",
      "authors": [
        "R Davidson"
      ],
      "year": "1984",
      "venue": "Approaches to emotion"
    },
    {
      "citation_id": "29",
      "title": "Brain lateralization of emotional processing: historical roots and a future incorporating \"dominance",
      "authors": [
        "H Demaree",
        "D Everhart",
        "E Youngstrom",
        "D Harrison"
      ],
      "year": "2005",
      "venue": "Behavioral and cognitive neuroscience reviews"
    },
    {
      "citation_id": "30",
      "title": "Eeg emotion enhancement using task-specific domain adversarial neural network",
      "authors": [
        "K.-M Ding",
        "T Kimura",
        "K.-I Fukui",
        "M Numao"
      ],
      "year": "2021",
      "venue": "2021 International Joint Conference on Neural Networks (IJCNN)"
    },
    {
      "citation_id": "31",
      "title": "An efficient lstm network for emotion recognition from multichannel eeg signals",
      "authors": [
        "X Du",
        "C Ma",
        "G Zhang",
        "J Li",
        "Y.-K Lai",
        "G Zhao",
        "X Deng",
        "Y.-J Liu",
        "H Wang"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "32",
      "title": "Basic emotions. Handbook of cognition and emotion",
      "authors": [
        "P Ekman"
      ],
      "year": "1999",
      "venue": "Basic emotions. Handbook of cognition and emotion"
    },
    {
      "citation_id": "33",
      "title": "A wavelet-based approach to emotion classification using eda signals",
      "authors": [
        "H Feng",
        "H Golshan",
        "M Mahoor"
      ],
      "year": "2018",
      "venue": "Expert Systems with Applications"
    },
    {
      "citation_id": "34",
      "title": "Crosssubject eeg-based emotion recognition through neural networks with stratified normalization",
      "authors": [
        "J Fernandez",
        "N Guttenberg",
        "O Witkowski",
        "A Pasquali"
      ],
      "year": "2021",
      "venue": "Frontiers in neuroscience"
    },
    {
      "citation_id": "35",
      "title": "Unsupervised visual domain adaptation using subspace alignment",
      "authors": [
        "B Fernando",
        "A Habrard",
        "M Sebban",
        "T Tuytelaars"
      ],
      "year": "2013",
      "venue": "Proceedings of the IEEE international conference on computer vision"
    },
    {
      "citation_id": "36",
      "title": "Statistical methods for research workers",
      "authors": [
        "R Fisher"
      ],
      "year": "1992",
      "venue": "Breakthroughs in statistics"
    },
    {
      "citation_id": "37",
      "title": "Emotion recognition with domain adaptation based on few-shot eeg learning",
      "authors": [
        "S Furukawa",
        "T Sakuma",
        "S Kato"
      ],
      "year": "2021",
      "venue": "2021 IEEE 10th Global Conference on Consumer Electronics (GCCE)"
    },
    {
      "citation_id": "38",
      "title": "Unsupervised domain adaptation by backpropagation",
      "authors": [
        "Y Ganin",
        "V Lempitsky"
      ],
      "year": "2015",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "39",
      "title": "Domain-adversarial training of neural networks",
      "authors": [
        "Y Ganin",
        "E Ustinova",
        "H Ajakan",
        "P Germain",
        "H Larochelle",
        "F Laviolette",
        "M Marchand",
        "V Lempitsky"
      ],
      "year": "2016",
      "venue": "The journal of machine learning research"
    },
    {
      "citation_id": "40",
      "title": "Time domain feature extraction and classification of eeg data for brain computer interface",
      "authors": [
        "P Geethanjali",
        "Y Mohan",
        "J Sen"
      ],
      "year": "2012",
      "venue": "2012 9th International Conference on Fuzzy Systems and Knowledge Discovery"
    },
    {
      "citation_id": "41",
      "title": "A robust data-driven approach identifies four personality types across four large data sets",
      "authors": [
        "M Gerlach",
        "B Farb",
        "W Revelle",
        "L Nunes Amaral"
      ],
      "year": "2018",
      "venue": "Nature human behaviour"
    },
    {
      "citation_id": "42",
      "title": "Scatter component analysis: A unified framework for domain adaptation and domain generalization",
      "authors": [
        "M Ghifary",
        "D Balduzzi",
        "W Kleijn",
        "M Zhang"
      ],
      "year": "2016",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "43",
      "title": "Verification, analytical validation, and clinical validation (v3): the foundation of determining fit-for-purpose for biometric monitoring technologies (biomets)",
      "authors": [
        "J Goldsack",
        "A Coravos",
        "J Bakker",
        "B Bent",
        "A Dowling",
        "C Fitzer-Attas",
        "A Godfrey",
        "J Godino",
        "N Gujar",
        "E Izmailova"
      ],
      "year": "2020",
      "venue": "Verification, analytical validation, and clinical validation (v3): the foundation of determining fit-for-purpose for biometric monitoring technologies (biomets)"
    },
    {
      "citation_id": "44",
      "title": "Geodesic flow kernel for unsupervised domain adaptation",
      "authors": [
        "B Gong",
        "Y Shi",
        "F Sha",
        "K Grauman"
      ],
      "year": "2012",
      "venue": "2012 IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "45",
      "title": "Eeg-based emotion detection using unsupervised transfer learning",
      "authors": [
        "H Gonzalez",
        "J Yoo",
        "I Elfadel"
      ],
      "year": "2019",
      "venue": "2019 41st Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)"
    },
    {
      "citation_id": "46",
      "title": "A kernel method for the two-sample-problem",
      "authors": [
        "A Gretton",
        "K Borgwardt",
        "M Rasch",
        "B Schölkopf",
        "A Smola"
      ],
      "year": "2006",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "47",
      "title": "Multi-source domain transfer discriminative dictionary learning modeling for electroencephalogram-based emotion recognition",
      "authors": [
        "X Gu",
        "W Cai",
        "M Gao",
        "Y Jiang",
        "X Ning",
        "P Qian"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Computational Social Systems"
    },
    {
      "citation_id": "48",
      "title": "Cross-subject emotion recognition using flexible analytic wavelet transform from eeg signals",
      "authors": [
        "V Gupta",
        "M Chopda",
        "R Pachori"
      ],
      "year": "2018",
      "venue": "IEEE Sensors Journal"
    },
    {
      "citation_id": "49",
      "title": "Learning subject-generalized topographical eeg embeddings using deep variational autoencoders and domain-adversarial regularization",
      "authors": [
        "J Hagad",
        "T Kimura",
        "K.-I Fukui",
        "M Numao"
      ],
      "year": "2021",
      "venue": "Sensors"
    },
    {
      "citation_id": "50",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "K He",
        "X Zhang",
        "S Ren",
        "J Sun"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "51",
      "title": "An adversarial discriminative temporal convolutional network for eeg-based cross-domain emotion recognition",
      "authors": [
        "Z He",
        "Y Zhong",
        "J Pan"
      ],
      "year": "2022",
      "venue": "Computers in biology and medicine"
    },
    {
      "citation_id": "52",
      "title": "Cross-day eegbased emotion recognition using transfer component analysis",
      "authors": [
        "Z He",
        "N Zhuang",
        "G Bao",
        "Y Zeng",
        "B Yan"
      ],
      "year": "2022",
      "venue": "Electronics"
    },
    {
      "citation_id": "53",
      "title": "A machine learning emotion detection platform to support affective well being",
      "authors": [
        "M Healy",
        "R Donovan",
        "P Walsh",
        "H Zheng"
      ],
      "year": "2018",
      "venue": "2018 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)"
    },
    {
      "citation_id": "54",
      "title": "Detecting epilepsy in eeg signals using time, frequency and time-frequency domain features",
      "authors": [
        "D Hernández",
        "L Trujillo",
        "O Villanueva",
        "O Romo-Fewell"
      ],
      "year": "2018",
      "venue": "Computer science and engineering-theory and applications"
    },
    {
      "citation_id": "55",
      "title": "Manifold feature fusion with dynamical feature selection for cross-subject emotion recognition",
      "authors": [
        "Y Hua",
        "X Zhong",
        "B Zhang",
        "Z Yin",
        "J Zhang"
      ],
      "year": "2021",
      "venue": "Brain Sciences"
    },
    {
      "citation_id": "56",
      "title": "Densely connected convolutional networks",
      "authors": [
        "G Huang",
        "Z Liu",
        "L Van Der Maaten",
        "K Weinberger"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "57",
      "title": "Learning cnn features from de features for eeg-based emotion recognition",
      "authors": [
        "S Hwang",
        "K Hong",
        "G Son",
        "H Byun"
      ],
      "year": "2020",
      "venue": "Pattern Analysis and Applications"
    },
    {
      "citation_id": "58",
      "title": "Subject-independent eeg-based emotion recognition using adversarial learning",
      "authors": [
        "S Hwang",
        "M Ki",
        "K Hong",
        "H Byun"
      ],
      "year": "2020",
      "venue": "2020 8th International Winter Conference on Brain-Computer Interface (BCI)"
    },
    {
      "citation_id": "59",
      "title": "Computational eeg analysis",
      "authors": [
        "C.-H Im"
      ],
      "year": "2018",
      "venue": "Computational eeg analysis"
    },
    {
      "citation_id": "60",
      "title": "A new segmentation method of electroencephalograms by use of akaike's information criterion",
      "authors": [
        "T Inouye",
        "S Toi",
        "Y Matsumoto"
      ],
      "year": "1995",
      "venue": "Cognitive brain research"
    },
    {
      "citation_id": "61",
      "title": "Feature extraction and selection for emotion recognition from eeg",
      "authors": [
        "R Jenke",
        "A Peer",
        "M Buss"
      ],
      "year": "2014",
      "venue": "IEEE Transactions on Affective computing"
    },
    {
      "citation_id": "62",
      "title": "A literature survey on domain adaptation of statistical classifiers",
      "authors": [
        "J Jiang"
      ],
      "year": "2008",
      "venue": "A literature survey on domain adaptation of statistical classifiers"
    },
    {
      "citation_id": "63",
      "title": "Cross-subject emotion recognition with a decision tree classifier based on sequential backward selection",
      "authors": [
        "W Jiang",
        "G Liu",
        "X Zhao",
        "F Yang"
      ],
      "year": "2019",
      "venue": "2019 11th International Conference on Intelligent Human-Machine Systems and Cybernetics (IHMSC)"
    },
    {
      "citation_id": "64",
      "title": "Removal of artifacts from eeg signals: a review",
      "authors": [
        "X Jiang",
        "G.-B Bian",
        "Z Tian"
      ],
      "year": "2019",
      "venue": "Sensors"
    },
    {
      "citation_id": "65",
      "title": "Eeg-based emotion recognition using domain adaptation network",
      "authors": [
        "Y.-M Jin",
        "Y.-D Luo",
        "W.-L Zheng",
        "B.-L Lu"
      ],
      "year": "2017",
      "venue": "2017 international conference on orange technologies (ICOT)"
    },
    {
      "citation_id": "66",
      "title": "Transductive inference for text classification using support vector machines",
      "authors": [
        "T Joachims"
      ],
      "year": "1999",
      "venue": "Icml"
    },
    {
      "citation_id": "67",
      "title": "Dreamer: A database for emotion recognition through eeg and ecg signals from wireless low-cost off-the-shelf devices",
      "authors": [
        "S Katsigiannis",
        "N Ramzan"
      ],
      "year": "2017",
      "venue": "IEEE journal of biomedical and health informatics"
    },
    {
      "citation_id": "68",
      "title": "Face morphing attack detection in the presence of post-processed image sources using neighborhood component analysis and decision tree classifier",
      "authors": [
        "O Kenneth",
        "S Bashir",
        "O Abisoye",
        "A Mohammed"
      ],
      "year": "2020",
      "venue": "International Conference on Information and Communication Technology and Applications"
    },
    {
      "citation_id": "69",
      "title": "Procedures for performing systematic reviews",
      "authors": [
        "B Kitchenham"
      ],
      "year": "2004",
      "venue": "Procedures for performing systematic reviews"
    },
    {
      "citation_id": "70",
      "title": "Siamese neural networks for one-shot image recognition",
      "authors": [
        "G Koch",
        "R Zemel",
        "R Salakhutdinov"
      ],
      "year": "2015",
      "venue": "ICML deep learning workshop"
    },
    {
      "citation_id": "71",
      "title": "Deap: A database for emotion analysis; using physiological signals",
      "authors": [
        "S Koelstra",
        "C Muhl",
        "M Soleymani",
        "J.-S Lee",
        "A Yazdani",
        "T Ebrahimi",
        "T Pun",
        "A Nijholt",
        "I Patras"
      ],
      "year": "2011",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "72",
      "title": "Eeg correlates of different emotional states elicited during watching music videos",
      "authors": [
        "E Kroupi",
        "A Yazdani",
        "T Ebrahimi"
      ],
      "year": "2011",
      "venue": "International Conference on Affective Computing and Intelligent Interaction"
    },
    {
      "citation_id": "73",
      "title": "Cross-subject and cross-device wearable eeg emotion recognition using frontal eeg under virtual reality scenes",
      "authors": [
        "F Kuang",
        "L Shu",
        "H Hua",
        "S Wu",
        "L Zhang",
        "X Xu",
        "Y Liu",
        "M Jiang"
      ],
      "year": "2021",
      "venue": "2021 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)"
    },
    {
      "citation_id": "74",
      "title": "Introducing the open affective standardized image set (oasis)",
      "authors": [
        "B Kurdi",
        "S Lozano",
        "M Banaji"
      ],
      "year": "2017",
      "venue": "Behavior research methods"
    },
    {
      "citation_id": "75",
      "title": "Domain adaptation techniques for eeg-based emotion recognition: a comparative study on two public datasets",
      "authors": [
        "Z Lan",
        "O Sourina",
        "L Wang",
        "R Scherer",
        "G Müller-Putz"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Cognitive and Developmental Systems"
    },
    {
      "citation_id": "76",
      "title": "International affective picture system (iaps): Affective ratings of pictures and instruction manual",
      "authors": [
        "P Lang"
      ],
      "year": "2005",
      "venue": "International affective picture system (iaps): Affective ratings of pictures and instruction manual"
    },
    {
      "citation_id": "77",
      "title": "A statistically robust eeg re-referencing procedure to mitigate reference effect",
      "authors": [
        "K Lepage",
        "M Kramer",
        "C Chu"
      ],
      "year": "2014",
      "venue": "Journal of neuroscience methods"
    },
    {
      "citation_id": "78",
      "title": "Cross-subject emotion recognition using deep adaptation networks",
      "authors": [
        "H Li",
        "Y.-M Jin",
        "W.-L Zheng",
        "B.-L Lu"
      ],
      "year": "2018",
      "venue": "International conference on neural information processing"
    },
    {
      "citation_id": "79",
      "title": "Foit: Fast online instance transfer for improved eeg emotion recognition",
      "authors": [
        "J Li",
        "H Chen",
        "T Cai"
      ],
      "year": "2020",
      "venue": "2020 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)"
    },
    {
      "citation_id": "80",
      "title": "Cross-subject eeg emotion recognition with self-organized graph neural network",
      "authors": [
        "J Li",
        "S Li",
        "J Pan",
        "F Wang"
      ],
      "year": "2021",
      "venue": "Frontiers in Neuroscience"
    },
    {
      "citation_id": "81",
      "title": "Domain adaptation for eeg emotion recognition based on latent representation similarity",
      "authors": [
        "J Li",
        "S Qiu",
        "C Du",
        "Y Wang",
        "H He"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Cognitive and Developmental Systems"
    },
    {
      "citation_id": "82",
      "title": "Multisource transfer learning for cross-subject eeg emotion recognition",
      "authors": [
        "J Li",
        "S Qiu",
        "Y.-Y Shen",
        "C.-L Liu",
        "H He"
      ],
      "year": "2019",
      "venue": "IEEE transactions on cybernetics"
    },
    {
      "citation_id": "83",
      "title": "Eeg responses to emotional videos can quantitatively predict big-five personality traits",
      "authors": [
        "W Li",
        "X Hu",
        "X Long",
        "L Tang",
        "J Chen",
        "F Wang",
        "D Zhang"
      ],
      "year": "2020",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "84",
      "title": "Can emotion be transferred?-a review on transfer learning for eeg-based emotion recognition",
      "authors": [
        "W Li",
        "W Huan",
        "B Hou",
        "Y Tian",
        "Z Zhang",
        "A Song"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Cognitive and Developmental Systems"
    },
    {
      "citation_id": "85",
      "title": "Exploring eeg features in cross-subject emotion recognition",
      "authors": [
        "X Li",
        "D Song",
        "P Zhang",
        "Y Zhang",
        "Y Hou",
        "B Hu"
      ],
      "year": "2018",
      "venue": "Frontiers in neuroscience"
    },
    {
      "citation_id": "86",
      "title": "Variational autoencoder based latent factor decoding of multichannel eeg for emotion recognition",
      "authors": [
        "X Li",
        "Z Zhao",
        "D Song",
        "Y Zhang",
        "C Niu",
        "J Zhang",
        "J Huo",
        "J Li"
      ],
      "year": "2019",
      "venue": "2019 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)"
    },
    {
      "citation_id": "87",
      "title": "Latent factor decoding of multi-channel eeg for emotion recognition through autoencoder-like neural networks",
      "authors": [
        "X Li",
        "Z Zhao",
        "D Song",
        "Y Zhang",
        "J Pan",
        "L Wu",
        "J Huo",
        "C Niu",
        "D Wang"
      ],
      "year": "2020",
      "venue": "Frontiers in neuroscience"
    },
    {
      "citation_id": "88",
      "title": "A novel bi-hemispheric discrepancy model for eeg emotion recognition",
      "authors": [
        "Y Li",
        "L Wang",
        "W Zheng",
        "Y Zong",
        "L Qi",
        "Z Cui",
        "T Zhang",
        "T Song"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Cognitive and Developmental Systems"
    },
    {
      "citation_id": "89",
      "title": "Adaptive batch normalization for practical domain adaptation",
      "authors": [
        "Y Li",
        "N Wang",
        "J Shi",
        "X Hou",
        "J Liu"
      ],
      "year": "2018",
      "venue": "Pattern Recognition"
    },
    {
      "citation_id": "90",
      "title": "A novel neural network model based on cerebral hemispheric asymmetry for eeg emotion recognition",
      "authors": [
        "Y Li",
        "W Zheng",
        "Z Cui",
        "T Zhang",
        "Y Zong"
      ],
      "year": "2018",
      "venue": "IJCAI"
    },
    {
      "citation_id": "91",
      "title": "From regional to global brain: A novel hierarchical spatial-temporal neural network model for eeg emotion recognition",
      "authors": [
        "Y Li",
        "W Zheng",
        "L Wang",
        "Y Zong",
        "Z Cui"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "92",
      "title": "A bihemisphere domain adversarial neural network model for eeg emotion recognition",
      "authors": [
        "Y Li",
        "W Zheng",
        "Y Zong",
        "Z Cui",
        "T Zhang",
        "X Zhou"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "93",
      "title": "Reducing the calibration effort of eeg emotion recognition using domain adaptation with soft labels",
      "authors": [
        "Z Li",
        "H Chen",
        "M Jin",
        "J Li"
      ],
      "year": "2021",
      "venue": "2021 43rd Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC)"
    },
    {
      "citation_id": "94",
      "title": "The prisma statement for reporting systematic reviews and meta-analyses of studies that evaluate health care interventions: explanation and elaboration",
      "authors": [
        "A Liberati",
        "D Altman",
        "J Tetzlaff",
        "C Mulrow",
        "P Gøtzsche",
        "J Ioannidis",
        "M Clarke",
        "P Devereaux",
        "J Kleijnen",
        "D Moher"
      ],
      "year": "2009",
      "venue": "Journal of clinical epidemiology"
    },
    {
      "citation_id": "95",
      "title": "Constructing a personalized cross-day eeg-based emotionclassification model using transfer learning",
      "authors": [
        "Y.-P Lin"
      ],
      "year": "2019",
      "venue": "IEEE journal of biomedical and health informatics"
    },
    {
      "citation_id": "96",
      "title": "Improving cross-day eeg-based emotion classification using robust principal component analysis",
      "authors": [
        "Y.-P Lin",
        "P.-K Jao",
        "Y.-H Yang"
      ],
      "year": "2017",
      "venue": "Frontiers in computational neuroscience"
    },
    {
      "citation_id": "97",
      "title": "Eeg-based emotion recognition in music listening",
      "authors": [
        "Y.-P Lin",
        "C.-H Wang",
        "T.-P Jung",
        "T.-L Wu",
        "S.-K Jeng",
        "J.-R Duann",
        "J.-H Chen"
      ],
      "year": "2010",
      "venue": "IEEE Transactions on Biomedical Engineering"
    },
    {
      "citation_id": "98",
      "title": "Domain adaptation for cross-subject emotion recognition by subject clustering",
      "authors": [
        "J Liu",
        "X Shen",
        "S Song",
        "D Zhang"
      ],
      "year": "2021",
      "venue": "2021 10th International IEEE/EMBS Conference on Neural Engineering (NER)"
    },
    {
      "citation_id": "99",
      "title": "Subjectindependent emotion recognition of eeg signals based on dynamic empirical convolutional neural network",
      "authors": [
        "S Liu",
        "X Wang",
        "L Zhao",
        "J Zhao",
        "Q Xin",
        "S Wang"
      ],
      "year": "2020",
      "venue": "IEEE/ACM Transactions on Computational Biology and Bioinformatics"
    },
    {
      "citation_id": "100",
      "title": "Learning transferable features with deep adaptation networks",
      "authors": [
        "M Long",
        "Y Cao",
        "J Wang",
        "M Jordan"
      ],
      "year": "2015",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "101",
      "title": "Transfer sparse coding for robust image representation",
      "authors": [
        "M Long",
        "G Ding",
        "J Wang",
        "J Sun",
        "Y Guo",
        "P Yu"
      ],
      "year": "2013",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "102",
      "title": "Dry eeg electrodes",
      "authors": [
        "M Lopez-Gordo",
        "D Sanchez-Morillo",
        "F Valle"
      ],
      "year": "2014",
      "venue": "Sensors"
    },
    {
      "citation_id": "103",
      "title": "A review of classification algorithms for eeg-based brain-computer interfaces: a 10 year update",
      "authors": [
        "F Lotte",
        "L Bougrain",
        "A Cichocki",
        "M Clerc",
        "M Congedo",
        "A Rakotomamonjy",
        "F Yger"
      ],
      "year": "2018",
      "venue": "Journal of neural engineering"
    },
    {
      "citation_id": "104",
      "title": "Progressive lowrank subspace alignment based on semi-supervised joint domain adaption for personalized emotion recognition",
      "authors": [
        "J Luo",
        "M Wu",
        "Z Wang",
        "Y Chen",
        "Y Yang"
      ],
      "year": "2021",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "105",
      "title": "Wasserstein-distance-based multi-source adversarial domain adaptation for emotion recognition and vigilance estimation",
      "authors": [
        "Y Luo",
        "B.-L Lu"
      ],
      "year": "2021",
      "venue": "2021 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)"
    },
    {
      "citation_id": "106",
      "title": "Wgan domain adaptation for eeg-based emotion recognition",
      "authors": [
        "Y Luo",
        "S.-Y Zhang",
        "W.-L Zheng",
        "B.-L Lu"
      ],
      "year": "2018",
      "venue": "International Conference on Neural Information Processing"
    },
    {
      "citation_id": "107",
      "title": "Reducing the subject variability of eeg signals with adversarial domain generalization",
      "authors": [
        "B.-Q Ma",
        "H Li",
        "W.-L Zheng",
        "B.-L Lu"
      ],
      "year": "2019",
      "venue": "International Conference on Neural Information Processing"
    },
    {
      "citation_id": "108",
      "title": "Supervised dictionary learning",
      "authors": [
        "J Mairal",
        "J Ponce",
        "G Sapiro",
        "A Zisserman",
        "F Bach"
      ],
      "year": "2008",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "109",
      "title": "Domain generalization via invariant feature representation",
      "authors": [
        "K Muandet",
        "D Balduzzi",
        "B Schölkopf"
      ],
      "year": "2013",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "110",
      "title": "The fractal dimension of eeg as a physical measure of conscious human brain activities",
      "authors": [
        "X Jinghua"
      ],
      "year": "1988",
      "venue": "Bulletin of Mathematical Biology"
    },
    {
      "citation_id": "111",
      "title": "Attentive adversarial network for largescale sleep staging",
      "authors": [
        "S Nasiri",
        "G Clifford"
      ],
      "year": "2020",
      "venue": "Machine Learning for Healthcare Conference"
    },
    {
      "citation_id": "112",
      "title": "A domain adaptation sparse representation classifier for cross-domain electroencephalogram-based emotion classification",
      "authors": [
        "T Ni",
        "Y Ni",
        "J Xue",
        "S Wang"
      ],
      "year": "2021",
      "venue": "Frontiers in Psychology"
    },
    {
      "citation_id": "113",
      "title": "Cross-subject eeg emotion recognition using domain adaptive few-shot learning networks",
      "authors": [
        "R Ning",
        "C Chen",
        "T Zhang"
      ],
      "year": "2021",
      "venue": "2021 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)"
    },
    {
      "citation_id": "114",
      "title": "A review on the attention mechanism of deep learning",
      "authors": [
        "Z Niu",
        "G Zhong",
        "H Yu"
      ],
      "year": "2021",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "115",
      "title": "A novel eeg feature extraction method using hjorth parameter",
      "authors": [
        "S.-H Oh",
        "Y.-R Lee",
        "H.-N Kim"
      ],
      "year": "2014",
      "venue": "International Journal of Electronics and Electrical Engineering"
    },
    {
      "citation_id": "116",
      "title": "Analysis and design of echo state networks",
      "authors": [
        "M Ozturk",
        "D Xu",
        "J Principe"
      ],
      "year": "2007",
      "venue": "Neural computation"
    },
    {
      "citation_id": "117",
      "title": "Domain adaptation via transfer component analysis",
      "authors": [
        "S Pan",
        "I Tsang",
        "J Kwok",
        "Q Yang"
      ],
      "year": "2010",
      "venue": "Domain adaptation via transfer component analysis"
    },
    {
      "citation_id": "118",
      "title": "A survey on transfer learning",
      "authors": [
        "S Pan",
        "Q Yang"
      ],
      "year": "2009",
      "venue": "IEEE Transactions on knowledge and data engineering"
    },
    {
      "citation_id": "119",
      "title": "Subject independent emotion recognition from eeg using vmd and deep learning",
      "authors": [
        "P Pandey",
        "K Seeja"
      ],
      "year": "2019",
      "venue": "Subject independent emotion recognition from eeg using vmd and deep learning"
    },
    {
      "citation_id": "120",
      "title": "Feature extraction of eeg for emotion recognition using hjorth features and higher order crossings",
      "authors": [
        "A Patil",
        "C Deshmukh",
        "A Panat"
      ],
      "year": "2016",
      "venue": "2016 Conference on Advances in Signal Processing (CASP)"
    },
    {
      "citation_id": "121",
      "title": "Self-weighted semi-supervised classification for joint eeg-based emotion recognition and affective activation patterns mining",
      "authors": [
        "Y Peng",
        "W Kong",
        "F Qin",
        "F Nie",
        "J Fang",
        "B.-L Lu",
        "A Cichocki"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Instrumentation and Measurement"
    },
    {
      "citation_id": "122",
      "title": "A survey on feature extraction methods for eeg based emotion recognition",
      "authors": [
        "S Phadikar",
        "N Sinha",
        "R Ghosh"
      ],
      "year": "2019",
      "venue": "International Conference on Innovation in Modern Science and Technology"
    },
    {
      "citation_id": "123",
      "title": "The neurophysiological bases of emotion: An fmri study of the affective circumplex using emotiondenoting words",
      "authors": [
        "J Posner",
        "J Russell",
        "A Gerber",
        "D Gorman",
        "T Colibazzi",
        "S Yu",
        "Z Wang",
        "A Kangarlu",
        "H Zhu",
        "B Peterson"
      ],
      "year": "2009",
      "venue": "Human brain mapping"
    },
    {
      "citation_id": "124",
      "title": "Learning densenet features from eeg based spectrograms for subject independent emotion recognition",
      "authors": [
        "A Pusarla",
        "B Singh",
        "C Tripathi"
      ],
      "year": "2022",
      "venue": "Biomedical Signal Processing and Control"
    },
    {
      "citation_id": "125",
      "title": "Dataset shift in machine learning",
      "authors": [
        "J Quinonero-Candela",
        "M Sugiyama",
        "A Schwaighofer",
        "N Lawrence"
      ],
      "year": "2008",
      "venue": "Dataset shift in machine learning"
    },
    {
      "citation_id": "126",
      "title": "Cross-corpus eeg-based emotion recognition",
      "authors": [
        "S Rayatdoost",
        "M Soleymani"
      ],
      "year": "2018",
      "venue": "2018 IEEE 28th International Workshop on Machine Learning for Signal Processing"
    },
    {
      "citation_id": "127",
      "title": "Bringing emotion recognition out of the lab into real life: Recent advances in sensors and machine learning",
      "authors": [
        "S Saganowski"
      ],
      "year": "2022",
      "venue": "Electronics"
    },
    {
      "citation_id": "128",
      "title": "Emotion recognition using wearables: A systematic literature review-work-in-progress",
      "authors": [
        "S Saganowski",
        "A Dutkowiak",
        "A Dziadek",
        "M Dzieżyc",
        "J Komoszyńska",
        "W Michalska",
        "A Polak",
        "M Ujma",
        "P Kazienko"
      ],
      "year": "2020",
      "venue": "2020 IEEE International Conference on Pervasive Computing and Communications Workshops (PerCom Workshops)"
    },
    {
      "citation_id": "129",
      "title": "Maximum classifier discrepancy for unsupervised domain adaptation",
      "authors": [
        "K Saito",
        "K Watanabe",
        "Y Ushiku",
        "T Harada"
      ],
      "year": "2018",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "130",
      "title": "Kernel principal component analysis",
      "authors": [
        "B Schölkopf",
        "A Smola",
        "K.-R Müller"
      ],
      "year": "1997",
      "venue": "International conference on artificial neural networks"
    },
    {
      "citation_id": "131",
      "title": "Towards adaptive classification for bci",
      "authors": [
        "P Shenoy",
        "M Krauledat",
        "B Blankertz",
        "R Rao",
        "K.-R Müller"
      ],
      "year": "2006",
      "venue": "Journal of neural engineering"
    },
    {
      "citation_id": "132",
      "title": "Multimodal emotion recognition in response to videos",
      "authors": [
        "M Soleymani",
        "M Pantic",
        "T Pun"
      ],
      "year": "2011",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "133",
      "title": "Eeg emotion recognition using dynamical graph convolutional neural networks",
      "authors": [
        "T Song",
        "W Zheng",
        "P Song",
        "Z Cui"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "134",
      "title": "Bioelectrical signal processing in cardiac and neurological applications",
      "authors": [
        "L Sörnmo",
        "P Laguna"
      ],
      "year": "2005",
      "venue": "Bioelectrical signal processing in cardiac and neurological applications"
    },
    {
      "citation_id": "135",
      "title": "Ascertain: Emotion and personality recognition using commercial sensors",
      "authors": [
        "R Subramanian",
        "J Wache",
        "M Abadi",
        "R Vieriu",
        "S Winkler",
        "N Sebe"
      ],
      "year": "2016",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "136",
      "title": "Eeg-based emotion recognition: A state-of-the-art review of current trends and opportunities. Computational intelligence and neuroscience",
      "authors": [
        "N Suhaimi",
        "J Mountstephens",
        "J Teo"
      ],
      "year": "2020",
      "venue": "Eeg-based emotion recognition: A state-of-the-art review of current trends and opportunities. Computational intelligence and neuroscience"
    },
    {
      "citation_id": "137",
      "title": "Inceptionv4, inception-resnet and the impact of residual connections on learning",
      "authors": [
        "C Szegedy",
        "S Ioffe",
        "V Vanhoucke",
        "A Alemi"
      ],
      "year": "2017",
      "venue": "Thirty-first AAAI conference on artificial intelligence"
    },
    {
      "citation_id": "138",
      "title": "Multi-source co-adaptation for eeg-based emotion recognition by mining correlation information",
      "authors": [
        "J Tao",
        "Y Dan"
      ],
      "year": "2021",
      "venue": "Frontiers in Neuroscience"
    },
    {
      "citation_id": "139",
      "title": "From primary emotions to the spectrum of affect: An evolutionary neurosociology of the emotions",
      "authors": [
        "W Tenhouten"
      ],
      "year": "2017",
      "venue": "Neuroscience and social science"
    },
    {
      "citation_id": "140",
      "title": "Personality first in emotion: a deep neural network based on electroencephalogram channel attention for cross-subject emotion recognition",
      "authors": [
        "Z Tian",
        "D Huang",
        "S Zhou",
        "Z Zhao",
        "D Jiang"
      ],
      "year": "2021",
      "venue": "Royal Society open science"
    },
    {
      "citation_id": "141",
      "title": "Eeg-based bci emotion recognition: A survey",
      "authors": [
        "E Torres",
        "E Torres",
        "M Hernández-Álvarez",
        "S Yoo"
      ],
      "year": "2020",
      "venue": "Sensors"
    },
    {
      "citation_id": "142",
      "title": "Adversarial discriminative domain adaptation",
      "authors": [
        "E Tzeng",
        "J Hoffman",
        "K Saenko",
        "T Darrell"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "143",
      "title": "Deep domain confusion: Maximizing for domain invariance",
      "authors": [
        "E Tzeng",
        "J Hoffman",
        "N Zhang",
        "K Saenko",
        "T Darrell"
      ],
      "year": "2014",
      "venue": "Deep domain confusion: Maximizing for domain invariance",
      "arxiv": "arXiv:1412.3474"
    },
    {
      "citation_id": "144",
      "title": "24 transductive inference and semi-supervised learning",
      "authors": [
        "V Vapnik"
      ],
      "year": "2006",
      "venue": "24 transductive inference and semi-supervised learning"
    },
    {
      "citation_id": "145",
      "title": "Time domain parameters as a feature for eeg-based brain-computer interfaces",
      "authors": [
        "C Vidaurre",
        "N Krämer",
        "B Blankertz",
        "A Schlögl"
      ],
      "year": "2009",
      "venue": "Neural Networks"
    },
    {
      "citation_id": "146",
      "title": "Emotion recognition with convolutional neural network and eeg-based efdms",
      "authors": [
        "F Wang",
        "S Wu",
        "W Zhang",
        "Z Xu",
        "Y Zhang",
        "C Wu",
        "S Coleman"
      ],
      "year": "2020",
      "venue": "Neuropsychologia"
    },
    {
      "citation_id": "147",
      "title": "A deep multi-source adaptation transfer network for cross-subject electroencephalogram emotion recognition",
      "authors": [
        "F Wang",
        "W Zhang",
        "Z Xu",
        "J Ping",
        "H Chu"
      ],
      "year": "2021",
      "venue": "Neural Computing and Applications"
    },
    {
      "citation_id": "148",
      "title": "Eeg-based emotion recognition using frequency domain features and support vector machines",
      "authors": [
        "X.-W Wang",
        "D Nie",
        "B.-L Lu"
      ],
      "year": "2011",
      "venue": "International conference on neural information processing"
    },
    {
      "citation_id": "149",
      "title": "Cross-subject eeg emotion classification based on few-label adversarial domain adaption",
      "authors": [
        "Y Wang",
        "J Liu",
        "Q Ruan",
        "S Wang",
        "C Wang"
      ],
      "year": "2021",
      "venue": "Expert Systems with Applications"
    },
    {
      "citation_id": "150",
      "title": "A prototype-based spd matrix network for domain adaptation eeg emotion recognition",
      "authors": [
        "Y Wang",
        "S Qiu",
        "X Ma",
        "H He"
      ],
      "year": "2021",
      "venue": "Pattern Recognition"
    },
    {
      "citation_id": "151",
      "title": "Relative effectiveness and validity of mood induction procedures: A meta-analysis",
      "authors": [
        "R Westermann",
        "K Spies",
        "G Stahl",
        "F Hesse"
      ],
      "year": "1996",
      "venue": "European Journal of social psychology"
    },
    {
      "citation_id": "152",
      "title": "Robust principal component analysis: Exact recovery of corrupted low-rank matrices via convex optimization",
      "authors": [
        "J Wright",
        "A Ganesh",
        "S Rao",
        "Y Peng",
        "Y Ma"
      ],
      "year": "2009",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "153",
      "title": "Transfer learning for eeg-based braincomputer interfaces: A review of progress made since",
      "authors": [
        "D Wu",
        "Y Xu",
        "B.-L Lu"
      ],
      "year": "2016",
      "venue": "IEEE Transactions on Cognitive and Developmental Systems"
    },
    {
      "citation_id": "154",
      "title": "Feature transfer learning in eeg-based emotion recognition",
      "authors": [
        "B Xue",
        "Z Lv",
        "J Xue"
      ],
      "year": "2020",
      "venue": "2020 Chinese Automation Congress (CAC)"
    },
    {
      "citation_id": "155",
      "title": "Multi-method fusion of cross-subject emotion recognition based on high-dimensional eeg features",
      "authors": [
        "F Yang",
        "X Zhao",
        "W Jiang",
        "P Gao",
        "G Liu"
      ],
      "year": "2019",
      "venue": "Frontiers in computational neuroscience"
    },
    {
      "citation_id": "156",
      "title": "Improving session-to-session transfer performance of emotion recognition using adaptive support vector machine",
      "authors": [
        "K Yang",
        "G Bao",
        "Y Zeng",
        "L Tong",
        "J Shu",
        "B Yan"
      ],
      "year": "2020",
      "venue": "In Journal of Physics: Conference Series"
    },
    {
      "citation_id": "157",
      "title": "Cross-subject eeg-based emotion recognition using adversarial domain adaption with attention mechanism",
      "authors": [
        "Y Ye",
        "X Zhu",
        "Y Li",
        "T Pan",
        "W He"
      ],
      "year": "2021",
      "venue": "2021 43rd Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC)"
    },
    {
      "citation_id": "158",
      "title": "Cross-subject eeg feature selection for emotion recognition using transfer recursive feature elimination",
      "authors": [
        "Z Yin",
        "Y Wang",
        "L Liu",
        "W Zhang",
        "J Zhang"
      ],
      "year": "2017",
      "venue": "Frontiers in neurorobotics"
    },
    {
      "citation_id": "159",
      "title": "Classification of human emotions from eeg signals using statistical features and neural network",
      "authors": [
        "C Yuen",
        "W San San",
        "T Seong",
        "M Rizon"
      ],
      "year": "2009",
      "venue": "International Journal of Integrated Engineering"
    },
    {
      "citation_id": "160",
      "title": "Relieff-based eeg sensor selection methods for emotion recognition",
      "authors": [
        "J Zhang",
        "M Chen",
        "S Zhao",
        "S Hu",
        "Z Shi",
        "Y Cao"
      ],
      "year": "2016",
      "venue": "Sensors"
    },
    {
      "citation_id": "161",
      "title": "Crosssubject eeg-based emotion recognition with deep domain confusion",
      "authors": [
        "W Zhang",
        "F Wang",
        "Y Jiang",
        "Z Xu",
        "S Wu",
        "Y Zhang"
      ],
      "year": "2019",
      "venue": "International conference on intelligent robotics and applications"
    },
    {
      "citation_id": "162",
      "title": "Eeg feature selection for emotion recognition based on cross-subject recursive feature elimination",
      "authors": [
        "W Zhang",
        "Z Yin"
      ],
      "year": "2020",
      "venue": "2020 39th Chinese Control Conference (CCC)"
    },
    {
      "citation_id": "163",
      "title": "Individual similarity guided transfer modeling for eeg-based emotion recognition",
      "authors": [
        "X Zhang",
        "W Liang",
        "T Ding",
        "J Pan",
        "J Shen",
        "X Huang",
        "J Gao"
      ],
      "year": "2019",
      "venue": "2019 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)"
    },
    {
      "citation_id": "164",
      "title": "Writer adaptation with style transfer mapping",
      "authors": [
        "X.-Y Zhang",
        "C.-L Liu"
      ],
      "year": "2012",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "165",
      "title": "Spectral and time-frequency analysis",
      "authors": [
        "Z Zhang"
      ],
      "year": "2019",
      "venue": "EEG Signal Processing and feature extraction"
    },
    {
      "citation_id": "166",
      "title": "Frontal eeg asymmetry and middle line power difference in discrete emotions",
      "authors": [
        "G Zhao",
        "Y Zhang",
        "Y Ge"
      ],
      "year": "2018",
      "venue": "Frontiers in behavioral neuroscience"
    },
    {
      "citation_id": "167",
      "title": "Plug-and-play domain adaptation for cross-subject eeg-based emotion recognition",
      "authors": [
        "L.-M Zhao",
        "X Yan",
        "B.-L Lu"
      ],
      "year": "2021",
      "venue": "Proceedings of the 35th AAAI Conference on Artificial Intelligence. sn"
    },
    {
      "citation_id": "168",
      "title": "Investigating critical frequency bands and channels for eeg-based emotion recognition with deep neural networks",
      "authors": [
        "W.-L Zheng",
        "B.-L Lu"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on autonomous mental development"
    },
    {
      "citation_id": "169",
      "title": "Personalizing eeg-based affective models with transfer learning",
      "authors": [
        "W.-L Zheng",
        "B.-L Lu"
      ],
      "year": "2016",
      "venue": "Proceedings of the twenty-fifth international joint conference on artificial intelligence"
    },
    {
      "citation_id": "170",
      "title": "Transfer components between subjects for eeg-based emotion recognition",
      "authors": [
        "W.-L Zheng",
        "Y.-Q Zhang",
        "J.-Y Zhu",
        "B.-L Lu"
      ],
      "year": "2015",
      "venue": "2015 international conference on affective computing and intelligent interaction (ACII)"
    },
    {
      "citation_id": "171",
      "title": "Eeg-based emotion recognition using regularized graph neural networks",
      "authors": [
        "P Zhong",
        "D Wang",
        "C Miao"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "172",
      "title": "An eeg emotion recognition method based on transfer learning and echo state network for hilcps",
      "authors": [
        "J Zhou",
        "S Chu",
        "X Li",
        "F Xiao",
        "L Sun"
      ],
      "year": "2020",
      "venue": "Microprocessors and Microsystems"
    },
    {
      "citation_id": "173",
      "title": "Graph neural networks: A review of methods and applications",
      "authors": [
        "J Zhou",
        "G Cui",
        "S Hu",
        "Z Zhang",
        "C Yang",
        "Z Liu",
        "L Wang",
        "C Li",
        "M Sun"
      ],
      "year": "2020",
      "venue": "Graph neural networks: A review of methods and applications"
    },
    {
      "citation_id": "174",
      "title": "Maximum mean discrepancy based multiple kernel learning for incomplete multimodality neuroimaging data",
      "authors": [
        "X Zhu",
        "K.-H Thung",
        "E Adeli",
        "Y Zhang",
        "D Shen"
      ],
      "year": "2017",
      "venue": "International Conference on Medical Image Computing and Computer-Assisted Intervention"
    }
  ]
}