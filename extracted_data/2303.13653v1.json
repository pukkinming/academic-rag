{
  "paper_id": "2303.13653v1",
  "title": "Efficient Neural Architecture Search For Emotion Recognition",
  "published": "2023-03-23T20:21:26Z",
  "authors": [
    "Monu Verma",
    "Murari Mandal",
    "Satish Kumar Reddy",
    "Yashwanth Reddy Meedimale",
    "Santosh Kumar Vipparthi"
  ],
  "keywords": [
    "Human emotion",
    "micro-expression",
    "macro-expression",
    "neural architecture search (NAS)",
    "deep learning"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Automated human emotion recognition from facial expressions is a well-studied problem and still remains a very challenging task. Some efficient or accurate deep learning models have been presented in the literature. However, it is quite difficult to design a model that is both efficient and accurate at the same time. Moreover, identifying the minute feature variations in facial regions for both macro and micro-expressions requires expertise in network design. In this paper, we proposed to search for a highly efficient and robust neural architecture for both macro and micro-level facial expression recognition. To the best of our knowledge, this is the first attempt to design a NAS-based solution for both macro and micro-expression recognition. We produce lightweight models with a gradient-based architecture search algorithm. To maintain consistency between macro and micro-expressions, we utilize dynamic imaging and convert microexpression sequences into a single frame, preserving the spatiotemporal features in the facial regions. The EmoNAS has evaluated over 13 datasets (7 macro expression datasets: CK+, DISFA, MUG, ISED, OULU-VIS CASIA, FER2013,",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Introduction",
      "text": "Human emotion recognition from visual content has attracted significant attention in the last few decades. Facial appearances are one of the most discriminative features of emotional responses  Ekman & Friesen (1971) . Therefore, researchers have primarily focused on developing robust facial expression recognition (FER) systems for emotion recognition. It has numerous applications in human-computer interaction, behavior profiling, and smart healthcare solutions.\n\nFacial expressions can be classified into two types: macro and micro-expressions.\n\nMacro expressions are considered as a prevailing display of emotions and usually last up to 3 to 4 minutesEkman  (2003) . Whereas, micro-expressions are generated when someone tries to hide their actual feelings. Micro-expression appears on the facial region for a fraction of time (1/3-1/22) secondsVerma et al.  (2019b) .\n\nThe existing state-of-the-art convolutional neural network (CNN) based methods have primarily improved upon the FER performance and few others have focused on improving the efficiency through smaller network design  Verma et al. (2019a) . However, state-of-the-art approaches require the substantial effort of human expertise to design an effective neural network architecture. Due to the human efforts involved in deep network design, there is room for improvement in both efficiency and accuracy. Moreover, the subtle variations in certain facial regions lead to changes in emotion class. There is a need for developing robust and lightweight FER models for real-world applications.\n\nRecently, there has been a growing interest in developing algorithmic solutions to automatically design an architecture for deep learning models. These algorithms are known as neural architecture search (NAS). Inspired by the differentiable architecture search algorithm  Liu et al. (2018b) , in this paper, we present EmoNAS to search for the most robust and efficient neural architecture for FER. The training and evaluation procedure of the proposed EmoNAS is shown in Figure  1 . As discussed earlier, macro and micro expressions are different in nature, so it is challenging to design a network that can efficiently work for both types of expressions. (more detailed differences between macro and micro expressions are discussed in the supplementary document). Therefore, solutions for both MaEs and MEs are non-identical in the literature. Therefore, Most of the existing FER algorithms are either designed for macro or micro expression classification. Some of the concepts utilize the macro adaptive features for micro-expression classification  Sun et al. (2020) ;  Yang et al. (2021) . However, there is no single algorithm to solve macro and micro-expression recognition.\n\nWe made the first attempt to introduce a universal NAS algorithm to solve the generic macro facial expressions as well as micro facial expressions. Usually, a single instance of an image is sufficient for macro expression recognition analysis whereas, MER requires spatiotemporal data due to its fleeting and short-lived nature. Therefore, to maintain a uniform experimental setting, we adopted the dynamic imaging concept to extract a single instance feature map consisting of the spatiotemporal features from the temporal sequence of frames. Therefore, this paper aims to provide a uniform search and training framework to target the challenges in FER and MER using an EmoNAS. Our proposed framework EmoNAS has the following contributions:\n\n1. We propose a differentiable architecture search-based algorithm named EmoNAS to handle the challenges in macro and micro-expression with the optimized CNN models. We propose to employ a uniform number of cells in the architecture design as used in the search phase. Moreover, the search space is restricted to a shallower structure to obtain lightweight models for real-time applications.\n\n2. The EmoNAS achieves remarkable efficiency improvement over the existing deep learning models in terms of the final model computational complexity.\n\n3. The effectiveness of EmoNAS is validated on 13 datasets: 7 macro facial expression recognition (FER) and 6 micro-expression recognition (MER) datasets. It achieves highly competitive results and outperforms most of all the existing state-of-the-art methods in these 13 datasets. The impact of selecting a different number of cells and nodes is extensively studied by conducting 9 ablation experiments on CK+, DISFSA, CASME-I, and CAS(ME) 2 datasets.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Related Work",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Macro-Expression",
      "text": "Macro-expression recognition, commonly referred to as facial expression recognition (FER), has been widely studied in the literature. The deep learning algorithms for FER  Pons & Masip (2017) ;  Xie et al. (2020)  have far outperformed the traditional hard-crafted approaches  Mandal et al. (2019) ;  Iqbal et al. (2020) .\n\nResearchers have developed numerous  CNN Verma et al. (2019a) ;  Zhang et al. (2016)  for FER.  Fan et al. Fan et al. (2020)    More recently, a composite database for MER was presented along with the benchmark experimental evaluation protocol in  Zhang et al. (2020) . Subsequently, Xia et al.  Xia et al. (2020)  studied the influence of learning complexity on the composite database. They conclude that the low-resolution input and shallower architecture are more suitable for composite-dataset problems in comparison to the deep architectures. A more detailed study of various MER methods can be found in  Zhou et al. (2020) .",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Micro-Expression",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Micro",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Neural Architecture Search",
      "text": "Recently NAS algorithms intended for automatically searching and designing CNN architectures have achieved very competitive performance in computer vision tasks such as image classification  Real et al. (2019)  and object detection  Zoph et al. (2018) . Despite their remarkable performance, the earlier NAS algorithms were computationally expensive and demands huge memory footprints.\n\nThe reinforcement learning (RL)  Zoph et al. (2018)  NAS requires 2000 GPU days to get existing architecture for CIFAR-10 and ImageNet. Similarly, another NAS algorithm  Real et al. (2019)  needs 3150 GPU days of evaluation.\n\nFurthermore, various NAS algorithms  Bender et al. (2018) ;  Baker et al. (2017) ;  Liu et al. (2018a)  have been introduced to speed up the search process. The RL, evolution,  MCTS Negrinho & Gordon (2017) ,  SMBO Liu et al. (2018a)  based NAS algorithms consider the architecture search as a black-box optimization problem over a discrete space, which leads to a huge number of architecture assessments. Therefore,  Liu et al. (2018b)  relaxed the search space to be continuous to optimize the architecture through gradient descent with respect to its validation set performance. This is the most effective solution to accelerate the search process. Therefore, we use this optimization technique in our work.\n\nRecently  Yu et al. Yu et al. (2020b,a,c)   The network design process for the existing deep learning FER and MER methods requires a lot of manual effort. Usually, the designed network performs well over a few expression datasets but does not transfer well to other expression datasets. Moreover, the macro-and micro-expression recognition tasks require quite different algorithmic approaches to obtain robust performance. Thus, it is difficult to design a single network to effectively solve both the FER and MER tasks. The NAS algorithms offer an alternative approach to universally solving both the FER and MER problems by automatically searching for efficient and  robust architectures. Motivated by this, we propose the EmoNAS algorithm and experimentally show its effectiveness in FER and MER. To the best of our knowledge, this is the first attempt to solve both macro-and micro-expression tasks with neural architecture search.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Dynamic Imaging",
      "text": "The main aim of this paper is to provide a uniform search and training framework to address the challenges in FER and MER using a NAS method. Usually, a single instance of the image if sufficient for FER analysis whereas, MER requires spatiotemporal data due to its fleeting and short-lived nature. Therefore, to maintain a uniform experimental setting, we adopted the dynamic imaging concept to extract a single instance feature map consisting of the spatiotemporal features from the temporal sequence of frames. Most of the existing MER approaches  Van Quang et al. (2019) ;  Li et al. (2020a) ;  Liong et al. (2019b) ;  Liu et al. (2019b)  rely only on the apex frame for the analysis. However, some studies emphasize the importance of dynamic aspects for detecting the subtle changes  Ambadar et al. (2005)  and their effect on the performance of MER.\n\nIn a micro-expression (MEs) video, each frame has its own significance in the identification of the emotion class. Some recent approaches Reddy Sai Prasanna  Teja et al. (2019) ;  Li et al. (2019a)  utilized video sequences to design an effective MER system. However, all publicly available MER datasets hold videos with a variant number of frames. In order to normalize the video sequences, some approaches have utilized time interpolation algorithms. This may lose or alter the domain knowledge of micro-expressions by shearing or filling holes in between the frames. To overcome the above-mentioned issues and embed the subtle facial changes, we have utilized the dynamic imaging concept  Bilen et al. (2016)  to generate a single instance of an image. Dynamic imaging represents the video information into a single instance by conserving high stake active dynamics of MEs. It also ensures uniform search and training architecture for both macro and micro-expression recognition.\n\nThe dynamic imaging technique has been effectively used in the recent literature for micro-expression recognition  Verma et al. (2019b) . The dynamic image interprets the content of the video by aiming at the facial moving regions and compresses that into a single instance. The dynamic image is an RGB response holding the spatiotemporal features of a video sequence. The dynamic image d * is calculated by Eq. 1-Eq. 5.\n\nwhere Z p and N ∈ q represent the p th intermediate motion image and a total number of frames in a video V , respectively. The intermediate motion image is calculated by using Eq. 2.\n\nwhere F (p) implies the frame weight and is calculated by using Eq. 3.\n\nwhere V p ∈ V represents the p th frame of the video V, p = 1, 2, ..., q and R is a ranking function which upgrade the rank of frames as follows:\n\nwhere q implies the total frames in video sequences. l is a index of the frames in range  [1, q] . We have shown sample dynamic responses in Figure  2 . It can be observed that both the uniform and non-uniform features of a video stream are embedded in a single frame. We use these dynamic images to search and train the NAS models for micro-expression recognition.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "Emonas",
      "text": "The proposed EmoNAS algorithm for architecture search is depicted in to repetitive cross-correlation and pooling operations. We are also motivated by the fact that the properties of deep and shallow networks are quite different which has been studied in the literature  Srivastava et al. (2015) . Therefore, we  m,n) that transforms input x (m) into response feature maps. Each cell has 2 input nodes and 1 output node. The cell outputs from the previous layers are used as input nodes. Eventually, the cell structure is formed by learning the set of operations on its edges. A node k in a cell c can be defined by using 5-tuples\n\ntensors, selection of employed layers over selected input tensors and method used to form an output tensor OT C l . Further, the output tensor OT C of a cell can be calculated by using Eq. 6.\n\nwhere || indicate the concatenation operation.\n\nThe set of candidate operations are defined by κ, where each operation is denoted by o(•). The continuous search space is used by relaxing the choice of a particular operation to a softmax over all possible operations as given in Eq.\n\n7.\n\nwhere the operation mixing weights for a pair of nodes (m, n) are learned by a vector α (m,n) of dimension |κ|. After the completion of search, a discrete architecture is retrieved by replacing each mixed operation with the most likely operation. We leverage the approximation scheme given in  Liu et al. (2018b)  to perform the optimization.\n\nLet λ train and λ val represent the training and the validation loss, respectively. These losses are calculated based on both the architecture α and weights w in the network. The goal for EmoNAS is to discover α * that minimizes the validation loss λ val (w * , α * ). The weights w * allied with the architecture are computed by minimizing the training loss w * = argmin w λ train (w, α * ). Thus, the bilevel optimization problem with α as the upper-level and w as the lowerlevel variable is represented in Eq. 8 and Eq. 9.\n\nThe detailed procedure and approximation technique is outlined in  Liu et al. (2018b) .",
      "page_start": 10,
      "page_end": 11
    },
    {
      "section_name": "Network Configurations",
      "text": "The candidate operation space κ consists of the following set of 7 operations:\n\n3 × 3 depthwise-separable convolution, 5 × 5 depthwise-separable convolution, identity, 3×3 average pooling, 3×3 max pooling, 3×3 dilated convolution, 5×5 dilated convolution and skip. connection. The network search and evaluation process is discussed below.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Architecture Search",
      "text": "We use 5 cells with each containing 7 nodes. We intentionally select lower number of cells to ensure discovery of lightweight and efficient networks. Similarly, the search space is also limited to only 7 operations. These design decisions helped us to create very lightweight architectures for FER over different datasets. We were also motivated by the success of shallower CNN models in FER to search the shallower architectures through NAS. To compose the nodes in the discrete architecture, we retain the top-2 strongest operations (from distinct nodes) among all the candidate operations. The results analysis discussed in Section 5 validate that search space with 5 cells achieve better performance as compared to DARTS with significant performance improvement over all the FER datasets.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Architecture Evaluation",
      "text": "In order to evaluate the discovered cell, we stack same 5 copies of the cells (searched by architecture search), but untied weights with using either stride 1 or stride 2, as shown in Figure  1",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Experiments And Results",
      "text": "",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Experimental Details",
      "text": "This section discusses the implementation details, dataset, and evaluation strategies. Further, a comparative study of the proposed EmoNAS and stateof-the-art approaches is presented. We carry out the ablation study and com- putational analysis in the following subsection. document.",
      "page_start": 14,
      "page_end": 15
    },
    {
      "section_name": "Implementation Details",
      "text": "Initially, to search for the best possible cell structure, we used 50 epochs and 16 for batch size due to GPU memory constraints. The main goal of the cell search is to learn the best hyper-parameter α that defines the best possible operations in the cell structure. We used stochastic gradient descent (SGD) optimizer with a momentum of 0.9, a cosine learning rate of 0.007, and a weight decay of 3e -4 . Further, we stacked 5 sets of discovered cells with 7 hidden nodes",
      "page_start": 15,
      "page_end": 15
    },
    {
      "section_name": "Datasets And Evaluation Settings",
      "text": "To evaluate the proposed EmoNAS, we conduct experiments over 7 macro-    6 . Similar to macro-expression, the models are evaluated on completely unseen subjects in leave-one-person-out manner.",
      "page_start": 16,
      "page_end": 16
    },
    {
      "section_name": "Macro-Expression Results Analysis",
      "text": "We compute the classification accuracy to evaluate the proposed EmoNAS on the macro-expression datasets CK+, MUG, ISED, DISFA, OULO-VIS-Strong, FER2013, and RAF-DB. The results on all the datasets are shown in Table 1, Table  2 , Table  3 , Table  4  and 27.62%) in 6-and 7-class problems, respectively. In comparison to the traditional designed CNN networks, the proposed NAS based models show improvement in both the quantitative performance (Table  2 ) and computational complexity (Table  14 ). The results also show that the proposed method is effective for both posed and spontaneous FER problems.",
      "page_start": 17,
      "page_end": 20
    },
    {
      "section_name": "Raf-Db",
      "text": "We also conduct experiments on the in-the-wild RAF-DB dataset. We search and train the EmoNAS with the seven expression labels provided in the RAF-DB.  Furthermore, the results reveal that the EmoNAS attains the highest accuracy when identifying the category of disgust, fear, and surprise. It also maintains good accuracy in the remaining categories, leading to overall accuracy, better than the existing state-of-the-art. Therefore, the features learned and chosen by EmoNAS should contain more discriminative information for FER. This illustrates the effectiveness and superiority of the NAS-based search for the most robust architecture over the traditional CNN models.",
      "page_start": 22,
      "page_end": 22
    },
    {
      "section_name": "Micro-Expression Results Analysis",
      "text": "Usually, the macro expression recognition algorithms rely on static images for analysis. Whereas, the micro-expression (MEs) recognition algorithms require a video sequence as input. For micro-expression, we use dynamic imaging to generate a single instance of an image micro-expressions. DI ensures uniform search and training architecture for both macro and micro-expression recognition problems. We use the generated dynamic images to search and train the NAS models for micro-expression.",
      "page_start": 22,
      "page_end": 22
    },
    {
      "section_name": "Ablation Study",
      "text": "We study the effect of multiple components of EmoNAS through ablation analysis to understand its behavior for the task of FER. We conduct experiments on two macro-expression datasets CK+, DISFA, and two micro-expression datasets.\n\nThe ablation results are shown in Table  9 , Table  10 , Table  11 , and Table  12 .\n\nWe change the number of nodes within each cell to 6, 7, and 8 nodes. We stack these cells with 5, 8, 10, and 15 repetitions to obtain 12 different CNN structures. Thus, for each dataset, we conduct 12 ablation experiments.\n\nFrom Table  9  and Table  10  we can see that increasing the number nodes to 8 or decreasing to 6 resulted in marginal decrease or increase in the accuracy for CK+ and DISFA. Similarly, increasing the number of cells to 10/15 doesn't improve the performance substantially in both datasets. Moreover, a higher number of cells would increase the network complexity. Therefore, in most",
      "page_start": 23,
      "page_end": 23
    },
    {
      "section_name": "Speed, Memory And Complexity Analysis",
      "text": "The models generated through EmoNAS search are very lightweight and fast.",
      "page_start": 28,
      "page_end": 28
    },
    {
      "section_name": "Conclusions",
      "text": "To the best of our knowledge, this paper presents the first attempt at a NAS-based approach for the task of FER in both macro and micro-level facial expressions. We proposed EmoNAS which is based on the optimization techniques presented in DARTS to expedite the searching process. The design decisions for EmoNAS are the result of careful analysis of the FER domain-specific challenges. The same is validated by its superior performance over DART and P-DART methods. The architecture search also led to the formulation of shal-lower and lightweight CNNs. The resulting models achieve better performance compared to the existing state-of-the-art FER methods in 13 benchmark (  7 FER and 6 MER) datasets. The searched models obtain higher accuracy with a fraction of the computational cost and very high inference speed. In the future, shared parameters-based schemes can be investigated in the search process to discover even better performance-aware architectures. Furthermore, the number of skip connections can be optimized separately for a better trade-off between complexity and performance. The decision of selecting the number of cells in evaluation could also be included as a parameter for optimization while searching. The proposed framework is designed to work with single image input. This requires pre-processing of the video data into a single instance feature map for MER analysis. In the future, we plan to design a framework to handle the image and raw video data input for both macro-and micro-expression problems.",
      "page_start": 29,
      "page_end": 30
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The training and evaluation procedure of the proposed EmoNAS. In the training",
      "page": 3
    },
    {
      "caption": "Figure 1: As discussed earlier, macro and micro expressions are diﬀer-",
      "page": 4
    },
    {
      "caption": "Figure 2: Sample visual representation of the dynamic image response for micro-expression",
      "page": 8
    },
    {
      "caption": "Figure 2: It can be",
      "page": 10
    },
    {
      "caption": "Figure 3: The objective is to",
      "page": 10
    },
    {
      "caption": "Figure 3: An overview of the cell architecture.",
      "page": 11
    },
    {
      "caption": "Figure 3: The nodes represent the feature maps",
      "page": 11
    },
    {
      "caption": "Figure 1: to generate a network. The network is generated",
      "page": 13
    },
    {
      "caption": "Figure 4: Illustration of diﬀerent expression samples of macro datasets with challenging tasks",
      "page": 14
    },
    {
      "caption": "Figure 5: The ﬁnal regular and reduction cell structures discovered by EmoNAS on (a,b) CK+",
      "page": 15
    },
    {
      "caption": "Figure 4: We adopted a very strict experiment setting of sub-",
      "page": 16
    },
    {
      "caption": "Figure 6: The ﬁnal regular and reduction cell structures discovered by EmoNAS on (a,b)",
      "page": 17
    },
    {
      "caption": "Figure 5: We also conduct experiments on 6 micro-expression datasets CASME-I Yan",
      "page": 17
    },
    {
      "caption": "Figure 6: Similar to",
      "page": 18
    }
  ],
  "tables": [
    {
      "caption": "Table 1: Comparative results of the proposed EmoNAS and existing state-of-the-art FER",
      "page": 16
    },
    {
      "caption": "Table 2: Comparative results of the proposed EmoNAS and existing state-of-the-art FER",
      "page": 18
    },
    {
      "caption": "Table 2: , Table 3, Table 4, Table 5. The results for VGG16, VGG19,",
      "page": 18
    },
    {
      "caption": "Table 1: The architecture discovered by the proposed EmoNAS outperforms the exist-",
      "page": 18
    },
    {
      "caption": "Table 14: ). From Table 14 we can observe that our model",
      "page": 19
    },
    {
      "caption": "Table 14: Also, recent vision transformer-based models Aouayeb",
      "page": 19
    },
    {
      "caption": "Table 14: 5.2.2. MUG, ISED and DISFA",
      "page": 19
    },
    {
      "caption": "Table 2: The MUG consists of posed facial expressions whereas, ISED and",
      "page": 19
    },
    {
      "caption": "Table 3: Comparative results of the proposed EmoNAS and existing state-of-the-art methods",
      "page": 20
    },
    {
      "caption": "Table 4: Comparative results of the proposed EmoNAS and existing state-of-the-art FER",
      "page": 20
    },
    {
      "caption": "Table 2: ) and computational",
      "page": 20
    },
    {
      "caption": "Table 5: Comparative results of the proposed EmoNAS and existing methods on RAF-DB",
      "page": 21
    },
    {
      "caption": "Table 3: , it is evident that the proposed models derived from EmoNAS obtain",
      "page": 21
    },
    {
      "caption": "Table 4: The FER 2013 is a challenging benchmark FER dataset. The pre-",
      "page": 21
    },
    {
      "caption": "Table 5: shows the class-wise accuracy and the overall accuracy obtained by",
      "page": 22
    },
    {
      "caption": "Table 6: Comparative results of the proposed EmoNAS and existing approaches on CASME-I",
      "page": 23
    },
    {
      "caption": "Table 7: Comparative results of the proposed EmoNAS and existing approaches on CASMEˆ2",
      "page": 23
    },
    {
      "caption": "Table 6: shows the classiﬁcation accuracy results",
      "page": 24
    },
    {
      "caption": "Table 7: shows the classiﬁcation accuracy results",
      "page": 24
    },
    {
      "caption": "Table 8: Recognition accuracy and unweighted average recall (UAR) comparison on SMIC",
      "page": 25
    },
    {
      "caption": "Table 9: EmoNAS ablation analysis on CK+ with 6 expressions",
      "page": 25
    },
    {
      "caption": "Table 10: EmoNAS ablation analysis on DISFA with 6 expressions",
      "page": 26
    },
    {
      "caption": "Table 8: In SMIC, the proposed EmoNAS obtains",
      "page": 26
    },
    {
      "caption": "Table 9: , Table 10, Table 11, and Table 12.",
      "page": 26
    },
    {
      "caption": "Table 9: and Table 10 we can see that increasing the number nodes to",
      "page": 26
    },
    {
      "caption": "Table 11: EmoNAS ablation analysis on CASME-I",
      "page": 27
    },
    {
      "caption": "Table 12: EmoNAS ablation analysis on CAS(ME)ˆ2",
      "page": 27
    },
    {
      "caption": "Table 11: and Table 12. The best performance is achieved with",
      "page": 27
    },
    {
      "caption": "Table 6: , 7 and 8.",
      "page": 27
    },
    {
      "caption": "Table 13: Memory, speed, and complexity analysis of EmoNAS models discovered on macro:",
      "page": 28
    },
    {
      "caption": "Table 13: shows the memory, speed, and computational complexity of the models",
      "page": 28
    },
    {
      "caption": "Table 14: The EmoNAS models require the lowest number of operations",
      "page": 28
    },
    {
      "caption": "Table 14: Comparative analysis of the complexity of the EmoNAS with existing methods.",
      "page": 29
    },
    {
      "caption": "Table 14: Particularly, proposed EmoNAS is 155×",
      "page": 29
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "The mug facial expression database",
      "authors": [
        "N Aifanti",
        "C Papachristou",
        "A Delopoulos"
      ],
      "year": "2010",
      "venue": "11th International Workshop on Image Analysis for Multimedia Interactive Services WIAMIS 10"
    },
    {
      "citation_id": "2",
      "title": "Deciphering the enigmatic face: The importance of facial dynamics in interpreting subtle facial expressions",
      "authors": [
        "Z Ambadar",
        "J Schooler",
        "J Cohn"
      ],
      "year": "2005",
      "venue": "Psychological science"
    },
    {
      "citation_id": "3",
      "title": "Learning vision transformer with squeeze and excitation for facial expression recognition",
      "authors": [
        "M Aouayeb",
        "W Hamidouche",
        "C Soladie",
        "K Kpalma",
        "R Seguier"
      ],
      "year": "2021",
      "venue": "Learning vision transformer with squeeze and excitation for facial expression recognition",
      "arxiv": "arXiv:2107.03107"
    },
    {
      "citation_id": "4",
      "title": "Accelerating neural architecture search using performance prediction",
      "authors": [
        "B Baker",
        "O Gupta",
        "R Raskar",
        "N Naik"
      ],
      "year": "2017",
      "venue": "Accelerating neural architecture search using performance prediction",
      "arxiv": "arXiv:1705.10823"
    },
    {
      "citation_id": "5",
      "title": "Understanding and simplifying one-shot architecture search",
      "authors": [
        "G Bender",
        "P.-J Kindermans",
        "B Zoph",
        "V Vasudevan",
        "Q Le"
      ],
      "year": "2018",
      "venue": "ICML"
    },
    {
      "citation_id": "6",
      "title": "Dynamic image networks for action recognition",
      "authors": [
        "H Bilen",
        "B Fernando",
        "E Gavves",
        "A Vedaldi",
        "S Gould"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "7",
      "title": "Identityfree facial expression recognition using conditional generative adversarial network",
      "authors": [
        "J Cai",
        "Z Meng",
        "A.-S Khan",
        "Z Li",
        "J O'reilly",
        "Y Tong"
      ],
      "year": "2019",
      "venue": "Identityfree facial expression recognition using conditional generative adversarial network"
    },
    {
      "citation_id": "8",
      "title": "Progressive differentiable architecture search: Bridging the depth gap between search and evaluation",
      "authors": [
        "X Chen",
        "L Xie",
        "J Wu",
        "Q Tian"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE International Conference on Computer Vision"
    },
    {
      "citation_id": "9",
      "title": "Progressive differentiable architecture search: Bridging the depth gap between search and evaluation",
      "authors": [
        "X Chen",
        "L Xie",
        "J Wu",
        "Q Tian"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE International Conference on Computer Vision"
    },
    {
      "citation_id": "10",
      "title": "Samm: A spontaneous micro-facial movement dataset",
      "authors": [
        "A Davison",
        "C Lansley",
        "N Costen",
        "K Tan",
        "M Yap"
      ],
      "year": "2016",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "11",
      "title": "Darwin, deception, and facial expression",
      "authors": [
        "P Ekman"
      ],
      "year": "2003",
      "venue": "Annals of the New York Academy of Sciences"
    },
    {
      "citation_id": "12",
      "title": "Constants across cultures in the face and emotion",
      "authors": [
        "P Ekman",
        "W Friesen"
      ],
      "year": "1971",
      "venue": "Journal of personality and social psychology"
    },
    {
      "citation_id": "13",
      "title": "Multi-region ensemble convolutional neural network for facial expression recognition",
      "authors": [
        "Y Fan",
        "J Lam",
        "V Li"
      ],
      "year": "2018",
      "venue": "International Conference on Artificial Neural Networks"
    },
    {
      "citation_id": "14",
      "title": "Facial expression recognition with deeplysupervised attention network",
      "authors": [
        "Y Fan",
        "V Li",
        "J Lam"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "15",
      "title": "Offapexnet on micro-expression recognition system",
      "authors": [
        "Y Gan",
        "S.-T Liong",
        "W.-C Yau",
        "Y.-C Huang",
        "L.-K Tan"
      ],
      "year": "2019",
      "venue": "Signal Processing: Image Communication"
    },
    {
      "citation_id": "16",
      "title": "Local learning with deep and handcrafted features for facial expression recognition",
      "authors": [
        "M.-I Georgescu",
        "R Ionescu",
        "M Popescu"
      ],
      "year": "2018",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "17",
      "title": "Deep learning approaches for facial emotion recognition: A case study on fer-2013",
      "authors": [
        "P Giannopoulos",
        "I Perikos",
        "I Hatzilygeroudis"
      ],
      "year": "2018",
      "venue": "Advances in hybridization of intelligent methods"
    },
    {
      "citation_id": "18",
      "title": "Dynamic texture recognition using local binary patterns with an application to facial expressions",
      "authors": [
        "Z Guoying",
        "M Pietikainen"
      ],
      "year": "2007",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "19",
      "title": "Fuzzy histogram of optical flow orientations for micro-expression recognition",
      "authors": [
        "S Happy",
        "A Routray"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "20",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "K He",
        "X Zhang",
        "S Ren",
        "J Sun"
      ],
      "year": "2015",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "21",
      "title": "Mobilenets: Efficient convolutional neural networks for mobile vision applications",
      "authors": [
        "A Howard",
        "M Zhu",
        "B Chen",
        "D Kalenichenko",
        "W Wang",
        "T Weyand",
        "M Andreetto",
        "H Adam"
      ],
      "year": "2017",
      "venue": "Mobilenets: Efficient convolutional neural networks for mobile vision applications"
    },
    {
      "citation_id": "22",
      "title": "Local learning to improve bag of visual words model for facial expression recognition",
      "authors": [
        "R Ionescu",
        "M Popescu",
        "C Grozea"
      ],
      "year": "2013",
      "venue": "Workshop on challenges in representation learning, ICML"
    },
    {
      "citation_id": "23",
      "title": "Facial expression recognition with active local shape pattern and learned-size block representations",
      "authors": [
        "M Iqbal",
        "B Ryu",
        "A Ramirez Rivera",
        "F Makhmudkhujaev",
        "O Chae",
        "S Bae"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing",
      "doi": "10.1109/TAFFC.2020.2995432"
    },
    {
      "citation_id": "24",
      "title": "Challenges in Representation Learning: Facial Expression Recognition Challenge",
      "authors": [
        "Kaggle",
        "Com"
      ],
      "year": "2019",
      "venue": "Challenges in Representation Learning: Facial Expression Recognition Challenge"
    },
    {
      "citation_id": "25",
      "title": "Dual-stream shallow networks for facial micro-expression recognition",
      "authors": [
        "H.-Q Khor",
        "J See",
        "S.-T Liong",
        "R Phan",
        "W Lin"
      ],
      "year": "2019",
      "venue": "2019 IEEE International Conference on Image Processing"
    },
    {
      "citation_id": "26",
      "title": "Do deep neural networks learn facial action units when doing expression recognition?",
      "authors": [
        "P Khorrami",
        "T Paine",
        "T Huang"
      ],
      "year": "2015",
      "venue": "IEEE International Conference on Computer Vision Workshop (ICCVW)"
    },
    {
      "citation_id": "27",
      "title": "Multi-objective based spatio-temporal feature representation learning robust to expression intensity variations for facial expression recognition",
      "authors": [
        "D Kim",
        "W Baddar",
        "J Jang",
        "Y Ro"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "28",
      "title": "Facial expression recognition based on squeeze vision transformer",
      "authors": [
        "S Kim",
        "J Nam",
        "B Ko"
      ],
      "year": "2022",
      "venue": "Sensors"
    },
    {
      "citation_id": "29",
      "title": "A compact deep learning model for robust facial expression recognition",
      "authors": [
        "C.-M Kuo",
        "S.-H Lai",
        "M Sarkis"
      ],
      "year": "2018",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition workshops"
    },
    {
      "citation_id": "30",
      "title": "Micro-expression recognition based on 3d flow convolutional neural network",
      "authors": [
        "J Li",
        "Y Wang",
        "J See",
        "W Liu"
      ],
      "year": "2019",
      "venue": "Pattern Analysis and Applications"
    },
    {
      "citation_id": "31",
      "title": "Deep facial expression recognition: A survey",
      "authors": [
        "S Li",
        "W Deng"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "32",
      "title": "A deeper look at facial expression dataset bias",
      "authors": [
        "S Li",
        "W Deng"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing",
      "doi": "10.1109/TAFFC.2020.2973158"
    },
    {
      "citation_id": "33",
      "title": "Reliable crowdsourcing and deep localitypreserving learning for expression recognition in the wild",
      "authors": [
        "S Li",
        "W Deng",
        "J Du"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "34",
      "title": "Autofernet: A facial expression recognition network with architecture search",
      "authors": [
        "S Li",
        "W Li",
        "S Wen",
        "K Shi",
        "Y Yang",
        "P Zhou",
        "T Huang"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Network Science and Engineering"
    },
    {
      "citation_id": "35",
      "title": "A spontaneous micro-expression database: Inducement, collection and baseline",
      "authors": [
        "X Li",
        "T Pfister",
        "X Huang",
        "G Zhao",
        "M Pietikäinen"
      ],
      "year": "2013",
      "venue": "2013 10th IEEE International Conference and Workshops on Automatic face and gesture recognition (fg)"
    },
    {
      "citation_id": "36",
      "title": "Joint local and global information learning with single apex frame detection for micro-expression recognition",
      "authors": [
        "Y Li",
        "X Huang",
        "G Zhao"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Image Processing"
    },
    {
      "citation_id": "37",
      "title": "Facial expression recognition in the wild using multi-level features and attention mechanisms",
      "authors": [
        "Y Li",
        "G Lu",
        "J Li",
        "Z Zhang",
        "D Zhang"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "38",
      "title": "Occlusion aware facial expression recognition using cnn with attention mechanism",
      "authors": [
        "Y Li",
        "J Zeng",
        "S Shan",
        "X Chen"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Image Processing"
    },
    {
      "citation_id": "39",
      "title": "Shallow triple stream three-dimensional cnn (ststnet) for micro-expression recognition",
      "authors": [
        "S.-T Liong",
        "Y Gan",
        "J See",
        "H.-Q Khor",
        "Y.-C Huang"
      ],
      "year": "2019",
      "venue": "2019 14th IEEE International Conference on Automatic Face & Gesture Recognition"
    },
    {
      "citation_id": "40",
      "title": "Shallow triple stream three-dimensional cnn (ststnet) for micro-expression recognition",
      "authors": [
        "S.-T Liong",
        "Y Gan",
        "H.-Q John See",
        "Y.-C Huang"
      ],
      "year": "2019",
      "venue": "Gesture Recognition"
    },
    {
      "citation_id": "41",
      "title": "Auto-deeplab: Hierarchical neural architecture search for semantic image segmentation",
      "authors": [
        "C Liu",
        "L.-C Chen",
        "F Schroff",
        "H Adam",
        "W Hua",
        "A Yuille",
        "L Fei-Fei"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "42",
      "title": "Progressive neural architecture search",
      "authors": [
        "C Liu",
        "B Zoph",
        "M Neumann",
        "J Shlens",
        "W Hua",
        "L.-J Li",
        "L Fei-Fei",
        "A Yuille",
        "J Huang",
        "K Murphy"
      ],
      "year": "2018",
      "venue": "Proceedings of the European Conference on Computer Vision (ECCV)"
    },
    {
      "citation_id": "43",
      "title": "Darts: Differentiable architecture search",
      "authors": [
        "H Liu",
        "K Simonyan",
        "Y Yang"
      ],
      "year": "2018",
      "venue": "Darts: Differentiable architecture search",
      "arxiv": "arXiv:1806.09055"
    },
    {
      "citation_id": "44",
      "title": "Facial expression recognition with cnn ensemble",
      "authors": [
        "K Liu",
        "M Zhang",
        "Z Pan"
      ],
      "year": "2016",
      "venue": "International Conference on Cyberworlds (CW)"
    },
    {
      "citation_id": "45",
      "title": "Facial expression recognition via a boosted deep belief network",
      "authors": [
        "P Liu",
        "S Han",
        "Z Meng",
        "Y Tong"
      ],
      "year": "2014",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "46",
      "title": "A neural micro-expression recognizer",
      "authors": [
        "Y Liu",
        "H Du",
        "L Zheng",
        "T Gedeon"
      ],
      "year": "2019",
      "venue": "2019 14th IEEE international conference on automatic face & gesture recognition"
    },
    {
      "citation_id": "47",
      "title": "Sparse mdmo: Learning a discriminative feature for spontaneous micro-expression recognition",
      "authors": [
        "Y.-J Liu",
        "B.-J Li",
        "Y.-K Lai"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "48",
      "title": "Facial expression recognition with convolutional neural networks: Coping with few data and the training sample order",
      "authors": [
        "A Lopes",
        "E De Aguiar",
        "A De Souza",
        "T Oliveira-Santos"
      ],
      "year": "2017",
      "venue": "Pattern Recognition"
    },
    {
      "citation_id": "49",
      "title": "The extended cohn-kanade dataset (ck+): A complete dataset for action unit and emotion-specified expression",
      "authors": [
        "P Lucey",
        "J Cohn",
        "T Kanade",
        "J Saragih",
        "Z Ambadar",
        "I Matthews"
      ],
      "year": "2010",
      "venue": "2010 ieee computer society conference on computer vision and pattern recognition-workshops"
    },
    {
      "citation_id": "50",
      "title": "Regional adaptive affinitive patterns (radap) with logical operators for facial expression recognition",
      "authors": [
        "M Mandal",
        "M Verma",
        "S Mathur",
        "S Vipparthi",
        "S Murala",
        "K Deveerasetty"
      ],
      "year": "2019",
      "venue": "IET Image Processing"
    },
    {
      "citation_id": "51",
      "title": "Disfa: A spontaneous facial action intensity database",
      "authors": [
        "S Mavadati",
        "M Mahoor",
        "K Bartlett",
        "P Trinh",
        "J Cohn"
      ],
      "year": "2013",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "52",
      "title": "Going deeper in facial expression recognition using deep neural networks",
      "authors": [
        "A Mollahosseini",
        "D Chan",
        "M Mahoor"
      ],
      "year": "2016",
      "venue": "IEEE Winter Conference on Applications of Computer Vision (WACV)"
    },
    {
      "citation_id": "53",
      "title": "Deeparchitect: Automatically designing and training deep architectures",
      "authors": [
        "R Negrinho",
        "G Gordon"
      ],
      "year": "2017",
      "venue": "Deeparchitect: Automatically designing and training deep architectures",
      "arxiv": "arXiv:1704.08792"
    },
    {
      "citation_id": "54",
      "title": "A comparison between shallow and deep architecture classifiers on small dataset",
      "authors": [
        "K Pasupa",
        "W Sunhem"
      ],
      "year": "2016",
      "venue": "2016 8th International Conference on Information Technology and Electrical Engineering (ICITEE)"
    },
    {
      "citation_id": "55",
      "title": "Supervised committee of convolutional neural networks in automated facial expression analysis",
      "authors": [
        "G Pons",
        "D Masip"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "56",
      "title": "Cas(me) 2: A database for spontaneous macro-expression and micro-expression spotting and recognition",
      "authors": [
        "F Qu",
        "S.-J Wang",
        "W.-J Yan",
        "H Li",
        "S Wu",
        "X Fu"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "57",
      "title": "Regularized evolution for image classifier architecture search",
      "authors": [
        "E Real",
        "A Aggarwal",
        "Y Huang",
        "Q Le"
      ],
      "year": "2019",
      "venue": "Proceedings of the aaai conference on artificial intelligence"
    },
    {
      "citation_id": "58",
      "title": "Spontaneous facial micro-expression recognition using 3d spatiotemporal convolutional neural networks",
      "year": "2019",
      "venue": "2019 International Joint Conference on Neural Networks (IJCNN)"
    },
    {
      "citation_id": "59",
      "title": "The indian spontaneous expression database for emotion recognition",
      "authors": [
        "S Happy",
        "Priyadarshi Patnaik"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "60",
      "title": "Megc 2019the second facial micro-expressions grand challenge",
      "authors": [
        "J See",
        "M Yap",
        "J Li",
        "X Hong",
        "S.-J Wang"
      ],
      "year": "2019",
      "venue": "Gesture Recognition"
    },
    {
      "citation_id": "61",
      "title": "Very deep convolutional networks for large-scale image recognition",
      "authors": [
        "K Simonyan",
        "A Zisserman"
      ],
      "year": "2014",
      "venue": "Very deep convolutional networks for large-scale image recognition"
    },
    {
      "citation_id": "62",
      "title": "Recognizing spontaneous micro-expression using a three-stream convolutional neural network",
      "authors": [
        "B Song",
        "K Li",
        "Y Zong",
        "J Zhu",
        "W Zheng",
        "J Shi",
        "L Zhao"
      ],
      "year": "2019",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "63",
      "title": "Training very deep networks",
      "authors": [
        "R Srivastava",
        "K Greff",
        "J Schmidhuber"
      ],
      "year": "2015",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "64",
      "title": "Dynamic micro-expression recognition using knowledge distillation",
      "authors": [
        "B Sun",
        "S Cao",
        "D Li",
        "J He",
        "L Yu"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing",
      "doi": "10.1109/TAFFC.2020.2986962"
    },
    {
      "citation_id": "65",
      "title": "Capsulenet for microexpression recognition",
      "authors": [
        "N Van Quang",
        "J Chun",
        "T Tokuyama"
      ],
      "year": "2019",
      "venue": "2019 14th IEEE International Conference on Automatic Face & Gesture Recognition"
    },
    {
      "citation_id": "66",
      "title": "Automer: Spatiotemporal neural architecture search for microexpression recognition",
      "authors": [
        "M Verma",
        "M Reddy",
        "Y Meedimale",
        "M Mandal",
        "S Vipparthi"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Neural Networks and Learning Systems"
    },
    {
      "citation_id": "67",
      "title": "Hinet: Hybrid inherited feature learning network for facial expression recognition",
      "authors": [
        "M Verma",
        "S Vipparthi",
        "G Singh"
      ],
      "year": "2019",
      "venue": "IEEE Letters of the Computer Society"
    },
    {
      "citation_id": "68",
      "title": "Affectivenet: Affectivemotion feature learning for micro expression recognition",
      "authors": [
        "M Verma",
        "S Vipparthi",
        "G Singh"
      ],
      "year": "2020",
      "venue": "Affectivenet: Affectivemotion feature learning for micro expression recognition",
      "doi": "10.1109/MMUL.2020.3021659"
    },
    {
      "citation_id": "69",
      "title": "Non-linearities improve originet based on active imaging for micro expression recognition",
      "authors": [
        "M Verma",
        "S Vipparthi",
        "G Singh"
      ],
      "year": "2020",
      "venue": "2020 International Joint Conference on Neural Networks (IJCNN)",
      "doi": "10.1109/IJCNN48605.2020.9207718"
    },
    {
      "citation_id": "70",
      "title": "Learnet: Dynamic imaging network for micro expression recognition",
      "authors": [
        "M Verma",
        "S Vipparthi",
        "G Singh",
        "S Murala"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Image Processing"
    },
    {
      "citation_id": "71",
      "title": "a). Micro-attention for microexpression recognition",
      "authors": [
        "C Wang",
        "M Peng",
        "T Bi",
        "T Chen"
      ],
      "year": "2018",
      "venue": "a). Micro-attention for microexpression recognition",
      "arxiv": "arXiv:1811.02360"
    },
    {
      "citation_id": "72",
      "title": "Micro-attention for microexpression recognition",
      "authors": [
        "C Wang",
        "M Peng",
        "T Bi",
        "T Chen"
      ],
      "year": "2020",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "73",
      "title": "Suppressing uncertainties for large-scale facial expression recognition",
      "authors": [
        "K Wang",
        "X Peng",
        "J Yang",
        "S Lu",
        "Y Qiao"
      ],
      "year": "2020",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "74",
      "title": "Micro-expression recognition with small sample size by transferring long-term convolutional neural network",
      "authors": [
        "S.-J Wang",
        "B.-J Li",
        "Y.-J Liu",
        "W.-J Yan",
        "X Ou",
        "X Huang",
        "F Xu",
        "X Fu"
      ],
      "year": "2018",
      "venue": "In Neurocomputing"
    },
    {
      "citation_id": "75",
      "title": "A fine-grained facial expression database for end-to-end multi-pose facial expression recognition",
      "authors": [
        "W Wang",
        "Q Sun",
        "T Chen",
        "C Cao",
        "Z Zheng",
        "G Xu",
        "H Qiu",
        "Y Fu"
      ],
      "year": "2019",
      "venue": "A fine-grained facial expression database for end-to-end multi-pose facial expression recognition"
    },
    {
      "citation_id": "76",
      "title": "Distract your attention: multi-head cross attention network for facial expression recognition",
      "authors": [
        "Z Wen",
        "W Lin",
        "T Wang",
        "G Xu"
      ],
      "year": "2021",
      "venue": "Distract your attention: multi-head cross attention network for facial expression recognition",
      "arxiv": "arXiv:2109.07270"
    },
    {
      "citation_id": "77",
      "title": "Cross-database microexpression recognition with deep convolutional networks",
      "authors": [
        "Z Xia",
        "H Liang",
        "X Hong",
        "X Feng"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 3rd International Conference on Biometric Engineering and Applications"
    },
    {
      "citation_id": "78",
      "title": "Revealing the invisible with model and data shrinking for composite-database microexpression recognition",
      "authors": [
        "Z Xia",
        "W Peng",
        "H.-Q Khor",
        "X Feng",
        "G Zhao"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Image Processing"
    },
    {
      "citation_id": "79",
      "title": "Facial expression recognition using hierarchical features with deep comprehensive multipatches aggregation convolutional neural networks",
      "authors": [
        "S Xie",
        "H Hu"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "80",
      "title": "Adversarial graph representation adaptation for cross-domain facial expression recognition",
      "authors": [
        "Y Xie",
        "T Chen",
        "T Pu",
        "H Wu",
        "L Lin"
      ],
      "year": "2020",
      "venue": "Proceedings of the 28th ACM international conference on Multimedia"
    },
    {
      "citation_id": "81",
      "title": "Microexpression identification and categorization using a facial dynamics map",
      "authors": [
        "F Xu",
        "J Zhang",
        "J Wang"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "82",
      "title": "Casme ii: An improved spontaneous micro-expression database and the baseline evaluation",
      "authors": [
        "W.-J Yan",
        "X Li",
        "S.-J Wang",
        "G Zhao",
        "Y.-J Liu",
        "Y.-H Chen",
        "X Fu"
      ],
      "year": "2014",
      "venue": "PloS one"
    },
    {
      "citation_id": "83",
      "title": "Casme database: a dataset of spontaneous micro-expressions collected from neutralized faces",
      "authors": [
        "W.-J Yan",
        "Q Wu",
        "Y.-J Liu",
        "S.-J Wang",
        "X Fu"
      ],
      "year": "2013",
      "venue": "2013 10th IEEE international conference and workshops on automatic face and gesture recognition (FG)"
    },
    {
      "citation_id": "84",
      "title": "Merta: microexpression recognition with ternary attentions",
      "authors": [
        "B Yang",
        "J Cheng",
        "Y Yang",
        "B Zhang",
        "J Li"
      ],
      "year": "2021",
      "venue": "Multimedia Tools and Applications"
    },
    {
      "citation_id": "85",
      "title": "Identity-adaptive facial expression recognition through expression regeneration using conditional generative adversarial networks",
      "authors": [
        "H Yang",
        "Z Zhang",
        "L Yin"
      ],
      "year": "2018",
      "venue": "th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2018)"
    },
    {
      "citation_id": "86",
      "title": "A main directional mean optical flow feature for spontaneous micro-expression recognition",
      "authors": [
        "L Yong-Jin",
        "Z Jin-Kai",
        "Y Wen-Jing",
        "W Su-Jing",
        "Z Guoying",
        "F Xiaolan"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "87",
      "title": "Auto-fas: Searching lightweight networks for face anti-spoofing",
      "authors": [
        "Z Yu",
        "Y Qin",
        "X Xu",
        "C Zhao",
        "Z Wang",
        "Z Lei",
        "G Zhao"
      ],
      "year": "2020",
      "venue": "ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "88",
      "title": "Nas-fas: Staticdynamic central difference network search for face anti-spoofing",
      "authors": [
        "Z Yu",
        "J Wan",
        "Y Qin",
        "X Li",
        "S Li",
        "G Zhao"
      ],
      "year": "2020",
      "venue": "Nas-fas: Staticdynamic central difference network search for face anti-spoofing",
      "arxiv": "arXiv:2011.02062"
    },
    {
      "citation_id": "89",
      "title": "Searching central difference convolutional networks for face anti-spoofing",
      "authors": [
        "Z Yu",
        "C Zhao",
        "Z Wang",
        "Y Qin",
        "Z Su",
        "X Li",
        "F Zhou",
        "G Zhao"
      ],
      "year": "2020",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "90",
      "title": "A deep neural network-driven feature learning method for multi-view facial expression recognition",
      "authors": [
        "T Zhang",
        "W Zheng",
        "Z Cui",
        "Y Zong",
        "J Yan",
        "K Yan"
      ],
      "year": "2016",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "91",
      "title": "Cross-database micro-expression recognition: A benchmark",
      "authors": [
        "T Zhang",
        "Y Zong",
        "W Zheng",
        "C Chen",
        "X Hong",
        "C Tang",
        "Z Cui",
        "G Zhao"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Knowledge and Data Engineering",
      "doi": "10.1109/TKDE.2020.2985365"
    },
    {
      "citation_id": "92",
      "title": "Facial expression recognition from near-infrared videos",
      "authors": [
        "G Zhao",
        "X Huang",
        "M Taini",
        "S Li",
        "M Pietikäinen"
      ],
      "year": "2011",
      "venue": "Image and Vision Computing"
    },
    {
      "citation_id": "93",
      "title": "Feature selection mechanism in cnns for facial expression recognition",
      "authors": [
        "S Zhao",
        "H Cai",
        "H Liu",
        "J Zhang",
        "S Chen"
      ],
      "year": "2018",
      "venue": "BMVC"
    },
    {
      "citation_id": "94",
      "title": "Poster: A pyramid crossfusion transformer network for facial expression recognition",
      "authors": [
        "C Zheng",
        "M Mendieta",
        "C Chen"
      ],
      "year": "2022",
      "venue": "Poster: A pyramid crossfusion transformer network for facial expression recognition",
      "arxiv": "arXiv:2204.04083"
    },
    {
      "citation_id": "95",
      "title": "Dual-inception network for cross-database micro-expression recognition",
      "authors": [
        "L Zhou",
        "Q Mao",
        "L Xue"
      ],
      "year": "2019",
      "venue": "2019 14th IEEE International Conference on Automatic Face & Gesture Recognition"
    },
    {
      "citation_id": "96",
      "title": "A survey of micro-expression recognition",
      "authors": [
        "L Zhou",
        "X Shao",
        "Q Mao"
      ],
      "year": "2020",
      "venue": "Image and Vision Computing"
    },
    {
      "citation_id": "97",
      "title": "Learning transferable architectures for scalable image recognition",
      "authors": [
        "B Zoph",
        "V Vasudevan",
        "J Shlens",
        "Q Le"
      ],
      "year": "2018",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    }
  ]
}