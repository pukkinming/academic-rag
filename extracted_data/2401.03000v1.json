{
  "paper_id": "2401.03000v1",
  "title": "Bridging Modalities: Knowledge Distillation And Masked Training For Translating Multi-Modal Emotion Recognition To Uni-Modal, Speech-Only Emotion Recognition",
  "published": "2024-01-04T22:42:14Z",
  "authors": [
    "Muhammad Muaz",
    "Nathan Paull",
    "Jahnavi Malagavalli"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "This paper presents an innovative approach to address the challenges of translating multi-modal emotion recognition models to a more practical and resource-efficient uni-modal counterpart, specifically focusing on speech-only emotion recognition. Recognizing emotions from speech signals is a critical task with applications in human-computer interaction, affective computing, and mental health assessment. However, existing state-of-the-art models often rely on multi-modal inputs, incorporating information from multiple sources such as facial expressions and gestures, which may not be readily available or feasible in real-world scenarios. To tackle this issue, we propose a novel framework that leverages knowledge distillation and masked training techniques.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Emotion recognition, a pivotal aspect of human-computer interaction and affective computing, has witnessed remarkable progress with the advent of multi-modal models that incorporate information from diverse sources such as facial expressions, gestures, and speech. However, the practical deployment of such sophisticated models is often hindered by the inherent complexity and resource demands associated with processing multiple modalities. In real-world scenarios, obtaining comprehensive multi-modal data may be challenging or impractical, limiting the applicability of these advanced models.\n\nThis paper addresses the gap between the theoretical advancements in multi-modal emotion recognition and the practical constraints of real-world applications, with a specific focus on translating these models to a more accessible and efficient uni-modal counterpart-speech-only emotion recognition. Recognizing emotions from speech is crucial in numerous domains, including human-computer interfaces, virtual assistants, and mental health assessment. Our objective is to develop a novel framework that preserves the rich understanding encapsulated in multi-modal models while tailoring it to the constraints and requirements of a speech-only context.\n\nTo achieve this, we explore two independent strategies, leveraging knowledge distillation and masked training. Knowledge distillation allows the transfer of nuanced information from a teacher model, originally trained on multi-modal inputs, to a more streamlined student model designed for uni-modal, speech-only emotion recognition. Concurrently, we introduce masked training, a novel training approach designed to reinforce the model's ability to generalize to incomplete inputs, while providing grounding from other modalities. Additionally, we investigate the impact of deep learning-generated audio feature embeddings on the introduced models.\n\nThrough a series of experiments, we evaluate the effectiveness of our proposed framework in bridging the gap between multi-modal and uni-modal emotion recognition. The results not only showcase an increased performance of multi-modal models but also demonstrate the competitive capabilities of our proposed models in the challenging domain of speech-only emotion recognition. This work contributes to the ongoing dialogue in the field, emphasizing the need for practical and efficient emotion recognition systems tailored for real-world applications where only speech-related information is readily available.\n\nAll code for this work can be found at: https://github.com/m-muaz/Cogmen_SLT",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Related Work",
      "text": "In the realm of multimodal sentiment analysis, recent research has explored various facets. Addressing cross-modal interaction learning, long-term dependencies in multimodal interactions, and the fusion of unimodal and cross-modal cues,  [1]  find that learning cross-modal interactions significantly benefits the task. Their experiments on CMU Multimodal Opinion level Sentiment Intensity (CMU-MOSI) and CMU Multimodal Opinion Sentiment and Emotion Intensity (CMU-MOSEI) datasets demonstrate accuracies of 83.9% and 81.1%, respectively, showcasing a noteworthy improvement over the current state-of-the-art. Another approach, presented by  [2] , focuses on bimodal emotion recognition using face and speech analysis, leveraging Hidden Markov Models (HMMs) for temporal dynamics and introducing novel methods for feature selection. Additionally,  [3]  contribute a Transformer-based joint-encoding (TBJE) for Emotion Recognition and Sentiment Analysis, incorporating modular co-attention and a glimpse layer for multimodal encoding.  [4]  propose a graph-based neural network for conversational emotion analysis, modeling context-sensitive and speaker-sensitive dependences, with an absolute improvement of 1%-2% over state-of-the-art strategies demonstrated on the MELD dataset. Lastly,  [5]  introduces COGMEN, a Contextualized Graph Neural Network for Multimodal Emotion Recognition, leveraging local and global information in conversations to achieve stateof-the-art results on IEMOCAP and MOSEI datasets. These works collectively showcase diverse strategies and advancements in multimodal sentiment analysis and emotion recognition, emphasizing the importance of cross-modal interactions, dynamic modeling, joint-encoding, and graph-based approaches.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Approach",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Dataset",
      "text": "In this work, we leverage the IEMOCAP4 dataset, a rich and diverse corpus that encompasses speech, video, and text features, facilitating a holistic understanding of emotional expressions. The dataset is specifically curated to recognize four distinct emotion classes: neutral, happy, sad, and angry.\n\nOur focus within the IEMOCAP4 dataset is on exploiting the speech and video modalities, mirroring the sensory perception of human emotions. Restricting our analysis to these modalities aligns with the practical constraints of real-world applications, where obtaining text-based features may not always be feasible. The decision to utilize speech and video features allows us to create a more applicable and resource-efficient model for scenarios where only these modalities are readily available.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Model Architecture",
      "text": "To benchmark the performance of our proposed framework for translating multi-modal emotion recognition to uni-modal, speech-only emotion recognition, we integrate the state-of-the-art COGMEN architecture as shown in Figure  1 . The COGMEN architecture stands as the pinnacle in multi-modal emotion recognition on the IEMOCAP4 dataset, incorporating three key components that capture and exploit the inter-modal dependencies inherent in emotional expressions. The first component is a transformer-based feature extractor used to capture complex intra-utterance dependencies. This component is instrumental in extracting high-level representations from the input data, enabling the model to discern nuanced patterns associated with different emotional states.\n\nThe second component is a GNN, where utterances are connected to their i-precursor and j-successor utterances in time, forming a temporal graph that encapsulates the dynamic evolution of emotions over a length of conversation. This enables the model to leverage the temporal context and relationships between utterances, facilitating a more holistic understanding of emotional dynamics. In our experiments and in the original COGMEN paper i and j are both set to 5.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Feature Embeddings",
      "text": "COGMEN utilizes the OpenSmile toolkit  [6]  to extract audio features (size 100). This toolkit integrates algorithms from speech processing and Music Information Retrieval, employing classic feature extractors such as the Mel-band filter-bank. In our project, we investigate the impact of employing deep learning feature extractors like HuBERT  [7]  on the predictions of the base COGMEN, Masked COGMEN, and Knowledge Distillation COGMEN models. HuBERT features are extracted from its second-to-last layer, preceding the classification layer (size 1024 after applying mean over dim = 1). Given the input audio feature size of 100, a linear projection is applied to reduce the HuBERT output feature dimension to 100. Despite these generated features, the models exhibited poor performance, as detailed in the appendix. A potential reason for this may be the linear projection layer altering the feature values. Further exploration is needed to draw conclusive results.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Knowledge Distillation",
      "text": "The SOTA performance of COGMEN  [5]  architecture showcases that multi-modality indeed outperforms uni-modal approaches which is evident from the fact that in nature humans and other living beings utilize information received from multiple senses. However, incorporating multiple inputs comes with additional computational cost. Even when resources are abundant, having resourceefficient networks means more requests can be served at a lower cost. Along these lines, we try to answer the question: Can we get a speech-only emotion recognition model while still maintaining the performance of the larger and efficient model? To answer this, we focused along the domain of knowledge distillation  [8] . Knowledge distillation is a general technique for supervising the training of \"student\" neural networks by capturing and transferring the knowledge of trained \"teacher\" networks (In our context, \"teacher\" model refers to audio-visual input COGMEN model and \"student\" model refers to audio only COGMEN model). The trained teacher network provides additional semantic knowledge besides the usual data supervision. However, the question of how to encoder and utilize the teacher's knowledge such that student's performance is maximized.\n\nFor this, we hypothesize that supervision of multiple modalities (audio, video, text, etc.) may have generated more rich embedding distributions/representations 1  . So, it would be beneficial if we can use that rich embedding space to guide the student network. In order to generate the supervision signal for the student model, we utilize triplet loss. Formally, let X, P and N be the student model embedding representation , teacher model embedding representation for positive sample, and teacher embedding for negative sample, respectively. The loss function can then be described by means of p-norm:\n\nL triplet (X, P, N ) = max(∥f (X) -g(P )∥ p -∥f (X) -g(N )∥ p + α, 0) where α is a margin between positive and negative pairs and f and g be the student and teacher embedding networks. For our experiments, α has been set to 1 and p to 2.\n\nIn addition to the signal from multi-class cross entropy signal to force the model to accurately predict emotions, the triplet loss will generate the guidance signal for the student to mimic the embeddings learnt by the teacher network. Therefore, the student model is trained to optimize the following multi-term loss:\n\nwhere MC-CE refers to multi-class cross entropy loss. For our experiments, we kept both alpha parameters to 1 and it seems to be working for our experiments. However, future works could explore the impact of different parameters values either via manual value setting or making these parameters learnable.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Masked Training",
      "text": "In the pursuit of adapting the COGMEN architecture to a uni-modal, speech-only emotion recognition context, our proposed framework incorporates a novel training strategy known as masked training. This technique aims to refine the model's attention mechanisms specifically for speech-related features while maintaining performance on multiple modalities. Our masked training routine stochastically chooses between four distinct masking scenarios. As shown in the figure above these four masking scenarios are non-masking, fully masked audio, fully masked video, and randomly masked samples. In our exploration we found that choosing non-masking with a probability of 0.1, fully masking audio or video each with a probability of 0.3, and then with a probability of 0.3, choosing to randomly mask samples. In the case of randomly masking samples, each sample is chosen as the start of a mask with probability of 0.2 and the masks are of length 1 for both audio and video. It is worth noting that the probability of choosing the random sample masking is the probability of choosing this masking strategy, which is different from the probability of masking each sample if this masking strategy is chosen.\n\nThis method provides a large number of hyper-parameters to explore such as the probability of choosing each masking method and the length of sample masks for each modality. While we believe we have found a high performing set of hyper-parameters, we do not exclude the possibility that other hyper-parameters made lead to better results, however, most other hyper-parameter choices that we explored led to worse results.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Results",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Knowledge Distillation",
      "text": "Our teacher model and student model will be referred to as COGMEN T -AV and COGMEN S -A respectively to indicate that teacher model is the multi-modal COGMEN model while student model is the uni-modal network. The above table  1  shows the F1 metric scores of both teacher and student models during inference.\n\nSeqContext means the number of transformer layers used in the Context Extractor module of the COGMEN model while, GNN refers to the number of transformer layers in the GraphTransformer module of COGMEN. The first row shows that if both teacher and student model are using the same architecture then, the student can achieve similar performance while only relying on speech input modality. Moreover, as show in the Figure  3  , it can be seen that the distribution learnt by student model closely resembles the distribution of its teacher. Among all the emotions, the student model is more uncertain during the prediction of sad emotion.\n\nThe second corresponds to the ablation experiment aimed to analyze whether or not the student model show robustness when number of transformer layers, responsible for extracting contextual information, are reduced. Moreover, the third row corresponds to the experiment focused to analyze the impact of reducing number of transformer layers in the graph transformer module. From the results, it can clearly seen that reducing the number of transformer layers in the context extractor (Initial stage) of student model have a significant impact as compared to the variation in number of layer in the Graph Transformer (later stage). However, the student model is pretty robust against both variations as we only see a marginal drop of ≈ 0.04 and ≈ 0.02. These results also show that the initial stage of context extractor plays a significant role in model' performance which is intuitive as if the model extracts good contextual representations from input then it can help the model generate better representations during the intermediate and later stage of the model. All in all, it can be concluded that the uni-modal student model effectively achieves the same performance as of its teacher while also showing robustness to the variation in different aspects of the underlying model.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Masked Training",
      "text": "Our masked training adjusted COGMEN model will be called COGMEN-Mask, which we will compare to an audio-only COGMEN model (COGMEN-A) and an audio+video COGMEN model that will be tested in an audio-only context (COGMEN-AV). As shown, the COGMEN-AV model sees a large decrease in performance when directly applied to the audio-only context with an F1 decrease of over 0.2 and an audio-only performance far lower than the COGMEN-A model. Additionally, we see that our model, COGMEN-Mask, not only is able to mitigate this F1 loss, seeing a performance decrease of approximately 0.02, but is able to outperform both the COGMEN-AV and COGMEN-A models in both inference contexts. With additional adjustments to the COGMEN model regarding the number of sequential layers and GNN heads, we have seen audio-only performance increase up to 0.674, however, this gave an audiovideo score of 0.622, performing worse when given the additional video modality. We chose not to incorporate this score in the table as audio-only outperforming audio-video implies that the model is not generalizing as well as the model recorded in the table. This loss of generalization appears to be caused by an increase in the number of sequential layers from the default of two. Because COGMEN-Mask was more effective in audio-video emotion recognition we chose to explore using COGMEN-Mask as our teacher model in a knowledge distillation training setup. The student model from this setup was able to achieve an audio-only score of 0.650, which while less that the COGMEN-Mask model, implies that the knowledge distillation setup is likely to benefit from any performance gains found in the teacher model.\n\nThe last question regarding masked training that we seek to answer in this paper is whether the COGMEN-A model is truly an adequate baseline for the COGMEN-Mask model, i.e. would a uni-modal version of our masked training routine create similar benefits for the COGMEN-A model.\n\nOur observations when running a uni-modal adaptation of the masked training on the COGMEN-A model yielded better results than the default COGMEN-A model, with the best F1 score we saw being 0.654, however, this still fails to exceed the performance of the COGMEN-Mask model and required adjustments to the number of sequential layers and GNN heads in the COGMEN model, thus requiring greater hyper-parameter search.\n\nOverall, these results show that COGMEN-Mask not only achieves better translation from multi-modal to uni-modal contexts, but additionally seems to gain some knowledge grounding by being trained in a multi-modal context. Additionally, we ran experiments to discover how valuable COGMEN's GNN module is to the observed masked training performance increase. Our findings indicate that random input masking is able to provide performance increases in both audio-only and audio-video contexts without the inclusion of a GNN, suggesting that this training method will generalize across model architectures. For full results see: Table  3 .",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Conclusion",
      "text": "We found that both knowledge distillation and masked training are effective avenues for preventing performance drop-off when translating from multi-modal to speech-only emotion recognition. The masked training not only mitigated performance drop-off but even improved over both the audiovideo and audio-only benchmarks, this came at the cost of greater hyper-parameter tuning. This hyper-parameter space will continue to increase as the number of modalities increases which may make this method intractable for models with a high number of modalities or long training times. The knowledge distillation approach does not have this modality dependent hyper-parameter growth and instead only adds two hyper-parameters to control the balance between predictive and contrastive loss, which may ultimately make it the more desirable option between the two approaches discussed in this paper.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Future Work",
      "text": "The baseline COGMEN model that we have built our work on top of mainly achieves its SOTA multi-modal emotion recognition performance from the text modality as compared to the speech or vision modalities. This is rather interesting as humans often experience the opposite, struggling to recognize emotion in a text-only modality while excelling in any context where speech and/or vision are present. The audio features used in our most successful experiments came from OpenSmile with features coming from learned feature extractors such as HuBERT and other Transformers showing a decrease in performance. It is possible that SSMs  [9, 10, 11] , being designed to learn on high frame-rate/continuous data, may provide a reduction in the human-machine modality preference gap.\n\nAltering the COGMEN model so that emotion recognition is done in a sequential or even real-time manner is another place for future research. The current graph used by the GNN uses both past and future utterances for emotion extraction. Limiting all edges in the graph such that an utterance can only predict using itself an prior utterances would bring this model closer to real-time emotion recognition. Exploration of masked training and knowledge distillation in this domain may similarly increase performance of these models. Similarly, using a knowledge distillation setup, one could use a bi-directional COGMEN teacher to instruct a sequential COGMEN student.\n\nThe COGMEN model is an intermediate fusion model, performing independent feature embeddings for each modality before they are fused in the transformer layer. The level at which this fusion occurs should not aggressively affect the knowledge distillation approach, however, there is a chance that masked training adjusted for early or late modality fusion may see results that are quite different.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Appendices A Input Masked Emotion Recognition Without A Gnn",
      "text": "At the end of our COGMEN-Mask experiments we were left with a lingering question, \"How important is the GNN to the performance of the COGMEN-Mask model?\" Because the input masking is stochastic, it is unlikely that an utterance will share the same input mask as its neighboring utterances in the past and future, as such it is possible that the GNN is vital to the performance gain from random input masking. This meant that experiments that removed the GNN component would be key to understanding the effectiveness of this input masking method across varied underlying models.\n\nWithout using a completely new model, we decided to use the COGMEN model, but adjusted the utterance graph to be completely disjoint, such that all utterances were only connected to themselves. This makes the COGMEN GNN a simple feed-forward layer that does not rely on any external information. After making this adjustment we re-ran the masking experiments and listed our results in the table below. These experiments clearly showed how the masked input method is highly performant even without the underlying GNN, thus implying that it is likely to improve performance across all model classes. It is worth noting that the COGMEN-AV results appear to be an outlier compared to all other results. We are unsure why this is, however, it is possible that the extra modalities provide too much noise to the learning process and without normalization from a GNN or randomly masked input, the model will fail to converge.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "B Feature Embeddings Results And Analysis",
      "text": "The HuBERT-generated audio features passed to the base, Masked, and Knowledge Distillation COGMEN models resulted in the following F1 scores. Table  4  presents the results of the original COGMEN model when trained and tested with 'a' and 'av' modalities, respectively, using COGMENgenerated audio features and HuBERT audio features. Please note that some data points had to be removed from the COGMEN features as they were not present in Kaggle's IECOMAP dataset. This reduction in data points might be another reason for the reduction in the F2 scores compared to the COGMEN A and COGMEN AV models results in Table  2 .   4  and 5 , the F1 scores of the models trained on COGMEN audio features are higher than those of HuBERT audio features. This might be because the HuBERT features were passed to a linear projection layer, which changes the actual feature values.\n\nAnother reason for these poor results might be the hyperparameter settings. The hyperparameters used were the same as those used in each of the three models described in the paper. Confusion matrices in Figures  5  and 6  for these models with HuBERT features on the test dataset are not good due to the aforementioned reasons.",
      "page_start": 9,
      "page_end": 9
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The COGMEN architecture stands as the pinnacle in multi-modal",
      "page": 2
    },
    {
      "caption": "Figure 1: COGMEN Model Architecture [5]",
      "page": 3
    },
    {
      "caption": "Figure 2: An illustration of the 4 input masking scenarios and their probabilities used during training.",
      "page": 4
    },
    {
      "caption": "Figure 3: , it can be seen that the distribution learnt by student",
      "page": 5
    },
    {
      "caption": "Figure 3: Comparison of the confusion matrix for the teacher and student models.",
      "page": 5
    },
    {
      "caption": "Figure 4: Comparison of the confusion matrix for the COGMEN-A and COGMEN-Mask models.",
      "page": 6
    },
    {
      "caption": "Figure 5: Confusion matrices with HuBERT audio features on Base COGMEN",
      "page": 10
    },
    {
      "caption": "Figure 6: Confusion matrices of models with HuBERT audio features.",
      "page": 10
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Abstract": "This paper presents an innovative approach to address the challenges of translating"
        },
        {
          "Abstract": "multi-modal emotion recognition models to a more practical and resource-efficient"
        },
        {
          "Abstract": "uni-modal counterpart, specifically focusing on speech-only emotion recognition."
        },
        {
          "Abstract": "Recognizing emotions from speech signals is a critical task with applications in"
        },
        {
          "Abstract": "human-computer interaction, affective computing, and mental health assessment."
        },
        {
          "Abstract": "However, existing state-of-the-art models often rely on multi-modal inputs, incor-"
        },
        {
          "Abstract": "porating information from multiple sources such as facial expressions and gestures,"
        },
        {
          "Abstract": "which may not be readily available or feasible in real-world scenarios. To tackle"
        },
        {
          "Abstract": "this issue, we propose a novel framework that leverages knowledge distillation and"
        },
        {
          "Abstract": "masked training techniques."
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "masked training techniques.": "1\nIntroduction"
        },
        {
          "masked training techniques.": "Emotion recognition, a pivotal aspect of human-computer interaction and affective computing, has"
        },
        {
          "masked training techniques.": "witnessed remarkable progress with the advent of multi-modal models that incorporate information"
        },
        {
          "masked training techniques.": "from diverse sources such as facial expressions, gestures, and speech. However, the practical deploy-"
        },
        {
          "masked training techniques.": "ment of such sophisticated models is often hindered by the inherent complexity and resource demands"
        },
        {
          "masked training techniques.": "associated with processing multiple modalities. In real-world scenarios, obtaining comprehensive"
        },
        {
          "masked training techniques.": "multi-modal data may be challenging or impractical,\nlimiting the applicability of these advanced"
        },
        {
          "masked training techniques.": "models."
        },
        {
          "masked training techniques.": "This paper addresses the gap between the theoretical advancements in multi-modal emotion recogni-"
        },
        {
          "masked training techniques.": "tion and the practical constraints of real-world applications, with a specific focus on translating these"
        },
        {
          "masked training techniques.": "models to a more accessible and efficient uni-modal counterpart—speech-only emotion recognition."
        },
        {
          "masked training techniques.": "Recognizing emotions from speech is crucial\nin numerous domains,\nincluding human-computer"
        },
        {
          "masked training techniques.": "interfaces, virtual assistants, and mental health assessment. Our objective is to develop a novel"
        },
        {
          "masked training techniques.": "framework that preserves the rich understanding encapsulated in multi-modal models while tailoring"
        },
        {
          "masked training techniques.": "it to the constraints and requirements of a speech-only context."
        },
        {
          "masked training techniques.": "To achieve this, we explore two independent strategies, leveraging knowledge distillation and masked"
        },
        {
          "masked training techniques.": "training. Knowledge distillation allows the transfer of nuanced information from a teacher model,"
        },
        {
          "masked training techniques.": "originally trained on multi-modal inputs, to a more streamlined student model designed for uni-modal,"
        },
        {
          "masked training techniques.": "speech-only emotion recognition. Concurrently, we introduce masked training, a novel\ntraining"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "approach designed to reinforce the model’s ability to generalize to incomplete inputs, while providing": "grounding from other modalities. Additionally, we investigate the impact of deep learning-generated"
        },
        {
          "approach designed to reinforce the model’s ability to generalize to incomplete inputs, while providing": "audio feature embeddings on the introduced models."
        },
        {
          "approach designed to reinforce the model’s ability to generalize to incomplete inputs, while providing": "Through a series of experiments, we evaluate the effectiveness of our proposed framework in"
        },
        {
          "approach designed to reinforce the model’s ability to generalize to incomplete inputs, while providing": "bridging the gap between multi-modal and uni-modal emotion recognition. The results not only"
        },
        {
          "approach designed to reinforce the model’s ability to generalize to incomplete inputs, while providing": "showcase an increased performance of multi-modal models but also demonstrate the competitive"
        },
        {
          "approach designed to reinforce the model’s ability to generalize to incomplete inputs, while providing": "capabilities of our proposed models in the challenging domain of speech-only emotion recognition."
        },
        {
          "approach designed to reinforce the model’s ability to generalize to incomplete inputs, while providing": "This work contributes to the ongoing dialogue in the field, emphasizing the need for practical and"
        },
        {
          "approach designed to reinforce the model’s ability to generalize to incomplete inputs, while providing": "efficient emotion recognition systems tailored for real-world applications where only speech-related"
        },
        {
          "approach designed to reinforce the model’s ability to generalize to incomplete inputs, while providing": "information is readily available."
        },
        {
          "approach designed to reinforce the model’s ability to generalize to incomplete inputs, while providing": "All code for this work can be found at: https://github.com/m-muaz/Cogmen_SLT"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Figure 1: COGMEN Model Architecture [5]": "The first component is a transformer-based feature extractor used to capture complex intra-utterance"
        },
        {
          "Figure 1: COGMEN Model Architecture [5]": "dependencies. This component is instrumental in extracting high-level representations from the input"
        },
        {
          "Figure 1: COGMEN Model Architecture [5]": "data, enabling the model to discern nuanced patterns associated with different emotional states."
        },
        {
          "Figure 1: COGMEN Model Architecture [5]": "The second component is a GNN, where utterances are connected to their i-precursor and j-successor"
        },
        {
          "Figure 1: COGMEN Model Architecture [5]": "utterances in time, forming a temporal graph that encapsulates the dynamic evolution of emotions"
        },
        {
          "Figure 1: COGMEN Model Architecture [5]": "over a length of conversation. This enables the model to leverage the temporal context and relation-"
        },
        {
          "Figure 1: COGMEN Model Architecture [5]": "ships between utterances, facilitating a more holistic understanding of emotional dynamics. In our"
        },
        {
          "Figure 1: COGMEN Model Architecture [5]": "experiments and in the original COGMEN paper i and j are both set to 5."
        },
        {
          "Figure 1: COGMEN Model Architecture [5]": "3.3\nFeature Embeddings"
        },
        {
          "Figure 1: COGMEN Model Architecture [5]": "COGMEN utilizes the OpenSmile toolkit\n[6]\nto extract audio features (size 100).\nThis toolkit"
        },
        {
          "Figure 1: COGMEN Model Architecture [5]": "integrates algorithms from speech processing and Music Information Retrieval, employing classic"
        },
        {
          "Figure 1: COGMEN Model Architecture [5]": "feature extractors such as the Mel-band filter-bank.\nIn our project, we investigate the impact of"
        },
        {
          "Figure 1: COGMEN Model Architecture [5]": "employing deep learning feature extractors like HuBERT [7] on the predictions of the base COGMEN,"
        },
        {
          "Figure 1: COGMEN Model Architecture [5]": "Masked COGMEN, and Knowledge Distillation COGMEN models. HuBERT features are extracted"
        },
        {
          "Figure 1: COGMEN Model Architecture [5]": "from its second-to-last layer, preceding the classification layer (size 1024 after applying mean over"
        },
        {
          "Figure 1: COGMEN Model Architecture [5]": "dim = 1). Given the input audio feature size of 100, a linear projection is applied to reduce the"
        },
        {
          "Figure 1: COGMEN Model Architecture [5]": "HuBERT output feature dimension to 100. Despite these generated features, the models exhibited"
        },
        {
          "Figure 1: COGMEN Model Architecture [5]": "poor performance, as detailed in the appendix. A potential reason for this may be the linear projection"
        },
        {
          "Figure 1: COGMEN Model Architecture [5]": "layer altering the feature values. Further exploration is needed to draw conclusive results."
        },
        {
          "Figure 1: COGMEN Model Architecture [5]": "3.4\nKnowledge Distillation"
        },
        {
          "Figure 1: COGMEN Model Architecture [5]": "The SOTA performance of COGMEN [5] architecture showcases that multi-modality indeed outper-"
        },
        {
          "Figure 1: COGMEN Model Architecture [5]": "forms uni-modal approaches which is evident from the fact that in nature humans and other living"
        },
        {
          "Figure 1: COGMEN Model Architecture [5]": "beings utilize information received from multiple senses. However, incorporating multiple inputs"
        },
        {
          "Figure 1: COGMEN Model Architecture [5]": "comes with additional computational cost. Even when resources are abundant, having resource-"
        },
        {
          "Figure 1: COGMEN Model Architecture [5]": "efficient networks means more requests can be served at a lower cost. Along these lines, we try to"
        },
        {
          "Figure 1: COGMEN Model Architecture [5]": "answer the question: Can we get a speech-only emotion recognition model while still maintaining"
        },
        {
          "Figure 1: COGMEN Model Architecture [5]": "the performance of\nthe larger and efficient model? To answer this, we focused along the domain"
        },
        {
          "Figure 1: COGMEN Model Architecture [5]": "of knowledge distillation [8]. Knowledge distillation is a general\ntechnique for supervising the"
        },
        {
          "Figure 1: COGMEN Model Architecture [5]": "training of \"student\" neural networks by capturing and transferring the knowledge of trained \"teacher\""
        },
        {
          "Figure 1: COGMEN Model Architecture [5]": "networks (In our context, \"teacher\" model refers to audio-visual input COGMEN model and \"student\""
        },
        {
          "Figure 1: COGMEN Model Architecture [5]": "model refers to audio only COGMEN model). The trained teacher network provides additional"
        },
        {
          "Figure 1: COGMEN Model Architecture [5]": "semantic knowledge besides the usual data supervision. However, the question of how to encoder and"
        },
        {
          "Figure 1: COGMEN Model Architecture [5]": "utilize the teacher’s knowledge such that student’s performance is maximized."
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "For this, we hypothesize that supervision of multiple modalities (audio, video, text, etc.) may have": "generated more rich embedding distributions/representations 1. So, it would be beneficial if we can"
        },
        {
          "For this, we hypothesize that supervision of multiple modalities (audio, video, text, etc.) may have": "use that rich embedding space to guide the student network."
        },
        {
          "For this, we hypothesize that supervision of multiple modalities (audio, video, text, etc.) may have": "signal for the student model, we utilize triplet loss. Formally, let X, P and N be the student model"
        },
        {
          "For this, we hypothesize that supervision of multiple modalities (audio, video, text, etc.) may have": "embedding representation , teacher model embedding representation for positive sample, and teacher"
        },
        {
          "For this, we hypothesize that supervision of multiple modalities (audio, video, text, etc.) may have": "embedding for negative sample, respectively. The loss function can then be described by means of"
        },
        {
          "For this, we hypothesize that supervision of multiple modalities (audio, video, text, etc.) may have": "p-norm:"
        },
        {
          "For this, we hypothesize that supervision of multiple modalities (audio, video, text, etc.) may have": "Ltriplet(X, P, N ) = max(∥f (X) − g(P )∥p − ∥f (X) − g(N )∥p + α, 0)"
        },
        {
          "For this, we hypothesize that supervision of multiple modalities (audio, video, text, etc.) may have": "where α is a margin between positive and negative pairs and f and g be the student and teacher"
        },
        {
          "For this, we hypothesize that supervision of multiple modalities (audio, video, text, etc.) may have": "embedding networks. For our experiments, α has been set to 1 and p to 2."
        },
        {
          "For this, we hypothesize that supervision of multiple modalities (audio, video, text, etc.) may have": "In addition to the signal from multi-class cross entropy signal to force the model to accurately predict"
        },
        {
          "For this, we hypothesize that supervision of multiple modalities (audio, video, text, etc.) may have": "emotions, the triplet loss will generate the guidance signal for the student to mimic the embeddings"
        },
        {
          "For this, we hypothesize that supervision of multiple modalities (audio, video, text, etc.) may have": "learnt by the teacher network. Therefore,\nthe student model"
        },
        {
          "For this, we hypothesize that supervision of multiple modalities (audio, video, text, etc.) may have": "multi-term loss:"
        },
        {
          "For this, we hypothesize that supervision of multiple modalities (audio, video, text, etc.) may have": "L = α1Ltriplet + α2LM C−CE"
        },
        {
          "For this, we hypothesize that supervision of multiple modalities (audio, video, text, etc.) may have": "where MC-CE refers to multi-class cross entropy loss. For our experiments, we kept both alpha"
        },
        {
          "For this, we hypothesize that supervision of multiple modalities (audio, video, text, etc.) may have": "parameters to 1 and it seems to be working for our experiments. However, future works could explore"
        },
        {
          "For this, we hypothesize that supervision of multiple modalities (audio, video, text, etc.) may have": "the impact of different parameters values either via manual value setting or making these parameters"
        },
        {
          "For this, we hypothesize that supervision of multiple modalities (audio, video, text, etc.) may have": "learnable."
        },
        {
          "For this, we hypothesize that supervision of multiple modalities (audio, video, text, etc.) may have": "3.5\nMasked Training"
        },
        {
          "For this, we hypothesize that supervision of multiple modalities (audio, video, text, etc.) may have": "In the pursuit of adapting the COGMEN architecture to a uni-modal, speech-only emotion recognition"
        },
        {
          "For this, we hypothesize that supervision of multiple modalities (audio, video, text, etc.) may have": ""
        },
        {
          "For this, we hypothesize that supervision of multiple modalities (audio, video, text, etc.) may have": "This technique aims to refine the model’s attention mechanisms specifically for speech-related features"
        },
        {
          "For this, we hypothesize that supervision of multiple modalities (audio, video, text, etc.) may have": "while maintaining performance on multiple modalities. Our masked training routine stochastically"
        },
        {
          "For this, we hypothesize that supervision of multiple modalities (audio, video, text, etc.) may have": "chooses between four distinct masking scenarios."
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "respectively to indicate that teacher model is the multi-modal COGMEN model while student model": "Table 1: F1 Scores on the Test Data"
        },
        {
          "respectively to indicate that teacher model is the multi-modal COGMEN model while student model": "COGMEN Models"
        },
        {
          "respectively to indicate that teacher model is the multi-modal COGMEN model while student model": "COGMENT -AV [SeqContext:4, GNN:7]"
        },
        {
          "respectively to indicate that teacher model is the multi-modal COGMEN model while student model": "COGMENS-A [SeqContext:4, GNN:7]"
        },
        {
          "respectively to indicate that teacher model is the multi-modal COGMEN model while student model": "COGMENT -AV [SeqContext:4, GNN:7]"
        },
        {
          "respectively to indicate that teacher model is the multi-modal COGMEN model while student model": "COGMENS-A [SeqContext:2, GNN:7]"
        },
        {
          "respectively to indicate that teacher model is the multi-modal COGMEN model while student model": "COGMENT -AV [SeqContext:4, GNN:7]"
        },
        {
          "respectively to indicate that teacher model is the multi-modal COGMEN model while student model": "COGMENS-A [SeqContext:4, GNN:3]"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table 2: Results of the masked-training routine compared to baseline models with no training",
      "data": [
        {
          "All\nin all,\nit can be concluded that\nthe uni-modal student model effectively achieves the same": "performance as of its teacher while also showing robustness to the variation in different aspects of the"
        },
        {
          "All\nin all,\nit can be concluded that\nthe uni-modal student model effectively achieves the same": "underlying model."
        },
        {
          "All\nin all,\nit can be concluded that\nthe uni-modal student model effectively achieves the same": "4.2\nMasked Training"
        },
        {
          "All\nin all,\nit can be concluded that\nthe uni-modal student model effectively achieves the same": "Our masked training adjusted COGMEN model will be called COGMEN-Mask, which we will"
        },
        {
          "All\nin all,\nit can be concluded that\nthe uni-modal student model effectively achieves the same": "compare to an audio-only COGMEN model (COGMEN-A) and an audio+video COGMEN model"
        },
        {
          "All\nin all,\nit can be concluded that\nthe uni-modal student model effectively achieves the same": "that will be tested in an audio-only context (COGMEN-AV)."
        },
        {
          "All\nin all,\nit can be concluded that\nthe uni-modal student model effectively achieves the same": "Table 2: Results of\nthe masked-training routine compared to baseline models with no training"
        },
        {
          "All\nin all,\nit can be concluded that\nthe uni-modal student model effectively achieves the same": "adjustments."
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "uni-modal version of our masked training routine create similar benefits for the COGMEN-A model.": "Our observations when running a uni-modal adaptation of the masked training on the COGMEN-A"
        },
        {
          "uni-modal version of our masked training routine create similar benefits for the COGMEN-A model.": "model yielded better results than the default COGMEN-A model, with the best F1 score we saw"
        },
        {
          "uni-modal version of our masked training routine create similar benefits for the COGMEN-A model.": "being 0.654, however, this still fails to exceed the performance of the COGMEN-Mask model and"
        },
        {
          "uni-modal version of our masked training routine create similar benefits for the COGMEN-A model.": "required adjustments to the number of sequential layers and GNN heads in the COGMEN model,"
        },
        {
          "uni-modal version of our masked training routine create similar benefits for the COGMEN-A model.": "thus requiring greater hyper-parameter search."
        },
        {
          "uni-modal version of our masked training routine create similar benefits for the COGMEN-A model.": "Overall, these results show that COGMEN-Mask not only achieves better translation from multi-modal"
        },
        {
          "uni-modal version of our masked training routine create similar benefits for the COGMEN-A model.": "to uni-modal contexts, but additionally seems to gain some knowledge grounding by being trained in"
        },
        {
          "uni-modal version of our masked training routine create similar benefits for the COGMEN-A model.": "a multi-modal context. Additionally, we ran experiments to discover how valuable COGMEN’s GNN"
        },
        {
          "uni-modal version of our masked training routine create similar benefits for the COGMEN-A model.": "module is to the observed masked training performance increase. Our findings indicate that random"
        },
        {
          "uni-modal version of our masked training routine create similar benefits for the COGMEN-A model.": "input masking is able to provide performance increases in both audio-only and audio-video contexts"
        },
        {
          "uni-modal version of our masked training routine create similar benefits for the COGMEN-A model.": "without the inclusion of a GNN, suggesting that this training method will generalize across model"
        },
        {
          "uni-modal version of our masked training routine create similar benefits for the COGMEN-A model.": "architectures. For full results see: Table 3."
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "References": "[1] Ayush Kumar and Jithendra Vepa. Gated mechanism for attention based multimodal sentiment analysis,"
        },
        {
          "References": "2020."
        },
        {
          "References": "Semantic Audiovisual Data Fusion for Automatic Emotion\n[2] Dragos Datcu and Leon J. M. Rothkrantz."
        },
        {
          "References": "Recognition, chapter 16, pages 411–435. John Wiley and Sons, Ltd, 2015."
        },
        {
          "References": "[3]\nJean-Benoit Delbrouck, Noé Tits, Mathilde Brousmiche, and Stéphane Dupont. A transformer-based"
        },
        {
          "References": "joint-encoding for emotion recognition and sentiment analysis.\nIn Second Grand-Challenge and Work-"
        },
        {
          "References": "shop on Multimodal Language (Challenge-HML), pages 1–7, Seattle, USA, July 2020. Association for"
        },
        {
          "References": "Computational Linguistics."
        },
        {
          "References": "[4] Zheng Lian, Jianhua Tao, Bin Liu, Jian Huang, Zhanlei Yang, and Rongjun Li. Conversational Emotion"
        },
        {
          "References": "Recognition Using Self-Attention Mechanisms and Graph Neural Networks.\nIn Proc. Interspeech 2020,"
        },
        {
          "References": "pages 2347–2351, 2020."
        },
        {
          "References": "[5] Abhinav Joshi, Ashwani Bhat, Ayush Jain, Atin Singh, and Ashutosh Modi. COGMEN: COntextualized"
        },
        {
          "References": "the 2022 Conference of\nthe North\nGNN based multimodal emotion recognitioN.\nIn Proceedings of"
        },
        {
          "References": "American Chapter of\nthe Association for Computational Linguistics: Human Language Technologies,"
        },
        {
          "References": "pages 4148–4164, Seattle, United States, July 2022. Association for Computational Linguistics."
        },
        {
          "References": "[6]\nFlorian Eyben, Martin Wöllmer, and Björn Schuller. Opensmile: The munich versatile and fast open-source"
        },
        {
          "References": "audio feature extractor.\nIn Proceedings of the 18th ACM International Conference on Multimedia, MM"
        },
        {
          "References": "’10, page 1459–1462, New York, NY, USA, 2010. Association for Computing Machinery."
        },
        {
          "References": "[7] Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov, and"
        },
        {
          "References": "Abdelrahman Mohamed. Hubert: Self-supervised speech representation learning by masked prediction of"
        },
        {
          "References": "hidden units, 2021."
        },
        {
          "References": "[8] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network, 2015."
        },
        {
          "References": "[9] Albert Gu, Karan Goel, and Christopher Ré. Efficiently modeling long sequences with structured state"
        },
        {
          "References": "spaces, 2022."
        },
        {
          "References": "[10] Ramin Hasani, Mathias Lechner, Tsun-Hsuan Wang, Makram Chahine, Alexander Amini, and Daniela"
        },
        {
          "References": "Rus. Liquid structural state-space models, 2022."
        },
        {
          "References": "[11]\nJimmy T. H. Smith, Andrew Warrington, and Scott W. Linderman.\nSimplified state space layers for"
        },
        {
          "References": "sequence modeling, 2023."
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "A": "At",
          "Input Masked Emotion Recognition without a GNN": "the end of our COGMEN-Mask experiments we were left with a lingering question, \"How"
        },
        {
          "A": "",
          "Input Masked Emotion Recognition without a GNN": "important is the GNN to the performance of the COGMEN-Mask model?\" Because the input masking"
        },
        {
          "A": "is stochastic,",
          "Input Masked Emotion Recognition without a GNN": "it\nis unlikely that an utterance will share the same input mask as its neighboring"
        },
        {
          "A": "",
          "Input Masked Emotion Recognition without a GNN": "utterances in the past and future, as such it is possible that the GNN is vital to the performance gain"
        },
        {
          "A": "",
          "Input Masked Emotion Recognition without a GNN": "from random input masking. This meant that experiments that removed the GNN component would"
        },
        {
          "A": "",
          "Input Masked Emotion Recognition without a GNN": "be key to understanding the effectiveness of this input masking method across varied underlying"
        },
        {
          "A": "models.",
          "Input Masked Emotion Recognition without a GNN": ""
        },
        {
          "A": "Without using a completely new model, we decided to use the COGMEN model, but adjusted the",
          "Input Masked Emotion Recognition without a GNN": ""
        },
        {
          "A": "",
          "Input Masked Emotion Recognition without a GNN": "utterance graph to be completely disjoint, such that all utterances were only connected to themselves."
        },
        {
          "A": "",
          "Input Masked Emotion Recognition without a GNN": "This makes the COGMEN GNN a simple feed-forward layer that does not rely on any external"
        },
        {
          "A": "",
          "Input Masked Emotion Recognition without a GNN": "information. After making this adjustment we re-ran the masking experiments and listed our results"
        },
        {
          "A": "in the table below.",
          "Input Masked Emotion Recognition without a GNN": ""
        },
        {
          "A": "",
          "Input Masked Emotion Recognition without a GNN": "Table 3: Results of the masked-training routine with a disjoint utterance graph."
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "This makes the COGMEN GNN a simple feed-forward layer that does not rely on any external": ""
        },
        {
          "This makes the COGMEN GNN a simple feed-forward layer that does not rely on any external": "in the table below."
        },
        {
          "This makes the COGMEN GNN a simple feed-forward layer that does not rely on any external": ""
        },
        {
          "This makes the COGMEN GNN a simple feed-forward layer that does not rely on any external": "Models"
        },
        {
          "This makes the COGMEN GNN a simple feed-forward layer that does not rely on any external": "COGMEN-A [5]"
        },
        {
          "This makes the COGMEN GNN a simple feed-forward layer that does not rely on any external": "COGMEN-AV [5]"
        },
        {
          "This makes the COGMEN GNN a simple feed-forward layer that does not rely on any external": "COGMEN-Mask (Ours)"
        },
        {
          "This makes the COGMEN GNN a simple feed-forward layer that does not rely on any external": "COGMEN-A + Mask"
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Table 5: F1 scores on the Test Data": "A inference"
        },
        {
          "Table 5: F1 scores on the Test Data": "0.57"
        },
        {
          "Table 5: F1 scores on the Test Data": "0.2282"
        },
        {
          "Table 5: F1 scores on the Test Data": "0.5964"
        },
        {
          "Table 5: F1 scores on the Test Data": "0.3738"
        },
        {
          "Table 5: F1 scores on the Test Data": "-"
        },
        {
          "Table 5: F1 scores on the Test Data": "-"
        }
      ],
      "page": 10
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Gated mechanism for attention based multimodal sentiment analysis",
      "authors": [
        "Ayush Kumar",
        "Jithendra Vepa"
      ],
      "year": "2020",
      "venue": "Gated mechanism for attention based multimodal sentiment analysis"
    },
    {
      "citation_id": "2",
      "title": "Semantic Audiovisual Data Fusion for Automatic Emotion Recognition",
      "authors": [
        "Dragos Datcu",
        "Leon Rothkrantz"
      ],
      "year": "2015",
      "venue": "Semantic Audiovisual Data Fusion for Automatic Emotion Recognition"
    },
    {
      "citation_id": "3",
      "title": "A transformer-based joint-encoding for emotion recognition and sentiment analysis",
      "authors": [
        "Jean-Benoit Delbrouck",
        "Noé Tits",
        "Mathilde Brousmiche",
        "Stéphane Dupont"
      ],
      "year": "2020",
      "venue": "Second Grand-Challenge and Workshop on Multimodal Language (Challenge-HML)"
    },
    {
      "citation_id": "4",
      "title": "Conversational Emotion Recognition Using Self-Attention Mechanisms and Graph Neural Networks",
      "authors": [
        "Zheng Lian",
        "Jianhua Tao",
        "Bin Liu",
        "Jian Huang",
        "Zhanlei Yang",
        "Rongjun Li"
      ],
      "year": "2020",
      "venue": "Proc. Interspeech 2020"
    },
    {
      "citation_id": "5",
      "title": "COGMEN: COntextualized GNN based multimodal emotion recognitioN",
      "authors": [
        "Abhinav Joshi",
        "Ashwani Bhat",
        "Ayush Jain",
        "Atin Singh",
        "Ashutosh Modi"
      ],
      "year": "2022",
      "venue": "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies"
    },
    {
      "citation_id": "6",
      "title": "Opensmile: The munich versatile and fast open-source audio feature extractor",
      "authors": [
        "Florian Eyben",
        "Martin Wöllmer",
        "Björn Schuller"
      ],
      "year": "2010",
      "venue": "Proceedings of the 18th ACM International Conference on Multimedia, MM '10"
    },
    {
      "citation_id": "7",
      "title": "Self-supervised speech representation learning by masked prediction of hidden units",
      "authors": [
        "Wei-Ning Hsu",
        "Benjamin Bolte",
        "Hubert Yao-Hung",
        "Kushal Tsai",
        "Ruslan Lakhotia",
        "Abdelrahman Salakhutdinov",
        "Mohamed",
        "Hubert"
      ],
      "year": "2021",
      "venue": "Self-supervised speech representation learning by masked prediction of hidden units"
    },
    {
      "citation_id": "8",
      "title": "Distilling the knowledge in a neural network",
      "authors": [
        "Geoffrey Hinton",
        "Oriol Vinyals",
        "Jeff Dean"
      ],
      "year": "2015",
      "venue": "Distilling the knowledge in a neural network"
    },
    {
      "citation_id": "9",
      "title": "Efficiently modeling long sequences with structured state spaces",
      "authors": [
        "Albert Gu",
        "Karan Goel",
        "Christopher Ré"
      ],
      "year": "2022",
      "venue": "Efficiently modeling long sequences with structured state spaces"
    },
    {
      "citation_id": "10",
      "title": "Liquid structural state-space models",
      "authors": [
        "Ramin Hasani",
        "Mathias Lechner",
        "Tsun-Hsuan Wang",
        "Makram Chahine",
        "Alexander Amini",
        "Daniela Rus"
      ],
      "year": "2022",
      "venue": "Liquid structural state-space models"
    },
    {
      "citation_id": "11",
      "title": "Simplified state space layers for sequence modeling",
      "authors": [
        "T Jimmy",
        "Andrew Smith",
        "Scott Warrington",
        "Linderman"
      ],
      "year": "2023",
      "venue": "Simplified state space layers for sequence modeling"
    }
  ]
}