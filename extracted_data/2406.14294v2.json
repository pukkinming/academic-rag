{
  "paper_id": "2406.14294v2",
  "title": "Dasb -Discrete Audio And Speech Benchmark",
  "published": "2024-06-20T13:23:27Z",
  "authors": [
    "Pooneh Mousavi",
    "Luca Della Libera",
    "Jarod Duret",
    "Artem Ploujnikov",
    "Cem Subakan",
    "Mirco Ravanelli"
  ],
  "keywords": [
    "spotting",
    "and intent classification",
    "as well as generative tasks such as speech enhancement",
    "separation",
    "and text-to-speech. Our results show that",
    "on average",
    "highlighting the need for further research in this field. Preprint. Under review."
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Discrete audio tokens have recently gained considerable attention for their potential to connect audio and language processing, enabling the creation of modern multimodal large language models. Ideal audio tokens must effectively preserve phonetic and semantic content along with paralinguistic information, speaker identity, and other details. While several types of audio tokens have been recently proposed, identifying the optimal tokenizer for various tasks is challenging due to the inconsistent evaluation settings in existing studies. To address this gap, we release the Discrete Audio and Speech Benchmark (DASB), a comprehensive leaderboard for benchmarking discrete audio tokens across a wide range of discriminative tasks, including speech recognition, speaker identification and verification, emotion recognition, keyword spotting, and intent classification, as well as generative tasks such as speech enhancement, separation, and text-to-speech. Our results show that, on average, semantic tokens outperform compression tokens across most discriminative and generative tasks. However, the performance gap between semantic tokens and standard continuous representations remains substantial, highlighting the need for further research in this field. Preprint. Under review.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Traditional speech and audio processing systems have long relied on handcrafted low-level features such as Mel-Frequency Cepstral Coefficients and Filterbanks  [1] . Recently, self-supervised learning (SSL) led to outstanding performance improvements by learning more complex, robust, and general speech features through deep neural networks. Notable models include Wav2Vec2  [2] , WavLM  [3] , and HuBERT  [4] . In all these cases, the rich information in speech and audio signals is encoded into a sequence of continuous vectors. Even though continuous vectors have proven effective in capturing the complex details embedded in speech and audio, there is a growing interest in discrete representations. Discrete audio representations, known as audio tokens, transform the original waveform into a finite set of vectors. These tokens are derived using methods such as quantization of self-supervised learning (SSL) models  [5, 6, 7] , neural compression techniques (codecs)  [8, 9] , or hybrid approaches  [8, 9]  that combine both methods.\n\nWhat is driving the interest in audio tokens? Arguably, this trend is linked to the remarkable success of autoregressive Large Language Models (LLMs) such as LLama  [10] , PALM  [11] , BERT  [12] , and GPT  [13] . Unlike audio, these models operate on text, which is inherently discrete. Inspired by their effectiveness, researchers are exploring audio language models  [14, 15, 16, 17, 18, 19, 20, 21]  by representing the audio as a sequence of discrete tokens. Moreover, audio and text tokens can be naturally combined, paving the way for the development of modern multi-modal LLMs  [22]  capable of processing audio, text, and visual data. Discrete tokens also simplify audio generation tasks like speech enhancement and synthesis by turning them into classification problems instead of regression models  [23] . Finally, they also enable efficient data compression for better transmission and storage. The main drawback of audio tokens is the inevitable loss of information The workflow of DASB consists of three steps. First, a discrete audio encoder converts the audio signal into discrete tokens (left). Then, the tokens are combined using attention and fed to a neural model for the final prediction (middle). For generative tasks, the predicted tokens are passed to a discrete decoder, which converts them back into an audio waveform (right). Both the encoder and decoder are pretrained and frozen during downstream model training. introduced by the discretization process. We ideally aim for audio tokens that preserves crucial information of the original waveform, including phonetic and linguistic content, speaker identities, emotions, and other paralinguistic cues. However, despite the growing trend toward audio tokens, there is still a lack of standardized evaluation benchmarks, with different studies employing varied experimental settings  [24, 25, 26, 27, 28] . Without a consistent framework for measuring and comparing performance, it becomes challenging to determine which audio tokens perform optimally across various tasks.\n\nTo address this gap, we introduce the Discrete Audio and Speech Benchmark (DASB). DASB systematically assesses various audio tokens across several common speech processing tasks. In particular, our contribution is the following:\n\n• We benchmark a diverse set of discrete audio encoders from all three categories: semantic (Discrete HuBERT, Discrete WavLM, Discrete Wav2Vec2), compression (EnCodec  [29] , DAC  [30] ), and hybrid (SpeechTokenizer  [8] ). • We consider a wide range of discriminative tasks, including speech, speaker, emotion recognition, keyword spotting, and intent classification. We also tackle generative tasks, such as speech enhancement, separation, and text-to-speech. For a more reliable assessment, we consider different downstream architectures for each task, following the insights in  [31] . To the best of our knowledge, this is the first comprehensive benchmark of audio tokens that covers both discriminative and generative tasks. • We publicly release DASB 1  as a modular code repository built on the popular SpeechBrain  [32]  toolkit and licensed under Apache 2.0.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work",
      "text": "Several research efforts have recently explored using discrete audio tokens as an alternative to continuous features. Some studies focused on using discrete features for speech recognition and speech translation  [28, 27, 33] , specifically evaluating the tokens obtained from the quantized versions of the HuBERT model. Similarly, Yang et al.  [34]  examined discrete features for speech recognition and text-to-speech. Audio tokens have been proposed for speech enhancement as well. For example, Wang et al.  [26]  investigated the application of semantic tokens to speech enhancement, whereas Erdogan et al.  [35]  proposed a hybrid tokenizer called TokenSplit for both speech enhancement and separation.\n\nWhile previous studies investigated the use of compression or hybrid tokens  [36, 37, 38] , these efforts were often limited to specific applications and a few audio tokenizers. In particular, previous benchmarking attempts focused on one category of tokenizers, either semantic or compression-based, and mostly on discriminative or generative tasks. For instance, Puvvada et al.  [25]  compared the performance of EnCodec and DAC  [30]  for speaker verification, speaker diarization, and speech recognition. Mousavi et al.  [39]  benchmarked various discriminative and generative tasks with semantic tokens. Wu et al.  [24]  provided a comprehensive study of the quality of resynthesized sound with compression and hybrid tokenizers. The latter attempt used pretrained models for speech, speaker, and emotion recognition, and assessed how much information is preserved by feeding them resynthesized audio. However, it did not address the direct use of tokenized input for training downstream tasks, nor did it deeply analyze the role of semantic tokens. Our analyses, instead, suggest that semantic tokens outperform other tokenizers.\n\nTo the best of our knowledge, the proposed DASB benchmark is the first to compare several audio tokenizers from three categories (semantic, compression, and hybrid) across many discriminative and generative speech tasks of broad practical interests. Moreover, unlike previous works on discrete audio tokens, we draw inspiration from the findings presented in  [31]  for reliably benchmarking continuous SSL representations and we consider different downstream architectures for each task. Similar to the approach taken by SUPERB  [40]  for continuous representation, we offer a standardized evaluation benchmark where researchers can easily evaluate novel audio tokens.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Benchmark Design",
      "text": "The pipeline of DASB, illustrated in Fig.  1 , consists of three components: Audio Encoder, Downstream Model, and Audio Decoder. The main features of the considered tokenizers are summarized in Table  1 , while Figure  2  reports the time and memory resources required by both encoders and decoders. The following subsections describe each module in detail.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Discrete Audio Encoder",
      "text": "The audio encoder converts the audio signal into a sequence of discrete tokens. It is pretrained on large amounts of unlabeled data and remains frozen during the training of downstream tasks.\n\nDifferent encoders may compress the information in the original waveform at different rates. The compression level is measured by the bitrate, defined as:\n\nwhere C is the number of codebooks, V is the number of vectors in each codebook (vocabulary), and R is the rate of codes per second. It is worth mentioning that a single sequence of tokens might be insufficient to capture the rich and complex information embedded in speech signals. The encoders thus often output multiple discrete sequences, with each sequence corresponding to a different codebook C. The encoders can operate at different bitrates simply by adjusting the number of codebooks C. For a fairer comparison, we define three different distinct bitrate ranges that we have identified from the literature  [41, 42, 43] : low (0-1.5 kbps), medium (2.9-6 kbps), and high (24 kbps). We consider this approach to prevent the trivial conclusion that some audio tokens perform better than others simply due to a higher bitrate.\n\nThe design of DASB is flexible, allowing for easy integration and benchmarking of various tokenizers. Using the terminology from  [44, 8] , we categorize audio tokens into three classes: semantic, compression, and hybrid tokenizers. Semantic tokens  [5, 6, 7]  are generated by clustering or quantizing layers from SSL models  [2, 3, 4] . The tokenization process typically involves selecting specific layers from a pretrained SSL model and applying the k-means algorithm to group their representations. Semantic tokens primarily capture high-level information, such as phonetic, semantic, and syntactic information. They are not optimized for waveform reconstruction, making them potentially better suited for discriminative tasks like speech recognition. Recent studies, however, have shown that semantic tokens can also be effective in generative tasks  [26, 34, 39] . We adopt the tokenization algorithm proposed in  [39] . In particular, we consider three widely-used open-source SSL models: Wav2Vec2-large, WavLM-large, and HuBERT-large, each composed of 24 layers. Then, we cluster  six of these layers using the k-means algorithm and select two layers from the lower part  (1, 3)  to capture low-level information, two from the middle layers  (7, 12) , and two from the higher layers  (18, 23)  to encode content and meaning as well.\n\nCompression tokens  [43, 29, 30]  are mainly used for audio compression. They are trained to accurately reconstruct the original audio, making them potentially suitable for audio generation tasks. We integrated two publicly available compression-based tokenizers in our baseline. EnCodec  [29]  has three main components: (i) an encoder network E consisting of a 1D convolution followed by a two-layer LSTM that processes the audio input and produces a latent representation z; (ii) a quantization layer Q that compresses z into z q using Residual Vector Quantization (RVQ)  [43] , where distinct codebooks quantizes residuals in multiple steps; and (iii) a decoder network G that mirrors the encoder and reconstructs the time-domain signal x from z q . The system is trained end-to-end to minimize reconstruction loss over time and frequency domains. It also adopts a perceptual loss using discriminators at different resolutions. EnCodec offers multiple models at low to medium bitrates (1.5 to 24 kbps). DAC  [30]  is an improved version of EnCodec. It combines advances in high-fidelity audio generation with better vector quantization techniques from the image domain, along with improved adversarial and reconstruction losses. DAC also supports quantizer dropout, allowing a single model to support variable bitrates.\n\nHybrid tokenizers  [8, 9]  unify semantic and acoustic tokens by disentangling different aspects of speech information hierarchically. SpeechTokenizer  [8]  is a unified speech tokenizer for large language models. It combines semantic and acoustic tokens, separating different speech information across RVQ layers. The model is based on RVQ-GANs, similar to EnCodec, and uses a convolutional encoder-decoder network from EnCodec. A two-layer BiLSTM replaces the original two-layer LSTM to improve semantic modeling. A semantic teacher guides the first RVQ quantizer, allowing the first layer tokens to capture content information effectively. With a residual structure, the subsequent quantizers capture the remaining paralinguistic information. SpeechTokenizer employs HuBERT as the semantic teacher to capture content information. The training procedure maximizes the cosine similarity between the RVQ layer outputs and the semantic teacher representations. HuBERT Layer 9 units represent semantic tokens, while EnCodec codes represent acoustic tokens.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Downstream Model",
      "text": "In this step, we employ neural networks to solve supervised tasks of common interest. To achieve this, we first assign each discrete token to a corresponding embedding vector through a lookup table. Subsequently, we dynamically combine the embeddings from different codebooks using attention, enabling the model to adjust the importance of codebooks for the specific task of interest. The attention mechanism consists of a simple multi-layer perceptron (MLP) that takes the embeddings of the audio tokens as input. The MLP generates a score for each selected codebook, which is normalized by a softmax function as shown in the following equations:\n\nwhere d c,t is the discrete token obtained from codebook c, at time t and, z c,t represents the score assigned to codebook c at time t by the MLP function f . The function emb(•) refers to the lookup table that assigns embeddings to each discrete token. The variable a c,t denotes the attention assigned to the codebook c at time t, and lastly h t is the representation that is fed to the downstream MLP model. We would like to note that the MLP learns different codebook combinations h t at each time step t depending on the attention weights a c,t , enabling the model to extract the necessary information when required. We also considered summing all embeddings, as has been done in previous literature  [25, 24] , but found that attention weights performed slightly better.\n\nThe combined representations h t are fed into neural models designed for different tasks. The downstream models are jointly trained with their attention and embedding layers in an end-to-end fashion. For discriminative tasks, the model outputs either a single prediction (e.g., for emotion recognition) or a sequence of predictions (e.g., for speech recognition). For generative tasks (e.g., speech enhancement), the neural network outputs the targeted tokens per codebook, with the output shape being C × L, where L is the sequence length. The predicted audio tokens are converted into an audio waveform via an audio decoder, as explained in the next section.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Discrete Audio Decoder",
      "text": "The decoder, used for generative tasks only, converts the predicted tokens into audio signals. The decoder is frozen during training. The choice of decoder depends on the encoder used in the first step. For compression and hybrid tokenizers, we use their built-in decoder. For semantic tokens, we use the scalable vocoder proposed in  [39] , which is a modified HiFi-GAN  [45]  pretrained with LibriSpeech-960h  [46] . The scalable vocoder accepts a variable number of multi-layer semantic tokens as input and can handle different bitrates using a layer dropout mechanism.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Experiments",
      "text": "In the following sections, we describe the discriminative and generative tasks considered in our experiments. For a more reliable evaluation, we tested two downstream architectures for each task.\n\nFor detailed information about the hyperparameters used in each experiment, refer to Appendix F.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Discriminative Tasks",
      "text": "For the downstream architectures and training procedures, we follow the best-performing approaches for classic continuous self-supervised representations proposed in  [31] .\n\n• Automatic Speech Recognition (ASR): The goal of ASR is converting speech signals into written text. We address two ASR tasks. The first task involves English ASR using the popular LibriSpeech dataset  [46] . Training and validation are performed on the train-clean-100 and devclean subsets, respectively, while testing is conducted on the test-clean and test-other subsets.\n\nThe downstream architecture for this task consists of two layers of Bidirectional Long Short-Term Memory (BiLSTM) followed by a linear layer for mapping audio to characters. The second architecture utilizes ContextNet  [47]  with unitary strides to maintain the frame rate of the encoder models. Additionally, we explore low-resource languages, specifically Welsh (Cymraeg) and Basque (Euskera) datasets extracted from CommonVoice 17.0  [48] . Here, we evaluate the performance using both the BiLSTM architecture and a two-layer dense neural network mapping frame representations to character probabilities. We use the Word Error Rate (WER) as the error metric for all ASR tasks. • Speaker Identification/Verification (SID, SV): Speaker Identification involves classifying each utterance by its speaker identity as a multi-class classification, with the same predefined set of speakers for both training and testing. The evaluation metric is the accuracy. Automatic Speaker Verification (ASV), instead, involves training a binary classifier to determine whether the speakers in a pair of utterances are the same. The evaluation metric adopted in this case is the equal error rate (EER). We use the widely-used VoxCeleb1  [49]  train and test splits for both tasks. First, we test the X-vector  [50]  architecture with AM-Softmax  [51]  loss for training the speaker embeddings. For verification, we use the cosine similarity between speaker representations. As a second architecture, we replace the X-vectors with an ECAPA-TDNN neural network  [52] .\n\n• Emotion Recognition (ER): The task involves predicting one of the four classes: happy, sad, angry, and neutral. We use the popular IEMOCAP  [53]  dataset, which contains about 10k samples from 10 speakers. As a first architecture, we directly input the representations into a linear classification layer after averaging them along the time axis. For the second downstream architecture, we use ECAPA-TDNN. The evaluation metric is the accuracy.\n\n• Intent Classification (IC): This task aims to determine the intention or purpose given utterance a speech recording. In particular, we here aim to classify each utterance into one of 18 scenarios, including calendar, email, and alarm. For this task, we utilize the SLURP dataset  [54] , which comprises around 72k audio recordings of single-turn user interactions with a home assistant. We employ ECAPA-TDNN and a two-layer BiLSTM (followed by a linear classifier) as downstream architectures. We evaluate the performance using the accuracy.\n\n• Keyword Spotting (KS): Keyword Spotting involves detecting predefined keywords by classifying utterances into a set of specified words. We use the Speech Commands dataset v1.0  [55]  for this task, as done in SUPERB. The dataset includes ten classes of keywords, a class for silence, and an unknown class to account for false positives. We employ both the X-vector and ECAPA-TDNN architectures. The evaluation metric is the accuracy.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Generative Tasks",
      "text": "• Speech Enhancement (SE): Speech enhancement aims to improve audio quality by cleaning up noisy input recordings. For this task, we utilize the popular VoiceBank dataset  [56] . We employ two downstream architectures: a non-autoregressive Conformer encoder  [57] , and a convolutional recurrent deep neural network (CRDNN). The input tokens are extracted from the noisy signal, while target tokens from the clean one. Training is performed using the cross-entropy loss. The speech quality is assessed using the deep noise suppression mean opinion score (DNSMOS)  [58] .\n\nThe intelligibility is evaluated through the differential word error rate (dWER)  [59] , which measures the WER between the transcribed enhanced signal and the transcribed target signal. The transcriptions are obtained using the small version of Whisper  [60] . Additionally, to measure speaker fidelity, we use the cosine similarity (SpkSim) between X-vectors extracted from the enhanced signal and the target signal using the base variant of WavLM  [3]  fine-tuned for speaker verification.\n\n• Speech Separation (SS): Speech separation aims to isolate individual voices from an audio recording containing multiple speakers. For this task, we use the Libri2Mix dataset  [61] , which contains mixtures of two overlapping speakers. We employ two downstream architectures: a non-autoregressive Conformer encoder  [57] , and a convolutional recurrent deep neural network (CRDNN). The input tokens are extracted from the mixture, while target tokens from each of the two sources. Training is performed using the cross-entropy loss with permutation invariant training  [62] . To measure performance, we employ the same metrics as speech enhancement.\n\n• Text-to-Speech (TTS): The task involves generating raw speech audio from a given text input.\n\nFor downstream architectures, we consider both a small and a large autoregressive Transformer  [63] . We train all models on the LJSpeech dataset  [64] . To assess the speech quality, we use a pretrained UTokyo-SaruLab System (UTMOS)  [65] , which is specifically designed for TTS and trained to predict human Mean Opinion scores. To measure pronunciation accuracy, we use the dWER metric. This involves comparing the transcriptions provided by a speech recognizer for the synthesized speech sample with the transcriptions from the ground truth. As for enhancement and separation, we considered the small version of Whisper  [60] .",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Results",
      "text": "",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Comparison Of Discrete Audio Models",
      "text": "Tables  2  and 3  show the performance of discriminative and generative tasks among the two downstream architectures explored. For each value in the table, we report the best performance obtained  with the two downstream architectures. Detailed results for each architecture, along with the settings for each experiment using continuous SSL models, are provided in Appendix E.\n\nWe observe a significant variation in the tokenizer performance across different tasks. This result suggests that the optimal choice of tokenizer depends on the specific task at hand. However, some interesting patterns emerge. For instance, semantic tokens significantly outperform compression tokens for most discriminative tasks. This trend is due to the ability of semantic tokens to capture high-level information from the audio signal as also observed in existing findings in the literature  [44] .\n\nThe only exceptions are speaker recognition tasks, where EnCodec achieves the best results. This suggests that compression tokens better encode speaker information. It is consistent with a previous study  [66]  that shows discrete tokens obtained from quantization of SSL layers remove speaker information.\n\nSemantic tokens show the best performance for generative tasks as well, achieving the highest MOS and dWER scores, indicating better overall quality and intelligibility in the generated outputs. However, for preserving speaker identity, compression tokens are more effective, as shown by superior speaker similarity (SpkSim) metrics. We think our findings for generative tasks are particularly interesting. While prior research efforts  [26, 39]  explored the use of semantic tokens for generation, they did not include a comparison with the performance of compression tokens. It is important to remark that the success observed with both semantic tokens relies heavily on the effectiveness of the decoder architecture used in our benchmark. Our scalable decoder minimizes distortions and artifacts in the generated speech, leading to better performance on various generative tasks.\n\nIn Table  4  (right), we present the ranking aggregation for the considered tokenizers (medium bitrate). Each model is individually ranked for every task, and we compute the average position across all ranks. This analysis shows that discrete WavLM generates the top-performing audio tokens. While the continuous version of WavLM ranks highest in the SUPERB benchmark, our findings demonstrate for the first time that this model maintains strong performance even after tokenization.\n\nOur comparison between discrete tokens and the best continuous baseline reveals a significant performance gap favoring continuous representations. This suggests that tokenization loses valuable information, such as phonetics, speaker identity, and emotion. Addressing this information loss is a key challenge for future generations of audio tokens.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Impact Of Bitrate",
      "text": "We also study the impact of different bitrates on the performance of the tokenizers. Tables  2  and 3  show that a medium bitrate achieves the best results for both discriminative and generative tasks. Interestingly, higher bitrates, when available (e.g., for EnCodec and DAC), tend to degrade performance. While higher bitrates can potentially preserve more information, they also increase the output dimensionality of the model, making the task more challenging to solve. In some cases, we found the task so challenging with high bitrates that the model did not converge, as observed in the case of TTS. It is worth noting that semantic tokens have a lower bitrate than compression tokens, as shown in Table  1 . For example, in the medium bitrate range, discrete WavLM has a bitrate of 2.9 kbps, while EnCodec has 6.0 kbps. This difference is due to the varying number of codebooks (6 vs. 8) and sampling rates (16 kHz vs. 24 kHz). Despite their lower bitrate, semantic tokens provide better performance.\n\nAnother aspect we investigate is the efficiency of the encoders and decoders, as this could impact some applications. Figure  2  shows the time and memory usage for each encoder-decoder pair across all bitrate ranges. For semantic tokens, the encoder is a large neural network and is computationally demanding. In contrast, the decoders are based on a compact HiFi-GAN model and are very efficient. For streaming tasks  [67]  where time and memory are critical, EnCodec turned out to be a better candidate due to the efficiency of both its encoder and decoder models.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Analysis Of Discrete Audio Decoder",
      "text": "Finally, we present a comparative evaluation of the decoders in Table  4  (left). The decoder evaluation is conducted on the LibriSpeech test-clean subset using a speech re-synthesis task, where we extract the tokens from each discrete audio encoder and reconstruct the speech using the associated decoders. Then, we evaluate the reconstructed speech based on speaker similarity, Mean Opinion Score (MOS), and differential Word Error Rate (dWER). The goal of this experiment is to establish if a given system is able to provide a high-fidelity reconstruction of the input audio after encoding it in the discrete space. This is especially important to establish for generative tasks.\n\nThe results show that the built-in decoders of compression tokens outperform other models in preserving speaker similarity, further confirming that current semantic tokens do not adequately preserve speaker information. Compression-based tokens also achieve better dWER scores. However, in terms of speech quality (assessed with DNSMOS), there are no significant differences between semantic and compression-based tokens. This trend indicates that while semantic tokens produce good-quality audio, they may be slightly more prone to semantic degradation (e.g., mispronunciations of words or phonemes). As expected, continuous baselines perform better than their discrete counterparts. Additional analysis on low and high settings can be found in Appendix D.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "Conclusion",
      "text": "This paper introduces DASB, a comprehensive benchmark designed to evaluate the performance of discrete audio tokens across diverse tasks of broad interest. We employ various evaluation metrics, downstream architectures, and bitrates for more robust comparisons. Interestingly, our findings reveal that semantic tokens outperform, on average, compression tokens in both generative and discriminative tasks. In particular, discrete WavLM emerged as the top-performing model, making it a natural candidate for adoption in multi-modal text+audio LLMs. A significant performance gap, however, persists when compared to traditional self-supervised continuous representations. This highlights the need for further research, which we believe is essential for better incorporating audio tokens into large multimodal language models.\n\nOne limitation we encountered is the proprietary nature of some audio tokenizers, such as Soundstream (author?)  [43] , which are not publicly accessible. Additionally, the benchmark is currently limited to speech tasks, but we plan to broaden it including music and sound processing. Our goal is to help the research community establish a shared benchmark and evaluation protocol for discrete audio representations. We will thus keep expanding DASB by continuously incorporating novel tokenizers and tasks.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "D Additional Analysis Of Discrete Audio Decoders",
      "text": "In this section, we expand on the results from Section 5.3 by including low and high bitrates in addition to the medium bitrate. Additionally, for speaker similarity, we measure the cosine similarity between X-vectors extracted from the reconstructed and target signals using two different models: WavLM (SpkSim WavLM) and ECAPA-TDNN (SpkSim ECAPA), both fine-tuned for speaker verification. When analyzing low and high bitrate settings, distinct trends emerge compared to the medium bitrate. At low bitrates, the models perform worse in terms of speaker similarity, MOS, and dWER, as expected. This is particularly pronounced for compressionbased decoders, where the degradation is more significant in terms of dWER. In contrast, at high bitrate, there is an overall small improvement in all metrics. In particular, the DAC model consistently outperforms EnCodec across all evaluated metrics, even for high bitrate settings.\n\nThe analysis shows that bitrate significantly impacts the performance of discrete decoders. Higher bitrates better preserve speech characteristics and result in lower error rates, while lower bitrates degrade these aspects. This highlights the trade-off between bitrate and speech synthesis quality.",
      "page_start": 16,
      "page_end": 16
    },
    {
      "section_name": "E Additional Results",
      "text": "Tables 9 -12 show the results obtained with 2 different downstream architectures. Note that table  7  indicates the first and second architectures explored for each task. For the continuous baseline, we follow the same architecture as the discrete experiments except for TTS. For the TTS continuous baseline, we use a modified Tacotron2  [75]  architecture enhanced with guided attention  [76]  that predicts SSL representations instead of Mel spectrograms.\n\nVarying the architecture of the downstream decoder leads significant variations in task performance. For ASR tasks, BiLSTM performs better. For classification tasks, ECAPA-TDNN shows the best performance, except for keyword spotting where X-vector is slightly better. For speech enhancement and separation, Conformer shows the best performance. For TTS, a notable pattern is observed: semantic tokens yield the best results with shallow models, while acoustic and hybrid tokens perform better with deeper models but still underperform compared to semantic tokens. One reason might be that discrete SSL models retain higher-level features closer to phonetic transcriptions, requiring lower-capacity models to capture the relationship between raw text and such representations. Higher-capacity models can lead to slower training and potential overfitting. In contrast, shallow models appear to underfit acoustic tokens, resulting in high dWERs and speech-like sounds with only surface resemblance to the original sentence, rather than intelligible speech.",
      "page_start": 17,
      "page_end": 17
    },
    {
      "section_name": "F Hyperparameters",
      "text": "We maintain the embedding dimension at 1024 for consistency across all experiments. Training continues until convergence or until the validation loss stops improving. For detailed settings for each experiment, please visit the GitHub repository.",
      "page_start": 19,
      "page_end": 19
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The workflow of DASB consists of three steps. First, a discrete audio encoder converts the",
      "page": 2
    },
    {
      "caption": "Figure 1: , consists of three components: Audio Encoder, Down-",
      "page": 3
    },
    {
      "caption": "Figure 2: reports the time and memory resources required by both encoders and",
      "page": 3
    },
    {
      "caption": "Figure 2: Time and memory required to process an utterance of 16 seconds for encoders and decoders",
      "page": 4
    },
    {
      "caption": "Figure 2: shows the time and memory usage for each encoder-decoder pair across",
      "page": 8
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Abstract": "Discrete audio tokens have recently gained considerable attention for their potential"
        },
        {
          "Abstract": "to connect audio and language processing, enabling the creation of modern multi-"
        },
        {
          "Abstract": "modal large language models. Ideal audio tokens must effectively preserve phonetic"
        },
        {
          "Abstract": "and semantic content along with paralinguistic information, speaker identity, and"
        },
        {
          "Abstract": "other details. While several\ntypes of audio tokens have been recently proposed,"
        },
        {
          "Abstract": "identifying the optimal tokenizer for various tasks is challenging due to the incon-"
        },
        {
          "Abstract": "sistent evaluation settings in existing studies. To address this gap, we release the"
        },
        {
          "Abstract": "Discrete Audio and Speech Benchmark (DASB), a comprehensive leaderboard"
        },
        {
          "Abstract": "for benchmarking discrete audio tokens across a wide range of discriminative"
        },
        {
          "Abstract": "tasks, including speech recognition, speaker identification and verification, emotion"
        },
        {
          "Abstract": "recognition, keyword spotting, and intent classification, as well as generative tasks"
        },
        {
          "Abstract": "such as speech enhancement, separation, and text-to-speech. Our results show that,"
        },
        {
          "Abstract": "on average, semantic tokens outperform compression tokens across most discrim-"
        },
        {
          "Abstract": "inative and generative tasks. However,\nthe performance gap between semantic"
        },
        {
          "Abstract": "tokens and standard continuous representations remains substantial, highlighting"
        },
        {
          "Abstract": "the need for further research in this field."
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "the need for further research in this field.": "1\nIntroduction"
        },
        {
          "the need for further research in this field.": "Traditional speech and audio processing systems have long relied on handcrafted low-level features"
        },
        {
          "the need for further research in this field.": "such as Mel-Frequency Cepstral Coefficients and Filterbanks [1]. Recently, self-supervised learning"
        },
        {
          "the need for further research in this field.": "(SSL) led to outstanding performance improvements by learning more complex, robust, and general"
        },
        {
          "the need for further research in this field.": "speech features through deep neural networks. Notable models include Wav2Vec2 [2], WavLM [3],"
        },
        {
          "the need for further research in this field.": "and HuBERT [4].\nIn all\nthese cases,\nthe rich information in speech and audio signals is encoded"
        },
        {
          "the need for further research in this field.": "into a sequence of continuous vectors. Even though continuous vectors have proven effective in"
        },
        {
          "the need for further research in this field.": "capturing the complex details embedded in speech and audio, there is a growing interest in discrete"
        },
        {
          "the need for further research in this field.": "representations. Discrete audio representations, known as audio tokens,\ntransform the original"
        },
        {
          "the need for further research in this field.": "waveform into a finite set of vectors. These tokens are derived using methods such as quantization"
        },
        {
          "the need for further research in this field.": "of self-supervised learning (SSL) models [5, 6, 7], neural compression techniques (codecs)[8, 9], or"
        },
        {
          "the need for further research in this field.": "hybrid approaches [8, 9] that combine both methods."
        },
        {
          "the need for further research in this field.": "What is driving the interest in audio tokens? Arguably, this trend is linked to the remarkable success"
        },
        {
          "the need for further research in this field.": "of autoregressive Large Language Models (LLMs) such as LLama [10], PALM [11], BERT [12],"
        },
        {
          "the need for further research in this field.": "and GPT [13]. Unlike audio,\nthese models operate on text, which is inherently discrete.\nInspired"
        },
        {
          "the need for further research in this field.": "by their effectiveness,\nresearchers are exploring audio language models [14, 15, 16, 17, 18, 19,"
        },
        {
          "the need for further research in this field.": "20, 21] by representing the audio as a sequence of discrete tokens. Moreover, audio and text"
        },
        {
          "the need for further research in this field.": "tokens can be naturally combined, paving the way for\nthe development of modern multi-modal"
        },
        {
          "the need for further research in this field.": "LLMs [22] capable of processing audio, text, and visual data. Discrete tokens also simplify audio"
        },
        {
          "the need for further research in this field.": "generation tasks like speech enhancement and synthesis by turning them into classification problems"
        },
        {
          "the need for further research in this field.": "instead of regression models\n[23]. Finally,\nthey also enable efficient data compression for better"
        },
        {
          "the need for further research in this field.": "transmission and storage. The main drawback of audio tokens is the inevitable loss of information"
        },
        {
          "the need for further research in this field.": "Preprint. Under review."
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Figure 1: The workflow of DASB consists of three steps. First, a discrete audio encoder converts the": "audio signal into discrete tokens (left). Then, the tokens are combined using attention and fed to a"
        },
        {
          "Figure 1: The workflow of DASB consists of three steps. First, a discrete audio encoder converts the": "neural model for the final prediction (middle). For generative tasks, the predicted tokens are passed to"
        },
        {
          "Figure 1: The workflow of DASB consists of three steps. First, a discrete audio encoder converts the": "a discrete decoder, which converts them back into an audio waveform (right). Both the encoder and"
        },
        {
          "Figure 1: The workflow of DASB consists of three steps. First, a discrete audio encoder converts the": "decoder are pretrained and frozen during downstream model training."
        },
        {
          "Figure 1: The workflow of DASB consists of three steps. First, a discrete audio encoder converts the": "introduced by the discretization process. We ideally aim for audio tokens that preserves crucial"
        },
        {
          "Figure 1: The workflow of DASB consists of three steps. First, a discrete audio encoder converts the": "information of the original waveform, including phonetic and linguistic content, speaker identities,"
        },
        {
          "Figure 1: The workflow of DASB consists of three steps. First, a discrete audio encoder converts the": "emotions, and other paralinguistic cues. However, despite the growing trend toward audio tokens,"
        },
        {
          "Figure 1: The workflow of DASB consists of three steps. First, a discrete audio encoder converts the": "there is still a lack of standardized evaluation benchmarks, with different studies employing varied"
        },
        {
          "Figure 1: The workflow of DASB consists of three steps. First, a discrete audio encoder converts the": "experimental settings [24, 25, 26, 27, 28]. Without a consistent\nframework for measuring and"
        },
        {
          "Figure 1: The workflow of DASB consists of three steps. First, a discrete audio encoder converts the": "comparing performance, it becomes challenging to determine which audio tokens perform optimally"
        },
        {
          "Figure 1: The workflow of DASB consists of three steps. First, a discrete audio encoder converts the": "across various tasks."
        },
        {
          "Figure 1: The workflow of DASB consists of three steps. First, a discrete audio encoder converts the": "To address this gap, we introduce the Discrete Audio and Speech Benchmark (DASB). DASB"
        },
        {
          "Figure 1: The workflow of DASB consists of three steps. First, a discrete audio encoder converts the": "systematically assesses various audio tokens across several common speech processing tasks.\nIn"
        },
        {
          "Figure 1: The workflow of DASB consists of three steps. First, a discrete audio encoder converts the": "particular, our contribution is the following:"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "systematically assesses various audio tokens across several common speech processing tasks.\nIn": "particular, our contribution is the following:"
        },
        {
          "systematically assesses various audio tokens across several common speech processing tasks.\nIn": "• We benchmark a diverse set of discrete audio encoders from all\nthree categories:\nsemantic"
        },
        {
          "systematically assesses various audio tokens across several common speech processing tasks.\nIn": "(Discrete HuBERT, Discrete WavLM, Discrete Wav2Vec2), compression (EnCodec [29], DAC"
        },
        {
          "systematically assesses various audio tokens across several common speech processing tasks.\nIn": "[30]), and hybrid (SpeechTokenizer [8])."
        },
        {
          "systematically assesses various audio tokens across several common speech processing tasks.\nIn": "• We consider a wide range of discriminative tasks, including speech, speaker, emotion recognition,"
        },
        {
          "systematically assesses various audio tokens across several common speech processing tasks.\nIn": "keyword spotting, and intent classification. We also tackle generative tasks, such as speech"
        },
        {
          "systematically assesses various audio tokens across several common speech processing tasks.\nIn": "enhancement,\nseparation, and text-to-speech.\nFor a more reliable assessment, we consider"
        },
        {
          "systematically assesses various audio tokens across several common speech processing tasks.\nIn": "different downstream architectures for each task,\nfollowing the insights in [31]. To the best"
        },
        {
          "systematically assesses various audio tokens across several common speech processing tasks.\nIn": "of our knowledge,\nthis is the first comprehensive benchmark of audio tokens that covers both"
        },
        {
          "systematically assesses various audio tokens across several common speech processing tasks.\nIn": "discriminative and generative tasks."
        },
        {
          "systematically assesses various audio tokens across several common speech processing tasks.\nIn": "• We publicly release DASB1 as a modular code repository built on the popular SpeechBrain [32]"
        },
        {
          "systematically assesses various audio tokens across several common speech processing tasks.\nIn": "toolkit and licensed under Apache 2.0."
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Erdogan et al. [35] proposed a hybrid tokenizer called TokenSplit for both speech enhancement and": "separation."
        },
        {
          "Erdogan et al. [35] proposed a hybrid tokenizer called TokenSplit for both speech enhancement and": "While previous studies investigated the use of compression or hybrid tokens [36, 37, 38],\nthese"
        },
        {
          "Erdogan et al. [35] proposed a hybrid tokenizer called TokenSplit for both speech enhancement and": "efforts were often limited to specific applications and a few audio tokenizers. In particular, previous"
        },
        {
          "Erdogan et al. [35] proposed a hybrid tokenizer called TokenSplit for both speech enhancement and": "benchmarking attempts focused on one category of tokenizers, either semantic or compression-based,"
        },
        {
          "Erdogan et al. [35] proposed a hybrid tokenizer called TokenSplit for both speech enhancement and": "and mostly on discriminative or generative tasks. For instance, Puvvada et al.\n[25] compared the"
        },
        {
          "Erdogan et al. [35] proposed a hybrid tokenizer called TokenSplit for both speech enhancement and": "performance of EnCodec and DAC [30] for speaker verification, speaker diarization, and speech"
        },
        {
          "Erdogan et al. [35] proposed a hybrid tokenizer called TokenSplit for both speech enhancement and": "recognition. Mousavi et al.\n[39] benchmarked various discriminative and generative tasks with"
        },
        {
          "Erdogan et al. [35] proposed a hybrid tokenizer called TokenSplit for both speech enhancement and": "semantic tokens. Wu et al.\n[24] provided a comprehensive study of the quality of resynthesized"
        },
        {
          "Erdogan et al. [35] proposed a hybrid tokenizer called TokenSplit for both speech enhancement and": "sound with compression and hybrid tokenizers. The latter attempt used pretrained models for speech,"
        },
        {
          "Erdogan et al. [35] proposed a hybrid tokenizer called TokenSplit for both speech enhancement and": "speaker, and emotion recognition, and assessed how much information is preserved by feeding"
        },
        {
          "Erdogan et al. [35] proposed a hybrid tokenizer called TokenSplit for both speech enhancement and": "them resynthesized audio. However, it did not address the direct use of tokenized input for training"
        },
        {
          "Erdogan et al. [35] proposed a hybrid tokenizer called TokenSplit for both speech enhancement and": "downstream tasks, nor did it deeply analyze the role of semantic tokens. Our analyses,\ninstead,"
        },
        {
          "Erdogan et al. [35] proposed a hybrid tokenizer called TokenSplit for both speech enhancement and": "suggest that semantic tokens outperform other tokenizers."
        },
        {
          "Erdogan et al. [35] proposed a hybrid tokenizer called TokenSplit for both speech enhancement and": "To the best of our knowledge, the proposed DASB benchmark is the first to compare several audio"
        },
        {
          "Erdogan et al. [35] proposed a hybrid tokenizer called TokenSplit for both speech enhancement and": "tokenizers from three categories (semantic, compression, and hybrid) across many discriminative"
        },
        {
          "Erdogan et al. [35] proposed a hybrid tokenizer called TokenSplit for both speech enhancement and": "and generative speech tasks of broad practical interests. Moreover, unlike previous works on discrete"
        },
        {
          "Erdogan et al. [35] proposed a hybrid tokenizer called TokenSplit for both speech enhancement and": "audio tokens, we draw inspiration from the findings presented in [31] for reliably benchmarking"
        },
        {
          "Erdogan et al. [35] proposed a hybrid tokenizer called TokenSplit for both speech enhancement and": "continuous SSL representations and we consider different downstream architectures for each task."
        },
        {
          "Erdogan et al. [35] proposed a hybrid tokenizer called TokenSplit for both speech enhancement and": "Similar to the approach taken by SUPERB [40] for continuous representation, we offer a standardized"
        },
        {
          "Erdogan et al. [35] proposed a hybrid tokenizer called TokenSplit for both speech enhancement and": "evaluation benchmark where researchers can easily evaluate novel audio tokens."
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Table 1: Key Features of the Discrete Audio Encoders. #Params is computed for medium bitrate.": ""
        },
        {
          "Table 1: Key Features of the Discrete Audio Encoders. #Params is computed for medium bitrate.": "Model"
        },
        {
          "Table 1: Key Features of the Discrete Audio Encoders. #Params is computed for medium bitrate.": ""
        },
        {
          "Table 1: Key Features of the Discrete Audio Encoders. #Params is computed for medium bitrate.": "Discrete HuBERT"
        },
        {
          "Table 1: Key Features of the Discrete Audio Encoders. #Params is computed for medium bitrate.": "Discrete WavLM"
        },
        {
          "Table 1: Key Features of the Discrete Audio Encoders. #Params is computed for medium bitrate.": "Discrete Wav2Vec2"
        },
        {
          "Table 1: Key Features of the Discrete Audio Encoders. #Params is computed for medium bitrate.": "EnCodec [29]"
        },
        {
          "Table 1: Key Features of the Discrete Audio Encoders. #Params is computed for medium bitrate.": "DAC [30]"
        },
        {
          "Table 1: Key Features of the Discrete Audio Encoders. #Params is computed for medium bitrate.": "SpeechTokenizer [8]"
        },
        {
          "Table 1: Key Features of the Discrete Audio Encoders. #Params is computed for medium bitrate.": "Discrete HuBERT"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "DAC [30]": "SpeechTokenizer [8]",
          "22.4M": "85.3M",
          "24KHz": "16KHz",
          "1.5": "1.0",
          "6.0": "4.0",
          "24.0": "-",
          "2": "2",
          "8": "8"
        },
        {
          "DAC [30]": "",
          "22.4M": "Discrete WavLM",
          "24KHz": "",
          "1.5": "",
          "6.0": "",
          "24.0": "EnCodec",
          "2": "",
          "8": ""
        },
        {
          "DAC [30]": "",
          "22.4M": "",
          "24KHz": "",
          "1.5": "",
          "6.0": "",
          "24.0": "",
          "2": "",
          "8": ""
        },
        {
          "DAC [30]": "",
          "22.4M": "",
          "24KHz": "",
          "1.5": "0.25",
          "6.0": "",
          "24.0": "",
          "2": "",
          "8": ""
        },
        {
          "DAC [30]": "",
          "22.4M": "1.6",
          "24KHz": "",
          "1.5": "",
          "6.0": "",
          "24.0": "",
          "2": "",
          "8": ""
        },
        {
          "DAC [30]": "",
          "22.4M": "",
          "24KHz": "",
          "1.5": "",
          "6.0": "",
          "24.0": "",
          "2": "",
          "8": ""
        },
        {
          "DAC [30]": "",
          "22.4M": "",
          "24KHz": "",
          "1.5": "",
          "6.0": "",
          "24.0": "",
          "2": "",
          "8": ""
        },
        {
          "DAC [30]": "",
          "22.4M": "1.4",
          "24KHz": "",
          "1.5": "0.20",
          "6.0": "",
          "24.0": "",
          "2": "",
          "8": ""
        },
        {
          "DAC [30]": "",
          "22.4M": "",
          "24KHz": "",
          "1.5": "",
          "6.0": "",
          "24.0": "",
          "2": "",
          "8": ""
        },
        {
          "DAC [30]": "",
          "22.4M": "1.2",
          "24KHz": "",
          "1.5": "",
          "6.0": "",
          "24.0": "",
          "2": "",
          "8": ""
        },
        {
          "DAC [30]": "",
          "22.4M": "",
          "24KHz": "",
          "1.5": "0.15",
          "6.0": "",
          "24.0": "",
          "2": "",
          "8": ""
        },
        {
          "DAC [30]": "",
          "22.4M": "Memory(GB)\n1.0",
          "24KHz": "",
          "1.5": "Time(s)",
          "6.0": "",
          "24.0": "",
          "2": "",
          "8": ""
        },
        {
          "DAC [30]": "",
          "22.4M": "0.8",
          "24KHz": "",
          "1.5": "",
          "6.0": "",
          "24.0": "",
          "2": "",
          "8": ""
        },
        {
          "DAC [30]": "",
          "22.4M": "",
          "24KHz": "",
          "1.5": "0.10",
          "6.0": "",
          "24.0": "",
          "2": "",
          "8": ""
        },
        {
          "DAC [30]": "",
          "22.4M": "",
          "24KHz": "",
          "1.5": "",
          "6.0": "",
          "24.0": "",
          "2": "",
          "8": ""
        },
        {
          "DAC [30]": "",
          "22.4M": "0.6",
          "24KHz": "",
          "1.5": "",
          "6.0": "",
          "24.0": "",
          "2": "",
          "8": ""
        },
        {
          "DAC [30]": "",
          "22.4M": "",
          "24KHz": "",
          "1.5": "",
          "6.0": "",
          "24.0": "",
          "2": "",
          "8": ""
        },
        {
          "DAC [30]": "",
          "22.4M": "0.4",
          "24KHz": "",
          "1.5": "0.05",
          "6.0": "",
          "24.0": "",
          "2": "",
          "8": ""
        },
        {
          "DAC [30]": "2",
          "22.4M": "",
          "24KHz": "8",
          "1.5": "",
          "6.0": "2",
          "24.0": "8",
          "2": "16",
          "8": "1"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "0.6": "0.4"
        },
        {
          "0.6": "0.05\n0.05\n0.4"
        },
        {
          "0.6": "1\n2\n4\n8\n16\n1\n2\n4\n8\n16\n1\n2\n4\n8\n16\n1\n2\n4\n8\n16"
        },
        {
          "0.6": "Bitrate (kbps)\nBitrate (kbps)\nBitrate (kbps)\nBitrate (kbps)"
        },
        {
          "0.6": "Figure 2: Time and memory required to process an utterance of 16 seconds for encoders and decoders"
        },
        {
          "0.6": "of the considered audio tokenizers on an NVIDIA GeForce RTX 3070 GPU @ 8 GB."
        },
        {
          "0.6": "six of these layers using the k-means algorithm and select two layers from the lower part (1, 3) to"
        },
        {
          "0.6": "capture low-level information, two from the middle layers (7, 12), and two from the higher layers (18,"
        },
        {
          "0.6": "23) to encode content and meaning as well."
        },
        {
          "0.6": "Compression tokens [43, 29, 30] are mainly used for audio compression.\nThey are trained to"
        },
        {
          "0.6": "accurately reconstruct the original audio, making them potentially suitable for audio generation tasks."
        },
        {
          "0.6": "We integrated two publicly available compression-based tokenizers in our baseline. EnCodec [29]"
        },
        {
          "0.6": "has three main components:\n(i) an encoder network E consisting of a 1D convolution followed"
        },
        {
          "0.6": "by a two-layer LSTM that processes the audio input and produces a latent representation z; (ii) a"
        },
        {
          "0.6": "quantization layer Q that compresses z into zq using Residual Vector Quantization (RVQ) [43], where"
        },
        {
          "0.6": "distinct codebooks quantizes residuals in multiple steps; and (iii) a decoder network G that mirrors"
        },
        {
          "0.6": "the encoder and reconstructs the time-domain signal ˆx from zq. The system is trained end-to-end"
        },
        {
          "0.6": "to minimize reconstruction loss over time and frequency domains. It also adopts a perceptual loss"
        },
        {
          "0.6": "using discriminators at different resolutions. EnCodec offers multiple models at\nlow to medium"
        },
        {
          "0.6": "bitrates (1.5 to 24 kbps). DAC [30] is an improved version of EnCodec.\nIt combines advances in"
        },
        {
          "0.6": "high-fidelity audio generation with better vector quantization techniques from the image domain,"
        },
        {
          "0.6": "along with improved adversarial and reconstruction losses. DAC also supports quantizer dropout,"
        },
        {
          "0.6": "allowing a single model to support variable bitrates."
        },
        {
          "0.6": "Hybrid tokenizers [8, 9] unify semantic and acoustic tokens by disentangling different aspects"
        },
        {
          "0.6": "of speech information hierarchically. SpeechTokenizer [8] is a unified speech tokenizer for large"
        },
        {
          "0.6": "language models. It combines semantic and acoustic tokens, separating different speech information"
        },
        {
          "0.6": "across RVQ layers. The model is based on RVQ-GANs, similar to EnCodec, and uses a convolutional"
        },
        {
          "0.6": "encoder-decoder network from EnCodec. A two-layer BiLSTM replaces the original two-layer LSTM"
        },
        {
          "0.6": "to improve semantic modeling. A semantic teacher guides the first RVQ quantizer, allowing the first"
        },
        {
          "0.6": "layer tokens to capture content\ninformation effectively. With a residual structure,\nthe subsequent"
        },
        {
          "0.6": "quantizers capture the remaining paralinguistic information. SpeechTokenizer employs HuBERT as"
        },
        {
          "0.6": "the semantic teacher to capture content information. The training procedure maximizes the cosine"
        },
        {
          "0.6": "similarity between the RVQ layer outputs and the semantic teacher representations. HuBERT Layer"
        },
        {
          "0.6": "9 units represent semantic tokens, while EnCodec codes represent acoustic tokens."
        },
        {
          "0.6": "3.2\nDownstream Model"
        },
        {
          "0.6": "In this step, we employ neural networks to solve supervised tasks of common interest. To achieve"
        },
        {
          "0.6": "this, we first assign each discrete token to a corresponding embedding vector through a lookup table."
        },
        {
          "0.6": "Subsequently, we dynamically combine the embeddings from different codebooks using attention,"
        },
        {
          "0.6": "enabling the model\nto adjust\nthe importance of codebooks for the specific task of interest. The"
        },
        {
          "0.6": "attention mechanism consists of a simple multi-layer perceptron (MLP) that takes the embeddings"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "For the downstream architectures and training procedures, we follow the best-performing approaches": "for classic continuous self-supervised representations proposed in\n[31]."
        },
        {
          "For the downstream architectures and training procedures, we follow the best-performing approaches": "• Automatic Speech Recognition (ASR): The goal of ASR is converting speech signals into"
        },
        {
          "For the downstream architectures and training procedures, we follow the best-performing approaches": "written text. We address two ASR tasks. The first task involves English ASR using the popular"
        },
        {
          "For the downstream architectures and training procedures, we follow the best-performing approaches": "LibriSpeech dataset [46]. Training and validation are performed on the train-clean-100 and dev-"
        },
        {
          "For the downstream architectures and training procedures, we follow the best-performing approaches": "clean subsets,\nrespectively, while testing is conducted on the test-clean and test-other subsets."
        },
        {
          "For the downstream architectures and training procedures, we follow the best-performing approaches": "The downstream architecture for this task consists of two layers of Bidirectional Long Short-"
        },
        {
          "For the downstream architectures and training procedures, we follow the best-performing approaches": "Term Memory (BiLSTM)\nfollowed by a linear\nlayer\nfor mapping audio to characters.\nThe"
        },
        {
          "For the downstream architectures and training procedures, we follow the best-performing approaches": "second architecture utilizes ContextNet [47] with unitary strides to maintain the frame rate of the"
        },
        {
          "For the downstream architectures and training procedures, we follow the best-performing approaches": "encoder models. Additionally, we explore low-resource languages, specifically Welsh (Cymraeg)"
        },
        {
          "For the downstream architectures and training procedures, we follow the best-performing approaches": "and Basque (Euskera) datasets extracted from CommonVoice 17.0 [48]. Here, we evaluate the"
        },
        {
          "For the downstream architectures and training procedures, we follow the best-performing approaches": "performance using both the BiLSTM architecture and a two-layer dense neural network mapping"
        },
        {
          "For the downstream architectures and training procedures, we follow the best-performing approaches": "frame representations to character probabilities. We use the Word Error Rate (WER) as the error"
        },
        {
          "For the downstream architectures and training procedures, we follow the best-performing approaches": "metric for all ASR tasks."
        },
        {
          "For the downstream architectures and training procedures, we follow the best-performing approaches": "• Speaker Identification/Verification (SID, SV): Speaker Identification involves classifying each"
        },
        {
          "For the downstream architectures and training procedures, we follow the best-performing approaches": "utterance by its speaker identity as a multi-class classification, with the same predefined set of"
        },
        {
          "For the downstream architectures and training procedures, we follow the best-performing approaches": "speakers for both training and testing. The evaluation metric is the accuracy. Automatic Speaker"
        },
        {
          "For the downstream architectures and training procedures, we follow the best-performing approaches": "Verification (ASV), instead, involves training a binary classifier to determine whether the speakers"
        },
        {
          "For the downstream architectures and training procedures, we follow the best-performing approaches": "in a pair of utterances are the same. The evaluation metric adopted in this case is the equal error rate"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "(EER). We use the widely-used VoxCeleb1 [49] train and test splits for both tasks. First, we test the": "X-vector [50] architecture with AM-Softmax [51] loss for training the speaker embeddings. For"
        },
        {
          "(EER). We use the widely-used VoxCeleb1 [49] train and test splits for both tasks. First, we test the": "verification, we use the cosine similarity between speaker representations. As a second architecture,"
        },
        {
          "(EER). We use the widely-used VoxCeleb1 [49] train and test splits for both tasks. First, we test the": "we replace the X-vectors with an ECAPA-TDNN neural network [52]."
        },
        {
          "(EER). We use the widely-used VoxCeleb1 [49] train and test splits for both tasks. First, we test the": ""
        },
        {
          "(EER). We use the widely-used VoxCeleb1 [49] train and test splits for both tasks. First, we test the": "angry, and neutral. We use the popular\nIEMOCAP [53] dataset, which contains about 10k"
        },
        {
          "(EER). We use the widely-used VoxCeleb1 [49] train and test splits for both tasks. First, we test the": "samples from 10 speakers. As a first architecture, we directly input\nthe representations into a"
        },
        {
          "(EER). We use the widely-used VoxCeleb1 [49] train and test splits for both tasks. First, we test the": "linear classification layer after averaging them along the time axis. For the second downstream"
        },
        {
          "(EER). We use the widely-used VoxCeleb1 [49] train and test splits for both tasks. First, we test the": "architecture, we use ECAPA-TDNN. The evaluation metric is the accuracy."
        },
        {
          "(EER). We use the widely-used VoxCeleb1 [49] train and test splits for both tasks. First, we test the": "Intent Classification (IC): This task aims to determine the intention or purpose given utterance a"
        },
        {
          "(EER). We use the widely-used VoxCeleb1 [49] train and test splits for both tasks. First, we test the": "speech recording.\nIn particular, we here aim to classify each utterance into one of 18 scenarios,"
        },
        {
          "(EER). We use the widely-used VoxCeleb1 [49] train and test splits for both tasks. First, we test the": "including calendar, email, and alarm. For this task, we utilize the SLURP dataset [54], which"
        },
        {
          "(EER). We use the widely-used VoxCeleb1 [49] train and test splits for both tasks. First, we test the": "comprises around 72k audio recordings of single-turn user interactions with a home assistant. We"
        },
        {
          "(EER). We use the widely-used VoxCeleb1 [49] train and test splits for both tasks. First, we test the": "employ ECAPA-TDNN and a two-layer BiLSTM (followed by a linear classifier) as downstream"
        },
        {
          "(EER). We use the widely-used VoxCeleb1 [49] train and test splits for both tasks. First, we test the": "architectures. We evaluate the performance using the accuracy."
        },
        {
          "(EER). We use the widely-used VoxCeleb1 [49] train and test splits for both tasks. First, we test the": ""
        },
        {
          "(EER). We use the widely-used VoxCeleb1 [49] train and test splits for both tasks. First, we test the": "utterances into a set of specified words. We use the Speech Commands dataset v1.0 [55] for this"
        },
        {
          "(EER). We use the widely-used VoxCeleb1 [49] train and test splits for both tasks. First, we test the": "task, as done in SUPERB. The dataset includes ten classes of keywords, a class for silence, and an"
        },
        {
          "(EER). We use the widely-used VoxCeleb1 [49] train and test splits for both tasks. First, we test the": "unknown class to account for false positives. We employ both the X-vector and ECAPA-TDNN"
        },
        {
          "(EER). We use the widely-used VoxCeleb1 [49] train and test splits for both tasks. First, we test the": "architectures. The evaluation metric is the accuracy."
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Table 2: Benchmarking results for discriminative tasks.": ""
        },
        {
          "Table 2: Benchmarking results for discriminative tasks.": ""
        },
        {
          "Table 2: Benchmarking results for discriminative tasks.": ""
        },
        {
          "Table 2: Benchmarking results for discriminative tasks.": ""
        },
        {
          "Table 2: Benchmarking results for discriminative tasks.": "Other"
        },
        {
          "Table 2: Benchmarking results for discriminative tasks.": ""
        },
        {
          "Table 2: Benchmarking results for discriminative tasks.": "21.14"
        },
        {
          "Table 2: Benchmarking results for discriminative tasks.": "27.56"
        },
        {
          "Table 2: Benchmarking results for discriminative tasks.": "28.65"
        },
        {
          "Table 2: Benchmarking results for discriminative tasks.": "77.04"
        },
        {
          "Table 2: Benchmarking results for discriminative tasks.": "83.61"
        },
        {
          "Table 2: Benchmarking results for discriminative tasks.": "43.12"
        },
        {
          "Table 2: Benchmarking results for discriminative tasks.": ""
        },
        {
          "Table 2: Benchmarking results for discriminative tasks.": "18.95"
        },
        {
          "Table 2: Benchmarking results for discriminative tasks.": "20.35"
        },
        {
          "Table 2: Benchmarking results for discriminative tasks.": "21.32"
        },
        {
          "Table 2: Benchmarking results for discriminative tasks.": "74.24"
        },
        {
          "Table 2: Benchmarking results for discriminative tasks.": "81.48"
        },
        {
          "Table 2: Benchmarking results for discriminative tasks.": "41.21"
        },
        {
          "Table 2: Benchmarking results for discriminative tasks.": ""
        },
        {
          "Table 2: Benchmarking results for discriminative tasks.": "72.56"
        },
        {
          "Table 2: Benchmarking results for discriminative tasks.": "99.38"
        },
        {
          "Table 2: Benchmarking results for discriminative tasks.": ""
        },
        {
          "Table 2: Benchmarking results for discriminative tasks.": "7.04"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Table 3: Benchmarking results for generative tasks. N.C. indicates “Not Converged\".": ""
        },
        {
          "Table 3: Benchmarking results for generative tasks. N.C. indicates “Not Converged\".": ""
        },
        {
          "Table 3: Benchmarking results for generative tasks. N.C. indicates “Not Converged\".": "DNSMOS ↑"
        },
        {
          "Table 3: Benchmarking results for generative tasks. N.C. indicates “Not Converged\".": ""
        },
        {
          "Table 3: Benchmarking results for generative tasks. N.C. indicates “Not Converged\".": "3.33"
        },
        {
          "Table 3: Benchmarking results for generative tasks. N.C. indicates “Not Converged\".": "3.26"
        },
        {
          "Table 3: Benchmarking results for generative tasks. N.C. indicates “Not Converged\".": "3.55"
        },
        {
          "Table 3: Benchmarking results for generative tasks. N.C. indicates “Not Converged\".": "3.15"
        },
        {
          "Table 3: Benchmarking results for generative tasks. N.C. indicates “Not Converged\".": "3.30"
        },
        {
          "Table 3: Benchmarking results for generative tasks. N.C. indicates “Not Converged\".": "3.18"
        },
        {
          "Table 3: Benchmarking results for generative tasks. N.C. indicates “Not Converged\".": ""
        },
        {
          "Table 3: Benchmarking results for generative tasks. N.C. indicates “Not Converged\".": "3.48"
        },
        {
          "Table 3: Benchmarking results for generative tasks. N.C. indicates “Not Converged\".": "3.48"
        },
        {
          "Table 3: Benchmarking results for generative tasks. N.C. indicates “Not Converged\".": "3.54"
        },
        {
          "Table 3: Benchmarking results for generative tasks. N.C. indicates “Not Converged\".": "3.10"
        },
        {
          "Table 3: Benchmarking results for generative tasks. N.C. indicates “Not Converged\".": "3.49"
        },
        {
          "Table 3: Benchmarking results for generative tasks. N.C. indicates “Not Converged\".": "3.49"
        },
        {
          "Table 3: Benchmarking results for generative tasks. N.C. indicates “Not Converged\".": ""
        },
        {
          "Table 3: Benchmarking results for generative tasks. N.C. indicates “Not Converged\".": "2.87"
        },
        {
          "Table 3: Benchmarking results for generative tasks. N.C. indicates “Not Converged\".": "2.95"
        },
        {
          "Table 3: Benchmarking results for generative tasks. N.C. indicates “Not Converged\".": ""
        },
        {
          "Table 3: Benchmarking results for generative tasks. N.C. indicates “Not Converged\".": "3.49"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table 4: (right),wepresenttherankingaggregationfortheconsideredtokenizers(mediumbitrate).",
      "data": [
        {
          "Table 4: (left) Evaluating various discrete decoders on the speech re-synthesis task (medium bitrate).": "(right) Ranking aggregation for models (medium bitrate)."
        },
        {
          "Table 4: (left) Evaluating various discrete decoders on the speech re-synthesis task (medium bitrate).": "DNSMOS ↑"
        },
        {
          "Table 4: (left) Evaluating various discrete decoders on the speech re-synthesis task (medium bitrate).": ""
        },
        {
          "Table 4: (left) Evaluating various discrete decoders on the speech re-synthesis task (medium bitrate).": "3.68"
        },
        {
          "Table 4: (left) Evaluating various discrete decoders on the speech re-synthesis task (medium bitrate).": "3.64"
        },
        {
          "Table 4: (left) Evaluating various discrete decoders on the speech re-synthesis task (medium bitrate).": "3.71"
        },
        {
          "Table 4: (left) Evaluating various discrete decoders on the speech re-synthesis task (medium bitrate).": "3.54"
        },
        {
          "Table 4: (left) Evaluating various discrete decoders on the speech re-synthesis task (medium bitrate).": ""
        },
        {
          "Table 4: (left) Evaluating various discrete decoders on the speech re-synthesis task (medium bitrate).": "3.74"
        },
        {
          "Table 4: (left) Evaluating various discrete decoders on the speech re-synthesis task (medium bitrate).": ""
        },
        {
          "Table 4: (left) Evaluating various discrete decoders on the speech re-synthesis task (medium bitrate).": "3.58"
        },
        {
          "Table 4: (left) Evaluating various discrete decoders on the speech re-synthesis task (medium bitrate).": ""
        },
        {
          "Table 4: (left) Evaluating various discrete decoders on the speech re-synthesis task (medium bitrate).": "3.73"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table 4: (right),wepresenttherankingaggregationfortheconsideredtokenizers(mediumbitrate).",
      "data": [
        {
          "EnCodec": "",
          "4.11": "",
          "3.93": "",
          "4.23": ""
        },
        {
          "EnCodec": "DAC",
          "4.11": "5.55",
          "3.93": "4 .06",
          "4.23": "4.64"
        },
        {
          "EnCodec": "",
          "4.11": "",
          "3.93": "",
          "4.23": ""
        },
        {
          "EnCodec": "SpeechTokenizer",
          "4.11": "3.44",
          "3.93": "3.81",
          "4.23": "3.64"
        },
        {
          "EnCodec": "",
          "4.11": "",
          "3.93": "",
          "4.23": ""
        },
        {
          "EnCodec": "",
          "4.11": "",
          "3.93": "",
          "4.23": ""
        },
        {
          "EnCodec": "",
          "4.11": "",
          "3.93": "",
          "4.23": ""
        },
        {
          "EnCodec": "",
          "4.11": "",
          "3.93": "",
          "4.23": ""
        },
        {
          "EnCodec": "",
          "4.11": "",
          "3.93": "",
          "4.23": "It is important to"
        },
        {
          "EnCodec": "",
          "4.11": "",
          "3.93": "",
          "4.23": ""
        },
        {
          "EnCodec": "",
          "4.11": "",
          "3.93": "",
          "4.23": ""
        },
        {
          "EnCodec": "",
          "4.11": "",
          "3.93": "",
          "4.23": ""
        },
        {
          "EnCodec": "",
          "4.11": "",
          "3.93": "",
          "4.23": ""
        },
        {
          "EnCodec": "",
          "4.11": "",
          "3.93": "",
          "4.23": ""
        },
        {
          "EnCodec": "",
          "4.11": "",
          "3.93": "",
          "4.23": ""
        },
        {
          "EnCodec": "",
          "4.11": "",
          "3.93": "",
          "4.23": ""
        },
        {
          "EnCodec": "",
          "4.11": "",
          "3.93": "",
          "4.23": ""
        },
        {
          "EnCodec": "",
          "4.11": "",
          "3.93": "",
          "4.23": ""
        },
        {
          "EnCodec": "",
          "4.11": "",
          "3.93": "",
          "4.23": ""
        },
        {
          "EnCodec": "",
          "4.11": "",
          "3.93": "",
          "4.23": ""
        },
        {
          "EnCodec": "",
          "4.11": "",
          "3.93": "",
          "4.23": ""
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "The results show that the built-in decoders of compression tokens outperform other models in pre-": "serving speaker similarity, further confirming that current semantic tokens do not adequately preserve"
        },
        {
          "The results show that the built-in decoders of compression tokens outperform other models in pre-": "speaker information. Compression-based tokens also achieve better dWER scores. However, in terms"
        },
        {
          "The results show that the built-in decoders of compression tokens outperform other models in pre-": "of speech quality (assessed with DNSMOS), there are no significant differences between semantic"
        },
        {
          "The results show that the built-in decoders of compression tokens outperform other models in pre-": "and compression-based tokens. This trend indicates that while semantic tokens produce good-quality"
        },
        {
          "The results show that the built-in decoders of compression tokens outperform other models in pre-": "audio, they may be slightly more prone to semantic degradation (e.g., mispronunciations of words"
        },
        {
          "The results show that the built-in decoders of compression tokens outperform other models in pre-": "or phonemes). As expected, continuous baselines perform better than their discrete counterparts."
        },
        {
          "The results show that the built-in decoders of compression tokens outperform other models in pre-": "Additional analysis on low and high settings can be found in Appendix D."
        },
        {
          "The results show that the built-in decoders of compression tokens outperform other models in pre-": "6\nConclusion"
        },
        {
          "The results show that the built-in decoders of compression tokens outperform other models in pre-": "This paper introduces DASB, a comprehensive benchmark designed to evaluate the performance of"
        },
        {
          "The results show that the built-in decoders of compression tokens outperform other models in pre-": "discrete audio tokens across diverse tasks of broad interest. We employ various evaluation metrics,"
        },
        {
          "The results show that the built-in decoders of compression tokens outperform other models in pre-": "downstream architectures, and bitrates for more robust comparisons.\nInterestingly, our findings"
        },
        {
          "The results show that the built-in decoders of compression tokens outperform other models in pre-": "reveal\nthat semantic tokens outperform, on average, compression tokens in both generative and"
        },
        {
          "The results show that the built-in decoders of compression tokens outperform other models in pre-": "discriminative tasks. In particular, discrete WavLM emerged as the top-performing model, making it"
        },
        {
          "The results show that the built-in decoders of compression tokens outperform other models in pre-": "a natural candidate for adoption in multi-modal text+audio LLMs. A significant performance gap,"
        },
        {
          "The results show that the built-in decoders of compression tokens outperform other models in pre-": "however, persists when compared to traditional self-supervised continuous representations. This"
        },
        {
          "The results show that the built-in decoders of compression tokens outperform other models in pre-": "highlights the need for further research, which we believe is essential for better incorporating audio"
        },
        {
          "The results show that the built-in decoders of compression tokens outperform other models in pre-": "tokens into large multimodal language models."
        },
        {
          "The results show that the built-in decoders of compression tokens outperform other models in pre-": "One limitation we encountered is the proprietary nature of some audio tokenizers, such as Sound-"
        },
        {
          "The results show that the built-in decoders of compression tokens outperform other models in pre-": "stream (author?) [43], which are not publicly accessible. Additionally, the benchmark is currently"
        },
        {
          "The results show that the built-in decoders of compression tokens outperform other models in pre-": "limited to speech tasks, but we plan to broaden it including music and sound processing. Our goal is"
        },
        {
          "The results show that the built-in decoders of compression tokens outperform other models in pre-": "to help the research community establish a shared benchmark and evaluation protocol for discrete"
        },
        {
          "The results show that the built-in decoders of compression tokens outperform other models in pre-": "audio representations. We will\nthus keep expanding DASB by continuously incorporating novel"
        },
        {
          "The results show that the built-in decoders of compression tokens outperform other models in pre-": "tokenizers and tasks."
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "donating part of the GPU computing resources needed for this work.": "References"
        },
        {
          "donating part of the GPU computing resources needed for this work.": "[1] Lawrence Rabiner and Biing-Hwang Juang. Fundamentals of Speech Recognition. Prentice-Hall Signal"
        },
        {
          "donating part of the GPU computing resources needed for this work.": "Processing Series, 1993."
        },
        {
          "donating part of the GPU computing resources needed for this work.": "[2] Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed, and Michael Auli. wav2vec 2.0: A framework for"
        },
        {
          "donating part of the GPU computing resources needed for this work.": "self-supervised learning of speech representations.\nIn International Conference on Neural Information"
        },
        {
          "donating part of the GPU computing resources needed for this work.": "Processing Systems (NeurIPS), volume 33, pages 12449–12460, 2020."
        },
        {
          "donating part of the GPU computing resources needed for this work.": "[3] Sanyuan Chen, Chengyi Wang, Zhengyang Chen, Yu Wu, Shujie Liu, Zhuo Chen, Jinyu Li, Naoyuki"
        },
        {
          "donating part of the GPU computing resources needed for this work.": "Kanda, Takuya Yoshioka, Xiong Xiao, et al. WavLM: Large-scale self-supervised pre-training for full"
        },
        {
          "donating part of the GPU computing resources needed for this work.": "stack speech processing.\nIEEE Journal of Selected Topics in Signal Processing, 16(6):1505–1518, 2022."
        },
        {
          "donating part of the GPU computing resources needed for this work.": "[4] Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov, and"
        },
        {
          "donating part of the GPU computing resources needed for this work.": "Abdelrahman Mohamed. HuBERT: Self-supervised speech representation learning by masked prediction"
        },
        {
          "donating part of the GPU computing resources needed for this work.": "of hidden units.\nIEEE/ACM Transactions on Audio, Speech, and Language Processing, 29:3451–3460,"
        },
        {
          "donating part of the GPU computing resources needed for this work.": "2021."
        },
        {
          "donating part of the GPU computing resources needed for this work.": "[5] Adam Polyak, Yossi Adi, Jade Copet, Eugene Kharitonov, Kushal Lakhotia, Wei-Ning Hsu, Abdelrah-"
        },
        {
          "donating part of the GPU computing resources needed for this work.": "man Mohamed, and Emmanuel Dupoux. Speech resynthesis from discrete disentangled self-supervised"
        },
        {
          "donating part of the GPU computing resources needed for this work.": "representations.\nIn Interspeech, pages 3615–3619, 2021."
        },
        {
          "donating part of the GPU computing resources needed for this work.": "[6] Dan Wells, Hao Tang, and Korin Richmond. Phonetic analysis of self-supervised representations of English"
        },
        {
          "donating part of the GPU computing resources needed for this work.": "speech.\nIn Interspeech, pages 3583–3587, 2022."
        },
        {
          "donating part of the GPU computing resources needed for this work.": "[7] Yu-An Chung, Yu Zhang, Wei Han, Chung-Cheng Chiu, James Qin, Ruoming Pang, and Yonghui Wu."
        },
        {
          "donating part of the GPU computing resources needed for this work.": "w2v-BERT: Combining contrastive learning and masked language modeling for self-supervised speech"
        },
        {
          "donating part of the GPU computing resources needed for this work.": "pre-training. In IEEE Automatic Speech Recognition and Understanding Workshop (ASRU), pages 244–250,"
        },
        {
          "donating part of the GPU computing resources needed for this work.": "2021."
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "[8] Xin Zhang, Dong Zhang, Shimin Li, Yaqian Zhou, and Xipeng Qiu. SpeechTokenizer: Unified speech": "tokenizer for speech large language models.\nIn International Conference on Learning Representations"
        },
        {
          "[8] Xin Zhang, Dong Zhang, Shimin Li, Yaqian Zhou, and Xipeng Qiu. SpeechTokenizer: Unified speech": "(ICLR), 2024."
        },
        {
          "[8] Xin Zhang, Dong Zhang, Shimin Li, Yaqian Zhou, and Xipeng Qiu. SpeechTokenizer: Unified speech": "[9] Zhihao Du, Shiliang Zhang, Kai Hu, and Siqi Zheng.\nFunCodec: A fundamental,\nreproducible and"
        },
        {
          "[8] Xin Zhang, Dong Zhang, Shimin Li, Yaqian Zhou, and Xipeng Qiu. SpeechTokenizer: Unified speech": "integrable open-source toolkit for neural speech codec. arXiv preprint arXiv:2309.07405, 2023."
        },
        {
          "[8] Xin Zhang, Dong Zhang, Shimin Li, Yaqian Zhou, and Xipeng Qiu. SpeechTokenizer: Unified speech": "[10] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay"
        },
        {
          "[8] Xin Zhang, Dong Zhang, Shimin Li, Yaqian Zhou, and Xipeng Qiu. SpeechTokenizer: Unified speech": "Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al.\nLlama 2: Open foundation and"
        },
        {
          "[8] Xin Zhang, Dong Zhang, Shimin Li, Yaqian Zhou, and Xipeng Qiu. SpeechTokenizer: Unified speech": "fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023."
        },
        {
          "[8] Xin Zhang, Dong Zhang, Shimin Li, Yaqian Zhou, and Xipeng Qiu. SpeechTokenizer: Unified speech": "[11] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts,"
        },
        {
          "[8] Xin Zhang, Dong Zhang, Shimin Li, Yaqian Zhou, and Xipeng Qiu. SpeechTokenizer: Unified speech": "Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. PaLM: scaling language"
        },
        {
          "[8] Xin Zhang, Dong Zhang, Shimin Li, Yaqian Zhou, and Xipeng Qiu. SpeechTokenizer: Unified speech": "modeling with pathways. Journal of Machine Learning Research, 24, 2024."
        },
        {
          "[8] Xin Zhang, Dong Zhang, Shimin Li, Yaqian Zhou, and Xipeng Qiu. SpeechTokenizer: Unified speech": "[12]\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.\nBERT: Pre-training of deep"
        },
        {
          "[8] Xin Zhang, Dong Zhang, Shimin Li, Yaqian Zhou, and Xipeng Qiu. SpeechTokenizer: Unified speech": "bidirectional transformers for language understanding.\nIn Jill Burstein, Christy Doran, and Thamar Solorio,"
        },
        {
          "[8] Xin Zhang, Dong Zhang, Shimin Li, Yaqian Zhou, and Xipeng Qiu. SpeechTokenizer: Unified speech": "the North American Chapter of\nthe Association for Computational Linguistics\neditors, Conference of"
        },
        {
          "[8] Xin Zhang, Dong Zhang, Shimin Li, Yaqian Zhou, and Xipeng Qiu. SpeechTokenizer: Unified speech": "(NAACL): Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171–4186, 2019."
        },
        {
          "[8] Xin Zhang, Dong Zhang, Shimin Li, Yaqian Zhou, and Xipeng Qiu. SpeechTokenizer: Unified speech": "[13] Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian, Zhilin Yang, and Jie Tang.\nGPT"
        },
        {
          "[8] Xin Zhang, Dong Zhang, Shimin Li, Yaqian Zhou, and Xipeng Qiu. SpeechTokenizer: Unified speech": "understands, too. AI Open, 2023."
        },
        {
          "[8] Xin Zhang, Dong Zhang, Shimin Li, Yaqian Zhou, and Xipeng Qiu. SpeechTokenizer: Unified speech": "[14] Aaron van den Oord, Oriol Vinyals, and Koray Kavukcuoglu. Neural discrete representation learning.\nIn"
        },
        {
          "[8] Xin Zhang, Dong Zhang, Shimin Li, Yaqian Zhou, and Xipeng Qiu. SpeechTokenizer: Unified speech": "Proceedings of the 31st International Conference on Neural Information Processing Systems, NIPS’17,"
        },
        {
          "[8] Xin Zhang, Dong Zhang, Shimin Li, Yaqian Zhou, and Xipeng Qiu. SpeechTokenizer: Unified speech": "2017."
        },
        {
          "[8] Xin Zhang, Dong Zhang, Shimin Li, Yaqian Zhou, and Xipeng Qiu. SpeechTokenizer: Unified speech": "[15] Paul K. Rubenstein, Chulayuth Asawaroengchai, Duc Dung Nguyen, Ankur Bapna, Zalán Borsos, Félix"
        },
        {
          "[8] Xin Zhang, Dong Zhang, Shimin Li, Yaqian Zhou, and Xipeng Qiu. SpeechTokenizer: Unified speech": "de Chaumont Quitry, Peter Chen, Dalia El Badawy, Wei Han, Eugene Kharitonov, Hannah Muckenhirn,"
        },
        {
          "[8] Xin Zhang, Dong Zhang, Shimin Li, Yaqian Zhou, and Xipeng Qiu. SpeechTokenizer: Unified speech": "et al. AudioPaLM: A large language model that can speak and listen. arXiv preprint arXiv:2306.12925,"
        },
        {
          "[8] Xin Zhang, Dong Zhang, Shimin Li, Yaqian Zhou, and Xipeng Qiu. SpeechTokenizer: Unified speech": "2023."
        },
        {
          "[8] Xin Zhang, Dong Zhang, Shimin Li, Yaqian Zhou, and Xipeng Qiu. SpeechTokenizer: Unified speech": "[16]\nJiaming Wang, Zhihao Du, Qian Chen, Yunfei Chu, Zhifu Gao, Zerui Li, Kai Hu, Xiaohuan Zhou, Jin Xu,"
        },
        {
          "[8] Xin Zhang, Dong Zhang, Shimin Li, Yaqian Zhou, and Xipeng Qiu. SpeechTokenizer: Unified speech": "Ziyang Ma, Wen Wang, Siqi Zheng, Chang Zhou, Zhijie Yan, and Shiliang Zhang. LauraGPT: Listen,"
        },
        {
          "[8] Xin Zhang, Dong Zhang, Shimin Li, Yaqian Zhou, and Xipeng Qiu. SpeechTokenizer: Unified speech": "attend, understand, and regenerate audio with GPT. arXiv preprint arXiv:2310.04673, 2023."
        },
        {
          "[8] Xin Zhang, Dong Zhang, Shimin Li, Yaqian Zhou, and Xipeng Qiu. SpeechTokenizer: Unified speech": "[17] Tianrui Wang, Long Zhou, Ziqiang Zhang, Yu Wu, Shujie Liu, Yashesh Gaur, Zhuo Chen, Jinyu Li, and"
        },
        {
          "[8] Xin Zhang, Dong Zhang, Shimin Li, Yaqian Zhou, and Xipeng Qiu. SpeechTokenizer: Unified speech": "Furu Wei. VioLA: Unified codec language models for speech recognition, synthesis, and translation. arXiv"
        },
        {
          "[8] Xin Zhang, Dong Zhang, Shimin Li, Yaqian Zhou, and Xipeng Qiu. SpeechTokenizer: Unified speech": "preprint arXiv:2305.16107, 2023."
        },
        {
          "[8] Xin Zhang, Dong Zhang, Shimin Li, Yaqian Zhou, and Xipeng Qiu. SpeechTokenizer: Unified speech": "[18] Chengyi Wang, Sanyuan Chen, Yu Wu, Ziqiang Zhang, Long Zhou, Shujie Liu, Zhuo Chen, Yanqing Liu,"
        },
        {
          "[8] Xin Zhang, Dong Zhang, Shimin Li, Yaqian Zhou, and Xipeng Qiu. SpeechTokenizer: Unified speech": "Huaming Wang, Jinyu Li, Lei He, Sheng Zhao, and Furu Wei. Neural codec language models are zero-shot"
        },
        {
          "[8] Xin Zhang, Dong Zhang, Shimin Li, Yaqian Zhou, and Xipeng Qiu. SpeechTokenizer: Unified speech": "text to speech synthesizers. arXiv preprint arXiv:2301.02111, 2023."
        },
        {
          "[8] Xin Zhang, Dong Zhang, Shimin Li, Yaqian Zhou, and Xipeng Qiu. SpeechTokenizer: Unified speech": "[19] Xiaofei Wang, Manthan Thakker, Zhuo Chen, Naoyuki Kanda, Sefik Emre Eskimez, Sanyuan Chen, Min"
        },
        {
          "[8] Xin Zhang, Dong Zhang, Shimin Li, Yaqian Zhou, and Xipeng Qiu. SpeechTokenizer: Unified speech": "Tang, Shujie Liu, Jinyu Li, and Takuya Yoshioka. SpeechX: Neural codec language model as a versatile"
        },
        {
          "[8] Xin Zhang, Dong Zhang, Shimin Li, Yaqian Zhou, and Xipeng Qiu. SpeechTokenizer: Unified speech": "speech transformer. arXiv preprint arXiv:2308.06873, 2023."
        },
        {
          "[8] Xin Zhang, Dong Zhang, Shimin Li, Yaqian Zhou, and Xipeng Qiu. SpeechTokenizer: Unified speech": "[20] Andrea Agostinelli et al. MusicLM: Generating music from text. arXiv preprint arXiv:2301.11325, 2023."
        },
        {
          "[8] Xin Zhang, Dong Zhang, Shimin Li, Yaqian Zhou, and Xipeng Qiu. SpeechTokenizer: Unified speech": "[21]\nFelix Kreuk, Gabriel Synnaeve, Adam Polyak, Uriel Singer, Alexandre Défossez, Jade Copet, Devi Parikh,"
        },
        {
          "[8] Xin Zhang, Dong Zhang, Shimin Li, Yaqian Zhou, and Xipeng Qiu. SpeechTokenizer: Unified speech": "Yaniv Taigman, and Yossi Adi. AudioGen: Textually guided audio generation.\nIn International Conference"
        },
        {
          "[8] Xin Zhang, Dong Zhang, Shimin Li, Yaqian Zhou, and Xipeng Qiu. SpeechTokenizer: Unified speech": "on Learning Representations (ICLR), 2023."
        },
        {
          "[8] Xin Zhang, Dong Zhang, Shimin Li, Yaqian Zhou, and Xipeng Qiu. SpeechTokenizer: Unified speech": "arXiv preprint\nGemini: A family of highly capable multimodal models.\n[22] Gemini Team, Google."
        },
        {
          "[8] Xin Zhang, Dong Zhang, Shimin Li, Yaqian Zhou, and Xipeng Qiu. SpeechTokenizer: Unified speech": "arXiv:2312.11805, 2023."
        },
        {
          "[8] Xin Zhang, Dong Zhang, Shimin Li, Yaqian Zhou, and Xipeng Qiu. SpeechTokenizer: Unified speech": "[23]\nIan J. Goodfellow, Yoshua Bengio, and Aaron Courville. Deep Learning. MIT Press, 2016."
        },
        {
          "[8] Xin Zhang, Dong Zhang, Shimin Li, Yaqian Zhou, and Xipeng Qiu. SpeechTokenizer: Unified speech": "[24] Haibin Wu, Ho-Lam Chung, Yi-Cheng Lin, Yuan-Kuei Wu, Xuanjun Chen, Yu-Chi Pai, Hsiu-Hsuan Wang,"
        },
        {
          "[8] Xin Zhang, Dong Zhang, Shimin Li, Yaqian Zhou, and Xipeng Qiu. SpeechTokenizer: Unified speech": "Kai-Wei Chang, Alexander H Liu, and Hung-yi Lee. Codec-SUPERB: An in-depth analysis of sound"
        },
        {
          "[8] Xin Zhang, Dong Zhang, Shimin Li, Yaqian Zhou, and Xipeng Qiu. SpeechTokenizer: Unified speech": "codec models. arXiv preprint arXiv:2402.13071, 2024."
        },
        {
          "[8] Xin Zhang, Dong Zhang, Shimin Li, Yaqian Zhou, and Xipeng Qiu. SpeechTokenizer: Unified speech": "[25] Krishna C. Puvvada, Nithin Rao Koluguri, Kunal Dhawan, Jagadeesh Balam, and Boris Ginsburg. Discrete"
        },
        {
          "[8] Xin Zhang, Dong Zhang, Shimin Li, Yaqian Zhou, and Xipeng Qiu. SpeechTokenizer: Unified speech": "audio representation as an alternative to Mel-spectrograms for speaker and speech recognition.\nIn IEEE"
        },
        {
          "[8] Xin Zhang, Dong Zhang, Shimin Li, Yaqian Zhou, and Xipeng Qiu. SpeechTokenizer: Unified speech": "International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 12111–12115,"
        },
        {
          "[8] Xin Zhang, Dong Zhang, Shimin Li, Yaqian Zhou, and Xipeng Qiu. SpeechTokenizer: Unified speech": "2024."
        },
        {
          "[8] Xin Zhang, Dong Zhang, Shimin Li, Yaqian Zhou, and Xipeng Qiu. SpeechTokenizer: Unified speech": "[26] Ziqian Wang, Xinfa Zhu, Zihan Zhang, YuanJun Lv, Ning Jiang, Guoqing Zhao, and Lei Xie. SELM:"
        },
        {
          "[8] Xin Zhang, Dong Zhang, Shimin Li, Yaqian Zhou, and Xipeng Qiu. SpeechTokenizer: Unified speech": "Speech enhancement using discrete tokens and language models.\nIn IEEE International Conference on"
        },
        {
          "[8] Xin Zhang, Dong Zhang, Shimin Li, Yaqian Zhou, and Xipeng Qiu. SpeechTokenizer: Unified speech": "Acoustics, Speech and Signal Processing (ICASSP), pages 11561–11565, 2024."
        },
        {
          "[8] Xin Zhang, Dong Zhang, Shimin Li, Yaqian Zhou, and Xipeng Qiu. SpeechTokenizer: Unified speech": "[27] Dong Zhang, Rong Ye, Tom Ko, Mingxuan Wang, and Yaqian Zhou. DUB: Discrete unit back-translation"
        },
        {
          "[8] Xin Zhang, Dong Zhang, Shimin Li, Yaqian Zhou, and Xipeng Qiu. SpeechTokenizer: Unified speech": "for speech translation.\nIn Findings of the Association for Computational Linguistics: ACL, pages 7147–"
        },
        {
          "[8] Xin Zhang, Dong Zhang, Shimin Li, Yaqian Zhou, and Xipeng Qiu. SpeechTokenizer: Unified speech": "7164, 2023."
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "[28] Xuankai Chang, Brian Yan, Kwanghee Choi, Jee-Weon Jung, Yichen Lu, Soumi Maiti, Roshan Sharma,": "Jiatong Shi, Jinchuan Tian, Shinji Watanabe, Yuya Fujita, et al. Exploring speech recognition, translation,"
        },
        {
          "[28] Xuankai Chang, Brian Yan, Kwanghee Choi, Jee-Weon Jung, Yichen Lu, Soumi Maiti, Roshan Sharma,": "and understanding with discrete speech units: A comparative study.\nIn IEEE International Conference on"
        },
        {
          "[28] Xuankai Chang, Brian Yan, Kwanghee Choi, Jee-Weon Jung, Yichen Lu, Soumi Maiti, Roshan Sharma,": "Acoustics, Speech and Signal Processing (ICASSP), pages 11481–11485, 2024."
        },
        {
          "[28] Xuankai Chang, Brian Yan, Kwanghee Choi, Jee-Weon Jung, Yichen Lu, Soumi Maiti, Roshan Sharma,": "[29] Alexandre Défossez, Jade Copet, Gabriel Synnaeve, and Yossi Adi. High fidelity neural audio compression."
        },
        {
          "[28] Xuankai Chang, Brian Yan, Kwanghee Choi, Jee-Weon Jung, Yichen Lu, Soumi Maiti, Roshan Sharma,": "Transactions on Machine Learning Research (TMLR), 2023."
        },
        {
          "[28] Xuankai Chang, Brian Yan, Kwanghee Choi, Jee-Weon Jung, Yichen Lu, Soumi Maiti, Roshan Sharma,": "[30] Rithesh Kumar, Prem Seetharaman, Alejandro Luebs, Ishaan Kumar, and Kundan Kumar. High-fidelity au-"
        },
        {
          "[28] Xuankai Chang, Brian Yan, Kwanghee Choi, Jee-Weon Jung, Yichen Lu, Soumi Maiti, Roshan Sharma,": "dio compression with improved RVQGAN.\nIn International Conference on Neural Information Processing"
        },
        {
          "[28] Xuankai Chang, Brian Yan, Kwanghee Choi, Jee-Weon Jung, Yichen Lu, Soumi Maiti, Roshan Sharma,": "Systems (NeurIPS), 2023."
        },
        {
          "[28] Xuankai Chang, Brian Yan, Kwanghee Choi, Jee-Weon Jung, Yichen Lu, Soumi Maiti, Roshan Sharma,": "[31]\nSalah Zaiem, Youcef Kemiche, Titouan Parcollet, Slim Essid, and Mirco Ravanelli. Speech self-supervised"
        },
        {
          "[28] Xuankai Chang, Brian Yan, Kwanghee Choi, Jee-Weon Jung, Yichen Lu, Soumi Maiti, Roshan Sharma,": "representation benchmarking: Are we doing it right? In Interspeech, pages 2873–2877, 2023."
        },
        {
          "[28] Xuankai Chang, Brian Yan, Kwanghee Choi, Jee-Weon Jung, Yichen Lu, Soumi Maiti, Roshan Sharma,": "[32] Mirco Ravanelli, Titouan Parcollet, Peter Plantinga, Aku Rouhe, Samuele Cornell, Loren Lugosch, Cem"
        },
        {
          "[28] Xuankai Chang, Brian Yan, Kwanghee Choi, Jee-Weon Jung, Yichen Lu, Soumi Maiti, Roshan Sharma,": "Subakan, Nauman Dawalatabad, Abdelwahab Heba, Jianyuan Zhong, et al.\nSpeechBrain: A general-"
        },
        {
          "[28] Xuankai Chang, Brian Yan, Kwanghee Choi, Jee-Weon Jung, Yichen Lu, Soumi Maiti, Roshan Sharma,": "purpose speech toolkit. arXiv preprint arXiv:2106.04624, 2021."
        },
        {
          "[28] Xuankai Chang, Brian Yan, Kwanghee Choi, Jee-Weon Jung, Yichen Lu, Soumi Maiti, Roshan Sharma,": "[33] Xuankai Chang, Brian Yan, Yuya Fujita, Takashi Maekaku, and Shinji Watanabe. Exploration of efficient"
        },
        {
          "[28] Xuankai Chang, Brian Yan, Kwanghee Choi, Jee-Weon Jung, Yichen Lu, Soumi Maiti, Roshan Sharma,": "end-to-end ASR using discretized input from self-supervised learning.\nIn Interspeech, pages 1399–1403,"
        },
        {
          "[28] Xuankai Chang, Brian Yan, Kwanghee Choi, Jee-Weon Jung, Yichen Lu, Soumi Maiti, Roshan Sharma,": "2023."
        },
        {
          "[28] Xuankai Chang, Brian Yan, Kwanghee Choi, Jee-Weon Jung, Yichen Lu, Soumi Maiti, Roshan Sharma,": "[34] Yifan Yang, Feiyu Shen, Chenpeng Du, Ziyang Ma, Kai Yu, Daniel Povey, and Xie Chen.\nTowards"
        },
        {
          "[28] Xuankai Chang, Brian Yan, Kwanghee Choi, Jee-Weon Jung, Yichen Lu, Soumi Maiti, Roshan Sharma,": "universal speech discrete tokens: A case study for ASR and TTS.\nIn IEEE International Conference on"
        },
        {
          "[28] Xuankai Chang, Brian Yan, Kwanghee Choi, Jee-Weon Jung, Yichen Lu, Soumi Maiti, Roshan Sharma,": "Acoustics, Speech and Signal Processing (ICASSP), pages 10401–10405, 2024."
        },
        {
          "[28] Xuankai Chang, Brian Yan, Kwanghee Choi, Jee-Weon Jung, Yichen Lu, Soumi Maiti, Roshan Sharma,": "[35] Hakan Erdogan, Scott Wisdom, Xuankai Chang, Zalán Borsos, Marco Tagliasacchi, Neil Zeghidour, and"
        },
        {
          "[28] Xuankai Chang, Brian Yan, Kwanghee Choi, Jee-Weon Jung, Yichen Lu, Soumi Maiti, Roshan Sharma,": "John R Hershey. TokenSplit: Using discrete speech representations for direct, refined, and transcript-"
        },
        {
          "[28] Xuankai Chang, Brian Yan, Kwanghee Choi, Jee-Weon Jung, Yichen Lu, Soumi Maiti, Roshan Sharma,": "conditioned speech separation and recognition.\nIn Interspeech, pages 3462–3466, 2023."
        },
        {
          "[28] Xuankai Chang, Brian Yan, Kwanghee Choi, Jee-Weon Jung, Yichen Lu, Soumi Maiti, Roshan Sharma,": "[36] Siyang Wang and Éva Székely. Evaluating text-to-speech synthesis from a large discrete token-based"
        },
        {
          "[28] Xuankai Chang, Brian Yan, Kwanghee Choi, Jee-Weon Jung, Yichen Lu, Soumi Maiti, Roshan Sharma,": "International Conference on Computational Linguistics, Language\nspeech language model.\nIn Joint"
        },
        {
          "[28] Xuankai Chang, Brian Yan, Kwanghee Choi, Jee-Weon Jung, Yichen Lu, Soumi Maiti, Roshan Sharma,": "Resources and Evaluation (LREC-COLING), pages 6464–6474, 2024."
        },
        {
          "[28] Xuankai Chang, Brian Yan, Kwanghee Choi, Jee-Weon Jung, Yichen Lu, Soumi Maiti, Roshan Sharma,": "[37] Chengyi Wang, Sanyuan Chen, Yu Wu, Ziqiang Zhang, Long Zhou, Shujie Liu, Zhuo Chen, Yanqing Liu,"
        },
        {
          "[28] Xuankai Chang, Brian Yan, Kwanghee Choi, Jee-Weon Jung, Yichen Lu, Soumi Maiti, Roshan Sharma,": "Huaming Wang, Jinyu Li, et al. Neural codec language models are zero-shot text to speech synthesizers."
        },
        {
          "[28] Xuankai Chang, Brian Yan, Kwanghee Choi, Jee-Weon Jung, Yichen Lu, Soumi Maiti, Roshan Sharma,": "arXiv preprint arXiv:2301.02111, 2023."
        },
        {
          "[28] Xuankai Chang, Brian Yan, Kwanghee Choi, Jee-Weon Jung, Yichen Lu, Soumi Maiti, Roshan Sharma,": "[38] Eugene Kharitonov, Damien Vincent, Zalán Borsos, Raphaël Marinier, Sertan Girgin, Olivier Pietquin,"
        },
        {
          "[28] Xuankai Chang, Brian Yan, Kwanghee Choi, Jee-Weon Jung, Yichen Lu, Soumi Maiti, Roshan Sharma,": "Matt Sharifi, Marco Tagliasacchi, and Neil Zeghidour. Speak, read and prompt: High-fidelity text-to-speech"
        },
        {
          "[28] Xuankai Chang, Brian Yan, Kwanghee Choi, Jee-Weon Jung, Yichen Lu, Soumi Maiti, Roshan Sharma,": "with minimal supervision. Transactions of the Association for Computational Linguistics, 11:1703–1718,"
        },
        {
          "[28] Xuankai Chang, Brian Yan, Kwanghee Choi, Jee-Weon Jung, Yichen Lu, Soumi Maiti, Roshan Sharma,": "2023."
        },
        {
          "[28] Xuankai Chang, Brian Yan, Kwanghee Choi, Jee-Weon Jung, Yichen Lu, Soumi Maiti, Roshan Sharma,": "[39] Pooneh Mousavi, Jarod Duret, Salah Zaiem, Luca Della Libera, Artem Ploujnikov, Cem Subakan, and"
        },
        {
          "[28] Xuankai Chang, Brian Yan, Kwanghee Choi, Jee-Weon Jung, Yichen Lu, Soumi Maiti, Roshan Sharma,": "Mirco Ravanelli. How should we extract discrete audio tokens from self-supervised models?, 2024."
        },
        {
          "[28] Xuankai Chang, Brian Yan, Kwanghee Choi, Jee-Weon Jung, Yichen Lu, Soumi Maiti, Roshan Sharma,": "[40]\nShu wen Yang, Po-Han Chi, Yung-Sung Chuang, Cheng-I Jeff Lai, Kushal Lakhotia, Yist Y. Lin, Andy T."
        },
        {
          "[28] Xuankai Chang, Brian Yan, Kwanghee Choi, Jee-Weon Jung, Yichen Lu, Soumi Maiti, Roshan Sharma,": "Liu, Jiatong Shi, Xuankai Chang, Guan-Ting Lin, et al. SUPERB: Speech Processing Universal PERfor-"
        },
        {
          "[28] Xuankai Chang, Brian Yan, Kwanghee Choi, Jee-Weon Jung, Yichen Lu, Soumi Maiti, Roshan Sharma,": "mance Benchmark.\nIn Interspeech, pages 1194–1198, 2021."
        },
        {
          "[28] Xuankai Chang, Brian Yan, Kwanghee Choi, Jee-Weon Jung, Yichen Lu, Soumi Maiti, Roshan Sharma,": "[41]\nJean-Marc Valin, Koen Vos, and Timothy Terriberry. Definition of the Opus audio codec. Technical report,"
        },
        {
          "[28] Xuankai Chang, Brian Yan, Kwanghee Choi, Jee-Weon Jung, Yichen Lu, Soumi Maiti, Roshan Sharma,": "2012."
        },
        {
          "[28] Xuankai Chang, Brian Yan, Kwanghee Choi, Jee-Weon Jung, Yichen Lu, Soumi Maiti, Roshan Sharma,": "[42] Martin Dietz, Markus Multrus, Vaclav Eksler, Vladimir Malenovsky, Erik Norvell, Harald Pobloth, Lei"
        },
        {
          "[28] Xuankai Chang, Brian Yan, Kwanghee Choi, Jee-Weon Jung, Yichen Lu, Soumi Maiti, Roshan Sharma,": "Miao, Zhe Wang, Lasse Laaksonen, Adriana Vasilache, et al. Overview of the EVS codec architecture.\nIn"
        },
        {
          "[28] Xuankai Chang, Brian Yan, Kwanghee Choi, Jee-Weon Jung, Yichen Lu, Soumi Maiti, Roshan Sharma,": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 5698–5702,"
        },
        {
          "[28] Xuankai Chang, Brian Yan, Kwanghee Choi, Jee-Weon Jung, Yichen Lu, Soumi Maiti, Roshan Sharma,": "2015."
        },
        {
          "[28] Xuankai Chang, Brian Yan, Kwanghee Choi, Jee-Weon Jung, Yichen Lu, Soumi Maiti, Roshan Sharma,": "[43] Neil Zeghidour, Alejandro Luebs, Ahmed Omran, Jan Skoglund, and Marco Tagliasacchi. SoundStream:"
        },
        {
          "[28] Xuankai Chang, Brian Yan, Kwanghee Choi, Jee-Weon Jung, Yichen Lu, Soumi Maiti, Roshan Sharma,": "An end-to-end neural audio codec.\nIEEE/ACM Transactions on Audio, Speech, and Language Processing,"
        },
        {
          "[28] Xuankai Chang, Brian Yan, Kwanghee Choi, Jee-Weon Jung, Yichen Lu, Soumi Maiti, Roshan Sharma,": "pages 495–507, 2021."
        },
        {
          "[28] Xuankai Chang, Brian Yan, Kwanghee Choi, Jee-Weon Jung, Yichen Lu, Soumi Maiti, Roshan Sharma,": "[44] Zalán Borsos, Raphaël Marinier, Damien Vincent, Eugene Kharitonov, Olivier Pietquin, Matt Sharifi,"
        },
        {
          "[28] Xuankai Chang, Brian Yan, Kwanghee Choi, Jee-Weon Jung, Yichen Lu, Soumi Maiti, Roshan Sharma,": "Dominik Roblek, Olivier Teboul, David Grangier, Marco Tagliasacchi, and Neil Zeghidour. AudioLM:"
        },
        {
          "[28] Xuankai Chang, Brian Yan, Kwanghee Choi, Jee-Weon Jung, Yichen Lu, Soumi Maiti, Roshan Sharma,": "IEEE/ACM Transactions on Audio, Speech, and\nA language modeling approach to audio generation."
        },
        {
          "[28] Xuankai Chang, Brian Yan, Kwanghee Choi, Jee-Weon Jung, Yichen Lu, Soumi Maiti, Roshan Sharma,": "Language Processing, 31:2523–2533, 2023."
        },
        {
          "[28] Xuankai Chang, Brian Yan, Kwanghee Choi, Jee-Weon Jung, Yichen Lu, Soumi Maiti, Roshan Sharma,": "[45] Dongchao Yang, Songxiang Liu, Rongjie Huang, Jinchuan Tian, Chao Weng, and Yuexian Zou. HiFi-"
        },
        {
          "[28] Xuankai Chang, Brian Yan, Kwanghee Choi, Jee-Weon Jung, Yichen Lu, Soumi Maiti, Roshan Sharma,": "Codec: Group-residual vector quantization for high fidelity audio codec. arXiv preprint arXiv:2305.02765,"
        },
        {
          "[28] Xuankai Chang, Brian Yan, Kwanghee Choi, Jee-Weon Jung, Yichen Lu, Soumi Maiti, Roshan Sharma,": "2023."
        },
        {
          "[28] Xuankai Chang, Brian Yan, Kwanghee Choi, Jee-Weon Jung, Yichen Lu, Soumi Maiti, Roshan Sharma,": "[46] Matˇej Korvas, Ondˇrej Plátek, Ondˇrej Dušek, Lukáš Žilka, and Filip Jurˇcíˇcek. Free English and Czech"
        },
        {
          "[28] Xuankai Chang, Brian Yan, Kwanghee Choi, Jee-Weon Jung, Yichen Lu, Soumi Maiti, Roshan Sharma,": "telephone speech corpus shared under the CC-BY-SA 3.0 license. In International Conference on Language"
        },
        {
          "[28] Xuankai Chang, Brian Yan, Kwanghee Choi, Jee-Weon Jung, Yichen Lu, Soumi Maiti, Roshan Sharma,": "Resources and Evaluation (LREC), pages 4423–4428, 2014."
        }
      ],
      "page": 11
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "[47] Wei Han, Zhengdong Zhang, Yu Zhang, Jiahui Yu, Chung-Cheng Chiu, James Qin, Anmol Gulati, Ruoming": "Pang, and Yonghui Wu. ContextNet:\nImproving convolutional neural networks for automatic speech"
        },
        {
          "[47] Wei Han, Zhengdong Zhang, Yu Zhang, Jiahui Yu, Chung-Cheng Chiu, James Qin, Anmol Gulati, Ruoming": "recognition with global context.\nIn Interspeech, pages 3610–3614, 2020."
        },
        {
          "[47] Wei Han, Zhengdong Zhang, Yu Zhang, Jiahui Yu, Chung-Cheng Chiu, James Qin, Anmol Gulati, Ruoming": "[48] Rosana Ardila, Megan Branson, Kelly Davis, Michael Kohler, Josh Meyer, Michael Henretty, Reuben"
        },
        {
          "[47] Wei Han, Zhengdong Zhang, Yu Zhang, Jiahui Yu, Chung-Cheng Chiu, James Qin, Anmol Gulati, Ruoming": "Morais, Lindsay Saunders, Francis Tyers, and Gregor Weber. Common Voice: A massively-multilingual"
        },
        {
          "[47] Wei Han, Zhengdong Zhang, Yu Zhang, Jiahui Yu, Chung-Cheng Chiu, James Qin, Anmol Gulati, Ruoming": "speech corpus.\nIn Language Resources and Evaluation Conference (LREC), pages 4218–4222, 2020."
        },
        {
          "[47] Wei Han, Zhengdong Zhang, Yu Zhang, Jiahui Yu, Chung-Cheng Chiu, James Qin, Anmol Gulati, Ruoming": "[49] Arsha Nagrani, Joon Son Chung, and Andrew Zisserman. VoxCeleb: A large-scale speaker identification"
        },
        {
          "[47] Wei Han, Zhengdong Zhang, Yu Zhang, Jiahui Yu, Chung-Cheng Chiu, James Qin, Anmol Gulati, Ruoming": "dataset.\nIn Interspeech, pages 2616–2620, 2017."
        },
        {
          "[47] Wei Han, Zhengdong Zhang, Yu Zhang, Jiahui Yu, Chung-Cheng Chiu, James Qin, Anmol Gulati, Ruoming": "[50] David Snyder, Daniel Garcia-Romero, Gregory Sell, Daniel Povey, and Sanjeev Khudanpur. X-vectors:"
        },
        {
          "[47] Wei Han, Zhengdong Zhang, Yu Zhang, Jiahui Yu, Chung-Cheng Chiu, James Qin, Anmol Gulati, Ruoming": "Robust DNN embeddings for speaker recognition.\nIn IEEE International Conference on Acoustics, Speech"
        },
        {
          "[47] Wei Han, Zhengdong Zhang, Yu Zhang, Jiahui Yu, Chung-Cheng Chiu, James Qin, Anmol Gulati, Ruoming": "and Signal Processing (ICASSP), pages 5329–5333, 2018."
        },
        {
          "[47] Wei Han, Zhengdong Zhang, Yu Zhang, Jiahui Yu, Chung-Cheng Chiu, James Qin, Anmol Gulati, Ruoming": "[51] Feng Wang, Jian Cheng, Weiyang Liu, and Haijun Liu. Additive margin softmax for face verification."
        },
        {
          "[47] Wei Han, Zhengdong Zhang, Yu Zhang, Jiahui Yu, Chung-Cheng Chiu, James Qin, Anmol Gulati, Ruoming": "IEEE Signal Processing Letters, 25(7):926–930, 2018."
        },
        {
          "[47] Wei Han, Zhengdong Zhang, Yu Zhang, Jiahui Yu, Chung-Cheng Chiu, James Qin, Anmol Gulati, Ruoming": "ECAPA-TDNN: Emphasized channel\n[52] Brecht Desplanques, Jenthe Thienpondt, and Kris Demuynck."
        },
        {
          "[47] Wei Han, Zhengdong Zhang, Yu Zhang, Jiahui Yu, Chung-Cheng Chiu, James Qin, Anmol Gulati, Ruoming": "attention, propagation and aggregation in TDNN based speaker verification.\nIn Interspeech, pages 3830–"
        },
        {
          "[47] Wei Han, Zhengdong Zhang, Yu Zhang, Jiahui Yu, Chung-Cheng Chiu, James Qin, Anmol Gulati, Ruoming": "3834, 2020."
        },
        {
          "[47] Wei Han, Zhengdong Zhang, Yu Zhang, Jiahui Yu, Chung-Cheng Chiu, James Qin, Anmol Gulati, Ruoming": "[53] Carlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe Kazemzadeh, Emily Mower, Samuel Kim, Jeannette N"
        },
        {
          "[47] Wei Han, Zhengdong Zhang, Yu Zhang, Jiahui Yu, Chung-Cheng Chiu, James Qin, Anmol Gulati, Ruoming": "Chang, Sungbok Lee, and Shrikanth S Narayanan.\nIEMOCAP: Interactive emotional dyadic motion"
        },
        {
          "[47] Wei Han, Zhengdong Zhang, Yu Zhang, Jiahui Yu, Chung-Cheng Chiu, James Qin, Anmol Gulati, Ruoming": "capture database. Language resources and evaluation, 42:335–359, 2008."
        },
        {
          "[47] Wei Han, Zhengdong Zhang, Yu Zhang, Jiahui Yu, Chung-Cheng Chiu, James Qin, Anmol Gulati, Ruoming": "[54] Emanuele Bastianelli, Andrea Vanzo, Pawel Swietojanski, and Verena Rieser. SLURP: A spoken language"
        },
        {
          "[47] Wei Han, Zhengdong Zhang, Yu Zhang, Jiahui Yu, Chung-Cheng Chiu, James Qin, Anmol Gulati, Ruoming": "understanding resource package.\nIn Conference on Empirical Methods in Natural Language Processing"
        },
        {
          "[47] Wei Han, Zhengdong Zhang, Yu Zhang, Jiahui Yu, Chung-Cheng Chiu, James Qin, Anmol Gulati, Ruoming": "(EMNLP), pages 7252–7262, 2020."
        },
        {
          "[47] Wei Han, Zhengdong Zhang, Yu Zhang, Jiahui Yu, Chung-Cheng Chiu, James Qin, Anmol Gulati, Ruoming": "[55] Pete Warden. Speech Commands: A dataset for limited-vocabulary speech recognition. arXiv preprint"
        },
        {
          "[47] Wei Han, Zhengdong Zhang, Yu Zhang, Jiahui Yu, Chung-Cheng Chiu, James Qin, Anmol Gulati, Ruoming": "arXiv:1804.03209, 2018."
        },
        {
          "[47] Wei Han, Zhengdong Zhang, Yu Zhang, Jiahui Yu, Chung-Cheng Chiu, James Qin, Anmol Gulati, Ruoming": "Investigating RNN-based\n[56] Cassia Valentini-Botinhao, Xin Wang, Shinji Takaki, and Junichi Yamagishi."
        },
        {
          "[47] Wei Han, Zhengdong Zhang, Yu Zhang, Jiahui Yu, Chung-Cheng Chiu, James Qin, Anmol Gulati, Ruoming": "speech enhancement methods for noise-robust\ntext-to-speech.\nIn Speech Synthesis Workshop, pages"
        },
        {
          "[47] Wei Han, Zhengdong Zhang, Yu Zhang, Jiahui Yu, Chung-Cheng Chiu, James Qin, Anmol Gulati, Ruoming": "146–152, 2016."
        },
        {
          "[47] Wei Han, Zhengdong Zhang, Yu Zhang, Jiahui Yu, Chung-Cheng Chiu, James Qin, Anmol Gulati, Ruoming": "[57] Anmol Gulati, James Qin, Chung-Cheng Chiu, Niki Parmar, Yu Zhang, Jiahui Yu, Wei Han, Shibo Wang,"
        },
        {
          "[47] Wei Han, Zhengdong Zhang, Yu Zhang, Jiahui Yu, Chung-Cheng Chiu, James Qin, Anmol Gulati, Ruoming": "Zhengdong Zhang, Yonghui Wu, and Ruoming Pang. Conformer: Convolution-augmented transformer for"
        },
        {
          "[47] Wei Han, Zhengdong Zhang, Yu Zhang, Jiahui Yu, Chung-Cheng Chiu, James Qin, Anmol Gulati, Ruoming": "speech recognition.\nIn Interspeech, pages 5036–5040, 2020."
        },
        {
          "[47] Wei Han, Zhengdong Zhang, Yu Zhang, Jiahui Yu, Chung-Cheng Chiu, James Qin, Anmol Gulati, Ruoming": "[58] Chandan KA Reddy, Vishak Gopal, and Ross Cutler. DNSMOS P.835: A non-intrusive perceptual objective"
        },
        {
          "[47] Wei Han, Zhengdong Zhang, Yu Zhang, Jiahui Yu, Chung-Cheng Chiu, James Qin, Anmol Gulati, Ruoming": "speech quality metric to evaluate noise suppressors. In IEEE International Conference on Acoustics, Speech"
        },
        {
          "[47] Wei Han, Zhengdong Zhang, Yu Zhang, Jiahui Yu, Chung-Cheng Chiu, James Qin, Anmol Gulati, Ruoming": "and Signal Processing (ICASSP), 2022."
        },
        {
          "[47] Wei Han, Zhengdong Zhang, Yu Zhang, Jiahui Yu, Chung-Cheng Chiu, James Qin, Anmol Gulati, Ruoming": "[59] Zhong-Qiu Wang et al. Sequential multi-frame neural beamforming for speech separation and enhancement."
        },
        {
          "[47] Wei Han, Zhengdong Zhang, Yu Zhang, Jiahui Yu, Chung-Cheng Chiu, James Qin, Anmol Gulati, Ruoming": "In IEEE Spoken Language Technology (SLT) Workshop, pages 905–911, 2021."
        },
        {
          "[47] Wei Han, Zhengdong Zhang, Yu Zhang, Jiahui Yu, Chung-Cheng Chiu, James Qin, Anmol Gulati, Ruoming": "[60] Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever. Robust"
        },
        {
          "[47] Wei Han, Zhengdong Zhang, Yu Zhang, Jiahui Yu, Chung-Cheng Chiu, James Qin, Anmol Gulati, Ruoming": "speech recognition via large-scale weak supervision. arXiv preprint arXiv:2212.04356, 2022."
        },
        {
          "[47] Wei Han, Zhengdong Zhang, Yu Zhang, Jiahui Yu, Chung-Cheng Chiu, James Qin, Anmol Gulati, Ruoming": "[61]\nJoris Cosentino, Manuel Pariente, Samuele Cornell, Antoine Deleforge, and Emmanuel Vincent. LibriMix:"
        },
        {
          "[47] Wei Han, Zhengdong Zhang, Yu Zhang, Jiahui Yu, Chung-Cheng Chiu, James Qin, Anmol Gulati, Ruoming": "An open-source dataset for generalizable speech separation. arXiv preprint arXiv:2005.11262, 2020."
        },
        {
          "[47] Wei Han, Zhengdong Zhang, Yu Zhang, Jiahui Yu, Chung-Cheng Chiu, James Qin, Anmol Gulati, Ruoming": "[62] M. Kolbæk, D. Yu, Z.-H. Tan, and J. Jensen. Multitalker speech separation with utterance-level permutation"
        },
        {
          "[47] Wei Han, Zhengdong Zhang, Yu Zhang, Jiahui Yu, Chung-Cheng Chiu, James Qin, Anmol Gulati, Ruoming": "IEEE/ACM Transactions on Audio, Speech, and\ninvariant\ntraining of deep recurrent neural networks."
        },
        {
          "[47] Wei Han, Zhengdong Zhang, Yu Zhang, Jiahui Yu, Chung-Cheng Chiu, James Qin, Anmol Gulati, Ruoming": "Language Processing, 25:1901–1913, 2017."
        },
        {
          "[47] Wei Han, Zhengdong Zhang, Yu Zhang, Jiahui Yu, Chung-Cheng Chiu, James Qin, Anmol Gulati, Ruoming": "[63] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz"
        },
        {
          "[47] Wei Han, Zhengdong Zhang, Yu Zhang, Jiahui Yu, Chung-Cheng Chiu, James Qin, Anmol Gulati, Ruoming": "Kaiser, and Illia Polosukhin. Attention is all you need.\nIn International Conference on Neural Information"
        },
        {
          "[47] Wei Han, Zhengdong Zhang, Yu Zhang, Jiahui Yu, Chung-Cheng Chiu, James Qin, Anmol Gulati, Ruoming": "Processing Systems (NeurIPS), pages 6000–6010, 2017."
        },
        {
          "[47] Wei Han, Zhengdong Zhang, Yu Zhang, Jiahui Yu, Chung-Cheng Chiu, James Qin, Anmol Gulati, Ruoming": "[64] Keith Ito. The LJ speech dataset. https://keithito.com/LJ-Speech-Dataset/, 2017."
        },
        {
          "[47] Wei Han, Zhengdong Zhang, Yu Zhang, Jiahui Yu, Chung-Cheng Chiu, James Qin, Anmol Gulati, Ruoming": "[65] Saeki Takaaki et al. UTMOS: UTokyo-SaruLab System for VoiceMOS Challenge 2022.\nIn Interspeech,"
        },
        {
          "[47] Wei Han, Zhengdong Zhang, Yu Zhang, Jiahui Yu, Chung-Cheng Chiu, James Qin, Anmol Gulati, Ruoming": "pages 4521–4525, 2022."
        },
        {
          "[47] Wei Han, Zhengdong Zhang, Yu Zhang, Jiahui Yu, Chung-Cheng Chiu, James Qin, Anmol Gulati, Ruoming": "[66] Benjamin van Niekerk, Marc-André Carbonneau, Julian Zaïdi, Matthew Baas, Hugo Seuté, and Herman"
        },
        {
          "[47] Wei Han, Zhengdong Zhang, Yu Zhang, Jiahui Yu, Chung-Cheng Chiu, James Qin, Anmol Gulati, Ruoming": "Kamper.\nA comparison of discrete and soft speech units for\nimproved voice conversion.\nIn IEEE"
        },
        {
          "[47] Wei Han, Zhengdong Zhang, Yu Zhang, Jiahui Yu, Chung-Cheng Chiu, James Qin, Anmol Gulati, Ruoming": "International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 6562–6566, 2022."
        },
        {
          "[47] Wei Han, Zhengdong Zhang, Yu Zhang, Jiahui Yu, Chung-Cheng Chiu, James Qin, Anmol Gulati, Ruoming": "[67] Yi-Chiao Wu, Israel D. Gebru, Dejan Markovi´c, and Alexander Richard. Audiodec: An open-source"
        },
        {
          "[47] Wei Han, Zhengdong Zhang, Yu Zhang, Jiahui Yu, Chung-Cheng Chiu, James Qin, Anmol Gulati, Ruoming": "streaming high-fidelity neural audio codec.\nIn IEEE International Conference on Acoustics, Speech and"
        },
        {
          "[47] Wei Han, Zhengdong Zhang, Yu Zhang, Jiahui Yu, Chung-Cheng Chiu, James Qin, Anmol Gulati, Ruoming": "Signal Processing (ICASSP), pages 1–5, 2023."
        },
        {
          "[47] Wei Han, Zhengdong Zhang, Yu Zhang, Jiahui Yu, Chung-Cheng Chiu, James Qin, Anmol Gulati, Ruoming": "[68] Harishchandra Dubey, Ashkan Aazami, Vishak Gopal, Babak Naderi, Sebastian Braun, Ross Cutler, Alex"
        },
        {
          "[47] Wei Han, Zhengdong Zhang, Yu Zhang, Jiahui Yu, Chung-Cheng Chiu, James Qin, Anmol Gulati, Ruoming": "Ju, Mehdi Zohourian, Min Tang, Mehrsa Golestaneh, et al. ICASSP 2023 deep noise suppression challenge."
        },
        {
          "[47] Wei Han, Zhengdong Zhang, Yu Zhang, Jiahui Yu, Chung-Cheng Chiu, James Qin, Anmol Gulati, Ruoming": "IEEE Open Journal of Signal Processing, 2024."
        }
      ],
      "page": 12
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "[69]\nJort F Gemmeke, Daniel PW Ellis, Dylan Freedman, Aren Jansen, Wade Lawrence, R Channing Moore,": "Manoj Plakal, and Marvin Ritter. Audio Set: An ontology and human-labeled dataset for audio events.\nIn"
        },
        {
          "[69]\nJort F Gemmeke, Daniel PW Ellis, Dylan Freedman, Aren Jansen, Wade Lawrence, R Channing Moore,": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 776–780,"
        },
        {
          "[69]\nJort F Gemmeke, Daniel PW Ellis, Dylan Freedman, Aren Jansen, Wade Lawrence, R Channing Moore,": "2017."
        },
        {
          "[69]\nJort F Gemmeke, Daniel PW Ellis, Dylan Freedman, Aren Jansen, Wade Lawrence, R Channing Moore,": "[70] Eduardo Fonseca, Xavier Favory, Jordi Pons, Frederic Font, and Xavier Serra. FSD50K: an open dataset"
        },
        {
          "[69]\nJort F Gemmeke, Daniel PW Ellis, Dylan Freedman, Aren Jansen, Wade Lawrence, R Channing Moore,": "of human-labeled sound events.\nIEEE/ACM Transactions on Audio, Speech, and Language Processing,"
        },
        {
          "[69]\nJort F Gemmeke, Daniel PW Ellis, Dylan Freedman, Aren Jansen, Wade Lawrence, R Channing Moore,": "30:829–852, 2021."
        },
        {
          "[69]\nJort F Gemmeke, Daniel PW Ellis, Dylan Freedman, Aren Jansen, Wade Lawrence, R Channing Moore,": "[71] Dmitry Bogdanov, Minz Won, Philip Tovstogan, Alastair Porter, and Xavier Serra. The MTG-Jamendo"
        },
        {
          "[69]\nJort F Gemmeke, Daniel PW Ellis, Dylan Freedman, Aren Jansen, Wade Lawrence, R Channing Moore,": "dataset for automatic music tagging.\nIn International Conference on Machine Learning (ICML), 2019."
        },
        {
          "[69]\nJort F Gemmeke, Daniel PW Ellis, Dylan Freedman, Aren Jansen, Wade Lawrence, R Channing Moore,": "[72] Gautham J Mysore. Can we automatically transform speech recorded on common consumer devices in"
        },
        {
          "[69]\nJort F Gemmeke, Daniel PW Ellis, Dylan Freedman, Aren Jansen, Wade Lawrence, R Channing Moore,": "real-world environments into professional production quality speech?—A dataset, insights, and challenges."
        },
        {
          "[69]\nJort F Gemmeke, Daniel PW Ellis, Dylan Freedman, Aren Jansen, Wade Lawrence, R Channing Moore,": "IEEE Signal Processing Letters, 22(8):1006–1010, 2014."
        },
        {
          "[69]\nJort F Gemmeke, Daniel PW Ellis, Dylan Freedman, Aren Jansen, Wade Lawrence, R Channing Moore,": "[73]\nJunichi Yamagishi, Christophe Veaux, and Kirsten MacDonald. CSTR VCTK Corpus: English multi-"
        },
        {
          "[69]\nJort F Gemmeke, Daniel PW Ellis, Dylan Freedman, Aren Jansen, Wade Lawrence, R Channing Moore,": "speaker corpus for CSTR voice cloning toolkit (version 0.92), 2019."
        },
        {
          "[69]\nJort F Gemmeke, Daniel PW Ellis, Dylan Freedman, Aren Jansen, Wade Lawrence, R Channing Moore,": "[74] Zafar Rafii, Antoine Liutkus, Fabian-Robert Stöter, Stylianos Ioannis Mimilakis, and Rachel Bittner. The"
        },
        {
          "[69]\nJort F Gemmeke, Daniel PW Ellis, Dylan Freedman, Aren Jansen, Wade Lawrence, R Channing Moore,": "MUSDB18 corpus for music separation. https://doi.org/10.5281/zenodo.1117372, 2017."
        },
        {
          "[69]\nJort F Gemmeke, Daniel PW Ellis, Dylan Freedman, Aren Jansen, Wade Lawrence, R Channing Moore,": "[75]\nJonathan Shen, Ruoming Pang, Ron J. Weiss, Mike Schuster, Navdeep Jaitly, Zongheng Yang, Zhifeng"
        },
        {
          "[69]\nJort F Gemmeke, Daniel PW Ellis, Dylan Freedman, Aren Jansen, Wade Lawrence, R Channing Moore,": "Chen, Yu Zhang, Yuxuan Wang, Rj Skerrv-Ryan, Rif A. Saurous, Yannis Agiomvrgiannakis, and Yonghui"
        },
        {
          "[69]\nJort F Gemmeke, Daniel PW Ellis, Dylan Freedman, Aren Jansen, Wade Lawrence, R Channing Moore,": "Wu. Natural TTS synthesis by conditioning WaveNet on Mel spectrogram predictions.\nIn IEEE Interna-"
        },
        {
          "[69]\nJort F Gemmeke, Daniel PW Ellis, Dylan Freedman, Aren Jansen, Wade Lawrence, R Channing Moore,": "tional Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 4779–4783, 2018."
        },
        {
          "[69]\nJort F Gemmeke, Daniel PW Ellis, Dylan Freedman, Aren Jansen, Wade Lawrence, R Channing Moore,": "[76] Hideyuki Tachibana, Katsuya Uenoyama, and Shunsuke Aihara.\nEfficiently trainable text-to-speech"
        },
        {
          "[69]\nJort F Gemmeke, Daniel PW Ellis, Dylan Freedman, Aren Jansen, Wade Lawrence, R Channing Moore,": "system based on deep convolutional networks with guided attention.\nIn IEEE International Conference on"
        },
        {
          "[69]\nJort F Gemmeke, Daniel PW Ellis, Dylan Freedman, Aren Jansen, Wade Lawrence, R Channing Moore,": "Acoustics, Speech and Signal Processing (ICASSP), pages 4784–4788, 2018."
        }
      ],
      "page": 13
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "A": "A.1",
          "General Information": "Computational Resources"
        },
        {
          "A": "We designed our benchmark to be computationally accessible. Every task runs on GPUs with 32 GB or more of",
          "General Information": ""
        },
        {
          "A": "",
          "General Information": "VRAM. Tasks like keyword spotting takes only 8 hours, while speech recognition (e.g, ASR-Basque) might"
        },
        {
          "A": "",
          "General Information": "require about 30 hours on a single NVIDIA V100 GPU."
        },
        {
          "A": "A.2",
          "General Information": "Impact"
        },
        {
          "A": "We believe DASB can have a positive impact on the research community. We do not foresee a direct negative",
          "General Information": ""
        },
        {
          "A": "",
          "General Information": "societal impact or misuse of our benchmark. However, we acknowledge that DASB can potentially accelerate"
        },
        {
          "A": "progress in multi-modal",
          "General Information": ""
        },
        {
          "A": "",
          "General Information": "negative uses that society is still working to assess."
        },
        {
          "A": "A.3",
          "General Information": "Hosting and Maintenance Plan"
        },
        {
          "A": "DASB platform is",
          "General Information": "hosted\nand\nversion-tracked"
        },
        {
          "A": "",
          "General Information": "speechbrain/benchmarks. DASB is a community-driven and open-source initiative. We plan to extend"
        },
        {
          "A": "",
          "General Information": "it by running additional experiments and including new audio tokenizers and tasks. We welcome external"
        },
        {
          "A": "contributors.",
          "General Information": ""
        },
        {
          "A": "A.4",
          "General Information": "Licensing"
        },
        {
          "A": "",
          "General Information": "Our work is licensed under Apache 2.0 (https://www.apache.org/licenses/LICENSE-2.0)."
        }
      ],
      "page": 14
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Our work is licensed under Apache 2.0 (https://www.apache.org/licenses/LICENSE-2.0).": ""
        },
        {
          "Our work is licensed under Apache 2.0 (https://www.apache.org/licenses/LICENSE-2.0).": "Model"
        },
        {
          "Our work is licensed under Apache 2.0 (https://www.apache.org/licenses/LICENSE-2.0).": "HuBERT-large"
        },
        {
          "Our work is licensed under Apache 2.0 (https://www.apache.org/licenses/LICENSE-2.0).": "WavLM-large"
        },
        {
          "Our work is licensed under Apache 2.0 (https://www.apache.org/licenses/LICENSE-2.0).": "Wav2vec2-large"
        },
        {
          "Our work is licensed under Apache 2.0 (https://www.apache.org/licenses/LICENSE-2.0).": "EnCodec"
        },
        {
          "Our work is licensed under Apache 2.0 (https://www.apache.org/licenses/LICENSE-2.0).": "DAC"
        },
        {
          "Our work is licensed under Apache 2.0 (https://www.apache.org/licenses/LICENSE-2.0).": "SpeechTokenizer"
        }
      ],
      "page": 14
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Table 6: Features of the Considered Discrete Audio Encoders.": "Dataset"
        },
        {
          "Table 6: Features of the Considered Discrete Audio Encoders.": "LibriSpeech960[46]"
        },
        {
          "Table 6: Features of the Considered Discrete Audio Encoders.": "LibriSpeech960[46]"
        },
        {
          "Table 6: Features of the Considered Discrete Audio Encoders.": "LibriSpeech960[46]"
        },
        {
          "Table 6: Features of the Considered Discrete Audio Encoders.": "DNS [68], CommonVoice [48], AudioSet [69],"
        },
        {
          "Table 6: Features of the Considered Discrete Audio Encoders.": "FSD50K [70], and Jamendo [71]"
        },
        {
          "Table 6: Features of the Considered Discrete Audio Encoders.": "DAPS[72], DNS [68], CommonVoice [48],"
        },
        {
          "Table 6: Features of the Considered Discrete Audio Encoders.": "VCTK [73], MUSDB [74], and Jamendo [71]"
        },
        {
          "Table 6: Features of the Considered Discrete Audio Encoders.": "LibriSpeech960[46]"
        }
      ],
      "page": 15
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Table 7: Dataset and Downstream Models": "1st Architecture"
        },
        {
          "Table 7: Dataset and Downstream Models": "BiLSTM"
        },
        {
          "Table 7: Dataset and Downstream Models": "BiLSTM"
        },
        {
          "Table 7: Dataset and Downstream Models": "ECAPA-TDNN"
        },
        {
          "Table 7: Dataset and Downstream Models": "ECAPA-TDNN"
        },
        {
          "Table 7: Dataset and Downstream Models": "X -Vectors"
        },
        {
          "Table 7: Dataset and Downstream Models": "BiLSTM + Linear"
        },
        {
          "Table 7: Dataset and Downstream Models": "Conformer"
        },
        {
          "Table 7: Dataset and Downstream Models": "Conformer"
        },
        {
          "Table 7: Dataset and Downstream Models": "Shallow Transformer"
        }
      ],
      "page": 15
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "D": "",
          "Additional Analysis of Discrete Audio Decoders": "In this section, we expand on the results from Section 5.3 by including low and high bitrates in addition to"
        },
        {
          "D": "",
          "Additional Analysis of Discrete Audio Decoders": "the medium bitrate. Additionally, for speaker similarity, we measure the cosine similarity between X-vectors"
        },
        {
          "D": "",
          "Additional Analysis of Discrete Audio Decoders": "extracted from the reconstructed and target signals using two different models: WavLM (SpkSim WavLM) and"
        },
        {
          "D": "",
          "Additional Analysis of Discrete Audio Decoders": "ECAPA-TDNN (SpkSim ECAPA), both fine-tuned for speaker verification. When analyzing low and high bitrate"
        },
        {
          "D": "",
          "Additional Analysis of Discrete Audio Decoders": "settings, distinct trends emerge compared to the medium bitrate. At low bitrates, the models perform worse in"
        },
        {
          "D": "",
          "Additional Analysis of Discrete Audio Decoders": "terms of speaker similarity, MOS, and dWER, as expected. This is particularly pronounced for compression-"
        },
        {
          "D": "",
          "Additional Analysis of Discrete Audio Decoders": "based decoders, where the degradation is more significant in terms of dWER. In contrast, at high bitrate, there is"
        },
        {
          "D": "",
          "Additional Analysis of Discrete Audio Decoders": "an overall small improvement in all metrics. In particular, the DAC model consistently outperforms EnCodec"
        },
        {
          "D": "across all evaluated metrics, even for high bitrate settings.",
          "Additional Analysis of Discrete Audio Decoders": ""
        },
        {
          "D": "The analysis shows that bitrate significantly impacts the performance of discrete decoders. Higher bitrates better",
          "Additional Analysis of Discrete Audio Decoders": ""
        },
        {
          "D": "",
          "Additional Analysis of Discrete Audio Decoders": "preserve speech characteristics and result in lower error rates, while lower bitrates degrade these aspects. This"
        },
        {
          "D": "highlights the trade-off between bitrate and speech synthesis quality.",
          "Additional Analysis of Discrete Audio Decoders": ""
        },
        {
          "D": "Table 8: Evaluation of various discrete decoders for the speech re-synthesis task at low, medium, and",
          "Additional Analysis of Discrete Audio Decoders": ""
        },
        {
          "D": "high bitrates.",
          "Additional Analysis of Discrete Audio Decoders": ""
        }
      ],
      "page": 16
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Table 8: Evaluation of various discrete decoders for the speech re-synthesis task at low, medium, and": "high bitrates."
        },
        {
          "Table 8: Evaluation of various discrete decoders for the speech re-synthesis task at low, medium, and": "Models/Metrics"
        },
        {
          "Table 8: Evaluation of various discrete decoders for the speech re-synthesis task at low, medium, and": ""
        },
        {
          "Table 8: Evaluation of various discrete decoders for the speech re-synthesis task at low, medium, and": "Discrete Hubert"
        },
        {
          "Table 8: Evaluation of various discrete decoders for the speech re-synthesis task at low, medium, and": "Discrete WavLM"
        },
        {
          "Table 8: Evaluation of various discrete decoders for the speech re-synthesis task at low, medium, and": "Discrete Wav2Vec2"
        },
        {
          "Table 8: Evaluation of various discrete decoders for the speech re-synthesis task at low, medium, and": "EnCodec"
        },
        {
          "Table 8: Evaluation of various discrete decoders for the speech re-synthesis task at low, medium, and": "DAC"
        },
        {
          "Table 8: Evaluation of various discrete decoders for the speech re-synthesis task at low, medium, and": "SpeechTokenizer"
        },
        {
          "Table 8: Evaluation of various discrete decoders for the speech re-synthesis task at low, medium, and": ""
        },
        {
          "Table 8: Evaluation of various discrete decoders for the speech re-synthesis task at low, medium, and": "Discrete Hubert"
        },
        {
          "Table 8: Evaluation of various discrete decoders for the speech re-synthesis task at low, medium, and": "Discrete WavLM"
        },
        {
          "Table 8: Evaluation of various discrete decoders for the speech re-synthesis task at low, medium, and": "Discrete Wav2Vec2"
        },
        {
          "Table 8: Evaluation of various discrete decoders for the speech re-synthesis task at low, medium, and": "EnCodec"
        },
        {
          "Table 8: Evaluation of various discrete decoders for the speech re-synthesis task at low, medium, and": "DAC"
        },
        {
          "Table 8: Evaluation of various discrete decoders for the speech re-synthesis task at low, medium, and": "SpeechTokenizer"
        },
        {
          "Table 8: Evaluation of various discrete decoders for the speech re-synthesis task at low, medium, and": ""
        },
        {
          "Table 8: Evaluation of various discrete decoders for the speech re-synthesis task at low, medium, and": "EnCodec"
        },
        {
          "Table 8: Evaluation of various discrete decoders for the speech re-synthesis task at low, medium, and": "DAC"
        }
      ],
      "page": 16
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "E": "",
          "Additional Results": "Tables 9 - 12 show the results obtained with 2 different downstream architectures. Note that table 7 indicates"
        },
        {
          "E": "",
          "Additional Results": "the first and second architectures explored for each task. For the continuous baseline, we follow the same"
        },
        {
          "E": "",
          "Additional Results": "architecture as the discrete experiments except for TTS. For the TTS continuous baseline, we use a modified"
        },
        {
          "E": "",
          "Additional Results": "Tacotron2 [75] architecture enhanced with guided attention [76] that predicts SSL representations instead of Mel"
        },
        {
          "E": "spectrograms.",
          "Additional Results": ""
        },
        {
          "E": "Varying the architecture of the downstream decoder leads to significant variations in task performance. For ASR",
          "Additional Results": ""
        },
        {
          "E": "",
          "Additional Results": "tasks, BiLSTM performs better. For classification tasks, ECAPA-TDNN shows the best performance, except"
        },
        {
          "E": "",
          "Additional Results": "for keyword spotting where X-vector is slightly better. For speech enhancement and separation, Conformer"
        },
        {
          "E": "",
          "Additional Results": "shows the best performance. For TTS, a notable pattern is observed: semantic tokens yield the best results with"
        },
        {
          "E": "",
          "Additional Results": "shallow models, while acoustic and hybrid tokens perform better with deeper models but still underperform"
        },
        {
          "E": "",
          "Additional Results": "compared to semantic tokens. One reason might be that discrete SSL models retain higher-level features closer"
        },
        {
          "E": "",
          "Additional Results": "to phonetic transcriptions, requiring lower-capacity models to capture the relationship between raw text and"
        },
        {
          "E": "",
          "Additional Results": "such representations. Higher-capacity models can lead to slower training and potential overfitting. In contrast,"
        },
        {
          "E": "",
          "Additional Results": "shallow models appear to underfit acoustic tokens, resulting in high dWERs and speech-like sounds with only"
        },
        {
          "E": "",
          "Additional Results": "surface resemblance to the original sentence, rather than intelligible speech."
        },
        {
          "E": "",
          "Additional Results": ""
        },
        {
          "E": "",
          "Additional Results": "ASR-En"
        }
      ],
      "page": 17
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "surface resemblance to the original sentence, rather than intelligible speech.": ""
        },
        {
          "surface resemblance to the original sentence, rather than intelligible speech.": ""
        },
        {
          "surface resemblance to the original sentence, rather than intelligible speech.": "Models/Tasks"
        },
        {
          "surface resemblance to the original sentence, rather than intelligible speech.": ""
        },
        {
          "surface resemblance to the original sentence, rather than intelligible speech.": ""
        },
        {
          "surface resemblance to the original sentence, rather than intelligible speech.": ""
        },
        {
          "surface resemblance to the original sentence, rather than intelligible speech.": ""
        },
        {
          "surface resemblance to the original sentence, rather than intelligible speech.": "Discrete Hubert"
        },
        {
          "surface resemblance to the original sentence, rather than intelligible speech.": "Discrete WavLM"
        },
        {
          "surface resemblance to the original sentence, rather than intelligible speech.": "Discrete Wav2Vec2"
        },
        {
          "surface resemblance to the original sentence, rather than intelligible speech.": "EnCodec"
        },
        {
          "surface resemblance to the original sentence, rather than intelligible speech.": "DAC"
        },
        {
          "surface resemblance to the original sentence, rather than intelligible speech.": "SpeechTokenizer"
        },
        {
          "surface resemblance to the original sentence, rather than intelligible speech.": ""
        },
        {
          "surface resemblance to the original sentence, rather than intelligible speech.": "Discrete Hubert"
        },
        {
          "surface resemblance to the original sentence, rather than intelligible speech.": "Discrete WavLM"
        },
        {
          "surface resemblance to the original sentence, rather than intelligible speech.": "Discrete Wav2Vec2"
        },
        {
          "surface resemblance to the original sentence, rather than intelligible speech.": "EnCodec"
        },
        {
          "surface resemblance to the original sentence, rather than intelligible speech.": "DAC"
        },
        {
          "surface resemblance to the original sentence, rather than intelligible speech.": "SpeechTokenizer"
        },
        {
          "surface resemblance to the original sentence, rather than intelligible speech.": ""
        },
        {
          "surface resemblance to the original sentence, rather than intelligible speech.": "EnCodec"
        },
        {
          "surface resemblance to the original sentence, rather than intelligible speech.": "DAC"
        },
        {
          "surface resemblance to the original sentence, rather than intelligible speech.": ""
        },
        {
          "surface resemblance to the original sentence, rather than intelligible speech.": "SSL"
        }
      ],
      "page": 17
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Table 10: Results for discriminative tasks with the second downstream architecture.": ""
        },
        {
          "Table 10: Results for discriminative tasks with the second downstream architecture.": ""
        },
        {
          "Table 10: Results for discriminative tasks with the second downstream architecture.": "WER ↓"
        },
        {
          "Table 10: Results for discriminative tasks with the second downstream architecture.": ""
        },
        {
          "Table 10: Results for discriminative tasks with the second downstream architecture.": "Clean"
        },
        {
          "Table 10: Results for discriminative tasks with the second downstream architecture.": ""
        },
        {
          "Table 10: Results for discriminative tasks with the second downstream architecture.": "11.99"
        },
        {
          "Table 10: Results for discriminative tasks with the second downstream architecture.": "14.98"
        },
        {
          "Table 10: Results for discriminative tasks with the second downstream architecture.": "15.45"
        },
        {
          "Table 10: Results for discriminative tasks with the second downstream architecture.": "90.84"
        },
        {
          "Table 10: Results for discriminative tasks with the second downstream architecture.": "125.00"
        },
        {
          "Table 10: Results for discriminative tasks with the second downstream architecture.": "26.50"
        },
        {
          "Table 10: Results for discriminative tasks with the second downstream architecture.": ""
        },
        {
          "Table 10: Results for discriminative tasks with the second downstream architecture.": "10.91"
        },
        {
          "Table 10: Results for discriminative tasks with the second downstream architecture.": "11.12"
        },
        {
          "Table 10: Results for discriminative tasks with the second downstream architecture.": "12.48"
        },
        {
          "Table 10: Results for discriminative tasks with the second downstream architecture.": "124.00"
        },
        {
          "Table 10: Results for discriminative tasks with the second downstream architecture.": "124.00"
        },
        {
          "Table 10: Results for discriminative tasks with the second downstream architecture.": "118.00"
        },
        {
          "Table 10: Results for discriminative tasks with the second downstream architecture.": ""
        },
        {
          "Table 10: Results for discriminative tasks with the second downstream architecture.": "124.00"
        },
        {
          "Table 10: Results for discriminative tasks with the second downstream architecture.": "122.00"
        },
        {
          "Table 10: Results for discriminative tasks with the second downstream architecture.": ""
        },
        {
          "Table 10: Results for discriminative tasks with the second downstream architecture.": "10.05"
        }
      ],
      "page": 18
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Table 11: Results for generative tasks with the first downstream architecture. N.C. indicates “Not": "Converged\"."
        },
        {
          "Table 11: Results for generative tasks with the first downstream architecture. N.C. indicates “Not": ""
        },
        {
          "Table 11: Results for generative tasks with the first downstream architecture. N.C. indicates “Not": "Models/Tasks"
        },
        {
          "Table 11: Results for generative tasks with the first downstream architecture. N.C. indicates “Not": ""
        },
        {
          "Table 11: Results for generative tasks with the first downstream architecture. N.C. indicates “Not": ""
        },
        {
          "Table 11: Results for generative tasks with the first downstream architecture. N.C. indicates “Not": "Discrete Hubert"
        },
        {
          "Table 11: Results for generative tasks with the first downstream architecture. N.C. indicates “Not": "Discrete WavLM"
        },
        {
          "Table 11: Results for generative tasks with the first downstream architecture. N.C. indicates “Not": "Discrete Wav2Vec2"
        },
        {
          "Table 11: Results for generative tasks with the first downstream architecture. N.C. indicates “Not": "EnCodec"
        },
        {
          "Table 11: Results for generative tasks with the first downstream architecture. N.C. indicates “Not": "DAC"
        },
        {
          "Table 11: Results for generative tasks with the first downstream architecture. N.C. indicates “Not": "SpeechTokenizer"
        },
        {
          "Table 11: Results for generative tasks with the first downstream architecture. N.C. indicates “Not": ""
        },
        {
          "Table 11: Results for generative tasks with the first downstream architecture. N.C. indicates “Not": "Discrete HuBERT"
        },
        {
          "Table 11: Results for generative tasks with the first downstream architecture. N.C. indicates “Not": "Discrete WavLM"
        },
        {
          "Table 11: Results for generative tasks with the first downstream architecture. N.C. indicates “Not": "Discrete Wav2Vec2"
        },
        {
          "Table 11: Results for generative tasks with the first downstream architecture. N.C. indicates “Not": "EnCodec"
        },
        {
          "Table 11: Results for generative tasks with the first downstream architecture. N.C. indicates “Not": "DAC"
        },
        {
          "Table 11: Results for generative tasks with the first downstream architecture. N.C. indicates “Not": "SpeechTokenizer"
        },
        {
          "Table 11: Results for generative tasks with the first downstream architecture. N.C. indicates “Not": ""
        },
        {
          "Table 11: Results for generative tasks with the first downstream architecture. N.C. indicates “Not": "EnCodec"
        },
        {
          "Table 11: Results for generative tasks with the first downstream architecture. N.C. indicates “Not": "DAC"
        },
        {
          "Table 11: Results for generative tasks with the first downstream architecture. N.C. indicates “Not": ""
        },
        {
          "Table 11: Results for generative tasks with the first downstream architecture. N.C. indicates “Not": "SSL"
        }
      ],
      "page": 18
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Table 12: Results for generative tasks with the second downstream architecture. N.C. indicates “Not": "Converged\"."
        },
        {
          "Table 12: Results for generative tasks with the second downstream architecture. N.C. indicates “Not": ""
        },
        {
          "Table 12: Results for generative tasks with the second downstream architecture. N.C. indicates “Not": "Models/Tasks"
        },
        {
          "Table 12: Results for generative tasks with the second downstream architecture. N.C. indicates “Not": ""
        },
        {
          "Table 12: Results for generative tasks with the second downstream architecture. N.C. indicates “Not": ""
        },
        {
          "Table 12: Results for generative tasks with the second downstream architecture. N.C. indicates “Not": "Discrete HuBERT"
        },
        {
          "Table 12: Results for generative tasks with the second downstream architecture. N.C. indicates “Not": "Discrete WavLM"
        },
        {
          "Table 12: Results for generative tasks with the second downstream architecture. N.C. indicates “Not": "Discrete Wav2Vec2"
        },
        {
          "Table 12: Results for generative tasks with the second downstream architecture. N.C. indicates “Not": "EnCodec"
        },
        {
          "Table 12: Results for generative tasks with the second downstream architecture. N.C. indicates “Not": "DAC"
        },
        {
          "Table 12: Results for generative tasks with the second downstream architecture. N.C. indicates “Not": "SpeechTokenizer"
        },
        {
          "Table 12: Results for generative tasks with the second downstream architecture. N.C. indicates “Not": ""
        },
        {
          "Table 12: Results for generative tasks with the second downstream architecture. N.C. indicates “Not": "Discrete HuBERT"
        },
        {
          "Table 12: Results for generative tasks with the second downstream architecture. N.C. indicates “Not": "Discrete WavLM"
        },
        {
          "Table 12: Results for generative tasks with the second downstream architecture. N.C. indicates “Not": "Discrete Wav2Vec2"
        },
        {
          "Table 12: Results for generative tasks with the second downstream architecture. N.C. indicates “Not": "EnCodec"
        },
        {
          "Table 12: Results for generative tasks with the second downstream architecture. N.C. indicates “Not": "DAC"
        },
        {
          "Table 12: Results for generative tasks with the second downstream architecture. N.C. indicates “Not": "SpeechTokenizer"
        },
        {
          "Table 12: Results for generative tasks with the second downstream architecture. N.C. indicates “Not": ""
        },
        {
          "Table 12: Results for generative tasks with the second downstream architecture. N.C. indicates “Not": "EnCodec"
        },
        {
          "Table 12: Results for generative tasks with the second downstream architecture. N.C. indicates “Not": "DAC"
        },
        {
          "Table 12: Results for generative tasks with the second downstream architecture. N.C. indicates “Not": ""
        },
        {
          "Table 12: Results for generative tasks with the second downstream architecture. N.C. indicates “Not": "SSL"
        }
      ],
      "page": 19
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Fundamentals of Speech Recognition",
      "authors": [
        "Lawrence Rabiner",
        "Biing-Hwang Juang"
      ],
      "year": "1993",
      "venue": "Fundamentals of Speech Recognition"
    },
    {
      "citation_id": "2",
      "title": "wav2vec 2.0: A framework for self-supervised learning of speech representations",
      "authors": [
        "Alexei Baevski",
        "Yuhao Zhou",
        "Abdelrahman Mohamed",
        "Michael Auli"
      ],
      "year": "2020",
      "venue": "International Conference on Neural Information Processing Systems (NeurIPS)"
    },
    {
      "citation_id": "3",
      "title": "WavLM: Large-scale self-supervised pre-training for full stack speech processing",
      "authors": [
        "Sanyuan Chen",
        "Chengyi Wang",
        "Zhengyang Chen",
        "Yu Wu",
        "Shujie Liu",
        "Zhuo Chen",
        "Jinyu Li",
        "Naoyuki Kanda",
        "Takuya Yoshioka",
        "Xiong Xiao"
      ],
      "year": "2022",
      "venue": "IEEE Journal of Selected Topics in Signal Processing"
    },
    {
      "citation_id": "4",
      "title": "Self-supervised speech representation learning by masked prediction of hidden units",
      "authors": [
        "Wei-Ning Hsu",
        "Benjamin Bolte",
        "Hubert Yao-Hung",
        "Kushal Tsai",
        "Ruslan Lakhotia",
        "Abdelrahman Salakhutdinov",
        "Mohamed",
        "Hubert"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "5",
      "title": "Speech resynthesis from discrete disentangled self-supervised representations",
      "authors": [
        "Adam Polyak",
        "Yossi Adi",
        "Jade Copet",
        "Eugene Kharitonov",
        "Kushal Lakhotia",
        "Wei-Ning Hsu"
      ],
      "year": "2021",
      "venue": "Abdelrahman Mohamed, and Emmanuel Dupoux"
    },
    {
      "citation_id": "6",
      "title": "Phonetic analysis of self-supervised representations of English speech",
      "authors": [
        "Dan Wells",
        "Hao Tang",
        "Korin Richmond"
      ],
      "year": "2022",
      "venue": "Interspeech"
    },
    {
      "citation_id": "7",
      "title": "Ruoming Pang, and Yonghui Wu. w2v-BERT: Combining contrastive learning and masked language modeling for self-supervised speech pre-training",
      "authors": [
        "Yu-An Chung",
        "Yu Zhang",
        "Wei Han",
        "Chung-Cheng Chiu",
        "James Qin"
      ],
      "year": "2021",
      "venue": "IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)"
    },
    {
      "citation_id": "8",
      "title": "Unified speech tokenizer for speech large language models",
      "authors": [
        "Xin Zhang",
        "Dong Zhang",
        "Shimin Li",
        "Yaqian Zhou",
        "Xipeng Qiu",
        "Speechtokenizer"
      ],
      "venue": "International Conference on Learning Representations (ICLR)"
    },
    {
      "citation_id": "9",
      "title": "FunCodec: A fundamental, reproducible and integrable open-source toolkit for neural speech codec",
      "authors": [
        "Zhihao Du",
        "Shiliang Zhang",
        "Kai Hu",
        "Siqi Zheng"
      ],
      "year": "2023",
      "venue": "FunCodec: A fundamental, reproducible and integrable open-source toolkit for neural speech codec",
      "arxiv": "arXiv:2309.07405"
    },
    {
      "citation_id": "10",
      "title": "Llama 2: Open foundation and fine-tuned chat models",
      "authors": [
        "Hugo Touvron",
        "Louis Martin",
        "Kevin Stone",
        "Peter Albert",
        "Amjad Almahairi",
        "Yasmine Babaei",
        "Nikolay Bashlykov",
        "Soumya Batra",
        "Prajjwal Bhargava",
        "Shruti Bhosale"
      ],
      "year": "2023",
      "venue": "Llama 2: Open foundation and fine-tuned chat models",
      "arxiv": "arXiv:2307.09288"
    },
    {
      "citation_id": "11",
      "title": "PaLM: scaling language modeling with pathways",
      "authors": [
        "Aakanksha Chowdhery",
        "Sharan Narang",
        "Jacob Devlin",
        "Maarten Bosma",
        "Gaurav Mishra",
        "Adam Roberts",
        "Paul Barham",
        "Hyung Chung",
        "Charles Sutton",
        "Sebastian Gehrmann"
      ],
      "year": "2024",
      "venue": "Journal of Machine Learning Research"
    },
    {
      "citation_id": "12",
      "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "Jacob Devlin",
        "Ming-Wei Chang",
        "Kenton Lee",
        "Kristina Toutanova"
      ],
      "year": "2019",
      "venue": "Conference of the North American Chapter of the Association for Computational Linguistics (NAACL): Human Language Technologies"
    },
    {
      "citation_id": "13",
      "title": "GPT understands, too. AI Open",
      "authors": [
        "Xiao Liu",
        "Yanan Zheng",
        "Zhengxiao Du",
        "Ming Ding",
        "Yujie Qian",
        "Zhilin Yang",
        "Jie Tang"
      ],
      "year": "2023",
      "venue": "GPT understands, too. AI Open"
    },
    {
      "citation_id": "14",
      "title": "Neural discrete representation learning",
      "authors": [
        "Aaron Van Den Oord",
        "Oriol Vinyals",
        "Koray Kavukcuoglu"
      ],
      "year": "2017",
      "venue": "Proceedings of the 31st International Conference on Neural Information Processing Systems, NIPS'17"
    },
    {
      "citation_id": "15",
      "title": "A large language model that can speak and listen",
      "authors": [
        "Paul Rubenstein",
        "Chulayuth Asawaroengchai",
        "Dung Duc",
        "Ankur Nguyen",
        "Zalán Bapna",
        "Félix Borsos",
        "De Chaumont",
        "Peter Quitry",
        "Dalia Chen",
        "Wei Badawy",
        "Eugene Han",
        "Hannah Kharitonov",
        "Muckenhirn"
      ],
      "year": "2023",
      "venue": "A large language model that can speak and listen",
      "arxiv": "arXiv:2306.12925"
    },
    {
      "citation_id": "16",
      "title": "LauraGPT: Listen, attend, understand, and regenerate audio with GPT",
      "authors": [
        "Jiaming Wang",
        "Zhihao Du",
        "Qian Chen",
        "Yunfei Chu",
        "Zhifu Gao",
        "Zerui Li",
        "Kai Hu",
        "Xiaohuan Zhou",
        "Jin Xu",
        "Ziyang Ma",
        "Wen Wang",
        "Siqi Zheng",
        "Chang Zhou",
        "Zhijie Yan",
        "Shiliang Zhang"
      ],
      "year": "2023",
      "venue": "LauraGPT: Listen, attend, understand, and regenerate audio with GPT",
      "arxiv": "arXiv:2310.04673"
    },
    {
      "citation_id": "17",
      "title": "VioLA: Unified codec language models for speech recognition, synthesis, and translation",
      "authors": [
        "Tianrui Wang",
        "Long Zhou",
        "Ziqiang Zhang",
        "Yu Wu",
        "Shujie Liu",
        "Yashesh Gaur",
        "Zhuo Chen",
        "Jinyu Li",
        "Furu Wei"
      ],
      "year": "2023",
      "venue": "VioLA: Unified codec language models for speech recognition, synthesis, and translation",
      "arxiv": "arXiv:2305.16107"
    },
    {
      "citation_id": "18",
      "title": "Neural codec language models are zero-shot text to speech synthesizers",
      "authors": [
        "Chengyi Wang",
        "Sanyuan Chen",
        "Yu Wu",
        "Ziqiang Zhang",
        "Long Zhou",
        "Shujie Liu",
        "Zhuo Chen",
        "Yanqing Liu",
        "Huaming Wang",
        "Jinyu Li",
        "Lei He",
        "Sheng Zhao",
        "Furu Wei"
      ],
      "year": "2023",
      "venue": "Neural codec language models are zero-shot text to speech synthesizers",
      "arxiv": "arXiv:2301.02111"
    },
    {
      "citation_id": "19",
      "title": "Neural codec language model as a versatile speech transformer",
      "authors": [
        "Xiaofei Wang",
        "Manthan Thakker",
        "Zhuo Chen",
        "Naoyuki Kanda",
        "Sefik Eskimez",
        "Sanyuan Chen",
        "Min Tang",
        "Shujie Liu",
        "Jinyu Li",
        "Takuya Yoshioka",
        "Speechx"
      ],
      "year": "2023",
      "venue": "Neural codec language model as a versatile speech transformer",
      "arxiv": "arXiv:2308.06873"
    },
    {
      "citation_id": "20",
      "title": "Generating music from text",
      "authors": [
        "Andrea Agostinelli"
      ],
      "year": "2023",
      "venue": "Generating music from text",
      "arxiv": "arXiv:2301.11325"
    },
    {
      "citation_id": "21",
      "title": "AudioGen: Textually guided audio generation",
      "authors": [
        "Felix Kreuk",
        "Gabriel Synnaeve",
        "Adam Polyak",
        "Uriel Singer",
        "Alexandre Défossez",
        "Jade Copet",
        "Devi Parikh",
        "Yaniv Taigman",
        "Yossi Adi"
      ],
      "venue": "International Conference on Learning Representations (ICLR)"
    },
    {
      "citation_id": "22",
      "title": "A family of highly capable multimodal models",
      "authors": [
        "Google Gemini Team",
        "Gemini"
      ],
      "year": "2023",
      "venue": "A family of highly capable multimodal models",
      "arxiv": "arXiv:2312.11805"
    },
    {
      "citation_id": "23",
      "title": "Deep Learning",
      "authors": [
        "Ian Goodfellow",
        "Yoshua Bengio",
        "Aaron Courville"
      ],
      "year": "2016",
      "venue": "Deep Learning"
    },
    {
      "citation_id": "24",
      "title": "Codec-SUPERB: An in-depth analysis of sound codec models",
      "authors": [
        "Haibin Wu",
        "Ho-Lam Chung",
        "Yi-Cheng Lin",
        "Yuan-Kuei Wu",
        "Xuanjun Chen",
        "Yu-Chi Pai",
        "Hsiu-Hsuan Wang",
        "Kai-Wei Chang",
        "Alexander Liu",
        "Hung-Yi Lee"
      ],
      "year": "2024",
      "venue": "Codec-SUPERB: An in-depth analysis of sound codec models",
      "arxiv": "arXiv:2402.13071"
    },
    {
      "citation_id": "25",
      "title": "Discrete audio representation as an alternative to Mel-spectrograms for speaker and speech recognition",
      "authors": [
        "Krishna Puvvada",
        "Nithin Rao Koluguri",
        "Kunal Dhawan",
        "Jagadeesh Balam",
        "Boris Ginsburg"
      ],
      "year": "2024",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "26",
      "title": "SELM: Speech enhancement using discrete tokens and language models",
      "authors": [
        "Ziqian Wang",
        "Xinfa Zhu",
        "Zihan Zhang",
        "Yuanjun Lv",
        "Ning Jiang",
        "Guoqing Zhao",
        "Lei Xie"
      ],
      "year": "2024",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "27",
      "title": "DUB: Discrete unit back-translation for speech translation",
      "authors": [
        "Dong Zhang",
        "Rong Ye",
        "Tom Ko",
        "Mingxuan Wang",
        "Yaqian Zhou"
      ],
      "year": "2023",
      "venue": "Findings of the Association for Computational Linguistics: ACL"
    },
    {
      "citation_id": "28",
      "title": "Exploring speech recognition, translation, and understanding with discrete speech units: A comparative study",
      "authors": [
        "Xuankai Chang",
        "Brian Yan",
        "Kwanghee Choi",
        "Jee-Weon Jung",
        "Yichen Lu",
        "Soumi Maiti",
        "Roshan Sharma",
        "Jiatong Shi",
        "Jinchuan Tian",
        "Shinji Watanabe",
        "Yuya Fujita"
      ],
      "year": "2024",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "29",
      "title": "High fidelity neural audio compression",
      "authors": [
        "Alexandre Défossez",
        "Jade Copet",
        "Gabriel Synnaeve",
        "Yossi Adi"
      ],
      "venue": "Transactions on Machine Learning Research"
    },
    {
      "citation_id": "30",
      "title": "High-fidelity audio compression with improved RVQGAN",
      "authors": [
        "Rithesh Kumar",
        "Prem Seetharaman",
        "Alejandro Luebs",
        "Ishaan Kumar",
        "Kundan Kumar"
      ],
      "venue": "International Conference on Neural Information Processing Systems (NeurIPS)"
    },
    {
      "citation_id": "31",
      "title": "Speech self-supervised representation benchmarking: Are we doing it right?",
      "authors": [
        "Salah Zaiem",
        "Youcef Kemiche",
        "Titouan Parcollet",
        "Slim Essid",
        "Mirco Ravanelli"
      ],
      "year": "2023",
      "venue": "Interspeech"
    },
    {
      "citation_id": "32",
      "title": "A generalpurpose speech toolkit",
      "authors": [
        "Mirco Ravanelli",
        "Titouan Parcollet",
        "Peter Plantinga",
        "Aku Rouhe",
        "Samuele Cornell",
        "Loren Lugosch",
        "Cem Subakan",
        "Nauman Dawalatabad",
        "Abdelwahab Heba",
        "Jianyuan Zhong"
      ],
      "year": "2021",
      "venue": "A generalpurpose speech toolkit",
      "arxiv": "arXiv:2106.04624"
    },
    {
      "citation_id": "33",
      "title": "Exploration of efficient end-to-end ASR using discretized input from self-supervised learning",
      "authors": [
        "Xuankai Chang",
        "Brian Yan",
        "Yuya Fujita",
        "Takashi Maekaku",
        "Shinji Watanabe"
      ],
      "year": "2023",
      "venue": "Interspeech"
    },
    {
      "citation_id": "34",
      "title": "Towards universal speech discrete tokens: A case study for ASR and TTS",
      "authors": [
        "Yifan Yang",
        "Feiyu Shen",
        "Chenpeng Du",
        "Ziyang Ma",
        "Kai Yu",
        "Daniel Povey",
        "Xie Chen"
      ],
      "year": "2024",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "35",
      "title": "TokenSplit: Using discrete speech representations for direct, refined, and transcriptconditioned speech separation and recognition",
      "authors": [
        "Hakan Erdogan",
        "Scott Wisdom",
        "Xuankai Chang",
        "Zalán Borsos",
        "Marco Tagliasacchi",
        "Neil Zeghidour",
        "John Hershey"
      ],
      "year": "2023",
      "venue": "TokenSplit: Using discrete speech representations for direct, refined, and transcriptconditioned speech separation and recognition"
    },
    {
      "citation_id": "36",
      "title": "Evaluating text-to-speech synthesis from a large discrete token-based speech language model",
      "authors": [
        "Siyang Wang",
        "Éva Székely"
      ],
      "year": "2024",
      "venue": "Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING)"
    },
    {
      "citation_id": "37",
      "title": "Neural codec language models are zero-shot text to speech synthesizers",
      "authors": [
        "Chengyi Wang",
        "Sanyuan Chen",
        "Yu Wu",
        "Ziqiang Zhang",
        "Long Zhou",
        "Shujie Liu",
        "Zhuo Chen",
        "Yanqing Liu",
        "Huaming Wang",
        "Jinyu Li"
      ],
      "year": "2023",
      "venue": "Neural codec language models are zero-shot text to speech synthesizers",
      "arxiv": "arXiv:2301.02111"
    },
    {
      "citation_id": "38",
      "title": "Speak, read and prompt: High-fidelity text-to-speech with minimal supervision",
      "authors": [
        "Eugene Kharitonov",
        "Damien Vincent",
        "Zalán Borsos",
        "Raphaël Marinier",
        "Sertan Girgin",
        "Olivier Pietquin",
        "Matt Sharifi",
        "Marco Tagliasacchi",
        "Neil Zeghidour"
      ],
      "year": "2023",
      "venue": "Transactions of the Association for Computational Linguistics"
    },
    {
      "citation_id": "39",
      "title": "How should we extract discrete audio tokens from self-supervised models?",
      "authors": [
        "Pooneh Mousavi",
        "Jarod Duret",
        "Salah Zaiem",
        "Luca Della Libera",
        "Artem Ploujnikov",
        "Cem Subakan",
        "Mirco Ravanelli"
      ],
      "year": "2024",
      "venue": "How should we extract discrete audio tokens from self-supervised models?"
    },
    {
      "citation_id": "40",
      "title": "SUPERB: Speech Processing Universal PERformance Benchmark",
      "authors": [
        "Yang Shu Wen",
        "Po-Han Chi",
        "Yung-Sung Chuang",
        "Cheng-I Jeff Lai",
        "Kushal Lakhotia",
        "Yist Lin",
        "Andy Liu",
        "Jiatong Shi",
        "Xuankai Chang",
        "Guan-Ting Lin"
      ],
      "year": "2021",
      "venue": "SUPERB: Speech Processing Universal PERformance Benchmark"
    },
    {
      "citation_id": "41",
      "title": "Definition of the Opus audio codec",
      "authors": [
        "Jean-Marc Valin",
        "Koen Vos",
        "Timothy Terriberry"
      ],
      "year": "2012",
      "venue": "Definition of the Opus audio codec"
    },
    {
      "citation_id": "42",
      "title": "Overview of the EVS codec architecture",
      "authors": [
        "Martin Dietz",
        "Markus Multrus",
        "Vaclav Eksler",
        "Vladimir Malenovsky",
        "Erik Norvell",
        "Harald Pobloth",
        "Lei Miao",
        "Zhe Wang",
        "Lasse Laaksonen",
        "Adriana Vasilache"
      ],
      "year": "2015",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "43",
      "title": "SoundStream: An end-to-end neural audio codec",
      "authors": [
        "Neil Zeghidour",
        "Alejandro Luebs",
        "Ahmed Omran",
        "Jan Skoglund",
        "Marco Tagliasacchi"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "44",
      "title": "AudioLM: A language modeling approach to audio generation",
      "authors": [
        "Zalán Borsos",
        "Raphaël Marinier",
        "Damien Vincent",
        "Eugene Kharitonov",
        "Olivier Pietquin",
        "Matt Sharifi",
        "Dominik Roblek",
        "Olivier Teboul",
        "David Grangier",
        "Marco Tagliasacchi",
        "Neil Zeghidour"
      ],
      "year": "2023",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "45",
      "title": "Group-residual vector quantization for high fidelity audio codec",
      "authors": [
        "Dongchao Yang",
        "Songxiang Liu",
        "Rongjie Huang",
        "Jinchuan Tian",
        "Chao Weng",
        "Yuexian Zou",
        "Hifi-Codec"
      ],
      "year": "2023",
      "venue": "Group-residual vector quantization for high fidelity audio codec",
      "arxiv": "arXiv:2305.02765"
    },
    {
      "citation_id": "46",
      "title": "Free English and Czech telephone speech corpus shared under the CC-BY-SA 3.0 license",
      "authors": [
        "Matěj Korvas",
        "Ondřej Plátek",
        "Ondřej Dušek",
        "Lukáš Žilka",
        "Filip Jurčíček"
      ],
      "year": "2014",
      "venue": "International Conference on Language Resources and Evaluation (LREC)"
    },
    {
      "citation_id": "47",
      "title": "ContextNet: Improving convolutional neural networks for automatic speech recognition with global context. In Interspeech",
      "authors": [
        "Wei Han",
        "Zhengdong Zhang",
        "Yu Zhang",
        "Jiahui Yu",
        "Chung-Cheng Chiu",
        "James Qin",
        "Anmol Gulati",
        "Ruoming Pang",
        "Yonghui Wu"
      ],
      "year": "2020",
      "venue": "ContextNet: Improving convolutional neural networks for automatic speech recognition with global context. In Interspeech"
    },
    {
      "citation_id": "48",
      "title": "Common Voice: A massively-multilingual speech corpus",
      "authors": [
        "Rosana Ardila",
        "Megan Branson",
        "Kelly Davis",
        "Michael Kohler",
        "Josh Meyer",
        "Michael Henretty",
        "Reuben Morais",
        "Lindsay Saunders",
        "Francis Tyers",
        "Gregor Weber"
      ],
      "year": "2020",
      "venue": "Language Resources and Evaluation Conference (LREC)"
    },
    {
      "citation_id": "49",
      "title": "VoxCeleb: A large-scale speaker identification dataset",
      "authors": [
        "Arsha Nagrani",
        "Son Chung",
        "Andrew Zisserman"
      ],
      "year": "2017",
      "venue": "Interspeech"
    },
    {
      "citation_id": "50",
      "title": "X-vectors: Robust DNN embeddings for speaker recognition",
      "authors": [
        "David Snyder",
        "Daniel Garcia-Romero",
        "Gregory Sell",
        "Daniel Povey",
        "Sanjeev Khudanpur"
      ],
      "year": "2018",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "51",
      "title": "Additive margin softmax for face verification",
      "authors": [
        "Feng Wang",
        "Jian Cheng",
        "Weiyang Liu",
        "Haijun Liu"
      ],
      "year": "2018",
      "venue": "IEEE Signal Processing Letters"
    },
    {
      "citation_id": "52",
      "title": "ECAPA-TDNN: Emphasized channel attention, propagation and aggregation in TDNN based speaker verification",
      "authors": [
        "Brecht Desplanques",
        "Jenthe Thienpondt",
        "Kris Demuynck"
      ],
      "year": "2020",
      "venue": "Interspeech"
    },
    {
      "citation_id": "53",
      "title": "IEMOCAP: Interactive emotional dyadic motion capture database. Language resources and evaluation",
      "authors": [
        "Carlos Busso",
        "Murtaza Bulut",
        "Chi-Chun Lee",
        "Abe Kazemzadeh",
        "Emily Mower",
        "Samuel Kim",
        "Jeannette Chang",
        "Sungbok Lee",
        "Shrikanth S Narayanan"
      ],
      "year": "2008",
      "venue": "IEMOCAP: Interactive emotional dyadic motion capture database. Language resources and evaluation"
    },
    {
      "citation_id": "54",
      "title": "SLURP: A spoken language understanding resource package",
      "authors": [
        "Emanuele Bastianelli",
        "Andrea Vanzo",
        "Pawel Swietojanski",
        "Verena Rieser"
      ],
      "year": "2020",
      "venue": "Conference on Empirical Methods in Natural Language Processing (EMNLP)"
    },
    {
      "citation_id": "55",
      "title": "Speech Commands: A dataset for limited-vocabulary speech recognition",
      "authors": [
        "Pete Warden"
      ],
      "year": "2018",
      "venue": "Speech Commands: A dataset for limited-vocabulary speech recognition",
      "arxiv": "arXiv:1804.03209"
    },
    {
      "citation_id": "56",
      "title": "Investigating RNN-based speech enhancement methods for noise-robust text-to-speech",
      "authors": [
        "Cassia Valentini-Botinhao",
        "Xin Wang",
        "Shinji Takaki",
        "Junichi Yamagishi"
      ],
      "year": "2016",
      "venue": "Speech Synthesis Workshop"
    },
    {
      "citation_id": "57",
      "title": "Conformer: Convolution-augmented transformer for speech recognition",
      "authors": [
        "Anmol Gulati",
        "James Qin",
        "Chung-Cheng Chiu",
        "Niki Parmar",
        "Yu Zhang",
        "Jiahui Yu",
        "Wei Han",
        "Shibo Wang",
        "Zhengdong Zhang",
        "Yonghui Wu",
        "Ruoming Pang"
      ],
      "year": "2020",
      "venue": "Conformer: Convolution-augmented transformer for speech recognition"
    },
    {
      "citation_id": "58",
      "title": "A non-intrusive perceptual objective speech quality metric to evaluate noise suppressors",
      "authors": [
        "K Chandan",
        "Vishak Reddy",
        "Ross Gopal",
        "Cutler",
        "Dnsmos P"
      ],
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "59",
      "title": "Sequential multi-frame neural beamforming for speech separation and enhancement",
      "authors": [
        "Zhong-Qiu Wang"
      ],
      "year": "2021",
      "venue": "IEEE Spoken Language Technology (SLT) Workshop"
    },
    {
      "citation_id": "60",
      "title": "Robust speech recognition via large-scale weak supervision",
      "authors": [
        "Alec Radford",
        "Jong Kim",
        "Tao Xu",
        "Greg Brockman",
        "Christine Mcleavey",
        "Ilya Sutskever"
      ],
      "year": "2022",
      "venue": "Robust speech recognition via large-scale weak supervision",
      "arxiv": "arXiv:2212.04356"
    },
    {
      "citation_id": "61",
      "title": "LibriMix: An open-source dataset for generalizable speech separation",
      "authors": [
        "Joris Cosentino",
        "Manuel Pariente",
        "Samuele Cornell",
        "Antoine Deleforge",
        "Emmanuel Vincent"
      ],
      "year": "2020",
      "venue": "LibriMix: An open-source dataset for generalizable speech separation",
      "arxiv": "arXiv:2005.11262"
    },
    {
      "citation_id": "62",
      "title": "Multitalker speech separation with utterance-level permutation invariant training of deep recurrent neural networks",
      "authors": [
        "M Kolbaek",
        "D Yu",
        "Z.-H Tan",
        "J Jensen"
      ],
      "year": "2017",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "63",
      "title": "Attention is all you need",
      "authors": [
        "Ashish Vaswani",
        "Noam Shazeer",
        "Niki Parmar",
        "Jakob Uszkoreit",
        "Llion Jones",
        "Aidan Gomez",
        "Lukasz Kaiser",
        "Illia Polosukhin"
      ],
      "year": "2017",
      "venue": "International Conference on Neural Information Processing Systems (NeurIPS)"
    },
    {
      "citation_id": "64",
      "title": "The LJ speech dataset",
      "authors": [
        "Keith Ito"
      ],
      "year": "2017",
      "venue": "The LJ speech dataset"
    },
    {
      "citation_id": "65",
      "title": "UTMOS: UTokyo-SaruLab System for VoiceMOS Challenge 2022",
      "authors": [
        "Saeki Takaaki"
      ],
      "year": "2022",
      "venue": "Interspeech"
    },
    {
      "citation_id": "66",
      "title": "A comparison of discrete and soft speech units for improved voice conversion",
      "authors": [
        "Marc-André Benjamin Van Niekerk",
        "Julian Carbonneau",
        "Matthew Zaïdi",
        "Hugo Baas",
        "Herman Seuté",
        "Kamper"
      ],
      "year": "2022",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "67",
      "title": "Audiodec: An open-source streaming high-fidelity neural audio codec",
      "authors": [
        "Yi-Chiao Wu",
        "Israel Gebru",
        "Dejan Marković",
        "Alexander Richard"
      ],
      "year": "2023",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "68",
      "title": "ICASSP 2023 deep noise suppression challenge",
      "authors": [
        "Harishchandra Dubey",
        "Ashkan Aazami",
        "Vishak Gopal",
        "Babak Naderi",
        "Sebastian Braun",
        "Ross Cutler",
        "Alex Ju",
        "Mehdi Zohourian",
        "Min Tang",
        "Mehrsa Golestaneh"
      ],
      "year": "2024",
      "venue": "IEEE Open Journal of Signal Processing"
    },
    {
      "citation_id": "69",
      "title": "Audio Set: An ontology and human-labeled dataset for audio events",
      "authors": [
        "Jort F Gemmeke",
        "P Daniel",
        "Dylan Ellis",
        "Aren Freedman",
        "Wade Jansen",
        "R Channing Lawrence",
        "Manoj Moore",
        "Marvin Plakal",
        "Ritter"
      ],
      "year": "2017",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "70",
      "title": "FSD50K: an open dataset of human-labeled sound events",
      "authors": [
        "Eduardo Fonseca",
        "Xavier Favory",
        "Jordi Pons",
        "Frederic Font",
        "Xavier Serra"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "71",
      "title": "The MTG-Jamendo dataset for automatic music tagging",
      "authors": [
        "Dmitry Bogdanov",
        "Minz Won",
        "Philip Tovstogan",
        "Alastair Porter",
        "Xavier Serra"
      ],
      "year": "2019",
      "venue": "International Conference on Machine Learning (ICML)"
    },
    {
      "citation_id": "72",
      "title": "Can we automatically transform speech recorded on common consumer devices in real-world environments into professional production quality speech?-A dataset, insights, and challenges",
      "authors": [
        "J Gautham",
        "Mysore"
      ],
      "year": "2014",
      "venue": "IEEE Signal Processing Letters"
    },
    {
      "citation_id": "73",
      "title": "CSTR VCTK Corpus: English multispeaker corpus for CSTR voice cloning toolkit",
      "authors": [
        "Junichi Yamagishi",
        "Christophe Veaux",
        "Kirsten Macdonald"
      ],
      "year": "2019",
      "venue": "CSTR VCTK Corpus: English multispeaker corpus for CSTR voice cloning toolkit"
    },
    {
      "citation_id": "74",
      "title": "Stylianos Ioannis Mimilakis, and Rachel Bittner. The MUSDB18 corpus for music separation",
      "authors": [
        "Zafar Rafii",
        "Antoine Liutkus",
        "Fabian-Robert Stöter"
      ],
      "year": "2017",
      "venue": "Stylianos Ioannis Mimilakis, and Rachel Bittner. The MUSDB18 corpus for music separation",
      "doi": "10.5281/zenodo.1117372"
    },
    {
      "citation_id": "75",
      "title": "Natural TTS synthesis by conditioning WaveNet on Mel spectrogram predictions",
      "authors": [
        "Jonathan Shen",
        "Ruoming Pang",
        "Ron Weiss",
        "Mike Schuster",
        "Navdeep Jaitly",
        "Zongheng Yang",
        "Zhifeng Chen",
        "Yu Zhang",
        "Yuxuan Wang",
        "Rj Skerrv-Ryan",
        "Rif Saurous",
        "Yannis Agiomvrgiannakis",
        "Yonghui Wu"
      ],
      "year": "2018",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "76",
      "title": "Efficiently trainable text-to-speech system based on deep convolutional networks with guided attention",
      "authors": [
        "Hideyuki Tachibana",
        "Katsuya Uenoyama",
        "Shunsuke Aihara"
      ],
      "year": "2018",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    }
  ]
}