{
  "paper_id": "2011.01631v2",
  "title": "Robust Latent Representations Via Cross-Modal Translation And Alignment",
  "published": "2020-11-03T11:18:04Z",
  "authors": [
    "Vandana Rajan",
    "Alessio Brutti",
    "Andrea Cavallaro"
  ],
  "keywords": [
    "Cross-modal knowledge transfer",
    "multi-modal training uni-modal testing",
    "emotion recognition"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Multi-modal learning relates information across observation modalities of the same physical phenomenon to leverage complementary information. Most multi-modal machine learning methods require that all the modalities used for training are also available for testing. This is a limitation when signals from some modalities are unavailable or severely degraded. To address this limitation, we aim to improve the testing performance of uni-modal systems using multiple modalities during training only. The proposed multi-modal training framework uses cross-modal translation and correlation-based latent space alignment to improve the representations of a worse performing (or weaker) modality. The translation from the weaker to the better performing (or stronger) modality generates a multi-modal intermediate encoding that is representative of both modalities. This encoding is then correlated with the stronger modality representation in a shared latent space. We validate the proposed framework on the AVEC 2016 dataset (RECOLA) for continuous emotion recognition and show the effectiveness of the framework that achieves state-ofthe-art (uni-modal) performance for weaker modalities.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "The term modality refers to the particular form in which something exists or is experienced or expressed  [1] . Most physical phenomena we experience consist of multiple modalities; for example, we can see, hear and touch the rain; objects around us may have their own characteristic shape, sound and smell. The information to explain an event is often unevenly spread across the modalities that capture this event. Multi-modal machine learning uses multiple modalities to model or explain an event, whereas uni-modal or mono-modal machine learning uses only one of these modalities  [2] .\n\nThe uni-modal performance of individual modalities on any task may be significantly different  [3] ; modalities whose individual performance is comparatively better (worse) are referred to as stronger (weaker) modalities  [4] . For a given task, we can rank the available modalities according to their uni-modal performance. Multi-modal fusion methods combine the supplementary and complementary information provided by these modalities to improve performance compared to uni-modal methods  [5] [6]  [7] . However, in general, most multi-modal fusion techniques require for the testing phase the simultaneous presence of all the modalities that were used during the model training phase  [1] . This requirement becomes a severe limitation in case one or more sensors are missing or their signals are severely corrupted by noise during testing, unless such situations are explicitly handled by the modelling framework  [8] . Thus, it would be desirable to improve the testing performance of individual modalities using other modalities during training  [3] [9]  [10] . In particular, since the individual modalities corresponding to the same physical phenomenon might not perform equally well on the downstream task, our aim is to improve the uni-modal testing performance of a weaker modality by exploiting a stronger modality during training.\n\nTo this end, we propose Stronger Enhancing Weaker (SEW), a framework for improving the testing performance of a weaker modality by exploiting a stronger modality during the training phase only. SEW is a supervised neural network framework for knowledge transfer across modalities. During training, the stronger modality serves as an auxiliary or guiding modality that helps to create weaker-modality representations that are more discriminative than the representations obtained using uni-modal training for a classification or regression task. We achieve this by combining weaker-to-stronger modality translation and feature alignment with the stronger modality representations. This solution is based on the intuition that inter-modal translation can create intermediate representations that capture joint information between both modalities. Explicitly aligning the intermediate and the stronger modality representations further encourages the framework to discover components of the weaker modality that are maximally correlated with the stronger modality. Note that, after using SEW for training, the stronger modality is no longer required at testing. We show the effectiveness of our framework on the AVEC 2016 audio-visual continuous emotion recognition tasks and show that SEW improves the uni-modal performance of weaker modalities.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Related Work",
      "text": "Most works on multi-modal training for uni-modal performance enhancement are designed for tasks where the different modalities are different types of images. For example, multi-modal training using RGB and depth images improves the uni-modal performance for hand gesture recognition  [3] . This is achieved by forcing the modality-specific parts of the network to learn a common correlation matrix for their intermediate feature maps. Depth images also improve the test-time performance of RGB images for action recognition using an adversarial loss for feature alignment  [10] . However, the modalities considered in these methods are images of equal size and the uni-modal networks have the same architecture, thus preventing their direct application to distinct modalities like audio, video and text, whose feature types and dimensionality differ. A few works have been proposed to address this problem  [9] [11]  [12] . For sentiment analysis, a sequence-to-sequence network with cyclic translation across modalities generates an intermediate representation that is robust to missing modalities during testing  [11] . A multimodal co-learning framework improves the uni-modal performance of the text modality via training using audio, video and text modalities  [12] . However, these methods primarily benefit the uni-modal performance of the text modality, which is the stronger modality for the task. In contrast, we aim to explicitly improve the weaker modality using the stronger modality during training. A joint audiovisual training and cross-modal triplet loss can be used to develop a face/speech emotion recognition system using multi-modal training  [9] . However, in such a system the weaker modality may degrade the performance of the stronger modality  [9] .",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Proposed Method",
      "text": "In this section, we describe our proposed Stronger Enhancing Weaker (SEW), a supervised neural network framework, which uses jointly the stronger and the weaker modality representations during training to improve the testing performance of the weaker modality (Figure  1 ). The key concepts of our framework are intermodality translation and feature alignment. These concepts are implemented using four main modules: an inter-modal translator, an intra-modal auto-encoder, a feature alignment module and a task-specific regressor or classifier. These modules are described next.\n\nThe inter-modal translator contains an encoder, WE and a decoder, SD1. The translator takes the features of the weaker modality, MW, as input and produces the features of the stronger modality, MSW, as output. The encoder of the inter-modal translator, creates intermediate representations, msw, that capture joint information across modalities. This is achieved by using a translation loss, Ltr, between the true, MS, and the predicted, MSW, features of the stronger modality:\n\n(\n\nWE is encouraged to discover components of the weaker modality that are inclined towards the stronger modality by increasing the alignment between msw and the representations of the stronger modality. For this purpose, we project the stronger modality features into the same latent space as msw. We use an intra-modal autoencoder to create stronger modality representations, mss, of the same dimensionality as that of the inter-modal translator representations, msw. To this end, we employ an auto-encoding loss, Lae, between true, MS, and predicted, MS, features:\n\n(\n\nFor modality reconstructions, we use Mean-Square-Error (MSE) as Ltr and Lae  [11] .\n\nA feature alignment loss, L al , ensures that the intermediate representations of the inter-modal translator are maximally aligned to the stronger modality representations:\n\nFollowing  [13] [14], we use Canonical Correlation Analysis (CCA) for feature alignment, such that L al = -CCA. CCA for deep neural networks, also known as Deep CCA or DCCA, is a method to learn complex nonlinear transformations of data from two different modalities, such that the resulting representations are highly linearly correlated  [15] . For a training set of size p, Ms ∈ R d 1 ×p and Mw ∈ R d 2 ×p are the input matrices corresponding to the stronger and the weaker modalities, respectively. mss ∈ R d×p and msw ∈ R d×p are the representations obtained by nonlinear transformations introduced by the layers in the encoders SE and WE, respectively. Note\n\nIf mss and msw are the mean-centred versions of mss and msw, respectively, then the total correlation of the top-K components of mss and msw is the sum of the top-K singular values of the matrix,\n\n, in which the self (Σs, Σw) and cross covariance (Σsw) matrices are given by where r1 > 0 and r2 > 0 are regularisation constants. We use the gradient of correlation obtained on the training data to determine (θ es , θ ew ).\n\nFinally, the task-specific regressor or classification module, which takes the inter-modal translator representations as input, ensures the discriminative ability of the resulting latent space. We use a prediction loss, Lpr, that operates on the true, T l , and predicted task labels, P l , as:\n\nThe total training loss, L combines the four components:\n\nwhere α, β and γ are hyper-parameters. After training, all the components except the encoder, WE, and the regressor, R, are removed and the stronger modality is not required at the testing (deployment) phase.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Validation",
      "text": "In this section, we compare the performance of SEW with other unimodal methods  [16] [17]  [18] [19] and a state-of-the-art cross-modal knowledge transfer method  [9] . We describe the dataset, the evaluation metrics, the details about the architecture and training, and present an ablation study, which quantifies the contributions of different parts of SEW.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Dataset And Evaluation Measures",
      "text": "We use RECOLA, the AVEC 2016 emotion recognition dataset  [16] , which contains audiovisual recordings of spontaneous and natural interactions from 27 French-speaking participants. Continuous dimensional emotion annotations (in the range [-1,1]) in terms of both arousal (level of activation or intensity) and valence (level of positiveness or negativeness) are provided with a constant frame rate of 40 ms for the first five minutes of each recording, by averaging the annotations from all annotators and also taking the inter-evaluator agreement into consideration  [16] . The dataset is equally divided into three sets, by balancing gender, age, and mother-tongue of the participants with each set consisting of nine unique recordings, resulting in 67.5k segments in total for each part (training, development and test). Since, the test labels are not publicly available, we report the results on the development set. We have used the same audio and video features as the AVEC 2015 and 2016 baselines  [16]  for a fair comparison with the previous literature. These are 88-D extended Geneva Minimalistic Acoustic Parameter Set (eGeMAPS) features extracted using openSMILE, LGBP-TOP based 168-D video-appearance features and 49 facial landmarks based 632-D video-geometric features. It is to be noted that the dataset provides separate features for arousal and valence. As in  [9] [16], to compensate for the delay in annotation, we shift the ground-truth labels back in time by 2.4 s. This dataset is ideal for our objective, since the uni-modal performance of audio and video features varies considerably for arousal and valence, as reported in  [9]  and confirmed by our experiments (see Table  1 ). As in the AVEC 2016 challenge, we use the Concordance Correlation Coefficient (CCC) (eq. 10) as the primary evaluation metric:\n\nwhere x and y are the true and the predicted labels, respectively, and µx, µy, σx, σy and σxy refer to their means, variances and covariance, respectively. We also evaluate the binary classification results by separating the true and predicted annotations into negative [-1,0] and positive (0,1] classes.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Experiments",
      "text": "In order to identify the stronger and weaker modalities, we first assess the unimodal performances of audio, video-geometric and video-appearance features for arousal and valence using a regressor similar to  [9] . The regressor consists of 4 single time-step GRU-RNN layers, each made up of 120 neurons, followed by a linear layer and trained using the MSE loss. The unimodal results thus obtained are shown in Table  1 . For arousal, the performance of audio surpasses both video-geometric and video-appearance features. For valence, the video-geometric features outperform audio and video-appearance features. Thus, we have 5 cases for crossmodal knowledge transfer from stronger to weaker modalities, namely video-geo(+audio) and video-app(+audio) for arousal and audio(+video-geo), video-app(+audio) and video-app(+video-geo) for valence, where the modality in parenthesis indicates the stronger modality.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Architecture And Training",
      "text": "Because the proposed method combines multi-modal data with different characteristics, it was necessary to adjust various architectural parameters according to the characteristics of the given modalities rather than solving the problem using a generic model. Hence, the encoders and decoders of both inter-modal translator and intramodal auto-encoder of the 5 multi-modal combinations vary from each other. Specifically, the encoder and decoder for each modality differ in terms of the number of linear layers and the number of neurons in each layer. Since the provided video-appearance features were already refined using PCA, we did not reduce the dimensionality further and used a single linear layer of size 168 for both its encoder and decoder. Thus, for all combinations that contain videoappearance features, the size of the latent dimension was 168. For all the rest, it was 128. The encoder and decoder for video-geometric features use linear layers of size [512, 256, 128] and [256, 512, 632], respectively with tanh activation between layers. For audio features, these were [108, 128] and  [108, 88] . Note that 632 and 88 were chosen to match the dimensionality of the video-geometric and audio features, respectively. All the models were developed, trained and tested using PyTorch. We used the SGD optimiser with learning rate 0.001, momentum 0.7 and weight decay regularisation. The batch size was 32. The number of CCA components, K, was 10 in all the experiments. The contribution of each loss component was equally important:   [17]  0.502 0.512 0.519 0.529 MTL (PU)  [18]  0.508 0.502 0.506 0.468 DDAT (RE)  [19]  0.544 0.539 0.508 0.528 DDAT (PU)  [19]  0.513 0.518 0.498 0.514 EmoBed  [9]  0.527 0.549 0.521 0.564 SEW 0.565 0.544 0.552 0.554",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Results",
      "text": "Table  2  reports the results using the full SEW framework as well as after ablating individual components. The bottom row provides the uni-modal results for the weaker modalities for ease of comparison with the SEW results. Comparing the last 2 rows, we can see that the SEW-(CCA&SD1) results are close to the uni-modal results of the weaker modality. This is as expected since SEW-(CCA&SD1) contains only the WE and regressor with no interaction with the stronger modality. In all the 5 cases, SEW was able to improve the results from the uni-modal and SEW-(CCA&SD1) models both in terms of CCC and binary accuracy. For arousal video-geo(+audio) and video-app(+audio), removing the CCA based alignment causes a drop of 0.053 and 0.036, respectively in CCC and 2.9% and 4.1%, respectively in binary accuracy. The corresponding numbers for valence audio(+video-geo) are 0.056 and 2.5%, respectively. These observations support the significance of the CCA based distribution alignment in the SEW framework. For valence video-app(+audio) and video-app(+video-geo), removing the decoder of the inter-modal translator causes a drop of 0.040 and 0.044, respectively in CCC and 4.4% and 4.8%, respectively in binary accuracy, which indicates the effectiveness of the weaker-to-stronger modality translation.\n\nIn Table  3 , we compare the best uni-modal results of SEW with the 4 most relevant uni-modal models  [16] [17]  [18] [19] and a cross-modal training method  [9]  in terms of CCC.  [16]  provides the baseline results on the RECOLA dataset for the AVEC 2016 challenge. The uni-modal baseline used an SVM based classifier on the individual features. SEW significantly outperforms the baseline uni-modal results for all the weaker modalities considered. Our method is also able to improve the uni-modal results for all the cases from  [19] , which uses difficulty awareness based training and  [17]   [18]  which uses multi-task learning. SEW outperforms EmoBed  [9]  for arousal video-geo(+audio) and valence audio(+video-geo) by a margin of 0.038 and 0.031, respectively, in CCC. For arousal video-app(+audio), the performance of SEW and EmoBed are very close (0.544 and 0.549, respectively). However, for the valence video-app features, EmoBed outperforms SEW. The top and bottom rows of Table  2  show that SEW improves the unimodal performance of the weaker modalities. Specifically, to the best of our knowledge, the best results to date on the uni-modal performance of arousal video-geometric features and valence audio features have been achieved by SEW.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Conclusion",
      "text": "We exploited the gap between the uni-modal performance of different modalities on a task to improve, using a stronger modality in a new training framework, the performance of the weaker modality. Our proposed framework, Stronger Enhancing Weaker (SEW), enables cross-modal knowledge transfer from the stronger to the weaker modality. The results of SEW on the RECOLA dataset for the task of continuous emotion recognition show its ability to improve the uni-modal performance of a weaker modality using a stronger modality during training.\n\nFuture work includes applying the SEW framework to other tasks involving different features and modalities as well as extending SEW to cope with multi-modal sequential data.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: ). The key concepts of our framework are inter-",
      "page": 2
    },
    {
      "caption": "Figure 1: The proposed SEW training framework. (MS,MW) de-",
      "page": 2
    }
  ],
  "tables": [
    {
      "caption": "Table 3: Performance comparison of SEW with other methods in audio(+video-geo)byamarginof0.038and0.031,respectively,in",
      "data": [
        {
          "video-geo(+audio)": "CCC",
          "video-app(+audio)": "CCC",
          "audio(+video-geo)": "CCC",
          "video-app(+video-geo)": "CCC"
        },
        {
          "video-geo(+audio)": "0.565\n0.532\n0.512\n0.514\n0.484\n0.482",
          "video-app(+audio)": "0.554\n0.539\n0.532\n0.514\n0.497\n0.489",
          "audio(+video-geo)": "0.552\n0.486\n0.496\n0.556\n0.545\n0.543",
          "video-app(+video-geo)": "0.549\n0.540\n0.546\n0.505\n0.491\n0.489"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 3: Performance comparison of SEW with other methods in audio(+video-geo)byamarginof0.038and0.031,respectively,in",
      "data": [
        {
          "video-geo": "0.379\n0.502\n0.508\n0.544\n0.513\n0.527\n0.565",
          "video-app": "0.474\n0.529\n0.468\n0.528\n0.514\n0.564\n0.554",
          "audio": "0.455\n0.519\n0.506\n0.508\n0.498\n0.521\n0.552"
        }
      ],
      "page": 4
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Multimodal machine learning: A survey and taxonomy",
      "authors": [
        "Tadas Baltrušaitis",
        "Chaitanya Ahuja",
        "Louis-Philippe Morency"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "3",
      "title": "Tutorial on Multimodal Machine Learning",
      "authors": [
        "Louis-Philippe Morency",
        "Tadas Baltrusaitis"
      ],
      "year": "2020",
      "venue": "Tutorial on Multimodal Machine Learning"
    },
    {
      "citation_id": "4",
      "title": "Improving the performance of unimodal dynamic handgesture recognition with multimodal training",
      "authors": [
        "Mahdi Abavisani",
        "Reza Hamid",
        "Joze Vaezi",
        "M Vishal",
        "Patel"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "5",
      "title": "Cross and learn: Cross-modal self-supervision",
      "authors": [
        "Nawid Sayed",
        "Biagio Brattoli",
        "Björn Ommer"
      ],
      "year": "2018",
      "venue": "Proceedings of the German Conference on Pattern Recognition"
    },
    {
      "citation_id": "6",
      "title": "Multimodal emotion recognition from expressive faces, body gestures and speech",
      "authors": [
        "George Caridakis",
        "Ginevra Castellano",
        "Loic Kessous",
        "Amaryllis Raouzaiou",
        "Lori Malatesta",
        "Stelios Asteriadis",
        "Kostas Karpouzis"
      ],
      "year": "2007",
      "venue": "Proceedings of the International Conference on Artificial Intelligence Applications and Innovations"
    },
    {
      "citation_id": "7",
      "title": "End-to-end multimodal emotion recognition using deep neural networks",
      "authors": [
        "Panagiotis Tzirakis",
        "George Trigeorgis",
        "A Mihalis",
        "Björn Nicolaou",
        "Stefanos Schuller",
        "Zafeiriou"
      ],
      "year": "2017",
      "venue": "IEEE Journal of Selected Topics in Signal Processing"
    },
    {
      "citation_id": "8",
      "title": "Dense multimodal fusion for hierarchically joint representation",
      "authors": [
        "Di Hu",
        "Chengze Wang",
        "Feiping Nie",
        "Xuelong Li"
      ],
      "year": "2019",
      "venue": "Proceedings of the International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "9",
      "title": "M3er: Multiplicative multimodal emotion recognition using facial, textual, and speech cues",
      "authors": [
        "Trisha Mittal",
        "Uttaran Bhattacharya",
        "Rohan Chandra"
      ],
      "year": "2020",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "10",
      "title": "Emobed: Strengthening monomodal emotion recognition via training with crossmodal emotion embeddings",
      "authors": [
        "Jing Han",
        "Zixing Zhang",
        "Zhao Ren",
        "Björn Schuller"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "11",
      "title": "Learning with privileged information via adversarial discriminative modality distillation",
      "authors": [
        "C Nuno",
        "Pietro Garcia",
        "Vittorio Morerio",
        "Murino"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "12",
      "title": "Found in translation: Learning robust joint representations by cyclic translations between modalities",
      "authors": [
        "Hai Pham",
        "Paul Liang",
        "Thomas Manzini",
        "Louis-Philippe Morency",
        "Barnabás Póczos"
      ],
      "year": "2019",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "13",
      "title": "Foundations of multimodal co-learning",
      "authors": [
        "Amir Zadeh",
        "Paul Liang",
        "Louis-Philippe Morency"
      ],
      "year": "2020",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "14",
      "title": "Multi-modal sentiment analysis using deep canonical correlation analysis",
      "authors": [
        "Zhongkai Sun",
        "K Prathusha",
        "William Sarma",
        "Erik Sethares",
        "Bucy"
      ],
      "year": "2019",
      "venue": "Proceedings of Interspeech"
    },
    {
      "citation_id": "15",
      "title": "Audio-visual fusion for sentiment classification using cross-modal autoencoder",
      "authors": [
        "Sri Harsha Dumpala",
        "Rupayan Chakraborty",
        "Sunil Kumar"
      ],
      "year": "2019",
      "venue": "Proceedings of the 32nd Conference on Neural Information Processing Systems"
    },
    {
      "citation_id": "16",
      "title": "Deep canonical correlation analysis",
      "authors": [
        "Galen Andrew",
        "Raman Arora",
        "Jeff Bilmes",
        "Karen Livescu"
      ],
      "year": "2013",
      "venue": "Proceedings of the International Conference on Machine Learning"
    },
    {
      "citation_id": "17",
      "title": "AVEC 2016: Depression, mood, and emotion recognition workshop and challenge",
      "authors": [
        "Michel Valstar",
        "Jonathan Gratch",
        "Björn Schuller",
        "Fabien Ringeval",
        "Denis Lalanne",
        "Mercedes Torres",
        "Stefan Scherer",
        "Giota Stratou",
        "Roddy Cowie",
        "Maja Pantic"
      ],
      "year": "2016",
      "venue": "Proceedings of the 6th International Workshop on Audio/Visual Emotion Challenge"
    },
    {
      "citation_id": "18",
      "title": "Reconstruction-error-based learning for continuous emotion recognition in speech",
      "authors": [
        "Jing Han",
        "Zixing Zhang",
        "Fabien Ringeval",
        "Björn Schuller"
      ],
      "year": "2017",
      "venue": "Proceedings of the International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "19",
      "title": "From hard to soft: Towards more human-like emotion recognition by modelling the perception uncertainty",
      "authors": [
        "Jing Han",
        "Zixing Zhang",
        "Maximilian Schmitt",
        "Maja Pantic",
        "Björn Schuller"
      ],
      "year": "2017",
      "venue": "Proceedings of the 25th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "20",
      "title": "Dynamic difficulty awareness training for continuous emotion prediction",
      "authors": [
        "Zixing Zhang",
        "Jing Han",
        "Eduardo Coutinho",
        "Björn Schuller"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Multimedia"
    }
  ]
}