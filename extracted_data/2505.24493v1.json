{
  "paper_id": "2505.24493v1",
  "title": "Melt: Towards Automated Multimodal Emotion Data Annotation By Leveraging Llm Embedded Knowledge",
  "published": "2025-05-30T11:45:36Z",
  "authors": [
    "Xin Jing",
    "Jiadong Wang",
    "Iosif Tsangko",
    "Andreas Triantafyllopoulos",
    "Björn W. Schuller"
  ],
  "keywords": [
    "affective compution",
    "human-computer interaction",
    "computational paralinguistics",
    "large language model"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Although speech emotion recognition (SER) has advanced significantly with deep learning, annotation remains a major hurdle. Human annotation is not only costly but also subject to inconsistencies-annotators often have different preferences and may lack the necessary contextual knowledge, which can lead to varied and inaccurate labels. Meanwhile, Large Language Models (LLMs) have emerged as a scalable alternative for annotating text data. However, the potential of LLMs to perform audio data annotation without human supervision has yet to be thoroughly investigated. To address these problems, we apply GPT-4o to annotate a multimodal dataset collected from the sitcom \"Friends\", using only textual cues as inputs. By crafting structured text prompts, our methodology capitalizes on the knowledge GPT-4o has accumulated during its training, showcasing that it can generate accurate and contextually relevant annotations without direct access to multimodal inputs. Therefore, we propose MELT , a multimodal emotion dataset fully annotated by GPT-4o. We demonstrate the effectiveness of MELT by fine-tuning four self-supervised learning (SSL) backbones and assessing speech emotion recognition performance across emotion datasets. Additionally, our subjective experiments' results demonstrate a consistence performance improvement on SER. Our data can be accessed at https://github.com/KeiKinn/meltdataset.git",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Recognizing human emotion and responding accordingly is a cornerstone of human-computer interaction  [1] . The progress made by contemporary deep-learning-based emotion recognition heavily depends on the availability of well-annotated datasets  [2] . However, ensuring accurate and consistent annotation requires multiple annotators and validation, resulting in significant financial costs that limit dataset scale and diversity.\n\nIn addition, researches has demonstrated that accurately applying emotional data annotation schemes requires annotators to incorporate contextual knowledge to capture characters' emotions effectively  [3, 4] . Meanwhile, research  [5]  indicates that individual preferences and cultural backgrounds significantly influence emotion recognition. However, contextual understanding and individual preferences are not a primary factor in the selection of annotators. On Amazon Mechanical Turk (AMT), a widely used crowdsourcing platform for data annotation, there is a lack of qualification tests to ensure annotators' familiarity with the target samples. With the introduction of the OpenAI's Generative Pre-trained Transformer (GPT) models, LLMs have demonstrated the potential of performing more complex tasks with scaling  [6, 7, 8] . Several studies have evaluated the adaptability and broad applicability of LLMs as annotators using existing datasets  [9, 10] . For instance, Gilardi et al.  [11]  found that ChatGPT outperformed crowd workers by approximately 25% points on average, with its intercoder agreement surpassing humans across all evaluated tasks. Due to the input constraints, these studies predominantly concentrate on text-based datasets. In the domain of audio, WavCaps  [12]  utilized ChatGPT to compile large-scale, high-quality audio captions, further highlighting the potential of LLMs in generating reliable annotations. This was facilitated by the use of tags describing the audio files that comprise WavCaps; thus, ChatGPT did not introduce novel information, but rather reframed the information provided by humans. Pengi  [13]  introduces an Audio Language Model by reframing all audio tasks as text-generation tasks, which accepts an audio recording and a text prompt as inputs and subsequently outputs free-form text. The SECap  [14]  framework utilizes the LLaMA decoder to generate fluent and coherent captions describing emotional speech by leveraging Q-Former embeddings. However, these approaches not only rely on datasets with existing high-quality emotion annotationswhich are limited in scale due to the high costs of collectionbut also require additional audio features as input for LLM decoders to generate captions. As a result, the potential of LLMs to automatically annotated audio datasets with captions without any human labor, solely leveraging their contextual understanding, remains relatively underexplored, highlighting a gap in the current research landscape.\n\nTrained on an extensive corpus of web-sourced data  [8, 15] , frontier LLMs like GPT-4o encode knowledge regarding culturally significant content. This is especially true for widely popular media, such as the television series \"Friends\". We consider the vast corpus of internet data that the model has been trained upon as an implicit \"collective knowledge base\", reflecting the shared understanding and engagement of a broad audience. We exploit this knowledge to re-annotate MELD -a multimodal dataset derived from \"Friends\". In this work, we propose the Multimodal Emotion-Lines Dataset Labeled with LLM Con-texT Knowledge (MELT) extending GPT-4o's annotation capabilities from text-only emotion data to multimodal data by leveraging its embedded knowledge.\n\nOur accompanying experiments indicate that MELT aligns more closely with human preferences, as evidenced by subjective evaluations. Furthermore, models trained on MELT exhibit improved generalization and robustness across different speech emotion recognition (SER) datasets, highlighting LLMs' potential for tasks that extend beyond conventional text-based approaches. This exploration demonstrates the advantages of LLMs in creating scalable and efficient annotation pipelines, while also demonstrating their ability to address complex con-textual tasks. To the best of our knowledge, this work is the first to explore the potential of GPT models as annotators for multimodal emotion data, leveraging insights from the knowledge it has assimilated in its training. The summary of our contributions is as follows:\n\n• We propose a context-aware automatic annotation method using GPT-4o, with consistent emotional tone across samples, resulting in the MELT, which outperforms MELD, the human-labeled counterpart. • We propose a prompting framework with cross-validation and Chain-of-thoughts (CoT) reasoning for multimodal emotion annotation. • A significant reduction in costs (≤ $10) to achieve highquality annotations compared to traditional methods. The rest of this paper is organized as follows: Section 2 presents our methodology, focusing on prompting framework. Section 3 provides a detailed analysis of MELT . The objective and subjective experiments are described in Section 4. Results and analysis are presented and discussed in Section 5. Finally, we conclude the paper and discuss the future work in Section 6.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Methodology",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Data Preparation",
      "text": "Multimodal EmotionLines Dataset (MELD)  [16] , built from the TV series \"Friends\", comprises 1,433 dialogues and 13,708 utterances. Each utterance is annotated with one of seven categories (Joy, Sadness, Fear, Anger, Surprise, Disgust, Neutral) based on a majority vote among three annotators. MELT is derived from MELD using the following steps: Firstly, utterances shorter than one second were excluded, as classifying short speech remains a significant challenge in SER  [17, 18] . Subsequently, we excluded characters whose names do not provide enough context for GPT-4o to maintain consistency. MELD includes 260 unique characters in the training set and 100 in the test set, with some overlap. Certain characters, such as \"1st Customer\" and \"Receptionist,\" lack clear identifiers like names or gender in the textual modality, which conflicted with the prompt design guidelines in Section 2.3.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Gpt Model Selection",
      "text": "We utilize the OpenAI API 1 to access the 'gpt-4o-2024-08-06' model with a temperature of 1.0 for speech emotion annotation. For simplicity, 'GPT-4o' is used throughout the following sections. GPT-4o, with its October 2023 knowledge base cutoff 2 , integrates updated data, reducing reliance on fine-tuning or retrieval-augmented generation (RAG) methods  [8] .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Prompt Engineering",
      "text": "Effective prompt engineering significantly impacts the performance of LLMs  [19] . To design a prompt that optimize performance while ensuring stability and reproducibility, we adhere to the following principles: • Clear, Contextual, and Specific: Include as much relevant context as possible while avoiding ambiguity in instructions to enhance the model's understanding of the task. • Chain of Thought (CoT) Prompting: Break tasks into distinct, logical steps to guide the model's reasoning process.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Subjective Experiment",
      "text": "To assess the emotion annotation quality, we invited 20 participants, comprising 11 males and 9 females to conduct a Mean Opinion Score (MOS) experiment. Participants were instructed to watch the video clips and were presented with two options-MELT and MELD annotations, without knowing their sources -from which they were asked to select the description they deemed more appropriate.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Objective Experiments",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Objective Evaluation System",
      "text": "The SER evaluation system in Fig.  2  consists of two main components: 1) a pretrained Self-supervised learning (SSL) backbone, which is initialized by the pretrained weight on huggingface.co 3  and 2) a classification module.\n\nWe applied four pretrained SSL weights: 'facebook/wav2vec2-base-960h  [20] ' (wav2vec 2.0 base) , 'audeering/wav2vec2-largerobust-12-ft-emotion-msp-dim  [21] ' (wav2vec 2.0 Aud), 'facebook/hubert-base-ls960  [22] ' (Hubert Base), and 'microsoft/wavlm-base-plus  [23] ' (WavLM Base+). The classification module consists of two fully connected layers with a ReLU activation function between them. The classification head is dynamically adjusted to match the number of emotion categories in the test dataset. We apply the weighted sum of hidden states as the input of this classification module.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Datasets",
      "text": "Training Datasets: Both MELT and MELD are employed as training datasets in our evaluation system. To ensure a fair comparison, we impose two conditions on data filtering: 1) only emotion categories present in the test set are retained by filtering out others, and 2) training data is restricted to audio samples that are common to both datasets. Out-of-domain Datasets: We additionally perform crosscorpus testing on the following datasets to test the generalizability of the trained models. IEMOCAP  [24] : We follow the convention in the literature to use a subset of the original labels by merging 'happy' and 'excited', resulting total of 5,531 utterances, with four labels (1,103 angry, 1,636 happy, 1,708 neutral, and 1,084 sad). TESS  [25] : The set consists of 2,800 clips representing seven emotions: anger, disgust, fear, happiness, pleasant surprise, sadness, and neutral. To align with MELT , we set the data labeled 'pleasant surprise' to 'surprise'. RAVDESS  [26] : Its speech portion includes 1,440 utterances across 8 emotion categories: neutral, calm, happy, sad, angry, fearful, surprise, disgust. During the evaluation, we excluded the 'calm' category, resulting 1,248 utterances in test set. CREMA-D  [27] : It contains 7,442 clips recorded, featuring both facial and vocal expressions across six basic emotional states: happy, sad, angry, fearful, disgust, and neutral.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Experiment Settings",
      "text": "The training process employed the Adam optimizer with a batch size of 32, a learning rate of 0.001, and a dropout rate of 0.2 for 100 epochs. Audio data are resampled to 16 kHz and randomly cropped or padded to 5 seconds. During testing, batch size was set to 1, and no length adjustment was required. All experiments were conducted in a Python 3.10.8 and PyTorch 2.4.1 environment on a single Nvidia RTX 3090 GPU.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Melt Meld",
      "text": "Figure  3 : MOS results on every emotion categories.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Results And Analysis",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Performance",
      "text": "The overall MOS result is shown in Fig.  3 , participants overall demonstrated a preference for MELT annotations. The high agreement (> 70%) for 'anger' and 'surprise' indicates that GPT-4o effectively integrates internet-sourced knowledge to capture the diversity in expressions. However, an opposite trend is observed for 'fear' and 'sadness', as they mostly transition to 'neutral', as shown in Fig.  1 . These reannotated audio clips often feature lower arousal and subtler emotional contexts. The comparable preferences suggest possible biases stemming from GPT-4o's reliance on internet-derived emotional knowledge. Classification results are summarized in Table  3 . Commonly used evaluation metrics, unweighted accuracy recall (UAR), accuracy (ACC), and F1 score, are applied to evaluate the performance of the SER task. Our first observation is that the in-domain results trained on MELT generally outperform the original MELD, showcasing how our annotation improves upon the general SSL-based benchmark architecture  [28] .\n\nFor the TESS and RAVDESS, where MELT shares the same emotion classes, there existed two different patterns in the experimental results. For RAVDESS, all models demonstrated considerable and consistent performance improvements. Meanwhile, TESS showed a mixed outcome: while wav2vec 2.0 Base and HuBERT Base achieved substantial UAR performance gains (19.25% and 23.74%, respectively), the remaining two models exhibited minimal improvements in UAR and ACC, with a slight decline in F1 scores compared to results on the MELD dataset. For IEMOCAP and CREMA-D, which only partially overlap with MELT , the test performance varies across different model backbones. wav2vec 2.0 Aud achieves the most substantial performance improvement on both datasets, while wav2vec 2.0 Base shows relatively modest gains. This highlights the importance of pretraining in enhancing model performance, particularly when addressing datasets with limited or partial alignment to the training annotations.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Audio Characteristic",
      "text": "To further analyze the annotation performance of the GPT-4o on audio characteristics, we adapted the query generation pipeline proposed by ParaCLAP  [29]  to categorize descriptions and extract the extended Geneva Minimalistic Acoustic Parameter Set (eGeMAPS)  [30]  using openSMILE  [31] . The pitch and loudness attributes are binned according to their distribution (bottom 30 %, middle 40 %, top 30 %) and pseudo-captions are mapped accordingly (i. e., low/mid/high). Across both the training and test sets, all metrics consistently surpass random guessing, demonstrating that GPT-4o effectively captures relevant characteristics from its embedded knowledge. Notably, the performance of pitch exceeds that of loudness, likely reflecting human preferences in voice descriptions  [32] , which may contribute to biases in the output. These outcomes, while still falling short of state-of-the-art results, showcase the potential of LLMs with further refinement for tasks traditionally dominated by handcrafted features.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Conclusion",
      "text": "In this work, we introduce MELT , a multimodal emotion dataset fully annotated by GPT-4o using a context-aware automatic annotation method. To achieve this, we developed a prompting strategy incorporating cross-validation and CoT reasoning to ensure consistent and accurate annotations. The MOS and classification results highlights that MELT provides better generalization and robustness, demonstrating the feasibility of adapting LLMs for multimodal tasks and sets the stage for future innovations in multimodal annotation frameworks. Despite the promising results, the reliance on GPT-4o raises concerns about model-specific biases and hallucinations, which can affect annotation quality. Future work will aim to investigate hybrid annotation pipelines that combine LLMs with human-inthe-loop methods to improve scalability and reliability.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: , nearly half of MELD’s utterances align",
      "page": 3
    },
    {
      "caption": "Figure 1: Confusion matrix and inter-label transition matrix of",
      "page": 3
    },
    {
      "caption": "Figure 2: Speech emotion recognition evaluation system",
      "page": 3
    },
    {
      "caption": "Figure 2: consists of two main",
      "page": 3
    },
    {
      "caption": "Figure 3: MOS results on every emotion categories.",
      "page": 4
    },
    {
      "caption": "Figure 3: , participants over-",
      "page": 4
    },
    {
      "caption": "Figure 1: These reannotated audio clips of-",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "3GLAM – Group on Language, Audio, & Music, Imperial College London, UK": "xin.jing@tum.de"
        },
        {
          "3GLAM – Group on Language, Audio, & Music, Imperial College London, UK": "complex tasks with scaling [6, 7, 8]. Several studies have eval-"
        },
        {
          "3GLAM – Group on Language, Audio, & Music, Imperial College London, UK": "uated the adaptability and broad applicability of LLMs as an-"
        },
        {
          "3GLAM – Group on Language, Audio, & Music, Imperial College London, UK": ""
        },
        {
          "3GLAM – Group on Language, Audio, & Music, Imperial College London, UK": "notators using existing datasets [9, 10].\nFor\ninstance, Gilardi"
        },
        {
          "3GLAM – Group on Language, Audio, & Music, Imperial College London, UK": ""
        },
        {
          "3GLAM – Group on Language, Audio, & Music, Imperial College London, UK": "et al. [11] found that ChatGPT outperformed crowd workers by"
        },
        {
          "3GLAM – Group on Language, Audio, & Music, Imperial College London, UK": ""
        },
        {
          "3GLAM – Group on Language, Audio, & Music, Imperial College London, UK": "approximately 25% points on average, with its intercoder agree-"
        },
        {
          "3GLAM – Group on Language, Audio, & Music, Imperial College London, UK": ""
        },
        {
          "3GLAM – Group on Language, Audio, & Music, Imperial College London, UK": "ment surpassing humans across all evaluated tasks. Due to the"
        },
        {
          "3GLAM – Group on Language, Audio, & Music, Imperial College London, UK": ""
        },
        {
          "3GLAM – Group on Language, Audio, & Music, Imperial College London, UK": "input constraints,\nthese studies predominantly concentrate on"
        },
        {
          "3GLAM – Group on Language, Audio, & Music, Imperial College London, UK": ""
        },
        {
          "3GLAM – Group on Language, Audio, & Music, Imperial College London, UK": "text-based datasets.\nIn the domain of audio, WavCaps [12] uti-"
        },
        {
          "3GLAM – Group on Language, Audio, & Music, Imperial College London, UK": ""
        },
        {
          "3GLAM – Group on Language, Audio, & Music, Imperial College London, UK": "lized ChatGPT to compile large-scale, high-quality audio cap-"
        },
        {
          "3GLAM – Group on Language, Audio, & Music, Imperial College London, UK": ""
        },
        {
          "3GLAM – Group on Language, Audio, & Music, Imperial College London, UK": "tions, further highlighting the potential of LLMs in generating"
        },
        {
          "3GLAM – Group on Language, Audio, & Music, Imperial College London, UK": ""
        },
        {
          "3GLAM – Group on Language, Audio, & Music, Imperial College London, UK": "reliable annotations. This was facilitated by the use of tags de-"
        },
        {
          "3GLAM – Group on Language, Audio, & Music, Imperial College London, UK": ""
        },
        {
          "3GLAM – Group on Language, Audio, & Music, Imperial College London, UK": "scribing the audio files that comprise WavCaps; thus, ChatGPT"
        },
        {
          "3GLAM – Group on Language, Audio, & Music, Imperial College London, UK": ""
        },
        {
          "3GLAM – Group on Language, Audio, & Music, Imperial College London, UK": "did not introduce novel information, but rather reframed the in-"
        },
        {
          "3GLAM – Group on Language, Audio, & Music, Imperial College London, UK": ""
        },
        {
          "3GLAM – Group on Language, Audio, & Music, Imperial College London, UK": "formation provided by humans. Pengi [13] introduces an Audio"
        },
        {
          "3GLAM – Group on Language, Audio, & Music, Imperial College London, UK": ""
        },
        {
          "3GLAM – Group on Language, Audio, & Music, Imperial College London, UK": "Language Model by reframing all audio tasks as text-generation"
        },
        {
          "3GLAM – Group on Language, Audio, & Music, Imperial College London, UK": ""
        },
        {
          "3GLAM – Group on Language, Audio, & Music, Imperial College London, UK": "tasks, which accepts an audio recording and a text prompt as in-"
        },
        {
          "3GLAM – Group on Language, Audio, & Music, Imperial College London, UK": ""
        },
        {
          "3GLAM – Group on Language, Audio, & Music, Imperial College London, UK": "puts and subsequently outputs free-form text. The SECap [14]"
        },
        {
          "3GLAM – Group on Language, Audio, & Music, Imperial College London, UK": ""
        },
        {
          "3GLAM – Group on Language, Audio, & Music, Imperial College London, UK": "framework utilizes the LLaMA decoder to generate fluent and"
        },
        {
          "3GLAM – Group on Language, Audio, & Music, Imperial College London, UK": ""
        },
        {
          "3GLAM – Group on Language, Audio, & Music, Imperial College London, UK": "coherent captions describing emotional speech by leveraging Q-"
        },
        {
          "3GLAM – Group on Language, Audio, & Music, Imperial College London, UK": ""
        },
        {
          "3GLAM – Group on Language, Audio, & Music, Imperial College London, UK": "Former embeddings. However,\nthese approaches not only rely"
        },
        {
          "3GLAM – Group on Language, Audio, & Music, Imperial College London, UK": ""
        },
        {
          "3GLAM – Group on Language, Audio, & Music, Imperial College London, UK": "on datasets with existing high-quality emotion annotations—"
        },
        {
          "3GLAM – Group on Language, Audio, & Music, Imperial College London, UK": ""
        },
        {
          "3GLAM – Group on Language, Audio, & Music, Imperial College London, UK": "which are limited in scale due to the high costs of collection—"
        },
        {
          "3GLAM – Group on Language, Audio, & Music, Imperial College London, UK": ""
        },
        {
          "3GLAM – Group on Language, Audio, & Music, Imperial College London, UK": "but also require additional audio features as input for LLM de-"
        },
        {
          "3GLAM – Group on Language, Audio, & Music, Imperial College London, UK": ""
        },
        {
          "3GLAM – Group on Language, Audio, & Music, Imperial College London, UK": "coders to generate captions. As a result,\nthe potential of LLMs"
        },
        {
          "3GLAM – Group on Language, Audio, & Music, Imperial College London, UK": ""
        },
        {
          "3GLAM – Group on Language, Audio, & Music, Imperial College London, UK": "to automatically annotated audio datasets with captions without"
        },
        {
          "3GLAM – Group on Language, Audio, & Music, Imperial College London, UK": ""
        },
        {
          "3GLAM – Group on Language, Audio, & Music, Imperial College London, UK": "any human labor, solely leveraging their contextual understand-"
        },
        {
          "3GLAM – Group on Language, Audio, & Music, Imperial College London, UK": ""
        },
        {
          "3GLAM – Group on Language, Audio, & Music, Imperial College London, UK": "ing, remains relatively underexplored, highlighting a gap in the"
        },
        {
          "3GLAM – Group on Language, Audio, & Music, Imperial College London, UK": ""
        },
        {
          "3GLAM – Group on Language, Audio, & Music, Imperial College London, UK": "current research landscape."
        },
        {
          "3GLAM – Group on Language, Audio, & Music, Imperial College London, UK": "Trained on an extensive corpus of web-sourced data [8, 15],"
        },
        {
          "3GLAM – Group on Language, Audio, & Music, Imperial College London, UK": ""
        },
        {
          "3GLAM – Group on Language, Audio, & Music, Imperial College London, UK": "frontier LLMs like GPT-4o encode knowledge regarding cultur-"
        },
        {
          "3GLAM – Group on Language, Audio, & Music, Imperial College London, UK": "ally significant content. This is especially true for widely popu-"
        },
        {
          "3GLAM – Group on Language, Audio, & Music, Imperial College London, UK": ""
        },
        {
          "3GLAM – Group on Language, Audio, & Music, Imperial College London, UK": "lar media, such as the television series “Friends”. We consider"
        },
        {
          "3GLAM – Group on Language, Audio, & Music, Imperial College London, UK": ""
        },
        {
          "3GLAM – Group on Language, Audio, & Music, Imperial College London, UK": "the vast corpus of internet data that the model has been trained"
        },
        {
          "3GLAM – Group on Language, Audio, & Music, Imperial College London, UK": ""
        },
        {
          "3GLAM – Group on Language, Audio, & Music, Imperial College London, UK": "upon as an implicit “collective knowledge base”, reflecting the"
        },
        {
          "3GLAM – Group on Language, Audio, & Music, Imperial College London, UK": ""
        },
        {
          "3GLAM – Group on Language, Audio, & Music, Imperial College London, UK": "shared understanding and engagement of a broad audience. We"
        },
        {
          "3GLAM – Group on Language, Audio, & Music, Imperial College London, UK": ""
        },
        {
          "3GLAM – Group on Language, Audio, & Music, Imperial College London, UK": "exploit\nthis knowledge to re-annotate MELD – a multimodal"
        },
        {
          "3GLAM – Group on Language, Audio, & Music, Imperial College London, UK": ""
        },
        {
          "3GLAM – Group on Language, Audio, & Music, Imperial College London, UK": "dataset derived from “Friends”.\nIn this work, we propose the"
        },
        {
          "3GLAM – Group on Language, Audio, & Music, Imperial College London, UK": ""
        },
        {
          "3GLAM – Group on Language, Audio, & Music, Imperial College London, UK": "Multimodal Emotion-Lines Dataset Labeled with LLM Con-"
        },
        {
          "3GLAM – Group on Language, Audio, & Music, Imperial College London, UK": ""
        },
        {
          "3GLAM – Group on Language, Audio, & Music, Imperial College London, UK": "texT Knowledge (MELT) extending GPT-4o’s annotation ca-"
        },
        {
          "3GLAM – Group on Language, Audio, & Music, Imperial College London, UK": ""
        },
        {
          "3GLAM – Group on Language, Audio, & Music, Imperial College London, UK": "pabilities from text-only emotion data to multimodal data by"
        },
        {
          "3GLAM – Group on Language, Audio, & Music, Imperial College London, UK": ""
        },
        {
          "3GLAM – Group on Language, Audio, & Music, Imperial College London, UK": "leveraging its embedded knowledge."
        },
        {
          "3GLAM – Group on Language, Audio, & Music, Imperial College London, UK": ""
        },
        {
          "3GLAM – Group on Language, Audio, & Music, Imperial College London, UK": "Our accompanying experiments indicate that MELT aligns"
        },
        {
          "3GLAM – Group on Language, Audio, & Music, Imperial College London, UK": "more closely with human preferences, as evidenced by subjec-"
        },
        {
          "3GLAM – Group on Language, Audio, & Music, Imperial College London, UK": "tive evaluations. Furthermore, models trained on MELT exhibit"
        },
        {
          "3GLAM – Group on Language, Audio, & Music, Imperial College London, UK": "improved generalization and robustness across different speech"
        },
        {
          "3GLAM – Group on Language, Audio, & Music, Imperial College London, UK": "emotion recognition (SER) datasets, highlighting LLMs’ po-"
        },
        {
          "3GLAM – Group on Language, Audio, & Music, Imperial College London, UK": "tential\nfor\ntasks\nthat\nextend beyond conventional\ntext-based"
        },
        {
          "3GLAM – Group on Language, Audio, & Music, Imperial College London, UK": "approaches.\nThis exploration demonstrates the advantages of"
        },
        {
          "3GLAM – Group on Language, Audio, & Music, Imperial College London, UK": "LLMs\nin creating scalable and efficient annotation pipelines,"
        },
        {
          "3GLAM – Group on Language, Audio, & Music, Imperial College London, UK": "while also demonstrating their ability to address complex con-"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "textual tasks. To the best of our knowledge, this work is the first": "to explore the potential of GPT models as annotators for multi-",
          "• Cross-Validation:\nIncorporate requests for known or easily": "verifiable information to reduce the likelihood of generating"
        },
        {
          "textual tasks. To the best of our knowledge, this work is the first": "modal emotion data,\nleveraging insights from the knowledge it",
          "• Cross-Validation:\nIncorporate requests for known or easily": "incorrect or unrelated content."
        },
        {
          "textual tasks. To the best of our knowledge, this work is the first": "has assimilated in its training.",
          "• Cross-Validation:\nIncorporate requests for known or easily": "with\nPrefilling\n• Guide Output\nResponses:\nStructure"
        },
        {
          "textual tasks. To the best of our knowledge, this work is the first": "The summary of our contributions is as follows:",
          "• Cross-Validation:\nIncorporate requests for known or easily": "prompts (e. g., JSON or XML)\nto direct\nthe model’s output"
        },
        {
          "textual tasks. To the best of our knowledge, this work is the first": "• We propose a context-aware automatic annotation method",
          "• Cross-Validation:\nIncorporate requests for known or easily": "to ensure consistency and ease of post-processing."
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "modal emotion data,\nleveraging insights from the knowledge it": "has assimilated in its training.",
          "incorrect or unrelated content.": "with\n• Guide Output"
        },
        {
          "modal emotion data,\nleveraging insights from the knowledge it": "The summary of our contributions is as follows:",
          "incorrect or unrelated content.": "prompts (e. g., JSON or XML)"
        },
        {
          "modal emotion data,\nleveraging insights from the knowledge it": "• We propose a context-aware automatic annotation method",
          "incorrect or unrelated content.": ""
        },
        {
          "modal emotion data,\nleveraging insights from the knowledge it": "using GPT-4o, with consistent emotional\ntone across\nsam-",
          "incorrect or unrelated content.": ""
        },
        {
          "modal emotion data,\nleveraging insights from the knowledge it": "",
          "incorrect or unrelated content.": "Given\nthe\nfollowing\nline"
        },
        {
          "modal emotion data,\nleveraging insights from the knowledge it": "ples, resulting in the MELT, which outperforms MELD,\nthe",
          "incorrect or unrelated content.": ""
        },
        {
          "modal emotion data,\nleveraging insights from the knowledge it": "",
          "incorrect or unrelated content.": "Friends\ncharacter,\nthe"
        },
        {
          "modal emotion data,\nleveraging insights from the knowledge it": "human-labeled counterpart.",
          "incorrect or unrelated content.": ""
        },
        {
          "modal emotion data,\nleveraging insights from the knowledge it": "",
          "incorrect or unrelated content.": "\"[speaker]\nat"
        },
        {
          "modal emotion data,\nleveraging insights from the knowledge it": "• We propose\na prompting framework with cross-validation",
          "incorrect or unrelated content.": "utterance]\""
        },
        {
          "modal emotion data,\nleveraging insights from the knowledge it": "and Chain-of-thoughts (CoT) reasoning for multimodal emo-",
          "incorrect or unrelated content.": ""
        },
        {
          "modal emotion data,\nleveraging insights from the knowledge it": "",
          "incorrect or unrelated content.": "Please\ndescribe\nhow\nthe"
        },
        {
          "modal emotion data,\nleveraging insights from the knowledge it": "tion annotation.",
          "incorrect or unrelated content.": ""
        },
        {
          "modal emotion data,\nleveraging insights from the knowledge it": "",
          "incorrect or unrelated content.": "sound.\nInclude\ndetails"
        },
        {
          "modal emotion data,\nleveraging insights from the knowledge it": "• A significant\nreduction in costs\n(≤ $10)\nto achieve high-",
          "incorrect or unrelated content.": ""
        },
        {
          "modal emotion data,\nleveraging insights from the knowledge it": "",
          "incorrect or unrelated content.": "-\nthe\nemotion\nexpressed,"
        },
        {
          "modal emotion data,\nleveraging insights from the knowledge it": "quality annotations compared to traditional methods.",
          "incorrect or unrelated content.": ""
        },
        {
          "modal emotion data,\nleveraging insights from the knowledge it": "",
          "incorrect or unrelated content.": "-\nthe\nloudness,"
        },
        {
          "modal emotion data,\nleveraging insights from the knowledge it": "",
          "incorrect or unrelated content.": "-\nthe\npitch,"
        },
        {
          "modal emotion data,\nleveraging insights from the knowledge it": "The rest of\nthis paper\nis organized as follows:\nSection 2",
          "incorrect or unrelated content.": ""
        },
        {
          "modal emotion data,\nleveraging insights from the knowledge it": "",
          "incorrect or unrelated content.": "-\nthe\nrhythm\nspeed,"
        },
        {
          "modal emotion data,\nleveraging insights from the knowledge it": "presents our methodology,\nfocusing on prompting framework.",
          "incorrect or unrelated content.": ""
        },
        {
          "modal emotion data,\nleveraging insights from the knowledge it": "",
          "incorrect or unrelated content.": "-\nthe\noverall\nemotional"
        },
        {
          "modal emotion data,\nleveraging insights from the knowledge it": "Section 3 provides a detailed analysis of MELT . The objective",
          "incorrect or unrelated content.": ""
        },
        {
          "modal emotion data,\nleveraging insights from the knowledge it": "and subjective experiments are described in Section 4. Results",
          "incorrect or unrelated content.": ""
        },
        {
          "modal emotion data,\nleveraging insights from the knowledge it": "",
          "incorrect or unrelated content.": "Format\nyour\nresponse:"
        },
        {
          "modal emotion data,\nleveraging insights from the knowledge it": "and analysis are presented and discussed in Section 5. Finally,",
          "incorrect or unrelated content.": "-\nProvide\nthe\ncharacter’s"
        },
        {
          "modal emotion data,\nleveraging insights from the knowledge it": "we conclude the paper and discuss the future work in Section 6.",
          "incorrect or unrelated content.": "context."
        },
        {
          "modal emotion data,\nleveraging insights from the knowledge it": "",
          "incorrect or unrelated content.": "-\nThe\nemotion\nlabel\nmust"
        },
        {
          "modal emotion data,\nleveraging insights from the knowledge it": "",
          "incorrect or unrelated content.": "following\nlist:\n[Anger,"
        },
        {
          "modal emotion data,\nleveraging insights from the knowledge it": "2. Methodology",
          "incorrect or unrelated content.": ""
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Table 2: Distribution of emotions in MELT and filtered MELD": ""
        },
        {
          "Table 2: Distribution of emotions in MELT and filtered MELD": "for training and test splits.\nlabel ∆% is the label change rate"
        },
        {
          "Table 2: Distribution of emotions in MELT and filtered MELD": ""
        },
        {
          "Table 2: Distribution of emotions in MELT and filtered MELD": ""
        },
        {
          "Table 2: Distribution of emotions in MELT and filtered MELD": "Train Set\nTest Set"
        },
        {
          "Table 2: Distribution of emotions in MELT and filtered MELD": "Emo."
        },
        {
          "Table 2: Distribution of emotions in MELT and filtered MELD": ""
        },
        {
          "Table 2: Distribution of emotions in MELT and filtered MELD": "MELT\nMELD\nMELT\nMELD"
        },
        {
          "Table 2: Distribution of emotions in MELT and filtered MELD": "anger\n464\n852\n127\n270"
        },
        {
          "Table 2: Distribution of emotions in MELT and filtered MELD": "disgust\n473\n215\n124\n59"
        },
        {
          "Table 2: Distribution of emotions in MELT and filtered MELD": ""
        },
        {
          "Table 2: Distribution of emotions in MELT and filtered MELD": "fear\n418\n208\n103\n42"
        },
        {
          "Table 2: Distribution of emotions in MELT and filtered MELD": "joy\n1578\n1244\n389\n262"
        },
        {
          "Table 2: Distribution of emotions in MELT and filtered MELD": ""
        },
        {
          "Table 2: Distribution of emotions in MELT and filtered MELD": "neutral\n2519\n3172\n660\n827"
        },
        {
          "Table 2: Distribution of emotions in MELT and filtered MELD": ""
        },
        {
          "Table 2: Distribution of emotions in MELT and filtered MELD": "sadness\n363\n570\n90\n170"
        },
        {
          "Table 2: Distribution of emotions in MELT and filtered MELD": ""
        },
        {
          "Table 2: Distribution of emotions in MELT and filtered MELD": "surprise\n1209\n763\n304\n167"
        },
        {
          "Table 2: Distribution of emotions in MELT and filtered MELD": ""
        },
        {
          "Table 2: Distribution of emotions in MELT and filtered MELD": "46.43\n47.52\nlabel ∆%"
        },
        {
          "Table 2: Distribution of emotions in MELT and filtered MELD": ""
        },
        {
          "Table 2: Distribution of emotions in MELT and filtered MELD": "Total\n7024\n7024\n1797\n1797"
        },
        {
          "Table 2: Distribution of emotions in MELT and filtered MELD": ""
        },
        {
          "Table 2: Distribution of emotions in MELT and filtered MELD": ""
        },
        {
          "Table 2: Distribution of emotions in MELT and filtered MELD": "test\nsets maintaining an approximate 4:1 ratio across\nlabels."
        },
        {
          "Table 2: Distribution of emotions in MELT and filtered MELD": ""
        },
        {
          "Table 2: Distribution of emotions in MELT and filtered MELD": "As\nshown in Fig. 1, nearly half of MELD’s utterances align"
        },
        {
          "Table 2: Distribution of emotions in MELT and filtered MELD": ""
        },
        {
          "Table 2: Distribution of emotions in MELT and filtered MELD": "with GPT-4o’s annotations,\nand the overall\nlabel proportions"
        },
        {
          "Table 2: Distribution of emotions in MELT and filtered MELD": ""
        },
        {
          "Table 2: Distribution of emotions in MELT and filtered MELD": "between the two datasets are similar for most emotions. How-"
        },
        {
          "Table 2: Distribution of emotions in MELT and filtered MELD": ""
        },
        {
          "Table 2: Distribution of emotions in MELT and filtered MELD": "ever,\n‘sad’ and ‘anger’ exhibit\nsignificant\nreannotation, with"
        },
        {
          "Table 2: Distribution of emotions in MELT and filtered MELD": ""
        },
        {
          "Table 2: Distribution of emotions in MELT and filtered MELD": "most samples being relabeled as ‘neutral’ and ‘surprise’, respec-"
        },
        {
          "Table 2: Distribution of emotions in MELT and filtered MELD": ""
        },
        {
          "Table 2: Distribution of emotions in MELT and filtered MELD": "tively.\nThis likely reflects overlapping features or ambiguous"
        },
        {
          "Table 2: Distribution of emotions in MELT and filtered MELD": ""
        },
        {
          "Table 2: Distribution of emotions in MELT and filtered MELD": "original annotations. While labels such as ‘neutral’ and ‘joy’"
        },
        {
          "Table 2: Distribution of emotions in MELT and filtered MELD": ""
        },
        {
          "Table 2: Distribution of emotions in MELT and filtered MELD": "remain consistent, emotions like ‘fear’ and ‘disgust’ are more"
        },
        {
          "Table 2: Distribution of emotions in MELT and filtered MELD": "frequently reannotated, potentially due to inherent ambiguity."
        },
        {
          "Table 2: Distribution of emotions in MELT and filtered MELD": ""
        },
        {
          "Table 2: Distribution of emotions in MELT and filtered MELD": "The percentage of label changes is comparable in both datasets"
        },
        {
          "Table 2: Distribution of emotions in MELT and filtered MELD": "(46.43% in training and 47.52% in testing), with slightly higher"
        },
        {
          "Table 2: Distribution of emotions in MELT and filtered MELD": ""
        },
        {
          "Table 2: Distribution of emotions in MELT and filtered MELD": "changes in the test set."
        },
        {
          "Table 2: Distribution of emotions in MELT and filtered MELD": ""
        },
        {
          "Table 2: Distribution of emotions in MELT and filtered MELD": ""
        },
        {
          "Table 2: Distribution of emotions in MELT and filtered MELD": ""
        },
        {
          "Table 2: Distribution of emotions in MELT and filtered MELD": ""
        },
        {
          "Table 2: Distribution of emotions in MELT and filtered MELD": ""
        },
        {
          "Table 2: Distribution of emotions in MELT and filtered MELD": ""
        },
        {
          "Table 2: Distribution of emotions in MELT and filtered MELD": ""
        },
        {
          "Table 2: Distribution of emotions in MELT and filtered MELD": ""
        },
        {
          "Table 2: Distribution of emotions in MELT and filtered MELD": ""
        },
        {
          "Table 2: Distribution of emotions in MELT and filtered MELD": "Training dataset"
        },
        {
          "Table 2: Distribution of emotions in MELT and filtered MELD": ""
        },
        {
          "Table 2: Distribution of emotions in MELT and filtered MELD": ""
        },
        {
          "Table 2: Distribution of emotions in MELT and filtered MELD": ""
        },
        {
          "Table 2: Distribution of emotions in MELT and filtered MELD": ""
        },
        {
          "Table 2: Distribution of emotions in MELT and filtered MELD": ""
        },
        {
          "Table 2: Distribution of emotions in MELT and filtered MELD": ""
        },
        {
          "Table 2: Distribution of emotions in MELT and filtered MELD": ""
        },
        {
          "Table 2: Distribution of emotions in MELT and filtered MELD": ""
        },
        {
          "Table 2: Distribution of emotions in MELT and filtered MELD": "Test dataset"
        },
        {
          "Table 2: Distribution of emotions in MELT and filtered MELD": ""
        },
        {
          "Table 2: Distribution of emotions in MELT and filtered MELD": "Figure 1: Confusion matrix and inter-label transition matrix of"
        },
        {
          "Table 2: Distribution of emotions in MELT and filtered MELD": "the training and test dataset"
        },
        {
          "Table 2: Distribution of emotions in MELT and filtered MELD": ""
        },
        {
          "Table 2: Distribution of emotions in MELT and filtered MELD": ""
        },
        {
          "Table 2: Distribution of emotions in MELT and filtered MELD": "“Emo \nSSL"
        },
        {
          "Table 2: Distribution of emotions in MELT and filtered MELD": "Prediction”\nbackbone"
        },
        {
          "Table 2: Distribution of emotions in MELT and filtered MELD": ""
        },
        {
          "Table 2: Distribution of emotions in MELT and filtered MELD": "Audio wav"
        },
        {
          "Table 2: Distribution of emotions in MELT and filtered MELD": ""
        },
        {
          "Table 2: Distribution of emotions in MELT and filtered MELD": "Figure 2: Speech emotion recognition evaluation system"
        },
        {
          "Table 2: Distribution of emotions in MELT and filtered MELD": ""
        },
        {
          "Table 2: Distribution of emotions in MELT and filtered MELD": ""
        },
        {
          "Table 2: Distribution of emotions in MELT and filtered MELD": ""
        },
        {
          "Table 2: Distribution of emotions in MELT and filtered MELD": "4. Experiments"
        },
        {
          "Table 2: Distribution of emotions in MELT and filtered MELD": ""
        },
        {
          "Table 2: Distribution of emotions in MELT and filtered MELD": "4.1.\nSubjective Experiment"
        },
        {
          "Table 2: Distribution of emotions in MELT and filtered MELD": ""
        },
        {
          "Table 2: Distribution of emotions in MELT and filtered MELD": "To assess\nthe emotion annotation quality, we invited 20 par-"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table 3: Com- Theseoutcomes, whilestillfallingshortofstate-of-the-artre-",
      "data": [
        {
          "Table 3: Speech Emotion Recognition Performance of the Evaluation system on four Emotion Recognition Datasets. The ‘#cl’ indicates": ""
        },
        {
          "Table 3: Speech Emotion Recognition Performance of the Evaluation system on four Emotion Recognition Datasets. The ‘#cl’ indicates": "Backbone"
        },
        {
          "Table 3: Speech Emotion Recognition Performance of the Evaluation system on four Emotion Recognition Datasets. The ‘#cl’ indicates": ""
        },
        {
          "Table 3: Speech Emotion Recognition Performance of the Evaluation system on four Emotion Recognition Datasets. The ‘#cl’ indicates": "wav2vec 2.0 Base"
        },
        {
          "Table 3: Speech Emotion Recognition Performance of the Evaluation system on four Emotion Recognition Datasets. The ‘#cl’ indicates": ""
        },
        {
          "Table 3: Speech Emotion Recognition Performance of the Evaluation system on four Emotion Recognition Datasets. The ‘#cl’ indicates": "wav2vec 2.0 Aud"
        },
        {
          "Table 3: Speech Emotion Recognition Performance of the Evaluation system on four Emotion Recognition Datasets. The ‘#cl’ indicates": ""
        },
        {
          "Table 3: Speech Emotion Recognition Performance of the Evaluation system on four Emotion Recognition Datasets. The ‘#cl’ indicates": "Hubert Base"
        },
        {
          "Table 3: Speech Emotion Recognition Performance of the Evaluation system on four Emotion Recognition Datasets. The ‘#cl’ indicates": ""
        },
        {
          "Table 3: Speech Emotion Recognition Performance of the Evaluation system on four Emotion Recognition Datasets. The ‘#cl’ indicates": "WavLM Base+"
        },
        {
          "Table 3: Speech Emotion Recognition Performance of the Evaluation system on four Emotion Recognition Datasets. The ‘#cl’ indicates": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 3: Com- Theseoutcomes, whilestillfallingshortofstate-of-the-artre-",
      "data": [
        {
          "accordingly (i. e., low/mid/high).": ""
        },
        {
          "accordingly (i. e., low/mid/high).": "Table 4: Performance metrics for pitch and loudness."
        },
        {
          "accordingly (i. e., low/mid/high).": "Data Split"
        },
        {
          "accordingly (i. e., low/mid/high).": "Train"
        },
        {
          "accordingly (i. e., low/mid/high).": ""
        },
        {
          "accordingly (i. e., low/mid/high).": "Test"
        },
        {
          "accordingly (i. e., low/mid/high).": ""
        },
        {
          "accordingly (i. e., low/mid/high).": "Train"
        },
        {
          "accordingly (i. e., low/mid/high).": ""
        },
        {
          "accordingly (i. e., low/mid/high).": "Test"
        },
        {
          "accordingly (i. e., low/mid/high).": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 3: Com- Theseoutcomes, whilestillfallingshortofstate-of-the-artre-",
      "data": [
        {
          "0.4568\n0.4887\n0.4575\nWavLM Base+\nMELT\n0.3114": "MELD\n0.4188\n0.4570\n0.4102\n0.3114",
          "0.3244\n0.2724\n0.2174\n0.3135\n0.3007\n0.2480\n0.3114\n0.2372": "0.2405\n0.3114\n0.2827\n0.2276\n0.1553\n0.3069\n0.2912\n0.2018"
        },
        {
          "0.4568\n0.4887\n0.4575\nWavLM Base+\nMELT\n0.3114": "",
          "0.3244\n0.2724\n0.2174\n0.3135\n0.3007\n0.2480\n0.3114\n0.2372": "wav2vec 2.0 Base shows relatively modest gains.\nThis high-"
        },
        {
          "0.4568\n0.4887\n0.4575\nWavLM Base+\nMELT\n0.3114": "ANG\n0.7062\n0.2938",
          "0.3244\n0.2724\n0.2174\n0.3135\n0.3007\n0.2480\n0.3114\n0.2372": ""
        },
        {
          "0.4568\n0.4887\n0.4575\nWavLM Base+\nMELT\n0.3114": "",
          "0.3244\n0.2724\n0.2174\n0.3135\n0.3007\n0.2480\n0.3114\n0.2372": "lights the importance of pretraining in enhancing model perfor-"
        },
        {
          "0.4568\n0.4887\n0.4575\nWavLM Base+\nMELT\n0.3114": "",
          "0.3244\n0.2724\n0.2174\n0.3135\n0.3007\n0.2480\n0.3114\n0.2372": "mance, particularly when addressing datasets with limited or"
        },
        {
          "0.4568\n0.4887\n0.4575\nWavLM Base+\nMELT\n0.3114": "SUR\n0.7039\n0.2961",
          "0.3244\n0.2724\n0.2174\n0.3135\n0.3007\n0.2480\n0.3114\n0.2372": ""
        },
        {
          "0.4568\n0.4887\n0.4575\nWavLM Base+\nMELT\n0.3114": "",
          "0.3244\n0.2724\n0.2174\n0.3135\n0.3007\n0.2480\n0.3114\n0.2372": "partial alignment to the training annotations."
        },
        {
          "0.4568\n0.4887\n0.4575\nWavLM Base+\nMELT\n0.3114": "JOY\n0.5882\n0.4118",
          "0.3244\n0.2724\n0.2174\n0.3135\n0.3007\n0.2480\n0.3114\n0.2372": ""
        },
        {
          "0.4568\n0.4887\n0.4575\nWavLM Base+\nMELT\n0.3114": "",
          "0.3244\n0.2724\n0.2174\n0.3135\n0.3007\n0.2480\n0.3114\n0.2372": "5.2. Audio characteristic"
        },
        {
          "0.4568\n0.4887\n0.4575\nWavLM Base+\nMELT\n0.3114": "DIS\n0.5638\n0.4362",
          "0.3244\n0.2724\n0.2174\n0.3135\n0.3007\n0.2480\n0.3114\n0.2372": ""
        },
        {
          "0.4568\n0.4887\n0.4575\nWavLM Base+\nMELT\n0.3114": "",
          "0.3244\n0.2724\n0.2174\n0.3135\n0.3007\n0.2480\n0.3114\n0.2372": "To further analyze the annotation performance of the GPT-4o on"
        },
        {
          "0.4568\n0.4887\n0.4575\nWavLM Base+\nMELT\n0.3114": "NEU\n0.5248\n0.4752",
          "0.3244\n0.2724\n0.2174\n0.3135\n0.3007\n0.2480\n0.3114\n0.2372": ""
        },
        {
          "0.4568\n0.4887\n0.4575\nWavLM Base+\nMELT\n0.3114": "",
          "0.3244\n0.2724\n0.2174\n0.3135\n0.3007\n0.2480\n0.3114\n0.2372": "audio characteristics, we adapted the query generation pipeline"
        },
        {
          "0.4568\n0.4887\n0.4575\nWavLM Base+\nMELT\n0.3114": "",
          "0.3244\n0.2724\n0.2174\n0.3135\n0.3007\n0.2480\n0.3114\n0.2372": "proposed by ParaCLAP [29] to categorize descriptions and ex-"
        },
        {
          "0.4568\n0.4887\n0.4575\nWavLM Base+\nMELT\n0.3114": "FEA\n0.4759\n0.5241",
          "0.3244\n0.2724\n0.2174\n0.3135\n0.3007\n0.2480\n0.3114\n0.2372": ""
        },
        {
          "0.4568\n0.4887\n0.4575\nWavLM Base+\nMELT\n0.3114": "",
          "0.3244\n0.2724\n0.2174\n0.3135\n0.3007\n0.2480\n0.3114\n0.2372": "tract the extended Geneva Minimalistic Acoustic Parameter Set"
        },
        {
          "0.4568\n0.4887\n0.4575\nWavLM Base+\nMELT\n0.3114": "",
          "0.3244\n0.2724\n0.2174\n0.3135\n0.3007\n0.2480\n0.3114\n0.2372": "(eGeMAPS)\n[30] using openSMILE [31]. The pitch and loud-"
        },
        {
          "0.4568\n0.4887\n0.4575\nWavLM Base+\nMELT\n0.3114": "SAD\n0.4356\n0.5644",
          "0.3244\n0.2724\n0.2174\n0.3135\n0.3007\n0.2480\n0.3114\n0.2372": ""
        },
        {
          "0.4568\n0.4887\n0.4575\nWavLM Base+\nMELT\n0.3114": "",
          "0.3244\n0.2724\n0.2174\n0.3135\n0.3007\n0.2480\n0.3114\n0.2372": "ness attributes are binned according to their distribution (bottom"
        },
        {
          "0.4568\n0.4887\n0.4575\nWavLM Base+\nMELT\n0.3114": "MELT\nMELD",
          "0.3244\n0.2724\n0.2174\n0.3135\n0.3007\n0.2480\n0.3114\n0.2372": ""
        },
        {
          "0.4568\n0.4887\n0.4575\nWavLM Base+\nMELT\n0.3114": "",
          "0.3244\n0.2724\n0.2174\n0.3135\n0.3007\n0.2480\n0.3114\n0.2372": "30 %, middle 40 %, top 30 %) and pseudo-captions are mapped"
        },
        {
          "0.4568\n0.4887\n0.4575\nWavLM Base+\nMELT\n0.3114": "",
          "0.3244\n0.2724\n0.2174\n0.3135\n0.3007\n0.2480\n0.3114\n0.2372": "accordingly (i. e., low/mid/high)."
        },
        {
          "0.4568\n0.4887\n0.4575\nWavLM Base+\nMELT\n0.3114": "Figure 3: MOS results on every emotion categories.",
          "0.3244\n0.2724\n0.2174\n0.3135\n0.3007\n0.2480\n0.3114\n0.2372": ""
        },
        {
          "0.4568\n0.4887\n0.4575\nWavLM Base+\nMELT\n0.3114": "",
          "0.3244\n0.2724\n0.2174\n0.3135\n0.3007\n0.2480\n0.3114\n0.2372": "Table 4: Performance metrics for pitch and loudness."
        },
        {
          "0.4568\n0.4887\n0.4575\nWavLM Base+\nMELT\n0.3114": "5. Results and Analysis",
          "0.3244\n0.2724\n0.2174\n0.3135\n0.3007\n0.2480\n0.3114\n0.2372": "UAR\nACC\nF1\nChar.\nData Split"
        },
        {
          "0.4568\n0.4887\n0.4575\nWavLM Base+\nMELT\n0.3114": "5.1. Performance",
          "0.3244\n0.2724\n0.2174\n0.3135\n0.3007\n0.2480\n0.3114\n0.2372": "Train\n0.4761\n0.5213\n0.4376"
        },
        {
          "0.4568\n0.4887\n0.4575\nWavLM Base+\nMELT\n0.3114": "",
          "0.3244\n0.2724\n0.2174\n0.3135\n0.3007\n0.2480\n0.3114\n0.2372": "Pitch"
        },
        {
          "0.4568\n0.4887\n0.4575\nWavLM Base+\nMELT\n0.3114": "",
          "0.3244\n0.2724\n0.2174\n0.3135\n0.3007\n0.2480\n0.3114\n0.2372": "Test\n0.4880\n0.5493\n0.4534"
        },
        {
          "0.4568\n0.4887\n0.4575\nWavLM Base+\nMELT\n0.3114": "The overall MOS result\nis shown in Fig. 3, participants over-",
          "0.3244\n0.2724\n0.2174\n0.3135\n0.3007\n0.2480\n0.3114\n0.2372": ""
        },
        {
          "0.4568\n0.4887\n0.4575\nWavLM Base+\nMELT\n0.3114": "all demonstrated a preference for MELT annotations. The high",
          "0.3244\n0.2724\n0.2174\n0.3135\n0.3007\n0.2480\n0.3114\n0.2372": "Train\n0.4103\n0.4546\n0.4005"
        },
        {
          "0.4568\n0.4887\n0.4575\nWavLM Base+\nMELT\n0.3114": "",
          "0.3244\n0.2724\n0.2174\n0.3135\n0.3007\n0.2480\n0.3114\n0.2372": "Loud."
        },
        {
          "0.4568\n0.4887\n0.4575\nWavLM Base+\nMELT\n0.3114": "agreement\n(> 70%)\nfor\n‘anger’ and ‘surprise’\nindicates\nthat",
          "0.3244\n0.2724\n0.2174\n0.3135\n0.3007\n0.2480\n0.3114\n0.2372": "Test\n0.3904\n0.4374\n0.3824"
        },
        {
          "0.4568\n0.4887\n0.4575\nWavLM Base+\nMELT\n0.3114": "GPT-4o effectively integrates\ninternet-sourced knowledge\nto",
          "0.3244\n0.2724\n0.2174\n0.3135\n0.3007\n0.2480\n0.3114\n0.2372": ""
        },
        {
          "0.4568\n0.4887\n0.4575\nWavLM Base+\nMELT\n0.3114": "capture the diversity in expressions. However, an opposite trend",
          "0.3244\n0.2724\n0.2174\n0.3135\n0.3007\n0.2480\n0.3114\n0.2372": "Across both the training and test sets, all metrics consis-"
        },
        {
          "0.4568\n0.4887\n0.4575\nWavLM Base+\nMELT\n0.3114": "is observed for ‘fear’ and ‘sadness’, as they mostly transition to",
          "0.3244\n0.2724\n0.2174\n0.3135\n0.3007\n0.2480\n0.3114\n0.2372": "tently surpass random guessing, demonstrating that GPT-4o ef-"
        },
        {
          "0.4568\n0.4887\n0.4575\nWavLM Base+\nMELT\n0.3114": "‘neutral’, as shown in Fig. 1. These reannotated audio clips of-",
          "0.3244\n0.2724\n0.2174\n0.3135\n0.3007\n0.2480\n0.3114\n0.2372": "fectively captures\nrelevant characteristics\nfrom its embedded"
        },
        {
          "0.4568\n0.4887\n0.4575\nWavLM Base+\nMELT\n0.3114": "ten feature lower arousal and subtler emotional contexts. The",
          "0.3244\n0.2724\n0.2174\n0.3135\n0.3007\n0.2480\n0.3114\n0.2372": "knowledge.\nNotably,\nthe performance of pitch exceeds\nthat"
        },
        {
          "0.4568\n0.4887\n0.4575\nWavLM Base+\nMELT\n0.3114": "comparable preferences suggest possible biases stemming from",
          "0.3244\n0.2724\n0.2174\n0.3135\n0.3007\n0.2480\n0.3114\n0.2372": "of\nloudness,\nlikely reflecting human preferences in voice de-"
        },
        {
          "0.4568\n0.4887\n0.4575\nWavLM Base+\nMELT\n0.3114": "GPT-4o’s reliance on internet-derived emotional knowledge.",
          "0.3244\n0.2724\n0.2174\n0.3135\n0.3007\n0.2480\n0.3114\n0.2372": "scriptions [32], which may contribute to biases in the output."
        },
        {
          "0.4568\n0.4887\n0.4575\nWavLM Base+\nMELT\n0.3114": "Classification results are summarized in Table 3.\nCom-",
          "0.3244\n0.2724\n0.2174\n0.3135\n0.3007\n0.2480\n0.3114\n0.2372": "These outcomes, while still falling short of state-of-the-art re-"
        },
        {
          "0.4568\n0.4887\n0.4575\nWavLM Base+\nMELT\n0.3114": "monly\nused\nevaluation metrics,\nunweighted\naccuracy\nrecall",
          "0.3244\n0.2724\n0.2174\n0.3135\n0.3007\n0.2480\n0.3114\n0.2372": "sults, showcase the potential of LLMs with further refinement"
        },
        {
          "0.4568\n0.4887\n0.4575\nWavLM Base+\nMELT\n0.3114": "(UAR), accuracy (ACC), and F1 score, are applied to evaluate",
          "0.3244\n0.2724\n0.2174\n0.3135\n0.3007\n0.2480\n0.3114\n0.2372": "for tasks traditionally dominated by handcrafted features."
        },
        {
          "0.4568\n0.4887\n0.4575\nWavLM Base+\nMELT\n0.3114": "the performance of the SER task. Our first observation is that",
          "0.3244\n0.2724\n0.2174\n0.3135\n0.3007\n0.2480\n0.3114\n0.2372": ""
        },
        {
          "0.4568\n0.4887\n0.4575\nWavLM Base+\nMELT\n0.3114": "the in-domain results\ntrained on MELT generally outperform",
          "0.3244\n0.2724\n0.2174\n0.3135\n0.3007\n0.2480\n0.3114\n0.2372": ""
        },
        {
          "0.4568\n0.4887\n0.4575\nWavLM Base+\nMELT\n0.3114": "",
          "0.3244\n0.2724\n0.2174\n0.3135\n0.3007\n0.2480\n0.3114\n0.2372": "6. Conclusion"
        },
        {
          "0.4568\n0.4887\n0.4575\nWavLM Base+\nMELT\n0.3114": "the original MELD, showcasing how our annotation improves",
          "0.3244\n0.2724\n0.2174\n0.3135\n0.3007\n0.2480\n0.3114\n0.2372": ""
        },
        {
          "0.4568\n0.4887\n0.4575\nWavLM Base+\nMELT\n0.3114": "upon the general SSL-based benchmark architecture [28].",
          "0.3244\n0.2724\n0.2174\n0.3135\n0.3007\n0.2480\n0.3114\n0.2372": "In\nthis work, we\nintroduce MELT ,\na multimodal\nemotion"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "7. Acknowledgment": "",
          "[16]\nS. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria, and": "R. Mihalcea, “Meld: A multimodal multi-party dataset for emo-"
        },
        {
          "7. Acknowledgment": "Bj¨orn W. Schuller is also with the Munich Data Science Insti-",
          "[16]\nS. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria, and": ""
        },
        {
          "7. Acknowledgment": "",
          "[16]\nS. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria, and": "tion\nrecognition\nin\nconversations,”\nin Proceedings"
        },
        {
          "7. Acknowledgment": "tute and the Konrad Zuse School of Excellence in Reliable AI,",
          "[16]\nS. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria, and": "Meeting of\nthe Association for Computational Linguistics, Flo-"
        },
        {
          "7. Acknowledgment": "both in Munich, Germany. We are grateful to the China Schol-",
          "[16]\nS. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria, and": "rence, Italy, 2019, pp. 527–536."
        },
        {
          "7. Acknowledgment": "arship Council\n(CSC), Grant # 202006290013 to support\nthis",
          "[16]\nS. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria, and": "[17] M. D. Pell and S. A. Kotz, “On the time course of vocal emotion"
        },
        {
          "7. Acknowledgment": "work. We would also like to thank Prof. Xinzhou Xu and ev-",
          "[16]\nS. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria, and": "recognition,” PlOS one, vol. 6, no. 11, p. e27256, 2011."
        },
        {
          "7. Acknowledgment": "eryone involved in the MOS experiments for\ntheir\ninvaluable",
          "[16]\nS. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria, and": ""
        },
        {
          "7. Acknowledgment": "",
          "[16]\nS. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria, and": "[18]\nP. Kumawat and A. Routray, “Applying tdnn architectures for an-"
        },
        {
          "7. Acknowledgment": "contributions.",
          "[16]\nS. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria, and": "alyzing duration dependencies on speech emotion recognition.” in"
        },
        {
          "7. Acknowledgment": "",
          "[16]\nS. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria, and": "Interspeech, 2021, pp. 3410–3414."
        },
        {
          "7. Acknowledgment": "8. References",
          "[16]\nS. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria, and": "[19] M. M. Amin, R. Mao, E. Cambria, and B. W. Schuller, “A wide"
        },
        {
          "7. Acknowledgment": "",
          "[16]\nS. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria, and": "evaluation of chatgpt on affective computing tasks,” IEEE Trans-"
        },
        {
          "7. Acknowledgment": "[1] A. Triantafyllopoulos, L. Christ, A. Gebhard, X. Jing, A. Kathan,",
          "[16]\nS. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria, and": ""
        },
        {
          "7. Acknowledgment": "",
          "[16]\nS. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria, and": "actions on Affective Computing, vol. 15, no. 4, pp. 2204–2212,"
        },
        {
          "7. Acknowledgment": "M. Milling, I. Tsangko, S. Amiriparian, and B. W. Schuller, “Be-",
          "[16]\nS. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria, and": ""
        },
        {
          "7. Acknowledgment": "",
          "[16]\nS. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria, and": "2024."
        },
        {
          "7. Acknowledgment": "yond deep learning: Charting the next frontiers of affective com-",
          "[16]\nS. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria, and": ""
        },
        {
          "7. Acknowledgment": "",
          "[16]\nS. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria, and": "[20] A. Baevski, Y. Zhou, A. Mohamed,\nand M. Auli,\n“wav2vec"
        },
        {
          "7. Acknowledgment": "puting,” Intelligent Computing, vol. 3, p. 0089, 2024.",
          "[16]\nS. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria, and": ""
        },
        {
          "7. Acknowledgment": "",
          "[16]\nS. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria, and": "2.0: A framework for self-supervised learning of speech repre-"
        },
        {
          "7. Acknowledgment": "[2] X. Jing, K. Zhou, A. Triantafyllopoulos, and B. W. Schuller, “En-",
          "[16]\nS. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria, and": ""
        },
        {
          "7. Acknowledgment": "",
          "[16]\nS. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria, and": "sentations,” Advances in neural\ninformation processing systems,"
        },
        {
          "7. Acknowledgment": "hancing emotional text-to-speech controllability with natural lan-",
          "[16]\nS. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria, and": ""
        },
        {
          "7. Acknowledgment": "",
          "[16]\nS. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria, and": "vol. 33, pp. 12 449–12 460, 2020."
        },
        {
          "7. Acknowledgment": "guage guidance through contrastive learning and diffusion mod-",
          "[16]\nS. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria, and": ""
        },
        {
          "7. Acknowledgment": "",
          "[16]\nS. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria, and": "[21]\nJ. Wagner, A. Triantafyllopoulos, H. Wierstorf, M. Schmitt,"
        },
        {
          "7. Acknowledgment": "els,”\nin Proceedings of",
          "[16]\nS. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria, and": ""
        },
        {
          "7. Acknowledgment": "",
          "[16]\nS. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria, and": "F. Burkhardt, F. Eyben, and B. W. Schuller, “Dawn of the trans-"
        },
        {
          "7. Acknowledgment": "Speech and Signal Processing (ICASSP), Hyderabad,India, 2025,",
          "[16]\nS. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria, and": ""
        },
        {
          "7. Acknowledgment": "",
          "[16]\nS. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria, and": "former era in speech emotion recognition:\nclosing the valence"
        },
        {
          "7. Acknowledgment": "pp. 1–5.",
          "[16]\nS. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria, and": ""
        },
        {
          "7. Acknowledgment": "",
          "[16]\nS. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria, and": "gap,” IEEE Transactions on Pattern Analysis and Machine Intel-"
        },
        {
          "7. Acknowledgment": "[3] R. Kosti, J. M. Alvarez, A. Recasens, and A. Lapedriza, “Emotion",
          "[16]\nS. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria, and": ""
        },
        {
          "7. Acknowledgment": "",
          "[16]\nS. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria, and": "ligence, vol. 45, no. 9, pp. 10 745–10 759, 2023."
        },
        {
          "7. Acknowledgment": "recognition in context,” in Proceedings of the IEEE conference on",
          "[16]\nS. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria, and": ""
        },
        {
          "7. Acknowledgment": "computer vision and pattern recognition, 2017, pp. 1667–1675.",
          "[16]\nS. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria, and": "[22] W.-N. Hsu, B. Bolte, Y.-H. H. Tsai, K. Lakhotia, R. Salakhutdi-"
        },
        {
          "7. Acknowledgment": "",
          "[16]\nS. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria, and": "nov, and A. Mohamed, “Hubert: Self-supervised speech represen-"
        },
        {
          "7. Acknowledgment": "¨\n[4] Y. Etesam,\nO. N. Yalc¸ın, C. Zhang,\nand A. Lim,\n“Contextual",
          "[16]\nS. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria, and": ""
        },
        {
          "7. Acknowledgment": "",
          "[16]\nS. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria, and": "tation learning by masked prediction of hidden units,” IEEE/ACM"
        },
        {
          "7. Acknowledgment": "emotion recognition using large vision language models,” arXiv",
          "[16]\nS. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria, and": ""
        },
        {
          "7. Acknowledgment": "",
          "[16]\nS. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria, and": "transactions on audio, speech, and language processing, vol. 29,"
        },
        {
          "7. Acknowledgment": "preprint arXiv:2405.08992, 2024.",
          "[16]\nS. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria, and": ""
        },
        {
          "7. Acknowledgment": "",
          "[16]\nS. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria, and": "pp. 3451–3460, 2021."
        },
        {
          "7. Acknowledgment": "[5] N. Andalibi and J. Buss, “The human in emotion recognition on",
          "[16]\nS. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria, and": ""
        },
        {
          "7. Acknowledgment": "",
          "[16]\nS. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria, and": "[23]\nS. Chen, C. Wang, Z. Chen, Y. Wu, S. Liu, Z. Chen,\nJ. Li,"
        },
        {
          "7. Acknowledgment": "the\nsocial media: Attitudes, outcomes,\nrisks,” in Proceedings of",
          "[16]\nS. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria, and": ""
        },
        {
          "7. Acknowledgment": "",
          "[16]\nS. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria, and": "N. Kanda, T. Yoshioka, X. Xiao et al., “Wavlm: Large-scale self-"
        },
        {
          "7. Acknowledgment": "2020 CHI Conference on Human Factors in Computing Systems,",
          "[16]\nS. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria, and": ""
        },
        {
          "7. Acknowledgment": "",
          "[16]\nS. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria, and": "supervised pre-training for\nfull\nstack speech processing,” IEEE"
        },
        {
          "7. Acknowledgment": "New York, NY, USA, 2020, p. 1–16.",
          "[16]\nS. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria, and": ""
        },
        {
          "7. Acknowledgment": "",
          "[16]\nS. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria, and": "Journal of Selected Topics in Signal Processing, vol. 16, no. 6,"
        },
        {
          "7. Acknowledgment": "[6] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei,\nI. Sutskever",
          "[16]\nS. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria, and": "pp. 1505–1518, 2022."
        },
        {
          "7. Acknowledgment": "et al.,\n“Language models are unsupervised multitask learners,”",
          "[16]\nS. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria, and": ""
        },
        {
          "7. Acknowledgment": "",
          "[16]\nS. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria, and": "[24] C. Busso, M. Bulut, C.-C. Lee, A. Kazemzadeh, E. Mower,"
        },
        {
          "7. Acknowledgment": "OpenAI blog, vol. 1, no. 8, p. 9, 2019.",
          "[16]\nS. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria, and": ""
        },
        {
          "7. Acknowledgment": "",
          "[16]\nS. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria, and": "S. Kim,\nJ. N. Chang, S. Lee, and S. S. Narayanan, “Iemocap:"
        },
        {
          "7. Acknowledgment": "[7]\nT. B. Brown,\n“Language models are few-shot\nlearners,” arXiv",
          "[16]\nS. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria, and": "Interactive emotional dyadic motion capture database,” Language"
        },
        {
          "7. Acknowledgment": "preprint arXiv:2005.14165, 2020.",
          "[16]\nS. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria, and": "resources and evaluation, vol. 42, pp. 335–359, 2008."
        },
        {
          "7. Acknowledgment": "[8] A. Hurst, A. Lerer, A. P. Goucher, A. Perelman, A. Ramesh,",
          "[16]\nS. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria, and": "[25] M. K.\nPichora-Fuller\nand K. Dupuis,\n“Toronto\nemotional"
        },
        {
          "7. Acknowledgment": "A. Clark, A. Ostrow, A. Welihinda, A. Hayes, A. Radford et al.,",
          "[16]\nS. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria, and": "speech set\n(tess).”\nBorealis, 2020.\n[Online]. Available:\nhttps:"
        },
        {
          "7. Acknowledgment": "“Gpt-4o system card,” arXiv preprint arXiv:2410.21276, 2024.",
          "[16]\nS. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria, and": "//doi.org/10.5683/SP2/E8H2MF"
        },
        {
          "7. Acknowledgment": "[9]\nZ. Tan, D. Li, S. Wang, A. Beigi, B.\nJiang, A. Bhattacharjee,",
          "[16]\nS. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria, and": ""
        },
        {
          "7. Acknowledgment": "",
          "[16]\nS. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria, and": "[26]\nS. R. Livingstone and F. A. Russo,\n“The ryerson audio-visual"
        },
        {
          "7. Acknowledgment": "M. Karami, J. Li, L. Cheng, and H. Liu, “Large language models",
          "[16]\nS. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria, and": ""
        },
        {
          "7. Acknowledgment": "",
          "[16]\nS. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria, and": "database of emotional\nspeech and song (ravdess): A dynamic,"
        },
        {
          "7. Acknowledgment": "for data annotation: A survey,” arXiv preprint arXiv:2402.13446,",
          "[16]\nS. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria, and": ""
        },
        {
          "7. Acknowledgment": "",
          "[16]\nS. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria, and": "multimodal set of facial and vocal expressions in north american"
        },
        {
          "7. Acknowledgment": "2024.",
          "[16]\nS. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria, and": ""
        },
        {
          "7. Acknowledgment": "",
          "[16]\nS. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria, and": "english,” PloS one, vol. 13, no. 5, 2018."
        },
        {
          "7. Acknowledgment": "[10] M. Aldeen,\nJ. Luo, A. Lian, V. Zheng, A. Hong, P. Yetukuri,",
          "[16]\nS. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria, and": ""
        },
        {
          "7. Acknowledgment": "",
          "[16]\nS. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria, and": "[27] H. Cao, D. G. Cooper, M. K. Keutmann, R. C. Gur, A. Nenkova,"
        },
        {
          "7. Acknowledgment": "and L. Cheng,\n“Chatgpt vs. human annotators: A comprehen-",
          "[16]\nS. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria, and": ""
        },
        {
          "7. Acknowledgment": "",
          "[16]\nS. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria, and": "and R. Verma, “Crema-d: Crowd-sourced emotional multimodal"
        },
        {
          "7. Acknowledgment": "sive analysis of chatgpt\nfor\ntext annotation,” in Proceedings of",
          "[16]\nS. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria, and": ""
        },
        {
          "7. Acknowledgment": "",
          "[16]\nS. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria, and": "actors dataset,” IEEE Transactions on Affective Computing, vol. 5,"
        },
        {
          "7. Acknowledgment": "International Conference on Machine Learning and Applications",
          "[16]\nS. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria, and": ""
        },
        {
          "7. Acknowledgment": "",
          "[16]\nS. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria, and": "no. 4, pp. 377–390, 2014."
        },
        {
          "7. Acknowledgment": "(ICMLA), Florida, USA, 2023, pp. 602–609.",
          "[16]\nS. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria, and": ""
        },
        {
          "7. Acknowledgment": "",
          "[16]\nS. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria, and": "[28] H. Wu, H.-C. Chou, K.-W. Chang, L. Goncalves, J. Du, J.-S. R."
        },
        {
          "7. Acknowledgment": "[11]\nF. Gilardi, M. Alizadeh,\nand M. Kubli,\n“Chatgpt outperforms",
          "[16]\nS. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria, and": ""
        },
        {
          "7. Acknowledgment": "",
          "[16]\nS. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria, and": "Jang, C.-C. Lee, and H.-Y. Lee, “Emo-superb: An in-depth look"
        },
        {
          "7. Acknowledgment": "crowd workers for text-annotation tasks,” Proceedings of the Na-",
          "[16]\nS. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria, and": ""
        },
        {
          "7. Acknowledgment": "",
          "[16]\nS. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria, and": "at speech emotion recognition,” arXiv preprint arXiv:2402.13018,"
        },
        {
          "7. Acknowledgment": "tional Academy of Sciences, vol. 120, no. 30, 2023.",
          "[16]\nS. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria, and": ""
        },
        {
          "7. Acknowledgment": "",
          "[16]\nS. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria, and": "2024."
        },
        {
          "7. Acknowledgment": "[12] X. Mei, C. Meng, H. Liu, Q. Kong, T. Ko, C. Zhao, M. D. Plumb-",
          "[16]\nS. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria, and": ""
        },
        {
          "7. Acknowledgment": "",
          "[16]\nS. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria, and": "[29] X.\nJing, A. Triantafyllopoulos,\nand B.\nSchuller,\n“Paraclap–"
        },
        {
          "7. Acknowledgment": "ley, Y. Zou, and W. Wang, “Wavcaps: A chatgpt-assisted weakly-",
          "[16]\nS. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria, and": ""
        },
        {
          "7. Acknowledgment": "",
          "[16]\nS. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria, and": "towards a general\nlanguage-audio model\nfor computational par-"
        },
        {
          "7. Acknowledgment": "labelled audio captioning dataset for audio-language multimodal",
          "[16]\nS. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria, and": ""
        },
        {
          "7. Acknowledgment": "",
          "[16]\nS. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria, and": "International Speech Com-\nalinguistic tasks,” in Proceedings of"
        },
        {
          "7. Acknowledgment": "research,” IEEE/ACM Transactions on Audio, Speech, and Lan-",
          "[16]\nS. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria, and": ""
        },
        {
          "7. Acknowledgment": "",
          "[16]\nS. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria, and": "munication Association (INTERSPEECH), Kos\nIsland, Greece,"
        },
        {
          "7. Acknowledgment": "guage Processing, vol. 32, pp. 3339–3354, 2024.",
          "[16]\nS. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria, and": ""
        },
        {
          "7. Acknowledgment": "",
          "[16]\nS. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria, and": "September 2024."
        },
        {
          "7. Acknowledgment": "[13]\nS. Deshmukh, B. Elizalde, R. Singh, and H. Wang, “Pengi: An",
          "[16]\nS. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria, and": ""
        },
        {
          "7. Acknowledgment": "",
          "[16]\nS. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria, and": "[30]\nF. Eyben, K. R. Scherer, B. W. Schuller, J. Sundberg, E. Andr´e,"
        },
        {
          "7. Acknowledgment": "audio language model for audio tasks,” Advances in Neural Infor-",
          "[16]\nS. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria, and": ""
        },
        {
          "7. Acknowledgment": "",
          "[16]\nS. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria, and": "C. Busso, L. Y. Devillers,\nJ. Epps, P. Laukka, S. S. Narayanan"
        },
        {
          "7. Acknowledgment": "mation Processing Systems, vol. 36, pp. 18 090–18 108, 2023.",
          "[16]\nS. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria, and": ""
        },
        {
          "7. Acknowledgment": "",
          "[16]\nS. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria, and": "et al., “The geneva minimalistic acoustic parameter set (gemaps)"
        },
        {
          "7. Acknowledgment": "[14] Y. Xu, H. Chen,\nJ. Yu, Q. Huang, Z. Wu, S.-X. Zhang, G. Li,",
          "[16]\nS. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria, and": "for voice research and affective computing,” IEEE transactions on"
        },
        {
          "7. Acknowledgment": "Y\n. Luo, and R. Gu, “Secap: Speech emotion captioning with large",
          "[16]\nS. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria, and": "affective computing, vol. 7, no. 2, pp. 190–202, 2015."
        },
        {
          "7. Acknowledgment": "the AAAI Conference on Ar-\nlanguage model,” in Proceedings of",
          "[16]\nS. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria, and": ""
        },
        {
          "7. Acknowledgment": "",
          "[16]\nS. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria, and": "[31]\nF. Eyben, M. W¨ollmer, and B. Schuller, “openSMILE: the Munich"
        },
        {
          "7. Acknowledgment": "tificial Intelligence, vol. 38, 2024, pp. 19 323–19 331.",
          "[16]\nS. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria, and": ""
        },
        {
          "7. Acknowledgment": "",
          "[16]\nS. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria, and": "versatile and fast open-source audio feature extractor,” in Proc. the"
        },
        {
          "7. Acknowledgment": "[15] H. Touvron, T. Lavril, G.\nIzacard, X. Martinet, M.-A. Lachaux,",
          "[16]\nS. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria, and": "International Conference on Multimedia, 2010, pp. 1459–1462."
        },
        {
          "7. Acknowledgment": "T. Lacroix, B. Rozi`ere, N. Goyal, E. Hambro, F. Azhar et al.,",
          "[16]\nS. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria, and": ""
        },
        {
          "7. Acknowledgment": "",
          "[16]\nS. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria, and": "[32] N. Lavan, “How do we describe other people from voices and"
        },
        {
          "7. Acknowledgment": "“Llama: Open and efficient foundation language models,” arXiv",
          "[16]\nS. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria, and": ""
        },
        {
          "7. Acknowledgment": "",
          "[16]\nS. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria, and": "faces?” Cognition, vol. 230, p. 105253, 2023."
        },
        {
          "7. Acknowledgment": "preprint arXiv:2302.13971, 2023.",
          "[16]\nS. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria, and": ""
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Beyond deep learning: Charting the next frontiers of affective computing",
      "authors": [
        "A Triantafyllopoulos",
        "L Christ",
        "A Gebhard",
        "X Jing",
        "A Kathan",
        "M Milling",
        "I Tsangko",
        "S Amiriparian",
        "B Schuller"
      ],
      "year": "2024",
      "venue": "Intelligent Computing"
    },
    {
      "citation_id": "3",
      "title": "Enhancing emotional text-to-speech controllability with natural language guidance through contrastive learning and diffusion models",
      "authors": [
        "X Jing",
        "K Zhou",
        "A Triantafyllopoulos",
        "B Schuller"
      ],
      "year": "2025",
      "venue": "Proceedings of International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "4",
      "title": "Emotion recognition in context",
      "authors": [
        "R Kosti",
        "J Alvarez",
        "A Recasens",
        "A Lapedriza"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "5",
      "title": "Contextual emotion recognition using large vision language models",
      "authors": [
        "Y Etesam",
        "Ö Yalc ¸ın",
        "C Zhang",
        "A Lim"
      ],
      "year": "2024",
      "venue": "Contextual emotion recognition using large vision language models",
      "arxiv": "arXivarXiv:2405.08992"
    },
    {
      "citation_id": "6",
      "title": "The human in emotion recognition on social media: Attitudes, outcomes, risks",
      "authors": [
        "N Andalibi",
        "J Buss"
      ],
      "year": "2020",
      "venue": "Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems"
    },
    {
      "citation_id": "7",
      "title": "Language models are unsupervised multitask learners",
      "authors": [
        "A Radford",
        "J Wu",
        "R Child",
        "D Luan",
        "D Amodei",
        "I Sutskever"
      ],
      "year": "2019",
      "venue": "OpenAI blog"
    },
    {
      "citation_id": "8",
      "title": "Language models are few-shot learners",
      "authors": [
        "T Brown"
      ],
      "year": "2020",
      "venue": "Language models are few-shot learners",
      "arxiv": "arXiv:2005.14165"
    },
    {
      "citation_id": "9",
      "title": "Gpt-4o system card",
      "authors": [
        "A Hurst",
        "A Lerer",
        "A Goucher",
        "A Perelman",
        "A Ramesh",
        "A Clark",
        "A Ostrow",
        "A Welihinda",
        "A Hayes",
        "A Radford"
      ],
      "year": "2024",
      "venue": "Gpt-4o system card",
      "arxiv": "arXiv:2410.21276"
    },
    {
      "citation_id": "10",
      "title": "Large language models for data annotation: A survey",
      "authors": [
        "Z Tan",
        "D Li",
        "S Wang",
        "A Beigi",
        "B Jiang",
        "A Bhattacharjee",
        "M Karami",
        "J Li",
        "L Cheng",
        "H Liu"
      ],
      "year": "2024",
      "venue": "Large language models for data annotation: A survey",
      "arxiv": "arXiv:2402.13446"
    },
    {
      "citation_id": "11",
      "title": "Chatgpt vs. human annotators: A comprehensive analysis of chatgpt for text annotation",
      "authors": [
        "M Aldeen",
        "J Luo",
        "A Lian",
        "V Zheng",
        "A Hong",
        "P Yetukuri",
        "L Cheng"
      ],
      "year": "2023",
      "venue": "Proceedings of International Conference on Machine Learning and Applications (ICMLA)"
    },
    {
      "citation_id": "12",
      "title": "Chatgpt outperforms crowd workers for text-annotation tasks",
      "authors": [
        "F Gilardi",
        "M Alizadeh",
        "M Kubli"
      ],
      "year": "2023",
      "venue": "Proceedings of the National Academy of Sciences"
    },
    {
      "citation_id": "13",
      "title": "Wavcaps: A chatgpt-assisted weaklylabelled audio captioning dataset for audio-language multimodal research",
      "authors": [
        "X Mei",
        "C Meng",
        "H Liu",
        "Q Kong",
        "T Ko",
        "C Zhao",
        "M Plumbley",
        "Y Zou",
        "W Wang"
      ],
      "year": "2024",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "14",
      "title": "Pengi: An audio language model for audio tasks",
      "authors": [
        "S Deshmukh",
        "B Elizalde",
        "R Singh",
        "H Wang"
      ],
      "year": "2023",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "15",
      "title": "Secap: Speech emotion captioning with large language model",
      "authors": [
        "Y Xu",
        "H Chen",
        "J Yu",
        "Q Huang",
        "Z Wu",
        "S.-X Zhang",
        "G Li",
        "Y Luo",
        "R Gu"
      ],
      "year": "2024",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "16",
      "title": "Llama: Open and efficient foundation language models",
      "authors": [
        "H Touvron",
        "T Lavril",
        "G Izacard",
        "X Martinet",
        "M.-A Lachaux",
        "T Lacroix",
        "B Rozière",
        "N Goyal",
        "E Hambro",
        "F Azhar"
      ],
      "year": "2023",
      "venue": "Llama: Open and efficient foundation language models",
      "arxiv": "arXiv:2302.13971"
    },
    {
      "citation_id": "17",
      "title": "Meld: A multimodal multi-party dataset for emotion recognition in conversations",
      "authors": [
        "S Poria",
        "D Hazarika",
        "N Majumder",
        "G Naik",
        "E Cambria",
        "R Mihalcea"
      ],
      "year": "2019",
      "venue": "Proceedings of Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "18",
      "title": "On the time course of vocal emotion recognition",
      "authors": [
        "M Pell",
        "S Kotz"
      ],
      "year": "2011",
      "venue": "PlOS one"
    },
    {
      "citation_id": "19",
      "title": "Applying tdnn architectures for analyzing duration dependencies on speech emotion recognition",
      "authors": [
        "P Kumawat",
        "A Routray"
      ],
      "year": "2021",
      "venue": "Interspeech"
    },
    {
      "citation_id": "20",
      "title": "A wide evaluation of chatgpt on affective computing tasks",
      "authors": [
        "M Amin",
        "R Mao",
        "E Cambria",
        "B Schuller"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "21",
      "title": "wav2vec 2.0: A framework for self-supervised learning of speech representations",
      "authors": [
        "A Baevski",
        "Y Zhou",
        "A Mohamed",
        "M Auli"
      ],
      "year": "2020",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "22",
      "title": "Dawn of the transformer era in speech emotion recognition: closing the valence gap",
      "authors": [
        "J Wagner",
        "A Triantafyllopoulos",
        "H Wierstorf",
        "M Schmitt",
        "F Burkhardt",
        "F Eyben",
        "B Schuller"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "23",
      "title": "Hubert: Self-supervised speech representation learning by masked prediction of hidden units",
      "authors": [
        "W.-N Hsu",
        "B Bolte",
        "Y.-H Tsai",
        "K Lakhotia",
        "R Salakhutdinov",
        "A Mohamed"
      ],
      "year": "2021",
      "venue": "IEEE/ACM transactions on audio, speech, and language processing"
    },
    {
      "citation_id": "24",
      "title": "Wavlm: Large-scale selfsupervised pre-training for full stack speech processing",
      "authors": [
        "S Chen",
        "C Wang",
        "Z Chen",
        "Y Wu",
        "S Liu",
        "Z Chen",
        "J Li",
        "N Kanda",
        "T Yoshioka",
        "X Xiao"
      ],
      "year": "2022",
      "venue": "IEEE Journal of Selected Topics in Signal Processing"
    },
    {
      "citation_id": "25",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "26",
      "title": "Toronto emotional speech set (tess)",
      "authors": [
        "M Pichora-Fuller",
        "K Dupuis"
      ],
      "year": "2020",
      "venue": "Borealis",
      "doi": "10.5683/SP2/E8H2MF"
    },
    {
      "citation_id": "27",
      "title": "The ryerson audio-visual database of emotional speech and song (ravdess): A dynamic, multimodal set of facial and vocal expressions in north american english",
      "authors": [
        "S Livingstone",
        "F Russo"
      ],
      "year": "2018",
      "venue": "PloS one"
    },
    {
      "citation_id": "28",
      "title": "Crema-d: Crowd-sourced emotional multimodal actors dataset",
      "authors": [
        "H Cao",
        "D Cooper",
        "M Keutmann",
        "R Gur",
        "A Nenkova",
        "R Verma"
      ],
      "year": "2014",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "29",
      "title": "Emo-superb: An in-depth look at speech emotion recognition",
      "authors": [
        "H Wu",
        "H.-C Chou",
        "K.-W Chang",
        "L Goncalves",
        "J Du",
        "J.-S Jang",
        "C.-C Lee",
        "H.-Y Lee"
      ],
      "year": "2024",
      "venue": "Emo-superb: An in-depth look at speech emotion recognition",
      "arxiv": "arXiv:2402.13018"
    },
    {
      "citation_id": "30",
      "title": "Paraclaptowards a general language-audio model for computational paralinguistic tasks",
      "authors": [
        "X Jing",
        "A Triantafyllopoulos",
        "B Schuller"
      ],
      "year": "2024",
      "venue": "Proceedings of International Speech Communication Association (INTERSPEECH)"
    },
    {
      "citation_id": "31",
      "title": "The geneva minimalistic acoustic parameter set (gemaps) for voice research and affective computing",
      "authors": [
        "F Eyben",
        "K Scherer",
        "B Schuller",
        "J Sundberg",
        "E André",
        "C Busso",
        "L Devillers",
        "J Epps",
        "P Laukka",
        "S Narayanan"
      ],
      "year": "2015",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "32",
      "title": "openSMILE: the Munich versatile and fast open-source audio feature extractor",
      "authors": [
        "F Eyben",
        "M Wöllmer",
        "B Schuller"
      ],
      "year": "2010",
      "venue": "Proc. the International Conference on Multimedia"
    },
    {
      "citation_id": "33",
      "title": "How do we describe other people from voices and faces?",
      "authors": [
        "N Lavan"
      ],
      "year": "2023",
      "venue": "Cognition"
    }
  ]
}