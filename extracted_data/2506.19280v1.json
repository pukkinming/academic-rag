{
  "paper_id": "2506.19280v1",
  "title": "Emotion Detection On User Front-Facing App Interfaces For Enhanced Schedule Optimization: A Machine Learning Approach",
  "published": "2025-06-24T03:21:46Z",
  "authors": [
    "Feiting Yang",
    "Antoine Moevus",
    "Steve Lévesque"
  ],
  "keywords": [
    "Machine Learning",
    "Deep Learning",
    "UI/UX",
    "HCI",
    "Calendar Optimization",
    "Computer Activity",
    "Emotion Detection"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Human-Computer Interaction (HCI) has evolved significantly to incorporate emotion recognition capabilities, creating unprecedented opportunities for adaptive and personalized user experiences. This paper explores the integration of emotion detection into calendar applications, enabling user interfaces to dynamically respond to users' emotional states and stress levels, thereby enhancing both productivity and engagement. We present and evaluate two complementary approaches to emotion detection: a biometric-based method utilizing heart rate (HR) data extracted from electrocardiogram (ECG) signals processed through Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU) neural networks to predict the emotional dimensions of Valence, Arousal, and Dominance; and a behavioral method analyzing computer activity through multiple machine learning models to classify emotions based on fine-grained user interactions such as mouse movements, clicks, and keystroke patterns. Our comparative analysis, from real-world datasets, reveals that while both approaches demonstrate effectiveness, the computer activity-based method delivers superior consistency and accuracy, particularly for mouse-related interactions, which achieved approximately 90% accuracy. Furthermore, GRU networks outperformed LSTM models in the biometric approach, with Valence prediction reaching 84.38% accuracy.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "Human-Computer Interaction (HCI) has traditionally focused on enhancing user experiences by creating systems that are functional, intuitive, and engaging. However, conventional digital productivity tools often neglect a critical dimension of user experience: the emotional state of the user. In recent years, the integration of emotion recognition into HCI has introduced new opportunities for developing truly personalized interactions that adapt to users' emotional states, addressing a significant gap in current interface design  [1] .\n\nThe growing challenge of information overload and schedule management in digital environments demands more sophisticated approaches to interface design. Studies indicate that knowledge workers increasingly struggle with information Accepted in IISA 2025 and pending publication in IEEE. overload, which negatively impacts their cognitive performance and well-being  [2] -  [4] . This problem is compounded by ineffective scheduling and time management systems. A survey of 182 senior managers across various industries found that 71% considered meetings unproductive and inefficient, while 65% reported that meetings prevented them from completing their own work  [5] . Despite these challenges, current calendar and scheduling applications remain emotionally unaware, failing to consider how stress levels and emotional states significantly affect productivity and decision-making  [6] . This gap between user needs and system capabilities leads to suboptimal experiences and decreased efficiency in time management.\n\nEmotion recognition in HCI involves identifying a user's emotional state through three primary signal categories: physiological, behavioral, and contextual. Physiological techniques analyze biometric data such as heart rate, facial expressions, and EEG signals to predict distinct emotional states characterized by dimensions of Valence (positive/negative), Arousal (intensity), and Dominance (sense of control)  [7] -  [10] . Behavioral cues from device interactions, including mouse movements, clicks, and keystrokes, can detect shifts in emotions such as stress or relaxation  [11] ,  [12] . Contextual signals derived from user input text or speech provide additional emotional insights  [13] ,  [14] . By interpreting these multidimensional emotional signals, systems can dynamically adjust to meet users' evolving needs.\n\nSchedule management through a graphical calendar represents a particularly compelling application domain for emotion-aware HCI, offering significant potential to enhance both user experience and productivity. Throughout this paper, we will use the term \"calendar optimization\" to refer to schedule optimization through a graphical dynamic calendar. For this work we focus on an emotion-aware calendar system that can assess a user's emotional state through computer activity patterns and/or physiological data. This system then performs calendar optimization at two levels: i) interface adaptation-modifying visual elements and interaction pat-terns based on detected emotions, and ii) scheduling intelligence-applying constraint-based algorithms to generate emotionally-optimized event arrangements. For instance, detecting elevated stress levels might trigger interface simplification and suggest scheduling breaks to mitigate cognitive load, while positive emotional states could prompt recommendations for more challenging tasks when the user is best equipped to handle them.\n\nThe integration of emotion recognition with calendar optimization can be further enhanced through Constraint Satisfaction Problem (CSP) algorithms, which excel at finding optimal solutions within predefined constraints  [15] . When augmented with emotional awareness, these algorithms can dynamically adjust scheduling by considering not only traditional constraints like time and resources, but also the user's current and predicted emotional states. This creates a uniquely personalized and adaptive experience that aligns tasks with emotional readiness, potentially transforming how users interact with time management tools.\n\nThis paper explores two complementary approaches for emotion detection specifically suited for calendar optimization applications. Our first approach leverages biometric signals, extracting HR data from ECG signals to predict the emotional dimensions of Valence, Arousal, and Dominance by using LSTM networks and GRU networks. Our second approach analyses computer activity patterns, employing multiple machine learning models including Random Forest, Support Vector Machines, and ensemble methods to classify emotions based on fine-grained user interactions. By combining these approaches, we address the limitations of single-modality emotion detection systems and provide a more robust foundation for emotion-aware interface design.\n\nThe innovation of our work lies in: i) the comparative analysis of biometric and behavioral approaches to emotion detection in a calendar optimization context; ii) the development of specific models for detecting emotional dimensions relevant to productivity; and iii) establishing an empirical foundation for emotion-driven UI/UX design that can significantly enhance scheduling systems.\n\nThe remainder of this paper is organized as follows: Section I-A describes the global emotion-based planification system, Section II provides an overview of the common approaches to emotion recognition, along with a detailed description of the deep learning models employed. Section III explores two novel approaches based on computer activity and heart rate signals. Section IV explains the experimental results obtained. Finally, Section V concludes the paper and outlines directions for future work.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "A. System Architecture",
      "text": "The architecture of our emotion-based calendar optimization system is illustrated in Fig.  1 . This integrated system consists of three primary components working in concert. The first component is the a front-facing interface with which users interact. It serves both as an input mechanism and visualization layer. This interface simultaneously collects behavioral data while channeling it to the Emotion Detection System (EDS). The second component is the EDS. It functions as the system's perceptual core, processing two distinct data streams: i) physiological signals captured from wearable devices such as heart rate data from fitness trackers or smartwatches, and ii) behavioral patterns extracted from computer interactions including mouse movements, keystroke dynamics, and click patterns. Advanced machine learning models within the EDS process these multimodal inputs to classify the user's current emotional state along the dimensions of Valence, Arousal, and Dominance.\n\nThe third component is an adaptive scheduling engine powered by a CSP solver. This component receives three input categories: the emotional state vector from the EDS, explicit user-defined parameters (start/end times, task priorities), and implicit contextual variables (efficiency metrics, multitasking capacity). The CSP algorithm processes these constraints through an optimization function that balances productivity requirements with emotional well-being considerations. The output is a dynamically generated calendar schedule that not only satisfies temporal constraints but also aligns task difficulty and cognitive demands with the user's detected emotional state, creating a personalized scheduling experience that adapts in real-time to changing emotional conditions.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Ii. Related Work",
      "text": "This section examines the foundational techniques that inform our emotion-aware calendar optimization system. We first present constraint satisfaction algorithms for calendar scheduling, then survey the current landscape of emotion detection methodologies across multiple modalities. Finally, we review the neural network architectures employed in our approach.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "A. Constraint Satisfaction",
      "text": "CSPs are a class of computational problems that involve finding an assignment of values to a set of variables such that a predefined set of constraints is satisfied. A CSP instance can be formally represented as a triplet (V, D, C), where:\n\nIn emotion-aware calendar optimization, we extend the traditional CSP framework to incorporate emotional parameters that influence scheduling decisions.\n\nThe formulation of this extended problem is as follows: Variables and Parameters: Let E 1 , E 2 , . . . , E n represent the events to be scheduled, where n is the total number of events. Each event E i has the following properties:\n\n-Multitasking capability m i ∈ {0, 1}, indicating whether the event can be performed concurrently with other events -Priority coefficient p i ∈ [0, 1], representing the relative importance of the event\n\n• Emotional context:\n\n-Emotional state vector that represents the user emotions detected at scheduling time i. E.g.,\n\n, where ε v , ε a , and ε d represent the valence, arousal, and dominance components Domains: Each event variable represents a set of feasible domains. For example:\n\n• The start time s i typically belongs to a set of available time slots, such as {9 : 00 AM, 9 : 30 AM, . . . , 6 : 00 PM}, • The end time e i is determined by the start time and duration, i.e., e i = s i + d i . Thus, the domain for the start time can be expressed as:\n\nConstraints Formulation: Several constraints govern the scheduling of events, including:\n\n• Temporal Exclusivity Constraint: Prevents temporal overlap between non-multitaskable events:\n\n• Priority-Based Sequencing Constraint: Enforces scheduling high-priority events earlier when appropriate:\n\n• Emotional Regulation Constraint: Ensures appropriate pacing based on detected emotional states:",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "B. Global Objective Function",
      "text": "The optimal solution minimizes the following multiobjective function:\n\nwhere:\n\n• S = {s 1 , s 2 , . . . , s n } represents a schedule (assignment of start times) • D = {D 1 , D 2 , . . . , D n } represents the duration of each tasks • f temporal (S) measures temporal efficiency (e.g., minimizing idle time) • f cognitive (S) evaluates cognitive load distribution • f emotional (S) quantifies emotional well-being preservation • f j (S) can be any arbitrary objective function complementing a specific business domain or environment • α 1 , α 2 , α 3 , . . . , α i are weights reflecting the relative importance of each objective This emotionally-informed CSP formulation enables calendars to adapt scheduling decisions based on real-time emotional states, creating a personalized, environment agnostic experience that balances productivity with user well-being.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "C. Emotion Detection 1) Facial Recognition:",
      "text": "The most common and well-known method of emotion recognition is facial recognition which is reliably associated with specific facial behaviors  [16] . It has been observed that both humans and animals exhibit certain muscular movements linked to particular mental states  [7] . Image classification systems using various CNN-based models applied to large, high-resolution image datasets have achieved competitive accuracy, with the highest reaching 95%  [17] -  [19] . Another approach for facial emotion detection is through facial emotion recognition in videos. Some researchers have proposed a CNN-LSTM-based neural network, which was trained on the CREMA-D dataset and tested on the RAVDEES dataset for six basic emotions. Others enhanced the method by integrating a bi-modal user interface paradigm to complement the visual-facial modality with an additional audio-lingual modality  [20] .\n\n2) Text-based emotion recognition: Human language understanding in Natural Language Processing (NLP) research has become a significant area of study, with emotion detection being a crucial aspect. The process of text-based emotion recognition  [14]  involves five key steps (Fig.  2 ). First, a dataset is collected, typically consisting of text data. Commonly used and well-formatted datasets include SemEval, Stanford Sentiment Treebank (SST), and the International Survey of Emotional Antecedents and Reactions (ISEAR). The next step is preprocessing, which involves tokenization, normalization, removal of stopwords, part-of-speech tagging, stemming, and lemmatization to prepare the text for analysis. Feature extraction follows, utilizing techniques such as bag of words, n-grams, TF-IDF, and word embeddings to convert the text into a format suitable for modelling. During the model development stage, machine learning algorithms, such as K-Nearest Neighbors (KNN) and Naive Bayes  [21] , or deep learning models, including Bi-LSTM  [22]  are employed for emotion classification. Additionally, hybrid models that integrate both machine learning and deep learning approaches, such as Convolutional Neural Networks (CNN) and Bi-GRU models  [13]  are also explored. The performance of the developed models is evaluated by comparing them with existing approaches, utilizing standard evaluation metrics such as accuracy, precision, and recall. 3) Device activity: Using computer activity data, such as keyboard and mouse interactions, is a non-invasive, low-cost method for emotion detection. It is possible to apply different machine learning models, including decision trees, k-nearest neighbors (KNN), naive Bayes, AdaBoost, rotation forests, and Bayesian networks, to analyze keyboard activity  [23] . Rather than developing a single classifier for all participants and predefined emotional states, multiple binary classifiers were utilized to classify each emotion. The best results were achieved for fear and anger, with average performance for sadness, happiness, and boredom, and the poorest performance for surprise and disgust. Other researchers  [24]  employed 50fold cross-validation and applied KNN, multilayer perceptron (MLP), and support vector machines (SVM) as classifiers on self-collected mouse trajectory data to detect pleasant and unpleasant emotions. The KNN method (with k=5) and a 50% decay time yielded the best performance. The multimodal user interface aspect can also be applied with keyboard activity as a modality and tailored for mobile devices  [25] .\n\n4) Signal approach: A prevalent approach for emotion detection utilizes physiological signals, particularly electroencephalogram (EEG) signals, to recognize emotions by analyzing three primary influential features  [26] :\n\n• Valence: Positive, happy emotions are associated with higher frontal coherence in alpha waves and increased beta power in the right parietal lobe, in contrast to negative emotions. • Arousal: Excitement is characterized by higher beta power and coherence in the parietal lobe, accompanied by lower alpha activity. • Dominance: The strength of emotion is typically represented in the EEG by an increased beta-to-alpha activity ratio in the frontal lobe, along with heightened beta activity in the parietal lobe. Some researchers  [10]  have analyzed EEG signals and employed neural networks to classify six emotions based on emotional valence and arousal. Others  [9]  have utilized deep neural networks, such as LSTM, which have now achieved high prediction accuracy, reaching 99.9%.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "D. Deep Learning Models",
      "text": "1) LSTM model: A recurrent neural network (RNN) is a type of deep learning model designed to process sequential data, such as time series. However, in practice, RNNs often struggle to learn long-term dependencies in data due to issues like the vanishing gradient problem. LSTM networks  [27]  are a specialized variant of RNNs that address this limitation by effectively learning long-term dependencies. The architecture of an LSTM is depicted in Fig.  3 . At each time step t, the LSTM receives the input X t and the previous hidden state h t-1 . It consists of three primary gates: the forget gate, which determines which information from the previous cell state should be discarded; the input gate, which controls the incorporation of new information into the cell state; and the output gate, which regulates the information output from the cell state as the current hidden state h t . The cell state functions as the LSTM's long-term memory, while the hidden state captures short-term memory. These gates use sigmoid activation functions to manage the flow of information, and the tanh activation function is applied to scale the values in both the cell and hidden states.\n\n2) GRU model: The GRU  [28]  is a type of RNN architecture, closely related to the LSTM network. GRU is designed to model sequential data by enabling the selective retention or discarding of information over time. Compared to LSTM, GRU has a more streamlined architecture with fewer parameters, which often leads to improved training efficiency and reduced computational complexity.The GRU architecture, as depicted in Fig.  4 , consists of several key components designed to control the flow of information across time steps. It uses two gates: the reset gate (r t ) and the update gate (z t ). The reset gate determines how much of the previous hidden state should be discarded when calculating the candidate's hidden state, while the update gate controls how much of the previous hidden state should be retained. The candidate hidden state (h ′ t ) is calculated using the input at the current time step (x t ) and the reset gate, and is passed through a tanh activation function. The final hidden state (h t ) is a weighted sum of the previous hidden state and the candidate hidden state, with the update gate controlling the contribution of each.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "A. Dataset 1) Dreamer [29] Dataset For Biometric Signals Analysis:",
      "text": "The DREAMER dataset consists of physiological recordings and ratings collected from 23 volunteers during an experiment in which they viewed 18 movie clips. The clips were selected and rated by Gabert-Quillen et al., and EEG and ECG signals were recorded during the experiment. Each participant provided self-reported mood ratings based on perceived arousal, potency, and dominance, using a 5-point scale.\n\nThe dataset includes two main variables: \"stimuli\" and \"baseline.\" The \"stimuli\" variable contains data corresponding to the 18 movie clips, while the \"baseline\" variable includes data from neutral clips shown prior to each movie clip. The records for each movie clip are denoted as baselinei and stimulii, representing the data for the i th movie clip.\n\nFor the ECG recordings, each entry is structured as an M×2 matrix, where M represents the number of samples, and each column contains samples from the two ECG channels.\n\n2) DUX  [30]  Dataset for computer activity analysis: This dataset captures data from user interactions with the keyboard and mouse, alongside emotion data collected through the iMotions facial coding module and manual annotation. The dataset encompasses consistent data across all 36 test sessions with triggers enabled. Each file stores event data in a row-wise format, including event types (e.g., key presses, mouse clicks), target UI elements, modifier key states, and mouse coordinates. Additionally, the dataset features 12 mood intensities automatically detected by the iMotions system, as well as corresponding manually annotated mood data. These emotional intensities range from 0% to 100%.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "B. Methodology 1) Biometric Signals:",
      "text": "For data preparation, we retrieved the \"stimuli\" ECG data along with three dimensions of emotion measurement (Valence, Arousal, and Dominance) for 23 participants. Each participant's dataset contained 18 records corresponding to 18 clips from the original raw signal. The heart rate signal was extracted from the ECG data Fig.  5  by identifying the R-peak and calculating the R-R interval in seconds, from which the HR in beats per minute was derived. Since two ECG channels were available, we plotted the HR alongside the corresponding ECG signals from both channels (HR 1 and HR 2), as shown in Fig.  6 . The results indicated that the HR signals from both channels were identical. Therefore, for the training input, we selected HR 1. Next, we performed signal normalization and applied zeropadding to ensure all signals had the same length. Finally, the 5-point scale for each emotion dimension was transformed into two categories: low and high. Given that the data is time series, input sequences of a specified window size were generated from the heart rate data and corresponding labels, where each sequence represents a window of time steps and the label corresponds to the next emotion value. The three distinct models were then trained using a LSTM architecture with a dropout rate of 0.5 to mitigate overfitting due to the small dataset,and three separate models were trained utilizing GRU. The target labels corresponded to the three dimensions of emotion measurement. 2) Computer Activity: Fig.  7  represents the workflow for using computer activity to predict emotion. For data preparation, the dataset was partitioned into six sub-datasets based on distinct computer activities: four related to mouse actions (MouseMovement, MouseClick, MouseButtonUp, and MouseButtonDown) and two related to keyboard activities (KeyPressed and KeyReleased). Each sub-dataset was then labeled with the ground truth emotion value, which was determined by selecting the highest emotion score among the 12 mood intensities automatically detected by the iMotions system.\n\na) MouseMovement Sub-dataset: The input values for this sub-dataset consist of the x and y coordinates of the mouse cursor within the application at the time of the event. The predicted value is the emotion type associated with the highest intensity score detected.\n\nb) MouseClick, MouseButtonUp, and MouseButtonDown Sub-datasets: In these sub-datasets, the input values include the mouse button indicator (denoting which mouse button is pressed or released), along with the x and y coordinates of the mouse cursor in the application at the time of the event. The predicted value is the emotion type, as detected by the iMotions system. c) KeyPressed and KeyReleased Sub-datasets: These sub-datasets include several input features for keyboard events:\n\n• alt, control, shift, meta: Boolean values indicating whether the respective modifier keys were pressed during the event.\n\n• key: The key being pressed or released, with personal information replaced by \"ANONYMIZED\" where necessary. • repeat: A boolean value indicating whether the event was repeated due to the key being held down (FALSE for the first emission, TRUE for subsequent emissions).\n\nThe predicted value is the emotion type detected at the time of the event. Due to the imbalance in the data distribution across the different sub-datasets, we applied the Synthetic Minority Oversampling Technique (SMOTE)  [31]  to address this issue. Specifically, the resampling was performed based on the number of records within each sub-dataset. For the Mouse-Movement and KeyReleased sub-datasets, we resampled to 5,000 records for each emotion category. For the MouseClick, KeyPressed, MouseButtonUp, and MouseButtonDown subdatasets, we resampled to 500 records per emotion category. Subsequently, several machine learning models were applied to evaluate and compare their performance across the different sub-datasets. These models included a baseline logistic regression, random forest, SVM, decision tree, MLP, and Naive Bayes. Additionally, ensemble methods, such as gradient boosting machine, XGBoost, AdaBoost, and Bagging-Classifier, were employed.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Iv. Experimental Evaluation",
      "text": "",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "A. Biometric Analysis Results",
      "text": "To evaluate the performance of using heart rate signals for detecting levels of three emotion dimensions, cross-entropy (CE) loss (Eq. 1) was employed as the loss function, and accuracy (Eq. 2), based on correct predictions, was used to measure model performance.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Cross-Entropy Loss",
      "text": "where:\n\n• N denotes the total number of classes.\n\n• y i is the true label for class i (1 if the class corresponds to the true label, 0 otherwise). • p i represents the predicted probability for class i, derived from the model's output.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Accuracy = Number Of Correct Predictions Total Number Of Predictions (2)",
      "text": "Table  I  presents the performance of the LSTM and GRU models for the three emotional measurement dimensions. The test accuracy for the three emotion dimensions varies across the different architectures post-training. For the LSTM model, the accuracy for Valence (75%) is slightly higher than for Arousal (71.88%) and significantly higher than for Dominance (62.5%). The CE loss for all dimensions is below 0.025, with Arousal exhibiting the highest CE loss (0.0230), which is similar to Dominance's CE loss (0.0219), while Valence shows the lowest CE loss (0.0164).\n\nFor the GRU model, Valence achieves the highest accuracy (84.88%) and the lowest CE loss (0.0143) among all results. This is followed by Arousal, with an accuracy of 71.88% and a loss of 0.0187. The Dominance label shows the lowest accuracy (68.75%) and a loss of 0.0197.\n\nBased on these performance metrics, it can be concluded that both the LSTM and GRU models effectively predict the two levels (low and high) of the three emotion dimensions. However, the GRU model outperforms the LSTM model, exhibiting higher accuracy and lower loss for the same labels. Additionally, the performance of the Dominance label is the least favorable, which can be attributed to the fact that Dominance is more closely related to electrodermal activity and electromyography, while HR may not be a major contributing factor. The slightly lower performance of the Arousal label, compared to Valence, may be due to the nature of the data. Arousal is more pronounced in dynamic situations (e.g., during exercise), whereas in static contexts (such as the experiment involving the viewing of 18 movie clips), it is less noticeable. Furthermore, the variability in the baseline heart rate among individuals may reduce the model's generalization ability.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "B. Computer Activity Results",
      "text": "The accuracy (Eq. 2) was used as the primary metric to evaluate the performance of the machine learning models and ensemble methods for each sub-dataset. Table  II  presents the performance of various machine learning models and ensemble methods applied to the Mouse Movement activity, with the Random Forest model achieving the highest accuracy of 86.59%. Table II displays the performance for the Mouse Click activity, where the Decision Tree model outperforms others with an accuracy of 93.86%. Table  III  shows the results for the Key Pressed activity, with the ensemble method Bagging Classifier attaining the best performance at 53%. Table  III  highlights the performance of the Key Released activity, where the Support Vector Machine (SVM), XGBoost, AdaBoost, and Multi-layer Perceptron models all achieve the highest accuracy of 96.31%. Table IV presents the performance for the Mouse Button Up activity, with the ensemble method LightGBM achieving the highest accuracy of 93.86%. Finally, Table  IV  shows the results for the Mouse Button Down activity, where Random Forest and XGBoost models both achieve the highest accuracy of 93.17%.\n\nAcross all the tables, it can be observed that among the machine learning models, Random Forest performs the best, while XGBoost emerges as the most effective ensemble method. However, the performance of the machine learning models generally surpasses that of the ensemble methods.\n\nAmong all the computer activity sub-datasets, mouse-related activities consistently demonstrated stable and high performance in emotion recognition, with accuracy levels generally around 90%. This stability suggests that mouse movements and interactions are strong indicators for detecting emotional states. In contrast, the performance of keyboard-related activities was more variable. Specifically, the Key Released activity achieved the highest accuracy 96.31%, while the Key Pressed activity exhibited significantly lower accuracy. One possible explanation for this discrepancy may be the quality of the Key Pressed sub-dataset, which could influence the model's ability to accurately detect emotions from keyboard interactions.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "V. Conclusion",
      "text": "This study presents and evaluates two complementary approaches for emotion detection within the novel context of calendar optimization systems. Our extensive comparative analysis yields several significant insights with implications for emotion-aware interface design. Future research will pursue an extension of our analysis to mobile platforms by incorporating touch-based interaction metrics and developing specialized models for touchscreen dynamics. Moreover, we will enhance the biometric approach by investigating multimodal fusion techniques and we aim to develop and evaluate a fully integrated emotion-aware calendar optimization system in realworld settings.",
      "page_start": 8,
      "page_end": 8
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: This integrated system consists",
      "page": 2
    },
    {
      "caption": "Figure 1: Overview of the generic system for emotion-based calendar optimiza-",
      "page": 2
    },
    {
      "caption": "Figure 2: ). First, a",
      "page": 4
    },
    {
      "caption": "Figure 2: The process of text-based emotion recognition [14]",
      "page": 4
    },
    {
      "caption": "Figure 3: At each time step t,",
      "page": 4
    },
    {
      "caption": "Figure 3: The architecture of LSTM model",
      "page": 5
    },
    {
      "caption": "Figure 4: , consists of several key components",
      "page": 5
    },
    {
      "caption": "Figure 4: The architecture of GRU model",
      "page": 5
    },
    {
      "caption": "Figure 5: by identifying the R-peak and calculating the R-R interval",
      "page": 5
    },
    {
      "caption": "Figure 5: An example of ECG stimuli signal",
      "page": 6
    },
    {
      "caption": "Figure 6: Heart rate signals from both ECG channels (HR 1 and HR 2).",
      "page": 6
    },
    {
      "caption": "Figure 7: represents the workflow for",
      "page": 6
    },
    {
      "caption": "Figure 7: Computer activity workflow",
      "page": 6
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Method": "Logistic Regression",
          "Movement\n(Acc.)": "24.25%",
          "Click (Acc.)": "57.14%"
        },
        {
          "Method": "Random Forest",
          "Movement\n(Acc.)": "86.59%",
          "Click (Acc.)": "93.57%"
        },
        {
          "Method": "Support Vector Machine",
          "Movement\n(Acc.)": "51.75%",
          "Click (Acc.)": "64.42%"
        },
        {
          "Method": "XGBoost",
          "Movement\n(Acc.)": "84.35%",
          "Click (Acc.)": "93.57%"
        },
        {
          "Method": "LightGBM",
          "Movement\n(Acc.)": "83.2%",
          "Click (Acc.)": "92.86%"
        },
        {
          "Method": "Decision Tree",
          "Movement\n(Acc.)": "85.15%",
          "Click (Acc.)": "93.86%"
        },
        {
          "Method": "AdaBoost",
          "Movement\n(Acc.)": "30.66%",
          "Click (Acc.)": "51.29%"
        },
        {
          "Method": "Multi-layer Perceptrons",
          "Movement\n(Acc.)": "48.09%",
          "Click (Acc.)": "63.29%"
        },
        {
          "Method": "Naive Bayes",
          "Movement\n(Acc.)": "24.23%",
          "Click (Acc.)": "53.43%"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Method": "Logistic Regression",
          "Pressed (Acc.)": "31.5%",
          "Released (Acc.)": "24.28%"
        },
        {
          "Method": "Random Forest",
          "Pressed (Acc.)": "52.625%",
          "Released (Acc.)": "96.28%"
        },
        {
          "Method": "Support Vector Machine",
          "Pressed (Acc.)": "35.62%",
          "Released (Acc.)": "96.31%"
        },
        {
          "Method": "XGBoost",
          "Pressed (Acc.)": "52.25%",
          "Released (Acc.)": "96.31%"
        },
        {
          "Method": "LightGBM",
          "Pressed (Acc.)": "51.625%",
          "Released (Acc.)": "95.75%"
        },
        {
          "Method": "Decision Tree",
          "Pressed (Acc.)": "52.625%",
          "Released (Acc.)": "96.25%"
        },
        {
          "Method": "AdaBoost",
          "Pressed (Acc.)": "30.375%",
          "Released (Acc.)": "96.31%"
        },
        {
          "Method": "Multi-layer Perceptrons",
          "Pressed (Acc.)": "32.5%",
          "Released (Acc.)": "96.31%"
        },
        {
          "Method": "Bagging classifier",
          "Pressed (Acc.)": "53%",
          "Released (Acc.)": "96.28%"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Method": "Logistic Regression",
          "Up (Acc.)": "49.57%",
          "Down (Acc.)": "53%"
        },
        {
          "Method": "Random Forest",
          "Up (Acc.)": "93.43%",
          "Down (Acc.)": "93.71%"
        },
        {
          "Method": "Support Vector Machine",
          "Up (Acc.)": "64.57%",
          "Down (Acc.)": "65.14%"
        },
        {
          "Method": "XGBoost",
          "Up (Acc.)": "93.43%",
          "Down (Acc.)": "93.71%"
        },
        {
          "Method": "LightGBM",
          "Up (Acc.)": "93.86%",
          "Down (Acc.)": "89.71%"
        },
        {
          "Method": "Decision Tree",
          "Up (Acc.)": "93.71%",
          "Down (Acc.)": "92.14%"
        },
        {
          "Method": "AdaBoost",
          "Up (Acc.)": "61%",
          "Down (Acc.)": "51.71%"
        },
        {
          "Method": "Multi-layer Perceptrons",
          "Up (Acc.)": "64.51%",
          "Down (Acc.)": "61%"
        },
        {
          "Method": "Naive Bayes",
          "Up (Acc.)": "56.29%",
          "Down (Acc.)": "48.57%"
        }
      ],
      "page": 7
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Human-computer interaction using emotion recognition from facial expression",
      "authors": [
        "F Abdat",
        "C Maaoui",
        "A Pruski"
      ],
      "year": "2011",
      "venue": "2011 UKSim 5th European Symposium on Computer Modeling and Simulation"
    },
    {
      "citation_id": "2",
      "title": "Dealing with information overload: a comprehensive review",
      "authors": [
        "M Arnold",
        "M Goldschmitt",
        "T Rigotti"
      ],
      "year": "2023",
      "venue": "Frontiers in psychology"
    },
    {
      "citation_id": "3",
      "title": "in Information Ergonomics: A theoretical approach and practical experience in transportation",
      "authors": [
        "H Bubb"
      ],
      "year": "2012",
      "venue": "in Information Ergonomics: A theoretical approach and practical experience in transportation"
    },
    {
      "citation_id": "4",
      "title": "Coping with information overload in email communication: Evaluation of a training intervention",
      "authors": [
        "R Soucek",
        "K Moser"
      ],
      "year": "2010",
      "venue": "online Interactivity: Role of Technology in Behavior Change"
    },
    {
      "citation_id": "5",
      "title": "Stop the meeting madness",
      "authors": [
        "L Perlow",
        "C Hadley",
        "E Eun"
      ],
      "year": "2017",
      "venue": "Harvard Business Review"
    },
    {
      "citation_id": "6",
      "title": "Stress-sensitive it-systems at work: insights from an empirical investigation",
      "authors": [
        "M Fellmann",
        "F Lambusch",
        "A Waller"
      ],
      "year": "2019",
      "venue": "Business Information Systems: 22nd International Conference"
    },
    {
      "citation_id": "7",
      "title": "Facial emotion detection using deep learning",
      "authors": [
        "A Jaiswal",
        "A Raju",
        "S Deb"
      ],
      "year": "2020",
      "venue": "2020 International Conference for Emerging Technology (INCET)"
    },
    {
      "citation_id": "8",
      "title": "A cnn-lstm based deep neural networks for facial emotion detection in videos",
      "authors": [
        "A Hans",
        "S Rao"
      ],
      "year": "2021",
      "venue": "International Journal of Advances in Signal and Image Sciences"
    },
    {
      "citation_id": "9",
      "title": "Deepher: Human emotion recognition using an eeg-based deep learning network model",
      "authors": [
        "A Kumar",
        "A Kumar"
      ],
      "year": "2021",
      "venue": "Engineering Proceedings"
    },
    {
      "citation_id": "10",
      "title": "Eeg-based human interface for disabled individuals: Emotion expression with neural networks",
      "authors": [
        "A Choppin"
      ],
      "year": "2000",
      "venue": "Eeg-based human interface for disabled individuals: Emotion expression with neural networks"
    },
    {
      "citation_id": "11",
      "title": "Mouse trajectories and state anxiety: Feature selection with random forest",
      "authors": [
        "T Yamauchi"
      ],
      "year": "2013",
      "venue": "2013 Humaine Association Conference on Affective Computing and Intelligent Interaction"
    },
    {
      "citation_id": "12",
      "title": "A review of emotion recognition methods based on keystroke dynamics and mouse movements",
      "authors": [
        "A Kołakowska"
      ],
      "year": "2013",
      "venue": "The 6th International Conference on Human System Interaction (HSI)"
    },
    {
      "citation_id": "13",
      "title": "Text-based emotion recognition using deep learning approach",
      "authors": [
        "S Bharti",
        "S Varadhaganapathy",
        "R Gupta",
        "P Shukla",
        "M Bouye",
        "S Hingaa",
        "A Mahmoud"
      ],
      "year": "2022",
      "venue": "Computational Intelligence and Neuroscience"
    },
    {
      "citation_id": "14",
      "title": "A review on sentiment analysis and emotion detection from text",
      "authors": [
        "P Nandwani",
        "R Verma"
      ],
      "year": "2021",
      "venue": "Social Network Analysis and Mining"
    },
    {
      "citation_id": "15",
      "title": "Constraint satisfaction problems: Algorithms and applications",
      "authors": [
        "S Brailsford",
        "C Potts",
        "B Smith"
      ],
      "year": "1999",
      "venue": "European Journal of Operational Research"
    },
    {
      "citation_id": "16",
      "title": "Constants across cultures in the face and emotion",
      "authors": [
        "P Ekman",
        "W Friesen"
      ],
      "year": "1971",
      "venue": "Journal of Personality and Social Psychology"
    },
    {
      "citation_id": "17",
      "title": "Imagenet classification with deep convolutional neural networks",
      "authors": [
        "A Krizhevsky",
        "I Sutskever",
        "G Hinton"
      ],
      "year": "2012",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "18",
      "title": "Facial expression recognition via deep learning",
      "authors": [
        "Y Lv",
        "Z Feng",
        "C Xu"
      ],
      "year": "2014",
      "venue": "Smart Computing (SMARTCOMP) 2014 International Conference on"
    },
    {
      "citation_id": "19",
      "title": "Emotion recognition using deep convolutional neural networks",
      "authors": [
        "E Correa",
        "A Jonker",
        "M Ozo",
        "R Stolk"
      ],
      "year": "2016",
      "venue": "Emotion recognition using deep convolutional neural networks"
    },
    {
      "citation_id": "20",
      "title": "Emotion recognition: Empirical studies towards the combination of audio-lingual and visual-facial modalities through multi-attribute decision making",
      "authors": [
        "M Virvou",
        "G Tsihrintzis",
        "E Alepis",
        "I Stathopoulou",
        "K Kabassi"
      ],
      "year": "2012",
      "venue": "Int. J. Artif. Intell. Tools",
      "doi": "10.1142/S0218213012400015"
    },
    {
      "citation_id": "21",
      "title": "Emotion detection framework for twitter data using supervised classifiers",
      "authors": [
        "M Suhasini",
        "B Srinivasu"
      ],
      "year": "2020",
      "venue": "Data Engineering and Communication Technology: Proceedings of 3rd ICDECT-2K19"
    },
    {
      "citation_id": "22",
      "title": "Attention-based modeling for emotion detection and classification in textual conversations",
      "authors": [
        "W Ragheb",
        "J Azé",
        "S Bringay",
        "M Servajean"
      ],
      "year": "2019",
      "venue": "Attention-based modeling for emotion detection and classification in textual conversations",
      "arxiv": "arXiv:1906.07020"
    },
    {
      "citation_id": "23",
      "title": "Recognizing emotions on the basis of keystroke dynamics",
      "authors": [
        "A Kołakowska"
      ],
      "year": "2015",
      "venue": "2015 8th International Conference on Human System Interaction (HSI)"
    },
    {
      "citation_id": "24",
      "title": "Your mouse can tell about your emotions",
      "authors": [
        "P Lali",
        "M Naghizadeh",
        "H Nasrollahi",
        "H Moradi",
        "M Mirian"
      ],
      "year": "2014",
      "venue": "2014 4th International Conference on Computer and Knowledge Engineering (ICCKE)"
    },
    {
      "citation_id": "25",
      "title": "Multimodal object oriented user interfaces in mobile affective interaction",
      "authors": [
        "E Alepis",
        "M Virvou"
      ],
      "year": "2012",
      "venue": "Multim. Tools Appl",
      "doi": "10.1007/s11042-011-0744-y"
    },
    {
      "citation_id": "26",
      "title": "Eeg-based emotion recognition",
      "authors": [
        "D Bos"
      ],
      "year": "2006",
      "venue": "The influence of visual and auditory stimuli"
    },
    {
      "citation_id": "27",
      "title": "Long short-term memory",
      "authors": [
        "S Hochreiter",
        "J Schmidhuber"
      ],
      "year": "1997",
      "venue": "Neural Computation"
    },
    {
      "citation_id": "28",
      "title": "juergen@idsia.ch",
      "authors": [
        "Switzerland Lugano"
      ],
      "venue": "juergen@idsia.ch"
    },
    {
      "citation_id": "29",
      "title": "Empirical evaluation of gated recurrent neural networks on sequence modeling",
      "authors": [
        "J Chung",
        "C Gulcehre",
        "K Cho",
        "Y Bengio"
      ],
      "year": "2014",
      "venue": "Empirical evaluation of gated recurrent neural networks on sequence modeling"
    },
    {
      "citation_id": "30",
      "title": "Dreamer: A database for emotion recognition through eeg and ecg signals from wireless low-cost offthe-shelf devices",
      "authors": [
        "S Katsigiannis",
        "N Ramzan"
      ],
      "year": "2018",
      "venue": "IEEE Journal of Biomedical and Health Informatics"
    },
    {
      "citation_id": "31",
      "title": "Dux: a dataset of user interactions and user emotions",
      "authors": [
        "D Leppich",
        "C Bieber",
        "K Proschek",
        "P Harms",
        "U Schubert"
      ],
      "year": "2023",
      "venue": "i-com",
      "doi": "10.1515/icom-2023-0014"
    },
    {
      "citation_id": "32",
      "title": "Smote component reference",
      "authors": [
        "Microsoft"
      ],
      "year": "2025",
      "venue": "Smote component reference"
    }
  ]
}