{
  "paper_id": "2202.01252v2",
  "title": "Speaker Normalization For Self-Supervised Speech Emotion Recognition",
  "published": "2022-02-02T19:30:47Z",
  "authors": [
    "Itai Gat",
    "Hagai Aronowitz",
    "Weizhong Zhu",
    "Edmilson Morais",
    "Ron Hoory"
  ],
  "keywords": [
    "Speech emotion recognition",
    "speaker normalization",
    "self-supervised learning"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Large speech emotion recognition datasets are hard to obtain, and small datasets may contain biases. Deep-net-based classifiers, in turn, are prone to exploit those biases and find shortcuts such as speaker characteristics. These shortcuts usually harm a model's ability to generalize. To address this challenge, we propose a gradient-based adversary learning framework that learns a speech emotion recognition task while normalizing speaker characteristics from the feature representation. We demonstrate the efficacy of our method on both speaker-independent and speaker-dependent settings and obtain new state-of-the-art results on the challenging IEMOCAP dataset.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Over the last decade, advances in speech processing have been extremely impressive. Challenges like personal voice assistants that seemed daunting merely ten years ago are now part of our day-to-day routine. A crucial part of those systems is to interact with the user and understand his intents and emotions. Understanding the user's emotion can build trust between the user and the system and improve the user experience. Improving the accuracy of speech emotion recognition has, therefore, been a major research focus in recent years.\n\nReported improvements are, to a large extent, due to the availability of large general purpose and task-specific datasets  [1, 2] , computational performance advances (e.g., for GPUs), and a better understanding of how to encode inductive biases into deep neural nets. The use of self-supervised learning methods has played a crucial role in the advancements of textual and visual modalities due to their ability to learn strong feature representations from unlabeled data. Recently, these techniques have shown success in the speech processing community as well.\n\nThe annotation of speech emotion recognition is challenging, because it is affected by the annotator's bias towards linguistic, cultural, and social constraints. Speech emotion recognition, in particular, yields biased spurious correlations between speaker characteristics and the emotional class of the recording. Several datasets have been created over the years for the training and evaluation of machine learning methods for speech emotion recognition  [3, 4, 5] . Most of them are relatively small. The difficulty in learning from biased and small datasets are the main challenges facing the deep learning research community.\n\nThe classic self-supervised learning process relies on a representation trained on a large unlabeled dataset, and a downstream task trained on a relatively small labeled dataset. Generally, our method enhances a downstream task performance by using a third dataset with labels different from the downstream task labels. For example, in this work, for speaker emotion recognition, our method normalizes undesired characteristics from the self-supervised representation to improve performance on the speech emotion recognition task. We carry this out by learning a feature representation that excels at speech emotion recognition while being robust enough for speaker characteristics (see Fig.  1 ). Our proposed method outperforms the current state-of-the-art results for both speaker-dependent and speaker-independent settings.\n\nIn summary, we propose a general framework for speaker characteristics normalization from a self-supervised representation. We address the small dataset settings issue and propose a framework for it on the IEMOCAP benchmark. Through extensive experiments, we show that our method outperforms the current speech emotion recognition state-ofthe-art results on several setups.\n\nThe remainder of the work is organized as follows: Sec. 2 provides an overview of related work. Sec. 3 describes our proposed method. Sec. 4 reports the experiments and results of our method. Finally, Sec. 5 concludes the paper.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Self-Supervised Trained Models",
      "text": "The field of deep learning research has significantly benefited from self-supervised learning. In this paradigm, a taskindependent model is pre-trained using large volumes of unlabeled data, and a task-specific model is trained over the self-supervised model. The majority of self-supervision techniques in speech processing fall into three categories. In the first category, different architectures combined with a con- The architecture of our method: In the forward pass, we learn both speaker identification (SID) and emotion tasks. Speaker features enable a good fit to the training dataset. However, they hinder generalization for unknown speakers. To improve generalization for an unknown speaker, we propose disentangling speaker features from the upstream representation by negating the gradients from the downstream model back to the input in the backward pass.\n\nstructive InfoNCE loss are used  [6, 7, 8, 9, 10] . The second category is based on masked token classification  [11] . The third category employs reconstruction loss using various techniques such as generation of future frames  [12, 13]  and the encoder-decoder approach to reconstruct masked parts of the input  [14, 15, 16] .",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Feature Normalization",
      "text": "Deep neural nets excel at fitting with the training set. As a consequence, they tend to heavily leverage superficial correlations between the data features and the labels. While this enhances performance on the training set, it may harm the generalization capabilities of the model. To overcome this, various methods propose to eliminate the model's ability to learn undesirable cues. Nagrani et al.  [17]  suggest using a \"confusion loss,\" which is a cross-entropy loss computed by comparing the prediction to a uniform distribution. Ganin et al.  [18]  use extra knowledge regarding the data-domain to tackle a domain adaptation problem. They propose to normalize domain features by negating gradients of a loss that predict the domain label. In contrast to those methods, we normalize cues based on a task rather than a domain. Additionally, our method focuses on the self-supervised representation framework.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Emotion Recognition",
      "text": "Speech emotion recognition predicts an emotion based on an utterance. The most widely used benchmark for speech emotion recognition is the interactive emotional dyadic motion capture database (IEMOCAP)  [3] . For the purpose of speech emotion recognition, early end-to-end methods combine a convolutional neural network (CNN) and a long short-term memory (LSTM)  [19, 20, 21] . Later, attention-based models outperformed the CNN and LSTM combination due to their ability to focus on specific parts of the input  [22, 23, 24] . In recent years, self-supervised learning models have generated significant interest in speech processing research due to their ability to learn high-quality representations from unlabeled data. Reflecting this, Yang et al.  [25]  demonstrate in their benchmark that self-supervised models produce state-of-theart results in emotion recognition. We present a method for enhancing speech emotion recognition by combining self-supervised models with the normalization of speaker characteristics.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Method",
      "text": "Learning a discriminative model amounts to fit function f w : X → Y from the space of data x ∈ X to the space of labels y ∈ Y using parameters w. The parameters of the function f w (x) are learned by fitting to a training data {(x 1 , y 1 ), ..., (x m , y m )} using a loss function (f w (x i ), y i ).\n\nRecent works suggest an upstream-downstream architecture where a discriminative learner f w is composed of a pretrained upstream learner h w1 : X → R k that maps a data point to an embedding representation in R k . Then, a downstream learner g w2 : R k → Y maps the output of h w1 to the label space Y. This architecture can also be viewed as a composition of h w1 and g w2 , i.e., f w (x) = (g w2 • h w1 )(x).\n\nA discriminative learning algorithm searches for parameters w to best fit the relation of (x i , y i ) in the training data. However, it can sometimes exploit undesired cues. For example, in speech emotion recognition, we seek to encourage the learner to ignore speaker characteristics such as gender or any other speaker-specific features. In the following, we propose an approach for learning a task while normalizing cues from a different task (possibly from another dataset) in an upstreamdownstream architecture.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Speaker Normalization",
      "text": "The upstream-downstream approach allows us to learn more than one task above a single upstream model. An example of this is solving both speaker identification and emotion recognition tasks. In our method, we consider three discriminative learners. The first is an upstream model h w , the second is an emotion recognition learner g wer , and the third is a speaker identification classifier g wid . To prevent the emotion downstream task from leveraging undesired speaker characteristics, we suggest to normalize them from the upstream representation so that the emotion classifier is unable to leverage those cues. We propose doing this by negating the gradients of the upstream model with respect to the speaker identification task. For the sake of simplicity, we describe our method using stochastic gradient descent (SGD), but it can easily be applied to any optimizer.\n\nOur approach consists of two steps. The first step is a standard gradient-based optimization. For example, in standard gradient-based learning using the SGD algorithm, the upstream and emotion downstream weights update step is\n\nwhere η is a learning rate and er is an emotion recognition loss, e.g., cross-entropy loss. In the second step, we normalize speaker id features from the upstream model by\n\nwhere λ is a hyperparameter that sets to which extent we want to normalize speaker features from the upstream model by performing a gradient ascent step on the upstream model, with respect to the speaker id loss id . We illustrate our method in Fig.  1 . It is important to note that the steps are independent, so the data used for emotion recognition does not need to be labeled with speaker identification, and vice versa. Next, we present two approaches for training our method.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Training Strategies",
      "text": "Self-supervised upstream models often have many parameters. For example, in the HuBERT Large model  [11] , there are 317 million parameters, and in HuBERT X-Large  [11] , there are nearly a billion parameters. Thus, fine-tuning such networks can be computationally hard. We propose two training procedures:\n\n1. Speaker normalization projector: We introduce a new non-linear layer with parameters w, which we use as a Method 5-fold SD AUC * MDRE  [26]  -71.8 -DS-LSTM  [27]  -72.7 -MOMA  [28]  -74.8 -WISE  [29]  66.5 --wav2vec2-PT  [30]  67.2 --ACTC  [31]  69.0 --AttPool  [32]  71.7 --HuBERT Base  [11, 25]  68.9 gate between the upstream and the downstream model.\n\nIn the emotion recognition step, we add w to the optimization of the upstream model. However, in the speaker ID task, we modify Eq. 3 to optimize solely w. In both tasks, we do not change the optimization of the downstream models. This allows us to skip the upstream's optimization step, which spares the gradients computation overhead in the speaker ID step.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Train All Parameters:",
      "text": "In this approach, we train the parameters of both upstream and downstream models according to what we describe above.\n\nIn the following section, we discuss both training strategies on speaker-independent and speaker-dependent settings using various training set sizes. We present improvements from state-of-the-art using both training strategies.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Experiments",
      "text": "This section presents our approach and compares it to previous work. First, we describe our experimental setup (see Sec. 4.1). We then show that our approach outperformed the current state-of-the-art results on the IEMOCAP benchmark (see Sec. 4.2). We also present a low-resource evaluation setup and show the efficacy of our method on it (see Sec. 4.3).",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Experimental Setup",
      "text": "We performed our experiments on the IEMOCAP dataset  [3] . According to the traditional evaluation process, we used balanced classes: neutral, happy, sad, and angry. There are ten different speakers in the dataset (five female and five male). For the speaker identification task, we used the VoxCeleb dataset  [1] , which is annotated with 1, 251 speakers and more than 100, 000 utterances. For the upstream model, we used both the HuBERT base and large models  [11] . For the downstream model, we used a non-linear projection from the temporal dimension of HuBERT. The range of hyperparameters we considered for λ is [0.01, 0.0001], although in the end we obtained the best results using λ = 0.001. Accordingly, we used this value of λ in all the experiments. For all experiments, we use a single NVIDIA Tesla V100 GPU.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Emotion Recognition",
      "text": "Recent studies suggest various evaluation setups for emotion recognition. Those setups can be divided into two categories: speaker-independent and speaker-dependent. For speakerindependent, we performed five-fold cross-validation, where utterances from two speakers are used for testing and utterances from eight other speakers are used for training and validation in each fold. The stopping criteria play a crucial role in speaker out-of-distribution evaluation as well. We report the accuracy of the test set based on the best epoch on the validation set.\n\nIn this work, we investigate generalization capabilities to an unknown speaker. Therefore, we focus on the speakerindependent setup. Nevertheless, we show that our method also improves the speaker-dependent setup where the traintest split is random and includes all speakers in both the train and test sets.\n\nIn Table  1 , we present results of our method on both the speaker-independent (five-fold) and speaker-dependent (SD) setups. For speaker-independent, our second training procedure produced a state-of-the-art (SOTA) result, outperforming the current SOTA by 2.3% absolute. We show that the improvement is consistent with both HuBERT Large and Base. Moreover, our method improves the SOTA results for speaker-dependent settings by 0.5% absolute.\n\nWe further evaluated our method's ability to normalize speaker information from an upstream model. We trained a classifier twice on a fixed upstream (i.e., without fine-tuning the upstream model) HuBERT Large for speaker identification. First, we trained a downstream model on HuBERT before our speaker normalization method. We obtained an accuracy of 60.7%. We then trained an additional speaker ID downstream model on a fixed HuBERT that was trained with our proposed method. This resulted in 45.9% accuracy. Thus, as desired, our method harmed the speaker ID features of the upstream model. Fig.  2 : A comparison of our proposed methods with the Hu-BERT baseline for small data sets: We train classifiers on different amounts of training data per label (x-axis). We observe that the normalization of speaker features from the upstream representation using our method enhances the generalization capabilities of HuBERT.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Small Data Settings",
      "text": "A high-standard data collection and annotation is often an expensive process. In the following, we suggest quantitative evaluation settings for small data setup.\n\nTo test speech emotion recognition with few resources, we propose increasing the number of samples per class in the training set. Then, to stabilize the results, we run each step five times with different random splits and compute each step's mean. Finally, to quantify the overall performance of a given method, we compute the area under the curve (AUC) of Fig.  2 . Intuitively, the AUC score reflects an average of the scores for each setup we evaluate. Fig.  2  presents results for our proposed low-resource setup. In each step, we trained a HuBERT Large model with and without our methods. We report the AUC of each method in Table  1 . By using our method, we were able to improve both the Base and Large HuBERT models. In Fig.  2  we note that our method improves HuBERT accuracy in all settings. This can also be observed in the AUC improvement in Table  1 .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Conclusion",
      "text": "In this paper, we presented a framework for speaker characteristics normalization from a self-supervised feature representation. Our approach combines discriminative learning of a task with adversary learning of another task. Furthermore, our method enables to use of different datasets for each task. We tested it on top of various models and achieved strong stateof-the-art results in speech emotion recognition. We also proposed studying low-resource settings using a modified version of IEMOCAP and showed the success of our method on it.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: ). Our proposed",
      "page": 1
    },
    {
      "caption": "Figure 1: The architecture of our method: In the forward pass, we learn both speaker identiﬁcation (SID) and emotion tasks.",
      "page": 2
    },
    {
      "caption": "Figure 1: It is important to note that the steps are independent,",
      "page": 3
    },
    {
      "caption": "Figure 2: A comparison of our proposed methods with the Hu-",
      "page": 4
    },
    {
      "caption": "Figure 2: Intuitively, the AUC score reﬂects an average of the",
      "page": 4
    },
    {
      "caption": "Figure 2: presents results for our proposed low-resource",
      "page": 4
    },
    {
      "caption": "Figure 2: we note that our method improves HuBERT accuracy in all",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table 1: , we present results of our method on both",
      "data": [
        {
          "Column_1": "",
          "Column_2": "",
          "Column_3": "",
          "Column_4": "",
          "Column_5": "",
          "Column_6": "",
          "Column_7": "HuBE",
          "Column_8": "RT",
          "Column_9": "",
          "Column_10": "",
          "Column_11": "",
          "Column_12": ""
        },
        {
          "Column_1": "",
          "Column_2": "",
          "Column_3": "",
          "Column_4": "",
          "Column_5": "",
          "Column_6": "",
          "Column_7": "HuBE",
          "Column_8": "RT +",
          "Column_9": "Ours",
          "Column_10": "(SNP",
          "Column_11": ")",
          "Column_12": ""
        },
        {
          "Column_1": "",
          "Column_2": "",
          "Column_3": "",
          "Column_4": "",
          "Column_5": "",
          "Column_6": "",
          "Column_7": "HuBE",
          "Column_8": "RT +",
          "Column_9": "Ours",
          "Column_10": "(TAP",
          "Column_11": ")",
          "Column_12": ""
        }
      ],
      "page": 4
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Voxceleb: a large-scale speaker identification dataset",
      "authors": [
        "Arsha Nagrani",
        "Son Chung",
        "Andrew Zisserman"
      ],
      "year": "2017",
      "venue": "Voxceleb: a large-scale speaker identification dataset"
    },
    {
      "citation_id": "3",
      "title": "Librispeech: An asr corpus based on public domain audio books",
      "authors": [
        "Vassil Panayotov",
        "Guoguo Chen",
        "Daniel Povey",
        "Sanjeev Khudanpur"
      ],
      "year": "2015",
      "venue": "ICASSP"
    },
    {
      "citation_id": "4",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "Carlos Busso",
        "Murtaza Bulut",
        "Chi-Chun Lee",
        "Abe Kazemzadeh",
        "Emily Mower",
        "Samuel Kim",
        "Jeannette Chang",
        "Sungbok Lee",
        "Shrikanth S Narayanan"
      ],
      "year": "2004",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "5",
      "title": "The ryerson audio-visual database of emotional speech and song (ravdess): A dynamic, multimodal set of facial and vocal expressions in north american english",
      "authors": [
        "Steven Livingstone",
        "Frank Russo"
      ],
      "year": "2018",
      "venue": "PLOS ONE"
    },
    {
      "citation_id": "6",
      "title": "Combining frame and turn-level information for robust recognition of emotions within speech",
      "authors": [
        "Bogdan Vlasenko",
        "Bjorn Schuller",
        "Andreas Wendemuth",
        "Gerhard Rigoll"
      ],
      "year": "2007",
      "venue": "Combining frame and turn-level information for robust recognition of emotions within speech"
    },
    {
      "citation_id": "7",
      "title": "Representation learning with contrastive predictive coding",
      "authors": [
        "Aaron Van Den Oord",
        "Yazhe Li",
        "Oriol Vinyals"
      ],
      "year": "2018",
      "venue": "Representation learning with contrastive predictive coding",
      "arxiv": "arXiv:1807.03748"
    },
    {
      "citation_id": "8",
      "title": "Representation learning with contrastive predictive coding",
      "authors": [
        "Aaron Van Den Oord",
        "Yazhe Li",
        "Oriol Vinyals"
      ],
      "year": "2018",
      "venue": "Representation learning with contrastive predictive coding",
      "arxiv": "arXiv:1807.03748"
    },
    {
      "citation_id": "9",
      "title": "Unsupervised pretraining transfers well across languages",
      "authors": [
        "Morgane Riviere",
        "Armand Joulin",
        "Pierre-Emmanuel Mazaré",
        "Emmanuel Dupoux"
      ],
      "year": "2020",
      "venue": "ICASSP"
    },
    {
      "citation_id": "10",
      "title": "wav2vec: Unsupervised pre-training for speech recognition",
      "authors": [
        "Steffen Schneider",
        "Alexei Baevski",
        "Ronan Collobert",
        "Michael Auli"
      ],
      "year": "2019",
      "venue": "wav2vec: Unsupervised pre-training for speech recognition"
    },
    {
      "citation_id": "11",
      "title": "wav2vec 2.0: A framework for self-supervised learning of speech representations",
      "authors": [
        "Alexei Baevski",
        "Henry Zhou",
        "Abdelrahman Mohamed",
        "Michael Auli"
      ],
      "year": "2020",
      "venue": "NeurIPS"
    },
    {
      "citation_id": "12",
      "title": "Hubert: Self-supervised speech representation learning by masked prediction of hidden units",
      "authors": [
        "Wei-Ning Hsu",
        "Benjamin Bolte",
        "Hubert Yao-Hung",
        "Kushal Tsai",
        "Ruslan Lakhotia",
        "Abdelrahman Salakhutdinov",
        "Mohamed"
      ],
      "year": "2021",
      "venue": "Hubert: Self-supervised speech representation learning by masked prediction of hidden units",
      "arxiv": "arXiv:2106.07447"
    },
    {
      "citation_id": "13",
      "title": "An unsupervised autoregressive model for speech representation learning",
      "authors": [
        "Yu-An Chung",
        "Wei-Ning Hsu",
        "Hao Tang",
        "James Glass"
      ],
      "year": "2019",
      "venue": "An unsupervised autoregressive model for speech representation learning",
      "arxiv": "arXiv:1904.03240"
    },
    {
      "citation_id": "14",
      "title": "Vector-quantized autoregressive predictive coding",
      "authors": [
        "Yu-An Chung",
        "Hao Tang",
        "James Glass"
      ],
      "year": "2020",
      "venue": "Vector-quantized autoregressive predictive coding"
    },
    {
      "citation_id": "15",
      "title": "Mockingjay: Unsupervised speech representation learning with deep bidirectional transformer encoders",
      "authors": [
        "Andy Liu",
        "Shu-Wen Yang",
        "Po-Han Chi",
        "Po-Chun Hsu",
        "Hung-Yi Lee"
      ],
      "year": "2020",
      "venue": "ICASSP"
    },
    {
      "citation_id": "16",
      "title": "Tera: Selfsupervised learning of transformer encoder representation for speech",
      "authors": [
        "Andy Liu",
        "Shang-Wen Li",
        "Hung-Yi Lee"
      ],
      "venue": "TASLP"
    },
    {
      "citation_id": "17",
      "title": "Nonautoregressive predictive coding for learning speech representations from local dependencies",
      "authors": [
        "Yu-An Alexander H Liu",
        "James Chung",
        "Glass"
      ],
      "year": "2020",
      "venue": "Nonautoregressive predictive coding for learning speech representations from local dependencies",
      "arxiv": "arXiv:2011.00406"
    },
    {
      "citation_id": "18",
      "title": "Disentangled speech embeddings using crossmodal self-supervision",
      "authors": [
        "Arsha Nagrani",
        "Son Joon",
        "Samuel Chung",
        "Andrew Albanie",
        "Zisserman"
      ],
      "year": "2020",
      "venue": "ICASSP"
    },
    {
      "citation_id": "19",
      "title": "Unsupervised domain adaptation by backpropagation",
      "authors": [
        "Yaroslav Ganin",
        "Victor Lempitsky"
      ],
      "year": "2015",
      "venue": "ICML"
    },
    {
      "citation_id": "20",
      "title": "Convolutional rnn: An enhanced model for extracting features from sequential data",
      "authors": [
        "Gil Keren",
        "Björn Schuller"
      ],
      "year": "2016",
      "venue": "IJCNN"
    },
    {
      "citation_id": "21",
      "title": "Automatic speech emotion recognition using recurrent neural networks with local attention",
      "authors": [
        "Seyedmahdad Mirsamadi",
        "Emad Barsoum",
        "Cha Zhang"
      ],
      "year": "2017",
      "venue": "ICASSP"
    },
    {
      "citation_id": "22",
      "title": "Adieu features? end-to-end speech emotion recognition using a deep convolutional recurrent network",
      "authors": [
        "F George Trigeorgis",
        "R Ringeval",
        "E Brueckner",
        "Mihalis Marchi",
        "Björn Nicolaou",
        "S Schuller",
        "Zafeiriou"
      ],
      "year": "2016",
      "venue": "ICASSP"
    },
    {
      "citation_id": "23",
      "title": "Speaker attentive speech emotion recognition",
      "authors": [
        "Clément Le Moine",
        "Nicolas Obin",
        "Axel Roebel"
      ],
      "venue": "Speaker attentive speech emotion recognition"
    },
    {
      "citation_id": "24",
      "title": "3-d convolutional recurrent neural networks with attention model for speech emotion recognition",
      "authors": [
        "Mingyi Chen",
        "Xuanji He",
        "Jing Yang",
        "Han Zhang"
      ],
      "year": "2018",
      "venue": "IEEE Signal Processing Letters"
    },
    {
      "citation_id": "25",
      "title": "Improved end-to-end speech emotion recognition using self attention mechanism and multitask learning",
      "authors": [
        "Yuanchao Li",
        "Tianyu Zhao",
        "Tatsuya Kawahara"
      ],
      "year": "2019",
      "venue": "Improved end-to-end speech emotion recognition using self attention mechanism and multitask learning"
    },
    {
      "citation_id": "26",
      "title": "Superb: Speech processing universal performance benchmark",
      "authors": [
        "Shu-Wen Yang",
        "Po-Han Chi",
        "Yung-Sung Chuang",
        "Cheng-I Jeff Lai",
        "Kushal Lakhotia",
        "Andy Yist Y Lin",
        "Jiatong Liu",
        "Xuankai Shi",
        "Guan-Ting Chang",
        "Lin"
      ],
      "year": "2021",
      "venue": "Superb: Speech processing universal performance benchmark",
      "arxiv": "arXiv:2105.01051"
    },
    {
      "citation_id": "27",
      "title": "Multimodal speech emotion recognition using audio and text",
      "authors": [
        "Seunghyun Yoon",
        "Seokhyun Byun",
        "Kyomin Jung"
      ],
      "year": "2018",
      "venue": "SLT"
    },
    {
      "citation_id": "28",
      "title": "Speech emotion recognition with dual-sequence lstm architecture",
      "authors": [
        "Jianyou Wang",
        "Michael Xue",
        "Ryan Culhane",
        "Enmao Diao",
        "Jie Ding",
        "Vahid Tarokh"
      ],
      "year": "2020",
      "venue": "ICASSP"
    },
    {
      "citation_id": "29",
      "title": "Removing bias with residual mixture of multi-view attention for speech emotion recognition",
      "authors": [
        "Asif Jalal",
        "Rosanna Milner",
        "Thomas Hain",
        "Roger Moore"
      ],
      "year": "2020",
      "venue": "Removing bias with residual mixture of multi-view attention for speech emotion recognition"
    },
    {
      "citation_id": "30",
      "title": "WISE: Word-Level Interaction-Based Multimodal Fusion for Speech Emotion Recognition",
      "authors": [
        "Guang Shen",
        "Riwei Lai",
        "Rui Chen",
        "Yu Zhang",
        "Kejia Zhang",
        "Qilong Han",
        "Hongtao Song"
      ],
      "venue": "WISE: Word-Level Interaction-Based Multimodal Fusion for Speech Emotion Recognition"
    },
    {
      "citation_id": "31",
      "title": "Emotion Recognition from Speech Using wav2vec 2.0 Embeddings",
      "authors": [
        "Leonardo Pepino",
        "Pablo Riera",
        "Luciana Ferrer"
      ],
      "venue": "Emotion Recognition from Speech Using wav2vec 2.0 Embeddings"
    },
    {
      "citation_id": "32",
      "title": "Attention-Enhanced Connectionist Temporal Classification for Discrete Speech Emotion Recognition",
      "authors": [
        "Ziping Zhao",
        "Zhongtian Bao",
        "Zixing Zhang",
        "Nicholas Cummins",
        "Haishuai Wang",
        "Björn Schuller"
      ],
      "year": "2019",
      "venue": "Attention-Enhanced Connectionist Temporal Classification for Discrete Speech Emotion Recognition"
    },
    {
      "citation_id": "33",
      "title": "An Attention Pooling Based Representation Learning Method for Speech Emotion Recognition",
      "authors": [
        "Pengcheng Li",
        "Yan Song",
        "Ian Mcloughlin",
        "Wu Guo",
        "Lirong Dai"
      ],
      "year": "2018",
      "venue": "An Attention Pooling Based Representation Learning Method for Speech Emotion Recognition"
    }
  ]
}