{
  "paper_id": "2306.08657v1",
  "title": "Emersk -Explainable Multimodal Emotion Recognition With Situational Knowledge",
  "published": "2023-06-14T17:52:37Z",
  "authors": [
    "Mijanur Palash",
    "Bharat Bhargava"
  ],
  "keywords": [
    "Emotion Recognition",
    "Deep Learning",
    "Multimodal",
    "Convolutional neural network (CNN)",
    "LSTM"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Automatic emotion recognition has recently gained significant attention due to the growing popularity of deep learning algorithms. One of the primary challenges in emotion recognition is effectively utilizing the various cues (modalities) available in the data. Another challenge is providing a proper explanation of the outcome of the learning. To address these challenges, we present Explainable Multimodal Emotion Recognition with Situational Knowledge (EMERSK), a generalized and modular system for human emotion recognition and explanation using visual information. Our system can handle multiple modalities, including facial expressions, posture, and gait, in a flexible and modular manner. The network consists of different modules that can be added or removed depending on the available data. We utilize a two-stream network architecture with convolutional neural networks (CNNs) and encoder-decoder style attention mechanisms to extract deep features from face images. Similarly, CNNs and recurrent neural networks (RNNs) with Long Shortterm Memory (LSTM) are employed to extract features from posture and gait data. We also incorporate deep features from the background as contextual information for the learning process. The deep features from each module are fused using an early fusion network. Furthermore, we leverage situational knowledge derived from the location type and adjective-noun pair (ANP) extracted from the scene, as well as the spatio-temporal average distribution of emotions, to generate explanations. Ablation studies demonstrate that each sub-network can independently perform emotion recognition, and combining them in a multimodal approach significantly improves overall recognition performance. Extensive experiments conducted on various benchmark datasets, including GroupWalk, validate the superior performance of our approach compared to other state-of-the-art methods.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "E MOTION shapes our social life by influencing our com- munications with others. Therefore, automatic human emotion recognition (ER) holds significant potential in various aspects of our lives. In the current era of online learning, which has become prevalent due to the Covid-19 pandemic, an integrated ER system can help teachers maintain an effective learning environment by providing insights into the emotional state of students. Similarly, a car equipped with driver emotion recognition capability can proactively prevent road rage or accidents by alerting the driver when they are tired, frustrated, or angry. Furthermore, the implementation of an ER system in CCTV cameras can enable the detection of individuals displaying anger near sensitive locations such as schools or Moreover, we perceive other people's emotions using both visual and non-visual cues. Visual cues include facial expressions, posture, gestures, eye movement, and walking gait, to name a few. Non-visual cues encompass speech, text, brain signals, and EEG signals, among others. Working with visual cues is more common than working with non-visual cues, as visual cues are more easily obtainable. Facial expressions and postures can be observed directly by looking at a person or analyzing images or videos captured by regular cellphones, CCTV cameras, or similar devices. However, the same convenience does not apply to non-visual cues. For instance, obtaining a brain scan requires specialized instruments to be attached to the individual, and obtaining permission for such procedures can be challenging. Moreover, the knowledge of them being recorded can potentially influence the subject's emotional state. Therefore, this work primarily focuses on visual cues, specifically facial expressions, postures, and gaits, for the purpose of emotion recognition.\n\nMany of the existing works use only one type of cue (unimodal) such as facial expression  [1] ,  [2]  or gait  [3]  etc. However, solely depending on a single mode can make the arXiv:2306.08657v1 [cs.CV] 14 Jun 2023 model less reliable in wild deployment. For example, a facial expression model trained on many of the existing benchmark datasets with no masked sample will perform poorly when encountering a subject wearing mask, which is very common during the Covid-19 pandemic. Similarly, a person's body may be blocked from the camera view by an obstacle and we may not have posture or gait information. So considering multiple modes at the same time can make the model more reliable. Moreover, several prior works show that combining multiple cues (multimodal) can results in higher accuracy in automatic emotion recognition  [4] ,  [5] . A person with a smiley face (mode 1) is possibly happy, but if we know they have open arms and stand straight (mode 2) we can be much more confident in our deduction.\n\nLikewise, it is well accepted that situation around a person plays a non-trivial role in shaping their general feelings  [6] . Deciding emotional state may prove inadequate or wrong if we do not consider the situation around the subject. For example, in the cropped photo at figure  1(a) , the subject appears as an angry person. But the background in the uncropped photo in figure  1 (b) gives us more information about the situation. We see the England national team fans celebrating wildly when the team qualifies for the FIFA World Cup 2018 semifinals after 28 years, and they are extremely happy and excited. We see people around the subject laughing and relaxed which would be different around an angry person. Thus the background and the emotional status of other people around the subject also give us valuable clues. Moreover, people in a particular area at a particular time tend to have similar feelings as a group. We do not normally see people very happy at a funeral or entryway to a hospital. When there is a goal in a soccer match, all the supporters of that team would be happy together while the opposition would be unhappy. One recent research shows that the students experienced a twoday lift in their mood when the university's football team was victorious  [7] . Consequently, some of recent works showed improved performance by considering background contextual information  [5] ,  [8] ,  [9] .\n\nExplainable artificial intelligence  [10]  deals with the transparency of AI-based systems. The lack of interpretability in black-box-style deep learning models has been known to create trust and reliability issues, necessitating a greater emphasis on explainability. Various methods have been proposed to generate explanations of deep learning models, including Saliency Map  [11] , Grad-CAM  [12] , SHAP  [13] , and LIME  [14] . To this end, in  [15] , the authors proposed an explainable pipeline utilizing facial action units and an agnostic LIME model to visualize the active regions of the face. Additionally, in  [16] , the authors employed Saliency Map and Grad-CAM methods to generate an explainable model for driver's facial emotion recognition. This growing body of research highlights the importance of explainable AI in ensuring the reliability and trustworthiness of AI-based systems.\n\nIn this work, we introduce EMERSK, an novel and modular system of emotion recognition that takes into account situational knowledge to generate explainable results. By leveraging multi-modal data and context from the background, EMERSK is able to classify emotions with greater accu-racy. Additionally, the system creates situational knowledge by utilizing place categories, Adjective-Noun Pairs (ANP), and average emotion scores, which serves to enhance the interpretability of the results. Overall, the main contributions of this work are:\n\n• We propose a modular architecture for emotion recognition from multiple cues, where each mode works independently on one cue and can be easily added or removed. • We offer a working prototype which uses face, posture and gait as cues and shows improved performance in comparison with prior works. • We investigate the trade-off between energy cost and performance gain in multimodal emotion recognition. • We introduce a new approach for constructing situational knowledge using individual modes classification, place type, Adjective-Noun Pairs (ANPs) extracted from the scene, and average emotion score. • We present a novel method for explainable emotion recognition by leveraging the generated situational knowledge. The remainder of the article is structured as follows: In section II we discuss related works. Next, we describe our proposed architecture in section III. Then we report our experimental results in section IV. Finally, concluding remarks and future works are discussed in section V.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Related Work",
      "text": "Unimodal works deal with single data modalities. Among different modalities used in the literature, the facial expression is the most widely used one. Gan et al.  [17]  achieved improved accuracy on FER-2013 data set using ensemble CNN and a label perturbation strategy. In  [18] , authors used the ensemble method and transfer learning with VGG16 and RESENT-50 to overcome the limitations of the CNN-only methods. In  [19] -  [26]  authors proposed various methods to identify the emotion based on individuals walking face, posture and gait using different deep learning techniques including CNN, encoder-decoder network, self-attention, recurrent neural network (RNN) and long short-term memory (LSTM) etc.\n\nApart from the uni-modal works, several researchers combined multiple modalities for automatic emotion recognition. In  [27] , authors combined multiple visual and audio features for automatic emotion recognition using kernel and Support Vector Machines (SVM) methods. In  [28] , authors used the face and upper body gestures for multi-modal emotion recognition. In  [29] , authors combined facial, vocal features, and body movements (posture and gesture) to discriminate 14 emotion categories. In  [30] , authors used facial features, speech contours, and gestures. Similarly, in  [31] -  [36]  authors proposed various methods to identify the emotion based on multimodal data.\n\nRecently some researchers proposed emotion recognition systems with context. In  [5] , authors leveraged psychological interpretation of context and used depth-based CNN to model the socio-dynamic interactions between the agents. In  [8] , authors used two-stream architecture where one stream takes only the visible part of the body of the subject, and the other takes the whole image as input. In  [9] , they followed similar architecture with one stream focusing on the face of the subject while the other takes the whole image without the face to generate the context. In  [37] , authors combined cues from face, text and speech with multiplicative fusion.\n\nGroup emotion recognition deals with the emotion analysis of a group. Researchers in  [38] -  [40]  showed the interaction between individual and group emotions and proposed several methods to determine the group emotion from individual emotion using face, body and other cues.\n\nThe Places Database  [41]  is a collection of 10 million labeled scene photographs from the environment types encountered in the world. It also provides pre-trained scene classification CNNs (Places-CNNs) which can be used for scene category and attribute identification. In  [42] , the authors constructed a Visual Sentiment Ontology (VSO) database consisting of more than 3,000 Adjective Noun Pairs (ANPs). They also propose SentiBank, a visual concept detector library that can be used to detect the presence of various ANPs in any image.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Iii. Proposed Method: Emersk",
      "text": "In this section we discuss different parts of our proposed modular architecture. From the input data x ∈ (I, V ), where I represents image type and V represents video type data we collects available data modes m ∈ (m 1 , m 2 , ..., m n ). The possible modes can be the facial expression, posture, gait, background etc. as discussed earlier. However other modes such as voice or text can be used similarly. Each mode m i is processed by corresponding module M i . During training time each module is trained to classify emotion from that particular data mode only. However, in the multimodal situation, we simultaneously classify emotions and also collect deep feature representation f i for that mode.\n\nAll the features from each module are fused to generate the final feature set F ∈ (f 1 , f 2 , ..., f n ). This final feature set F is passed through a classification network to classify them into k groups of emotions. Similarly, individual module feature sets (f i ) are also passed through separate classification networks to generate emotion classification (O i ) based on that module only. The resulting final emotion from the multimodal operation can be compared with the individual modular results (O i ) for explanation generation (section IV-H) or anomaly detection (section IV-I).\n\nThis variable modular architecture provides flexibility. For example, to detect the emotion of students in a Zoom class, we can get the face, upper-body posture, voice etc. but not gait. Similarly, a system which monitors the emotions of the crowd using CCTV footage would not get voice or brain scan information. Moreover, in both of these situations, where we are working with live videos, we would have the subject's background and situational knowledge. Our target in this work is to provide a general architecture of emotion recognition which can work with most types of data for different application scenarios. With a flexible modular architecture proposed here, it is possible further fine-tune the model based on the available data modules and specific task requirements.\n\nFor the remaining part of this work, we would only use face, posture, gait, background and situational knowledge as our data modes to demonstrate the benefit of this modular architecture. These particular modes are selected due to the ease of training as they normally co-exist and are available in many datasets. Also, most emotion recognition tasks such as classroom or crowd situations mentioned above most likely  would have data in these modes.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "A. Module 1: Face",
      "text": "The face module deals with the face area of the input data. It performs face detection, face deep feature generation and classification of emotion from the face data.\n\n1) Face Detection and Cropping: For face detection, we leveraged the technique outlined in the works of Bazarevsky et el.  [43] . It detects the face area using 6 landmarks on the face. It also supports the detection of multiple faces from an image. Blazeface uses a very compact feature extractor convolutional neural network designed specifically for lightweight but accurate face detection.\n\n2) Facial Feature Extraction: We employed a two-stream network for face feature extraction, as depicted in Figure  3 . In the lower stream, a convolutional neural network (CNN) is used as a feature extractor. Each input image is treated as a tensor of size C × W × H, where C, W , and H represent the number of channels, width, and height, respectively. The 2D convolution and polling kernels can be represented as a tensor of size k ×k. We employed a network with 3 convolutions and 3 max-pooling layers using a convolution kernel of dimensions 3 × 3 and a pooling kernel of 2 × 2 with stride 1 and necessary padding.\n\nThe upper stream is an encoder-decoder style attention module. We constructed our encoder-decoder network by following the methods used in  [44] -  [47]  in image segmentation. The encoder part comprises of four 3 × 3 convolution layers with ReLU and 2 × 2 max-pooling layers. The decoder part utilizes layers of 2 × 2 up-sampling, de-convolution  [46] , and ReLU. The corresponding layers of the encoder and decoder are joined with copy and concatenation links. The up-sampling layer utilizes memorized max-pooling indices from the corresponding encoder and generates a sparse feature map. The deconvolution layers enlarges and densifies the sparse feature map through convolution-like operations with trainable decoder filter banks. This encoder-decoder based attention mechanism has demonstrated improved performance in recent works in facial expression recognition such as  [26] . The encoding segment converts the input data into a lower-dimensional feature space and the decoding segment reconstructs the original input from the encoded features. The attention mechanism allows the decoder to selectively attend to certain parts of the input image. During our experiments, we observed that the two stream architecture outperforms the architectures based solely on CNN.\n\nThe resulting output from the two streams was multiplied and passed through another two convolution and maxpooling layers. One fully connected (FC) layer was added after the convolutional layers. For multimodal operation, we collected the face deep feature set (F P ) from this FC layer. However, for stand-alone operation, two additional FC layers were used for training, and cross-entropy loss was utilized as the training loss function.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "B. Module 2: Posture",
      "text": "The term 'posture' refers to our physical form, the way we hold ourselves during activities like sitting, standing or sleeping etc. Posture provides a plethora of information about our feelings and emotions. The posture module is responsible for emotion recognition from the posture data. The architecture of posture module is shown in figure  5 .\n\n1) Posture Detection: We need a proper model of the human body to identify posture correctly. In this work, we use the kinematic approach to human body representation. Our model is 2D with 20 different body points as shown in figure  4 . We use BlazePose  [43]  to find these 20 body points. BlazePose provides human pose tracking using machine learning (ML) and offers 2D landmarks of a body from a single RGB frame. For each video frame, BlazePose provides the location of the body points in the frame. This information then can be used to generate more data regarding the pose.  2) Pose Visual Feature Set: Following the works of the authors in  [3]  and  [48] , we extract 24 features from the locations of the body points. These features f 0 , f 1 , . . . , f 23 are listed in the table I. They are in the form of volume, angle, distance and area. In  [48] , they only used upper body features such as the area of the triangle between hands and neck, distances between hand, shoulder, and hip joints, and angles at neck and back. However, they overlooked the lower body such as foot joints, which also can convey important information, particularly while walking. In  [3] , they added areas, distances, and angles of the feet joints in the posture feature formulation. Finally a posture visual feature vector V ∈ f 24 is created using these features f for each subject in a frame.\n\n3) Deep Feature Generation: We also generate a deep feature set D ∈ f 100 d from the RGB input image beside the calculated visible feature set V ∈ f 24 . From our experimental results, we found that adding deep features with visible features resulted in increased accuracy due to CNN's ability to capture subtle points not captured by the visible feature. To generate these deep features f d1 , f d2 , . . . , f d99 we first extract the rest of the body from the RGB image and also remove the face from it. For deep feature extraction, we use a CNN as shown in figure  5 . For each input image, we collect the weights w 1 , w 2 , . . . , w 99 at the output of the first fully connected layer.\n\n4) Final Posture Feature Set: Final posture feature set F P is created by concatenating the visible (V ) and the deep (D) feature sets (F P = [V, D]). All the feature values are normalized to be bounded between 0 and 1 (0 ≤ f ≤ 1). This feature set F P is used for multi-modal emotion recognition. However, for the stand-alone emotion classification task from",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "C. Module 3: Gait",
      "text": "Gait is defined as the way a person walks. Walking is a complex movement which requires balance and coordination of multiple body parts such as the brain, bones, muscles, heart, lungs etc. Our gait conveys meaningful information regarding our emotions. Our two streams gait module architecture is shown in figure  6 .\n\n1) Gait Deep Features: We take inspiration from  [49]  and build a 3D CNN architecture for learning gait deep features from a video segment. A 3D CNN can capture temporal features as well. Each input video segment has 16 frames. We can represent the video segment as a tensor of size C × W × H × N , where C is the number of channels, W and H are width and height respectively and N is the number of frames in the segment. The 3D convolution and polling kernel can be similarly represented as a tensor of size d×k×k, where d is kernel depth and k is the spatial size. We used a network with 8 convolutions, 5 max-pooling and 2 fully connected layers as shown in the figure  6 . We use convolution kernel of dimension 3 × 3 × 3 and pooling kernel of 2 × 2 × 2 (except the first one which has pooling kernel size 1×2×2) with stride 1 and necessary padding in both spatial and temporal domain. RGB frames with the subject only while face and background removed are provided to the network.\n\nWe also use Long Short Term Memory (LSTM) network following the work of  [50] . This network can model longterm temporal dynamics as well as learn deep representations. We build an LSTM network by stacking 4 LSTM layers as shown in the figure. Same as the 3D CNN part, the RGB frame sequence with the body (background and face removed) is fed",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "D. Background Context",
      "text": "The architecture of background contextual feature extraction module is shown in figure  7 . Contextual information from the scene background helps to make more accurate recognition. To generate background contextual information we first remove the subject face and body from the image. The image is then passed through a CNN based feature extractor network to generate the deep feature set F B .",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "E. Final Feature Set",
      "text": "Final features set (F ) is the collection of features from each modules. We concatenate the output feature sets from face (F F ), posture (F P ), gait (F g ) and background (F B ) to get the final feature set F .\n\nThis final feature set F is passed through two FC layers for multimodal classification. During training, we use crossentropy as the loss function.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "F. Explainable Situational Knowledge Generation",
      "text": "1) Place and ANP: It has been shown in the literature that the distribution of emotion varies significantly among different place categories. For example, people in a 'ski slope' frequently appear happy while people in 'working environments' appear sadder. Similarly, ANPs such as 'young couple' is associated with happiness. This place and ANP information are used to generate an explanation for the result. We use pre-trained AlexNet provided with the Places dataset  [41]  which gives place categories such as 'classroom' and attributes such as 'no horizon' and 'enclosed area' etc. We also use Sentibank Adjective-Noun Pair (ANP) detectors  [42]  which give ANPs like 'young couple' and 'outdoor wedding' from images.\n\n2) Average Emotion: Average emotion is collected in a group setting where we have multiple subjects in the same frame or frames from the same geographical area (spatial), bounded by a threshold distance t l and, within a short time interval (temporal), bounded by a threshold time difference t d . For example, timestamped videos from multiple CCTV cameras around a complex can be used for each other. For each time interval T we collect numbers of emotions recognised from each emotion category N e where e ∈ (happy, sad, neutral, angry). We maintain a 2-D list of average emotion scores where each row is time and each column represents one emotion category e. Value of each list entry is calculated by\n\nN e e (N e ) Together with place attributes and ANPs, average emotion can give us more explanation for the emotion.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Iv. Experimental Results",
      "text": "In this section, we will be discussing our experimental results. We will first analyze the performance of each modality separately, and then we will present the overall multimodal performance. To avoid slowing down the multimodal operation, we refrained from using complex architecture for individual modules. Consequently, the stand-alone operation of individual modules may not always outperform the best results reported in the literature generated from complex models. Our aim is to achieve improved performance with simpler modules operating independently while achieving a greater improvement in the multimodal setting.\n\nAll experiments were conducted on a server PC with a 20-core 2.6 GHz CPU, 96 GB of available memory, and three GPU cards, each with 24.5 GB memory. To speed up computation, we used Pytorch multiprocessing and automatic mixed precision libraries. In cases, where no working implementation of a particular method is available, we used our best effort implementation of that method. We also selected datasets which are frequently used by recent works to allow for a direct comparison of our results with them.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "A. Datasets",
      "text": "In the present body of literature, a plethora of datasets have been proposed for the task of emotion recognition. Table  II  presents a comprehensive summary of the datasets employed in our work, highlighting their unique characteristics.  For instance, FER-2013  [1]  solely consists of facial images, without any accompanying information on posture, gait, or situational context. The Emotic  [8]  dataset, on the other hand, contains images with the face, upper body, and background. The CAER and CAER-S  [51]  datasets encompass video clips and images from TV shows, primarily focusing on upper body and facial images with background. The FABO  [28]  dataset comprises acted emotion videos depicting the face and upper body against a green screen. The EWalk  [3]  dataset is composed of acted videos featuring walking individuals with gaits and annotated emotions. Lastly, the GroupWalk  [5]  dataset contains 45 videos of real-world scenarios, encompassing face, posture, and gait information. This dataset also provides situational intelligence in the form of background and other people's emotions.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "B. Face Emotion Recognition",
      "text": "We leveraged the images from the datasets enlisted in Table  II  to train our facial expression recognition module. Subsequently, we evaluated its efficacy in recognizing emotions from facial expressions by benchmarking its accuracy against various state-of-the-art approaches proposed in the recent literature. The test accuracy of our method is computed as follows:\n\nWhere, #N c indicates the number of items correctly predicted and #N t indicates the total number of items in the test dataset.\n\nOur experimental results demonstrate that our proposed method achieves a test accuracy of 78.62% on the FER-2013 dataset. Our approach exhibits superior performance in comparison to several other contemporary works for facial emotion recognition, as evidenced by the results presented in Table  III . The closest performance in terms of accuracy was achieved by AMP-Net  [21] . Our approach outperforms them by a margin of 4.14%.\n\nFigure  8  shows the confusion matrices of our model for the FER-2013 dataset. From the confusion matrix of FER-2013, we can see some classes such as 'Happiness' and 'Surprise' are better recognizable while 'Disgust' and 'Fear' classes are not.\n\nIn addition, we also compare the result of our method with several recent works on the CAER-S dataset, as tabulated in Table  IV . Notably, our approach outperforms the closest result  reported by Li. et al.  [25]  by a margin of 7.58%. A summary of emotion recognition performance on various benchmark datasets using EMERSK face module is shown on Table V",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "C. Posture Emotion Recognition",
      "text": "For training and evaluating our posture module, we employed the FABO dataset, and achieved a test accuracy of 78.1%. Additionally, we investigated bimodal emotion recognition by combining facial expression and posture information, following prior research. The results of this experiment are presented in Table  VI . Notably, while our posture module alone may not be as effective as some of the more complex   approaches, our bimodal method surpasses them in performance. Specifically, our bimodal method outperforms the closest reported result by CMEFA  [31]  by a margin of 3.15%. We further evaluated our posture module in two additional benchmark datasets, namely Emotic and CAER-S. The results of the experiments on these datasets are shown in the table VII. Our method achieved 80.15% average test accuracy on these two datasets.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "D. Gait Emotion Recognition",
      "text": "We conducted an evaluation of our gait-based emotion recognition modules using the EWalk dataset. Our model achieved a test accuracy of 80.01% on this dataset. In Table VIII, we present the results from our study alongside those of other recent works. Our approach exhibits improved performance in comparison to several other contemporary works for gait emotion recognition and closely matches the best performing works of Tanmay et al.  [3] .",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "E. Multimodal Emotion Recognition Results",
      "text": "In this experiment, we tested overall performance of proposed EMERSK system when all the modalities work together. For this, we use the GroupWalk dataset which has all the required modalities we need. Additionally, for better comparison with other state of the art works, we use the mean     average precision (mAP) score as the performance metric in this dataset. mAP is a widely used evaluation metric which is a measures of the quality of the algorithm. Average precision (AP) is the area under the precision-recall curve, which measures how well the algorithm retrieves relevant instances. The mAP is calculated as the average of the AP values for all the classes in the dataset.\n\nOur model achieved an mAP of 76.3 on this dataset. From the table IX it is clear that our model outperforms all other works in this dataset. This is due to our usage of all the three available modalities (face, posture and gait) with background context effectively. From the results, we can see our work outpeforms the closest performing work of Mittal et al.  [5]  by an mAP value of 8.68.\n\nIn additional study, we evaluated the efficacy of our multimodal approach using GEMEP  [54] , another benchmark dataset. The GEneva Multimodal Emotion Portrayals (GEMEP) has video recordings of actors portraying various emotional states. It has annotated face and full body expressions of 18 emotions from 10 subjects. When we conducted experiments using a leave-one-subject-out (LOSO) approach, which ensures the same subject does not appear in both the training and test datasets, we achieved an accuracy of 79.4%.\n\nFurthermore, we compared our model with two recent studies by Santos et al.  [47]  and Tahghighi et al.  [24] . For this comparison, we adopted their methodology, allowing the same subjects to appear in both the training and test data. It's worth noting that this differs from the previously mentioned leave-one-subject-out (LOSO) approach. Our model achieved an accuracy of 99.02%, which outperforms the closest reported result by Tahghighi et al.  [24]  by a margin of 0.22%. The results of our experiments, along with those from the two aforementioned studies, are presented in Table  X .\n\nThe first approach yielded lower accuracy than the second one. This is largely because in the Leave-One-Subject-Out (LOSO) method, the model cannot memorize specific instances or characteristics of individual subjects. Instead, it must depend on learning patterns and features that can be generalized across different subjects. Moreover, it's required to perform effectively on subjects it hasn't encountered during Author Year Accuracy (%) Santosh et al.  [47]  2020 95.9   training.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "F. Ablation Experiment",
      "text": "We analyze the accuracy of each module of EMERSK in a stand-alone manner, and in various combinations with other modules. Tests are performed with the datasets listed in table II. The resulting outputs from these experiments are shown in table XI. We can see the face is most the effective source of information for emotion recognition. Adding background context improves accuracy but its effect reduces when more modes are used as multiple modes already give a good picture of the subject's emotion. Maximum accuracy is achieved when we use all three modes and contexts. to achieve maximum accuracy we should always strive for using all the available data modes.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "G. Computational Cost",
      "text": "We measure the average processing time it takes to identify emotion using each module individually and in a multimodal manner. For better results, we run each of them separately to identify emotions on the laptop computer without any GPU acceleration. We see facial emotion recognition is the fastest while gait recognition is the slowest among the three modalities.\n\nFrom table XII it is apparent that posture and gait take more processing time compared to the face module. However, in real-life situations such as a CCTV feed, a clear face may not be always available, and therefore, it is necessary to make use of these other modalities. Adding these modalities not only increases the accuracy but also the reliability of the system albeit increased computational cost. The trade-off can be decided based on specific system requirements.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "H. Situational Knowledge And Explanation Generation",
      "text": "Besides determining emotion we also provide a guideline to generate an explanation for that result. Our classification module generates emotion classification while our situational knowledge and explanation generation module provides human explainable reasoning by creating an idea of the situation around the subject. Individual modules tell us what information is available from the face, posture and gait cues. For instance, the subject in figure  9  red bounding box has a smiling face and open posture which indicates happiness. By extracting location type, location attributes and ANP information we create our situational knowledge which further enhances this reasoning. In the case of figure  9 , place category output is a classroom and the top three ANPs are early childhood, innoncent smile and creative kids. By combining all these a human understandable explanation can be constructed as \"the subject is a child in a classroom doing creative work and smiling, has a happy facial expression and a body posture showing happiness\".\n\nUsing emotion obtained from other subjects in yellow boxes in the photo and calculating the average emotion score described in section III-F2 we find the happiness score is highest (0.80) in the leading position followed by neutral (0.2). So we can conclude this is a happy environment which bolsters our model's output of the kid in the red box to be happy. This is the first effort for explainable emotion classification using multiple data modalities as per our knowledge. However, our explanation generation still requires further work. Therefore, we are planning to explore explanation generation in a more detailed manner in our future works.",
      "page_start": 9,
      "page_end": 10
    },
    {
      "section_name": "I. Anomaly Detection",
      "text": "Anomaly detection is a possible use for multimodal recognition. An anomaly can be defined as a situation when a person intentionally tries to fake emotion to evade the system. Trying to fake an emotion most of the time results in a mismatch among the modalities as changing emotional expressions in face, posture and gesture require significant effort and skill. The subject may be smiling while his body does not reflect happiness as such we are likely to get different results in the individual modules. If the modules differ beyond a certain threshold (t a ) in their output we tag that as an anomaly candidate. What to do with these anomaly data samples is a research question. Possible action can be further verification by a human in the loop and adaption of the model in a continuous learning fashion. However, we keep this part as future work.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "V. Conclusion And Future Works",
      "text": "In this paper, we present EMERSK, a modular system for emotion recognition from multimodal data. EMERSK uses state of the art deep learning techniques to extract deep features from facial expression, posture, gait and background for emotion recognition. Moreover, it constructs situational knowledge from the place categories, ANPs and average emotion scores. We show how this situational knowledge can be useful to generate an explanation for the classification output. From the results of different experiments in several benchmark datasets, we report that EMERSK has improved emotion recognition performance. In future, we plan to explore other modalities besides those used in this work. We also plan to automate the explanation generation process. Finally, we want to further explore anomaly detection and novelty in emotion recognition.",
      "page_start": 10,
      "page_end": 10
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Importance of context in emotion recognition. (a) With-",
      "page": 1
    },
    {
      "caption": "Figure 1: (a), the subject appears as an",
      "page": 2
    },
    {
      "caption": "Figure 1: (b) gives us more information about the situation. We",
      "page": 2
    },
    {
      "caption": "Figure 2: Proposed modular system architecture. M represents the data processing and feature generation module, F indicates",
      "page": 3
    },
    {
      "caption": "Figure 3: EMERSK face module.",
      "page": 4
    },
    {
      "caption": "Figure 3: In the lower stream, a convolutional neural network (CNN) is",
      "page": 4
    },
    {
      "caption": "Figure 5: 1) Posture Detection: We need a proper model of the",
      "page": 4
    },
    {
      "caption": "Figure 4: We use BlazePose [43] to find these 20 body points. BlazePose",
      "page": 4
    },
    {
      "caption": "Figure 4: Kinematic representation of a human body.",
      "page": 5
    },
    {
      "caption": "Figure 5: EMERSK posture module.",
      "page": 5
    },
    {
      "caption": "Figure 5: For each input image, we collect the weights",
      "page": 5
    },
    {
      "caption": "Figure 6: 1) Gait Deep Features: We take inspiration from [49] and",
      "page": 5
    },
    {
      "caption": "Figure 6: We use convolution kernel",
      "page": 5
    },
    {
      "caption": "Figure 6: EMERSK gait module.",
      "page": 6
    },
    {
      "caption": "Figure 7: EMERSK background context module.",
      "page": 6
    },
    {
      "caption": "Figure 7: Contextual information from the",
      "page": 6
    },
    {
      "caption": "Figure 8: shows the confusion matrices of our model for the",
      "page": 7
    },
    {
      "caption": "Figure 8: Confusion matrix on the FER-2013 dataset: the vertical",
      "page": 7
    },
    {
      "caption": "Figure 9: Explanation generation.",
      "page": 9
    },
    {
      "caption": "Figure 9: red bounding box has a smiling",
      "page": 9
    },
    {
      "caption": "Figure 9: , place category output is",
      "page": 9
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Feature Type": "The angle at",
          "Feature Description": "Neck by both shoulders\nRight shoulder by neck and left shoulder\nLeft shoulder by neck and right shoulder\nNeck by vertical and back\nNeck by head and back\nRight shoulder by the right arm and neck\nLeft shoulder by the left arm and neck\nRight elbow by the right arm and right\nforearm\nLeft elbow by the left arm and left\nforearm\nHip between y-axis and torso"
        },
        {
          "Feature Type": "Distance between",
          "Feature Description": "Right hand and the hips\nLeft hand and the hips\nRight\nfoot and the hips\nLeft\nfoot and the hips\nRight hand and the right shoulder\nLeft hand and the left shoulder\nRight elbow and the hips\nLeft elbow and the hips"
        },
        {
          "Feature Type": "Area of\nthe triangle by",
          "Feature Description": "Both hands and neck\nBoth shoulders and neck\nBoth feet and the hips\nBoth hands and hips\nBoth elbows and neck"
        },
        {
          "Feature Type": "Space taken",
          "Feature Description": "Bounding box volume"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Name": "FER-2013 [1]\nEmotic [8]\nCAER-S [51]\nCAER [51]\nFABO [28]\nEWalk [3]\nGroupWalk [5]",
          "No of Items": "32,298\n23,571\n70,000\n13201\n206\n1384\n45",
          "Type": "Image\nImage\nImage\nVideo\nVideo\nVideo\nVideo",
          "Data Modes": "Face\nFace and Posture\nFace and Posture\nFace and Posture\nFace and Posture\nPosture and Gait\nFace, Posture and Gait",
          "Setting": "Posed\nWild\nTV shows\nTV shows\nPosed\nPosed\nWild",
          "Situational Intelligence": "No\nBackground\nBackground\nBackground\nNo\nBackground\nBackground and Group",
          "Classes": "N, S, H, A, Sr, F and D\nN, S, H, A, Sr, F, D and 19 other classes\nN, S, H, A, Sr, F and D\nN, S, H, A, Sr, F and D\nN, S, H, A, Sr, F, D, B, P and Ax\nN, S, H and A\nN, S, H and A"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Author": "Renda et al.\n[20]\nGan et al.\n[17]\nJoseph et al.\n[22]\nA-C [23]\nAMP-Net\n[21]\nEMERSK",
          "Year": "2019\n2019\n2021\n2022\n2022\n2023",
          "Accuracy(%)": "71\n73.73\n67\n72.03\n74.48\n78.62"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Author": "Daoudi et al.\n[52]\nLi et al.\n[53]\nBhatia et al.\n[36]\nTanmay et al.\n[3]\nEMERSK",
          "Year": "2017\n2018\n2021\n2022\n2023",
          "Accuracy (%)": "42.52\n53.73\n77.81\n80.07\n80.01"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Dataset": "CK+\nFER-2013\nAffectNets\nCAER-S\nFABO",
          "Test Accuracy(%)": "98.6\n78.62\n71.5\n92.4\n98.1"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Author": "Kosti et al.\n[8]\nLee et al.\n[9]\nMittal et al.\n[5]\nMittal et al.\n[35]\nEMERSK",
          "Year": "2019\n2019\n2020\n2021\n2023",
          "Average Precision (mAP)": "58.42\n62.58\n65.83\n67.62\n76.3"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Author": "",
          "Year": "",
          "Accuracy (%)": "Posture"
        },
        {
          "Author": "CCCNN [32]\nSun et al.\n[33]\nMIBDL [34]\nCMEFA [31]\nEMERSK",
          "Year": "2016\n2018\n2021\n2023\n2023",
          "Accuracy (%)": "74.8\n90.51\n79.71\n88.42\n78.1"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "F": "✓",
          "P": "",
          "G": "",
          "Accuracy without C (%)": "68.1",
          "Accuracy with C (%)": "69.7"
        },
        {
          "F": "",
          "P": "✓",
          "G": "",
          "Accuracy without C (%)": "42.3",
          "Accuracy with C (%)": "45.6"
        },
        {
          "F": "",
          "P": "",
          "G": "✓",
          "Accuracy without C (%)": "34.8",
          "Accuracy with C (%)": "37.2"
        },
        {
          "F": "✓",
          "P": "✓",
          "G": "",
          "Accuracy without C (%)": "70.1",
          "Accuracy with C (%)": "71.05"
        },
        {
          "F": "✓",
          "P": "",
          "G": "✓",
          "Accuracy without C (%)": "69.71",
          "Accuracy with C (%)": "70.9"
        },
        {
          "F": "✓",
          "P": "✓",
          "G": "✓",
          "Accuracy without C (%)": "71.5",
          "Accuracy with C (%)": "71.82"
        }
      ],
      "page": 9
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Fer-2013 learn facial expressions from an image",
      "venue": "Fer-2013 learn facial expressions from an image"
    },
    {
      "citation_id": "2",
      "title": "Going deeper in facial expression recognition using deep neural networks",
      "authors": [
        "A Mollahosseini",
        "D Chan"
      ],
      "year": "2016",
      "venue": "2016 IEEE Winter conference on applications of computer vision"
    },
    {
      "citation_id": "3",
      "title": "Learning gait emotions using affective and deep features",
      "authors": [
        "T Randhavane",
        "U Bhattacharya",
        "P Kabra",
        "K Kapsaskis",
        "K Gray",
        "D Manocha",
        "A Bera"
      ],
      "year": "2022",
      "venue": "Proceedings of the 15th ACM SIGGRAPH Conference on Motion, Interaction and Games"
    },
    {
      "citation_id": "4",
      "title": "Multimodal semi-automated affect detection from cues, gross body language, and facial features",
      "authors": [
        "A Graesser"
      ],
      "year": "2010",
      "venue": "User Modeling and User-Adapted Interaction"
    },
    {
      "citation_id": "5",
      "title": "Emoticon: Context-aware multimodal emotion recognition using frege's principle",
      "authors": [
        "T Mittal",
        "P Guhan",
        "U Bhattacharya",
        "R Chandra",
        "A Bera",
        "D Manocha"
      ],
      "year": "2020",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "6",
      "title": "Situational knowledge and emotions",
      "authors": [
        "M Conway",
        "D Bekerian"
      ],
      "year": "1987",
      "venue": "Cognition and emotion"
    },
    {
      "citation_id": "7",
      "title": "The football boost? testing three models on impacts on sports spectators' selfesteem",
      "authors": [
        "S Knobloch-Westerwick",
        "J Abdallah",
        "A Billings"
      ],
      "year": "2020",
      "venue": "Communication & Sport"
    },
    {
      "citation_id": "8",
      "title": "Context based emotion recognition using emotic dataset",
      "authors": [
        "R Kosti",
        "J Alvarez",
        "A Recasens",
        "A Lapedriza"
      ],
      "year": "2019",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "9",
      "title": "Context-aware emotion recognition networks",
      "authors": [
        "J Lee",
        "S Kim",
        "S Kim",
        "J Park",
        "K Sohn"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision"
    },
    {
      "citation_id": "10",
      "title": "Peeking inside the black-box: a survey on explainable artificial intelligence (xai)",
      "authors": [
        "A Adadi",
        "M Berrada"
      ],
      "year": "2018",
      "venue": "IEEE access"
    },
    {
      "citation_id": "11",
      "title": "Deep inside convolutional networks: Visualising image classification models and saliency maps",
      "authors": [
        "K Simonyan",
        "A Vedaldi",
        "A Zisserman"
      ],
      "year": "2013",
      "venue": "Deep inside convolutional networks: Visualising image classification models and saliency maps",
      "arxiv": "arXiv:1312.6034"
    },
    {
      "citation_id": "12",
      "title": "Grad-cam: Visual explanations from deep networks via gradient-based localization",
      "authors": [
        "R Selvaraju",
        "M Cogswell",
        "A Das",
        "R Vedantam",
        "D Parikh",
        "D Batra"
      ],
      "year": "2017",
      "venue": "Proceedings"
    },
    {
      "citation_id": "13",
      "title": "A unified approach to interpreting model predictions",
      "authors": [
        "S Lundberg",
        "S.-I Lee"
      ],
      "year": "2017",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "14",
      "title": "explaining the predictions of any classifier",
      "authors": [
        "M Ribeiro",
        "S Singh",
        "C Guestrin"
      ],
      "year": "2016",
      "venue": "Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining"
    },
    {
      "citation_id": "15",
      "title": "A hybrid explainable ai framework applied to global and local facial expression recognition",
      "authors": [
        "M Deramgozin",
        "S Jovanovic",
        "H Rabah",
        "N Ramzan"
      ],
      "year": "2021",
      "venue": "2021 IEEE International Conference on Imaging Systems and Techniques (IST)"
    },
    {
      "citation_id": "16",
      "title": "Explainable model selection of a convolutional neural network for driver's facial emotion identification",
      "authors": [
        "A Kandeel",
        "H Abbas",
        "H Hassanein"
      ],
      "year": "2021",
      "venue": "Pattern Recognition. ICPR International Workshops and Challenges: Virtual Event"
    },
    {
      "citation_id": "17",
      "title": "Facial expression recognition boosted by soft label with a diverse ensemble",
      "authors": [
        "Y Gan",
        "J Chen",
        "L Xu"
      ],
      "year": "2019",
      "venue": "Pattern Recognition Letters"
    },
    {
      "citation_id": "18",
      "title": "Resnet-50 and vgg-16 for recognizing facial emotions",
      "authors": [
        "P Dhankhar"
      ],
      "year": "2019",
      "venue": "International Journal of Innovations in Engineering and Technology (IJIET)"
    },
    {
      "citation_id": "19",
      "title": "Step: Spatial temporal graph convolutional networks for emotion perception from gaits",
      "authors": [
        "U Bhattacharya",
        "T Mittal",
        "R Chandra",
        "T Randhavane",
        "A Bera",
        "D Manocha"
      ],
      "year": "2020",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "20",
      "title": "Comparing ensemble strategies for deep learning: An application to facial expression recognition",
      "authors": [
        "A Renda",
        "M Barsacchi",
        "A Bechini",
        "F Marcelloni"
      ],
      "year": "2019",
      "venue": "Expert Systems with Applications"
    },
    {
      "citation_id": "21",
      "title": "Adaptive multilayer perceptual attention network for facial expression recognition",
      "authors": [
        "H Liu",
        "H Cai",
        "Q Lin",
        "X Li",
        "H Xiao"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Circuits and Systems for Video Technology"
    },
    {
      "citation_id": "22",
      "title": "Facial expression recognition for the blind using deep learning",
      "authors": [
        "J Joseph",
        "S Mathew"
      ],
      "year": "2021",
      "venue": "2021 IEEE 4th International Conference on Computing, Power and Communication Technologies (GUCON)"
    },
    {
      "citation_id": "23",
      "title": "Ad-corre: Adaptive correlation-based loss for facial expression recognition in the wild",
      "authors": [
        "A Fard",
        "M Mahoor"
      ],
      "year": "2022",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "24",
      "title": "Deformable convolutional lstm for human body emotion recognition",
      "authors": [
        "P Tahghighi",
        "A Koochari",
        "M Jalali"
      ],
      "year": "2021",
      "venue": "Pattern Recognition. ICPR International Workshops and Challenges: Virtual Event"
    },
    {
      "citation_id": "25",
      "title": "Human emotion recognition with relational region-level analysis",
      "authors": [
        "W Li",
        "X Dong",
        "Y Wang"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "26",
      "title": "Feratt: Facial expression recognition with attention net",
      "authors": [
        "P Marrero Fernandez",
        "F Guerrero Pena",
        "T Ren",
        "A Cunha"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops"
    },
    {
      "citation_id": "27",
      "title": "Multiple kernel learning for emotion recognition in the wild",
      "authors": [
        "K Sikka",
        "K Dykstra",
        "S Sathyanarayana",
        "G Littlewort",
        "M Bartlett"
      ],
      "year": "2013",
      "venue": "Proceedings of the 15th ACM on International conference on multimodal interaction"
    },
    {
      "citation_id": "28",
      "title": "Bi-modal emotion recognition from expressive face and body gestures",
      "authors": [
        "H Gunes",
        "M Piccardi"
      ],
      "year": "2007",
      "venue": "Journal of Network and Computer Applications"
    },
    {
      "citation_id": "29",
      "title": "Multimodal expression of emotion: Affect programs or componential appraisal patterns?",
      "authors": [
        "K Scherer",
        "H Ellgring"
      ],
      "year": "2007",
      "venue": "Emotion"
    },
    {
      "citation_id": "30",
      "title": "Automated analysis of body movement in emotionally expressive piano performances",
      "authors": [
        "G Castellano",
        "M Mortillaro",
        "A Camurri",
        "G Volpe",
        "K Scherer"
      ],
      "year": "2008",
      "venue": "Music Perception"
    },
    {
      "citation_id": "31",
      "title": "Coupled multimodal emotional feature analysis based on broad-deep fusion networks in human-robot interaction",
      "authors": [
        "L Chen",
        "M Li",
        "M Wu",
        "W Pedrycz",
        "K Hirota"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Neural Networks and Learning Systems"
    },
    {
      "citation_id": "32",
      "title": "Convolutional mkl based multimodal emotion recognition and sentiment analysis",
      "authors": [
        "S Poria",
        "I Chaturvedi",
        "E Cambria",
        "A Hussain"
      ],
      "year": "2016",
      "venue": "2016 IEEE 16th international conference on data mining (ICDM"
    },
    {
      "citation_id": "33",
      "title": "Affect recognition from facial movements and body gestures by hierarchical deep spatio-temporal features and fusion strategy",
      "authors": [
        "B Sun",
        "S Cao",
        "J He",
        "L Yu"
      ],
      "year": "2018",
      "venue": "Neural Networks"
    },
    {
      "citation_id": "34",
      "title": "Multimodal information-based broad and deep learning model for emotion understanding",
      "authors": [
        "M Li",
        "L Chen",
        "M Wu",
        "W Pedrycz",
        "K Hirota"
      ],
      "year": "2021",
      "venue": "2021 40th Chinese Control Conference (CCC)"
    },
    {
      "citation_id": "35",
      "title": "Multimodal and context-aware emotion perception model with multiplicative fusion",
      "authors": [
        "T Mittal",
        "A Bera",
        "D Manocha"
      ],
      "year": "2021",
      "venue": "IEEE MultiMedia"
    },
    {
      "citation_id": "36",
      "title": "A lstm-based approach for gait emotion recognition",
      "authors": [
        "Y Bhatia",
        "A Bari",
        "M Gavrilova"
      ],
      "year": "2021",
      "venue": "2021 IEEE 20th International Conference on Cognitive Informatics & Cognitive Computing (ICCI* CC)"
    },
    {
      "citation_id": "37",
      "title": "M3er: Multiplicative multimodal emotion recognition using facial, textual, and speech cues",
      "authors": [
        "T Mittal",
        "U Bhattacharya",
        "R Chandra",
        "A Bera",
        "D Manocha"
      ],
      "year": "2020",
      "venue": "Proceedings of the AAAI conference on artificial intelligence"
    },
    {
      "citation_id": "38",
      "title": "Cascade attention networks for group emotion recognition with face, body and image cues",
      "authors": [
        "K Wang",
        "X Zeng",
        "J Yang",
        "D Meng",
        "K Zhang",
        "X Peng",
        "Y Qiao"
      ],
      "year": "2018",
      "venue": "Proceedings of the 20th ACM international conference on multimodal interaction"
    },
    {
      "citation_id": "39",
      "title": "From individual to group-level emotion recognition: emoti w 5.0",
      "authors": [
        "T Gedeon",
        "A Dhall",
        "J Joshi",
        "J Hoey",
        "R Goecke",
        "S Ghosh"
      ],
      "year": "2021",
      "venue": "From individual to group-level emotion recognition: emoti w 5.0"
    },
    {
      "citation_id": "40",
      "title": "Automatic emotion recognition for groups: a review",
      "authors": [
        "E Veltmeijer",
        "C Gerritsen",
        "K Hindriks"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "41",
      "title": "Places: A 10 million image database for scene recognition",
      "authors": [
        "B Zhou",
        "A Lapedriza",
        "A Khosla",
        "A Oliva"
      ],
      "year": "2017",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "42",
      "title": "Large-scale visual sentiment ontology and detectors using adjective noun pairs",
      "authors": [
        "D Borth",
        "R Ji",
        "T Chen",
        "T Breuel",
        "S.-F Chang"
      ],
      "year": "2013",
      "venue": "Proceedings of the 21st ACM international conference on Multimedia"
    },
    {
      "citation_id": "43",
      "title": "Blazepose: On-device real-time body pose tracking",
      "authors": [
        "V Bazarevsky",
        "I Grishchenko",
        "K Raveendran",
        "T Zhu",
        "F Zhang",
        "M Grundmann"
      ],
      "year": "2020",
      "venue": "Blazepose: On-device real-time body pose tracking",
      "arxiv": "arXiv:2006.10204"
    },
    {
      "citation_id": "44",
      "title": "U-net: Convolutional networks for biomedical image segmentation",
      "authors": [
        "O Ronneberger",
        "P Fischer",
        "T Brox"
      ],
      "year": "2015",
      "venue": "Medical Image Computing and Computer-Assisted Intervention-MICCAI 2015: 18th International Conference"
    },
    {
      "citation_id": "45",
      "title": "Segnet: A deep convolutional encoder-decoder architecture for image segmentation",
      "authors": [
        "V Badrinarayanan",
        "A Kendall",
        "R Cipolla"
      ],
      "year": "2017",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "46",
      "title": "Learning deconvolution network for semantic segmentation",
      "authors": [
        "H Noh",
        "S Hong",
        "B Han"
      ],
      "year": "2015",
      "venue": "Learning deconvolution network for semantic segmentation"
    },
    {
      "citation_id": "47",
      "title": "Vision-based human emotion recognition using hog-klt feature",
      "authors": [
        "R Santhoshkumar",
        "M Geetha"
      ],
      "year": "2019",
      "venue": "Proceedings of First International Conference on Computing, Communications, and Cyber-Security"
    },
    {
      "citation_id": "48",
      "title": "Body expression recognition from animated 3d skeleton",
      "authors": [
        "A Crenn",
        "R Khan",
        "A Meyer",
        "S Bouakaz"
      ],
      "year": "2016",
      "venue": "2016 International Conference on 3D Imaging (IC3D)"
    },
    {
      "citation_id": "49",
      "title": "Learning spatiotemporal features with 3d convolutional networks",
      "authors": [
        "D Tran",
        "L Bourdev",
        "R Fergus",
        "L Torresani",
        "M Paluri"
      ],
      "year": "2015",
      "venue": "Proceedings of the IEEE international conference on computer vision"
    },
    {
      "citation_id": "50",
      "title": "View adaptive neural networks for high performance skeleton-based human action recognition",
      "authors": [
        "P Zhang",
        "C Lan",
        "J Xing",
        "W Zeng",
        "J Xue",
        "N Zheng"
      ],
      "year": "2019",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "51",
      "title": "Context-aware emotion recognition networks",
      "authors": [
        "J Lee",
        "S Kim",
        "S Kim",
        "J Park",
        "K Sohn"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision"
    },
    {
      "citation_id": "52",
      "title": "Emotion recognition by body movement representation on the manifold of symmetric positive definite matrices",
      "authors": [
        "M Daoudi",
        "S Berretti",
        "P Pala",
        "Y Delevoye",
        "A Del Bimbo"
      ],
      "year": "2017",
      "venue": "Image Analysis and Processing-ICIAP 2017: 19th International Conference"
    },
    {
      "citation_id": "53",
      "title": "Identifying emotions from non-contact gaits information based on microsoft kinects",
      "authors": [
        "B Li",
        "C Zhu",
        "S Li",
        "T Zhu"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "54",
      "title": "Mijanur Palash received a BS degree in electrical and electronic engineering from Bangladesh University of Engineering and Technology and an MS in electrical and computer engineering from Southern Illinois University Carbondale, IL. Currently, he is working toward PhD in computer science at Purdue University",
      "authors": [
        "T Bänziger",
        "K Scherer"
      ],
      "year": "2010",
      "venue": "Blueprint for affective computing: A sourcebook"
    },
    {
      "citation_id": "55",
      "title": "He received his PhD in Electrical engineering from Purdue University in 1974",
      "venue": "He is the founder of the IEEE Symposium on Reliable and Distributed Systems, the IEEE conference on Digital Library, and the ACM Conference on Information and Knowledge Management"
    }
  ]
}