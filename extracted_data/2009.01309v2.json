{
  "paper_id": "2009.01309v2",
  "title": "Convolutional Speech Recognition With Pitch And Voice Quality Features",
  "published": "2020-09-02T19:25:50Z",
  "authors": [
    "Guillermo Cámbara",
    "Jordi Luque",
    "Mireia Farrús"
  ],
  "keywords": [
    "automatic speech recognition",
    "convolutional neural networks",
    "pitch",
    "jitter",
    "shimmer"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "The effects of adding pitch and voice quality features such as jitter and shimmer to a state-of-the-art CNN model for Automatic Speech Recognition are studied in this work. Pitch features have been previously used for improving classical HMM and DNN baselines, while jitter and shimmer parameters have proven to be useful for tasks like speaker or emotion recognition. Up to our knowledge, this is the first work combining such pitch and voice quality features with modern convolutional architectures, showing improvements up to 7% and 3% relative WER points, for the publicly available Spanish Common Voice and LibriSpeech 100h datasets, respectively. Particularly, our work combines these features with mel-frequency spectral coefficients (MFSCs) to train a convolutional architecture with Gated Linear Units (Conv GLUs). Such models have shown to yield small word error rates, while being very suitable for parallel processing for online streaming recognition use cases. We have added pitch and voice quality functionality to Facebook's wav2letter speech recognition framework, and we provide with such code and recipes to the community, to carry on with further experiments. Besides, to the best of our knowledge, our Spanish Common Voice recipe is the first public Spanish recipe for wav2letter.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Neural network models applied to automatic speech recognition (ASR) task are consistently achieving state-of-the-art results in the field. Some of the best scoring architectures involve transformer-based acoustic models  [1] , LAS models  [2]  with SpecAugment data augmentation  [3]  or models strongly based on convolutional neural networks, like the ResNets and TDS ones in  [4] .\n\nSuch convolutional approaches have the advantage of being able to look at larger context windows, without the risk of vanishing gradients like in pure LSTM approaches, and being suitable for online streaming applications, while attaining low word error rate (WER) scores. Furthermore, following the trend of making systems as end-to-end as possible, even fully convolutional neural approaches have been proposed, and shown state-of-the-art performances  [5] . This fully convolutional architecture takes profit of stacking convolutional layers for efficient parallelization with gated linear units that prevent the gradients from vanishing as architectures go deeper  [6] .\n\nRecently, Facebook has outsourced wav2letter  [7] , a very fast speech recognition framework with recipes prepared for training and decoding with some of these modern models, with an emphasis on the convolutional ones. Most of the modern architectures work only on cepstral (MFCCs) and mel-frequency spectral coefficients (MFSCs) inputs, or even directly with the raw waveform, and tending towards the increasing of granularity at input level by usually augmenting the number of spectral parameters. Whilst it seems evident whether current end-to-end deep network architectures are able to automatically perform relevant feature extraction for speech tasks, psychical or functional properties, related to the underlying speech production system, become fuzzy or difficult to connect with the speech recognition performances. In addition, it is still unclear how the great quantity and different speech hand-crafted voice features, carefully developed along past years and based on our linguistic knowledge, might help and in which degree to the current speech network architectures. Some well-known speech recognition frameworks, like Kaldi  [8] , have incorporated the use of additional prosodic features, such as the pitch or the probability of voicing (POV). These are stacked into an input vector together with those cepstral/spectral ones, and then forwarded to classifiers like HMM or DNN ensembles. Nevertheless, the newest convolutional architectures have not yet been extensively applied along with such prosodic features, and frameworks like wav2letter, reaching state-of-the-art performances in ASR tasks, do not yet provide with integrated pitch functionality within feature extraction modules. Furthermore, in the last decades, jitter and shimmer have shown to be useful in a wide range of applications; e.g. detection of different speaking styles  [9] , age and gender classification  [10] , emotion detection  [11] , speaker recognition  [12] , speaker diarization  [13] , and Alzheimer's and Parkinson Diseases detection  [14, 15] , among others.\n\nThe main contribution of this work focus on assessing the value of adding pitch and voice quality features to the spectral coefficients, commonly employed in most of the deep speech recognition systems. To this end, the dimension of MFSC vector, at the input layer, is augmented by the prosodic features and error rates are reported for both Spanish and English speech recognition tasks. Experiments are carried out by using the Conv GLU model. It was proposed by  [16]  within the wav2letter's WSJ recipe and has reported state-ofthe-art performances for both LibriSpeech and WSJ datasets. To the best of our knowledge, this is the first attempt to use jitter and shimmer features within a modern deep neural-based speech recognition system while keeping easy to identify psychical/functional properties of the voice and linking them to the ASR performance. Furthermore, the recipes employed in this research have been published in a GitHub 1  repository, aiming to make easy to reproduce experiments on two public and freely available corpus: the Spanish Common Voice dataset  [17]  and the English LibriSpeech 100h partition  [18] .",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Prosodic And Voice Quality Features",
      "text": "Pitch -or fundamental perceived frequency-has proved to increase performance in ASR systems, significantly for tonal languages, like Punjabi  [19] , as well as for non-tonal ones like English  [20] . Jitter and shimmer represent the cycle-to-cycle variations of the fundamental frequency and amplitude, respectively. Since a long time, they have been considered relevant and often applied to detect voice pathologies, as in  [21, 22] , thus considered as measurements of voice quality. Although voice quality features differ intrinsically from those suprasegmental prosodic features, they have shown to be related to prosody. In  [23] , the authors showed that voice quality features are relevant markers signaling paralinguistic information, and that they should even be considered as prosodic carriers along with pitch and duration, for instance.\n\nIn ASR literature, some works have reported that prosodic information can raise the performance of speech recognizer systems. For instance, in  [24]  the authors built an ASR for dysarthric speech and  [25]  reports benefit on the use of jitter and shimmer for noisy speech recognition, both systems based on classical HMM acoustic modelling. In the case of deep networks, LSTMs was taken into  [26]  for acoustic emotion recognition task, however they did not perform ASR task on its own.\n\nFollowing previous works, we hypothesize that prosodic and voice quality features may boost robustness in convolutional like ASR systems. Moreover, they could play an even more important role in further speech tasks, including punctuation marks, emotion recognition or musical contexts, where additional prosodic information would be useful.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Methodology",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Data",
      "text": "The effect of adding pitch and voice quality features is evaluated by means of the Common Voice corpus in Spanish  [17]  and the LibriSpeech 100h dataset in English  [18] . Common Voice corpus is an open-source dataset that consists of recordings from volunteer contributors pronouncing scripted sentences, recorded at 48kHz rate and using own devices. The sentences come from original contributor donations and public domain movie scripts and it is continuously growing. Although there are already more than 100 hours of validated audio, we have kept a reduced partition of approximately 19.0 h for training, 2.7 h for development and 2.2 h for testing sets. The main criterion for the stratification of such partitions is to ensure that each one has exclusive speakers, while trying to keep a 80-10-10% proportion. Every sample can be down voted by the contributors if it is not clear enough, so we have discarded all samples containing at least one down vote, to keep the cherry picked recordings as clean as possible. Afterwards, we try to keep as balanced as possible the distributions by age, gender and accent. The Python scripts for obtaining such partition are provided in our public Git repository, along with other code necessary to reproduce our ASR recipes. Up to our knowledge, this is the first",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Feature Extraction",
      "text": "As recommended by wav2letter's Conv GLU recipes, raw audio is processed to extract static MFSCs, applying 40 filterbanks. This serves as our baseline, so on top of it we append pitch and voice quality related features. From now on in this work, when we talk about pitch features we refer to the following three features: the extracted pitch itself, plus the POV for each frame and the variation of pitch across two frames (delta-pitch). Being so, 40 MFSCs are always computed for each time frame, and if specified by the user in the configuration, the three pitch features (pitch, POV and delta-pitch) can be appended to them, plus jitter relative, jitter absolute, shimmer dB and/or shimmer relative.\n\nThere are various pitch extractor algorithms such as Yin  [27]  or getF0  [28] . However, we have decided to refactor the Kaldi's one from  [29]  within the feature extractor C++ class from wav2letter. Latter algorithm has been frequently tested along the recent years within a wide variety of ASR tasks. It is inspired by getF0 and finds the sequence of lags that maximizes the Normalized Cross Correlation Function (NCCF). It makes use of the Viterbi algorithm for obtaining the optimal lags and, in our implementation, it applies the logarithm to the pitch values as the only post-processing step. The logarithm compresses pitch values to the same order as the MFSCs, which are compressed by the logarithm as well, thus improving numerical stability later on during the training phase. Subtracting the weighted average pitch during post-processing has been discarded, since the reported gains in WER by Kaldi are only of a 0.1%, but we may implement them in future iterations.\n\nShimmer is computed measuring the peak-to-peak waveform amplitude at each period where the pitch is extracted, and then performing the corresponding operations, depending on whether we deal with shimmer dB or shimmer relative, see reference  [12] . With the pitch extracted at each period, the same can be done for jitter absolute and relative, by calculating the fundamental frequency differences between such cycles.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "System Architecture",
      "text": "Since our purpose is to study how pitch and voice quality features contribute to a convolutional acoustic model (AM), we have used the Conv GLU AM from wav2letter's Wall Street Journal (WSJ) recipe  [16] . This model has approximately 17M parameters with dropout applied after each of its 17 layers. The WSJ dataset contains around 80 hours of audio recordings, which is closer to the magnitude of our data than the full Lib-riSpeech recipe (about 1000 hours). We have not done an extensive exploration of architecture parameters, since it yields decent out of the box results with Common Voice and LibriSpeech 100h data.\n\nRegarding Common Voice's lexicon, we use a graphemebased one extracted from the approximately 9000 words from both the training and development partitions. We use the standard Spanish alphabet as tokens, plus the \"c ¸\" letter from Catalan and the vowels with diacritical marks, making a total of 37 tokens. The \"c ¸\" character is included because of the presence of some Catalan words in the dataset, like \"Barc ¸a\". The language model (LM) is a 4-gram model extracted with KenLM  [30]  from the training set. Since most of the sentences are shared across partitions, due to the scripted nature of the dataset, we expected an optimistic behavior after applying such LM. Therefore, we are also reporting results given by another 4-gram LM extracted from the Spanish Fisher+Callhome. The Fisher corpus splitting is taken from the Kaldi's recipe  [31] . Decoding across AM, lexicon and LM is done with the beam-search decoder provided by wav2letter  [32] . Furthermore, in order to assess the capacity of the AM by itself, we also evaluate without LM, choosing the final characters with the greedy best path from the predictions of the AM. For the LibriSpeech evaluation, the lexicon and the language model are the same as provided by wav2letter's Conv GLU LibriSpeech recipe. The lexicon is obtained from the train corpus and the language model is a 4-gram model also trained with KenLM.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Experiments",
      "text": "After some initial simulations, we have found that the most stable voice quality features are jitter relative and shimmer relative. Therefore, we try 5 different feature configurations: For each one, we compute WERs on Common Voice's dev and test sets. Decodings are performed without LM (NoLM), with both in-domain and out-domain LMs, from Common Voice's LM (CVLM) from Fisher+Callhome's LM (FCLM) databases, respectively. Therefore, we obtain 6 WERs for each one of the 5 feature configurations.\n\nBesides the features, the training configurations for each experiment are the same, all based on wav2letter's WSJ recipe. The inferred segmentation is taken out from wav2letter's Auto Segmentation Criterion (ASG)  [16] , inspired by CTC loss  [33] . The learning rate is tweaked to 7.3, and is decayed in a 20% every 10 epochs, a tuning done with the dev set. A 25 ms rolling window with a 10 ms stride is used for extracting all the features, jitter and shimmer are averaged across 500 ms windows.\n\nFor beam-search decoding, the following settings are tuned with the dev set: LM weight set to 2.5, word score set to 1, beam size set to 2500, beam threshold set to 25 and silence weight set to -0.4. In order to tune these, we have not run an extensive exploration of hyperparameters, but after a shallow search we found these to provide good results for both LMs. Furthermore, LibriSpeech WER is evaluated with devclean/other and test-clean/other partitions, using the same AM training recipe as in Common Voice. As we are scaling with a bigger dataset demanding a higher computational cost, the top three parameter configurations found with Common Voice experiments are selected in order to perform such evaluations. Decoding parameters are taken from wav2letter's LibriSpeech recipe.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Results And Discussion",
      "text": "The Table  1  reports the word error rates (WER, %) for each one of the 5 feature configurations, for the proposed decodings of Common Voice's dev and test sets, without LM (NoLM), with its own LM (CVLM) and the Fisher+Callhome LM (FCLM). For every evaluated case, the best WER score is always provided by one of the models using pitch features, or pitch with voice quality (jitter + shimmer) features, with gains between 1.38% and 7.36% relative WER points.\n\nFor the cases without LM, the model with MFSC and pitch features is the one with the best performance, with relative gains of 2.68% and 1.83% for dev and test sets, respectively. Additional features on the other models also improve the WER score, except for the case with pitch and shimmer only, which yields worse results across all experiments. On the other hand, decoding with CVLM achieves the best WER scores, when training with all the proposed features together: MFSCs, the 3 pitch features, jitter relative and shimmer relative. A 20.01% WER is obtained for the dev set, and a 22.90% WER for the test set. As it was expected, the CVLM improves drastically the predictions, because even though it is obtained from the train partition solely, many sentences are shared with the dev and test sets, due to the reduced vocabulary in this dataset.\n\nA more realistic approach is to decode by using an external LM. The FCLM language model is built from the training partition of the LDC Spanish Fisher+Callhome corpus. Although the LM enrollment is performed with less than 20 hours of audio (approximately 16k sentences), it still yields to a reasonable performance compared to the CVLMs decodings. With respect to the prosodic features, the FCLM beam decoding reaches the lower WER in development by using MFSCs only augmented with pitch features; that is, 37.57% WER. The lowest 42.95% WER score in the test set is given by the combination of all pitch and voice quality characteristics. Once again, the best results in terms of WER are provided by models with pitch features, or pitch features with the combination of jitter and shimmer, showing the potential of pitch and voice quality features to improve the performance of an ASR based on convolutional neural networks.\n\nNonetheless, it is worth noticing how the use of only pitch and shimmer features yields to worse performance for both AM and AM/LM decoding models. Previous behaviour is depicted in the Figure  1(a) , where using only shimmer dramatically affects the training stage of the model, making it worse and slower. However, training with pitch features or with pitch and jitter features seems to help at reaching better WER plateaus and at faster pace. While jitter is a measure of frequency instability in the wave, shimmer is a measure of amplitude instability. Being so, pitch and jitter characteristics might contribute to MFSCs spectral features with independent information, just by synchronising them in a simple concatenation like the proposed one. However, the inclusion of shimmer, which is related to amplitude -as opposed to the pitch and jitter, related to frequency-, is more likely to be understood as a perturbation throughout the convolutional layers that might difficult the acoustic model training.\n\nEven though, it is interesting to see how if shimmer is coupled with jitter and pitch characteristics altogether, the performance obtained yields to more robust results compared to the baseline and independently of decoding with CVLM and FCLM language models. Other studies already suggest the correlation between jitter and shimmer by the same index, that is, the Voice Handicap Index (VHI)  [34] , so the convolutional filters may be finding similar correlations, thus improving mutual information when coupled together with spectral features and promoting such as voice measurements as good feature candidates for enhancing the speech recognition of pathological voices. The latter being an interesting hypothesis to look for further evidence.\n\nThe impact of pitch and voice quality features is also reported by LibriSpeech experiments, see Table  2  and Figure  1(b) , with relative improvements of 2.94% and 2.06% for dev-clean and dev-other, respectively, and about 0.96% and 2.87% for test-clean and test-other. Gains seem to be more consistent for the \"other\" partitions, where there is more accent and prosody diversity than in \"clean\" ones.\n\nAppending pitch characteristics to MFSCs seems to slightly improve the ASR performance. Among them, MFSC+pitch and MFSC+pitch+jitter+shimmer combinations are the ones that provide the most robust behavior across all the experiment. All assessed features carry prosodic information and might aid the network on complementing the information conveyed by solely the magnitude spectrum. For instance, by helping on reducing the MFSC distortion which appears at the lower frequency region of the spectrum  [35] . Overall, they help boosting the Note that the approach employed for the feature combination has been quite simple, by just appending such features to the spectral ones at the input layer, without extensive postprocessing either nor adaptation of the model architecture. Being so, it is reasonable to think that there is still margin of improvement in the application of pitch and voice quality measurements to state-of-the-art convolutional neural models. Possible strategies comprises adapting the feature concatenation, maybe by dedicating exclusive filters to the new pitch and voice quality features with POV as a gating mechanism, especially after experimentally realising, not reported in this work, that the estimation of measurements like shimmer may benefit from different post-processing techniques.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Conclusions",
      "text": "In this study, we performed a preliminary exploration on the effects of pitch and jitter/shimmer voice quality measurements within the framework of the ASR task performed by convolutional neural network models. The experiments reported with a publicly available Spanish speech corpus showed consistent improvements on the model robustness, achieving a reduced relative 7% WER in some scenarios. Besides, such feature extraction functionalities are provided and integrated with wav2letter code to easily replicate our findings or directly applying pitch and voice quality features to wav2letter models. We also provide the recipe for the Common Voice Spanish dataset, the first recipe suited for wav2letter using a Spanish publicly available dataset. The recipe for LibriSpeech experiments is also provided, which achieves up to a 2.94% relative WER improvement. Further steps on the research of convolutional ASR with pitch and voice quality would imply adapting architectures for feature processing, or applying such characteristics for tasks including the presence of punctuation marks, emotion recognition and even pathological or singing voices. For the latter tasks, the importance of pitch and voice quality features is expected to become even more relevant.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Common Voice and LibriSpeech dev set WER (%)",
      "page": 4
    },
    {
      "caption": "Figure 1: (a), where using only shimmer dramati-",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "4Telef´onica Research, Barcelona, Spain": "jordi.luque@telefonica.com,\nmfarrus@ub.edu"
        },
        {
          "4Telef´onica Research, Barcelona, Spain": "training and decoding with some of these modern models, with"
        },
        {
          "4Telef´onica Research, Barcelona, Spain": "an emphasis on the convolutional ones. Most of the modern ar-"
        },
        {
          "4Telef´onica Research, Barcelona, Spain": ""
        },
        {
          "4Telef´onica Research, Barcelona, Spain": "chitectures work only on cepstral (MFCCs) and mel-frequency"
        },
        {
          "4Telef´onica Research, Barcelona, Spain": ""
        },
        {
          "4Telef´onica Research, Barcelona, Spain": "spectral coefﬁcients (MFSCs) inputs, or even directly with the"
        },
        {
          "4Telef´onica Research, Barcelona, Spain": ""
        },
        {
          "4Telef´onica Research, Barcelona, Spain": "raw waveform, and tending towards the increasing of granular-"
        },
        {
          "4Telef´onica Research, Barcelona, Spain": ""
        },
        {
          "4Telef´onica Research, Barcelona, Spain": "ity at input level by usually augmenting the number of spectral"
        },
        {
          "4Telef´onica Research, Barcelona, Spain": ""
        },
        {
          "4Telef´onica Research, Barcelona, Spain": "parameters. Whilst it seems evident whether current end-to-end"
        },
        {
          "4Telef´onica Research, Barcelona, Spain": ""
        },
        {
          "4Telef´onica Research, Barcelona, Spain": "deep network architectures are able to automatically perform"
        },
        {
          "4Telef´onica Research, Barcelona, Spain": ""
        },
        {
          "4Telef´onica Research, Barcelona, Spain": "relevant feature extraction for speech tasks, psychical or func-"
        },
        {
          "4Telef´onica Research, Barcelona, Spain": ""
        },
        {
          "4Telef´onica Research, Barcelona, Spain": "tional properties,\nrelated to the underlying speech production"
        },
        {
          "4Telef´onica Research, Barcelona, Spain": ""
        },
        {
          "4Telef´onica Research, Barcelona, Spain": "system, become fuzzy or difﬁcult\nto connect with the speech"
        },
        {
          "4Telef´onica Research, Barcelona, Spain": ""
        },
        {
          "4Telef´onica Research, Barcelona, Spain": "recognition performances. In addition, it is still unclear how the"
        },
        {
          "4Telef´onica Research, Barcelona, Spain": ""
        },
        {
          "4Telef´onica Research, Barcelona, Spain": "great quantity and different speech hand-crafted voice features,"
        },
        {
          "4Telef´onica Research, Barcelona, Spain": ""
        },
        {
          "4Telef´onica Research, Barcelona, Spain": "carefully developed along past years and based on our linguis-"
        },
        {
          "4Telef´onica Research, Barcelona, Spain": ""
        },
        {
          "4Telef´onica Research, Barcelona, Spain": "tic knowledge, might help and in which degree to the current"
        },
        {
          "4Telef´onica Research, Barcelona, Spain": ""
        },
        {
          "4Telef´onica Research, Barcelona, Spain": "speech network architectures."
        },
        {
          "4Telef´onica Research, Barcelona, Spain": ""
        },
        {
          "4Telef´onica Research, Barcelona, Spain": "Some well-known\nspeech\nrecognition\nframeworks,\nlike"
        },
        {
          "4Telef´onica Research, Barcelona, Spain": "Kaldi [8], have incorporated the use of additional prosodic fea-"
        },
        {
          "4Telef´onica Research, Barcelona, Spain": "tures,\nsuch as\nthe pitch or\nthe probability of voicing (POV)."
        },
        {
          "4Telef´onica Research, Barcelona, Spain": "These are stacked into an input vector together with those cep-"
        },
        {
          "4Telef´onica Research, Barcelona, Spain": "stral/spectral ones, and then forwarded to classiﬁers like HMM"
        },
        {
          "4Telef´onica Research, Barcelona, Spain": "or DNN ensembles. Nevertheless, the newest convolutional ar-"
        },
        {
          "4Telef´onica Research, Barcelona, Spain": "chitectures have not yet been extensively applied along with"
        },
        {
          "4Telef´onica Research, Barcelona, Spain": "such prosodic features, and frameworks like wav2letter, reach-"
        },
        {
          "4Telef´onica Research, Barcelona, Spain": "ing state-of-the-art performances in ASR tasks, do not yet pro-"
        },
        {
          "4Telef´onica Research, Barcelona, Spain": "vide with integrated pitch functionality within feature extraction"
        },
        {
          "4Telef´onica Research, Barcelona, Spain": "modules. Furthermore,\nin the last decades,\njitter and shimmer"
        },
        {
          "4Telef´onica Research, Barcelona, Spain": "have shown to be useful\nin a wide range of applications; e.g."
        },
        {
          "4Telef´onica Research, Barcelona, Spain": ""
        },
        {
          "4Telef´onica Research, Barcelona, Spain": "detection of different speaking styles [9], age and gender classi-"
        },
        {
          "4Telef´onica Research, Barcelona, Spain": ""
        },
        {
          "4Telef´onica Research, Barcelona, Spain": "ﬁcation [10], emotion detection [11], speaker recognition [12],"
        },
        {
          "4Telef´onica Research, Barcelona, Spain": ""
        },
        {
          "4Telef´onica Research, Barcelona, Spain": "speaker diarization [13], and Alzheimer’s and Parkinson Dis-"
        },
        {
          "4Telef´onica Research, Barcelona, Spain": ""
        },
        {
          "4Telef´onica Research, Barcelona, Spain": "eases detection [14, 15], among others."
        },
        {
          "4Telef´onica Research, Barcelona, Spain": ""
        },
        {
          "4Telef´onica Research, Barcelona, Spain": "The main contribution of this work focus on assessing the"
        },
        {
          "4Telef´onica Research, Barcelona, Spain": "value of adding pitch and voice quality features to the spectral"
        },
        {
          "4Telef´onica Research, Barcelona, Spain": "coefﬁcients, commonly employed in most of\nthe deep speech"
        },
        {
          "4Telef´onica Research, Barcelona, Spain": "recognition systems.\nTo this\nend,\nthe dimension of MFSC"
        },
        {
          "4Telef´onica Research, Barcelona, Spain": "vector, at\nthe input\nlayer,\nis augmented by the prosodic fea-"
        },
        {
          "4Telef´onica Research, Barcelona, Spain": "tures\nand error\nrates\nare\nreported for both Spanish and En-"
        },
        {
          "4Telef´onica Research, Barcelona, Spain": "glish speech recognition tasks.\nExperiments are carried out"
        },
        {
          "4Telef´onica Research, Barcelona, Spain": "by using the Conv GLU model.\nIt was proposed by [16]"
        },
        {
          "4Telef´onica Research, Barcelona, Spain": "within the wav2letter’s WSJ recipe and has reported state-of-"
        },
        {
          "4Telef´onica Research, Barcelona, Spain": "the-art performances for both LibriSpeech and WSJ datasets."
        },
        {
          "4Telef´onica Research, Barcelona, Spain": "To the best of our knowledge,\nthis is the ﬁrst attempt\nto use"
        },
        {
          "4Telef´onica Research, Barcelona, Spain": "jitter and shimmer features within a modern deep neural-based"
        },
        {
          "4Telef´onica Research, Barcelona, Spain": "speech recognition system while keeping easy to identify psy-"
        },
        {
          "4Telef´onica Research, Barcelona, Spain": "chical/functional properties of the voice and linking them to the"
        },
        {
          "4Telef´onica Research, Barcelona, Spain": "ASR performance.\nFurthermore,\nthe recipes employed in this"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "research have been published in a GitHub1\nrepository, aiming": "to make easy to reproduce experiments on two public and freely",
          "public repository with a wav2letter recipe for a publicly avail-": "able Spanish dataset. Besides,\nin order to provide with results"
        },
        {
          "research have been published in a GitHub1\nrepository, aiming": "available corpus:\nthe Spanish Common Voice dataset [17] and",
          "public repository with a wav2letter recipe for a publicly avail-": "for a popular benchmark, the proposal is also assessed with the"
        },
        {
          "research have been published in a GitHub1\nrepository, aiming": "the English LibriSpeech 100h partition [18].",
          "public repository with a wav2letter recipe for a publicly avail-": "aforementioned LibriSpeech 100h partition in English, consist-"
        },
        {
          "research have been published in a GitHub1\nrepository, aiming": "",
          "public repository with a wav2letter recipe for a publicly avail-": "ing of audio book recordings sampled at 16kHz."
        },
        {
          "research have been published in a GitHub1\nrepository, aiming": "2. PROSODIC AND VOICE QUALITY",
          "public repository with a wav2letter recipe for a publicly avail-": ""
        },
        {
          "research have been published in a GitHub1\nrepository, aiming": "",
          "public repository with a wav2letter recipe for a publicly avail-": "3.2. Feature Extraction"
        },
        {
          "research have been published in a GitHub1\nrepository, aiming": "FEATURES",
          "public repository with a wav2letter recipe for a publicly avail-": ""
        },
        {
          "research have been published in a GitHub1\nrepository, aiming": "",
          "public repository with a wav2letter recipe for a publicly avail-": "As recommended by wav2letter’s Conv GLU recipes, raw audio"
        },
        {
          "research have been published in a GitHub1\nrepository, aiming": "Pitch –or fundamental perceived frequency– has proved to in-",
          "public repository with a wav2letter recipe for a publicly avail-": ""
        },
        {
          "research have been published in a GitHub1\nrepository, aiming": "",
          "public repository with a wav2letter recipe for a publicly avail-": "is processed to extract static MFSCs, applying 40 ﬁlterbanks."
        },
        {
          "research have been published in a GitHub1\nrepository, aiming": "crease performance in ASR systems, signiﬁcantly for tonal lan-",
          "public repository with a wav2letter recipe for a publicly avail-": ""
        },
        {
          "research have been published in a GitHub1\nrepository, aiming": "",
          "public repository with a wav2letter recipe for a publicly avail-": "This serves as our baseline, so on top of it we append pitch and"
        },
        {
          "research have been published in a GitHub1\nrepository, aiming": "guages, like Punjabi [19], as well as for non-tonal ones like En-",
          "public repository with a wav2letter recipe for a publicly avail-": ""
        },
        {
          "research have been published in a GitHub1\nrepository, aiming": "",
          "public repository with a wav2letter recipe for a publicly avail-": "voice quality related features. From now on in this work, when"
        },
        {
          "research have been published in a GitHub1\nrepository, aiming": "glish [20]. Jitter and shimmer represent the cycle-to-cycle varia-",
          "public repository with a wav2letter recipe for a publicly avail-": ""
        },
        {
          "research have been published in a GitHub1\nrepository, aiming": "",
          "public repository with a wav2letter recipe for a publicly avail-": "we talk about pitch features we refer to the following three fea-"
        },
        {
          "research have been published in a GitHub1\nrepository, aiming": "tions of the fundamental frequency and amplitude, respectively.",
          "public repository with a wav2letter recipe for a publicly avail-": ""
        },
        {
          "research have been published in a GitHub1\nrepository, aiming": "",
          "public repository with a wav2letter recipe for a publicly avail-": "tures:\nthe extracted pitch itself, plus the POV for each frame"
        },
        {
          "research have been published in a GitHub1\nrepository, aiming": "Since a long time, they have been considered relevant and often",
          "public repository with a wav2letter recipe for a publicly avail-": ""
        },
        {
          "research have been published in a GitHub1\nrepository, aiming": "",
          "public repository with a wav2letter recipe for a publicly avail-": "and the variation of pitch across two frames (delta-pitch). Be-"
        },
        {
          "research have been published in a GitHub1\nrepository, aiming": "applied to detect voice pathologies, as in [21, 22],\nthus consid-",
          "public repository with a wav2letter recipe for a publicly avail-": ""
        },
        {
          "research have been published in a GitHub1\nrepository, aiming": "",
          "public repository with a wav2letter recipe for a publicly avail-": "ing so, 40 MFSCs are always computed for each time frame,"
        },
        {
          "research have been published in a GitHub1\nrepository, aiming": "ered as measurements of voice quality. Although voice quality",
          "public repository with a wav2letter recipe for a publicly avail-": ""
        },
        {
          "research have been published in a GitHub1\nrepository, aiming": "",
          "public repository with a wav2letter recipe for a publicly avail-": "and if speciﬁed by the user in the conﬁguration, the three pitch"
        },
        {
          "research have been published in a GitHub1\nrepository, aiming": "features differ intrinsically from those suprasegmental prosodic",
          "public repository with a wav2letter recipe for a publicly avail-": ""
        },
        {
          "research have been published in a GitHub1\nrepository, aiming": "",
          "public repository with a wav2letter recipe for a publicly avail-": "features (pitch, POV and delta-pitch) can be appended to them,"
        },
        {
          "research have been published in a GitHub1\nrepository, aiming": "features,\nthey have shown to be related to prosody.\nIn [23],\nthe",
          "public repository with a wav2letter recipe for a publicly avail-": ""
        },
        {
          "research have been published in a GitHub1\nrepository, aiming": "",
          "public repository with a wav2letter recipe for a publicly avail-": "plus jitter relative,\njitter absolute, shimmer dB and/or shimmer"
        },
        {
          "research have been published in a GitHub1\nrepository, aiming": "authors showed that voice quality features are relevant markers",
          "public repository with a wav2letter recipe for a publicly avail-": ""
        },
        {
          "research have been published in a GitHub1\nrepository, aiming": "",
          "public repository with a wav2letter recipe for a publicly avail-": "relative."
        },
        {
          "research have been published in a GitHub1\nrepository, aiming": "signaling paralinguistic information, and that\nthey should even",
          "public repository with a wav2letter recipe for a publicly avail-": ""
        },
        {
          "research have been published in a GitHub1\nrepository, aiming": "",
          "public repository with a wav2letter recipe for a publicly avail-": "There are various pitch extractor algorithms\nsuch as Yin"
        },
        {
          "research have been published in a GitHub1\nrepository, aiming": "be considered as prosodic carriers along with pitch and dura-",
          "public repository with a wav2letter recipe for a publicly avail-": ""
        },
        {
          "research have been published in a GitHub1\nrepository, aiming": "",
          "public repository with a wav2letter recipe for a publicly avail-": "[27] or getF0 [28]. However, we have decided to refactor the"
        },
        {
          "research have been published in a GitHub1\nrepository, aiming": "tion, for instance.",
          "public repository with a wav2letter recipe for a publicly avail-": ""
        },
        {
          "research have been published in a GitHub1\nrepository, aiming": "",
          "public repository with a wav2letter recipe for a publicly avail-": "Kaldi’s one from [29] within the feature extractor C++ class"
        },
        {
          "research have been published in a GitHub1\nrepository, aiming": "In ASR literature, some works have reported that prosodic",
          "public repository with a wav2letter recipe for a publicly avail-": ""
        },
        {
          "research have been published in a GitHub1\nrepository, aiming": "",
          "public repository with a wav2letter recipe for a publicly avail-": "from wav2letter.\nLatter algorithm has been frequently tested"
        },
        {
          "research have been published in a GitHub1\nrepository, aiming": "information can raise\nthe performance of\nspeech recognizer",
          "public repository with a wav2letter recipe for a publicly avail-": ""
        },
        {
          "research have been published in a GitHub1\nrepository, aiming": "",
          "public repository with a wav2letter recipe for a publicly avail-": "along the recent years within a wide variety of ASR tasks.\nIt"
        },
        {
          "research have been published in a GitHub1\nrepository, aiming": "systems.\nFor\ninstance,\nin [24]\nthe authors built an ASR for",
          "public repository with a wav2letter recipe for a publicly avail-": ""
        },
        {
          "research have been published in a GitHub1\nrepository, aiming": "",
          "public repository with a wav2letter recipe for a publicly avail-": "is inspired by getF0 and ﬁnds the sequence of\nlags that max-"
        },
        {
          "research have been published in a GitHub1\nrepository, aiming": "dysarthric speech and [25]\nreports beneﬁt on the use of\njitter",
          "public repository with a wav2letter recipe for a publicly avail-": ""
        },
        {
          "research have been published in a GitHub1\nrepository, aiming": "",
          "public repository with a wav2letter recipe for a publicly avail-": "imizes the Normalized Cross Correlation Function (NCCF). It"
        },
        {
          "research have been published in a GitHub1\nrepository, aiming": "and shimmer for noisy speech recognition, both systems based",
          "public repository with a wav2letter recipe for a publicly avail-": ""
        },
        {
          "research have been published in a GitHub1\nrepository, aiming": "",
          "public repository with a wav2letter recipe for a publicly avail-": "makes use of\nthe Viterbi algorithm for obtaining the optimal"
        },
        {
          "research have been published in a GitHub1\nrepository, aiming": "on classical HMM acoustic modelling.\nIn the case of deep net-",
          "public repository with a wav2letter recipe for a publicly avail-": ""
        },
        {
          "research have been published in a GitHub1\nrepository, aiming": "",
          "public repository with a wav2letter recipe for a publicly avail-": "lags and,\nin our implementation,\nit applies the logarithm to the"
        },
        {
          "research have been published in a GitHub1\nrepository, aiming": "works, LSTMs was taken into [26] for acoustic emotion recog-",
          "public repository with a wav2letter recipe for a publicly avail-": ""
        },
        {
          "research have been published in a GitHub1\nrepository, aiming": "",
          "public repository with a wav2letter recipe for a publicly avail-": "pitch values as the only post-processing step.\nThe logarithm"
        },
        {
          "research have been published in a GitHub1\nrepository, aiming": "nition task, however they did not perform ASR task on its own.",
          "public repository with a wav2letter recipe for a publicly avail-": ""
        },
        {
          "research have been published in a GitHub1\nrepository, aiming": "",
          "public repository with a wav2letter recipe for a publicly avail-": "compresses pitch values to the same order as the MFSCs, which"
        },
        {
          "research have been published in a GitHub1\nrepository, aiming": "Following previous works, we hypothesize that prosodic",
          "public repository with a wav2letter recipe for a publicly avail-": ""
        },
        {
          "research have been published in a GitHub1\nrepository, aiming": "",
          "public repository with a wav2letter recipe for a publicly avail-": "are compressed by the logarithm as well,\nthus improving nu-"
        },
        {
          "research have been published in a GitHub1\nrepository, aiming": "and voice quality features may boost\nrobustness\nin convolu-",
          "public repository with a wav2letter recipe for a publicly avail-": ""
        },
        {
          "research have been published in a GitHub1\nrepository, aiming": "",
          "public repository with a wav2letter recipe for a publicly avail-": "merical stability later on during the training phase. Subtracting"
        },
        {
          "research have been published in a GitHub1\nrepository, aiming": "tional\nlike ASR systems. Moreover,\nthey could play an even",
          "public repository with a wav2letter recipe for a publicly avail-": ""
        },
        {
          "research have been published in a GitHub1\nrepository, aiming": "",
          "public repository with a wav2letter recipe for a publicly avail-": "the weighted average pitch during post-processing has been dis-"
        },
        {
          "research have been published in a GitHub1\nrepository, aiming": "more important role in further speech tasks,\nincluding punctu-",
          "public repository with a wav2letter recipe for a publicly avail-": ""
        },
        {
          "research have been published in a GitHub1\nrepository, aiming": "",
          "public repository with a wav2letter recipe for a publicly avail-": "carded, since the reported gains in WER by Kaldi are only of a"
        },
        {
          "research have been published in a GitHub1\nrepository, aiming": "ation marks, emotion recognition or musical contexts, where",
          "public repository with a wav2letter recipe for a publicly avail-": ""
        },
        {
          "research have been published in a GitHub1\nrepository, aiming": "",
          "public repository with a wav2letter recipe for a publicly avail-": "0.1%, but we may implement them in future iterations."
        },
        {
          "research have been published in a GitHub1\nrepository, aiming": "additional prosodic information would be useful.",
          "public repository with a wav2letter recipe for a publicly avail-": ""
        },
        {
          "research have been published in a GitHub1\nrepository, aiming": "",
          "public repository with a wav2letter recipe for a publicly avail-": "Shimmer\nis computed measuring the peak-to-peak wave-"
        },
        {
          "research have been published in a GitHub1\nrepository, aiming": "",
          "public repository with a wav2letter recipe for a publicly avail-": "form amplitude at each period where the pitch is extracted, and"
        },
        {
          "research have been published in a GitHub1\nrepository, aiming": "3. METHODOLOGY",
          "public repository with a wav2letter recipe for a publicly avail-": "then performing the corresponding operations, depending on"
        },
        {
          "research have been published in a GitHub1\nrepository, aiming": "",
          "public repository with a wav2letter recipe for a publicly avail-": "whether we deal with shimmer dB or shimmer relative, see ref-"
        },
        {
          "research have been published in a GitHub1\nrepository, aiming": "3.1. Data",
          "public repository with a wav2letter recipe for a publicly avail-": ""
        },
        {
          "research have been published in a GitHub1\nrepository, aiming": "",
          "public repository with a wav2letter recipe for a publicly avail-": "erence [12]. With the pitch extracted at each period,\nthe same"
        },
        {
          "research have been published in a GitHub1\nrepository, aiming": "The effect of adding pitch and voice quality features is evalu-",
          "public repository with a wav2letter recipe for a publicly avail-": "can be done for\njitter absolute and relative, by calculating the"
        },
        {
          "research have been published in a GitHub1\nrepository, aiming": "ated by means of the Common Voice corpus in Spanish [17] and",
          "public repository with a wav2letter recipe for a publicly avail-": "fundamental frequency differences between such cycles."
        },
        {
          "research have been published in a GitHub1\nrepository, aiming": "the LibriSpeech 100h dataset\nin English [18]. Common Voice",
          "public repository with a wav2letter recipe for a publicly avail-": ""
        },
        {
          "research have been published in a GitHub1\nrepository, aiming": "corpus\nis\nan open-source dataset\nthat\nconsists of\nrecordings",
          "public repository with a wav2letter recipe for a publicly avail-": "3.3.\nSystem Architecture"
        },
        {
          "research have been published in a GitHub1\nrepository, aiming": "from volunteer\ncontributors pronouncing scripted sentences,",
          "public repository with a wav2letter recipe for a publicly avail-": ""
        },
        {
          "research have been published in a GitHub1\nrepository, aiming": "",
          "public repository with a wav2letter recipe for a publicly avail-": "Since our purpose is to study how pitch and voice quality fea-"
        },
        {
          "research have been published in a GitHub1\nrepository, aiming": "recorded at 48kHz rate and using own devices. The sentences",
          "public repository with a wav2letter recipe for a publicly avail-": ""
        },
        {
          "research have been published in a GitHub1\nrepository, aiming": "",
          "public repository with a wav2letter recipe for a publicly avail-": "tures contribute to a convolutional acoustic model\n(AM), we"
        },
        {
          "research have been published in a GitHub1\nrepository, aiming": "come from original contributor donations and public domain",
          "public repository with a wav2letter recipe for a publicly avail-": ""
        },
        {
          "research have been published in a GitHub1\nrepository, aiming": "",
          "public repository with a wav2letter recipe for a publicly avail-": "have used the Conv GLU AM from wav2letter’s Wall Street"
        },
        {
          "research have been published in a GitHub1\nrepository, aiming": "movie scripts and it\nis continuously growing. Although there",
          "public repository with a wav2letter recipe for a publicly avail-": ""
        },
        {
          "research have been published in a GitHub1\nrepository, aiming": "",
          "public repository with a wav2letter recipe for a publicly avail-": "Journal (WSJ) recipe [16]. This model has approximately 17M"
        },
        {
          "research have been published in a GitHub1\nrepository, aiming": "are already more than 100 hours of validated audio, we have",
          "public repository with a wav2letter recipe for a publicly avail-": ""
        },
        {
          "research have been published in a GitHub1\nrepository, aiming": "",
          "public repository with a wav2letter recipe for a publicly avail-": "parameters with dropout applied after each of\nits 17 layers."
        },
        {
          "research have been published in a GitHub1\nrepository, aiming": "kept a reduced partition of approximately 19.0 h for\ntraining,",
          "public repository with a wav2letter recipe for a publicly avail-": ""
        },
        {
          "research have been published in a GitHub1\nrepository, aiming": "",
          "public repository with a wav2letter recipe for a publicly avail-": "The WSJ dataset contains around 80 hours of audio recordings,"
        },
        {
          "research have been published in a GitHub1\nrepository, aiming": "2.7 h for development and 2.2 h for testing sets. The main crite-",
          "public repository with a wav2letter recipe for a publicly avail-": ""
        },
        {
          "research have been published in a GitHub1\nrepository, aiming": "",
          "public repository with a wav2letter recipe for a publicly avail-": "which is closer to the magnitude of our data than the full Lib-"
        },
        {
          "research have been published in a GitHub1\nrepository, aiming": "rion for the stratiﬁcation of such partitions is to ensure that each",
          "public repository with a wav2letter recipe for a publicly avail-": ""
        },
        {
          "research have been published in a GitHub1\nrepository, aiming": "",
          "public repository with a wav2letter recipe for a publicly avail-": "riSpeech recipe (about 1000 hours). We have not done an exten-"
        },
        {
          "research have been published in a GitHub1\nrepository, aiming": "one has exclusive speakers, while trying to keep a 80-10-10%",
          "public repository with a wav2letter recipe for a publicly avail-": ""
        },
        {
          "research have been published in a GitHub1\nrepository, aiming": "",
          "public repository with a wav2letter recipe for a publicly avail-": "sive exploration of architecture parameters, since it yields de-"
        },
        {
          "research have been published in a GitHub1\nrepository, aiming": "proportion. Every sample can be down voted by the contribu-",
          "public repository with a wav2letter recipe for a publicly avail-": ""
        },
        {
          "research have been published in a GitHub1\nrepository, aiming": "",
          "public repository with a wav2letter recipe for a publicly avail-": "cent out of the box results with Common Voice and LibriSpeech"
        },
        {
          "research have been published in a GitHub1\nrepository, aiming": "tors if it\nis not clear enough, so we have discarded all samples",
          "public repository with a wav2letter recipe for a publicly avail-": ""
        },
        {
          "research have been published in a GitHub1\nrepository, aiming": "",
          "public repository with a wav2letter recipe for a publicly avail-": "100h data."
        },
        {
          "research have been published in a GitHub1\nrepository, aiming": "containing at\nleast one down vote,\nto keep the cherry picked",
          "public repository with a wav2letter recipe for a publicly avail-": ""
        },
        {
          "research have been published in a GitHub1\nrepository, aiming": "",
          "public repository with a wav2letter recipe for a publicly avail-": "Regarding Common Voice’s lexicon, we use a grapheme-"
        },
        {
          "research have been published in a GitHub1\nrepository, aiming": "recordings as clean as possible. Afterwards, we try to keep as",
          "public repository with a wav2letter recipe for a publicly avail-": ""
        },
        {
          "research have been published in a GitHub1\nrepository, aiming": "",
          "public repository with a wav2letter recipe for a publicly avail-": "based one extracted from the approximately 9000 words from"
        },
        {
          "research have been published in a GitHub1\nrepository, aiming": "balanced as possible the distributions by age, gender and accent.",
          "public repository with a wav2letter recipe for a publicly avail-": ""
        },
        {
          "research have been published in a GitHub1\nrepository, aiming": "",
          "public repository with a wav2letter recipe for a publicly avail-": "both the training and development partitions. We use the stan-"
        },
        {
          "research have been published in a GitHub1\nrepository, aiming": "The Python scripts for obtaining such partition are provided in",
          "public repository with a wav2letter recipe for a publicly avail-": ""
        },
        {
          "research have been published in a GitHub1\nrepository, aiming": "",
          "public repository with a wav2letter recipe for a publicly avail-": "dard Spanish alphabet as tokens, plus the ”c¸” letter from Catalan"
        },
        {
          "research have been published in a GitHub1\nrepository, aiming": "our public Git repository, along with other code necessary to re-",
          "public repository with a wav2letter recipe for a publicly avail-": ""
        },
        {
          "research have been published in a GitHub1\nrepository, aiming": "",
          "public repository with a wav2letter recipe for a publicly avail-": "and the vowels with diacritical marks, making a total of 37 to-"
        },
        {
          "research have been published in a GitHub1\nrepository, aiming": "produce our ASR recipes. Up to our knowledge, this is the ﬁrst",
          "public repository with a wav2letter recipe for a publicly avail-": ""
        },
        {
          "research have been published in a GitHub1\nrepository, aiming": "",
          "public repository with a wav2letter recipe for a publicly avail-": "kens. The ”c¸” character is included because of the presence of"
        },
        {
          "research have been published in a GitHub1\nrepository, aiming": "1https://github.com/gcambara/wav2letter/tree/",
          "public repository with a wav2letter recipe for a publicly avail-": "some Catalan words in the dataset, like ”Barc¸a”. The language"
        },
        {
          "research have been published in a GitHub1\nrepository, aiming": "wav2letter_pitch",
          "public repository with a wav2letter recipe for a publicly avail-": "model (LM) is a 4-gram model extracted with KenLM [30] from"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table 1: WER percentages by augmenting spectral features with prosody and voice quality ones. The results are reported on the",
      "data": [
        {
          "Table 1: WER percentages by augmenting spectral": "Common Voice’s development (Dev) and test (Test) sets, comprising 2.7 hours and 2.2 hours, respectively. Error rates are obtained",
          "features with prosody and voice quality ones.": "",
          "The results are reported on the": ""
        },
        {
          "Table 1: WER percentages by augmenting spectral": "by using a greedy decoding without",
          "features with prosody and voice quality ones.": "",
          "The results are reported on the": ""
        },
        {
          "Table 1: WER percentages by augmenting spectral": "Common Voice’s training subset (CVLM) and on the training partition of the Spanish corpus Fisher-Callhome (FCLM).",
          "features with prosody and voice quality ones.": "",
          "The results are reported on the": ""
        },
        {
          "Table 1: WER percentages by augmenting spectral": "AM",
          "features with prosody and voice quality ones.": "",
          "The results are reported on the": ""
        },
        {
          "Table 1: WER percentages by augmenting spectral": "Features",
          "features with prosody and voice quality ones.": "CVLM-Dev",
          "The results are reported on the": "FCLM-Test"
        },
        {
          "Table 1: WER percentages by augmenting spectral": "MFSC",
          "features with prosody and voice quality ones.": "20.29",
          "The results are reported on the": "44.20"
        },
        {
          "Table 1: WER percentages by augmenting spectral": "+ Pitch",
          "features with prosody and voice quality ones.": "20.56",
          "The results are reported on the": "43.18"
        },
        {
          "Table 1: WER percentages by augmenting spectral": "+ Pitch + Jitter",
          "features with prosody and voice quality ones.": "20.28",
          "The results are reported on the": "43.26"
        },
        {
          "Table 1: WER percentages by augmenting spectral": "+ Pitch + Shimmer",
          "features with prosody and voice quality ones.": "23.30",
          "The results are reported on the": "50.60"
        },
        {
          "Table 1: WER percentages by augmenting spectral": "+ Pitch + Jitter + Shimmer",
          "features with prosody and voice quality ones.": "20.01",
          "The results are reported on the": "42.95"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table 1: WER percentages by augmenting spectral features with prosody and voice quality ones. The results are reported on the",
      "data": [
        {
          "1.38% and 7.36% relative WER points.": ""
        },
        {
          "1.38% and 7.36% relative WER points.": "For the cases without LM, the model with MFSC and pitch"
        },
        {
          "1.38% and 7.36% relative WER points.": ""
        },
        {
          "1.38% and 7.36% relative WER points.": "features is the one with the best performance, with relative gains"
        },
        {
          "1.38% and 7.36% relative WER points.": ""
        },
        {
          "1.38% and 7.36% relative WER points.": "of 2.68% and 1.83% for dev and test sets,\nrespectively. Addi-"
        },
        {
          "1.38% and 7.36% relative WER points.": "tional features on the other models also improve the WER score,"
        },
        {
          "1.38% and 7.36% relative WER points.": "except for the case with pitch and shimmer only, which yields"
        },
        {
          "1.38% and 7.36% relative WER points.": "worse results across all experiments. On the other hand, decod-"
        },
        {
          "1.38% and 7.36% relative WER points.": ""
        },
        {
          "1.38% and 7.36% relative WER points.": "ing with CVLM achieves the best WER scores, when training"
        },
        {
          "1.38% and 7.36% relative WER points.": ""
        },
        {
          "1.38% and 7.36% relative WER points.": "with all the proposed features together: MFSCs, the 3 pitch fea-"
        },
        {
          "1.38% and 7.36% relative WER points.": ""
        },
        {
          "1.38% and 7.36% relative WER points.": "tures,\njitter\nrelative and shimmer\nrelative. A 20.01% WER is"
        },
        {
          "1.38% and 7.36% relative WER points.": "obtained for\nthe dev set, and a 22.90% WER for\nthe test set."
        }
      ],
      "page": 3
    },
    {
      "caption": "Table 1: WER percentages by augmenting spectral features with prosody and voice quality ones. The results are reported on the",
      "data": [
        {
          "73.18\n77.04\n+ Pitch + Shimmer": "64.46\n69.51\n+ Pitch + Jitter + Shimmer",
          "23.30\n25.10\n46.90\n50.60": "20.01\n22.90\n38.63\n42.95"
        },
        {
          "73.18\n77.04\n+ Pitch + Shimmer": "the training set. Since most of the sentences are shared across",
          "23.30\n25.10\n46.90\n50.60": "Furthermore, LibriSpeech WER is\nevaluated with\ndev-"
        },
        {
          "73.18\n77.04\n+ Pitch + Shimmer": "partitions, due to the scripted nature of the dataset, we expected",
          "23.30\n25.10\n46.90\n50.60": "clean/other and test-clean/other partitions, using the same AM"
        },
        {
          "73.18\n77.04\n+ Pitch + Shimmer": "an optimistic behavior after applying such LM. Therefore, we",
          "23.30\n25.10\n46.90\n50.60": "training recipe as in Common Voice. As we are scaling with"
        },
        {
          "73.18\n77.04\n+ Pitch + Shimmer": "are also reporting results given by another 4-gram LM extracted",
          "23.30\n25.10\n46.90\n50.60": "a bigger dataset demanding a higher computational cost,\nthe"
        },
        {
          "73.18\n77.04\n+ Pitch + Shimmer": "from the Spanish Fisher+Callhome. The Fisher corpus splitting",
          "23.30\n25.10\n46.90\n50.60": "top three parameter conﬁgurations found with Common Voice"
        },
        {
          "73.18\n77.04\n+ Pitch + Shimmer": "is taken from the Kaldi’s recipe [31]. Decoding across AM, lex-",
          "23.30\n25.10\n46.90\n50.60": "experiments are selected in order to perform such evaluations."
        },
        {
          "73.18\n77.04\n+ Pitch + Shimmer": "icon and LM is done with the beam-search decoder provided by",
          "23.30\n25.10\n46.90\n50.60": "Decoding parameters are taken from wav2letter’s LibriSpeech"
        },
        {
          "73.18\n77.04\n+ Pitch + Shimmer": "wav2letter [32]. Furthermore, in order to assess the capacity of",
          "23.30\n25.10\n46.90\n50.60": "recipe."
        },
        {
          "73.18\n77.04\n+ Pitch + Shimmer": "the AM by itself, we also evaluate without LM, choosing the",
          "23.30\n25.10\n46.90\n50.60": ""
        },
        {
          "73.18\n77.04\n+ Pitch + Shimmer": "",
          "23.30\n25.10\n46.90\n50.60": "4. RESULTS AND DISCUSSION"
        },
        {
          "73.18\n77.04\n+ Pitch + Shimmer": "ﬁnal characters with the greedy best path from the predictions",
          "23.30\n25.10\n46.90\n50.60": ""
        },
        {
          "73.18\n77.04\n+ Pitch + Shimmer": "of the AM. For the LibriSpeech evaluation, the lexicon and the",
          "23.30\n25.10\n46.90\n50.60": "The Table 1 reports the word error rates (WER, %) for each one"
        },
        {
          "73.18\n77.04\n+ Pitch + Shimmer": "language model are the same as provided by wav2letter’s Conv",
          "23.30\n25.10\n46.90\n50.60": "of\nthe 5 feature conﬁgurations,\nfor\nthe proposed decodings of"
        },
        {
          "73.18\n77.04\n+ Pitch + Shimmer": "GLU LibriSpeech recipe. The lexicon is obtained from the train",
          "23.30\n25.10\n46.90\n50.60": "Common Voice’s dev and test sets, without LM (NoLM), with"
        },
        {
          "73.18\n77.04\n+ Pitch + Shimmer": "corpus and the language model\nis a 4-gram model also trained",
          "23.30\n25.10\n46.90\n50.60": "its own LM (CVLM) and the Fisher+Callhome LM (FCLM)."
        },
        {
          "73.18\n77.04\n+ Pitch + Shimmer": "with KenLM.",
          "23.30\n25.10\n46.90\n50.60": "For every evaluated case,\nthe best WER score is always pro-"
        },
        {
          "73.18\n77.04\n+ Pitch + Shimmer": "",
          "23.30\n25.10\n46.90\n50.60": "vided by one of the models using pitch features, or pitch with"
        },
        {
          "73.18\n77.04\n+ Pitch + Shimmer": "3.4. Experiments",
          "23.30\n25.10\n46.90\n50.60": "voice quality (jitter + shimmer)\nfeatures, with gains between"
        },
        {
          "73.18\n77.04\n+ Pitch + Shimmer": "",
          "23.30\n25.10\n46.90\n50.60": "1.38% and 7.36% relative WER points."
        },
        {
          "73.18\n77.04\n+ Pitch + Shimmer": "After some initial simulations, we have found that the most sta-",
          "23.30\n25.10\n46.90\n50.60": ""
        },
        {
          "73.18\n77.04\n+ Pitch + Shimmer": "",
          "23.30\n25.10\n46.90\n50.60": "For the cases without LM, the model with MFSC and pitch"
        },
        {
          "73.18\n77.04\n+ Pitch + Shimmer": "ble voice quality features are jitter relative and shimmer relative.",
          "23.30\n25.10\n46.90\n50.60": ""
        },
        {
          "73.18\n77.04\n+ Pitch + Shimmer": "",
          "23.30\n25.10\n46.90\n50.60": "features is the one with the best performance, with relative gains"
        },
        {
          "73.18\n77.04\n+ Pitch + Shimmer": "Therefore, we try 5 different feature conﬁgurations:",
          "23.30\n25.10\n46.90\n50.60": ""
        },
        {
          "73.18\n77.04\n+ Pitch + Shimmer": "",
          "23.30\n25.10\n46.90\n50.60": "of 2.68% and 1.83% for dev and test sets,\nrespectively. Addi-"
        },
        {
          "73.18\n77.04\n+ Pitch + Shimmer": "1.\n40 MFSCs only",
          "23.30\n25.10\n46.90\n50.60": "tional features on the other models also improve the WER score,"
        },
        {
          "73.18\n77.04\n+ Pitch + Shimmer": "2.\n40 MFSCs + 3 pitch features",
          "23.30\n25.10\n46.90\n50.60": "except for the case with pitch and shimmer only, which yields"
        },
        {
          "73.18\n77.04\n+ Pitch + Shimmer": "",
          "23.30\n25.10\n46.90\n50.60": "worse results across all experiments. On the other hand, decod-"
        },
        {
          "73.18\n77.04\n+ Pitch + Shimmer": "3.\n40 MFSCs + 3 pitch + 1 relative jitter",
          "23.30\n25.10\n46.90\n50.60": ""
        },
        {
          "73.18\n77.04\n+ Pitch + Shimmer": "",
          "23.30\n25.10\n46.90\n50.60": "ing with CVLM achieves the best WER scores, when training"
        },
        {
          "73.18\n77.04\n+ Pitch + Shimmer": "4.\n40 MFSCs + 3 pitch + 1 rel. shimmer",
          "23.30\n25.10\n46.90\n50.60": ""
        },
        {
          "73.18\n77.04\n+ Pitch + Shimmer": "",
          "23.30\n25.10\n46.90\n50.60": "with all the proposed features together: MFSCs, the 3 pitch fea-"
        },
        {
          "73.18\n77.04\n+ Pitch + Shimmer": "5.\n40 MFSCs + 3 pitch + 1 rel.\njitter + 1 rel. shimmer",
          "23.30\n25.10\n46.90\n50.60": ""
        },
        {
          "73.18\n77.04\n+ Pitch + Shimmer": "",
          "23.30\n25.10\n46.90\n50.60": "tures,\njitter\nrelative and shimmer\nrelative. A 20.01% WER is"
        },
        {
          "73.18\n77.04\n+ Pitch + Shimmer": "For each one, we compute WERs on Common Voice’s dev",
          "23.30\n25.10\n46.90\n50.60": "obtained for\nthe dev set, and a 22.90% WER for\nthe test set."
        },
        {
          "73.18\n77.04\n+ Pitch + Shimmer": "and test sets. Decodings are performed without LM (NoLM),",
          "23.30\n25.10\n46.90\n50.60": "As it was expected, the CVLM improves drastically the predic-"
        },
        {
          "73.18\n77.04\n+ Pitch + Shimmer": "with\nboth\nin-domain\nand\nout-domain LMs,\nfrom Common",
          "23.30\n25.10\n46.90\n50.60": "tions, because even though it is obtained from the train partition"
        },
        {
          "73.18\n77.04\n+ Pitch + Shimmer": "Voice’s LM (CVLM)\nfrom Fisher+Callhome’s LM (FCLM)",
          "23.30\n25.10\n46.90\n50.60": "solely, many sentences are shared with the dev and test sets, due"
        },
        {
          "73.18\n77.04\n+ Pitch + Shimmer": "databases, respectively. Therefore, we obtain 6 WERs for each",
          "23.30\n25.10\n46.90\n50.60": "to the reduced vocabulary in this dataset."
        },
        {
          "73.18\n77.04\n+ Pitch + Shimmer": "one of the 5 feature conﬁgurations.",
          "23.30\n25.10\n46.90\n50.60": "A more realistic approach is to decode by using an external"
        },
        {
          "73.18\n77.04\n+ Pitch + Shimmer": "Besides\nthe features,\nthe training conﬁgurations\nfor each",
          "23.30\n25.10\n46.90\n50.60": "LM. The FCLM language model\nis built from the training par-"
        },
        {
          "73.18\n77.04\n+ Pitch + Shimmer": "experiment are the same, all based on wav2letter’s WSJ recipe.",
          "23.30\n25.10\n46.90\n50.60": "tition of the LDC Spanish Fisher+Callhome corpus. Although"
        },
        {
          "73.18\n77.04\n+ Pitch + Shimmer": "The inferred segmentation is taken out from wav2letter’s Auto",
          "23.30\n25.10\n46.90\n50.60": "the LM enrollment\nis performed with less than 20 hours of au-"
        },
        {
          "73.18\n77.04\n+ Pitch + Shimmer": "Segmentation Criterion (ASG) [16], inspired by CTC loss [33].",
          "23.30\n25.10\n46.90\n50.60": "dio (approximately 16k sentences), it still yields to a reasonable"
        },
        {
          "73.18\n77.04\n+ Pitch + Shimmer": "The learning rate is tweaked to 7.3, and is decayed in a 20% ev-",
          "23.30\n25.10\n46.90\n50.60": "performance compared to the CVLMs decodings. With respect"
        },
        {
          "73.18\n77.04\n+ Pitch + Shimmer": "ery 10 epochs, a tuning done with the dev set. A 25 ms rolling",
          "23.30\n25.10\n46.90\n50.60": "to the prosodic features, the FCLM beam decoding reaches the"
        },
        {
          "73.18\n77.04\n+ Pitch + Shimmer": "window with a 10 ms stride is used for extracting all\nthe fea-",
          "23.30\n25.10\n46.90\n50.60": "lower WER in development by using MFSCs only augmented"
        },
        {
          "73.18\n77.04\n+ Pitch + Shimmer": "tures, jitter and shimmer are averaged across 500 ms windows.",
          "23.30\n25.10\n46.90\n50.60": "with pitch features;\nthat\nis, 37.57% WER. The lowest 42.95%"
        },
        {
          "73.18\n77.04\n+ Pitch + Shimmer": "For beam-search decoding, the following settings are tuned",
          "23.30\n25.10\n46.90\n50.60": "WER score in the test set is given by the combination of all pitch"
        },
        {
          "73.18\n77.04\n+ Pitch + Shimmer": "with the dev set: LM weight set to 2.5, word score set to 1, beam",
          "23.30\n25.10\n46.90\n50.60": "and voice quality characteristics. Once again,\nthe best\nresults"
        },
        {
          "73.18\n77.04\n+ Pitch + Shimmer": "size set to 2500, beam threshold set to 25 and silence weight set",
          "23.30\n25.10\n46.90\n50.60": "in terms of WER are provided by models with pitch features,"
        },
        {
          "73.18\n77.04\n+ Pitch + Shimmer": "to -0.4.\nIn order\nto tune these, we have not\nrun an extensive",
          "23.30\n25.10\n46.90\n50.60": "or pitch features with the combination of\njitter and shimmer,"
        },
        {
          "73.18\n77.04\n+ Pitch + Shimmer": "exploration of hyperparameters, but after a shallow search we",
          "23.30\n25.10\n46.90\n50.60": "showing the potential of pitch and voice quality features to im-"
        },
        {
          "73.18\n77.04\n+ Pitch + Shimmer": "found these to provide good results for both LMs.",
          "23.30\n25.10\n46.90\n50.60": "prove the performance of an ASR based on convolutional neural"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Table 2: LibriSpeech WER values for the three best performing": "combinations of features proposed in the Acoustic Model (AM)"
        },
        {
          "Table 2: LibriSpeech WER values for the three best performing": "in the Common Voice experiments: MFSC, MFSC+Pitch and"
        },
        {
          "Table 2: LibriSpeech WER values for the three best performing": "MFSC+Pitch+Shimmer+Jitter (shortened as ”All”). Decoding"
        },
        {
          "Table 2: LibriSpeech WER values for the three best performing": "is done with a 4-gram LM trained with LibriSpeech train set"
        },
        {
          "Table 2: LibriSpeech WER values for the three best performing": "transcripts."
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "AM\nWER (%)": "Features\ndev-clean\ndev-other\ntest-clean\ntest-other"
        },
        {
          "AM\nWER (%)": "10.22\n31.59\n10.38\n34.46\nMFSC"
        },
        {
          "AM\nWER (%)": ""
        },
        {
          "AM\nWER (%)": "9.94\n30.95\n10.28\n33.47\n+ Pitch"
        },
        {
          "AM\nWER (%)": ""
        },
        {
          "AM\nWER (%)": ""
        },
        {
          "AM\nWER (%)": "9.92\n30.94\n10.37\n33.57\n+ All"
        },
        {
          "AM\nWER (%)": ""
        },
        {
          "AM\nWER (%)": ""
        },
        {
          "AM\nWER (%)": "performance of\nthe convolutional acoustic model\nfor both the"
        },
        {
          "AM\nWER (%)": "different databases and the languages studied in this work."
        },
        {
          "AM\nWER (%)": ""
        },
        {
          "AM\nWER (%)": "Note that\nthe approach employed for\nthe feature combi-"
        },
        {
          "AM\nWER (%)": ""
        },
        {
          "AM\nWER (%)": "nation has been quite simple, by just appending such features"
        },
        {
          "AM\nWER (%)": ""
        },
        {
          "AM\nWER (%)": "to the spectral ones at\nthe input\nlayer, without extensive post-"
        },
        {
          "AM\nWER (%)": ""
        },
        {
          "AM\nWER (%)": "processing either nor adaptation of the model architecture. Be-"
        },
        {
          "AM\nWER (%)": ""
        },
        {
          "AM\nWER (%)": "ing so,\nit\nis reasonable to think that\nthere is still margin of im-"
        },
        {
          "AM\nWER (%)": ""
        },
        {
          "AM\nWER (%)": "provement in the application of pitch and voice quality measure-"
        },
        {
          "AM\nWER (%)": ""
        },
        {
          "AM\nWER (%)": "ments to state-of-the-art convolutional neural models. Possible"
        },
        {
          "AM\nWER (%)": ""
        },
        {
          "AM\nWER (%)": "strategies comprises adapting the feature concatenation, maybe"
        },
        {
          "AM\nWER (%)": ""
        },
        {
          "AM\nWER (%)": "by dedicating exclusive ﬁlters to the new pitch and voice qual-"
        },
        {
          "AM\nWER (%)": ""
        },
        {
          "AM\nWER (%)": "ity features with POV as a gating mechanism, especially after"
        },
        {
          "AM\nWER (%)": ""
        },
        {
          "AM\nWER (%)": "experimentally realising, not reported in this work, that the esti-"
        },
        {
          "AM\nWER (%)": ""
        },
        {
          "AM\nWER (%)": "mation of measurements like shimmer may beneﬁt from differ-"
        },
        {
          "AM\nWER (%)": ""
        },
        {
          "AM\nWER (%)": "ent post-processing techniques."
        },
        {
          "AM\nWER (%)": ""
        },
        {
          "AM\nWER (%)": ""
        },
        {
          "AM\nWER (%)": "5. CONCLUSIONS"
        },
        {
          "AM\nWER (%)": ""
        },
        {
          "AM\nWER (%)": "In this study, we performed a preliminary exploration on the"
        },
        {
          "AM\nWER (%)": "effects of pitch and jitter/shimmer voice quality measurements"
        },
        {
          "AM\nWER (%)": "within the framework of the ASR task performed by convolu-"
        },
        {
          "AM\nWER (%)": "tional neural network models. The experiments reported with a"
        },
        {
          "AM\nWER (%)": "publicly available Spanish speech corpus showed consistent im-"
        },
        {
          "AM\nWER (%)": "provements on the model robustness, achieving a reduced rela-"
        },
        {
          "AM\nWER (%)": "tive 7% WER in some scenarios. Besides, such feature extrac-"
        },
        {
          "AM\nWER (%)": "tion functionalities are provided and integrated with wav2letter"
        },
        {
          "AM\nWER (%)": "code to easily replicate our ﬁndings or directly applying pitch"
        },
        {
          "AM\nWER (%)": "and voice quality features to wav2letter models. We also pro-"
        },
        {
          "AM\nWER (%)": "vide the recipe for the Common Voice Spanish dataset, the ﬁrst"
        },
        {
          "AM\nWER (%)": "recipe suited for wav2letter using a Spanish publicly available"
        },
        {
          "AM\nWER (%)": "dataset.\nThe recipe for LibriSpeech experiments is also pro-"
        },
        {
          "AM\nWER (%)": "vided, which achieves up to a 2.94% relative WER improve-"
        },
        {
          "AM\nWER (%)": "ment. Further steps on the research of convolutional ASR with"
        },
        {
          "AM\nWER (%)": ""
        },
        {
          "AM\nWER (%)": "pitch and voice quality would imply adapting architectures for"
        },
        {
          "AM\nWER (%)": ""
        },
        {
          "AM\nWER (%)": "feature processing, or applying such characteristics for tasks in-"
        },
        {
          "AM\nWER (%)": ""
        },
        {
          "AM\nWER (%)": "cluding the presence of punctuation marks, emotion recognition"
        },
        {
          "AM\nWER (%)": ""
        },
        {
          "AM\nWER (%)": "and even pathological or singing voices. For the latter tasks, the"
        },
        {
          "AM\nWER (%)": ""
        },
        {
          "AM\nWER (%)": "importance of pitch and voice quality features is expected to"
        },
        {
          "AM\nWER (%)": ""
        },
        {
          "AM\nWER (%)": "become even more relevant."
        },
        {
          "AM\nWER (%)": ""
        },
        {
          "AM\nWER (%)": ""
        },
        {
          "AM\nWER (%)": "6. ACKNOWLEDGMENTS"
        },
        {
          "AM\nWER (%)": ""
        },
        {
          "AM\nWER (%)": "This work is a part of the INGENIOUS project, funded by the"
        },
        {
          "AM\nWER (%)": "European Union’s Horizon 2020 Research and Innovation Pro-"
        },
        {
          "AM\nWER (%)": "gramme and the Korean Government under Grant Agreement"
        },
        {
          "AM\nWER (%)": "No 833435.\nThe third author has been funded by the Agen-"
        },
        {
          "AM\nWER (%)": "cia Estatal de Investigaci´on (AEI), Ministerio de Ciencia,\nIn-"
        },
        {
          "AM\nWER (%)": "novaci´on y Universidades and the Fondo Social Europeo (FSE)"
        },
        {
          "AM\nWER (%)": "under grant RYC-2015-17239 (AEI/FSE, UE)."
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "7. REFERENCES": "",
          "[17] R. Ardila, M. Branson, K. Davis, M. Henretty, M. Kohler,": "J. Meyer, R. Morais, L. Saunders, F. M. Tyers, and G. Weber,"
        },
        {
          "7. REFERENCES": "[1] Y. Wang, A. Mohamed, D. Le, C. Liu, A. Xiao, J. Mahadeokar,",
          "[17] R. Ardila, M. Branson, K. Davis, M. Henretty, M. Kohler,": ""
        },
        {
          "7. REFERENCES": "",
          "[17] R. Ardila, M. Branson, K. Davis, M. Henretty, M. Kohler,": "“Common voice: A massively-multilingual speech corpus,” 2019."
        },
        {
          "7. REFERENCES": "H. Huang, A. Tjandra, X. Zhang, F. Zhang et al., “Transformer-",
          "[17] R. Ardila, M. Branson, K. Davis, M. Henretty, M. Kohler,": ""
        },
        {
          "7. REFERENCES": "based\nacoustic modeling\nfor\nhybrid\nspeech\nrecognition,”\nin",
          "[17] R. Ardila, M. Branson, K. Davis, M. Henretty, M. Kohler,": "[18] V. Panayotov, G. Chen, D. Povey,\nand S. Khudanpur,\n“Lib-"
        },
        {
          "7. REFERENCES": "ICASSP 2020-2020 IEEE International Conference on Acoustics,",
          "[17] R. Ardila, M. Branson, K. Davis, M. Henretty, M. Kohler,": "rispeech: An asr corpus based on public domain audio books,”"
        },
        {
          "7. REFERENCES": "Speech and Signal Processing (ICASSP).\nIEEE, 2020, pp. 6874–",
          "[17] R. Ardila, M. Branson, K. Davis, M. Henretty, M. Kohler,": "04 2015, pp. 5206–5210."
        },
        {
          "7. REFERENCES": "6878.",
          "[17] R. Ardila, M. Branson, K. Davis, M. Henretty, M. Kohler,": ""
        },
        {
          "7. REFERENCES": "",
          "[17] R. Ardila, M. Branson, K. Davis, M. Henretty, M. Kohler,": "[19]\nJ. Guglani and A. Mishra,\n“Automatic speech recognition sys-"
        },
        {
          "7. REFERENCES": "[2] W. Chan, N. Jaitly, Q. V. Le, and O. Vinyals, “Listen, attend and",
          "[17] R. Ardila, M. Branson, K. Davis, M. Henretty, M. Kohler,": "tem with pitch dependent features for punjabi\nlanguage on kaldi"
        },
        {
          "7. REFERENCES": "spell,” CoRR, vol.\nabs/1508.01211,\n2015.\n[Online]. Available:",
          "[17] R. Ardila, M. Branson, K. Davis, M. Henretty, M. Kohler,": "toolkit,” Applied Acoustics, vol. 167, p. 107386, 2020."
        },
        {
          "7. REFERENCES": "http://arxiv.org/abs/1508.01211",
          "[17] R. Ardila, M. Branson, K. Davis, M. Henretty, M. Kohler,": ""
        },
        {
          "7. REFERENCES": "",
          "[17] R. Ardila, M. Branson, K. Davis, M. Henretty, M. Kohler,": "[20] M. Magimai-Doss, T. Stephenson, and H. Bourlard, “Using pitch"
        },
        {
          "7. REFERENCES": "[3] D. S. Park, W. Chan, Y. Zhang, C.-C. Chiu, B. Zoph, E. D.",
          "[17] R. Ardila, M. Branson, K. Davis, M. Henretty, M. Kohler,": "frequency information in speech recognition,” 01 2003."
        },
        {
          "7. REFERENCES": "Cubuk,\nand Q. V. Le,\n“Specaugment: A simple data augmen-",
          "[17] R. Ardila, M. Branson, K. Davis, M. Henretty, M. Kohler,": ""
        },
        {
          "7. REFERENCES": "",
          "[17] R. Ardila, M. Branson, K. Davis, M. Henretty, M. Kohler,": "[21]\nJ. Kreiman and B. R. Gerratt, “Perception of aperiodicity in patho-"
        },
        {
          "7. REFERENCES": "tation method for automatic speech recognition,” arXiv preprint",
          "[17] R. Ardila, M. Branson, K. Davis, M. Henretty, M. Kohler,": ""
        },
        {
          "7. REFERENCES": "",
          "[17] R. Ardila, M. Branson, K. Davis, M. Henretty, M. Kohler,": "logical voice,” The Journal of\nthe Acoustical Society of America,"
        },
        {
          "7. REFERENCES": "arXiv:1904.08779, 2019.",
          "[17] R. Ardila, M. Branson, K. Davis, M. Henretty, M. Kohler,": ""
        },
        {
          "7. REFERENCES": "",
          "[17] R. Ardila, M. Branson, K. Davis, M. Henretty, M. Kohler,": "vol. 117, no. 4, pp. 2201–2211, 2005."
        },
        {
          "7. REFERENCES": "[4] G. Synnaeve, Q. Xu,\nJ. Kahn, T. Likhomanenko, E. Grave,",
          "[17] R. Ardila, M. Branson, K. Davis, M. Henretty, M. Kohler,": ""
        },
        {
          "7. REFERENCES": "",
          "[17] R. Ardila, M. Branson, K. Davis, M. Henretty, M. Kohler,": "[22] D. Michaelis, M. Fr¨ohlich, H. W. Strube, E. Kruse, B. Story, and"
        },
        {
          "7. REFERENCES": "V\n. Pratap, A. Sriram, V. Liptchinsky, and R. Collobert, “End-to-",
          "[17] R. Ardila, M. Branson, K. Davis, M. Henretty, M. Kohler,": ""
        },
        {
          "7. REFERENCES": "",
          "[17] R. Ardila, M. Branson, K. Davis, M. Henretty, M. Kohler,": "I. R. Titze, “Some simulations concerning jitter and shimmer mea-"
        },
        {
          "7. REFERENCES": "end asr: from supervised to semi-supervised learning with modern",
          "[17] R. Ardila, M. Branson, K. Davis, M. Henretty, M. Kohler,": ""
        },
        {
          "7. REFERENCES": "",
          "[17] R. Ardila, M. Branson, K. Davis, M. Henretty, M. Kohler,": "surement,” in 3rd International Workshop on Advances in Quan-"
        },
        {
          "7. REFERENCES": "architectures,” 2019.",
          "[17] R. Ardila, M. Branson, K. Davis, M. Henretty, M. Kohler,": ""
        },
        {
          "7. REFERENCES": "",
          "[17] R. Ardila, M. Branson, K. Davis, M. Henretty, M. Kohler,": "titative Laryngoscopy, Aachen, Germany, 1998, pp. 744–754."
        },
        {
          "7. REFERENCES": "[5] N. Zeghidour, Q. Xu, V. Liptchinsky, N. Usunier, G. Synnaeve,",
          "[17] R. Ardila, M. Branson, K. Davis, M. Henretty, M. Kohler,": ""
        },
        {
          "7. REFERENCES": "and R. Collobert, “Fully convolutional speech recognition,” 2018.",
          "[17] R. Ardila, M. Branson, K. Davis, M. Henretty, M. Kohler,": "[23] N. Campbell and P. Mokhtari, “Voice quality:\nthe 4th prosodic"
        },
        {
          "7. REFERENCES": "",
          "[17] R. Ardila, M. Branson, K. Davis, M. Henretty, M. Kohler,": "dimension,” pp. 2417–2420, 2003."
        },
        {
          "7. REFERENCES": "[6] Y. N. Dauphin, A. Fan, M. Auli,\nand D. Grangier,\n“Language",
          "[17] R. Ardila, M. Branson, K. Davis, M. Henretty, M. Kohler,": ""
        },
        {
          "7. REFERENCES": "modeling\nwith\ngated\nconvolutional\nnetworks,”\nCoRR,\nvol.",
          "[17] R. Ardila, M. Branson, K. Davis, M. Henretty, M. Kohler,": "[24] B.-F. Zaidi, M. Boudraa, S.-A. Selouani, D. Addou, and M. S."
        },
        {
          "7. REFERENCES": "abs/1612.08083, 2016.\n[Online]. Available:\nhttp://arxiv.org/abs/",
          "[17] R. Ardila, M. Branson, K. Davis, M. Henretty, M. Kohler,": "Yakoub,\n“Automatic\nrecognition system for dysarthric\nspeech"
        },
        {
          "7. REFERENCES": "1612.08083",
          "[17] R. Ardila, M. Branson, K. Davis, M. Henretty, M. Kohler,": "based on mfcc’s, pncc’s,\njitter and shimmer coefﬁcients,” in Sci-"
        },
        {
          "7. REFERENCES": "",
          "[17] R. Ardila, M. Branson, K. Davis, M. Henretty, M. Kohler,": "ence and Information Conference.\nSpringer, 2019, pp. 500–510."
        },
        {
          "7. REFERENCES": "[7] V. Pratap, A. Hannun, Q. Xu,\nJ. Cai,\nJ. Kahn, G. Synnaeve,",
          "[17] R. Ardila, M. Branson, K. Davis, M. Henretty, M. Kohler,": ""
        },
        {
          "7. REFERENCES": "V\n. Liptchinsky,\nand R. Collobert,\n“wav2letter++:\nThe\nfastest",
          "[17] R. Ardila, M. Branson, K. Davis, M. Henretty, M. Kohler,": "[25] H. Rahali, Z. Hajaiej, and N. Ellouze, “Robust features for noisy"
        },
        {
          "7. REFERENCES": "open-source speech recognition system,” 12 2018.",
          "[17] R. Ardila, M. Branson, K. Davis, M. Henretty, M. Kohler,": "speech recognition using jitter and shimmer,” International Jour-"
        },
        {
          "7. REFERENCES": "[8] D. Povey, A. Ghoshal, G. Boulianne, L. Burget, O. Glembek,",
          "[17] R. Ardila, M. Branson, K. Davis, M. Henretty, M. Kohler,": "nal of\nInnovative Computing,\nInformation and Control, vol. 11,"
        },
        {
          "7. REFERENCES": "N. Goel, M. Hannemann,\nP. Motl´ıˇcek, Y. Qian,\nP. Schwarz,",
          "[17] R. Ardila, M. Branson, K. Davis, M. Henretty, M. Kohler,": "pp. 955–963, 01 2015."
        },
        {
          "7. REFERENCES": "J. Silovsk´y, G. Stemmer, and K. Vesel, “The kaldi speech recog-",
          "[17] R. Ardila, M. Branson, K. Davis, M. Henretty, M. Kohler,": ""
        },
        {
          "7. REFERENCES": "",
          "[17] R. Ardila, M. Branson, K. Davis, M. Henretty, M. Kohler,": "[26]\nJ. Cho, R. Pappagari, P. Kulkarni,\nJ. Villalba, Y. Carmiel, and"
        },
        {
          "7. REFERENCES": "nition toolkit,” IEEE 2011 Workshop on Automatic Speech Recog-",
          "[17] R. Ardila, M. Branson, K. Davis, M. Henretty, M. Kohler,": ""
        },
        {
          "7. REFERENCES": "",
          "[17] R. Ardila, M. Branson, K. Davis, M. Henretty, M. Kohler,": "N. Dehak, “Deep neural networks for emotion recognition com-"
        },
        {
          "7. REFERENCES": "nition and Understanding, 01 2011.",
          "[17] R. Ardila, M. Branson, K. Davis, M. Henretty, M. Kohler,": ""
        },
        {
          "7. REFERENCES": "",
          "[17] R. Ardila, M. Branson, K. Davis, M. Henretty, M. Kohler,": "bining audio and transcripts,” 2019."
        },
        {
          "7. REFERENCES": "[9] R. E. Slyh, W. T. Nelson, and E. G. Hansen, “Analysis of mrate,",
          "[17] R. Ardila, M. Branson, K. Davis, M. Henretty, M. Kohler,": ""
        },
        {
          "7. REFERENCES": "",
          "[17] R. Ardila, M. Branson, K. Davis, M. Henretty, M. Kohler,": "[27] A. de Cheveign´e and H. Kawahara, “Yin, a fundamental frequency"
        },
        {
          "7. REFERENCES": "shimmer,\njitter,\nand f/sub 0/contour\nfeatures\nacross\nstress\nand",
          "[17] R. Ardila, M. Branson, K. Davis, M. Henretty, M. Kohler,": ""
        },
        {
          "7. REFERENCES": "",
          "[17] R. Ardila, M. Branson, K. Davis, M. Henretty, M. Kohler,": "the Acoustical\nestimator\nfor speech and music,” The Journal of"
        },
        {
          "7. REFERENCES": "speaking style in the susas database,” in 1999 IEEE International",
          "[17] R. Ardila, M. Branson, K. Davis, M. Henretty, M. Kohler,": ""
        },
        {
          "7. REFERENCES": "",
          "[17] R. Ardila, M. Branson, K. Davis, M. Henretty, M. Kohler,": "Society of America, vol. 111, no. 4, pp. 1917–1930, 2002."
        },
        {
          "7. REFERENCES": "Conference on Acoustics, Speech, and Signal Processing. Pro-",
          "[17] R. Ardila, M. Branson, K. Davis, M. Henretty, M. Kohler,": ""
        },
        {
          "7. REFERENCES": "ceedings. ICASSP99 (Cat. No. 99CH36258), vol. 4.\nIEEE, 1999,",
          "[17] R. Ardila, M. Branson, K. Davis, M. Henretty, M. Kohler,": "[28] D. Talkin, “A robust algorithm for pitch tracking ( rapt ),” 2005."
        },
        {
          "7. REFERENCES": "pp. 2091–2094.",
          "[17] R. Ardila, M. Branson, K. Davis, M. Henretty, M. Kohler,": ""
        },
        {
          "7. REFERENCES": "",
          "[17] R. Ardila, M. Branson, K. Davis, M. Henretty, M. Kohler,": "[29]\nP. Ghahremani, B. BabaAli, D. Povey, K. Riedhammer, J. Trmal,"
        },
        {
          "7. REFERENCES": "[10]\nF. Wittig and C. M¨uller, “Implicit feedback for user-adaptive sys-",
          "[17] R. Ardila, M. Branson, K. Davis, M. Henretty, M. Kohler,": "and S. Khudanpur, “A pitch extraction algorithm tuned for auto-"
        },
        {
          "7. REFERENCES": "tems by analyzing the users’ speech,” in Proceedings of the Work-",
          "[17] R. Ardila, M. Branson, K. Davis, M. Henretty, M. Kohler,": "matic speech recognition,” 2014 IEEE International Conference"
        },
        {
          "7. REFERENCES": "shop on Adaptivit¨at und Benutzermodellierung in interaktiven",
          "[17] R. Ardila, M. Branson, K. Davis, M. Henretty, M. Kohler,": "on Acoustics, Speech and Signal Processing (ICASSP), pp. 2494–"
        },
        {
          "7. REFERENCES": "Softwaresystemen (ABIS), Karlsruhe, Germany, 2003.",
          "[17] R. Ardila, M. Branson, K. Davis, M. Henretty, M. Kohler,": "2498, 2014."
        },
        {
          "7. REFERENCES": "[11] X. Li, J. Tao, M. T. Johnson, J. Soltis, A. Savage, K. M. Leong,",
          "[17] R. Ardila, M. Branson, K. Davis, M. Henretty, M. Kohler,": ""
        },
        {
          "7. REFERENCES": "",
          "[17] R. Ardila, M. Branson, K. Davis, M. Henretty, M. Kohler,": "[30] K. Heaﬁeld, “Kenlm: Faster and smaller language model queries,”"
        },
        {
          "7. REFERENCES": "and J. D. Newman, “Stress and emotion classiﬁcation using jitter",
          "[17] R. Ardila, M. Branson, K. Davis, M. Henretty, M. Kohler,": ""
        },
        {
          "7. REFERENCES": "",
          "[17] R. Ardila, M. Branson, K. Davis, M. Henretty, M. Kohler,": "the Sixth Workshop on Statistical Machine\nin Proceedings of"
        },
        {
          "7. REFERENCES": "and shimmer\nfeatures,” in 2007 IEEE International Conference",
          "[17] R. Ardila, M. Branson, K. Davis, M. Henretty, M. Kohler,": ""
        },
        {
          "7. REFERENCES": "",
          "[17] R. Ardila, M. Branson, K. Davis, M. Henretty, M. Kohler,": "Translation,\nser. WMT ’11.\nUSA: Association for Computa-"
        },
        {
          "7. REFERENCES": "on Acoustics, Speech and Signal Processing-ICASSP’07, vol. 4.",
          "[17] R. Ardila, M. Branson, K. Davis, M. Henretty, M. Kohler,": ""
        },
        {
          "7. REFERENCES": "",
          "[17] R. Ardila, M. Branson, K. Davis, M. Henretty, M. Kohler,": "tional Linguistics, 2011, p. 187–197."
        },
        {
          "7. REFERENCES": "IEEE, 2007, pp. IV–1081.",
          "[17] R. Ardila, M. Branson, K. Davis, M. Henretty, M. Kohler,": ""
        },
        {
          "7. REFERENCES": "",
          "[17] R. Ardila, M. Branson, K. Davis, M. Henretty, M. Kohler,": "[31] R.\nJ. Weiss,\nJ. Chorowski, N.\nJaitly, Y. Wu,\nand Z. Chen,"
        },
        {
          "7. REFERENCES": "[12] M. Farr´us, J. Hernando, and P. Ejarque, “Jitter and shimmer mea-",
          "[17] R. Ardila, M. Branson, K. Davis, M. Henretty, M. Kohler,": ""
        },
        {
          "7. REFERENCES": "",
          "[17] R. Ardila, M. Branson, K. Davis, M. Henretty, M. Kohler,": "“Sequence-to-sequence models\ncan\ndirectly\ntranslate\nforeign"
        },
        {
          "7. REFERENCES": "the Inter-\nsurements for speaker\nrecognition,” in Proceedings of",
          "[17] R. Ardila, M. Branson, K. Davis, M. Henretty, M. Kohler,": ""
        },
        {
          "7. REFERENCES": "",
          "[17] R. Ardila, M. Branson, K. Davis, M. Henretty, M. Kohler,": "speech,” 2017."
        },
        {
          "7. REFERENCES": "speech, Antwerp, Belgium, 2007.",
          "[17] R. Ardila, M. Branson, K. Davis, M. Henretty, M. Kohler,": ""
        },
        {
          "7. REFERENCES": "",
          "[17] R. Ardila, M. Branson, K. Davis, M. Henretty, M. Kohler,": "[32] V.\nLiptchinsky,\nG.\nSynnaeve,\nand\nR.\nCollobert,\n“Letter-"
        },
        {
          "7. REFERENCES": "[13] A. W. Zewoudie, J. Luque, and F. J. Hernando Peric´as, “Jitter and",
          "[17] R. Ardila, M. Branson, K. Davis, M. Henretty, M. Kohler,": ""
        },
        {
          "7. REFERENCES": "",
          "[17] R. Ardila, M. Branson, K. Davis, M. Henretty, M. Kohler,": "based\nspeech\nrecognition with\ngated\nconvnets,”\nArXiv,\nvol."
        },
        {
          "7. REFERENCES": "shimmer measurements for speaker diarization,” in VII Jornadas",
          "[17] R. Ardila, M. Branson, K. Davis, M. Henretty, M. Kohler,": ""
        },
        {
          "7. REFERENCES": "",
          "[17] R. Ardila, M. Branson, K. Davis, M. Henretty, M. Kohler,": "abs/1712.09444, 2017."
        },
        {
          "7. REFERENCES": "en Tecnolog´ıa del Habla and III Iberian SLTech Workshop: pro-",
          "[17] R. Ardila, M. Branson, K. Davis, M. Henretty, M. Kohler,": ""
        },
        {
          "7. REFERENCES": "ceedings: November 19-21, 2014: Escuela de Ingenier´ıa en Tele-",
          "[17] R. Ardila, M. Branson, K. Davis, M. Henretty, M. Kohler,": "[33] A. Graves, S. Fern´andez, F. Gomez, and J. Schmidhuber, “Con-"
        },
        {
          "7. REFERENCES": "comunicaci´on y Electr´onica Universidad de Las Palmas de Gran",
          "[17] R. Ardila, M. Branson, K. Davis, M. Henretty, M. Kohler,": "nectionist\ntemporal\nclassiﬁcation:\nLabelling\nunsegmented\nse-"
        },
        {
          "7. REFERENCES": "Canaria: Las Palmas de Gran Canaria, Spain, 2014, pp. 21–30.",
          "[17] R. Ardila, M. Branson, K. Davis, M. Henretty, M. Kohler,": "quence data with recurrent neural networks.”\nNew York, NY,"
        },
        {
          "7. REFERENCES": "",
          "[17] R. Ardila, M. Branson, K. Davis, M. Henretty, M. Kohler,": "USA: Association for Computing Machinery, 2006."
        },
        {
          "7. REFERENCES": "[14]\nS. Mirzaei, M.\nEl Yacoubi,\nS. Garcia-Salicetti,\nJ. Boudy,",
          "[17] R. Ardila, M. Branson, K. Davis, M. Henretty, M. Kohler,": ""
        },
        {
          "7. REFERENCES": "C. Kahindo, V. Cristancho-Lacroix, H. Kerherv´e,\nand A.-S.",
          "[17] R. Ardila, M. Branson, K. Davis, M. Henretty, M. Kohler,": "[34] A. Schindler, F. Mozzanica, M. Vedrody, P. Maruzzi, and F. Ot-"
        },
        {
          "7. REFERENCES": "Rigaud, “Two-stage feature selection of voice parameters for early",
          "[17] R. Ardila, M. Branson, K. Davis, M. Henretty, M. Kohler,": "taviani, “Correlation between the voice handicap index and voice"
        },
        {
          "7. REFERENCES": "alzheimer’s disease prediction,” IRBM, vol. 39, no. 6, pp. 430–",
          "[17] R. Ardila, M. Branson, K. Davis, M. Henretty, M. Kohler,": "measurements in four groups of patients with dysphonia,” Oto-"
        },
        {
          "7. REFERENCES": "435, 2018.",
          "[17] R. Ardila, M. Branson, K. Davis, M. Henretty, M. Kohler,": "laryngology–Head and Neck Surgery, vol. 141, no. 6, pp. 762–"
        },
        {
          "7. REFERENCES": "[15] A. Benba, A. Jilbab, and A. Hammouch, “Hybridization of best",
          "[17] R. Ardila, M. Branson, K. Davis, M. Henretty, M. Kohler,": "769, 2009."
        },
        {
          "7. REFERENCES": "acoustic cues for detecting persons with parkinson’s disease,” in",
          "[17] R. Ardila, M. Branson, K. Davis, M. Henretty, M. Kohler,": ""
        },
        {
          "7. REFERENCES": "",
          "[17] R. Ardila, M. Branson, K. Davis, M. Henretty, M. Kohler,": "[35]\nI. C. Yadav, S. Shahnawazuddin, and G. Pradhan, “Addressing"
        },
        {
          "7. REFERENCES": "2014 Second World Conference on Complex Systems\n(WCCS).",
          "[17] R. Ardila, M. Branson, K. Davis, M. Henretty, M. Kohler,": ""
        },
        {
          "7. REFERENCES": "",
          "[17] R. Ardila, M. Branson, K. Davis, M. Henretty, M. Kohler,": "noise and pitch sensitivity of speech recognition system through"
        },
        {
          "7. REFERENCES": "IEEE, 2014, pp. 622–625.",
          "[17] R. Ardila, M. Branson, K. Davis, M. Henretty, M. Kohler,": ""
        },
        {
          "7. REFERENCES": "",
          "[17] R. Ardila, M. Branson, K. Davis, M. Henretty, M. Kohler,": "variational mode decomposition based spectral smoothing,” Digit."
        },
        {
          "7. REFERENCES": "[16] R. Collobert, C. Puhrsch,\nand G. Synnaeve,\n“Wav2letter:\nan",
          "[17] R. Ardila, M. Branson, K. Davis, M. Henretty, M. Kohler,": "Signal Process., vol. 86, pp. 55–64, 2019.\n[Online]. Available:"
        },
        {
          "7. REFERENCES": "end-to-end\nconvnet-based\nspeech\nrecognition\nsystem,” CoRR,",
          "[17] R. Ardila, M. Branson, K. Davis, M. Henretty, M. Kohler,": "https://doi.org/10.1016/j.dsp.2018.12.013"
        },
        {
          "7. REFERENCES": "vol. abs/1609.03193, 2016.\n[Online]. Available: http://arxiv.org/",
          "[17] R. Ardila, M. Branson, K. Davis, M. Henretty, M. Kohler,": ""
        },
        {
          "7. REFERENCES": "abs/1609.03193",
          "[17] R. Ardila, M. Branson, K. Davis, M. Henretty, M. Kohler,": ""
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Transformerbased acoustic modeling for hybrid speech recognition",
      "authors": [
        "Y Wang",
        "A Mohamed",
        "D Le",
        "C Liu",
        "A Xiao",
        "J Mahadeokar",
        "H Huang",
        "A Tjandra",
        "X Zhang",
        "F Zhang"
      ],
      "year": "2020",
      "venue": "ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "3",
      "title": "Listen, attend and spell",
      "authors": [
        "W Chan",
        "N Jaitly",
        "Q Le",
        "O Vinyals"
      ],
      "year": "2015",
      "venue": "CoRR"
    },
    {
      "citation_id": "4",
      "title": "Specaugment: A simple data augmentation method for automatic speech recognition",
      "authors": [
        "D Park",
        "W Chan",
        "Y Zhang",
        "C.-C Chiu",
        "B Zoph",
        "E Cubuk",
        "Q Le"
      ],
      "year": "2019",
      "venue": "Specaugment: A simple data augmentation method for automatic speech recognition",
      "arxiv": "arXiv:1904.08779"
    },
    {
      "citation_id": "5",
      "title": "End-toend asr: from supervised to semi-supervised learning with modern architectures",
      "authors": [
        "G Synnaeve",
        "Q Xu",
        "J Kahn",
        "T Likhomanenko",
        "E Grave",
        "V Pratap",
        "A Sriram",
        "V Liptchinsky",
        "R Collobert"
      ],
      "year": "2019",
      "venue": "End-toend asr: from supervised to semi-supervised learning with modern architectures"
    },
    {
      "citation_id": "6",
      "title": "Fully convolutional speech recognition",
      "authors": [
        "N Zeghidour",
        "Q Xu",
        "V Liptchinsky",
        "N Usunier",
        "G Synnaeve",
        "R Collobert"
      ],
      "year": "2018",
      "venue": "Fully convolutional speech recognition"
    },
    {
      "citation_id": "7",
      "title": "Language modeling with gated convolutional networks",
      "authors": [
        "Y Dauphin",
        "A Fan",
        "M Auli",
        "D Grangier"
      ],
      "year": "2016",
      "venue": "CoRR"
    },
    {
      "citation_id": "8",
      "title": "wav2letter++: The fastest open-source speech recognition system",
      "authors": [
        "V Pratap",
        "A Hannun",
        "Q Xu",
        "J Cai",
        "J Kahn",
        "G Synnaeve",
        "V Liptchinsky",
        "R Collobert"
      ],
      "venue": "wav2letter++: The fastest open-source speech recognition system"
    },
    {
      "citation_id": "9",
      "title": "The kaldi speech recognition toolkit",
      "authors": [
        "D Povey",
        "A Ghoshal",
        "G Boulianne",
        "L Burget",
        "O Glembek",
        "N Goel",
        "M Hannemann",
        "P Motlíček",
        "Y Qian",
        "P Schwarz",
        "J Silovský",
        "G Stemmer",
        "K Vesel"
      ],
      "venue": "IEEE 2011 Workshop on Automatic Speech Recognition and Understanding"
    },
    {
      "citation_id": "10",
      "title": "Analysis of mrate, shimmer, jitter, and f/sub 0/contour features across stress and speaking style in the susas database",
      "authors": [
        "R Slyh",
        "W Nelson",
        "E Hansen"
      ],
      "year": "1999",
      "venue": "1999 IEEE International Conference on Acoustics, Speech, and Signal Processing"
    },
    {
      "citation_id": "11",
      "title": "Implicit feedback for user-adaptive systems by analyzing the users' speech",
      "authors": [
        "F Wittig",
        "C Müller"
      ],
      "year": "2003",
      "venue": "Proceedings of the Workshop on Adaptivität und Benutzermodellierung in interaktiven Softwaresystemen (ABIS)"
    },
    {
      "citation_id": "12",
      "title": "Stress and emotion classification using jitter and shimmer features",
      "authors": [
        "X Li",
        "J Tao",
        "M Johnson",
        "J Soltis",
        "A Savage",
        "K Leong",
        "J Newman"
      ],
      "year": "2007",
      "venue": "2007 IEEE International Conference on Acoustics, Speech and Signal Processing-ICASSP'07"
    },
    {
      "citation_id": "13",
      "title": "Jitter and shimmer measurements for speaker recognition",
      "authors": [
        "M Farrús",
        "J Hernando",
        "P Ejarque"
      ],
      "year": "2007",
      "venue": "Proceedings of the Interspeech"
    },
    {
      "citation_id": "14",
      "title": "Jitter and shimmer measurements for speaker diarization",
      "authors": [
        "A Zewoudie",
        "J Luque",
        "F Hernando Pericás"
      ],
      "year": "2014",
      "venue": "VII Jornadas en Tecnología del Habla and III Iberian SLTech Workshop: proceedings: November 19-21"
    },
    {
      "citation_id": "15",
      "title": "Two-stage feature selection of voice parameters for early alzheimer's disease prediction",
      "authors": [
        "S Mirzaei",
        "M Yacoubi",
        "S Garcia-Salicetti",
        "J Boudy",
        "C Kahindo",
        "V Cristancho-Lacroix",
        "H Kerhervé",
        "A.-S Rigaud"
      ],
      "year": "2018",
      "venue": "IRBM"
    },
    {
      "citation_id": "16",
      "title": "Hybridization of best acoustic cues for detecting persons with parkinson's disease",
      "authors": [
        "A Benba",
        "A Jilbab",
        "A Hammouch"
      ],
      "year": "2014",
      "venue": "2014 Second World Conference on Complex Systems (WCCS)"
    },
    {
      "citation_id": "17",
      "title": "Wav2letter: an end-to-end convnet-based speech recognition system",
      "authors": [
        "R Collobert",
        "C Puhrsch",
        "G Synnaeve"
      ],
      "year": "2016",
      "venue": "CoRR"
    },
    {
      "citation_id": "18",
      "title": "Common voice: A massively-multilingual speech corpus",
      "authors": [
        "R Ardila",
        "M Branson",
        "K Davis",
        "M Henretty",
        "M Kohler",
        "J Meyer",
        "R Morais",
        "L Saunders",
        "F Tyers",
        "G Weber"
      ],
      "year": "2019",
      "venue": "Common voice: A massively-multilingual speech corpus"
    },
    {
      "citation_id": "19",
      "title": "Librispeech: An asr corpus based on public domain audio books",
      "authors": [
        "V Panayotov",
        "G Chen",
        "D Povey",
        "S Khudanpur"
      ],
      "year": "2015",
      "venue": "Librispeech: An asr corpus based on public domain audio books"
    },
    {
      "citation_id": "20",
      "title": "Automatic speech recognition system with pitch dependent features for punjabi language on kaldi toolkit",
      "authors": [
        "J Guglani",
        "A Mishra"
      ],
      "year": "2020",
      "venue": "Applied Acoustics"
    },
    {
      "citation_id": "21",
      "title": "Using pitch frequency information in speech recognition",
      "authors": [
        "M Magimai-Doss",
        "T Stephenson",
        "H Bourlard"
      ],
      "year": "2003",
      "venue": "Using pitch frequency information in speech recognition"
    },
    {
      "citation_id": "22",
      "title": "Perception of aperiodicity in pathological voice",
      "authors": [
        "J Kreiman",
        "B Gerratt"
      ],
      "year": "2005",
      "venue": "The Journal of the Acoustical Society of America"
    },
    {
      "citation_id": "23",
      "title": "Some simulations concerning jitter and shimmer measurement",
      "authors": [
        "D Michaelis",
        "M Fröhlich",
        "H Strube",
        "E Kruse",
        "B Story",
        "I Titze"
      ],
      "year": "1998",
      "venue": "3rd International Workshop on Advances in Quantitative Laryngoscopy"
    },
    {
      "citation_id": "24",
      "title": "Voice quality: the 4th prosodic dimension",
      "authors": [
        "N Campbell",
        "P Mokhtari"
      ],
      "year": "2003",
      "venue": "Voice quality: the 4th prosodic dimension"
    },
    {
      "citation_id": "25",
      "title": "Automatic recognition system for dysarthric speech based on mfcc's, pncc's, jitter and shimmer coefficients",
      "authors": [
        "B.-F Zaidi",
        "M Boudraa",
        "S.-A Selouani",
        "D Addou",
        "M Yakoub"
      ],
      "year": "2019",
      "venue": "Science and Information Conference"
    },
    {
      "citation_id": "26",
      "title": "Robust features for noisy speech recognition using jitter and shimmer",
      "authors": [
        "H Rahali",
        "Z Hajaiej",
        "N Ellouze"
      ],
      "venue": "International Journal of Innovative Computing"
    },
    {
      "citation_id": "27",
      "title": "Deep neural networks for emotion recognition combining audio and transcripts",
      "authors": [
        "J Cho",
        "R Pappagari",
        "P Kulkarni",
        "J Villalba",
        "Y Carmiel",
        "N Dehak"
      ],
      "year": "2019",
      "venue": "Deep neural networks for emotion recognition combining audio and transcripts"
    },
    {
      "citation_id": "28",
      "title": "Yin, a fundamental frequency estimator for speech and music",
      "authors": [
        "A De Cheveigné",
        "H Kawahara"
      ],
      "year": "2002",
      "venue": "The Journal of the Acoustical Society of America"
    },
    {
      "citation_id": "29",
      "title": "A robust algorithm for pitch tracking ( rapt )",
      "authors": [
        "D Talkin"
      ],
      "year": "2005",
      "venue": "A robust algorithm for pitch tracking ( rapt )"
    },
    {
      "citation_id": "30",
      "title": "A pitch extraction algorithm tuned for automatic speech recognition",
      "authors": [
        "P Ghahremani",
        "B Babaali",
        "D Povey",
        "K Riedhammer",
        "J Trmal",
        "S Khudanpur"
      ],
      "year": "2014",
      "venue": "2014 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "31",
      "title": "Kenlm: Faster and smaller language model queries",
      "authors": [
        "K Heafield"
      ],
      "year": "2011",
      "venue": "Proceedings of the Sixth Workshop on Statistical Machine Translation, ser. WMT '11. USA: Association for Computational Linguistics"
    },
    {
      "citation_id": "32",
      "title": "Sequence-to-sequence models can directly translate foreign speech",
      "authors": [
        "R Weiss",
        "J Chorowski",
        "N Jaitly",
        "Y Wu",
        "Z Chen"
      ],
      "year": "2017",
      "venue": "Sequence-to-sequence models can directly translate foreign speech"
    },
    {
      "citation_id": "33",
      "title": "Letterbased speech recognition with gated convnets",
      "authors": [
        "V Liptchinsky",
        "G Synnaeve",
        "R Collobert"
      ],
      "year": "2017",
      "venue": "ArXiv"
    },
    {
      "citation_id": "34",
      "title": "Connectionist temporal classification: Labelling unsegmented sequence data with recurrent neural networks",
      "authors": [
        "A Graves",
        "S Fernández",
        "F Gomez",
        "J Schmidhuber"
      ],
      "year": "2006",
      "venue": "Connectionist temporal classification: Labelling unsegmented sequence data with recurrent neural networks"
    },
    {
      "citation_id": "35",
      "title": "Correlation between the voice handicap index and voice measurements in four groups of patients with dysphonia",
      "authors": [
        "A Schindler",
        "F Mozzanica",
        "M Vedrody",
        "P Maruzzi",
        "F Ottaviani"
      ],
      "year": "2009",
      "venue": "Otolaryngology-Head and Neck Surgery"
    },
    {
      "citation_id": "36",
      "title": "Addressing noise and pitch sensitivity of speech recognition system through variational mode decomposition based spectral smoothing",
      "authors": [
        "I Yadav",
        "S Shahnawazuddin",
        "G Pradhan"
      ],
      "year": "2019",
      "venue": "Digit. Signal Process",
      "doi": "10.1016/j.dsp.2018.12.013"
    }
  ]
}