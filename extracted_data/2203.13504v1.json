{
  "paper_id": "2203.13504v1",
  "title": "Emocaps: Emotion Capsule Based Model For Conversational Emotion Recognition",
  "published": "2022-03-25T08:42:57Z",
  "authors": [
    "Zaijing Li",
    "Fengxiao Tang",
    "Ming Zhao",
    "Yusen Zhu"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Emotion recognition in conversation (ERC) aims to analyze the speaker's state and identify their emotion in the conversation. Recent works in ERC focus on context modeling but ignore the representation of contextual emotional tendency. In order to extract multimodal information and the emotional tendency of the utterance effectively, we propose a new structure named Emoformer to extract multimodal emotion vectors from different modalities and fuse them with sentence vector to be an emotion capsule. Furthermore, we design an end-to-end ERC model called Emo-Caps, which extracts emotion vectors through the Emoformer structure and obtain the emotion classification results from a context analysis model. Through the experiments with two benchmark datasets, our model shows better performance than the existing state-of-the-art models.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Emotion recognition in conversation (ERC) is a work that recognizes the speaker's emotion and its influencing factors in the process of conversation. Nowadays, social media such as Facebook and Twitter generate a large amount of dialogue data with various modalities of textual, audio, and video all the time. The study of speaker emotional tendency has huge potential value in the fields of public opinion analysis, shopping, and consumption. Therefore, conversation emotion recognition has attracted more and more attention from researchers and companies.\n\nIn ERC, existing research mainly focuses on the way of contextual information modeling  (Majumder et al., 2019; Ghosal et al., 2019) . However, These models have some shortcomings due to their inability to better extract the grammatical and semantic information of the utterance. Recent studies  (Yuzhao Mao et al., 2020; Weizhou Shen     Li et al. (2021)  proposed a new expression vector, \"emotion vector\" for ERC, which is obtained by mapping from sentence vector, but only for textual modality. Meanwhile, existing studies  (Song et al., 2004; Dellaert et al., 1996; Amir, 1998)  have shown that only textual information is not enough for emotional presentation, the tone and intonation reflect the speaker's emotions to a certain extent, and the facial expressions also express the inner feelings of the speaker in most cases. As shown in Figure  1 , different modalities contain different information, and all are slightly flawed, so multi-modal based information can better identify the speaker's emotion than a single modality in ERC.\n\nIn order to identify the speaker's emotion in conversation effectively, it is necessary to obtain good utterance features. Also we can't ignore the role of the utterance's emotional tendency. As shown in Figure  2 , the emotional tendency of the utterance itself is like an \"offset vector\", which makes the neutral utterance have an \"emotional direction\". For single-sentence emotion classification, emotional tendency is consistent with the results of emotion recognition, while in ERC, the influence of the context may cause the emotional tendency to be inconsistent with the result of emotion recognition. However, emotional tendency can provide features for the model so that model can \"understand\" the reason for emotional reversal.\n\nSo, we propose a new multi-modal emotional tendency extraction method called Emoformer, which is a Transformer-based model but doesn't include the decoder part. As shown in Figure  3 , Emoformer extracts the emotional tendency, i.e., emotion vector, from the modal features through the multi-head self-attention layer and feed-forward layer. More details we will analysis in Section 3.\n\nBased on the Emoformer, we further propose an end-to-end ERC model to classify the emotion based on multi-modal information, named Emo-Caps. Specifically, we employ the Emoformer structure to extract emotion vectors of textual, audio, and visual features. Then, we merge the emotion vectors of the three modalities with the sentence vector to an emotion capsule. Finally, we employ a context analysis model to get the final result of the emotion classification.\n\nIn general, the contributions of this paper are as follows:\n\n• We innovatively introduce the concept of emotion vectors to multi-modal emotion recognition and propose a new emotion feature extraction structure, Emoformer, which is used to jointly extract emotion vectors of three modalities and merge them with sentence vector to the emotion capsule.\n\n• Based on Emoformer, we further propose an end-to-end emotion recognition model named EmoCaps to identify the emotion from multimodal conversation.\n\n• Our model and the existing state-of-the-art model are tested on MELD and IEMOCAP datasets. The test results show that our model has the best performance both in multimodality and text-modality.\n\nThe rest of the paper is organized as follows: Section 2 discusses related works; Section 3 introduces the proposed EmoCaps model in detail; Section 4 and 5 present the experiment setups on two benchmark datasets and the analysis of experiment results; Finally, Section 6 concludes the paper.\n\n2 Related Work 2.1 Emotion Recognition in Conversation  Poria et al. (2017)  use Biredectional LSTM  (Hochreiter and Schmidhuber 1997)  in ERC, which builds context information without differentiating among the speakers. ICON  (Hazarika et al., 2018b)  is an extension of CMN  (Hazarika et al.,2018) , which contains another GRU structure to connect the output in the CMN model to distinguish the speaker relationship.  Majumder et al. (2019)  use three GRUs to obtain context information and update the speaker status.  Ghosal et al. (2019)  construct a conversation into a graph, then use a graph convolutional neural network to convert the emotion classification task of the conversation into a node classification problem of the graph.  Ghosal et al. (2020)  use common sense knowledge to learn the interaction of interlocutors.  Shen et al. (2021)  design a directed acyclic neural network for encoding the utterances.  Hu et al. (2021)  propose the DialogueCRN to fully understand the conversational context from a cognitive perspective.  et al. (2017)  propose the TFN model, which is a multi-modal method using the tensor outer product.  Liang et al. (2018)  propose the model which use a multi-level attention mechanism to extract different modal interaction information.  Cai et al. (2019)  propose a hierarchical fusion model to model graphic information for irony recognition. But the above models are not applied in ERC.  Hazarika et al. (2018b)  propose the CMN model in ERC, which uses a GRU structure to store multimodal data information and considers the role of contextual information in conversation emotion recognition. Jingwen  Hu et al. (2021)  propose the MMGCN model, which is a graph convolutional neural network model based on a multi-modal hybrid approach.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Multi-Modal Emotion Recognition",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Zadeh",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Transformer Models",
      "text": "Inspired by the self-attention mechanism  (Bengio et al. 2014 ), the Transformer is proposed for computing representations and efficiently obtaining long-distance contextual information without using sequence  (Vaswani et al. 2017 ), which has achieved great success in the field of computer vision and audio processing (Tianyang  Lin et al. 2021) .  Devlin et al. (2019)  use the Transformer structure to train a large-scale general-purpose text corpus to obtain a language model with syntactic and semantic information. By employing a transformerbased pretraining model,  Hazarika et al. (2020)  transfer the context-level weight of the generated conversation model to the conversation emotion recognition model. Yuzhao  Mao et al. (2020)  use Transformer to explore differentiated emotional behaviors from the perspective of within and between models. Weizhou  Shen et al. (2020)  use the XL-Net model for conversation emotion recognition (DialogXL) to obtain longer-term contextual information. The above-mentioned algorithms use a transformer-based structure but they are not applied for multi-modal models.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Methodology",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Problem Definition",
      "text": "Given a dialogue:u 1 , u 2 , u 3 , . . . , u n , where n is the number of utterances. The purpose of conversation emotion recognition is to input a dialogue and identify the correct emotion classification of each sentence in the dialogue from the emotion label set y:y 1 , y 2 , y 3 , ..., y m , where m is the number of emotional label.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Unimodal Feature Extraction",
      "text": "We extract the features of the utterance u, represented as U. In particular, when the input data is multi-modal, features of utterance U can be expressed as:  Specifically, we use 3D-CNN with three fully connected layers to get a 512-dimensional vector.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Our Method",
      "text": "We assume that the emotion of the utterances in the dialogue depends on three factors:\n\n• The emotional tendency of the utterance itself.\n\n• Emotional information contained in different modal of utterance.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "• Context Information",
      "text": "Based on the above three factors, our model Emo-Caps is modeled as follows: We obtain three modal features of dialogue data: textual, audio, and visual; and input them into the Emoformer structure, then get the emotion vector of three modals and fuse them with sentence vector; finally, the context analysis model is used to obtain the emotion recognition result. The framework of the model is shown in Figure  4 . It is worth noting that our text features, i.e., the sentence vector, are encoded by a transformer-based pre-trained language model, so we no longer use the self-attention mechanism but directly employ a mapping network to extract the emotion vector, then the residual structure concats sentence vector with emotion vector.\n\nEmoformer Block Existing methods mainly use CNN, TextCNN, GRU, etc., to extract text feature vectors, which extract grammatical information weakly. At the same time, they only take the original feature vectors without emotional tendency as input. Based on this, we propose to use the Emoformer structure to extract the emotion vectors of various modalities. As shown in Figure  3 , Emoformer has an Encoder structure similar to Transformer, but does not include the Decoder structure.\n\nA multi-head attention layer is used to get the emotional tendency feature from the original feature, both are connected through the residual structure, then emotion vector is obtained through a mapping network composed of 5 fully connected layers. The self-attention layer can be used to extract features that contain emotional tendencies or emotional factors effectively, and the residual structure ensures the integrity of the original information; finally the mapping network decouples features and reduces feature dimensions. Identical to  Vaswani et al. (2017) , for a given input feature U , we calculate three matrix of query\n\nwhere T Q , T K , T V represent the sequence length of the Q, K, V , and\n\nThen we can express the formula of the selfattention layer as:\n\nwhere A is the weight of value V , d k is equal to the dimension of u. In this way, multiple selfattention layers are concatenated to get Multi-Head Attention layer:\n\nwhere A 1 , ..., A h are the output of self-attention layers, h is the number of layers, and W is the weight parameter.\n\nThen a residual connection with normalization layer is used to normalize the output of Multi-Head attention layer, and a Feed Forward layer is employed to get the output of the self-attention parts:\n\n(5)\n\nwhere W 1 , W 2 are the weight parameter, b 1 , b 2 are the bias parameter. Finally, the orignal features U and the output of self-attention parts G are connected through the residual structure, and a mapping network is employed to get the final output E:\n\nwhere the Map represents the mapping network, which consists of 5 fully connected layers.\n\nCombine the above Eq. (  2 ) to (  9 ), we can get different modality emotion vectors from different input channels with Emoformer:\n\nwhere U a , U v , U t represent the original input of audio,visual and textual features, and E a , E v , E t represent the emotion vectors of modalities. Emotion Capsule For the composition of the emotion capsule, we are based on the following rules: the text feature vector of the utterance contains grammatical and semantic features, emotion vector represents the emotional tendency of the utterance. Both are the main sources of conversation emotion recognition. Textual features most intuitively represent the meaning, emotions, characteristics, etc., of the utterance. However, visual features and audio features contain a few of emotional factors and emotional features, which can provide some emotional clues when the text features do not have sufficient emotional inclination. Therefore, sentence vector concats with three modalities' emotion vector to be an emotion capsule, which just like a \"capsule\", the emotion vector is \"wrapped\" by the sentence vector and \"absorbed\" by the context analysis model to determine the speaker's emotion finally. Our emotion capsule O can be expressed as:\n\nContext Modeling Since the same emotion has different expressions, and the same expression can express different emotions in different contexts, it is very difficult to infer the true emotions from a single word  (Barrett, 2017) . According to Grice's theory of implicature (1957), the meaning of a sentence can be canceled, so it is necessary to integrate context to infer the true meaning of a sentence. Therefore, contextual information is an indispensable part of conversation emotion recognition. Context information is divided into two parts: the information obtained from the previous moment is named emotional clue traceability, and the information obtained from the next moment is named emotional reasoning.\n\nIn this paper, we employ a Bi-directional LSTM model as the context analysis model to extract contextual information. In a conversation, we form a batch of emotion capsules of all utterances into the Bi-LSTM model in the order of dialogue, and each LSTM cell corresponds to an emotion capsule. For the time i, in the forward propagation sequence, the contextual information C i at this moment is composed of the hidden state output of the LSTM cells at all previous moments, that is, the emotional clue traceability; in the backpropagation sequence, the contextual information at this moment is composed of the hidden state output of the LSTM cells at all following moments, which is emotional reasoning. The two fed into an MLP with fully connected layers and get the values of the utterance u i under each emotion label: Finally we choose the max value as the emotion label y for the i-th utterance:\n\n4 Experiment Setting",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Implementation",
      "text": "For textual data, we use BERT model to obtain the sentence vector then get the textual emotion vector from a mapping network. For audio and visual data, we use Emoformer obtain the audio and visual emotion vector.\n\nAs for the hyperparameter settings, we follow   Li et al. (2021) . For both of MELD dataset and IEMOCAP dataset, the epochs is set to 80, the learning rate is set to 0.0001, and the dropout rate is set to 0.1. The detailed parameter setting is shown in Table  1 .",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Results And Analysis",
      "text": "Our proposed model is compared with other stateof-the-art models on the IEMOCAP dataset and MELD dataset, which are under the same parameter conditions. The experimental results are shown in Table  2  and Table 3 , our model has the best performance on both datasets.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Compare With Other Baseline Models",
      "text": "On the one hand, compared with existed methods, our model encodes sentences through a pre-trained language model to obtain a better utterance representation. On the other hand, our emotion capsule contains the emotional tendency of the utterance itself, combined with contextual information, can more effectively identify the speaker's emotion.  of our assumptions about the emotional factors in ERC.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Various Modality",
      "text": "Table  4  shows the performance of our model on the MELD dataset and the IEMOCAP dataset under different modality combinations. It is easy to find that the performance of multi-modal input is better than single-modal input. At the same time, among the three modalities of textual, audio, and visual, the textual modal has better performance than the other two modalities.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Error Analysis",
      "text": "As shown in Table  4 , the performance of audio and visual modal is not good enough. For audio features, the frequency and amplitude of the sound features can only reflect the intensity of the speaker's emotions, not the specific emotional tendencies. Therefore, when certain emotions have similar frequencies and amplitudes, it is difficult to correctly distinguish the speaker's emotions only through audio data. For example, for two emotions of excited and fear, the frequency and amplitude characteristics in the audio mode are both at high values. Thus it's hard to distinguish the two emotions. For visual features, It is easy for us to judge the speaker's expression by facial features, but when the speaker hides his own expression, the video feature is not enough to judge the speaker's emotion. In addition, for a single video modality, the emotional changes in the context are unexplainable.\n\nWhen the textual modality is added, the performance is significantly improved. In other words, textual modality play a major role in conversation emotion recognition, while audio and visual modalities can help improve the accuracy of recognition, which is consistent with the previous assumptions.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Impact Of Speaker Embedding",
      "text": "In order to analyze the impact of speaker modeling on conversation emotion recognition, we use a variant of DialogueRNN as a context modeling model to test its performance on two benchmark datasets. As shown in Table  5 , the performance of the DialogueRNN-based model on the MELD dataset is better than the LSTM-based model. The reason is that most of the MELD dataset belongs to multi-person dialogue situations, so the speaker modeling model (DialogueRNN-based) is more effective in identifying speaker emotions than the model not using speaker modeling (LSTM-based). However, in the IEMOCAP dataset, which is based on two-person dialogue situations, speaker modeling becomes insignificant.\n\nFurthermore, compared with the LSTM-based model, using DialogueRNN-based or other models that include speaker modeling structures consumes more computing resources and time.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Case Study",
      "text": "Figure  5  shows the influence of emotion vector when emotion reversal in a conversation. At the beginning of the conversation, both speakers are in a neutral emotional state, while utterance 4 changes the situation that speakers' emotion turn into surprise and sadness. The sentence vector doesn't \"understand\" the reason why emotion change, but the emotion vector contains a negative emotion ten-dency which easier to get the correct emotion label. Utterance 7 shows that when the context is in the sad emotion, the emotion vector makes the utterance \"biased\" to \"sad\", while the sentence vector is in a neutral emotion. It proves the role of emotion vector in ERC.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Conclusion",
      "text": "In this paper, we propose a new multi-modal feature extraction structure, Emoformer, which is based on the transformer structure. Further, we design a new ERC model, namely EmoCaps. First, we use Emoformer structures to extract the emotion vectors of textual, audio, and visual modalities, then fuse the three modalities emotion vectors and sentence vectors to be an emotion capsule; finally, we employ a context analysis model to get the emotion recognition result. We conduct comparative experiments on two benchmark datasets. The experimental results show that our model performs better than the existing state-of-the-art models. The experimental results also verified the rationality of our hypothesis.",
      "page_start": 8,
      "page_end": 8
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Visualization of the heatmap for an utterance",
      "page": 1
    },
    {
      "caption": "Figure 1: , different modalities",
      "page": 1
    },
    {
      "caption": "Figure 2: , the emotional tendency of the utterance it-",
      "page": 1
    },
    {
      "caption": "Figure 2: A map for emotion classiﬁcation. The dashed",
      "page": 2
    },
    {
      "caption": "Figure 3: Schematic diagram of Emoformer. The map-",
      "page": 2
    },
    {
      "caption": "Figure 4: Framework illustration of the EmoCaps based ERC model.",
      "page": 4
    },
    {
      "caption": "Figure 4: It is worth noting that our text",
      "page": 4
    },
    {
      "caption": "Figure 5: Visualization of the heatmap in the \"Friends\"",
      "page": 8
    },
    {
      "caption": "Figure 5: shows the inﬂuence of emotion vector",
      "page": 8
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Parameter Setting": "",
          "Dataset": "IEMOCAP"
        },
        {
          "Parameter Setting": "Epochs\nLr\nDr\nBatch size\nDim-T\nDim-V\nDim-A",
          "Dataset": "80\n0.0001\n0.1\n30\n100\n256\n100"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table 2: Experimental results (F1 score) on the IEMOCAP dataset. Average means weighted average. Some of",
      "data": [
        {
          "IEMOCAP": "Happy"
        },
        {
          "IEMOCAP": "43.40\n30.38\n33.18\n42.75\n-\n-\n-\n42.34"
        },
        {
          "IEMOCAP": "71.91"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table 3: Experimental results (F1 score) on the MELD dataset. Average means weighted average. The CMN",
      "data": [
        {
          "MELD": "Neutral"
        },
        {
          "MELD": "73.80\n73.50\n-\n-\n-\n-\n-"
        },
        {
          "MELD": "77.12"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table 3: Experimental results (F1 score) on the MELD dataset. Average means weighted average. The CMN",
      "data": [
        {
          "Modality": "",
          "Dataset": "IEMOCAP"
        },
        {
          "Modality": "Text\nAudio\nVideo\nT+A\nT+V\nT+V+A",
          "Dataset": "69.49\n33.00\n31.64\n71.39\n71.30\n71.77"
        }
      ],
      "page": 7
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Towards an automatic classification of emotions in speech",
      "authors": [
        "Noam Amir",
        "Samuel Ron"
      ],
      "year": "1998",
      "venue": "Fifth International Conference on Spoken Language Processing"
    },
    {
      "citation_id": "2",
      "title": "Neural machine translation by jointly learning to align and translate",
      "authors": [
        "D Bahdanau",
        "K Cho",
        "Y Bengio"
      ],
      "year": "2014",
      "venue": "Computer Science"
    },
    {
      "citation_id": "3",
      "title": "Iemocap: Interactive emotional dyadic motion capture database. Language resources and evaluation",
      "authors": [
        "Carlos Busso",
        "Murtaza Bulut",
        "Chi-Chun Lee",
        "Abe Kazemzadeh",
        "Emily Mower",
        "Samuel Kim",
        "Jeannette Chang",
        "Sungbok Lee",
        "Shrikanth S Narayanan"
      ],
      "year": "2008",
      "venue": "Iemocap: Interactive emotional dyadic motion capture database. Language resources and evaluation"
    },
    {
      "citation_id": "4",
      "title": "Multimodal sarcasm detection in twitter with hierarchical fusion model",
      "authors": [
        "Yitao Cai",
        "Huiyu Cai",
        "Xiaojun Wan"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "5",
      "title": "Recognizing emotion in speech",
      "authors": [
        "F Dellaert",
        "T Polzin",
        "A Waibel"
      ],
      "year": "1996",
      "venue": "Proceeding of Fourth International Conference on Spoken Language Processing. ICSLP '96",
      "doi": "10.1109/ICSLP.1996.608022"
    },
    {
      "citation_id": "6",
      "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "J Devlin",
        "Ming-Wei Chang",
        "Kenton Lee",
        "Kristina Toutanova"
      ],
      "year": "2019",
      "venue": "NAACL-HLT"
    },
    {
      "citation_id": "7",
      "title": "Opensmile: the munich versatile and fast open-source audio feature extractor",
      "authors": [
        "Florian Eyben",
        "Martin Wöllmer",
        "Björn Schuller"
      ],
      "year": "2010",
      "venue": "Proceedings of the 18th ACM international conference on Multimedia"
    },
    {
      "citation_id": "8",
      "title": "Cosmic: Commonsense knowledge for emotion identification in conversations. Findings of the Association for Computational Linguistics",
      "authors": [
        "Deepanway Ghosal",
        "Navonil Majumder",
        "Alexander Gelbukh"
      ],
      "year": "2020",
      "venue": "Cosmic: Commonsense knowledge for emotion identification in conversations. Findings of the Association for Computational Linguistics"
    },
    {
      "citation_id": "9",
      "title": "Dialoguegcn: A graph convolutional neural network for emotion recognition in conversation",
      "authors": [
        "Deepanway Ghosal",
        "Navonil Majumder",
        "Soujanya Poria",
        "Niyati Chhaya",
        "Alexander Gelbukh"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing"
    },
    {
      "citation_id": "10",
      "title": "2018a. Icon: interactive conversational memory network for multimodal emotion detection",
      "authors": [
        "Devamanyu Hazarika",
        "Soujanya Poria",
        "Rada Mihalcea",
        "Erik Cambria",
        "Roger Zimmermann"
      ],
      "venue": "Proceedings of the 2018 conference on empirical methods in natural language processing"
    },
    {
      "citation_id": "11",
      "title": "Conversational memory network for emotion recognition in dyadic dialogue videos",
      "authors": [
        "Devamanyu Hazarika",
        "Soujanya Poria",
        "Amir Zadeh",
        "Erik Cambria",
        "Louis-Philippe Morency",
        "Roger Zimmermann"
      ],
      "year": "2018",
      "venue": "Proceedings of the conference"
    },
    {
      "citation_id": "12",
      "title": "Conversational transfer learning for emotion recognition",
      "authors": [
        "Devamanyu Hazarika",
        "Soujanya Poria",
        "Roger Zimmermann",
        "Rada Mihalcea"
      ],
      "year": "2021",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "13",
      "title": "Long short-term memory",
      "authors": [
        "Sepp Hochreiter",
        "Jürgen Schmidhuber"
      ],
      "year": "1997",
      "venue": "Neural computation"
    },
    {
      "citation_id": "14",
      "title": "2021a. Dialoguecrn: Contextual reasoning networks for emotion recognition in conversations",
      "authors": [
        "Dou Hu",
        "Lingwei Wei",
        "Xiaoyong Huai"
      ],
      "venue": "2021a. Dialoguecrn: Contextual reasoning networks for emotion recognition in conversations"
    },
    {
      "citation_id": "15",
      "title": "2021b. Mmgcn: Multimodal fusion via deep graph convolution network for emotion recognition in conversation",
      "authors": [
        "Jingwen Hu",
        "Yuchen Liu",
        "Jinming Zhao",
        "Qin Jin"
      ],
      "venue": "2021b. Mmgcn: Multimodal fusion via deep graph convolution network for emotion recognition in conversation"
    },
    {
      "citation_id": "16",
      "title": "Seover: Sentence-level emotion orientation vector based conversation emotion recognition model",
      "authors": [
        "Zaijing Li",
        "Fengxiao Tang",
        "Tieyu Sun",
        "Yusen Zhu",
        "Ming Zhao"
      ],
      "year": "2021",
      "venue": "Neural Information Processing"
    },
    {
      "citation_id": "17",
      "title": "A survey of transformers. Navonil Majumder, Soujanya Poria, Devamanyu Hazarika, Rada Mihalcea, Alexander Gelbukh, and Erik Cambria",
      "authors": [
        "Tianyang Lin",
        "Yuxin Wang",
        "Xiangyang Liu",
        "Xipeng Qiu"
      ],
      "year": "2019",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "18",
      "title": "Dialoguetrm: Exploring the intra-and inter-modal emotional behaviors in the conversation",
      "authors": [
        "Yuzhao Mao",
        "Qi Sun",
        "Guang Liu",
        "Xiaojie Wang",
        "Weiguo Gao",
        "Xuan Li",
        "Jianping Shen"
      ],
      "year": "2020",
      "venue": "Dialoguetrm: Exploring the intra-and inter-modal emotional behaviors in the conversation"
    },
    {
      "citation_id": "19",
      "title": "Context-dependent sentiment analysis in user-generated videos",
      "authors": [
        "Soujanya Poria",
        "Erik Cambria",
        "Devamanyu Hazarika",
        "Navonil Majumder",
        "Amir Zadeh",
        "Louis-Philippe Morency"
      ],
      "year": "2017",
      "venue": "Proceedings of the 55th annual meeting of the association for computational linguistics"
    },
    {
      "citation_id": "20",
      "title": "Meld: A multimodal multi-party dataset for emotion recognition in conversations",
      "authors": [
        "Soujanya Poria",
        "Devamanyu Hazarika",
        "Navonil Majumder",
        "Gautam Naik",
        "Erik Cambria",
        "Rada Mihalcea"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "21",
      "title": "Dialogxl: All-in-one xlnet for multiparty conversation emotion recognition",
      "authors": [
        "Weizhou Shen",
        "Junqing Chen",
        "Xiaojun Quan",
        "Zhixian Xie"
      ],
      "year": "2020",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "22",
      "title": "Directed acyclic graph network for conversational emotion recognition",
      "authors": [
        "Weizhou Shen",
        "Siyue Wu",
        "Yunyi Yang",
        "Xiaojun Quan"
      ],
      "year": "2021",
      "venue": "Directed acyclic graph network for conversational emotion recognition"
    },
    {
      "citation_id": "23",
      "title": "Audio-visual based emotion recognition -a new approach",
      "authors": [
        "Mingli Song",
        "Jiajun Bu",
        "Chun Chen",
        "Nan Li"
      ],
      "year": "2004",
      "venue": "Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition",
      "doi": "10.1109/CVPR.2004.1315276"
    },
    {
      "citation_id": "24",
      "title": "Attention is all you need",
      "authors": [
        "Ashish Vaswani",
        "Noam Shazeer",
        "Niki Parmar",
        "Jakob Uszkoreit",
        "Llion Jones",
        "Aidan Gomez",
        "Lukasz Kaiser",
        "Illia Polosukhin"
      ],
      "year": "2017",
      "venue": "Attention is all you need"
    },
    {
      "citation_id": "25",
      "title": "Tensor fusion network for multimodal sentiment analysis",
      "authors": [
        "Amir Zadeh",
        "Minghai Chen",
        "Soujanya Poria",
        "Erik Cambria",
        "Louis-Philippe Morency"
      ],
      "year": "2017",
      "venue": "Proceedings of the 2017 conference on empirical methods in natural language processing"
    },
    {
      "citation_id": "26",
      "title": "Multi-attention recurrent network for human communication comprehension",
      "authors": [
        "Amir Zadeh",
        "Paul Liang",
        "Soujanya Poria"
      ],
      "year": "2018",
      "venue": "Thirty-Second AAAI Conference on Artificial Intelligence"
    }
  ]
}