{
  "paper_id": "2205.10466v1",
  "title": "A Survey On Physiological Signal Based Emotion Recognition",
  "published": "2022-05-20T23:59:44Z",
  "authors": [
    "Zeeshan Ahmad",
    "Naimul Khan"
  ],
  "keywords": [
    "Data annotation",
    "physiological signals",
    "data variability",
    "emotion models",
    "challenges",
    "review"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Physiological Signals are the most reliable form of signals for emotion recognition, as they cannot be controlled deliberately by the subject. Existing review papers on emotion recognition based on physiological signals surveyed only the regular steps involved in the workflow of emotion recognition such as preprocessing, feature extraction, and classification. While these are important steps, such steps are required for any signal processing application. Emotion recognition poses its own set of challenges that are very important to address for a robust system. Thus, to bridge the gap in the existing literature, in this paper, we review the effect of inter-subject data variance on emotion recognition, important data annotation techniques for emotion recognition and their comparison, data preprocessing techniques for each physiological signal, data splitting techniques for improving the generalization of emotion recognition models and different multimodal fusion techniques and their comparison. Finally we discuss key challenges and future directions in this field.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "Emotion is a psychological response to some external stimulus and internal cognitive processes, supported by a series of physiological activities going on in human body. Thus, emotion recognition is a promising and challenging work area which enables us to recognize the emotions of a person for stress detection and management, risk prevention, mental health, and interpersonal relations.\n\nEmotion recognition is an emerging research area due to its numerous applications in our daily life. Applications include areas such as developing models for inspecting driver emotions  [1] , health care  [2]  [3], software engineering  [4]  and entertainment  [5] .\n\nDifferent modalities of data can be used for emotion recognition. They are commonly divided into behavioral and physiological modalities. Behavioral modalities includes emotion recognition from facial expressions  [6]    [7]  [8]  [9]  [10], from gestures  [11]    [12]  [13] and from speech  [14]  [15]  [16] , while physiological modalities include emotion recognition from physiological signals such as Electroencephalogram (EEG), Electrocardiogram (ECG), Galvanic skin response (GSR), Electrodermal Activity (EDA) and so forth  [17]    [18]  [19]  [20]    [21] .\n\nBehavioral modalities can be effectively controlled by user and the quality of expressing them may be significantly influenced by personality of the user and the current environment of © 2022 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works.\n\nthe subject  [22] . On the other hand, physiological signals are continuously available and cannot be controlled intentionally or consciously. The commonly used physiological signals are ECG, EEG and GSR signals.\n\nMany papers exist in literature where review on emotion recognition using physiological signals is presented.\n\nIn  [23] , a comprehensive review on physiological signalbased emotion recognition was presented that includes emotion models, emotion elicitation methods, the published emotional physiological datasets, features, classifiers, and the frameworks for emotion recognition based on the physiological signal. In  [24] , literature review provides a concise analysis of physiological signals and instruments used to monitor them, emotion recognition, emotion models and emotional stimulation approaches. The authors also discuss selected works on emotional recognition by physiological signals in wearable devices. In  [25] , different emotion recognition methods using physiological signals by machine learning techniques were explained. This paper also reviewed different stages like data collection, data processing, feature extraction and classification models for each recognition method. In  [26]  recent advancements in emotion recognition research using physiological signals, including emotion models and stimulation, preprocessing, feature extraction and classification methodologies were presented. In  [27] , the current state-ofthe art of emotion recognition was presented. This paper also presented the main challenges and future opportunities that lie ahead, in particular for the development of novel machine learning (ML) algorithms in the context of emotion recognition using physiological signals.\n\nExisting review papers on emotion recognition based on physiological signals looked for only the regular steps involved in the workflow of emotion recognition such as preprocessing, feature extraction, and classification but did not discuss the set of important challenges that are particular to emotion recognition. None of the aforemtioned works elaborated the problems during data annotion, the different kinds of data annotion methods for emotion recognition and advantages/disadvantages of each method. Furthermore, in the current review papers, the data preprocessing techniques are bundled together and presented as generic techniques for any physiological signal. In our opinion, each physiological signal presents its own unique set of challenges when it comes to preprocessing, and the preprocessing steps should be discussed separately. Moreover, inter-subject data variability has huge impact on emotion recognition. The existing reviews neither discuss this effect nor provide recommendations to reduce inter-subject variability. In addition to this, the comparison of data splitting techniques, such as subject independent and subject dependent, is not provided for better emotion recgonition and generalization of the trained classification models. Also, the comparison and advantages of multimodal fusion methods are not provided in these review papers.\n\nTo adress the above shortcomings of the existing reviews, in this paper, we are focussed on the missing but essential elements of emotion recognition. These are 1) Important data annotation techniques for emotion recognition and their comparison. 2) Data preproceessing techniques for each physiological signal. 3) Effect of Inter-subject data variance on emotion recognition. 4) Data splitting techniques for improving the generalization of emotion recognition models. 5) Different multimodal fusion techniques and their comparison. This paper is organized as follows: in section II, the emotion models are explained. In section VI, the effect of inter-subject data variance on emotion recognition is described. In section IV, the data annotation techniques for emotion recognition are illustrated. Section V demonstrates the data preprocessing technique for each physiological signal. In section VII, data splitting techniques for improved generalization of emotion recognition models are analyzed. Section VIII provides the details of different multimodal fusion techniques for emotion recognition. Section IX summarizes future challenges of emotion recognition using physiological signals and finally we conclude the paper in section X.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Emotion Models",
      "text": "Over the past decades, different emotion models have been proposed by researchers based on the quantitative analysis and the emergence of new emotions categories. Based on the recent research, emotional states can be represented with two models: discrete and multidimensional.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "A. Discrete Or Categorical Models",
      "text": "Discrete models are the most commonly used models because they contain a list of distinct emotion classes that are easy to recognize and understand. Ekman  [29]  and Plutchik  [28]  are amongst those scientists that present the concept of discrete emotion states. Ekman briefed that there are six basic emotions such as happy, sad, anger, fear, surprise, and disgust, and all the other emotions are derived from these six basic emotions. Plutchik presented a famous wheel model to describe eight discrete emotions. These emotions are joy, trust, fear, surprise, sadness, disgust, anger and anticipation, as shown in Fig.  1 . The model describes the relations among emotion concepts, which are analogous to the colors on a color wheel. The cone's vertical dimension represents intensity, and the circle represents degrees of similarity among the emotions. The eight sectors are designed to indicate that there are eight primary emotion dimensions and rest of the motion dimensions are derived from them.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "B. Continuous Or Multidimensional Models",
      "text": "All emotional states listed in discrete set of emotions cannot be confined by a single word and they need a range of intensities for description. For instance, a person may feel less or more excited OR become less or more afraid in response to a particular stimulus. Thus, to cover the range of intensities in emotions, multidimensional models such as 2D and 3D are proposed. Amongst 2D models, the model presented in  [30]  is the famous model that describes the emotion along the dimensions of High Arousal (HA) and Low Arousal (LA), and High Valence (HV) and Low Valence (LV) as shown in Fig.  2 . The 2D model classifies emotions based on two dimensional data consisting of valence and arousal value. On the other hand, the 3D model deals with valence, arousal, and dominance. Valence indicates the level of pleasure, Arousal indicates the level of excitation and Dominance indicates the level of controlling or dominating emotion. The 3D model is shown in Fig.  3 .",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Iii. Databases",
      "text": "Several datasets for physiological based emotion recognition are publicly-available,like AMIGOS  [31] , ASCERTAIN  [32] , BIO VID EMO DB  [33] , DEAP  [34] , DREAMER  [35] , MAHNOB-HCI  [36] , MPED  [37] , SEED  [38]  as shown in Table  I . All datasets, other than SEED, are multimodal and provide more than one modalities of physiological signals.\n\nAs can be seen, some of these datasets utilize the continuous emotion model, while the others use the discrete model. Most of these datasets are unfortunately limited to a small number of subjects due to the elaborate process of data collection. Some details of these datasets such as annotation and preprocessing are discussed in the following sections.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Iv. Data Annotation",
      "text": "Data annotation is amongst the most important steps after data acquisition and eventually for emotion recognition. However, this subject is not addressed in detail in exisiting literature. For better discussion on this matter, we divide the data annotation procedure into discrete and continuous data annotation.\n\nIn discrete annotation, participants are given questionnaire after the stimulai and they are asked to rate their feelings between some scores ranging from 1 to 5 or from 0 to 9 and then the final annotations are decided as shown in Fig  4 .\n\nOn the other hand in continuous annotations, the participants are required to continuously annotate the data in real time using some human computer interface (HCI) mechanism as shown in Fig  5 .\n\nIn sections IV-A and IV-B, we provide the review on discrete and continuous annotation.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "A. Discrete Annotation",
      "text": "In  [37] , three psychological questionnaires PANAS  [40] , SAM  [41]  and DES  [42] , were used for data annotation of seven emotion categories such as joy, funny, anger, sad, disgust, fear and neutrality. Besides questionnaires, T-test was also conducted to evaluate the effectiveness of the selected emotion categories.\n\nIn  [43]  and  [32] , participants were briefed on details of the experiments including the principal experimental tasks and Fig.  1 : Plutchik Wheel of Discrete Emotions  [28]  later on the self-report questionnaires were filled by the participants according to their experiences during stimuli. These questionnaires were finally used for data annotation. In  [19] , for automatic ECG Based emotion recognition, music is used as stimuli and in the self-assessment stages, the participants were asked to indicate how strongly they had experienced the corresponding feeling during stimulus presentation and then participants gave marks ranging from 1 to 5 for each of the nine emotion categories.\n\nIn  [44] , the questionnaire, divided into three parts, one for each activity, in each of which the participants had to select the emotions they felt before, during, and after the activity, was given to the participants. Based on the participants marking, 28 emotions are arranged on a 2-dimensional plane with Valence and Arousal at each axis as shown in Fig  2 , according to the model proposed in  [45] . In  [46] , the data was collected in VR environment. The data annotation was based on the subjects' responses to the two questions regarding level of excitement and pleasantness which were graphed on a 9 square grid, a discretized version of the arousal-valence model  [47] . Each grid shows how many subjects reported feeling the corresponding level of \"pleasantness\" and \"excitement\" for each VR session. The valence scale was measured as \"Unpleasant\", \"Neutral\", or \"Pleasant\" (from left to right) and the arousal scale was measured as \"Relaxing\", \"Neutral\", or \"Exiting\" (from bottom to top). The results are very asymmetrical with the majority of the sessions rated as \"Exciting\" and \"Pleasant\".",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "B. Continuous Annotation",
      "text": "In  [39] , participant viewed a 360 • videos as stimuli and they rated emotional states (valence and arousal) continuously using the joystick. It is observed that collecting continuous annotations can be used to evaluate the performance of finegrained emotion recognition algorithms such as weakly supervised learning or regression. In  [48] , the authors claim that the lack of continuous annotations is the reason why they failed to validate their weakly-supervised algorithm for fine-grained emotion recognition since the continuous labels allow the algorithms to learn the precise mappings between the dynamic emotional changes and input signals. Moreover, if only discrete annotations are available, ML algorithms can overfit because the discrete labels represent only the most salient or recent emotion rather than the dynamic emotional changes that may occur within stimuli  [49] . A software for continuous emotion annotation is introduced in  [50]  and is called EMuJoy ( Emotion measurement with Music by using a Joystick ). It is better than Schubert's 2DES software  [51] . It helps subjects to generate self reports for different media   This contrasts with Schubert's software, where mouse is used. In  [52] , stimuli videos were also continuously annotated along valence and arousal dimensions. Long-short-term-memory recurrent neural networks (LSTM-RNN) and continuous conditional random fields (CCRF) were utilized in detecting emotions automatically and continuously. Furthermore, the effect of lag on continuous emotion detection is also studied. The database is annotated using a joystick and delays from 250ms up to 4 seconds were in annotations. It is oberved that 250 ms increased the detection performance whereas longer delays deteriorated it. This shows the advantage of using joystick over mouse. Authors in  [53]  analyzed the effect of lag on continuous emotion detection on SEMAINE database  [54]  They found a delay of 2 seconds will improve their emotion detection results. SEMAINE database is annotated by Feeltrace  [55]  using a mouse as annotation interface. Thus the observations in  [53]  also show that the response time of joystick is less than mouse. Another software named DARMA is introduced in  [56]  for continuous measurement system that synchronizes media playback and the continuous recording of two-dimensional measurements. These measurements can be observational or self-reported and are provided in real-time Fig.  5 : Continuous annotation using HCI mechanism  [39]  through the manipulation of a computer joystick.\n\nTouch events are dynamic and can be bidirectional i.e 2D along Valence-arousal plane. During Continuous annotation, participants are instructed to annotate their emotion experience by moving the joystick head into one of the four quadrants. The movement of the joystick head maps the emotions into a 2D Valence-arousal plane, in which the x axis indicates valence while the y axis indicates arousal. Since the nature of emotions is time-varying, therefore, during annotation participants could lose the control over the speed i.e the movement of a joystick which could lead collection of less precise ground truth labels. Thus, the training of participants on HCI mechanism and and study of lag for continuous emotion detection are two important factors for precise continuous annotation.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "C. Summary Of The Section",
      "text": "There is no definite or clear-cut advantage of discrete annotation on continuous annotation and vice versa. However, the choice of annotation method depends on the application. For example, In Biofeedback systems, participants learn how to control physical and psychological effects of stress using feed back signals. Biofeedback is a mind-body technique that involves using visual or auditory feedback to teach people to recognize the physical signs and symptoms of stress and anxiety, such as increased heart rate, body temperature, and muscle tension. Since stress level changes during biofeedback mechanism, therefore, real time continuous annotation is more important in this case  [57] . The choice of annotation also depends length of the data, the number of emotion categories, number of participants, feature extraction method and the choice of classifier.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "A. Eeg Preprocessing",
      "text": "In addition to noise, the EEG signals are affected by Electrooculography (EOG) with frequency below 4 Hz. This artifact is caused by the facial muscle movement or eye movement  [59] ,  [60] . Different methods that have been used for EEG preprocessing or cleaning include rejection method, linear filtering, statistical methods such as linear regression and Independent Component Analysis (ICA).\n\n1) Rejection Method: Rejection method involves both manual and automatic rejection of epochs contaminated with artifacts. Manual rejection needs less computation but it is more laborous than automatic rejection. In automatic rejection pre-determined threshold is used to remove the artifactcontaminated trials from the data. The common disadvantages associated with the rejection method are the sampling bias  [61]  and loss of valuable information  [62] .\n\n2) Linear Filtering: Linear filtering is simple and easy to apply and is beneficial mostly when artifacts located in certain frequency bands do not overlap with the signal of interest For example, low-pass filtering can be used to remove EMG artifacts and high-pass filtering can be used to remove EOG artifacts  [38] . However, linear filtering flops when EEG signal and the EMG or EOG artifacts lie in the same frequency band.\n\n3) Linear Regression: Linear regression using least square method has been used for EEG signal processing for removing EOG based artifacts. In Linear regression using least square criteria, residual is calculated by subtracting the EOG signal from the EEG signal and then this residual is optimized to get the cleaned EEG signal  [63] . This method is not suitable for EMG based artifact removal because EMG data is collected from multiple muscles groups and therefore EMG data has no single reference.\n\n4) Independent Component Analysis: ICA is amongst those methods that do not require reference artifacts for cleaning the EEG signal and thus making the components independent  [64] . The disadvantage associated with ICA is that it requires prior visual inspection to identify the artifact part.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "B. Ecg Preprocessing",
      "text": "ECG signal is usually corrupted by various noise sources and artifacts. These noises and artifacts deteriorate the signal quality and affect the detection of QRS.\n\n1) Filtering: Filtering is the simplest method of removing noise and artifacts from the ECG signal and to improve the signal to noise ratio (SNR) of the QRS complex. In ECG signal, the three critical frequency regions include the very  High-pass filters allow only higher frequencies to pass through them. They are used to suppress low-frequency components the ECG signal. Low frequency components include motion artifact, respiratory variations, and baseline wander.\n\nIn  [66] , high pass filters with cut-off frequency of 0.5 Hz was selected to waive off the baseline wander.\n\nLow-pass filters are usually employed to eliminate highfrequency components from the ECG signal. These compoents include noise, muscle artifacts and powerline interference  [67] .\n\nSince bandpass filters remove most of the unwanted components from the ECG signal, therefore, it is extensively used to preprocess ECG. The structure of Band-pass filter comprises of both a high-pass and low-pass filter. In  [68] , bandpass filter is used to remove muscle noise and baseline wander from ECG.\n\n2) Normalization: In ECG, the amplitude of QRS complex amplitude increase from birth to adolescence and then begins to decrease afterward  [69] . Due to this, ECG signals suffer from inter-subject variability. To overcome this problem, amplitude normalization is performed. In  [70] , a method of normalizing ECG signals to a standard heart rate to lower the false rate detection, was introduced. The commonly used normalization techniques are min-max normalization  [71] , maximum division normalization  [72]  and Z-score normalization  [73] .",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "C. Gsr Preprocessing",
      "text": "There is still no standard methods for GSR preprocessing. In this section, we are disscussing a few methods to deal with noise and artifacts present in raw GSR signal.\n\n1) Empirical Mode Decomposition: Empirical Mode Decomposition (EMD) is the technique that best addresses the nonlinear and nonstationary nature of the signal while removing noise and artifact and was introduced in  [74]  as a tool to adaptively decompose a signal into a collection of AM-FM components. The EMD relies on a fully data-driven mechanism that does not require any a priori known basis like Fourier and wavelet-based. EMD decomposes the signal into a sum of intrinsic mode functions (IMFs). An IMF is defined as a function with equal number of extrema and zero crossings (or at most differed by one) with its envelopes, as defined by all the local maxima and minima, being symmetric with respect to zero.\n\nAfter extraction of IMFs from a time series signal the residue tends to become a monotonic function, such that no more IMFs can be extracted. Finally, after the iterative process, the input signal is decomposed into a sum of IMF functions and a residue.\n\nIn  [58] , an algorithm, modified from EMD, called Empirical iterative algorithm is proposed for denoising the GSR from motion artefacts and quantization noise. The algorithm does not rely on the shifting process of EMD, and provides the filtered signal directly as an output of each iteration.\n\n2) Kalman Filtering: Low-pass and moving average filters have been used for GSR preprocessing  [26] , however, the Kalman filter is the better choice. The Kalman filter is a modelbased filter and a state estimator. It employs a mathematical model of the process producing the signal to be filtered  [75] . In  [76] , an extended Kalman filter is used to remove noise and artifact from GSR signal in real time. The Kalman filter used in this article is comparable to a 3rd order Butterworth low pass filter with a similar frequency response. However, the kalman filter is more robust than its counter-part butterworth low pass filter while suppressing the noise and artifacts.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "D. 1D To 2D Conversion",
      "text": "Physiological signals such as EEG, ECG and GSR are 1D signal in raw form. In  [77]  and  [78] , raw ECG signal was converted into 2D form i.e into three statistical images namely Gramian Angular Field (GAF) images, Recurrence Plot (RP) images and Markov Transition Field (MTF) images as shown in Fig.  7 . Experimental results show the superiority of 1D to 2D preprocessing. In  [79] , ECG and GSR are converted in 2D scalogram for better emotion recognition. Furthermore, in  [80] , ECG and GSR signals are converted in 2D RP images for improved emotion recognition as compared to 1D form signals.\n\nThus, the transformation from 1D to 2D has been shown to be an important preprocessing step for physiological signals.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Vi. Inter-Subject Data Variance",
      "text": "In this section, we will review the effect of inter-subject data variance on emotion recognition. We will also discuuss the reasons of inter-subject data variance and recommend solutions for the problem.\n\nThere exists few papers  [43] ,  [32] ,  [81] ,  [82] ,  [83]  and  [84] , which only reported the problem of inter-subject data variance and its effect on degradation of emotion recognition accuracy, but they did not shed light on the possible reasons behind inter-subject data variability.\n\nTo derive our point home, we performed experiments on ECG data of two datasets, WESAD dataset  [85]  and our own Ryerson Multimedia Research Laboratory (RML) dataset  [86] . The issue of inter-subject data variability was particularly apparent in the RML dataset, since we controlled the data collection procss.\n\nTo observe the high inter-subject variablity, we utilize leaveone-subject-out (LOSO) cross-validation (CV) for all our experiments, where the models are trained with data from all-butone subjects, and tested on the held out subject. The average results across all subjects are presented in Tables II, III, IV and V. In Tables  II  and IV  show the results of experiments conducted with 1D ECG for both datasets. To see the affect of data preprocessing on inter-subject data variability, we transform the raw ECG data into spectrograms. ResNet-18  [87]  was trained on these spectrgrams and results are shown in Tables  III  and V  From Tables II, III, IV and V, we observe the inter-subject data variance across all the metrics i.e accuracy, precision, recall and F 1 score. We also observe that by transforming 1D data to 2D using spectrograms have increased the accuracy of the affective state but did not reduce the data variance significantly.\n\nLooking at the WESAD results, we can see that the average accuracy is only 72%. This is significacntly lower than the average accuracy achieved from ECG in x,y,z. However, x,y,z used 5 or 10-fold cross validation, which means that a segment   of data for each subject was always present in the training set. This is an unrealistic scenario for practical applications. Therefore, dealing with inter-subject variability and conducting experiments in a LOSO setting is very important. We elaborate on this point further in Section VII.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "A. Statistical Analysis",
      "text": "We perform statistical analysis by plotting errorbars on 1D and 2D WESAD data to notice the variability in accuracy of each individual subject across the overall mean value of accuracy as shown in Figures  8  and 9 . The errorbars showing the spread or variability in the data. Figures  8  and 9  can also be used to detect the outlier subjects from the dataset that could be removed later on for reducing the variability amongst the remaining subjects.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "B. Reasons Of Inter-Subject Data Variance",
      "text": "Following could be the reason for inter-subject variability.\n\n• Some users feel uncomfortable when wearing a sensor on their bodies and behave unnaturally and unexpectedly during data collection. • Body movements during data collection adversely effect the quality of recorded data. For instance, head and eyeball movement during EEG data collection and hand movement during EMG data collection are the sources of degradation of these data. This difference in their response depends upon their mental strength and physical stability. For instance, a movie clip can instill fear in some users while other users may feel excitement while watching the it. • The length of the collected data is also amongst the major reasons of creating subject diversity. A long video is more likely to elicit diverse emotional states  [88] , while the data with limited samples will create problem during the training of the model and classification of the affective states.\n\n• Experimental environment and quality of sensors/ equipment are also the reasons of variance in data. For instance, uncalibrated sensors may create errors at different stages of data collection.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "C. Reducing Inter-Subject Data Variance",
      "text": "Following actions could be taken to remove the inter-subject variability.\n\n• The collected sample size for each subject should be same. The length of the data should be set carefully to avoid variance in the data.\n\n• The users or subjects should be trained to remain calm and behave normally during data collection or wearing a sensor.\n\n• Different channels of the data collecting equipment should be calibrated against the same standards so that the intra-variance in the data for the subject can be reduced.\n\n• In  [89] , statistics-based unsupervised domain adaptation algorithm was proposed for reducing the inter-subject variance. A new baseline model was introduced to tackle the classification task, and then two novel domain adaptation objective functions, Cluster-aligning loss and Cluster-separating loss were presented to mitigate the negative impacts caused by domain shifts. We believe more research in the area of domain adaptation could help tackle inter-subject data variability. VII. DATA SPLITTING There are two major types of data splitting techniques that are being used for emotion recognition using physiological signals. These two methods of splitting are called subject independent and subject dependent.\n\nIn subject independent case, the model is trained from data of various subjects and then tested on data of new subjects (that are not included in the training part). In the subject dependent case, the training and testing part is randomly distributed among all the subjects.",
      "page_start": 10,
      "page_end": 11
    },
    {
      "section_name": "A. Subject Independent Splitting",
      "text": "The most commonly used subject independent data splitting method is leave-one-subject-out (LOSO) crossvalidation. In LOSO, tha model is trained on (k-1) subjects and leave one subject for testing and this process is repeated for every subject of the dataset.\n\nIn  [90]  and  [91] , LOSO cross validation technique is used for DEAP dataset containing 32 subjects. Thus, the experiment is repeated for every subject and the final performance is considered as the average of all experiments. In  [92] , Leave-one-subject out method is conducted to evaluate the performance, and the final performance is calculated by the averaging method. In  [93] , EEG-based emotion classification was conducted by a leave-one-subject out validation (LOSOV). One subject among the 20 subjects is used for testing while the others were used for the training set. The experiments were repeated for all subjects and the mean and standard deviation of the accuracies were used to evaluate the model performance.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "B. Subject Dependent Splitting",
      "text": "In subject dependent splitting, various strategies are used. In one of the strategies, training and testing parts are taken from a single subject with a particular splitting percentage. In another setting, few trials of same subject are taken as a training part and rest of them are used as testing. Furthermore, in another method of subject dependent splitting, testing and training parts are randomly selected across all the subjects either by particular percenatge or by k-fold cross validation. For k = 5, the concept of cross validation is shown in Fig.  10 . The final performance is the average of all the five evaluations.\n\nIn  [37] , physiological signals of 7 trials out of 28 were selected as testing for each subject and rest of the trials are used for training. This was repeated for every subject. In  [94] , models are evaluated using 10-fold cross validation performed on each subject. This was repeated on all 32 subjects of DEAP datasets and the final evaluation about accuracies and F1 scores are based on averaging method. In  [95] , physiological samples of all subjects of DEAP dataset are used to evaluate the model, The 10-fold cross validation technique is used to analyze the emotion recognition performance. All samples are randomly divided into ten subsets. One subset is regarded as a test set, and another subset as a validation set. The remaining subsets are regarded as a training set. The above process is repeated ten times until all subsets are tested.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "C. Summary Of The Section",
      "text": "Subject-dependent classification is usually performed to investigate the individual variability between subjects on emotion recognition from their physiological signals.\n\nIn real application scenarios, Emotional recognition system with subject independent classification is considered more pratical as the system has to predict emotions of each subject or patient separately when implemented for emotion recognition task.\n\nIn  [84] , comparison between subject dependent and subject Independent setting is provided. it is observed that subject independent splitting show 3% lower accuracy because of high inter-subject variability. However, the generalized capability of subject independent model on unseen data is more as these model learn inter-subject variability.\n\nThus it is more valuable to develop a good subjectindependent emotional recognition model so that they can generalize well on unseen data or new patients.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Viii. Multimodal Fusion",
      "text": "The purpose of multimodal fusion is to obtain complementary information from different physiological modalities to improve the emotion recognition performance. Fusing different modalities alleviates the weaknesses of individual modalities by integrating complementary information from the modalities.\n\nIn  [96] , new emotional discriminative space is constructed utilizing Discriminative canonical correlation analysis (DCCA) is used from physiological signals with the assistance of EEG signals. Finally, machine learning techniques are utilized to build emotion recognizer. Authors in  [97]  trained feedforward neural networks using both the fused and nonfused signals. Experiments show that the fused method outperforms each individual signal across all emotions tested. In  [98] , feature level fusion is performed between the features from two modalities i.e facial expression images and EEG signals. Feature map of size 128 × 128 is constructed by concatenating the facial images and their corresponding EEG feature maps. Emotion recognition model is finally trained on these feature maps. Experimental results show the improved performance of multimodal features over single modality features. In  [99] , hybrid fusion of face data, EEG and GSR is performed. First, features from EEG and GSR modalities are integrated for estimating the level of arousal. Final fusion was the late fusion of EEG, GSR and face modalities.\n\nAuthors in  [100]  presented a new emotion recognition framework based on Decision fusion, where three separate classifiers were trained on ECG, EMG and skin conductance level(SCL). The majority voting principle was used to determine a final classification result on the three outputs of the separate classifiers.\n\nIn  [101] , multidomain features such as features from time domain, frequency domain and wavelet domain are fused with different combinations to identify stable features which would best represent the underlying EEG signal characteristics and performed better classification. Multidomain feature fusion is performed using concatenation to obtained final feature vector. In  [102] , weighted average fusion of physiological signal was done to classify valence and arousal. Overall performance was quantified as the percentage of correctly classified emotional states per video. It is also observed that the classification performance obtained was slightly better for arousal than valence. Authors in  [103]  provide the study about combining the EEG data and audio visual features of the video. Then, PCA algorithm was applied to reduce the feature dimensions. In  [104] , feature level fusion of sensor data from EEG and EDA sensors was performed for human emotion detection. Nine features (delta, theta, low alpha, high alpha, low beta, high beta, low gamma, high gamma and galvanic skin response) are selected for training a neural network model. The classification performance of each modality was also evaluated separately. It is observed that multimodal emotion recognition is better than using a single modality. In  [105] , authors developed two late multi-modal data fusion methods with deep CNN models to estimate positive and negative affects (PA and NA) scores and explored the effect of the two late fusion methods and different combination of the physiological signals on the performance. In the first late fusion method, a separate CNN is trained for each modality and in the second late fusion method, the average of the three classes' (baseline, stress, and amusement) probabilities across the pretrained CNNs is calculated, and the emotion class with the highest average probability is selected. In the case of the PA or NA estimation, the average of the estimated scores across the pretrained CNNs is provided as the estimated affect score. In  [86] , multidomain fusion of ECG signal is performed for multilevel stress assessment. First, the ECG signal was converted into image and then images are made multimodal and multidomain by transforming them into frequency and time-spectrum domain using Discrete Fourier Transform (DFT) and Gabor wavelet transform (GWT) respectively. Features in different domains are extracted by Convolutional Neural networks (CNNs) and then decision level fusion is performed for improving the classification accuracy as shown in Fig.  11 .",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "A. Summary Of The Section",
      "text": "Above review shows that the fusion of the physiological modalities exhibits better performance than the single modality in terms of classification accuracy for both arousal and valence dimensions. Furthermore, it is observed from the review that the two major fusion methods practiced for physiological data features are feature fusion and decision fusion, however, feature fusion is adopted by the reseracher more than decision level. The greatest advantage of feature level fusion is that it utilizes the correlation among the modalities at an early stage. Furthermore, only one classifier is required to perform a task, making the training process less tedious. The disadvantage with decision level fusion is the use of more than one classifier. This makes the task time consuming.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Ix. Future Challenges",
      "text": "The work flow for emotion recognition consists of many steps such as data acquisition, data annotation, data preprocessing, feature extraction and selection, and recognition. The problems and challenges are in each step of emotion recognition and are expalined in this section.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "A. Data Acquisition",
      "text": "One of the significant challenge in acquiring physiological signals is to deal with noise, baseline drifts, different artifacts due to body movements, different responses of participants to different stimuli and low graded signal. The data acquiring devices carry noise which corrupts the signal and induced artifacts that superimposed on the signal. The second challenge is the setting of stable and noiseless lab and selection of stumuli so that genuine emotions are induced which are closed to real world feelings. Another challenge in acquiring a high quality data is the various responses of participants or subjects to the same stimulai. This causes large inter-subject variability in the data and is also the reason that most of the avaiable emotion recognition datasets are of short duration.\n\nTo overcome the above challeges, well-designed labs and proper selection of stimuli are necessary. Furthermore, subjects must be properly trained to avoid large variance in the data and possibility of gather datasets with long duration and less inter-subject variability.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "B. Data Annotation",
      "text": "High volume research has been conducted on emotion recognition, even then there is no uniform standards to annotate data. Discrete and continuous data annotation techniques are commonly used, however, due to unavailability of uniform annotation standard, there is no compatibility between the datasets. Furthermore, all emotions can neither be listed as discrete nor continuous as different emotions need different range of intensities for description. The problem with existing datasets is that some of them are annotated with four emotions, some are with basic six emotions or with eight emotions. Due to this different emotion labels, datasets are not compatible and emotion recognition algorithms donot work equally well on these datasets.\n\nTo face the above mentioned challenges of data annotation, hybrid data annotation technique should be introduced where each emotion is labelled according to its range of energy. However, care must be taken while selecting new standards of data annotation because hybrid annotation technique may lead to imbalance data.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "C. Feature Extraction And Fusion",
      "text": "Features for emotion recognition are categorized into time domain features and transform domain features. Time domain features mostly include statistical features of first and higher orders. Transform domain features are further divided into frequency domain and time-frequency domain such as feature extracted from wavelet transform. Still, there is no set of features that gurantees to work for all models and situations.\n\nOne of the solution of the above problem is to design adaptive intelligent systems that can automatically select the best features for the classifier model. Furthermore, another solution is to perform multimodel fusion. In section VIII, it is explained that commonly practiced fusion techniques for emotion recognition are feature level and decision level fusion. However, for feature level fusion, concatenation is mostly used and for decision level fusion, majority voting is mostly practiced. Thus, a need of more improved and intelligent based fusion methods is arising because the simple feature level and decision level fusion cannot counter the non-stationary and nonlinear nature of features.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "D. Generalization Of Models",
      "text": "One of the biggest challenges in emotion recognition is to design the models that can generalize well on unseen data or new datasets. The main obstacles are the limited samples in the datasets, non-standard data splitting techniques and large inter-subject data variability. The commonly used data splitting techniques are subject dependent and subject independent techniques. It is explained in section VII, that the models trained on subject independent settings are more likely to generalize well. However, exisitng subject-independent recognition models are not intelligent enough to perform well in realistic and real-time applications.\n\nSince while testing emotion recognition model has to face an unseen subjects, therefore, one of the solution is to train and validate the model on large number of subjects so that it can generalize well on testing. The problem of limited dataset can be solved by carefully applying data augmentation techniques because that techniques could lead to data imbalance and overfitting of the model.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "E. Modern Machine Learning Methods",
      "text": "Advanced machine learning tools need to developed for mitigating the challenges of emotion recognition using physiological signals. For instance, one of the modern technique which is being used is transfer learning. In transfer learning, the knowledge learning while solving one task can be applied for different but related work. For example, the rich features learned by a machine learning model while doing emotion recognition based on EEG data from a large dataset could also be applicable for EEG data of other dataset. This can easily solve the problem of small dataset where less data samples are available. Transfer learning methods are getting success but still lot of research work is required to be done for physiological signal based transfer learning methods for emotion recognition.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "X. Conclusion",
      "text": "In this paper, we provide a review on the physiological signal based emotion recognition and bridge the gap in the existing literature by mainly focussing on crucial elements lacking in the existing literature. These crucial elements include the effect of inter-subject data variance on emotion recognition, important data annotation techniques for emotion recognition and their comparison, data preprocessing techniques, data splitting techniques for improving the generalization of emotion recognition model and different multimodal fusion techniques and their comparison. We also discuss the future challenges of this existing research area.",
      "page_start": 12,
      "page_end": 12
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The model describes the relations among",
      "page": 2
    },
    {
      "caption": "Figure 2: The 2D model classiﬁes emotions based on two",
      "page": 2
    },
    {
      "caption": "Figure 3: III. DATABASES",
      "page": 2
    },
    {
      "caption": "Figure 4: On the other hand in continuous annotations, the participants",
      "page": 2
    },
    {
      "caption": "Figure 5: In sections IV-A and IV-B, we provide the review on",
      "page": 2
    },
    {
      "caption": "Figure 1: Plutchik Wheel of Discrete Emotions [28]",
      "page": 3
    },
    {
      "caption": "Figure 2: , according to the",
      "page": 3
    },
    {
      "caption": "Figure 2: 2D Valence-arousal Model",
      "page": 4
    },
    {
      "caption": "Figure 3: 3D Emotion Model",
      "page": 5
    },
    {
      "caption": "Figure 4: Steps involving discrete annotation",
      "page": 5
    },
    {
      "caption": "Figure 5: Continuous annotation using HCI mechanism [39]",
      "page": 6
    },
    {
      "caption": "Figure 6: (a) Raw GSR: rest and motion phases. Signals corresponding to the movements involving the right hand are delimited",
      "page": 7
    },
    {
      "caption": "Figure 7: Transformation of ECG signal into GAF, RP and MTF",
      "page": 7
    },
    {
      "caption": "Figure 7: Experimental results show the superiority of 1D",
      "page": 8
    },
    {
      "caption": "Figure 8: Error-bars showing Inter-subject variability in terms of",
      "page": 9
    },
    {
      "caption": "Figure 9: Error-bars showing Inter-subject variability in terms of",
      "page": 9
    },
    {
      "caption": "Figure 10: Conceptual Visualization of 5-fold cross validation.",
      "page": 10
    },
    {
      "caption": "Figure 10: The ﬁnal performance is the average of all the ﬁve evaluations.",
      "page": 10
    },
    {
      "caption": "Figure 11: Fig. 11: Multimodal Multidomain Fusion of ECG Signal.",
      "page": 11
    }
  ],
  "tables": [],
  "citations": [
    {
      "citation_id": "1",
      "title": "Enhancing safety of transport by road by online monitoring of driver emotions",
      "authors": [
        "S Nadai",
        "M D'incà",
        "F Parodi",
        "M Benza",
        "A Trotta",
        "E Zero",
        "L Zero",
        "R Sacile"
      ],
      "year": "2016",
      "venue": "2016 11th System of Systems Engineering Conference (SoSE)"
    },
    {
      "citation_id": "2",
      "title": "Autosense: unobtrusively wearable sensor suite for inferring the onset, causality, and consequences of stress in the field",
      "authors": [
        "E Ertin",
        "N Stohs",
        "S Kumar",
        "A Raij",
        "M Al'absi",
        "S Shah"
      ],
      "year": "2011",
      "venue": "Proceedings of the 9th ACM conference on embedded networked sensor systems"
    },
    {
      "citation_id": "3",
      "title": "Towards detecting programmers' stress on the basis of keystroke dynamics",
      "authors": [
        "A Kołakowska"
      ],
      "year": "2016",
      "venue": "2016 Federated Conference on Computer Science and Information Systems (FedCSIS)"
    },
    {
      "citation_id": "4",
      "title": "Emotion recognition and its application in software engineering",
      "authors": [
        "A Kołakowska",
        "A Landowska",
        "M Szwoch",
        "W Szwoch",
        "M Wróbel"
      ],
      "year": "2013",
      "venue": "2013 6th International Conference on Human System Interactions (HSI)"
    },
    {
      "citation_id": "5",
      "title": "Using different information channels for affect-aware video games-a case study",
      "authors": [
        "M Szwoch",
        "W Szwoch"
      ],
      "year": "2018",
      "venue": "International Conference on Image Processing and Communications"
    },
    {
      "citation_id": "6",
      "title": "Emotion recognition for cognitive edge computing using deep learning",
      "authors": [
        "G Muhammad",
        "M Hossain"
      ],
      "year": "2021",
      "venue": "IEEE Internet of Things Journal"
    },
    {
      "citation_id": "7",
      "title": "Impact of affective multimedia content on the electroencephalogram and facial expressions",
      "authors": [
        "S Siddharth",
        "T.-P Jung",
        "T Sejnowski"
      ],
      "year": "2019",
      "venue": "Scientific Reports"
    },
    {
      "citation_id": "8",
      "title": "Intelligent facial emotion recognition based on stationary wavelet entropy and jaya algorithm",
      "authors": [
        "S.-H Wang",
        "P Phillips",
        "Z.-C Dong",
        "Y.-D Zhang"
      ],
      "year": "2018",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "9",
      "title": "An emotion recognition model based on facial recognition in virtual learning environment",
      "authors": [
        "D Yang",
        "A Alsadoon",
        "P Prasad",
        "A Singh",
        "A Elchouemi"
      ],
      "year": "2018",
      "venue": "Procedia Computer Science"
    },
    {
      "citation_id": "10",
      "title": "Facial emotion recognition based on biorthogonal wavelet entropy, fuzzy support vector machine, and stratified cross validation",
      "authors": [
        "Y.-D Zhang",
        "Z.-J Yang",
        "H.-M Lu",
        "X.-X Zhou",
        "P Phillips",
        "Q.-M Liu",
        "S.-H Wang"
      ],
      "year": "2016",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "11",
      "title": "Bodily expression for automatic affect recognition",
      "authors": [
        "H Gunes",
        "C Shan",
        "S Chen",
        "Y Tian"
      ],
      "year": "2015",
      "venue": "Bodily expression for automatic affect recognition"
    },
    {
      "citation_id": "12",
      "title": "Adaptive body gesture representation for automatic emotion recognition",
      "authors": [
        "S Piana",
        "A Staglianò",
        "F Odone",
        "A Camurri"
      ],
      "year": "2016",
      "venue": "ACM Transactions on Interactive Intelligent Systems (TiiS)"
    },
    {
      "citation_id": "13",
      "title": "Survey on emotional body gesture recognition",
      "authors": [
        "F Noroozi",
        "C Corneanu",
        "D Kamińska",
        "T Sapiński",
        "S Escalera",
        "G Anbarjafari"
      ],
      "year": "2018",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "14",
      "title": "An improved speech emotion recognition algorithm based on deep belief network",
      "authors": [
        "H Zheng",
        "Y Yang"
      ],
      "year": "2019",
      "venue": "2019 IEEE International Conference on Power, Intelligent Computing and Systems (ICPICS)"
    },
    {
      "citation_id": "15",
      "title": "Survey of deep representation learning for speech emotion recognition",
      "authors": [
        "S Latif",
        "R Rana",
        "S Khalifa",
        "R Jurdak",
        "J Qadir",
        "B Schuller"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "16",
      "title": "Speech emotion recognition: Emotional models, databases, features, preprocessing methods, supporting modalities, and classifiers",
      "authors": [
        "M Akc",
        "K Oguz"
      ],
      "year": "2020",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "17",
      "title": "Physiological-signal-based emotion recognition: An odyssey from methodology to philosophy",
      "authors": [
        "W Li",
        "Z Zhang",
        "A Song"
      ],
      "year": "2021",
      "venue": "Measurement"
    },
    {
      "citation_id": "18",
      "title": "Eeg-based emotion recognition: a state-of-the-art review of current trends and opportunities",
      "authors": [
        "N Suhaimi",
        "J Mountstephens",
        "J Teo"
      ],
      "year": "2020",
      "venue": "Computational intelligence and neuroscience"
    },
    {
      "citation_id": "19",
      "title": "Automatic ecg-based emotion recognition in music listening",
      "authors": [
        "Y.-L Hsu",
        "J.-S Wang",
        "W.-C Chiang",
        "C.-H Hung"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "20",
      "title": "Self-supervised learning for ecg-based emotion recognition",
      "authors": [
        "P Sarkar",
        "A Etemad"
      ],
      "year": "2020",
      "venue": "ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "21",
      "title": "Emotion recognition of gsr based on an improved quantum neural network",
      "authors": [
        "Q Zhang",
        "X Lai",
        "G Liu"
      ],
      "year": "2016",
      "venue": "2016 8th International Conference on Intelligent Human-Machine Systems and Cybernetics (IHMSC)"
    },
    {
      "citation_id": "22",
      "title": "Honest signals",
      "authors": [
        "A Pentland",
        "T Heibeck"
      ],
      "year": "2008",
      "venue": "Honest signals"
    },
    {
      "citation_id": "23",
      "title": "A review of emotion recognition using physiological signals",
      "authors": [
        "L Shu",
        "J Xie",
        "M Yang",
        "Z Li",
        "Z Li",
        "D Liao",
        "X Xu",
        "X Yang"
      ],
      "year": "2018",
      "venue": "Sensors"
    },
    {
      "citation_id": "24",
      "title": "A survey of emotion recognition using physiological signal in wearable devices",
      "authors": [
        "H Wijasena",
        "R Ferdiana",
        "S Wibirama"
      ],
      "year": "2021",
      "venue": "2021 International Conference on Artificial Intelligence and Mechatronics Systems (AIMS)"
    },
    {
      "citation_id": "25",
      "title": "Recent survey on emotion recognition using physiological signals",
      "authors": [
        "E Joy",
        "R Joseph",
        "M Lakshmi",
        "W Joseph",
        "M Rajeswari"
      ],
      "year": "2021",
      "venue": "2021 7th International Conference on Advanced Computing and Communication Systems (ICACCS)"
    },
    {
      "citation_id": "26",
      "title": "Emotion recognition measurement based on physiological signals",
      "authors": [
        "X Fan",
        "Y Yan",
        "X Wang",
        "H Yan",
        "Y Li",
        "L Xie",
        "E Yin"
      ],
      "year": "2020",
      "venue": "2020 13th International Symposium on Computational Intelligence and Design"
    },
    {
      "citation_id": "27",
      "title": "A review, current challenges, and future possibilities on emotion recognition using machine learning and physiological signals",
      "authors": [
        "P Bota",
        "C Wang",
        "A Fred",
        "H Da",
        "Silva"
      ],
      "year": "2019",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "28",
      "title": "The nature of emotions: Human emotions have deep evolutionary roots, a fact that may explain their complexity and provide tools for clinical practice",
      "authors": [
        "R Plutchik"
      ],
      "year": "2001",
      "venue": "American scientist"
    },
    {
      "citation_id": "29",
      "title": "An argument for basic emotions",
      "authors": [
        "P Ekman"
      ],
      "year": "1992",
      "venue": "Cognition & emotion"
    },
    {
      "citation_id": "30",
      "title": "A circumplex model of affect",
      "authors": [
        "J Russell"
      ],
      "year": "1980",
      "venue": "Journal of personality and social psychology"
    },
    {
      "citation_id": "31",
      "title": "Amigos: A dataset for affect, personality and mood research on individuals and groups",
      "authors": [
        "J Miranda-Correa",
        "M Abadi",
        "N Sebe",
        "I Patras"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "32",
      "title": "Ascertain: Emotion and personality recognition using commercial sensors",
      "authors": [
        "R Subramanian",
        "J Wache",
        "M Abadi",
        "R Vieriu",
        "S Winkler",
        "N Sebe"
      ],
      "year": "2016",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "33",
      "title": "biovid emo db\": A multimodal database for emotion analyses validated by subjective ratings",
      "authors": [
        "L Zhang",
        "S Walter",
        "X Ma",
        "P Werner",
        "A Al-Hamadi",
        "H Traue",
        "S Gruss"
      ],
      "year": "2016",
      "venue": "2016 IEEE Symposium Series on Computational Intelligence (SSCI)"
    },
    {
      "citation_id": "34",
      "title": "Deap: A database for emotion analysis; using physiological signals",
      "authors": [
        "S Koelstra",
        "C Muhl",
        "M Soleymani",
        "J.-S Lee",
        "A Yazdani",
        "T Ebrahimi",
        "T Pun",
        "A Nijholt",
        "I Patras"
      ],
      "year": "2011",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "35",
      "title": "Dreamer: A database for emotion recognition through eeg and ecg signals from wireless low-cost offthe-shelf devices",
      "authors": [
        "S Katsigiannis",
        "N Ramzan"
      ],
      "year": "2017",
      "venue": "IEEE journal of biomedical and health informatics"
    },
    {
      "citation_id": "36",
      "title": "A multimodal database for affect recognition and implicit tagging",
      "authors": [
        "M Soleymani",
        "J Lichtenauer",
        "T Pun",
        "M Pantic"
      ],
      "year": "2011",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "37",
      "title": "Mped: A multi-modal physiological emotion database for discrete emotion recognition",
      "authors": [
        "T Song",
        "W Zheng",
        "C Lu",
        "Y Zong",
        "X Zhang",
        "Z Cui"
      ],
      "year": "2019",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "38",
      "title": "Investigating critical frequency bands and channels for eeg-based emotion recognition with deep neural networks",
      "authors": [
        "W.-L Zheng",
        "B.-L Lu"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on Autonomous Mental Development"
    },
    {
      "citation_id": "39",
      "title": "Ceap-360vr: A continuous physiological and behavioral emotion annotation dataset for 360 vr videos",
      "authors": [
        "T Xue",
        "A Ali",
        "T Zhang",
        "G Ding",
        "P Cesar"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "40",
      "title": "Development and validation of brief measures of positive and negative affect: the panas scales",
      "authors": [
        "D Watson",
        "L Clark",
        "A Tellegen"
      ],
      "year": "1988",
      "venue": "Journal of personality and social psychology"
    },
    {
      "citation_id": "41",
      "title": "Measuring emotion: the self-assessment manikin and the semantic differential",
      "authors": [
        "M Bradley",
        "P Lang"
      ],
      "year": "1994",
      "venue": "Journal of behavior therapy and experimental psychiatry"
    },
    {
      "citation_id": "42",
      "title": "Emotion elicitation using films",
      "authors": [
        "J Gross",
        "R Levenson"
      ],
      "year": "1995",
      "venue": "Cognition & emotion"
    },
    {
      "citation_id": "43",
      "title": "Behavioral and physiological signalsbased deep multimodal approach for mobile emotion recognition",
      "authors": [
        "K Yang",
        "C Wang",
        "Y Gu",
        "Z Sarsenbayeva",
        "B Tag",
        "T Dingler",
        "G Wadley",
        "J Goncalves"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "44",
      "title": "Examining human-horse interaction by means of affect recognition via physiological signals",
      "authors": [
        "T Althobaiti",
        "S Katsigiannis",
        "D West",
        "N Ramzan"
      ],
      "year": "2019",
      "venue": "Ieee access"
    },
    {
      "citation_id": "45",
      "title": "Evidence for a three-factor theory of emotions",
      "authors": [
        "J Russell",
        "A Mehrabian"
      ],
      "year": "1977",
      "venue": "Journal of research in Personality"
    },
    {
      "citation_id": "46",
      "title": "Physiological measurement for emotion recognition in virtual reality",
      "authors": [
        "L Hinkle",
        "K Roudposhti",
        "V Metsis"
      ],
      "year": "2019",
      "venue": "2019 2nd International Conference on Data Intelligence and Security (ICDIS)"
    },
    {
      "citation_id": "47",
      "title": "The circumplex model of affect: An integrative approach to affective neuroscience, cognitive development, and psychopathology",
      "authors": [
        "J Posner",
        "J Russell",
        "B Peterson"
      ],
      "year": "2005",
      "venue": "Development and psychopathology"
    },
    {
      "citation_id": "48",
      "title": "Multiple instance learning for emotion recognition using physiological signals",
      "authors": [
        "L Romeo",
        "A Cavallo",
        "L Pepa",
        "N Berthouze",
        "M Pontil"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "49",
      "title": "Duration neglect in retrospective evaluations of affective episodes",
      "authors": [
        "B Fredrickson",
        "D Kahneman"
      ],
      "year": "1993",
      "venue": "Journal of personality and social psychology"
    },
    {
      "citation_id": "50",
      "title": "Emujoy: Software for continuous measurement of perceived emotions in music",
      "authors": [
        "F Nagel",
        "R Kopiez",
        "O Grewe",
        "E Altenmüller"
      ],
      "year": "2007",
      "venue": "Behavior Research Methods"
    },
    {
      "citation_id": "51",
      "title": "Measuring emotion continuously: Validity and reliability of the two-dimensional emotion-space",
      "authors": [
        "E Schubert"
      ],
      "year": "1999",
      "venue": "Australian Journal of Psychology"
    },
    {
      "citation_id": "52",
      "title": "Analysis of eeg signals and facial expressions for continuous emotion detection",
      "authors": [
        "M Soleymani",
        "S Asghari-Esfeden",
        "Y Fu",
        "M Pantic"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "53",
      "title": "Analysis and compensation of the reaction lag of evaluators in continuous emotional annotations",
      "authors": [
        "S Mariooryad",
        "C Busso"
      ],
      "year": "2013",
      "venue": "2013 Humaine Association Conference on Affective Computing and Intelligent Interaction"
    },
    {
      "citation_id": "54",
      "title": "The semaine database: Annotated multimodal records of emotionally colored conversations between a person and a limited agent",
      "authors": [
        "G Mckeown",
        "M Valstar",
        "R Cowie",
        "M Pantic",
        "M Schroder"
      ],
      "year": "2011",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "55",
      "title": "'feeltrace': An instrument for recording perceived emotion in real time",
      "authors": [
        "R Cowie",
        "E Douglas-Cowie",
        "S Savvidou",
        "E Mcmahon",
        "M Sawey",
        "M Schröder"
      ],
      "year": "2000",
      "venue": "ISCA tutorial and research workshop (ITRW) on speech and emotion"
    },
    {
      "citation_id": "56",
      "title": "Darma: Software for dual axis rating and media annotation",
      "authors": [
        "J Girard",
        "A Wright"
      ],
      "year": "2018",
      "venue": "Behavior research methods"
    },
    {
      "citation_id": "57",
      "title": "Multimodal archiving, real-time annotation and information visualization in a biofeedback system for stroke patient rehabilitation",
      "authors": [
        "W Xu",
        "Y Chen",
        "H Sundaram",
        "T Rikakis"
      ],
      "year": "2006",
      "venue": "Proceedings of the 3rd ACM workshop on Continuous archival and retrival of personal experences"
    },
    {
      "citation_id": "58",
      "title": "A data driven empirical iterative algorithm for gsr signal pre-processing",
      "authors": [
        "A Gautam",
        "N Simões-Capela",
        "G Schiavone",
        "A Acharyya",
        "W Raedt",
        "C Van Hoof"
      ],
      "year": "2018",
      "venue": "2018 26th European Signal Processing Conference"
    },
    {
      "citation_id": "59",
      "title": "High-frequency brain activity and muscle artifacts in meg/eeg: a review and recommendations",
      "authors": [
        "S Muthukumaraswamy"
      ],
      "year": "2013",
      "venue": "Frontiers in human neuroscience"
    },
    {
      "citation_id": "60",
      "title": "Emg and eog artifacts in brain computer interface systems: A survey",
      "authors": [
        "M Fatourechi",
        "A Bashashati",
        "R Ward",
        "G Birch"
      ],
      "year": "2007",
      "venue": "Clinical neurophysiology"
    },
    {
      "citation_id": "61",
      "title": "Dealing with artifacts: The eog contamination of the eventrelated brain potential",
      "authors": [
        "G Gratton"
      ],
      "year": "1998",
      "venue": "Behavior Research Methods, Instruments, & Computers"
    },
    {
      "citation_id": "62",
      "title": "Optimal spatial filtering of single trial eeg during imagined hand movement",
      "authors": [
        "H Ramoser",
        "J Muller-Gerking",
        "G Pfurtscheller"
      ],
      "year": "2000",
      "venue": "IEEE transactions on rehabilitation engineering"
    },
    {
      "citation_id": "63",
      "title": "Eog correction: a comparison of four methods",
      "authors": [
        "R Croft",
        "J Chandler",
        "R Barry",
        "N Cooper",
        "A Clarke"
      ],
      "year": "2005",
      "venue": "Psychophysiology"
    },
    {
      "citation_id": "64",
      "title": "A hybrid ica-wavelet transform for automated artefact removal in eeg-based emotion recognition",
      "authors": [
        "A Bigirimana",
        "N Siddique",
        "D Coyle"
      ],
      "year": "2016",
      "venue": "2016 IEEE International Conference on Systems, Man, and Cybernetics"
    },
    {
      "citation_id": "65",
      "title": "Ecg signal preprocessing and svm classifier-based abnormality detection in remote healthcare applications",
      "authors": [
        "C Venkatesan",
        "P Karthigaikumar",
        "A Paul",
        "S Satheeskumaran",
        "R Kumar"
      ],
      "year": "2018",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "66",
      "title": "Classification of paroxysmal and persistent atrial fibrillation in ambulatory ecg recordings",
      "authors": [
        "R Alcaraz",
        "F Sandberg",
        "L Sörnmo",
        "J Rieta"
      ],
      "year": "2011",
      "venue": "IEEE Transactions on Biomedical Engineering"
    },
    {
      "citation_id": "67",
      "title": "An efficient optimized feature selection with machine learning approach for ecg biometric recognition",
      "authors": [
        "K Patro",
        "A Prakash",
        "M Rao",
        "P Kumar"
      ],
      "year": "2020",
      "venue": "IETE Journal of Research"
    },
    {
      "citation_id": "68",
      "title": "Ecg-based authentication using timing-aware domainspecific architecture",
      "authors": [
        "R Cordeiro",
        "D Gajaria",
        "A Limaye",
        "T Adegbija",
        "N Karimian",
        "F Tehranipoor"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems"
    },
    {
      "citation_id": "69",
      "title": "Chou's electrocardiography in clinical practice: adult and pediatric",
      "authors": [
        "B Surawicz",
        "T Knilans"
      ],
      "year": "2008",
      "venue": "Chou's electrocardiography in clinical practice: adult and pediatric"
    },
    {
      "citation_id": "70",
      "title": "Human identification system based ecg signal",
      "authors": [
        "S Saechia",
        "J Koseeyaporn",
        "P Wardkein"
      ],
      "year": "2005",
      "venue": "TENCON 2005-2005 IEEE Region 10 Conference"
    },
    {
      "citation_id": "71",
      "title": "Ecg data compression using truncated singular value decomposition",
      "authors": [
        "J.-J Wei",
        "C.-J Chang",
        "N.-K Chou",
        "G.-J Jan"
      ],
      "year": "2001",
      "venue": "IEEE Transactions on Information Technology in Biomedicine"
    },
    {
      "citation_id": "72",
      "title": "Human identification using time normalized qt signal and the qrs complex of the ecg",
      "authors": [
        "M Tawfik",
        "H Selim",
        "T Kamal"
      ],
      "year": "2010",
      "venue": "in 2010 7th International Symposium on Communication Systems, Networks & Digital Signal Processing"
    },
    {
      "citation_id": "73",
      "title": "Ecg biometrics: A robust short-time frequency analysis",
      "authors": [
        "I Odinaka",
        "P.-H Lai",
        "A Kaplan",
        "J O'sullivan",
        "E Sirevaag",
        "S Kristjansson",
        "A Sheffield",
        "J Rohrbaugh"
      ],
      "year": "2010",
      "venue": "2010 IEEE International Workshop on Information Forensics and Security"
    },
    {
      "citation_id": "74",
      "title": "The empirical mode decomposition and the hilbert spectrum for nonlinear and non-stationary time series analysis",
      "authors": [
        "N Huang",
        "Z Shen",
        "S Long",
        "M Wu",
        "H Shih",
        "Q Zheng",
        "N.-C Yen",
        "C Tung",
        "H Liu"
      ],
      "year": "1971",
      "venue": "Proceedings of the Royal Society of London. Series A: mathematical, physical and engineering sciences"
    },
    {
      "citation_id": "75",
      "title": "Bayesian estimation and tracking: a practical guide",
      "authors": [
        "A Haug"
      ],
      "year": "2012",
      "venue": "Bayesian estimation and tracking: a practical guide"
    },
    {
      "citation_id": "76",
      "title": "Modelbased filtering for artifact and noise suppression with state estimation for electrodermal activity measurements in real time",
      "authors": [
        "C Tronstad",
        "O Staal",
        "S Saelid",
        "Ø Martinsen"
      ],
      "year": "2015",
      "venue": "2015 37th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)"
    },
    {
      "citation_id": "77",
      "title": "Ecg heart-beat classification using multimodal image fusion",
      "authors": [
        "Z Ahmad",
        "A Tabassum",
        "L Guan",
        "N Khan"
      ],
      "year": "2021",
      "venue": "ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "78",
      "title": "Ecg heartbeat classification using multimodal fusion",
      "authors": [
        "Z Ahmad",
        "A Tabassum",
        "L Guan",
        "N Khan"
      ],
      "year": "2021",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "79",
      "title": "Emotion charting using real-time monitoring of physiological signals",
      "authors": [
        "A Rahim",
        "A Sagheer",
        "K Nadeem",
        "M Dar",
        "A Rahim",
        "U Akram"
      ],
      "year": "2019",
      "venue": "2019 International Conference on Robotics and Automation in Industry (ICRAI)"
    },
    {
      "citation_id": "80",
      "title": "Multi-modal emotion recognition using recurrence plots and transfer learning on physiological signals",
      "authors": [
        "R Elalamy",
        "M Fanourakis",
        "G Chanel"
      ],
      "year": "2021",
      "venue": "2021 9th International Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "81",
      "title": "Cross-subject emotion recognition using flexible analytic wavelet transform from eeg signals",
      "authors": [
        "V Gupta",
        "M Chopda",
        "R Pachori"
      ],
      "year": "2018",
      "venue": "IEEE Sensors Journal"
    },
    {
      "citation_id": "82",
      "title": "Eeg-based emotion recognition using multi-scale window deep forest",
      "authors": [
        "H Yao",
        "H He",
        "S Wang",
        "Z Xie"
      ],
      "year": "2019",
      "venue": "2019 IEEE Symposium Series on Computational Intelligence (SSCI)"
    },
    {
      "citation_id": "83",
      "title": "Facial expression-based emotion classification using electrocardiogram and respiration signals",
      "authors": [
        "D Wickramasuriya",
        "M Tessmer",
        "R Faghih"
      ],
      "year": "2019",
      "venue": "IEEE Healthcare Innovations and Point of Care Technologies"
    },
    {
      "citation_id": "84",
      "title": "Wedea: A new eeg-based framework for emotion recognition",
      "authors": [
        "S Kim",
        "H.-J Yang",
        "N Nguyen",
        "S Prabhakar",
        "S.-W Lee"
      ],
      "year": "2021",
      "venue": "IEEE Journal of Biomedical and Health Informatics"
    },
    {
      "citation_id": "85",
      "title": "Introducing wesad, a multimodal dataset for wearable stress and affect detection",
      "authors": [
        "P Schmidt",
        "A Reiss",
        "R Duerichen",
        "C Marberger",
        "K Van Laerhoven"
      ],
      "year": "2018",
      "venue": "Proceedings of the 20th ACM international conference on multimodal interaction"
    },
    {
      "citation_id": "86",
      "title": "Multi-level stress assessment using multidomain fusion of ecg signal",
      "authors": [
        "Z Ahmad",
        "N Khan"
      ],
      "year": "2020",
      "venue": "2020 42nd Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC)"
    },
    {
      "citation_id": "87",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "K He",
        "X Zhang",
        "S Ren",
        "J Sun"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "88",
      "title": "Using deep convolutional neural network for emotion detection on a physiological signals dataset (amigos)",
      "authors": [
        "L Santamaria-Granados",
        "M Munoz-Organero",
        "G Ramirez-Gonzalez",
        "E Abdulhay",
        "N Arunkumar"
      ],
      "year": "2018",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "89",
      "title": "Unsupervised domain adaptation for ecg arrhythmia classification",
      "authors": [
        "M Chen",
        "G Wang",
        "Z Ding",
        "J Li",
        "H Yang"
      ],
      "year": "2020",
      "venue": "2020 42nd Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC)"
    },
    {
      "citation_id": "90",
      "title": "Emotion recognition from multimodal physiological signals using a regularized deep fusion of kernel machine",
      "authors": [
        "X Zhang",
        "J Liu",
        "J Shen",
        "S Li",
        "K Hou",
        "B Hu",
        "J Gao",
        "T Zhang"
      ],
      "year": "2020",
      "venue": "IEEE transactions on cybernetics"
    },
    {
      "citation_id": "91",
      "title": "Internal emotion classification using eeg signal with sparse discriminative ensemble",
      "authors": [
        "H Ullah",
        "M Uzair",
        "A Mahmood",
        "M Ullah",
        "S Khan",
        "F Cheikh"
      ],
      "year": "2019",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "92",
      "title": "Hyperdimensional computing-based multimodality emotion recognition with physiological signals",
      "authors": [
        "E.-J Chang",
        "A Rahimi",
        "L Benini",
        "A.-Y Wu"
      ],
      "year": "2019",
      "venue": "2019 IEEE International Conference on Artificial Intelligence Circuits and Systems (AICAS)"
    },
    {
      "citation_id": "93",
      "title": "An ai-edge platform with multimodal wearable physiological signals monitoring sensors for affective computing applications",
      "authors": [
        "C.-J Yang",
        "N Fahier",
        "C.-Y He",
        "W.-C Li",
        "W.-C Fang"
      ],
      "year": "2020",
      "venue": "2020 IEEE International Symposium on Circuits and Systems (ISCAS)"
    },
    {
      "citation_id": "94",
      "title": "A comparative study of machine learning techniques for emotion recognition from peripheral physiological signals",
      "authors": [
        "S Vijayakumar",
        "R Flynn",
        "N Murray"
      ],
      "year": "2020",
      "venue": "2020 31st Irish Signals and Systems Conference (ISSC)"
    },
    {
      "citation_id": "95",
      "title": "Emotion recognition using three-dimensional feature and convolutional neural network from multichannel eeg signals",
      "authors": [
        "H Chao",
        "L Dong"
      ],
      "year": "2020",
      "venue": "IEEE sensors journal"
    },
    {
      "citation_id": "96",
      "title": "Discriminative-cca promoted by eeg signals for physiological-based emotion recognition",
      "authors": [
        "W Zhao",
        "Z Zhao",
        "C Li"
      ],
      "year": "2018",
      "venue": "2018 First Asian Conference on Affective Computing and Intelligent Interaction (ACII Asia"
    },
    {
      "citation_id": "97",
      "title": "Emotion recognition using fused physiological signals",
      "authors": [
        "D Fabiano",
        "S Canavan"
      ],
      "year": "2019",
      "venue": "2019 8th International Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "98",
      "title": "An eegbased multi-modal emotion database with both posed and authentic facial actions for emotion analysis",
      "authors": [
        "X Li",
        "X Zhang",
        "H Yang",
        "W Duan",
        "W Dai",
        "L Yin"
      ],
      "year": "2020",
      "venue": "2020 15th IEEE International Conference on Automatic Face and Gesture Recognition"
    },
    {
      "citation_id": "99",
      "title": "Cross-subject multimodal emotion recognition based on hybrid fusion",
      "authors": [
        "Y Cimtay",
        "E Ekmekcioglu",
        "S Caglar-Ozhan"
      ],
      "year": "2020",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "100",
      "title": "Wt feature based emotion recognition from multi-channel physiological signals with decision fusion",
      "authors": [
        "J Xie",
        "X Xu",
        "L Shu"
      ],
      "year": "2018",
      "venue": "2018 first asian conference on affective computing and intelligent interaction"
    },
    {
      "citation_id": "101",
      "title": "Multi-domain feature fusion for emotion classification using deap dataset",
      "authors": [
        "M Khateeb",
        "S Anwar",
        "M Alnowami"
      ],
      "year": "2021",
      "venue": "Ieee Access"
    },
    {
      "citation_id": "102",
      "title": "Biosignal-based multimodal emotion recognition in a valence-arousal affective framework applied to immersive video visualization",
      "authors": [
        "J Pinto",
        "A Fred",
        "H Da Silva"
      ],
      "year": "2019",
      "venue": "2019 41st Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)"
    },
    {
      "citation_id": "103",
      "title": "Exploiting eeg signals and audiovisual feature fusion for video emotion recognition",
      "authors": [
        "B Xing",
        "H Zhang",
        "K Zhang",
        "L Zhang",
        "X Wu",
        "X Shi",
        "S Yu",
        "S Zhang"
      ],
      "year": "2019",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "104",
      "title": "Emotional state estimation using sensor fusion of eeg and eda",
      "authors": [
        "M Yasemin",
        "M Sarıkaya",
        "G Ince"
      ],
      "year": "2019",
      "venue": "2019 41st Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)"
    },
    {
      "citation_id": "105",
      "title": "Multi-modal physiological data fusion for affect estimation using deep learning",
      "authors": [
        "M Hssayeni",
        "B Ghoraani"
      ],
      "year": "2021",
      "venue": "IEEE Access"
    }
  ]
}