{
  "paper_id": "2508.06640v1",
  "title": "Rethinking Key-Frame-Based Micro-Expression Recognition: A Robust And Accurate Framework Against Key-Frame Errors",
  "published": "2025-08-08T18:40:07Z",
  "authors": [
    "Zheyuan Zhang",
    "Weihao Tang",
    "Hong Chen"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Micro-expression recognition (MER) is a highly challenging task in affective computing. With the reduced-sized micro-expression (ME) input that contains key information based on key-frame indexes, key-frame-based methods have significantly improved the performance of MER. However, most of these methods focus on improving the performance with relatively accurate key-frame indexes, while ignoring the difficulty of obtaining accurate key-frame indexes and the objective existence of key-frame index errors, which impedes them from moving towards practical applications. In this paper, we propose CausalNet, a novel framework to achieve robust MER facing key-frame index errors while maintaining accurate recognition. To enhance robustness, CausalNet takes the representation of the entire ME sequence as the input. To address the information redundancy brought by the complete ME range input and maintain accurate recognition, first, the Causal Motion Position Learning Module (CMPLM) is proposed to help the model locate the muscle movement areas related to Action Units (AUs), thereby reducing the attention to other redundant areas. Second, the Causal Attention Block (CAB) is proposed to deeply learn the causal relationships between the muscle contraction and relaxation movements in MEs. Empirical experiments have demonstrated that on popular ME benchmarks, the CausalNet has achieved robust MER under different levels of key-frame index noise. Meanwhile, it has surpassed state-of-the-art (SOTA) methods on several standard MER benchmarks when using the provided annotated key-frames. Code is available at https://github. com/tony19980810/CausalNet.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Introduction",
      "text": "Expression is the manifestation of emotion directed toward given stimuli  [8, 10, 19, 41, 42] . In the real world, ex-Figure  1 . Compared with existing advanced methods, CausalNet ranks first in both robustness and accuracy assessment on the composite dataset including SAMM  [4] , CASME II  [35]  and SMIC  [13] . The robustness assessment is based on the mean values of UAR and UF1 under three different levels of key-frame index errors (see Sec. 4.2), while the accuracy assessment is through the standard evaluation with the annotated key-frames (see Sec. 4.3).\n\npressions can be roughly divided into two types: macroexpressions and micro-expressions (MEs). The former exhibits strong facial muscle movements for a long duration and is relatively easy to recognize. The latter is an involuntary, fleeting, and unconscious facial expression that occurs when a person fails to control their facial expressions. Therefore, the emotion conveyed by such expressions is relatively more genuine  [18, 21] . Nevertheless, when MEs occur, the muscle movements are weak, and the duration is short (less than 0.5 seconds  [5] ), which poses a huge challenge for both humans and machines to recognize. Therefore, accurately spotting and recognizing MEs is of great significance in affective computing  [14, 20, 23, 43] .\n\nOnset, apex, and offset frames correspond to the start, peak, and end of the muscle movements in MEs respectively. Based on the locations of these key-frames, keyframe-based methods with the core idea of \"less is more\"  [15]  filter out redundant frames without obvious expression information and take partial ME frames as input (e.g., only the apex frame  [27, 32]  and the onset-apex dynamic repre-sentation  [1, 3, 6, 12, 24, 30-33, 37, 38, 45] ). Owing to the extraction of crucial ME information, these methods have propelled the accuracy of MER to an entirely new level. However, despite the significant improvement in the accuracy of MER, the recognition performance in the practical applications of MEs has not been enhanced to the same extent. This has led us to rethink key-frame-based MER: In the research on improving MER performance, is the default use of relatively accurate key-frames annotated by experts for MER in line with the requirements of practical applications? Has the great difficulty in obtaining accurate keyframe locations and the impact of key-frame errors on MER been overlooked? Regardless of the answers to the above questions, an objective fact is that even among different experienced experts, because of individual biases, their keyframe annotation results for the same ME sample will still show certain differences. This implies that key-frame errors are always present, whether through manual annotation or by using automatic spotting algorithms. To quantify and evaluate the impact of errors on existing methods, we conduct experiments on the composite dataset of three ME benchmarks  [4, 13, 35]  through two evaluation approaches: manually introducing different levels of errors based on the annotated key-frames provided by the datasets and using the spotted indexes by the automatic spotting algorithm. Experimental results show that the recognition performance of existing key-frame-based methods is severely affected (see Sec. 4.2 for details). The low robustness of these methods hinders their practical applications.\n\nIn this paper, we focus on key-frame-based MER and propose CausalNet, a framework that is robust to key-frame index errors and can maintain accurate MER at the same time as shown in Fig.  1 . In terms of robustness, Causal-Net uses the onset-offset full ME sequence representation (onset-apex optical flow (OF) and apex-offset OF) as the input to achieve robust MER. Compared with partial ME input like onset-apex frames or only apex frames, a complete representation of MEs has better robustness in capturing the key information of facial muscle movement. In terms of accurate MER, we design two parts of networks: First, to enable the model to focus on the key areas and reduce attention to redundant information, we develop a causal motion position learning module. It provides the model with the position information of muscle movement by learning the causal relationship between the directions of muscle contraction and relaxation in the onset-apex and apex-offset phases. Second, to deeply explore the causal relationships between onset-apex and apex-offset MEs and address the weakening of the model's perception ability of local features as feature fusion deepens  [26] , we propose the causal attention block. Its unique design enables the model to perceive the overall temporal trend of MEs while maintaining sensitivity to local key ME information.\n\nIn brief, the core contributions of our work are: • We propose a novel framework using onset-apex OF and apex-offset OF as inputs to achieve robust and accurate MER facing key-frame index errors.  ods, CausalNet is not less robust than video-based methods to key-frame index errors. In addition, it effectively tackles information redundancy and shows stronger MER performance.\n\nOur study represents an initial endeavor to explore the enhancement of robustness in key-frame-based MER against key-frame errors. The proposed method aims to maintain robust and accurate MER, both in the absence and presence of key-frame errors.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "The Proposed Causalnet",
      "text": "Enhancing the robustness against key-frame index errors is a significant focus of CausalNet. As shown in Fig.  2 , CausalNet achieves robust MER by using onset-apex and apex-offset OFs to represent the whole ME sequence. To mitigate the focus of the model on redundant information and maintain accurate MER, CMPLM is proposed to learn AU-related position information through the OF direction maps, thus reducing attention to non-expression areas. In the CAB, the spatial-temporal causal attention is responsible for the information interaction. Meanwhile, based on the causality in the temporal dimension, it generates shortrange and long-range feature representations, so as to per-ceive the overall temporal trend of MEs while maintaining sensitivity to local key ME information. Causal relation mining enhances causal relationship learning through in-depth information interaction and mining of long-range features.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Causal Motion Position Learning Module",
      "text": "Causal Relations in MEs. As shown in Fig.  3 , we separately calculate the motion direction from the OFs, and visualize the OF motion directions using different hues. The visualized colors of the muscle movements in the AU regions (shown within the red solid-line boxes) show that the motion directions of muscles between the onset-apex and apex-offset phases are almost opposite. This indicates that the muscle movements in MEs follow specific patterns. During the onset-apex phase, the muscles contract, while during the apex-offset phase, the muscles relax. The directions of muscle movements in these two phases are almost opposite, forming a causal relationship.\n\nBuilding upon this discovery, we devise the causal motion position learning module, which takes the motion directions of the onset-apex and apex-offset OFs as inputs. The aim is to locate the AU-related muscle movement re-  gions by perceiving the changes in the motion directions of AU-related muscles during the contraction and relaxation phases. Subsequently, through position embedding, this valuable information is channeled to the main branch of the model. This empowers the model to concentrate its attention on the AU-related muscle movements, effectively filtering out other redundant information. The module consists of two distinct components: The direction encoder is responsible for feature extraction of the OF direction map, and the absolute positional cross attention enables information interaction between the two OF direction features, so as to keenly perceive the location details of the AU-related muscle movement regions. Introducing Absolute Position Information. Since the input face images are all cropped and aligned, the approximate regions of AU positions in the images are relatively fixed. In this scenario, the absolute position information of image patches enables the model to precisely identify the specific facial regions corresponding to different patches. The introduction of it will empower the model to more effectively capture the spatial relationships among facial features, thereby enhancing the performance of MER. To achieve this goal, inspired by StableMask  [36]  in natural language processing, we introduce pseudo-attention scores which converts the attention matrix into a non-right stochas-tic matrix (the sum of the elements in each row is no longer equal to 1) to encode the absolute position information. Absolute Positional Cross Attention uses a sparse attention mechanism for local information interaction. It adds pseudo-attention scores to the masked positions of the attention score matrix to encode absolute positional information. Specifically, after feature extraction by the direction encoder on two OF direction maps, two feature x pos1 , x pos2 ∈ R m 2 ×D is obtained. m stands for the resolution of the feature map and is equal to 2, while D denotes the feature dimension. For x pos1 (same for x pos2 ), the absolute positional cross attention can be expressed as:\n\nwhere K pos2 and V pos2 are obtained through a 1×1 convolution of x pos2 , while Q pos1 is obtained via a 1×1 convolution of x pos1 . √ d k serves as the scaling factor. SM is the softmax function. ⊙ represents element-wise multiplication, and M is a two-valued mask matrix to mask out certain locations according to the relative position of query (at i) and key/value (at j) tokens. (x i , y i ) and (x j , y j ) are the spatial locations of i and j. P is the two-valued matrix containing pseudo-attention score. Visualizations of P and M are shown in Fig.  4 (b) . γ is a positive decay rate hyperparameter. r is the neighborhood radius and equal to 1. After the absolute positional cross attention,\n\nHow Pseudo-attention Score Encodes Absolute Position. Since the original pseudo-attention is used together with the causal mask (first of Fig.  4  (a)), to adapt to the sparse attention, we made changes to the P (second of Fig.  4 (b) ). Here, we prove that the modified P can still encode absolute position information using the same example in StableMask  [36] : whether the model can encode positional information for an identical input sequence. let A i denote the real attention scores of the i-th row (A = QK T / √ d k ), P i denote the pseudo-attention scores in the i-th row , and j means the j-th column, we have:\n\nwhere SM Ai∪Pi (A i ) and SM Ai∪Pi (P i ) are the real/pseudo attention in each row. For an identical input sequence X = [x, x, x, x] (in our scenario, X is an OF image that contains 4 patches x), i̸ =j exp(A i,j ) remains constant as i increases as each row has three equal real attention score because of identical input (our M masks one query per row ), and i=j exp(P i,j ) for pseudo attention in mask position increases as i increases (exp(-4γ) < ... < exp(-γ)), we have\n\nwhich means after Eq. (  2 ), the output attention values become monotonic instead of being all the same:\n\nwhere W V is the weight matrix of V , and v is a single vector within V . This indicates that P in our method is consistent with the properties of StableMask. Different absolute positions have different sums of attention score, and the sum of attention score changes monotonically as the absolute position increases, thereby encoding the absolute positions of an identical input sequence.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Causal Attention Block",
      "text": "Spatial-temporal Causal Attention. As shown in Fig.  5 , this module first conducts sparse spatial information interaction, and its mechanism is consistent with that of the absolute positional cross attention. Then, the module performs causality-based information interaction in the temporal dimension to generate short-range and long-range representations. Compared with self-attention, due to causality, it ensures that as the feature fusion deepens, the short-range features only focus on short-range information. Specifically, after feature extraction by the encoder from two OF images, a feature x ∈ R 2×m 2 ×D composed of two tokens x ′ 1,2 ∈ R m 2 ×D is obtained. After adding the positional information from CMPLM, the two tokens are concatenated bidirectionally to get x f or and x back as the input for spatial-temporal causal attention. The purpose of the bidirectional features are to generate two short-range features that focus on the apex-offset range and the onset-apex range and two long-range features of the onset-offset range. The formula for spatial attention is as follows:\n\nwhere for the query (at i) and key/value (at j) tokens, t i = t j . t i , t j ∈ {1, 2} represents the temporal dimension for the tokens.\n\nFor efficient temporal attention, we let tokens at the same spatial positions interact as each token already holds global information after spatial interaction. The interaction adheres to causal attention, meaning previous tokens in the temporal dimension don't incorporate information from subsequent ones. Specifically, we only calculate attention for the second temporal-dimension token, and the formula is as follows:\n\nwhere for the query (at i) and key/value (at j) tokens, (x i , y i ) = (x j , y j ), t j ∈ {1, 2} and t i ∈ {2} because the above-mentioned temporal attention is only applied to the second OF feature to ensure causality. In the end, Causal Relation Mining is proposed to deeply explore the relation between the contraction and expansion of ME in the long-range features through information interaction. In terms of formulation, the process is as follows: , HTNet  [34] , SRMCL  [1]  and CausalNet. * represents that for these methods, relevant results of CDE task with annotated key-frames have not been reported in their paper. So we reproduce the results based on the provided code.\n\nwhere y long ∈ R m 2 ×D . In the end, y long , y f or 1 and y back are concatenated together to form y all ∈ R 3×m 2 ×D and sent to a fully-connected layer for classification. Implementation Details. The OF images (with a size of 28×28×3) consist of three channels (horizontal, vertical, and optical strain elements)  [34] . The experiments are conducted using PyTorch 2.2.1 and Python 3.8. In the training stage, we set the learning rate to 5e-5 and use a maximum of 800 epochs with the adaptive moment estimation optimizer  [11] . γ can be referred to in the code link, and we employ HTNet  [34]  for all encoders.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Experimental Results",
      "text": "",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Robust Mer With Inaccurate Key Frames",
      "text": "Error Setting. We assume key-frame errors follow a normal distribution centered on the datasets' manually annotated key-frame indexes. We set standard deviations (STDs) of 10, 20, and 30 frames and introduce them to apex, onset, and offset frames on the composite datasets to analyze performance changes of existing open-source state-of-the-art (SOTA) key-frame-based methods  [1, 7, 12, 32, 34, 38, 44]  and CausalNet. Given SMIC's half data acquisition frame rate of CASME II and SAMM, its STD is halved. It is worth noting that MMEW is not involved in this experiment. In addition to the manual error addition, we employ the apex frame spotting algorithm  [16]  for apex frame localization to introduce key-frame errors from automatic detection (single database evaluation using CASME II is conducted here). The sequence range for spotting is the annotated onset-offset sequences, where the predicted onset and offset frames are 25 frames apart from the predicted apex frame.\n\nRobustness Evaluation under Key-frames with Errors. In Fig.  6  (a) and (b), for manually-introduced errors on the composite dataset, the results show that CausalNet is in the leading position under the metrics of all three STDs. Specifically, when the STD is 10, 20, and 30 frames respectively, compared with the second-best method, the UF1 of CausalNet has improvements of 2.70%, 1.85%, and 1.42% respectively. In terms of the UAR, there are improvements of 2.47%, 2.70%, and 2.01% respectively. In Fig.  6  (c), for automatic apex-frames spotting on CASME II, CausalNet also achieves the best result with improvements of 2.62% and 1.06% for UAR and UF1 respectively, which demon-Figure  7 . Visualization of OFs and corresponding attention maps  [29]  in ground truth (GT) key-frames (red) and key-frames with errors (blue) on the three kind of key-frame-based method. On the right side is the visualization of the GT and the key-frames with errors on the time axis. For better visualization, we set the backbone to ResNet18  [9]  and the input size of the OFs to 224×224×3. The AU regions related to facial expressions have been marked with red boxes. Situations where there is less facial expression information in the apex frame or OF due to key-frame errors have been labeled as \"low quality\".\n\nstrates better robustness against key-frame errors.\n\nAnalysis of How CausalNet Achieves Robust MER. We have listed three situations where the apex-based and onsetapex-based methods are significantly affected by errors, as shown in Fig.  7 . In (a), the ME information is completely outside the onset-apex range, resulting in the onset-apex OF having no muscle information of AU 4. This will have a huge impact on the methods based only on the apex frame or the onset-apex OF. However, CausalNet encompasses a more comprehensive ME motion range, making it more probable to incorporate the peak muscle movement process. The muscle movement information lost is compensated in the apex-offset OF. In (b), since the onset frame with errors is too close to the apex frame, the relative motion between the two frames is not obvious. As a result, the quality of the calculated onset-apex OF is poor, which in turn affects the onset-apex-based MER. However, the apex-framebased method and CausalNet show good robustness in this situation. In (c), compared with the ground truth, the keyframes with errors are shifted backward as a whole. As a result, there is less facial expression information in the apex frame, which has a significant impact on the apex-based MER, while the onset-apex-based method and CausalNet remain robust in this situation. In addition, the situations where CausalNet is more robust to errors than other keyframe-based MER methods are not limited to the above three cases. The more comprehensive ME range input enables CausalNet to have better robustness.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Comparision With Sota Methods",
      "text": "The complete ME range input makes CausalNet more robust to key-frame errors.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Ablation Studies",
      "text": "To analyze the functions of each component within Causal-Net, we conduct ablation studies with the annotated key frames provided by the datasets. All the results are calculated from 5 independent runs, which are shown in Tab.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Conclusions",
      "text": "In this paper, we propose a novel framework using onsetapex OF and apex-offset OF as inputs to represent complete ME muscle movement for robust MER even with inaccurate key frame indexes. The causal motion position learning module and the causal attention block are proposed to enhance the attention of the network on key information to handle the whole ME sequence input range. Our method achieves robust MER under different levels of key frame index noise. Moreover, when using provided key-frames from the dataset, the proposed framework shows competitive performance in various standard ME benchmarks.\n\nLimitations. Although CausalNet demonstrates better robustness against key-frame errors, it still remains ineffective in handling cases where the expression lies entirely outside the onset-offset interval (i.e., f rame errors of f set < f rame GT onset or f rame GT of f set < f rame error onset ). In this situation, when large key-frame errors are present, further increasing the input range for MER provides limited improvement in recognition performance. Instead, optimizing the spotting algorithm or enhancing the training quality of manual annotations is likely to have a greater impact on improving current MER performance than refining the MER algorithm itself. These tasks will be explored in future work.",
      "page_start": 8,
      "page_end": 8
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Compared with existing advanced methods, CausalNet",
      "page": 1
    },
    {
      "caption": "Figure 1: In terms of robustness, Causal-",
      "page": 2
    },
    {
      "caption": "Figure 2: The overview of the proposed CausalNet with two main parts: the CMPLM (blue part) and the CAB (yellow part). CMPLM",
      "page": 3
    },
    {
      "caption": "Figure 3: , we sepa-",
      "page": 3
    },
    {
      "caption": "Figure 3: ME sequence, corresponding OFs and OF direction maps. We use different hues to represent different angles of motion directions",
      "page": 4
    },
    {
      "caption": "Figure 4: Visualizations of M and P in StableMask [36] (a) and",
      "page": 4
    },
    {
      "caption": "Figure 4: (b). γ is a positive decay rate hyperparam-",
      "page": 4
    },
    {
      "caption": "Figure 4: (a)), to adapt to the sparse atten-",
      "page": 5
    },
    {
      "caption": "Figure 4: (b)). Here,",
      "page": 5
    },
    {
      "caption": "Figure 5: Visualization of the proposed spatial-temporal causal",
      "page": 5
    },
    {
      "caption": "Figure 6: Results of robust MER evaluation under key frames with different levels of errors on the composite dataset (a and b) and",
      "page": 6
    },
    {
      "caption": "Figure 6: (a) and (b), for manually-introduced errors on",
      "page": 6
    },
    {
      "caption": "Figure 7: Visualization of OFs and corresponding attention maps [29] in ground truth (GT) key-frames (red) and key-frames with errors",
      "page": 7
    },
    {
      "caption": "Figure 7: In (a), the ME information is completely",
      "page": 7
    }
  ],
  "tables": [
    {
      "caption": "Table 1: Method Comparison on the CASME II, SMIC, SAMM and the composite dataset. The results of CausalNet are calculated from",
      "page": 8
    },
    {
      "caption": "Table 2: Method Comparison on the MMEW for three-class task. -",
      "page": 8
    },
    {
      "caption": "Table 3: Results of ablation studies of causal motion posi-",
      "page": 8
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Boosting micro-expression recognition via self-expression reconstruction and memory contrastive learning",
      "authors": [
        "Yongtang Bao",
        "Chenxi Wu",
        "Peng Zhang",
        "Caifeng Shan",
        "Yue Qi",
        "Xianye Ben"
      ],
      "year": "2008",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "2",
      "title": "Video-based facial micro-expression analysis: A survey of datasets, features and algorithms",
      "authors": [
        "Xianye Ben",
        "Yi Ren",
        "Junping Zhang",
        "Su-Jing Wang",
        "Kidiyo Kpalma",
        "Weixiao Meng",
        "Yong-Jin Liu"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "3",
      "title": "Multi-level flowdriven attention network for micro-expression recognition",
      "authors": [
        "Wenhao Cai",
        "Junli Zhao",
        "Ran Yi",
        "Minjing Yu",
        "Fuqing Duan",
        "Zhenkuan Pan",
        "Yong-Jin Liu",
        "Mfdan"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Circuits and Systems for Video Technology"
    },
    {
      "citation_id": "4",
      "title": "Samm: A spontaneous micro-facial movement dataset",
      "authors": [
        "Adrian Davison",
        "Cliff Lansley",
        "Nicholas Costen",
        "Kevin Tan",
        "Moi Hoon"
      ],
      "year": "2006",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "5",
      "title": "Constants across cultures in the face and emotion",
      "authors": [
        "Paul Ekman",
        "Wallace Friesen"
      ],
      "year": "1971",
      "venue": "Journal of personality and social psychology"
    },
    {
      "citation_id": "6",
      "title": "Selfme: Self-supervised motion learning for micro-expression recognition",
      "authors": [
        "Xinqi Fan",
        "Xueli Chen",
        "Mingjie Jiang",
        "Ali Shahid",
        "Hong Yan"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "7",
      "title": "Off-apexnet on micro-expression recognition system",
      "authors": [
        "Y Gan",
        "Sze-Teng Liong",
        "Wei-Chuen Yau",
        "Yen-Chang Huang",
        "Lit-Ken Tan"
      ],
      "year": "2019",
      "venue": "Signal Processing: Image Communication"
    },
    {
      "citation_id": "8",
      "title": "Kf-tmiaf: An efficient key-framesbased temporal modeling of images and au features structure for masked facial expression recognition",
      "authors": [
        "Huijie Gu",
        "Hanpu Wang",
        "Shiwei He",
        "Mengyan Li",
        "Jianmeng Zhou",
        "Tong Chen"
      ],
      "year": "2024",
      "venue": "2024 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)"
    },
    {
      "citation_id": "9",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "Kaiming He",
        "Xiangyu Zhang",
        "Shaoqing Ren",
        "Jian Sun"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "10",
      "title": "Weighted spatiotemporal feature and multi-task learning for masked facial expression recognition",
      "authors": [
        "Shiwei He",
        "Yingjuan Jia",
        "Hanpu Wang",
        "Xinyu Liu",
        "Jianmeng Zhou",
        "Huijie Gu",
        "Mengyan Li",
        "Tong Chen"
      ],
      "year": "2025",
      "venue": "Computational Visual Media"
    },
    {
      "citation_id": "11",
      "title": "Adam: A method for stochastic optimization",
      "authors": [
        "P Diederik",
        "Jimmy Kingma",
        "Ba"
      ],
      "year": "2014",
      "venue": "Adam: A method for stochastic optimization",
      "arxiv": "arXiv:1412.6980"
    },
    {
      "citation_id": "12",
      "title": "Mmnet: Muscle motion-guided network for microexpression recognition",
      "authors": [
        "Hanting Li",
        "Mingzhe Sui",
        "Zhaoqing Zhu",
        "Feng Zhao"
      ],
      "year": "2022",
      "venue": "Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence, IJCAI-22"
    },
    {
      "citation_id": "13",
      "title": "A spontaneous micro-expression database: Inducement, collection and baseline",
      "authors": [
        "Xiaobai Li",
        "Tomas Pfister",
        "Xiaohua Huang",
        "Guoying Zhao",
        "Matti Pietikäinen"
      ],
      "year": "2013",
      "venue": "2013 10th IEEE International Conference and Workshops on Automatic face and gesture recognition (fg)"
    },
    {
      "citation_id": "14",
      "title": "Deep learning for micro-expression recognition: A survey",
      "authors": [
        "Yante Li",
        "Jinsheng Wei",
        "Yang Liu",
        "Janne Kauttonen",
        "Guoying Zhao"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "15",
      "title": "Less is more: Micro-expression recognition from video using apex frame",
      "authors": [
        "Sze-Teng Liong",
        "John See",
        "Koksheik Wong",
        "Raphael C.-W Phan"
      ],
      "year": "2018",
      "venue": "Signal Processing: Image Communication"
    },
    {
      "citation_id": "16",
      "title": "Long video micro-expression spotting based on occ theory",
      "authors": [
        "Bingtong Liu",
        "Zheyuan Zhang",
        "Ju Zhou",
        "Hanpu Wang",
        "Tong Chen"
      ],
      "year": "2023",
      "venue": "2023 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)"
    },
    {
      "citation_id": "17",
      "title": "Tgmae: Selfsupervised micro-expression recognition with temporal gaussian masked autoencoder",
      "authors": [
        "Shifeng Liu",
        "Xinglong Mao",
        "Sirui Zhao",
        "Chaoyou Fu",
        "Ying Yu",
        "Tong Xu",
        "Enhong Chen"
      ],
      "year": "2024",
      "venue": "2024 IEEE International Conference on Multimedia and Expo (ICME)"
    },
    {
      "citation_id": "18",
      "title": "Mer-clip: Au-guided vision-language alignment for micro-expression recognition",
      "authors": [
        "Shifeng Liu",
        "Xinglong Mao",
        "Sirui Zhao",
        "Peiming Li",
        "Tong Xu",
        "Enhong Chen"
      ],
      "year": "2025",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "19",
      "title": "Human emotion and sto2: Dataset, pattern, and recognition of basic emotions",
      "authors": [
        "Xinyu Liu",
        "Tong Chen",
        "Ju Zhou",
        "Hanpu Wang",
        "Guangyuan Liu",
        "Xiaolan Fu"
      ],
      "year": "2025",
      "venue": "Pattern Recognition"
    },
    {
      "citation_id": "20",
      "title": "A lightweight dualstream network with an adaptive strategy for efficient microexpression recognition",
      "authors": [
        "Xinyu Liu",
        "Ju Zhou",
        "Feng Chen",
        "Shigang Li",
        "Hanpu Wang",
        "Yingjuan Jia",
        "Yuhao Shan"
      ],
      "year": "2025",
      "venue": "Sensors"
    },
    {
      "citation_id": "21",
      "title": "A lightweight dualstream network with an adaptive strategy for efficient microexpression recognition",
      "authors": [
        "Xinyu Liu",
        "Ju Zhou",
        "Feng Chen",
        "Shigang Li",
        "Hanpu Wang",
        "Yingjuan Jia",
        "Yuhao Shan"
      ],
      "venue": "Sensors"
    },
    {
      "citation_id": "22",
      "title": "Entire-detail motion dual-branch network for microexpression recognition",
      "authors": [
        "Bingyang Ma",
        "Lu Wang",
        "Qingfen Wang",
        "Haoran Wang",
        "Ruolin Li",
        "Lisheng Xu",
        "Yongchun Li",
        "Hongchao Wei"
      ],
      "year": "2025",
      "venue": "Pattern Recognition Letters"
    },
    {
      "citation_id": "23",
      "title": "A multi-prior fusion network for video-based micro-expression recognition",
      "authors": [
        "Chuang Ma",
        "Shaokai Zhao",
        "Yu Pei",
        "Liang Xie",
        "Erwei Yin",
        "Ye Yan"
      ],
      "year": "2025",
      "venue": "ICASSP 2025 -2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "24",
      "title": "Micron-bert: Bert-based facial micro-expression recognition",
      "authors": [
        "Xuan-Bac Nguyen",
        "Chi Duong",
        "Xin Li",
        "Susan Gauch",
        "Han-Seok Seo",
        "Khoa Luu"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "25",
      "title": "Diverse local facial behaviors learning from enhanced expression flow for microexpression recognition",
      "authors": [
        "Rongrong Ni",
        "Biao Yang",
        "Xu Zhou",
        "Siyang Song",
        "Xiaofeng Liu"
      ],
      "year": "2023",
      "venue": "Knowledge-Based Systems"
    },
    {
      "citation_id": "26",
      "title": "Conformer: Local features coupling global representations for visual recognition",
      "authors": [
        "Zhiliang Peng",
        "Wei Huang",
        "Shanzhi Gu",
        "Lingxi Xie",
        "Yaowei Wang",
        "Jianbin Jiao",
        "Qixiang Ye"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF international conference on computer vision"
    },
    {
      "citation_id": "27",
      "title": "Capsulenet for micro-expression recognition",
      "authors": [
        "Nguyen Van Quang",
        "Jinhee Chun",
        "Takeshi Tokuyama"
      ],
      "year": "2019",
      "venue": "2019 14th IEEE International Conference on Automatic Face and Gesture Recognition (FG 2019)"
    },
    {
      "citation_id": "28",
      "title": "Megc 2019-the second facial microexpressions grand challenge",
      "authors": [
        "John See",
        "Hoon Moi",
        "Jingting Yap",
        "Xiaopeng Li",
        "Su-Jing Hong",
        "Wang"
      ],
      "year": "2019",
      "venue": "2019 14th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2019)"
    },
    {
      "citation_id": "29",
      "title": "Grad-cam: Visual explanations from deep networks via gradient-based localization",
      "authors": [
        "Michael Ramprasaath R Selvaraju",
        "Abhishek Cogswell",
        "Ramakrishna Das",
        "Devi Vedantam",
        "Dhruv Parikh",
        "Batra"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE International Conference on Computer Vision"
    },
    {
      "citation_id": "30",
      "title": "Microexpression to macroexpression: Facial expression magnification by single input",
      "authors": [
        "Yaqi Song",
        "Tong Chen",
        "Shigang Li",
        "Jianfeng Li"
      ],
      "year": "2024",
      "venue": "2024 IEEE International Conference on Robotics and Automation (ICRA)"
    },
    {
      "citation_id": "31",
      "title": "Rnas-mer: A refined neural architecture search with hybrid spatiotemporal operations for micro-expression recognition",
      "authors": [
        "Monu Verma",
        "Priyanka Lubal",
        "Santosh Vipparthi",
        "Mohamed Abdel-Mottaleb"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision"
    },
    {
      "citation_id": "32",
      "title": "Micro-attention for micro-expression recognition",
      "authors": [
        "Chongyang Wang",
        "Min Peng",
        "Tao Bi",
        "Tong Chen"
      ],
      "year": "2006",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "33",
      "title": "A cross-database micro-expression recognition framework based on meta-learning",
      "authors": [
        "Hanpu Wang",
        "Ju Zhou",
        "Xinyu Liu",
        "Yingjuan Jia",
        "Tong Chen"
      ],
      "year": "2025",
      "venue": "Applied Intelligence"
    },
    {
      "citation_id": "34",
      "title": "Htnet for micro-expression recognition",
      "authors": [
        "Zhifeng Wang",
        "Kaihao Zhang",
        "Wenhan Luo",
        "Ramesh Sankaranarayana"
      ],
      "year": "2008",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "35",
      "title": "Casme ii: An improved spontaneous micro-expression database and the baseline evaluation",
      "authors": [
        "Wen-Jing Yan",
        "Xiaobai Li",
        "Su-Jing Wang",
        "Guoying Zhao",
        "Yong-Jin Liu",
        "Yu-Hsin Chen",
        "Xiaolan Fu"
      ],
      "year": "2006",
      "venue": "PloS one"
    },
    {
      "citation_id": "36",
      "title": "Stablemask: Refining causal masking in decoder-only transformer",
      "authors": [
        "Xuzheng Qingyu Yin",
        "Xiang He",
        "Yu Zhuang",
        "Jianhua Zhao",
        "Xiaoyu Yao",
        "Qiang Shen",
        "Zhang"
      ],
      "year": "2024",
      "venue": "Forty-first International Conference on Machine Learning"
    },
    {
      "citation_id": "37",
      "title": "Feature representation learning with adaptive displacement generation and trans-former fusion for micro-expression recognition",
      "authors": [
        "Zhijun Zhai",
        "Jianhui Zhao",
        "Chengjiang Long",
        "Wenju Xu",
        "Shuangjiang He",
        "Huijuan Zhao"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "38",
      "title": "Off-tanet: A lightweight neural micro-expression recognizer with optical flow features and integrated attention mechanism",
      "authors": [
        "Jiahao Zhang",
        "Feng Liu",
        "Aimin Zhou"
      ],
      "year": "2021",
      "venue": "Pacific Rim International Conference on Artificial Intelligence"
    },
    {
      "citation_id": "39",
      "title": "Short and long range relation based spatio-temporal transformer for micro-expression recognition",
      "authors": [
        "Liangfei Zhang",
        "Xiaopeng Hong",
        "Ognjen Arandjelović",
        "Guoying Zhao"
      ],
      "year": "1973",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "40",
      "title": "Hfa-net: hierarchical feature aggregation network for micro-expression recognition",
      "authors": [
        "Meng Zhang",
        "Wenzhong Yang",
        "Liejun Wang",
        "Zhonghua Wu",
        "Danny Chen"
      ],
      "year": "2025",
      "venue": "Complex & Intelligent Systems"
    },
    {
      "citation_id": "41",
      "title": "Masked facial expression recognition based on temporal overlap module and action unit graph convolutional network",
      "authors": [
        "Zheyuan Zhang",
        "Bingtong Liu",
        "Ju Zhou",
        "Hanpu Wang",
        "Xinyu Liu",
        "Bing Lin",
        "Tong Chen"
      ],
      "year": "2025",
      "venue": "Journal of Visual Communication and Image Representation"
    },
    {
      "citation_id": "42",
      "title": "Seeing through the mask: Recognition of genuine emotion through masked facial expression",
      "authors": [
        "Ju Zhou",
        "Xinyu Liu",
        "Hanpu Wang",
        "Zheyuan Zhang",
        "Tong Chen",
        "Xiaolan Fu",
        "Guangyuan Liu"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Computational Social Systems"
    },
    {
      "citation_id": "43",
      "title": "Ulme-gan: a generative adversarial network for micro-expression sequence generation",
      "authors": [
        "Ju Zhou",
        "Sirui Sun",
        "Haolin Xia",
        "Xinyu Liu",
        "Hanpu Wang",
        "Tong Chen"
      ],
      "year": "2024",
      "venue": "Applied Intelligence"
    },
    {
      "citation_id": "44",
      "title": "Dual-inception network for cross-database micro-expression recognition",
      "authors": [
        "Ling Zhou",
        "Qirong Mao",
        "Luoyang Xue"
      ],
      "year": "2019",
      "venue": "2019 14th IEEE International Conference on Automatic Face and Gesture Recognition (FG 2019)"
    },
    {
      "citation_id": "45",
      "title": "Feature refinement: An expression-specific feature learning and fusion method for micro-expression recognition",
      "authors": [
        "Ling Zhou",
        "Qirong Mao",
        "Xiaohua Huang",
        "Feifei Zhang",
        "Zhihong Zhang"
      ],
      "year": "2008",
      "venue": "Pattern Recognition"
    },
    {
      "citation_id": "46",
      "title": "Learning to rank onsetoccurring-offset representations for micro-expression recognition",
      "authors": [
        "Jie Zhu",
        "Yuan Zong",
        "Jingang Shi",
        "Cheng Lu",
        "Hongli Chang",
        "Wenming Zheng"
      ],
      "year": "2025",
      "venue": "IEEE Transactions on Affective Computing"
    }
  ]
}