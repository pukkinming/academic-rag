{
  "paper_id": "2407.07653v1",
  "title": "Affectgpt: Dataset And Framework For Explainable Multimodal Emotion Recognition",
  "published": "2024-07-10T13:34:14Z",
  "authors": [
    "Zheng Lian",
    "Haiyang Sun",
    "Licai Sun",
    "Jiangyan Yi",
    "Bin Liu",
    "Jianhua Tao"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Explainable Multimodal Emotion Recognition (EMER) is an emerging task that aims to achieve reliable and accurate emotion recognition. However, due to the high annotation cost, the existing dataset (denoted as EMER-Fine) is small, making it difficult to perform supervised training. To reduce the annotation cost and expand the dataset size, this paper reviews the previous dataset construction process. Then, we simplify the annotation pipeline, avoid manual checks, and replace the closedsource models with open-source models. Finally, we build EMER-Coarse, a coarsely-labeled dataset containing large-scale samples. Besides the dataset, we propose a two-stage training framework AffectGPT. The first stage exploits EMER-Coarse to learn a coarse mapping between multimodal inputs and emotion-related descriptions; the second stage uses EMER-Fine to better align with manuallychecked results. Experimental results demonstrate the effectiveness of our proposed method on the challenging EMER task. To facilitate further research, we will make the code and dataset available at: https://github.com/zeroQiaoba/AffectGPT.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Emotion recognition is an important research topic in human-computer interaction. Its main goal is to predict the most likely label from a fixed space  [1]  (such as the seven basic emotions in Ekman's theory  [2] ). However, emotions are complex. Limiting the label space and fixing the number of predictions may lead to inaccurate descriptions of emotions. Meanwhile, traditional emotion recognition lacks the explanation process, which is crucial to enhance the annotation reliability.\n\nTo this end, researchers propose a new task called Explainable Multimodal Emotion Recognition (EMER)  [3] . Unlike traditional emotion recognition, EMER exploits multi-modal and multi-faceted clues to predict emotions in an open-vocabulary (OV) manner. These clues can also serve as support and evidence for these predictions. Therefore, EMER provides a promising way for accurate and reliable emotion recognition. However, due to the high annotation cost, previous works only contain a small number of labeled samples (denoted as EMER-Fine)  [3] . These samples can only evaluate the performance of pre-trained systems and are not enough for supervised training.\n\nTo reduce the annotation cost, we review the previous dataset construction process. It contains four steps: pre-labeling audio and video clues, manually checking these clues, disambiguating subtitles, and translating to obtain bilingual descriptions  [3] . This process relies on manual checks and closedsource models. To reduce the annotation cost, we try to avoid manual checks and use open-source models instead. Then, we build EMER-Coarse, a coarsely-labeled dataset containing large-scale data. Since emotion recognition focuses on identifying human emotional states, we construct this dataset based on MER2024-SEMI  [4] , which contains 115,595 human-centric videos.\n\nBesides EMER-Coarse, we propose AffectGPT, a two-stage training framework for EMER. In the first stage, we use large-scale EMER-Coarse to learn a coarse alignment between multimodal inputs and emotion-related descriptions. In the second stage, we use small-scale EMER-Fine to better align with manually-checked results. The main contributions of this paper can be summarized as follows:\n\n• (Dataset) We build EMER-Coarse, a large-scale dataset for EMER. This dataset contains 115,595 samples, much more than previous datasets and sufficient for supervised training. • (Method) We propose AffectGPT, a two-stage framework for EMER. The first stage learns a coarse mapping and the second stage better aligns with manually-checked results. • (Performance) Experimental results demonstrate the effectiveness of this framework. Our systematic analysis can also provide some inspiration for subsequent researchers.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Task And Evaluation",
      "text": "This section reviews the task definition and evaluation metrics of EMER. Unlike traditional emotion recognition, EMER aims to predict emotions in an explainable and open-vocabulary manner. Following previous works  [3] , we focus on emotion recognition and use the overlap between predicted and annotated results as the evaluation metric. Since we do not fix the label space, different models may generate synonyms. To remove their impacts, we first group all labels using GPT-3.5  [5]  (\"gpt-3.5turbo-16k-0613\"): Please assume the role of an expert in the field of emotions. We provide a set of emotions. Please group the emotions, with each group containing emotions with the same meaning.\n\nDirectly output the results. The output format should be a list containing multiple lists.\n\nSpecifically, assume that G(•) is the GPT-generated mapping function between labels and group IDs. {y i } M i=1 and {ŷ i } N i=1 are the annotated and predicted labels, respectively. Here, M and N are the number of labels. Before metric calculation, we first map each label into its group ID:\n\nThen, we calculate the average of precision and recall as the final metric:",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Emer-Coarse",
      "text": "This section reviews the previous dataset construction pipeline  [3]  and attempts to reduce the annotation cost. Specifically, the previous pipeline consists of four steps: pre-labeling to generate multimodal clues, manual checking these clues, disambiguation of subtitles, and translation to obtain bilingual descriptions. The main cost lies in manual checks and the use of closed-source models for pre-labeling, disambiguation, and translation. To reduce the cost, we try to avoid manual checks and replace these closed-source models with open-source models. In this section, we test the mainstream open-source LLMs and MLLMs. Since the results vary slightly between distinct runs, we run all experiments twice and report the average score and standard deviation.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Pre-Labeling",
      "text": "Previously, the pre-labeling process relied on the closed-source GPT-4 (\"gpt-4-vision-preview\").\n\nTo find its replacement, we evaluate the performance of some representative open-source MLLMs.\n\nAccording to previous findings  [3] , adding subtitles using a two-step strategy can achieve better performance, i.e., first extracting emotion-related descriptions from MLLMs and then using them to disambiguate the subtitle. In this section, we follow this strategy and report results in Table  1 . In this table, some results are taken from previous works  [3]  as they follow the same experimental setup.\n\nBesides the single MLLM, can we achieve better performance if we combine different MLLMs? To answer this question, we further select the top-performing audio and video MLLMs and report the performance of their combinations. In Table  1 , we observe that these combinations usually bring performance improvement. Among them, the combination of SALMONN and Chat-UniVi performs best, even surpassing GPT-4. Therefore, we use it for pre-labeling.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Disambiguation And Translation",
      "text": "Disambiguation and translation deal with plain text data and these modules previously relied on GPT-3.5. To find its alternative, we test some typical open-source LLMs. Experimental results are shown in Table  2 . We observe that if only the translation module is replaced with open-source LLMs, the performance drop is small. But if we replace both translation and disambiguation, the performance drop is obvious. These results show that for non-complex tasks (e.g., translation), the performance of open-source LLMs is close to GPT-3.5. But for complex tasks (e.g., disambiguation), there is still a gap between open-source LLMs and GPT-3.5. The reason may be that we do not test larger LLMs due to limited GPU memory. Generally, larger LLMs help solve more complex tasks, which is left for our future work. Meanwhile, we observe that Qwen2-7B performs better than LLaMA3-8B in translation. Therefore, we use Qwen2-7B for translation and GPT-3.5 for disambiguation. This replacement reduces the OpenAI API call cost and maintains the overall performance.\n\nFinally, we use the above strategy to automatically annotate MER2024-SEMI  [4] . These annotation results take into account all acoustic, visual, and lexical clues. Since these results have not been manually checked, there may be some inaccuracies. We call this dataset EMER-Coarse. Training Process The first stage uses EMER-Coarse to learn a coarse alignment between multimodal inputs and emotion-related outputs. The second stage uses EMER-Fine to better align with manually-checked results. Considering that EMER-Fine has more reliable labels, we evaluate the performance of different systems on it. However, the second stage is also trained on EMER-Fine, so we further split it into training and test sets. The statistics are shown in Table  3 . During training, we freeze the weights of the acoustic encoder, visual encoder, and LLM, and only train Q-Former to learn the mapping between unimodal encoders and LLM.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Results And Discussion",
      "text": "AffectGPT is a two-stage training framework. To verify its effectiveness, we perform ablation studies on each stage. Considering that Video-LLaMA provides pretrained Q-Formers, we first reveal their necessity and study whether AffectGPT can be trained directly on randomly initialized weights. Then, we study the impact of different LLMs and discuss the necessity of each stage. Finally, we show the performance of AffectGPT on the EMER task. For convenience, in this section, we abbreviate the first stage as stage1 and the second stage as stage2.\n\nDuring training, AffectGPT learns a mapping between audio-video-text inputs and emotion-related outputs. These outputs are in English and have already considered the disambiguation process (see Section 3). In the previous evaluation pipeline (see Table  1 ), we need additional translation and disambiguation operations, which increases the evaluation cost. To reduce the cost, in this section, we extract emotion labels directly from the output of AffectGPT for performance evaluation.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Ablation Study On Stage1",
      "text": "Choice of Evaluation Set Video-LLaMA provides pretrained Q-Formers. In this section, we try to analyze whether these weights can help the model converge and achieve better performance. Before comparing different initialization strategies, we need to determine which dataset should be used for evaluation. In this paper, we have three choices: the training set, the test set, and the entire EMER-Fine. In Figure  1 , we present the results on different datasets. We observe that increasing the number of samples can reduce the fluctuation of accuracy and help us draw more reliable conclusions. Therefore, in stage1, we evaluate the performance on the entire EMER-Fine. It should be noted that further increasing the dataset size may obtain more stable results, therefore we plan to expand EMER-Fine in the future.    . Interestingly, we observe that the training loss of LLaMA-2 is lower than that of Vicuna, but Vicuna performs better than LLaMA-2 in emotion recognition. The reason may be that we fix the weights of LLMs and do not use LoRA for supervised fine-tuning, which may limit the performance of LLaMA-2 on downstream tasks. Meanwhile, these results also prove that there is no strong correlation between training loss and test accuracy. From another perspective, these results also show that LLMs affect the performance of AffectGPT. Therefore, we plan to explore the impact of other LLMs in the future.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Ablation Study On Stage2",
      "text": "Choice of Evaluation Set In stage1, we choose the entire EMER-Fine for performance evaluation. But for stage2, which part of the dataset should we use? Figure  4  shows the results on different sets.\n\nIn Figure  4 (b), we observe that the training accuracy steadily improves with increasing epochs. These results prove that our model can well fit training data. It is not appropriate to use the training accuracy for performance evaluation. In Figure  4 (c), we observe that the test accuracy fluctuates greatly. The reason may be that the test data is limited. Therefore, in subsequent analysis, we use the smoothed test accuracy for performance evaluation.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Main Results",
      "text": "In Table  4",
      "page_start": 7,
      "page_end": 7
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: , we present the results on different datasets. We observe that increasing the",
      "page": 4
    },
    {
      "caption": "Figure 1: Ablation study on stage1. In these figures, we train models with different initialization",
      "page": 5
    },
    {
      "caption": "Figure 2: reveals the impact of different initialization strategies.",
      "page": 5
    },
    {
      "caption": "Figure 2: (a) shows the curve of training loss. We observe that the model converges around 100",
      "page": 5
    },
    {
      "caption": "Figure 2: (b) shows the emotion recognition results. We observe that",
      "page": 5
    },
    {
      "caption": "Figure 2: Impact of different initialization strategies. We plot the curve of training loss and accuracy.",
      "page": 5
    },
    {
      "caption": "Figure 3: (a) shows the training loss and Figure 3(b) shows the",
      "page": 6
    },
    {
      "caption": "Figure 3: Impact of different LLMs. We use the random initialization strategy and evaluate the",
      "page": 6
    },
    {
      "caption": "Figure 4: shows the results on different sets.",
      "page": 6
    },
    {
      "caption": "Figure 4: (b), we observe that the training accuracy steadily improves with increasing epochs. These",
      "page": 6
    },
    {
      "caption": "Figure 4: (c), we observe that the test accuracy fluctuates greatly. The",
      "page": 6
    },
    {
      "caption": "Figure 4: Ablation study on stage2. In these figures, we show the results on different subsets.",
      "page": 6
    },
    {
      "caption": "Figure 5: From the training loss",
      "page": 7
    },
    {
      "caption": "Figure 5: Necessity of two-stage training framework.",
      "page": 7
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Translate\nDisambiguate": "GPT-3.5\nGPT-3.5",
          "English\nAvg\nRecalls\nAccuracys": "59.47±0.08\n51.62±0.00\n67.31±0.15",
          "Chinese\nAvg\nRecalls\nAccuracys": "57.54±0.06\n51.65±0.06\n63.42±0.06"
        },
        {
          "Translate\nDisambiguate": "LLaMA3-8B\nGPT-3.5\nLLaMA3-8B\nLLaMA3-8B",
          "English\nAvg\nRecalls\nAccuracys": "57.13±0.27\n49.63±0.32\n64.64±0.22\n55.50±0.09\n49.91±0.04\n61.08±0.22",
          "Chinese\nAvg\nRecalls\nAccuracys": "55.50±0.02\n50.85±0.19\n60.15±0.16\n52.59±0.74\n47.03±0.42\n58.15±1.05"
        },
        {
          "Translate\nDisambiguate": "Qwen2-7B\nGPT-3.5\nQwen2-7B\nQwen2-7B",
          "English\nAvg\nRecalls\nAccuracys": "58.22±0.11\n49.68±0.21\n66.76±0.00\n53.38±0.60\n44.74±0.67\n62.01±0.54",
          "Chinese\nAvg\nRecalls\nAccuracys": "56.65±0.27\n52.95±0.23\n60.36±0.32\n55.15±0.03\n47.92±0.06\n62.37±0.12"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Stage1\nStage2": "–\n–\n–\nbest",
          "Pretrained Weights\nAvg\nRecalls\nAccuracys": "28.64\n32.22\n25.05\n61.75\n62.03\n61.46",
          "Random Weights\nAvg\nRecalls\nAccuracys": "05.87\n07.58\n04.17\n58.22\n59.60\n56.84"
        },
        {
          "Stage1\nStage2": "50-epoch\n–\n50-epoch\nbest",
          "Pretrained Weights\nAvg\nRecalls\nAccuracys": "53.82\n48.04\n59.60\n62.78\n63.11\n62.45",
          "Random Weights\nAvg\nRecalls\nAccuracys": "50.06\n42.36\n57.76\n65.08\n64.29\n65.86"
        },
        {
          "Stage1\nStage2": "100-epoch\n–\n100-epoch\nbest",
          "Pretrained Weights\nAvg\nRecalls\nAccuracys": "56.65\n47.53\n65.78\n64.56\n64.49\n64.62",
          "Random Weights\nAvg\nRecalls\nAccuracys": "48.04\n40.51\n55.56\n62.88\n65.91\n59.85"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Models": "Otter\nVideoChat\nVideoChat2\nVideo-LLaVA\nVideo-LLaMA\nVideo-ChatGPT\nLLaMA-VID\nmPLUG-Owl\nChat-UniVi",
          "Support Modality": "Video, Text\nVideo, Text\nVideo, Text\nVideo, Text\nVideo, Text\nVideo, Text\nVideo, Text\nVideo, Text\nVideo, Text",
          "Link": "https://github.com/Luodian/Otter\nhttps://github.com/OpenGVLab/Ask-Anything/tree/main/video_chat\nhttps://github.com/OpenGVLab/Ask-Anything/tree/main/video_chat2\nhttps://github.com/PKU-YuanGroup/Video-LLaVA\nhttps://github.com/DAMO-NLP-SG/Video-LLaMA\nhttps://github.com/mbzuai-oryx/Video-ChatGPT\nhttps://github.com/dvlab-research/LLaMA-VID\nhttps://github.com/X-PLUG/mPLUG-Owl\nhttps://github.com/PKU-YuanGroup/Chat-UniVi"
        },
        {
          "Models": "SALMONN\nQwen-Audio\nSECap",
          "Support Modality": "Audio, Text\nAudio, Text\nAudio, Text",
          "Link": "https://github.com/bytedance/SALMONN\nhttps://github.com/QwenLM/Qwen-Audio\nhttps://github.com/thuhcsi/SECap"
        },
        {
          "Models": "OneLLM\nPandaGPT",
          "Support Modality": "Audio, Video, Text\nAudio, Video, Text",
          "Link": "https://github.com/csuhan/OneLLM\nhttps://github.com/yxuansu/PandaGPT"
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "SECap + mPLUG-Owl\n√ √ √\nSALMONN + Video-ChatGPT\n√ √ √\nSECap + Video-ChatGPT\n√ √ √\nSECap + Chat-UniVi\n√ √ √\nSALMONN + mPLUG-Owl\n√ √ √\nSALMONN + Chat-UniVi\n√ √ √": "EMER(Multi)",
          "57.71±0.05 50.05±0.23 65.38±0.33 55.22±0.22 51.65±0.27 58.79±0.16\n58.71±0.24 53.16±0.17 64.26±0.31 55.10±0.16 53.44±0.14 56.76±0.19\n57.41±0.09 52.03±0.04 62.79±0.14 56.49±0.02 56.50±0.01 56.48±0.05\n59.13±0.08 48.85±0.29 69.41±0.13 56.49±0.14 52.38±0.07 60.59±0.22\n59.77±0.05 51.77±0.01 67.76±0.11 55.94±0.21 51.74±0.19 60.14±0.23\n59.47±0.08 51.62±0.00 67.31±0.15 57.54±0.06 51.65±0.06 63.42±0.06": "80.05±0.24 80.03±0.37 80.07±0.10 85.20±0.03 87.09±0.00 83.31±0.05"
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "SECap + mPLUG-Owl\n√ √ √\nSALMONN + Video-ChatGPT\n√ √ √\nSECap + Video-ChatGPT\n√ √ √\nSECap + Chat-UniVi\n√ √ √\nSALMONN + mPLUG-Owl\n√ √ √\nSALMONN + Chat-UniVi\n√ √ √": "EMER(Multi)",
          "56.07±0.02 48.11±0.38 64.02±0.35 54.27±0.21 50.73±0.18 57.81±0.24\n58.46±0.18 53.09±0.04 63.84±0.32 55.17±0.05 52.60±0.04 57.74±0.14\n57.16±0.02 52.13±0.00 62.18±0.05 56.84±0.11 57.76±0.06 55.91±0.16\n58.82±0.08 48.22±0.20 69.42±0.03 54.74±0.03 51.03±0.10 58.44±0.05\n58.44±0.00 50.91±0.08 65.98±0.08 55.27±0.18 51.22±0.16 59.33±0.19\n58.69±0.04 50.59±0.01 66.79±0.09 57.85±0.05 52.51±0.05 63.20±0.04": "80.23±0.25 79.81±0.44 80.65±0.06 84.68±0.02 87.02±0.09 82.34±0.06"
        }
      ],
      "page": 11
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "SECap + mPLUG-Owl\n√ √ √\nSALMONN + Video-ChatGPT\n√ √ √\nSECap + Video-ChatGPT\n√ √ √\nSECap + Chat-UniVi\n√ √ √\nSALMONN + mPLUG-Owl\n√ √ √\nSALMONN + Chat-UniVi\n√ √ √": "EMER (Multi)",
          "64.42±0.32 57.95±0.38 70.90±0.26 59.00±0.25 55.30±0.63 62.70±0.13\n59.71±0.47 53.48±0.69 65.93±0.25 54.82±0.63 56.82±0.88 52.83±0.38\n58.43±0.35 51.60±0.19 65.26±0.51 55.11±0.35 51.45±0.32 58.76±0.38\n60.38±0.07 51.39±0.66 69.37±0.53 63.71±0.83 57.98±0.74 69.43±0.91\n65.15±0.26 55.28±0.26 75.03±0.26 58.57±0.35 53.78±0.32 63.36±0.38\n62.64±0.22 55.84±0.06 69.44±0.38 56.25±0.11 48.17±0.09 64.33±0.13": "79.31±0.19 80.91±0.13 77.70±0.25 87.29±0.19 87.37±0.38 87.20±0.00"
        }
      ],
      "page": 11
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Merbench: A unified evaluation benchmark for multimodal emotion recognition",
      "authors": [
        "Zheng Lian",
        "Licai Sun",
        "Yong Ren",
        "Hao Gu",
        "Haiyang Sun",
        "Lan Chen",
        "Bin Liu",
        "Jianhua Tao"
      ],
      "year": "2024",
      "venue": "Merbench: A unified evaluation benchmark for multimodal emotion recognition",
      "arxiv": "arXiv:2401.03429"
    },
    {
      "citation_id": "2",
      "title": "Universal facial expressions of emotion",
      "authors": [
        "Paul Ekman",
        "Dacher Keltner"
      ],
      "year": "1970",
      "venue": "California mental health research digest"
    },
    {
      "citation_id": "3",
      "title": "Explainable multimodal emotion reasoning",
      "authors": [
        "Zheng Lian",
        "Licai Sun",
        "Mingyu Xu",
        "Haiyang Sun",
        "Ke Xu",
        "Zhuofan Wen",
        "Shun Chen",
        "Bin Liu",
        "Jianhua Tao"
      ],
      "year": "2023",
      "venue": "Explainable multimodal emotion reasoning",
      "arxiv": "arXiv:2306.15401"
    },
    {
      "citation_id": "4",
      "title": "Mer 2024: Semi-supervised learning, noise robustness, and open-vocabulary multimodal emotion recognition",
      "authors": [
        "Zheng Lian",
        "Haiyang Sun",
        "Licai Sun",
        "Zhuofan Wen",
        "Siyuan Zhang",
        "Shun Chen",
        "Hao Gu",
        "Jinming Zhao",
        "Ziyang Ma",
        "Xie Chen"
      ],
      "year": "2024",
      "venue": "Mer 2024: Semi-supervised learning, noise robustness, and open-vocabulary multimodal emotion recognition",
      "arxiv": "arXiv:2404.17113"
    },
    {
      "citation_id": "5",
      "title": "",
      "authors": [
        "Openai",
        "Chatgpt"
      ],
      "year": "2022",
      "venue": ""
    },
    {
      "citation_id": "6",
      "title": "Qwen-audio: Advancing universal audio understanding via unified large-scale audio-language models",
      "authors": [
        "Yunfei Chu",
        "Jin Xu",
        "Xiaohuan Zhou",
        "Qian Yang",
        "Shiliang Zhang",
        "Zhijie Yan",
        "Chang Zhou",
        "Jingren Zhou"
      ],
      "year": "2023",
      "venue": "Qwen-audio: Advancing universal audio understanding via unified large-scale audio-language models",
      "arxiv": "arXiv:2311.07919"
    },
    {
      "citation_id": "7",
      "title": "One framework to align all modalities with language",
      "authors": [
        "Jiaming Han",
        "Kaixiong Gong",
        "Yiyuan Zhang",
        "Jiaqi Wang",
        "Kaipeng Zhang",
        "Dahua Lin",
        "Yu Qiao",
        "Peng Gao",
        "Xiangyu Yue",
        "Onellm"
      ],
      "year": "2023",
      "venue": "One framework to align all modalities with language",
      "arxiv": "arXiv:2312.03700"
    },
    {
      "citation_id": "8",
      "title": "Secap: Speech emotion captioning with large language model",
      "authors": [
        "Yaoxun Xu",
        "Hangting Chen",
        "Jianwei Yu",
        "Qiaochu Huang",
        "Zhiyong Wu",
        "Shi-Xiong Zhang",
        "Guangzhi Li",
        "Yi Luo",
        "Rongzhi Gu"
      ],
      "year": "2024",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "9",
      "title": "Salmonn: Towards generic hearing abilities for large language models",
      "authors": [
        "Changli Tang",
        "Wenyi Yu",
        "Guangzhi Sun",
        "Xianzhao Chen",
        "Tian Tan",
        "Wei Li",
        "Lu Lu",
        "M Zejun",
        "Chao Zhang"
      ],
      "year": "2023",
      "venue": "The Twelfth International Conference on Learning Representations"
    },
    {
      "citation_id": "10",
      "title": "Otter: A multi-modal model with in-context instruction tuning",
      "authors": [
        "Bo Li",
        "Yuanhan Zhang",
        "Liangyu Chen",
        "Jinghao Wang",
        "Jingkang Yang",
        "Ziwei Liu"
      ],
      "year": "2023",
      "venue": "Otter: A multi-modal model with in-context instruction tuning",
      "arxiv": "arXiv:2305.03726"
    },
    {
      "citation_id": "11",
      "title": "Videochat: Chat-centric video understanding",
      "authors": [
        "Kunchang Li",
        "Yinan He",
        "Yi Wang",
        "Yizhuo Li",
        "Wenhai Wang",
        "Ping Luo",
        "Yali Wang",
        "Limin Wang",
        "Yu Qiao"
      ],
      "year": "2023",
      "venue": "Videochat: Chat-centric video understanding",
      "arxiv": "arXiv:2305.06355"
    },
    {
      "citation_id": "12",
      "title": "Video-llama: An instruction-tuned audio-visual language model for video understanding",
      "authors": [
        "Hang Zhang",
        "Xin Li",
        "Lidong Bing"
      ],
      "year": "2023",
      "venue": "Video-llama: An instruction-tuned audio-visual language model for video understanding",
      "arxiv": "arXiv:2306.02858"
    },
    {
      "citation_id": "13",
      "title": "Video-llava: Learning united visual representation by alignment before projection",
      "authors": [
        "Bin Lin",
        "Bin Zhu",
        "Yang Ye",
        "Munan Ning",
        "Jin Peng",
        "Li Yuan"
      ],
      "year": "2023",
      "venue": "Video-llava: Learning united visual representation by alignment before projection",
      "arxiv": "arXiv:2311.10122"
    },
    {
      "citation_id": "14",
      "title": "Mvbench: A comprehensive multi-modal video understanding benchmark",
      "authors": [
        "Kunchang Li",
        "Yali Wang",
        "Yinan He",
        "Yizhuo Li",
        "Yi Wang",
        "Yi Liu",
        "Zun Wang",
        "Jilan Xu",
        "Guo Chen",
        "Ping Luo",
        "Limin Wang",
        "Yu Qiao"
      ],
      "year": "2024",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "15",
      "title": "Llama-vid: An image is worth 2 tokens in large language models",
      "authors": [
        "Yanwei Li",
        "Chengyao Wang",
        "Jiaya Jia"
      ],
      "year": "2023",
      "venue": "Llama-vid: An image is worth 2 tokens in large language models",
      "arxiv": "arXiv:2311.17043"
    },
    {
      "citation_id": "16",
      "title": "mplug-owl: Modularization empowers large language models with multimodality",
      "authors": [
        "Qinghao Ye",
        "Haiyang Xu",
        "Guohai Xu",
        "Jiabo Ye",
        "Ming Yan",
        "Yiyang Zhou",
        "Junyang Wang",
        "Anwen Hu",
        "Pengcheng Shi",
        "Yaya Shi"
      ],
      "year": "2023",
      "venue": "mplug-owl: Modularization empowers large language models with multimodality",
      "arxiv": "arXiv:2304.14178"
    },
    {
      "citation_id": "17",
      "title": "Video-chatgpt: Towards detailed video understanding via large vision and language models",
      "authors": [
        "Muhammad Maaz",
        "Hanoona Rasheed",
        "Salman Khan",
        "Fahad Shahbaz Khan"
      ],
      "year": "2023",
      "venue": "Video-chatgpt: Towards detailed video understanding via large vision and language models",
      "arxiv": "arXiv:2306.05424"
    },
    {
      "citation_id": "18",
      "title": "Chat-univi: Unified visual representation empowers large language models with image and video understanding",
      "authors": [
        "Jin Peng",
        "Ryuichi Takanobu",
        "Caiwan Zhang",
        "Xiaochun Cao",
        "Li Yuan"
      ],
      "year": "2023",
      "venue": "Chat-univi: Unified visual representation empowers large language models with image and video understanding",
      "arxiv": "arXiv:2311.08046"
    },
    {
      "citation_id": "19",
      "title": "Gpt-4v(ision) system card",
      "year": "2023",
      "venue": "Gpt-4v(ision) system card"
    },
    {
      "citation_id": "20",
      "title": "Visual instruction tuning",
      "authors": [
        "Haotian Liu",
        "Chunyuan Li",
        "Qingyang Wu",
        "Yong Jae Lee"
      ],
      "year": "2023",
      "venue": "Visual instruction tuning",
      "arxiv": "arXiv:2304.08485"
    },
    {
      "citation_id": "21",
      "title": "Minigpt-4: Enhancing vision-language understanding with advanced large language models",
      "authors": [
        "Deyao Zhu",
        "Jun Chen",
        "Xiaoqian Shen",
        "Xiang Li",
        "Mohamed Elhoseiny"
      ],
      "year": "2023",
      "venue": "Minigpt-4: Enhancing vision-language understanding with advanced large language models",
      "arxiv": "arXiv:2304.10592"
    }
  ]
}