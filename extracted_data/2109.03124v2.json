{
  "paper_id": "2109.03124v2",
  "title": "Ganser: A Self-Supervised Data Augmentation Framework For Eeg-Based Emotion Recognition",
  "published": "2021-09-07T14:42:55Z",
  "authors": [
    "Zhi Zhang",
    "Sheng-hua Zhong",
    "Yan Liu"
  ],
  "keywords": [
    "EEG-based emotion recognition",
    "data augmentation",
    "generative adversarial networks"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "The data scarcity problem in Electroencephalography (EEG) based affective computing results into difficulty in building an effective model with high accuracy and stability using machine learning algorithms especially deep learning models. Data augmentation has recently achieved considerable performance improvement for deep learning models-increased accuracy, stability, and reduced over-fitting. In this paper, we propose a novel data augmentation framework, namely Generative Adversarial Network-based Self-supervised Data Augmentation (GANSER). As the first to combine adversarial training with self-supervised learning for EEG-based emotion recognition, the proposed framework can generate high-quality and high-diversity simulated EEG samples. In particular, we utilize adversarial training to learn an EEG generator and force the generated EEG signals to approximate the distribution of real samples, ensuring the quality of augmented samples. A transformation function is employed to mask parts of EEG signals and force the generator to synthesize potential EEG signals based on the remaining parts, to produce a wide variety of samples. The masking possibility during transformation is introduced as prior knowledge to guide to extract distinguishable features for simulated EEG signals and generalize the classifier to the augmented sample space. Finally, extensive experiments demonstrate our proposed method can help emotion recognition for performance gain and achieve stateof-the-art results.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "Emotions are manifest in each action of our daily life behaviors. Understanding emotions is one of the most important aspects of human development and growth, and, therefore, it is an important tile for the emulation of human intelligence  [1] . Thus, affective computing and automatic emotion recognition are key for AI advancement  [2]  and all the research fields that stem from them. Electroencephalography (EEG) measures oscillations in the brain, which reflect the synchronized activity of neurons. It is thought that the changes in these oscillations are correlated with the cognitive process, and can be used to reveal important information about human emotional states. As a kind of physiological signal, EEG has the advantage of being difficult to hide or disguise. Compared with other physiological signals, it has excellent time resolution, which is similar to the nuanced changes of emotional states in time scale. Owing to the rapid development of noninvasive, easyto-use and inexpensive recording devices, EEG-based emotion recognition has received an increasing amount of attention in both research and applications  [3] .\n\nNevertheless, EEG also subjects to several limitations. First, as an aggregate signal from the activity of millions of neurons, EEG suffers from a low signal-to-noise ratio (SNR)  [4] . Second, EEG is generally recorded using tens to hundreds of electrodes simultaneously, and the sampling time usually exceeds a few seconds in each trial. Thus, the original feature dimension of an EEG sample is not low. However, in a typical dataset for cognitive neuroscience tasks, it usually contains only some hundred to a few thousand samples (i.e., experimental trials). It leads to a very low initial ratio of samples to features. Third, EEG is a non-stationary signal and its statistics varying over time. The inherent variabilities in brain anatomy, head size, and dynamics across trails/subjects considerably limit the generalizability of EEG analyses across subjects, and even across trials within a single subject performing a single task. The second limitation brings huge difficulties to the use of machine learning models, while the other two exacerbate this difficulty.\n\nIn the past few years, deep learning methods have achieved breakthrough performance for EEG-based emotion recognition. Unfortunately, deep learning models are typically very complex, i.e., have many free parameters (or degrees of freedom) to fit  [5] . Thus, if we lack enough data to train, considered the case of EEG-based emotion recognition that has a low initial ratio of samples to features, training such deep learning models risks overfitting those models to specific quirks of the training set. It also severely limits the generalizability of these models. Data augmentation is considered as one of the effective technologies for solving the data scarcity problem. It is usually a process of generating the new realistic-like data by applying a transformation to the real data  [6] . It also holds the promise to increase the accuracy and stability of the classification or regression. To overcome the data scarcity problem, in this paper, we propose a Generative Adversarial Network-based Self-supervised Data Augmentation (GANSER) framework for EEG-based emotion recognition. The proposed framework comprises two networks, including the Adversarial Augmentation Network (AAN) and Multi-factor Training Network (MTN). In the AAN, we propose a Masking Transformation operation to mask parts of EEG signals and then force the proposed Generative Adversarial Network (GAN) seeking to synthesize potential EEG signals based on the remaining parts. Here, the UNet, Channel Masking operation and STNet are employed to model the spatio-temporal features of EEG signals while adversarial training forces the generated EEG signals to approximate the distribution of real ones, ensuring the quality of simulated EEG signals. Next, in the MTN, the simulated EEG signals are utilized for training the emotion recognition models as augmented samples. The Multi-factor Self-supervised Learning loss is proposed to introduce the masking possibility as prior knowledge to guide the model extracting distinguishable features for simulated EEG signals and generalize the classifier to the augmented sample space.\n\nIn summary, the contributions of this paper can be highlighted as follows. (i) This paper proposes GANSER, permitting to tackle the bottleneck of data scarcity for EEGbased emotion recognition. (ii) In this paper, we are the first to combine adversarial training with self-supervised learning to synthesize real-like diverse EEG signals, and utilize the augmented EEG samples to self-supervise emotion recognition learning. On the one hand, adversarial training is designed to learn an EEG generator and force the synthesized EEG signals fitting the real distribution to augment real-like high-quality samples. On the other hand, a transformation function is employed to mask parts of EEG signals and force the generator seeking to synthesize diverse augmented samples different from given samples. The prior knowledge during transformation is utilized to guide the self-supervised learning upon augmented samples. (iii) Extensive experiments are carried out, and the results show that our proposed deep framework significantly outperforms the existing state-of-the-art methods. Finally, we adopt a quantitative assessment approach for EEG analysis to evaluate the quality and diversity of the augmented samples, and visualization results are provided for qualitative analysis.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Related Work",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "A. Eeg-Based Emotion Recognition",
      "text": "Recent years, boosted by the success of deep neural networks, deep learning-based emotion recognition  [7] -  [11]  has received an increasing amount of attention in both research and applications and these studies seek to explore end-to-end methods to tackle the EEG-based emotion recognition task.\n\nIn detail, deep neural networks such as recurrent neural networks (RNNs), 2D/3D convolutional neural networks (CNNs), or both were employed for feature extraction and classification. In 2016, Zhang et al.  [12]  proposed a spatialtemporal recurrent neural network (STRNN) to investigate both spatial and temporal dependencies of EEG signals and achieve the state-of-the-art. In 2018, Li et al. further  [13]  proposed a hybrid deep learning structure based on a CNN and an RNN for emotion recognition based on multi-channel EEG signals. The proposed method showed effectiveness in the trial-level emotion recognition task. In the same year, Salama et al.  [14]  employed 3D CNNs to classify human emotion. To feed an EEG signal into inputs of a 3D CNN, they divided the 2D shape (channel√ótime) of EEG data into 6-s segments and stacked them along the third axis. In the following study, Moon et al.  [15]  pointed out the limitation that only signals or features from individual electrodes are considered and employed brain connectivity features to account for synchronous activations of different brain regions. In 2020, Moon et al.  [16]  further improved their research and introduced three different types of connectivity measures to model brain connectivity with a CNN. Furthermore, two data-driven methods are proposed to construct the connectivity matrix and maximize classification performance. Luo et al.  [17]  found that EEG signals significantly varied depending on the individual and imposed difficulty in achieving satisfactory classification performance. They proposed a Wasserstein GAN-based framework to solve the domain shift problem by narrowing down the gap between the probability distribution of different subjects. Recently, Moon et al.  [16]  proposed to learn feature space mapping and perform individuality detachment to reduce subject-related information from EEG signals. The proposed method can effectively discard the subject-related information and perform well on emotion recognition tasks.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "B. Data Augmentation For Eeg-Based Emotion Recognition",
      "text": "In the field of image processing and computer vision, there are two simple and direct ways to augment data: geometric transformation and noise addition. Krell and Su et al. suggested using rotational distortions, which were similar to affine/rotational distortions of images, to generate augmented EEG data  [18] . Relying on relevant combinations and distortions of the original trials, Lotte proposed three methods to obtain artificial EEG trials  [19] . Different from geometric transformations, Wang et al. generated new features by adding Gaussian noises with different standard deviations to the original EEG feature and applied several deep learning models to verify the effect  [20] . Other data augmentation methods include sliding window, sampling, the Fourier transform, and recombination of segmentation  [5] . All of the abovementioned methods reported that the data scarcity problem had been alleviated, and the performance of the classifiers was improved  [21] .\n\nRecently, Generative Adversarial Networks (GANs) have revealed their potential in generating EEG signals that mimic real ones, utilized in the emotion recognition task  [21] -  [23]  and a wide variety of applications  [24] -  [28] . A conditional version of the Wasserstein Generative Adversarial Network (WGAN) was used to augment EEG data for emotion recognition in  [22] . They tried different sizes for the augmented data, and they found that doubling the data led to the highest performance increment comparing to other sizes. An SVM classifier trained on the augmented dataset improved 2.97% for the SEED dataset from 83.99% to 86.96%. Luo et al. proposed to use a conditional Boundary Equilibrium GAN (cBEGAN) to generate artificial differential entropy features of original EEG data, eye movement data and their concatenations for multi-modal emotion recognition. The main advantage of it is that the proposed GAN has good stability and a very quick convergence speed  [23] . Luo et al. proposed three methods for augmenting EEG training data to enhance the performance of emotion recognition models, including conditional Wasserstein GAN, selective variational autoencoder, and selective WGAN  [21] . They trained SVM and deep neural networks on original and augmented training datasets. The experimental results showed that the augmented training datasets enhance the performance of EEG-based emotion recognition.\n\nThough lots of efforts have been made, the research on data augmentation for emotion recognition is far from close. For example, while a human can easily decide whether an augmented dataset, e.g., of cats or other images, still resembles the original class, the same is not true of augmented signals. How to measure the quality and diversity of augmented samples and synthesize high-quality and diverse augmented samples deserves further exploration.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Iii. Proposed Method A. Overall Framework",
      "text": "This paper designs a Generative Adversarial Network-based Self-supervised Emotion Recognition (GANSER) framework for EEG-based emotion recognition. Illustrated in Fig.  1 , the proposed framework comprises two networks, the Adversarial Augmentation Network (AAN), and the Multi-factor Training Network (MTN). Taking real EEG samples as input, the AAN is first designed to synthesize high-quality and diverse augmented EEG samples. Then, the EEG-based emotion recognition classifier can be learned on the augmented EEG samples and finish the self-supervised learning under the guidance of the proposed MTN. In the remainder of this section, we will detail the network architecture of AAN and MTN proposed in this paper.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "B. Adversarial Augmentation Network",
      "text": "In the AAN, the Masking Transformation operation is first proposed to cut out part of the data points of the given EEG randomly. Then, a GAN is required to synthesize EEG signals fitting the real data distribution based on the remained EEG signals. By restoring the missing data points of EEG signals, the proposed GAN would be able to recognize the feature distribution of source EEG signals and introduce new data points to generate new EEG signals.\n\nTo be specific, given a 32 channel EEG signal with onesecond length (sampled to 128 Hz) with the size of 32 √ó 128, we first follow the pre-processing of existing work  [29]  to apply baseline removal, measuring the differences between baseline signals and the given signal. Then, the results of 32 channels are transformed into 9 √ó 9 maps according to the electrodes' location based on the international 10-20 system  [29] . As a result, the given EEG signal can be denoted by e ‚àà R 128√ó9√ó9 .\n\nThen, as we described before, we design the Masking Transformation operation to cut out partial signal values of the given e and force the following GAN to restore the missing parts fitting the remained information to involve potential reallike samples different from the input EEG signal. In detail, we first randomly sample a matrix r ‚àà R 128√ó9√ó9 of the same size as e with uniform distribution U ‚àº [0, 1) and utilize r as the probability matrix representing probabilities of signal values being masked. Then, the parameter œÑ is sampled from the uniform distribution U ‚àº [œÑ min , œÑ max ] as the threshold to determine which data point should be masked. In this way, the obtained EEG signals Œ¥(e, œÑ ) transformed from e based on the threshold œÑ can be defined by:\n\nHere, a large œÑ means random masking ignores more parts of the signal values of source EEG samples. In this case, the feature distribution of source EEG signals is hard to preserve due to the limited remained signal values. As a result, we can avoid learning an identity mapping and produce simulated EEG signals different from original signals, ensuring the diversity of augmented samples. Conversely, the generated EEG signal can be similar to the source EEG signal forced to fit the distribution of given signals. Thus, in this paper, we utilize œÑ as an augmentation factor to represent the augmented sample's diversity and difference from the original sample.\n\nThen, based on Œ¥(e, œÑ ), we design a GAN to synthesize simulated EEG samples and ensure the generated EEG signals fit the feature distribution of real samples. Unlike the Masking Transformation operation, focusing on involving the diversity for synthesized EEG samples at the signal value level, the proposed GAN is responsible for learning the distribution of realistic EEG signals at the feature level. In this way, the generated augmented samples are further forced to preserve the natural features of real samples. Finally, realistic and diverse samples can lead to better classification performance for emotion recognition.\n\nThe designed GAN is composed of two networks, i.e., a generator G and a discriminator D, optimized to minimize a two-player min-max game. Here, the generator G is trained to generate the simulated EEG sample G(Œ¥(e, œÑ )) taking the EEG signal e as input. The discriminator D is required to distinguish whether the given EEG signals are simulated or real, while G learns to fool the discriminator and try to make simulated samples close to real ones. Due to the instability problems of the traditional training procedure of GANs, different from previous GANs proposed for emotion recognition, we utilize a modified version of Wasserstein GAN Gradient Penalty, i.e., WGAN-GP  [30] , for combining adversarial supervision and the random masking augmentation EEG ùëí or ùê∫(ùõø(ùëí, ùúè)) strategy. Then, the loss function of G can be formulated as Eq.\n\n(\n\n:\n\nwhere P e denotes the distribution of the given real EEG signals and e represents an EEG signal sample from it. Meanwhile, the goal of D is to minimize the loss function illustrated in Eq. (  3 ):\n\nwhere P √™ is defined sampling uniformly along straight lines between pairs of points sampled from the data distribution P e and the generator distribution among G(Œ¥(e, œÑ )). The gradient of the discriminator D is denoted by ‚àáD(√™), and Œª p is a hyperparameter presenting the weight of the penalty term.\n\nIn this way, the Wasserstein distance is used to compare the distributions of the generated samples and real samples, where the Lipschitz-continuous map ensures the property of a uniformly continuous distribution. This design can limit the normal of the derivation from growing too large  [30] . Utilizing Eq. (  2 ) and Eq. (  3 ), G and D are optimized in turn. By optimizing the adversarial loss, D is able to distinguish real distribution from simulated distribution, while G improves the ability to construct samples closer to real EEG signals.\n\nRegarding the network architectures of G and D, unlike the existing work focusing on generating feature representation or single-channel EEG signals, this paper aims to tackle the data augmentation problem for general emotion recognition methods and synthesize high-resolution EEG samples.\n\nFor the generator G, we aim to synthesize missing signal values of given EEG signals to augment new EEG samples. In this case, one of the natural ideas is to utilize an auto-encoder to reconstruct EEG signals directly in a down-sample and upsample fashion. However, EEG signals contain abundant details reflected by time-series variance. It is challenging for existing auto-encoder-based networks to generate low-distorted EEG signals, because down-sampling of auto-encoder applied to high-resolution time sequences can lead to details missing and the reconstructed signals could be smoothed. To tackle this bottleneck, in this paper, we integrate an adaption version of UNet on EEG signals as the proposed generator for EEG signal synchronizing.\n\nAs shown in Fig.  2 (a), the proposed UNet consists of an encoder, a decoder, and skip connections. The encoder takes the EEG representation as the input and utilizes four twodimensional convolutional layers followed by LeakyReLU as an activation function to down-sample the EEG signals and extract feature maps. Based on the deep stack of convolutional layers, spatio-temporal patterns of original EEG signals are captured from detail to abstract. Then, in the decoder, three de-convolutional layers are applied to up-sample feature maps to high spatio-temporal resolution by synthesizing the missing signal values to generate new EEG samples based on extracted features. Here, skip connections are designed between the convolutional and symmetric de-convolutional layers to fuse shallow feature maps to favor de-convolutional layers to supplement high-resolution details.\n\nFurther, this paper finds EEG signals are sparse on the spatial dimension due to limited numbers of electrodes placed on the cap, which brings challenges for UNet to synthesize real-like EEG signals. To be specific, given a 32 channel EEG signal, we transform 32 channels into 9 √ó 9 maps according to the location of electrodes. In the locations where electrodes do not exist, signal values are unknown or unmeasured, and the signal values are represented as zeros. Forcing the UNet to fit unmeasured signal values in the locations where electrodes do not exist, signal values to 0.0, the generator is required to predict mutated low signal values into original spatially dense and continuous signals, which is at variance with objective reality and inevitably affects the modeling of measured signal values to fit the real distribution of original EEG signals. Thus, this paper first introduces the Channel Masking operation to improve the ability of UNet for synthesizing EEG signals. For the first step, we propose to build the channel mask m, a prior binary mask with the size of 9 √ó 9, and set the values where electrodes exist and signals are measured to one, while defining the other locations to zero. Then, we apply elementwise multiply between the designed prior mask and the output of UNet, i.e., the synthesized EEG signals, to artificially reset the signal values where electrodes do not exist to zero. In this way, the proposed generator only needs to focus on fitting the signal values where electrodes exist and neglecting the unreal mutation of EEG signal values caused by the nonexistent electrodes.\n\nAs the last part, illustrated in Fig.  2 (b), we design a novel network architecture, STNet, to analyze the complex spatiotemporal features of EEG signals, and utilize STNet as the dis-criminator. In detail, the designed STNet comprises three twodimensional convolutional layers, a separable convolutional layer and an Inception block. For the first step, input EEG signals are analyzed by two-dimensional convolutional layers to extract feature maps from signal values of each electrode and their spatio-temporal neighbors to summarize high-level features. Then, due to the fact that the recognition of specific emotions is only related to local patterns of spatial features or temporal features, we introduce a separable convolutional layer  [31]  to decouple the modeling of spatio-temporal information.\n\nHere, the utilized separable convolutional layer containing a depth-wise convolutional layer and a point-wise convolution layer to capture spatial correlation and temporal correlations of extracted feature maps, respectively. Recognizing the pattern of emotions requires analyzing EEG signals at different spatial scales, and thus we further introduce an Inception block  [32]  containing three types of filters of different sizes to extract multi-scale feature maps. By fusing these feature maps, the pattern of emotions related to multiple electrode signals and local electrode signals can both be adaptively captured. Finally, the classification results are produced. For more implementation details of network architectures, please refer to the supplemental material.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "C. Multi-Factor Training Network",
      "text": "By optimizing the adversarial loss, the trained generator of AAN can produce augmented samples varying in signal details but fitting the feature distribution of real samples. For the next step, we train classifier C and fine-tune the trained classifier further utilizing the learned generator of AAN to generate augmented samples.\n\nIn this stage, how to utilize augmented samples for supervision is a crucial problem. It is known that high valence (arousal) and low valence (arousal) are far from being clearcut and distinguished by an artificial threshold. Thus, if the shift of augmented EEG varies in wide limits, the threshold between high valence (arousal) and low valence (arousal) can be exceeded, and the augmented EEG would change to a different category of the original EEG. To tackle this bottleneck, we explore to seek a self-supervised learning framework to supervise emotion recognition training based on augmented samples and uncertainty labels. It is important to acknowledge that in the field of computer vision, selfsupervised learning frameworks already allow for reasonable performance without the acquisition of large training sets and well-labeled training samples. For example, Dosovitskiy et al.  [33]  proposed to learn a network to discriminate between a set of surrogate classes formed by applying various transformations to a randomly sampled \"seed\" image patch. Then, by learning to classify different transformed samples of seed images to the same surrogate categories, the proposed network can extract discriminative features favoring better classification performance.\n\nThus, inspired by the significant progress made by selfsupervised learning, this paper proposes the MTN for EEGbased emotion recognition. As described before, the aug-mented samples are synchronized based on parts of original EEG signal values, and thus the augmented samples should preserve the feature distribution of original samples to some extent, although not the same. Based on these observations, different from existing work, which directly creates a set of surrogate classes, this paper designs a set of surrogate confidence, measured by augmentation factor œÑ , learning to restrict the feature distribution difference between real samples and augmented samples under given surrogate confidence. To be specific, in the case where the augmentation factor œÑ is large, limited signal values of augmented samples remain from the source EEG signal, and the generator G cannot capture and preserve the feature of the original EEG signal during synchronizing. Thus, the feature distribution of the augmented EEG signals should be constrained to fit the original EEG signals' feature distribution under low confidence. Conversely, if the augmentation factor œÑ is small, most original EEG signal values are preserved during augmentation. We should force the feature distribution of augmented samples close to the original samples with high confidence. Finally, we propose Multifactor Self-supervised Learning loss to assign different weights for restricting the feature distribution difference between augmented EEG signals and real samples based on corresponding surrogate confidence. Combining the cross-entropy loss for supervising real samples' training as usual, and the total loss function can be formulated as:\n\n(4)\n\nwhere we disregard the last fully connected layer of the classifier C and utilize the remaining part as a feature extractor C x to process EEG signals and produce feature vectors. The ground-truth label of given EEG signal e i is denoted by y i , n is the number of samples in a mini-batch, and Œª a is the hyperparameter utilized to represent the importance of classifying augmented samples correctly. In Eq. (  4 ), the cross-entropy between the ground truth labels and corresponding prediction results of the real EEG signals are formulated as -1 n n i=0 y i log (C(e i )). Meanwhile, the feature distribution difference between the original EEG signals and the corresponding augmented signals is denoted by\n\n2 . Based on augmentation factor œÑ i , we compute the surrogate confidence (1 -œÑ i ) to assign large weights to different augmented samples, to narrow the feature distribution difference when most of the original EEG signal values are preserved in the augmented sample.\n\nIt is worth noting that existing data augmentation methods mostly provide augmented samples in an offline fashion, generating a preset number of samples and saving them as training samples for the first step. Then, during optimization, augmented samples are loaded and fed into models with original samples. In this paper, inspired by self-supervised learning approaches, we attempt to explore an alternative strategy. On the one hand, the separated stages of augmentation and training are joined together as an end-to-end pipeline. Given a batch of samples, we first utilize the generator G in AAN to augment EEG signals and pair the real ones. Then, we utilize these samples to optimize the classifier C with Eq. (  4 ) in the current batch. Avoiding the cost of saving and reloading, the proposed method is more efficient. On the other hand, in this paper, augmented samples are regenerated between epochs in runtime. Benefit from the randomness of the Masking Transformation operation, augmented samples of the corresponding batch between epochs are different, but both sampled near the real distributions. In this way, instead of overfitting on a preset number of augmented samples, randomly synthesized EEG signals can approximate the distribution of EEG real signals without number limitation.\n\nFinally, the proposed Multi-factor Self-supervised Learning loss can adaptively guide the feature extractor to extract distinguishable feature representation for simulated EEG signals and generalize the classifier to the augmented sample space.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Iv. Implementation Details",
      "text": "In this section, we supplement the implementation details of our proposed framework, GANSER, including the network architectures of the designed generator G, discriminator D and classifier C, and the hyper-parameters utilized in the experiments.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "A. Network Architectures",
      "text": "In the proposed GAN, the generator G is trained to generate the simulated EEG sample G(Œ¥(e, œÑ )) taking the EEG signal e as input. The network architecture is illustrated in Fig.  3 . It consists of a contracting path (left side) and an expansive path (right side). The contracting path follows the typical architecture of an encoder convolutional neural network to extract to down-sample the EEG signals G(Œ¥(e, œÑ )) and extract feature maps. It consists of the repeated application of one 3√ó3 convolutional layer, two 5√ó5 convolutional layer, another 3 √ó 3 convolutional layer, each followed by a leaky rectified linear unit (LeakyReLU). At each convolutional layer, we set the stride to one and halved the number of feature channels. Meanwhile, the expansive path follows a decoder convolutional neural network architecture with skip connections to up-sample feature maps to a high spatio-temporal resolution to generate new EEG samples based on extracted features. Every step in the expansive path consists of concatenation with the correspondingly feature map from the contracting path and one 3 √ó 3 convolutional layer with the stride of one, doubling the number of feature channels. Each convolutional layer is followed by the LeakyReLU activation function. After the final convolutional layer, the Channel Masking operation is introduced to build the channel mask m, a prior binary mask with the size of 9 √ó 9, and set the values where electrodes exist and signals are measured to 1.0, while defining the other locations to 0.0. Then, we apply element-wise multiply between the designed prior mask and the output of UNet, i.e.,   The discriminator D is required to distinguish whether the given EEG signals are simulated or real. As shown in Fig.  3 , in this paper, we design a novel network architecture, STNet, to analyze the complex spatio-temporal features of EEG signals. As shown in Fig.  4 , given a real EEG signal or generated EEG signal, the input EEG signal is provided as the input of three convolutional blocks to extract lowresolution features. The first convolutional block uses a 3 √ó 3 convolutional layer, while the second and third convolutional block uses a 5 √ó 5 convolutional layer. Each convolutional layer halves the number of input feature channels, and the stride of all convolutional layers is set to one. Following each convolutional layer, a scaled exponential linear unit (SELU) is utilized as an activation function. For the next step, we introduce a separable convolutional layer  [31]  to decouple the modeling of spatio-temporal information. In detail, the separable convolutional layer factorizes a standard 3 √ó 3 convolutional layer into a 3 √ó 3 depth-wise convolution and a 1 √ó 1 point-wise convolution and splits the computation into two steps: the depth-wise convolution applies a single convolutional filter per input channel, and the point-wise convolution is used to create a linear combination of the output of the depth-wise convolution. The stride of both depth-wise convolution and point-wise convolution is set to one, and the point-wise convolution is followed by SELU. Moreover, we find recognizing the pattern of emotions requires analyzing EEG signals at different spatial scales, thus introducing an Inception block inspired by  [32] . In detail, this paper combines a 1 √ó 1 convolutional layer, a 3 √ó 3 convolutional layer, and a 5 √ó 5 convolutional layer to utilize their output filter banks concatenated into single feature maps forming the input of the next stage. At each convolutional layer, we set the stride to one and halved the number of feature channels. Finally, the feature maps extracted by convolutional layers of each EEG signal are reshaped to a 1 √ó 2592(32 √ó 9 √ó 9) feature vector, and two linear layers are utilized to map the feature vector to a scalar. Here, the first linear layer has 1024 nodes and is followed by the SELU activation function, while the second linear layer has one node to predict the score of the given EEG signal being fake.\n\nBecause the designed STNet shows good effectiveness in EEG analysis, in this paper, the classifier C shares the design of STNet as the discriminator D, except for several modifications to fit the classification problem formulation of emotion recognition. In detail, as shown in Fig.  4 , the dashed box represents operations that exist in the classifier C but do not exist in the discriminator D. In the classifier C, we employ dropout  [34]  operation with p = 0.5 after the activation function of convolutional layers to address the problem of over-fitting. Here, p denotes the probability of an element to be zeroed. The number of nodes in the last linear layer is set to two or four, corresponding to the categories of emotions. Moreover, an additional softmax function is set to follow the last linear layer as the activation function to produce the final outputs representing a categorical distribution.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "B. Other Configuration",
      "text": "In this paper, we use PyTorch  [35]  to implement our networks based on eight NVIDIA Tesla V100 GPUs. For the networks G, D and C, this paper adopts the Adam optimizer  [36]  to minimize the loss functions. Here, the coefficient Œ≤ 1 used for computing running averages of the gradient is set to 0.9, and the coefficient Œ≤ 2 used for computing running averages of the square is 0.99. Besides, the weight decay of the L2 penalty is set to 0.0005, and the batch size is 64. Table  I  reports other hyper-parameters utilized in the proposed GANSER. Here, lr G denotes the learning rate of the generator G, lr D corresponds to the learning rate of the discriminator D, and lr C represents the learning rate of the classifier D. In addition, Œª p is a hyper-parameter presenting the weight of the penalty term in Eq. (  3 ), and Œª a denotes the hyperparameter in Eq. (  4 ) utilized to represent the importance of classifying augmented samples correctly. The parameter œÑ is sampled from the uniform distribution U ‚àº [œÑ min , œÑ max ]. Moreover, in the proposed GANSER framework, the AAN is first trained to generate augmented EEG samples, and MTN is then introduced to train the classifier for the emotion recognition task. Thus, as shown in Table  I , we report all hyper-parameters utilized in these two stages.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "V. Experiments",
      "text": "",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "A. Dataset And Preprocessing On Deap",
      "text": "To evaluate the proposed method, we conduct experiments on the widely-used EEG-based emotion recognition dataset DEAP  [37]  to demonstrate the performance gain of our proposed method. To be specific, the DEAP dataset recorded 32channel EEG signals and 8-channel peripheral physiological signals of 32 subjects when watching 40 one-minute-long music videos. After watching each video, participants rate their arousal levels, valence, liking, and dominance from one to nine for each video. The EEG signals and rating values are utilized to construct the emotion recognition task.\n\nFor EEG signals of each trail, the two preprocessing steps pre-given by the DEAP dataset are first employed. Here, the recorded EEG signals are first down-sampled to a 128 Hz sampling rate. Then, obtained EEG signals are processed with a band-pass filter from 4Hz to 45Hz to remove physiological and power frequency noises  [37] . In each trial of the preprocessed dataset, the contained EEG signals consist of a three-secondlong baseline signal recorded in relax state and a 60-secondlong experimental signal recorded under stimulation. Further, we use a non-overlapping sliding window to separate the trial data into one-second-long chunks and construct the separated EEG signals as data samples. Here, the sliding window size is set to 128 to separate one-second chunks under a sampling rate of 128 Hz. For the next step, to reduce the effect of basic emotional state, following existing work, we remove a mean baseline value from each epoch  [38] . Finally, the total number of EEG samples from 40 trials is 40 √ó 60 = 2400.\n\nIn terms of the emotional rating value of each trail in the range of 1.0 to 9.0 in the arousal and valence domains, the median 5.0 was used as the threshold to divide the rating value into two categories. In the cases where the emotional rating value is rated more than 5.0, the corresponding EEG signals are labeled as high arousal or valence. On the contrary, for the ones less than or equal to 5.0, this paper labels the corresponding EEG signals as low arousal or valence. Finally, given EEG signals to predict corresponding labeled categories, the emotion recognition task is formulated as a binary classification problem.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "B. Overall Performance On Deap",
      "text": "In this section, to validate the performance of the proposed framework, we give the overall performance evaluated on the DEAP dataset and compare other state-of-the-art works and competitive GAN-based studies on the emotion recognition task.\n\nIn the experiments, data samples in the DEAP dataset are split into five folds at random and five-fold cross-validation is used to evaluate all models. To evaluate our proposed method, we first utilize 80% randomly shuffled data samples as training data to train the AAN for 300 epochs. In the following step, we fix the AAN parameters and then take the pre-trained",
      "page_start": 8,
      "page_end": 11
    },
    {
      "section_name": "Method",
      "text": "Valence Arousal Four SOTA CDCN  [39]  92.24 92.92 -MMResLSTM  [40]  92.87 92.30 -PCRNN  [29]  90.8 91.03 -CNNLSTM  [41]  90.62 86.13 -MergedLSTM  [42]  84 AAN to generate augmented samples. Then, the proposed classifier is learned on each fold for 300 epochs, and we supplement the augmented samples generated by AAN for fine-tuning of 300 epochs with the help of MTN. Finally, the fine-tuned models are utilized for evaluation. To assess the overall performance, the average classification accuracies over five folds are reported. Illustrated in Table  II , we first compared our proposed GANSER with five state-of-the-art studies, i.e., CDCN  [39] , MMResLSTM  [40] , PCRNN  [29] , CNNLSTM  [41] , and MergedLSTM  [42] , on the DEAP dataset, respect to the emotion dimensions including valence and arousal. These studies develop different network architectures and strategies for emotion recognition. For example, CNNLSTM  [41]  combined convolutional neural networks and long short-term memory networks to extract distinguishable features, and MMResLSTM  [40]  further utilized multi-modal information to improve the classification performance. From Table  II , we can find the proposed method outperform all state-of-the-art studies on both valence and arousal dimension. Although the designed classifier requires lightweight training parameters and only consists of convolutional layers, the proposed method shows great classification performance of over 93% for twodimensional classification tasks and considerably outperforms the second-best method by a margin of near 1.0%. The comparison results demonstrate the effectiveness of our proposed methods for EEG-based emotion recognition.\n\nFurther, to verify the capability of our proposed method in the field of data augmentation for emotion recognition, we further compare the GANSER with several GAN-based data augmentation frameworks. Especially, following the experimental setting of existing GAN-based methods  [21] ,  [43] , we formulate the emotion recognition task as a four-category classification problem, which aims at distinguishing EEG signals of four categories: high valence and high arousal, high valence and low arousal, low valence and high arousal, and low valence and high arousal. In Table  II , it can be found that the proposed GANSER significantly outperforms existing GAN-based data augmentation frameworks with a large margin of over 8.0%. We can also find that even in the formulation of four-category classification, our proposed method can correctly classify nearly 90% EEG signals at valence and arousal dimensions simultaneously due to the well-designed AAN and MTN.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "C. Ablation Study On Deap",
      "text": "In this section, we further conduct an ablation study to investigate the performance gain brought by each key component in our model, including the GAN, the Masking Transformation operation, and the Multi-factor Self-supervised Learning loss (MSL), and provide the performance of stripped-down versions by removing these components one by one.\n\nWhen the Multi-factor Self-supervised Learning loss is ablated, the augmented samples are treated as an equivalent of given training samples to optimize the cross-entropy based on the label of original EEG signals. After removing the Masking Transformation operation, the proposed generator G takes EEG signal e as input directly instead of the masked EEG signal Œ¥(e, œÑ ). While the GAN is further removed, no data augmentation operation is employed, and the classifier is finetuned based on given training samples. Finally, we follow the above experimental setting to train and evaluate stripped-down versions of our approach and report the average classification accuracies of different models in Table  III .\n\nIt has been shown that our proposed classifier is a strong and effective baseline even in the case where no data augmentation is utilized, and the classifier is fine-tuned based on the original training dataset. The proposed method can achieve the performance of 91.39% and 92.22% at valence and arousal dimensions, comparable to the latest arts designed based on complex network architectures or applying multimodal information. Meanwhile, the design of both GAN and Masking Transformation operation brings a performance gain of about 1.0% and obviously improves the classification accuracy of EEG signals. This phenomenon demonstrates that with the help of designed components, the framework synthesizes simulated EEG signals to favor learning the EEG signal patterns for emotion recognition. Finally, the additional Multi-factor Self-supervised Learning loss succeeds in enhancing the classification performance to over 93.5% by further considering the uncertainty of augmented samples' labels and providing further guidance for model training in a self-supervised learning fashion. While the overall framework is introduced to improve emotion recognition accuracy, the well-designed network architecture of GAN is especially crucial to synthesize real-like and diverse EEG signals. Thus, in this section, qualitative and quantitative experiments are carried out to assess the quality of the synthesized EEG samples and demonstrate the contribution of our designed GAN architectures for EEG signal stimulation.\n\nIn terms of qualitative experiments, it is known that judging the quality and diversity of samples generated by GAN-based methods is a challenging task due to the difficulty of measuring the distribution difference between real samples and generated samples. Following the existing work in the computer vision field, this paper adapts Fr√©chet Inception Distance (FID) for the EEG analysis and designs Fr√©chet STNet Distance (FSTD) to assess the quality of EEG signals generated by GANs.\n\nIn detail, we first train two proposed STNet on 80% training samples to distinguish different emotions at the assessed dimension, e.g., valence dimension. Then, we utilize the learned STNet as the feature extractor and provide EEG samples as input to extract the output of the penultimate fully connected layer. Finally, compute the sample means ¬µ, ¬µ g and the sample covariance matrices Œ£, Œ£ g of real samples and generated samples' the feature distributions, and the FSTD is then the Wasserstein distance between the two multivariate normal distributions N (¬µ, Œ£) and N (¬µ g , Œ£ g ):\n\nwhere a small FSTD value indicates a high similarity between the generated samples and real data distribution. Illustrated in Table  IV , FSTD scores of different strippeddown GAN-based architectures of our approach are compared in terms of valence and arousal dimensions. The average FSTD scores are also provided to assess overall performance. Here, to explore the contribution of each design in the network architecture, we use the above experimental setting to train and evaluate different combinations of the UNet network architecture (UNet), the Channel Masking operation, and the STNet architecture (STNet). Each ablated combination removed one of the components. Especially when UNet is removed, we utilize an auto-encoder of the same parameter volume as an alternative following the existing work. After eliminating the Channel Masking operation, the output of UNet is directly utilized as generated EEG signals. While STNet is ablated, we replace the separable convolutional layer and the Inception block with convolutional layers to produce output feature maps of the corresponding size.\n\nAs shown in Table  IV , it can be found that if any one of the proposed components is removed, the average FSTD scores will rise. It means that every proposed module contributes to the quality and diversity of augmented samples, without which the feature distribution of the generated sample would be different from the real sample. Especially, we can also find STNet affects the FSTD score to the greatest extent, and in both valence and arousal dimensions. Because favored by the designed separable convolutional layer and the Inception block, STNet can model the complex spatio-temporal feature distribution of EEG signals and force the generated samples to fit the real feature distribution. Meanwhile, the lack of the Channel Masking operation also leads to a relatively sharp increase of FSTD scores at the valence dimension. This phenomenon is because the difference between channels in the EEG signal is crucial for valence recognition, and the Channel Masking operation can reduce the difficulty of synthesizing the difference between channels by giving prior information. Further, arousal analysis is highly related to the signal value scale. In these cases, UNet preserves the modeled scale information to the greatest extent and thus improves the quality of augmented samples.\n\nTo further enhance the revelation of the data augmentation performance of our proposed framework, in this section, we carry out a visualization experiment on the DEAP dataset. To be specific, the experimental setting in this section is consistent with that in Section V-B. After training is completed, the learned AAN is utilized to synthesize EEG signals based on given data samples. The original EEG signals and the generated simulated EEG signals are provided in Fig.  5 .\n\nIn detail, EEG signals of different categories are sampled for visualization, including high valence and low arousal (first column) and low valence and high arousal (second column), respectively. For each case, the original real EEG signals are presented in the first row, the second row depicts synthesized EEG signals by our proposed method, and the third row shows the generated EEG signals by an ablated version of our approach replacing the UNet network architecture with autoencoder. Since the raw data of EEG signals is not conducive to visually representing the EEG signals' characteristics, the topographic maps of EEG signals sampled at 0.0s, 0.25s, 0.5s, and 0.75s are provided where red denotes high energy values, and blue represents low energy values. As shown in Fig.  5 , it can be found that the activated areas shown by real EEG signals and corresponding simulated EEG signals symmetric by our proposed method are almost consistent at spatial and temporal dimensions, demonstrating the generated data have similar spatio-temporal data distributions as the real data. Meanwhile, we can also observe a slight shift in the activation area and the change in the activation degree in the augmented EEG samples, indicating that our methods could synthesize diverse simulated samples and avoid identity mapping. On the",
      "page_start": 9,
      "page_end": 10
    },
    {
      "section_name": "E. Dataset And Preprocessing On Dreamer",
      "text": "The DREAMER dataset contains EEG data of 23 subjects, which are collected via 14 EEG electrodes from the subjects when they are watching 18 film clips. Each film clip lasts 65 seconds long to 393 seconds long. The average length of film clips is 199 seconds. The data collection begins with a neutral film clip watching to help the subjects return to the neutral emotional state in each new trial of data collection and serve as the baseline signals. As the pre-given preprocessing steps, all the EEG signals are recorded at a sampling rate of 128 Hz and filtered by band-pass Hamming with linear phase FIR filters. The artifact subspace reconstruction (ASR) method is used for artifact removal. After watching a film clip, subjects rate their levels of arousal and valence from 1.0 to 5.0. Finally, there are experimental signals, baseline signals, and labels in the DREAMER dataset.\n\nFor EEG signals of each trail, we use a non-overlapping sliding window to separate the trail data into one-secondlong chunks and construct the separated EEG signals as data samples. Here, the sliding window size is set to 128 to separate one-second chunks under a sampling rate of 128 Hz. For the next step, to reduce the effect of basic emotional state, following existing work, we removed a mean baseline value from each epoch  [38] . The length of each experimental signal in the DREAMER dataset is different because each film clip lasts from 65 seconds long to 393 seconds long. As a result, we get a different number of EEG samples for each experimental signal of the DREAMER dataset.\n\nIn terms of the emotional rating value of each trial, the median 3.0 is used as the threshold to divide the trials according to the levels of Valence and Arousal. That is, the label is low when the rating is less than or equal to 3.0, and the label is high when the rating is greater than 3.0. In this way, the recognition task is actually a binary classification problem for each emotion dimension.",
      "page_start": 11,
      "page_end": 12
    },
    {
      "section_name": "F. Overall Performance On Dreamer",
      "text": "In this section, without changing the network architecture and other configurations, we directly implement our proposed method on the DREAMER dataset and report the performance of our proposed method as a baseline. Notably, the contained EEG data in the DREAMER dataset are collected via 14 EEG electrodes, the EEG signals of 14 channels instead of 32 in the DEAP dataset are transformed into 9√ó9 maps according to the electrodes' location based on the international 10-20 system. Thus, the representation of EEG signals in the DREAMER dataset is more sparse. Then, experiments are carried out to compare our proposed method with several state-of-the-art works.\n\nIn this experiment, following the experimental setting of existing works, data samples in the DREAMER dataset are split into ten folds at random, and ten-fold cross-validation is used to evaluate all models. Notably, such an experimental setting is different from the configuration of existing studies reported on the DEAP dataset, where five-fold cross-validation is generally used. To evaluate our proposed method, we first utilize 80% randomly shuffled data samples as training data to train the AAN for 300 epochs. In the following step, we fix the AAN parameters and then take the pre-trained AAN to generate augmented samples. Then, the proposed classifier is learned on training folds for 300 epochs, and we supplement the augmented samples generated by AAN for fine-tuning of 300 epochs with the help of MTN. Finally, the finetuned models are utilized for evaluation on the corresponding",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Method",
      "text": "Valence Arousal SOTA STRNN  [12]  70.80 80.30 PCRNN  [29]  79.93 81.48 DT  [44]  75.53 75.74 MLP  [44]  83.64 83.71 ContCNN  [44]  84.54 84.84 Proposed GANSER 85.28 84.  16  test folds. To assess the overall performance, the average classification accuracies over ten test folds are reported. Illustrated in Table  V , we first compared our proposed GANSER with five state-of-the-art studies, including the spatial-temporal recurrent neural network (STRNN)  [12] , the parallel convolutional recurrent neural network (PCRNN)  [29] , the decision tree (DT)  [44] , the multi-layer perceptron (MLP)  [44] , and the continuous CNN (ContCNN)  [44] , on the DREAMER dataset, respect to the emotion dimensions including valence and arousal. These studies develop different network architectures and strategies for emotion recognition. For example, Zhang et al.  [12]  proposed a spatial-temporal recurrent neural network (STRNN) to investigate both spatial and temporal dependencies of EEG signals and achieve the stateof-the-art. In the experiment, the STRNN  [12]  is implemented on the DREAMER dataset to see how much the designed RNN architecture can improve the discriminant ability. Further, the parallel convolutional recurrent neural network (PCRNN) reported by  [29]  is compared, which introduces baseline signals into pre-processing and proposes a hybrid neural network combining CNN and RNN to learn the compositional spatialtemporal feature of EEG signals. In  [44] , the authors proposed a 3D representation of the EEG segment to combine features of signals from different frequency bands while preserving spatial information among channels. Then, the performance of the Decision Tree (DT) and the Multi-Layer Perceptron (MLP) is reported to demonstrate the discriminant ability of the proposed EEG representation. Further, the authors  [44]  introduced the continuous CNN (ContCNN) to utilize the combination of features of multiple bands to complement each other and achieves comparable results. In this paper, the reported performance of DT, MLP, and ContCNN are all considered for the comparison experiment. From Table  V , we can find the proposed method outperforms most state-ofthe-art studies on both valence and arousal dimensions. Especially, although the designed classifier requires lightweight training parameters and only consists of convolutional layers, the proposed method shows great classification performance of about 85% for two-dimensional classification tasks and considerably outperforms all the state-of-the-art methods at valence dimension.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Vi. Conclusion And Future Work",
      "text": "In this paper, we propose a novel Generative Adversarial Network-based Self-supervised Data Augmentation (GANSER) framework, consisting of Adversarial Augmentation Network (AAN) and Multi-factor Training Network (MTN). In the proposed framework, we are first to combine adversarial training with self-supervised learning to tackle the EEG-based emotion recognition task. The design of AAN employs the Masking Transformation operation to mask parts of EEG signals and then force a well-designed GAN to generate high-quality and high-diversity simulated EEG samples. Here, simple but effective network architectures, e.g., the UNet with Channel Masking operation and STNet, are employed to capture the complex spatio-temporal features of EEG signals. Further, to effectively utilize simulated EEG signals, we introduce MTN, where the Multi-factor Self-supervised Learning loss is proposed to utilize the masking possibility of the Masking Transformation operation as prior knowledge and guide the feature extraction process of simulated EEG signals for generalizing the classifier to the augmented sample space.\n\nBy applying the designed framework on the benchmark datasets for emotion recognition, DEAP, and DREAMER, the experimental results show that the designed model can exploit the natural feature of real EEG signals to synthesize highquality and diverse simulated EEG signals and finally improve the classification performance. In the future, we will seek to explore a semantic data augmentation framework in which the influence of the environment noise, artifacts, and the feature at valence or arousal dimensions of simulated EEG signals can be further controlled and modified.",
      "page_start": 12,
      "page_end": 13
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Generative Adversarial Network-based Self-supervised Emotion Recognition (GANSER).",
      "page": 3
    },
    {
      "caption": "Figure 2: Network architectures of proposed generator G, discriminator D, and",
      "page": 4
    },
    {
      "caption": "Figure 2: (a), the proposed UNet consists of an",
      "page": 5
    },
    {
      "caption": "Figure 2: (b), we design a novel",
      "page": 5
    },
    {
      "caption": "Figure 3: It consists of a contracting path (left side) and an expansive",
      "page": 6
    },
    {
      "caption": "Figure 3: The network architecture of the generator G in the proposed GANSER framework.",
      "page": 7
    },
    {
      "caption": "Figure 4: The network architecture of the STNet utilized by the discriminator D and the classiÔ¨Åer C in the proposed GANSER framework. Especially, the",
      "page": 7
    },
    {
      "caption": "Figure 3: , in this paper, we design a novel network architecture,",
      "page": 7
    },
    {
      "caption": "Figure 4: , given a real EEG signal",
      "page": 7
    },
    {
      "caption": "Figure 4: , the dashed",
      "page": 8
    },
    {
      "caption": "Figure 5: In detail, EEG signals of different categories are sampled",
      "page": 10
    },
    {
      "caption": "Figure 5: A few qualitative results are showing the real EEG signals (the Ô¨Årst line), augmented EEG signals (the second line) synthesized by the proposed",
      "page": 11
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Method": "GAN\nMT\nMSL",
          "Metric": "Valence\nArousal"
        },
        {
          "Method": "",
          "Metric": "91.39\n92.22"
        },
        {
          "Method": "(cid:88)",
          "Metric": "92.27\n92.76"
        },
        {
          "Method": "(cid:88)\n(cid:88)",
          "Metric": "93.28\n93.12"
        },
        {
          "Method": "(cid:88)\n(cid:88)\n(cid:88)",
          "Metric": "93.52\n94.21"
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Method": "SOTA",
          "Valence": "92.24",
          "Arousal": "92.92",
          "Four": "-"
        },
        {
          "Method": "",
          "Valence": "92.87",
          "Arousal": "92.30",
          "Four": "-"
        },
        {
          "Method": "",
          "Valence": "90.8",
          "Arousal": "91.03",
          "Four": "-"
        },
        {
          "Method": "",
          "Valence": "90.62",
          "Arousal": "86.13",
          "Four": "-"
        },
        {
          "Method": "",
          "Valence": "84.89",
          "Arousal": "83.85",
          "Four": "-"
        },
        {
          "Method": "GAN-based",
          "Valence": "-",
          "Arousal": "-",
          "Four": "81.32"
        },
        {
          "Method": "",
          "Valence": "-",
          "Arousal": "-",
          "Four": "49.10"
        },
        {
          "Method": "Proposed",
          "Valence": "93.52",
          "Arousal": "94.21",
          "Four": "89.74"
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Method": "UNet\nCM\nSTNet",
          "Valence": "",
          "Arousal": "",
          "Mean": ""
        },
        {
          "Method": "(cid:88)\n(cid:88)",
          "Valence": "96.78",
          "Arousal": "80.81",
          "Mean": "88.80"
        },
        {
          "Method": "(cid:88)\n(cid:88)",
          "Valence": "103.62",
          "Arousal": "72.26",
          "Mean": "87.94"
        },
        {
          "Method": "(cid:88)\n(cid:88)",
          "Valence": "278.76",
          "Arousal": "286.63",
          "Mean": "282.70"
        },
        {
          "Method": "(cid:88)\n(cid:88)\n(cid:88)",
          "Valence": "49.61",
          "Arousal": "36.61",
          "Mean": "43.11"
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Method": "SOTA",
          "Valence": "70.80",
          "Arousal": "80.30"
        },
        {
          "Method": "",
          "Valence": "79.93",
          "Arousal": "81.48"
        },
        {
          "Method": "",
          "Valence": "75.53",
          "Arousal": "75.74"
        },
        {
          "Method": "",
          "Valence": "83.64",
          "Arousal": "83.71"
        },
        {
          "Method": "",
          "Valence": "84.54",
          "Arousal": "84.84"
        },
        {
          "Method": "Proposed",
          "Valence": "85.28",
          "Arousal": "84.16"
        }
      ],
      "page": 12
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Affective computing and sentiment analysis",
      "authors": [
        "E Cambria",
        "D Das",
        "S Bandyopadhyay",
        "A Feraco"
      ],
      "year": "2017",
      "venue": "A Practical Guide to Sentiment Analysis"
    },
    {
      "citation_id": "2",
      "title": "The emotion machine: Commonsense thinking, artificial intelligence, and the future of the human mind",
      "authors": [
        "M Minsky"
      ],
      "year": "2007",
      "venue": "The emotion machine: Commonsense thinking, artificial intelligence, and the future of the human mind"
    },
    {
      "citation_id": "3",
      "title": "EEG-based emotion recognition using regularized graph neural networks",
      "authors": [
        "P Zhong",
        "D Wang",
        "C Miao"
      ],
      "year": "2020",
      "venue": "Trans. Affective Computing"
    },
    {
      "citation_id": "4",
      "title": "Deep learning-based electroencephalography analysis: a systematic review",
      "authors": [
        "Y Roy",
        "H Banville",
        "I Albuquerque",
        "A Gramfort",
        "T Falk",
        "J Faubert"
      ],
      "year": "2019",
      "venue": "Journal of Neural Engineering"
    },
    {
      "citation_id": "5",
      "title": "Data augmentation for deeplearning-based electroencephalography",
      "authors": [
        "E Lashgari",
        "D Liang",
        "U Maoz"
      ],
      "year": "2020",
      "venue": "Journal of Neuroscience Methods"
    },
    {
      "citation_id": "6",
      "title": "Adaptive data augmentation for image classification",
      "authors": [
        "A Fawzi",
        "H Samulowitz",
        "D Turaga",
        "P Frossard"
      ],
      "year": "2016",
      "venue": "ICIP"
    },
    {
      "citation_id": "7",
      "title": "EEG emotion recognition using dynamical graph convolutional neural networks",
      "authors": [
        "T Song",
        "W Zheng",
        "P Song",
        "Z Cui"
      ],
      "year": "2020",
      "venue": "Trans. Affective Computing"
    },
    {
      "citation_id": "8",
      "title": "An efficient LSTM network for emotion recognition from multichannel EEG signals",
      "authors": [
        "X Du",
        "C Ma",
        "G Zhang",
        "J Li",
        "Y.-K Lai",
        "G Zhao",
        "X Deng",
        "Y.-J Liu",
        "H Wang"
      ],
      "year": "2020",
      "venue": "Trans. Affective Computing"
    },
    {
      "citation_id": "9",
      "title": "EEGbased emotion recognition via channel-wise attention and self attention",
      "authors": [
        "W Tao",
        "C Li",
        "R Song",
        "J Cheng",
        "Y Liu",
        "F Wan",
        "X Chen"
      ],
      "year": "2020",
      "venue": "Trans. Affective Computing"
    },
    {
      "citation_id": "10",
      "title": "Emotion recognition based on high-resolution EEG recordings and reconstructed brain sources",
      "authors": [
        "H Becker",
        "J Fleureau",
        "P Guillotel",
        "F Wendling",
        "I Merlet",
        "L Albera"
      ],
      "year": "2020",
      "venue": "Trans. Affective Computing"
    },
    {
      "citation_id": "11",
      "title": "SparseDGCNN: Recognizing emotion from multichannel EEG signals",
      "authors": [
        "G Zhang",
        "M Yu",
        "Y.-J Liu",
        "G Zhao",
        "D Zhang",
        "W Zheng"
      ],
      "year": "2021",
      "venue": "Trans. Affective Computing"
    },
    {
      "citation_id": "12",
      "title": "Spatial-temporal recurrent neural network for emotion recognition",
      "authors": [
        "T Zhang",
        "W Zheng",
        "Z Cui",
        "Y Zong",
        "Y Li"
      ],
      "year": "2018",
      "venue": "Trans. Cybernetics"
    },
    {
      "citation_id": "13",
      "title": "Emotion recognition from multi-channel EEG data through convolutional recurrent neural network",
      "authors": [
        "X Li",
        "D Song",
        "P Zhang",
        "G Yu",
        "Y Hou",
        "B Hu"
      ],
      "year": "2016",
      "venue": "BIBM"
    },
    {
      "citation_id": "14",
      "title": "EEG-based emotion recognition using 3d convolutional neural networks",
      "authors": [
        "E Salama",
        "R El-Khoribi",
        "M Shoman",
        "M Shalaby"
      ],
      "year": "2018",
      "venue": "IJACSA"
    },
    {
      "citation_id": "15",
      "title": "Convolutional neural network approach for eeg-based emotion recognition using brain connectivity and its spatial information",
      "authors": [
        "S.-E Moon",
        "S Jang",
        "J.-S Lee"
      ],
      "year": "2018",
      "venue": "ICASSP"
    },
    {
      "citation_id": "16",
      "title": "Madenet: Disentangling individuality of EEG signals through feature space mapping and detachment",
      "authors": [
        "S.-E Moon",
        "J.-S Lee"
      ],
      "year": "2020",
      "venue": "EMBC"
    },
    {
      "citation_id": "17",
      "title": "Wgan domain adaptation for EEG-based emotion recognition",
      "authors": [
        "Y Luo",
        "S.-Y Zhang",
        "W.-L Zheng",
        "B.-L Lu"
      ],
      "year": "2018",
      "venue": "ICNIP"
    },
    {
      "citation_id": "18",
      "title": "Rotational data augmentation for electroencephalographic data",
      "authors": [
        "M Krell",
        "S Kim"
      ],
      "year": "2017",
      "venue": "EMBC"
    },
    {
      "citation_id": "19",
      "title": "Signal processing approaches to minimize or suppress calibration time in oscillatory activity-based brain-computer interfaces",
      "authors": [
        "F Lotte"
      ],
      "year": "2015",
      "venue": "PIEEE"
    },
    {
      "citation_id": "20",
      "title": "Data augmentation for EEG-based emotion recognition with deep convolutional neural networks",
      "authors": [
        "F Wang",
        "S -H. Zhong",
        "J Peng",
        "J Jiang",
        "Y Liu"
      ],
      "year": "2018",
      "venue": "MM"
    },
    {
      "citation_id": "21",
      "title": "Data augmentation for enhancing EEG-based emotion recognition with deep generative models",
      "authors": [
        "Y Luo",
        "L.-Z Zhu",
        "Z.-Y Wan",
        "B.-L Lu"
      ],
      "year": "2020",
      "venue": "Journal of Neural Engineering"
    },
    {
      "citation_id": "22",
      "title": "EEG data augmentation for emotion recognition using a conditional Wasserstein GAN",
      "authors": [
        "Y Luo",
        "B.-L Lu"
      ],
      "year": "2018",
      "venue": "EMBC"
    },
    {
      "citation_id": "23",
      "title": "A GAN-based data augmentation method for multimodal emotion recognition",
      "authors": [
        "Y Luo",
        "L.-Z Zhu",
        "B.-L Lu"
      ],
      "year": "2019",
      "venue": "ISNN"
    },
    {
      "citation_id": "24",
      "title": "Deep EEG super-resolution: Upsampling EEG spatial resolution with generative adversarial networks",
      "authors": [
        "I Corley",
        "Y Huang"
      ],
      "year": "2018",
      "venue": "BHI"
    },
    {
      "citation_id": "25",
      "title": "Depersonalized cross-subject vigilance estimation with adversarial domain generalization",
      "authors": [
        "B.-Q Ma",
        "H Li",
        "Y Luo",
        "B.-L Lu"
      ],
      "year": "2019",
      "venue": "IJCNN"
    },
    {
      "citation_id": "26",
      "title": "Simulating brain signals: Creating synthetic EEG data via neural-based generative models for improved ssvep classification",
      "authors": [
        "N Aznan",
        "A Atapour-Abarghouei",
        "S Bonner",
        "J Connolly",
        "N Moubayed",
        "T Breckon"
      ],
      "year": "2019",
      "venue": "IJCNN"
    },
    {
      "citation_id": "27",
      "title": "EEG signal reconstruction using a generative adversarial network with wasserstein distance and temporal-spatial-frequency loss",
      "authors": [
        "T.-J Luo",
        "Y Fan",
        "L Chen",
        "G Guo",
        "C Zhou"
      ],
      "year": "2020",
      "venue": "Frontiers in Neuroinformatics"
    },
    {
      "citation_id": "28",
      "title": "MIEEG-GAN: Generating artificial motor imagery electroencephalography signals",
      "authors": [
        "S Roy",
        "S Dora",
        "K Mccreadie",
        "G Prasad"
      ],
      "year": "2020",
      "venue": "IJCNN"
    },
    {
      "citation_id": "29",
      "title": "Emotion recognition from multi-channel EEG through parallel convolutional recurrent neural network",
      "authors": [
        "Y Yang",
        "Q Wu",
        "M Qiu",
        "Y Wang",
        "X Chen"
      ],
      "year": "2018",
      "venue": "IJCNN"
    },
    {
      "citation_id": "30",
      "title": "Improved training of Wasserstein GANs",
      "authors": [
        "I Gulrajani",
        "F Ahmed",
        "M Arjovsky",
        "V Dumoulin",
        "A Courville"
      ],
      "year": "2017",
      "venue": "NIPS"
    },
    {
      "citation_id": "31",
      "title": "Xception: Deep learning with depthwise separable convolutions",
      "authors": [
        "F Chollet"
      ],
      "year": "2017",
      "venue": "CVPR"
    },
    {
      "citation_id": "32",
      "title": "Going deeper with convolutions",
      "authors": [
        "C Szegedy",
        "W Liu",
        "Y Jia",
        "P Sermanet",
        "S Reed",
        "D Anguelov",
        "D Erhan",
        "V Vanhoucke",
        "A Rabinovich"
      ],
      "year": "2015",
      "venue": "CVPR"
    },
    {
      "citation_id": "33",
      "title": "Discriminative unsupervised feature learning with exemplar convolutional neural networks",
      "authors": [
        "A Dosovitskiy",
        "P Fischer",
        "J Springenberg",
        "M Riedmiller",
        "T Brox"
      ],
      "year": "2015",
      "venue": "TPAMI"
    },
    {
      "citation_id": "34",
      "title": "Dropout: A simple way to prevent neural networks from overfitting",
      "authors": [
        "N Srivastava",
        "G Hinton",
        "A Krizhevsky",
        "I Sutskever",
        "R Salakhutdinov"
      ],
      "year": "2014",
      "venue": "The Journal of Machine Learning Research"
    },
    {
      "citation_id": "35",
      "title": "PyTorch: An imperative style, high-performance deep learning library",
      "authors": [
        "A Paszke",
        "S Gross",
        "F Massa",
        "A Lerer",
        "J Bradbury",
        "G Chanan",
        "T Killeen",
        "Z Lin",
        "N Gimelshein",
        "L Antiga"
      ],
      "year": "2019",
      "venue": "NIPS"
    },
    {
      "citation_id": "36",
      "title": "Adam: A method for stochastic optimization",
      "authors": [
        "D Kingma",
        "J Ba"
      ],
      "year": "2014",
      "venue": "Adam: A method for stochastic optimization"
    },
    {
      "citation_id": "37",
      "title": "DEAP: A database for emotion analysis; using physiological signals",
      "authors": [
        "S Koelstra",
        "C Muhl",
        "M Soleymani",
        "J.-S Lee",
        "A Yazdani",
        "T Ebrahimi",
        "T Pun",
        "A Nijholt",
        "I Patras"
      ],
      "year": "2011",
      "venue": "Trans. Affective Computing"
    },
    {
      "citation_id": "38",
      "title": "EEGbased emotion recognition using an end-to-end regional-asymmetric convolutional neural network",
      "authors": [
        "H Cui",
        "A Liu",
        "X Zhang",
        "X Chen",
        "K Wang",
        "X Chen"
      ],
      "year": "2020",
      "venue": "KBS"
    },
    {
      "citation_id": "39",
      "title": "A channelfused dense convolutional network for EEG-based emotion recognition",
      "authors": [
        "Z Gao",
        "X Wang",
        "Y Yang",
        "Y Li",
        "K Ma",
        "G Chen"
      ],
      "year": "2020",
      "venue": "Trans. Cognitive and Developmental Systems"
    },
    {
      "citation_id": "40",
      "title": "Emotion recognition using multimodal residual lstm network",
      "authors": [
        "J Ma",
        "H Tang",
        "W.-L Zheng",
        "B.-L Lu"
      ],
      "year": "2019",
      "venue": "ACMMM"
    },
    {
      "citation_id": "41",
      "title": "EEG-based emotion recognition with deep convolutional neural networks",
      "authors": [
        "M Ozdemir",
        "M Degirmenci",
        "E Izci",
        "A Akan"
      ],
      "year": "2020",
      "venue": "Biomedical Engineering"
    },
    {
      "citation_id": "42",
      "title": "Merged lstm model for emotion classification using EEG signals",
      "authors": [
        "A Garg",
        "A Kapoor",
        "A Bedi",
        "R Sunkaria"
      ],
      "year": "2019",
      "venue": "ICDSE"
    },
    {
      "citation_id": "43",
      "title": "Multi-reservoirs eeg signal feature sensing and recognition method based on generative adversarial networks",
      "authors": [
        "Y Dong",
        "F Ren"
      ],
      "year": "2020",
      "venue": "Computer Communications"
    },
    {
      "citation_id": "44",
      "title": "Continuous convolutional neural network with 3d input for EEG-based emotion recognition",
      "authors": [
        "Y Yang",
        "Q Wu",
        "Y Fu",
        "X Chen"
      ],
      "year": "2018",
      "venue": "ICNIP"
    }
  ]
}