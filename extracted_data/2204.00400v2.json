{
  "paper_id": "2204.00400v2",
  "title": "Probing Speech Emotion Recognition Transformers For Linguistic Knowledge",
  "published": "2022-04-01T12:47:45Z",
  "authors": [
    "Andreas Triantafyllopoulos",
    "Johannes Wagner",
    "Hagen Wierstorf",
    "Maximilian Schmitt",
    "Uwe Reichel",
    "Florian Eyben",
    "Felix Burkhardt",
    "Björn W. Schuller"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Large, pre-trained neural networks consisting of self-attention layers (transformers) have recently achieved state-of-the-art results on several speech emotion recognition (SER) datasets. These models are typically pre-trained in self-supervised manner with the goal to improve automatic speech recognition performance -and thus, to understand linguistic information. In this work, we investigate the extent in which this information is exploited during SER fine-tuning. Using a reproducible methodology based on open-source tools, we synthesise prosodically neutral speech utterances while varying the sentiment of the text. Valence predictions of the transformer model are very reactive to positive and negative sentiment content, as well as negations, but not to intensifiers or reducers, while none of those linguistic features impact arousal or dominance. These findings show that transformers can successfully leverage linguistic information to improve their valence predictions, and that linguistic analysis should be included in their testing.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Recently, deep neural networks (DNNs) consisting of selfattention layers (i. e., transformers) have provided state-of-theart results for speech emotion recognition (SER) and have substantially improved valence prediction  [1, 2, 3, 4, 5] . These models are typically pre-trained on large corpora in a self-supervised fashion, with the main goal of improving automatic speech recognition performance; thus, they capture a large amount of linguistic information that is beneficial for that task. Accordingly, valence is often easier to predict from text rather than audio information  [6, 7] . This raises the question whether transformer models fine-tuned for SER partially rely on that information for improving their valence performance, as opposed to utilising exclusively paralinguistic cues. Furthermore, if transformers turn out to leverage linguistic information, they might also be fallible to the same risks and biases faced by natural language processing (NLP) models  [8, 9, 10] .\n\nPreliminary findings indicate that transformers make use of linguistic information for valence prediction; we found that WAV2VEC2.0 variants fine-tuned to predict arousal, valence, and dominance retain a large part of their valence, but not its arousal or dominance performance when tested on neutral speech synthesised from transcriptions  [1] . The models additionally exhibited high reactivity to the sentiment of the text in their valence predictions. Interestingly, both these trends were only evident after fine-tuning the self-attention layers for SER, and not when simply training an output head on the pre-trained embeddings. Moreover, the fine-tuning of those layers proved crucial in obtaining state-of-the-art valence recognition. Overall, this suggests that linguistic information is needed for obtaining good valence performance (while not so for arousal or dominance) and that this information is uncovered by fine-tuning the transformer layers.\n\nPrevious works that analysed the representations of WAV2VEC2.0 lend further evidence to the hypothesis that its intermediate layers contain traces of linguistic information. These works rely on the process of feature probing  [11, 12, 13, 14] , whereby a simple model (i. e., probe) is trained to predict interpretable features using the intermediate representations of the model to be tested. For example, Shah et al.  [13]  found evidence of linguistic knowledge in the middle and deeper layers of the base WAV2VEC2.0 model (w2v2-b), with acoustic knowledge being more concentrated in the shallower layers. For the large variant (w2v2-L), Pasad et al.  [12]  found that it follows a similar pattern, with shallower layers focusing more on acoustic properties and middle ones on linguistics; however, the trend is reversed towards the last layers with the transformer layers exhibiting an autoencoder-style behaviour and reconstructing their input, thus placing again an emphasis on acoustics. This is consistent with the pre-training task of WAV2VEC2.0  [15] , masked token prediction, which tries to reconstruct the (discretised) inputs of the transformer. They further found that automatic speech recognition (ASR) fine-tuning breaks this autoencoder-style behaviour by letting output representations deviate from the input to learn task-specific information.\n\nThe main contribution of this work relies on providing comprehensive, reproducible probing processes based on publiclyavailable tools with an emphasis on the linguistic information learnt by SER models. It is based on three probing methodologies: (a) re-synthesising speech signals from automatic transcriptions of the test partition using ESPNET  [16, 17] , (b) using CHECKLIST  [18]  to generate a test suite of utterances that contain text-based emotional information, which is also synthesised using ESPNET, and (c) feature probing, where we follow the work of Shah et al.  [13]  to detect traces of acoustic and linguistic knowledge in the intermediate representations of our model. We use this process to characterise the behaviour of our recent, stateof-the-art SER model  [1] , and contrast it to the behaviour of the original embeddings (i. e., freezing the transformer layers) in order to better understand the impact of fine-tuning. In particular, this lets us investigate whether the fine-tuning is necessary for adapting to acoustic mismatches between the pre-training and downstream domains, as previously shown for convolutional neural networks (CNNs)  [19] , or to better leverage linguistic information. This type of behavioural testing goes beyond past work that typically investigates SER models' robustness with respect to noise and small perturbations  [20, 21, 22]  or fairness  [23, 24] , thus, providing better insights into the inner workings of SER models. We begin by briefly describing the process used to train the models probed here. More details can be found in Wagner et al.  [1] . The model follows the w2v2-L architecture  [15]  and has been pretrained on 63k hours of data sourced from 4 different corpora, resulting in a model which we refer to as w2v2-L-robust  [25] . w2v2-L-robust is adapted for prediction by adding a simple head, consisting of an average pooling layer which aggregates the embeddings of the last hidden layer, and an output linear layer. It is then fine-tuned on multitask emotional dimension prediction (arousal, valence, and dominance) on MSP-Podcast (v1.7)  [26] . The dataset consists of roughly 84 hours of naturalistic speech from podcast recordings. The original labels are annotated on a 7-point Likert scale, which we normalise into the interval of 0 to 1. In-domain results are reported on the test-1 split. The test-1 split contains 12, 902 samples (54% female / 46% male) from 60 speakers (30 female / 30 male). The samples have a combined length of roughly 21 hours, and vary between 1.92 s and 11.94 s per sample.\n\nFor fine-tuning on the downstream task, we use the Adam optimiser with concordance correlation coefficient (CCC) loss, which is commonly used as loss function for dimensional SER  [27] , and a fixed learning rate of We run for 5 epochs with a batch size of 32 and keep the checkpoint with best performance on the development set. Training instances are cropped/padded to a fixed length of 8 seconds.\n\nIn order to understand the impact of fine-tuning several layers, we experiment with two variants: w2v2-L-emo-frz and w2v2-L-emo-ft. Both are using the same output head, but for the former, we only train this added head, whereas for the latter, we additionally fine-tune the transformer layers (while always keeping the original CNN weights). According to Wang et al.  [3] , such a partial fine-tuning yields better results than a full fine-tuning including the CNN-based feature encoder. These models are trained using a single random seed, for which the performance is reported, as we found that fine-tuning from a pre-trained state leads to stable training behaviour  [1] .",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Probing #1: Re-Synthesised Transcriptions",
      "text": "The first probing experiment is motivated by Wagner et al.  [1] , where we synthesised neutral-sounding speech using the transcriptions of MSP-Podcast. In the present work, instead of using Google Text-to-Speech and the manual transcriptions (which cover only a subset of the dataset), we use open-source tools to automatically transcribe and re-synthesise each utterance. For transcriptions, we use the wav2vec2-base-960h speech recognition model.  1  While these are less accurate than manual transcriptions (word error rate on the 50 334 transcribed samples is 34.7%) , they have the added benefit of a) covering the entire dataset, and b) allowing us to extract linguistic features for probing (Section 2.4). The resulting transcriptions are synthesised using a transformer model trained with a guided attention loss and using phoneme inputs  [17] , which gave the highest MOS scores when trained on LJ Speech  [28] . This model is freely available through ESPNET 2    [16, 17] .\n\nESPNET is able to synthesise realistic-sounding neutral speech which contains some prosodic fluctuations resulting from sentence structure, but this variation is not (intentionally) carrying any emotional information, as it has only been trained to synthesise the target utterance agnostic to emotion. Thus, on average, any emotion would manifest only in the text, rather than in the paralinguistics; and, therefore, any SER model that performs well on the resulting samples would have to utilise linguistic information. This is tested by computing the CCC performance on the synthesised samples.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Probing #2: Checklist And Tts",
      "text": "We further use the CHECKLIST toolkit 3  [18]  as another means of gauging model dependence on linguistics. CHECKLIST is a toolkit which allows the user to generate automatic tests for NLP models. It contains a set of expanding templates which allows the user to fill in keywords and automatically generates test instances for the intended behaviour. Ribeiro et al.  [18]  use this functionality to benchmark several NLP models, including sentiment models, and measure their success and failure rate. They have used data from the airlines domain (e. g., \"That was a wonderful aircraft.\"). To be comparable with previous work, we use the test suites generated by the original authors. Even though this does not fit the domain our models were trained on (podcast data), we expect our model to generalise to a reasonable extent.\n\nCHECKLIST works by generating a set of test sentences. However, our models rely on the spoken word. We thus use ESPNET to synthesise them. The same considerations as in Section 2.2 apply here; namely, that any emotional information will be attributable to text, rather than acoustics.\n\nContrary to Ribeiro et al.  [18] , we do not use CHECKLIST for testing, but for probing. That is, we do not a-priori expect the model to produce a specific outcome (e. g., high valence for positive words). Rather, we investigate what the model predicts in an attempt to better gauge its reliance on linguistic content for making predictions. Therefore, any emotional information is (on average) explicitly attributed to linguistics.\n\nOur probing begins with negative, neutral, and positive words in isolation (e. g., \"dreadful\", \"commercial\", \"excellent\"); this tests the behaviour of the models when the relationship of linguistics to sentiment is straightforward. Then, we introduce context (e. g., \"That was a(n) dreadful/commercial/excellent flight\"); this does not influence the sentiment, but adds more prosodic fluctuation as the utterances become longer. As a more finegrained test, we add intensifiers/reducers (e. g., \"That was a really/somewhat excellent flight\") to positive/negative phrases, which are expected to impact (increase/decrease) valence and, potentially, arousal. Finally, we add negations to negative, neutral, and positive words in context; this inverts the sentiment for negative/positive and leaves neutral unchanged. Note that the sentiment test suite proposed by Ribeiro et al.  [18]  includes additional tests (e. g., for robustness to small variations in the text or fairness with respect to linguistic content). These we exclude, as we do not consider them relevant for our main question, which is whether (and to what extent) our models rely on linguistics to make their predictions.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Probing #3: Feature Probing",
      "text": "Feature probing has emerged as an interesting paradigm for understanding what auditory DNNs are learning  [11, 12, 13] . In the present study, we follow the recipe of Shah et al.  [13] . We train a 3-layer feed-forward neural network (with hidden sizes [768, 128], Adam optimiser with learning rate 0.0001, batch size of 64, 100 epochs, and exponential learning rate on validation loss plateau with a factor of 0.9 and a patience of 5 epochs) on the output representation of each transformer layer of w2v2-L-emo-ft and w2v2-L-emo-frz to predict the following set of acoustic and linguistic features, which are proposed by Shah et al.  [13] . As linguistic features, we use the number of: 1. unique words, 2. adjectives, 3. adverbs, 4. nouns, 5. verbs, 6. pronouns, 7. conjunctions, 8. subjects, 9. direct objects, as well as 10. type/token ratio (hence referred to as \"word complexity\" as in Shah et al.  [13] ), and 11. the depth of the syntax tree. We additionally add the number of negations, as this turned out important during our CHECKLIST probing. These features are all extracted using the Stanford CoreNLP toolkit 4    [29] . As acoustic features we use: 1. total signal duration, 2. zero crossing rate, 3. mean pitch, 4. local jitter, 5. local shimmer, 6. energy entropy, 7. spectral centroid, and 8. voiced to unvoiced ratio. The acoustic features are extracted using the ComParE2016  [30]  feature set of our openSMILE toolkit  5  [31] -with the exception of duration, which is obtained with audiofile.  6  We evaluate predictive performance using root mean squared error (RMSE).",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Results And Discussion",
      "text": "Our discussion begins with the results of our first probing: testing the performance on re-synthesised transcriptions. Table  1  shows the performance of w2v2-L-emo-ft and w2v2-L-emo-frz on the original test set of MSP-Podcast and its re-synthesised version. Consistent with our previous results  [1] , the fine-tuned model obtains a competitive valence performance on the re-synthesised transcriptions (CCC: .386). This is comparable to previous stateof-the-art works like that of Li et al.  [32] , which reported a valence CCC of .377 on original data, showing that linguistic information alone is sufficient for obtaining good performance on that dimension. This is not true for arousal and dominance -which is consistent with previous findings showing that linguistics are not as competitive for those two dimensions  [6] . Interestingly, the valence results only hold after in-domain finetuning of transformer layers on the original data; when keeping the original pre-trained weights, the model performance drops to chance-level. This is surprising given the fact that w2v2-L-robust must contain at least surface-level linguistic information (i. e., phonemes, words) as it yields state-of-the-art ASR results with minimal fine-tuning  [25] . Nevertheless, the results of our first probing experiment show that w2v2-L-emo-ft has some dependence on linguistic knowledge.\n\nFigure  1  then shows an overview of our second probing process. It shows the distributions of predicted emotional dimensions for (negative/neutral/positive) words in isolation, in context, and in the presence of negations for w2v2-L-emo-ft. We are primarily interested in two things: (a) a comparison of how model predictions change for each word category in isolation and in context for each of the three emotional dimensions (i. e., the same colour should be compared across all columns for the first and second rows), and (b) a comparison of how model predictions change when adding negations (i. e., the same colour should be compared within each column between the second and third row). These comparisons are quantified by statistical tests (pairwise t-tests between negative-neutral and neutral-positive for the first two rows, or negative-negative etc. for negations; 5% 95% CIs obtained with bootstrapping), while also being qualitatively described through visual inspections. Consistent with our previous results, we did not observe any large or significant differences for arousal and dominance, so we only discuss changes to valence for brevity. Furthermore, w2v2-L-emo-frz showed little reactivity to most probing experiments; the only substantial (but not statistically significant) difference is seen between valence predictions of negative (mean: .526; CI: [.376-.683]) and neutral words in isolation (mean: .602; CI: [.499-.732]). All other differences were marginal, showing that w2v2-L-emo-frz depends little on linguistic information. In contrast, w2v2-L-emo-ft shows several interesting trends, which we proceed to discuss in the following paragraphs.\n\nNegative words in isolation obtain lower valence scores (mean: .412; CI: [.172-.655] than neutral (mean: .509; CI: [.430-.584]) or positive ones (mean: .588; CI: [.400-.711]); the difference between negative and positive was significant. Valence is lower for negative (mean: .395; CI: [.180-.626]) than for neutral words in context (mean: .565; CI: [.437-.681]), with the difference being significant. Accordingly, positive words in context are predicted more positively (mean: .692; CI: [.394-.820]) than neutral words in context (mean: .565; CI: [.437-.681]) -but this difference is not significant. Surprisingly, negations seem to have a consistently negative impact on valence scores -even for negative utterances which should lead to more positive scores. Both, positive (mean: .509; CI: [.355-.751]) and negative phrases (mean: .372; CI: [.223-.542]), are scored lower than their counterparts without negation, but these differences are also not significant. Interestingly, adding negations to neutral words does result in a statistically significant reduction of valence predictions (mean: .450; CI: [.357-.606]). We return to the impact of negations later.\n\nThe last part of this probing experiment concerns intensifiers and reducers. These largely leave all dimensions unaffected (CIs overlap, p-values > .05). The only exception are the valence predictions of negative words, which are somewhat impacted by intensifiers (mean: .439; CI: [.180-.745]), but this difference is not significant, either. Thus, these higher-level semantics seem to leave the model overall unaffected.\n\nOur last probing methodology sheds more light onto the inner workings of the self-attention layers, and how they are impacted by fine-tuning. Figure  2  shows the RMSE ratio between w2v2-L-emo-ft and w2v2-L-emo-frz when probing their intermediate representations with various linguistic features. This shows relative changes caused by fine-tuning. Values below 100 % mean that the w2v2-L-emo-ft model is better at predicting features than w2v2-L-emo-frz. We hypothesise that the network will increase its dependence (thus decreasing the ratio) on the features that are most useful for making predictions, leave unaffected the amount of information it contains for features that are already present in its representations to a sufficient extent, and decrease it for any that are potentially harmful.\n\nMost features are unaffected by fine-tuning, with their  RMSE ratio fluctuating around 100 %. The only ones showing substantial change are negations, word count and complexity, and duration. The network seems to decrease its dependence on the 'surface-level' features of word count and complexity  [13] , indicating that those are not needed for emotion recognition. This reduction is only evident in the middle layers  (8) (9) (10) (11) (12) (13) (14) (15) (16) (17) (18) (19) (20) .\n\nThe most outstanding changes in information for a given feature are seen for negations (RMSE ratio decreases by 70 %) and duration (RMSE ratio increases by 150 %). Evidently, the network considers the latter an uninformative feature (potentially because MSP-Podcast contains utterances of different lengths but with similar labels thus making duration a confounding feature). In contrast, the former is considered an important feature for its downstream tasks -which is consistent with the high reactivity to negations seen for CHECKLIST. We further investigate this by computing the Pearson correlation coefficient (PCC) between negations and the valence error on the (original) MSP-Podcast test set (ytrue -ypred). The PCC shows a small positive trend (.132) for valence, but not for arousal (.005) or dominance (-.003). This means that w2v2-L-emo-ft tends to under-predict (ytrue > ypred) as the number of negations increases. We further computed the PCC between the number of negations and ground truth valence annotations on the training set: these show a small, but non-negligible negative trend (-.142) -whereas no such trend exists between negations and arousal (.033) or dominance (.018). We hypothesise that w2v2-L-emo-ft picks up this spurious correlation between negations and valence; which explains why negations lead to lower valence scores in CHECKLIST tests.\n\nIn summary, valence predictions of w2v2-L-emo-ft are impacted by linguistic information, while arousal and dominance are unaffected by it. Furthermore, fine-tuning its self-attention layers is necessary to exploit this linguistic information. This explains previous findings that linguistics are not as suitable as acoustics for arousal/dominance prediction on MSP-Podcast  [6] , and that distilling linguistic knowledge to an acoustic network helps with valence prediction  [2] . It also shows that using tests from the NLP domain will become necessary as speech 'foundational' models  [10]  become the dominant paradigm for SER.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Conclusion",
      "text": "We presented a three-stage probing methodology for quantifying the dependence of SER models on linguistic information, and used it to analyse the behaviour of a recent state-of-the-art model. Our approach demonstrates that the success of transformer-based architectures for the valence dimension can be partially attributed to linguistic knowledge encoded in their self-attention layers. It further helped us uncover a potentially spurious correlation between valence and negations which could hamper performance in real-world applications. As our probing pipeline is based on open-source libraries and is thus fully reproducible, we expect it to prove a useful tool for analysing future SER models. Future work could extend our methodology by expanding the set of probing features or utilising emotional voice conversion  [33]  to control the emotional expressivity of synthesised samples as another parameter  [34] .",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: then shows an overview of our second probing",
      "page": 3
    },
    {
      "caption": "Figure 2: shows the RMSE ratio between",
      "page": 3
    },
    {
      "caption": "Figure 1: w2v2-L-emo-ft behaviour on negative/neutral/positive text samples generated with CHECKLIST and synthesised with ESPNET.",
      "page": 4
    },
    {
      "caption": "Figure 2: RMSE ratio (in percentage) for acoustic (top) and",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "3 GLAM – Group on Language, Audio, & Music, Imperial College, UK": "andreas.triantafyllopoulos@uni-a.de"
        },
        {
          "3 GLAM – Group on Language, Audio, & Music, Imperial College, UK": "Abstract"
        },
        {
          "3 GLAM – Group on Language, Audio, & Music, Imperial College, UK": ""
        },
        {
          "3 GLAM – Group on Language, Audio, & Music, Imperial College, UK": "Large, pre-trained neural networks consisting of self-attention"
        },
        {
          "3 GLAM – Group on Language, Audio, & Music, Imperial College, UK": ""
        },
        {
          "3 GLAM – Group on Language, Audio, & Music, Imperial College, UK": "layers (transformers) have recently achieved state-of-the-art re-"
        },
        {
          "3 GLAM – Group on Language, Audio, & Music, Imperial College, UK": "sults on several\nspeech emotion recognition (SER) datasets."
        },
        {
          "3 GLAM – Group on Language, Audio, & Music, Imperial College, UK": ""
        },
        {
          "3 GLAM – Group on Language, Audio, & Music, Imperial College, UK": "These models are typically pre-trained in self-supervised manner"
        },
        {
          "3 GLAM – Group on Language, Audio, & Music, Imperial College, UK": ""
        },
        {
          "3 GLAM – Group on Language, Audio, & Music, Imperial College, UK": "with the goal to improve automatic speech recognition perfor-"
        },
        {
          "3 GLAM – Group on Language, Audio, & Music, Imperial College, UK": ""
        },
        {
          "3 GLAM – Group on Language, Audio, & Music, Imperial College, UK": "mance – and thus, to understand linguistic information. In this"
        },
        {
          "3 GLAM – Group on Language, Audio, & Music, Imperial College, UK": ""
        },
        {
          "3 GLAM – Group on Language, Audio, & Music, Imperial College, UK": "work, we investigate the extent\nin which this information is"
        },
        {
          "3 GLAM – Group on Language, Audio, & Music, Imperial College, UK": ""
        },
        {
          "3 GLAM – Group on Language, Audio, & Music, Imperial College, UK": "exploited during SER ﬁne-tuning. Using a reproducible method-"
        },
        {
          "3 GLAM – Group on Language, Audio, & Music, Imperial College, UK": ""
        },
        {
          "3 GLAM – Group on Language, Audio, & Music, Imperial College, UK": "ology based on open-source tools, we synthesise prosodically"
        },
        {
          "3 GLAM – Group on Language, Audio, & Music, Imperial College, UK": ""
        },
        {
          "3 GLAM – Group on Language, Audio, & Music, Imperial College, UK": "neutral speech utterances while varying the sentiment of the text."
        },
        {
          "3 GLAM – Group on Language, Audio, & Music, Imperial College, UK": ""
        },
        {
          "3 GLAM – Group on Language, Audio, & Music, Imperial College, UK": "Valence predictions of the transformer model are very reactive"
        },
        {
          "3 GLAM – Group on Language, Audio, & Music, Imperial College, UK": ""
        },
        {
          "3 GLAM – Group on Language, Audio, & Music, Imperial College, UK": "to positive and negative sentiment content, as well as negations,"
        },
        {
          "3 GLAM – Group on Language, Audio, & Music, Imperial College, UK": ""
        },
        {
          "3 GLAM – Group on Language, Audio, & Music, Imperial College, UK": "but not to intensiﬁers or reducers, while none of those linguistic"
        },
        {
          "3 GLAM – Group on Language, Audio, & Music, Imperial College, UK": ""
        },
        {
          "3 GLAM – Group on Language, Audio, & Music, Imperial College, UK": "features impact arousal or dominance. These ﬁndings show that"
        },
        {
          "3 GLAM – Group on Language, Audio, & Music, Imperial College, UK": ""
        },
        {
          "3 GLAM – Group on Language, Audio, & Music, Imperial College, UK": "transformers can successfully leverage linguistic information to"
        },
        {
          "3 GLAM – Group on Language, Audio, & Music, Imperial College, UK": ""
        },
        {
          "3 GLAM – Group on Language, Audio, & Music, Imperial College, UK": "improve their valence predictions, and that\nlinguistic analysis"
        },
        {
          "3 GLAM – Group on Language, Audio, & Music, Imperial College, UK": ""
        },
        {
          "3 GLAM – Group on Language, Audio, & Music, Imperial College, UK": "should be included in their testing."
        },
        {
          "3 GLAM – Group on Language, Audio, & Music, Imperial College, UK": ""
        },
        {
          "3 GLAM – Group on Language, Audio, & Music, Imperial College, UK": "Index Terms: speech emotion recognition, transformers"
        },
        {
          "3 GLAM – Group on Language, Audio, & Music, Imperial College, UK": ""
        },
        {
          "3 GLAM – Group on Language, Audio, & Music, Imperial College, UK": ""
        },
        {
          "3 GLAM – Group on Language, Audio, & Music, Imperial College, UK": "1.\nIntroduction"
        },
        {
          "3 GLAM – Group on Language, Audio, & Music, Imperial College, UK": ""
        },
        {
          "3 GLAM – Group on Language, Audio, & Music, Imperial College, UK": "Recently,\ndeep neural networks\n(DNNs)\nconsisting of\nself-"
        },
        {
          "3 GLAM – Group on Language, Audio, & Music, Imperial College, UK": ""
        },
        {
          "3 GLAM – Group on Language, Audio, & Music, Imperial College, UK": "attention layers (i. e., transformers) have provided state-of-the-"
        },
        {
          "3 GLAM – Group on Language, Audio, & Music, Imperial College, UK": ""
        },
        {
          "3 GLAM – Group on Language, Audio, & Music, Imperial College, UK": "art results for speech emotion recognition (SER) and have sub-"
        },
        {
          "3 GLAM – Group on Language, Audio, & Music, Imperial College, UK": ""
        },
        {
          "3 GLAM – Group on Language, Audio, & Music, Imperial College, UK": "stantially improved valence prediction [1, 2, 3, 4, 5]. These mod-"
        },
        {
          "3 GLAM – Group on Language, Audio, & Music, Imperial College, UK": "els are typically pre-trained on large corpora in a self-supervised"
        },
        {
          "3 GLAM – Group on Language, Audio, & Music, Imperial College, UK": "fashion, with the main goal of improving automatic speech recog-"
        },
        {
          "3 GLAM – Group on Language, Audio, & Music, Imperial College, UK": "nition performance; thus, they capture a large amount of linguis-"
        },
        {
          "3 GLAM – Group on Language, Audio, & Music, Imperial College, UK": "tic information that\nis beneﬁcial\nfor\nthat\ntask.\nAccordingly,"
        },
        {
          "3 GLAM – Group on Language, Audio, & Music, Imperial College, UK": "valence is often easier\nto predict\nfrom text\nrather\nthan audio"
        },
        {
          "3 GLAM – Group on Language, Audio, & Music, Imperial College, UK": "information [6, 7]. This raises the question whether transformer"
        },
        {
          "3 GLAM – Group on Language, Audio, & Music, Imperial College, UK": "models ﬁne-tuned for SER partially rely on that information for"
        },
        {
          "3 GLAM – Group on Language, Audio, & Music, Imperial College, UK": "improving their valence performance, as opposed to utilising"
        },
        {
          "3 GLAM – Group on Language, Audio, & Music, Imperial College, UK": "exclusively paralinguistic cues.\nFurthermore,\nif transformers"
        },
        {
          "3 GLAM – Group on Language, Audio, & Music, Imperial College, UK": "turn out to leverage linguistic information,\nthey might also be"
        },
        {
          "3 GLAM – Group on Language, Audio, & Music, Imperial College, UK": "fallible to the same risks and biases faced by natural language"
        },
        {
          "3 GLAM – Group on Language, Audio, & Music, Imperial College, UK": "processing (NLP) models [8, 9, 10]."
        },
        {
          "3 GLAM – Group on Language, Audio, & Music, Imperial College, UK": "Preliminary ﬁndings indicate that\ntransformers make use"
        },
        {
          "3 GLAM – Group on Language, Audio, & Music, Imperial College, UK": "of linguistic information for valence prediction; we found that"
        },
        {
          "3 GLAM – Group on Language, Audio, & Music, Imperial College, UK": "WAV2VEC2.0 variants ﬁne-tuned to predict arousal, valence, and"
        },
        {
          "3 GLAM – Group on Language, Audio, & Music, Imperial College, UK": "dominance retain a large part of their valence, but not its arousal"
        },
        {
          "3 GLAM – Group on Language, Audio, & Music, Imperial College, UK": "or dominance performance when tested on neutral speech synthe-"
        },
        {
          "3 GLAM – Group on Language, Audio, & Music, Imperial College, UK": "sised from transcriptions [1]. The models additionally exhibited"
        },
        {
          "3 GLAM – Group on Language, Audio, & Music, Imperial College, UK": "high reactivity to the sentiment of the text in their valence pre-"
        },
        {
          "3 GLAM – Group on Language, Audio, & Music, Imperial College, UK": "dictions. Interestingly, both these trends were only evident after"
        },
        {
          "3 GLAM – Group on Language, Audio, & Music, Imperial College, UK": "ﬁne-tuning the self-attention layers for SER, and not when sim-"
        },
        {
          "3 GLAM – Group on Language, Audio, & Music, Imperial College, UK": "ply training an output head on the pre-trained embeddings. More-"
        },
        {
          "3 GLAM – Group on Language, Audio, & Music, Imperial College, UK": "over, the ﬁne-tuning of those layers proved crucial in obtaining"
        },
        {
          "3 GLAM – Group on Language, Audio, & Music, Imperial College, UK": "state-of-the-art valence recognition. Overall, this suggests that"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "2. Methodology": "",
          "synthesise the target utterance agnostic to emotion. Thus, on": "average, any emotion would manifest only in the text,\nrather"
        },
        {
          "2. Methodology": "2.1. Model training",
          "synthesise the target utterance agnostic to emotion. Thus, on": ""
        },
        {
          "2. Methodology": "",
          "synthesise the target utterance agnostic to emotion. Thus, on": "than in the paralinguistics; and, therefore, any SER model that"
        },
        {
          "2. Methodology": "We begin by brieﬂy describing the process used to train the mod-",
          "synthesise the target utterance agnostic to emotion. Thus, on": "performs well on the resulting samples would have to utilise"
        },
        {
          "2. Methodology": "els probed here. More details can be found in Wagner et al. [1].",
          "synthesise the target utterance agnostic to emotion. Thus, on": "linguistic information. This is tested by computing the CCC"
        },
        {
          "2. Methodology": "The model follows the w2v2-L architecture [15] and has been pre-",
          "synthesise the target utterance agnostic to emotion. Thus, on": "performance on the synthesised samples."
        },
        {
          "2. Methodology": "trained on 63k hours of data sourced from 4 different corpora,",
          "synthesise the target utterance agnostic to emotion. Thus, on": ""
        },
        {
          "2. Methodology": "resulting in a model which we refer to as w2v2-L-robust [25].",
          "synthesise the target utterance agnostic to emotion. Thus, on": "2.3. Probing #2: CHECKLIST and TTS"
        },
        {
          "2. Methodology": "w2v2-L-robust is adapted for prediction by adding a simple head,",
          "synthesise the target utterance agnostic to emotion. Thus, on": ""
        },
        {
          "2. Methodology": "",
          "synthesise the target utterance agnostic to emotion. Thus, on": "We further use the CHECKLIST toolkit3 [18] as another means"
        },
        {
          "2. Methodology": "consisting of an average pooling layer which aggregates the em-",
          "synthesise the target utterance agnostic to emotion. Thus, on": ""
        },
        {
          "2. Methodology": "",
          "synthesise the target utterance agnostic to emotion. Thus, on": "of gauging model dependence on linguistics. CHECKLIST is"
        },
        {
          "2. Methodology": "beddings of the last hidden layer, and an output linear layer. It",
          "synthesise the target utterance agnostic to emotion. Thus, on": ""
        },
        {
          "2. Methodology": "",
          "synthesise the target utterance agnostic to emotion. Thus, on": "a toolkit which allows the user to generate automatic tests for"
        },
        {
          "2. Methodology": "is then ﬁne-tuned on multitask emotional dimension prediction",
          "synthesise the target utterance agnostic to emotion. Thus, on": ""
        },
        {
          "2. Methodology": "",
          "synthesise the target utterance agnostic to emotion. Thus, on": "NLP models.\nIt contains a set of expanding templates which"
        },
        {
          "2. Methodology": "(arousal, valence, and dominance) on MSP-Podcast (v1.7) [26].",
          "synthesise the target utterance agnostic to emotion. Thus, on": ""
        },
        {
          "2. Methodology": "",
          "synthesise the target utterance agnostic to emotion. Thus, on": "allows the user to ﬁll in keywords and automatically generates"
        },
        {
          "2. Methodology": "The dataset consists of roughly 84 hours of naturalistic speech",
          "synthesise the target utterance agnostic to emotion. Thus, on": ""
        },
        {
          "2. Methodology": "",
          "synthesise the target utterance agnostic to emotion. Thus, on": "test instances for the intended behaviour. Ribeiro et al. [18] use"
        },
        {
          "2. Methodology": "from podcast recordings. The original labels are annotated on",
          "synthesise the target utterance agnostic to emotion. Thus, on": ""
        },
        {
          "2. Methodology": "",
          "synthesise the target utterance agnostic to emotion. Thus, on": "this functionality to benchmark several NLP models, including"
        },
        {
          "2. Methodology": "a 7-point Likert scale, which we normalise into the interval of",
          "synthesise the target utterance agnostic to emotion. Thus, on": ""
        },
        {
          "2. Methodology": "",
          "synthesise the target utterance agnostic to emotion. Thus, on": "sentiment models, and measure their success and failure rate."
        },
        {
          "2. Methodology": "In-domain results are reported on the test-1 split. The\n0 to 1.",
          "synthesise the target utterance agnostic to emotion. Thus, on": ""
        },
        {
          "2. Methodology": "",
          "synthesise the target utterance agnostic to emotion. Thus, on": "They have used data from the airlines domain (e. g., “That was a"
        },
        {
          "2. Methodology": "test-1 split contains 12, 902 samples (54% female / 46% male)",
          "synthesise the target utterance agnostic to emotion. Thus, on": ""
        },
        {
          "2. Methodology": "",
          "synthesise the target utterance agnostic to emotion. Thus, on": "wonderful aircraft.”). To be comparable with previous work, we"
        },
        {
          "2. Methodology": "from 60 speakers (30 female / 30 male). The samples have a",
          "synthesise the target utterance agnostic to emotion. Thus, on": ""
        },
        {
          "2. Methodology": "",
          "synthesise the target utterance agnostic to emotion. Thus, on": "use the test suites generated by the original authors. Even though"
        },
        {
          "2. Methodology": "combined length of roughly 21 hours, and vary between 1.92 s",
          "synthesise the target utterance agnostic to emotion. Thus, on": ""
        },
        {
          "2. Methodology": "",
          "synthesise the target utterance agnostic to emotion. Thus, on": "this does not ﬁt the domain our models were trained on (podcast"
        },
        {
          "2. Methodology": "and 11.94 s per sample.",
          "synthesise the target utterance agnostic to emotion. Thus, on": ""
        },
        {
          "2. Methodology": "",
          "synthesise the target utterance agnostic to emotion. Thus, on": "data), we expect our model to generalise to a reasonable extent."
        },
        {
          "2. Methodology": "For ﬁne-tuning on the downstream task, we use the Adam",
          "synthesise the target utterance agnostic to emotion. Thus, on": ""
        },
        {
          "2. Methodology": "",
          "synthesise the target utterance agnostic to emotion. Thus, on": "CHECKLIST works by generating a set of\ntest sentences."
        },
        {
          "2. Methodology": "optimiser with concordance correlation coefﬁcient (CCC) loss,",
          "synthesise the target utterance agnostic to emotion. Thus, on": ""
        },
        {
          "2. Methodology": "",
          "synthesise the target utterance agnostic to emotion. Thus, on": "However, our models rely on the spoken word. We thus use"
        },
        {
          "2. Methodology": "which is\ncommonly used as\nloss\nfunction for dimensional",
          "synthesise the target utterance agnostic to emotion. Thus, on": ""
        },
        {
          "2. Methodology": "",
          "synthesise the target utterance agnostic to emotion. Thus, on": "ESPNET to synthesise them.\nThe same considerations as in"
        },
        {
          "2. Methodology": "SER [27], and a ﬁxed learning rate of 1e−4. We run for 5",
          "synthesise the target utterance agnostic to emotion. Thus, on": ""
        },
        {
          "2. Methodology": "",
          "synthesise the target utterance agnostic to emotion. Thus, on": "Section 2.2 apply here; namely, that any emotional information"
        },
        {
          "2. Methodology": "epochs with a batch size of 32 and keep the checkpoint with",
          "synthesise the target utterance agnostic to emotion. Thus, on": ""
        },
        {
          "2. Methodology": "",
          "synthesise the target utterance agnostic to emotion. Thus, on": "will be attributable to text, rather than acoustics."
        },
        {
          "2. Methodology": "best performance on the development set. Training instances are",
          "synthesise the target utterance agnostic to emotion. Thus, on": ""
        },
        {
          "2. Methodology": "",
          "synthesise the target utterance agnostic to emotion. Thus, on": "Contrary to Ribeiro et al. [18], we do not use CHECKLIST"
        },
        {
          "2. Methodology": "cropped/padded to a ﬁxed length of 8 seconds.",
          "synthesise the target utterance agnostic to emotion. Thus, on": ""
        },
        {
          "2. Methodology": "",
          "synthesise the target utterance agnostic to emotion. Thus, on": "for testing, but for probing. That is, we do not a-priori expect"
        },
        {
          "2. Methodology": "In order\nto understand the impact of ﬁne-tuning several",
          "synthesise the target utterance agnostic to emotion. Thus, on": ""
        },
        {
          "2. Methodology": "",
          "synthesise the target utterance agnostic to emotion. Thus, on": "the model to produce a speciﬁc outcome (e. g., high valence for"
        },
        {
          "2. Methodology": "layers, we experiment with two variants: w2v2-L-emo-frz and",
          "synthesise the target utterance agnostic to emotion. Thus, on": ""
        },
        {
          "2. Methodology": "",
          "synthesise the target utterance agnostic to emotion. Thus, on": "positive words). Rather, we investigate what the model predicts"
        },
        {
          "2. Methodology": "w2v2-L-emo-ft. Both are using the same output head, but for",
          "synthesise the target utterance agnostic to emotion. Thus, on": ""
        },
        {
          "2. Methodology": "",
          "synthesise the target utterance agnostic to emotion. Thus, on": "in an attempt to better gauge its reliance on linguistic content for"
        },
        {
          "2. Methodology": "the former, we only train this added head, whereas for the latter,",
          "synthesise the target utterance agnostic to emotion. Thus, on": ""
        },
        {
          "2. Methodology": "",
          "synthesise the target utterance agnostic to emotion. Thus, on": "making predictions. Therefore, any emotional information is (on"
        },
        {
          "2. Methodology": "we additionally ﬁne-tune the transformer layers (while always",
          "synthesise the target utterance agnostic to emotion. Thus, on": ""
        },
        {
          "2. Methodology": "",
          "synthesise the target utterance agnostic to emotion. Thus, on": "average) explicitly attributed to linguistics."
        },
        {
          "2. Methodology": "keeping the original CNN weights). According to Wang et al.",
          "synthesise the target utterance agnostic to emotion. Thus, on": ""
        },
        {
          "2. Methodology": "",
          "synthesise the target utterance agnostic to emotion. Thus, on": "Our probing begins with negative, neutral,\nand positive"
        },
        {
          "2. Methodology": "[3], such a partial ﬁne-tuning yields better results than a full",
          "synthesise the target utterance agnostic to emotion. Thus, on": ""
        },
        {
          "2. Methodology": "",
          "synthesise the target utterance agnostic to emotion. Thus, on": "words in isolation (e. g., “dreadful”, “commercial”, “excellent”);"
        },
        {
          "2. Methodology": "ﬁne-tuning including the CNN-based feature encoder. These",
          "synthesise the target utterance agnostic to emotion. Thus, on": ""
        },
        {
          "2. Methodology": "",
          "synthesise the target utterance agnostic to emotion. Thus, on": "this tests the behaviour of the models when the relationship of lin-"
        },
        {
          "2. Methodology": "models are trained using a single random seed, for which the",
          "synthesise the target utterance agnostic to emotion. Thus, on": ""
        },
        {
          "2. Methodology": "",
          "synthesise the target utterance agnostic to emotion. Thus, on": "guistics to sentiment is straightforward. Then, we introduce con-"
        },
        {
          "2. Methodology": "performance is reported, as we found that ﬁne-tuning from a",
          "synthesise the target utterance agnostic to emotion. Thus, on": ""
        },
        {
          "2. Methodology": "",
          "synthesise the target utterance agnostic to emotion. Thus, on": "text (e. g., “That was a(n) dreadful/commercial/excellent ﬂight”);"
        },
        {
          "2. Methodology": "pre-trained state leads to stable training behaviour [1].",
          "synthesise the target utterance agnostic to emotion. Thus, on": ""
        },
        {
          "2. Methodology": "",
          "synthesise the target utterance agnostic to emotion. Thus, on": "this does not\ninﬂuence the sentiment, but adds more prosodic"
        },
        {
          "2. Methodology": "",
          "synthesise the target utterance agnostic to emotion. Thus, on": "ﬂuctuation as the utterances become longer. As a more ﬁne-"
        },
        {
          "2. Methodology": "2.2. Probing #1: Re-synthesised transcriptions",
          "synthesise the target utterance agnostic to emotion. Thus, on": ""
        },
        {
          "2. Methodology": "",
          "synthesise the target utterance agnostic to emotion. Thus, on": "grained test, we add intensiﬁers/reducers (e. g., “That was a"
        },
        {
          "2. Methodology": "The ﬁrst probing experiment is motivated by Wagner et al. [1],",
          "synthesise the target utterance agnostic to emotion. Thus, on": "really/somewhat excellent ﬂight”) to positive/negative phrases,"
        },
        {
          "2. Methodology": "where we synthesised neutral-sounding speech using the tran-",
          "synthesise the target utterance agnostic to emotion. Thus, on": "which are expected to impact (increase/decrease) valence and,"
        },
        {
          "2. Methodology": "scriptions of MSP-Podcast. In the present work, instead of using",
          "synthesise the target utterance agnostic to emotion. Thus, on": "potentially, arousal. Finally, we add negations to negative, neu-"
        },
        {
          "2. Methodology": "Google Text-to-Speech and the manual\ntranscriptions (which",
          "synthesise the target utterance agnostic to emotion. Thus, on": "tral, and positive words in context; this inverts the sentiment for"
        },
        {
          "2. Methodology": "cover only a subset of the dataset), we use open-source tools to",
          "synthesise the target utterance agnostic to emotion. Thus, on": "negative/positive and leaves neutral unchanged. Note that\nthe"
        },
        {
          "2. Methodology": "automatically transcribe and re-synthesise each utterance. For",
          "synthesise the target utterance agnostic to emotion. Thus, on": "sentiment test suite proposed by Ribeiro et al. [18] includes addi-"
        },
        {
          "2. Methodology": "transcriptions, we use the wav2vec2-base-960h speech recog-",
          "synthesise the target utterance agnostic to emotion. Thus, on": "tional tests (e. g., for robustness to small variations in the text or"
        },
        {
          "2. Methodology": "nition model.1 While these are less accurate than manual tran-",
          "synthesise the target utterance agnostic to emotion. Thus, on": "fairness with respect to linguistic content). These we exclude, as"
        },
        {
          "2. Methodology": "scriptions (word error rate on the 50 334 transcribed samples is",
          "synthesise the target utterance agnostic to emotion. Thus, on": "we do not consider them relevant for our main question, which"
        },
        {
          "2. Methodology": "they have the added beneﬁt of a) covering the entire\n34.7%) ,",
          "synthesise the target utterance agnostic to emotion. Thus, on": "is whether (and to what extent) our models rely on linguistics to"
        },
        {
          "2. Methodology": "dataset, and b) allowing us to extract linguistic features for prob-",
          "synthesise the target utterance agnostic to emotion. Thus, on": "make their predictions."
        },
        {
          "2. Methodology": "ing (Section 2.4). The resulting transcriptions are synthesised",
          "synthesise the target utterance agnostic to emotion. Thus, on": ""
        },
        {
          "2. Methodology": "using a transformer model trained with a guided attention loss",
          "synthesise the target utterance agnostic to emotion. Thus, on": "2.4. Probing #3: Feature probing"
        },
        {
          "2. Methodology": "and using phoneme inputs [17], which gave the highest MOS",
          "synthesise the target utterance agnostic to emotion. Thus, on": ""
        },
        {
          "2. Methodology": "",
          "synthesise the target utterance agnostic to emotion. Thus, on": "Feature probing has emerged as an interesting paradigm for un-"
        },
        {
          "2. Methodology": "scores when trained on LJ Speech [28]. This model\nis freely",
          "synthesise the target utterance agnostic to emotion. Thus, on": ""
        },
        {
          "2. Methodology": "",
          "synthesise the target utterance agnostic to emotion. Thus, on": "derstanding what auditory DNNs are learning [11, 12, 13].\nIn"
        },
        {
          "2. Methodology": "available through ESPNET2 [16, 17].",
          "synthesise the target utterance agnostic to emotion. Thus, on": ""
        },
        {
          "2. Methodology": "",
          "synthesise the target utterance agnostic to emotion. Thus, on": "the present study, we follow the recipe of Shah et al. [13]. We"
        },
        {
          "2. Methodology": "ESPNET is able to synthesise realistic-sounding neutral",
          "synthesise the target utterance agnostic to emotion. Thus, on": ""
        },
        {
          "2. Methodology": "",
          "synthesise the target utterance agnostic to emotion. Thus, on": "train a 3-layer feed-forward neural network (with hidden sizes"
        },
        {
          "2. Methodology": "speech which contains some prosodic ﬂuctuations resulting from",
          "synthesise the target utterance agnostic to emotion. Thus, on": ""
        },
        {
          "2. Methodology": "",
          "synthesise the target utterance agnostic to emotion. Thus, on": "[768, 128], Adam optimiser with learning rate 0.0001, batch"
        },
        {
          "2. Methodology": "sentence structure, but\nthis variation is not (intentionally) car-",
          "synthesise the target utterance agnostic to emotion. Thus, on": ""
        },
        {
          "2. Methodology": "",
          "synthesise the target utterance agnostic to emotion. Thus, on": "learning rate on val-\nsize of 64, 100 epochs, and exponential"
        },
        {
          "2. Methodology": "rying any emotional information, as it has only been trained to",
          "synthesise the target utterance agnostic to emotion. Thus, on": ""
        },
        {
          "2. Methodology": "",
          "synthesise the target utterance agnostic to emotion. Thus, on": "idation loss plateau with a factor of 0.9 and a patience of 5"
        },
        {
          "2. Methodology": "1https://huggingface.co/facebook/wav2vec2-base-960h",
          "synthesise the target utterance agnostic to emotion. Thus, on": ""
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Table 1: CCC for (A)rousal, (V)alence, and (D)ominance pre-": ""
        },
        {
          "Table 1: CCC for (A)rousal, (V)alence, and (D)ominance pre-": "diction when evaluating w2v2-L-emo-frz and w2v2-L-emo-ft"
        },
        {
          "Table 1: CCC for (A)rousal, (V)alence, and (D)ominance pre-": ""
        },
        {
          "Table 1: CCC for (A)rousal, (V)alence, and (D)ominance pre-": "on the original test recordings of MSP-Podcast vs TTS samples"
        },
        {
          "Table 1: CCC for (A)rousal, (V)alence, and (D)ominance pre-": ""
        },
        {
          "Table 1: CCC for (A)rousal, (V)alence, and (D)ominance pre-": "generated with ESPNET from automatic transcriptions created"
        },
        {
          "Table 1: CCC for (A)rousal, (V)alence, and (D)ominance pre-": ""
        },
        {
          "Table 1: CCC for (A)rousal, (V)alence, and (D)ominance pre-": "with wav2vec2.0."
        },
        {
          "Table 1: CCC for (A)rousal, (V)alence, and (D)ominance pre-": ""
        },
        {
          "Table 1: CCC for (A)rousal, (V)alence, and (D)ominance pre-": "Data\nModel\nA\nV\nD"
        },
        {
          "Table 1: CCC for (A)rousal, (V)alence, and (D)ominance pre-": ""
        },
        {
          "Table 1: CCC for (A)rousal, (V)alence, and (D)ominance pre-": "w2v2-L-emo-ft\n.745\n.634\n.635"
        },
        {
          "Table 1: CCC for (A)rousal, (V)alence, and (D)ominance pre-": "Original∗"
        },
        {
          "Table 1: CCC for (A)rousal, (V)alence, and (D)ominance pre-": "w2v2-L-emo-frz\n.696\n.592\n.400"
        },
        {
          "Table 1: CCC for (A)rousal, (V)alence, and (D)ominance pre-": ""
        },
        {
          "Table 1: CCC for (A)rousal, (V)alence, and (D)ominance pre-": "w2v2-L-emo-ft\n.041\n.386\n.048"
        },
        {
          "Table 1: CCC for (A)rousal, (V)alence, and (D)ominance pre-": "Synthesised"
        },
        {
          "Table 1: CCC for (A)rousal, (V)alence, and (D)ominance pre-": "w2v2-L-emo-frz\n.014\n.015\n.024"
        },
        {
          "Table 1: CCC for (A)rousal, (V)alence, and (D)ominance pre-": ""
        },
        {
          "Table 1: CCC for (A)rousal, (V)alence, and (D)ominance pre-": "* Results taken from Wagner et al. [1]."
        },
        {
          "Table 1: CCC for (A)rousal, (V)alence, and (D)ominance pre-": ""
        },
        {
          "Table 1: CCC for (A)rousal, (V)alence, and (D)ominance pre-": ""
        },
        {
          "Table 1: CCC for (A)rousal, (V)alence, and (D)ominance pre-": ""
        },
        {
          "Table 1: CCC for (A)rousal, (V)alence, and (D)ominance pre-": "epochs) on the output representation of each transformer layer of"
        },
        {
          "Table 1: CCC for (A)rousal, (V)alence, and (D)ominance pre-": ""
        },
        {
          "Table 1: CCC for (A)rousal, (V)alence, and (D)ominance pre-": "w2v2-L-emo-ft and w2v2-L-emo-frz to predict the following set"
        },
        {
          "Table 1: CCC for (A)rousal, (V)alence, and (D)ominance pre-": ""
        },
        {
          "Table 1: CCC for (A)rousal, (V)alence, and (D)ominance pre-": "of acoustic and linguistic features, which are proposed by Shah et"
        },
        {
          "Table 1: CCC for (A)rousal, (V)alence, and (D)ominance pre-": ""
        },
        {
          "Table 1: CCC for (A)rousal, (V)alence, and (D)ominance pre-": "al. [13]. As linguistic features, we use the number of: 1. unique"
        },
        {
          "Table 1: CCC for (A)rousal, (V)alence, and (D)ominance pre-": ""
        },
        {
          "Table 1: CCC for (A)rousal, (V)alence, and (D)ominance pre-": "words, 2. adjectives, 3. adverbs, 4. nouns, 5. verbs, 6. pro-"
        },
        {
          "Table 1: CCC for (A)rousal, (V)alence, and (D)ominance pre-": ""
        },
        {
          "Table 1: CCC for (A)rousal, (V)alence, and (D)ominance pre-": "nouns, 7. conjunctions, 8. subjects, 9. direct objects, as well"
        },
        {
          "Table 1: CCC for (A)rousal, (V)alence, and (D)ominance pre-": ""
        },
        {
          "Table 1: CCC for (A)rousal, (V)alence, and (D)ominance pre-": "as 10. type/token ratio (hence referred to as “word complexity”"
        },
        {
          "Table 1: CCC for (A)rousal, (V)alence, and (D)ominance pre-": ""
        },
        {
          "Table 1: CCC for (A)rousal, (V)alence, and (D)ominance pre-": "as in Shah et al. [13]), and 11. the depth of the syntax tree."
        },
        {
          "Table 1: CCC for (A)rousal, (V)alence, and (D)ominance pre-": ""
        },
        {
          "Table 1: CCC for (A)rousal, (V)alence, and (D)ominance pre-": "We additionally add the number of negations, as this turned out"
        },
        {
          "Table 1: CCC for (A)rousal, (V)alence, and (D)ominance pre-": ""
        },
        {
          "Table 1: CCC for (A)rousal, (V)alence, and (D)ominance pre-": "important during our CHECKLIST probing. These features are"
        },
        {
          "Table 1: CCC for (A)rousal, (V)alence, and (D)ominance pre-": ""
        },
        {
          "Table 1: CCC for (A)rousal, (V)alence, and (D)ominance pre-": "all extracted using the Stanford CoreNLP toolkit4 [29]. As acous-"
        },
        {
          "Table 1: CCC for (A)rousal, (V)alence, and (D)ominance pre-": ""
        },
        {
          "Table 1: CCC for (A)rousal, (V)alence, and (D)ominance pre-": "tic features we use: 1. total signal duration, 2. zero crossing"
        },
        {
          "Table 1: CCC for (A)rousal, (V)alence, and (D)ominance pre-": ""
        },
        {
          "Table 1: CCC for (A)rousal, (V)alence, and (D)ominance pre-": "rate, 3. mean pitch, 4. local jitter, 5. local shimmer, 6. energy"
        },
        {
          "Table 1: CCC for (A)rousal, (V)alence, and (D)ominance pre-": ""
        },
        {
          "Table 1: CCC for (A)rousal, (V)alence, and (D)ominance pre-": "entropy, 7. spectral centroid, and 8. voiced to unvoiced ratio."
        },
        {
          "Table 1: CCC for (A)rousal, (V)alence, and (D)ominance pre-": ""
        },
        {
          "Table 1: CCC for (A)rousal, (V)alence, and (D)ominance pre-": "The acoustic features are extracted using the ComParE2016 [30]"
        },
        {
          "Table 1: CCC for (A)rousal, (V)alence, and (D)ominance pre-": ""
        },
        {
          "Table 1: CCC for (A)rousal, (V)alence, and (D)ominance pre-": "feature set of our openSMILE toolkit5\n[31] – with the excep-"
        },
        {
          "Table 1: CCC for (A)rousal, (V)alence, and (D)ominance pre-": ""
        },
        {
          "Table 1: CCC for (A)rousal, (V)alence, and (D)ominance pre-": "tion of duration, which is obtained with audioﬁle.6 We evaluate"
        },
        {
          "Table 1: CCC for (A)rousal, (V)alence, and (D)ominance pre-": ""
        },
        {
          "Table 1: CCC for (A)rousal, (V)alence, and (D)ominance pre-": "predictive performance using root mean squared error (RMSE)."
        },
        {
          "Table 1: CCC for (A)rousal, (V)alence, and (D)ominance pre-": ""
        },
        {
          "Table 1: CCC for (A)rousal, (V)alence, and (D)ominance pre-": ""
        },
        {
          "Table 1: CCC for (A)rousal, (V)alence, and (D)ominance pre-": "3. Results and discussion"
        },
        {
          "Table 1: CCC for (A)rousal, (V)alence, and (D)ominance pre-": ""
        },
        {
          "Table 1: CCC for (A)rousal, (V)alence, and (D)ominance pre-": "Our discussion begins with the results of our ﬁrst probing:\ntesting"
        },
        {
          "Table 1: CCC for (A)rousal, (V)alence, and (D)ominance pre-": ""
        },
        {
          "Table 1: CCC for (A)rousal, (V)alence, and (D)ominance pre-": "the performance on re-synthesised transcriptions. Table 1 shows"
        },
        {
          "Table 1: CCC for (A)rousal, (V)alence, and (D)ominance pre-": ""
        },
        {
          "Table 1: CCC for (A)rousal, (V)alence, and (D)ominance pre-": "the performance of w2v2-L-emo-ft and w2v2-L-emo-frz on the"
        },
        {
          "Table 1: CCC for (A)rousal, (V)alence, and (D)ominance pre-": ""
        },
        {
          "Table 1: CCC for (A)rousal, (V)alence, and (D)ominance pre-": "original test set of MSP-Podcast and its re-synthesised version."
        },
        {
          "Table 1: CCC for (A)rousal, (V)alence, and (D)ominance pre-": ""
        },
        {
          "Table 1: CCC for (A)rousal, (V)alence, and (D)ominance pre-": "Consistent with our previous results [1],\nthe ﬁne-tuned model"
        },
        {
          "Table 1: CCC for (A)rousal, (V)alence, and (D)ominance pre-": ""
        },
        {
          "Table 1: CCC for (A)rousal, (V)alence, and (D)ominance pre-": "obtains a competitive valence performance on the re-synthesised"
        },
        {
          "Table 1: CCC for (A)rousal, (V)alence, and (D)ominance pre-": ""
        },
        {
          "Table 1: CCC for (A)rousal, (V)alence, and (D)ominance pre-": "transcriptions (CCC: .386). This is comparable to previous state-"
        },
        {
          "Table 1: CCC for (A)rousal, (V)alence, and (D)ominance pre-": ""
        },
        {
          "Table 1: CCC for (A)rousal, (V)alence, and (D)ominance pre-": "of-the-art works like that of Li et al.\n[32], which reported a"
        },
        {
          "Table 1: CCC for (A)rousal, (V)alence, and (D)ominance pre-": ""
        },
        {
          "Table 1: CCC for (A)rousal, (V)alence, and (D)ominance pre-": "linguistic\nvalence CCC of .377 on original data, showing that"
        },
        {
          "Table 1: CCC for (A)rousal, (V)alence, and (D)ominance pre-": ""
        },
        {
          "Table 1: CCC for (A)rousal, (V)alence, and (D)ominance pre-": "information alone is sufﬁcient for obtaining good performance"
        },
        {
          "Table 1: CCC for (A)rousal, (V)alence, and (D)ominance pre-": ""
        },
        {
          "Table 1: CCC for (A)rousal, (V)alence, and (D)ominance pre-": "on that dimension. This is not true for arousal and dominance"
        },
        {
          "Table 1: CCC for (A)rousal, (V)alence, and (D)ominance pre-": ""
        },
        {
          "Table 1: CCC for (A)rousal, (V)alence, and (D)ominance pre-": "– which is consistent with previous ﬁndings showing that\nlin-"
        },
        {
          "Table 1: CCC for (A)rousal, (V)alence, and (D)ominance pre-": ""
        },
        {
          "Table 1: CCC for (A)rousal, (V)alence, and (D)ominance pre-": "guistics are not as competitive for\nthose two dimensions [6]."
        },
        {
          "Table 1: CCC for (A)rousal, (V)alence, and (D)ominance pre-": ""
        },
        {
          "Table 1: CCC for (A)rousal, (V)alence, and (D)ominance pre-": "Interestingly, the valence results only hold after in-domain ﬁne-"
        },
        {
          "Table 1: CCC for (A)rousal, (V)alence, and (D)ominance pre-": ""
        },
        {
          "Table 1: CCC for (A)rousal, (V)alence, and (D)ominance pre-": "tuning of transformer layers on the original data; when keeping"
        },
        {
          "Table 1: CCC for (A)rousal, (V)alence, and (D)ominance pre-": ""
        },
        {
          "Table 1: CCC for (A)rousal, (V)alence, and (D)ominance pre-": "the original pre-trained weights, the model performance drops to"
        },
        {
          "Table 1: CCC for (A)rousal, (V)alence, and (D)ominance pre-": ""
        },
        {
          "Table 1: CCC for (A)rousal, (V)alence, and (D)ominance pre-": "chance-level. This is surprising given the fact that w2v2-L-robust"
        },
        {
          "Table 1: CCC for (A)rousal, (V)alence, and (D)ominance pre-": ""
        },
        {
          "Table 1: CCC for (A)rousal, (V)alence, and (D)ominance pre-": "must contain at least surface-level\nlinguistic information (i. e.,"
        },
        {
          "Table 1: CCC for (A)rousal, (V)alence, and (D)ominance pre-": ""
        },
        {
          "Table 1: CCC for (A)rousal, (V)alence, and (D)ominance pre-": "phonemes, words) as it yields state-of-the-art ASR results with"
        },
        {
          "Table 1: CCC for (A)rousal, (V)alence, and (D)ominance pre-": ""
        },
        {
          "Table 1: CCC for (A)rousal, (V)alence, and (D)ominance pre-": "minimal ﬁne-tuning [25]. Nevertheless, the results of our ﬁrst"
        },
        {
          "Table 1: CCC for (A)rousal, (V)alence, and (D)ominance pre-": ""
        },
        {
          "Table 1: CCC for (A)rousal, (V)alence, and (D)ominance pre-": "probing experiment show that w2v2-L-emo-ft has some depen-"
        },
        {
          "Table 1: CCC for (A)rousal, (V)alence, and (D)ominance pre-": ""
        },
        {
          "Table 1: CCC for (A)rousal, (V)alence, and (D)ominance pre-": "dence on linguistic knowledge."
        },
        {
          "Table 1: CCC for (A)rousal, (V)alence, and (D)ominance pre-": ""
        },
        {
          "Table 1: CCC for (A)rousal, (V)alence, and (D)ominance pre-": "Figure 1 then shows an overview of our second probing"
        },
        {
          "Table 1: CCC for (A)rousal, (V)alence, and (D)ominance pre-": ""
        },
        {
          "Table 1: CCC for (A)rousal, (V)alence, and (D)ominance pre-": "process.\nIt shows the distributions of predicted emotional di-"
        },
        {
          "Table 1: CCC for (A)rousal, (V)alence, and (D)ominance pre-": ""
        },
        {
          "Table 1: CCC for (A)rousal, (V)alence, and (D)ominance pre-": "mensions for (negative/neutral/positive) words in isolation,\nin"
        },
        {
          "Table 1: CCC for (A)rousal, (V)alence, and (D)ominance pre-": ""
        },
        {
          "Table 1: CCC for (A)rousal, (V)alence, and (D)ominance pre-": "4https://stanfordnlp.github.io/CoreNLP"
        },
        {
          "Table 1: CCC for (A)rousal, (V)alence, and (D)ominance pre-": "5https://audeering.github.io/opensmile"
        },
        {
          "Table 1: CCC for (A)rousal, (V)alence, and (D)ominance pre-": "6https://github.com/audeering/audioﬁle"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "80": "",
          "pitch": "centroid"
        },
        {
          "80": "",
          "pitch": "duration"
        },
        {
          "80": "RMSEft/RMSEfrz·",
          "pitch": ""
        },
        {
          "80": "",
          "pitch": "negations (#)"
        },
        {
          "80": "140",
          "pitch": ""
        },
        {
          "80": "",
          "pitch": "adjectives (#)"
        },
        {
          "80": "120",
          "pitch": "objects (#)"
        },
        {
          "80": "",
          "pitch": ""
        },
        {
          "80": "",
          "pitch": "depth"
        },
        {
          "80": "100",
          "pitch": "nouns (#)"
        },
        {
          "80": "",
          "pitch": "subjects (#)"
        },
        {
          "80": "80",
          "pitch": ""
        },
        {
          "80": "",
          "pitch": "conjunctions (#)"
        },
        {
          "80": "",
          "pitch": ""
        },
        {
          "80": "",
          "pitch": "verbs (#)"
        },
        {
          "80": "",
          "pitch": "pronouns (#)"
        },
        {
          "80": "",
          "pitch": ""
        },
        {
          "80": "",
          "pitch": "complexity"
        },
        {
          "80": "",
          "pitch": "words (#)"
        },
        {
          "80": "",
          "pitch": ""
        },
        {
          "80": "",
          "pitch": "for acoustic (top) and"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Figure 1: w2v2-L-emo-ft behaviour on negative/neutral/positive text samples generated with CHECKLIST and synthesised with ESPNET.": ""
        },
        {
          "Figure 1: w2v2-L-emo-ft behaviour on negative/neutral/positive text samples generated with CHECKLIST and synthesised with ESPNET.": "trend (.132) for valence, but not for arousal (.005) or dominance"
        },
        {
          "Figure 1: w2v2-L-emo-ft behaviour on negative/neutral/positive text samples generated with CHECKLIST and synthesised with ESPNET.": ""
        },
        {
          "Figure 1: w2v2-L-emo-ft behaviour on negative/neutral/positive text samples generated with CHECKLIST and synthesised with ESPNET.": "(−.003). This means that w2v2-L-emo-ft tends to under-predict"
        },
        {
          "Figure 1: w2v2-L-emo-ft behaviour on negative/neutral/positive text samples generated with CHECKLIST and synthesised with ESPNET.": "(ytrue > ypred) as the number of negations increases. We further"
        },
        {
          "Figure 1: w2v2-L-emo-ft behaviour on negative/neutral/positive text samples generated with CHECKLIST and synthesised with ESPNET.": ""
        },
        {
          "Figure 1: w2v2-L-emo-ft behaviour on negative/neutral/positive text samples generated with CHECKLIST and synthesised with ESPNET.": "computed the PCC between the number of negations and ground"
        },
        {
          "Figure 1: w2v2-L-emo-ft behaviour on negative/neutral/positive text samples generated with CHECKLIST and synthesised with ESPNET.": ""
        },
        {
          "Figure 1: w2v2-L-emo-ft behaviour on negative/neutral/positive text samples generated with CHECKLIST and synthesised with ESPNET.": "truth valence annotations on the training set:\nthese show a small,"
        },
        {
          "Figure 1: w2v2-L-emo-ft behaviour on negative/neutral/positive text samples generated with CHECKLIST and synthesised with ESPNET.": "but non-negligible negative trend (−.142) - whereas no such"
        },
        {
          "Figure 1: w2v2-L-emo-ft behaviour on negative/neutral/positive text samples generated with CHECKLIST and synthesised with ESPNET.": ""
        },
        {
          "Figure 1: w2v2-L-emo-ft behaviour on negative/neutral/positive text samples generated with CHECKLIST and synthesised with ESPNET.": "trend exists between negations and arousal (.033) or dominance"
        },
        {
          "Figure 1: w2v2-L-emo-ft behaviour on negative/neutral/positive text samples generated with CHECKLIST and synthesised with ESPNET.": "(.018). We hypothesise that w2v2-L-emo-ft picks up this spuri-"
        },
        {
          "Figure 1: w2v2-L-emo-ft behaviour on negative/neutral/positive text samples generated with CHECKLIST and synthesised with ESPNET.": "ous correlation between negations and valence; which explains"
        },
        {
          "Figure 1: w2v2-L-emo-ft behaviour on negative/neutral/positive text samples generated with CHECKLIST and synthesised with ESPNET.": "why negations lead to lower valence scores in CHECKLIST tests."
        },
        {
          "Figure 1: w2v2-L-emo-ft behaviour on negative/neutral/positive text samples generated with CHECKLIST and synthesised with ESPNET.": ""
        },
        {
          "Figure 1: w2v2-L-emo-ft behaviour on negative/neutral/positive text samples generated with CHECKLIST and synthesised with ESPNET.": "In summary, valence predictions of w2v2-L-emo-ft are im-"
        },
        {
          "Figure 1: w2v2-L-emo-ft behaviour on negative/neutral/positive text samples generated with CHECKLIST and synthesised with ESPNET.": ""
        },
        {
          "Figure 1: w2v2-L-emo-ft behaviour on negative/neutral/positive text samples generated with CHECKLIST and synthesised with ESPNET.": "pacted by linguistic information, while arousal and dominance"
        },
        {
          "Figure 1: w2v2-L-emo-ft behaviour on negative/neutral/positive text samples generated with CHECKLIST and synthesised with ESPNET.": ""
        },
        {
          "Figure 1: w2v2-L-emo-ft behaviour on negative/neutral/positive text samples generated with CHECKLIST and synthesised with ESPNET.": "are unaffected by it. Furthermore, ﬁne-tuning its self-attention"
        },
        {
          "Figure 1: w2v2-L-emo-ft behaviour on negative/neutral/positive text samples generated with CHECKLIST and synthesised with ESPNET.": ""
        },
        {
          "Figure 1: w2v2-L-emo-ft behaviour on negative/neutral/positive text samples generated with CHECKLIST and synthesised with ESPNET.": "layers is necessary to exploit\nthis linguistic information. This"
        },
        {
          "Figure 1: w2v2-L-emo-ft behaviour on negative/neutral/positive text samples generated with CHECKLIST and synthesised with ESPNET.": ""
        },
        {
          "Figure 1: w2v2-L-emo-ft behaviour on negative/neutral/positive text samples generated with CHECKLIST and synthesised with ESPNET.": "explains previous ﬁndings that linguistics are not as suitable as"
        },
        {
          "Figure 1: w2v2-L-emo-ft behaviour on negative/neutral/positive text samples generated with CHECKLIST and synthesised with ESPNET.": ""
        },
        {
          "Figure 1: w2v2-L-emo-ft behaviour on negative/neutral/positive text samples generated with CHECKLIST and synthesised with ESPNET.": "acoustics for arousal/dominance prediction on MSP-Podcast [6],"
        },
        {
          "Figure 1: w2v2-L-emo-ft behaviour on negative/neutral/positive text samples generated with CHECKLIST and synthesised with ESPNET.": ""
        },
        {
          "Figure 1: w2v2-L-emo-ft behaviour on negative/neutral/positive text samples generated with CHECKLIST and synthesised with ESPNET.": "and that distilling linguistic knowledge to an acoustic network"
        },
        {
          "Figure 1: w2v2-L-emo-ft behaviour on negative/neutral/positive text samples generated with CHECKLIST and synthesised with ESPNET.": ""
        },
        {
          "Figure 1: w2v2-L-emo-ft behaviour on negative/neutral/positive text samples generated with CHECKLIST and synthesised with ESPNET.": "helps with valence prediction [2]. It also shows that using tests"
        },
        {
          "Figure 1: w2v2-L-emo-ft behaviour on negative/neutral/positive text samples generated with CHECKLIST and synthesised with ESPNET.": "from the NLP domain will become necessary as speech ‘founda-"
        },
        {
          "Figure 1: w2v2-L-emo-ft behaviour on negative/neutral/positive text samples generated with CHECKLIST and synthesised with ESPNET.": "tional’ models [10] become the dominant paradigm for SER."
        },
        {
          "Figure 1: w2v2-L-emo-ft behaviour on negative/neutral/positive text samples generated with CHECKLIST and synthesised with ESPNET.": ""
        },
        {
          "Figure 1: w2v2-L-emo-ft behaviour on negative/neutral/positive text samples generated with CHECKLIST and synthesised with ESPNET.": ""
        },
        {
          "Figure 1: w2v2-L-emo-ft behaviour on negative/neutral/positive text samples generated with CHECKLIST and synthesised with ESPNET.": "4. Conclusion"
        },
        {
          "Figure 1: w2v2-L-emo-ft behaviour on negative/neutral/positive text samples generated with CHECKLIST and synthesised with ESPNET.": ""
        },
        {
          "Figure 1: w2v2-L-emo-ft behaviour on negative/neutral/positive text samples generated with CHECKLIST and synthesised with ESPNET.": "We presented a three-stage probing methodology for quantifying"
        },
        {
          "Figure 1: w2v2-L-emo-ft behaviour on negative/neutral/positive text samples generated with CHECKLIST and synthesised with ESPNET.": "the dependence of SER models on linguistic information, and"
        },
        {
          "Figure 1: w2v2-L-emo-ft behaviour on negative/neutral/positive text samples generated with CHECKLIST and synthesised with ESPNET.": "used it to analyse the behaviour of a recent state-of-the-art model."
        },
        {
          "Figure 1: w2v2-L-emo-ft behaviour on negative/neutral/positive text samples generated with CHECKLIST and synthesised with ESPNET.": ""
        },
        {
          "Figure 1: w2v2-L-emo-ft behaviour on negative/neutral/positive text samples generated with CHECKLIST and synthesised with ESPNET.": "Our approach demonstrates that the success of transformer-based"
        },
        {
          "Figure 1: w2v2-L-emo-ft behaviour on negative/neutral/positive text samples generated with CHECKLIST and synthesised with ESPNET.": ""
        },
        {
          "Figure 1: w2v2-L-emo-ft behaviour on negative/neutral/positive text samples generated with CHECKLIST and synthesised with ESPNET.": "architectures for the valence dimension can be partially attributed"
        },
        {
          "Figure 1: w2v2-L-emo-ft behaviour on negative/neutral/positive text samples generated with CHECKLIST and synthesised with ESPNET.": ""
        },
        {
          "Figure 1: w2v2-L-emo-ft behaviour on negative/neutral/positive text samples generated with CHECKLIST and synthesised with ESPNET.": "to linguistic knowledge encoded in their self-attention layers."
        },
        {
          "Figure 1: w2v2-L-emo-ft behaviour on negative/neutral/positive text samples generated with CHECKLIST and synthesised with ESPNET.": ""
        },
        {
          "Figure 1: w2v2-L-emo-ft behaviour on negative/neutral/positive text samples generated with CHECKLIST and synthesised with ESPNET.": "It further helped us uncover a potentially spurious correlation"
        },
        {
          "Figure 1: w2v2-L-emo-ft behaviour on negative/neutral/positive text samples generated with CHECKLIST and synthesised with ESPNET.": ""
        },
        {
          "Figure 1: w2v2-L-emo-ft behaviour on negative/neutral/positive text samples generated with CHECKLIST and synthesised with ESPNET.": "between valence and negations which could hamper performance"
        },
        {
          "Figure 1: w2v2-L-emo-ft behaviour on negative/neutral/positive text samples generated with CHECKLIST and synthesised with ESPNET.": ""
        },
        {
          "Figure 1: w2v2-L-emo-ft behaviour on negative/neutral/positive text samples generated with CHECKLIST and synthesised with ESPNET.": "in real-world applications. As our probing pipeline is based on"
        },
        {
          "Figure 1: w2v2-L-emo-ft behaviour on negative/neutral/positive text samples generated with CHECKLIST and synthesised with ESPNET.": ""
        },
        {
          "Figure 1: w2v2-L-emo-ft behaviour on negative/neutral/positive text samples generated with CHECKLIST and synthesised with ESPNET.": "open-source libraries and is thus fully reproducible, we expect it"
        },
        {
          "Figure 1: w2v2-L-emo-ft behaviour on negative/neutral/positive text samples generated with CHECKLIST and synthesised with ESPNET.": ""
        },
        {
          "Figure 1: w2v2-L-emo-ft behaviour on negative/neutral/positive text samples generated with CHECKLIST and synthesised with ESPNET.": "to prove a useful tool for analysing future SER models. Future"
        },
        {
          "Figure 1: w2v2-L-emo-ft behaviour on negative/neutral/positive text samples generated with CHECKLIST and synthesised with ESPNET.": ""
        },
        {
          "Figure 1: w2v2-L-emo-ft behaviour on negative/neutral/positive text samples generated with CHECKLIST and synthesised with ESPNET.": "work could extend our methodology by expanding the set of"
        },
        {
          "Figure 1: w2v2-L-emo-ft behaviour on negative/neutral/positive text samples generated with CHECKLIST and synthesised with ESPNET.": ""
        },
        {
          "Figure 1: w2v2-L-emo-ft behaviour on negative/neutral/positive text samples generated with CHECKLIST and synthesised with ESPNET.": "probing features or utilising emotional voice conversion [33]"
        },
        {
          "Figure 1: w2v2-L-emo-ft behaviour on negative/neutral/positive text samples generated with CHECKLIST and synthesised with ESPNET.": ""
        },
        {
          "Figure 1: w2v2-L-emo-ft behaviour on negative/neutral/positive text samples generated with CHECKLIST and synthesised with ESPNET.": "to control the emotional expressivity of synthesised samples as"
        },
        {
          "Figure 1: w2v2-L-emo-ft behaviour on negative/neutral/positive text samples generated with CHECKLIST and synthesised with ESPNET.": ""
        },
        {
          "Figure 1: w2v2-L-emo-ft behaviour on negative/neutral/positive text samples generated with CHECKLIST and synthesised with ESPNET.": "another parameter [34]."
        },
        {
          "Figure 1: w2v2-L-emo-ft behaviour on negative/neutral/positive text samples generated with CHECKLIST and synthesised with ESPNET.": ""
        },
        {
          "Figure 1: w2v2-L-emo-ft behaviour on negative/neutral/positive text samples generated with CHECKLIST and synthesised with ESPNET.": ""
        },
        {
          "Figure 1: w2v2-L-emo-ft behaviour on negative/neutral/positive text samples generated with CHECKLIST and synthesised with ESPNET.": "5. Acknowledgements"
        },
        {
          "Figure 1: w2v2-L-emo-ft behaviour on negative/neutral/positive text samples generated with CHECKLIST and synthesised with ESPNET.": ""
        },
        {
          "Figure 1: w2v2-L-emo-ft behaviour on negative/neutral/positive text samples generated with CHECKLIST and synthesised with ESPNET.": "This work has received funding from the DFG’s Reinhart Kosel-"
        },
        {
          "Figure 1: w2v2-L-emo-ft behaviour on negative/neutral/positive text samples generated with CHECKLIST and synthesised with ESPNET.": "leck project No. 442218748 (AUDI0NOMOUS)."
        },
        {
          "Figure 1: w2v2-L-emo-ft behaviour on negative/neutral/positive text samples generated with CHECKLIST and synthesised with ESPNET.": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "6. References": "",
          "[17]": "",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": "T. Toda, K. Takeda, Y. Zhang, and X. Tan, “Espnet-TTS: Uniﬁed,"
        },
        {
          "6. References": "J. Wagner, A. Triantafyllopoulos, H. Wierstorf, M. Schmitt, F.",
          "[17]": "",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": ""
        },
        {
          "6. References": "",
          "[17]": "",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": "reproducible, and integratable open source end-to-end text-to-"
        },
        {
          "6. References": "Burkhardt, F. Eyben, and B. W. Schuller, “Dawn of the trans-",
          "[17]": "",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": ""
        },
        {
          "6. References": "",
          "[17]": "",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": "speech toolkit,” in Proceedings ICASSP, IEEE, Barcelona, Spain,"
        },
        {
          "6. References": "former era in speech emotion recognition: Closing the valence",
          "[17]": "",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": ""
        },
        {
          "6. References": "",
          "[17]": "",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": "2020, pp. 7654–7658."
        },
        {
          "6. References": "gap,” arXiv preprint arXiv:2203.07378, 2022.",
          "[17]": "",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": ""
        },
        {
          "6. References": "",
          "[17]": "[18]",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": "M. T. Ribeiro, T. Wu, C. Guestrin, and S. Singh, “Beyond accu-"
        },
        {
          "6. References": "S. Srinivasan, Z. Huang, and K. Kirchhoff, “Representation learn-",
          "[17]": "",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": ""
        },
        {
          "6. References": "",
          "[17]": "",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": "racy: Behavioral\ntesting of nlp models with checklist,” in Pro-"
        },
        {
          "6. References": "ing through cross-modal conditional teacher-student training for",
          "[17]": "",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": ""
        },
        {
          "6. References": "",
          "[17]": "",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": "ceedings ACL, Seattle, USA, 2020, pp. 4902–4912."
        },
        {
          "6. References": "speech emotion recognition,” arXiv preprint arXiv:2112.00158,",
          "[17]": "",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": ""
        },
        {
          "6. References": "2021.",
          "[17]": "[19]",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": "A. Triantafyllopoulos and B. W. Schuller, “The role of task and"
        },
        {
          "6. References": "",
          "[17]": "",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": "acoustic similarity in audio transfer learning: Insights from the"
        },
        {
          "6. References": "Y\n. Wang, A. Boumadane, and A. Heba, “A ﬁne-tuned wav2vec",
          "[17]": "",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": ""
        },
        {
          "6. References": "",
          "[17]": "",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": "speech emotion recognition case,” in Proceedings ICASSP, IEEE,"
        },
        {
          "6. References": "2.0/hubert benchmark for speech emotion recognition, speaker",
          "[17]": "",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": ""
        },
        {
          "6. References": "",
          "[17]": "",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": "Toronto, Canada, 2021, pp. 7268–7272."
        },
        {
          "6. References": "veriﬁcation and spoken language understanding,” arXiv preprint",
          "[17]": "",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": ""
        },
        {
          "6. References": "arXiv:2111.02735, 2021.",
          "[17]": "[20]",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": "A. Triantafyllopoulos, G. Keren, J. Wagner, I. Steiner, and B. W."
        },
        {
          "6. References": "",
          "[17]": "",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": "Schuller, “Towards robust speech emotion recognition using deep"
        },
        {
          "6. References": "L. Pepino, P. Riera, and L. Ferrer, “Emotion recognition from",
          "[17]": "",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": ""
        },
        {
          "6. References": "",
          "[17]": "",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": "residual networks for speech enhancement,” in Proceedings IN-"
        },
        {
          "6. References": "speech using wav2vec 2.0 embeddings,” Brno, Czech Republic,",
          "[17]": "",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": ""
        },
        {
          "6. References": "",
          "[17]": "",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": "TERSPEECH, Graz, Austria, 2019, pp. 1691–1695."
        },
        {
          "6. References": "2021, pp. 3400–3404.",
          "[17]": "",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": ""
        },
        {
          "6. References": "",
          "[17]": "[21]",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": "C. Oates, A. Triantafyllopoulos, I. Steiner, and B. W. Schuller,"
        },
        {
          "6. References": "S. Liu, A. Mallol-Ragolta, E. Parada-Cabeleiro, K. Qian, X. Jing,",
          "[17]": "",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": ""
        },
        {
          "6. References": "",
          "[17]": "",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": "“Robust speech emotion recognition under different encoding"
        },
        {
          "6. References": "A. Kathan, B. Hu, and B. W. Schuller, “Audio self-supervised",
          "[17]": "",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": ""
        },
        {
          "6. References": "",
          "[17]": "",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": "conditions,” in Proceedings INTERSPEECH, Graz, Austria, 2019,"
        },
        {
          "6. References": "learning: A survey,” arXiv preprint arXiv:2203.01205, 2022.",
          "[17]": "",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": ""
        },
        {
          "6. References": "",
          "[17]": "",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": "pp. 3935–3939."
        },
        {
          "6. References": "A. Triantafyllopoulos, U. Reichel, S. Liu, S. Huber, F. Eyben,",
          "[17]": "",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": ""
        },
        {
          "6. References": "",
          "[17]": "[22]",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": "M. Jaiswal and E. M. Provost, “Best practices for noise-based"
        },
        {
          "6. References": "and B. W. Schuller, “Multistage linguistic conditioning of convo-",
          "[17]": "",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": ""
        },
        {
          "6. References": "",
          "[17]": "",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": "augmentation to improve the performance of emotion recognition"
        },
        {
          "6. References": "lutional layers for speech emotion recognition,” arXiv preprint",
          "[17]": "",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": ""
        },
        {
          "6. References": "",
          "[17]": "",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": "”in the wild”,” arXiv preprint arXiv:2104.08806, 2021."
        },
        {
          "6. References": "arXiv:2110.06650, 2021.",
          "[17]": "",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": ""
        },
        {
          "6. References": "",
          "[17]": "[23]",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": "M. Mohamed and B. Schuller, “Normalise for fairness: A sim-"
        },
        {
          "6. References": "L. Stappen, A. Baird, L. Christ, L. Schumann, B. Sertolli, E.-M.",
          "[17]": "",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": ""
        },
        {
          "6. References": "",
          "[17]": "",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": "ple normalisation technique for fairness in regression machine"
        },
        {
          "6. References": "Messner, E. Cambria, G. Zhao, and B. W. Schuller, “The muse",
          "[17]": "",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": ""
        },
        {
          "6. References": "",
          "[17]": "",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": "learning problems,” arXiv preprint arXiv:2202.00993, 2022."
        },
        {
          "6. References": "2021 multimodal sentiment analysis challenge: Sentiment, emo-",
          "[17]": "",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": ""
        },
        {
          "6. References": "tion, physiological-emotion, and stress,” in Proceedings ACM",
          "[17]": "[24]",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": "C. Gorrostieta, R. Lotﬁan, K. Taylor, R. Brutti, and J. Kane, “Gen-"
        },
        {
          "6. References": "International Conference on Multimedia (ACM MM), Chengdu,",
          "[17]": "",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": "der de-biasing in speech emotion recognition,” in Proceedings"
        },
        {
          "6. References": "China: ACM, 2021, pp. 5706–5707.",
          "[17]": "",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": "INTERSPEECH, Graz, Austria, 2019, pp. 2823–2827."
        },
        {
          "6. References": "C. Aspillaga, A. Carvallo, and V. Araujo, “Stress test evaluation",
          "[17]": "[25]",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": "W.-N. Hsu, A. Sriram, A. Baevski, T. Likhomanenko, Q. Xu, V."
        },
        {
          "6. References": "of transformer-based models in natural language understanding",
          "[17]": "",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": "Pratap, J. Kahn, A. Lee, R. Collobert, G. Synnaeve, and M. Auli,"
        },
        {
          "6. References": "tasks,” in Proceedings of the 12th Conference on Language Re-",
          "[17]": "",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": "“Robust wav2vec 2.0: Analyzing domain shift in self-supervised"
        },
        {
          "6. References": "sources and Evaluation (LREC 2020), Marseille, 2020, pp. 1882–",
          "[17]": "",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": "pre-training,” arXiv preprint arXiv:2104.01027, 2021."
        },
        {
          "6. References": "1894.",
          "[17]": "",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": ""
        },
        {
          "6. References": "",
          "[17]": "[26]",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": "R. Lotﬁan and C. Busso, “Building naturalistic emotionally bal-"
        },
        {
          "6. References": "E. M. Bender, T. Gebru, A. McMillan-Major, and S. Shmitchell,",
          "[17]": "",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": "anced speech corpus by retrieving emotional speech from existing"
        },
        {
          "6. References": "“On the dangers of stochastic parrots: Can language models be",
          "[17]": "",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": "podcast recordings,” IEEE Transactions on Affective Computing,"
        },
        {
          "6. References": "too big?” In Proceedings FAccT, Virtual Event, Canada, 2021,",
          "[17]": "",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": "vol. 10, no. 4, pp. 471–483, 2019."
        },
        {
          "6. References": "pp. 610–623.",
          "[17]": "",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": ""
        },
        {
          "6. References": "",
          "[17]": "[27]",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": "S. Parthasarathy and C. Busso, “Jointly predicting arousal, va-"
        },
        {
          "6. References": "R. Bommasani, D. A. Hudson, E. Adeli, R. Altman, S. Arora, S.",
          "[17]": "",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": "lence and dominance with multi-task learning,” in Proceedings"
        },
        {
          "6. References": "von Arx, M. S. Bernstein, J. Bohg, A. Bosselut, E. Brunskill, et",
          "[17]": "",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": "INTERSPEECH, Stockholm, Sweden, 2017, pp. 1103–1107."
        },
        {
          "6. References": "al., “On the opportunities and risks of foundation models,” arXiv",
          "[17]": "",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": ""
        },
        {
          "6. References": "",
          "[17]": "[28]",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": "K. Ito and L. Johnson, The LJ speech dataset, https://keithito."
        },
        {
          "6. References": "preprint arXiv:2108.07258, 2021.",
          "[17]": "",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": ""
        },
        {
          "6. References": "",
          "[17]": "",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": "com/LJ-Speech-Dataset/, 2017."
        },
        {
          "6. References": "D. Ma, N. Ryant, and M. Liberman, “Probing acoustic represen-",
          "[17]": "",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": ""
        },
        {
          "6. References": "",
          "[17]": "[29]",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": "C. D. Manning, M. Surdeanu, J. Bauer, J. R. Finkel, S. Bethard,"
        },
        {
          "6. References": "tations for phonetic properties,” in Proceedings ICASSP, IEEE,",
          "[17]": "",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": ""
        },
        {
          "6. References": "",
          "[17]": "",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": "and D. McClosky, “The stanford corenlp natural language pro-"
        },
        {
          "6. References": "Toronto, Canada, 2021, pp. 311–315.",
          "[17]": "",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": ""
        },
        {
          "6. References": "",
          "[17]": "",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": "cessing toolkit,” in Proceedings ACL: system demonstrations,"
        },
        {
          "6. References": "A. Pasad, J.-C. Chou, and K. Livescu, “Layer-wise analysis of",
          "[17]": "",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": "Baltimore, USA, 2014, pp. 55–60."
        },
        {
          "6. References": "a self-supervised speech representation model,” in Proceedings",
          "[17]": "",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": ""
        },
        {
          "6. References": "",
          "[17]": "[30]",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": "B. Schuller, S. Steidl, A. Batliner, J. Hirschberg, J. K. Burgoon,"
        },
        {
          "6. References": "ASRU, Cartagena, Colombia, 2021, pp. 914–921.",
          "[17]": "",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": ""
        },
        {
          "6. References": "",
          "[17]": "",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": "A. Baird, A. Elkins, Y. Zhang, E. Coutinho, K. Evanini, et al.,"
        },
        {
          "6. References": "J. Shah, Y. K. Singla, C. Chen, and R. R. Shah, “What all do",
          "[17]": "",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": "“The INTERSPEECH 2016 computational paralinguistics chal-"
        },
        {
          "6. References": "audio transformer models hear? probing acoustic representa-",
          "[17]": "",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": "lenge: Deception, sincerity & native language,” in Proceedings"
        },
        {
          "6. References": "tions for\nlanguage delivery and its structure,” arXiv preprint",
          "[17]": "",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": "INTERSPEECH, San Francisco, USA, 2016, pp. 2001–2005."
        },
        {
          "6. References": "arXiv:2101.00387, 2021.",
          "[17]": "",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": ""
        },
        {
          "6. References": "",
          "[17]": "[31]",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": "F. Eyben, M. W¨ollmer, and B. Schuller, “Opensmile: The munich"
        },
        {
          "6. References": "Y\n.-A. Chung, Y. Belinkov, and J. Glass, “Similarity analysis of",
          "[17]": "",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": "versatile and fast open-source audio feature extractor,” in Pro-"
        },
        {
          "6. References": "self-supervised speech representations,” in Proceedings ICASSP,",
          "[17]": "",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": "ceedings ACM International Conference on Multimedia, 2010,"
        },
        {
          "6. References": "IEEE, Toronto, Canada, 2021, pp. 3040–3044.",
          "[17]": "",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": "pp. 1459–1462."
        },
        {
          "6. References": "A. Baevski, Y. Zhou, A. Mohamed, and M. Auli, “Wav2vec 2.0:",
          "[17]": "[32]",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": "M. Li, B. Yang, J. Levy, A. Stolcke, V. Rozgic, S. Matsoukas, C."
        },
        {
          "6. References": "A framework for self-supervised learning of speech representa-",
          "[17]": "",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": "Papayiannis, D. Bone, and C. Wang, “Contrastive unsupervised"
        },
        {
          "6. References": "tions,” in Proceedings NeurIPS, Vancouver, BC, Canada, 2020,",
          "[17]": "",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": "learning for speech emotion recognition,” in Proceedings ICASSP,"
        },
        {
          "6. References": "pp. 12 449–12 460.",
          "[17]": "",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": "IEEE, Toronto, Canada, 2021, pp. 6329–6333."
        },
        {
          "6. References": "S. Watanabe, T. Hori, S. Karita, T. Hayashi, J. Nishitoba, Y. Unno,",
          "[17]": "[33]",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": "K. Zhou, B. Sisman, R. Liu, and H. Li, “Emotional voice con-"
        },
        {
          "6. References": "N. Enrique Yalta Soplin, J. Heymann, M. Wiesner, N. Chen,",
          "[17]": "",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": "version: Theory, databases and esd,” Speech Communication,"
        },
        {
          "6. References": "A. Renduchintala, and T. Ochiai, “ESPnet: End-to-end speech",
          "[17]": "",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": "vol. 137, pp. 1–18, 2022."
        },
        {
          "6. References": "processing toolkit,” in Proceedings INTERSPEECH, Hyderabad,",
          "[17]": "",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": ""
        },
        {
          "6. References": "",
          "[17]": "[34]",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": "K. Zhou, B. Sisman, R. Rana, B. W. Schuller, and H. Li, “Emotion"
        },
        {
          "6. References": "India, 2018, pp. 2207–2211.",
          "[17]": "",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": ""
        },
        {
          "6. References": "",
          "[17]": "",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": "intensity and its control for emotional voice conversion,” arXiv"
        },
        {
          "6. References": "",
          "[17]": "",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": "preprint arXiv:2201.03967, 2022."
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "6. References": "",
          "[17]": "",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": "T. Toda, K. Takeda, Y. Zhang, and X. Tan, “Espnet-TTS: Uniﬁed,"
        },
        {
          "6. References": "J. Wagner, A. Triantafyllopoulos, H. Wierstorf, M. Schmitt, F.",
          "[17]": "",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": ""
        },
        {
          "6. References": "",
          "[17]": "",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": "reproducible, and integratable open source end-to-end text-to-"
        },
        {
          "6. References": "Burkhardt, F. Eyben, and B. W. Schuller, “Dawn of the trans-",
          "[17]": "",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": ""
        },
        {
          "6. References": "",
          "[17]": "",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": "speech toolkit,” in Proceedings ICASSP, IEEE, Barcelona, Spain,"
        },
        {
          "6. References": "former era in speech emotion recognition: Closing the valence",
          "[17]": "",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": ""
        },
        {
          "6. References": "",
          "[17]": "",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": "2020, pp. 7654–7658."
        },
        {
          "6. References": "gap,” arXiv preprint arXiv:2203.07378, 2022.",
          "[17]": "",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": ""
        },
        {
          "6. References": "",
          "[17]": "[18]",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": "M. T. Ribeiro, T. Wu, C. Guestrin, and S. Singh, “Beyond accu-"
        },
        {
          "6. References": "S. Srinivasan, Z. Huang, and K. Kirchhoff, “Representation learn-",
          "[17]": "",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": ""
        },
        {
          "6. References": "",
          "[17]": "",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": "racy: Behavioral\ntesting of nlp models with checklist,” in Pro-"
        },
        {
          "6. References": "ing through cross-modal conditional teacher-student training for",
          "[17]": "",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": ""
        },
        {
          "6. References": "",
          "[17]": "",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": "ceedings ACL, Seattle, USA, 2020, pp. 4902–4912."
        },
        {
          "6. References": "speech emotion recognition,” arXiv preprint arXiv:2112.00158,",
          "[17]": "",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": ""
        },
        {
          "6. References": "2021.",
          "[17]": "[19]",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": "A. Triantafyllopoulos and B. W. Schuller, “The role of task and"
        },
        {
          "6. References": "",
          "[17]": "",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": "acoustic similarity in audio transfer learning: Insights from the"
        },
        {
          "6. References": "Y\n. Wang, A. Boumadane, and A. Heba, “A ﬁne-tuned wav2vec",
          "[17]": "",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": ""
        },
        {
          "6. References": "",
          "[17]": "",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": "speech emotion recognition case,” in Proceedings ICASSP, IEEE,"
        },
        {
          "6. References": "2.0/hubert benchmark for speech emotion recognition, speaker",
          "[17]": "",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": ""
        },
        {
          "6. References": "",
          "[17]": "",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": "Toronto, Canada, 2021, pp. 7268–7272."
        },
        {
          "6. References": "veriﬁcation and spoken language understanding,” arXiv preprint",
          "[17]": "",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": ""
        },
        {
          "6. References": "arXiv:2111.02735, 2021.",
          "[17]": "[20]",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": "A. Triantafyllopoulos, G. Keren, J. Wagner, I. Steiner, and B. W."
        },
        {
          "6. References": "",
          "[17]": "",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": "Schuller, “Towards robust speech emotion recognition using deep"
        },
        {
          "6. References": "L. Pepino, P. Riera, and L. Ferrer, “Emotion recognition from",
          "[17]": "",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": ""
        },
        {
          "6. References": "",
          "[17]": "",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": "residual networks for speech enhancement,” in Proceedings IN-"
        },
        {
          "6. References": "speech using wav2vec 2.0 embeddings,” Brno, Czech Republic,",
          "[17]": "",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": ""
        },
        {
          "6. References": "",
          "[17]": "",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": "TERSPEECH, Graz, Austria, 2019, pp. 1691–1695."
        },
        {
          "6. References": "2021, pp. 3400–3404.",
          "[17]": "",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": ""
        },
        {
          "6. References": "",
          "[17]": "[21]",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": "C. Oates, A. Triantafyllopoulos, I. Steiner, and B. W. Schuller,"
        },
        {
          "6. References": "S. Liu, A. Mallol-Ragolta, E. Parada-Cabeleiro, K. Qian, X. Jing,",
          "[17]": "",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": ""
        },
        {
          "6. References": "",
          "[17]": "",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": "“Robust speech emotion recognition under different encoding"
        },
        {
          "6. References": "A. Kathan, B. Hu, and B. W. Schuller, “Audio self-supervised",
          "[17]": "",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": ""
        },
        {
          "6. References": "",
          "[17]": "",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": "conditions,” in Proceedings INTERSPEECH, Graz, Austria, 2019,"
        },
        {
          "6. References": "learning: A survey,” arXiv preprint arXiv:2203.01205, 2022.",
          "[17]": "",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": ""
        },
        {
          "6. References": "",
          "[17]": "",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": "pp. 3935–3939."
        },
        {
          "6. References": "A. Triantafyllopoulos, U. Reichel, S. Liu, S. Huber, F. Eyben,",
          "[17]": "",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": ""
        },
        {
          "6. References": "",
          "[17]": "[22]",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": "M. Jaiswal and E. M. Provost, “Best practices for noise-based"
        },
        {
          "6. References": "and B. W. Schuller, “Multistage linguistic conditioning of convo-",
          "[17]": "",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": ""
        },
        {
          "6. References": "",
          "[17]": "",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": "augmentation to improve the performance of emotion recognition"
        },
        {
          "6. References": "lutional layers for speech emotion recognition,” arXiv preprint",
          "[17]": "",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": ""
        },
        {
          "6. References": "",
          "[17]": "",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": "”in the wild”,” arXiv preprint arXiv:2104.08806, 2021."
        },
        {
          "6. References": "arXiv:2110.06650, 2021.",
          "[17]": "",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": ""
        },
        {
          "6. References": "",
          "[17]": "[23]",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": "M. Mohamed and B. Schuller, “Normalise for fairness: A sim-"
        },
        {
          "6. References": "L. Stappen, A. Baird, L. Christ, L. Schumann, B. Sertolli, E.-M.",
          "[17]": "",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": ""
        },
        {
          "6. References": "",
          "[17]": "",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": "ple normalisation technique for fairness in regression machine"
        },
        {
          "6. References": "Messner, E. Cambria, G. Zhao, and B. W. Schuller, “The muse",
          "[17]": "",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": ""
        },
        {
          "6. References": "",
          "[17]": "",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": "learning problems,” arXiv preprint arXiv:2202.00993, 2022."
        },
        {
          "6. References": "2021 multimodal sentiment analysis challenge: Sentiment, emo-",
          "[17]": "",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": ""
        },
        {
          "6. References": "tion, physiological-emotion, and stress,” in Proceedings ACM",
          "[17]": "[24]",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": "C. Gorrostieta, R. Lotﬁan, K. Taylor, R. Brutti, and J. Kane, “Gen-"
        },
        {
          "6. References": "International Conference on Multimedia (ACM MM), Chengdu,",
          "[17]": "",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": "der de-biasing in speech emotion recognition,” in Proceedings"
        },
        {
          "6. References": "China: ACM, 2021, pp. 5706–5707.",
          "[17]": "",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": "INTERSPEECH, Graz, Austria, 2019, pp. 2823–2827."
        },
        {
          "6. References": "C. Aspillaga, A. Carvallo, and V. Araujo, “Stress test evaluation",
          "[17]": "[25]",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": "W.-N. Hsu, A. Sriram, A. Baevski, T. Likhomanenko, Q. Xu, V."
        },
        {
          "6. References": "of transformer-based models in natural language understanding",
          "[17]": "",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": "Pratap, J. Kahn, A. Lee, R. Collobert, G. Synnaeve, and M. Auli,"
        },
        {
          "6. References": "tasks,” in Proceedings of the 12th Conference on Language Re-",
          "[17]": "",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": "“Robust wav2vec 2.0: Analyzing domain shift in self-supervised"
        },
        {
          "6. References": "sources and Evaluation (LREC 2020), Marseille, 2020, pp. 1882–",
          "[17]": "",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": "pre-training,” arXiv preprint arXiv:2104.01027, 2021."
        },
        {
          "6. References": "1894.",
          "[17]": "",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": ""
        },
        {
          "6. References": "",
          "[17]": "[26]",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": "R. Lotﬁan and C. Busso, “Building naturalistic emotionally bal-"
        },
        {
          "6. References": "E. M. Bender, T. Gebru, A. McMillan-Major, and S. Shmitchell,",
          "[17]": "",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": "anced speech corpus by retrieving emotional speech from existing"
        },
        {
          "6. References": "“On the dangers of stochastic parrots: Can language models be",
          "[17]": "",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": "podcast recordings,” IEEE Transactions on Affective Computing,"
        },
        {
          "6. References": "too big?” In Proceedings FAccT, Virtual Event, Canada, 2021,",
          "[17]": "",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": "vol. 10, no. 4, pp. 471–483, 2019."
        },
        {
          "6. References": "pp. 610–623.",
          "[17]": "",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": ""
        },
        {
          "6. References": "",
          "[17]": "[27]",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": "S. Parthasarathy and C. Busso, “Jointly predicting arousal, va-"
        },
        {
          "6. References": "R. Bommasani, D. A. Hudson, E. Adeli, R. Altman, S. Arora, S.",
          "[17]": "",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": "lence and dominance with multi-task learning,” in Proceedings"
        },
        {
          "6. References": "von Arx, M. S. Bernstein, J. Bohg, A. Bosselut, E. Brunskill, et",
          "[17]": "",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": "INTERSPEECH, Stockholm, Sweden, 2017, pp. 1103–1107."
        },
        {
          "6. References": "al., “On the opportunities and risks of foundation models,” arXiv",
          "[17]": "",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": ""
        },
        {
          "6. References": "",
          "[17]": "[28]",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": "K. Ito and L. Johnson, The LJ speech dataset, https://keithito."
        },
        {
          "6. References": "preprint arXiv:2108.07258, 2021.",
          "[17]": "",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": ""
        },
        {
          "6. References": "",
          "[17]": "",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": "com/LJ-Speech-Dataset/, 2017."
        },
        {
          "6. References": "D. Ma, N. Ryant, and M. Liberman, “Probing acoustic represen-",
          "[17]": "",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": ""
        },
        {
          "6. References": "",
          "[17]": "[29]",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": "C. D. Manning, M. Surdeanu, J. Bauer, J. R. Finkel, S. Bethard,"
        },
        {
          "6. References": "tations for phonetic properties,” in Proceedings ICASSP, IEEE,",
          "[17]": "",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": ""
        },
        {
          "6. References": "",
          "[17]": "",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": "and D. McClosky, “The stanford corenlp natural language pro-"
        },
        {
          "6. References": "Toronto, Canada, 2021, pp. 311–315.",
          "[17]": "",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": ""
        },
        {
          "6. References": "",
          "[17]": "",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": "cessing toolkit,” in Proceedings ACL: system demonstrations,"
        },
        {
          "6. References": "A. Pasad, J.-C. Chou, and K. Livescu, “Layer-wise analysis of",
          "[17]": "",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": "Baltimore, USA, 2014, pp. 55–60."
        },
        {
          "6. References": "a self-supervised speech representation model,” in Proceedings",
          "[17]": "",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": ""
        },
        {
          "6. References": "",
          "[17]": "[30]",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": "B. Schuller, S. Steidl, A. Batliner, J. Hirschberg, J. K. Burgoon,"
        },
        {
          "6. References": "ASRU, Cartagena, Colombia, 2021, pp. 914–921.",
          "[17]": "",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": ""
        },
        {
          "6. References": "",
          "[17]": "",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": "A. Baird, A. Elkins, Y. Zhang, E. Coutinho, K. Evanini, et al.,"
        },
        {
          "6. References": "J. Shah, Y. K. Singla, C. Chen, and R. R. Shah, “What all do",
          "[17]": "",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": "“The INTERSPEECH 2016 computational paralinguistics chal-"
        },
        {
          "6. References": "audio transformer models hear? probing acoustic representa-",
          "[17]": "",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": "lenge: Deception, sincerity & native language,” in Proceedings"
        },
        {
          "6. References": "tions for\nlanguage delivery and its structure,” arXiv preprint",
          "[17]": "",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": "INTERSPEECH, San Francisco, USA, 2016, pp. 2001–2005."
        },
        {
          "6. References": "arXiv:2101.00387, 2021.",
          "[17]": "",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": ""
        },
        {
          "6. References": "",
          "[17]": "[31]",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": "F. Eyben, M. W¨ollmer, and B. Schuller, “Opensmile: The munich"
        },
        {
          "6. References": "Y\n.-A. Chung, Y. Belinkov, and J. Glass, “Similarity analysis of",
          "[17]": "",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": "versatile and fast open-source audio feature extractor,” in Pro-"
        },
        {
          "6. References": "self-supervised speech representations,” in Proceedings ICASSP,",
          "[17]": "",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": "ceedings ACM International Conference on Multimedia, 2010,"
        },
        {
          "6. References": "IEEE, Toronto, Canada, 2021, pp. 3040–3044.",
          "[17]": "",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": "pp. 1459–1462."
        },
        {
          "6. References": "A. Baevski, Y. Zhou, A. Mohamed, and M. Auli, “Wav2vec 2.0:",
          "[17]": "[32]",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": "M. Li, B. Yang, J. Levy, A. Stolcke, V. Rozgic, S. Matsoukas, C."
        },
        {
          "6. References": "A framework for self-supervised learning of speech representa-",
          "[17]": "",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": "Papayiannis, D. Bone, and C. Wang, “Contrastive unsupervised"
        },
        {
          "6. References": "tions,” in Proceedings NeurIPS, Vancouver, BC, Canada, 2020,",
          "[17]": "",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": "learning for speech emotion recognition,” in Proceedings ICASSP,"
        },
        {
          "6. References": "pp. 12 449–12 460.",
          "[17]": "",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": "IEEE, Toronto, Canada, 2021, pp. 6329–6333."
        },
        {
          "6. References": "S. Watanabe, T. Hori, S. Karita, T. Hayashi, J. Nishitoba, Y. Unno,",
          "[17]": "[33]",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": "K. Zhou, B. Sisman, R. Liu, and H. Li, “Emotional voice con-"
        },
        {
          "6. References": "N. Enrique Yalta Soplin, J. Heymann, M. Wiesner, N. Chen,",
          "[17]": "",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": "version: Theory, databases and esd,” Speech Communication,"
        },
        {
          "6. References": "A. Renduchintala, and T. Ochiai, “ESPnet: End-to-end speech",
          "[17]": "",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": "vol. 137, pp. 1–18, 2022."
        },
        {
          "6. References": "processing toolkit,” in Proceedings INTERSPEECH, Hyderabad,",
          "[17]": "",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": ""
        },
        {
          "6. References": "",
          "[17]": "[34]",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": "K. Zhou, B. Sisman, R. Rana, B. W. Schuller, and H. Li, “Emotion"
        },
        {
          "6. References": "India, 2018, pp. 2207–2211.",
          "[17]": "",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": ""
        },
        {
          "6. References": "",
          "[17]": "",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": "intensity and its control for emotional voice conversion,” arXiv"
        },
        {
          "6. References": "",
          "[17]": "",
          "T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,": "preprint arXiv:2201.03967, 2022."
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Dawn of the transformer era in speech emotion recognition: Closing the valence gap",
      "authors": [
        "J Wagner",
        "A Triantafyllopoulos",
        "H Wierstorf",
        "M Schmitt",
        "F Burkhardt",
        "F Eyben",
        "B Schuller"
      ],
      "year": "2022",
      "venue": "Dawn of the transformer era in speech emotion recognition: Closing the valence gap",
      "arxiv": "arXiv:2203.07378"
    },
    {
      "citation_id": "3",
      "title": "Representation learning through cross-modal conditional teacher-student training for speech emotion recognition",
      "authors": [
        "S Srinivasan",
        "Z Huang",
        "K Kirchhoff"
      ],
      "year": "2021",
      "venue": "Representation learning through cross-modal conditional teacher-student training for speech emotion recognition",
      "arxiv": "arXiv:2112.00158"
    },
    {
      "citation_id": "4",
      "title": "A fine-tuned wav2vec 2.0/hubert benchmark for speech emotion recognition, speaker verification and spoken language understanding",
      "authors": [
        "Y Wang",
        "A Boumadane",
        "A Heba"
      ],
      "year": "2021",
      "venue": "A fine-tuned wav2vec 2.0/hubert benchmark for speech emotion recognition, speaker verification and spoken language understanding",
      "arxiv": "arXiv:2111.02735"
    },
    {
      "citation_id": "5",
      "title": "Emotion recognition from speech using wav2vec 2.0 embeddings",
      "authors": [
        "L Pepino",
        "P Riera",
        "L Ferrer"
      ],
      "year": "2021",
      "venue": "Emotion recognition from speech using wav2vec 2.0 embeddings"
    },
    {
      "citation_id": "6",
      "title": "Audio self-supervised learning: A survey",
      "authors": [
        "S Liu",
        "A Mallol-Ragolta",
        "E Parada-Cabeleiro",
        "K Qian",
        "X Jing",
        "A Kathan",
        "B Hu",
        "B Schuller"
      ],
      "year": "2022",
      "venue": "Audio self-supervised learning: A survey",
      "arxiv": "arXiv:2203.01205"
    },
    {
      "citation_id": "7",
      "title": "Multistage linguistic conditioning of convolutional layers for speech emotion recognition",
      "authors": [
        "A Triantafyllopoulos",
        "U Reichel",
        "S Liu",
        "S Huber",
        "F Eyben",
        "B Schuller"
      ],
      "year": "2021",
      "venue": "Multistage linguistic conditioning of convolutional layers for speech emotion recognition",
      "arxiv": "arXiv:2110.06650"
    },
    {
      "citation_id": "8",
      "title": "The muse 2021 multimodal sentiment analysis challenge: Sentiment, emotion, physiological-emotion, and stress",
      "authors": [
        "L Stappen",
        "A Baird",
        "L Christ",
        "L Schumann",
        "B Sertolli",
        "E.-M Messner",
        "E Cambria",
        "G Zhao",
        "B Schuller"
      ],
      "year": "2021",
      "venue": "Proceedings ACM International Conference on Multimedia (ACM MM)"
    },
    {
      "citation_id": "9",
      "title": "Stress test evaluation of transformer-based models in natural language understanding tasks",
      "authors": [
        "C Aspillaga",
        "A Carvallo",
        "V Araujo"
      ],
      "year": "2020",
      "venue": "Proceedings of the 12th Conference on Language Resources and Evaluation (LREC 2020)"
    },
    {
      "citation_id": "10",
      "title": "On the dangers of stochastic parrots: Can language models be too big?",
      "authors": [
        "E Bender",
        "T Gebru",
        "A Mcmillan-Major",
        "S Shmitchell"
      ],
      "year": "2021",
      "venue": "Proceedings FAccT, Virtual Event"
    },
    {
      "citation_id": "11",
      "title": "On the opportunities and risks of foundation models",
      "authors": [
        "R Bommasani",
        "D Hudson",
        "E Adeli",
        "R Altman",
        "S Arora",
        "S Arx",
        "M Bernstein",
        "J Bohg",
        "A Bosselut",
        "E Brunskill"
      ],
      "year": "2021",
      "venue": "On the opportunities and risks of foundation models",
      "arxiv": "arXiv:2108.07258"
    },
    {
      "citation_id": "12",
      "title": "Probing acoustic representations for phonetic properties",
      "authors": [
        "D Ma",
        "N Ryant",
        "M Liberman"
      ],
      "year": "2021",
      "venue": "Proceedings ICASSP, IEEE"
    },
    {
      "citation_id": "13",
      "title": "Layer-wise analysis of a self-supervised speech representation model",
      "authors": [
        "A Pasad",
        "J.-C Chou",
        "K Livescu"
      ],
      "year": "2021",
      "venue": "Proceedings ASRU"
    },
    {
      "citation_id": "14",
      "title": "What all do audio transformer models hear? probing acoustic representations for language delivery and its structure",
      "authors": [
        "J Shah",
        "Y Singla",
        "C Chen",
        "R Shah"
      ],
      "year": "2021",
      "venue": "What all do audio transformer models hear? probing acoustic representations for language delivery and its structure",
      "arxiv": "arXiv:2101.00387"
    },
    {
      "citation_id": "15",
      "title": "Similarity analysis of self-supervised speech representations",
      "authors": [
        "Y.-A Chung",
        "Y Belinkov",
        "J Glass"
      ],
      "year": "2021",
      "venue": "Proceedings ICASSP, IEEE"
    },
    {
      "citation_id": "16",
      "title": "Wav2vec 2.0: A framework for self-supervised learning of speech representations",
      "authors": [
        "A Baevski",
        "Y Zhou",
        "A Mohamed",
        "M Auli"
      ],
      "year": "2020",
      "venue": "Proceedings NeurIPS"
    },
    {
      "citation_id": "17",
      "title": "ESPnet: End-to-end speech processing toolkit",
      "authors": [
        "S Watanabe",
        "T Hori",
        "S Karita",
        "T Hayashi",
        "J Nishitoba",
        "Y Unno",
        "N Yalta Soplin",
        "J Heymann",
        "M Wiesner",
        "N Chen",
        "A Renduchintala",
        "T Ochiai"
      ],
      "year": "2018",
      "venue": "Proceedings INTERSPEECH"
    },
    {
      "citation_id": "18",
      "title": "Espnet-TTS: Unified, reproducible, and integratable open source end-to-end text-tospeech toolkit",
      "authors": [
        "T Hayashi",
        "R Yamamoto",
        "K Inoue",
        "T Yoshimura",
        "S Watanabe",
        "T Toda",
        "K Takeda",
        "Y Zhang",
        "X Tan"
      ],
      "year": "2020",
      "venue": "Proceedings ICASSP, IEEE"
    },
    {
      "citation_id": "19",
      "title": "Beyond accuracy: Behavioral testing of nlp models with checklist",
      "authors": [
        "M Ribeiro",
        "T Wu",
        "C Guestrin",
        "S Singh"
      ],
      "year": "2020",
      "venue": "Proceedings ACL"
    },
    {
      "citation_id": "20",
      "title": "The role of task and acoustic similarity in audio transfer learning: Insights from the speech emotion recognition case",
      "authors": [
        "A Triantafyllopoulos",
        "B Schuller"
      ],
      "year": "2021",
      "venue": "Proceedings ICASSP, IEEE"
    },
    {
      "citation_id": "21",
      "title": "Towards robust speech emotion recognition using deep residual networks for speech enhancement",
      "authors": [
        "A Triantafyllopoulos",
        "G Keren",
        "J Wagner",
        "I Steiner",
        "B Schuller"
      ],
      "year": "2019",
      "venue": "Proceedings IN-TERSPEECH"
    },
    {
      "citation_id": "22",
      "title": "Robust speech emotion recognition under different encoding conditions",
      "authors": [
        "C Oates",
        "A Triantafyllopoulos",
        "I Steiner",
        "B Schuller"
      ],
      "year": "2019",
      "venue": "Proceedings INTERSPEECH"
    },
    {
      "citation_id": "23",
      "title": "Best practices for noise-based augmentation to improve the performance of emotion recognition \"in the wild",
      "authors": [
        "E Provost"
      ],
      "year": "2021",
      "venue": "Best practices for noise-based augmentation to improve the performance of emotion recognition \"in the wild",
      "arxiv": "arXiv:2104.08806"
    },
    {
      "citation_id": "24",
      "title": "Normalise for fairness: A simple normalisation technique for fairness in regression machine learning problems",
      "authors": [
        "M Mohamed",
        "B Schuller"
      ],
      "year": "2022",
      "venue": "Normalise for fairness: A simple normalisation technique for fairness in regression machine learning problems",
      "arxiv": "arXiv:2202.00993"
    },
    {
      "citation_id": "25",
      "title": "Gender de-biasing in speech emotion recognition",
      "authors": [
        "C Gorrostieta",
        "R Lotfian",
        "K Taylor",
        "R Brutti",
        "J Kane"
      ],
      "year": "2019",
      "venue": "Proceedings INTERSPEECH"
    },
    {
      "citation_id": "26",
      "title": "Robust wav2vec 2.0: Analyzing domain shift in self-supervised pre-training",
      "authors": [
        "W.-N Hsu",
        "A Sriram",
        "A Baevski",
        "T Likhomanenko",
        "Q Xu",
        "V Pratap",
        "J Kahn",
        "A Lee",
        "R Collobert",
        "G Synnaeve",
        "M Auli"
      ],
      "year": "2021",
      "venue": "Robust wav2vec 2.0: Analyzing domain shift in self-supervised pre-training",
      "arxiv": "arXiv:2104.01027"
    },
    {
      "citation_id": "27",
      "title": "Building naturalistic emotionally balanced speech corpus by retrieving emotional speech from existing podcast recordings",
      "authors": [
        "R Lotfian",
        "C Busso"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "28",
      "title": "Jointly predicting arousal, valence and dominance with multi-task learning",
      "authors": [
        "S Parthasarathy",
        "C Busso"
      ],
      "year": "2017",
      "venue": "Proceedings INTERSPEECH"
    },
    {
      "citation_id": "29",
      "title": "The LJ speech dataset",
      "authors": [
        "K Ito",
        "L Johnson"
      ],
      "year": "2017",
      "venue": "The LJ speech dataset"
    },
    {
      "citation_id": "30",
      "title": "The stanford corenlp natural language processing toolkit",
      "authors": [
        "C Manning",
        "M Surdeanu",
        "J Bauer",
        "J Finkel",
        "S Bethard",
        "D Mcclosky"
      ],
      "year": "2014",
      "venue": "Proceedings ACL: system demonstrations"
    },
    {
      "citation_id": "31",
      "title": "The INTERSPEECH 2016 computational paralinguistics challenge: Deception, sincerity & native language",
      "authors": [
        "B Schuller",
        "S Steidl",
        "A Batliner",
        "J Hirschberg",
        "J Burgoon",
        "A Baird",
        "A Elkins",
        "Y Zhang",
        "E Coutinho",
        "K Evanini"
      ],
      "year": "2016",
      "venue": "Proceedings INTERSPEECH"
    },
    {
      "citation_id": "32",
      "title": "Opensmile: The munich versatile and fast open-source audio feature extractor",
      "authors": [
        "F Eyben",
        "M Wöllmer",
        "B Schuller"
      ],
      "year": "2010",
      "venue": "Proceedings ACM International Conference on Multimedia"
    },
    {
      "citation_id": "33",
      "title": "Contrastive unsupervised learning for speech emotion recognition",
      "authors": [
        "M Li",
        "B Yang",
        "J Levy",
        "A Stolcke",
        "V Rozgic",
        "S Matsoukas",
        "C Papayiannis",
        "D Bone",
        "C Wang"
      ],
      "year": "2021",
      "venue": "Proceedings ICASSP, IEEE"
    },
    {
      "citation_id": "34",
      "title": "Emotional voice conversion: Theory, databases and esd",
      "authors": [
        "K Zhou",
        "B Sisman",
        "R Liu",
        "H Li"
      ],
      "year": "2022",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "35",
      "title": "Emotion intensity and its control for emotional voice conversion",
      "authors": [
        "K Zhou",
        "B Sisman",
        "R Rana",
        "B Schuller",
        "H Li"
      ],
      "year": "2022",
      "venue": "Emotion intensity and its control for emotional voice conversion",
      "arxiv": "arXiv:2201.03967"
    }
  ]
}