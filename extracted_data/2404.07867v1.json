{
  "paper_id": "2404.07867v1",
  "title": "The Power Of Properties: Uncovering The Influential Factors In Emotion Classification",
  "published": "2024-04-11T16:01:00Z",
  "authors": [
    "Tim Büchner",
    "Niklas Penzel",
    "Orlando Guntinas-Lichius",
    "Joachim Denzler"
  ],
  "keywords": [
    "Facial Emotion Recognition",
    "Property Analysis",
    "Model Behavior",
    "Facial Asymmetry",
    "Medical Facial Analysis",
    "Facial Palsy"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Facial expression-based human emotion recognition is a critical research area in psychology and medicine. State-of-the-art classification performance is only reached by end-to-end trained neural networks. Nevertheless, such black-box models lack transparency in their decisionmaking processes, prompting efforts to ascertain the rules that underlie classifiers' decisions. Analyzing single inputs alone fails to expose systematic learned biases. These biases can be characterized as facial properties summarizing abstract information like age or medical conditions. Therefore, understanding a model's prediction behavior requires an analysis rooted in causality along such selected properties. We demonstrate that up to 91.25% of classifier output behavior changes are statistically significant concerning basic properties. Among those are age, gender, and facial symmetry. Furthermore, the medical usage of surface electromyography significantly influences emotion prediction. We introduce a workflow to evaluate explicit properties and their impact. These insights might help medical professionals select and apply classifiers regarding their specialized data and properties.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Human emotion recognition via facial expressions is essential in psychology and medical research. High classification performance is achieved by neural networks trained end-to-end and tailored for broad usage on large datasets. The application of these black box models to unseen data often leads to irregular behavior. Previous research, for example,  [22] , finds a loss in predictive performance when observing various so-called subpopulation shifts. While we follow a similar approach, the performance decline attributed to subpopulation shifts is insufficient. To uncover potential causes, we model these shifts as changes in what we call properties. Consequently, by investigating different property manifestations, we can uncover deviations in the model behavior. Hence, we go beyond simple predictive performance and uncover influential factors in emotion classification.\n\nSome of these potential factors (properties) such as age or gender, have a broad impact on the visual appearance of a face. Regarding emotion recognition, this is even more evident in the case of medical conditions influencing mimicry. Particularly in facial palsy cases, where there is unilateral paralysis of the facial nerve. The pronounced facial asymmetry may impact a model's prediction or muscle activation studies with joint surface electromyography (sEMG)  [2, 3, 9] .\n\nWe assess HSEmotion-7  [18]  and ResidualMaskNet  [13] , specifically their application on medically acquired facial data. Both models can predict the six basic emotions after Ekman  [7] , with an additional class for neutral expressions. We record 36 healthy probands to investigate the presence of sEMG electrodes and additionally their artificial removal  [2, 3] . Further, we capture 36 patients with facial palsy to evaluate the influence of facial asymmetry  [4] . This setup enables us to capture a multitude of relevant properties. Allowing us to extend a general performance analysis for subpopulation shifts  [22]  by utilizing additional explanations  [16] . These explanations are based on causal principles  [16] , permitting us to test for statistically significant deviations in model behavior.\n\nOur experimental results reveal two principal insights. First, both models exhibit a variation in predictive performance; Second, and more critically, there is a significant (p < 0.01) behavior change concerning the manifestations of varying properties. While for some, the shift in model behavior only significantly occurs for some emotions, other properties are important irrespective of the predicted class. Examples include visibly attached sEMG electrodes and whether a person suffers from unilateral facial palsy. We find statistically significant changes in the model behavior in up to 91.25% of the analyzed properties.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Facial Emotion Classification",
      "text": "Facial emotion classification is a broad research field with many possible applications, especially in psychology and medicine. State-of-the-art performance is currently attainable exclusively through end-to-end trained convolutional neural networks. Our study focuses on two such models: HSEmotion-7 (HSE-7)  [18]  and ResidualMaskNet (RMN)  [13] . Both models predict the six basic emotions defined by Ekman  [7] , with an added class for neutral expressions.\n\nA general reduction in prediction accuracy is anticipated  [22] . However, our interests are the subpopulation shifts resulting from different property manifestations. Therefore, we do not fine-tune to prevent distorting the interpretability of the general behavior analysis. Toward this goal, we require a custom evaluation dataset that captures properties typically unaccounted for in large datasets.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Emotion Evaluation Dataset",
      "text": "Our facial emotion images were captured using a standardized procedure where participants had to mimic the six basic emotions four times in a random se- quence. Therefore, we eliminate the risk of human-annotation bias, depending exclusively on the participants' capacity for facial mimicry. Using a frontal camera, we collected data from 36 healthy probands (18-67 years, 17 male, 19 female) four times with and twice without attached high-resolution surface electromyography (HR-sEMG)  [9] . We follow the work of  [2, 3]  to remove sEMG electrodes artificially. For each of these groups, we list the accuracy per model and emotion in Table  1 . Furthermore, 36 patients (25-72 years, 8 male, 28 female) with unilateral chronic synkinetic facial palsy, which is presumed to be a significant factor affecting classification, were recorded three times using the 3dMD face system (3dMD LLC, Georgia, USA). This system creates 3D scans of the patients. We simulate a frontal view by calculating the camera position based on facial landmarks  [4] , ensuring consistency among participants; see Table  1 . The first set of recordings only includes happy expressions. Combined, our dataset consists of 8,952 annotated images for evaluation.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Facial Properties: Selection And Manifestations",
      "text": "To evaluate the model's behavior regarding subpopulation shifts, we must select properties. The selection criteria are discussed herein, while Table  2  provides an overview comprising the manifestation types: scalar [S] or binary  [B] . The medical setup grants access to bodily properties typically unaccounted for in large datasets. These include age, weight, gender, and presence of facial palsy. Additionally, we utilize the experiment repetitions to evaluate model behavior across recordings for the same participant. Individual recording sessions are also scrutinized to detect any learned expressions. We indicate sEMG attachment and its subsequent artificial removal for the probands. We selected four computed metrics to assess facial symmetry, as facial palsy alone is insufficient for impact evaluation. We compute the lateral volume difference using the 3D patient scans  [4] . Further, two properties leverage facial landmark symmetry  [21] . LPISP calculates the image similarity between facial halves, with images aligned and center-cropped along the eye-line to reduce rotation artifacts  [2, 23] .\n\nIt should be noted that a property like age encompasses a complex mixture of features, including wrinkles, hair color, or age spots. This type of subdivision can be accomplished for nearly all chosen properties. While a more comprehensive analysis would give more insight, it is not feasible within the scope of this study.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Methodology",
      "text": "Interested in emotion recognition model behavior relative to subpopulation shifts, we base the analysis on selected properties and related manifestations. Following the approach of  [22] , we assess accuracy change with respect to these shifts, see Table  1 . We build our main analysis upon a causally-grounded method detailed in  [16]  to detect statistically significant behavioral shifts in the model in response to property manifestation. In  [16] , the authors develop a structural causal model (SCM) encompassing supervised learning building on the causality framework of Pearl  [10] . Using this SCM together with Reichenbach's common cause principle  [14] , the question of whether a trained classifier uses a property becomes a statistical conditional independence (CI) test  [16] . The selection of suitable CI tests is a vital hyperparameter choice. However, in  [19] , the authors prove that there cannot be a non-parametric CI test that controls for Type-I errors (false positives) in all cases. Following the analysis in  [11, 12, 15] , we form a committee of nonlinear tests to assess a model's feature usage  [12] , specifically conditional HSIC  [8] , RCoT  [20] , and CMIknn  [17] . We report the consensus results with a significance level of p < 0.01 in Table  3 . angry ResMaskNet  [13]  angry Since both models output logits use softmax  [13, 18] , observable changes in one logit may be counterbalanced by opposing logit groups. Hence, we verify each emotion to address this, as displayed in Table  3 . This approach enables more profound insight into property utilization beyond simple summary statistics.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Results",
      "text": "We investigate subpopulation shifts resulting from different property manifestations by applying black-box neural networks. Specifically, we focus on Residual-MaskNet (RMN)  [13]  and HSEmotion-7 (HSE-7)  [18]  for facial emotion recognition. Each image of our dataset has all properties, as referenced in Table  2 . Using these images, we capture the logit activations of the pre-trained RMN and HSE-7. Hence, using the reference annotations, we test for significant changes in behavior  [16] . Statistically significant results (p < 0.01) are denoted per model and emotion in Table  3 . Of the 84 possible combinations, RMN utilizes information in 91.25% ( 73 /84) and for HSE-7 in 87.50% ( 70 /84) of cases. The findings show that the models use properties in their decision-making processes. We now examine the models' behavior concerning several properties in detail.\n\nFirst, we focus on self-declared gender. Other work  [6]  demonstrated that gender (B G ) affects emotion classification, a claim we corroborate for 11 of 14 cases as seen in Table  3 . The visualization, see Fig.  1a , displays the logit activation distribution regarding men and women. We choose disgusted for HSE-7 due to its significance and difficulty in execution  [2, 5, 9] ; see Table  3   property S L , we plot a regression analysis by estimating Gaussians with a sliding window  [11] . For all visualizations, we only use images of the corresponding emotion  [16] .\n\ninvestigations focus on happy expressions due to the observed prediction stability for probands and patients; see Table  3 . The displayed distributions indicate HSE-7 struggles to classify disgusted expressions for men compared to women. Moreover, the RMN more frequently attributes happy expressions to women. Secondly, we investigate the attached (R E ) and artificially removed (R R ) sEMG electrodes, as they are influential for both models and all seven emotions; see Table  3 . As argued above, we visualize the happy logit distribution concerning R E and R R in Fig.  1b . The overall lower activations are anticipated for the attached case  [2, 3 ]. Yet we see in Table  1  that the quality of the removal highly depends on the model and emotion when recovering predicted performance. For instance, regarding happy expressions, the RMN recovers a higher predictive performance and logit distribution (see Fig.  1b ). We assume that the recovery introduced artifacts or the sEMG influences the participants' facial mimicry.\n\nLastly, we assess whether facial palsy-induced asymmetry (B F ) or symmetry in general (S) impacts model behavior. In Table  3 , we observe in 77.14% ( 54 /70) of combinations a significant influence of facial symmetry. Especially, facial palsy (B F ) and different LPIPS (S L ) manifestations influence the behavior of both models and all seven emotions. We follow  [11]  and regress the property manifestation trend by estimating Gaussians with a sliding window approach for LPIPS. Again, focusing on happy expressions, Fig.  1d  show that higher facial symmetry leads to higher logit activation on average. However, facial palsy specifically, see Fig.  1c , shows increased uncertainty. The indicated quartiles suggest a strong bimodal distribution with many high and low activations. This suggests that the models are influenced by facial symmetry. Hence, their application for unilateral facial palsy patients should be approached with caution.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Conclusion",
      "text": "This work studied emotion classifiers applied in a medical context. To go beyond performance metrics, we used the causal-based framework from  [16] . We demonstrated that up to 91.25% of classifier output behavior changes are statistically significant concerning varying properties, including age, gender, and facial symmetry. To obtain such properties unaccounted in other datasets, we recorded 36 probands and 36 patients with facial palsy, a disease affecting facial expressions.\n\nTo summarize, we observe differences in model behavior regarding gender and facial symmetry. Hence, their application on medical conditions should be approached with care. Additionally, the obstruction of facial features during medical studies can significantly impact the model behavior, as we observe for attached sEMG electrodes. However, our observations do not necessarily indicate harmful biases but open the discussion beyond simple predictive performance.\n\nFinally, our selected properties are not exhaustive and do not cover all possible biases. Many other properties related to different downstream tasks could be conceived and studied in the future. Additionally, different models and property aware training are promising  [1, 6] . We hope to prompt more extensive analysis and inspire researchers to analyze facial recognition models beyond prediction performance.",
      "page_start": 7,
      "page_end": 7
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: a, displays the logit activa-",
      "page": 5
    },
    {
      "caption": "Figure 1: Model behavior visualization: In the binary [B] case, we use violin plots for",
      "page": 6
    },
    {
      "caption": "Figure 1: b. The overall lower activations are anticipated for the",
      "page": 6
    },
    {
      "caption": "Figure 1: b). We assume that the recovery",
      "page": 6
    },
    {
      "caption": "Figure 1: d show that higher facial symmetry",
      "page": 6
    },
    {
      "caption": "Figure 1: c, shows increased uncertainty. The indicated quartiles suggest a strong bi-",
      "page": 6
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "2 Dept. of Otorhinolaryngology, Jena University Hospital, 07747 Jena, Germany": "orlando.guntinas@uni-jena.de"
        },
        {
          "2 Dept. of Otorhinolaryngology, Jena University Hospital, 07747 Jena, Germany": "Abstract. Facial expression-based human emotion recognition is a crit-"
        },
        {
          "2 Dept. of Otorhinolaryngology, Jena University Hospital, 07747 Jena, Germany": "ical research area in psychology and medicine. State-of-the-art classifica-"
        },
        {
          "2 Dept. of Otorhinolaryngology, Jena University Hospital, 07747 Jena, Germany": "tion performance is only reached by end-to-end trained neural networks."
        },
        {
          "2 Dept. of Otorhinolaryngology, Jena University Hospital, 07747 Jena, Germany": "Nevertheless, such black-box models lack transparency in their decision-"
        },
        {
          "2 Dept. of Otorhinolaryngology, Jena University Hospital, 07747 Jena, Germany": "making processes, prompting efforts to ascertain the rules that underlie"
        },
        {
          "2 Dept. of Otorhinolaryngology, Jena University Hospital, 07747 Jena, Germany": "classifiers’ decisions. Analyzing single inputs alone fails to expose system-"
        },
        {
          "2 Dept. of Otorhinolaryngology, Jena University Hospital, 07747 Jena, Germany": "atic learned biases. These biases can be characterized as facial properties"
        },
        {
          "2 Dept. of Otorhinolaryngology, Jena University Hospital, 07747 Jena, Germany": "summarizing abstract information like age or medical conditions. There-"
        },
        {
          "2 Dept. of Otorhinolaryngology, Jena University Hospital, 07747 Jena, Germany": "fore, understanding a model’s prediction behavior\nrequires an analysis"
        },
        {
          "2 Dept. of Otorhinolaryngology, Jena University Hospital, 07747 Jena, Germany": "rooted in causality along such selected properties. We demonstrate that"
        },
        {
          "2 Dept. of Otorhinolaryngology, Jena University Hospital, 07747 Jena, Germany": "up to 91.25% of classifier output behavior changes are statistically signifi-"
        },
        {
          "2 Dept. of Otorhinolaryngology, Jena University Hospital, 07747 Jena, Germany": "cant concerning basic properties. Among those are age, gender, and facial"
        },
        {
          "2 Dept. of Otorhinolaryngology, Jena University Hospital, 07747 Jena, Germany": "symmetry. Furthermore, the medical usage of surface electromyography"
        },
        {
          "2 Dept. of Otorhinolaryngology, Jena University Hospital, 07747 Jena, Germany": "significantly influences emotion prediction. We introduce a workflow to"
        },
        {
          "2 Dept. of Otorhinolaryngology, Jena University Hospital, 07747 Jena, Germany": "evaluate explicit properties and their impact. These insights might help"
        },
        {
          "2 Dept. of Otorhinolaryngology, Jena University Hospital, 07747 Jena, Germany": "medical professionals select and apply classifiers regarding their special-"
        },
        {
          "2 Dept. of Otorhinolaryngology, Jena University Hospital, 07747 Jena, Germany": "ized data and properties."
        },
        {
          "2 Dept. of Otorhinolaryngology, Jena University Hospital, 07747 Jena, Germany": "· Model\nKeywords: Facial Emotion Recognition · Property Analysis"
        },
        {
          "2 Dept. of Otorhinolaryngology, Jena University Hospital, 07747 Jena, Germany": "Behavior · Facial Asymmetry · Medical Facial Analysis · Facial Palsy"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "1\nIntroduction": "Human emotion recognition via facial expressions is essential\nin psychology and"
        },
        {
          "1\nIntroduction": "medical research. High classification performance is achieved by neural networks"
        },
        {
          "1\nIntroduction": "trained end-to-end and tailored for broad usage on large datasets. The applica-"
        },
        {
          "1\nIntroduction": "tion of these black box models to unseen data often leads to irregular behavior."
        },
        {
          "1\nIntroduction": "Previous research, for example,\n[22], finds a loss in predictive performance when"
        },
        {
          "1\nIntroduction": "observing various so-called subpopulation shifts. While we follow a similar ap-"
        },
        {
          "1\nIntroduction": "proach, the performance decline attributed to subpopulation shifts is insufficient."
        },
        {
          "1\nIntroduction": "To uncover potential causes, we model\nthese shifts as changes in what we call"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "2\nBüchner, T. et al.": "properties. Consequently, by investigating different property manifestations, we"
        },
        {
          "2\nBüchner, T. et al.": "can uncover deviations in the model behavior. Hence, we go beyond simple pre-"
        },
        {
          "2\nBüchner, T. et al.": "dictive performance and uncover influential\nfactors in emotion classification."
        },
        {
          "2\nBüchner, T. et al.": "Some of\nthese potential\nfactors\n(properties)\nsuch as age or gender, have a"
        },
        {
          "2\nBüchner, T. et al.": "broad impact on the visual appearance of a face. Regarding emotion recognition,"
        },
        {
          "2\nBüchner, T. et al.": "this is even more evident in the case of medical conditions influencing mimicry."
        },
        {
          "2\nBüchner, T. et al.": "Particularly in facial palsy cases, where there is unilateral paralysis of the facial"
        },
        {
          "2\nBüchner, T. et al.": "nerve. The pronounced facial asymmetry may impact a model’s prediction or"
        },
        {
          "2\nBüchner, T. et al.": "muscle activation studies with joint surface electromyography (sEMG) [2, 3, 9]."
        },
        {
          "2\nBüchner, T. et al.": "We assess HSEmotion-7 [18] and ResidualMaskNet [13], specifically their ap-"
        },
        {
          "2\nBüchner, T. et al.": "plication on medically acquired facial data. Both models can predict the six basic"
        },
        {
          "2\nBüchner, T. et al.": "emotions after Ekman [7], with an additional class for neutral expressions. We"
        },
        {
          "2\nBüchner, T. et al.": "record 36 healthy probands to investigate the presence of sEMG electrodes and"
        },
        {
          "2\nBüchner, T. et al.": "additionally their artificial removal\n[2, 3]. Further, we capture 36 patients with"
        },
        {
          "2\nBüchner, T. et al.": "facial palsy to evaluate the influence of\nfacial asymmetry [4]. This setup enables"
        },
        {
          "2\nBüchner, T. et al.": "us to capture a multitude of relevant properties. Allowing us to extend a general"
        },
        {
          "2\nBüchner, T. et al.": "performance analysis for subpopulation shifts [22] by utilizing additional expla-"
        },
        {
          "2\nBüchner, T. et al.": "nations [16]. These explanations are based on causal principles [16], permitting"
        },
        {
          "2\nBüchner, T. et al.": "us to test for statistically significant deviations in model behavior."
        },
        {
          "2\nBüchner, T. et al.": "Our\nexperimental\nresults\nreveal\ntwo principal\ninsights. First, both models"
        },
        {
          "2\nBüchner, T. et al.": "exhibit a variation in predictive performance; Second, and more critically, there is"
        },
        {
          "2\nBüchner, T. et al.": "a significant (p < 0.01) behavior change concerning the manifestations of varying"
        },
        {
          "2\nBüchner, T. et al.": "properties. While for some, the shift in model behavior only significantly occurs"
        },
        {
          "2\nBüchner, T. et al.": "for some emotions, other properties are important irrespective of the predicted"
        },
        {
          "2\nBüchner, T. et al.": "class. Examples include visibly attached sEMG electrodes and whether a person"
        },
        {
          "2\nBüchner, T. et al.": "suffers\nfrom unilateral\nfacial palsy. We find statistically significant changes\nin"
        },
        {
          "2\nBüchner, T. et al.": "the model behavior in up to 91.25% of the analyzed properties."
        }
      ],
      "page": 2
    },
    {
      "caption": "Table 1: Furthermore, 36 patients (25-72 years, 8 male, 28 female) with",
      "data": [
        {
          "out fine-tuning) for probands and patients. A reference image is shown for each": "set. The patients are recorded only for the happy expression without sEMG."
        },
        {
          "out fine-tuning) for probands and patients. A reference image is shown for each": ""
        },
        {
          "out fine-tuning) for probands and patients. A reference image is shown for each": "sEMG"
        },
        {
          "out fine-tuning) for probands and patients. A reference image is shown for each": "Emotion"
        },
        {
          "out fine-tuning) for probands and patients. A reference image is shown for each": "angry"
        },
        {
          "out fine-tuning) for probands and patients. A reference image is shown for each": "disgusted"
        },
        {
          "out fine-tuning) for probands and patients. A reference image is shown for each": "fearful"
        },
        {
          "out fine-tuning) for probands and patients. A reference image is shown for each": "happy"
        },
        {
          "out fine-tuning) for probands and patients. A reference image is shown for each": "sad"
        },
        {
          "out fine-tuning) for probands and patients. A reference image is shown for each": "surprised"
        },
        {
          "out fine-tuning) for probands and patients. A reference image is shown for each": "Mean Acc."
        }
      ],
      "page": 3
    },
    {
      "caption": "Table 2: We list our selected properties used to examine model behavior. Their",
      "data": [
        {
          "Table 2: We list our selected properties used to examine model behavior. Their": "manifestations are scalars [S] or a binary [B] value."
        },
        {
          "Table 2: We list our selected properties used to examine model behavior. Their": ""
        },
        {
          "Table 2: We list our selected properties used to examine model behavior. Their": "Age [S]"
        },
        {
          "Table 2: We list our selected properties used to examine model behavior. Their": ""
        },
        {
          "Table 2: We list our selected properties used to examine model behavior. Their": "Weight [S]"
        },
        {
          "Table 2: We list our selected properties used to examine model behavior. Their": ""
        },
        {
          "Table 2: We list our selected properties used to examine model behavior. Their": "Gender [B]"
        },
        {
          "Table 2: We list our selected properties used to examine model behavior. Their": ""
        },
        {
          "Table 2: We list our selected properties used to examine model behavior. Their": "Facial Palsy [B]"
        },
        {
          "Table 2: We list our selected properties used to examine model behavior. Their": ""
        },
        {
          "Table 2: We list our selected properties used to examine model behavior. Their": "Participant ID [S]"
        },
        {
          "Table 2: We list our selected properties used to examine model behavior. Their": ""
        },
        {
          "Table 2: We list our selected properties used to examine model behavior. Their": "Session ID [S]"
        },
        {
          "Table 2: We list our selected properties used to examine model behavior. Their": ""
        },
        {
          "Table 2: We list our selected properties used to examine model behavior. Their": "Attached sEMG [B]"
        },
        {
          "Table 2: We list our selected properties used to examine model behavior. Their": ""
        },
        {
          "Table 2: We list our selected properties used to examine model behavior. Their": "Removed sEMG [B]"
        },
        {
          "Table 2: We list our selected properties used to examine model behavior. Their": ""
        },
        {
          "Table 2: We list our selected properties used to examine model behavior. Their": "Facial Volume [S]"
        },
        {
          "Table 2: We list our selected properties used to examine model behavior. Their": ""
        },
        {
          "Table 2: We list our selected properties used to examine model behavior. Their": "eye-level dev.\n[S]"
        },
        {
          "Table 2: We list our selected properties used to examine model behavior. Their": ""
        },
        {
          "Table 2: We list our selected properties used to examine model behavior. Their": "midline dev.\n[S]"
        },
        {
          "Table 2: We list our selected properties used to examine model behavior. Their": ""
        },
        {
          "Table 2: We list our selected properties used to examine model behavior. Their": "LPIPS [S]"
        },
        {
          "Table 2: We list our selected properties used to examine model behavior. Their": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 3: This approach enables more",
      "data": [
        {
          "Table 3: Significant changes (p < 0.01) per model and emotion are denoted with": ""
        },
        {
          "Table 3: Significant changes (p < 0.01) per model and emotion are denoted with": ""
        },
        {
          "Table 3: Significant changes (p < 0.01) per model and emotion are denoted with": ""
        },
        {
          "Table 3: Significant changes (p < 0.01) per model and emotion are denoted with": "angry"
        },
        {
          "Table 3: Significant changes (p < 0.01) per model and emotion are denoted with": "disgusted"
        },
        {
          "Table 3: Significant changes (p < 0.01) per model and emotion are denoted with": "fearful"
        },
        {
          "Table 3: Significant changes (p < 0.01) per model and emotion are denoted with": "happy"
        },
        {
          "Table 3: Significant changes (p < 0.01) per model and emotion are denoted with": "sad"
        },
        {
          "Table 3: Significant changes (p < 0.01) per model and emotion are denoted with": "surprised"
        },
        {
          "Table 3: Significant changes (p < 0.01) per model and emotion are denoted with": "neutral"
        },
        {
          "Table 3: Significant changes (p < 0.01) per model and emotion are denoted with": ""
        },
        {
          "Table 3: Significant changes (p < 0.01) per model and emotion are denoted with": "angry"
        },
        {
          "Table 3: Significant changes (p < 0.01) per model and emotion are denoted with": "disgusted"
        },
        {
          "Table 3: Significant changes (p < 0.01) per model and emotion are denoted with": "fearful"
        },
        {
          "Table 3: Significant changes (p < 0.01) per model and emotion are denoted with": "happy"
        },
        {
          "Table 3: Significant changes (p < 0.01) per model and emotion are denoted with": "sad"
        },
        {
          "Table 3: Significant changes (p < 0.01) per model and emotion are denoted with": "surprised"
        },
        {
          "Table 3: Significant changes (p < 0.01) per model and emotion are denoted with": "neutral"
        },
        {
          "Table 3: Significant changes (p < 0.01) per model and emotion are denoted with": ""
        }
      ],
      "page": 5
    },
    {
      "caption": "Table 3: The displayed distributions indicate",
      "data": [
        {
          "Büchner, T. et al.": "Disgusted\nHappy"
        },
        {
          "Büchner, T. et al.": ""
        },
        {
          "Büchner, T. et al.": ""
        },
        {
          "Büchner, T. et al.": ""
        },
        {
          "Büchner, T. et al.": ""
        },
        {
          "Büchner, T. et al.": "♂ ♀\n♂ ♀"
        },
        {
          "Büchner, T. et al.": ""
        },
        {
          "Büchner, T. et al.": ""
        },
        {
          "Büchner, T. et al.": "HSE-7\nRMN"
        },
        {
          "Büchner, T. et al.": "(a) Gender (BG)"
        },
        {
          "Büchner, T. et al.": ""
        },
        {
          "Büchner, T. et al.": ""
        },
        {
          "Büchner, T. et al.": ""
        },
        {
          "Büchner, T. et al.": ""
        },
        {
          "Büchner, T. et al.": "absent"
        },
        {
          "Büchner, T. et al.": "present"
        },
        {
          "Büchner, T. et al.": ""
        },
        {
          "Büchner, T. et al.": ""
        },
        {
          "Büchner, T. et al.": ""
        },
        {
          "Büchner, T. et al.": ""
        },
        {
          "Büchner, T. et al.": ""
        },
        {
          "Büchner, T. et al.": ""
        },
        {
          "Büchner, T. et al.": "HSE-7\nRMN"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table 3: The displayed distributions indicate",
      "data": [
        {
          "0.00\n0.00": "0.0"
        },
        {
          "0.00\n0.00": "0.1\n0.2\n0.3\n0.4\n0.1\n0.2\n0.3\n0.4\nHSE-7\nRMN"
        },
        {
          "0.00\n0.00": "(c) Facial palsy (BF )\n(d) LPIPS (SL) symmetry value for lateral"
        },
        {
          "0.00\n0.00": "Fig. 1: Model behavior visualization: In the binary [B] case, we use violin plots for"
        },
        {
          "0.00\n0.00": "logit distributions (a, b, c). For the scalar [S] property SL, we plot a regression"
        },
        {
          "0.00\n0.00": "analysis by estimating Gaussians with a sliding window [11]. For all visualiza-"
        },
        {
          "0.00\n0.00": "tions, we only use images of the corresponding emotion [16]."
        },
        {
          "0.00\n0.00": "investigations focus on happy expressions due to the observed prediction stabil-"
        },
        {
          "0.00\n0.00": "ity for probands and patients; see Table 3. The displayed distributions indicate"
        },
        {
          "0.00\n0.00": "HSE-7 struggles to classify disgusted expressions for men compared to women."
        },
        {
          "0.00\n0.00": "Moreover, the RMN more frequently attributes happy expressions to women."
        },
        {
          "0.00\n0.00": "Secondly, we\ninvestigate\nthe attached (RE) and artificially removed (RR)"
        },
        {
          "0.00\n0.00": "sEMG electrodes, as they are influential for both models and all seven emotions;"
        },
        {
          "0.00\n0.00": "see Table 3. As argued above, we visualize the happy logit distribution concerning"
        },
        {
          "0.00\n0.00": "lower activations are anticipated for\nthe\nRE and RR in Fig. 1b. The overall"
        },
        {
          "0.00\n0.00": "attached case [2, 3]. Yet we see in Table 1 that the quality of the removal highly"
        },
        {
          "0.00\n0.00": "depends on the model and emotion when recovering predicted performance. For"
        },
        {
          "0.00\n0.00": "instance,\nregarding happy\nexpressions,\nthe RMN recovers a higher predictive"
        },
        {
          "0.00\n0.00": "performance and logit distribution (see Fig. 1b). We assume that the recovery"
        },
        {
          "0.00\n0.00": "introduced artifacts or the sEMG influences the participants’\nfacial mimicry."
        },
        {
          "0.00\n0.00": "Lastly, we assess whether facial palsy-induced asymmetry (BF ) or symmetry"
        },
        {
          "0.00\n0.00": "in general (S) impacts model behavior. In Table 3, we observe in 77.14% (54/70)"
        },
        {
          "0.00\n0.00": "of combinations a significant influence of facial symmetry. Especially, facial palsy"
        },
        {
          "0.00\n0.00": "(BF ) and different LPIPS (SL) manifestations influence the behavior of both"
        },
        {
          "0.00\n0.00": "models and all seven emotions. We follow [11] and regress the property manifes-"
        },
        {
          "0.00\n0.00": "tation trend by estimating Gaussians with a sliding window approach for LPIPS."
        },
        {
          "0.00\n0.00": "Again, focusing on happy expressions, Fig. 1d show that higher facial symmetry"
        },
        {
          "0.00\n0.00": "leads to higher logit activation on average. However, facial palsy specifically, see"
        },
        {
          "0.00\n0.00": "Fig. 1c, shows increased uncertainty. The indicated quartiles suggest a strong bi-"
        },
        {
          "0.00\n0.00": "modal distribution with many high and low activations. This suggests that the"
        },
        {
          "0.00\n0.00": "models are influenced by facial symmetry. Hence, their application for unilateral"
        },
        {
          "0.00\n0.00": "facial palsy patients should be approached with caution."
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Influential Factors in Emotion Classification\n7": "5\nConclusion"
        },
        {
          "Influential Factors in Emotion Classification\n7": "This work studied emotion classifiers applied in a medical context. To go beyond"
        },
        {
          "Influential Factors in Emotion Classification\n7": "performance metrics, we used the causal-based framework from [16]. We demon-"
        },
        {
          "Influential Factors in Emotion Classification\n7": "strated that up to 91.25% of classifier output behavior changes are statistically"
        },
        {
          "Influential Factors in Emotion Classification\n7": "significant concerning varying properties,\nincluding age, gender, and facial sym-"
        },
        {
          "Influential Factors in Emotion Classification\n7": "metry. To obtain such properties unaccounted in other datasets, we recorded 36"
        },
        {
          "Influential Factors in Emotion Classification\n7": "probands and 36 patients with facial palsy, a disease affecting facial expressions."
        },
        {
          "Influential Factors in Emotion Classification\n7": "To summarize, we observe differences\nin model behavior\nregarding gender"
        },
        {
          "Influential Factors in Emotion Classification\n7": "and facial symmetry. Hence, their application on medical conditions should be"
        },
        {
          "Influential Factors in Emotion Classification\n7": "approached with care. Additionally,\nthe obstruction of\nfacial\nfeatures during"
        },
        {
          "Influential Factors in Emotion Classification\n7": "medical studies can significantly impact the model behavior, as we observe for"
        },
        {
          "Influential Factors in Emotion Classification\n7": "attached sEMG electrodes. However, our observations do not necessarily indicate"
        },
        {
          "Influential Factors in Emotion Classification\n7": "harmful biases but open the discussion beyond simple predictive performance."
        },
        {
          "Influential Factors in Emotion Classification\n7": "Finally, our selected properties are not exhaustive and do not cover all possi-"
        },
        {
          "Influential Factors in Emotion Classification\n7": "ble biases. Many other properties related to different downstream tasks could be"
        },
        {
          "Influential Factors in Emotion Classification\n7": "conceived and studied in the future. Additionally, different models and property"
        },
        {
          "Influential Factors in Emotion Classification\n7": "aware training are promising [1, 6]. We hope to prompt more extensive analysis"
        },
        {
          "Influential Factors in Emotion Classification\n7": "and inspire researchers\nto analyze facial\nrecognition models beyond prediction"
        },
        {
          "Influential Factors in Emotion Classification\n7": "performance."
        },
        {
          "Influential Factors in Emotion Classification\n7": "Acknowledgments. Partially supported by Deutsche Forschungsgemeinschaft (DFG"
        },
        {
          "Influential Factors in Emotion Classification\n7": "- German Research Foundation) project 427899908 BRIDGING THE GAP: MIMICS"
        },
        {
          "Influential Factors in Emotion Classification\n7": "AND MUSCLES (DE 735/15-1 and GU 463/12-1)."
        },
        {
          "Influential Factors in Emotion Classification\n7": "Disclosure of Interests. The authors have no competing interests to declare."
        },
        {
          "Influential Factors in Emotion Classification\n7": "References"
        },
        {
          "Influential Factors in Emotion Classification\n7": "1. Blunk, J., Penzel, N., Bodesheim, P., Denzler, J.: Beyond debiasing: Actively steer-"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Disclosure of Interests. The authors have no competing interests to declare.": "References"
        },
        {
          "Disclosure of Interests. The authors have no competing interests to declare.": "1. Blunk, J., Penzel, N., Bodesheim, P., Denzler, J.: Beyond debiasing: Actively steer-"
        },
        {
          "Disclosure of Interests. The authors have no competing interests to declare.": "ing feature\nselection via loss\nregularization.\nIn: DAGM German Conference on"
        },
        {
          "Disclosure of Interests. The authors have no competing interests to declare.": "Pattern Recognition (DAGM-GCPR) (2023)"
        },
        {
          "Disclosure of Interests. The authors have no competing interests to declare.": "2. Büchner, T., Guntinas-Lichius, O., Denzler, J.:\nImproved obstructed facial\nfea-"
        },
        {
          "Disclosure of Interests. The authors have no competing interests to declare.": "ture\nreconstruction for\nemotion recognition with minimal\nchange\ncyclegans.\nIn:"
        },
        {
          "Disclosure of Interests. The authors have no competing interests to declare.": "Advanced Concepts for Intelligent Vision Systems (Acivs). pp. 262–274. Springer-"
        },
        {
          "Disclosure of Interests. The authors have no competing interests to declare.": "Nature (august 2023). https://doi.org/10.1007/978-3-031-45382-3_22"
        },
        {
          "Disclosure of Interests. The authors have no competing interests to declare.": "3. Büchner, T., Sickert, S., Volk, G.F., Anders, C., Guntinas-Lichius, O., Denzler,"
        },
        {
          "Disclosure of Interests. The authors have no competing interests to declare.": "J.: Let’s get\nthe facs\nstraight\n-\nreconstructing obstructed facial\nfeatures.\nIn:\nIn-"
        },
        {
          "Disclosure of Interests. The authors have no competing interests to declare.": "ternational Conference on Computer Vision Theory and Applications (VISAPP)."
        },
        {
          "Disclosure of Interests. The authors have no competing interests to declare.": "SciTePress (march 2023). https://doi.org/10.5220/0011619900003417"
        },
        {
          "Disclosure of Interests. The authors have no competing interests to declare.": "4. Büchner, T., Sickert, S., Volk, G.F., Guntinas-Lichius, O., Denzler, J.: From Faces"
        },
        {
          "Disclosure of Interests. The authors have no competing interests to declare.": "to Volumes - Measuring Volumetric Asymmetry in 3D Facial Palsy Scans. In: Ad-"
        },
        {
          "Disclosure of Interests. The authors have no competing interests to declare.": "vances in Visual Computing. Lecture Notes in Computer Science, Springer Nature"
        },
        {
          "Disclosure of Interests. The authors have no competing interests to declare.": "Switzerland (2023). https://doi.org/10.1007/978-3-031-47969-4_10"
        },
        {
          "Disclosure of Interests. The authors have no competing interests to declare.": "5. Büchner, T., Sickert, S., Graßme, R., Anders, C., Guntinas-Lichius, O., Denzler,"
        },
        {
          "Disclosure of Interests. The authors have no competing interests to declare.": "J.: Using 2d and 3d face\nrepresentations\nto generate\ncomprehensive\nfacial\nelec-"
        },
        {
          "Disclosure of Interests. The authors have no competing interests to declare.": "tromyography intensity maps.\nIn:\nInternational Symposium on Visual Comput-"
        },
        {
          "Disclosure of Interests. The authors have no competing interests to declare.": "ing (ISVC). pp. 136–147 (2023). https://doi.org/10.1007/978-3-031-47966-3_11,"
        },
        {
          "Disclosure of Interests. The authors have no competing interests to declare.": "https://link.springer.com/chapter/10.1007/978-3-031-47966-3_11"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "8\nBüchner, T. et al.": "6. Chen, Y., Joo, J.: Understanding and Mitigating Annotation Bias\nin Facial Ex-"
        },
        {
          "8\nBüchner, T. et al.": "pression Recognition.\nIn:\n2021\nIEEE/CVF International Conference\non Com-"
        },
        {
          "8\nBüchner, T. et al.": "puter Vision (ICCV). pp. 14960–14971. IEEE, Montreal, QC, Canada (Oct 2021)."
        },
        {
          "8\nBüchner, T. et al.": "https://doi.org/10.1109/ICCV48922.2021.01471"
        },
        {
          "8\nBüchner, T. et al.": "7. Ekman, P.: An argument for basic emotions. Cognition and Emotion 6(3-4), 169–"
        },
        {
          "8\nBüchner, T. et al.": "200 (1992). https://doi.org/10.1080/02699939208411068"
        },
        {
          "8\nBüchner, T. et al.": "8. Fukumizu, K., Gretton, A., Sun, X., Schölkopf, B.: Kernel measures of conditional"
        },
        {
          "8\nBüchner, T. et al.": "dependence. Advances in neural\ninformation processing systems 20 (2007)"
        },
        {
          "8\nBüchner, T. et al.": "9. Guntinas-Lichius, O., Trentzsch, V., Mueller, N., Heinrich, M., Kuttenreich, A.M.,"
        },
        {
          "8\nBüchner, T. et al.": "Dobel, C., et al.: High-resolution surface electromyographic activities of facial mus-"
        },
        {
          "8\nBüchner, T. et al.": "cles during the\nsix basic\nemotional\nexpressions\nin healthy adults: a prospective"
        },
        {
          "8\nBüchner, T. et al.": "observational study. Scientific Reports 13(1), 19214 (2023)"
        },
        {
          "8\nBüchner, T. et al.": "10. Pearl, J.: Causality. Cambridge university press (2009)"
        },
        {
          "8\nBüchner, T. et al.": "11. Penzel, N., Kierdorf,\nJ., Roscher, R., Denzler,\nJ.: Analyzing\nthe\nbehavior\nof"
        },
        {
          "8\nBüchner, T. et al.": "cauliflower harvest-readiness models by investigating feature relevances.\nIn: 2023"
        },
        {
          "8\nBüchner, T. et al.": "IEEE/CVF International Conference on Computer Vision Workshops (ICCVW)."
        },
        {
          "8\nBüchner, T. et al.": "pp. 572–581. IEEE (2023)"
        },
        {
          "8\nBüchner, T. et al.": "12. Penzel, N., Reimers, C., Bodesheim, P., Denzler, J.: Investigating neural network"
        },
        {
          "8\nBüchner, T. et al.": "training on a feature level using conditional independence. In: European Conference"
        },
        {
          "8\nBüchner, T. et al.": "on Computer Vision. pp. 383–399. Springer (2022)"
        },
        {
          "8\nBüchner, T. et al.": "13. Pham, L., Vu, T.H., Tran, T.A.: Facial\nexpression\nrecognition\nusing\nresidual"
        },
        {
          "8\nBüchner, T. et al.": "masking network. In: 2020 25th International Conference on Pattern Recognition"
        },
        {
          "8\nBüchner, T. et al.": "(ICPR). pp. 4513–4519 (2021). https://doi.org/10.1109/ICPR48806.2021.9411919"
        },
        {
          "8\nBüchner, T. et al.": "14. Reichenbach, H.: The direction of time, vol. 65. Univ of California Press (1956)"
        },
        {
          "8\nBüchner, T. et al.": "15. Reimers, C., Penzel, N., Bodesheim, P., Runge, J., Denzler, J.: Conditional depen-"
        },
        {
          "8\nBüchner, T. et al.": "dence tests reveal the usage of abcd rule features and bias variables in automatic"
        },
        {
          "8\nBüchner, T. et al.": "skin lesion classification.\nIn: Proceedings of the IEEE/CVF Conference on Com-"
        },
        {
          "8\nBüchner, T. et al.": "puter Vision and Pattern Recognition. pp. 1810–1819 (2021)"
        },
        {
          "8\nBüchner, T. et al.": "16. Reimers, C., Runge, J., Denzler, J.: Determining the relevance of features for deep"
        },
        {
          "8\nBüchner, T. et al.": "neural networks. In: European Conference on Computer Vision. Springer (2020)"
        },
        {
          "8\nBüchner, T. et al.": "17. Runge, J.: Conditional\nindependence testing based on a nearest-neighbor estima-"
        },
        {
          "8\nBüchner, T. et al.": "tor of conditional mutual\ninformation.\nIn:\nInternational Conference on Artificial"
        },
        {
          "8\nBüchner, T. et al.": "Intelligence and Statistics. PMLR (2018)"
        },
        {
          "8\nBüchner, T. et al.": "18. Savchenko, A.: Facial expression recognition with adaptive frame rate based on"
        },
        {
          "8\nBüchner, T. et al.": "multiple\ntesting\ncorrection.\nIn:\nInternational Conference\non Machine Learning."
        },
        {
          "8\nBüchner, T. et al.": "vol. 202. PMLR (2023), https://proceedings.mlr.press/v202/savchenko23a.html"
        },
        {
          "8\nBüchner, T. et al.": "19. Shah, R.D., Peters, J.: The hardness of conditional\nindependence testing and the"
        },
        {
          "8\nBüchner, T. et al.": "generalised covariance measure. The Annals of Statistics 48(3), 1514–1538 (2020)"
        },
        {
          "8\nBüchner, T. et al.": "20. Strobl, E.V., Zhang, K., Visweswaran, S.: Approximate kernel-based conditional"
        },
        {
          "8\nBüchner, T. et al.": "independence\ntests\nfor\nfast non-parametric\ncausal discovery. Journal of Causal"
        },
        {
          "8\nBüchner, T. et al.": "Inference (2019)"
        },
        {
          "8\nBüchner, T. et al.": "21. Wei, W., Ho, E.S.L., McCay, K.D., Damaševičius, R., Maskeli¯unas, R., Espos-"
        },
        {
          "8\nBüchner, T. et al.": "ito, A.: Assessing Facial\nSymmetry\nand Attractiveness\nusing Augmented Re-"
        },
        {
          "8\nBüchner, T. et al.": "(2022). https://doi.org/10.1007/\nality. Pattern Analysis and Applications 25(3)"
        },
        {
          "8\nBüchner, T. et al.": "s10044-021-00975-z"
        },
        {
          "8\nBüchner, T. et al.": "22. Yang, Y., Zhang, H., Katabi, D., Ghassemi, M.: Change is hard: A closer look at"
        },
        {
          "8\nBüchner, T. et al.": "subpopulation shift. arXiv preprint arXiv:2302.12254 (2023)"
        },
        {
          "8\nBüchner, T. et al.": "23. Zhang, R.,\nIsola, P., Efros, A.A., Shechtman, E., Wang, O.: The Unreasonable"
        },
        {
          "8\nBüchner, T. et al.": "Effectiveness of Deep Features as a Perceptual Metric. Proceedings of\nthe IEEE"
        },
        {
          "8\nBüchner, T. et al.": "Conference on Computer Vision and Pattern Recognition (Apr 2018). https://doi."
        },
        {
          "8\nBüchner, T. et al.": "org/10.48550/arXiv.1801.03924"
        }
      ],
      "page": 8
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Beyond debiasing: Actively steering feature selection via loss regularization",
      "authors": [
        "J Blunk",
        "N Penzel",
        "P Bodesheim",
        "J Denzler"
      ],
      "year": "2023",
      "venue": "DAGM German Conference on Pattern Recognition"
    },
    {
      "citation_id": "2",
      "title": "Improved obstructed facial feature reconstruction for emotion recognition with minimal change cyclegans",
      "authors": [
        "T Büchner",
        "O Guntinas-Lichius",
        "J Denzler"
      ],
      "year": "2023",
      "venue": "Advanced Concepts for Intelligent Vision Systems (Acivs)",
      "doi": "10.1007/978-3-031-45382-3_22"
    },
    {
      "citation_id": "3",
      "title": "Let's get the facs straight -reconstructing obstructed facial features",
      "authors": [
        "T Büchner",
        "S Sickert",
        "G Volk",
        "C Anders",
        "O Guntinas-Lichius",
        "J Denzler"
      ],
      "year": "2023",
      "venue": "ternational Conference on Computer Vision Theory and Applications (VISAPP)",
      "doi": "10.5220/0011619900003417"
    },
    {
      "citation_id": "4",
      "title": "From Faces to Volumes -Measuring Volumetric Asymmetry in 3D Facial Palsy Scans",
      "authors": [
        "T Büchner",
        "S Sickert",
        "G Volk",
        "O Guntinas-Lichius",
        "J Denzler"
      ],
      "year": "2023",
      "venue": "Advances in Visual Computing",
      "doi": "10.1007/978-3-031-47969-4_10"
    },
    {
      "citation_id": "5",
      "title": "Using 2d and 3d face representations to generate comprehensive facial electromyography intensity maps",
      "authors": [
        "T Büchner",
        "S Sickert",
        "R Graßme",
        "C Anders",
        "O Guntinas-Lichius",
        "J Denzler"
      ],
      "year": "2023",
      "venue": "International Symposium on Visual Computing (ISVC)",
      "doi": "10.1007/978-3-031-47966-3_11"
    },
    {
      "citation_id": "6",
      "title": "Understanding and Mitigating Annotation Bias in Facial Expression Recognition",
      "authors": [
        "Y Chen",
        "J Joo"
      ],
      "year": "2021",
      "venue": "2021 IEEE/CVF International Conference on Computer Vision (ICCV)",
      "doi": "10.1109/ICCV48922.2021.01471"
    },
    {
      "citation_id": "7",
      "title": "An argument for basic emotions",
      "authors": [
        "P Ekman"
      ],
      "year": "1992",
      "venue": "Cognition and Emotion",
      "doi": "10.1080/02699939208411068"
    },
    {
      "citation_id": "8",
      "title": "Kernel measures of conditional dependence",
      "authors": [
        "K Fukumizu",
        "A Gretton",
        "X Sun",
        "B Schölkopf"
      ],
      "year": "2007",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "9",
      "title": "High-resolution surface electromyographic activities of facial muscles during the six basic emotional expressions in healthy adults: a prospective observational study",
      "authors": [
        "O Guntinas-Lichius",
        "V Trentzsch",
        "N Mueller",
        "M Heinrich",
        "A Kuttenreich",
        "C Dobel"
      ],
      "year": "2023",
      "venue": "Scientific Reports"
    },
    {
      "citation_id": "10",
      "title": "",
      "authors": [
        "J Pearl"
      ],
      "year": "2009",
      "venue": ""
    },
    {
      "citation_id": "11",
      "title": "Analyzing the behavior of cauliflower harvest-readiness models by investigating feature relevances",
      "authors": [
        "N Penzel",
        "J Kierdorf",
        "R Roscher",
        "J Denzler"
      ],
      "year": "2023",
      "venue": "2023 IEEE/CVF International Conference on Computer Vision Workshops (ICCVW)"
    },
    {
      "citation_id": "12",
      "title": "Investigating neural network training on a feature level using conditional independence",
      "authors": [
        "N Penzel",
        "C Reimers",
        "P Bodesheim",
        "J Denzler"
      ],
      "year": "2022",
      "venue": "European Conference on Computer Vision"
    },
    {
      "citation_id": "13",
      "title": "Facial expression recognition using residual masking network",
      "authors": [
        "L Pham",
        "T Vu",
        "T Tran"
      ],
      "year": "2020",
      "venue": "25th International Conference on Pattern Recognition (ICPR)",
      "doi": "10.1109/ICPR48806.2021.9411919"
    },
    {
      "citation_id": "14",
      "title": "The direction of time",
      "authors": [
        "H Reichenbach"
      ],
      "year": "1956",
      "venue": "The direction of time"
    },
    {
      "citation_id": "15",
      "title": "Conditional dependence tests reveal the usage of abcd rule features and bias variables in automatic skin lesion classification",
      "authors": [
        "C Reimers",
        "N Penzel",
        "P Bodesheim",
        "J Runge",
        "J Denzler"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "16",
      "title": "Determining the relevance of features for deep neural networks",
      "authors": [
        "C Reimers",
        "J Runge",
        "J Denzler"
      ],
      "year": "2020",
      "venue": "European Conference on Computer Vision"
    },
    {
      "citation_id": "17",
      "title": "Conditional independence testing based on a nearest-neighbor estimator of conditional mutual information",
      "authors": [
        "J Runge"
      ],
      "year": "2018",
      "venue": "International Conference on Artificial Intelligence and Statistics"
    },
    {
      "citation_id": "18",
      "title": "Facial expression recognition with adaptive frame rate based on multiple testing correction",
      "authors": [
        "A Savchenko"
      ],
      "year": "2023",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "19",
      "title": "The hardness of conditional independence testing and the generalised covariance measure",
      "authors": [
        "R Shah",
        "J Peters"
      ],
      "year": "2020",
      "venue": "The Annals of Statistics"
    },
    {
      "citation_id": "20",
      "title": "Approximate kernel-based conditional independence tests for fast non-parametric causal discovery",
      "authors": [
        "E Strobl",
        "K Zhang",
        "S Visweswaran"
      ],
      "year": "2019",
      "venue": "Journal of Causal Inference"
    },
    {
      "citation_id": "21",
      "title": "Assessing Facial Symmetry and Attractiveness using Augmented Reality",
      "authors": [
        "W Wei",
        "E Ho",
        "K Mccay",
        "R Damaševičius",
        "R Maskeliūnas",
        "A Esposito"
      ],
      "year": "2022",
      "venue": "Pattern Analysis and Applications",
      "doi": "10.1007/s10044-021-00975-z"
    },
    {
      "citation_id": "22",
      "title": "Change is hard: A closer look at subpopulation shift",
      "authors": [
        "Y Yang",
        "H Zhang",
        "D Katabi",
        "M Ghassemi"
      ],
      "year": "2023",
      "venue": "Change is hard: A closer look at subpopulation shift",
      "arxiv": "arXiv:2302.12254"
    },
    {
      "citation_id": "23",
      "title": "The Unreasonable Effectiveness of Deep Features as a Perceptual Metric",
      "authors": [
        "R Zhang",
        "P Isola",
        "A Efros",
        "E Shechtman",
        "O Wang"
      ],
      "year": "2018",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition",
      "doi": "10.48550/arXiv.1801.03924"
    }
  ]
}