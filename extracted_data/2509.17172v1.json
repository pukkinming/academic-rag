{
  "paper_id": "2509.17172v1",
  "title": "Synergynet: Fusing Generative Priors And State-Space Models For Facial Beauty Prediction",
  "published": "2025-09-21T17:36:42Z",
  "authors": [
    "Djamel Eddine Boukhari"
  ],
  "keywords": [
    "Facial Beauty Prediction",
    "State-Space Models (SSMs)",
    "Diffusion Models",
    "Generative Priors",
    "Vision Mamba (Vim)",
    "Hybrid Architecture"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "The automated prediction of facial beauty is a benchmark task in affective computing that requires a sophisticated understanding of both local aesthetic details (e.g., skin texture) and global facial harmony (e.g., symmetry, proportions). Existing models, based on either Convolutional Neural Networks (CNNs) or Vision Transformers (ViTs), exhibit inherent architectural biases that limit their performance; CNNs excel at local feature extraction but struggle with long-range dependencies, while ViTs model global relationships at a significant computational cost. This paper introduces the Mamba-Diffusion Network (MD-Net), a novel dual-stream architecture that resolves this tradeoff by delegating specialized roles to state-of-the-art models. The first stream leverages a frozen U-Net encoder from a pre-trained latent diffusion model, providing a powerful generative prior for fine-grained aesthetic qualities. The second stream employs a Vision Mamba (Vim), a modern state-space model, to efficiently capture global facial structure with linear-time complexity. By synergistically integrating these complementary representations through a cross-attention mechanism, MD-Net creates a holistic and nuanced feature space for prediction. Evaluated on the SCUT-FBP5500 benchmark, MD-Net sets a new state-of-the-art, achieving a Pearson Correlation of 0.9235 and demonstrating the significant potential of hybrid architectures that fuse generative and sequential modeling paradigms for complex visual assessment tasks.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "The human perception of facial beauty is a profound cognitive phenomenon, weaving together evolutionary instincts, cultural norms, and individual subjectivity. Despite this complexity, a remarkable cross-cultural consensus exists in aesthetic judgments  [1] , suggesting that our perception is guided by a set of learnable, quantifiable visual patterns  [2] . Automating the prediction of facial beauty, a task known as Facial Beauty Prediction (FBP), has thus emerged as a benchmark problem in computer vision and affective computing  [3] . Beyond its immediate application, FBP serves as an ideal testbed for a model's ability to learn a nuanced, human-aligned understanding of subtle and holistic visual concepts  [4] .\n\nThe progression of FBP methodologies has mirrored the broader evolution of computer vision. Early attempts were rooted in feature engineering, attempting to codify classical aesthetic canons like the golden ratio or facial symmetry into handcrafted geometric features  [5] . While insightful, these approaches were fundamentally limited; they failed to capture the intricate interplay of skin texture, color harmony, lighting, and the holistic \"gestalt\" that defines human perception  [6] .\n\nThe advent of deep learning, specifically Convolutional Neural Networks (CNNs)  [7] , marked a paradigm shift  [8] . Models like ResNet  [9] , pre-trained on massive datasets such as ImageNet, demonstrated a powerful ability to learn hierarchical feature representations directly from pixel data. When fine-tuned for FBP, these models achieved state-ofthe-art performance by automatically discovering relevant visual cues  [10] . However, the core strength of CNNs their strong inductive bias for local patterns and spatial hierarchies is also their primary weakness for this task  [11] . The convolutional operator, with its intrinsically limited receptive field, excels at identifying local features (e.g., the texture of skin, the shape of an eye) but struggles to explicitly model the long-range spatial dependencies that govern global facial harmony and proportionality  [12] . A beautiful face is more than just an aggregate of beautiful parts; it is their synergistic arrangement.\n\nTo overcome this limitation, the field turned to Vision Transformers (ViTs)  [13] . By eschewing convolutions in favor of a global self-attention mechanism, ViTs can model the relationship between any two regions of the face. This makes them theoretically adept at assessing global concepts like bilateral symmetry or the geometric ratios between distant facial landmarks  [14] . While promising, ViTs introduce their own set of challenges. Their self-attention mechanism incurs a computational cost that is quadratic with respect to the number of image patches, making them resource-intensive  [15] . Furthermore, their lack of a strong inductive bias often necessitates pre-training on colossal datasets to achieve competitive performance  [16] . This architectural trade off between the locality of CNNs and the complexity of ViTs represents a critical bottleneck for further progress in FBP.\n\nIn this paper, we argue that the next leap in performance requires a move beyond this monolithic architectural dichotomy. The nuanced task of FBP, which demands a simultaneous appreciation of both fine-grained local details and overarching global structure, is better addressed by a specialist, synergistic system. We propose that these two distinct perceptual axes should be delegated to the models best suited for each.\n\nTo this end, we introduce the Mamba-Diffusion Network (MD-Net), a novel, dual-stream architecture founded on two central hypotheses:\n\n1. Generative Priors for Fine-Grained Aesthetics: The encoder of a latent diffusion model [?], trained for a denoising-reconstruction task on billions of internet images, has learned an unparalleled representation of what constitutes a high-quality, aesthetically coherent visual signal. We hypothesize that these features, which form a potent \"generative prior,\" are a far superior foundation for assessing local aesthetic quality (e.g., skin texture, lighting) than the class-discriminative features of models trained for classification.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Efficient Long-Range Modeling With State-Space Models:",
      "text": "The task of capturing global facial structureproportions, symmetry, and harmony-is fundamentally a long-range dependency problem. We hypothesize that modern State-Space Models (SSMs) like Vision Mamba (Vim) [?], which match the power of Transformers with linear-time complexity, are the ideal architectural choice for efficiently and effectively modeling this holistic context.\n\nBy intelligently fusing the features from these two powerful, specialized streams using a cross-attention mechanism, MD-Net creates a rich, comprehensive facial representation that is simultaneously aware of local quality and global harmony. Our work presents the following key contributions:\n\n• We propose MD-Net, a novel dual-stream hybrid architecture for FBP that, for the first time, integrates a diffusion model encoder and a Vision Mamba model.\n\n• We demonstrate the utility of leveraging a frozen, pre-trained diffusion encoder as a superior feature extractor for a subjective, fine-grained regression task, challenging the prevailing reliance on classification-based backbones.\n\n• We achieve a new state-of-the-art on the FBP5500 benchmark, substantially outperforming established CNN and ViT-based baselines and thereby validating the power of our synergistic design.\n\nThis paper is structured as follows: Section 2 reviews the relevant literature. Section 3 provides a detailed exposition of the MD-Net architecture and our experimental protocol. Section 4 presents our quantitative results and ablation studies. Section 5 discusses the implications of our findings, acknowledges limitations, and proposes directions for future research. Finally, Section 6 concludes the paper.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Related Work",
      "text": "Our research is situated at the intersection of three key domains: automated facial beauty prediction, the use of generative models as feature extractors, and the application of modern state-space models to computer vision. This section reviews the literature in these areas to contextualize the contribution of our work.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Automated Facial Beauty Prediction (Fbp)",
      "text": "The automated prediction of facial attractiveness has evolved in lockstep with the advancements in machine learning and computer vision.\n\nTraditional Approaches. Early methods relied on feature engineering, where domain expertise was used to design features presumed to be correlated with attractiveness. These often included geometric ratios based on facial landmarks (e.g., the golden ratio), symmetry measurements, and texture descriptors like Local Binary Patterns (LBP)  [17] . While foundational, these methods were constrained by the expressive power of their handcrafted features and struggled to capture the holistic and subtle nuances of human perception.\n\nConvolutional Neural Networks (CNNs). The advent of deep learning revolutionized FBP. CNNs, particularly deep architectures like VGGNet, ResNet  [9] , and EfficientNet, became the dominant paradigm. By fine-tuning models pre-trained on large-scale datasets like ImageNet, researchers achieved significant performance improvements  [18] . The strength of CNNs lies in their ability to learn a rich hierarchy of features automatically, from simple edges to complex facial components. However, their primary limitation is a strong architectural inductive bias towards local features, stemming from their limited receptive fields. This makes it inherently challenging for standard CNNs to explicitly model the long-range spatial dependencies crucial for assessing global facial harmony and proportions.\n\nVision Transformers (ViTs). To address the locality limitation of CNNs, researchers began adopting Vision Transformers  [13] . By treating an image as a sequence of patches and applying a self-attention mechanism, ViTs can model the relationship between any two regions of the face, regardless of their spatial distance. This makes them theoretically well-suited for capturing global context. However, ViTs are not without drawbacks. The self-attention mechanism has a computational complexity of O(n 2 ) with respect to the number of patches, making it computationally expensive. Moreover, their lack of a strong inductive bias means they typically require massive datasets or sophisticated training strategies to achieve high performance.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Generative Models As Feature Extractors",
      "text": "The prevailing practice in computer vision is to use features from models pre-trained on discriminative tasks (e.g., ImageNet classification). Our work challenges this convention by leveraging features from a generative model.\n\nLatent Diffusion Models (LDMs), such as Stable Diffusion  [19] , represent the state-of-the-art in image synthesis. Their training objective is to learn to denoise a latent representation of an image conditioned on a text prompt. This process forces the model's U-Net encoder to learn a deeply semantic and visually rich representation of the natural image manifold. It must capture not just object identity but also fine-grained texture, lighting, style, and composition to enable high-fidelity reconstruction. We posit that these features, which form a potent generative prior, are better aligned with the assessment of subjective visual qualities like aesthetics than the class-discriminative features of classification models  [20] . The use of generative models as feature extractors is an emerging area, and its application to a subjective regression task like FBP remains largely unexplored.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "State-Space Models (Ssms) For Vision",
      "text": "Transformers have been the dominant architecture for sequence modeling, but their quadratic complexity remains a bottleneck. Recently, State-Space Models have emerged as a highly promising alternative.\n\nMamba  [21]  is a novel SSM architecture that matches or exceeds the performance of Transformers on various sequence modeling tasks but with linear-time complexity. It achieves this through a selective scan mechanism, which allows it to dynamically focus on or ignore information as it processes a sequence. This efficiency and power make it a compelling alternative for tasks requiring long-range dependency modeling.\n\nVision Mamba (Vim)  [22]  adapts the Mamba architecture for computer vision. By treating an image as a sequence of patches, Vim applies a bidirectional Mamba model to efficiently capture both local and global visual context  [23] . As a very recent development, its potential across the full spectrum of vision tasks is still being discovered. Our work is the first, to our knowledge, to apply Vision Mamba to the domain of computational aesthetics.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Positioning Our Contribution",
      "text": "Our proposed MD-Net stands in contrast to prior work by rejecting a monolithic architectural approach. We identify the limitations of existing models-the local bias of CNNs and the computational cost of ViTs-and propose a specialist, synergistic system. MD-Net is the first architecture to:\n\n1. Explicitly leverage the rich generative prior from a pre-trained latent diffusion model's encoder for fine-grained aesthetic feature extraction in the FBP task.\n\n2. Employ a modern state-space model (Vision Mamba) to efficiently model the global, long-range structural properties of a face.\n\n3. Synergistically fuse these two complementary representations using a cross-attention mechanism, creating a more comprehensive and powerful model for facial beauty prediction.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Methodology",
      "text": "This section provides a rigorous and reproducible description of the proposed Mamba-Diffusion Network (MD-Net).\n\nWe first detail the dataset and the data preparation pipeline. Next, we present an in-depth breakdown of the MD-Net architecture, elaborating on the design of its dual streams and the fusion mechanism. Finally, we formalize the training and evaluation protocol, including the choice of loss function, optimizer, and the precise algorithm followed.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Dataset And Preprocessing",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Dataset Specification",
      "text": "All experiments are conducted on the public FBP5500 dataset  [24] , a standard benchmark for the Facial Beauty Prediction (FBP) task. This dataset consists of 5,500 facial images of individuals with diverse ages, genders, and ethnicities. Each image is associated with a ground-truth beauty score on a continuous scale from 1 to 5, representing the mean rating from 60 independent human annotators. For experimental consistency, we adhere to the official cross-validation split designated in the file cross_validation_5, which partitions the dataset into a fixed training set and a disjoint test set.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Image Preprocessing And Augmentation",
      "text": "To prepare the images for the network, we apply a standardized preprocessing pipeline:\n\n1. Resizing: All images are resized to a fixed resolution of 224 × 224 pixels to match the expected input dimensions of the pre-trained models. To mitigate overfitting, we apply random horizontal flipping with a probability of 0.5 to the training images only. This augmentation technique enhances the model's generalization by ensuring it does not develop a lateral bias.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Architectural Design Of Md-Net",
      "text": "The core of our contribution is MD-Net, a dual-stream architecture designed to produce a comprehensive facial representation by synergistically combining local aesthetic details and global structural information. The architecture is illustrated in Figure  1 .",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Stream I: Aesthetic Feature Extraction Using A Diffusion Prior",
      "text": "This stream leverages the U-Net encoder from the Stable Diffusion v1.5 model  [19] . The encoder's weights are frozen during training, acting as a powerful, static feature extractor. We posit that its training objective (denoising-based reconstruction) imbues it with a rich generative prior for visual quality, making it more suitable for aesthetic assessment than a classification-based prior. We extract the output feature maps from its four primary down_blocks, providing a multi-scale representation of fine-grained facial details.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Stream Ii: Global Structural Modeling Using Vision Mamba (Vim)",
      "text": "This stream employs a vim-tiny model  [22] , a visual State-Space Model (SSM). Vim models long-range dependencies with linear-time complexity O(n), making it highly efficient for capturing holistic facial properties like symmetry and proportionality. The entire Vim model is fine-tuned during training. We remove its original classification head and use the final aggregated feature vector as a compact representation of the global facial structure.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Synergistic Feature Fusion Module",
      "text": "We implement a cross-attention mechanism to intelligently fuse the two streams. The global feature vector from the Mamba stream serves as the Query, while the sequence of multi-scale feature maps from the diffusion stream serves as the Key and Value. This allows the model to dynamically up-weight the importance of specific local aesthetic details based on the overall facial structure.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Training Protocol And Evaluation",
      "text": "The entire training and evaluation procedure is formalized in Algorithm 1.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Input Facial Image",
      "text": "Stream I: Aesthetic Features",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Loss Function And Optimization",
      "text": "We employ the Smooth L1 Loss function, which is more robust to outliers than Mean Squared Error. The loss for a prediction ŷ and true value y is defined as:\n\nwhere we use the standard hyperparameter value β = 1.0. The model's trainable parameters are optimized using the AdamW optimizer  [25]  with a base learning rate of 1 × 10 -5 and a weight decay of 0.01. The learning rate is dynamically adjusted using a Cosine Annealing Scheduler. To manage GPU memory, training is performed using Automatic Mixed Precision (AMP).",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Experiments",
      "text": "To empirically validate the efficacy and architectural design of our proposed Mamba-Diffusion Network (MD-Net), we conducted a series of comprehensive experiments. This section provides a detailed account of our experimental setup, the metrics used for evaluation, a comparison against a broad range of state-of-the-art methods, our implementation and hyperparameter choices, and finally, presents an in-depth quantitative analysis and discussion of the results.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Experimental Setup",
      "text": "",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Dataset",
      "text": "All experiments are performed on the SCUT-FBP5500 dataset  [24] , the de facto standard for the FBP task. This dataset contains 5,500 facial images with significant diversity in terms of age, gender, and ethnicity. Each image is annotated with a continuous beauty score from 1 to 5, which is the mean score from 60 human annotators  [26] . To ensure fair, direct, and reproducible comparisons with prior work, we strictly adhere to the official cross_validation_5 split, which provides a fixed partitioning of the dataset into predefined training and testing sets.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Hardware And Software",
      "text": "All model training and inference procedures were conducted on a single server equipped with an NVIDIA A100 GPU with 40GB of VRAM. Our implementation is built using the PyTorch v1.13 deep learning framework, with CUDA v11.7 for GPU acceleration. Key external libraries include Hugging Face's diffusers library (v0.14) for accessing the pre-trained Stable Diffusion model and the official vim library for the Vision Mamba implementation.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Evaluation Metrics",
      "text": "To provide a multi-faceted and rigorous assessment of model performance, we employ three standard regression metrics that are ubiquitously used in the FBP literature  [27] .",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Pearson Correlation (Pc):",
      "text": "This is considered the primary metric for the FBP task. PC measures the linear relationship between the vector of predicted scores ( Ŷ ) and the vector of ground-truth scores (Y ). It evaluates how well the model's ranking of attractiveness aligns with the human consensus ranking, which is crucial for a subjective task like beauty prediction. A value closer to 1.0 indicates a stronger positive correlation  [28] .\n\n2. Mean Absolute Error (MAE): MAE provides a direct, interpretable measure of the average magnitude of the prediction error, without considering its direction. It is less sensitive to large outlier errors compared to RMSE  [29] .\n\n3. Root Mean Squared Error (RMSE): RMSE is another measure of the difference between predicted and actual values. By squaring the errors before averaging, it penalizes larger errors more heavily than MAE, making it a useful metric for understanding the variance and the presence of significant mispredictions in the model's outputs  [30] .",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Comparison With State-Of-The-Art",
      "text": "To thoroughly contextualize the performance of MD-Net, we compare it against a comprehensive list of published results on the SCUT-FBP5500 dataset. This comparison spans multiple generations of computer vision models, allowing for a clear demonstration of progress. The comparison methods are grouped into two categories:\n\n• Classic and Early Deep Learning Methods: This group includes foundational CNN architectures like AlexNet  [8] , as well as more powerful and deeper models such as ResNet-50  [9]  and its variant ResNeXt-50  [9] . These models serve as strong, general-purpose vision baselines.\n\n• Advanced Methods and State-of-the-Art: This category includes recent and highly specialized models designed specifically for FBP. These methods incorporate more complex mechanisms such as spatial and channel-wise attention (CNN + SCA  [31] ), label distribution learning (CNN + LDL  [32] ), dynamic attentive convolutions (DyAttenConv  [33] ), and the previous state-of-the-art model, R3CNN  [34] , which uniquely integrates relative ranking into the objective. A strong performance against this group demonstrates a true advancement in the field.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Implementation And Hyperparameter Details",
      "text": "Reproducibility is paramount to our experimental design. Our MD-Net model was trained following the protocol described in Algorithm 1. We fine-tuned only the trainable parameters: the Vision Mamba stream, the cross-attention fusion module, and the final MLP regression head. Crucially, the weights of the Stable Diffusion U-Net encoder remained frozen throughout training to preserve its powerful generative prior. Automatic Mixed Precision (AMP) was enabled to accelerate training and reduce the GPU memory footprint, allowing for a larger batch size. All specific hyperparameters are detailed in Table  1 .",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Quantitative Results And Discussion",
      "text": "The main performance comparison of MD-Net against all other methods is presented in Table  2 . Our proposed model establishes a new state-of-the-art on the SCUT-FBP5500 benchmark, achieving superior performance by a significant margin across all three evaluation metrics.\n\nThe Pearson Correlation score of 0.9235 is a key result, representing a new benchmark for alignment with human aesthetic consensus. This score surpasses the previous best-in-class R3CNN (0.9142), indicating that MD-Net's predictions are more linearly correlated with human judgments than any prior method. This strongly supports our central hypothesis: that the synergistic fusion of a rich generative prior for local aesthetics (from the diffusion encoder) and an efficient model for global structure (Vision Mamba) yields a more holistic and accurate facial representation.\n\nBeyond just improved ranking, MD-Net demonstrates superior predictive accuracy. It achieves the lowest error scores ever reported on this benchmark, with an MAE of 0.2006 and an RMSE of 0.2580. These results represent a substantial reduction in error compared to the previous state-of-the-art (R3CNN's 0.2120 MAE and 0.2800 RMSE). This demonstrates that MD-Net's predictions are not only better ranked but are also numerically closer to the ground-truth scores. The significant improvement over a range of strong and diverse methods, from pure CNNs to attention-based models, validates the novelty and effectiveness of our hybrid architectural design.   [9]  0.8900 0.2419 0.3166 ResNeXt-50  [9]  0.8997 0.2291 0.3017\n\nAdvanced Methods and State-of-the-Art CNN + SCA  [31]  0.9003 0.2287 0.3014 CNN + LDL  [32]  0.9031 --DyAttenConv  [33]  0.9056 0.2199 0.2950 R3CNN (ResNeXt-50)  [34]  0.9142 0.2120 0.2800\n\nOur Proposed Method MD-Net(Ours) 0.9235 0.2006 0.2580",
      "page_start": 9,
      "page_end": 10
    },
    {
      "section_name": "In-Depth Ablation Studies",
      "text": "To deconstruct the sources of MD-Net's performance and empirically validate our specific architectural choices, we conducted a rigorous ablation study. We systematically evaluated variations of our model by removing or altering key components. The results, summarized in Table  3 , provide clear insights into the contribution of each part of the system.\n\nTable  3 : Ablation study of MD-Net's components. Each component provides a significant and complementary contribution to the model's overall performance. The key insights drawn from this study are:",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Configuration",
      "text": "• Synergy of Dual Streams is Critical: The most significant finding is that removing either the Mamba stream (row B) or the Diffusion stream (row C) leads to a substantial drop in performance. This confirms that the two streams capture distinct, non-redundant, and complementary information. It is not merely an ensemble effect; rather, the combination is essential. Notably, the Diffusion-only model (PC=0.9081) on its own is a very strong baseline, outperforming ResNet-50 and nearly matching ResNeXt-50. This strongly validates our hypothesis about the utility of generative priors for assessing aesthetic quality. However, the full model's large performance gain over either individual stream demonstrates that a combination of local quality and global structure is required to achieve the highest level of performance.\n\n• Intelligent Fusion is Superior: We evaluated the importance of our fusion mechanism by replacing the cross-attention module with a simpler strategy: concatenating the two feature vectors and passing them through an MLP (row D). While this model still performs well (PC=0.9126), it is significantly weaker than the full MD-Net. This proves that the method of fusion is a critical design choice. The cross-attention mechanism, which allows the global structural features to dynamically query and attend to the most salient local aesthetic features, provides a more powerful and contextually-aware integration than a simple, static concatenation.\n\nCollectively, these ablation results provide strong evidence that every major architectural component of MD-Net is a well-justified and necessary contributor to its state-of-the-art performance.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Discussion",
      "text": "The empirical results presented in Section 4 strongly validate the architectural design of the Mamba-Diffusion Network (MD-Net) and its underlying principles. The state-of-the-art performance is not merely an incremental improvement but rather the result of a paradigm shift from monolithic architectures to a synergistic, multi-paradigm system. We attribute the success of MD-Net to three primary factors.\n\nFirst, the power of generative priors for aesthetic assessment. The exceptional performance of the Diffusion-only model in our ablation study (Table  3 , row B), which surpassed the strong ViT-Base baseline, provides compelling evidence for our core hypothesis. The U-Net encoder of a latent diffusion model, trained on a denoising-reconstruction objective, learns a representation that is deeply sensitive to the fine-grained details that constitute visual quality-texture fidelity, lighting coherence, sharpness, and subtle color gradients. This \"aesthetic prior\" appears to be fundamentally more aligned with the FBP task than the class-discriminative features learned by models trained on classification tasks like ImageNet.\n\nSecond, the necessity of efficient global context modeling. While the diffusion prior provides a powerful signal for local quality, the ablation results clearly show that it is insufficient on its own. The significant performance leap of the full MD-Net over the Diffusion-only variant underscores the criticality of global facial structure. Facial beauty is not judged on texture alone but on the harmonious interplay of proportions, symmetry, and the geometric arrangement of features. The Vision Mamba stream, with its linear-time complexity and aptitude for modeling long-range dependencies, efficiently captures this holistic context. The synergy is evident: the model learns to evaluate local details within the framework of the global structure.\n\nThird, the efficacy of intelligent feature fusion. The superiority of cross-attention over simple feature concatenation (Table  3 , row D) highlights the importance of how the two streams communicate. Cross-attention provides a mechanism for dynamic, context-aware integration. The global representation (from Mamba) can selectively \"query\" the multi-scale local feature maps (from the diffusion encoder), allowing the model to focus on the most salient aesthetic details given a particular facial structure. This is a more powerful and nuanced fusion strategy than simply combining two independent feature vectors.\n\nBeyond the specific task of FBP, our findings suggest a broader implication for computer vision: complex visual recognition tasks that depend on both micro-level details and macro-level composition may benefit significantly from hybrid architectures that delegate these specialized roles to the most suitable models, such as combining generative and sequential modeling paradigms.",
      "page_start": 11,
      "page_end": 12
    },
    {
      "section_name": "Limitations",
      "text": "Despite the strong performance, it is crucial to acknowledge the limitations of this work.\n\n• Dataset and Annotation Bias: The foremost limitation is the model's reliance on the FBP5500 dataset. The concept of \"beauty\" learned by MD-Net is a proxy for the consensus of the 60 annotators for this specific dataset. These annotations are subject to demographic, cultural, and individual biases. Therefore, our model's predictions do not represent an objective or universal measure of beauty but rather reflect the specific distribution and preferences inherent in its training data.\n\n• Interpretability Challenges: While our dual-stream design is motivated by an intuitive separation of concerns (local vs. global), the model remains a \"black box\" to a significant extent. The complex, non-linear interactions within the Mamba blocks and the cross-attention module make it difficult to definitively isolate and prove which specific facial attributes (e.g., \"symmetry,\" \"skin clarity\") are being measured by each component.\n\n• Computational Complexity: MD-Net is a large and computationally intensive model. Its dual-stream nature, particularly the inclusion of the large U-Net encoder, makes both training and inference more resourcedemanding compared to a standard ResNet-50. This presents a practical barrier to deployment in resourceconstrained environments.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Future Work",
      "text": "The success and limitations of MD-Net open up several promising avenues for future research.\n\n• Fairness and Bias Mitigation: Directly addressing the dataset bias is a critical next step. Future work should focus on training and evaluating models on more diverse, cross-cultural datasets. Furthermore, employing fairness-aware machine learning techniques to quantify and mitigate biases across different demographic subgroups (e.g., ethnicity, gender) is essential for developing more equitable and responsible models.\n\n• Enhancing Model Interpretability: To move beyond our architectural intuition, future research could apply advanced explainability techniques. Methods such as Concept Activation Vectors (CAV) could be used to probe whether the Mamba stream is indeed learning concepts like \"symmetry,\" or visualizing the cross-attention maps could reveal which fine-grained features are prioritized for different global structures.\n\n• Model Compression and Efficiency: To address the computational cost, future work could explore model compression techniques. Knowledge distillation, where the large MD-Net acts as a \"teacher\" to train a much smaller, faster \"student\" model (e.g., a MobileNet), could create a lightweight version that retains most of the performance. Quantization and pruning are also viable avenues for creating more efficient versions of the model.\n\n• Generalization to Other Aesthetic Domains: The core principle of MD-Net-fusing a generative prior for local quality with a sequential model for global structure-is highly generalizable. We plan to investigate the application of this architecture to other subjective, aesthetic assessment tasks, such as predicting the aesthetic quality of photography, art, and user interface design.",
      "page_start": 11,
      "page_end": 12
    },
    {
      "section_name": "Conclusion",
      "text": "In this paper, we addressed the inherent architectural trade-off between local feature extraction and global context modeling in the complex task of Facial Beauty Prediction (FBP). We challenged the prevailing reliance on monolithic architectures by proposing the Mamba-Diffusion Network (MD-Net), a novel, dual-stream hybrid model designed for synergistic feature representation. Our central thesis was that the nuanced human perception of beauty, which relies on both fine-grained aesthetic details and holistic facial harmony, is best replicated by a system that delegates these specialized roles to the most suitable modern architectures.\n\nOur proposed MD-Net successfully operationalized this principle by integrating two powerful and complementary components:\n\n1. A frozen U-Net encoder from a pre-trained latent diffusion model, which provides a rich, unparalleled generative prior for local aesthetic quality.\n\n2. A fine-tuned Vision Mamba (Vim), a modern state-space model that efficiently captures long-range spatial dependencies and global facial structure with linear-time complexity.\n\nBy intelligently fusing these representations using a cross-attention mechanism, MD-Net creates a comprehensive and potent feature space that is simultaneously sensitive to both local texture and global proportionality.\n\nOur comprehensive experiments on the standard SCUT-FBP5500 benchmark have empirically validated our approach. MD-Net not only surpassed strong CNN and Transformer-based baselines but also set a new state-of-the-art, achieving a Pearson Correlation of 0.9235, along with the lowest MAE and RMSE scores reported to date. Rigorous ablation studies further confirmed that the synergy between the two streams and the sophisticated fusion mechanism were both critical to this success.\n\nBeyond the specific application of FBP, our work carries broader implications. It demonstrates a promising new architectural paradigm for complex visual assessment tasks: instead of forcing a single architecture to be a generalist, we can achieve superior performance by fusing the specialized strengths of disparate state-of-the-art models, such as those from the generative and sequential modeling domains. We believe that such hybrid, multi-paradigm approaches will be a key driver of future progress in developing more nuanced, human-aligned computer vision systems.",
      "page_start": 12,
      "page_end": 12
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The architecture of the Mamba-Diffusion Network (MD-Net). An input image is processed in parallel by a",
      "page": 6
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "September 23, 2025": "ABSTRACT"
        },
        {
          "September 23, 2025": "The automated prediction of facial beauty is a benchmark task in affective computing that requires"
        },
        {
          "September 23, 2025": "a sophisticated understanding of both local aesthetic details (e.g., skin texture) and global facial"
        },
        {
          "September 23, 2025": "harmony (e.g., symmetry, proportions). Existing models, based on either Convolutional Neural"
        },
        {
          "September 23, 2025": "Networks (CNNs) or Vision Transformers (ViTs), exhibit\ninherent architectural biases that\nlimit"
        },
        {
          "September 23, 2025": "their performance; CNNs excel at local feature extraction but struggle with long-range dependencies,"
        },
        {
          "September 23, 2025": "while ViTs model global relationships at a significant computational cost. This paper introduces the"
        },
        {
          "September 23, 2025": "Mamba-Diffusion Network (MD-Net), a novel dual-stream architecture that resolves this trade-"
        },
        {
          "September 23, 2025": "off by delegating specialized roles to state-of-the-art models. The first stream leverages a frozen"
        },
        {
          "September 23, 2025": "U-Net encoder\nfrom a pre-trained latent diffusion model, providing a powerful generative prior"
        },
        {
          "September 23, 2025": "for fine-grained aesthetic qualities. The second stream employs a Vision Mamba (Vim), a modern"
        },
        {
          "September 23, 2025": "state-space model,\nto efficiently capture global\nfacial structure with linear-time complexity. By"
        },
        {
          "September 23, 2025": "synergistically integrating these complementary representations through a cross-attention mechanism,"
        },
        {
          "September 23, 2025": "MD-Net creates a holistic and nuanced feature space for prediction. Evaluated on the SCUT-FBP5500"
        },
        {
          "September 23, 2025": "benchmark, MD-Net sets a new state-of-the-art, achieving a Pearson Correlation of 0.9235 and"
        },
        {
          "September 23, 2025": "demonstrating the significant potential of hybrid architectures that fuse generative and sequential"
        },
        {
          "September 23, 2025": "modeling paradigms for complex visual assessment tasks."
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "1\nIntroduction": "The human perception of facial beauty is a profound cognitive phenomenon, weaving together evolutionary instincts,"
        },
        {
          "1\nIntroduction": "cultural norms, and individual subjectivity. Despite this complexity, a remarkable cross-cultural consensus exists in"
        },
        {
          "1\nIntroduction": ""
        },
        {
          "1\nIntroduction": "Automating the prediction of facial beauty, a task known as Facial Beauty Prediction (FBP), has thus emerged as a"
        },
        {
          "1\nIntroduction": "benchmark problem in computer vision and affective computing"
        },
        {
          "1\nIntroduction": "an ideal testbed for a model’s ability to learn a nuanced, human-aligned understanding of subtle and holistic visual"
        },
        {
          "1\nIntroduction": "concepts\n[4]."
        },
        {
          "1\nIntroduction": "The progression of FBP methodologies has mirrored the broader evolution of computer vision. Early attempts were"
        },
        {
          "1\nIntroduction": "rooted in feature engineering, attempting to codify classical aesthetic canons like the golden ratio or facial symmetry"
        },
        {
          "1\nIntroduction": "into handcrafted geometric features [5]. While insightful, these approaches were fundamentally limited; they failed to"
        },
        {
          "1\nIntroduction": "capture the intricate interplay of skin texture, color harmony, lighting, and the holistic \"gestalt\" that defines human"
        },
        {
          "1\nIntroduction": "perception\n[6]."
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "A PREPRINT - SEPTEMBER 23, 2025": "The advent of deep learning, specifically Convolutional Neural Networks (CNNs) [7], marked a paradigm shift [8]."
        },
        {
          "A PREPRINT - SEPTEMBER 23, 2025": "Models like ResNet [9], pre-trained on massive datasets such as ImageNet, demonstrated a powerful ability to learn"
        },
        {
          "A PREPRINT - SEPTEMBER 23, 2025": "hierarchical feature representations directly from pixel data. When fine-tuned for FBP, these models achieved state-of-"
        },
        {
          "A PREPRINT - SEPTEMBER 23, 2025": "the-art performance by automatically discovering relevant visual cues [10]. However, the core strength of CNNs their"
        },
        {
          "A PREPRINT - SEPTEMBER 23, 2025": "strong inductive bias for local patterns and spatial hierarchies is also their primary weakness for this task [11]. The"
        },
        {
          "A PREPRINT - SEPTEMBER 23, 2025": "convolutional operator, with its intrinsically limited receptive field, excels at identifying local features (e.g., the texture"
        },
        {
          "A PREPRINT - SEPTEMBER 23, 2025": "of skin, the shape of an eye) but struggles to explicitly model the long-range spatial dependencies that govern global"
        },
        {
          "A PREPRINT - SEPTEMBER 23, 2025": "facial harmony and proportionality [12]. A beautiful face is more than just an aggregate of beautiful parts; it is their"
        },
        {
          "A PREPRINT - SEPTEMBER 23, 2025": "synergistic arrangement."
        },
        {
          "A PREPRINT - SEPTEMBER 23, 2025": "To overcome this limitation, the field turned to Vision Transformers (ViTs) [13]. By eschewing convolutions in favor of a"
        },
        {
          "A PREPRINT - SEPTEMBER 23, 2025": "global self-attention mechanism, ViTs can model the relationship between any two regions of the face. This makes them"
        },
        {
          "A PREPRINT - SEPTEMBER 23, 2025": "theoretically adept at assessing global concepts like bilateral symmetry or the geometric ratios between distant facial"
        },
        {
          "A PREPRINT - SEPTEMBER 23, 2025": "landmarks [14]. While promising, ViTs introduce their own set of challenges. Their self-attention mechanism incurs"
        },
        {
          "A PREPRINT - SEPTEMBER 23, 2025": "a computational cost that is quadratic with respect to the number of image patches, making them resource-intensive"
        },
        {
          "A PREPRINT - SEPTEMBER 23, 2025": "[15]. Furthermore, their lack of a strong inductive bias often necessitates pre-training on colossal datasets to achieve"
        },
        {
          "A PREPRINT - SEPTEMBER 23, 2025": "competitive performance [16]. This architectural trade off between the locality of CNNs and the complexity of ViTs"
        },
        {
          "A PREPRINT - SEPTEMBER 23, 2025": "represents a critical bottleneck for further progress in FBP."
        },
        {
          "A PREPRINT - SEPTEMBER 23, 2025": "In this paper, we argue that the next leap in performance requires a move beyond this monolithic architectural dichotomy."
        },
        {
          "A PREPRINT - SEPTEMBER 23, 2025": "The nuanced task of FBP, which demands a simultaneous appreciation of both fine-grained local details and overarching"
        },
        {
          "A PREPRINT - SEPTEMBER 23, 2025": "global structure, is better addressed by a specialist, synergistic system. We propose that these two distinct perceptual"
        },
        {
          "A PREPRINT - SEPTEMBER 23, 2025": "axes should be delegated to the models best suited for each."
        },
        {
          "A PREPRINT - SEPTEMBER 23, 2025": "To this end, we introduce the Mamba-Diffusion Network (MD-Net), a novel, dual-stream architecture founded on two"
        },
        {
          "A PREPRINT - SEPTEMBER 23, 2025": "central hypotheses:"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "To this end, we introduce the Mamba-Diffusion Network (MD-Net), a novel, dual-stream architecture founded on two": "central hypotheses:"
        },
        {
          "To this end, we introduce the Mamba-Diffusion Network (MD-Net), a novel, dual-stream architecture founded on two": "1. Generative Priors for Fine-Grained Aesthetics: The encoder of a latent diffusion model [?], trained for"
        },
        {
          "To this end, we introduce the Mamba-Diffusion Network (MD-Net), a novel, dual-stream architecture founded on two": "a denoising-reconstruction task on billions of internet images, has learned an unparalleled representation of"
        },
        {
          "To this end, we introduce the Mamba-Diffusion Network (MD-Net), a novel, dual-stream architecture founded on two": "what constitutes a high-quality, aesthetically coherent visual signal. We hypothesize that these features, which"
        },
        {
          "To this end, we introduce the Mamba-Diffusion Network (MD-Net), a novel, dual-stream architecture founded on two": "form a potent \"generative prior,\" are a far superior foundation for assessing local aesthetic quality (e.g., skin"
        },
        {
          "To this end, we introduce the Mamba-Diffusion Network (MD-Net), a novel, dual-stream architecture founded on two": "texture, lighting) than the class-discriminative features of models trained for classification."
        },
        {
          "To this end, we introduce the Mamba-Diffusion Network (MD-Net), a novel, dual-stream architecture founded on two": "2. Efficient Long-Range Modeling with State-Space Models: The task of capturing global facial structure—"
        },
        {
          "To this end, we introduce the Mamba-Diffusion Network (MD-Net), a novel, dual-stream architecture founded on two": "proportions, symmetry, and harmony—is fundamentally a long-range dependency problem. We hypothesize"
        },
        {
          "To this end, we introduce the Mamba-Diffusion Network (MD-Net), a novel, dual-stream architecture founded on two": "that modern State-Space Models (SSMs) like Vision Mamba (Vim) [?], which match the power of Transformers"
        },
        {
          "To this end, we introduce the Mamba-Diffusion Network (MD-Net), a novel, dual-stream architecture founded on two": "with linear-time complexity, are the ideal architectural choice for efficiently and effectively modeling this"
        },
        {
          "To this end, we introduce the Mamba-Diffusion Network (MD-Net), a novel, dual-stream architecture founded on two": "holistic context."
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "2\nRelated Work": "Our\nresearch is situated at\nthe intersection of\nthree key domains:\nautomated facial beauty prediction,\nthe use of"
        },
        {
          "2\nRelated Work": "generative models as feature extractors, and the application of modern state-space models to computer vision. This"
        },
        {
          "2\nRelated Work": "section reviews the literature in these areas to contextualize the contribution of our work."
        },
        {
          "2\nRelated Work": "2.1\nAutomated Facial Beauty Prediction (FBP)"
        },
        {
          "2\nRelated Work": "The automated prediction of facial attractiveness has evolved in lockstep with the advancements in machine learning"
        },
        {
          "2\nRelated Work": "and computer vision."
        },
        {
          "2\nRelated Work": "Traditional Approaches. Early methods relied on feature engineering, where domain expertise was used to design"
        },
        {
          "2\nRelated Work": "features presumed to be correlated with attractiveness. These often included geometric ratios based on facial landmarks"
        },
        {
          "2\nRelated Work": "(e.g., the golden ratio), symmetry measurements, and texture descriptors like Local Binary Patterns (LBP) [17]. While"
        },
        {
          "2\nRelated Work": "foundational, these methods were constrained by the expressive power of their handcrafted features and struggled to"
        },
        {
          "2\nRelated Work": "capture the holistic and subtle nuances of human perception."
        },
        {
          "2\nRelated Work": "Convolutional Neural Networks (CNNs). The advent of deep learning revolutionized FBP. CNNs, particularly deep"
        },
        {
          "2\nRelated Work": "architectures like VGGNet, ResNet [9], and EfficientNet, became the dominant paradigm. By fine-tuning models"
        },
        {
          "2\nRelated Work": "pre-trained on large-scale datasets like ImageNet, researchers achieved significant performance improvements [18]. The"
        },
        {
          "2\nRelated Work": "strength of CNNs lies in their ability to learn a rich hierarchy of features automatically, from simple edges to complex"
        },
        {
          "2\nRelated Work": "facial components. However, their primary limitation is a strong architectural inductive bias towards local features,"
        },
        {
          "2\nRelated Work": "stemming from their limited receptive fields. This makes it\ninherently challenging for standard CNNs to explicitly"
        },
        {
          "2\nRelated Work": "model the long-range spatial dependencies crucial for assessing global facial harmony and proportions."
        },
        {
          "2\nRelated Work": "Vision Transformers (ViTs). To address the locality limitation of CNNs, researchers began adopting Vision Trans-"
        },
        {
          "2\nRelated Work": "formers [13]. By treating an image as a sequence of patches and applying a self-attention mechanism, ViTs can model"
        },
        {
          "2\nRelated Work": "the relationship between any two regions of the face, regardless of their spatial distance. This makes them theoretically"
        },
        {
          "2\nRelated Work": "well-suited for capturing global context. However, ViTs are not without drawbacks. The self-attention mechanism has"
        },
        {
          "2\nRelated Work": "a computational complexity of O(n2) with respect\nto the number of patches, making it computationally expensive."
        },
        {
          "2\nRelated Work": "Moreover, their lack of a strong inductive bias means they typically require massive datasets or sophisticated training"
        },
        {
          "2\nRelated Work": "strategies to achieve high performance."
        },
        {
          "2\nRelated Work": "2.2\nGenerative Models as Feature Extractors"
        },
        {
          "2\nRelated Work": "The prevailing practice in computer vision is to use features from models pre-trained on discriminative tasks (e.g.,"
        },
        {
          "2\nRelated Work": "ImageNet classification). Our work challenges this convention by leveraging features from a generative model."
        },
        {
          "2\nRelated Work": "Latent Diffusion Models (LDMs), such as Stable Diffusion [19], represent the state-of-the-art in image synthesis. Their"
        },
        {
          "2\nRelated Work": "training objective is to learn to denoise a latent representation of an image conditioned on a text prompt. This process"
        },
        {
          "2\nRelated Work": "forces the model’s U-Net encoder to learn a deeply semantic and visually rich representation of the natural\nimage"
        },
        {
          "2\nRelated Work": "manifold. It must capture not just object identity but also fine-grained texture, lighting, style, and composition to enable"
        },
        {
          "2\nRelated Work": "high-fidelity reconstruction. We posit that these features, which form a potent generative prior, are better aligned"
        },
        {
          "2\nRelated Work": "with the assessment of subjective visual qualities like aesthetics than the class-discriminative features of classification"
        },
        {
          "2\nRelated Work": "models\n[20]. The use of generative models as feature extractors is an emerging area, and its application to a subjective"
        },
        {
          "2\nRelated Work": "regression task like FBP remains largely unexplored."
        },
        {
          "2\nRelated Work": "2.3\nState-Space Models (SSMs) for Vision"
        },
        {
          "2\nRelated Work": "Transformers have been the dominant architecture for sequence modeling, but their quadratic complexity remains a"
        },
        {
          "2\nRelated Work": "bottleneck. Recently, State-Space Models have emerged as a highly promising alternative."
        },
        {
          "2\nRelated Work": "Mamba [21] is a novel SSM architecture that matches or exceeds the performance of Transformers on various sequence"
        },
        {
          "2\nRelated Work": "modeling tasks but with linear-time complexity. It achieves this through a selective scan mechanism, which allows it to"
        },
        {
          "2\nRelated Work": "dynamically focus on or ignore information as it processes a sequence. This efficiency and power make it a compelling"
        },
        {
          "2\nRelated Work": "alternative for tasks requiring long-range dependency modeling."
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "A PREPRINT - SEPTEMBER 23, 2025": ""
        },
        {
          "A PREPRINT - SEPTEMBER 23, 2025": ""
        },
        {
          "A PREPRINT - SEPTEMBER 23, 2025": ""
        },
        {
          "A PREPRINT - SEPTEMBER 23, 2025": ""
        },
        {
          "A PREPRINT - SEPTEMBER 23, 2025": ""
        },
        {
          "A PREPRINT - SEPTEMBER 23, 2025": ""
        },
        {
          "A PREPRINT - SEPTEMBER 23, 2025": ""
        },
        {
          "A PREPRINT - SEPTEMBER 23, 2025": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "A PREPRINT - SEPTEMBER 23, 2025": ""
        },
        {
          "A PREPRINT - SEPTEMBER 23, 2025": ""
        },
        {
          "A PREPRINT - SEPTEMBER 23, 2025": ""
        },
        {
          "A PREPRINT - SEPTEMBER 23, 2025": ""
        },
        {
          "A PREPRINT - SEPTEMBER 23, 2025": ""
        },
        {
          "A PREPRINT - SEPTEMBER 23, 2025": ""
        },
        {
          "A PREPRINT - SEPTEMBER 23, 2025": "its training objective (denoising-based"
        },
        {
          "A PREPRINT - SEPTEMBER 23, 2025": ""
        },
        {
          "A PREPRINT - SEPTEMBER 23, 2025": ""
        },
        {
          "A PREPRINT - SEPTEMBER 23, 2025": ""
        },
        {
          "A PREPRINT - SEPTEMBER 23, 2025": ""
        },
        {
          "A PREPRINT - SEPTEMBER 23, 2025": ""
        },
        {
          "A PREPRINT - SEPTEMBER 23, 2025": ""
        },
        {
          "A PREPRINT - SEPTEMBER 23, 2025": ""
        },
        {
          "A PREPRINT - SEPTEMBER 23, 2025": ""
        },
        {
          "A PREPRINT - SEPTEMBER 23, 2025": ""
        },
        {
          "A PREPRINT - SEPTEMBER 23, 2025": ""
        },
        {
          "A PREPRINT - SEPTEMBER 23, 2025": ""
        },
        {
          "A PREPRINT - SEPTEMBER 23, 2025": ""
        },
        {
          "A PREPRINT - SEPTEMBER 23, 2025": ""
        },
        {
          "A PREPRINT - SEPTEMBER 23, 2025": ""
        },
        {
          "A PREPRINT - SEPTEMBER 23, 2025": ""
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Input Facial Image": ""
        },
        {
          "Input Facial Image": ""
        },
        {
          "Input Facial Image": ""
        },
        {
          "Input Facial Image": ""
        },
        {
          "Input Facial Image": ""
        },
        {
          "Input Facial Image": ""
        },
        {
          "Input Facial Image": ""
        },
        {
          "Input Facial Image": ""
        },
        {
          "Input Facial Image": ""
        },
        {
          "Input Facial Image": ""
        },
        {
          "Input Facial Image": ""
        },
        {
          "Input Facial Image": ""
        },
        {
          "Input Facial Image": ""
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Algorithm 1: MD-Net Training and Evaluation Procedure": "Input\n:Training dataloader Dtrain, Test dataloader Dtest"
        },
        {
          "Algorithm 1: MD-Net Training and Evaluation Procedure": "Input\n:Epochs E, Learning rate η, Weight decay λ"
        },
        {
          "Algorithm 1: MD-Net Training and Evaluation Procedure": "Output :Optimized model parameters θ∗"
        },
        {
          "Algorithm 1: MD-Net Training and Evaluation Procedure": "▷\nInitialization"
        },
        {
          "Algorithm 1: MD-Net Training and Evaluation Procedure": "Initialize MD-Net model M with parameters θ"
        },
        {
          "Algorithm 1: MD-Net Training and Evaluation Procedure": ""
        },
        {
          "Algorithm 1: MD-Net Training and Evaluation Procedure": "Initialize AdamW optimizer O with (θtrainable, η, λ)"
        },
        {
          "Algorithm 1: MD-Net Training and Evaluation Procedure": "Initialize cosine annealing scheduler S"
        },
        {
          "Algorithm 1: MD-Net Training and Evaluation Procedure": "Initialize Smooth L1 loss function LS1"
        },
        {
          "Algorithm 1: MD-Net Training and Evaluation Procedure": ""
        },
        {
          "Algorithm 1: MD-Net Training and Evaluation Procedure": "for e ← 1 to E do"
        },
        {
          "Algorithm 1: MD-Net Training and Evaluation Procedure": "▷\nTraining\nPhase"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "7": "",
          "for e ← 1 to E do": "▷\nTraining\nPhase"
        },
        {
          "7": "8",
          "for e ← 1 to E do": "M.train()"
        },
        {
          "7": "9",
          "for e ← 1 to E do": "foreach batch (x, y) in Dtrain do"
        },
        {
          "7": "10",
          "for e ← 1 to E do": "x, y ← x.to(device), y.to(device)"
        },
        {
          "7": "11",
          "for e ← 1 to E do": "O.zero_grad()"
        },
        {
          "7": "12",
          "for e ← 1 to E do": "y ← M(x)"
        },
        {
          "7": "13",
          "for e ← 1 to E do": "loss ← LS1(ˆy, y)"
        },
        {
          "7": "14",
          "for e ← 1 to E do": "loss.backward()"
        },
        {
          "7": "15",
          "for e ← 1 to E do": "O.step()"
        },
        {
          "7": "16",
          "for e ← 1 to E do": "S.step()"
        },
        {
          "7": "",
          "for e ← 1 to E do": "▷\nEvaluation\nPhase"
        },
        {
          "7": "17",
          "for e ← 1 to E do": "M.eval()"
        },
        {
          "7": "18",
          "for e ← 1 to E do": "Ytrue, Ypred ← {}, {}"
        },
        {
          "7": "19",
          "for e ← 1 to E do": "foreach batch (x, y) in Dtest do"
        },
        {
          "7": "20",
          "for e ← 1 to E do": "with torch.no_grad(): do { x, y ← x.to(device), y.to(device)"
        },
        {
          "7": "21",
          "for e ← 1 to E do": "y ← M(x)"
        },
        {
          "7": "22",
          "for e ← 1 to E do": "Append y to Ytrue, and ˆy to Ypred"
        },
        {
          "7": "23",
          "for e ← 1 to E do": "}"
        },
        {
          "7": "24",
          "for e ← 1 to E do": "P Ccurrent ← PearsonCorrelation(Ytrue, Ypred)"
        },
        {
          "7": "",
          "for e ← 1 to E do": "▷\nCheckpoint\nif\nimproved"
        },
        {
          "7": "25",
          "for e ← 1 to E do": "if P Ccurrent > P Cbest then"
        },
        {
          "7": "26",
          "for e ← 1 to E do": "P Cbest ← P Ccurrent"
        },
        {
          "7": "27",
          "for e ← 1 to E do": "θ∗ ← θ"
        },
        {
          "7": "28",
          "for e ← 1 to E do": "Save checkpoint with parameters θ∗"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "4\nExperiments": "To empirically validate the efficacy and architectural design of our proposed Mamba-Diffusion Network (MD-Net), we"
        },
        {
          "4\nExperiments": ""
        },
        {
          "4\nExperiments": "the metrics used for evaluation, a comparison against a broad range of state-of-the-art methods, our implementation and"
        },
        {
          "4\nExperiments": "hyperparameter choices, and finally, presents an in-depth quantitative analysis and discussion of the results."
        },
        {
          "4\nExperiments": "4.1\nExperimental Setup"
        },
        {
          "4\nExperiments": "4.1.1\nDataset"
        },
        {
          "4\nExperiments": "All experiments are performed on the SCUT-FBP5500 dataset [24], the de facto standard for the FBP task. This dataset"
        },
        {
          "4\nExperiments": "contains 5,500 facial images with significant diversity in terms of age, gender, and ethnicity. Each image is annotated"
        },
        {
          "4\nExperiments": "with a continuous beauty score from 1 to 5, which is the mean score from 60 human annotators"
        },
        {
          "4\nExperiments": ""
        },
        {
          "4\nExperiments": "which provides a fixed partitioning of the dataset into predefined training and testing sets."
        },
        {
          "4\nExperiments": "4.1.2\nHardware and Software"
        },
        {
          "4\nExperiments": "All model training and inference procedures were conducted on a single server equipped with an NVIDIA A100 GPU"
        },
        {
          "4\nExperiments": "with 40GB of VRAM. Our implementation is built using the PyTorch v1.13 deep learning framework, with CUDA"
        },
        {
          "4\nExperiments": "v11.7 for GPU acceleration. Key external libraries include Hugging Face’s diffusers library (v0.14) for accessing the"
        },
        {
          "4\nExperiments": "pre-trained Stable Diffusion model and the official vim library for the Vision Mamba implementation."
        },
        {
          "4\nExperiments": "4.2\nEvaluation Metrics"
        },
        {
          "4\nExperiments": "To provide a multi-faceted and rigorous assessment of model performance, we employ three standard regression metrics"
        },
        {
          "4\nExperiments": "that are ubiquitously used in the FBP literature\n[27]."
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "A PREPRINT - SEPTEMBER 23, 2025": "• Classic and Early Deep Learning Methods: This group includes foundational CNN architectures like"
        },
        {
          "A PREPRINT - SEPTEMBER 23, 2025": "AlexNet [8], as well as more powerful and deeper models such as ResNet-50 [9] and its variant ResNeXt-"
        },
        {
          "A PREPRINT - SEPTEMBER 23, 2025": "50 [9]. These models serve as strong, general-purpose vision baselines."
        },
        {
          "A PREPRINT - SEPTEMBER 23, 2025": "• Advanced Methods and State-of-the-Art: This category includes recent and highly specialized models"
        },
        {
          "A PREPRINT - SEPTEMBER 23, 2025": "designed specifically for FBP. These methods incorporate more complex mechanisms such as spatial and"
        },
        {
          "A PREPRINT - SEPTEMBER 23, 2025": "channel-wise attention (CNN + SCA [31]), label distribution learning (CNN + LDL [32]), dynamic attentive"
        },
        {
          "A PREPRINT - SEPTEMBER 23, 2025": "convolutions (DyAttenConv [33]), and the previous state-of-the-art model, R3CNN [34], which uniquely"
        },
        {
          "A PREPRINT - SEPTEMBER 23, 2025": "integrates relative ranking into the learning objective. A strong performance against this group demonstrates a"
        },
        {
          "A PREPRINT - SEPTEMBER 23, 2025": "true advancement in the field."
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "integrates relative ranking into the learning objective. A strong performance against this group demonstrates a": "true advancement in the field."
        },
        {
          "integrates relative ranking into the learning objective. A strong performance against this group demonstrates a": "4.4\nImplementation and Hyperparameter Details"
        },
        {
          "integrates relative ranking into the learning objective. A strong performance against this group demonstrates a": "Reproducibility is paramount\nto our experimental design. Our MD-Net model was trained following the protocol"
        },
        {
          "integrates relative ranking into the learning objective. A strong performance against this group demonstrates a": "described in Algorithm 1. We fine-tuned only the trainable parameters:\nthe Vision Mamba stream, the cross-attention"
        },
        {
          "integrates relative ranking into the learning objective. A strong performance against this group demonstrates a": "fusion module, and the final MLP regression head. Crucially,\nthe weights of the Stable Diffusion U-Net encoder"
        },
        {
          "integrates relative ranking into the learning objective. A strong performance against this group demonstrates a": "remained frozen throughout training to preserve its powerful generative prior. Automatic Mixed Precision (AMP) was"
        },
        {
          "integrates relative ranking into the learning objective. A strong performance against this group demonstrates a": "enabled to accelerate training and reduce the GPU memory footprint, allowing for a larger batch size. All specific"
        },
        {
          "integrates relative ranking into the learning objective. A strong performance against this group demonstrates a": "hyperparameters are detailed in Table 1."
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Table 1: Key hyperparameters used for training MD-Net.": "Hyperparameter"
        },
        {
          "Table 1: Key hyperparameters used for training MD-Net.": "Optimizer"
        },
        {
          "Table 1: Key hyperparameters used for training MD-Net.": "Base Learning Rate (η)"
        },
        {
          "Table 1: Key hyperparameters used for training MD-Net.": "Weight Decay (λ)"
        },
        {
          "Table 1: Key hyperparameters used for training MD-Net.": "Learning Rate Scheduler"
        },
        {
          "Table 1: Key hyperparameters used for training MD-Net.": "Batch Size"
        },
        {
          "Table 1: Key hyperparameters used for training MD-Net.": "Training Epochs (E)"
        },
        {
          "Table 1: Key hyperparameters used for training MD-Net.": "Image Size"
        },
        {
          "Table 1: Key hyperparameters used for training MD-Net.": "Loss Function"
        },
        {
          "Table 1: Key hyperparameters used for training MD-Net.": "Software Environment"
        }
      ],
      "page": 9
    },
    {
      "caption": "Table 3: Ablation study of MD-Net’s components. Each component provides a significant and complementary",
      "data": [
        {
          "Table 2: Comparison with SOTA methods on the SCUT-FBP5500 dataset. Our proposed method is shown in bold. (↑": "indicates higher is better, ↓ indicates lower is better)."
        },
        {
          "Table 2: Comparison with SOTA methods on the SCUT-FBP5500 dataset. Our proposed method is shown in bold. (↑": "Category"
        },
        {
          "Table 2: Comparison with SOTA methods on the SCUT-FBP5500 dataset. Our proposed method is shown in bold. (↑": ""
        },
        {
          "Table 2: Comparison with SOTA methods on the SCUT-FBP5500 dataset. Our proposed method is shown in bold. (↑": ""
        },
        {
          "Table 2: Comparison with SOTA methods on the SCUT-FBP5500 dataset. Our proposed method is shown in bold. (↑": ""
        },
        {
          "Table 2: Comparison with SOTA methods on the SCUT-FBP5500 dataset. Our proposed method is shown in bold. (↑": ""
        },
        {
          "Table 2: Comparison with SOTA methods on the SCUT-FBP5500 dataset. Our proposed method is shown in bold. (↑": ""
        },
        {
          "Table 2: Comparison with SOTA methods on the SCUT-FBP5500 dataset. Our proposed method is shown in bold. (↑": ""
        },
        {
          "Table 2: Comparison with SOTA methods on the SCUT-FBP5500 dataset. Our proposed method is shown in bold. (↑": ""
        },
        {
          "Table 2: Comparison with SOTA methods on the SCUT-FBP5500 dataset. Our proposed method is shown in bold. (↑": ""
        },
        {
          "Table 2: Comparison with SOTA methods on the SCUT-FBP5500 dataset. Our proposed method is shown in bold. (↑": ""
        },
        {
          "Table 2: Comparison with SOTA methods on the SCUT-FBP5500 dataset. Our proposed method is shown in bold. (↑": ""
        },
        {
          "Table 2: Comparison with SOTA methods on the SCUT-FBP5500 dataset. Our proposed method is shown in bold. (↑": ""
        }
      ],
      "page": 10
    },
    {
      "caption": "Table 3: Ablation study of MD-Net’s components. Each component provides a significant and complementary",
      "data": [
        {
          "Table 3: Ablation study of MD-Net’s components.": "contribution to the model’s overall performance.",
          "Each component provides a significant and complementary": ""
        },
        {
          "Table 3: Ablation study of MD-Net’s components.": "",
          "Each component provides a significant and complementary": "PC ↑"
        },
        {
          "Table 3: Ablation study of MD-Net’s components.": "(A)",
          "Each component provides a significant and complementary": "0.9235"
        },
        {
          "Table 3: Ablation study of MD-Net’s components.": "",
          "Each component provides a significant and complementary": ""
        },
        {
          "Table 3: Ablation study of MD-Net’s components.": "(B)",
          "Each component provides a significant and complementary": "0.9081"
        },
        {
          "Table 3: Ablation study of MD-Net’s components.": "(C)",
          "Each component provides a significant and complementary": "0.9023"
        },
        {
          "Table 3: Ablation study of MD-Net’s components.": "",
          "Each component provides a significant and complementary": ""
        },
        {
          "Table 3: Ablation study of MD-Net’s components.": "(D)",
          "Each component provides a significant and complementary": "0.9126"
        }
      ],
      "page": 10
    },
    {
      "caption": "Table 3: Ablation study of MD-Net’s components. Each component provides a significant and complementary",
      "data": [
        {
          "The key insights drawn from this study are:": ""
        },
        {
          "The key insights drawn from this study are:": ""
        },
        {
          "The key insights drawn from this study are:": ""
        },
        {
          "The key insights drawn from this study are:": ""
        },
        {
          "The key insights drawn from this study are:": ""
        },
        {
          "The key insights drawn from this study are:": ""
        },
        {
          "The key insights drawn from this study are:": ""
        },
        {
          "The key insights drawn from this study are:": ""
        },
        {
          "The key insights drawn from this study are:": "•"
        },
        {
          "The key insights drawn from this study are:": ""
        },
        {
          "The key insights drawn from this study are:": ""
        },
        {
          "The key insights drawn from this study are:": ""
        },
        {
          "The key insights drawn from this study are:": ""
        },
        {
          "The key insights drawn from this study are:": ""
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "5\nDiscussion": "The empirical results presented in Section 4 strongly validate the architectural design of the Mamba-Diffusion Network"
        },
        {
          "5\nDiscussion": "(MD-Net) and its underlying principles. The state-of-the-art performance is not merely an incremental improvement but"
        },
        {
          "5\nDiscussion": "rather the result of a paradigm shift from monolithic architectures to a synergistic, multi-paradigm system. We attribute"
        },
        {
          "5\nDiscussion": "the success of MD-Net to three primary factors."
        },
        {
          "5\nDiscussion": "First, the power of generative priors for aesthetic assessment. The exceptional performance of the Diffusion-only"
        },
        {
          "5\nDiscussion": "model\nin our ablation study (Table 3, row B), which surpassed the strong ViT-Base baseline, provides compelling"
        },
        {
          "5\nDiscussion": "evidence for our core hypothesis. The U-Net encoder of a latent diffusion model, trained on a denoising-reconstruction"
        },
        {
          "5\nDiscussion": "objective, learns a representation that is deeply sensitive to the fine-grained details that constitute visual quality—texture"
        },
        {
          "5\nDiscussion": "fidelity, lighting coherence, sharpness, and subtle color gradients. This \"aesthetic prior\" appears to be fundamentally"
        },
        {
          "5\nDiscussion": "more aligned with the FBP task than the class-discriminative features learned by models trained on classification tasks"
        },
        {
          "5\nDiscussion": "like ImageNet."
        },
        {
          "5\nDiscussion": "Second, the necessity of efficient global context modeling. While the diffusion prior provides a powerful signal for"
        },
        {
          "5\nDiscussion": "local quality, the ablation results clearly show that it is insufficient on its own. The significant performance leap of the"
        },
        {
          "5\nDiscussion": "full MD-Net over the Diffusion-only variant underscores the criticality of global facial structure. Facial beauty is not"
        },
        {
          "5\nDiscussion": "judged on texture alone but on the harmonious interplay of proportions, symmetry, and the geometric arrangement of"
        },
        {
          "5\nDiscussion": "features. The Vision Mamba stream, with its linear-time complexity and aptitude for modeling long-range dependencies,"
        },
        {
          "5\nDiscussion": "efficiently captures this holistic context. The synergy is evident:\nthe model learns to evaluate local details within the"
        },
        {
          "5\nDiscussion": "framework of the global structure."
        },
        {
          "5\nDiscussion": "Third, the efficacy of intelligent feature fusion. The superiority of cross-attention over simple feature concatenation"
        },
        {
          "5\nDiscussion": "(Table 3, row D) highlights the importance of how the two streams communicate. Cross-attention provides a mechanism"
        },
        {
          "5\nDiscussion": "for dynamic, context-aware integration. The global representation (from Mamba) can selectively \"query\" the multi-scale"
        },
        {
          "5\nDiscussion": "local feature maps (from the diffusion encoder), allowing the model to focus on the most salient aesthetic details given a"
        },
        {
          "5\nDiscussion": "particular facial structure. This is a more powerful and nuanced fusion strategy than simply combining two independent"
        },
        {
          "5\nDiscussion": "feature vectors."
        },
        {
          "5\nDiscussion": "Beyond the specific task of FBP, our findings suggest a broader implication for computer vision:\ncomplex visual"
        },
        {
          "5\nDiscussion": "recognition tasks that depend on both micro-level details and macro-level composition may benefit significantly from"
        },
        {
          "5\nDiscussion": "hybrid architectures that delegate these specialized roles to the most suitable models, such as combining generative and"
        },
        {
          "5\nDiscussion": "sequential modeling paradigms."
        },
        {
          "5\nDiscussion": "5.1\nLimitations"
        },
        {
          "5\nDiscussion": "Despite the strong performance, it is crucial to acknowledge the limitations of this work."
        }
      ],
      "page": 11
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Despite the strong performance, it is crucial to acknowledge the limitations of this work.": "• Dataset and Annotation Bias: The foremost\nlimitation is the model’s reliance on the FBP5500 dataset."
        },
        {
          "Despite the strong performance, it is crucial to acknowledge the limitations of this work.": "The concept of \"beauty\" learned by MD-Net\nis a proxy for\nthe consensus of\nthe 60 annotators for\nthis"
        },
        {
          "Despite the strong performance, it is crucial to acknowledge the limitations of this work.": "specific dataset. These annotations are subject to demographic, cultural, and individual biases. Therefore, our"
        },
        {
          "Despite the strong performance, it is crucial to acknowledge the limitations of this work.": "model’s predictions do not represent an objective or universal measure of beauty but rather reflect the specific"
        },
        {
          "Despite the strong performance, it is crucial to acknowledge the limitations of this work.": "distribution and preferences inherent in its training data."
        },
        {
          "Despite the strong performance, it is crucial to acknowledge the limitations of this work.": "•\nInterpretability Challenges: While our dual-stream design is motivated by an intuitive separation of concerns"
        },
        {
          "Despite the strong performance, it is crucial to acknowledge the limitations of this work.": "(local vs. global), the model remains a \"black box\" to a significant extent. The complex, non-linear interactions"
        },
        {
          "Despite the strong performance, it is crucial to acknowledge the limitations of this work.": "within the Mamba blocks and the cross-attention module make it difficult to definitively isolate and prove"
        },
        {
          "Despite the strong performance, it is crucial to acknowledge the limitations of this work.": "which specific facial attributes (e.g., \"symmetry,\" \"skin clarity\") are being measured by each component."
        },
        {
          "Despite the strong performance, it is crucial to acknowledge the limitations of this work.": "• Computational Complexity: MD-Net is a large and computationally intensive model. Its dual-stream nature,"
        },
        {
          "Despite the strong performance, it is crucial to acknowledge the limitations of this work.": "particularly the inclusion of\nthe large U-Net encoder, makes both training and inference more resource-"
        },
        {
          "Despite the strong performance, it is crucial to acknowledge the limitations of this work.": "demanding compared to a standard ResNet-50. This presents a practical barrier to deployment in resource-"
        },
        {
          "Despite the strong performance, it is crucial to acknowledge the limitations of this work.": "constrained environments."
        }
      ],
      "page": 11
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "A PREPRINT - SEPTEMBER 23, 2025": "• Fairness and Bias Mitigation: Directly addressing the dataset bias is a critical next step. Future work should"
        },
        {
          "A PREPRINT - SEPTEMBER 23, 2025": "focus on training and evaluating models on more diverse, cross-cultural datasets. Furthermore, employing"
        },
        {
          "A PREPRINT - SEPTEMBER 23, 2025": "fairness-aware machine learning techniques to quantify and mitigate biases across different demographic"
        },
        {
          "A PREPRINT - SEPTEMBER 23, 2025": "subgroups (e.g., ethnicity, gender) is essential for developing more equitable and responsible models."
        },
        {
          "A PREPRINT - SEPTEMBER 23, 2025": "• Enhancing Model Interpretability: To move beyond our architectural intuition, future research could apply"
        },
        {
          "A PREPRINT - SEPTEMBER 23, 2025": "advanced explainability techniques. Methods such as Concept Activation Vectors (CAV) could be used to probe"
        },
        {
          "A PREPRINT - SEPTEMBER 23, 2025": "whether the Mamba stream is indeed learning concepts like \"symmetry,\" or visualizing the cross-attention"
        },
        {
          "A PREPRINT - SEPTEMBER 23, 2025": "maps could reveal which fine-grained features are prioritized for different global structures."
        },
        {
          "A PREPRINT - SEPTEMBER 23, 2025": "• Model Compression and Efficiency: To address the computational cost, future work could explore model"
        },
        {
          "A PREPRINT - SEPTEMBER 23, 2025": "compression techniques. Knowledge distillation, where the large MD-Net acts as a \"teacher\" to train a much"
        },
        {
          "A PREPRINT - SEPTEMBER 23, 2025": "smaller, faster \"student\" model (e.g., a MobileNet), could create a lightweight version that retains most of the"
        },
        {
          "A PREPRINT - SEPTEMBER 23, 2025": "performance. Quantization and pruning are also viable avenues for creating more efficient versions of the"
        },
        {
          "A PREPRINT - SEPTEMBER 23, 2025": "model."
        },
        {
          "A PREPRINT - SEPTEMBER 23, 2025": "• Generalization to Other Aesthetic Domains: The core principle of MD-Net—fusing a generative prior for"
        },
        {
          "A PREPRINT - SEPTEMBER 23, 2025": "local quality with a sequential model for global structure—is highly generalizable. We plan to investigate the"
        },
        {
          "A PREPRINT - SEPTEMBER 23, 2025": "application of this architecture to other subjective, aesthetic assessment tasks, such as predicting the aesthetic"
        },
        {
          "A PREPRINT - SEPTEMBER 23, 2025": "quality of photography, art, and user interface design."
        }
      ],
      "page": 12
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "6\nConclusion": "In this paper, we addressed the inherent architectural"
        },
        {
          "6\nConclusion": "modeling in the complex task of Facial Beauty Prediction (FBP). We challenged the prevailing reliance on monolithic"
        },
        {
          "6\nConclusion": "architectures by proposing the Mamba-Diffusion Network (MD-Net), a novel, dual-stream hybrid model designed for"
        },
        {
          "6\nConclusion": "synergistic feature representation. Our central thesis was that the nuanced human perception of beauty, which relies"
        },
        {
          "6\nConclusion": "on both fine-grained aesthetic details and holistic facial harmony, is best replicated by a system that delegates these"
        },
        {
          "6\nConclusion": "specialized roles to the most suitable modern architectures."
        },
        {
          "6\nConclusion": "Our proposed MD-Net successfully operationalized this principle by integrating two powerful and complementary"
        },
        {
          "6\nConclusion": "components:"
        }
      ],
      "page": 12
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "A PREPRINT - SEPTEMBER 23, 2025": "[2] Djamel Eddine Boukhari,et al. \"A comprehensive review of facial beauty prediction using deep learning techniques.\""
        },
        {
          "A PREPRINT - SEPTEMBER 23, 2025": "Engineering Applications of Artificial Intelligence 161 (2025): 112009."
        },
        {
          "A PREPRINT - SEPTEMBER 23, 2025": "[3] H. Knight and O. Keith, “Ranking facial attractiveness,” The European Journal of Orthodontics, vol. 27, no. 4 pp."
        },
        {
          "A PREPRINT - SEPTEMBER 23, 2025": "340-348, 2005."
        },
        {
          "A PREPRINT - SEPTEMBER 23, 2025": "[4] D. E. Boukhari, A. Chemsa, R. Ajgou, et al., An Ensemble of Deep Convolutional Neural Networks Models for"
        },
        {
          "A PREPRINT - SEPTEMBER 23, 2025": "Facial Beauty Prediction, Journal of Advanced Computational Intelligence and Intelligent Informatics, vol. 27 no."
        },
        {
          "A PREPRINT - SEPTEMBER 23, 2025": "5. 2023."
        },
        {
          "A PREPRINT - SEPTEMBER 23, 2025": "[5] Rossetti, Alberto, et al. \"The role of\nthe golden proportion in the evaluation of\nfacial esthetics.\" The Angle"
        },
        {
          "A PREPRINT - SEPTEMBER 23, 2025": "Orthodontist 83.5 (2013): 801-808."
        },
        {
          "A PREPRINT - SEPTEMBER 23, 2025": "[6] Schmid, Kendra, David Marx, and Ashok Samal. \"Computation of a face attractiveness index based on neoclassical"
        },
        {
          "A PREPRINT - SEPTEMBER 23, 2025": "canons, symmetry, and golden ratios.\" Pattern Recognition 41.8 (2008): 2710-2717."
        },
        {
          "A PREPRINT - SEPTEMBER 23, 2025": "and Nash,\nR.\n(2015). An\nintroduction\nto\nconvolutional\nneural\nnetworks.\narXiv\npreprint\n[7] O’shea, K.,"
        },
        {
          "A PREPRINT - SEPTEMBER 23, 2025": "arXiv:1511.08458."
        },
        {
          "A PREPRINT - SEPTEMBER 23, 2025": "[8] Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E. Hinton. \"Imagenet classification with deep convolutional neural"
        },
        {
          "A PREPRINT - SEPTEMBER 23, 2025": "networks.\" Advances in neural information processing systems 25 (2012)."
        },
        {
          "A PREPRINT - SEPTEMBER 23, 2025": "[9] K. He, X. Zhang, S. Ren et al., Deep residual learning for image recognition. IEEE Conference on Computer Vision"
        },
        {
          "A PREPRINT - SEPTEMBER 23, 2025": "and Pattern Recognition (CVPR), Las Vegas, NV, USA, pp. 770-778, 2016."
        },
        {
          "A PREPRINT - SEPTEMBER 23, 2025": "[10] Boukhari, Djamel Eddine, and Ali Chemsa. \"SCAT: The Self-Correcting Aesthetic Transformer for Explainable"
        },
        {
          "A PREPRINT - SEPTEMBER 23, 2025": "Facial Beauty Prediction.\" (2025)."
        },
        {
          "A PREPRINT - SEPTEMBER 23, 2025": "[11] Boukhari, Djamel Eddine, and Ali Chemsa. \"An Uncertainty-Aware and Explainable Deep Learning Model for"
        },
        {
          "A PREPRINT - SEPTEMBER 23, 2025": "Facial Beauty Prediction.\" (2025)."
        },
        {
          "A PREPRINT - SEPTEMBER 23, 2025": "[12] Djamel Eddine Boukhari, et al. \"Facial Beauty Prediction Using an Ensemble of Deep Convolutional Neural"
        },
        {
          "A PREPRINT - SEPTEMBER 23, 2025": "Networks.\" Engineering Proceedings 56.1 (2023): 125."
        },
        {
          "A PREPRINT - SEPTEMBER 23, 2025": "[13] Dosovitskiy, Alexey, et al. \"An image is worth 16x16 words: Transformers for image recognition at scale.\" arXiv"
        },
        {
          "A PREPRINT - SEPTEMBER 23, 2025": "preprint arXiv:2010.11929 (2020)."
        },
        {
          "A PREPRINT - SEPTEMBER 23, 2025": "[14] D. Eddine Boukhari, A. Chemsa and Z.\n-E. Baarir, \"Facial Beauty Prediction Using Global Context Vision"
        },
        {
          "A PREPRINT - SEPTEMBER 23, 2025": "Transformer,\" 2025 International Symposium on iNnovative Informatics of Biskra (ISNIB), Biskra, Algeria, 2025."
        },
        {
          "A PREPRINT - SEPTEMBER 23, 2025": "[15] Boukhari, Djamel Eddine. \"Scale-interaction transformer:\na hybrid cnn-transformer model for facial beauty"
        },
        {
          "A PREPRINT - SEPTEMBER 23, 2025": "prediction.\" arXiv preprint arXiv:2509.05078 (2025)."
        },
        {
          "A PREPRINT - SEPTEMBER 23, 2025": "[16] Boukhari, Djamel Eddine, Ali Chemsa, and Zine-Eddine Baarir. \"MobileViT architecture for Facial Beauty"
        },
        {
          "A PREPRINT - SEPTEMBER 23, 2025": "Prediction.\" 2024 International Conference on Telecommunications and Intelligent Systems (ICTIS). IEEE, 2024."
        },
        {
          "A PREPRINT - SEPTEMBER 23, 2025": "[17] A Kagian, G Dror, T Leyvand, et al., A machine learning predictor of facial attractiveness revealing human-like"
        },
        {
          "A PREPRINT - SEPTEMBER 23, 2025": "psychophysical biases. Vision research, vol 48, no 2, pp. 235-243, 2008."
        },
        {
          "A PREPRINT - SEPTEMBER 23, 2025": "[18]\nI Lebedeva,Y Guo and F Ying. Transfer learning adaptive facial attractiveness assessment. Journal of Physics:"
        },
        {
          "A PREPRINT - SEPTEMBER 23, 2025": "Conference Series. vol. 1922, no. 1, 2021."
        },
        {
          "A PREPRINT - SEPTEMBER 23, 2025": "[19] Rombach, Robin, et al. \"High-resolution image synthesis with latent diffusion models.\" Proceedings of\nthe"
        },
        {
          "A PREPRINT - SEPTEMBER 23, 2025": "IEEE/CVF conference on computer vision and pattern recognition. 2022."
        },
        {
          "A PREPRINT - SEPTEMBER 23, 2025": "[20] Boukhari, Djamel Eddine. \"Generative Pre-training for Subjective Tasks: A Diffusion Transformer-Based Frame-"
        },
        {
          "A PREPRINT - SEPTEMBER 23, 2025": "work for Facial Beauty Prediction.\" arXiv preprint arXiv:2507.20363 (2025)."
        },
        {
          "A PREPRINT - SEPTEMBER 23, 2025": "[21] Gu, Albert, and Tri Dao. \"Mamba: Linear-time sequence modeling with selective state spaces.\" arXiv preprint"
        },
        {
          "A PREPRINT - SEPTEMBER 23, 2025": "arXiv:2312.00752 (2023)."
        },
        {
          "A PREPRINT - SEPTEMBER 23, 2025": "[22] Zhu, Lianghui, et al. \"Vision mamba: Efficient visual representation learning with bidirectional state space model.\""
        },
        {
          "A PREPRINT - SEPTEMBER 23, 2025": "arXiv preprint arXiv:2401.09417 (2024)."
        },
        {
          "A PREPRINT - SEPTEMBER 23, 2025": "[23] Boukhari, Djamel Eddine. \"Mamba-CNN: A Hybrid Architecture for Efficient and Accurate Facial Beauty"
        },
        {
          "A PREPRINT - SEPTEMBER 23, 2025": "Prediction.\" arXiv preprint arXiv:2509.01431 (2025)."
        },
        {
          "A PREPRINT - SEPTEMBER 23, 2025": "[24] L. Liang, L. Lin, L. Jin et al., SCUT-FBP5500: A diverse benchmark dataset for multi-paradigm facial beauty"
        },
        {
          "A PREPRINT - SEPTEMBER 23, 2025": "prediction. 24th International Conference on Pattern Recognition (ICPR), Beijing, China, pp. 1598-1603, 2018."
        },
        {
          "A PREPRINT - SEPTEMBER 23, 2025": "[25] Loshchilov, Ilya, and Frank Hutter. \"Decoupled weight decay regularization.\" arXiv preprint arXiv:1711.05101"
        },
        {
          "A PREPRINT - SEPTEMBER 23, 2025": "(2017)."
        },
        {
          "A PREPRINT - SEPTEMBER 23, 2025": "[26] Djamel Eddine Boukhari, Ali Chemsa, and Riadh Ajgou. \"Facial Beauty Prediction Based on Vision Transformer.\""
        },
        {
          "A PREPRINT - SEPTEMBER 23, 2025": "International Journal of Electrical and Electronic Engineering and Telecommunications, ISSN (2023): 2319-2518."
        }
      ],
      "page": 13
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "A PREPRINT - SEPTEMBER 23, 2025": ""
        },
        {
          "A PREPRINT - SEPTEMBER 23, 2025": "CAAI Transactions on Intelligence Technology, pp. 1–14, 2023."
        },
        {
          "A PREPRINT - SEPTEMBER 23, 2025": "I Lebedeva, F Ying, and Y Guo. Personalized facial beauty assessment: a meta-learning approach. The Visual"
        },
        {
          "A PREPRINT - SEPTEMBER 23, 2025": "Computer: International Journal of Computer Graphics, Vol. 39, no. 3,pp. 1095–1107, 2023."
        },
        {
          "A PREPRINT - SEPTEMBER 23, 2025": ""
        },
        {
          "A PREPRINT - SEPTEMBER 23, 2025": "International Conference on Acoustics, Speech and Signal Processing (ICASSP), Brighton, UK, pp. 4045-4049,"
        },
        {
          "A PREPRINT - SEPTEMBER 23, 2025": ""
        },
        {
          "A PREPRINT - SEPTEMBER 23, 2025": "J Gan, L Xiang, Y Zhai, et al., 2M BeautyNet: Facial beauty prediction based on multi-task transfer learning. EEE"
        },
        {
          "A PREPRINT - SEPTEMBER 23, 2025": "Access, vol. 8, pp. 20245-20256, 2020."
        },
        {
          "A PREPRINT - SEPTEMBER 23, 2025": ""
        },
        {
          "A PREPRINT - SEPTEMBER 23, 2025": ""
        },
        {
          "A PREPRINT - SEPTEMBER 23, 2025": "Transactions on Multimedia 20.8 (2017): 2196-2208."
        },
        {
          "A PREPRINT - SEPTEMBER 23, 2025": ""
        },
        {
          "A PREPRINT - SEPTEMBER 23, 2025": "Information and Systems 107.2 (2024): 239-243."
        },
        {
          "A PREPRINT - SEPTEMBER 23, 2025": ""
        },
        {
          "A PREPRINT - SEPTEMBER 23, 2025": "for Facial Beauty Prediction. IEEE Trans. Affect. Comput. 2019, 1."
        },
        {
          "A PREPRINT - SEPTEMBER 23, 2025": ""
        }
      ],
      "page": 14
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Computer Models for Facial Beauty Analysis",
      "authors": [
        "D Zhang",
        "F Chen",
        "Y Xu"
      ],
      "year": "2016",
      "venue": "Computer Models for Facial Beauty Analysis"
    },
    {
      "citation_id": "2",
      "title": "A comprehensive review of facial beauty prediction using deep learning techniques",
      "authors": [
        "Djamel Eddine Boukhari"
      ],
      "year": "2025",
      "venue": "Engineering Applications of Artificial Intelligence"
    },
    {
      "citation_id": "3",
      "title": "Ranking facial attractiveness",
      "authors": [
        "H Knight",
        "O Keith"
      ],
      "year": "2005",
      "venue": "The European Journal of Orthodontics"
    },
    {
      "citation_id": "4",
      "title": "An Ensemble of Deep Convolutional Neural Networks Models for Facial Beauty Prediction",
      "authors": [
        "D Boukhari",
        "A Chemsa",
        "R Ajgou"
      ],
      "year": "2023",
      "venue": "Journal of Advanced Computational Intelligence and Intelligent Informatics"
    },
    {
      "citation_id": "5",
      "title": "The role of the golden proportion in the evaluation of facial esthetics",
      "authors": [
        "Alberto Rossetti"
      ],
      "year": "2013",
      "venue": "The Angle Orthodontist"
    },
    {
      "citation_id": "6",
      "title": "Computation of a face attractiveness index based on neoclassical canons, symmetry, and golden ratios",
      "authors": [
        "Kendra Schmid",
        "David Marx",
        "Ashok Samal"
      ],
      "year": "2008",
      "venue": "Pattern Recognition"
    },
    {
      "citation_id": "7",
      "title": "An introduction to convolutional neural networks",
      "authors": [
        "K O'shea",
        "R Nash"
      ],
      "year": "2015",
      "venue": "An introduction to convolutional neural networks",
      "arxiv": "arXiv:1511.08458"
    },
    {
      "citation_id": "8",
      "title": "Imagenet classification with deep convolutional neural networks",
      "authors": [
        "Alex Krizhevsky",
        "Ilya Sutskever",
        "Geoffrey Hinton"
      ],
      "year": "2012",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "9",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "K He",
        "X Zhang",
        "S Ren"
      ],
      "year": "2016",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "10",
      "title": "SCAT: The Self-Correcting Aesthetic Transformer for Explainable Facial Beauty Prediction",
      "authors": [
        "Djamel Boukhari",
        "Ali Eddine",
        "Chemsa"
      ],
      "year": "2025",
      "venue": "SCAT: The Self-Correcting Aesthetic Transformer for Explainable Facial Beauty Prediction"
    },
    {
      "citation_id": "11",
      "title": "An Uncertainty-Aware and Explainable Deep Learning Model for Facial Beauty Prediction",
      "authors": [
        "Djamel Boukhari",
        "Ali Eddine",
        "Chemsa"
      ],
      "year": "2025",
      "venue": "An Uncertainty-Aware and Explainable Deep Learning Model for Facial Beauty Prediction"
    },
    {
      "citation_id": "12",
      "title": "Facial Beauty Prediction Using an Ensemble of Deep Convolutional Neural Networks",
      "authors": [
        "Djamel Eddine Boukhari"
      ],
      "year": "2023",
      "venue": "Engineering Proceedings"
    },
    {
      "citation_id": "13",
      "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
      "authors": [
        "Alexey Dosovitskiy"
      ],
      "year": "2020",
      "venue": "An image is worth 16x16 words: Transformers for image recognition at scale",
      "arxiv": "arXiv:2010.11929"
    },
    {
      "citation_id": "14",
      "title": "Facial Beauty Prediction Using Global Context Vision Transformer",
      "authors": [
        "D Boukhari",
        "A Chemsa",
        "Z. -E Baarir"
      ],
      "year": "2025",
      "venue": "2025 International Symposium on iNnovative Informatics of Biskra (ISNIB)"
    },
    {
      "citation_id": "15",
      "title": "Scale-interaction transformer: a hybrid cnn-transformer model for facial beauty prediction",
      "authors": [
        "Djamel Boukhari",
        "Eddine"
      ],
      "year": "2025",
      "venue": "Scale-interaction transformer: a hybrid cnn-transformer model for facial beauty prediction",
      "arxiv": "arXiv:2509.05078"
    },
    {
      "citation_id": "16",
      "title": "MobileViT architecture for Facial Beauty Prediction",
      "authors": [
        "Djamel Boukhari",
        "Ali Eddine",
        "Zine-Eddine Chemsa",
        "Baarir"
      ],
      "year": "2024",
      "venue": "2024 International Conference on Telecommunications and Intelligent Systems (ICTIS)"
    },
    {
      "citation_id": "17",
      "title": "A machine learning predictor of facial attractiveness revealing human-like psychophysical biases",
      "authors": [
        "Kagian",
        "Dror",
        "Leyvand"
      ],
      "year": "2008",
      "venue": "Vision research"
    },
    {
      "citation_id": "18",
      "title": "Transfer learning adaptive facial attractiveness assessment",
      "authors": [
        "Y Lebedeva",
        "F Guo",
        "Ying"
      ],
      "year": "2021",
      "venue": "Journal of Physics: Conference Series"
    },
    {
      "citation_id": "19",
      "title": "High-resolution image synthesis with latent diffusion models",
      "authors": [
        "Robin Rombach"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "20",
      "title": "Generative Pre-training for Subjective Tasks: A Diffusion Transformer-Based Framework for Facial Beauty Prediction",
      "authors": [
        "Djamel Boukhari",
        "Eddine"
      ],
      "year": "2025",
      "venue": "Generative Pre-training for Subjective Tasks: A Diffusion Transformer-Based Framework for Facial Beauty Prediction",
      "arxiv": "arXiv:2507.20363"
    },
    {
      "citation_id": "21",
      "title": "Mamba: Linear-time sequence modeling with selective state spaces",
      "authors": [
        "Albert Gu",
        "Tri Dao"
      ],
      "year": "2023",
      "venue": "Mamba: Linear-time sequence modeling with selective state spaces",
      "arxiv": "arXiv:2312.00752"
    },
    {
      "citation_id": "22",
      "title": "Vision mamba: Efficient visual representation learning with bidirectional state space model",
      "authors": [
        "Lianghui Zhu"
      ],
      "year": "2024",
      "venue": "Vision mamba: Efficient visual representation learning with bidirectional state space model",
      "arxiv": "arXiv:2401.09417"
    },
    {
      "citation_id": "23",
      "title": "Mamba-CNN: A Hybrid Architecture for Efficient and Accurate Facial Beauty Prediction",
      "authors": [
        "Djamel Boukhari",
        "Eddine"
      ],
      "year": "2025",
      "venue": "Mamba-CNN: A Hybrid Architecture for Efficient and Accurate Facial Beauty Prediction",
      "arxiv": "arXiv:2509.01431"
    },
    {
      "citation_id": "24",
      "title": "SCUT-FBP5500: A diverse benchmark dataset for multi-paradigm facial beauty prediction",
      "authors": [
        "L Liang",
        "L Lin",
        "L Jin"
      ],
      "year": "2018",
      "venue": "24th International Conference on Pattern Recognition (ICPR)"
    },
    {
      "citation_id": "25",
      "title": "Decoupled weight decay regularization",
      "authors": [
        "Ilya Loshchilov",
        "Frank Hutter"
      ],
      "year": "2017",
      "venue": "Decoupled weight decay regularization",
      "arxiv": "arXiv:1711.05101"
    },
    {
      "citation_id": "26",
      "title": "Facial Beauty Prediction Based on Vision Transformer",
      "authors": [
        "Eddine Djamel",
        "Ali Boukhari",
        "Riadh Chemsa",
        "Ajgou"
      ],
      "year": "2023",
      "venue": "International Journal of Electrical and Electronic Engineering and Telecommunications"
    },
    {
      "citation_id": "27",
      "title": "Geometric prior guided hybrid deep neural network for facial beauty analysis",
      "authors": [
        "T Peng",
        "M Li",
        "F Chen"
      ],
      "year": "2023",
      "venue": "CAAI Transactions on Intelligence Technology"
    },
    {
      "citation_id": "28",
      "title": "Personalized facial beauty assessment: a meta-learning approach. The Visual Computer",
      "authors": [
        "Lebedeva",
        "Y Ying",
        "Guo"
      ],
      "year": "2023",
      "venue": "International Journal of Computer Graphics"
    },
    {
      "citation_id": "29",
      "title": "Improving Facial Attractiveness Prediction via Co-attention Learning",
      "authors": [
        "S Shi",
        "F Gao",
        "X Meng"
      ],
      "year": "2019",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "30",
      "title": "2M BeautyNet: Facial beauty prediction based on multi-task transfer learning",
      "authors": [
        "L Gan",
        "Y Xiang",
        "Zhai"
      ],
      "year": "2020",
      "venue": "EEE Access"
    },
    {
      "citation_id": "31",
      "title": "Deep learning for facial beauty prediction",
      "authors": [
        "K Cao",
        "H Choi",
        "Jung"
      ],
      "year": "2020",
      "venue": "Information"
    },
    {
      "citation_id": "32",
      "title": "Label distribution-based facial attractiveness computation by deep residual learning",
      "authors": [
        "Yang-Yu Fan"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "33",
      "title": "Dynamic attentive convolution for facial beauty prediction",
      "authors": [
        "Zhishu Sun"
      ],
      "year": "2024",
      "venue": "IEICE TRANSACTIONS on Information and Systems"
    },
    {
      "citation_id": "34",
      "title": "Regression Guided by Relative Ranking Using Convolutional Neural Network (R3CNN) for Facial Beauty Prediction",
      "authors": [
        "L Lin",
        "L Liang",
        "L Jin"
      ],
      "year": "2019",
      "venue": "IEEE Trans. Affect. Comput"
    }
  ]
}