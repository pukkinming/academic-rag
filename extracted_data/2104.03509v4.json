{
  "paper_id": "2104.03509v4",
  "title": "Py-Feat: Python Facial Expression Analysis Toolbox",
  "published": "2021-04-08T04:52:21Z",
  "authors": [
    "Jin Hyun Cheong",
    "Eshin Jolly",
    "Tiankang Xie",
    "Sophie Byrne",
    "Matthew Kenney",
    "Luke J. Chang"
  ],
  "keywords": [
    "facial expressions",
    "affective computing",
    "computer vision",
    "affect",
    "emotion"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Studying facial expressions is a notoriously difficult endeavor. Recent advances in the field of affective computing have yielded impressive progress in automatically detecting facial expressions from pictures and videos. However, much of this work has yet to be widely disseminated in social science domains such as psychology. Current state of the art models require considerable domain expertise that is not traditionally incorporated into social science training programs. Furthermore, there is a notable absence of user-friendly and open-source software that provides a comprehensive set of tools and functions that support facial expression research. In this paper, we introduce Py-Feat, an open-source Python toolbox that provides support for detecting, preprocessing, analyzing, and visualizing facial expression data. Py-Feat makes it easy for domain experts to disseminate and benchmark computer vision models and also for end users to quickly process, analyze, and visualize face expression data. We hope this platform will facilitate increased use of facial expression data in human behavior research.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Introduction",
      "text": "Facial expressions can reveal insights into an individual's internal mental state and provide nonverbal channels to aid in interpersonal and cross-species communication  1, 2  . One of the main challenges to studying facial expressions has been arriving at a consensus understanding as to how to best represent and objectively measure expressions. The Facial Affect Coding System (FACS)  3  is one of the most popular systems to reliably  4  quantify the intensity of groups of facial muscles referred to as action units (AUs). However, extracting facial expression information using FACS coding can be a laborious and time-intensive process. Becoming a certified FACS coder requires 100 hours of training, and manual labeling is slow (e.g., one minute of video can take an hour  5  ) and inherently contains cultural biases and errors  6, 7  . Facial electromyography (EMG) provides one method to objectively record from a finite number of facial muscles at a high temporal resolution  8, 9  , but requires specialized recording equipment that restricts data collection to the laboratory and can visually obscure the face making it less ideal for social contexts.\n\nAutomated methods using techniques from computer vision have emerged as a promising approach to extract representations of facial expressions from pictures, videos, and depth cameras both inside and outside the laboratory. Participants can be untethered from cumbersome wires and can naturally engage in tasks such as watching a movie or having a conversation  [10] [11] [12] [13] [14]  . In addition to AUs, computer vision techniques have provided alternative embedding spaces to represent facial expressions such as facial landmarks  15  or lower dimensional latent representations  16  . These tools have a number of applications relevant to psychology such as predicting the intensity of emotions  [17] [18] [19] [20]  and other affective states such as pain  21, 22  , distinguishing between genuine and fake expressions  23  , detecting signs of depression 24   , inferring traits such as personality  [25] [26] [27]  or political orientations  28  , and predicting the development of interpersonal relationships  12, 14  . Though facial expression research has seen rapid growth in affective computing facilitated by recent advances in machine learning, adoption in fields outside the domain of computer science such as psychology has been surprisingly slow.\n\nIn our view, there are at least two specific barriers contributing to the slow adoption of automated methods in social science fields such as psychology. First, there is a relatively high barrier to entry to training and accessing state of the art models capable of quantifying facial expressions. This requires knowledge of computer vision techniques, neural network architectures, and access to large labeled datasets and computational infrastructure that include Graphics Processing Units (GPUs). Though there are impressive efforts to share high quality datasets  [29] [30] [31] [32] [33] [34] [35]  , there are still difficulties sharing this data involving participants' privacy, complicated end user agreements, expensive handling fees, contacting data curators, and finding affordable and stable long-term hosting solutions. Though hundreds of models have been developed to characterize facial expressions, no standards have emerged for disseminating these models to end users. These models are typically reported in conference proceedings, occasionally shared on open code repositories such as Github, and require considerable domain knowledge as they have been developed using a multitude of computer languages, rarely have documentation, and occasionally have restrictive licensing. Each model may require the data to be preprocessed in a specific way or rely on additional features (e.g., landmarks, predefined regions of interest). Because there are currently no generally agreed upon standards for training and benchmarking beyond data competitions (e.g., WIDER, 300W, FERA, etc), each model is typically trained on different datasets, which makes it difficult to benchmark the models using the same dataset to aid in the model selection process  17, 36  . Platforms such as paperswithcode.com are helping to standardize the dissemination and benchmarking of models, but sharing state of the art models has not yet become a norm in the field. Other domains such as natural language processing and reinforcement learning have begun to overcome this issue with a variety of high quality software platforms such as Stanza  37  , SpaCy, OpenAI Gym  38  , and HuggingFace.\n\nSecond, there is a notable lack of free open-source software to aid in detecting, preprocessing, analyzing, and visualizing facial expressions (Table  1 ). Commercial software options such as Affdex (Affectiva Inc) available through iMotions  39  and Noldus FaceReader 40 can be expensive, have limited functionality, and typically do not employ state of the art models  [41] [42] [43]  (see  17, 20  for commercial software performance comparisons). Furthermore, due to strong interest from industry, there have been several free software packages such as the Computer Expression Recognition Toolbox  44  , Intraface  15  , and Affectiva API  45  (Affectiva Inc) that have turned into commercial products or been acquired by larger technology companies such as Apple Inc or Meta and rendered unavailable to researchers. Currently, OpenFace  46  is the most widely used open-source software that allows users to extract facial landmarks and action units from face images and videos. However, OpenFace does not provide a comprehensive suite of tools for preprocessing, analyzing, and visualizing data, which would make these tools more accessible to non-domain experts. As an example, in other fields such as neuroscience, the rapid growth of neuroimaging research has been facilitated by the widespread use of free tools such as FSL  47  , AFNI  48  , SPM  49  , and NiLearn 50 that enables end users to preprocess, analyze, and visualize complex brain imaging data. We believe the broader emotion research community would greatly benefit from additional software platforms dedicated to facial expression analysis with functions for extracting, preprocessing, analyzing, and visualizing facial expression data.",
      "page_start": 3,
      "page_end": 19
    },
    {
      "section_name": "Facial Feature Detection",
      "text": "Preprocessing Analysis Free Facial landmarks Table  1 . Software comparison on functionalities and affordability. X indicates features provided by each package. Features from Py-Feat toolbox are shown in brackets. Facial landmarks are points pertaining to locations of key spatial positions of the face including the jaw, mouth, nose, eyes, and eyebrows. Action units are facial muscle groups defined by FACS  51  . Emotions refer to the detection of canonical emotional expressions. Headpose refers to the pitch, roll, and yaw orientations of the face. Gaze refers to the direction the eyes are looking. *iMotions is a platform and its feature extraction relies on the purchase of either the AFFDEX or FACET modules. **Detection of action units and analysis functionalities require a separate add-on purchase of The Action Unit Module and the Project Analysis Module for the Noldus FaceReader. ***We note that OpenFace can perform some preprocessing such as median face image subtraction and post-processing of AUs to correct for at-rest expressions.\n\nTo meet this need, we have created the Python Facial Expression Analysis Toolbox (Py-Feat) which is a free, open-source package dedicated to support the analysis of facial expression data. It provides tools to extract facial features like OpenFace  46  , but additionally provides modules for preprocessing, analyzing, and visualizing facial expression data (Figure  1 ). Py-Feat is designed to meet the needs of two distinct types of users. Py-Feat benefits computer vision researchers who can use our platform to disseminate their state of the art models to a broader audience and easily compare their models with others on the same benchmark metrics. It also benefits social science researchers looking for free and easy to use tools that can both detect and analyze facial expressions. In this paper, we outline the key components of the Py-Feat toolbox including the facial feature detection module and analysis tools, provide quantitative assessments of the performance of the detection models on benchmark data including the robustness of the models to real world data, and provide a tutorial of how the toolbox can be used to analyze an open face expression dataset.",
      "page_start": 20,
      "page_end": 20
    },
    {
      "section_name": "Figure 1. Facial Expressions Analysis Pipeline. Analysis Of Facial Expressions Begins With Recording Face Photos Or Videos Using A Recording Device Such As Webcams, Camcorders, Head Mounted Cameras, Or 360 Cameras. After Capturing The Face, Researchers Can Use Py-Feat To Detect Facial Features Such As The Location Of The Face Within A Rectangular Bounding Box, The Location Of Key Facial Landmarks, Action Units, And Emotions, And Check The Detection Results With Image Overlays And Bar Graphs. The Detection Results Can Be Preprocessed By Extracting Additional Features Such As Histogram Of Oriented Gradients (Hog) Or Multi-Wavelet Decomposition. Resulting Data Can Then Be Analyzed Within The Toolbox Using Statistical Methods Such As T-Tests, Regressions, And Intersubject Correlations. Visualization Functions Can Generate Face Images From Models Of Action Unit Activations To Show Vector Fields Depicting Landmark Movements And Heatmaps Of Facial Muscle Activations.",
      "text": "",
      "page_start": 20,
      "page_end": 20
    },
    {
      "section_name": "Py-Feat Design And Module Overview",
      "text": "Py-Feat is written in the Python programming language. We selected Python over other popular languages (e.g., Matlab, C, etc) for several reasons. First, Python is open source and completely free to use and compiles to all major operating systems (e.g., Mac, Windows, Unix). This makes the software accessible to the largest number of users. Second, Python is among the easiest programming languages to read and learn and is increasingly being taught in introduction to programming classes. Though we do not currently provide a graphical user interface (GUI) to Py-Feat, we believe it is highly easy to use with minimal background in programming (see our example code below). Third, Python has emerged as one of the primary languages used across academia and industry for data science. There is a vibrant developer community that has already created a rich library of tightly integrated high quality scientific computing packages for working with: arrays such as numpy  52  and pandas  53  ; scientific numerical routines with scipy  54  , machine learning algorithms with scikit-learn  55  , tensorflow  56  , and py-torch  57  ; and plotting with matplotlib  58  , seaborn  59  , and plotly. This makes it easy for Py-Feat to incorporate new functionality as it becomes available in other toolboxes, but also for Py-feat users to incorporate any Python package into other processing pipelines. Many of the core libraries are supported by big tech companies and are rapidly providing functionality to enable users to take advantage of newer innovations in hardware such as GPUs and distributed computing systems. In addition, Python libraries tend to have comprehensive documentation and testing and there are many excellent tutorials for learning how to use python online, which makes the language very accessible to beginners. For example, we have developed basic tutorials for learning to analyze data with Python on our DartBrains.org course  60  and more advanced tutorials on analyzing naturalistic neuroimaging data  61  . We have built a jupyter-book  62  to accompany our toolbox with tutorials on how to perform analyses that can be easily augmented by the user community (https://py-feat.org/). Py-Feat currently has two main modules for working with facial expression data. First, the Detector module makes it easy for users to detect facial expression features from image or video stimuli. We offer multiple models for extracting the primary face expression features that most end users will want to work with. This includes detecting faces in the stimuli and identifying the coordinates of the spatial location of a bounding box for each face. We also detect 68 facial landmarks, which are coordinates identifying the spatial location of the eyes, nose, mouth, and jaw. The bounding box and landmarks can be used in models to detect the head pose such as the face orientation in terms of rotation around axes in three-dimensional space. Py-Feat also detects higher level facial expression features such as AUs and basic emotion categories. We offer multiple models for each detector to keep the toolbox flexible for many use cases, but we also have picked sensible defaults for users who may be overwhelmed by the number of options. The features cover the majority of the ways in which facial expressions can be currently described by computer vision algorithms. Importantly, new features and models can be added to the toolbox as they become available in the field. The majority of the models in the toolbox are implemented in PyTorch  57  , which means they can also utilize Nvidia GPUs if they are available, which can dramatically speed up performance.\n\nIn addition, Py-feat also includes the Fex data module to work with the features extracted from the Detector module. This module includes methods for preprocessing, analyzing, and visualizing facial expression data. We offer an easy to use application programming interface (API) for slicing, grouping, sampling, and summarizing data as well as selecting different types of data (i.e., faceboxes, landmarks, action units, emotions, face poses), preprocessing facial expression time series data, extracting additional features from time series data, analyzing aggregates of facial expressions data, and visualizing intermediary preprocessing steps.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Py-Feat Performance",
      "text": "Computer vision models are highly complex and often employ completely different preprocessing steps and model architectures. All of the technical details about the architecture of each of the models and how they were trained can be found in the Supplementary Materials. To provide users with an estimate of how well these models are likely to perform on their own datasets, we report benchmark performance on datasets that were never used in training the models. Importantly, we primarily used benchmark datasets that are the standard for each domain in data competitions and include highly variable naturalistic images collected in the wild when possible.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Face Detection",
      "text": "One of the most basic steps in the facial feature detection process is to identify if there is a face in the image and where that face is located. Py-Feat includes three popular face detectors including Faceboxes 63 , Multi-task Convolutional Neural Network (MTCNN)  64, 65  , and RetinaFace",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "66",
      "text": ". These detectors are widely used in other open-source software  46  and are known to achieve fast and accurate face detection results even for partially occluded or non-frontal faces. Face detection results are reported as a rectangular bounding box of the face and includes a confidence score for each detected face. We benchmarked the face detection models on the validation set of the WIDER FACE dataset, which is a standard dataset containing images in the wild retrieved from the internet 67 , using average precision described in the WIDER Face technical paper  68  . Overall, we found that the Py-Feat implementations of each of the models achieved acceptable levels of performance, although lower than what was reported in the original papers 66 (Table  3 ). This may be a consequence of using different hyperparameters. We also observed decreased performance as the classification task becomes increasingly more difficult, which includes small, inverted, and highly occluded faces.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Landmark Detection",
      "text": "After a face is identified in an image, it is common to identify the facial landmarks, which are coordinate points in the image space outlining the jaw, mouth, nose, eyes, and eyebrows of a face. The distance and angular relationships between the landmarks can be used to represent face expressions and used to infer affective states such as pain  21  . Py-feat uses a standard 68-coordinate facial landmark scheme that is widely used across datasets and software  46, 69, 70  and currently includes three facial landmark detectors including the Practical Facial Landmark Detector (PFLD)  71  , MobileNets  72  , and MobileFaceNets 73 algorithms. We benchmarked these models on the 300 Faces in the Wild (300W) dataset  70, 74  , which is a standard used in data competitions and contains in-the-wild face images that vary across luminance, scale, pose, expressions and occlusion levels. We compute the average root mean squared error between the predicted and ground truth coordinates across the landmark points normalized by the interocular distance. Overall, we found that the Feat-MobileFaceNet performed the best on our benchmark.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Head Pose Detection",
      "text": "Another feature of a face expression beyond its location in an image or the location of specific parts of the face is the position of the head in three dimensional space. Rotations from a head on view can be described in terms of rotation around the x, y, and z planes and are referred to as pitch, roll, and yaw respectively. Py-feat includes support for the Img2Pose model. This model does not rely on prior face detections, so it can also be used as a face bounding box detector. The constrained version of Img2Pose is fine-tuned on the 300W-LP dataset, which only includes head poses in range (-90°to +90°). We benchmarked our head pose models using the BIWI Kinect dataset, which contains videos of participants rotating their heads according to specific pose instructions  75  . We computed the Mean Absolute Error in degrees for pitch, roll and yaw. Overall, we found that the constrained version of Img2Pose achieved a slightly better performance compared to the unconstrained version on our benchmark.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Model",
      "text": "",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Action Unit Detection",
      "text": "In addition to the basic properties of a face in an image, py-feat also includes models for detecting deviations of specific facial muscles (i.e., action units; AUs) from a neutral face expression using the FACS coding system. Py-feat currently contains two models for detecting action units. The architecture of the models are based on the highly robust and well-performing model used in OpenFace  46  , which extracts Histogram of Oriented Gradient (HOG) features from within the landmark coordinates using a convex hull algorithm, compresses the HOG representation using Principal Components Analysis (PCA), and finally uses these features to individually predict each of the 12 AUs using popular shallow learning methods based on kernels (i.e., linear Support Vector Machine; SVM  76  ), and ensemble learning (i.e., optimized gradient boosting; XGB 77 ) (see supplemental materials for training details). We compare the performance of our models to OpenFace and also FACET, which was previously available in iMotions before the company was acquired by Apple Inc. We benchmarked the AU detection models using the Extended DISFA Plus dataset  33  , which contains short videos of participants making posed facial expressions based on imitating a target image and also spontaneous facial expressions elicited from viewing experimental stimuli. We used F1 scores, an accuracy metric for binary classification, to quantify the performance of twelve different AUs. We found that the previously available FACET-iMotions achieved the best overall accuracy and was the best detector for AUs",
      "page_start": 9,
      "page_end": 10
    },
    {
      "section_name": "Emotion Detection",
      "text": "Finally, Py-feat also includes models for detecting the presence of specific emotion categories based on third party judgments. Emotion detectors are trained on manually posed or naturalistically elicited emotional facial expressions which allows detectors to classify new images based on how much a face resembles a canonical emotional facial expression. It is important to note that there is currently no consensus in the field if categorical representations of emotion are the most reliable and valid nosology of emotional facial expressions  78, 79  . For example, detecting a smiling face as happy does not necessarily imply that the individual is experiencing an internal subjective state of happiness  80  , as these types of latent state inferences require additional contextual information beyond a static image  81  . However, labeling specific configurations of AUs with the semantic concepts of emotions can still be useful in emotion research to characterize the contexts in which people tend to display these facial expressions or how the display of certain emotion expressions accompanies changes in learning  82  and social behaviors  14  . Py-feat includes two emotion detectors capable of detecting seven categories of emotions: anger, disgust, fear, happiness, sadness, surprise, and neutral. The Residual Masking Network (ResMaskNet)  83  is an end-to-end convolutional neural network model that combines deep residual networks with masking blocks. The masking blocks help focus the model's attention on local regions of interest to refine its feature map for more fine-grained predictions and the residual structure helps to maintain performances in deeper layers. We also provide a statistical learning model that uses Linear SVM  76  using a similar procedure as our AU models. We benchmarked our models using F1 scores on a random subset of 500 images from the AffectNet dataset  84  , which contains unposed expressions of emotions as they naturally occur in the wild outside of a carefully curated laboratory environment. We found that the Residual Masking Network model  83",
      "page_start": 10,
      "page_end": 11
    },
    {
      "section_name": "Robustness Experiments",
      "text": "While computer vision researchers typically focus on developing new face expression models that can outperform previous work on standard benchmarking datasets, end users are often more interested in how well the models perform on real world data collection contexts. This type of data is typically messier than the carefully curated open datasets. We intentionally selected benchmark datasets that contain spontaneous or naturalistic images collected outside the laboratory in the wild. In addition to these benchmarks, we also evaluated the robustness of the models included in Py-feat to different types of real world scenarios that are known to create problems for computer vision models including variations in luminance, occlusions of specific regions of the face, and also head rotation.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Luminance",
      "text": "To test the robustness of our model to different lighting conditions, we modified our benchmark datasets to include two different levels of luminance (low, where brightness factor uniformly sampled from [0.1, 0.8] for each image and high, where brightness factor uniformly sampled from [1.2, 1.9] for each image). This can be useful for knowing how the models might be impacted by inconsistent lighting or smaller variations in skin pigmentation. Overall, we found that the majority of the deep learning detectors were fairly robust to variations in luminance. However, the shallow learning detectors that rely on HOG features were more dramatically impacted by high and low levels of variance (Figure  2 ).",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Occlusion",
      "text": "In addition, we evaluated the performance of all of the detectors in three different occlusion contexts. Occlusions of the face are very common in real world data collection scenarios where a participant may cover their face with a hand, or be partially hidden behind some other physical object. We separately masked out the eyes, nose, and mouth on the benchmark datasets described above by applying a black mask to regions of the face using the facial landmark information (Figure  2A ). The pose and landmark models were fairly robust to facial occlusions. However, face detection substantially dropped with occlusions, particularly when the nose was masked. Occlusion of specific facial structures can also provide an interesting lesion test for higher level facial feature extraction such as action units and emotions. Consistent with our expectations, the AU detector performance dropped for AUs 1,2,4,5,6,9 when the eyes were masked, while performance dropped for AUs 12, 15, 20, 25,& 26 when the mouth was masked. AU 9 and 20 detection performance dropped when the nose was blocked. The emotion models were even more dramatically affected by occlusion of specific facial structures. Anger, fear, sadness, and surprise detection was substantially impacted by occlusion of the eyes, while disgust, happy, and neutral detection dropped when the mouth was blocked, and Anger, Disgust, Fear, and Sadness were degraded with occlusions to the nose.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Robustness Against Head Rotation",
      "text": "Most action unit models are trained using images in which the participants directly face the camera. However, in real world situations, faces are likely to be rotated relative to the camera position. Prior work has evaluated the performance of different AU detection algorithms on a new dataset, in which participants (N=12) were instructed to imitate specific facial expressions, while a camera recorded their expressions at specific rotation angles of 0°, 15°, 30°and 45°8  5  . Action units for each image were manually annotated by a trained FACS coder. We tested our py-feat-XGB AU detection model using this dataset and found that AU detection performance tends to decrease as rotation angles increase. However, the XGB model is fairly robust to rotation for most of the AUs except for AUs 9, 12, 17, & 26, where performance drops substantially for the largest 45° rotation (Figure  2G ).",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "Visualization",
      "text": "We provide several plotting tools to help visualize the Fex detection results in each stage of the analysis pipeline. In the facial feature detection stage, we offer the plot_detections function that overlays the face, facial landmarks, action units, and emotion detection results in a single figure (Figure  1 ). This function can be used to validate the detection results at each video frame or image. The Fex class also allows users to plot time series graphs as well, which can be useful for examining how detected action unit activities vary over time or if there are segments of missing data.\n\nIn addition, we provide a model which can be used to visualize how combinations of activated AUs will look like on a stylized anonymous face Figure  3 . This model visualizes the intensity of AUs overlaid onto a face in the approximate locations of where the facial muscles are located and also how AUs deform the face. Using this model, users can visualize the action units and their accompanying 2D landmark deformation on a standard face from any combination of action unit activations identified from their analyses (see supplemental materials for training details)  22, 86  . We hope to incorporate other types of visualization models as they become available.",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "Example Py-Feat Analysis Walkthrough",
      "text": "Py-feat easily facilitates numerous complex analyses. As a demonstration, we used a subset of the open video dataset from  87  in which participants were filmed while speaking in two conditions: delivering good news statements (e.g., \"your application has been accepted\" ) or bad news statements (e.g., \"your application was denied\"). A more comprehensive walkthrough using these data is included in the Py-Feat full analysis tutorial. Extracting facial features can be extracted in py-feat with relative ease using an intuitive API, and only requires two lines of code: one to initialize a Detector and another to process a video: detector = Detector() # initialize default detectors fex = detector.detect_video('video.mp4') # process each video frame\n\nThe fex object is a dataframe organized as frames by features, and contains all detections for every frame of the video including: faceboxes, landmarks, poses, action units, and emotions. Each fex object makes use of a special .sessions property that facilitates easy data aggregation and comparison. For example, we can compare the means of each condition of the data by setting sessions to the condition labels with .update_sessions(), followed by .extract_summary() to compute summary statistics aggregated by condition (Fig 4A ):\n\n# dictionary mapping video name to the condition it belonged to By_condition = fex.update_sessions( {'001': 'good_news', '002': 'bad_news', ...})\n\n# plot condition mean per action unit by_condition.extract_mean().aus.plot(kind=\"bar\")\n\nPy-feat also makes it easy to perform time series analyses using the .isc() method. For example, we can estimate the similarity between videos in terms of how their detected happiness varies over time (Fig 4B ):\n\n# calculate the pairwise similarity between videos in terms of their detected happiness intervideo_similarity = fex.isc(col = \"happiness\", method='pearson')",
      "page_start": 15,
      "page_end": 17
    },
    {
      "section_name": "# Visualize The Video X Video Correlation Matrix From Seaborn Import Heatmap Heatmap(Intervideo_Similarity)",
      "text": "Py-feat makes it simple to perform formal comparisons using the .regress() method. This method performs a \"mass-univariate\" style analysis 88 across all specified features. For example, we can use the experiment condition labels (\"good\" or \"bad\" news) as contrast codes and AUs as outcomes to perform a t-test on every AU. This returns the associated regression beta-values, standard-errors, t-statistics, p-values, degrees-of-freedom, and residuals for each AU:\n\n# setup mean difference contrast of good news > bad news by_condition_codes = fex.update_sessions({\"goodNews\": 1, \"badNews\": -1})\n\n# compare condition differences at every AU b, se, t, p, df, residuals = by_condition_codes.regress( X=\"sessions\", y=\"aus\", fit_intercept=True)\n\nPy-feat can just as easily facilitate a decoding analysis like the classification analysis performed by Watson and colleagues 87 using the .predict() method (Fig 4C These simple examples are only a fraction of the analyses that are possible using py-feat, but provide an example of how the toolbox makes it possible to conduct complex analyses with minimal python code.",
      "page_start": 17,
      "page_end": 17
    },
    {
      "section_name": "Discussion",
      "text": "In this paper, we describe the motivation, design principles, and core functionality of the open-source Python package Py-Feat. This package aims to bridge the gap between model developers creating new algorithms for detecting faces, facial landmarks, action units, and emotions with end users hoping to use these cutting edge models in their research. To achieve this, we designed an easy to use and open-source Python toolbox that allows researchers to quickly detect facial expressions from face images and videos and subsequently preprocess, analyze, and visualize the results. We hope this project will make facial expression analysis more accessible to researchers who may not have sufficient domain knowledge to implement these techniques themselves. In addition, Py-Feat provides a platform for model developers to disseminate their models to end-user researchers and compare the performance of their model with others included in the toolbox.\n\nAutomated detection of facial expressions has the potential to complement other techniques such as psychophysiology, brain imaging, and self-report  14, 22, 89  along with 3-D simulations  90  in improving our understanding of how emotions interact with perception, cognition, and social interactions and are impacted by our physical and mental health. Studying facial expressions is becoming increasingly more accessible to non-specialists. For example, recording participants has become more convenient with a number of affordable recording options such as webcams that can be used to record remote participants, open-source head mounted cameras allowing reliable face recordings in social settings  13  , as well as 360 cameras that can be used to record multiple individuals simultaneously. The primary goal of Py-Feat is to make the preprocessing, analysis, and visualization of these results similarly accessible and free of charge to non-specialists. Open source software focused on the full analysis pipeline has been instrumental in contributing to the rapid progress of research in other domains such as neuroimaging with FSL  47  , AFNI  48  , SPM  49  , and NiLearn 50 and natural language processing with Stanza 37 , SpaCy, and HuggingFace. We believe the broader emotion research community would greatly benefit from additional software platforms dedicated to facial expression analysis with functions for extracting, preprocessing, analyzing, and visualizing facial expression data.\n\nOur toolbox is designed to be flexible and dynamic and includes models that are performing near state of the art. However, there are several limitations that are important to note. First, our current implementations of some of the models are not performing as well as the original versions. This could be attributed to nuances in hyperparameter optimization, variations in random seeds, and variations in the benchmarking datasets. We anticipate that these models will improve over time as more datasets become available and also plan to continually incorporate new models as they become available. Benchmarking of new models will be added to a living document on our project website to allow users to make informed choices in selecting models. Second, we have not yet attempted to optimize our toolbox for speed. For example, we did not benchmark our models on processing time because we believe most users will be applying these detectors on batches of pre-recorded videos rather than in real-time applications. Currently, our models are able to process a single image in about 400 milliseconds with a GPU and about 1.5 seconds on a CPU. For users who need faster processing times on videos, processing can be sped up by temporally downsampling and skipping frames. We hope to optimize our code and improve processing time in future versions of our toolbox. Third, our models likely contain some degree of bias with respect to gender and race. We have attempted to use as much high quality publicly available data as possible to train our models and selected challenging real world datasets for benchmarking when available. This problem is inherent to the field of affective computing and will only improve as datasets increase in diversity and representation and preprocessing pipelines improve (e.g., faces with darker pigmentation are often more difficult to detect)  91, 92  . Fourth, our toolbox currently only includes detection of core facial features (i.e., facial landmarks, action units, and emotions) but there are additional signals in the face that can be informative for social science researchers. Head pose can be used to detect nodding or a shaking of the head which can be signals of consent or dissent in social interactions. Gaze extracted from face videos can be used to infer the attention of the recorded individual. Heart rate and respiration can also be extracted from face videos  93  which can be used to infer arousal or stress levels of the recorded individual. Models for detecting these facial features could be implemented in future versions of Py-Feat pending community interest.\n\nThe modular architecture of the Py-feat toolbox should theoretically be able to flexibly accommodate future developments in facial expression research. For example, adding improved models for our existing detection suite should be relatively straightforward assuming the models are trained using pytorch. New functionality can easily be added to the detector class in the form of a new method. Finally, new types of data can be accommodated by adding a new data loader class and data type specific models. For example, as 3D faces using depth cameras or thermal cameras become more ubiquitous accompanying rapid developments in virtual and augmented reality research, researchers can train new models to detect facial expression features, which can be incorporated into the toolbox without impacting extant functionality. We also hope that the research community will contribute new tutorials to our documentation to accelerate the pace of discovery in the field.\n\nIn summary, we introduce Py-Feat, an open source full stack framework implemented in Python for performing facial expression analysis from detection, preprocessing, analysis, and visualization. This work leverages efforts from the broader affective computing community by relying on high quality datasets, state of the art models, and building on other open source efforts such as OpenFace. We hope others in the community may be interested in improving this toolbox by providing feedback and bug reports, and also contributing bug fixes, new models and features. We have outlined our contribution guidelines as well as the necessary code and tutorials on how to replicate our work on our main project website (https://py-feat.org). We look forward to the increasing synergy between the fields of computer science and social science and welcome feedback and suggestions from the broader community as we continue to refine and add features to the Py-Feat platform.\n\n(eq2)\n\nand recall is the proportion of true positives relative to the ground truth:\n\n(eq3)\n\nF1 scores range from 0 to a perfect precision and recall of 1.0.",
      "page_start": 18,
      "page_end": 22
    },
    {
      "section_name": "Emotion Detectors",
      "text": "Emotion detectors are trained on manually posed or naturalistically elicited emotional facial expressions which allows detectors to classify new images based on how much a face resembles a canonical emotional facial expression. Py-feat also includes two emotion detectors. The Residual Masking Network (ResMaskNet)  83  is an end-to-end convolutional neural network model that combines deep residual networks with masking blocks. The masking blocks help focus the model's attention on local regions of interest to refine its feature map for more fine-grained predictions and the residual structure helps to maintain performances in deeper layers. ResMaskNet achieved state of the art performance on the facial expression recognition (FER) 2013  102  dataset at the time of preparing this article. Despite its accuracy, ResMaskNet has a large memory footprint (500MB) due to the depth of the architecture. We also trained an emotion detector model using an identical pipeline as our statistical learning AU models. This includes performing face alignment, applying a convex hull, and extracting HOG features, which are compressed using a PCA model that retains 95% of the variance. These features are used to classify the presence of each categorical emotion category using linear SVM implemented in scikit-learn  55  . The model was trained using the ExpW 103 , CK+  30  and JAFFE  104  facial expressions datasets with a 3-fold cross validation for identifying the best hyperparameters. Similar to AU detectors, we evaluate model performance with F1 scores for each emotion category.",
      "page_start": 22,
      "page_end": 23
    },
    {
      "section_name": "Au Visualization Model",
      "text": "Py-feat includes a model to visualize facial expression results on an anonymized and stylized face. Using this model, users can visualize the action units and their accompanying 2D landmark deformation on a standard face from any combination of action unit activations identified from their analyses. This can be useful for visualizing aspects of a model in an intuitive manner similar to how brain imaging software overlays statistical maps on a canonical brain  22, 86  .\n\nWe trained this action unit to landmark model on 20 action units (AUs 1, 2, 4, 5, 6, 7, 9, 10, 12,  14, 15, 17, 18, 20, 23, 24, 25, 26, 28, 43) with a subset of images from the EmotioNet 105 , BP4D  32  , and Extended DISFA Plus 33 datasets to balance the representation of each AU. We chose these datasets because they have both ground truth Action Unit labels. We used our toolbox with the Feat-RetinaFace face detector and MobileNets landmark detector to detect the landmarks on these images. We aligned these landmarks to a neutral face with an affine transformation using the facial landmarks and fit a Partial Least Squares Regression model with 20 components to predict these aligned landmarks from the ground truth action unit labels provided by the datasets using 3-fold cross-validation. Code to reproduce training and testing our visualization model is available in the Py-Feat Training Visualization Model Tutorial.\n\nOverall, the PLS model achieved a cross-validated r 2 of 0.155 in predicting landmark coordinate positions on 10,000 sample images. We used our model to illustrate how visualizations can be created in two ways. First we visualize emotions by detecting happy, sad, surprise, and anger expressions from single images in the CK+  30  dataset using the Residual Masking Network implemented in Py-Feat and then passing the AU vectors detected by the Feat-XGB AU classifier to our visualization model (Figure  3A ). This is all handled seamlessly using the detector.plot_detections(). In principle, Py-Feat's visualization model can generate a face from any 20 element array of numerical values between 0 and 1. This enables Py-Feat's second mode of visualization handled by the plot_face() and animate_face() functions, which can activate one or more of AUs and their underlying muscles e.g. AU1 (inner brow raiser), AU12 (lip corner puller), etc (Figure  3B ).",
      "page_start": 22,
      "page_end": 23
    },
    {
      "section_name": "Datasets",
      "text": "Training Datasets BP4D  32     68  Contains images collected in the wild retrieved from search engines (e.g., Google or Bing). The bounding boxes for each face were manually annotated with a total of 32,203 images with 393,703 labeled faces. This dataset is a standard for benchmarking face detection algorithms in data competitions and includes small, occluded, and upside down faces.\n\n300W  112  Contains both in-door and in-the-wild facial images retrieved from google searches. Facial landmarks for each image were semi-automatically annotated by the AOM algorithm 113,114   . The 300W dataset covers a wide variation in luminance, pose, identity, expression, occlusion and face size.\n\nNAMBA  85  contains 288 images collected from 12 Japanese participants (6 females and 6 males). Participants were told to imitate certain facial expressions, and a camera videotaped their expressions at angles of 0°, 15°, 30°and 45°. Facial action units (FACS) were annotated for each image by a professional annotator. The annotated AUs include AUs 1, 2, 4, 5, 6, 7, 9, 10, 12, 14, 15, 17, 18, 20, 23, 24, 25, 26, 27, and 43.  75  contains a total number of 15,678 frames collected from 20 subjects (6 females and 14 males) in a controlled in-door environment setting covering a wide range of poses. For each frame, a depth image, the corresponding rgb image, and the head pose annotation is provided. The head pose range covers about +-75 degrees yaw and +-60 degrees pitch.",
      "page_start": 24,
      "page_end": 25
    },
    {
      "section_name": "Biwi-Kinect",
      "text": "",
      "page_start": 25,
      "page_end": 25
    },
    {
      "section_name": "Robustness Tests",
      "text": "In addition to our assessing the performance of our detector models on standard benchmark datasets, we were also evaluated the robustness of the detector models included in Py-feat to different types of real world scenarios that are known to create problems for computer vision models including variations in luminance, occlusions of specific regions of the face, and also head rotation. A brief summary of these results are available in Figure  2  for the default models in py-feat. We have also included tables that include the results of our robustness experiments for all detector models included in the toolbox. Table  S1  includes results for all face detection models. Table  S2  includes results for the landmark detector models. Table  S3  includes results for pose estimation models. Table  S4  includes results for action unit detectors. Table  S5  includes results for the emotion category models. Finally, we include the performance of our action unit detector models in comparison to OpenFace on the Namba head rotation dataset  85  .",
      "page_start": 25,
      "page_end": 25
    },
    {
      "section_name": "Mtcnn",
      "text": "",
      "page_start": 25,
      "page_end": 25
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: ). Py-Feat",
      "page": 5
    },
    {
      "caption": "Figure 1: Facial expressions analysis pipeline. Analysis of facial expressions begins with recording face photos or",
      "page": 5
    },
    {
      "caption": "Figure 2: A). The pose and landmark models were fairly robust to facial occlusions.",
      "page": 12
    },
    {
      "caption": "Figure 2: Py-feat Detector Robustness Experiments. A) Example image for robustness manipulations. B)",
      "page": 13
    },
    {
      "caption": "Figure 1: ). This function can be used to validate the detection results at each video frame",
      "page": 14
    },
    {
      "caption": "Figure 3: This model visualizes the intensity of",
      "page": 14
    },
    {
      "caption": "Figure 3: | Demonstration of action unit to landmark visualization. (A): Facial expressions generated from AU",
      "page": 15
    },
    {
      "caption": "Figure 4: | Illustrative Py-Feat Analyses. (A): Average probability of action unit (AU) activation differences when",
      "page": 16
    },
    {
      "caption": "Figure 4: C). For example, we can use",
      "page": 17
    },
    {
      "caption": "Figure 4: D). A compelling use case is reconstructing the facial expression implied",
      "page": 18
    },
    {
      "caption": "Figure 3: A). This is all handled seamlessly using the",
      "page": 23
    },
    {
      "caption": "Figure 2: for the default models",
      "page": 25
    }
  ],
  "tables": [
    {
      "caption": "Table 1: ). Commercial software options such as",
      "data": [
        {
          "Facial feature detection": "Facial Action units Emotions Headpose Gaze\nlandmarks"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "each frame, adepthimage,thecorrespondingrgbimage,andtheheadposeannotationisprovided.",
          "For": ""
        },
        {
          "Column_1": "The head pose range covers about +-75 degrees yaw and +-60 degrees pitch.",
          "For": ""
        }
      ],
      "page": 25
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Acknowledgments": "The authors would like to acknowledge Filippo Rossi and Nathaniel Hanes for early discussions"
        },
        {
          "Acknowledgments": "of this work. We thank Mark Thornton, Emma Templeton, and Wasita Mahaphanit for providing"
        },
        {
          "Acknowledgments": "feedback on earlier drafts of this paper. We thank Shushi Namba, Wataru Sato and Sakiko"
        },
        {
          "Acknowledgments": "Yoshikawa for generously sharing their data with us. This work was supported by funding from"
        },
        {
          "Acknowledgments": "the National Institute of Mental Health R01MH116026, R56MH080716 and the National Science"
        },
        {
          "Acknowledgments": "Foundation CAREER 1848370."
        },
        {
          "Acknowledgments": "Conflict of interest"
        },
        {
          "Acknowledgments": "The authors declare no competing interests."
        }
      ],
      "page": 34
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "The Expression of the Emotions in Man and Animals",
      "authors": [
        "C Darwin"
      ],
      "year": "1886",
      "venue": "The Expression of the Emotions in Man and Animals"
    },
    {
      "citation_id": "2",
      "title": "Facial expression and emotion",
      "authors": [
        "P Ekman"
      ],
      "year": "1993",
      "venue": "Am. Psychol"
    },
    {
      "citation_id": "3",
      "title": "Facial action coding system: a technique for the measurement of facial movement",
      "authors": [
        "P Ekman",
        "W Friesen"
      ],
      "year": "1978",
      "venue": "Consulting Psychologists"
    },
    {
      "citation_id": "4",
      "title": "A psychometric evaluation of the facial action coding system for assessing spontaneous expression",
      "authors": [
        "M Sayette",
        "J Cohn",
        "J Wertz",
        "M Perrott"
      ],
      "year": "2001",
      "venue": "J. Nonverbal Behav"
    },
    {
      "citation_id": "5",
      "title": "Observer-based measurement of facial expression with the Facial Action Coding System. The handbook of emotion elicitation and assessment",
      "authors": [
        "J Cohn",
        "Z Ambadar",
        "P Ekman"
      ],
      "year": "2007",
      "venue": "Observer-based measurement of facial expression with the Facial Action Coding System. The handbook of emotion elicitation and assessment"
    },
    {
      "citation_id": "6",
      "title": "Ethnic bias in the recognition of facial expressions",
      "authors": [
        "J Kilbride",
        "M Yarczower"
      ],
      "year": "1983",
      "venue": "J. Nonverbal Behav"
    },
    {
      "citation_id": "7",
      "title": "Detection of emotions during learning with AutoTutor",
      "authors": [
        "A Graesser"
      ],
      "year": "2006",
      "venue": "Proceedings of the 28th annual meetings of the cognitive science society"
    },
    {
      "citation_id": "8",
      "title": "Pattern recognition of self-reported emotional state from multiple-site facial EMG activity during affective imagery",
      "authors": [
        "A Fridlund",
        "G Schwartz",
        "S Fowler"
      ],
      "year": "1984",
      "venue": "Psychophysiology"
    },
    {
      "citation_id": "9",
      "title": "Effects of positive and negative affect on electromyographic activity over zygomaticus major and corrugator supercilii",
      "authors": [
        "J Larsen",
        "C Norris",
        "J Cacioppo"
      ],
      "year": "2003",
      "venue": "Psychophysiology"
    },
    {
      "citation_id": "10",
      "title": "Alcohol and group formation: a multimodal investigation of the effects of alcohol on emotion and social bonding",
      "authors": [
        "M Sayette"
      ],
      "year": "2012",
      "venue": "Psychol. Sci"
    },
    {
      "citation_id": "11",
      "title": "Predicting movie ratings from audience behaviors",
      "authors": [
        "R Navarathna"
      ],
      "year": "2014",
      "venue": "IEEE Winter Conference on Applications of Computer Vision"
    },
    {
      "citation_id": "12",
      "title": "Affiliative zygomatic synchrony in co-present strangers",
      "authors": [
        "Y Golland",
        "D Mevorach",
        "N Levit-Binnun"
      ],
      "year": "2019",
      "venue": "Scientific Reports",
      "doi": "10.1038/s41598-019-40060-4"
    },
    {
      "citation_id": "13",
      "title": "Open source framework for recording facial expressions with head-mounted cameras",
      "authors": [
        "J Cheong",
        "S Brooks",
        "L Chang",
        "Facesync"
      ],
      "year": "2019",
      "venue": "F1000Res"
    },
    {
      "citation_id": "14",
      "title": "Synchronized affect in shared experiences strengthens social connection",
      "authors": [
        "J Cheong",
        "Z Molani",
        "S Sadhukha",
        "L Chang"
      ],
      "year": "2020",
      "venue": "Synchronized affect in shared experiences strengthens social connection",
      "doi": "10.31234/osf.io/bd9wn"
    },
    {
      "citation_id": "15",
      "title": "IEEE Int Conf Autom Face Gesture Recognit Workshops 1",
      "authors": [
        "F De La Torre"
      ],
      "year": "2015",
      "venue": "IEEE Int Conf Autom Face Gesture Recognit Workshops 1"
    },
    {
      "citation_id": "16",
      "title": "A compact embedding for facial expression similarity",
      "authors": [
        "R Vemulapalli",
        "A Agarwala"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "17",
      "title": "Facial expression analysis with AFFDEX and FACET: A validation study",
      "authors": [
        "S Stöckli",
        "M Schulte-Mecklenbeck",
        "S Borer",
        "A Samson"
      ],
      "year": "2018",
      "venue": "Behav. Res. Methods"
    },
    {
      "citation_id": "18",
      "title": "Using computer-vision and machine learning to automate facial coding of positive and negative affect intensity",
      "authors": [
        "N Haines",
        "M Southward",
        "J Cheavens",
        "T Beauchaine",
        "W.-Y Ahn"
      ],
      "year": "2019",
      "venue": "PLoS One"
    },
    {
      "citation_id": "19",
      "title": "Read My Face: Automatic Facial Coding Versus Psychophysiological Indicators of Emotional Valence and Arousal",
      "authors": [
        "T Höfling",
        "A Gerdes",
        "U Föhl",
        "G Alpers"
      ],
      "year": "2020",
      "venue": "Front. Psychol"
    },
    {
      "citation_id": "20",
      "title": "A performance comparison of eight commercially available automatic classifiers for facial affect recognition",
      "authors": [
        "D Dupré",
        "E Krumhuber",
        "D Küster",
        "G Mckeown"
      ],
      "year": "2020",
      "venue": "PLoS One"
    },
    {
      "citation_id": "21",
      "title": "Automatic Pain Assessment with Facial Activity Descriptors",
      "authors": [
        "P Werner"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "22",
      "title": "Socially transmitted placebo effects",
      "authors": [
        "P.-H Chen"
      ],
      "year": "2019",
      "venue": "Nat Hum Behav"
    },
    {
      "citation_id": "23",
      "title": "Automatic coding of facial expressions displayed during posed and genuine pain",
      "authors": [
        "G Littlewort",
        "M Bartlett",
        "K Lee"
      ],
      "year": "2009",
      "venue": "Image Vis. Comput"
    },
    {
      "citation_id": "24",
      "title": "Automatic Depression Detection via Facial Expressions Using Multiple Instance Learning",
      "authors": [
        "Y Wang"
      ],
      "year": "2020",
      "venue": "2020 IEEE 17th International Symposium on Biomedical Imaging (ISBI)"
    },
    {
      "citation_id": "25",
      "title": "Personality Judgments from Natural and Composite Facial Images: More Evidence For A 'Kernel Of Truth",
      "authors": [
        "I Penton-Voak",
        "N Pound",
        "A Little",
        "D Perrett"
      ],
      "year": "2006",
      "venue": "In Social Perception. Soc. Cogn"
    },
    {
      "citation_id": "26",
      "title": "What your Facebook Profile Picture Reveals about your Personality",
      "authors": [
        "C Segalin"
      ],
      "year": "2017",
      "venue": "Proceedings of the 25th ACM international conference on Multimedia",
      "doi": "10.1145/3123266.3123331"
    },
    {
      "citation_id": "27",
      "title": "Assessing the Big Five personality traits using real-life static facial images",
      "authors": [
        "A Kachur",
        "E Osin",
        "D Davydov",
        "K Shutilov",
        "A Novokshonov"
      ],
      "year": "2020",
      "venue": "Sci. Rep"
    },
    {
      "citation_id": "28",
      "title": "Facial recognition technology can expose political orientation from naturalistic facial images",
      "authors": [
        "M Kosinski"
      ],
      "year": "2021",
      "venue": "Sci. Rep"
    },
    {
      "citation_id": "29",
      "title": "Comprehensive database for facial expression analysis",
      "authors": [
        "T Kanade",
        "J Cohn",
        "Yingli Tian"
      ],
      "year": "2000",
      "venue": "Proceedings Fourth IEEE International Conference on Automatic Face and Gesture Recognition"
    },
    {
      "citation_id": "30",
      "title": "The Extended Cohn-Kanade Dataset (CK ): A complete dataset for action unit and emotion-specified expression",
      "authors": [
        "P Lucey"
      ],
      "year": "2010",
      "venue": "IEEE Computer Society Conference on Computer Vision and Pattern Recognition -Workshops Preprint at",
      "doi": "10.1109/cvprw.2010.5543262"
    },
    {
      "citation_id": "31",
      "title": "DISFA: A Spontaneous Facial Action Intensity Database",
      "authors": [
        "S Mavadati",
        "M Mahoor",
        "K Bartlett",
        "P Trinh",
        "J Cohn"
      ],
      "year": "2013",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "32",
      "title": "BP4D-Spontaneous: a high-resolution spontaneous 3D dynamic facial expression database",
      "authors": [
        "X Zhang"
      ],
      "year": "2014",
      "venue": "Image Vis. Comput"
    },
    {
      "citation_id": "33",
      "title": "Extended disfa dataset: Investigating posed and spontaneous facial expressions",
      "authors": [
        "M Mavadati",
        "P Sanger",
        "M Mahoor"
      ],
      "year": "2016",
      "venue": "Extended disfa dataset: Investigating posed and spontaneous facial expressions"
    },
    {
      "citation_id": "34",
      "title": "Multimodal spontaneous emotion corpus for human behavior analysis",
      "authors": [
        "Z Zhang"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "35",
      "title": "A Review of Dynamic Datasets for Facial Expression Research",
      "authors": [
        "E Krumhuber",
        "L Skora",
        "D Küster",
        "L Fou"
      ],
      "year": "2017",
      "venue": "Emot. Rev"
    },
    {
      "citation_id": "36",
      "title": "Emotion Recognition In The Wild Challenge 2014: Baseline, Data and Protocol",
      "authors": [
        "A Dhall",
        "R Goecke",
        "J Joshi",
        "K Sikka",
        "T Gedeon"
      ],
      "year": "2014",
      "venue": "Proceedings of the 16th International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "37",
      "title": "A Python Natural Language Processing Toolkit for Many Human Languages",
      "authors": [
        "P Qi",
        "Y Zhang",
        "Y Zhang",
        "J Bolton",
        "C Manning",
        "Stanza"
      ],
      "year": "2020",
      "venue": "A Python Natural Language Processing Toolkit for Many Human Languages"
    },
    {
      "citation_id": "38",
      "title": "",
      "authors": [
        "G Brockman"
      ],
      "year": "2016",
      "venue": ""
    },
    {
      "citation_id": "40",
      "title": "Advances in face and gesture analysis",
      "authors": [
        "H Van Kuilenburg",
        "M Den Uyl",
        "M Israël",
        "P Ivan"
      ],
      "year": "2008",
      "venue": "Measuring Behavior"
    },
    {
      "citation_id": "41",
      "title": "Gently does it: Humans outperform a software classifier in recognizing subtle, nonstereotypical facial expressions",
      "authors": [
        "N Yitzhak"
      ],
      "year": "2017",
      "venue": "Emotion"
    },
    {
      "citation_id": "42",
      "title": "Human and machine validation of 14 databases of dynamic facial expressions",
      "authors": [
        "E Krumhuber",
        "D Küster",
        "S Namba",
        "L Skora"
      ],
      "year": "2020",
      "venue": "Behav. Res. Methods",
      "doi": "10.3758/s13428-020-01443-y"
    },
    {
      "citation_id": "43",
      "title": "Emotion recognition from posed and spontaneous dynamic expressions: Human observers versus machine analysis",
      "authors": [
        "E Krumhuber",
        "D Küster",
        "S Namba",
        "D Shah",
        "M Calvo"
      ],
      "year": "2021",
      "venue": "Emotion"
    },
    {
      "citation_id": "44",
      "title": "The computer expression recognition toolbox (CERT)",
      "authors": [
        "G Littlewort"
      ],
      "year": "2011",
      "venue": "2011 IEEE International Conference on Automatic Face Gesture Recognition (FG)"
    },
    {
      "citation_id": "45",
      "title": "AFFDEX SDK: A Cross-Platform Real-Time Multi-Face Expression Recognition Toolkit",
      "authors": [
        "D Mcduff"
      ],
      "year": "2016",
      "venue": "Proceedings of the 2016 CHI Conference Extended Abstracts on Human Factors in Computing Systems"
    },
    {
      "citation_id": "46",
      "title": "OpenFace 2.0: Facial Behavior Analysis Toolkit",
      "authors": [
        "T Baltrusaitis",
        "A Zadeh",
        "Y Lim",
        "L Morency"
      ],
      "year": "2018",
      "venue": "2018 13th IEEE International Conference on Automatic Face Gesture Recognition"
    },
    {
      "citation_id": "47",
      "title": "Neuroimage",
      "authors": [
        "M Jenkinson",
        "C Beckmann",
        "T Behrens",
        "M Woolrich",
        "S Smith",
        "Fsl"
      ],
      "year": "2012",
      "venue": "Neuroimage"
    },
    {
      "citation_id": "48",
      "title": "AFNI: software for analysis and visualization of functional magnetic resonance neuroimages",
      "authors": [
        "R Cox"
      ],
      "year": "1996",
      "venue": "Comput. Biomed. Res"
    },
    {
      "citation_id": "49",
      "title": "Comparing functional (PET) images: the assessment of significant change",
      "authors": [
        "K Friston",
        "C Frith",
        "P Liddle",
        "R Frackowiak"
      ],
      "year": "1991",
      "venue": "J. Cereb. Blood Flow Metab"
    },
    {
      "citation_id": "50",
      "title": "Machine learning for neuroimaging with scikit-learn",
      "authors": [
        "A Abraham"
      ],
      "year": "2014",
      "venue": "Front. Neuroinform"
    },
    {
      "citation_id": "51",
      "title": "What the Face Reveals: Basic and Applied Studies of Spontaneous Expression Using the Facial Action Coding System (FACS)",
      "authors": [
        "P Ekman",
        "E Rosenberg"
      ],
      "year": "1997",
      "venue": "What the Face Reveals: Basic and Applied Studies of Spontaneous Expression Using the Facial Action Coding System (FACS)"
    },
    {
      "citation_id": "52",
      "title": "Array programming with NumPy",
      "authors": [
        "C Harris"
      ],
      "year": "2020",
      "venue": "Nature"
    },
    {
      "citation_id": "53",
      "title": "pandas: a foundational Python library for data analysis and statistics",
      "authors": [
        "W Mckinney",
        "Others"
      ],
      "year": "2011",
      "venue": "Python for High Performance and Scientific Computing"
    },
    {
      "citation_id": "54",
      "title": "Open source scientific tools for {Python}",
      "authors": [
        "E Jones",
        "T Oliphant",
        "P Peterson",
        "{scipy}"
      ],
      "year": "2001",
      "venue": "Open source scientific tools for {Python}"
    },
    {
      "citation_id": "55",
      "title": "Scikit-learn: Machine Learning in Python",
      "authors": [
        "F Pedregosa"
      ],
      "year": "2011",
      "venue": "J. Mach. Learn. Res"
    },
    {
      "citation_id": "56",
      "title": "Tensorflow: A system for large-scale machine learning",
      "authors": [
        "M Abadi"
      ],
      "year": "2016",
      "venue": "12th ${USENIX} symposium on operating systems design and implementation"
    },
    {
      "citation_id": "57",
      "title": "PyTorch: An Imperative Style, High-Performance Deep Learning Library",
      "authors": [
        "A Paszke"
      ],
      "year": "2019",
      "venue": "PyTorch: An Imperative Style, High-Performance Deep Learning Library"
    },
    {
      "citation_id": "58",
      "title": "Matplotlib: A 2D Graphics Environment",
      "authors": [
        "Hunter"
      ],
      "year": "2007",
      "venue": "Matplotlib: A 2D Graphics Environment"
    },
    {
      "citation_id": "59",
      "title": "seaborn: statistical data visualization",
      "authors": [
        "M Waskom"
      ],
      "year": "2021",
      "venue": "J. Open Source Softw"
    },
    {
      "citation_id": "60",
      "title": "ljchang/dartbrains: An online open access resource for learning functional neuroimaging analysis methods in Python",
      "authors": [
        "L Chang"
      ],
      "year": "2020",
      "venue": "ljchang/dartbrains: An online open access resource for learning functional neuroimaging analysis methods in Python",
      "doi": "10.5281/zenodo.3909718"
    },
    {
      "citation_id": "61",
      "title": "naturalistic-data-analysis/naturalistic_data_analysis: Version 1.0",
      "authors": [
        "L Chang"
      ],
      "year": "2020",
      "venue": "naturalistic-data-analysis/naturalistic_data_analysis: Version 1.0",
      "doi": "10.5281/zenodo.3937849"
    },
    {
      "citation_id": "63",
      "title": "FaceBoxes: A CPU real-time face detector with high accuracy",
      "authors": [
        "S Zhang"
      ],
      "year": "2017",
      "venue": "2017 IEEE International Joint Conference on Biometrics (IJCB)"
    },
    {
      "citation_id": "64",
      "title": "Multi-Task Cascaded Convolutional Networks Based Intelligent Fruit Detection for Designing Automated Robot",
      "authors": [
        "L Zhang"
      ],
      "year": "2019",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "65",
      "title": "Research on Face Detection Technology Based on MTCNN",
      "authors": [
        "N Zhang",
        "J Luo",
        "W Gao"
      ],
      "year": "2020",
      "venue": "2020 International Conference on Computer Network, Electronic and Automation (ICCNEA)"
    },
    {
      "citation_id": "66",
      "title": "Single-stage Dense Face Localisation in the Wild",
      "authors": [
        "J Deng"
      ],
      "year": "2019",
      "venue": "Single-stage Dense Face Localisation in the Wild"
    },
    {
      "citation_id": "67",
      "title": "Wider face: A face detection benchmark",
      "authors": [
        "S Yang",
        "P Luo",
        "C.-C Loy",
        "X Tang"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "68",
      "title": "WIDER FACE: A Face Detection Benchmark",
      "authors": [
        "S Yang",
        "P Luo",
        "C Loy",
        "X Tang"
      ],
      "year": "2015",
      "venue": "WIDER FACE: A Face Detection Benchmark"
    },
    {
      "citation_id": "69",
      "title": "The first facial landmark tracking in-the-wild challenge: Benchmark and results",
      "authors": [
        "J Shen"
      ],
      "year": "2015",
      "venue": "Proceedings of the IEEE international conference on computer vision workshops"
    },
    {
      "citation_id": "70",
      "title": "300 Faces In-The-Wild Challenge: database and results",
      "authors": [
        "C Sagonas",
        "E Antonakos",
        "G Tzimiropoulos",
        "S Zafeiriou",
        "M Pantic"
      ],
      "year": "2016",
      "venue": "Image Vis. Comput"
    },
    {
      "citation_id": "71",
      "title": "PFLD: A Practical Facial Landmark Detector",
      "authors": [
        "X Guo"
      ],
      "year": "2019",
      "venue": "PFLD: A Practical Facial Landmark Detector"
    },
    {
      "citation_id": "72",
      "title": "MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications",
      "authors": [
        "A Howard"
      ],
      "year": "2017",
      "venue": "MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications"
    },
    {
      "citation_id": "73",
      "title": "Efficient CNNs for Accurate Real-Time Face Verification on Mobile Devices",
      "authors": [
        "S Chen",
        "Y Liu",
        "X Gao",
        "Z Han",
        "Mobilefacenets"
      ],
      "year": "2018",
      "venue": "Efficient CNNs for Accurate Real-Time Face Verification on Mobile Devices"
    },
    {
      "citation_id": "74",
      "title": "A semi-automatic methodology for facial landmark annotation",
      "authors": [
        "C Sagonas",
        "G Tzimiropoulos",
        "S Zafeiriou",
        "M Pantic"
      ],
      "year": "2013",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition workshops"
    },
    {
      "citation_id": "75",
      "title": "Random Forests for Real Time 3D Face Analysis",
      "authors": [
        "G Fanelli",
        "M Dantone",
        "J Gall",
        "A Fossati",
        "L Van Gool"
      ],
      "year": "2013",
      "venue": "Int. J. Comput. Vis"
    },
    {
      "citation_id": "76",
      "title": "LIBSVM: A library for support vector machines",
      "authors": [
        "C.-C Chang",
        "C.-J Lin"
      ],
      "year": "2011",
      "venue": "ACM Trans. Intell. Syst. Technol"
    },
    {
      "citation_id": "77",
      "title": "XGBoost: A Scalable Tree Boosting System",
      "authors": [
        "T Chen",
        "C Guestrin"
      ],
      "year": "2016",
      "venue": "Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining"
    },
    {
      "citation_id": "78",
      "title": "Facial expressions of emotion are not culturally universal",
      "authors": [
        "R Jack",
        "O Garrod",
        "H Yu",
        "R Caldara",
        "P Schyns"
      ],
      "year": "2012",
      "venue": "Proc. Natl. Acad. Sci. U. S. A"
    },
    {
      "citation_id": "79",
      "title": "Sixteen facial expressions occur in similar contexts worldwide",
      "authors": [
        "A Cowen"
      ],
      "year": "2021",
      "venue": "Nature"
    },
    {
      "citation_id": "80",
      "title": "Emotional Expressions Reconsidered: Challenges to Inferring Emotion From Human Facial Movements",
      "authors": [
        "L Barrett",
        "R Adolphs",
        "S Marsella",
        "A Martinez",
        "S Pollak"
      ],
      "year": "2019",
      "venue": "Psychological Science in the Public Interest",
      "doi": "10.1177/1529100619832930"
    },
    {
      "citation_id": "81",
      "title": "Formalizing emotion concepts within a Bayesian model of theory of mind",
      "authors": [
        "R Saxe",
        "S Houlihan"
      ],
      "year": "2017",
      "venue": "Curr Opin Psychol"
    },
    {
      "citation_id": "82",
      "title": "Regret Induces Rapid Learning from Experience-based Decisions: A Model-based Facial Expression Analysis Approach",
      "authors": [
        "N Haines"
      ],
      "year": "2019",
      "venue": "bioRxiv",
      "doi": "10.1101/560011"
    },
    {
      "citation_id": "83",
      "title": "Facial Expression Recognition using Residual Masking Network",
      "authors": [
        "P Luan",
        "V Huynh",
        "T Tuan Anh"
      ],
      "year": "2020",
      "venue": "IEEE 25th International Conference on Pattern Recognition"
    },
    {
      "citation_id": "84",
      "title": "AffectNet: A database for facial expression, valence, and arousal computing in the wild",
      "authors": [
        "A Mollahosseini",
        "B Hasani",
        "M Mahoor"
      ],
      "year": "2019",
      "venue": "IEEE Trans. Affect. Comput"
    },
    {
      "citation_id": "85",
      "title": "Viewpoint Robustness of Automated Facial Action Unit Detection Systems",
      "authors": [
        "S Namba",
        "W Sato",
        "S Yoshikawa"
      ],
      "year": "2021",
      "venue": "NATO Adv. Sci. Inst. Ser. E Appl. Sci"
    },
    {
      "citation_id": "86",
      "title": "Endogenous variation in ventromedial prefrontal cortex state dynamics during naturalistic viewing reflects affective experience",
      "authors": [
        "L Chang"
      ],
      "year": "2021",
      "venue": "Sci Adv"
    },
    {
      "citation_id": "87",
      "title": "A data-driven characterisation of natural facial expressions when giving good and bad news",
      "authors": [
        "D Watson",
        "B Brown",
        "A Johnston"
      ],
      "year": "2020",
      "venue": "PLoS Comput. Biol"
    },
    {
      "citation_id": "88",
      "title": "Analysis of fMRI Time-Series Revisited-Again. Neuroimage",
      "authors": [
        "K Worsley",
        "K Friston"
      ],
      "year": "1995",
      "venue": "Analysis of fMRI Time-Series Revisited-Again. Neuroimage"
    },
    {
      "citation_id": "89",
      "title": "Endogenous variation in ventromedial prefrontal cortex state dynamics during naturalistic viewing reflects affective experience",
      "authors": [
        "L Chang"
      ],
      "year": "2018",
      "venue": "bioRxiv",
      "doi": "10.1101/487892"
    },
    {
      "citation_id": "90",
      "title": "Dynamic facial expressions of emotion transmit an evolving hierarchy of signals over time",
      "authors": [
        "R Jack",
        "O Garrod",
        "P Schyns"
      ],
      "year": "2014",
      "venue": "Curr. Biol"
    },
    {
      "citation_id": "91",
      "title": "Racial Influence on Automated Perceptions of Emotions",
      "authors": [
        "L Rhue"
      ],
      "year": "2018",
      "venue": "Racial Influence on Automated Perceptions of Emotions",
      "doi": "10.2139/ssrn.3281765"
    },
    {
      "citation_id": "92",
      "title": "Deep Learning for Face Recognition: Pride or Prejudiced?",
      "authors": [
        "S Nagpal",
        "M Singh",
        "R Singh",
        "M Vatsa"
      ],
      "year": "2019",
      "venue": "Deep Learning for Face Recognition: Pride or Prejudiced?"
    },
    {
      "citation_id": "93",
      "title": "Remote measurement of cognitive stress via heart rate variability",
      "authors": [
        "D Mcduff",
        "S Gontarek",
        "R Picard"
      ],
      "year": "2014",
      "venue": "Conf. Proc. IEEE Eng. Med. Biol. Soc"
    },
    {
      "citation_id": "94",
      "title": "Scikit-learn: Machine Learning in Python",
      "authors": [
        "F Pedregosa"
      ],
      "year": "2011",
      "venue": "J. Mach. Learn. Res"
    },
    {
      "citation_id": "95",
      "title": "Cross-dataset learning and person-specific normalisation for automatic Action Unit detection",
      "authors": [
        "T Baltrušaitis",
        "M Mahmoud",
        "P Robinson"
      ],
      "year": "2015",
      "venue": "2015 11th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG)"
    },
    {
      "citation_id": "96",
      "title": "Histograms of oriented gradients for human detection",
      "authors": [
        "N Dalal",
        "B Triggs"
      ],
      "year": "2005",
      "venue": "IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05)"
    },
    {
      "citation_id": "97",
      "title": "scikit-image: image processing in Python",
      "authors": [
        "S Van Der Walt"
      ],
      "year": "2014",
      "venue": "PeerJ"
    },
    {
      "citation_id": "98",
      "title": "Painful data: The UNBC-McMaster shoulder pain expression archive database",
      "authors": [
        "P Lucey",
        "J Cohn",
        "K Prkachin",
        "P Solomon",
        "I Matthews"
      ],
      "year": "2011",
      "venue": "2011 IEEE International Conference on Automatic Face Gesture Recognition (FG)"
    },
    {
      "citation_id": "99",
      "title": "EmotioNet Challenge: Recognition of facial expressions of emotion in the wild",
      "authors": [
        "Fabian Benitez-Quiroz",
        "C Srinivasan",
        "R Feng",
        "Q Wang",
        "Y Martinez"
      ],
      "year": "2017",
      "venue": "EmotioNet Challenge: Recognition of facial expressions of emotion in the wild"
    },
    {
      "citation_id": "100",
      "title": "Recognition of Action Units in the Wild with Deep Nets and a New Global-Local Loss",
      "authors": [
        "C Benitez-Quiroz",
        "Y Wang",
        "A Martinez"
      ],
      "year": "2017",
      "venue": "2017 IEEE International Conference on Computer Vision (ICCV)"
    },
    {
      "citation_id": "101",
      "title": "EmotioNet: An accurate, real-time algorithm for the automatic annotation of a million facial expressions in the wild",
      "authors": [
        "C Benitez-Quiroz",
        "R Srinivasan",
        "A Martinez"
      ],
      "year": "2016",
      "venue": "2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "102",
      "title": "Challenges in representation learning: A report on three machine learning contests",
      "authors": [
        "I Goodfellow"
      ],
      "year": "2015",
      "venue": "Neural Networks",
      "doi": "10.1016/j.neunet.2014.09.005"
    },
    {
      "citation_id": "103",
      "title": "From facial expression recognition to interpersonal relation prediction",
      "authors": [
        "Z Zhang",
        "P Luo",
        "C Loy",
        "X Tang"
      ],
      "year": "2018",
      "venue": "Int. J. Comput. Vis"
    },
    {
      "citation_id": "104",
      "title": "The Japanese Female Facial Expression (JAFFE) Dataset",
      "authors": [
        "M Lyons",
        "M Kamachi",
        "J Gyoba"
      ],
      "year": "1998",
      "venue": "The Japanese Female Facial Expression (JAFFE) Dataset",
      "doi": "10.5281/zenodo.3451524"
    },
    {
      "citation_id": "105",
      "title": "Emotionet: An accurate, real-time algorithm for the automatic annotation of a million facial expressions in the wild",
      "authors": [
        "Fabian Benitez-Quiroz",
        "C Srinivasan",
        "R Martinez"
      ],
      "year": "2016",
      "venue": "Proceedings"
    },
    {
      "citation_id": "106",
      "title": "Supervised Descent Method and Its Applications to Face Alignment",
      "authors": [
        "X Xiong",
        "F De La Torre"
      ],
      "year": "2013",
      "venue": "2013 IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "107",
      "title": "Multimodal Spontaneous Emotion Corpus for Human Behavior Analysis",
      "authors": [
        "Z Zhang"
      ],
      "year": "2016",
      "venue": "2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "108",
      "title": "DISFA: A spontaneous facial action intensity database",
      "authors": [
        "S Mavadati",
        "M Mahoor",
        "K Bartlett",
        "P Trinh",
        "J Cohn"
      ],
      "year": "2013",
      "venue": "IEEE Trans. Affect. Comput"
    },
    {
      "citation_id": "109",
      "title": "Extended DISFA Dataset: Investigating Posed and Spontaneous Facial Expressions",
      "authors": [
        "M Mavadati",
        "P Sanger",
        "M Mahoor"
      ],
      "year": "2016",
      "venue": "2016 IEEE Conference on Computer Vision and Pattern Recognition Workshops"
    },
    {
      "citation_id": "110",
      "title": "The Extended Cohn-Kanade Dataset (CK+): A complete dataset for action unit and emotion-specified expression",
      "authors": [
        "P Lucey"
      ],
      "year": "2010",
      "venue": "2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition -Workshops",
      "doi": "10.1109/cvprw.2010.5543262"
    },
    {
      "citation_id": "111",
      "title": "Painful data: The UNBC-McMaster shoulder pain expression archive database",
      "authors": [
        "P Lucey",
        "J Cohn",
        "K Prkachin",
        "P Solomon",
        "I Matthews"
      ],
      "year": "2011",
      "venue": "2011 IEEE International Conference on Automatic Face & Gesture Recognition (FG)"
    },
    {
      "citation_id": "112",
      "title": "300 faces in-the-wild challenge: The first facial landmark localization challenge",
      "authors": [
        "C Sagonas",
        "G Tzimiropoulos",
        "S Zafeiriou",
        "M Pantic"
      ],
      "year": "2013",
      "venue": "Proceedings of the IEEE international conference on computer vision workshops"
    },
    {
      "citation_id": "113",
      "title": "Generic Active Appearance Models Revisited",
      "authors": [
        "G Tzimiropoulos",
        "J Alabort-I-Medina",
        "S Zafeiriou",
        "M Pantic"
      ],
      "year": "2012",
      "venue": "Computer Vision -ACCV"
    },
    {
      "citation_id": "114",
      "title": "Active Orientation Models for Face Alignment In-the-Wild",
      "authors": [
        "G Tzimiropoulos",
        "J Alabort-I-Medina",
        "S Zafeiriou",
        "M Pantic"
      ],
      "year": "2014",
      "venue": "IEEE Trans. Inf. Forensics Secur"
    }
  ]
}