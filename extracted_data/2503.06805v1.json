{
  "paper_id": "2503.06805v1",
  "title": "Multimodal Emotion Recognition And Sentiment Analysis In Multi-Party Conversation Contexts",
  "published": "2025-03-09T23:14:19Z",
  "authors": [
    "Aref Farhadipour",
    "Hossein Ranjbar",
    "Masoumeh Chapariniya",
    "Teodora Vukovic",
    "Sarah Ebling",
    "Volker Dellwo"
  ],
  "keywords": [
    "Multimodal Emotion Recognition",
    "Multimodal Sentiment Analysis",
    "Modality Fusion"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Emotion recognition and sentiment analysis are pivotal tasks in speech and language processing, particularly in real-world scenarios involving multi-party, conversational data. This paper presents a multimodal approach to tackle these challenges on a well-known dataset. We propose a system that integrates four key modalities/channels using pre-trained models: RoBERTa for text, Wav2Vec2 for speech, a proposed Facial-Net for facial expressions, and a CNN+Transformer architecture trained from scratch for video analysis. Feature embeddings from each modality are concatenated to form a multimodal vector, which is then used to predict emotion and sentiment labels. The multimodal system demonstrates superior performance compared to unimodal approaches, achieving an accuracy of 66.36% for emotion recognition and 72.15% for sentiment analysis.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "With the rapid growth of technology, the role of machines in various sectors, such as resource management, entertainment, and human assistance, has become increasingly prominent. In these applications, machines are typically provided with data from the real world, and they are expected to respond appropriately. The input data can be diverse, including text, audio, video, or multimodal information. Each type of input requires specific processing techniques, and each modality has distinct challenges. In multimodal data processing, selecting and implementing an effective method for each modality is critical to achieving optimal performance.\n\nOne significant challenge that has gained considerable attention in recent years is enhancing the intelligence of Human-Machine Interaction (HMI). Users interacting with machines often wish to communicate their emotional states, making it imperative for the system to accurately perceive and respond to these emotions to facilitate a high level of intelligent interaction. Therefore, developing emotion and general sentiment recognition systems has become a crucial area of research.\n\nBased on the theory of emotions, the emotion recognition task identifies the inner sense based on the seven basic emotions. However, sentiment analysis deals with labelling the general opinion expressed as positive, negative, or neutral, for example, in customer feedback analysis.\n\nEmotion recognition in HMI involves multiple modalities, such as text, facial expressions, voice, and body movements, representing different aspects of human emotions. These modalities complement each other; for example, voice tone, facial expressions, and gestures can collectively modify or amplify the emotional content of spoken words.\n\nHowever, designing multimodal emotion recognition and sentiment analysis systems poses challenges due to the scarcity of large, high-quality multimodal datasets. Most of the current research is based on the IEMOCAP dataset  [1] , which is recorded under controlled studio conditions. While this dataset provides a solid foundation, real-world emotion recognition presents more significant challenges, especially in multi-party conversational settings. For this reason, we employ the MELD dataset  [2]  in this work, as it offers a more realistic setting with varied background conditions, multiple speakers, and environmental noise, providing a more challenging yet practical scenario for emotion recognition.\n\nIn this work, we propose a multimodal system designed for emotion recognition and sentiment analysis. This system is built upon four core components, each addressing a distinct modality. Specifically, we utilize Wav2Vec2  [3]  for voice, RoBERTa  [4]  for text, a proposed FacialNet for facial expressions, and a CNN+Transformer architecture trained from scratch for video analysis.\n\nThe remainder of this paper is organized as follows: Section 2 reviews related work in emotion recognition and sentiment analysis. Section 3 describes the dataset and the proposed system architecture. Experimental results are presented in Section 4, followed by conclusions in Section 5.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Previous Work",
      "text": "While some approaches focus on unimodal emotion recognition  [5] , recent advancements have favoured the integration of multiple data modalities and machine learning tech-arXiv:2503.06805v1 [cs.CV] 9 Mar 2025 niques. These multimodal methods aim to address the limitations of unimodal analysis by combining information from diverse sources to enhance emotion recognition performance.\n\nZheng et al.  [6]  proposed a two-stage framework focusing on facial sequences. Their approach employs multi-task learning to generate emotion-informed visual representations. Similarly, Hu et al.  [7]  developed the supervised adversarial contrastive learning framework, which incorporates contextual adversarial training alongside a Dual-LSTM structure to enhance emotion recognition from textual data.\n\nYun et al.  [8]  introduced the Teacher-leading Multimodal Fusion Network for emotion recognition, which employs cross-modal knowledge distillation. In this method, a language model is used to improve the performance of nonverbal modalities. Fu et al.  [9]  presented a bi-model system for emotion that uses prompts to generate commonsense knowledge for interlocutors based on historical utterances. This model incorporates an interlocutor commonsense identification task to fine-tune the system for identifying implicit speaker cues.\n\nChudasama et al.  [10]  introduced a multimodal fusion network, which extracts emotion-relevant features from visual, audio, and textual modalities. Their model uses a multihead attention-based fusion mechanism and introduces a novel feature extractor trained with adaptive margin-based triplet loss for the audio and visual modalities.\n\nIn another novel approach, Wu et al.  [11]  developed a system that applies large language models to speech emotion recognition. This method converts speech characteristics into natural language descriptions and incorporates them into text prompts, allowing for multimodal emotion analysis without modifying the model's architecture.\n\nMultimodal sentiment analysis in conversations has also gained traction as researchers seek to overcome the limitations of unimodal approaches. Poria et al.  [2]  compared various models, including Text-CNN, LSTM, and DialogueRNN, finding that multimodal fusion outperformed unimodal methods, with DialogueRNN excelling in multi-party situations. Shah et al.  [12]  took an ensemble learning approach to sentiment analysis, integrating RoBERTa for text and Wav2Vec2 for audio. Their study explored various ensemble techniques to improve sentiment classification accuracy through multimodal fusion.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Methodology",
      "text": "In the proposed system, we used several pre-trained models. The use of pre-trained models is driven by the fact that available multimodal emotion recognition and sentiment analysis datasets are often limited in size, making transfer learning an effective strategy to overcome this data scarcity challenge. Various fusion techniques are commonly employed in multimodal systems, including sensory, feature, and score fusion and based on prior work, feature fusion has consistently demonstrated superior results  [13] .\n\nFigure  1  depicts the proposed system's architecture for emotion recognition and sentiment analysis tasks. Two approaches are explored: unimodal and multimodal fusion strategies. In the multimodal mode, the feature vectors extracted from each system are concatenated to make a multimodal vector.\n\nFor the voice modality, Wav2Vec2 is applied in an audio classification framework. The model is initially trained on three emotion recognition datasets and later fine-tuned on the MELD dataset. A similar approach is adopted for RoBERTa, pre-trained on 58 million tweets for emotion recognition using the TweetEval benchmark before fine-tuning on MELD  [14] . The combined representation is then used to make final decisions regarding emotion and sentiment via an additional classifier. The multimodal feature vector is classified using a multi-layer perceptron.\n\nThe rest of this section outlines the materials and methods used in this study. We introduce the MELD dataset and briefly overview the four models employed for each modality: Wav2Vec2, RoBERTa, FacialNet, and CNN+Transformer.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Dataset",
      "text": "MELD is a comprehensive multimodal dataset designed to advance emotion recognition and sentiment analysis tasks. It incorporates multiple modalities, including text, audio, and visual features  [2] . One of the unique aspects of MELD is that it is derived from dialogues in the popular TV series Friends, which provides emotionally diverse conversations between multiple speakers in dynamic, real-world-like settings. This makes MELD particularly valuable for real-world emotion recognition tasks, offering over 1,400 dialogue instances and more than 13,000 individual utterances.\n\nEach utterance in the dataset is annotated with seven emotion and three sentiment labels. The seven emotions include anger, fear, disgust, joy, sadness, surprise, and neutrality, while sentiment labels are positive, negative, and neutral.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Roberta",
      "text": "RoBERTa (Robustly Optimized BERT Pretraining Approach) is a transformer-based language model built upon BERT (Bidirectional Encoder Representations from Transformers). It is designed to enhance performance by optimizing training strategies, including larger batch sizes, more training data, and removing the next sentence prediction objective. RoBERTa  [4]  employs a masked language model, which learns contextualized word representations by predicting randomly masked tokens within a sentence based on surrounding context. By leveraging its bidirectional context understanding, RoBERTa captures subtle linguistic cues such as tone, word choice, and sentence structure, contributing to detecting underlying emotions.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Wav2Vec2",
      "text": "Wav2Vec2  [3]  has revolutionized speech processing by applying self-supervised learning directly to raw audio data. The model consists of a convolutional feature extractor and a transformer-based contextual network that models temporal dependencies in the input. During training, segments of the input speech are masked, and the network is tasked with predicting the masked portions, thus learning rich speech representations.\n\nWav2Vec2 has demonstrated strong performance in various tasks, including speech recognition and emotion classification, due to its ability to model complex acoustic features directly from raw waveform data  [3] .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Cnn+Transformer",
      "text": "We developed a model from scratch that extracts visual features not limited to facial expressions to handle video understanding. While previous works primarily focus on facial analysis, our approach captures broader visual cues, including the speaker's posture, gestures, the presence of other individuals, and background elements, which may contribute valuable context to emotion recognition.\n\nOur architecture comprises three key components: a spatial modelling module, a temporal modelling module, and a classifier. We use MobileNetV2  [15]  as a feature extractor for spatial modelling. This architecture is chosen for its efficiency and low latency, making it well-suited for real-time applications.\n\nFor temporal modelling, we employ a transformer-based approach  [16, 17, 18]  that captures temporal dependencies between frames. Given that long-range dependencies may not always be necessary for emotion recognition, we implement a local attention mechanism  [19]  to focus on relevant temporal information. This module consists of two transformer layers, each with eight attention heads and a hidden size 1280. Finally, the extracted features are passed through a neural network classifier with a hidden size 512 to predict emotion labels.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Facialnet",
      "text": "The facial model in this work employs a frame selection strategy to extract relevant face frames from videos using a twostage process  [6] . First, TalkNet, an active speaker detection model, combines multimodal rules to detect potential speaker faces. Then, InfoMap clustering and face matching against a character face library are used to isolate the actual speaker's face sequence.\n\nFor feature extraction, we utilize an InceptionResNetv1 model  [20] , pre-trained on the CASIA-WebFace dataset  [21]  that is initially for the face recognition task. This model is responsible for processing face frames and generating visual representations. A bidirectional LSTM is employed to classify the extracted features. This LSTM applies an attention mechanism to compute a context vector passed through a fully connected layer to produce the final classification output.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Experimental Results",
      "text": "The proposed systems were evaluated on the MELD dataset, focusing on two tasks: emotion recognition and sentiment analysis. In the emotion recognition task, the model predicts one of the seven basic emotions from the input multimedia data, including voice, video frames, facial expressions, and the corresponding text of utterances. The system classifies each input file for sentiment analysis as positive, negative, or neutral.\n\nThe models were fine-tuned with a batch size of 16 over five epochs to avoid over-fitting, using AdamW for optimization  [22] . The video modality, which was trained from scratch, underwent training for 10 iterations. Tables  1  and 2  report the results for emotion recognition and sentiment analysis, respectively, for each unimodal system. As expected, the system performed better in sentiment analysis compared to emotion recognition. This can be attributed to the reduced number of sentiment classes, as multiple emotions are grouped into broader sentiment categories. The image-based systems, including face and video processing, faced significant challenges due to variations in background, camera angles, occlusions, and multiple faces in a frame. Consequently, the accuracy for these modalities was lower. Specifically, the video analysis system, which aimed to capture general visual information and gestures, achieved an accuracy of 36.14% for emotion recognition and 42.51% for sentiment analysis. However, Multiple facial frames extracted from each video exhibit diverse emotional expressions yet are assigned a single emotion label. The processing of several frames consequently for a single label leads to low accuracy of FacialNet, in such a way that emotion recognition achieved 22.61% and 38.98% for sentiment analysis.\n\nDespite the challenges of multi-party conversations and environmental noise, the voice modality achieved 51.49% accuracy for emotion recognition and 56.20% for sentiment analysis. The text modality demonstrated the highest performance, with 64.34% accuracy for emotion recognition and 69.21% for sentiment analysis, underscoring the effectiveness of linguistic cues in these tasks.\n\nThe primary challenge with the text modality was handling conversational speech by target speakers that make various affect expressions corresponding to each audience. However, this modality is generally cleaner compared to others, allowing the system to learn the parameters for each class effectively. Each modality may excel in different aspects of the dataset, as each one captures a unique facet of emotion. This variability highlights the potential benefits of a multimodal approach, where integrating multiple modalities can address individual weaknesses.\n\nIn a multimodal strategy, the objective is to construct a multimodal vector for each input file. The classifier's role is to process this multimodal vector, learn from labeled samples during training, and identify the crucial components of the multimodal representation. Various integration strategies have been explored, with their performance for emotion recognition detailed in Table  3 .\n\nTable  3  presents the results of different integration approaches for sentiment analysis, including both 3-modality and 4-modality fusion. The text modality proves to be the most effective, indicating its role as a primary modality. The highest accuracy, 66.36%, was achieved by integrating all four modalities, which represents a 2.02% improvement over the best-performing in unimodal mode. This result demonstrates that the classifier effectively leverages the information from multiple modalities.\n\nFor sentiment analysis, the performance results of different integration approaches are shown in Table  4 . The best performance, with an accuracy of 72.15%, was achieved through the integration of all four modalities. In this evaluation, the multimodal sentiment analysis outperforms the unimodal by 2.94% for accuracy.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Conclusion",
      "text": "Emotion recognition and sentiment analysis are closely related and challenging tasks in pattern recognition. With its multi-party and conversational nature, the MELD dataset presents significant challenges due to the extensive visual and auditory variability in each sample. This variability results in differing performance for each modality across files, highlighting the need for multimodal approaches in such scenarios.\n\nIn this work, we utilized pre-trained models including RoBERTa, Wav2Vec2, InceptionResNet, and also developed a model based on a CNN+Transformer architecture trained from scratch to capture comprehensive information for video analysis. By extracting feature embeddings from each modality and concatenating them into a multimodal representation, we achieved superior performance compared to unimodal approaches. Our evaluation demonstrates the effectiveness of multimodal approaches for emotion recognition and sentiment analysis in near-to-real-world situations. As future trends, segmenting the speaker out of frame could improve two visual systems, and a multi-network strategy based on arousal and valence to recognize the effects could be beneficial.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Proposed system with unimodal and multimodal fea-",
      "page": 2
    },
    {
      "caption": "Figure 1: depicts the proposed system’s architecture for",
      "page": 2
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Department of Computational Linguistics, University of Zurich, Zurich, Switzerland": "Based on the theory of emotions, the emotion recognition"
        },
        {
          "Department of Computational Linguistics, University of Zurich, Zurich, Switzerland": "task identifies the inner sense based on the seven basic emo-"
        },
        {
          "Department of Computational Linguistics, University of Zurich, Zurich, Switzerland": ""
        },
        {
          "Department of Computational Linguistics, University of Zurich, Zurich, Switzerland": "tions. However, sentiment analysis deals with labelling the"
        },
        {
          "Department of Computational Linguistics, University of Zurich, Zurich, Switzerland": ""
        },
        {
          "Department of Computational Linguistics, University of Zurich, Zurich, Switzerland": "general opinion expressed as positive, negative, or neutral, for"
        },
        {
          "Department of Computational Linguistics, University of Zurich, Zurich, Switzerland": ""
        },
        {
          "Department of Computational Linguistics, University of Zurich, Zurich, Switzerland": "example, in customer feedback analysis."
        },
        {
          "Department of Computational Linguistics, University of Zurich, Zurich, Switzerland": ""
        },
        {
          "Department of Computational Linguistics, University of Zurich, Zurich, Switzerland": "Emotion recognition in HMI involves multiple modalities,"
        },
        {
          "Department of Computational Linguistics, University of Zurich, Zurich, Switzerland": ""
        },
        {
          "Department of Computational Linguistics, University of Zurich, Zurich, Switzerland": "such as text, facial expressions, voice, and body movements,"
        },
        {
          "Department of Computational Linguistics, University of Zurich, Zurich, Switzerland": ""
        },
        {
          "Department of Computational Linguistics, University of Zurich, Zurich, Switzerland": "representing different\naspects of human emotions.\nThese"
        },
        {
          "Department of Computational Linguistics, University of Zurich, Zurich, Switzerland": ""
        },
        {
          "Department of Computational Linguistics, University of Zurich, Zurich, Switzerland": "modalities complement each other;\nfor example, voice tone,"
        },
        {
          "Department of Computational Linguistics, University of Zurich, Zurich, Switzerland": ""
        },
        {
          "Department of Computational Linguistics, University of Zurich, Zurich, Switzerland": "facial expressions,\nand gestures can collectively modify or"
        },
        {
          "Department of Computational Linguistics, University of Zurich, Zurich, Switzerland": ""
        },
        {
          "Department of Computational Linguistics, University of Zurich, Zurich, Switzerland": "amplify the emotional content of spoken words."
        },
        {
          "Department of Computational Linguistics, University of Zurich, Zurich, Switzerland": ""
        },
        {
          "Department of Computational Linguistics, University of Zurich, Zurich, Switzerland": "However,\ndesigning multimodal\nemotion\nrecognition"
        },
        {
          "Department of Computational Linguistics, University of Zurich, Zurich, Switzerland": ""
        },
        {
          "Department of Computational Linguistics, University of Zurich, Zurich, Switzerland": "and sentiment analysis systems poses challenges due to the"
        },
        {
          "Department of Computational Linguistics, University of Zurich, Zurich, Switzerland": ""
        },
        {
          "Department of Computational Linguistics, University of Zurich, Zurich, Switzerland": "scarcity of large, high-quality multimodal datasets. Most of"
        },
        {
          "Department of Computational Linguistics, University of Zurich, Zurich, Switzerland": ""
        },
        {
          "Department of Computational Linguistics, University of Zurich, Zurich, Switzerland": "the current\nresearch is based on the IEMOCAP dataset\n[1],"
        },
        {
          "Department of Computational Linguistics, University of Zurich, Zurich, Switzerland": ""
        },
        {
          "Department of Computational Linguistics, University of Zurich, Zurich, Switzerland": "which is recorded under controlled studio conditions. While"
        },
        {
          "Department of Computational Linguistics, University of Zurich, Zurich, Switzerland": ""
        },
        {
          "Department of Computational Linguistics, University of Zurich, Zurich, Switzerland": "this dataset provides a solid foundation,\nreal-world emotion"
        },
        {
          "Department of Computational Linguistics, University of Zurich, Zurich, Switzerland": ""
        },
        {
          "Department of Computational Linguistics, University of Zurich, Zurich, Switzerland": "recognition presents more significant challenges, especially"
        },
        {
          "Department of Computational Linguistics, University of Zurich, Zurich, Switzerland": ""
        },
        {
          "Department of Computational Linguistics, University of Zurich, Zurich, Switzerland": "in multi-party conversational\nsettings.\nFor\nthis\nreason, we"
        },
        {
          "Department of Computational Linguistics, University of Zurich, Zurich, Switzerland": "employ the MELD dataset [2] in this work, as it offers a more"
        },
        {
          "Department of Computational Linguistics, University of Zurich, Zurich, Switzerland": "realistic setting with varied background conditions, multiple"
        },
        {
          "Department of Computational Linguistics, University of Zurich, Zurich, Switzerland": "speakers, and environmental noise, providing a more chal-"
        },
        {
          "Department of Computational Linguistics, University of Zurich, Zurich, Switzerland": "lenging yet practical scenario for emotion recognition."
        },
        {
          "Department of Computational Linguistics, University of Zurich, Zurich, Switzerland": ""
        },
        {
          "Department of Computational Linguistics, University of Zurich, Zurich, Switzerland": "In this work, we propose a multimodal system designed"
        },
        {
          "Department of Computational Linguistics, University of Zurich, Zurich, Switzerland": ""
        },
        {
          "Department of Computational Linguistics, University of Zurich, Zurich, Switzerland": "for emotion recognition and sentiment analysis. This system"
        },
        {
          "Department of Computational Linguistics, University of Zurich, Zurich, Switzerland": ""
        },
        {
          "Department of Computational Linguistics, University of Zurich, Zurich, Switzerland": "is built upon four core components, each addressing a distinct"
        },
        {
          "Department of Computational Linguistics, University of Zurich, Zurich, Switzerland": ""
        },
        {
          "Department of Computational Linguistics, University of Zurich, Zurich, Switzerland": "modality.\nSpecifically, we utilize Wav2Vec2 [3]\nfor voice,"
        },
        {
          "Department of Computational Linguistics, University of Zurich, Zurich, Switzerland": ""
        },
        {
          "Department of Computational Linguistics, University of Zurich, Zurich, Switzerland": "RoBERTa [4]\nfor\ntext,\na proposed FacialNet\nfor\nfacial ex-"
        },
        {
          "Department of Computational Linguistics, University of Zurich, Zurich, Switzerland": ""
        },
        {
          "Department of Computational Linguistics, University of Zurich, Zurich, Switzerland": "pressions, and a CNN+Transformer architecture trained from"
        },
        {
          "Department of Computational Linguistics, University of Zurich, Zurich, Switzerland": ""
        },
        {
          "Department of Computational Linguistics, University of Zurich, Zurich, Switzerland": "scratch for video analysis."
        },
        {
          "Department of Computational Linguistics, University of Zurich, Zurich, Switzerland": ""
        },
        {
          "Department of Computational Linguistics, University of Zurich, Zurich, Switzerland": "The\nremainder\nof\nthis\npaper\nis\norganized\nas\nfollows:"
        },
        {
          "Department of Computational Linguistics, University of Zurich, Zurich, Switzerland": ""
        },
        {
          "Department of Computational Linguistics, University of Zurich, Zurich, Switzerland": "Section 2 reviews related work in emotion recognition and"
        },
        {
          "Department of Computational Linguistics, University of Zurich, Zurich, Switzerland": ""
        },
        {
          "Department of Computational Linguistics, University of Zurich, Zurich, Switzerland": "sentiment analysis.\nSection 3 describes the dataset and the"
        },
        {
          "Department of Computational Linguistics, University of Zurich, Zurich, Switzerland": ""
        },
        {
          "Department of Computational Linguistics, University of Zurich, Zurich, Switzerland": "proposed system architecture. Experimental\nresults are pre-"
        },
        {
          "Department of Computational Linguistics, University of Zurich, Zurich, Switzerland": ""
        },
        {
          "Department of Computational Linguistics, University of Zurich, Zurich, Switzerland": "sented in Section 4, followed by conclusions in Section 5."
        },
        {
          "Department of Computational Linguistics, University of Zurich, Zurich, Switzerland": ""
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "niques. These multimodal methods aim to address the limi-": "tations of unimodal analysis by combining information from"
        },
        {
          "niques. These multimodal methods aim to address the limi-": "diverse sources to enhance emotion recognition performance."
        },
        {
          "niques. These multimodal methods aim to address the limi-": "Zheng et al.\n[6] proposed a two-stage framework focus-"
        },
        {
          "niques. These multimodal methods aim to address the limi-": "ing on facial sequences. Their approach employs multi-task"
        },
        {
          "niques. These multimodal methods aim to address the limi-": "learning to generate emotion-informed visual representations."
        },
        {
          "niques. These multimodal methods aim to address the limi-": "Similarly, Hu et al.\n[7] developed the supervised adversarial"
        },
        {
          "niques. These multimodal methods aim to address the limi-": "contrastive learning framework, which incorporates contex-"
        },
        {
          "niques. These multimodal methods aim to address the limi-": "tual adversarial\ntraining alongside a Dual-LSTM structure to"
        },
        {
          "niques. These multimodal methods aim to address the limi-": "enhance emotion recognition from textual data."
        },
        {
          "niques. These multimodal methods aim to address the limi-": "Yun et al. [8] introduced the Teacher-leading Multimodal"
        },
        {
          "niques. These multimodal methods aim to address the limi-": "Fusion Network\nfor\nemotion\nrecognition, which\nemploys"
        },
        {
          "niques. These multimodal methods aim to address the limi-": "cross-modal knowledge distillation.\nIn this method, a lan-"
        },
        {
          "niques. These multimodal methods aim to address the limi-": "guage model\nis used to improve\nthe performance of non-"
        },
        {
          "niques. These multimodal methods aim to address the limi-": "verbal modalities. Fu et al.\n[9] presented a bi-model system"
        },
        {
          "niques. These multimodal methods aim to address the limi-": "for\nemotion\nthat\nuses\nprompts\nto\ngenerate\ncommonsense"
        },
        {
          "niques. These multimodal methods aim to address the limi-": "knowledge for\ninterlocutors based on historical utterances."
        },
        {
          "niques. These multimodal methods aim to address the limi-": "This model\nincorporates an interlocutor commonsense iden-"
        },
        {
          "niques. These multimodal methods aim to address the limi-": "tification task to fine-tune the system for identifying implicit"
        },
        {
          "niques. These multimodal methods aim to address the limi-": "speaker cues."
        },
        {
          "niques. These multimodal methods aim to address the limi-": "Chudasama et al.\n[10]\nintroduced a multimodal\nfusion"
        },
        {
          "niques. These multimodal methods aim to address the limi-": "network, which extracts emotion-relevant\nfeatures\nfrom vi-"
        },
        {
          "niques. These multimodal methods aim to address the limi-": "sual, audio, and textual modalities. Their model uses a multi-"
        },
        {
          "niques. These multimodal methods aim to address the limi-": "head\nattention-based\nfusion mechanism and\nintroduces\na"
        },
        {
          "niques. These multimodal methods aim to address the limi-": ""
        },
        {
          "niques. These multimodal methods aim to address the limi-": "novel\nfeature extractor\ntrained with adaptive margin-based"
        },
        {
          "niques. These multimodal methods aim to address the limi-": ""
        },
        {
          "niques. These multimodal methods aim to address the limi-": "triplet loss for the audio and visual modalities."
        },
        {
          "niques. These multimodal methods aim to address the limi-": "In another novel approach, Wu et al.\n[11] developed a"
        },
        {
          "niques. These multimodal methods aim to address the limi-": "system that applies large language models to speech emotion"
        },
        {
          "niques. These multimodal methods aim to address the limi-": ""
        },
        {
          "niques. These multimodal methods aim to address the limi-": "recognition. This method converts speech characteristics into"
        },
        {
          "niques. These multimodal methods aim to address the limi-": ""
        },
        {
          "niques. These multimodal methods aim to address the limi-": "natural language descriptions and incorporates them into text"
        },
        {
          "niques. These multimodal methods aim to address the limi-": ""
        },
        {
          "niques. These multimodal methods aim to address the limi-": "prompts, allowing for multimodal emotion analysis without"
        },
        {
          "niques. These multimodal methods aim to address the limi-": ""
        },
        {
          "niques. These multimodal methods aim to address the limi-": "modifying the model’s architecture."
        },
        {
          "niques. These multimodal methods aim to address the limi-": ""
        },
        {
          "niques. These multimodal methods aim to address the limi-": "Multimodal sentiment analysis in conversations has also"
        },
        {
          "niques. These multimodal methods aim to address the limi-": ""
        },
        {
          "niques. These multimodal methods aim to address the limi-": "gained traction as researchers seek to overcome the limita-"
        },
        {
          "niques. These multimodal methods aim to address the limi-": ""
        },
        {
          "niques. These multimodal methods aim to address the limi-": "tions of unimodal approaches. Poria et al. [2] compared vari-"
        },
        {
          "niques. These multimodal methods aim to address the limi-": ""
        },
        {
          "niques. These multimodal methods aim to address the limi-": "ous models, including Text-CNN, LSTM, and DialogueRNN,"
        },
        {
          "niques. These multimodal methods aim to address the limi-": ""
        },
        {
          "niques. These multimodal methods aim to address the limi-": "finding that multimodal fusion outperformed unimodal meth-"
        },
        {
          "niques. These multimodal methods aim to address the limi-": ""
        },
        {
          "niques. These multimodal methods aim to address the limi-": "ods, with DialogueRNN excelling in multi-party situations."
        },
        {
          "niques. These multimodal methods aim to address the limi-": ""
        },
        {
          "niques. These multimodal methods aim to address the limi-": "Shah et al.\n[12] took an ensemble learning approach to sen-"
        },
        {
          "niques. These multimodal methods aim to address the limi-": ""
        },
        {
          "niques. These multimodal methods aim to address the limi-": "timent analysis, integrating RoBERTa for text and Wav2Vec2"
        },
        {
          "niques. These multimodal methods aim to address the limi-": ""
        },
        {
          "niques. These multimodal methods aim to address the limi-": "for audio. Their study explored various ensemble techniques"
        },
        {
          "niques. These multimodal methods aim to address the limi-": ""
        },
        {
          "niques. These multimodal methods aim to address the limi-": "to improve sentiment classification accuracy through multi-"
        },
        {
          "niques. These multimodal methods aim to address the limi-": ""
        },
        {
          "niques. These multimodal methods aim to address the limi-": "modal fusion."
        },
        {
          "niques. These multimodal methods aim to address the limi-": ""
        },
        {
          "niques. These multimodal methods aim to address the limi-": ""
        },
        {
          "niques. These multimodal methods aim to address the limi-": "3. METHODOLOGY"
        },
        {
          "niques. These multimodal methods aim to address the limi-": ""
        },
        {
          "niques. These multimodal methods aim to address the limi-": "In the proposed system, we used several pre-trained models."
        },
        {
          "niques. These multimodal methods aim to address the limi-": "The use of pre-trained models is driven by the fact that avail-"
        },
        {
          "niques. These multimodal methods aim to address the limi-": "able multimodal emotion recognition and sentiment analy-"
        },
        {
          "niques. These multimodal methods aim to address the limi-": "sis datasets are often limited in size, making transfer\nlearn-"
        },
        {
          "niques. These multimodal methods aim to address the limi-": ""
        },
        {
          "niques. These multimodal methods aim to address the limi-": "ing an effective strategy to overcome this data scarcity chal-"
        },
        {
          "niques. These multimodal methods aim to address the limi-": "lenge. Various fusion techniques are commonly employed in"
        },
        {
          "niques. These multimodal methods aim to address the limi-": "multimodal systems, including sensory, feature, and score fu-"
        },
        {
          "niques. These multimodal methods aim to address the limi-": "sion and based on prior work, feature fusion has consistently"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table 1: Results in percentage for emotion recognition in",
      "data": [
        {
          "visual\nfeatures [2]. One of\nthe unique aspects of MELD is": ""
        },
        {
          "visual\nfeatures [2]. One of\nthe unique aspects of MELD is": "that\nit\nis derived from dialogues\nin the popular TV series"
        },
        {
          "visual\nfeatures [2]. One of\nthe unique aspects of MELD is": ""
        },
        {
          "visual\nfeatures [2]. One of\nthe unique aspects of MELD is": "Friends, which provides emotionally diverse conversations"
        },
        {
          "visual\nfeatures [2]. One of\nthe unique aspects of MELD is": ""
        },
        {
          "visual\nfeatures [2]. One of\nthe unique aspects of MELD is": "between multiple speakers\nin dynamic,\nreal-world-like set-"
        },
        {
          "visual\nfeatures [2]. One of\nthe unique aspects of MELD is": ""
        },
        {
          "visual\nfeatures [2]. One of\nthe unique aspects of MELD is": "tings. This makes MELD particularly valuable for real-world"
        },
        {
          "visual\nfeatures [2]. One of\nthe unique aspects of MELD is": "emotion recognition tasks, offering over 1,400 dialogue in-"
        },
        {
          "visual\nfeatures [2]. One of\nthe unique aspects of MELD is": "stances and more than 13,000 individual utterances."
        },
        {
          "visual\nfeatures [2]. One of\nthe unique aspects of MELD is": "Each utterance in the dataset is annotated with seven emo-"
        },
        {
          "visual\nfeatures [2]. One of\nthe unique aspects of MELD is": "tion and three sentiment\nlabels. The seven emotions include"
        },
        {
          "visual\nfeatures [2]. One of\nthe unique aspects of MELD is": "anger,\nfear,\ndisgust,\njoy,\nsadness,\nsurprise,\nand neutrality,"
        },
        {
          "visual\nfeatures [2]. One of\nthe unique aspects of MELD is": "while sentiment labels are positive, negative, and neutral."
        },
        {
          "visual\nfeatures [2]. One of\nthe unique aspects of MELD is": ""
        },
        {
          "visual\nfeatures [2]. One of\nthe unique aspects of MELD is": ""
        },
        {
          "visual\nfeatures [2]. One of\nthe unique aspects of MELD is": "3.2. RoBERTa"
        },
        {
          "visual\nfeatures [2]. One of\nthe unique aspects of MELD is": ""
        },
        {
          "visual\nfeatures [2]. One of\nthe unique aspects of MELD is": "RoBERTa (Robustly Optimized BERT Pretraining Approach)"
        },
        {
          "visual\nfeatures [2]. One of\nthe unique aspects of MELD is": "is\na\ntransformer-based\nlanguage model\nbuilt\nupon BERT"
        },
        {
          "visual\nfeatures [2]. One of\nthe unique aspects of MELD is": "(Bidirectional Encoder Representations from Transformers)."
        },
        {
          "visual\nfeatures [2]. One of\nthe unique aspects of MELD is": "It\nis designed to enhance performance by optimizing train-"
        },
        {
          "visual\nfeatures [2]. One of\nthe unique aspects of MELD is": "ing strategies,\nincluding larger batch sizes, more\ntraining"
        },
        {
          "visual\nfeatures [2]. One of\nthe unique aspects of MELD is": "data,\nand removing the next\nsentence prediction objective."
        },
        {
          "visual\nfeatures [2]. One of\nthe unique aspects of MELD is": "RoBERTa\n[4]\nemploys\na masked\nlanguage model, which"
        },
        {
          "visual\nfeatures [2]. One of\nthe unique aspects of MELD is": "learns contextualized word representations by predicting ran-"
        },
        {
          "visual\nfeatures [2]. One of\nthe unique aspects of MELD is": "domly masked tokens within a sentence based on surrounding"
        },
        {
          "visual\nfeatures [2]. One of\nthe unique aspects of MELD is": "context. By leveraging its bidirectional context understand-"
        },
        {
          "visual\nfeatures [2]. One of\nthe unique aspects of MELD is": "ing, RoBERTa captures subtle linguistic cues such as tone,"
        },
        {
          "visual\nfeatures [2]. One of\nthe unique aspects of MELD is": "word choice, and sentence structure, contributing to detecting"
        },
        {
          "visual\nfeatures [2]. One of\nthe unique aspects of MELD is": ""
        },
        {
          "visual\nfeatures [2]. One of\nthe unique aspects of MELD is": "underlying emotions."
        },
        {
          "visual\nfeatures [2]. One of\nthe unique aspects of MELD is": ""
        },
        {
          "visual\nfeatures [2]. One of\nthe unique aspects of MELD is": "3.3. Wav2Vec2"
        },
        {
          "visual\nfeatures [2]. One of\nthe unique aspects of MELD is": ""
        },
        {
          "visual\nfeatures [2]. One of\nthe unique aspects of MELD is": "Wav2Vec2 [3] has revolutionized speech processing by ap-"
        },
        {
          "visual\nfeatures [2]. One of\nthe unique aspects of MELD is": ""
        },
        {
          "visual\nfeatures [2]. One of\nthe unique aspects of MELD is": "plying self-supervised learning directly to raw audio data."
        },
        {
          "visual\nfeatures [2]. One of\nthe unique aspects of MELD is": ""
        },
        {
          "visual\nfeatures [2]. One of\nthe unique aspects of MELD is": "The model consists of a convolutional\nfeature extractor and"
        },
        {
          "visual\nfeatures [2]. One of\nthe unique aspects of MELD is": ""
        },
        {
          "visual\nfeatures [2]. One of\nthe unique aspects of MELD is": "a transformer-based contextual network that models tempo-"
        },
        {
          "visual\nfeatures [2]. One of\nthe unique aspects of MELD is": ""
        },
        {
          "visual\nfeatures [2]. One of\nthe unique aspects of MELD is": "ral dependencies in the input. During training, segments of"
        },
        {
          "visual\nfeatures [2]. One of\nthe unique aspects of MELD is": ""
        },
        {
          "visual\nfeatures [2]. One of\nthe unique aspects of MELD is": "the input speech are masked, and the network is tasked with"
        },
        {
          "visual\nfeatures [2]. One of\nthe unique aspects of MELD is": ""
        },
        {
          "visual\nfeatures [2]. One of\nthe unique aspects of MELD is": "predicting the masked portions,\nthus\nlearning rich speech"
        },
        {
          "visual\nfeatures [2]. One of\nthe unique aspects of MELD is": ""
        },
        {
          "visual\nfeatures [2]. One of\nthe unique aspects of MELD is": "representations."
        },
        {
          "visual\nfeatures [2]. One of\nthe unique aspects of MELD is": ""
        },
        {
          "visual\nfeatures [2]. One of\nthe unique aspects of MELD is": "Wav2Vec2 has demonstrated strong performance in vari-"
        },
        {
          "visual\nfeatures [2]. One of\nthe unique aspects of MELD is": ""
        },
        {
          "visual\nfeatures [2]. One of\nthe unique aspects of MELD is": "ous tasks,\nincluding speech recognition and emotion classifi-"
        },
        {
          "visual\nfeatures [2]. One of\nthe unique aspects of MELD is": ""
        },
        {
          "visual\nfeatures [2]. One of\nthe unique aspects of MELD is": "cation, due to its ability to model complex acoustic features"
        },
        {
          "visual\nfeatures [2]. One of\nthe unique aspects of MELD is": ""
        },
        {
          "visual\nfeatures [2]. One of\nthe unique aspects of MELD is": "directly from raw waveform data [3]."
        },
        {
          "visual\nfeatures [2]. One of\nthe unique aspects of MELD is": ""
        },
        {
          "visual\nfeatures [2]. One of\nthe unique aspects of MELD is": "3.4. CNN+Transformer"
        },
        {
          "visual\nfeatures [2]. One of\nthe unique aspects of MELD is": ""
        },
        {
          "visual\nfeatures [2]. One of\nthe unique aspects of MELD is": "We developed a model from scratch that extracts visual fea-"
        },
        {
          "visual\nfeatures [2]. One of\nthe unique aspects of MELD is": "tures not\nlimited to facial expressions\nto handle video un-"
        },
        {
          "visual\nfeatures [2]. One of\nthe unique aspects of MELD is": "derstanding. While previous works primarily focus on facial"
        },
        {
          "visual\nfeatures [2]. One of\nthe unique aspects of MELD is": "analysis, our approach captures broader visual cues, including"
        },
        {
          "visual\nfeatures [2]. One of\nthe unique aspects of MELD is": "the speaker’s posture, gestures, the presence of other individu-"
        },
        {
          "visual\nfeatures [2]. One of\nthe unique aspects of MELD is": "als, and background elements, which may contribute valuable"
        },
        {
          "visual\nfeatures [2]. One of\nthe unique aspects of MELD is": "context to emotion recognition."
        },
        {
          "visual\nfeatures [2]. One of\nthe unique aspects of MELD is": "Our architecture comprises three key components: a spa-"
        },
        {
          "visual\nfeatures [2]. One of\nthe unique aspects of MELD is": "tial modelling module, a temporal modelling module, and a"
        },
        {
          "visual\nfeatures [2]. One of\nthe unique aspects of MELD is": "classifier. We use MobileNetV2 [15] as a feature extractor"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table 3: presents the results of different integration ap-",
      "data": [
        {
          "multimodal vector for each input file. The classifier’s role is": ""
        },
        {
          "multimodal vector for each input file. The classifier’s role is": "learn from labeled sam-"
        },
        {
          "multimodal vector for each input file. The classifier’s role is": ""
        },
        {
          "multimodal vector for each input file. The classifier’s role is": "ples during training, and identify the crucial components of"
        },
        {
          "multimodal vector for each input file. The classifier’s role is": ""
        },
        {
          "multimodal vector for each input file. The classifier’s role is": "Various\nintegration strate-"
        },
        {
          "multimodal vector for each input file. The classifier’s role is": "gies have been explored, with their performance for emotion"
        },
        {
          "multimodal vector for each input file. The classifier’s role is": ""
        },
        {
          "multimodal vector for each input file. The classifier’s role is": ""
        },
        {
          "multimodal vector for each input file. The classifier’s role is": "the results of different\nintegration ap-"
        },
        {
          "multimodal vector for each input file. The classifier’s role is": "including both 3-modality"
        },
        {
          "multimodal vector for each input file. The classifier’s role is": "The text modality proves to be the"
        },
        {
          "multimodal vector for each input file. The classifier’s role is": "indicating its role as a primary modality. The"
        },
        {
          "multimodal vector for each input file. The classifier’s role is": "highest accuracy, 66.36%, was achieved by integrating all"
        },
        {
          "multimodal vector for each input file. The classifier’s role is": "four modalities, which represents a 2.02% improvement over"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 3: presents the results of different integration ap-",
      "data": [
        {
          "variability highlights the potential benefits of a multimodal": ""
        },
        {
          "variability highlights the potential benefits of a multimodal": "approach, where integrating multiple modalities can address"
        },
        {
          "variability highlights the potential benefits of a multimodal": ""
        },
        {
          "variability highlights the potential benefits of a multimodal": "individual weaknesses."
        },
        {
          "variability highlights the potential benefits of a multimodal": ""
        },
        {
          "variability highlights the potential benefits of a multimodal": "In a multimodal strategy,\nthe objective is to construct a"
        },
        {
          "variability highlights the potential benefits of a multimodal": ""
        },
        {
          "variability highlights the potential benefits of a multimodal": "multimodal vector for each input file. The classifier’s role is"
        },
        {
          "variability highlights the potential benefits of a multimodal": ""
        },
        {
          "variability highlights the potential benefits of a multimodal": "to process\nthis multimodal vector,\nlearn from labeled sam-"
        },
        {
          "variability highlights the potential benefits of a multimodal": ""
        },
        {
          "variability highlights the potential benefits of a multimodal": "ples during training, and identify the crucial components of"
        },
        {
          "variability highlights the potential benefits of a multimodal": ""
        },
        {
          "variability highlights the potential benefits of a multimodal": "the multimodal\nrepresentation.\nVarious\nintegration strate-"
        },
        {
          "variability highlights the potential benefits of a multimodal": "gies have been explored, with their performance for emotion"
        },
        {
          "variability highlights the potential benefits of a multimodal": ""
        },
        {
          "variability highlights the potential benefits of a multimodal": "recognition detailed in Table 3."
        },
        {
          "variability highlights the potential benefits of a multimodal": "Table 3 presents\nthe results of different\nintegration ap-"
        },
        {
          "variability highlights the potential benefits of a multimodal": "proaches\nfor\nsentiment analysis,\nincluding both 3-modality"
        },
        {
          "variability highlights the potential benefits of a multimodal": "and 4-modality fusion.\nThe text modality proves to be the"
        },
        {
          "variability highlights the potential benefits of a multimodal": "most effective,\nindicating its role as a primary modality. The"
        },
        {
          "variability highlights the potential benefits of a multimodal": "highest accuracy, 66.36%, was achieved by integrating all"
        },
        {
          "variability highlights the potential benefits of a multimodal": "four modalities, which represents a 2.02% improvement over"
        },
        {
          "variability highlights the potential benefits of a multimodal": "the best-performing in unimodal mode.\nThis result demon-"
        },
        {
          "variability highlights the potential benefits of a multimodal": "strates that the classifier effectively leverages the information"
        },
        {
          "variability highlights the potential benefits of a multimodal": ""
        },
        {
          "variability highlights the potential benefits of a multimodal": "from multiple modalities."
        },
        {
          "variability highlights the potential benefits of a multimodal": ""
        },
        {
          "variability highlights the potential benefits of a multimodal": ""
        },
        {
          "variability highlights the potential benefits of a multimodal": "For sentiment analysis,\nthe performance results of differ-"
        },
        {
          "variability highlights the potential benefits of a multimodal": ""
        },
        {
          "variability highlights the potential benefits of a multimodal": "ent integration approaches are shown in Table 4. The best per-"
        },
        {
          "variability highlights the potential benefits of a multimodal": ""
        },
        {
          "variability highlights the potential benefits of a multimodal": "formance, with an accuracy of 72.15%, was achieved through"
        },
        {
          "variability highlights the potential benefits of a multimodal": ""
        },
        {
          "variability highlights the potential benefits of a multimodal": "the integration of all four modalities.\nIn this evaluation,\nthe"
        },
        {
          "variability highlights the potential benefits of a multimodal": ""
        },
        {
          "variability highlights the potential benefits of a multimodal": "multimodal sentiment analysis outperforms the unimodal by"
        },
        {
          "variability highlights the potential benefits of a multimodal": ""
        },
        {
          "variability highlights the potential benefits of a multimodal": "2.94% for accuracy."
        },
        {
          "variability highlights the potential benefits of a multimodal": ""
        },
        {
          "variability highlights the potential benefits of a multimodal": ""
        },
        {
          "variability highlights the potential benefits of a multimodal": ""
        },
        {
          "variability highlights the potential benefits of a multimodal": ""
        },
        {
          "variability highlights the potential benefits of a multimodal": "5. CONCLUSION"
        },
        {
          "variability highlights the potential benefits of a multimodal": ""
        },
        {
          "variability highlights the potential benefits of a multimodal": ""
        },
        {
          "variability highlights the potential benefits of a multimodal": "Emotion recognition and sentiment analysis are closely re-"
        },
        {
          "variability highlights the potential benefits of a multimodal": "lated and challenging tasks in pattern recognition. With its"
        },
        {
          "variability highlights the potential benefits of a multimodal": "multi-party\nand\nconversational\nnature,\nthe MELD dataset"
        },
        {
          "variability highlights the potential benefits of a multimodal": "presents\nsignificant\nchallenges due\nto the\nextensive visual"
        },
        {
          "variability highlights the potential benefits of a multimodal": "and auditory variability in each sample. This variability re-"
        },
        {
          "variability highlights the potential benefits of a multimodal": "sults in differing performance for each modality across files,"
        },
        {
          "variability highlights the potential benefits of a multimodal": "highlighting\nthe\nneed\nfor multimodal\napproaches\nin\nsuch"
        },
        {
          "variability highlights the potential benefits of a multimodal": "scenarios."
        },
        {
          "variability highlights the potential benefits of a multimodal": ""
        },
        {
          "variability highlights the potential benefits of a multimodal": "In this work, we utilized pre-trained models\nincluding"
        },
        {
          "variability highlights the potential benefits of a multimodal": ""
        },
        {
          "variability highlights the potential benefits of a multimodal": "RoBERTa, Wav2Vec2, InceptionResNet, and also developed"
        },
        {
          "variability highlights the potential benefits of a multimodal": ""
        },
        {
          "variability highlights the potential benefits of a multimodal": "a model based on a CNN+Transformer architecture trained"
        },
        {
          "variability highlights the potential benefits of a multimodal": ""
        },
        {
          "variability highlights the potential benefits of a multimodal": "from scratch to capture comprehensive information for video"
        },
        {
          "variability highlights the potential benefits of a multimodal": ""
        },
        {
          "variability highlights the potential benefits of a multimodal": "analysis. By extracting feature embeddings from each modal-"
        },
        {
          "variability highlights the potential benefits of a multimodal": ""
        },
        {
          "variability highlights the potential benefits of a multimodal": "ity and concatenating them into a multimodal representation,"
        },
        {
          "variability highlights the potential benefits of a multimodal": ""
        },
        {
          "variability highlights the potential benefits of a multimodal": "we\nachieved superior performance\ncompared to unimodal"
        },
        {
          "variability highlights the potential benefits of a multimodal": "approaches.\nOur evaluation demonstrates\nthe effectiveness"
        },
        {
          "variability highlights the potential benefits of a multimodal": "of multimodal approaches for emotion recognition and sen-"
        },
        {
          "variability highlights the potential benefits of a multimodal": "timent analysis\nin near-to-real-world situations.\nAs\nfuture"
        },
        {
          "variability highlights the potential benefits of a multimodal": "trends,\nsegmenting the speaker out of\nframe could improve"
        },
        {
          "variability highlights the potential benefits of a multimodal": "two visual\nsystems, and a multi-network strategy based on"
        },
        {
          "variability highlights the potential benefits of a multimodal": "arousal and valence to recognize the effects could be benefi-"
        },
        {
          "variability highlights the potential benefits of a multimodal": "cial."
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "6. REFERENCES": "",
          "[11] Zehui Wu, Ziwei Gong, Lin Ai, Pengyuan Shi, Kaan": "Donbekci, and Julia Hirschberg, “Beyond silent letters:"
        },
        {
          "6. REFERENCES": "[1] Carlos Busso, Murtaza Bulut,\nChi-Chun Lee, Abe",
          "[11] Zehui Wu, Ziwei Gong, Lin Ai, Pengyuan Shi, Kaan": "Amplifying llms in emotion recognition with vocal nu-"
        },
        {
          "6. REFERENCES": "Kazemzadeh, Emily Mower, Samuel Kim, Jeannette N",
          "[11] Zehui Wu, Ziwei Gong, Lin Ai, Pengyuan Shi, Kaan": "ances,” arXiv preprint arXiv:2407.21315, 2024."
        },
        {
          "6. REFERENCES": "Chang,\nSungbok Lee,\nand\nShrikanth\nS Narayanan,",
          "[11] Zehui Wu, Ziwei Gong, Lin Ai, Pengyuan Shi, Kaan": ""
        },
        {
          "6. REFERENCES": "",
          "[11] Zehui Wu, Ziwei Gong, Lin Ai, Pengyuan Shi, Kaan": "[12] Shariq Shah, Hossein Ghomeshi, Edlira Vakaj, Em-"
        },
        {
          "6. REFERENCES": "“Iemocap:\nInteractive emotional dyadic motion capture",
          "[11] Zehui Wu, Ziwei Gong, Lin Ai, Pengyuan Shi, Kaan": ""
        },
        {
          "6. REFERENCES": "",
          "[11] Zehui Wu, Ziwei Gong, Lin Ai, Pengyuan Shi, Kaan": "mett Cooper, and Rasheed Mohammad, “An ensemble-"
        },
        {
          "6. REFERENCES": "database,” Language resources and evaluation, vol. 42,",
          "[11] Zehui Wu, Ziwei Gong, Lin Ai, Pengyuan Shi, Kaan": ""
        },
        {
          "6. REFERENCES": "",
          "[11] Zehui Wu, Ziwei Gong, Lin Ai, Pengyuan Shi, Kaan": "learning-based technique for bimodal sentiment analy-"
        },
        {
          "6. REFERENCES": "pp. 335–359, 2008.",
          "[11] Zehui Wu, Ziwei Gong, Lin Ai, Pengyuan Shi, Kaan": ""
        },
        {
          "6. REFERENCES": "",
          "[11] Zehui Wu, Ziwei Gong, Lin Ai, Pengyuan Shi, Kaan": "sis,” Big Data and Cognitive Computing, vol. 7, no. 2,"
        },
        {
          "6. REFERENCES": "[2] Soujanya Poria, Devamanyu Hazarika, Navonil Ma-",
          "[11] Zehui Wu, Ziwei Gong, Lin Ai, Pengyuan Shi, Kaan": "pp. 85, 2023."
        },
        {
          "6. REFERENCES": "jumder, Gautam Naik, Erik Cambria,\nand Rada Mi-",
          "[11] Zehui Wu, Ziwei Gong, Lin Ai, Pengyuan Shi, Kaan": ""
        },
        {
          "6. REFERENCES": "",
          "[11] Zehui Wu, Ziwei Gong, Lin Ai, Pengyuan Shi, Kaan": "[13] Aref\nFarhadipour, Masoumeh Chapariniya,\nTeodora"
        },
        {
          "6. REFERENCES": "halcea,\n“Meld: A multimodal multi-party dataset\nfor",
          "[11] Zehui Wu, Ziwei Gong, Lin Ai, Pengyuan Shi, Kaan": ""
        },
        {
          "6. REFERENCES": "",
          "[11] Zehui Wu, Ziwei Gong, Lin Ai, Pengyuan Shi, Kaan": "Vukovic,\nand Volker Dellwo,\n“Comparative analysis"
        },
        {
          "6. REFERENCES": "arXiv preprint\nemotion recognition in conversations,”",
          "[11] Zehui Wu, Ziwei Gong, Lin Ai, Pengyuan Shi, Kaan": ""
        },
        {
          "6. REFERENCES": "",
          "[11] Zehui Wu, Ziwei Gong, Lin Ai, Pengyuan Shi, Kaan": "of modality fusion approaches for audio-visual person"
        },
        {
          "6. REFERENCES": "arXiv:1810.02508, 2018.",
          "[11] Zehui Wu, Ziwei Gong, Lin Ai, Pengyuan Shi, Kaan": ""
        },
        {
          "6. REFERENCES": "",
          "[11] Zehui Wu, Ziwei Gong, Lin Ai, Pengyuan Shi, Kaan": "identification and verification,” 2024."
        },
        {
          "6. REFERENCES": "[3] Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed,",
          "[11] Zehui Wu, Ziwei Gong, Lin Ai, Pengyuan Shi, Kaan": ""
        },
        {
          "6. REFERENCES": "",
          "[11] Zehui Wu, Ziwei Gong, Lin Ai, Pengyuan Shi, Kaan": "[14] Francesco Barbieri, Jose Camacho-Collados, Leonardo"
        },
        {
          "6. REFERENCES": "and Michael Auli,\n“wav2vec 2.0: A framework for",
          "[11] Zehui Wu, Ziwei Gong, Lin Ai, Pengyuan Shi, Kaan": ""
        },
        {
          "6. REFERENCES": "",
          "[11] Zehui Wu, Ziwei Gong, Lin Ai, Pengyuan Shi, Kaan": "Neves, and Luis Espinosa-Anke,\n“Tweeteval: Unified"
        },
        {
          "6. REFERENCES": "self-supervised learning of speech representations,” Ad-",
          "[11] Zehui Wu, Ziwei Gong, Lin Ai, Pengyuan Shi, Kaan": ""
        },
        {
          "6. REFERENCES": "",
          "[11] Zehui Wu, Ziwei Gong, Lin Ai, Pengyuan Shi, Kaan": "benchmark and comparative evaluation for tweet classi-"
        },
        {
          "6. REFERENCES": "vances\nin neural\ninformation processing systems, vol.",
          "[11] Zehui Wu, Ziwei Gong, Lin Ai, Pengyuan Shi, Kaan": ""
        },
        {
          "6. REFERENCES": "",
          "[11] Zehui Wu, Ziwei Gong, Lin Ai, Pengyuan Shi, Kaan": "fication,” arXiv preprint arXiv:2010.12421, 2020."
        },
        {
          "6. REFERENCES": "33, pp. 12449–12460, 2020.",
          "[11] Zehui Wu, Ziwei Gong, Lin Ai, Pengyuan Shi, Kaan": ""
        },
        {
          "6. REFERENCES": "",
          "[11] Zehui Wu, Ziwei Gong, Lin Ai, Pengyuan Shi, Kaan": "[15] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey"
        },
        {
          "6. REFERENCES": "[4] Yinhan Liu,\n“Roberta: A robustly optimized bert pre-",
          "[11] Zehui Wu, Ziwei Gong, Lin Ai, Pengyuan Shi, Kaan": ""
        },
        {
          "6. REFERENCES": "",
          "[11] Zehui Wu, Ziwei Gong, Lin Ai, Pengyuan Shi, Kaan": "Zhmoginov, and Liang-Chieh Chen, “Mobilenetv2:\nIn-"
        },
        {
          "6. REFERENCES": "training approach,”\narXiv preprint arXiv:1907.11692,",
          "[11] Zehui Wu, Ziwei Gong, Lin Ai, Pengyuan Shi, Kaan": ""
        },
        {
          "6. REFERENCES": "",
          "[11] Zehui Wu, Ziwei Gong, Lin Ai, Pengyuan Shi, Kaan": "verted residuals and linear bottlenecks,” in Proceedings"
        },
        {
          "6. REFERENCES": "2019.",
          "[11] Zehui Wu, Ziwei Gong, Lin Ai, Pengyuan Shi, Kaan": ""
        },
        {
          "6. REFERENCES": "",
          "[11] Zehui Wu, Ziwei Gong, Lin Ai, Pengyuan Shi, Kaan": "of\nthe IEEE conference on computer vision and pattern"
        },
        {
          "6. REFERENCES": "",
          "[11] Zehui Wu, Ziwei Gong, Lin Ai, Pengyuan Shi, Kaan": "recognition, 2018, pp. 4510–4520."
        },
        {
          "6. REFERENCES": "[5] Aref Farhadipour and Pouya Taghipour,\n“Facial emo-",
          "[11] Zehui Wu, Ziwei Gong, Lin Ai, Pengyuan Shi, Kaan": ""
        },
        {
          "6. REFERENCES": "tion recognition under mask coverage using a data aug-",
          "[11] Zehui Wu, Ziwei Gong, Lin Ai, Pengyuan Shi, Kaan": ""
        },
        {
          "6. REFERENCES": "",
          "[11] Zehui Wu, Ziwei Gong, Lin Ai, Pengyuan Shi, Kaan": "Advances in\n[16] A Vaswani,\n“Attention is all you need,”"
        },
        {
          "6. REFERENCES": "mentation technique,”\nin 2023 13th International Con-",
          "[11] Zehui Wu, Ziwei Gong, Lin Ai, Pengyuan Shi, Kaan": ""
        },
        {
          "6. REFERENCES": "",
          "[11] Zehui Wu, Ziwei Gong, Lin Ai, Pengyuan Shi, Kaan": "Neural Information Processing Systems, 2017."
        },
        {
          "6. REFERENCES": "ference on Computer and Knowledge Engineering (IC-",
          "[11] Zehui Wu, Ziwei Gong, Lin Ai, Pengyuan Shi, Kaan": ""
        },
        {
          "6. REFERENCES": "CKE). IEEE, 2023, pp. 001–006.",
          "[11] Zehui Wu, Ziwei Gong, Lin Ai, Pengyuan Shi, Kaan": "[17] Hossein Ranjbar and Alireza Taheri,\n“Continuous sign"
        },
        {
          "6. REFERENCES": "",
          "[11] Zehui Wu, Ziwei Gong, Lin Ai, Pengyuan Shi, Kaan": "language recognition using intra-inter gloss attention,”"
        },
        {
          "6. REFERENCES": "[6] Wenjie Zheng, Jianfei Yu, Rui Xia, and Shijin Wang, “A",
          "[11] Zehui Wu, Ziwei Gong, Lin Ai, Pengyuan Shi, Kaan": "arXiv preprint arXiv:2406.18333, 2024."
        },
        {
          "6. REFERENCES": "facial expression-aware multimodal multi-task learning",
          "[11] Zehui Wu, Ziwei Gong, Lin Ai, Pengyuan Shi, Kaan": ""
        },
        {
          "6. REFERENCES": "",
          "[11] Zehui Wu, Ziwei Gong, Lin Ai, Pengyuan Shi, Kaan": "[18] Annette Rios, Uwe Reichel, Chirag Bhuvaneshwara,"
        },
        {
          "6. REFERENCES": "framework for emotion recognition in multi-party con-",
          "[11] Zehui Wu, Ziwei Gong, Lin Ai, Pengyuan Shi, Kaan": ""
        },
        {
          "6. REFERENCES": "",
          "[11] Zehui Wu, Ziwei Gong, Lin Ai, Pengyuan Shi, Kaan": "Panagiotis Filntisis, Petros Maragos, Felix Burkhardt,"
        },
        {
          "6. REFERENCES": "versations,” in Proceedings of the 61st Annual Meeting",
          "[11] Zehui Wu, Ziwei Gong, Lin Ai, Pengyuan Shi, Kaan": ""
        },
        {
          "6. REFERENCES": "",
          "[11] Zehui Wu, Ziwei Gong, Lin Ai, Pengyuan Shi, Kaan": "Florian Eyben, Bj¨orn Schuller, Fabrizio Nunnari, and"
        },
        {
          "6. REFERENCES": "of\nthe Association for Computational Linguistics (Vol-",
          "[11] Zehui Wu, Ziwei Gong, Lin Ai, Pengyuan Shi, Kaan": ""
        },
        {
          "6. REFERENCES": "",
          "[11] Zehui Wu, Ziwei Gong, Lin Ai, Pengyuan Shi, Kaan": "Sarah Ebling,\n“Multimodal\nrecognition\nof\nvalence,"
        },
        {
          "6. REFERENCES": "ume 1: Long Papers), 2023, pp. 15445–15459.",
          "[11] Zehui Wu, Ziwei Gong, Lin Ai, Pengyuan Shi, Kaan": ""
        },
        {
          "6. REFERENCES": "",
          "[11] Zehui Wu, Ziwei Gong, Lin Ai, Pengyuan Shi, Kaan": "arousal and dominance via late-fusion of text, audio and"
        },
        {
          "6. REFERENCES": "[7] Dou Hu, Yinan Bao, Lingwei Wei, Wei Zhou,\nand",
          "[11] Zehui Wu, Ziwei Gong, Lin Ai, Pengyuan Shi, Kaan": "facial expressions,” 2023."
        },
        {
          "6. REFERENCES": "Songlin Hu,\n“Supervised adversarial contrastive learn-",
          "[11] Zehui Wu, Ziwei Gong, Lin Ai, Pengyuan Shi, Kaan": ""
        },
        {
          "6. REFERENCES": "",
          "[11] Zehui Wu, Ziwei Gong, Lin Ai, Pengyuan Shi, Kaan": "[19] Rewon Child,\nScott Gray, Alec Radford,\nand\nIlya"
        },
        {
          "6. REFERENCES": "arXiv\ning for emotion recognition in conversations,”",
          "[11] Zehui Wu, Ziwei Gong, Lin Ai, Pengyuan Shi, Kaan": ""
        },
        {
          "6. REFERENCES": "",
          "[11] Zehui Wu, Ziwei Gong, Lin Ai, Pengyuan Shi, Kaan": "Sutskever,\n“Generating long sequences with sparse"
        },
        {
          "6. REFERENCES": "preprint arXiv:2306.01505, 2023.",
          "[11] Zehui Wu, Ziwei Gong, Lin Ai, Pengyuan Shi, Kaan": ""
        },
        {
          "6. REFERENCES": "",
          "[11] Zehui Wu, Ziwei Gong, Lin Ai, Pengyuan Shi, Kaan": "transformers,” arXiv preprint arXiv:1904.10509, 2019."
        },
        {
          "6. REFERENCES": "[8] Taeyang Yun, Hyunkuk Lim, Jeonghwan Lee, and Min",
          "[11] Zehui Wu, Ziwei Gong, Lin Ai, Pengyuan Shi, Kaan": ""
        },
        {
          "6. REFERENCES": "",
          "[11] Zehui Wu, Ziwei Gong, Lin Ai, Pengyuan Shi, Kaan": "[20] Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke,"
        },
        {
          "6. REFERENCES": "Song,\n“Telme: Teacher-leading multimodal fusion net-",
          "[11] Zehui Wu, Ziwei Gong, Lin Ai, Pengyuan Shi, Kaan": ""
        },
        {
          "6. REFERENCES": "",
          "[11] Zehui Wu, Ziwei Gong, Lin Ai, Pengyuan Shi, Kaan": "and Alexander Alemi,\n“Inception-v4,\ninception-resnet"
        },
        {
          "6. REFERENCES": "arXiv\nwork for emotion recognition in conversation,”",
          "[11] Zehui Wu, Ziwei Gong, Lin Ai, Pengyuan Shi, Kaan": ""
        },
        {
          "6. REFERENCES": "",
          "[11] Zehui Wu, Ziwei Gong, Lin Ai, Pengyuan Shi, Kaan": "and the impact of residual connections on learning,”\nin"
        },
        {
          "6. REFERENCES": "preprint arXiv:2401.12987, 2024.",
          "[11] Zehui Wu, Ziwei Gong, Lin Ai, Pengyuan Shi, Kaan": ""
        },
        {
          "6. REFERENCES": "",
          "[11] Zehui Wu, Ziwei Gong, Lin Ai, Pengyuan Shi, Kaan": "Proceedings of the AAAI conference on artificial intelli-"
        },
        {
          "6. REFERENCES": "",
          "[11] Zehui Wu, Ziwei Gong, Lin Ai, Pengyuan Shi, Kaan": "gence, 2017, vol. 31."
        },
        {
          "6. REFERENCES": "[9] Yumeng Fu,\n“Ckerc:\nJoint\nlarge language models with",
          "[11] Zehui Wu, Ziwei Gong, Lin Ai, Pengyuan Shi, Kaan": ""
        },
        {
          "6. REFERENCES": "commonsense knowledge\nfor\nemotion\nrecognition in",
          "[11] Zehui Wu, Ziwei Gong, Lin Ai, Pengyuan Shi, Kaan": ""
        },
        {
          "6. REFERENCES": "",
          "[11] Zehui Wu, Ziwei Gong, Lin Ai, Pengyuan Shi, Kaan": "[21] Dong Yi, Zhen Lei, Shengcai Liao,\nand Stan Z Li,"
        },
        {
          "6. REFERENCES": "conversation,” arXiv preprint arXiv:2403.07260, 2024.",
          "[11] Zehui Wu, Ziwei Gong, Lin Ai, Pengyuan Shi, Kaan": ""
        },
        {
          "6. REFERENCES": "",
          "[11] Zehui Wu, Ziwei Gong, Lin Ai, Pengyuan Shi, Kaan": "arXiv\n“Learning\nface\nrepresentation\nfrom scratch,”"
        },
        {
          "6. REFERENCES": "",
          "[11] Zehui Wu, Ziwei Gong, Lin Ai, Pengyuan Shi, Kaan": "preprint arXiv:1411.7923, 2014."
        },
        {
          "6. REFERENCES": "[10] Vishal Chudasama, Purbayan Kar, Ashish Gudmalwar,",
          "[11] Zehui Wu, Ziwei Gong, Lin Ai, Pengyuan Shi, Kaan": ""
        },
        {
          "6. REFERENCES": "Nirmesh Shah,\nPankaj Wasnik,\nand Naoyuki Onoe,",
          "[11] Zehui Wu, Ziwei Gong, Lin Ai, Pengyuan Shi, Kaan": ""
        },
        {
          "6. REFERENCES": "",
          "[11] Zehui Wu, Ziwei Gong, Lin Ai, Pengyuan Shi, Kaan": "[22]\nIlya\nLoshchilov\nand\nFrank\nHutter,\n“Decoupled"
        },
        {
          "6. REFERENCES": "“M2fnet:\nMulti-modal\nfusion\nnetwork\nfor\nemotion",
          "[11] Zehui Wu, Ziwei Gong, Lin Ai, Pengyuan Shi, Kaan": ""
        },
        {
          "6. REFERENCES": "",
          "[11] Zehui Wu, Ziwei Gong, Lin Ai, Pengyuan Shi, Kaan": "arXiv\npreprint\nweight\ndecay\nregularization,”"
        },
        {
          "6. REFERENCES": "recognition in conversation,”\nin IEEE/CVF, 2022, pp.",
          "[11] Zehui Wu, Ziwei Gong, Lin Ai, Pengyuan Shi, Kaan": ""
        },
        {
          "6. REFERENCES": "",
          "[11] Zehui Wu, Ziwei Gong, Lin Ai, Pengyuan Shi, Kaan": "arXiv:1711.05101, 2017."
        },
        {
          "6. REFERENCES": "4652–4661.",
          "[11] Zehui Wu, Ziwei Gong, Lin Ai, Pengyuan Shi, Kaan": ""
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "Carlos Busso",
        "Murtaza Bulut",
        "Chi-Chun Lee",
        "Abe Kazemzadeh",
        "Emily Mower",
        "Samuel Kim",
        "Jeannette Chang",
        "Sungbok Lee",
        "Shrikanth S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "3",
      "title": "Meld: A multimodal multi-party dataset for emotion recognition in conversations",
      "authors": [
        "Soujanya Poria",
        "Devamanyu Hazarika",
        "Navonil Majumder",
        "Gautam Naik",
        "Erik Cambria",
        "Rada Mihalcea"
      ],
      "year": "2018",
      "venue": "Meld: A multimodal multi-party dataset for emotion recognition in conversations",
      "arxiv": "arXiv:1810.02508"
    },
    {
      "citation_id": "4",
      "title": "wav2vec 2.0: A framework for self-supervised learning of speech representations",
      "authors": [
        "Alexei Baevski",
        "Yuhao Zhou",
        "Abdelrahman Mohamed",
        "Michael Auli"
      ],
      "year": "2020",
      "venue": "wav2vec 2.0: A framework for self-supervised learning of speech representations"
    },
    {
      "citation_id": "5",
      "title": "Roberta: A robustly optimized bert pretraining approach",
      "authors": [
        "Yinhan Liu"
      ],
      "year": "2019",
      "venue": "Roberta: A robustly optimized bert pretraining approach",
      "arxiv": "arXiv:1907.11692"
    },
    {
      "citation_id": "6",
      "title": "Facial emotion recognition under mask coverage using a data augmentation technique",
      "authors": [
        "Aref Farhadipour",
        "Pouya Taghipour"
      ],
      "year": "2023",
      "venue": "2023 13th International Conference on Computer and Knowledge Engineering"
    },
    {
      "citation_id": "7",
      "title": "A facial expression-aware multimodal multi-task learning framework for emotion recognition in multi-party conversations",
      "authors": [
        "Wenjie Zheng",
        "Jianfei Yu",
        "Rui Xia",
        "Shijin Wang"
      ],
      "year": "2023",
      "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "8",
      "title": "Supervised adversarial contrastive learning for emotion recognition in conversations",
      "authors": [
        "Dou Hu",
        "Yinan Bao",
        "Lingwei Wei",
        "Wei Zhou",
        "Songlin Hu"
      ],
      "year": "2023",
      "venue": "Supervised adversarial contrastive learning for emotion recognition in conversations",
      "arxiv": "arXiv:2306.01505"
    },
    {
      "citation_id": "9",
      "title": "Telme: Teacher-leading multimodal fusion network for emotion recognition in conversation",
      "authors": [
        "Taeyang Yun",
        "Hyunkuk Lim",
        "Jeonghwan Lee",
        "Min Song"
      ],
      "year": "2024",
      "venue": "Telme: Teacher-leading multimodal fusion network for emotion recognition in conversation",
      "arxiv": "arXiv:2401.12987"
    },
    {
      "citation_id": "10",
      "title": "Ckerc: Joint large language models with commonsense knowledge for emotion recognition in conversation",
      "authors": [
        "Yumeng Fu"
      ],
      "year": "2024",
      "venue": "Ckerc: Joint large language models with commonsense knowledge for emotion recognition in conversation",
      "arxiv": "arXiv:2403.07260"
    },
    {
      "citation_id": "11",
      "title": "M2fnet: Multi-modal fusion network for emotion recognition in conversation",
      "authors": [
        "Purbayan Vishal Chudasama",
        "Ashish Kar",
        "Nirmesh Gudmalwar",
        "Pankaj Shah",
        "Naoyuki Wasnik",
        "Onoe"
      ],
      "year": "2022",
      "venue": "IEEE/CVF"
    },
    {
      "citation_id": "12",
      "title": "Beyond silent letters: Amplifying llms in emotion recognition with vocal nuances",
      "authors": [
        "Zehui Wu",
        "Ziwei Gong",
        "Lin Ai",
        "Pengyuan Shi",
        "Kaan Donbekci",
        "Julia Hirschberg"
      ],
      "year": "2024",
      "venue": "Beyond silent letters: Amplifying llms in emotion recognition with vocal nuances",
      "arxiv": "arXiv:2407.21315"
    },
    {
      "citation_id": "13",
      "title": "An ensemblelearning-based technique for bimodal sentiment analysis",
      "authors": [
        "Shariq Shah",
        "Hossein Ghomeshi",
        "Edlira Vakaj",
        "Emmett Cooper",
        "Rasheed Mohammad"
      ],
      "year": "2023",
      "venue": "Big Data and Cognitive Computing"
    },
    {
      "citation_id": "14",
      "title": "Comparative analysis of modality fusion approaches for audio-visual person identification and verification",
      "authors": [
        "Aref Farhadipour",
        "Masoumeh Chapariniya",
        "Teodora Vukovic",
        "Volker Dellwo"
      ],
      "year": "2024",
      "venue": "Comparative analysis of modality fusion approaches for audio-visual person identification and verification"
    },
    {
      "citation_id": "15",
      "title": "Tweeteval: Unified benchmark and comparative evaluation for tweet classification",
      "authors": [
        "Francesco Barbieri",
        "Jose Camacho-Collados",
        "Leonardo Neves",
        "Luis Espinosa-Anke"
      ],
      "year": "2020",
      "venue": "Tweeteval: Unified benchmark and comparative evaluation for tweet classification",
      "arxiv": "arXiv:2010.12421"
    },
    {
      "citation_id": "16",
      "title": "Mobilenetv2: Inverted residuals and linear bottlenecks",
      "authors": [
        "Mark Sandler",
        "Andrew Howard",
        "Menglong Zhu",
        "Andrey Zhmoginov",
        "Liang-Chieh Chen"
      ],
      "year": "2018",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "17",
      "title": "Attention is all you need",
      "authors": [
        "Vaswani"
      ],
      "year": "2017",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "18",
      "title": "Continuous sign language recognition using intra-inter gloss attention",
      "authors": [
        "Hossein Ranjbar",
        "Alireza Taheri"
      ],
      "year": "2024",
      "venue": "Continuous sign language recognition using intra-inter gloss attention",
      "arxiv": "arXiv:2406.18333"
    },
    {
      "citation_id": "19",
      "title": "Multimodal recognition of valence, arousal and dominance via late-fusion of text, audio and facial expressions",
      "authors": [
        "Annette Rios",
        "Uwe Reichel",
        "Chirag Bhuvaneshwara",
        "Panagiotis Filntisis",
        "Petros Maragos",
        "Felix Burkhardt",
        "Florian Eyben",
        "Björn Schuller",
        "Fabrizio Nunnari",
        "Sarah Ebling"
      ],
      "year": "2023",
      "venue": "Multimodal recognition of valence, arousal and dominance via late-fusion of text, audio and facial expressions"
    },
    {
      "citation_id": "20",
      "title": "Generating long sequences with sparse transformers",
      "authors": [
        "Rewon Child",
        "Scott Gray",
        "Alec Radford",
        "Ilya Sutskever"
      ],
      "year": "2019",
      "venue": "Generating long sequences with sparse transformers",
      "arxiv": "arXiv:1904.10509"
    },
    {
      "citation_id": "21",
      "title": "Inception-v4, inception-resnet and the impact of residual connections on learning",
      "authors": [
        "Christian Szegedy",
        "Sergey Ioffe",
        "Vincent Vanhoucke",
        "Alexander Alemi"
      ],
      "year": "2017",
      "venue": "Proceedings of the AAAI conference on artificial intelligence"
    },
    {
      "citation_id": "22",
      "title": "Learning face representation from scratch",
      "authors": [
        "Dong Yi",
        "Zhen Lei",
        "Shengcai Liao",
        "Stan Li"
      ],
      "year": "2014",
      "venue": "Learning face representation from scratch",
      "arxiv": "arXiv:1411.7923"
    },
    {
      "citation_id": "23",
      "title": "Decoupled weight decay regularization",
      "authors": [
        "Ilya Loshchilov",
        "Frank Hutter"
      ],
      "year": "2017",
      "venue": "Decoupled weight decay regularization",
      "arxiv": "arXiv:1711.05101"
    }
  ]
}