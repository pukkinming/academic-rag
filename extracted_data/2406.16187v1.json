{
  "paper_id": "2406.16187v1",
  "title": "Evaluation And Comparison Of Emotionally Evocative Image Augmentation Methods",
  "published": "2024-06-23T18:43:46Z",
  "authors": [
    "Jan Ignatowicz",
    "Krzysztof Kutt",
    "Grzegorz J. Nalepa"
  ],
  "keywords": [
    "Generative Adversarial Networks",
    "Affective Computing",
    "Emotions",
    "Images"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Experiments in affective computing are based on stimulus datasets that, in the process of standardization, receive metadata describing which emotions each stimulus evokes. In this paper, we explore an approach to creating stimulus datasets for affective computing using generative adversarial networks (GANs). Traditional dataset preparation methods are costly and time consuming, prompting our investigation of alternatives. We conducted experiments with various GAN architectures, including Deep Convolutional GAN, Conditional GAN, Auxiliary Classifier GAN, Progressive Augmentation GAN, and Wasserstein GAN, alongside data augmentation and transfer learning techniques. Our findings highlight promising advances in the generation of emotionally evocative synthetic images, suggesting significant potential for future research and improvements in this domain.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction And Motivation",
      "text": "Affective computing (AfC)-an interdisciplinary area of research on emotions involving their recognition, processing, and simulation in computer systems  [24] -is based on, inter alia, carefully planned experiments in which participants are exposed to stimuli that evoke specific emotions  [18] . The reactions are then collected in an appropriate manner, recorded, and finally, relevant emotion models are developed based on them.\n\nAmong the most common types of stimuli are images, usually depicting scenes, faces, or objects intended to evoke specific emotions (see Fig.  3 ). These are grouped in datasets, which usually contain dozens to more than a thousand stimuli, along with metadata that specifies the emotion the image evokes  [6] . However, since a single study may use several hundred stimuli, the few existing datasets containing affective images contain too few stimuli relative to research needs. A participant taking part in a few studies has a high chance of encountering images that they are already familiar with, making their responses weaker due to habituation effect  [6] .\n\nThe solution to the problem would be to create new datasets or expand existing ones, but in a \"classical\" approach this is very costly, due to the need to present each new stimulus to a sufficiently large study group to provide standardized metadata (e.g.,  [19, 20] ). In this paper, we want to present a different solution, based on the use of generative models to automatically learn the natural features of a dataset. To the best of our knowledge, no one has previously reported an attempt to create a dataset of affective images in this manner.\n\nThe task of generative models is to produce new synthetic samples with features that look like the features of the real elements of the training set  [10] . The task of generative models is therefore unsupervised learning, in which the model tries to capture the properties of the dataset and represent them in a probabilistic rather than deterministic manner, as it must be able to generate many new samples. To adapt this approach to the needs of AfC, we divided the training set into subsets containing images that evoke distinct emotions and prepared an image label generative model for each emotion so selected.\n\nThe paper is organized as follows. In Sect. 2, we outline generative models considered in the study. Then, in Sect. 3, datasets used in the research are summarized. The data processing and models training are described in Sect. 4. The evaluation is reported in Sect. 5. In Sect. 6 DALL-E usage is presented as fulfillment of affective generation task and the paper is concluded in Sect. 7.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Generative Models",
      "text": "One of the most important attempts to handle with generative modeling problem was presented in  [11] . Ian J. Goodfellow introduced a new machine learning framework named Generative Adversarial Networks (GANs). The architecture of this framework is constructed with two distinct models, further called generator and discriminator. The generator is supposed to create new images similar to the training images while the discriminator has to claim whether achieved input image is real or fake.\n\nThe GAN training is performed on the minimax algorithm working according to the Nash equilibrium. Both networks (discriminative and generative) learn in the best possible way for them. With each learning epoch the generator tries to produce better fake images. Meanwhile the discriminator is still learning which examples are real or fake and tries to find them out every time. The equilibrium of this minimax game is obtained when the generator generates perfect fake images and the discriminator cannot judge which image is real and which is fake. It leads to the situation when discriminator always gives his response at 50% confidence level.\n\nThe research utilized various GAN architectures, notably the Deep Convolutional GAN (DCGAN)  [25]  which employs convolution layers for processing. Conditional GANs (cGAN)  [22]  and Auxiliary Classifier GANs (ACGAN)  [23]  were used for image generation with categorical emotion conditions, improving data generation for imbalanced datasets by embedding additional label information into both generator and discriminator networks. Progressive Augmentation GANs (PAGANs)  [33]  address discriminator stability by gradually increasing task difficulty, acting as a regularization technique. Lastly, Wasserstein GANs (WGAN)  [1]  and its variant with Gradient Penalty  [12]  focus on modifying the cost function to enhance training effectiveness, demonstrating an alternate approach to generator training that lessens the importance of network balance.\n\nDataset augmentation is crucial for machine learning, especially in computer vision, to prevent overfitting due to small datasets. Numerous methods  [28]  exist to artificially expand datasets, significantly enhancing model performance.\n\nTransfer Learning (TL) repurposes knowledge from one domain to another, crucial for tasks with limited data. It includes using pretrained ConvNets as feature extractors or fine-tuning them on new datasets, aiding in overcoming dataset size limitations, particularly when pretrained on extensive collections like ImageNet  [8] .\n\nPretrained models, such as BigGAN  [5]  and StyleGAN  [15] , provide a foundation for generating quality images. Despite the rise of TL, pretrained models, mainly for classification, remain pivotal. They can be retrained with affective datasets to augment data with new, label-linked images.\n\nEffective GAN evaluation metrics should balance  [3] ,  [4]  fidelity, diversity, and controllability of samples, align with human judgment, and maintain low complexity. Key metrics include the Inception Score (IS)  [27] , measuring quality and diversity; Fréchet Inception Distance (FID)  [14] , assessing similarity to real images; and Kernel Inception Distance (KID)  [2] , a variant of FID emphasizing statistical similarity.\n\n[a]\n\n[b]",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Datasets",
      "text": "This study employed six datasets of affective images, encompassing a wide array of emotions and scenes. The International Affective Picture System (IAPS)  [19]  includes a broad spectrum of images such as animals, people, everyday objects, and landscapes. The Geneva Affective Picture Database (GAPED)  [7]  focuses on images eliciting negative, neutral, and positive emotions through themes like moral violations, animal mistreatment, and more uplifting images of babies and landscapes. The Nencki Affective Picture System (NAPS)  [20]  provides high-quality photographs across five categories: people, faces, animals, objects, and landscapes. The Set of Fear Including Pictures (SFIP)  [21]  offers images specifically designed to induce fear, covering categories such as social exposure and small animals, filling a niche gap in affective imagery. The Open Affective Standardized Image Set (OASIS)  [17]  and EmoMadrid  [6]  both supply open-access images sourced from the Internet across categories like landscapes, animals, and objects.\n\nCollectively, these datasets contribute 5866 unique images to the research. Each image is annotated according to Russel's circumplex model of emotions, with two key dimensions: valence (ranging from pleasant to unpleasant) and arousal (ranging from calm to excitement), enabling the study of complex emotions, using just two dimensions  [9]  (see Fig.  1[a] ). Despite the original datasets using various scales for these dimensions, they were normalized to a consistent range of [-1,1] for this analysis. 5866 images were collected in total (for sample images, see Fig.  1 [b]), along with the ratings of valence and arousal (see Fig.  2[a] ). The amounts of images per quarter is shown in the Tab. 1. The representations of each quarter are not equal, indeed they are quantitatively distant from each other. The biggest difference is between the third and fourth quarter. All this is due to too little data, especially the third quarter has significant gaps in its representation. Most of the data provided in the third quarter was contributed by the SFIP collection.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Generation Of Affective Pictures",
      "text": "This section outlines the ablation study focused on data preparation, model implementation, and leveraging publicly available resources for dataset augmentation. Implementations draw on state-of-the-art techniques, tailored to specific data needs with enhancements for shared architectural elements. The foundation is the DCGAN architecture for image generation, expanded with conditional elements for class-specific image creation. Model variations stem from their unique development requirements, with a uniform training approach across all models.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Preprocessing",
      "text": "The prepared Affective Dataset contains all the images gathered in one place and all the labels juxtapositioned in the same schema along with connection two-dimensional labels to the images.\n\nFor dimension reduction reason and in order to apply labels to images, the two-dimensional valence-arousal space was divided into 13 classes (Fig.  2[b] ) to reflect model depicted on Fig.  1[a] . In summary, the Table  2  presents the amounts of data consisting of each category. By comparing this data, it is again confirmed, that there exists a problem with representations of some classes. The most unfilled with data spaces are for categories: Tense (50 images) and Excited (36 images). There can be also observed that spread of data representation is very wide; for Neutral category there are 1036 representing images whereas for Excited category there are only 36 representing images.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Generative Models",
      "text": "The study incorporated data augmentation to enhance model performance, utilizing the Pillow library to expand an Affective Dataset. Techniques included image enhancement with detail and edge filters, brightness adjustment (lightening by a factor of 1.2 and darkening by 0.9), and rotation at 90, 180, and 270 degrees. Each augmented image retained the original's category, leading to a sevenfold increase per image, resulting in a total of 46 928 images in the augmented dataset.\n\nA standard Deep Convolutional GAN architecture served as the benchmark, with the discriminator network evaluated in three configurations to determine optimal performance: Standard DCGAN with Batch Normalization, DCGAN with Batch Normalization and Dropout layers (DCGAN D.) and DCGAN using Spectral Normalization from Progressive Augmentation GAN (DCGAN SN). All three variations shared hyper-parameters, employing GAN hacks like differing learning rates for the generator and discriminator and setting the real label at 90% of truth. The CGAN models resemble DCGAN with class embeddings and have been adjusted similarly with dropout layers and Spectral Normalization. Hyper-parameters align with DCGAN, adding class numbers for CIFAR-10 and Affective Dataset.\n\nPAGAN and WGAN GP models build on DCGAN's architecture, sharing the same generator structure. WGAN GP differs by omitting the sigmoid function in the discriminator. PAGAN introduces extra channels to the discriminator's first convolutional layer based on the KID score, with label smoothing omitted to preserve augmentation effects. WGAN GP incorporates gradient penalty in its loss function, maintaining DCGAN's learning strategies.\n\nTransfer learning involved selecting prominent models from Pytorch and HuggingFace, fine-tuning them with the Affective Dataset. Chosen models include ResNet-18  [13] , ResNet-152, VGG19  [29] , and EfficientNet b7  [30] , with uniform training loops and specified hyper-parameters. The Affective Dataset was split into training and validation sets (80/20 ratio).\n\nFine-tuning the BigGAN  [5]  generator posed challenges due to the absence of a pretrained discriminator. Options include creating a discriminator or leveraging another pretrained model for classification. The former necessitates training from scratch, potentially pretraining on ImageNet, while the latter depends on successful classification task performance to replace the discriminator in fine-tuning BigGAN.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Evaluation",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Experiments Summary",
      "text": "To validate the constructed models' correctness, additional experiments were conducted using the CIFAR-10  [16]  dataset. This preliminary testing aimed to ensure the models were well-designed and capable of generating images resembling those in the real dataset, given GANs' susceptibility to issues like mode-collapse. Three discriminator variants (employing batch normalization, dropout layers, and spectral normalization) were tested across three datasets: CIFAR-10, the Affective Dataset, and the Augmented Affective Dataset, using FID and KID scores calculated every five epochs from image batches of 200.\n\nOverall, 36 tests were performed on the generative models, each trained for 100 epochs, determined as sufficient for stabilizing image generation based on FID and KID scores. Additionally, eight tests focused on fine-tuning pretrained models for classifying affective images, conducted over 25 epochs, though optimal performance generally emerged by the fifth epoch.\n\nThe underwhelming results from fine-tuning pretrained models indicated their unsuitability as discriminators. In attempts to fine-tune the BigGAN model with a DCGAN-like discriminator, early tests showed the generator produced flawed examples within just a few epochs, leading to the discontinuation of tests with pretrained generative models.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Generation Results",
      "text": "The generation results are presented in the Tab. 3. The first imposing conclusion is that augmenting the Affective Dataset significantly improves the performance, even up to 20% of improvement.\n\nUsing the dropout layers in discriminator networks also improves the performance in most cases. The best improvement was noticed for PAGAN model, up to 10%. The worst was noticed for WGAN GP. where the improvement is marginal. There can be noticed that colors in affective images are more expressive than those from CIFAR-10, which indeed reflects the real situation. However, in both cases it is quite difficult to find some objects, although for CIFAR-10 some objects resembling animals or cars can be spotted. For affective images it is hard to notice any specific objects, however the generated color palettes may resemble the real data.\n\nThe generated fakes presented in Figs.  4 [a] and  4[b]  shows that simply replacing in Discriminator the Batch Normalization with Spectral Normalization totally destroyed the networks. The FID and KID scores presented in Tab. 3 also reflect the case. The results for the CGAN model are shown in the Figs.  4 [c] and 4  [d] . For the CIFAR-10 there can be visually noticed some improvement in object detection. For Affective also appeared a motive with some objects in the center of images, however, the sharpness of colors is reduced.\n\nThe best fakes generated for the PAGAN and WGAN GP models are shown in Figs.  4 [e] and 4[f].\n\nBest FID score results for all models are juxtaposed in the Fig.  5 .",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Classification Results",
      "text": "The results of classification task are gathered and presented in the Tab. 4. There are presented accuracy scores calculated on both training and validation sets. The split between training set and validation set has been set at the 80 to 20 percents. The achieved 90% accuracy on training set and 20% accuracy on validation set means, that models strongly overfit. The   best accuracy of 23,27% was achieved by VGG19 on base affective dataset. Providing random assignment label for class the result would be about 7,7%, so that means the pretrained models was able to learn a bit on affective dataset and provide better results than random. In most cases, the augmented dataset seem providing some disturbances to the classifiers inferences.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Dall-E",
      "text": "One would question the approach to generating affective images together with the categories of affectivity assigned to them. In this case, we redefine the task of generating affective images and categorizing synthetic images. In exploring the capability of generative models to produce affective images, it's essential to research the mechanisms through which these images are not only generated but also categorized based on emotional responses they elicit. The advent of sophisticated generative models like DALL-E  [26] , which can produce high-resolution images from text descriptions, offers a compelling method for creating visual content that can induce specific emotional states in viewers.\n\nThe process begins by understanding the concept of \"affective images\", which are images specifically designed to evoke particular emotions such as happiness, frustration, sadness, etc. By prompting models like DALL-E we can specify the desired emotional impact. Such models can tailor the visual elements in the image to align with affective categories in order to generate an image making the viewer happy or frustrated. Fig.  6  presents two created images based on text descriptions, using prompts such as: \"generate image that will make watching person frustrated-happy-etc.\"\n\n[a]\n\n[b] Considering the functionality demonstrated in DALL-E, where two images are created from text prompts specifying different emotions, the model's ability to interpret and visualize such abstract concepts as emotions into tangible images underscores the advanced state of current AI technology in understanding and manipulating human emotional responses. These generated images, characterized by their high resolution and clarity can serve as tools for studying human affective responses. The quality of these images ensures that they are potent stimuli capable of reliably inducing the specified emotions.\n\nHowever, the generation of affective images leads to another complex task: categorizing these images within a framework that quantifies emotional responses, often referred to as the valence-arousal space. Valence measures how positive or negative an emotion is, while arousal quantifies the intensity of the emotion. By categorizing images in this space, researchers can more systematically study how different visual elements correlate with emotional impacts.",
      "page_start": 7,
      "page_end": 9
    },
    {
      "section_name": "Summary And Future Work",
      "text": "The task of generating emotionally charged images is challenging, yet feasible to some extent. Studies have shown models can capture features like color hues and saturation. Image augmentation can reduce the problem of insufficient pictures by up to 20%, but additional datasets are needed to represent emotions like Tense, Excited, Bored, and Tired more fully.\n\nUsing dropout layers in discriminators proves effective for regularization and performance, while spectral normalization's benefits vary by model, necessitating case-by-case testing. Limiting discriminator channel increases to two during training is advisable, as exceeding this number can degrade results.\n\nPretrained models, including BigGAN pre-trained on ImageNet, underperformed in generating affective images, indicating the complexity of capturing emotions over object categorization. Future work should leverage conditional Deep Convolutional GANs to generate images with specific emotional labels and enhance the Affective dataset with more comprehensive human evaluations.\n\nAn augmented dataset expanded the base dataset eightfold. Further studies should explore more extensive dataset augmentation and the potential of combining promising models, like enhancing conditional GANs with additional discriminator channels or applying gradient penalty for loss improvement.\n\nGiven that transfer learning showed limited success, focusing on developing models from scratch, possibly using advanced architectures trained on an augmented dataset, may yield better outcomes. Additionally, exploring image inpainting  [32]  as an alternative approach could offer valuable insights, requiring dataset preprocessing to generate missing image parts.\n\nRedefinig task to only classify existing/generated images by programs like DALL-E may be the clue to get more affective images ready for future studies.",
      "page_start": 9,
      "page_end": 10
    }
  ],
  "figures": [
    {
      "caption": "Figure 3: ). These are grouped in datasets, which usually contain dozens to more than a thousand stimuli, along",
      "page": 1
    },
    {
      "caption": "Figure 1: [a] Russel’s circumplex model of emotions [31], [b] Pseudo-randomly selected OASIS images (based on [17])",
      "page": 3
    },
    {
      "caption": "Figure 1: [a]). Despite the",
      "page": 3
    },
    {
      "caption": "Figure 1: [b]), along with the ratings of valence and arousal (see",
      "page": 3
    },
    {
      "caption": "Figure 2: [a]). The amounts of images per quarter is shown in the Tab. 1. The representations of each quarter are not equal, indeed",
      "page": 3
    },
    {
      "caption": "Figure 2: [a] Normalized ratings for images from all datasets, [b] Valence-arousal space divided into 13 categories (cf. Fig. 2[a])",
      "page": 4
    },
    {
      "caption": "Figure 2: [b]) to reflect model depicted on Fig. 1[a]. In summary, the Table 2 presents the amounts of",
      "page": 4
    },
    {
      "caption": "Figure 5: 5.3. Classification Results",
      "page": 6
    },
    {
      "caption": "Figure 3: [a] CIFAR-10 fakes by DCGAN D., [b] A. AFFECTIVE fakes by DCGAN D.",
      "page": 7
    },
    {
      "caption": "Figure 4: [a] CIFAR-10 fakes by DCGAN SN., [b] A. AFFECTIVE fakes by DCGAN SN., [c] CIFAR-10 fakes by CGAN D., [d] AFFECTIVE fakes by",
      "page": 7
    },
    {
      "caption": "Figure 5: Models best FID scores on A. AFFECTIVE dataset",
      "page": 8
    },
    {
      "caption": "Figure 6: presents two created images based on text descriptions,",
      "page": 8
    },
    {
      "caption": "Figure 6: Generated images by DALL-E [26] by prompting: [a] frustration [b] happiness.",
      "page": 8
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Dataset": "EmoMadrid",
          "Quarter I": "306",
          "Quarter II": "237",
          "Quarter III": "19",
          "Quarter IV": "251"
        },
        {
          "Dataset": "GAPED",
          "Quarter I": "22",
          "Quarter II": "360",
          "Quarter III": "100",
          "Quarter IV": "248"
        },
        {
          "Dataset": "IAPS",
          "Quarter I": "229",
          "Quarter II": "314",
          "Quarter III": "219",
          "Quarter IV": "420"
        },
        {
          "Dataset": "NAPS",
          "Quarter I": "192",
          "Quarter II": "480",
          "Quarter III": "45",
          "Quarter IV": "639"
        },
        {
          "Dataset": "OASIS",
          "Quarter I": "198",
          "Quarter II": "145",
          "Quarter III": "162",
          "Quarter IV": "395"
        },
        {
          "Dataset": "SFIP",
          "Quarter I": "0",
          "Quarter II": "17",
          "Quarter III": "326",
          "Quarter IV": "542"
        },
        {
          "Dataset": "ALL",
          "Quarter I": "946",
          "Quarter II": "1553",
          "Quarter III": "871",
          "Quarter IV": "2495"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Category": "Amount",
          "Angry": "628",
          "Bored": "124",
          "Calm": "776",
          "Content": "717",
          "Delighted": "249",
          "Depressed": "221",
          "Excited": "36"
        },
        {
          "Category": "Category",
          "Angry": "Frustrated",
          "Bored": "Happy",
          "Calm": "Neutral",
          "Content": "Relaxed",
          "Delighted": "Tense",
          "Depressed": "Tired",
          "Excited": ""
        },
        {
          "Category": "Amount",
          "Angry": "603",
          "Bored": "461",
          "Calm": "1036",
          "Content": "627",
          "Delighted": "50",
          "Depressed": "338",
          "Excited": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Model": "DCGAN",
          "Dataset": "CIFAR-10",
          "FID score": "119.9306",
          "KID score": "0.003294"
        },
        {
          "Model": "",
          "Dataset": "AFFECTIVE",
          "FID score": "226.8570",
          "KID score": "0.008311"
        },
        {
          "Model": "",
          "Dataset": "A. AFFECTIVE",
          "FID score": "173.2516",
          "KID score": "0.000991"
        },
        {
          "Model": "DCGAN D.",
          "Dataset": "CIFAR-10",
          "FID score": "108.3639",
          "KID score": "0.000300"
        },
        {
          "Model": "",
          "Dataset": "AFFECTIVE",
          "FID score": "198.9459",
          "KID score": "0.004590"
        },
        {
          "Model": "",
          "Dataset": "A. AFFECTIVE",
          "FID score": "168.1698",
          "KID score": "0.001028"
        },
        {
          "Model": "DCGAN SN.",
          "Dataset": "CIFAR-10",
          "FID score": "152.7030",
          "KID score": "0.005971"
        },
        {
          "Model": "",
          "Dataset": "AFFECTIVE",
          "FID score": "372.8274",
          "KID score": "0.025878"
        },
        {
          "Model": "",
          "Dataset": "A. AFFECTIVE",
          "FID score": "388.6158",
          "KID score": "0.027513"
        },
        {
          "Model": "CGAN",
          "Dataset": "CIFAR-10",
          "FID score": "159.3475",
          "KID score": "0.006392"
        },
        {
          "Model": "",
          "Dataset": "AFFECTIVE",
          "FID score": "232.0317",
          "KID score": "0.009073"
        },
        {
          "Model": "",
          "Dataset": "A. AFFECTIVE",
          "FID score": "193.2394",
          "KID score": "0.002538"
        },
        {
          "Model": "CGAN D.",
          "Dataset": "CIFAR-10",
          "FID score": "142.2123",
          "KID score": "0.004740"
        },
        {
          "Model": "",
          "Dataset": "AFFECTIVE",
          "FID score": "232.2346",
          "KID score": "0.009290"
        },
        {
          "Model": "",
          "Dataset": "A. AFFECTIVE",
          "FID score": "187.0331",
          "KID score": "0.001799"
        },
        {
          "Model": "CGAN SN.",
          "Dataset": "CIFAR-10",
          "FID score": "146.8453",
          "KID score": "0.005457"
        },
        {
          "Model": "",
          "Dataset": "AFFECTIVE",
          "FID score": "242.1963",
          "KID score": "0.010768"
        },
        {
          "Model": "",
          "Dataset": "A. AFFECTIVE",
          "FID score": "189.2623",
          "KID score": "0.001932"
        },
        {
          "Model": "PAGAN",
          "Dataset": "CIFAR-10",
          "FID score": "140.8431",
          "KID score": "0.003949"
        },
        {
          "Model": "",
          "Dataset": "AFFECTIVE",
          "FID score": "250.4171",
          "KID score": "0.011332"
        },
        {
          "Model": "",
          "Dataset": "A. AFFECTIVE",
          "FID score": "199.4383",
          "KID score": "0.008943"
        },
        {
          "Model": "PAGAN D.",
          "Dataset": "CIFAR-10",
          "FID score": "115.1276",
          "KID score": "0.002150"
        },
        {
          "Model": "",
          "Dataset": "AFFECTIVE",
          "FID score": "226.3566",
          "KID score": "0.008486"
        },
        {
          "Model": "",
          "Dataset": "A. AFFECTIVE",
          "FID score": "182.8493",
          "KID score": "0.003149"
        },
        {
          "Model": "PAGAN SN.",
          "Dataset": "CIFAR-10",
          "FID score": "115.8432",
          "KID score": "0.001387"
        },
        {
          "Model": "",
          "Dataset": "AFFECTIVE",
          "FID score": "227.9139",
          "KID score": "0.009626"
        },
        {
          "Model": "",
          "Dataset": "A. AFFECTIVE",
          "FID score": "184.2553",
          "KID score": "0.004537"
        },
        {
          "Model": "WGAN GP.",
          "Dataset": "CIFAR-10",
          "FID score": "110.9935",
          "KID score": "0.000946"
        },
        {
          "Model": "",
          "Dataset": "AFFECTIVE",
          "FID score": "201.5534",
          "KID score": "0.006299"
        },
        {
          "Model": "",
          "Dataset": "A. AFFECTIVE",
          "FID score": "181.5519",
          "KID score": "0.003346"
        },
        {
          "Model": "WGAN GP. D.",
          "Dataset": "CIFAR-10",
          "FID score": "116.9051",
          "KID score": "0.001132"
        },
        {
          "Model": "",
          "Dataset": "AFFECTIVE",
          "FID score": "202.8119",
          "KID score": "0.005879"
        },
        {
          "Model": "",
          "Dataset": "A. AFFECTIVE",
          "FID score": "180.0444",
          "KID score": "0.002024"
        },
        {
          "Model": "WGAN GP. SN.",
          "Dataset": "CIFAR-10",
          "FID score": "121.1699",
          "KID score": "0.001292"
        },
        {
          "Model": "",
          "Dataset": "AFFECTIVE",
          "FID score": "244.0882",
          "KID score": "0.010604"
        },
        {
          "Model": "",
          "Dataset": "A. AFFECTIVE",
          "FID score": "190.0199",
          "KID score": "0.003953"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Model": "",
          "AFFECTIVE": "Train",
          "A. AFFECTIVE": "Train"
        },
        {
          "Model": "ResNet18",
          "AFFECTIVE": "0.9069",
          "A. AFFECTIVE": "0.8925"
        },
        {
          "Model": "ResnNet152",
          "AFFECTIVE": "0.9143",
          "A. AFFECTIVE": "0.8933"
        },
        {
          "Model": "EfficientNet b7",
          "AFFECTIVE": "0.9007",
          "A. AFFECTIVE": "0.8945"
        },
        {
          "Model": "VGG19",
          "AFFECTIVE": "0.8525",
          "A. AFFECTIVE": "0.8898"
        }
      ],
      "page": 8
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Wasserstein generative adversarial networks",
      "authors": [
        "M Arjovsky",
        "S Chintala",
        "L Bottou"
      ],
      "year": "2017",
      "venue": "Proceedings of the 34th International Conference on Machine Learning, ICML, PMLR"
    },
    {
      "citation_id": "2",
      "title": "Demystifying mmd gans",
      "authors": [
        "M Bińkowski",
        "D Sutherland",
        "M Arbel",
        "A Gretton"
      ],
      "year": "2018",
      "venue": "Demystifying mmd gans",
      "doi": "10.48550/ARXIV.1801.01401"
    },
    {
      "citation_id": "3",
      "title": "Pros and cons of gan evaluation measures",
      "authors": [
        "A Borji"
      ],
      "year": "2018",
      "venue": "Pros and cons of gan evaluation measures",
      "doi": "10.48550/ARXIV.1802.03446"
    },
    {
      "citation_id": "4",
      "title": "Pros and cons of gan evaluation measures: New developments",
      "authors": [
        "A Borji"
      ],
      "year": "2021",
      "venue": "Pros and cons of gan evaluation measures: New developments",
      "doi": "10.48550/ARXIV.2103.09396"
    },
    {
      "citation_id": "5",
      "title": "Large scale GAN training for high fidelity natural image synthesis",
      "authors": [
        "A Brock",
        "J Donahue",
        "K Simonyan"
      ],
      "year": "2019",
      "venue": "International Conference on Learning Representations"
    },
    {
      "citation_id": "6",
      "title": "Emomadrid: An emotional pictures database for affect research",
      "authors": [
        "L Carretié",
        "M Tapia",
        "S López-Martín",
        "J Albert"
      ],
      "year": "2019",
      "venue": "Motivation and Emotion",
      "doi": "10.1007/s11031-019-09780-y"
    },
    {
      "citation_id": "7",
      "title": "The geneva affective picture database (gaped): a new 730-picture database focusing on valence and normative significance",
      "authors": [
        "E Dan-Glauser",
        "K Scherer"
      ],
      "year": "2011",
      "venue": "Behavior Research Methods",
      "doi": "10.3758/s13428-011-0064-1"
    },
    {
      "citation_id": "8",
      "title": "Imagenet: A large-scale hierarchical image database",
      "authors": [
        "J Deng",
        "W Dong",
        "R Socher",
        "L Li",
        "K Li",
        "L Fei-Fei"
      ],
      "year": "2009",
      "venue": "IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR)",
      "doi": "10.1109/CVPR.2009.5206848"
    },
    {
      "citation_id": "9",
      "title": "Human emotion recognition: Review of sensors and methods",
      "authors": [
        "A Dzedzickis",
        "A Kaklauskas",
        "V Bucinskas"
      ],
      "year": "2020",
      "venue": "Sensors",
      "doi": "10.3390/s20030592"
    },
    {
      "citation_id": "10",
      "title": "Generative Deep Learning: Teaching Machines to Paint, Write, Compose, and Play",
      "authors": [
        "D Foster"
      ],
      "year": "2019",
      "venue": "Generative Deep Learning: Teaching Machines to Paint, Write, Compose, and Play"
    },
    {
      "citation_id": "11",
      "title": "Generative adversarial nets",
      "authors": [
        "I Goodfellow",
        "J Pouget-Abadie",
        "M Mirza",
        "B Xu",
        "D Warde-Farley",
        "S Ozair",
        "A Courville",
        "Y Bengio"
      ],
      "year": "2014",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "12",
      "title": "Improved training of wasserstein gans",
      "authors": [
        "I Gulrajani",
        "F Ahmed",
        "M Arjovsky",
        "V Dumoulin",
        "A Courville"
      ],
      "year": "2017",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "13",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "K He",
        "X Zhang",
        "S Ren",
        "J Sun"
      ],
      "year": "2015",
      "venue": "Deep residual learning for image recognition",
      "doi": "10.48550/ARXIV.1512.03385"
    },
    {
      "citation_id": "14",
      "title": "Gans trained by a two time-scale update rule converge to a local nash equilibrium",
      "authors": [
        "M Heusel",
        "H Ramsauer",
        "T Unterthiner",
        "B Nessler",
        "S Hochreiter"
      ],
      "year": "2018",
      "venue": "Gans trained by a two time-scale update rule converge to a local nash equilibrium",
      "arxiv": "arXiv:1706.08500"
    },
    {
      "citation_id": "15",
      "title": "A style-based generator architecture for generative adversarial networks",
      "authors": [
        "T Karras",
        "S Laine",
        "T Aila"
      ],
      "year": "2018",
      "venue": "A style-based generator architecture for generative adversarial networks",
      "doi": "10.48550/ARXIV.1812.04948"
    },
    {
      "citation_id": "16",
      "title": "Cifar-10 (canadian institute for advanced research",
      "authors": [
        "A Krizhevsky",
        "V Nair",
        "G Hinton"
      ],
      "venue": "Cifar-10 (canadian institute for advanced research"
    },
    {
      "citation_id": "17",
      "title": "Introducing the open affective standardized image set (OASIS)",
      "authors": [
        "B Kurdi",
        "S Lozano",
        "M Banaji"
      ],
      "year": "2017",
      "venue": "Behav. Res. Methods"
    },
    {
      "citation_id": "18",
      "title": "BIRAFFE2, a multimodal dataset for emotion-based personalization in rich affective game environments",
      "authors": [
        "K Kutt",
        "D Drażyk",
        "L Żuchowska",
        "M Szelażek",
        "S Bobek",
        "G Nalepa"
      ],
      "year": "2022",
      "venue": "Scientific Data 9",
      "doi": "10.1038/s41597-022-01402-6"
    },
    {
      "citation_id": "19",
      "title": "International affective picture system (IAPS): Affective ratings of pictures and instruction manual",
      "authors": [
        "P Lang",
        "M Bradley",
        "B Cuthbert"
      ],
      "year": "2008",
      "venue": "International affective picture system (IAPS): Affective ratings of pictures and instruction manual"
    },
    {
      "citation_id": "20",
      "title": "The Nencki Affective Picture System (NAPS): Introduction to a novel, standardized, wide-range, high-quality, realistic picture database",
      "authors": [
        "A Marchewka",
        "Ł Żurawski",
        "K Jednoróg",
        "A Grabowska"
      ],
      "year": "2014",
      "venue": "Behavior Research Methods",
      "doi": "10.3758/s13428-013-0379-1"
    },
    {
      "citation_id": "21",
      "title": "The set of fear inducing pictures (SFIP): Development and validation in fearful and nonfearful individuals",
      "authors": [
        "J Michałowski",
        "D Droździel",
        "J Matuszewski",
        "W Koziejowski",
        "K Jednoróg",
        "A Marchewka"
      ],
      "year": "2017",
      "venue": "Behavior Research Methods",
      "doi": "10.3758/s13428-016-0797-y"
    },
    {
      "citation_id": "22",
      "title": "Conditional generative adversarial nets",
      "authors": [
        "M Mirza",
        "S Osindero"
      ],
      "year": "2014",
      "venue": "Conditional generative adversarial nets",
      "arxiv": "arXiv:1411.1784"
    },
    {
      "citation_id": "23",
      "title": "Conditional image synthesis with auxiliary classifier gans",
      "authors": [
        "A Odena",
        "C Olah",
        "J Shlens"
      ],
      "year": "2017",
      "venue": "Proceedings of the 34th International Conference on Machine Learning, ICML, PMLR"
    },
    {
      "citation_id": "24",
      "title": "Affective Computing",
      "authors": [
        "R Picard"
      ],
      "year": "1997",
      "venue": "Affective Computing"
    },
    {
      "citation_id": "25",
      "title": "Unsupervised representation learning with deep convolutional generative adversarial networks",
      "authors": [
        "A Radford",
        "L Metz",
        "S Chintala"
      ],
      "year": "2016",
      "venue": "4th International Conference on Learning Representations ICLR"
    },
    {
      "citation_id": "26",
      "title": "Zero-shot text-to-image generation",
      "authors": [
        "A Ramesh",
        "M Pavlov",
        "G Goh",
        "S Gray",
        "C Voss",
        "A Radford",
        "M Chen",
        "I Sutskever"
      ],
      "year": "2021",
      "venue": "Zero-shot text-to-image generation",
      "arxiv": "arXiv:2102.12092"
    },
    {
      "citation_id": "27",
      "title": "Improved techniques for training gans",
      "authors": [
        "T Salimans",
        "I Goodfellow",
        "W Zaremba",
        "V Cheung",
        "A Radford",
        "X Chen"
      ],
      "year": "2016",
      "venue": "Improved techniques for training gans",
      "doi": "10.48550/ARXIV.1606.03498"
    },
    {
      "citation_id": "28",
      "title": "A survey on image data augmentation for deep learning",
      "authors": [
        "C Shorten",
        "T Khoshgoftaar"
      ],
      "year": "2019",
      "venue": "J. Big Data",
      "doi": "10.1186/s40537-019-0197-0"
    },
    {
      "citation_id": "29",
      "title": "Very deep convolutional networks for large-scale image recognition",
      "authors": [
        "K Simonyan",
        "A Zisserman"
      ],
      "year": "2014",
      "venue": "Very deep convolutional networks for large-scale image recognition",
      "doi": "10.48550/ARXIV.1409.1556"
    },
    {
      "citation_id": "30",
      "title": "Efficientnet: Rethinking model scaling for convolutional neural networks URL",
      "authors": [
        "M Tan",
        "Q Le"
      ],
      "year": "2019",
      "venue": "Efficientnet: Rethinking model scaling for convolutional neural networks URL",
      "doi": "10.48550/ARXIV.1905.11946"
    },
    {
      "citation_id": "31",
      "title": "Building chinese affective resources in valence-arousal dimensions",
      "authors": [
        "L Yu",
        "L Lee",
        "S Hao",
        "J Wang",
        "Y He",
        "J Hu",
        "K Lai",
        "X Zhang"
      ],
      "year": "2016",
      "venue": "Building chinese affective resources in valence-arousal dimensions",
      "doi": "10.18653/v1/N16-1066"
    },
    {
      "citation_id": "32",
      "title": "High-resolution image inpainting with iterative confidence feedback and guided upsampling",
      "authors": [
        "Y Zeng",
        "Z Lin",
        "J Yang",
        "J Zhang",
        "E Shechtman",
        "H Lu"
      ],
      "year": "2020",
      "venue": "High-resolution image inpainting with iterative confidence feedback and guided upsampling",
      "arxiv": "arXiv:2005.11742"
    },
    {
      "citation_id": "33",
      "title": "Progressive augmentation of gans",
      "authors": [
        "D Zhang",
        "A Khoreva"
      ],
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems"
    }
  ]
}