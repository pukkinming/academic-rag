{
  "paper_id": "2409.09194v2",
  "title": "Hierarchical Hypercomplex Network For Multimodal Emotion Recognition",
  "published": "2024-09-13T21:07:49Z",
  "authors": [
    "Eleonora Lopez",
    "Aurelio Uncini",
    "Danilo Comminiello"
  ],
  "keywords": [
    "Hypercomplex networks",
    "multimodal emotion recognition",
    "EEG",
    "physiological signals"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Emotion recognition is relevant in various domains, ranging from healthcare to human-computer interaction. Physiological signals, being beyond voluntary control, offer reliable information for this purpose, unlike speech and facial expressions which can be controlled at will. They reflect genuine emotional responses, devoid of conscious manipulation, thereby enhancing the credibility of emotion recognition systems. Nonetheless, multimodal emotion recognition with deep learning models remains a relatively unexplored field. In this paper, we introduce a fully hypercomplex network with a hierarchical learning structure to fully capture correlations. Specifically, at the encoder level, the model learns intra-modal relations among the different channels of each input signal. Then, a hypercomplex fusion module learns inter-modal relations among the embeddings of the different modalities. The main novelty is in exploiting intra-modal relations by endowing the encoders with parameterized hypercomplex convolutions (PHCs) that thanks to hypercomplex algebra can capture inter-channel interactions within single modalities. Instead, the fusion module comprises parameterized hypercomplex multiplications (PHMs) that can model inter-modal correlations. The proposed architecture surpasses state-of-the-art models on the MAHNOB-HCI dataset for emotion recognition, specifically in classifying valence and arousal from electroencephalograms (EEGs) and peripheral physiological signals. The code of this study is available at https://github.com/ispamm/MHyEEG.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Introduction",
      "text": "Advancing the Brain-Comuter Interface (BCI) by understanding how the human brain encodes the world is at the core of neurocognitive research. A crucial part of this includes learning how emotions are related to brain signals such as electroencephalograms (EEGs). By deciphering the neural responses associated with different emotions, researchers can not only improve our understanding of human cognition but also pave the way for more sophisticated BCI systems capable of interpreting and responding to users' emotional states in real-time. However, emotions are intrinsically multi-modal, they are expressed through behavioral responses such as body language, facial expressions, and speech, as well as involuntary responses, which are reflected in physiological signals. Indeed, studies have shown that different emotions result in different responses of the brain which yield specific EEG signals  [1] . Moreover, heart rhythm changes according to different emotions which can be detected through electrocardiograms (ECGs)  [2] . Galvanic skin response (GSR) provides a measure of the resistance of the skin, which decreases when one is experiencing emotions such as stress or surprise  [3] . Finally, also the eyes can reveal insights into what emotions are being experienced, e.g. it has been shown that pupil diameter changes in different emotional states, increasing when feeling anger, fear, and anxiety or arousal and love  [4] . Given these relationships between physiological signals and emotional responses, researchers are starting to turn to the physiological approach for emotion recognition  [5] . Indeed, they are directly related to real emotions, unlike behavioral reactions, which can lead to systems that are prone to fake emotions and can be manipulated easily  [6] .\n\nNonetheless, existing works focus mostly on a single modality  [7, 8]  or rely on hand-crafted features extracted from the raw signals  [9] . Indeed, a multimodal approach yields a more powerful classifier since it takes into account the complementary information given by the different modalities, given that emotions are expressed in a multimodal way. Moreover, relying on handcrafted features requires extensive domain knowledge and represents a methodology rooted in the past. In contrast, contemporary deep learning encoders can autonomously acquire more discriminant features during the training. A recent work  [10] , addresses this problem by conducting a preliminary study and proposing a multimodal network equipped with a novel hypercomplex fusion module composed of parameterized hypercomplex multiplications (PHM). Parameterized hypercomplex neural networks (PHNNs) are models that operate in a hypercomplex domain which can be chosen directly by the user through a hyperparameter n. They represent a generalization of the more popular quaternion neural networks (QNNs) which are defined in the quaternion domain. Indeed, PHNNs can operate on any n-dimensional data, unlike quaternion models which work with quaternion data and are thus limited to 4dimensional inputs. Moreover, they retain the advantages of QNNs, yielding lightweight models with the number of parameters reduced by 1/n and with the ability to model local relations among data dimensions  [11] [12] [13] [14] [15] . However, the model introduced in  [10] , HyperFuseNet, still presents some limitations. The utilization of hypercomplex algebra is confined solely to the fusion module and is not integrated into the encoder itself. Additionally, in general, these models have a tendency to overfit excessively, resulting in inadequate generalization performance. Therefore, in this paper, we build upon this preliminary study in order to address these problems.\n\nWe propose a Hierarchical Hypercomplex (H2) model with a hierarchical structure wherein the encoders and the fusion module operate in the hypercomplex domain. The hierarchical structure allows to first learn embeddings of the single modalities exploiting relations among the channels that compose each modality through parameterized hypercomplex convolutions (PHCs). In contrast, the fusion module learns a fused representation from the embeddings of each modality, leveraging relations among the modalities themselves thanks to parameterized hypercomplex multiplications (PHMs). In detail, the encoders are tailored to each modality by setting the hyperparameter n equal to the number of dimensions of each input signal. In this way, each encoder operates within its own distinct hypercomplex domain which is the natural domain of the specific signal. Thus, the encoders yield an enriched embedding thanks to convolutions and hypercomplex algebra properties. Indeed, convolutions are well-suited for processing multidimensional data, while hypercomplex algebra endows the encoders with the ability to model and exploit correlations among the various dimensions within each individual modality. Certainly, the modalities we employ, i.e., EEG, ECG, and eye data, are all multidimensional (except for GSR) and inherently exhibit correlations between their channels, an essential aspect of their information content. Thus, with this hierarchical design, we increase the performance of the state-of-the-art arousal and valence classification on the MAHNOB-HCI database  [3] , achieving an F1-score of 0.557 and 0.685, respectively, which represents an improvement of 40.20% and 57.11%.\n\nThe rest of the paper is organized as follows. In section 2 we discuss the recent works on hypercomplex models and emotion recognition. In Section 3 we provide background on the theory behind hypercomplex networks and we describe the proposed architecture in detail. In Section 4 we discuss the experimental evaluation of our method. Finally, in Section 5 we draw the conclusions.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Works",
      "text": "Many studies have developed machine learning  [16, 17]  and deep learning-based  [8, 18]  systems for emotion recognition. Nevertheless, these methods require either extensive domain knowledge for extracting handcrafted features or employ popular extracted features such as differential entropy (DE) or power spectral density (PSD). These approaches do not allow the neural model to learn features on its own and require additional preprocessing steps  [9] . For these reasons, methods that learn directly from raw signals have started to emerge. A recent study has proposed a reinforcement learning approach inspired by brain emotion perception and shows the advantages of using raw signals  [7] . Furthermore, another approach to increase the performance of an emotion recognition system involves designing a multimodal framework  [9] . Indeed, recently proposed methods include a manifold learning-based technique for multimodal emotion recognition  [19] , a method to improve generalization across unseen target domains from EEG and eye movement signals  [20] , and a hypercomplexbased architecture, HyperFuseNet, with a novel fusion module  [10] . In this paper, we tackle the limitations of handcrafted features and single-modality approaches by extending the method proposed in a preliminary study  [10] , Hyper-FuseNet. Although this model learns directly from raw multimodal signals, it suffers from poor generalization ability. To overcome this limitation, we introduce a hierarchical design, resulting in substantial improvements, as demonstrated by the experimental results in Section 4.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Method",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Parameterized Hypercomplex Networks",
      "text": "Parameterized hypercomplex neural networks (PHNNs) have been proposed in order to overcome the limitations of the popular quaternion models while keeping their advantages and useful properties  [11, 12] . Indeed, quaternion models are limited to 4-dimensional inputs as they work with quaternions, i.e., q = q 0 + q 1 î + q 2 ij + q 3 κ where q i ∈ R and î, ij, κ ∈ Q. Instead, PHNNs are flexible to work with any n-dimensional data, where n ∈ N is a hyperparameter that users can adjust based on their specific requirements. This flexibility is achieved through a specific construction of the weight matrix within the layers comprising the PHNNs. Classic fully-connected and convolutional layers are equivalent to parameterized hypercomplex multiplication (PHM) layers and parameterized hypercomplex convolutional (PHC) layers, respectively. PHM and PHC layers are defined as standard linear and convolutional layers as:\n\nwhere x is the input and b PHM and b PHC are the bias terms. The core of these layers is how the weight matrix W is constructed through Kronecker products:\n\nIn this equation, matrices A i learn the algebra rules directly from the data, while matrices F i represent standard learnable weights of linear layers and filters of convolutional layers. Finally, n ∈ N defines the number domain in which the model operates, e.g. for n = 4 it is equivalent to a QNN. Thus, they do not rely on algebras that exist only for domains in accordance with the Cayley-Dickson system, i.e., only for n = 2 m where m ∈ N, such as quaternions (n = 4), octonions (n = 8), and so on. Instead, by modeling interactions between imaginary units through learnable matrices A i , they can be employed also for domains in which an algebra structure does not yet exist, e.g., for n = 3, 5, • • • . Moreover, this leads to a reduction of parameters by 1/n, yielding very efficient models. Finally, they have the ability to learn both global relations, i.e., intra-channel interactions, and local relations, i.e., inter-channel interactions, which real-valued networks tend to ignore  [21] .",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Hierarchical Hypercomplex Model",
      "text": "In this section, we describe the proposed Hierarchical Hypercomplex (H2) model for multimodal emotion recognition, depicted in Fig.  1 . The architecture has a hierarchical structure where encoders operating in different hypercomplex domains learn modality-specific embeddings, while the hypercomplex fusion module learns a fused embedding. Mainly, the hierarchical structure refers to the level of relations being considered, i.e., intra-modality and inter-modality. The encoders model relations between channels within a single modality, thus they exploit intra-modality relations. In contrast, the hypercomplex fusion module exploits relations among the different modalities themselves, i.e., inter-modal interactions.\n\nIn detail, the encoders of EEG, ECG, and eye signals comprise two PHC layers with two batch normalization (BN) layers and ReLU activation functions. Instead, being GSR a 1dimensional signal, its encoder is composed of a single PHM layer together with a BN layer and ReLU. Then, a global average pooling operation is applied to get the final latent representation. For PHC and PHM layers, we set n EEG = 10, n ECG = 3, n eye = 4, and n GSR = 1, in accordance with the number of channels of each different signal. In this way, the encoders are endowed with the ability to learn not only intrachannel relations, as any standard network, but also interchannel relations. Thus, they exploit the inherent correlations among channels of the single modalities which enrich the latent representation of each signal. Moreover, by setting the n parameter equal to the number of channels of each input signal, each encoder operates in a different hypercomplex domain which is also the natural domain of each input signal. The hypercomplex fusion module employed in this architecture is a new version of the one introduced in a preliminary work  [10] . Specifically, it is modified by removing one PHM layer and incorporating more dropout layers in order to reduce overfitting, which the HyperFuseNet architecture struggled with.\n\nTo conclude, the H2 model can be seen as an extension of the method proposed in  [10] , namely HyperFuseNet. The main difference lies in the encoders, which before were composed of standard fully-connected layers in the real domain, instead of convolutional layers in the hypercomplex domain. Therefore, HyperFuseNet did not have the hierarchical structure that we introduce in this paper, which is also the main advantage of the proposed network. Moreover, we revisited the overall structure by reducing the number of layers by 1 in both the encoders and the hypercomplex fusion module in order to make them more proportionate to the quantity and nature of the input data. Also, for this reason, we add dropout layers with a dropout rate of 0.5 between each PHM layer of the hypercomplex fusion module.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Experimental Results",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Dataset And Preprocessing",
      "text": "For training and evaluating our models, we utilize a publicly available database for affect recognition, MAHNOB-HCI  [3] . It provides synchronized recordings of 27 participants during an experiment in which each subject was shown fragments of movies that induced different emotional responses. In detail, the subjects were monitored with video cameras, a head-worn microphone, an eye gaze tracker, as well as physiological sensors measuring EEG, ECG, respiration amplitude, and skin temperature. In this work, we employ as modalities the EEG, ECG, GSR, all recorded at 256Hz, and eye data, recorded at 60Hz. The latter comprises gaze coordinates, eye distances, and pupil dimensions. Each synchronized recording is labeled on a scale of arousal (calm, medium aroused, and excited) and valence (unpleasant, neutral, and pleasant).\n\nWe apply the same preprocessing as in  [10] . The dataset provides EEG with 32 electrodes, among which we select 10 most related to emotion, i.e., F3, F4, F5, F6, F7, F8, T7, T8, P7, and P8  [22, 23] . EEG, ECG, and GSR are first downsampled to 128Hz. Secondly, EEG signals are referenced to the average reference. Then, EEG and ECG are filtered with a band-pass filter of 1-45Hz and 0.5-45Hz, respectively, a lowpass filter for GSR at 60Hz, and a final notch filter at 50Hz for each of them. To account for the initial offset of GSR signals, baseline correction is performed by adjusting it relative to the mean value within the preceding 200ms of each trial. For the other signals, this correction is automatically achieved after the high-pass filters. Lastly, for eye data, we consider the average between measurements of the left and right eye, and we maintain -1 values as they indicate blinks or rapid movements which can be related to an emotional response.\n\nSamples are constructed by extracting segments of 10s from the original 30s of each trial. The dataset is split into training (80%) and testing (20%) in a stratified manner. The training data is augmented with scaling and Gaussian noise addition as in  [10] .",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Experimental Setup",
      "text": "For evaluating our models, we utilize as metrics the accuracy and the F1-score, i.e., the harmonic mean of the precision and recall, which accounts for imbalanced data. The networks are trained with the Adam optimizer, cross-entropy loss, and the once-cycle policy with 0.425% of increasing steps, linear annealing strategy, dividing factors of 10, maximum learning rate of 7.96 × 10 -6 , and minimum and maximum momentum of 0.7403 and 0.8314, respectively. The number of epochs is set to 50, but we early stop the training when the F1-score does not improve for 10 epochs.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Results And Discussion",
      "text": "The main results of our experiments are reported in Tab. 1. Our model is compared against two state-of-the-art architectures that operate with raw signals, i.e., HyperFuseNet  [10]  and the multimodal model proposed by Dolmans et. al  [24]  which was originally designed for mental workload classification. The proposed hierarchical architecture significantly outperforms both HyperFuseNet and the other state-of-theart model, as demonstrated by the results in Tab. 1. In both arousal and valence classification, our model brings a substantial improvement, i.e., of 40.20% and 57.11% for the F1score, respectively. This great gap is due to several reasons. First, the other two models are quite prone to overfitting, thus when tested on unseen trials they are not able to generalize well. Instead, in our method we address these problems with additional dropout layers with a higher rate with respect to HyperFuseNet (which only had one dropout layer), reducing the overall number of layers by removing one in each encoder and fusion module, and, most importantly, integrating hypercomplex algebra also at encoder-level. Indeed, this last aspect allows to exploit both intra-modal and inter-modal relations in a hierarchical manner. This is the main advantage of the proposed model as we demonstrate in the ablation studies in the following section. In fact, it is not due to just a reduction of parameters given by hypercomplex algebra as this could result in underfitting, as we show in Section 4.4. Instead, thanks to its properties the encoders are able to leverage correlations among channels of single modalities, thus significantly outperforming HyperFuseNet. Moreover, Figure  2  depicts the t-SNE visualization  [25]  of the features learned from H2 and HyperFuseNet. From the image, it is clear that HyperFuseNet struggles to learn discriminant representations. In contrast, in the features learned by the H2 model clusters start to become discernible, although there is room for further improvement.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Ablation Studies",
      "text": "In this section, we study the impact of each added component of the proposed network H2 and we report the results in Tab. 2 for arousal and in Tab. 3 for valence. In the first line of both tables, we show the results obtained with HyperFuseNet for reference. We investigate first the impact of reducing the number of layers of the encoders and fusion module, as described in Section 3. This simple tweak already improves performance resulting in slightly less overfitting. Then, we substitute fully  connected layers with their hypercomplex counterpart, PHM layers with n m , m ∈ {EEG, GSR, ECG, eye} set as described in Section 3. In this scenario, the significant reduction in the number of parameters results in underperformance compared to using standard fully-connected layers. This also suggests that fully connected layers may not be optimal for this stage of the architecture in which the encoder should learn a modalityspecific representation from multidimensional signals. Therefore, we substitute these with standard convolutional layers in the real domain. This allows the model to overfit less with respect to linear layers and improves the performance for arousal classification. For valence classification, the models are slightly less prone to overfit thus the performance in this scenario is comparable to using fully-connected layers. Finally, we introduce hypercomplex algebra, thus integrating convolutions in the hypercomplex domain, i.e., PHC layers, resulting in our final proposed H2 model with the hierarchical learning structure. From the results, it is clear that this is the crucial component that allows to achieve such a great improvement. Indeed, the parameters are reduced even more with respect to PHM layers in the encoder, however, the combination of convolution operations and hypercomplex algebra results to be optimal. In this scenario, the number of parameters, 2.4 million, is more appropriate for the task and data with respect to HyperFuseNet. However, the reduction of parameters is not the main advantage, as also the model version with PHM layers had the same size but resulted in being suboptimal. Instead, what allows the H2 network to generalize well is the ability of its encoders to learn embeddings enriched with relations among channels of the different signals thanks to hypercomplex algebra.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Conclusion",
      "text": "In this work, we proposed a multimodal hierarchical fullyhypercomplex model for emotion recognition from EEG and peripheral physiological signals. Specifically, the network is equipped with a hierarchical learning structure, where encoders learn intra-modal correlations, i.e., they exploit both intra-channel and inter-channel relations of each signal, and the hypercomplex fusion module learns inter-modal relations.\n\nOur main contribution is the introduction of hypercomplex algebra also at the encoder level with PHC operations, which results in a hierarchical structure. We found that this simple step allows to achieve much better results than previous state-of-the-art methods. Certainly, while the hypercomplex fusion module leverages inter-modal relations, the hypercomplex encoders exploit intra-signal correlations which results in enriched latent representations and a final improvement of 40.20% and 57.11% for the F1-score on arousal and valence classification.",
      "page_start": 6,
      "page_end": 6
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: H2 model. The encoder learns enriched modality-specific embeddings by exploiting inter-channel relations within",
      "page": 3
    },
    {
      "caption": "Figure 1: The architecture has a hierarchical structure",
      "page": 3
    },
    {
      "caption": "Figure 2: t-SNE feature visualization.",
      "page": 4
    },
    {
      "caption": "Figure 2: depicts the",
      "page": 5
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "ABSTRACT": "Emotion recognition is relevant\nin various domains,\nranging",
          "1.\nINTRODUCTION": "Advancing the Brain-Comuter Interface (BCI) by understand-"
        },
        {
          "ABSTRACT": "from healthcare to human-computer\ninteraction.\nPhysiolog-",
          "1.\nINTRODUCTION": "ing how the human brain encodes the world is at\nthe core of"
        },
        {
          "ABSTRACT": "ical\nsignals, being beyond voluntary control, offer\nreliable",
          "1.\nINTRODUCTION": "neurocognitive research. A crucial part of this includes learn-"
        },
        {
          "ABSTRACT": "information for\nthis purpose, unlike speech and facial ex-",
          "1.\nINTRODUCTION": "ing how emotions are related to brain signals such as elec-"
        },
        {
          "ABSTRACT": "pressions which can be controlled at will. They reflect gen-",
          "1.\nINTRODUCTION": "troencephalograms\n(EEGs).\nBy deciphering the neural\nre-"
        },
        {
          "ABSTRACT": "uine emotional responses, devoid of conscious manipulation,",
          "1.\nINTRODUCTION": "sponses associated with different emotions,\nresearchers can"
        },
        {
          "ABSTRACT": "thereby enhancing the credibility of emotion recognition sys-",
          "1.\nINTRODUCTION": "not only improve our understanding of human cognition but"
        },
        {
          "ABSTRACT": "tems.\nNonetheless, multimodal\nemotion recognition with",
          "1.\nINTRODUCTION": "also pave the way for more sophisticated BCI systems capable"
        },
        {
          "ABSTRACT": "deep learning models remains a relatively unexplored field.",
          "1.\nINTRODUCTION": "of\ninterpreting and responding to users’ emotional states in"
        },
        {
          "ABSTRACT": "In this paper, we introduce a fully hypercomplex network",
          "1.\nINTRODUCTION": "real-time. However, emotions are intrinsically multi-modal,"
        },
        {
          "ABSTRACT": "with a hierarchical\nlearning structure to fully capture corre-",
          "1.\nINTRODUCTION": "they are expressed through behavioral responses such as body"
        },
        {
          "ABSTRACT": "lations.\nSpecifically, at\nthe encoder\nlevel,\nthe model\nlearns",
          "1.\nINTRODUCTION": "language, facial expressions, and speech, as well as involun-"
        },
        {
          "ABSTRACT": "intra-modal\nrelations among the different channels of each",
          "1.\nINTRODUCTION": "tary responses, which are reflected in physiological signals."
        },
        {
          "ABSTRACT": "input\nsignal.\nThen,\na hypercomplex fusion module learns",
          "1.\nINTRODUCTION": "Indeed, studies have shown that different emotions result\nin"
        },
        {
          "ABSTRACT": "inter-modal relations among the embeddings of the different",
          "1.\nINTRODUCTION": "different responses of the brain which yield specific EEG sig-"
        },
        {
          "ABSTRACT": "modalities. The main novelty is in exploiting intra-modal re-",
          "1.\nINTRODUCTION": "nals [1]. Moreover, heart\nrhythm changes according to dif-"
        },
        {
          "ABSTRACT": "lations by endowing the encoders with parameterized hyper-",
          "1.\nINTRODUCTION": "ferent emotions which can be detected through electrocardio-"
        },
        {
          "ABSTRACT": "complex convolutions\n(PHCs)\nthat\nthanks\nto hypercomplex",
          "1.\nINTRODUCTION": "grams (ECGs) [2]. Galvanic skin response (GSR) provides a"
        },
        {
          "ABSTRACT": "algebra can capture inter-channel\ninteractions within single",
          "1.\nINTRODUCTION": "measure of the resistance of the skin, which decreases when"
        },
        {
          "ABSTRACT": "modalities.\nInstead,\nthe fusion module comprises parameter-",
          "1.\nINTRODUCTION": "one is experiencing emotions such as stress or surprise [3]. Fi-"
        },
        {
          "ABSTRACT": "ized hypercomplex multiplications\n(PHMs)\nthat can model",
          "1.\nINTRODUCTION": "nally, also the eyes can reveal insights into what emotions are"
        },
        {
          "ABSTRACT": "inter-modal correlations. The proposed architecture surpasses",
          "1.\nINTRODUCTION": "being experienced, e.g.\nit has been shown that pupil diameter"
        },
        {
          "ABSTRACT": "state-of-the-art models on the MAHNOB-HCI dataset\nfor",
          "1.\nINTRODUCTION": "changes in different emotional states, increasing when feeling"
        },
        {
          "ABSTRACT": "emotion recognition,\nspecifically in classifying valence and",
          "1.\nINTRODUCTION": "anger, fear, and anxiety or arousal and love [4]. Given these"
        },
        {
          "ABSTRACT": "arousal\nfrom electroencephalograms (EEGs) and peripheral",
          "1.\nINTRODUCTION": "relationships between physiological signals and emotional re-"
        },
        {
          "ABSTRACT": "physiological signals. The code of this study is available at",
          "1.\nINTRODUCTION": "sponses,\nresearchers are starting to turn to the physiological"
        },
        {
          "ABSTRACT": "https://github.com/ispamm/MHyEEG.",
          "1.\nINTRODUCTION": "approach for emotion recognition [5].\nIndeed,\nthey are di-"
        },
        {
          "ABSTRACT": "",
          "1.\nINTRODUCTION": "rectly related to real emotions, unlike behavioral\nreactions,"
        },
        {
          "ABSTRACT": "Index Terms— Hypercomplex\nnetworks, multimodal",
          "1.\nINTRODUCTION": ""
        },
        {
          "ABSTRACT": "",
          "1.\nINTRODUCTION": "which can lead to systems that are prone to fake emotions and"
        },
        {
          "ABSTRACT": "emotion recognition, EEG, physiological signals",
          "1.\nINTRODUCTION": ""
        },
        {
          "ABSTRACT": "",
          "1.\nINTRODUCTION": "can be manipulated easily [6]."
        },
        {
          "ABSTRACT": "",
          "1.\nINTRODUCTION": "Nonetheless,\nexisting works\nfocus mostly on a\nsingle"
        },
        {
          "ABSTRACT": "This work was\nsupported by the\nItalian Ministry of University and",
          "1.\nINTRODUCTION": "modality [7, 8] or\nrely on hand-crafted features\nextracted"
        },
        {
          "ABSTRACT": "Research\n(MUR) within\nthe PRIN 2022 Program for\nthe\nproject\n“EX-",
          "1.\nINTRODUCTION": ""
        },
        {
          "ABSTRACT": "",
          "1.\nINTRODUCTION": "from the raw signals\n[9].\nIndeed,\na multimodal approach"
        },
        {
          "ABSTRACT": "EGETE:\nExplainable Generative Deep\nLearning Methods\nfor Medical",
          "1.\nINTRODUCTION": ""
        },
        {
          "ABSTRACT": "",
          "1.\nINTRODUCTION": "yields a more powerful classifier since it\ntakes into account"
        },
        {
          "ABSTRACT": "Signal\nand Image Processing”,\nunder grant number 2022ENK9LS, CUP",
          "1.\nINTRODUCTION": ""
        },
        {
          "ABSTRACT": "B53D23013030006,\nand also by the European Union under\nthe National",
          "1.\nINTRODUCTION": "the complementary information given by the different modal-"
        },
        {
          "ABSTRACT": "Plan for Complementary Investments to the Italian National Recovery and",
          "1.\nINTRODUCTION": ""
        },
        {
          "ABSTRACT": "",
          "1.\nINTRODUCTION": "ities,\ngiven\nthat\nemotions\nare\nexpressed\nin\na multimodal"
        },
        {
          "ABSTRACT": "Resilience Plan (NRRP) of NextGenerationEU, Project PNC 0000001 D3 4",
          "1.\nINTRODUCTION": ""
        },
        {
          "ABSTRACT": "",
          "1.\nINTRODUCTION": "way.\nMoreover,\nrelying on handcrafted features\nrequires"
        },
        {
          "ABSTRACT": "Health - SPOKE 1: Clinical use cases and new models of care supported",
          "1.\nINTRODUCTION": ""
        },
        {
          "ABSTRACT": "",
          "1.\nINTRODUCTION": "extensive domain knowledge and represents a methodology"
        },
        {
          "ABSTRACT": "by AI/E-Health based solutions, and Project “Future Artificial\nIntelligence",
          "1.\nINTRODUCTION": ""
        },
        {
          "ABSTRACT": "Research” (PE0000013 - FAIR - Spoke 5: High Quality AI).",
          "1.\nINTRODUCTION": "rooted in the past.\nIn contrast, contemporary deep learning"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "encoders can autonomously acquire more discriminant\nfea-": "tures during the training. A recent work [10], addresses this",
          "The rest of the paper is organized as follows.\nIn section": "2 we discuss the recent works on hypercomplex models and"
        },
        {
          "encoders can autonomously acquire more discriminant\nfea-": "problem by conducting a preliminary study and proposing",
          "The rest of the paper is organized as follows.\nIn section": "emotion recognition. In Section 3 we provide background on"
        },
        {
          "encoders can autonomously acquire more discriminant\nfea-": "a multimodal network equipped with a novel hypercomplex",
          "The rest of the paper is organized as follows.\nIn section": "the theory behind hypercomplex networks and we describe"
        },
        {
          "encoders can autonomously acquire more discriminant\nfea-": "fusion module\ncomposed\nof\nparameterized\nhypercomplex",
          "The rest of the paper is organized as follows.\nIn section": "the proposed architecture in detail.\nIn Section 4 we discuss"
        },
        {
          "encoders can autonomously acquire more discriminant\nfea-": "multiplications (PHM). Parameterized hypercomplex neural",
          "The rest of the paper is organized as follows.\nIn section": "the experimental evaluation of our method. Finally, in Section"
        },
        {
          "encoders can autonomously acquire more discriminant\nfea-": "networks (PHNNs) are models that operate in a hypercom-",
          "The rest of the paper is organized as follows.\nIn section": "5 we draw the conclusions."
        },
        {
          "encoders can autonomously acquire more discriminant\nfea-": "plex domain which can be chosen directly by the user through",
          "The rest of the paper is organized as follows.\nIn section": ""
        },
        {
          "encoders can autonomously acquire more discriminant\nfea-": "a hyperparameter n. They represent a generalization of\nthe",
          "The rest of the paper is organized as follows.\nIn section": ""
        },
        {
          "encoders can autonomously acquire more discriminant\nfea-": "",
          "The rest of the paper is organized as follows.\nIn section": "2. RELATED WORKS"
        },
        {
          "encoders can autonomously acquire more discriminant\nfea-": "more popular quaternion neural networks (QNNs) which are",
          "The rest of the paper is organized as follows.\nIn section": ""
        },
        {
          "encoders can autonomously acquire more discriminant\nfea-": "defined in the quaternion domain.\nIndeed, PHNNs can op-",
          "The rest of the paper is organized as follows.\nIn section": ""
        },
        {
          "encoders can autonomously acquire more discriminant\nfea-": "",
          "The rest of the paper is organized as follows.\nIn section": "Many studies have developed machine learning [16, 17] and"
        },
        {
          "encoders can autonomously acquire more discriminant\nfea-": "erate on any n-dimensional data, unlike quaternion models",
          "The rest of the paper is organized as follows.\nIn section": ""
        },
        {
          "encoders can autonomously acquire more discriminant\nfea-": "",
          "The rest of the paper is organized as follows.\nIn section": "deep learning-based [8, 18] systems for emotion recognition."
        },
        {
          "encoders can autonomously acquire more discriminant\nfea-": "which work with quaternion data and are thus limited to 4-",
          "The rest of the paper is organized as follows.\nIn section": ""
        },
        {
          "encoders can autonomously acquire more discriminant\nfea-": "",
          "The rest of the paper is organized as follows.\nIn section": "Nevertheless,\nthese methods require either extensive domain"
        },
        {
          "encoders can autonomously acquire more discriminant\nfea-": "dimensional\ninputs. Moreover,\nthey retain the advantages",
          "The rest of the paper is organized as follows.\nIn section": ""
        },
        {
          "encoders can autonomously acquire more discriminant\nfea-": "",
          "The rest of the paper is organized as follows.\nIn section": "knowledge for extracting handcrafted features or employ pop-"
        },
        {
          "encoders can autonomously acquire more discriminant\nfea-": "of QNNs, yielding lightweight models with the number of",
          "The rest of the paper is organized as follows.\nIn section": ""
        },
        {
          "encoders can autonomously acquire more discriminant\nfea-": "",
          "The rest of the paper is organized as follows.\nIn section": "ular extracted features\nsuch as differential entropy (DE) or"
        },
        {
          "encoders can autonomously acquire more discriminant\nfea-": "parameters\nreduced by 1/n and with the\nability to model",
          "The rest of the paper is organized as follows.\nIn section": ""
        },
        {
          "encoders can autonomously acquire more discriminant\nfea-": "",
          "The rest of the paper is organized as follows.\nIn section": "power spectral density (PSD). These approaches do not allow"
        },
        {
          "encoders can autonomously acquire more discriminant\nfea-": "local\nrelations among data dimensions\n[11–15].\nHowever,",
          "The rest of the paper is organized as follows.\nIn section": ""
        },
        {
          "encoders can autonomously acquire more discriminant\nfea-": "",
          "The rest of the paper is organized as follows.\nIn section": "the neural model\nto learn features on its own and require ad-"
        },
        {
          "encoders can autonomously acquire more discriminant\nfea-": "the model\nintroduced in [10], HyperFuseNet,\nstill presents",
          "The rest of the paper is organized as follows.\nIn section": ""
        },
        {
          "encoders can autonomously acquire more discriminant\nfea-": "",
          "The rest of the paper is organized as follows.\nIn section": "ditional preprocessing steps [9]. For these reasons, methods"
        },
        {
          "encoders can autonomously acquire more discriminant\nfea-": "some limitations.\nThe utilization of hypercomplex algebra",
          "The rest of the paper is organized as follows.\nIn section": ""
        },
        {
          "encoders can autonomously acquire more discriminant\nfea-": "",
          "The rest of the paper is organized as follows.\nIn section": "that learn directly from raw signals have started to emerge. A"
        },
        {
          "encoders can autonomously acquire more discriminant\nfea-": "is confined solely to the fusion module and is not\nintegrated",
          "The rest of the paper is organized as follows.\nIn section": ""
        },
        {
          "encoders can autonomously acquire more discriminant\nfea-": "",
          "The rest of the paper is organized as follows.\nIn section": "recent study has proposed a reinforcement learning approach"
        },
        {
          "encoders can autonomously acquire more discriminant\nfea-": "into the encoder itself. Additionally, in general, these models",
          "The rest of the paper is organized as follows.\nIn section": ""
        },
        {
          "encoders can autonomously acquire more discriminant\nfea-": "",
          "The rest of the paper is organized as follows.\nIn section": "inspired by brain emotion perception and shows the advan-"
        },
        {
          "encoders can autonomously acquire more discriminant\nfea-": "have a tendency to overfit excessively, resulting in inadequate",
          "The rest of the paper is organized as follows.\nIn section": ""
        },
        {
          "encoders can autonomously acquire more discriminant\nfea-": "",
          "The rest of the paper is organized as follows.\nIn section": "tages of using raw signals [7]. Furthermore, another approach"
        },
        {
          "encoders can autonomously acquire more discriminant\nfea-": "generalization performance.\nTherefore,\nin this paper, we",
          "The rest of the paper is organized as follows.\nIn section": ""
        },
        {
          "encoders can autonomously acquire more discriminant\nfea-": "",
          "The rest of the paper is organized as follows.\nIn section": "to increase the performance of an emotion recognition system"
        },
        {
          "encoders can autonomously acquire more discriminant\nfea-": "build upon this preliminary study in order\nto address these",
          "The rest of the paper is organized as follows.\nIn section": ""
        },
        {
          "encoders can autonomously acquire more discriminant\nfea-": "",
          "The rest of the paper is organized as follows.\nIn section": "involves designing a multimodal framework [9].\nIndeed, re-"
        },
        {
          "encoders can autonomously acquire more discriminant\nfea-": "problems.",
          "The rest of the paper is organized as follows.\nIn section": ""
        },
        {
          "encoders can autonomously acquire more discriminant\nfea-": "",
          "The rest of the paper is organized as follows.\nIn section": "cently proposed methods include a manifold learning-based"
        },
        {
          "encoders can autonomously acquire more discriminant\nfea-": "We propose\na Hierarchical Hypercomplex (H2) model",
          "The rest of the paper is organized as follows.\nIn section": "technique for multimodal emotion recognition [19], a method"
        },
        {
          "encoders can autonomously acquire more discriminant\nfea-": "with a hierarchical\nstructure wherein the encoders and the",
          "The rest of the paper is organized as follows.\nIn section": "to improve generalization across unseen target domains from"
        },
        {
          "encoders can autonomously acquire more discriminant\nfea-": "fusion module operate in the hypercomplex domain.\nThe",
          "The rest of the paper is organized as follows.\nIn section": "EEG and eye movement signals [20], and a hypercomplex-"
        },
        {
          "encoders can autonomously acquire more discriminant\nfea-": "hierarchical structure allows to first\nlearn embeddings of the",
          "The rest of the paper is organized as follows.\nIn section": "based architecture, HyperFuseNet, with a novel fusion mod-"
        },
        {
          "encoders can autonomously acquire more discriminant\nfea-": "single modalities exploiting relations among the channels that",
          "The rest of the paper is organized as follows.\nIn section": "ule [10].\nIn this paper, we tackle the limitations of hand-"
        },
        {
          "encoders can autonomously acquire more discriminant\nfea-": "compose each modality through parameterized hypercomplex",
          "The rest of the paper is organized as follows.\nIn section": "crafted features and single-modality approaches by extend-"
        },
        {
          "encoders can autonomously acquire more discriminant\nfea-": "convolutions (PHCs).\nIn contrast,\nthe fusion module learns a",
          "The rest of the paper is organized as follows.\nIn section": "ing the method proposed in a preliminary study [10], Hyper-"
        },
        {
          "encoders can autonomously acquire more discriminant\nfea-": "fused representation from the embeddings of each modality,",
          "The rest of the paper is organized as follows.\nIn section": "FuseNet. Although this model\nlearns directly from raw mul-"
        },
        {
          "encoders can autonomously acquire more discriminant\nfea-": "leveraging relations among the modalities themselves thanks",
          "The rest of the paper is organized as follows.\nIn section": "timodal signals, it suffers from poor generalization ability. To"
        },
        {
          "encoders can autonomously acquire more discriminant\nfea-": "to parameterized hypercomplex multiplications (PHMs).\nIn",
          "The rest of the paper is organized as follows.\nIn section": "overcome this limitation, we introduce a hierarchical design,"
        },
        {
          "encoders can autonomously acquire more discriminant\nfea-": "detail,\nthe encoders are tailored to each modality by setting",
          "The rest of the paper is organized as follows.\nIn section": "resulting in substantial improvements, as demonstrated by the"
        },
        {
          "encoders can autonomously acquire more discriminant\nfea-": "the hyperparameter n equal\nto the number of dimensions of",
          "The rest of the paper is organized as follows.\nIn section": "experimental results in Section 4."
        },
        {
          "encoders can autonomously acquire more discriminant\nfea-": "each input signal.\nIn this way, each encoder operates within",
          "The rest of the paper is organized as follows.\nIn section": ""
        },
        {
          "encoders can autonomously acquire more discriminant\nfea-": "its own distinct hypercomplex domain which is the natural",
          "The rest of the paper is organized as follows.\nIn section": ""
        },
        {
          "encoders can autonomously acquire more discriminant\nfea-": "",
          "The rest of the paper is organized as follows.\nIn section": "3. METHOD"
        },
        {
          "encoders can autonomously acquire more discriminant\nfea-": "domain of\nthe specific signal.\nThus,\nthe encoders yield an",
          "The rest of the paper is organized as follows.\nIn section": ""
        },
        {
          "encoders can autonomously acquire more discriminant\nfea-": "enriched embedding thanks to convolutions and hypercom-",
          "The rest of the paper is organized as follows.\nIn section": ""
        },
        {
          "encoders can autonomously acquire more discriminant\nfea-": "",
          "The rest of the paper is organized as follows.\nIn section": "3.1. Parameterized Hypercomplex Networks"
        },
        {
          "encoders can autonomously acquire more discriminant\nfea-": "plex algebra properties.\nIndeed, convolutions are well-suited",
          "The rest of the paper is organized as follows.\nIn section": ""
        },
        {
          "encoders can autonomously acquire more discriminant\nfea-": "for processing multidimensional data, while hypercomplex",
          "The rest of the paper is organized as follows.\nIn section": "Parameterized hypercomplex neural networks (PHNNs) have"
        },
        {
          "encoders can autonomously acquire more discriminant\nfea-": "algebra endows the encoders with the ability to model and ex-",
          "The rest of the paper is organized as follows.\nIn section": "been proposed in order\nto overcome the limitations of\nthe"
        },
        {
          "encoders can autonomously acquire more discriminant\nfea-": "ploit correlations among the various dimensions within each",
          "The rest of the paper is organized as follows.\nIn section": "popular quaternion models while keeping their advantages"
        },
        {
          "encoders can autonomously acquire more discriminant\nfea-": "individual modality. Certainly, the modalities we employ, i.e.,",
          "The rest of the paper is organized as follows.\nIn section": "and useful properties\n[11, 12].\nIndeed, quaternion models"
        },
        {
          "encoders can autonomously acquire more discriminant\nfea-": "EEG, ECG, and eye data, are all multidimensional (except for",
          "The rest of the paper is organized as follows.\nIn section": "are limited to 4-dimensional inputs as they work with quater-"
        },
        {
          "encoders can autonomously acquire more discriminant\nfea-": "GSR) and inherently exhibit correlations between their chan-",
          "The rest of the paper is organized as follows.\nIn section": "nions,\ni.e., q = q0 + q1ˆı + q2ij + q3ˆκ where qi ∈ R and"
        },
        {
          "encoders can autonomously acquire more discriminant\nfea-": "nels, an essential aspect of their information content. Thus,",
          "The rest of the paper is organized as follows.\nIn section": "ı, ij, ˆκ ∈ Q.\nInstead, PHNNs are flexible to work with any"
        },
        {
          "encoders can autonomously acquire more discriminant\nfea-": "with this hierarchical design, we increase the performance of",
          "The rest of the paper is organized as follows.\nIn section": "n-dimensional data, where n ∈ N is a hyperparameter\nthat"
        },
        {
          "encoders can autonomously acquire more discriminant\nfea-": "the state-of-the-art arousal and valence classification on the",
          "The rest of the paper is organized as follows.\nIn section": "users can adjust based on their specific requirements.\nThis"
        },
        {
          "encoders can autonomously acquire more discriminant\nfea-": "MAHNOB-HCI database [3], achieving an F1-score of 0.557",
          "The rest of the paper is organized as follows.\nIn section": "flexibility is achieved through a specific construction of\nthe"
        },
        {
          "encoders can autonomously acquire more discriminant\nfea-": "and 0.685, respectively, which represents an improvement of",
          "The rest of the paper is organized as follows.\nIn section": "weight matrix within the layers comprising the PHNNs. Clas-"
        },
        {
          "encoders can autonomously acquire more discriminant\nfea-": "40.20% and 57.11%.",
          "The rest of the paper is organized as follows.\nIn section": "sic fully-connected and convolutional\nlayers are equivalent"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Medium aroused/": "Neutral valence"
        },
        {
          "Medium aroused/": "Excited/Pleasant"
        },
        {
          "Medium aroused/": "learns enriched modality-specific embeddings by exploiting inter-channel\nrelations within"
        },
        {
          "Medium aroused/": ""
        },
        {
          "Medium aroused/": ""
        },
        {
          "Medium aroused/": ""
        },
        {
          "Medium aroused/": "global relations,\ni.e.,\nintra-channel\ninteractions, and local re-"
        },
        {
          "Medium aroused/": "lations, i.e., inter-channel interactions, which real-valued net-"
        },
        {
          "Medium aroused/": "works tend to ignore [21]."
        },
        {
          "Medium aroused/": ""
        },
        {
          "Medium aroused/": "3.2. Hierarchical Hypercomplex Model"
        },
        {
          "Medium aroused/": ""
        },
        {
          "Medium aroused/": "In this section, we describe the proposed Hierarchical Hyper-"
        },
        {
          "Medium aroused/": ""
        },
        {
          "Medium aroused/": "complex (H2) model for multimodal emotion recognition, de-"
        },
        {
          "Medium aroused/": "picted in Fig. 1. The architecture has a hierarchical structure"
        },
        {
          "Medium aroused/": "where encoders operating in different hypercomplex domains"
        },
        {
          "Medium aroused/": "learn modality-specific embeddings, while the hypercomplex"
        },
        {
          "Medium aroused/": "fusion module learns a fused embedding. Mainly,\nthe hier-"
        },
        {
          "Medium aroused/": "archical structure refers to the level of\nrelations being con-"
        },
        {
          "Medium aroused/": "sidered, i.e., intra-modality and inter-modality. The encoders"
        },
        {
          "Medium aroused/": ""
        },
        {
          "Medium aroused/": "model\nrelations between channels within a single modality,"
        },
        {
          "Medium aroused/": ""
        },
        {
          "Medium aroused/": "thus they exploit intra-modality relations. In contrast, the hy-"
        },
        {
          "Medium aroused/": "percomplex fusion module exploits relations among the dif-"
        },
        {
          "Medium aroused/": "ferent modalities\nthemselves,\ni.e.,\ninter-modal\ninteractions."
        },
        {
          "Medium aroused/": "In detail,\nthe encoders of EEG, ECG, and eye signals com-"
        },
        {
          "Medium aroused/": "prise two PHC layers with two batch normalization (BN) lay-"
        },
        {
          "Medium aroused/": "ers and ReLU activation functions.\nInstead, being GSR a 1-"
        },
        {
          "Medium aroused/": "dimensional signal, its encoder is composed of a single PHM"
        },
        {
          "Medium aroused/": "layer\ntogether with a BN layer and ReLU. Then,\na global"
        },
        {
          "Medium aroused/": "average pooling operation is applied to get\nthe final\nlatent"
        },
        {
          "Medium aroused/": "representation.\nFor PHC and PHM layers, we set nEEG ="
        },
        {
          "Medium aroused/": "10, nECG = 3, neye = 4, and nGSR = 1, in accordance with the"
        },
        {
          "Medium aroused/": "number of channels of each different signal.\nIn this way,\nthe"
        },
        {
          "Medium aroused/": "encoders are endowed with the ability to learn not only intra-"
        },
        {
          "Medium aroused/": "channel\nrelations,\nas any standard network, but also inter-"
        },
        {
          "Medium aroused/": "channel relations. Thus, they exploit the inherent correlations"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "HyperFuseNet\n0.397 ± 0.018": "H2 (ours)\n0.557 ± 0.011",
          "41.56 ± 1.33\n0.436 ± 0.022\n44.30 ± 2.01": "56.91 ± 0.99\n0.685 ± 0.015\n67.87 ± 1.48"
        },
        {
          "HyperFuseNet\n0.397 ± 0.018": "\u0000+\u0000\\\u0000S\u0000H\u0000U\u0000)\u0000X\u0000V\u0000H\u00001\u0000H\u0000W\n\u0000+\u0000\u0015\u0000\u0003\u0000P\u0000R\u0000G\u0000H\u0000O\u0000\u0003\u0000\u000b\u0000S\u0000U\u0000R\u0000S\u0000R\u0000V\u0000H\u0000G\u0000",
          "41.56 ± 1.33\n0.436 ± 0.022\n44.30 ± 2.01": "4. EXPERIMENTAL RESULTS"
        },
        {
          "HyperFuseNet\n0.397 ± 0.018": "\u0000&\u0000D\u0000O\u0000P\n\u0000&\u0000D\u0000O\u0000P",
          "41.56 ± 1.33\n0.436 ± 0.022\n44.30 ± 2.01": ""
        },
        {
          "HyperFuseNet\n0.397 ± 0.018": "\u00000\u0000H\u0000G\u0000L\u0000X\u0000P\u0000\u0003\u0000D\u0000U\u0000R\u0000X\u0000V\u0000H\u0000G\n\u00000\u0000H\u0000G\u0000L\u0000X\u0000P\u0000\u0003\u0000D\u0000U\u0000R\u0000X\u0000V\u0000H\u0000G",
          "41.56 ± 1.33\n0.436 ± 0.022\n44.30 ± 2.01": ""
        },
        {
          "HyperFuseNet\n0.397 ± 0.018": "\u0000(\u0000[\u0000F\u0000L\u0000W\u0000H\u0000G\n\u0000(\u0000[\u0000F\u0000L\u0000W\u0000H\u0000G",
          "41.56 ± 1.33\n0.436 ± 0.022\n44.30 ± 2.01": "4.1. Dataset and preprocessing"
        },
        {
          "HyperFuseNet\n0.397 ± 0.018": "\u0000$\u0000U\u0000R\u0000X\u0000V\u0000D\u0000O",
          "41.56 ± 1.33\n0.436 ± 0.022\n44.30 ± 2.01": "For training and evaluating our models, we utilize a publicly"
        },
        {
          "HyperFuseNet\n0.397 ± 0.018": "",
          "41.56 ± 1.33\n0.436 ± 0.022\n44.30 ± 2.01": "available database for affect recognition, MAHNOB-HCI [3]."
        },
        {
          "HyperFuseNet\n0.397 ± 0.018": "",
          "41.56 ± 1.33\n0.436 ± 0.022\n44.30 ± 2.01": "It provides synchronized recordings of 27 participants during"
        },
        {
          "HyperFuseNet\n0.397 ± 0.018": "",
          "41.56 ± 1.33\n0.436 ± 0.022\n44.30 ± 2.01": "an experiment in which each subject was shown fragments of"
        },
        {
          "HyperFuseNet\n0.397 ± 0.018": "",
          "41.56 ± 1.33\n0.436 ± 0.022\n44.30 ± 2.01": "movies that\ninduced different emotional responses.\nIn detail,"
        },
        {
          "HyperFuseNet\n0.397 ± 0.018": "",
          "41.56 ± 1.33\n0.436 ± 0.022\n44.30 ± 2.01": "the subjects were monitored with video cameras, a head-worn"
        },
        {
          "HyperFuseNet\n0.397 ± 0.018": "",
          "41.56 ± 1.33\n0.436 ± 0.022\n44.30 ± 2.01": "microphone, an eye gaze tracker, as well as physiological sen-"
        },
        {
          "HyperFuseNet\n0.397 ± 0.018": "\u00009\u0000D\u0000O\u0000H\u0000Q\u0000F\u0000H",
          "41.56 ± 1.33\n0.436 ± 0.022\n44.30 ± 2.01": "sors measuring EEG, ECG,\nrespiration amplitude, and skin"
        },
        {
          "HyperFuseNet\n0.397 ± 0.018": "",
          "41.56 ± 1.33\n0.436 ± 0.022\n44.30 ± 2.01": "temperature. In this work, we employ as modalities the EEG,"
        },
        {
          "HyperFuseNet\n0.397 ± 0.018": "\u00008\u0000Q\u0000S\u0000O\u0000H\u0000D\u0000V\u0000D\u0000Q\u0000W\n\u00008\u0000Q\u0000S\u0000O\u0000H\u0000D\u0000V\u0000D\u0000Q\u0000W",
          "41.56 ± 1.33\n0.436 ± 0.022\n44.30 ± 2.01": ""
        },
        {
          "HyperFuseNet\n0.397 ± 0.018": "\u00001\u0000H\u0000X\u0000W\u0000U\u0000D\u0000O\n\u00001\u0000H\u0000X\u0000W\u0000U\u0000D\u0000O",
          "41.56 ± 1.33\n0.436 ± 0.022\n44.30 ± 2.01": "ECG, GSR, all recorded at 256Hz, and eye data, recorded at"
        },
        {
          "HyperFuseNet\n0.397 ± 0.018": "\u00003\u0000O\u0000H\u0000D\u0000V\u0000D\u0000Q\u0000W\n\u00003\u0000O\u0000H\u0000D\u0000V\u0000D\u0000Q\u0000W",
          "41.56 ± 1.33\n0.436 ± 0.022\n44.30 ± 2.01": ""
        },
        {
          "HyperFuseNet\n0.397 ± 0.018": "",
          "41.56 ± 1.33\n0.436 ± 0.022\n44.30 ± 2.01": "60Hz. The latter comprises gaze coordinates, eye distances,"
        },
        {
          "HyperFuseNet\n0.397 ± 0.018": "",
          "41.56 ± 1.33\n0.436 ± 0.022\n44.30 ± 2.01": "and pupil dimensions. Each synchronized recording is labeled"
        },
        {
          "HyperFuseNet\n0.397 ± 0.018": "Fig. 2.\nt-SNE feature visualization.",
          "41.56 ± 1.33\n0.436 ± 0.022\n44.30 ± 2.01": "on a scale of arousal (calm, medium aroused, and excited) and"
        },
        {
          "HyperFuseNet\n0.397 ± 0.018": "",
          "41.56 ± 1.33\n0.436 ± 0.022\n44.30 ± 2.01": "valence (unpleasant, neutral, and pleasant)."
        },
        {
          "HyperFuseNet\n0.397 ± 0.018": "",
          "41.56 ± 1.33\n0.436 ± 0.022\n44.30 ± 2.01": "We apply the same preprocessing as in [10]. The dataset"
        },
        {
          "HyperFuseNet\n0.397 ± 0.018": "among channels of the single modalities which enrich the la-",
          "41.56 ± 1.33\n0.436 ± 0.022\n44.30 ± 2.01": "provides EEG with 32 electrodes, among which we select 10"
        },
        {
          "HyperFuseNet\n0.397 ± 0.018": "tent\nrepresentation of each signal. Moreover, by setting the",
          "41.56 ± 1.33\n0.436 ± 0.022\n44.30 ± 2.01": "most related to emotion,\ni.e., F3, F4, F5, F6, F7, F8, T7, T8,"
        },
        {
          "HyperFuseNet\n0.397 ± 0.018": "n parameter equal\nto the number of channels of each input",
          "41.56 ± 1.33\n0.436 ± 0.022\n44.30 ± 2.01": "P7, and P8 [22, 23]. EEG, ECG, and GSR are first downsam-"
        },
        {
          "HyperFuseNet\n0.397 ± 0.018": "signal, each encoder operates in a different hypercomplex do-",
          "41.56 ± 1.33\n0.436 ± 0.022\n44.30 ± 2.01": "pled to 128Hz. Secondly, EEG signals are referenced to the"
        },
        {
          "HyperFuseNet\n0.397 ± 0.018": "main which is also the natural domain of each input signal.",
          "41.56 ± 1.33\n0.436 ± 0.022\n44.30 ± 2.01": "average reference.\nThen, EEG and ECG are filtered with a"
        },
        {
          "HyperFuseNet\n0.397 ± 0.018": "The hypercomplex fusion module employed in this archi-",
          "41.56 ± 1.33\n0.436 ± 0.022\n44.30 ± 2.01": "band-pass filter of 1-45Hz and 0.5-45Hz, respectively, a low-"
        },
        {
          "HyperFuseNet\n0.397 ± 0.018": "tecture is a new version of the one introduced in a preliminary",
          "41.56 ± 1.33\n0.436 ± 0.022\n44.30 ± 2.01": "pass filter for GSR at 60Hz, and a final notch filter at 50Hz for"
        },
        {
          "HyperFuseNet\n0.397 ± 0.018": "work [10]. Specifically, it is modified by removing one PHM",
          "41.56 ± 1.33\n0.436 ± 0.022\n44.30 ± 2.01": "each of them. To account for the initial offset of GSR signals,"
        },
        {
          "HyperFuseNet\n0.397 ± 0.018": "layer and incorporating more dropout\nlayers in order\nto re-",
          "41.56 ± 1.33\n0.436 ± 0.022\n44.30 ± 2.01": "baseline correction is performed by adjusting it relative to the"
        },
        {
          "HyperFuseNet\n0.397 ± 0.018": "duce overfitting, which the HyperFuseNet architecture strug-",
          "41.56 ± 1.33\n0.436 ± 0.022\n44.30 ± 2.01": "mean value within the preceding 200ms of each trial. For the"
        },
        {
          "HyperFuseNet\n0.397 ± 0.018": "gled with.",
          "41.56 ± 1.33\n0.436 ± 0.022\n44.30 ± 2.01": "other signals,\nthis correction is automatically achieved after"
        },
        {
          "HyperFuseNet\n0.397 ± 0.018": "To conclude,\nthe H2 model can be seen as an extension",
          "41.56 ± 1.33\n0.436 ± 0.022\n44.30 ± 2.01": "the high-pass filters. Lastly, for eye data, we consider the av-"
        },
        {
          "HyperFuseNet\n0.397 ± 0.018": "of the method proposed in [10], namely HyperFuseNet. The",
          "41.56 ± 1.33\n0.436 ± 0.022\n44.30 ± 2.01": "erage between measurements of the left and right eye, and we"
        },
        {
          "HyperFuseNet\n0.397 ± 0.018": "main difference lies in the encoders, which before were com-",
          "41.56 ± 1.33\n0.436 ± 0.022\n44.30 ± 2.01": "maintain -1 values as they indicate blinks or rapid movements"
        },
        {
          "HyperFuseNet\n0.397 ± 0.018": "posed of standard fully-connected layers in the real domain,",
          "41.56 ± 1.33\n0.436 ± 0.022\n44.30 ± 2.01": "which can be related to an emotional response."
        },
        {
          "HyperFuseNet\n0.397 ± 0.018": "instead of convolutional\nlayers in the hypercomplex domain.",
          "41.56 ± 1.33\n0.436 ± 0.022\n44.30 ± 2.01": "Samples are constructed by extracting segments of 10s"
        },
        {
          "HyperFuseNet\n0.397 ± 0.018": "Therefore, HyperFuseNet did not have the hierarchical struc-",
          "41.56 ± 1.33\n0.436 ± 0.022\n44.30 ± 2.01": "from the original 30s of each trial. The dataset\nis split\ninto"
        },
        {
          "HyperFuseNet\n0.397 ± 0.018": "ture that we introduce in this paper, which is also the main",
          "41.56 ± 1.33\n0.436 ± 0.022\n44.30 ± 2.01": "training (80%) and testing (20%) in a stratified manner. The"
        },
        {
          "HyperFuseNet\n0.397 ± 0.018": "advantage of the proposed network. Moreover, we revisited",
          "41.56 ± 1.33\n0.436 ± 0.022\n44.30 ± 2.01": "training data is augmented with scaling and Gaussian noise"
        },
        {
          "HyperFuseNet\n0.397 ± 0.018": "the overall structure by reducing the number of\nlayers by 1",
          "41.56 ± 1.33\n0.436 ± 0.022\n44.30 ± 2.01": "addition as in [10]."
        },
        {
          "HyperFuseNet\n0.397 ± 0.018": "in both the encoders and the hypercomplex fusion module in",
          "41.56 ± 1.33\n0.436 ± 0.022\n44.30 ± 2.01": ""
        },
        {
          "HyperFuseNet\n0.397 ± 0.018": "order\nto make them more proportionate to the quantity and",
          "41.56 ± 1.33\n0.436 ± 0.022\n44.30 ± 2.01": ""
        },
        {
          "HyperFuseNet\n0.397 ± 0.018": "",
          "41.56 ± 1.33\n0.436 ± 0.022\n44.30 ± 2.01": "4.2. Experimental setup"
        },
        {
          "HyperFuseNet\n0.397 ± 0.018": "nature of the input data. Also, for this reason, we add dropout",
          "41.56 ± 1.33\n0.436 ± 0.022\n44.30 ± 2.01": ""
        },
        {
          "HyperFuseNet\n0.397 ± 0.018": "layers with a dropout rate of 0.5 between each PHM layer of",
          "41.56 ± 1.33\n0.436 ± 0.022\n44.30 ± 2.01": "For evaluating our models, we utilize as metrics the accuracy"
        },
        {
          "HyperFuseNet\n0.397 ± 0.018": "the hypercomplex fusion module.",
          "41.56 ± 1.33\n0.436 ± 0.022\n44.30 ± 2.01": "and the F1-score, i.e., the harmonic mean of the precision and"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 2: Ablations on arousal. In parentheses we report the",
      "data": [
        {
          "recall, which accounts\nfor\nimbalanced data.\nThe networks": ""
        },
        {
          "recall, which accounts\nfor\nimbalanced data.\nThe networks": "are trained with the Adam optimizer, cross-entropy loss, and"
        },
        {
          "recall, which accounts\nfor\nimbalanced data.\nThe networks": ""
        },
        {
          "recall, which accounts\nfor\nimbalanced data.\nThe networks": "the once-cycle policy with 0.425% of increasing steps, linear"
        },
        {
          "recall, which accounts\nfor\nimbalanced data.\nThe networks": ""
        },
        {
          "recall, which accounts\nfor\nimbalanced data.\nThe networks": "annealing strategy, dividing factors of 10, maximum learning"
        },
        {
          "recall, which accounts\nfor\nimbalanced data.\nThe networks": ""
        },
        {
          "recall, which accounts\nfor\nimbalanced data.\nThe networks": "rate of 7.96 × 10−6, and minimum and maximum momentum"
        },
        {
          "recall, which accounts\nfor\nimbalanced data.\nThe networks": "of 0.7403 and 0.8314, respectively. The number of epochs is"
        },
        {
          "recall, which accounts\nfor\nimbalanced data.\nThe networks": "set\nto 50, but we early stop the training when the F1-score"
        },
        {
          "recall, which accounts\nfor\nimbalanced data.\nThe networks": "does not improve for 10 epochs."
        },
        {
          "recall, which accounts\nfor\nimbalanced data.\nThe networks": ""
        },
        {
          "recall, which accounts\nfor\nimbalanced data.\nThe networks": ""
        },
        {
          "recall, which accounts\nfor\nimbalanced data.\nThe networks": "4.3. Results and discussion"
        },
        {
          "recall, which accounts\nfor\nimbalanced data.\nThe networks": "The main results of our experiments are reported in Tab. 1."
        },
        {
          "recall, which accounts\nfor\nimbalanced data.\nThe networks": "Our model\nis compared against\ntwo state-of-the-art architec-"
        },
        {
          "recall, which accounts\nfor\nimbalanced data.\nThe networks": ""
        },
        {
          "recall, which accounts\nfor\nimbalanced data.\nThe networks": "tures that operate with raw signals,\ni.e., HyperFuseNet\n[10]"
        },
        {
          "recall, which accounts\nfor\nimbalanced data.\nThe networks": ""
        },
        {
          "recall, which accounts\nfor\nimbalanced data.\nThe networks": "and the multimodal model proposed by Dolmans et.\nal [24]"
        },
        {
          "recall, which accounts\nfor\nimbalanced data.\nThe networks": ""
        },
        {
          "recall, which accounts\nfor\nimbalanced data.\nThe networks": "which was originally designed for mental workload classifi-"
        },
        {
          "recall, which accounts\nfor\nimbalanced data.\nThe networks": ""
        },
        {
          "recall, which accounts\nfor\nimbalanced data.\nThe networks": "cation.\nThe proposed hierarchical architecture significantly"
        },
        {
          "recall, which accounts\nfor\nimbalanced data.\nThe networks": ""
        },
        {
          "recall, which accounts\nfor\nimbalanced data.\nThe networks": "outperforms both HyperFuseNet and the other\nstate-of-the-"
        },
        {
          "recall, which accounts\nfor\nimbalanced data.\nThe networks": ""
        },
        {
          "recall, which accounts\nfor\nimbalanced data.\nThe networks": "art model, as demonstrated by the results in Tab. 1.\nIn both"
        },
        {
          "recall, which accounts\nfor\nimbalanced data.\nThe networks": ""
        },
        {
          "recall, which accounts\nfor\nimbalanced data.\nThe networks": "arousal and valence classification, our model brings a sub-"
        },
        {
          "recall, which accounts\nfor\nimbalanced data.\nThe networks": ""
        },
        {
          "recall, which accounts\nfor\nimbalanced data.\nThe networks": "stantial improvement, i.e., of 40.20% and 57.11% for the F1-"
        },
        {
          "recall, which accounts\nfor\nimbalanced data.\nThe networks": ""
        },
        {
          "recall, which accounts\nfor\nimbalanced data.\nThe networks": "score, respectively. This great gap is due to several reasons."
        },
        {
          "recall, which accounts\nfor\nimbalanced data.\nThe networks": "First, the other two models are quite prone to overfitting, thus"
        },
        {
          "recall, which accounts\nfor\nimbalanced data.\nThe networks": "when tested on unseen trials they are not able to generalize"
        },
        {
          "recall, which accounts\nfor\nimbalanced data.\nThe networks": ""
        },
        {
          "recall, which accounts\nfor\nimbalanced data.\nThe networks": "well.\nInstead, in our method we address these problems with"
        },
        {
          "recall, which accounts\nfor\nimbalanced data.\nThe networks": ""
        },
        {
          "recall, which accounts\nfor\nimbalanced data.\nThe networks": "additional dropout\nlayers with a higher\nrate with respect\nto"
        },
        {
          "recall, which accounts\nfor\nimbalanced data.\nThe networks": ""
        },
        {
          "recall, which accounts\nfor\nimbalanced data.\nThe networks": "HyperFuseNet (which only had one dropout\nlayer), reducing"
        },
        {
          "recall, which accounts\nfor\nimbalanced data.\nThe networks": ""
        },
        {
          "recall, which accounts\nfor\nimbalanced data.\nThe networks": "the overall number of layers by removing one in each encoder"
        },
        {
          "recall, which accounts\nfor\nimbalanced data.\nThe networks": ""
        },
        {
          "recall, which accounts\nfor\nimbalanced data.\nThe networks": "and fusion module, and, most importantly, integrating hyper-"
        },
        {
          "recall, which accounts\nfor\nimbalanced data.\nThe networks": ""
        },
        {
          "recall, which accounts\nfor\nimbalanced data.\nThe networks": "complex algebra also at encoder-level. Indeed, this last aspect"
        },
        {
          "recall, which accounts\nfor\nimbalanced data.\nThe networks": ""
        },
        {
          "recall, which accounts\nfor\nimbalanced data.\nThe networks": "allows to exploit both intra-modal and inter-modal\nrelations"
        },
        {
          "recall, which accounts\nfor\nimbalanced data.\nThe networks": ""
        },
        {
          "recall, which accounts\nfor\nimbalanced data.\nThe networks": "in a hierarchical manner. This is the main advantage of\nthe"
        },
        {
          "recall, which accounts\nfor\nimbalanced data.\nThe networks": ""
        },
        {
          "recall, which accounts\nfor\nimbalanced data.\nThe networks": "proposed model as we demonstrate in the ablation studies in"
        },
        {
          "recall, which accounts\nfor\nimbalanced data.\nThe networks": ""
        },
        {
          "recall, which accounts\nfor\nimbalanced data.\nThe networks": "the following section.\nIn fact, it is not due to just a reduction"
        },
        {
          "recall, which accounts\nfor\nimbalanced data.\nThe networks": ""
        },
        {
          "recall, which accounts\nfor\nimbalanced data.\nThe networks": "of parameters given by hypercomplex algebra as this could re-"
        },
        {
          "recall, which accounts\nfor\nimbalanced data.\nThe networks": ""
        },
        {
          "recall, which accounts\nfor\nimbalanced data.\nThe networks": "sult in underfitting, as we show in Section 4.4. Instead, thanks"
        },
        {
          "recall, which accounts\nfor\nimbalanced data.\nThe networks": ""
        },
        {
          "recall, which accounts\nfor\nimbalanced data.\nThe networks": "to its properties the encoders are able to leverage correlations"
        },
        {
          "recall, which accounts\nfor\nimbalanced data.\nThe networks": ""
        },
        {
          "recall, which accounts\nfor\nimbalanced data.\nThe networks": "among channels of single modalities,\nthus significantly out-"
        },
        {
          "recall, which accounts\nfor\nimbalanced data.\nThe networks": ""
        },
        {
          "recall, which accounts\nfor\nimbalanced data.\nThe networks": "performing HyperFuseNet. Moreover, Figure 2 depicts the"
        },
        {
          "recall, which accounts\nfor\nimbalanced data.\nThe networks": ""
        },
        {
          "recall, which accounts\nfor\nimbalanced data.\nThe networks": "t-SNE visualization [25] of the features learned from H2 and"
        },
        {
          "recall, which accounts\nfor\nimbalanced data.\nThe networks": ""
        },
        {
          "recall, which accounts\nfor\nimbalanced data.\nThe networks": "HyperFuseNet. From the image, it is clear that HyperFuseNet"
        },
        {
          "recall, which accounts\nfor\nimbalanced data.\nThe networks": ""
        },
        {
          "recall, which accounts\nfor\nimbalanced data.\nThe networks": "struggles to learn discriminant representations. In contrast, in"
        },
        {
          "recall, which accounts\nfor\nimbalanced data.\nThe networks": ""
        },
        {
          "recall, which accounts\nfor\nimbalanced data.\nThe networks": "the features learned by the H2 model clusters start to become"
        },
        {
          "recall, which accounts\nfor\nimbalanced data.\nThe networks": ""
        },
        {
          "recall, which accounts\nfor\nimbalanced data.\nThe networks": "discernible, although there is room for further improvement."
        },
        {
          "recall, which accounts\nfor\nimbalanced data.\nThe networks": ""
        },
        {
          "recall, which accounts\nfor\nimbalanced data.\nThe networks": ""
        },
        {
          "recall, which accounts\nfor\nimbalanced data.\nThe networks": "4.4. Ablation studies"
        },
        {
          "recall, which accounts\nfor\nimbalanced data.\nThe networks": ""
        },
        {
          "recall, which accounts\nfor\nimbalanced data.\nThe networks": "In this section, we study the impact of each added component"
        },
        {
          "recall, which accounts\nfor\nimbalanced data.\nThe networks": "of the proposed network H2 and we report the results in Tab. 2"
        },
        {
          "recall, which accounts\nfor\nimbalanced data.\nThe networks": "for arousal and in Tab. 3 for valence. In the first line of both ta-"
        },
        {
          "recall, which accounts\nfor\nimbalanced data.\nThe networks": "bles, we show the results obtained with HyperFuseNet for ref-"
        },
        {
          "recall, which accounts\nfor\nimbalanced data.\nThe networks": "erence. We investigate first the impact of reducing the number"
        },
        {
          "recall, which accounts\nfor\nimbalanced data.\nThe networks": "of layers of the encoders and fusion module, as described in"
        },
        {
          "recall, which accounts\nfor\nimbalanced data.\nThe networks": "Section 3. This simple tweak already improves performance"
        },
        {
          "recall, which accounts\nfor\nimbalanced data.\nThe networks": "resulting in slightly less overfitting. Then, we substitute fully"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "5. CONCLUSION": "",
          "[10] E. Lopez, E. Chiarantano, E. Grassucci, and D. Comminiello,": "“Hypercomplex multimodal\nemotion\nrecognition\nfrom eeg"
        },
        {
          "5. CONCLUSION": "",
          "[10] E. Lopez, E. Chiarantano, E. Grassucci, and D. Comminiello,": "and peripheral physiological\nsignals,” in IEEE Int. Conf. on"
        },
        {
          "5. CONCLUSION": "In this work, we proposed a multimodal hierarchical\nfully-",
          "[10] E. Lopez, E. Chiarantano, E. Grassucci, and D. Comminiello,": ""
        },
        {
          "5. CONCLUSION": "",
          "[10] E. Lopez, E. Chiarantano, E. Grassucci, and D. Comminiello,": "Acoust., Speech and Signal Process. Workshops (ICASSPW)."
        },
        {
          "5. CONCLUSION": "hypercomplex model for emotion recognition from EEG and",
          "[10] E. Lopez, E. Chiarantano, E. Grassucci, and D. Comminiello,": ""
        },
        {
          "5. CONCLUSION": "",
          "[10] E. Lopez, E. Chiarantano, E. Grassucci, and D. Comminiello,": "IEEE, 2023, pp. 1–5."
        },
        {
          "5. CONCLUSION": "peripheral physiological\nsignals.\nSpecifically,\nthe network",
          "[10] E. Lopez, E. Chiarantano, E. Grassucci, and D. Comminiello,": ""
        },
        {
          "5. CONCLUSION": "",
          "[10] E. Lopez, E. Chiarantano, E. Grassucci, and D. Comminiello,": "[11] E. Grassucci, A. Zhang,\nand D. Comminiello,\n“PHNNs:"
        },
        {
          "5. CONCLUSION": "is equipped with a hierarchical\nlearning structure, where en-",
          "[10] E. Lopez, E. Chiarantano, E. Grassucci, and D. Comminiello,": ""
        },
        {
          "5. CONCLUSION": "",
          "[10] E. Lopez, E. Chiarantano, E. Grassucci, and D. Comminiello,": "Lightweight neural networks via parameterized hypercomplex"
        },
        {
          "5. CONCLUSION": "coders learn intra-modal correlations,\ni.e.,\nthey exploit both",
          "[10] E. Lopez, E. Chiarantano, E. Grassucci, and D. Comminiello,": ""
        },
        {
          "5. CONCLUSION": "",
          "[10] E. Lopez, E. Chiarantano, E. Grassucci, and D. Comminiello,": "convolutions,” IEEE Trans. on Neural Netw. and Learn. Syst.,"
        },
        {
          "5. CONCLUSION": "intra-channel and inter-channel relations of each signal, and",
          "[10] E. Lopez, E. Chiarantano, E. Grassucci, and D. Comminiello,": ""
        },
        {
          "5. CONCLUSION": "",
          "[10] E. Lopez, E. Chiarantano, E. Grassucci, and D. Comminiello,": "2022."
        },
        {
          "5. CONCLUSION": "the hypercomplex fusion module learns inter-modal relations.",
          "[10] E. Lopez, E. Chiarantano, E. Grassucci, and D. Comminiello,": ""
        },
        {
          "5. CONCLUSION": "",
          "[10] E. Lopez, E. Chiarantano, E. Grassucci, and D. Comminiello,": "[12] A. Zhang, Y. Tay, S. Zhang, A. Chan, A. T. Luu, S. C. Hui, and"
        },
        {
          "5. CONCLUSION": "Our main contribution is the introduction of hypercomplex al-",
          "[10] E. Lopez, E. Chiarantano, E. Grassucci, and D. Comminiello,": ""
        },
        {
          "5. CONCLUSION": "",
          "[10] E. Lopez, E. Chiarantano, E. Grassucci, and D. Comminiello,": "J. Fu, “Beyond fully-connected layers with quaternions: Pa-"
        },
        {
          "5. CONCLUSION": "gebra also at\nthe encoder\nlevel with PHC operations, which",
          "[10] E. Lopez, E. Chiarantano, E. Grassucci, and D. Comminiello,": "rameterization of hypercomplex multiplications with 1/n pa-"
        },
        {
          "5. CONCLUSION": "results in a hierarchical structure. We found that\nthis sim-",
          "[10] E. Lopez, E. Chiarantano, E. Grassucci, and D. Comminiello,": "rameters,” Int. Conf. on Mach. Learn. (ICML), 2021."
        },
        {
          "5. CONCLUSION": "ple step allows to achieve much better\nresults than previous",
          "[10] E. Lopez, E. Chiarantano, E. Grassucci, and D. Comminiello,": "[13] E. Lopez, E. Grassucci, D. Capriotti, and D. Comminiello, “To-"
        },
        {
          "5. CONCLUSION": "state-of-the-art methods. Certainly, while the hypercomplex",
          "[10] E. Lopez, E. Chiarantano, E. Grassucci, and D. Comminiello,": "Int. Joint\nwards explaining hypercomplex neural networks,”"
        },
        {
          "5. CONCLUSION": "",
          "[10] E. Lopez, E. Chiarantano, E. Grassucci, and D. Comminiello,": "Conf. on Neural Netw. (IJCNN), pp. 1–8, 2024."
        },
        {
          "5. CONCLUSION": "fusion module leverages inter-modal relations, the hypercom-",
          "[10] E. Lopez, E. Chiarantano, E. Grassucci, and D. Comminiello,": ""
        },
        {
          "5. CONCLUSION": "plex encoders exploit\nintra-signal correlations which results",
          "[10] E. Lopez, E. Chiarantano, E. Grassucci, and D. Comminiello,": "[14] E.\nLopez,\nF. Betello,\nF. Carmignani,\nE. Grassucci,\nand"
        },
        {
          "5. CONCLUSION": "",
          "[10] E. Lopez, E. Chiarantano, E. Grassucci, and D. Comminiello,": "D. Comminiello, “Attention-map augmentation for hypercom-"
        },
        {
          "5. CONCLUSION": "in enriched latent representations and a final improvement of",
          "[10] E. Lopez, E. Chiarantano, E. Grassucci, and D. Comminiello,": ""
        },
        {
          "5. CONCLUSION": "",
          "[10] E. Lopez, E. Chiarantano, E. Grassucci, and D. Comminiello,": "plex breast cancer classification,” Pattern Recognit. Letters,"
        },
        {
          "5. CONCLUSION": "40.20% and 57.11% for the F1-score on arousal and valence",
          "[10] E. Lopez, E. Chiarantano, E. Grassucci, and D. Comminiello,": ""
        },
        {
          "5. CONCLUSION": "",
          "[10] E. Lopez, E. Chiarantano, E. Grassucci, and D. Comminiello,": "vol. 182, pp. 140–146, 2024."
        },
        {
          "5. CONCLUSION": "classification.",
          "[10] E. Lopez, E. Chiarantano, E. Grassucci, and D. Comminiello,": ""
        },
        {
          "5. CONCLUSION": "",
          "[10] E. Lopez, E. Chiarantano, E. Grassucci, and D. Comminiello,": "[15] E. Lopez, E. Grassucci, M. Valleriani, and D. Comminiello,"
        },
        {
          "5. CONCLUSION": "",
          "[10] E. Lopez, E. Chiarantano, E. Grassucci, and D. Comminiello,": "“Multi-view hypercomplex learning for breast cancer screen-"
        },
        {
          "5. CONCLUSION": "6. REFERENCES",
          "[10] E. Lopez, E. Chiarantano, E. Grassucci, and D. Comminiello,": "ing,” ArXiv preprint arXiv:2204.05798, 2022."
        },
        {
          "5. CONCLUSION": "",
          "[10] E. Lopez, E. Chiarantano, E. Grassucci, and D. Comminiello,": "[16] L. Abdel-Hamid, “An efficient machine learning-based emo-"
        },
        {
          "5. CONCLUSION": "[1] C. Li, B. Chen, Z. Zhao, N. Cummins, and B. W. Schuller,",
          "[10] E. Lopez, E. Chiarantano, E. Grassucci, and D. Comminiello,": ""
        },
        {
          "5. CONCLUSION": "",
          "[10] E. Lopez, E. Chiarantano, E. Grassucci, and D. Comminiello,": "tional valence recognition approach towards wearable EEG,”"
        },
        {
          "5. CONCLUSION": "“Hierarchical attention-based temporal convolutional networks",
          "[10] E. Lopez, E. Chiarantano, E. Grassucci, and D. Comminiello,": ""
        },
        {
          "5. CONCLUSION": "",
          "[10] E. Lopez, E. Chiarantano, E. Grassucci, and D. Comminiello,": "Sensors, vol. 23, no. 3, p. 1255, 2023."
        },
        {
          "5. CONCLUSION": "for EEG-based emotion recognition,”\nin IEEE Int. Conf. on",
          "[10] E. Lopez, E. Chiarantano, E. Grassucci, and D. Comminiello,": ""
        },
        {
          "5. CONCLUSION": "",
          "[10] E. Lopez, E. Chiarantano, E. Grassucci, and D. Comminiello,": "[17] Y.-J. Liu, M. Yu, G. Zhao, J. Song, Y. Ge, and Y. Shi, “Real-"
        },
        {
          "5. CONCLUSION": "Acoust.,\nSpeech\nand\nSignal Process.\n(ICASSP),\n2021,\npp.",
          "[10] E. Lopez, E. Chiarantano, E. Grassucci, and D. Comminiello,": ""
        },
        {
          "5. CONCLUSION": "",
          "[10] E. Lopez, E. Chiarantano, E. Grassucci, and D. Comminiello,": "time movie-induced discrete emotion recognition from EEG"
        },
        {
          "5. CONCLUSION": "1240–1244.",
          "[10] E. Lopez, E. Chiarantano, E. Grassucci, and D. Comminiello,": ""
        },
        {
          "5. CONCLUSION": "",
          "[10] E. Lopez, E. Chiarantano, E. Grassucci, and D. Comminiello,": "signals,” IEEE Trans. on Affect. Comput., vol. 9, no. 4, pp."
        },
        {
          "5. CONCLUSION": "[2] M. A. Hasnul et al., “Electrocardiogram-based emotion recog-",
          "[10] E. Lopez, E. Chiarantano, E. Grassucci, and D. Comminiello,": "550–562, 2017."
        },
        {
          "5. CONCLUSION": "nition systems and their applications in healthcare—a review,”",
          "[10] E. Lopez, E. Chiarantano, E. Grassucci, and D. Comminiello,": ""
        },
        {
          "5. CONCLUSION": "",
          "[10] E. Lopez, E. Chiarantano, E. Grassucci, and D. Comminiello,": "[18] X. Du, C. Ma, G. Zhang, J. Li, Y.-K. Lai, G. Zhao, X. Deng, Y.-"
        },
        {
          "5. CONCLUSION": "Sensors, vol. 21, no. 15, p. 5015, 2021.",
          "[10] E. Lopez, E. Chiarantano, E. Grassucci, and D. Comminiello,": ""
        },
        {
          "5. CONCLUSION": "",
          "[10] E. Lopez, E. Chiarantano, E. Grassucci, and D. Comminiello,": "J. Liu, and H. Wang, “An efficient LSTM network for emotion"
        },
        {
          "5. CONCLUSION": "",
          "[10] E. Lopez, E. Chiarantano, E. Grassucci, and D. Comminiello,": "recognition from multichannel EEG signals,” IEEE Trans. on"
        },
        {
          "5. CONCLUSION": "[3] M. Soleymani, J. Lichtenauer, T. Pun, and M. Pantic, “A mul-",
          "[10] E. Lopez, E. Chiarantano, E. Grassucci, and D. Comminiello,": ""
        },
        {
          "5. CONCLUSION": "",
          "[10] E. Lopez, E. Chiarantano, E. Grassucci, and D. Comminiello,": "Affect. Comput., vol. 13, no. 3, pp. 1528–1540, 2022."
        },
        {
          "5. CONCLUSION": "timodal database for affect recognition and implicit\ntagging,”",
          "[10] E. Lopez, E. Chiarantano, E. Grassucci, and D. Comminiello,": ""
        },
        {
          "5. CONCLUSION": "IEEE Trans. on Affect. Comput., vol. 3, no. 1, pp. 42–55, 2011.",
          "[10] E. Lopez, E. Chiarantano, E. Grassucci, and D. Comminiello,": "[19] Y. Zhang, C. Cheng,\nand Y. Zhang,\n“Multimodal\nemotion"
        },
        {
          "5. CONCLUSION": "",
          "[10] E. Lopez, E. Chiarantano, E. Grassucci, and D. Comminiello,": "recognition based on manifold learning and convolution neu-"
        },
        {
          "5. CONCLUSION": "[4] C.-L. Lee, W. Pei, Y.-C. Lin, A. Granmo, and K.-H. Liu, “Emo-",
          "[10] E. Lopez, E. Chiarantano, E. Grassucci, and D. Comminiello,": ""
        },
        {
          "5. CONCLUSION": "",
          "[10] E. Lopez, E. Chiarantano, E. Grassucci, and D. Comminiello,": "ral network,” Multimedia Tools and Appl., vol. 81, no. 23, pp."
        },
        {
          "5. CONCLUSION": "tion detection based on pupil variation,” in Healthcare, vol. 11,",
          "[10] E. Lopez, E. Chiarantano, E. Grassucci, and D. Comminiello,": ""
        },
        {
          "5. CONCLUSION": "",
          "[10] E. Lopez, E. Chiarantano, E. Grassucci, and D. Comminiello,": "33 253–33 268, 2022."
        },
        {
          "5. CONCLUSION": "no. 3, 2023, p. 322.",
          "[10] E. Lopez, E. Chiarantano, E. Grassucci, and D. Comminiello,": ""
        },
        {
          "5. CONCLUSION": "",
          "[10] E. Lopez, E. Chiarantano, E. Grassucci, and D. Comminiello,": "[20] X. Gong,\nC.\nP. Chen,\nB. Hu,\nand T. Zhang,\n“CiABL:"
        },
        {
          "5. CONCLUSION": "[5] Z. Fu, B. Zhang, X. He, Y. Li, H. Wang, and J. Huang, “Emo-",
          "[10] E. Lopez, E. Chiarantano, E. Grassucci, and D. Comminiello,": ""
        },
        {
          "5. CONCLUSION": "",
          "[10] E. Lopez, E. Chiarantano, E. Grassucci, and D. Comminiello,": "Completeness-induced\nadaptative\nbroad\nlearning\nfor\ncross-"
        },
        {
          "5. CONCLUSION": "tion recognition based on multi-modal physiological\nsignals",
          "[10] E. Lopez, E. Chiarantano, E. Grassucci, and D. Comminiello,": ""
        },
        {
          "5. CONCLUSION": "",
          "[10] E. Lopez, E. Chiarantano, E. Grassucci, and D. Comminiello,": "subject emotion recognition with EEG and eye movement sig-"
        },
        {
          "5. CONCLUSION": "and transfer\nlearning,” Frontiers in Neuroscience, vol. 16, p.",
          "[10] E. Lopez, E. Chiarantano, E. Grassucci, and D. Comminiello,": ""
        },
        {
          "5. CONCLUSION": "",
          "[10] E. Lopez, E. Chiarantano, E. Grassucci, and D. Comminiello,": "nals,” IEEE Trans. on Affect. Comput., 2024."
        },
        {
          "5. CONCLUSION": "1000716, 2022.",
          "[10] E. Lopez, E. Chiarantano, E. Grassucci, and D. Comminiello,": ""
        },
        {
          "5. CONCLUSION": "",
          "[10] E. Lopez, E. Chiarantano, E. Grassucci, and D. Comminiello,": "[21] D. Comminiello, E. Grassucci, D. P. Mandic, and A. Uncini,"
        },
        {
          "5. CONCLUSION": "[6] C. Busso,\nZ. Deng,\nS. Yildirim, M. Bulut,\nC. M. Lee,",
          "[10] E. Lopez, E. Chiarantano, E. Grassucci, and D. Comminiello,": "“Demystifying the hypercomplex:\nInductive biases in hyper-"
        },
        {
          "5. CONCLUSION": "A. Kazemzadeh, S. Lee, U. Neumann,\nand S. Narayanan,",
          "[10] E. Lopez, E. Chiarantano, E. Grassucci, and D. Comminiello,": "complex deep learning,” IEEE Signal Process. Mag., 2024."
        },
        {
          "5. CONCLUSION": "“Analysis\nof\nemotion\nrecognition\nusing\nfacial\nexpressions,",
          "[10] E. Lopez, E. Chiarantano, E. Grassucci, and D. Comminiello,": ""
        },
        {
          "5. CONCLUSION": "",
          "[10] E. Lopez, E. Chiarantano, E. Grassucci, and D. Comminiello,": "[22] A. Topic, M. Russo, M. Stella, and M. Saric, “Emotion recog-"
        },
        {
          "5. CONCLUSION": "speech and multimodal\ninformation,”\nin ACM Int. Conf. on",
          "[10] E. Lopez, E. Chiarantano, E. Grassucci, and D. Comminiello,": ""
        },
        {
          "5. CONCLUSION": "",
          "[10] E. Lopez, E. Chiarantano, E. Grassucci, and D. Comminiello,": "nition using a reduced set of EEG channels based on holo-"
        },
        {
          "5. CONCLUSION": "Multimodal Interfaces (ICMI), 2004, pp. 205–211.",
          "[10] E. Lopez, E. Chiarantano, E. Grassucci, and D. Comminiello,": ""
        },
        {
          "5. CONCLUSION": "",
          "[10] E. Lopez, E. Chiarantano, E. Grassucci, and D. Comminiello,": "graphic feature maps,” Sensors, vol. 22, no. 9, p. 3248, 2022."
        },
        {
          "5. CONCLUSION": "[7] D. Li, L. Xie, Z. Wang, and H. Yang, “Brain emotion percep-",
          "[10] E. Lopez, E. Chiarantano, E. Grassucci, and D. Comminiello,": "[23]\nJ. R. Msonda, Z. He, and C. Lu, “Feature reconstruction based"
        },
        {
          "5. CONCLUSION": "tion inspired EEG emotion recognition with deep reinforce-",
          "[10] E. Lopez, E. Chiarantano, E. Grassucci, and D. Comminiello,": "channel selection for emotion recognition using EEG,” in IEEE"
        },
        {
          "5. CONCLUSION": "ment learning,” IEEE Trans. on Neural Netw. and Learn. Syst.,",
          "[10] E. Lopez, E. Chiarantano, E. Grassucci, and D. Comminiello,": "Signal Process. in Med. and Biol. Symp. (SPMB), 2021, pp. 1–"
        },
        {
          "5. CONCLUSION": "2023.",
          "[10] E. Lopez, E. Chiarantano, E. Grassucci, and D. Comminiello,": "7."
        },
        {
          "5. CONCLUSION": "[8] Y. Zhang, Y. Zhang, and S. Wang, “An attention-based hybrid",
          "[10] E. Lopez, E. Chiarantano, E. Grassucci, and D. Comminiello,": "[24] T. C. Dolmans, M. Poel, J.-W. J. van’t Klooster, and B. P. Veld-"
        },
        {
          "5. CONCLUSION": "deep learning model\nfor EEG emotion recognition,” Signal,",
          "[10] E. Lopez, E. Chiarantano, E. Grassucci, and D. Comminiello,": "kamp, “Perceived mental workload classification using inter-"
        },
        {
          "5. CONCLUSION": "Image and Video Process., vol. 17, no. 5, pp. 2305–2313, 2023.",
          "[10] E. Lopez, E. Chiarantano, E. Grassucci, and D. Comminiello,": "mediate fusion multimodal deep learning,” Frontiers in Human"
        },
        {
          "5. CONCLUSION": "",
          "[10] E. Lopez, E. Chiarantano, E. Grassucci, and D. Comminiello,": "Neuroscience, vol. 14, p. 609096, 2021."
        },
        {
          "5. CONCLUSION": "[9] M. R.\nIslam et al.,\n“Emotion recognition from EEG signal",
          "[10] E. Lopez, E. Chiarantano, E. Grassucci, and D. Comminiello,": ""
        },
        {
          "5. CONCLUSION": "",
          "[10] E. Lopez, E. Chiarantano, E. Grassucci, and D. Comminiello,": "[25] L. Van der Maaten and G. Hinton, “Visualizing data using t-"
        },
        {
          "5. CONCLUSION": "focusing on deep learning and shallow learning techniques,”",
          "[10] E. Lopez, E. Chiarantano, E. Grassucci, and D. Comminiello,": ""
        },
        {
          "5. CONCLUSION": "",
          "[10] E. Lopez, E. Chiarantano, E. Grassucci, and D. Comminiello,": "SNE.” Journal of Mach. Learn. Res., vol. 9, no. 11, 2008."
        },
        {
          "5. CONCLUSION": "IEEE Access, vol. 9, pp. 94 601–94 624, 2021.",
          "[10] E. Lopez, E. Chiarantano, E. Grassucci, and D. Comminiello,": ""
        }
      ],
      "page": 6
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Hierarchical attention-based temporal convolutional networks for EEG-based emotion recognition",
      "authors": [
        "C Li",
        "B Chen",
        "Z Zhao",
        "N Cummins",
        "B Schuller"
      ],
      "year": "2021",
      "venue": "IEEE Int. Conf. on Acoust., Speech and Signal Process"
    },
    {
      "citation_id": "3",
      "title": "Electrocardiogram-based emotion recognition systems and their applications in healthcare-a review",
      "authors": [
        "M Hasnul"
      ],
      "year": "2021",
      "venue": "Sensors"
    },
    {
      "citation_id": "4",
      "title": "A multimodal database for affect recognition and implicit tagging",
      "authors": [
        "M Soleymani",
        "J Lichtenauer",
        "T Pun",
        "M Pantic"
      ],
      "year": "2011",
      "venue": "IEEE Trans. on Affect. Comput"
    },
    {
      "citation_id": "5",
      "title": "Emotion detection based on pupil variation",
      "authors": [
        "C.-L Lee",
        "W Pei",
        "Y.-C Lin",
        "A Granmo",
        "K.-H Liu"
      ],
      "year": "2023",
      "venue": "Healthcare"
    },
    {
      "citation_id": "6",
      "title": "Emotion recognition based on multi-modal physiological signals and transfer learning",
      "authors": [
        "Z Fu",
        "B Zhang",
        "X He",
        "Y Li",
        "H Wang",
        "J Huang"
      ],
      "year": "2022",
      "venue": "Frontiers in Neuroscience"
    },
    {
      "citation_id": "7",
      "title": "Analysis of emotion recognition using facial expressions, speech and multimodal information",
      "authors": [
        "C Busso",
        "Z Deng",
        "S Yildirim",
        "M Bulut",
        "C Lee",
        "A Kazemzadeh",
        "S Lee",
        "U Neumann",
        "S Narayanan"
      ],
      "year": "2004",
      "venue": "ACM Int. Conf. on Multimodal Interfaces (ICMI)"
    },
    {
      "citation_id": "8",
      "title": "Brain emotion perception inspired EEG emotion recognition with deep reinforcement learning",
      "authors": [
        "D Li",
        "L Xie",
        "Z Wang",
        "H Yang"
      ],
      "year": "2023",
      "venue": "IEEE Trans. on Neural Netw. and Learn. Syst"
    },
    {
      "citation_id": "9",
      "title": "An attention-based hybrid deep learning model for EEG emotion recognition",
      "authors": [
        "Y Zhang",
        "Y Zhang",
        "S Wang"
      ],
      "year": "2023",
      "venue": "Signal, Image and Video Process"
    },
    {
      "citation_id": "10",
      "title": "Emotion recognition from EEG signal focusing on deep learning and shallow learning techniques",
      "authors": [
        "M Islam"
      ],
      "year": "2021",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "11",
      "title": "Hypercomplex multimodal emotion recognition from eeg and peripheral physiological signals",
      "authors": [
        "E Lopez",
        "E Chiarantano",
        "E Grassucci",
        "D Comminiello"
      ],
      "year": "2023",
      "venue": "IEEE Int. Conf. on Acoust., Speech and Signal Process. Workshops (ICASSPW)"
    },
    {
      "citation_id": "12",
      "title": "PHNNs: Lightweight neural networks via parameterized hypercomplex convolutions",
      "authors": [
        "E Grassucci",
        "A Zhang",
        "D Comminiello"
      ],
      "year": "2022",
      "venue": "IEEE Trans. on Neural Netw. and Learn. Syst"
    },
    {
      "citation_id": "13",
      "title": "Beyond fully-connected layers with quaternions: Parameterization of hypercomplex multiplications with 1/n parameters",
      "authors": [
        "A Zhang",
        "Y Tay",
        "S Zhang",
        "A Chan",
        "A Luu",
        "S Hui",
        "J Fu"
      ],
      "venue": "Int. Conf. on Mach. Learn. (ICML)"
    },
    {
      "citation_id": "14",
      "title": "Towards explaining hypercomplex neural networks",
      "authors": [
        "E Lopez",
        "E Grassucci",
        "D Capriotti",
        "D Comminiello"
      ],
      "year": "2024",
      "venue": "Int. Joint Conf. on Neural Netw. (IJCNN)"
    },
    {
      "citation_id": "15",
      "title": "Attention-map augmentation for hypercomplex breast cancer classification",
      "authors": [
        "E Lopez",
        "F Betello",
        "F Carmignani",
        "E Grassucci",
        "D Comminiello"
      ],
      "year": "2024",
      "venue": "Pattern Recognit. Letters"
    },
    {
      "citation_id": "16",
      "title": "Multi-view hypercomplex learning for breast cancer screening",
      "authors": [
        "E Lopez",
        "E Grassucci",
        "M Valleriani",
        "D Comminiello"
      ],
      "year": "2022",
      "venue": "Multi-view hypercomplex learning for breast cancer screening",
      "arxiv": "arXiv:2204.05798"
    },
    {
      "citation_id": "17",
      "title": "An efficient machine learning-based emotional valence recognition approach towards wearable EEG",
      "authors": [
        "L Abdel-Hamid"
      ],
      "year": "2023",
      "venue": "Sensors"
    },
    {
      "citation_id": "18",
      "title": "Realtime movie-induced discrete emotion recognition from EEG signals",
      "authors": [
        "Y.-J Liu",
        "M Yu",
        "G Zhao",
        "J Song",
        "Y Ge",
        "Y Shi"
      ],
      "year": "2017",
      "venue": "IEEE Trans. on Affect. Comput"
    },
    {
      "citation_id": "19",
      "title": "An efficient LSTM network for emotion recognition from multichannel EEG signals",
      "authors": [
        "X Du",
        "C Ma",
        "G Zhang",
        "J Li",
        "Y.-K Lai",
        "G Zhao",
        "X Deng",
        "Y.-J Liu",
        "H Wang"
      ],
      "year": "2022",
      "venue": "IEEE Trans. on Affect. Comput"
    },
    {
      "citation_id": "20",
      "title": "Multimodal emotion recognition based on manifold learning and convolution neural network",
      "authors": [
        "Y Zhang",
        "C Cheng",
        "Y Zhang"
      ],
      "year": "2022",
      "venue": "Multimedia Tools and Appl"
    },
    {
      "citation_id": "21",
      "title": "CiABL: Completeness-induced adaptative broad learning for crosssubject emotion recognition with EEG and eye movement signals",
      "authors": [
        "X Gong",
        "C Chen",
        "B Hu",
        "T Zhang"
      ],
      "year": "2024",
      "venue": "IEEE Trans. on Affect. Comput"
    },
    {
      "citation_id": "22",
      "title": "Demystifying the hypercomplex: Inductive biases in hypercomplex deep learning",
      "authors": [
        "D Comminiello",
        "E Grassucci",
        "D Mandic",
        "A Uncini"
      ],
      "year": "2024",
      "venue": "IEEE Signal Process. Mag"
    },
    {
      "citation_id": "23",
      "title": "Emotion recognition using a reduced set of EEG channels based on holographic feature maps",
      "authors": [
        "A Topic",
        "M Russo",
        "M Stella",
        "M Saric"
      ],
      "year": "2022",
      "venue": "Sensors"
    },
    {
      "citation_id": "24",
      "title": "Feature reconstruction based channel selection for emotion recognition using EEG",
      "authors": [
        "J Msonda",
        "Z He",
        "C Lu"
      ],
      "year": "2021",
      "venue": "IEEE Signal Process. in Med. and Biol. Symp. (SPMB)"
    },
    {
      "citation_id": "25",
      "title": "Perceived mental workload classification using intermediate fusion multimodal deep learning",
      "authors": [
        "T Dolmans",
        "M Poel",
        "J.-W Van't Klooster",
        "B Veldkamp"
      ],
      "year": "2021",
      "venue": "Frontiers in Human Neuroscience"
    },
    {
      "citation_id": "26",
      "title": "Visualizing data using t-SNE",
      "authors": [
        "L Van Der Maaten",
        "G Hinton"
      ],
      "year": "2008",
      "venue": "Journal of Mach. Learn. Res"
    }
  ]
}