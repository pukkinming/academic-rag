{
  "paper_id": "2408.04123v1",
  "title": "Knowledge-Based Emotion Recognition Using Large Language Models",
  "published": "2024-08-07T23:18:16Z",
  "authors": [
    "Bin Han",
    "Cleo Yau",
    "Su Lei",
    "Jonathan Gratch"
  ],
  "keywords": [
    "facial emotion recognition",
    "bayesian cue integration",
    "large language models"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Emotion recognition in social situations is a complex task that requires integrating information from both facial expressions and the situational context. While traditional approaches to automatic emotion recognition have focused on decontextualized signals, recent research emphasizes the importance of context in shaping emotion perceptions. This paper contributes to the emerging field of context-based emotion recognition by leveraging psychological theories of human emotion perception to inform the design of automated methods. We propose an approach that combines emotion recognition methods with Bayesian Cue Integration (BCI) to integrate emotion inferences from decontextualized facial expressions and contextual knowledge inferred via Large-language Models. We test this approach in the context of interpreting facial expressions during a social task, the prisoner's dilemma. Our results provide clear support for BCI across a range of automatic emotion recognition methods. The best automated method achieved results comparable to human observers, suggesting the potential for this approach to advance the field of affective computing.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "People readily make inferences about others from emotions expressed in social situations and use these inferences to guide social actions. Yet the field of affective computing has struggled to endow machines with this basic level of emotional intelligence. Recent research has highlighted the importance of situational knowledge in shaping emotion perceptions  [1] . Whereas automatic emotion recognition has traditionally focused on recognizing emotions from decontextualized signals (e.g., facial expressions labeled without knowledge of the situation that evoked the expression  [2] ,  [3] ), research on human social cognition highlights that people integrate cues from both expressions and their rich understanding of the social situation  [4] . Yet \"context-based\" emotion recognition is still in its infancy. This paper contributes to this growing field by demonstrating how psychological theories of human emotion perception can inform the design of automated methods.\n\nThere are obvious benefits if emotion perceptions could be predicted from decontextualized signals alone, as was claimed possible by early psychological theories  [5] ,  [6] . Expressions could be easily collected and annotated without regard for context and the resulting algorithms utilized in any domain. Unfortunately, it is now clear that impressions formed from de-contextualized expressions often have little bearing on predicting what people feel, nor can they predict the inferences of observers knowledgeable of the social situation  [7] .\n\nAs a consequence, knowledge of the situational context must be incorporated into the recognition process. One obvious approach is to train recognition methods for specific contexts (e.g., emotional states of a driver  [8]  or patient in a mental health screening  [9] ), but this limits the generality of the resulting algorithm to these specific contexts. As an alternative, recent computer vision approaches have tried to infer the context by examining information in the background of an image or video (e.g., recognizing that a particular expression was produced in the context of a birthday party  [10] ). Unfortunately, the information that can be inferred in this way is often quite limited. In contrast, people engaged in social interactions often have rich semantic knowledge about the nature of the shared task including each party's recent actions.\n\nTwo recent innovations suggest how to incorporate situational knowledge while maintaining the advantages of decontextualized emotion recognition. First, research indicates people infer emotion from expressions using context-free methods and adjust based on situational knowledge. For example, Ong and colleagues' Bayesian Cue Integration (BCI) model shows that context-specific emotion judgments can be decomposed into judgments based on the expression alone and from the situation alone, then integrated via Bayesian inference  [11]  (see  [12]  for a similar approach). This implies that affective computing could similarly decompose the problem: i.e., utilize existing context-free emotion recognition methods to recognize emotions from expressions alone and then \"post-process\" the output of these algorithms assuming it was possible to make emotional inferences about situations.\n\nSecond, research into the emotional reasoning capabilities of Large Language Models (LLMs) suggests they are surprisingly accurate at predicting the emotions people feel across a wide range of situations. For example, Tak and Gratch found that GPT models accurately predict human emotional responses and appraisals across a wide range of situations  [13] , and Broekens demonstrated GPT's zero-shot abilities in tasks such as sentiment analysis and appraisal-based emotion elicitation  [14] . Additionally, Resendiz and Klinger's work on emotion-conditioned text generation further highlights LLMs' capabilities in handling emotion-related tasks  [15] .\n\nTogether, this suggests a general approach to contextdependent emotion recognition: First, predict the emotions people are likely to perceive from an emotional expression without context. Second, predict the emotions people are likely to perceive from a situational description. Finally, combine these separate sources of information with psychologicallyinspired models such as BCI (see Fig.  1 ).\n\nWe test this idea by examining how observers interpret facial expressions produced during an emotional social task (playing the prisoner's dilemma game for money). We first replicate prior findings that human observers need context (emotion ratings from decontextualized videos differ considerably from ratings when fine-grained details of the context are provided). We next systematically explore the utility of BCI for fullyautomated methods. Specifically, we apply the Bayesian approach to several context-free emotion recognition algorithms to assess the generality of the approach. We further investigate alternative LLMs for their ability to reason about emotional situations. Finally, we contrast BCI with alternative integration methods. To preview, our results provide clear support for BCI. The method improved accuracy across all of the contextfree methods we tested. Second, GPT-4 was found to be an effective method for predicting emotions from situations. The best-performing results achieved human-level performance (as judged by comparing it with the predictions of BCI using human, rather than machine judgments), We discuss these results and future directions, including the need to verify these findings on a broader range of situations.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Bayesian Cue Integration",
      "text": "We first introduce BCI  [11]  and illustrate how it captures human judgments in the Prisoner's Dilemma task before turning to automated approaches. BCI predicts context-based emotion judgments (i.e., judgments by human observers with extensive knowledge of the social context) from context-free judgments (i.e., judgments by human observers without any knowledge of the context) and context-only judgments (i.e., human judgments based on information about the context but without knowledge of the expression. Each of these judgments is represented as a probability distribution (i.e., the probability that a given human observer would make this judgment). The model assumes observers employ an intuitive theory for interpreting expressive and contextual cues: observers assume that the outcome of a social task (e.g., the joint decision in the prisoner's dilemma), influences emotions, which in turn affects facial expressions. The equation below captures this assumption (see  [11]  for details on its derivation), P (e|c, f ) ∝ P (e|f )P (e|c)\n\nIn Eq (1), P (e|f ) is the probability of reporting someone feels an emotion from the face alone. P (e|c) is the probability that someone would be rated as experiencing an emotion given only the situation (e.g., without seeing a person's face, how likely is a person to experience joy if we know they were just exploited). P (e) is the a priori probability that different emotions tend to occur. BCI predicts observers' beliefs about what someone feels, not the actual feelings of the person producing the expression. Thus, it is most applicable to predicting how the observer will behave in social settings  [16] ,  [17] , though prior psychological research also suggests that context-based perceptions are more consistent with self-reported feelings than contextfree perceptions  [18] . As BCI predictions are expressed as a probability distribution over labels, rather than a specific class, BCI is aligned with recent innovations in affective computing that leverage annotator variability as crucial information for improving recognition accuracy  [19] ,  [20] .",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "A. Archival Data",
      "text": "We first replicate the utility of BCI on a novel dataset, USC's Split-Steal corpus  [21] ,  [22] , before integrating it with automated methods. This is a large collection of participants that engaged in a 10-round prisoner's dilemma task. Participants could see each other but not speak and were incentivized by playing for lottery tickets for several $100 USD lotteries. The corpus consists of 7-second \"reaction shots\" when players learn of their joint choice on a given round. In each round, players can choose to cooperate (C) or defect (D), where cooperating is an attempt to split ten lottery tickets and defect is an attempt to steal all the tickets. The possible outcomes depend on the joint choice: both choose to equally split the tickets (CC), one player successfully steals from their partner (DC), one player is stolen from (CD); or both attempt to steal from each other (DD) with each receiving a single ticket.\n\nFor our replication, we chose 25 of the most expressive videos for each possible outcome in the game as we wanted to focus on how these expressions would alter inferences. The USC Split-Steal corpus includes automatically derived expressivity ratings and we simply selected the 25 most expressive clips for each possible outcome in the game. This resulted in a database of 100 7-second video clips, 25 from each of the four actual game outcomes: CC, DC, CD, and DD.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "B. Emotion Perception Ratings",
      "text": "We augmented this corpus following the procedure of Ong and colleagues  [11] . We recruited multiple annotators to estimate probability distributions that correspond to the probability that an emotion is perceived from the face alone (context-free), from the context alone (context-only) and from the face and context together (context-based). Annotators were recruited through Amazon Mechanical Turk and pre-tested to filter inattentive annotators. For each video, annotators were asked to identify one of 6 basic emotion (or neutral) they perceived the person to be feeling. Basic emotions were chosen to allow direct comparison to prior BCI results and because these are a natural language for rating perceived emotion (though the approach is easily extended to other schemes).\n\nSeparate groups of annotators were recruited to judge the videos with or without context. Within each group, each annotator rated 10 randomly selected videos and twenty ratings were obtained for each video in each context.\n\nContext-free: For P (e|f ), annotators were asked to rate emotions only seeing the video and without being told any other information. The only thing that could be inferred from the background was they are sitting in a room full of computers (see Fig.  2 ). They were simply told to watch the video and answer the questions. They were free to watch the video as many times as possible.\n\nContext-based: For P (e|c, f ), annotators first were provided a description of the game context, including the payoff for different choices, and quizzed to verify their understanding of the structure of the game. They then rated 10 videos, each showing the joint outcome of a round and the nonverbal reactions of the two players involved (see Fig.  2 ) For each video, annotators were instructed to evaluate the emotional reaction of the player (Player A) highlighted in the red box. They were allowed to watch the video multiple times to ensure an accurate assessment of the player's emotional expression.\n\nContext-only: For P (e|c), annotators first received the same description of the game and quiz provided in the context-based annotation task. Rather than seeing a video, they only saw a text description of one of the four possible game outcomes and were asked to predict what a player experiencing this outcome was likely to feel. We recruited 141 annotators from Amazon Mechanical Turk, of which 20 annotations were discarded due to failing the attention check.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "C. Emotion Probability Distribution",
      "text": "BCI adopts a probability distribution approach for emotion recognition, utilizing \"soft\" labels to represent the probability of each emotion, rather than producing a single label (e.g., Anger) from a video. This method addresses the challenges of subjective perception in image emotion recognition, as highlighted in previous research  [23] ,  [24] . Significantly, this approach is supported by research in emotion recognition. For example, Lotfian and Busso  [19]  formulated emotion perception as a probabilistic model, and Prabhu et al.  [20]  advocated for label uncertainty modeling in speech emotion recognition. These studies highlight the importance of using a probabilistic perspective in affective computing to better capture the complexities of emotional expression.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "D. Aggregate Analysis Of Emotion Ratings",
      "text": "Fig.  3  visualizes the elicited emotion probability distributions. For the sake of simplicity, rather than showing the human ratings for each individual video, we average across all the videos with the same game outcome.\n\nFor context-free perceptions (Fig.  3a ), Joy is the predominant emotion across all game outcomes, with the CC very likely to be rated as joyful (71%), DD being perceived as the least joyful (45%), and DC and CD falling in the middle. Despite the prevalence of joy, most individual videos failed to reach strong annotator consensus. Table  I  shows the fraction of videos with a clear majority label (over 50% agree on the label) and clear consensus (over 66% agree on the label).\n\nFig.  3b  illustrates the context-based results, where annotators had knowledge of both facial cues and the game outcome. Joy was still prevalent in the CC condition at 69%, and DC it was at 56%. However, the CD condition was marked by a 33% prevalence of Surprise, and DD by 34% Neutral. These results indicate that emotions in the CD and DD conditions are not overwhelmingly dominated by a single emotion label but rather display a mixed (for instance, CD has 33% Surprise and 32% Joy), supporting the necessity of an emotion distribution approach for more precise emotion recognition. Table  I  illustrates a diminished consensus for outcomes such as CD and DD, underscoring the insufficiency of a single-label approach. These results support that soft labeling strategies to more accurately capture the varied emotional nuances revealed when contextual information is incorporated.\n\nIn the context-only annotations (Fig.  3c ), there was a notable overestimation of Sadness and Anger in the CD condition when compared to context-based perceptions. Again, multiple emotions were again prominent, as in CD, 32% was Sadness and 29% was Anger, which also supports the need for an emotion distribution approach. A more detailed evaluation will be discussed in Section 4.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Iii. Automatic Approaches For Facial And Contextual Emotion Recognition",
      "text": "As illustrated in Fig.  1 , we automate context-based predictions by  (1)     [25] .\n\nA. P (e|f ) -Emotion Probability given by Face\n\nWe compare three alternatives for automatically recognizing emotions from decontextualized videos. This allows us to compare the accuracy of different approaches but also to examine if knowledge-based recognition can benefit a range of methods. We evaluate a commercial approach (FACET) and a state-of-the-art pre-trained model (EAC model). Each of these methods recognizes emotions frame-by-frame and may miss important information encoded in the dynamics of the video. Thus, we also train a dynamic LSTM model that can make predictions based on changes in expressions within a video. In evaluating each method, we use treat Context-free human annotations as ground truth (i.e., how well can each method predict what emotions are perceived by observers without access to the game context?).\n\n1) FACET: FACET is a commercial expression recognition method based on the Computer Expression Recognition Toolbox  [26]  and produces a distribution of emotion labels for each frame of video. Specifically, FACET provides 'evidence values' for each frame, indicating the likelihood of an expression corresponding to a specific emotion, with values ranging from -4 to +4. We set negative values to zero, average the evidence across all video frames and re-scale the result to ensure that the sum of probabilities equals 1.\n\n2) EAC: Erasing Attention Consistency (EAC) model  [27] , is a state-of-the art emotion recognition method based on ResNet. EAC analyzes videos frame-by-frame and we extract and average emotion probabilities from the softmax layer to determine the overall emotional distribution for each video. Specifically, we use a pre-trained EAC model with the RAF-DB dataset (Real-world Affective Faces)  [28] . The extracted probabilities are re-scaled to ensure that their sum equals 1.\n\n3) LSTM: The previous two methods ignore how expressions change across the 7-second video, yet some emotion impressions, such as surprise, might arise from quick facial movements. To capture these, we train an LSTM model that incorporates dynamics features. The model is trained using human context-free annotations as the ground truth and utilizes a range of input features, including sequences of Action Units (AUs), facial optical flows, gaze, and head pose data.\n\nWe employ OpenFace 2.0  [29]  to extract 12 Facial Action Units 1 , focusing on those commonly co-occurring AUs identified by prior work  [30] . For optical flows, the ZFace tool  [31]  is used to track the movement of dense facial landmarks (512 points) over time, allowing us to calculate the flow between each frame. This data helps the LSTM model to capture the dynamic expressions and subtle changes in the face that are crucial for dynamic emotions. Additionally, OpenFace provides gaze direction vectors and gaze angles. Also, head pose direction vectors are provided. To train the model, we pre-process and regularize the input. These include the normalization of input features to reduce potential bias, the incorporation of dropout layers to prevent overfitting, and scaling to accelerate the convergence of the training process. The validation strategy employed is Leave-One-Out Cross-Validation (LOOCV), providing a thorough assessment of the model's performance. Note, that as LSTM is fine tuned on the Split-Steal corpus, whereas the other approaches are pretrained, LSTM can be seen also as an attempt to create an upper-bound on the accuracy of context-free judgments.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "B. P (E|C) -Emotion Probability Given By Context",
      "text": "We compare GPT-3.5, GPT-4 2 , Llama 2, and Gemini 3 LLM models for their ability to infer likely emotions from a textual description of situational context. Models are given the identical descriptions and questions that were provided to the human annotators, with minor changes to get the models to produce standardized outputs (see  Fig 4) . The prompt contains three main components:\n\n• A general description of the prisoner's dilemma game.\n\n• The game outcome of each turn (CC,DC,CD, and DD).\n\n• A request for the emotional distribution (Basic emotion). For evaluation, human context-only annotations are used as ground truth (mentioned in Section 3c). We set the temperature parameter of each LLM model to the default when conducting the experiment. Each version was prompted 20 times for each description, and the results were averaged.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "C. Best Method For P (E|F ) And P (E|C)",
      "text": "We next compare the performance of different models for de-contextualized facial emotion recognition (P (e|f )) and context-based emotion recognition (P (e|c)).\n\nEvaluation Metrics: To assess the performance of our models, we employ three standard metrics. Kullback-Leibler divergence (KLD)  [32]  and Root Mean Square Error (RMSE) are standard metrics to compare the distance between two probability distributions  [23] ,  [33] , which is most appropriate given the variability in labels provided by the human annotators. KLD directly measures the discrepancy between two probability distributions, with lower values indicating 1 The selected AUs are  AU 1, 2, 4, 6, 7, 10, 12, 14, 15, 17, 25 , and 26. 2 GPT-3.5 and GPT-4 versions as of February 1, 2024 3 Llama 2 and Gemini versions as of  May 15, 2024  GPT Prompt -P (e|c) Imagine a scenario where two people, Player A and Player B, play a competitive game called \"Split or Steal.\" Players play multiple rounds with each other. In each round of the game, players each decide whether to split or steal from a pot of $10. If both choose \"split\", they each get $5. \"If both choose \"steal\", they each get $1. If one chooses \"split\" but the other chooses \"steal\", the stealer gets all $10. They make their choices secretly and their choices are revealed at the end of the round. Scenarios describe one round of the game. Imagine the feelings of Player A. In this round, Player A chooses \"steal\" and Player B chooses \"split.\" How does Player A experience emotions? Provide a probability distribution based on the following emotion list: Joy, Neutral, Surprise, Anger, Disgust, Fear, Sad. Ensure that the sum of probabilities is 1. Provide answer in the following format: \"Joy: prob 1, Neutral: prob 2, Surprise: prob 3, Anger: prob 4, Disgust: prob 5, Fear: prob 6, Sad: prob 7.\" Fig.  4 : GPT Prompt depicting a \"Split or Steal\" game scenario used to P (e|c) data on emotion probability distribution. better model performance. RMSE provides a measure of the average magnitude of the errors, again with lower values being preferable. Finally, we include F1 (weighted) to assess performance if the model was forced to provide a single label, though caution that F1 can be misleading as only about half of the videos had strong agreement amongst the annotators.\n\nBest model for P (e|f ): The LSTM model is distinguished by the lowest RMSE (0.095) and KLD (0.134), suggesting strong predictive accuracy and a high correlation with humanannotated data for capturing facial emotion nuances (as referenced in Table  II ). While the LSTM model is fine-tuned for IPD dataset, our emphasis is not on its distinct advantages. Rather, we focus on demonstrating how BCI enhances a range of facial emotion recognition methods. Best model for P (e|c): GPT-4, with the lowest RMSE (0.061) and KLD (0.101), indicates that its predictions for context-based emotions closely mirror human judgments (as seen in Table  II ). These results support our decision to employ LSTM for facial emotion recognition and GPT-4 for contextual emotion recognition for integration model P (e|c, f ), which promises to align closely with human emotional perception.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Face",
      "text": "",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Iv. Integration Of Facial And Contextual",
      "text": "EMOTION RECOGNITION We compare alternative methods for integrating facial and contextual cues for context-aware emotion recognition. We explore two methods: BCI and GPT-4 Integration method.\n\n1) Bayesian Cue Integration (BCI): We apply BCI as detailed in Eq. 1, using results of P (e|f ) from three facial emotion recognition methods  4  and P (e|c) from GPT-3.5 and GPT-4. Following  [11] , we do not explicitly calculate P (e) but normalize the product into a proper distribution by dividing each probability by the sum of the posteriors.\n\n2) GPT-4 Integration: We explore if GPT-4 by itself could integrate facial cues and contextual information to generate P (e|c, f ). This method leverages the advanced capabilities of GPT-4 to directly generate a context-aware emotion probability distribution. The integration process involves GPT-4 with a representation of P (e|f ) estimated by a context-free facial emotion recognition method. Based on previous research  [34] ,  [35] , LLMs have a better understanding when the input is represented in natural language rather than numerical value. Therefore, we add natural language descriptions corresponding to the probability levels to enhance the model's interpretability. For example, a Joy probability above 0.5 would be described as \"a high level of happiness\" within the input prompt.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "A. Overall Result",
      "text": "We compare the performance of knowledge-based recognition with the distribution of human context-based perceptions (see Table  III ). Two broad observations are immediately clear. First, GPT-4 generally yielded better performance when compared with GPT-3, though this benefit was strongest in combination with LSTM. Second, LSTM (which incorporated dynamic facial movements), clearly dominated the other context-free methods in predicting the distribution of human perceptions (even exceeding the performance of BCI using human labels as measured by KLD).\n\nLooking in more detail, using GPT-4 to predict P (e|c) yielded strong improvements in performance with LSTM across all three measures of performance, when compared with GPT-3. For FACET and EAC, performance was improved in only two of the three measures. This is interesting as GPT-4 was far better at capturing human context-only perceptions, suggesting there is some interaction between errors in the context-based distribution when combined with the contextfree judgments in these two methods. An analysis of the confusion matrices of EAC and FACET suggests they were harmed by their inability to recognize surprise.\n\nGPT-4 with BCI (denoted as LSTM+GPT-4 (BCI)) showed essentially equivalent performance with BCI using human perceptions as measured by KLD and RMSE, but not for F1. Note that KLD and RMSE capture the closeness between two distributions. In contrast, F1 forces the model to pick the most likely class, even if another class was almost equally likely. To understand the difference, we examine the individual videos and found that the difference in accuracy between LSTM+GPT-4 (BCI) and BCI with human distributions is due to differences in videos where the player was exploited (CD). Most of these videos showed an almost equivalent likelihood of being labeled as joy or surprise but the human contextbased labels tended to assign somewhat weight to joy, whereas LSTM+GPT-4 (BCI) assigned more way to surprise. This highlights the problematic consequences of using hard labels when the probability of perceiving multiple classes is high.\n\nInterestingly, using GPT-4 to perform the integration, in addition to reasoning about the situational context, yielded remarkably strong results. This approach actually improved over BCI for FACET and EAC, but yielded slightly worse performance than BCI with LSTM. A disadvantage of GPT-4 integration is this is a black box so it more difficult to gain insight into why the method produced improvements or deficits. Nonetheless, this suggests that there is promise it using LLMs as a replacement for the BCI approach.\n\nWe also look at improving integration with a nonlinear approach (training a Neural Network), but this performed somewhat worse than BCI and LSTM (GPT-4) (see supplemental materials  [25] ).",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Face+Context (Integration",
      "text": "",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "B. How Integration Improves Performance?",
      "text": "We perform further analysis to examine how BCI improves recognition performance. Fig.  5  illustrates the change in recognition performance as a function of the different game outcomes. Specifically, it shows the change distance between context-free and context-based estimates that result from using BCI with GPT-4 (measured by the change in KLD). Positive numbers indicate improved performance. This figure indicates that all methods improved their performance in predicting perceived emotions when the game outcome was disadvantageous to the player. For example, whereas a context-free method might predict joy, learning the person was exploited might change this to surprise). This did come at some cost in that methods became somewhat worse at predicting emotions when the game outcome was advantageous to the player. Across all game outcomes, performance improved. This emphasizes the significant role that context plays in accurately interpreting emotional perception across various integration methods.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "V. Conclusion",
      "text": "In this study, we explored the potential benefits of knowledge-based emotion recognition. Inspired by a psychological theory of human emotion perception -Bayesian Cue Integration -we find that context-free automatic recognition improved (across all methods tested) by incorporating zeroshot inferences from LLMs about the situational context. Specifically, we evaluated methods using naturalistic expressions produced during a two-person prisoner's dilemma task and found that the integration of facial cues and contextual information using BCI accurately predicted the ratings by human annotators knowledgeable of the situational context. Performance improved across all methods, with the LSTM and GPT-4 combination achieving the best performance (though it should be noted that LSTM was trained on the Split-Steal corpus while other methods used pre-trained models).\n\nKnowledge-based recognition showed the strongest improvements when the player experienced a negative outcome (i.e., they were exploited by their partner or both partners tried to exploit each other). This seems to be because players often showed smiles that, in the absence of context, were interpreted as joy but when seen in the light of context were interpreted more negatively. For example, in one anecdote, a player can be seen mouthing profanity at her partner while wryly smiling (a detail missed by annotators without access to the context). In contrast, context failed to improve the accuracy of emotion predictions when the player experienced a positive outcome. This seems to be because players almost always showed some smile as a result of the outcome. Together, these emphasize the accuracy of context-free emotion recognition will depend heavily on the context, highlighting that the integration of situational context as a vital component in interpreting emotional states, particularly in complex social situations. Thus, findings highlight the promise of knowledgebased approaches as a direction for future research in the field of affective computing.\n\nOur findings further reinforce prior studies that highlight the zero-shot social and emotional intelligence of LLMs. Without any fine-tuning, GPT-4 showed consistency with the emotions reported by humans given a description of an emotional situation. While here we only tested on a single task, in light of other studies, this suggests a robustness and adaptability to various domains and datasets. This universality is an advantage, broadening the applications of our approach across different areas of research and practical deployment.\n\nThe findings of this study open several avenues for future research in emotion recognition and affective computing. While our results show the potential of integrating facial expressions and situational context using BCI and LLMs, there is still room for refinement in closing the gap between automated methods and human emotion perception. BCI uses a simplified model of human emotion perception, which might not fully account for situations where display norms constrain emotional expression. For example, research on the prisoner's dilemma  [36]  reveals discrepancies between first-person and second-person reports of emotion, likely due to emotion regulation strategies not captured by BCI. Additionally, our focus was on perceived emotion, but we did not examine how well these perceptions align with actual feelings.\n\nCulture may also play a role in our findings. Research suggests that the influence of others' facial expressions is strongest in interdependent cultures  [37] . Given that our annotators were US-based and the culture of the US is low in interdependence, this might explain our results. Moreover, LLMs have been noted for their anglocentric tendencies, which may impact their efficacy in interpreting emotions from diverse cultural contexts  [38] . This highlights the need for caution in extrapolating our findings to other tasks, contexts, or cultures.\n\nMoreover, the use of probabilistic programming could offer a more sophisticated approach to modeling the complex interplay between facial expressions, context, and emotion perception  [39] . By developing more advanced probabilistic models, researchers could further reduce the gap between automated emotion recognition and human-like emotion understanding.\n\nOur method shows promise for structured scenarios with clear outcomes, like poker games or negotiations, where outcomes can be easily described for LLM prompts. However, extending this method to more unfolding scenarios, like depression interviews, poses challenges due to the difficulty in describing events in video in real-time for LLM prompts.\n\nFinally, though BCI has shown promise in explaining human context-based predictions across a range of settings, our current study only explored a single corpus. While it is encouraging that our computational findings mirror (and replicate) these psychological findings, future research must validate and extend our findings to other social tasks.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Ethical Statement",
      "text": "This paper examined how people form judgments of emotional expressions through re-analysis of previously collected data subject to ethical review and shared based on consent terms. The data was annotated with human coders and analyzed using commercial automatic facial analysis methods. These decisions introduce bias and limit result generality. The original data is demographically diverse but collected only in Los Angeles. Perceived-emotion judgments were obtained via Amazon Mechanical Turk workers. Automated methods have biases in tracking and characterizing people of color. Data was limited to the prisoner's dilemma game, necessitating replication across tasks, populations, and facial analysis tools for robust conclusions.",
      "page_start": 8,
      "page_end": 8
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Illustrates knowledge-based recognition: (1) an emotion distribution is estimated from facial cues alone (2) from",
      "page": 2
    },
    {
      "caption": "Figure 2: Facial reactions were annotated either without context",
      "page": 2
    },
    {
      "caption": "Figure 3: (a) Context-free, (b) Context-based, (c) Context-only. CC (mutual cooperation), DC (Player A exploit), CD (Player A",
      "page": 3
    },
    {
      "caption": "Figure 2: ). They were simply told to watch the video and",
      "page": 3
    },
    {
      "caption": "Figure 2: ) For each",
      "page": 3
    },
    {
      "caption": "Figure 3: visualizes the elicited emotion probability distribu-",
      "page": 4
    },
    {
      "caption": "Figure 3: a), Joy is the predom-",
      "page": 4
    },
    {
      "caption": "Figure 3: b illustrates the context-based results, where annota-",
      "page": 4
    },
    {
      "caption": "Figure 3: c), there was a notable",
      "page": 4
    },
    {
      "caption": "Figure 1: , we automate context-based pre-",
      "page": 4
    },
    {
      "caption": "Figure 4: ). The prompt contains",
      "page": 5
    },
    {
      "caption": "Figure 4: GPT Prompt depicting a “Split or Steal” game scenario",
      "page": 5
    },
    {
      "caption": "Figure 5: illustrates the change in",
      "page": 6
    },
    {
      "caption": "Figure 5: Enhancement in model performance (KLD) through",
      "page": 7
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Face\nContext": "FACET\n-\nEAC\n-\nLSTM\n-\n-\nGPT-3.5\n-\nGPT-4\n-\nLlama 2\n-\nGemini",
          "RMSE(↓)\nKLD(↓)\nF1-score(↑)": "0.689\n0.119\n0.688\n0.155\n2.421\n0.658\n0.095\n0.134\n0.660\n0.197\n2.579\n0.333\n0.061\n0.101\n0.750\n0.149\n0.343\n0.1\n0.137\n0.491\n0.333"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Face+Context\n(Integration)": "FACET+GPT-3 (BCI)\nFACET+GPT-4 (BCI)\nEAC+GPT-3 (BCI)\nEAC+GPT-4 (BCI)\nLSTM+GPT-3 (BCI)\nLSTM+GPT-4 (BCI)\nHuman+Human (BCI)\nFACET (GPT-4)\nEAC (GPT-4)\nLSTM (GPT-4)\nLSTM+GPT-4 (NN)",
          "KLD(↓)\nRMSE(↓)\nF1(↑)": "1.713\n0.215\n0.525\n1.829\n0.200\n0.565\n1.340\n0.200\n0.519\n1.330\n0.210\n0.527\n0.809\n0.162\n0.454\n0.346\n0.104\n0.649\n0.092\n0.782\n0.441\n0.648\n0.150\n0.528\n0.597\n0.155\n0.503\n0.354\n0.112\n0.530\n0.580\n0.151\n0.151"
        }
      ],
      "page": 6
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Context based emotion recognition using emotic dataset",
      "authors": [
        "R Kosti",
        "J Alvarez",
        "A Recasens",
        "A Lapedriza"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "2",
      "title": "Emotion recognition in the wild from videos using images",
      "authors": [
        "S Bargal",
        "E Barsoum",
        "C Ferrer",
        "C Zhang"
      ],
      "year": "2016",
      "venue": "Proceedings of the 18th ACM international conference on multimodal interaction"
    },
    {
      "citation_id": "3",
      "title": "A survey on facial emotion recognition techniques: A state-of-the-art literature review",
      "authors": [
        "F Canal",
        "T Müller",
        "J Matias",
        "G Scotton",
        "A De Sa Junior",
        "E Pozzebon",
        "A Sobieranski"
      ],
      "year": "2022",
      "venue": "Information Sciences"
    },
    {
      "citation_id": "4",
      "title": "Do facial expressions signal specific emotions? judging emotion from the face in context",
      "authors": [
        "J Carroll",
        "J Russell"
      ],
      "year": "1996",
      "venue": "Journal of personality and social psychology"
    },
    {
      "citation_id": "5",
      "title": "Constants across cultures in the face and emotion",
      "authors": [
        "P Ekman",
        "W Friesen"
      ],
      "year": "1971",
      "venue": "Journal of personality and social psychology"
    },
    {
      "citation_id": "6",
      "title": "An argument for basic emotions",
      "authors": [
        "P Ekman"
      ],
      "year": "1992",
      "venue": "Cognition & emotion"
    },
    {
      "citation_id": "7",
      "title": "Emotional expressions reconsidered: Challenges to inferring emotion from human facial movements",
      "authors": [
        "L Barrett",
        "R Adolphs",
        "S Marsella",
        "A Martinez",
        "S Pollak"
      ],
      "year": "2019",
      "venue": "Psychological science in the public interest"
    },
    {
      "citation_id": "8",
      "title": "Deep learning-based drivers emotion classification system in time series data for remote applications",
      "authors": [
        "R Naqvi",
        "M Arsalan",
        "A Rehman",
        "A Rehman",
        "W.-K Loh",
        "A Paul"
      ],
      "year": "2020",
      "venue": "Remote Sensing"
    },
    {
      "citation_id": "9",
      "title": "Facial expression emotion detection for real-time embedded systems",
      "authors": [
        "S Turabzadeh",
        "H Meng",
        "R Swash",
        "M Pleva",
        "J Juhar"
      ],
      "year": "2018",
      "venue": "Facial expression emotion detection for real-time embedded systems"
    },
    {
      "citation_id": "10",
      "title": "Emotion recognition in context",
      "authors": [
        "R Kosti",
        "J Alvarez",
        "A Recasens",
        "A Lapedriza"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "11",
      "title": "Affective cognition: Exploring lay theories of emotion",
      "authors": [
        "D Ong",
        "J Zaki",
        "N Goodman"
      ],
      "year": "2015",
      "venue": "Cognition"
    },
    {
      "citation_id": "12",
      "title": "Emotion prediction as computation over a generative theory of mind",
      "authors": [
        "S Houlihan",
        "M Kleiman-Weiner",
        "L Hewitt",
        "J Tenenbaum",
        "R Saxe"
      ],
      "year": "2023",
      "venue": "Philosophical Transactions of the Royal Society A"
    },
    {
      "citation_id": "13",
      "title": "Is gpt a computational model of emotion?",
      "authors": [
        "A Tak",
        "J Gratch"
      ],
      "year": "2023",
      "venue": "2023 11th International Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "14",
      "title": "Fine-grained affective processing capabilities emerging from large language models",
      "authors": [
        "J Broekens",
        "B Hilpert",
        "S Verberne",
        "K Baraka",
        "P Gebhard",
        "A Plaat"
      ],
      "year": "2023",
      "venue": "2023 11th International Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "15",
      "title": "Emotion-conditioned text generation through automatic prompt optimization",
      "authors": [
        "Y Resendiz",
        "R Klinger"
      ],
      "year": "2023",
      "venue": "Emotion-conditioned text generation through automatic prompt optimization",
      "arxiv": "arXiv:2308.04857"
    },
    {
      "citation_id": "16",
      "title": "Being and doing: The judicial use of remorse to construct character and community",
      "authors": [
        "R Weisman"
      ],
      "year": "2009",
      "venue": "Social & Legal Studies"
    },
    {
      "citation_id": "17",
      "title": "Expressing anger in conflict: when it helps and when it hurts",
      "authors": [
        "G Van Kleef",
        "S Côté"
      ],
      "year": "2007",
      "venue": "Journal of Applied Psychology"
    },
    {
      "citation_id": "18",
      "title": "Infusing context into emotion perception impacts emotion decoding accuracy",
      "authors": [
        "U Hess",
        "K Kafetsios"
      ],
      "year": "2022",
      "venue": "Experimental psychology"
    },
    {
      "citation_id": "19",
      "title": "Formulating emotion perception as a probabilistic model with application to categorical emotion classification",
      "authors": [
        "R Lotfian",
        "C Busso"
      ],
      "year": "2017",
      "venue": "2017 Seventh International Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "20",
      "title": "End-to-end label uncertainty modeling in speech emotion recognition using bayesian neural networks and label distribution learning",
      "authors": [
        "N Prabhu",
        "N Lehmann-Willenbrock",
        "T Gerkmann"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "21",
      "title": "Prisoner's dilemma: A study in conflict and cooperation",
      "authors": [
        "A Rapoport",
        "A Chammah"
      ],
      "year": "1965",
      "venue": "Prisoner's dilemma: A study in conflict and cooperation"
    },
    {
      "citation_id": "22",
      "title": "Emotional expressivity is a reliable signal of surprise",
      "authors": [
        "S Lei",
        "J Gratch"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "23",
      "title": "Approximating discrete probability distribution of image emotions by multi-modal features fusion",
      "authors": [
        "S Zhao",
        "G Ding",
        "Y Gao",
        "J Han"
      ],
      "year": "2017",
      "venue": "Transfer"
    },
    {
      "citation_id": "24",
      "title": "Aesthetics and emotions in images",
      "authors": [
        "D Joshi",
        "R Datta",
        "E Fedorovskaya",
        "Q.-T Luong",
        "J Wang",
        "J Li",
        "J Luo"
      ],
      "year": "2011",
      "venue": "IEEE Signal Processing Magazine"
    },
    {
      "citation_id": "25",
      "title": "In-depth analysis of emotion recognition through knowledge-based large language models",
      "authors": [
        "B Han",
        "C Yau",
        "S Lei",
        "J Gratch"
      ],
      "year": "2024",
      "venue": "In-depth analysis of emotion recognition through knowledge-based large language models"
    },
    {
      "citation_id": "26",
      "title": "The computer expression recognition toolbox (cert)",
      "authors": [
        "G Littlewort",
        "J Whitehill",
        "T Wu",
        "I Fasel",
        "M Frank",
        "J Movellan"
      ],
      "year": "2011",
      "venue": "2011 IEEE International Conference on Automatic Face & Gesture Recognition (FG)"
    },
    {
      "citation_id": "27",
      "title": "Learn from all: Erasing attention consistency for noisy label facial expression recognition",
      "authors": [
        "Y Zhang",
        "C Wang",
        "X Ling",
        "W Deng"
      ],
      "year": "2022",
      "venue": "European Conference on Computer Vision"
    },
    {
      "citation_id": "28",
      "title": "Reliable crowdsourcing and deep localitypreserving learning for expression recognition in the wild",
      "authors": [
        "S Li",
        "W Deng",
        "J Du"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "29",
      "title": "Openface 2.0: Facial behavior analysis toolkit",
      "authors": [
        "T Baltrusaitis",
        "A Zadeh",
        "Y Lim",
        "L.-P Morency"
      ],
      "year": "2018",
      "venue": "2018 13th IEEE international conference on automatic face & gesture recognition"
    },
    {
      "citation_id": "30",
      "title": "Refactoring facial expressions: An automatic analysis of natural occurring facial expressions in iterative social dilemma",
      "authors": [
        "G Stratou",
        "J Van Der Schalk",
        "R Hoegen",
        "J Gratch"
      ],
      "year": "2017",
      "venue": "2017 Seventh International Conference on Affective Computing and Intelligent Interaction"
    },
    {
      "citation_id": "31",
      "title": "Dense 3d face alignment from 2d videos in real-time",
      "authors": [
        "L Jeni",
        "J Cohn",
        "T Kanade"
      ],
      "year": "2015",
      "venue": "2015 11th IEEE international conference and workshops on automatic face and gesture recognition (FG)"
    },
    {
      "citation_id": "32",
      "title": "On information and sufficiency",
      "authors": [
        "S Kullback",
        "R Leibler"
      ],
      "year": "1951",
      "venue": "The annals of mathematical statistics"
    },
    {
      "citation_id": "33",
      "title": "A mixed bag of emotions: Model, predict, and transfer emotion distributions",
      "authors": [
        "K.-C Peng",
        "T Chen",
        "A Sadovnik",
        "A Gallagher"
      ],
      "year": "2015",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "34",
      "title": "Evaluating and enhancing structural understanding capabilities of large language models on tables via input designs",
      "authors": [
        "Y Sui",
        "M Zhou",
        "M Zhou",
        "S Han",
        "D Zhang"
      ],
      "year": "2023",
      "venue": "Evaluating and enhancing structural understanding capabilities of large language models on tables via input designs",
      "arxiv": "arXiv:2305.13062"
    },
    {
      "citation_id": "35",
      "title": "Exploring equation as a better intermediate meaning representation for numerical reasoning",
      "authors": [
        "D Wang",
        "L Dou",
        "W Zhang",
        "J Zeng",
        "W Che"
      ],
      "year": "2023",
      "venue": "Exploring equation as a better intermediate meaning representation for numerical reasoning",
      "arxiv": "arXiv:2308.10585"
    },
    {
      "citation_id": "36",
      "title": "How expression and context determine second-person judgments of emotion",
      "authors": [
        "J Hoegen",
        "G Lucas",
        "D Shore",
        "B Parkinson",
        "J Gratch"
      ],
      "year": "2023",
      "venue": "2023 11th International Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "37",
      "title": "Judging facial emotion expressions in context: The influence of culture and self-construal orientation",
      "authors": [
        "U Hess",
        "C Blaison",
        "K Kafetsios"
      ],
      "venue": "Journal of Nonverbal Behavior"
    },
    {
      "citation_id": "38",
      "title": "Multilingual language models are not multicultural: A case study in emotion",
      "authors": [
        "S Havaldar",
        "S Rai",
        "B Singhal",
        "L Guntuku",
        "L Ungar"
      ],
      "year": "2023",
      "venue": "Multilingual language models are not multicultural: A case study in emotion",
      "arxiv": "arXiv:2307.01370"
    },
    {
      "citation_id": "39",
      "title": "Applying probabilistic programming to affective computing",
      "authors": [
        "D Ong",
        "H Soh",
        "J Zaki",
        "N Goodman"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Affective Computing"
    }
  ]
}