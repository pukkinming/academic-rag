{
  "paper_id": "2207.05298v1",
  "title": "Multitask Learning From Augmented Auxiliary Data For Improving Speech Emotion Recognition",
  "published": "2022-07-12T04:12:13Z",
  "authors": [
    "Siddique Latif",
    "Rajib Rana",
    "Sara Khalifa",
    "Raja Jurdak",
    "Björn W. Schuller"
  ],
  "keywords": [
    "Speech emotion recognition",
    "multi task learning",
    "representation learning"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Despite the recent progress in speech emotion recognition (SER), state-of-the-art systems lack generalisation across different conditions. A key underlying reason for poor generalisation is the scarcity of emotion datasets, which is a significant roadblock to designing robust machine learning (ML) models. Recent works in SER focus on utilising multitask learning (MTL) methods to improve generalisation by learning shared representations. However, most of these studies propose MTL solutions with the requirement of meta labels for auxiliary tasks, which limits the training of SER systems. This paper proposes an MTL framework (MTL-AUG) that learns generalised representations from augmented data. We utilise augmentation-type classification and unsupervised reconstruction as auxiliary tasks, which allow training SER systems on augmented data without requiring any meta labels for auxiliary tasks. The semi-supervised nature of MTL-AUG allows for the exploitation of the abundant unlabelled data to further boost the performance of SER. We comprehensively evaluate the proposed framework in the following settings: (1) within corpus, (2) cross-corpus and cross-language, (3) noisy speech, (4) and adversarial attacks. Our evaluations using the widely used IEMOCAP, MSP-IMPROV, and EMODB datasets show improved results compared to existing state-of-the-art methods.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "S PEECH Emotion Recognition (SER) is an emerging area of research. Speech contains information about human emotions, which can be utilised by machine learning (ML) systems for automatic detection redefining humancomputer interactions. SER can help improve the quality of customer service by tracking customer-agent reactions. In healthcare, SER can be used for diagnosis and monitoring of affective behaviours  [1] ,  [2] . Service delivery in transport  [3] , forensics  [4] , education  [5] , media  [6]  can be improved by utilising SER.\n\nHuman emotion modelling is quite complex due to its dependency on many factors including speaker  [7] , gender  [8] , age  [9] , culture  [10] , and dialect  [11] . Researchers have explored various ML techniques, including hidden Markov models, support vector machines, and deep neural networks (DNNs) for SER, wherein DNNs have improved performance compared to the classical ML techniques. Deep belief networks (DBN)  [12] , convolutional neural networks (CNN)  [13] , and recurrent neural network (RNNs) have been successful in modelling emotions in speech and widely explored in SER  [14] ,  [15] ,  [16] ,  [17] . In particular, RNN architectures like short term memory (LSTM) networks  [18]  or bidirectional LSTM (BLSTM) combined with CNNs are a popular choice in SER for capturing emotional attributes and have been explored by many researchers  [19] ,  [20] . Studies  [15] ,  [21]  show that the CNN-LSTM can learn better emotional features for SER compared to using CNN or LSTM individually. This work presents a unique semisupervised configuration using CNN-BLSTM with attention mechanisms. We utilise an attention mechanism in our emotion classifier to combine the important emotional information extracted from the overall utterance and improve emotion classification performance.\n\nLiterature shows, SER models lack generalisation due to the single task-specific training and perform poorly when the data mismatch increases between the training and testing phases  [22] ,  [23] . Typically, generalisation of deep learning models is improved by training them on diverse data. For example, state-of-the-art models in computer vision are trained on thousands of labelled samples, and automatic speech recognition systems are trained on thousands of hours of transcribed data  [24] ,  [25] ,  [26] . In contrast, SER corpora are relatively small, and the creation of emotional corpora is a time consuming and expensive task  [22] ,  [27]  as emotion is subjective, and several annotators are usually required, which often have to repeatedly go through the speech material to annotate, e. g., affective dimension by affective dimension. To obtain data volume, most existing studies in SER attempt to train models on multiple corpora  [10] ,  [28] . However, standard benchmark datasets are also very limited, which creates tremendous barriers to achieving generalisation in SER systems  [17] .\n\nAn alternative technique to improve the generalisation of DL models is multitask learning (MTL)  [29] , which simultaneously solves the multiple relevant auxiliary tasks along with the primary task. MTL can use different aspects of the same data or get data supplement from the secondary tasks. In this way, models can be better regularised by capturing shared and essential high-level representations, leading to an improved generalisation of the system. MTL has been successfully used in SER by achieving promising performance. However, most of these MTL techniques present supervised auxiliary tasks, which require accurate annotations just like the primary emotion recognition tasks. Examples include emotional attributes (i.e., arousal, valence, and dominance) prediction  [30] ,  [31] , gender identification  [22] ,  [32] ,  [33] , speaker recognition  [22] ,  [34] , and secondary emotion learning  [35] . The MTL methods with any of the above auxiliary tasks need accurate meta labels that limit the SER models' training. In some scenarios, larger data can be utilised for auxiliary tasks like speaker and gender identification  [22] ; however, collecting speaker and gender labels is also time-and labour-intensive. This also makes the model's performance speaker-dependent in some cases. Moreover, a generalised representation for SER containing speaker and gender information might be used maliciously without the user's consent by an eavesdropping adversary  [36] ,  [37] .\n\nIn this paper, we propose a semi-supervised MTL framework that learns from augmented data-we call it MTL-AUG. It primarily classifies emotions and utilises data augmentation-type classification and unsupervised reconstruction as auxiliary tasks to learn generalised representations. We use types of augmentation as labels for data augmentation for classification as an auxiliary task. In this way, these auxiliary tasks do not require meta labelling performed by experts. Our idea is inspired by ConvNets, which learn image classification features by predicting the 2D image rotation that is applied to the input image  [38] . Such geometric transformation cannot be applied to the speech signal. Therefore, we propose to use speech-based augmentation types that enable multitask training to learn a generalised representation without requiring meta labels. We apply temporal, frequency, and mixup related augmentations to the input speech. This allows the model to learn temporal and frequency related variations applied to the input data through augmentation-type classification as an auxiliary task. Learning the temporal and frequency variations in the data helps the MTL model to improve SER performance. Our second auxiliary task of unsupervised reconstruction acts as a regulariser and improves the quality of learnt representations. Overall, both auxiliary tasks enable the proposed framework to effectively utilise the augmented and unlabelled data to improve the generalisation of the SER system.\n\nMost of previous MTL studies  [22] ,  [27] ,  [30] ,  [31] ,  [33] ,  [34] ,  [39]  evaluate the proposed models in within-corpus SER, and very few studies perform cross-corpus and crosslanguage SER. Moreover, none of these studies performs evaluations in noisy and adversarial attack settings. This is mainly due to the complexity of mismatch conditions in noisy and adversarial attacks. To show the advantage of our proposed MTL framework, we rigorously evaluate it against noisy and adversarial conditions. For evaluation, we use three widely used emotional databases: The interactive emotional dyadic motion capture (IEMOCAP)  [40]  database, MSP-IMPROV  [41] , and the EMODB data. We compare our framework's performance with multiple recent studies and baseline CNN-BLSTM implementations. The comparative results in within-corpus, cross-corpus, cross-language, noisy and adversarial settings show that the proposed MTL-AUG framework achieves considerably improved performance, which attests to the strong generalisation power of the proposed MTL-AUG framework.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Multi-Task Learning For Ser",
      "text": "Multitask learning (MTL)  [29]  aims to improve the generalisation of models by learning the similarities and differences among the given tasks from the training data. It has been successful to produce shared representation by simultaneously modelling multiple related tasks. The conventional single task learning technique ignores the information of related tasks and can increase the risk of overfitting  [23] . In contrast, MTL acts as a regulariser to reduce the risk of overfitting by introducing an inductive bias. Several MTL approaches  [42] ,  [43] ,  [44]  have been exploited in computer vision to address various problems with significantly improved results. The speech community also explored MTL approaches to improve the performance of the tasks, including automatic speech recognition  [45] , speaker identification  [46] , and also emotion classification  [47] .\n\nEyben et al.  [49]  were the first to explore MTL in SER. They empirically found that multi-task training of models help improve performance in contrast to single-task training. Xia et al.  [30]  presented a DBN based MTL model for SER and utilised activation and valence labels as an auxiliary task. They demonstrated that the performance of SER for categorical emotion could be enhanced using activation and valence label information as auxiliary tasks. Prthasarathy and Busso  [31]  presented a DNN-based MTL model that jointly learns the arousal, dominance, and valence from a given utterance. The authors found that joint training of the model with multiple emotional attributes enhances the performance compared to training with single attribute information. Ma et al.  [50]  used a multitask attention-based DNN model for SER and showed that a high performance could be achieved by optimising the model for joint classification of categorical emotions along with valence and activation labels classification. Similarly, Lotfian et al.  [35]  utilised a DNN based framework for modelling primary and secondary emotions. Based on the results, the authors showed that the performance of the primary classification task (categorical emotions) is enhanced by utilising the information of secondary emotions and emotional classes perceived by the evaluators.\n\nAnother way to implement MTL in SER is to use speaker and gender identification as auxiliary tasks. Multiple studies have explored this phenomenon to improve SER performance. In  [39] , the authors presented an LSTM-based MTL framework that uses speaker and gender classification as auxiliary tasks to improve the performance of the main task, emotion classification. In another study  [22] , the authors proposed an MTL framework that uses speaker and gender recognition as auxiliary tasks and used other speech corpora with speaker and gender labels and injected this data into the model. They showed that the performance could be significantly improved. Kim et al.  [48]  utilised gender and naturalness (natural or acted corpus) recognition as auxiliary tasks and evaluated the model using different corpora. They found that a performance gain can be achieved using gender or naturalness classification as auxiliary tasks. Other recent studies also utilised  [8] ,  [33]  gender-aware MTL SER models and found that emotion classification can be improved with additional gender label information. Previous studies on MTL demonstrate that the use of auxiliary tasks helps improve SER performance compared with STL. However, these approaches either use information about emotional attributes (activation, valence, etc.) or non-emotional attributes (speaker, gender, etc.) that are not widely available in real-life. Also, labelling speech data with such meta-information is a cumbersome and expensive process. Some studies  [22] ,  [27]  exploit the unsupervised reconstruction as auxiliary tasks; however, they also require additional labels for emotional attributes  [27] , and gender and speaker labels in  [22]  for their MTL frameworks.\n\nIn contrast to previous studies, we propose an MTL framework that improves the performance without requiring such meta labels by annotators. We propose using data transformation (or augmentation)-type recognition and unsupervised feature reconstruction as auxiliary tasks. This allows us to utilise the type of augmentation applied to the input data as labels for the auxiliary task to train the proposed MTL framework.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Data Augmentation In Ser",
      "text": "Data augmentation techniques have been used to generate additional training data for SER. For example, studies  [20] ,  [51]  show that the speed perturbation  [52]  data augmentation technique can improve the performance of an SER system by generating copies of each utterance with different speed effects. The mixup  [53]  technique augments an SER system by generating the synthetic sample as a linear combination of the original sample. In SER, Latif et al.  [15]  augment the SER system with mixup to achieve robustness against noisy conditions. They showed that augmentation techniques make the training data diverse and help improve performance. A new method of data augmentation is SpecAugment  [54]  and was proposed for automatic speech recognition, which is directly applied to the feature inputs of a neural network. In  [55] , the authors utilised the SpecAugment technique to augment their SER system with the duplicate samples by a factor of two. The authors highlighted that the data augmentation improves the robustness of the model by providing diverse training samples. Other studies  [20] ,  [51] ,  [56]  also achieve improved performance by exploiting data augmentation techniques to increase the training data. However, these studies only utilised the data augmentation in single-task learning to increase the training samples. In this paper, we propose to use data augmentation-type recognition as our auxiliary task in our proposed multitask learning framework. We hypothesise that multitask learning models are able to understand the concept of emotions while recognising the transformation performed on the input signal.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Ser Robust To Adversarial Attacks And Noise",
      "text": "In SER, it is essential to achieve robustness against perturbation/noise added to the input samples. However, very few studies focus on evaluating SER systems' robustness against noisy conditions and adversarial attacks. Huang et al.  [57]  used a CNN-LSTM model for robust SER. They found that CNN demonstrates a certain degree of noise robustness. In  [58] , the authors utilised deep residual networks for speech enhancement to remove noise from speech while preserving emotions for SER. Some other studies  [59] ,  [60]  also explored different noise removal frameworks for SER in noisy environments instead of achieving robustness by learning generalised representation. Based on the findings of data augmentation techniques to improve robustness  [61] ,  [62] , a recent study  [15]  evaluated the regularising effect of data augmentation to improve the robustness of SER. They show that data augmentation helps to improve the robustness of SER against noise and adversarial attacks. However, no study has evaluated data augmentation in MTL scenarios to learn generalised representation to improve robustness in SER.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Summary",
      "text": "We summarise the differences between our work and the existing literature in Table  1.  1) While some studies used reconstruction as an auxiliary task, no studies used augmentation-type classification as the auxiliary task.\n\nFig.  1 : Illustration of our proposed multitask framework for SER, which uses augmentation-type classification and reconstruction as auxiliary task to achieve better performance on the primary emotion classification task.\n\n2) None of the studies evaluated their models' generalisation ability against noisy conditions and adversarial attacks. 3) Most of the studies evaluated their model withincorpus settings by using training and testing data from the same corpus. Only a few studies evaluated the generalisation of proposed models in crosscorpus and even less in cross-language settings.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Methodology",
      "text": "The proposed MTL-AUG framework uses the augmentationtype classification and unsupervised reconstruction as auxiliary tasks to learn generalised emotional representations. Before we describe our framework, we briefly introduce speech data augmentation, especially the techniques used for this work.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Speech Data Augmentation",
      "text": "We use augmentation to introduce variability and volume in the data. Speech signals can be augmented/transformed using different techniques. We use the following three techniques: (1) speed perturbation  [52] , (2) mixup  [53] , and (3) SpecAugment  [54] . Speed perturbation is a very popular and widely used audio augmentation technique that produces a warped time signal. Given a speech signal x(t), time warping is performed by a factor α to produce the signal x(αt). In this way, speed perturbation changes the duration of a given speech signal. It can be applied directly on raw speech as we use in this paper. SpecAugment is used as a simple data augmentation method for Automatic Speech Recognition (ASR). It acts on the log-Mel spectrogram directly with a negligible amount of additional computational cost  [54] . In SpecAugment, training data can be augmented using spectro-temporal modifications to the original spectrograms by applying frequency and time masks. In frequency masking, a mask of size f is chosen from a uniform distribution (0 to F ) and consecutive log-Mel frequency channels [f 0 , f 0 +f ) are masked, where f 0 is chosen from [0, v -f ) and v represents the number of Mel-frequency channels. In the time masking, a mask size of t is chosen from a uniform distribution from 0 to T , and the consecutive time steps [t 0 , t 0 + t) are masked in time -here, t 0 is chosen from [0, τ -t) and τ represents log-Mel spectrogram time steps.\n\nMixup generates an augmented sample and its label by randomly mixing two inputs and their corresponding labels. This regularises the neural network to favour simple linear behaviour in-between training samples. It constructs augmented training examples as follows:\n\nwhere (x i , y i ) and (x j , y j ) are randomly selected two examples from training data, and λ ∈ [0, 1]. Mixup can be applied on the features as well as on the raw speech  [53] .\n\nWe use mixup on Mel-spectrograms. Data augmented using the above three techniques are fed to the proposed MTL-AUG framework to learn temporal, frequency, and mixup related changes applied to the data through the augmentation-type classification as auxiliary task. Note that, in SER, it is always important to capture spectro-temporal dynamics to accurately identify speech emotions  [15] ,  [63] ,  [64] . In our proposed framework, we model spectro-temporal and augmentation related dynamics through auxiliary tasks in an MTL setting, which helps improve the performance of the primary emotion classification task. We will explain our proposed MTL-AUG framework next.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Mtl-Aug Framework",
      "text": "where L pri and L aux represent the primary and auxiliary tasks, respectively. λ 1 is a hyper-parameter trading off primary and auxiliary tasks. Our primary task is optimised with an emotion classifier C E that takes the encoded representation (Z) by the encoder (E) network to perform an emotion classification. It uses BLSTM layers for contextual modelling and an attention layer to combine the most salient features given to a dense layer for discriminative feature representation before classification. For a given output sequence h i , utterance level important features are computed by the attention layer using:\n\nwhere α i represents the attention weights that can be computed as follows:\n\nwhere W is a trainable parameter. The output attentive representation R attentive computed by the attention layer is fed to the dense layer for emotion classification. Our intuition of using the attention layer for SER is that the emotional content is distributed over the speech utterances. The attention layer weighs information extracted from different pieces of utterance and combines them into a weighted sum that helps produce better emotion classification performance  [16] . The emotion classifier (C E ) is optimised using the sum of cross-entropy and centre loss functions:\n\nwhere L S and L C represent softmax cross-entropy loss and centre loss, respectively. λ 2 is the trade-off parameter between these two losses. The use of centre loss helps to minimise intra-class variations while maintaining separation between features of different classes by pulling them closer to their correspondence centres. The centre loss function can be defined as:\n\nwhere f (x i ) represents the deep features extracted from the last hidden layer and c yi ∈ R denotes y th i class centre of the deep features.\n\nThe secondary tasks in our framework are augmentation-type classification and reconstruction of the input speech features. In the reconstruction auxiliary task, the encoder and decoder networks minimise the reconstruction loss. The objective function for the autoencoder is:\n\nThe other auxiliary task is to classify the transformation applied to the input. For this, we use classifier C A that takes the encoder E output (Z = E θ (x)) and performs classification. We created augmented data by applying speed perturbation on raw speech, and the SpecAugment and mixup techniques to the Mel-spectrogram of emotional data samples. In augmentation-type classification, we also consider samples with no augmentation as one class. Thus, classifier C A is trained on the four-way classification task of recognising one of the four classes (i. e., speech perturbation, SpecAugment, mixup, and no augmentation).\n\nThe proposed framework is trained in a semi-supervised way as it uses both unsupervised and supervised learning  [65] . For the input X, the encoder network creates the latent code, which is an unsupervised process. The latent code is then used by the classifiers (C A ,C E ) with labels conforming to supervised learning. Note here that when using additional auxiliary data with no labels for emotion, the loss functions for augmentation-type classification and the autoencoding network are only calculated to update the encoder network.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Experimental Setup 4.1 Datasets",
      "text": "To evaluate the performance of our MTL-AUG model, we use three different datasets: IEMOCAP, EMODB, and MSP-IMPROV, which are commonly used for speech emotion classification research  [66] ,  [67] . Both, the IEMOCAP and the MSP-IMPROV datasets are collected by simulating naturalistic dyadic interactions among professional actors and have similar labelling schemes. EMODB contains audio samples in the German language, and we use it for cross-language evaluations. In order to use additional data for auxiliary tasks, we use the Librispeech  [68]  dataset.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Iemocap",
      "text": "This is a multimodal database containing 12 hours of recorded data  [40] . The recordings were collected during dyadic interactions from 10 professional actors (five males and five females). Dyadic interactions allowed the actors to perform spontaneous emotion in contrast to reading text with prototypical emotions  [69] . Each interaction is around five minutes long and segmented into smaller utterances of sentences. Each sentence is annotated by the participant and three annotators for categorical labels. Finally, an utterance is assigned a label if at least three annotators assigned the same label. Overall, this corpus contains nine emotions including angry, disgust, fearful, frustrated, sad, happy, excited, surprised, and neutral. Similar to prior studies  [14] ,  [20] ,  [22] , we use utterances of four categorical emotions, including angry, happy, neutral, and sad in this study by merging \"happy\" and \"excited\" as one emotion class \"happy\". The final dataset includes 5531 utterances (1103 angry, 1708 neutral, 1084 sad, and 1636 happy).",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Msp-Improv",
      "text": "This corpus is a multimodal emotional database recorded from 12 actors performing dyadic interactions  [41] , similar to IEMOCAP  [40] . The utterances in MSP-IMPROV are grouped into six sessions, and each session has recordings of one male, and one female actor. The scenarios were carefully designed to promote naturalness while maintaining control over lexical and emotional contents. The emotional labels were collected through perceptual evaluations using crowdsourcing  [70] . The utterances in this corpus are annotated in four categorical emotions: angry, happy, neutral, and sad. To be consistent with previous studies  [20] ,  [71] , we use all utterances with four emotions: anger (792), sad (885), neutral (3477), and happy (2644).",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Emodb",
      "text": "EMODB  [72]  is a popular and most widely used publicly available emotional dataset in the German Language. This corpus was recorded by the Institute of Communication Science, Technical University Berlin. EMODB contains audio recordings of seven emotions recorded by ten professional speakers in 10 German sentences. In this work, we select four basic emotions: angry, sad, neutral, and happy, to perform categorical cross-language SER as executed in  [73] .",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Librispeech",
      "text": "The LibriSpeech dataset  [68]  contains 1 000 hours of English read speech from 2 484 speakers. This corpus is derived from audiobooks and is commonly used for automatic speaker and speech recognition tasks  [74] ,  [75] . The training portion of LibriSpeech is divided into three subsets, with an approximate recording time of 100, 360, and 500 hours. Here, we choose the subset that contains 100 hours of recordings and use it as additional unlabelled data. These recordings span over 251 speakers.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Demand",
      "text": "We select the Diverse Environments Multichannel Acoustic Noise Database (DEMAND) dataset  [76]  as a source of our noise signal. DEMAND contains audio recordings of various real-world noises recorded in various indoor and outdoor settings. In our experiments, we select noise recordings with 16 kHz sampling rate to match with that of the audio recording of the speech emotion datasets.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Features And Augmentation-Types",
      "text": "We represent the speech utterances in log-Mel spectrograms, which is a popular 2D feature representation widely used for speech-related tasks, including SER. We apply overlapping Hamming windows with a size of 40 ms and with a 10 ms window shift. The height of the log-Mel spectrogram is 128. We set the length of utterances to 7.5 s. Longer utterances are cut at 7.5 s, and smaller utterances are padded with zeros. We select the length of the utterances based on validation results and previous studies  [20] ,  [33] .\n\nAs outlined above, we apply three augmentation-types, including speed perturbation, mixup, and SpecAugment. For the speed perturbation, we create two copies of each training utterance by applying the speed effect at 0.9 and 1.1. We apply speed perturbation on the raw speech using the Sox 1 audio manipulation tool, while we apply mixup and SpecAugment on the Mel spectrogram.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Hyperparameters",
      "text": "For all the experiments, we use the Adam optimiser with default parameters. We start training models with a learning rate of 0.0001 and calculate the validation accuracy at the end of each epoch. If the validation accuracy does not improve after five consecutive epochs, we halve the learning rate and restore the model to the best epoch. This process continues until the learning rate reaches below 0.00001. We apply a rectified linear unit (ReLU) as a non-linear activation function type, as it gave us better performance than leaky ReLU and hyperbolic tangent during validation.\n\nOur baseline model consists of the convolutional encoder network and Bidirectional LSTM (BLSTM)-based classification network. CNN layers in the encoder network produce the high-level feature representations. We use a larger kernel size for the first convolutional layer and reduce the kernel size in the remaining layers, as suggested by previous studies  [77] ,  [78] . Feature representations learnt by the encoder network are given to the BLSTM layer with 128 LSTM units for emotional context modelling. After the BLSTM layer, we apply an attention layer to aggregate the emotional content distributed over the different parts of the given utterance. The attentive features are fed to the fully connected layer with 128 hidden units to produce emotionally discriminative features for a softmax layer. The softmax layer uses the crossentropy loss to produce the posterior class probabilities by enabling the network to learn separable features. In addition, we also exploit the centre loss to reduce the features' intra-class variation to improve the classification performance.\n\nIn contrast to the baseline model, our MTL-AUG model contains two additional components: the decoder and augmentation-type classifier. The decoder network is used to reconstruct the input log-Mel spectrograms back from the encoded output by the encoder network. It has a similar architecture to the encoder, replacing convolutional layers with the transposed convolutional layers. The augmentation-type classifier takes the encoded representation and uses a BLSTM based classifier to classify different augmentation-types. We use one BLSTM layer with 256 LSTM units and two fully connected layers with 128 hidden units for auxiliary task classification. In addition, we use a dropout layer with a dropout rate of 0.3 between two dense layers. We decide on the dropout rate based on validation experiments.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Experiments And Results",
      "text": "All the experiments are performed in a speaker-independent manner. In particular, we follow a easily reproducible leaveone-speaker-out cross-validation scheme commonly used in the literature  [14] ,  [22] . For cross-language SER, we follow  [48] ,  [73]  and use IEMOCAP and EMODB for a four-class emotion classification task. We use LibriSpeech as additional unlabelled data; results are presented in this section as \"MTL-AUG (additional data)\". For all the experiments, we 1. http://sox.sourceforge.net repeated each experiment ten times and calculated the mean and standard deviation. Results are presented using the unweighted average recall rate (UAR), a widely accepted metric in the field.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Within Corpus Experiments",
      "text": "For the within-corpus setting, we compare the performance of the proposed model with the baseline. We also extend our evaluation by comparing the results with different multitask learning approaches  [22] ,  [47] ,  [79]  in Table  2 . Our proposed MTL-AUG achieves better results than the baseline CNN-BLSTM architecture and other MTL approaches. Some studies  [47] ,  [79]  use dimensional emotion prediction as a secondary task to improve the classification of categorical emotions. They use additional information labels annotated by experts for dimensional emotions to perform an auxiliary task in their MTL frameworks. In another MTL study,  [22] , speaker and gender identification are used as secondary tasks for shared generalised representation learning with multitasking semi-supervised adversarial autoencoder (SS-AAE). The authors also exploit the additional unlabelled data for the auxiliary task to boost the primary emotion classification task. However, this model also requires additional labels for speaker and gender and cannot exploit unlabelled data without this meta information. In contrast, we can utilise any speech data in the system without requiring information about the speaker and gender. In Table  2 , we present these results with MTL-AUG (additional data) that performs augmentation-type classification and reconstruction as the auxiliary tasks on the additional speech from LibriSpeech to learn generalised representations. As our proposed auxiliary tasks do not require additional annotation by experts, it makes the MTL training more practical, yet better performing than the existing studies.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Cross-Corpus And Cross-Language Evaluations",
      "text": "",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Cross-Corpus:",
      "text": "In this experiment, we perform a cross-corpus analysis to verify the generalisability of the proposed framework. We trained models on IEMOCAP, and testing is performed on the MSP-IMPROV data. We choose IEMOCAP as training data, since it is more balanced than other corpora. The other reason to select this scheme is for comparison with existing studies, which decided for a similar training  [22] ,  [79] ,  [80] . We select 30 % of the MSP-IMPROV data for parameter TABLE  3 : Cross-corpus evaluation results for emotion recognition.\n\nModel UAR (%) Attentive CNN (MTL)  [79]  45.7 Conditional-GAN (STL)  [80]  45.4 Semi-supervised AAE (MTL)  [22]  46.4±0. selection and 70 % as testing data. The training and testing data are randomly selected. We compare our results with different studies in Table  3 . In  [79] , the authors utilise the representations learnt from unlabelled data and feed it to an attention-based multitask CNN classifier. They show that the classifier's performance can be improved by using the representations from unlabelled data. In  [80] , the authors use the synthetic data generated by a generative adversarial network (GAN) to augment the emotional classifier. They show that augmentation can improve the generalisation that leads to performance improvement. A recent study  [22]  utilised a semi-supervised AAE in an MTL setting to improve the generalisation of SER systems. They use supervised auxiliary tasks, including speaker and gender identification. The authors show that the generalisation of SER systems can be improved by learning the speaker and gender information from the data. In contrast, our proposed MTL-AUG framework learns the generalised representations from the augmented data by learning augmentation-types changes applied to the data. These generalised representations help achieve improved results for cross-corpus SER.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Cross-Language:",
      "text": "We also evaluate the MTL-AUG setup on cross-language SER. For this experiment -as outlined above -we use the IEMOCAP and EMODB corpora. We compare the results with  [48]  for cross-language SER, where the authors used a multitask LSTM model with gender and naturalness as auxiliary tasks. The results of the comparison are presented in Table  4 . We train the models on IEMOCAP (English), and EMODB (German) is used for validation and testing for four class emotion classification. Similar to the crosscorpus experiments, we also achieve improved results for cross-language SER.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Evaluation Of Robustness To Noise",
      "text": "In this experiment, we evaluate the proposed model in noisy conditions. We compare our results with a recent study  [15]  that applies a deep architecture to learn a robust representation and exploits a combination of mixup and speed perturbation data augmentation techniques to achieve improved generalisation. We consider the same settings chosen in  [15]  and train the model on clean data and evaluate on noisy samples. For a fair comparison with  [15] , we select the same three signal-to-noise ratio (SNR) values [0,  10, 20]  and select five noises, including kitchen, park, cafeteria, station, and traffic, from the DEMAND dataset. These noises are randomly added to the testing data at three SNR values [0,  10, 20] . We also implemented models used  [57] ,  [81]  for robust SER to extend our comparison scope. In  [57] , authors use attentive CNN-BLSTM model for robust SER. Similarly, authors in  [81]  use attention based CNN model to perform noise robust SER. Results on the IEMOCAP data are compared with  [15] ,  [57] ,  [81]  and the baseline in Table  5 . In contrast to the deep networks used in  [15] ,  [57] ,  [81]  and baseline, we achieve better results. This shows that the proposed MTL approach enables the MTL-AUG to learn generalised representations, which help achieve robustness to perform SER in noisy conditions. Both \"baseline (+augmentation)\" and the deep DenseNet used in  [15]  are trained in STL setting exploiting the augmented data. We show in Table  6  that training the STL model with augmented data helps improve robustness against noisy conditions; however, these models do not have access to the latent information available in the augmented data. We use this extra information in our proposed MTL-AUG model, where we perform augmentation-type classification as an auxiliary task to exploit the augmented data in the MTL setting.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Adversarial Attacks",
      "text": "In adversarial settings, we choose two adversarial attacks, including the Fast Gradient Sign Method (FGSM)  [82]  and the Basic Iterative Method (BIM)  [83]  to evaluate the robustness of MTL-AUG. FGSM generates adversarial samples by adding a scaled perturbation in the direction of the gradient of the loss function. The BIM attack builds upon the FGSM attack by applying it multiple times iteratively with small instead of applying the adversarial noise in a single step. We apply these two attacks with the perturbation factor = 0.08, and the performance is reported in Table  6 . We compare our results with that of  [15] , where the authors consider the same adversarial attacks. In addition, we also use the implementation of robust models use in  [57] ,  [81]  for eval-uation against adversarial attacks. Comparisons show that we achieve better performance than these existing studies. In  [15] , the authors develop a deep architecture to learn a robust representation. In addition, they utilise speed perturbation and mixup augmentation in the STL setting to achieve generalisation. In contrast, we select augmentationtype classification as an auxiliary task in the MTL scenario. This facilitates generalisation in the network by learning the common representations for both primary and auxiliary tasks.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Selection Of Data Augmentation",
      "text": "In this experiment, we evaluate the model using different schemes in the auxiliary task of augmentation-type classification. We start with single augmentation and perform binary classification (augmented or not augmented) in the auxiliary task using different data augmentation techniques. Results are plotted in Figure  2 , which highlight that the performance of the MTL model with a single augmentationtype in the augmentation-type classifier is poorer than using multiple augmentation-types classification. This shows that giving the model more diverse augmented data helps to learn generalised representations compared to learning to classify single data augmentation.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Size Of Labelled Data",
      "text": "In this experiment, we change the amount of labelled data for training the models, and the results are compared with a semi-supervised AAE (SS-AAE)  [22] . We present the outcomes on IEMOCAP and MSP-IMPROV in Figure  3 . We plot the results with different percentages of labelled training data. The proposed framework improves the SER performance considerably against the baseline CNN-BLSTM. We also compare the results with SS-AAE  [22]  on the SER performance. Results are plotted in Figure  3 , where the red dot shows the performance achieved by SS-AAE  [22]  using 100 % of source data along with the unlabelled data of LibriSpeech. We achieve similar performance using 80-86 % of labelled training data as highlighted by a dotted blue line. This shows that the proposed MTL-AUG effectively learns the emotional representation from augmented data to improve the performance while reducing the required labelled data.  [84]  In this experiment, we validate the necessity and effectiveness of each module integrated with our proposed  framework. Results are presented in Table  7 . This experiment starts with the proposed framework containing all components, including the attention layer, centre loss, auxiliary augmentation-type classifier, and reconstruction decoder. We remove the auxiliary augmentation-type classifier and reconstruction decoder in models 2 and 3. We keep removing different components until we obtain a simple CNN-BLSTM (model 5) classifier without the attention, centre loss, augmentation-type classifier, and reconstruction decoder. We use model 4 as baseline classifier in other sections 5.1-5.6. There is a considerable drop in UAR (%) when one or more modules are removed from the proposed framework. When an STL CNN-BLSTM classifier (module 5) is used, we see a considerable performance drop for both within and cross-corpus SER. This shows that the STL CNN-BLSTM cannot learn better generalisation compared to the MTL framework using the augmentation-type classifier, the reconstruction decoder, or both as auxiliary tasks. This shows that auxiliary tasks promote generalised representations in the network by learning the shared representations. Overall, these ablation experiments show that all the proposed model components are chosen carefully to improve the SER performance effectively.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "Ablation Experiments",
      "text": "",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Conclusions And Outlook",
      "text": "This contribution addressed the open challenge of improving the generalisation of speech emotion recognition (SER) with novel auxiliary tasks that do not require any additional labels for training a multi-task learning (MTL) model. We proposed augmentation-type classification and reconstruction as auxiliary tasks that minimise the required labelled data by effectively utilising the information available in the augmented data and facilitating the utilisation of unlabelled data in a semi-supervised way. The key highlights are as follows:\n\n•\n\nThe multi-task model offers improved withincorpus, cross-corpus, and cross-language emotion classification. It also shows improved generalisation against noisy speech and adversarial attacks. This is due to the proposed auxiliary tasks that helps the model learn shared representations from augmented data.\n\n• Considerable improvements in results were found when additional unlabelled data was incorporated into the proposed MTL semi-supervised framework. This helped the model to regulate the generalised representations by learning from unlabelled data.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "•",
      "text": "We were able to reduce the amount of labelled training data by more than 10 % while achieving a similar performance reported by a recent related study  [22]  using 100 % training data.\n\nFuture work includes exploring multi-model auxiliary tasks to improve the primary task of speech emotion recognition by learning generalised representation.",
      "page_start": 9,
      "page_end": 9
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Illustration of our proposed multitask framework for SER, which uses augmentation-type classiﬁcation and reconstruction",
      "page": 4
    },
    {
      "caption": "Figure 1: describes the proposed semi-supervised MTL ar-",
      "page": 5
    },
    {
      "caption": "Figure 2: , which highlight that the",
      "page": 8
    },
    {
      "caption": "Figure 3: , where the",
      "page": 8
    },
    {
      "caption": "Figure 2: Results using single augmentation in the auxiliary task of augmentation-type classiﬁcation vs all.",
      "page": 9
    },
    {
      "caption": "Figure 3: Results for SER (UAR %) with changing the amount of",
      "page": 9
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Paper/Author (Year)": "",
          "Label dependent auxiliary tasks": "",
          "Label independent auxiliary tasks": "Reconstruction",
          "Evaluations": "within-\ncorpus"
        },
        {
          "Paper/Author (Year)": "Prthasarathy and\nBusso [31] (2017)",
          "Label dependent auxiliary tasks": "emotional attributes prediction",
          "Label independent auxiliary tasks": "(cid:55)",
          "Evaluations": ""
        },
        {
          "Paper/Author (Year)": "Xia et al. [30] (2017)",
          "Label dependent auxiliary tasks": "emotional attributes prediction",
          "Label independent auxiliary tasks": "(cid:55)",
          "Evaluations": ""
        },
        {
          "Paper/Author (Year)": "Kim et al. [48] (2017)",
          "Label dependent auxiliary tasks": "emotional attributes prediction +\ngender identiﬁcation",
          "Label independent auxiliary tasks": "(cid:55)",
          "Evaluations": ""
        },
        {
          "Paper/Author (Year)": "Lotﬁan et al. [35] (2018)",
          "Label dependent auxiliary tasks": "emotional attributes classiﬁcation",
          "Label independent auxiliary tasks": "(cid:55)",
          "Evaluations": ""
        },
        {
          "Paper/Author (Year)": "Tao et al. [39] (2018)",
          "Label dependent auxiliary tasks": "speaker classiﬁcation +\ngender classiﬁcation",
          "Label independent auxiliary tasks": "(cid:55)",
          "Evaluations": ""
        },
        {
          "Paper/Author (Year)": "Li et al. [33] (2019)",
          "Label dependent auxiliary tasks": "gender identiﬁcation",
          "Label independent auxiliary tasks": "(cid:55)",
          "Evaluations": ""
        },
        {
          "Paper/Author (Year)": "Prthasarathy and\nBusso [27] (2020)",
          "Label dependent auxiliary tasks": "emotional attributes prediction",
          "Label independent auxiliary tasks": "",
          "Evaluations": ""
        },
        {
          "Paper/Author (Year)": "Latif et al. [22] (2020)",
          "Label dependent auxiliary tasks": "speaker classiﬁcation +\ngender classiﬁcation",
          "Label independent auxiliary tasks": "",
          "Evaluations": ""
        },
        {
          "Paper/Author (Year)": "Peri et al. [34] (2021)",
          "Label dependent auxiliary tasks": "speaker identiﬁcation",
          "Label independent auxiliary tasks": "(cid:55)",
          "Evaluations": ""
        },
        {
          "Paper/Author (Year)": "Our Paper (2022)",
          "Label dependent auxiliary tasks": "None",
          "Label independent auxiliary tasks": "",
          "Evaluations": ""
        }
      ],
      "page": 3
    },
    {
      "caption": "Table 2: , we learningthespeakerandgenderinformationfromthedata.",
      "data": [
        {
          "Model": "Attentive CNN (MTL) [79]",
          "UAR (%)": "45.7"
        },
        {
          "Model": "Conditional-GAN (STL) [80]",
          "UAR (%)": "45.4"
        },
        {
          "Model": "Semi-supervised AAE (MTL) [22]",
          "UAR (%)": "46.4±0.32"
        },
        {
          "Model": "CNN-BLSTM (STL)(baseline)",
          "UAR (%)": "45.4±0.83"
        },
        {
          "Model": "CNN-BLSTM (STL)(baseline) (+ augmentations)",
          "UAR (%)": "46.2±1.3"
        },
        {
          "Model": "MTL-AUG",
          "UAR (%)": "47.2±0.41"
        },
        {
          "Model": "MTL-AUG (additional data)",
          "UAR (%)": "48.1±0.30"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table 2: , we learningthespeakerandgenderinformationfromthedata.",
      "data": [
        {
          "Model": "DBN (MTL) [47]",
          "IEMOCAP": "62.2",
          "MSP IMPROV": "-"
        },
        {
          "Model": "Attentive CNN (MTL) [79]",
          "IEMOCAP": "60.15",
          "MSP IMPROV": "-"
        },
        {
          "Model": "CNN (MTL) [22]",
          "IEMOCAP": "65.6±2.0",
          "MSP IMPROV": "59.5±2.4"
        },
        {
          "Model": "Semi-supervised AAE (MTL) [22]",
          "IEMOCAP": "66.7±1.4",
          "MSP IMPROV": "60.3±1.1"
        },
        {
          "Model": "CNN-BLSTM(baseline) (STL)",
          "IEMOCAP": "64.3±1.9",
          "MSP IMPROV": "57.2±2.1"
        },
        {
          "Model": "CNN-BLSTM(baseline)\n(STL+augmentations)",
          "IEMOCAP": "65.1±1.8",
          "MSP IMPROV": "58.5±1.7"
        },
        {
          "Model": "MTL-AUG",
          "IEMOCAP": "68.1±1.5",
          "MSP IMPROV": "61.4± 0.9"
        },
        {
          "Model": "MTL-AUG (additional data)",
          "IEMOCAP": "68.7±1.3",
          "MSP IMPROV": "62.1± 1.2"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table 2: , we learningthespeakerandgenderinformationfromthedata.",
      "data": [
        {
          "Model": "MTL-LSTM [48]",
          "IEMOCAP (English)\nto EMODB (German)": "43.4±1.8",
          "EMODB (German)\nto IEMOCAP (English)": "39.1±1.6"
        },
        {
          "Model": "CNN-BLSTM (STL)(baseline)",
          "IEMOCAP (English)\nto EMODB (German)": "42.1± 1.9",
          "EMODB (German)\nto IEMOCAP (English)": "38.4± 1.8"
        },
        {
          "Model": "CNN-BLSTM (STL)(baseline)\n(+ augmentations)",
          "IEMOCAP (English)\nto EMODB (German)": "43.6±1.5",
          "EMODB (German)\nto IEMOCAP (English)": "39.5± 1.7"
        },
        {
          "Model": "MTL-AUG",
          "IEMOCAP (English)\nto EMODB (German)": "45.7±1.3",
          "EMODB (German)\nto IEMOCAP (English)": "42.1±1.6"
        },
        {
          "Model": "MTL-AUG (additional data)",
          "IEMOCAP (English)\nto EMODB (German)": "46.8±1.4",
          "EMODB (German)\nto IEMOCAP (English)": "41.5± 1.6"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table 6: that training the STL model with augmented",
      "data": [
        {
          "Model": "",
          "Adversarial Attacks": "FSGM"
        },
        {
          "Model": "DenseNet (STL) (+augmentations) [15]",
          "Adversarial Attacks": "44.0± 1.1"
        },
        {
          "Model": "CNN-BLSTM +attention (STL) [57]",
          "Adversarial Attacks": "43.8± 1.5"
        },
        {
          "Model": "CNN +attention (STL) [81]",
          "Adversarial Attacks": "42.7± 1.7"
        },
        {
          "Model": "CNN-BLSTM (STL) (baseline)",
          "Adversarial Attacks": "42.5±1.5"
        },
        {
          "Model": "CNN-BLSTM (STL) (baseline)\n(+ augmentations)",
          "Adversarial Attacks": "44.6±1.4"
        },
        {
          "Model": "MTL-AUG",
          "Adversarial Attacks": "46.2±1.2"
        },
        {
          "Model": "MTL-AUG (additional data)",
          "Adversarial Attacks": "47.5±1.0"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table 6: that training the STL model with augmented",
      "data": [
        {
          "Model": "",
          "UAR (%)": "0 dB"
        },
        {
          "Model": "DenseNet (STL)\n(+augmentations) [15]",
          "UAR (%)": "34.2± 1.2"
        },
        {
          "Model": "CNN-BLSTM +attention (STL)\n[57]",
          "UAR (%)": "34.0±1.5"
        },
        {
          "Model": "CNN +attention (STL) [81]",
          "UAR (%)": "33.4±1.8"
        },
        {
          "Model": "CNN-BLSTM (STL) (baseline)",
          "UAR (%)": "33.5±1.5"
        },
        {
          "Model": "CNN-BLSTM (STL)(baseline)\n(+ augmentations)",
          "UAR (%)": "34.8±1.3"
        },
        {
          "Model": "MTL-AUG",
          "UAR (%)": "37.5±1.0"
        },
        {
          "Model": "MTL-AUG (additional data)",
          "UAR (%)": "39.1±1.3"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Model": "",
          "Conﬁguration": "",
          "Auxiliary tasks": "Augmentation-\ntype classiﬁer",
          "Centre loss": "",
          "Attention": "",
          "Within corpus\nUAR(%)": "IEMOCAP",
          "Cross-corpus\nUAR(%)": ""
        },
        {
          "Model": "1",
          "Conﬁguration": "",
          "Auxiliary tasks": "",
          "Centre loss": "",
          "Attention": "",
          "Within corpus\nUAR(%)": "68.1±1.5",
          "Cross-corpus\nUAR(%)": "47.2±0.41"
        },
        {
          "Model": "2",
          "Conﬁguration": "",
          "Auxiliary tasks": "(cid:55)",
          "Centre loss": "",
          "Attention": "",
          "Within corpus\nUAR(%)": "66.7±1.5",
          "Cross-corpus\nUAR(%)": "46.2±0.81"
        },
        {
          "Model": "3",
          "Conﬁguration": "",
          "Auxiliary tasks": "(cid:55)",
          "Centre loss": "",
          "Attention": "",
          "Within corpus\nUAR(%)": "65.1±1.7",
          "Cross-corpus\nUAR(%)": "45.8±1.0"
        },
        {
          "Model": "4",
          "Conﬁguration": "",
          "Auxiliary tasks": "(cid:55)",
          "Centre loss": "(cid:55)",
          "Attention": "",
          "Within corpus\nUAR(%)": "64.3±1.9",
          "Cross-corpus\nUAR(%)": "45.4±1.2"
        },
        {
          "Model": "5",
          "Conﬁguration": "",
          "Auxiliary tasks": "(cid:55)",
          "Centre loss": "(cid:55)",
          "Attention": "(cid:55)",
          "Within corpus\nUAR(%)": "62.8±2.1",
          "Cross-corpus\nUAR(%)": "43.6±1.5"
        }
      ],
      "page": 10
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Speech technology for healthcare: Opportunities, challenges, and state of the art",
      "authors": [
        "S Latif",
        "J Qadir",
        "A Qayyum",
        "M Usama",
        "S Younis"
      ],
      "year": "2020",
      "venue": "IEEE Reviews in Biomedical Engineering"
    },
    {
      "citation_id": "2",
      "title": "Automated screening for distress: A perspective for the future",
      "authors": [
        "R Rana",
        "S Latif",
        "R Gururajan",
        "A Gray",
        "G Mackenzie",
        "G Humphris",
        "J Dunn"
      ],
      "year": "2019",
      "venue": "European journal of cancer care"
    },
    {
      "citation_id": "3",
      "title": "Driver emotion recognition for intelligent vehicles: A survey",
      "authors": [
        "S Zepf",
        "J Hernandez",
        "A Schmitt",
        "W Minker",
        "R Picard"
      ],
      "year": "2020",
      "venue": "ACM Computing Surveys (CSUR)"
    },
    {
      "citation_id": "4",
      "title": "Prosodic cues of speech under stress: phonetic exploration of finnish emergency calls",
      "authors": [
        "L Tavi"
      ],
      "year": "2020",
      "venue": "Itä-Suomen yliopisto"
    },
    {
      "citation_id": "5",
      "title": "Affective computing in education: A systematic review and future research",
      "authors": [
        "E Yadegaridehkordi",
        "N Noor",
        "M Ayub",
        "H Affal",
        "N Hussin"
      ],
      "year": "2019",
      "venue": "Computers & Education"
    },
    {
      "citation_id": "6",
      "title": "Improved soccer action spotting using both audio and video streams",
      "authors": [
        "B Vanderplaetse",
        "S Dupont"
      ],
      "year": "2020",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops"
    },
    {
      "citation_id": "7",
      "title": "You're not you when you're angry: Robust emotion features emerge by recognizing speakers",
      "authors": [
        "Z Aldeneh",
        "E Provost"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "8",
      "title": "Multi-head attention for speech emotion recognition with auxiliary learning of gender recognition",
      "authors": [
        "A Nediyanchath",
        "P Paramasivam",
        "P Yenigalla"
      ],
      "year": "2020",
      "venue": "ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "9",
      "title": "Learning utterance-level representations for speech emotion and age/gender recognition using deep neural networks",
      "authors": [
        "Z.-Q Wang",
        "I Tashev"
      ],
      "year": "2017",
      "venue": "2017 IEEE international conference on acoustics, speech and signal processing (ICASSP)"
    },
    {
      "citation_id": "10",
      "title": "Cross lingual speech emotion recognition: Urdu vs. western languages",
      "authors": [
        "S Latif",
        "A Qayyum",
        "M Usman",
        "J Qadir"
      ],
      "year": "2018",
      "venue": "2018 International Conference on Frontiers of Information Technology (FIT)"
    },
    {
      "citation_id": "11",
      "title": "Evidence for cultural dialects in vocal emotion expression: acoustic classification within and across five nations",
      "authors": [
        "P Laukka",
        "D Neiberg",
        "H Elfenbein"
      ],
      "year": "2014",
      "venue": "Emotion"
    },
    {
      "citation_id": "12",
      "title": "A fast learning algorithm for deep belief nets",
      "authors": [
        "G Hinton",
        "S Osindero",
        "Y.-W Teh"
      ],
      "year": "2006",
      "venue": "Neural computation"
    },
    {
      "citation_id": "13",
      "title": "Handwritten digit recognition with a backpropagation network",
      "authors": [
        "Y Lecun",
        "B Boser",
        "J Denker",
        "D Henderson",
        "R Howard",
        "W Hubbard",
        "L Jackel"
      ],
      "year": "1989",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "14",
      "title": "Variational autoencoders for learning latent representations of speech emotion: A preliminary study",
      "authors": [
        "S Latif",
        "R Rana",
        "J Qadir",
        "J Epps"
      ],
      "year": "2018",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "15",
      "title": "Deep architecture enhancing robustness to noise, adversarial attacks, and cross-corpus setting for speech emotion recognition",
      "authors": [
        "S Latif",
        "R Rana",
        "S Khalifa",
        "R Jurdak",
        "B Schuller"
      ],
      "year": "2020",
      "venue": "Proc. Interspeech 2020"
    },
    {
      "citation_id": "16",
      "title": "Attentive convolutional neural network based speech emotion recognition: A study on the impact of input features, signal length, and acted speech",
      "authors": [
        "M Neumann",
        "N Vu"
      ],
      "year": "2017",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "17",
      "title": "Survey of deep representation learning for speech emotion recognition",
      "authors": [
        "S Latif",
        "R Rana",
        "S Khalifa",
        "R Jurdak",
        "J Qadir",
        "B Schuller"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "18",
      "title": "Long short-term memory",
      "authors": [
        "S Hochreiter",
        "J Schmidhuber"
      ],
      "year": "1997",
      "venue": "Neural computation"
    },
    {
      "citation_id": "19",
      "title": "Attention guided 3d cnn-lstm model for accurate speech based emotion recognition",
      "authors": [
        "O Atila"
      ],
      "year": "2021",
      "venue": "Applied Acoustics"
    },
    {
      "citation_id": "20",
      "title": "Direct Modelling of Speech Emotion from Raw Speech",
      "authors": [
        "S Latif",
        "R Rana",
        "S Khalifa",
        "R Jurdak",
        "J Epps"
      ],
      "year": "2019",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "21",
      "title": "End-to-end speech emotion recognition using deep neural networks",
      "authors": [
        "P Tzirakis",
        "J Zhang",
        "B Schuller"
      ],
      "year": "2018",
      "venue": "2018 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "22",
      "title": "Multi-task semi-supervised adversarial autoencoding for speech emotion recognition",
      "authors": [
        "S Latif",
        "R Rana",
        "S Khalifa",
        "R Jurdak",
        "J Epps",
        "B Schuller"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "23",
      "title": "Attention-augmented end-to-end multi-task learning for emotion prediction from speech",
      "authors": [
        "Z Zhang",
        "B Wu",
        "B Schuller"
      ],
      "year": "2019",
      "venue": "ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "24",
      "title": "Deep representation learning in speech processing: Challenges, recent advances, and future trends",
      "authors": [
        "S Latif",
        "R Rana",
        "S Khalifa",
        "R Jurdak",
        "J Qadir",
        "B Schuller"
      ],
      "year": "2020",
      "venue": "Deep representation learning in speech processing: Challenges, recent advances, and future trends",
      "arxiv": "arXiv:2001.00378"
    },
    {
      "citation_id": "25",
      "title": "Automatic speech recognition: a survey",
      "authors": [
        "M Malik",
        "M Malik",
        "K Mehmood",
        "I Makhdoom"
      ],
      "year": "2021",
      "venue": "Multimedia Tools and Applications"
    },
    {
      "citation_id": "26",
      "title": "A survey on deep reinforcement learning for audiobased applications",
      "authors": [
        "S Latif",
        "H Cuayáhuitl",
        "F Pervez",
        "F Shamshad",
        "H Ali",
        "E Cambria"
      ],
      "year": "2022",
      "venue": "Artificial Intelligence Review"
    },
    {
      "citation_id": "27",
      "title": "Semi-supervised speech emotion recognition with ladder networks",
      "authors": [
        "S Parthasarathy",
        "C Busso"
      ],
      "year": "2020",
      "venue": "IEEE/ACM transactions on audio, speech, and language processing"
    },
    {
      "citation_id": "28",
      "title": "Transfer learning for improving speech emotion classification accuracy",
      "authors": [
        "S Latif",
        "R Rana",
        "S Younis",
        "J Qadir",
        "J Epps"
      ],
      "year": "2018",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "29",
      "title": "Multitask learning",
      "authors": [
        "R Caruana"
      ],
      "year": "1997",
      "venue": "Machine learning"
    },
    {
      "citation_id": "30",
      "title": "A multi-task learning framework for emotion recognition using 2d continuous space",
      "authors": [
        "R Xia",
        "Y Liu"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "31",
      "title": "Jointly predicting arousal, valence and dominance with multi-task learning",
      "authors": [
        "S Parthasarathy",
        "C Busso"
      ],
      "year": "2017",
      "venue": "Jointly predicting arousal, valence and dominance with multi-task learning"
    },
    {
      "citation_id": "32",
      "title": "Multi-modal emotion recognition using semi-supervised learning and multiple neural networks in the wild",
      "authors": [
        "D Kim",
        "M Lee",
        "D Choi",
        "B Song"
      ],
      "year": "2017",
      "venue": "Proceedings of the 19th ACM International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "33",
      "title": "Improved end-to-end speech emotion recognition using self attention mechanism and multitask learning",
      "authors": [
        "Y Li",
        "T Zhao",
        "T Kawahara"
      ],
      "year": "2019",
      "venue": "Interspeech"
    },
    {
      "citation_id": "34",
      "title": "Disentanglement for audio-visual emotion recognition using multitask setup",
      "authors": [
        "R Peri",
        "S Parthasarathy",
        "C Bradshaw",
        "S Sundaram"
      ],
      "year": "2021",
      "venue": "ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "35",
      "title": "Predicting categorical emotions by jointly learning primary and secondary emotions through multitask learning",
      "authors": [
        "R Lotfian",
        "C Busso"
      ],
      "year": "2018",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "36",
      "title": "Privacy enhanced speech emotion communication using deep learning aided edge computing",
      "authors": [
        "H Ali",
        "F Hassan",
        "S Latif",
        "H Manzoor",
        "J Qadir"
      ],
      "year": "2021",
      "venue": "2021 IEEE International Conference on Communications Workshops (ICC Workshops)"
    },
    {
      "citation_id": "37",
      "title": "Privacy enhanced multimodal neural representations for emotion recognition",
      "authors": [
        "E Provost"
      ],
      "year": "2020",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "38",
      "title": "Unsupervised representation learning by predicting image rotations",
      "authors": [
        "S Gidaris",
        "P Singh",
        "N Komodakis"
      ],
      "year": "2018",
      "venue": "International Conference on Learning Representations"
    },
    {
      "citation_id": "39",
      "title": "Advanced lstm: A study about better time dependency modeling in emotion recognition",
      "authors": [
        "F Tao",
        "G Liu"
      ],
      "year": "2018",
      "venue": "2018 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "40",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "41",
      "title": "Msp-improv: An acted corpus of dyadic interactions to study emotion perception",
      "authors": [
        "C Busso",
        "S Parthasarathy",
        "A Burmania",
        "M Abdelwahab",
        "N Sadoughi",
        "E Provost"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "42",
      "title": "Heterogeneous multi-task learning for human pose estimation with deep convolutional neural network",
      "authors": [
        "S Li",
        "Z.-Q Liu",
        "A Chan"
      ],
      "year": "2014",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition workshops"
    },
    {
      "citation_id": "43",
      "title": "Facial landmark detection by deep multi-task learning",
      "authors": [
        "Z Zhang",
        "P Luo",
        "C Loy",
        "X Tang"
      ],
      "year": "2014",
      "venue": "European Conference on Computer Vision"
    },
    {
      "citation_id": "44",
      "title": "Learning hierarchical features for scene labeling",
      "authors": [
        "C Farabet",
        "C Couprie",
        "L Najman",
        "Y Lecun"
      ],
      "year": "2013",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "45",
      "title": "Multi-task self-supervised learning for robust speech recognition",
      "authors": [
        "M Ravanelli",
        "J Zhong",
        "S Pascual",
        "P Swietojanski",
        "J Monteiro",
        "J Trmal",
        "Y Bengio"
      ],
      "year": "2020",
      "venue": "ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "46",
      "title": "Multi-task learning for textdependent speaker verification",
      "authors": [
        "N Chen",
        "Y Qian",
        "K Yu"
      ],
      "year": "2015",
      "venue": "Sixteenth annual conference of the international speech communication association"
    },
    {
      "citation_id": "47",
      "title": "A multi-task learning framework for emotion recognition using 2d continuous space",
      "authors": [
        "R Xia",
        "Y Liu"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on affective computing"
    },
    {
      "citation_id": "48",
      "title": "Towards speech emotion recognition \"in the wild\" using aggregated corpora and deep multi-task learning",
      "authors": [
        "J Kim",
        "G Englebienne",
        "K Truong",
        "V Evers"
      ],
      "year": "2017",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "49",
      "title": "A multitask approach to continuous five-dimensional affect sensing in natural speech",
      "authors": [
        "F Eyben",
        "B Schuller"
      ],
      "year": "2012",
      "venue": "ACM Transactions on Interactive Intelligent Systems (TiiS)"
    },
    {
      "citation_id": "50",
      "title": "Speech emotion recognition via attention-based dnn from multitask learning",
      "authors": [
        "F Ma",
        "W Gu",
        "W Zhang",
        "S Ni",
        "S.-L Huang",
        "L Zhang"
      ],
      "year": "2018",
      "venue": "Proceedings of the 16th ACM Conference on Embedded Networked Sensor Systems"
    },
    {
      "citation_id": "51",
      "title": "Using regional saliency for speech emotion recognition",
      "authors": [
        "Z Aldeneh",
        "E Provost"
      ],
      "year": "2017",
      "venue": "2017 IEEE international conference on acoustics, speech and signal processing (ICASSP)"
    },
    {
      "citation_id": "52",
      "title": "Audio augmentation for speech recognition",
      "authors": [
        "T Ko",
        "V Peddinti",
        "D Povey",
        "S Khudanpur"
      ],
      "year": "2015",
      "venue": "Sixteenth Annual Conference of the International Speech Communication Association"
    },
    {
      "citation_id": "53",
      "title": "International Conference on Learning Representations",
      "authors": [
        "H Zhang",
        "M Cisse",
        "Y Dauphin",
        "D Lopez-Paz"
      ],
      "year": "2018",
      "venue": "International Conference on Learning Representations"
    },
    {
      "citation_id": "54",
      "title": "Specaugment: A simple data augmentation method for automatic speech recognition",
      "authors": [
        "D Park",
        "W Chan",
        "Y Zhang",
        "C.-C Chiu",
        "B Zoph",
        "E Cubuk",
        "Q Le"
      ],
      "year": "2019",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "55",
      "title": "Emotion recognition in public speaking scenarios utilising an lstm-rnn approach with attention",
      "authors": [
        "A Baird",
        "S Amiriparian",
        "M Milling",
        "B Schuller"
      ],
      "year": "2021",
      "venue": "2021 IEEE Spoken Language Technology Workshop (SLT)"
    },
    {
      "citation_id": "56",
      "title": "Federated learning for speech emotion recognition applications",
      "authors": [
        "S Latif",
        "S Khalifa",
        "R Rana",
        "R Jurdak"
      ],
      "year": "2020",
      "venue": "2020 19th ACM/IEEE International Conference on Information Processing in Sensor Networks (IPSN)"
    },
    {
      "citation_id": "57",
      "title": "Deep convolutional recurrent neural network with attention mechanism for robust speech emotion recognition",
      "authors": [
        "C.-W Huang",
        "S Narayanan"
      ],
      "year": "2017",
      "venue": "2017 IEEE international conference on multimedia and expo (ICME)"
    },
    {
      "citation_id": "58",
      "title": "Towards robust speech emotion recognition using deep residual networks for speech enhancement",
      "authors": [
        "A Triantafyllopoulos",
        "G Keren",
        "J Wagner",
        "I Steiner",
        "B Schuller"
      ],
      "year": "2019",
      "venue": "Towards robust speech emotion recognition using deep residual networks for speech enhancement"
    },
    {
      "citation_id": "59",
      "title": "Front-end feature compensation for noise robust speech emotion recognition",
      "authors": [
        "M Pandharipande",
        "R Chakraborty",
        "A Panda",
        "B Das",
        "S Kopparapu"
      ],
      "year": "2019",
      "venue": "2019 27th European Signal Processing Conference"
    },
    {
      "citation_id": "60",
      "title": "Improving noise robustness of speech emotion recognition system",
      "authors": [
        "Ł Juszkiewicz"
      ],
      "year": "2014",
      "venue": "Intelligent Distributed Computing VII"
    },
    {
      "citation_id": "61",
      "title": "mixup: Beyond empirical risk minimization",
      "authors": [
        "H Zhang",
        "M Cisse",
        "Y Dauphin",
        "D Lopez-Paz"
      ],
      "year": "2018",
      "venue": "mixup: Beyond empirical risk minimization"
    },
    {
      "citation_id": "62",
      "title": "Mixup inference: Better exploiting mixup to defend adversarial attacks",
      "authors": [
        "T Pang",
        "K Xu",
        "J Zhu"
      ],
      "year": "2019",
      "venue": "International Conference on Learning Representations"
    },
    {
      "citation_id": "63",
      "title": "Learning spectro-temporal features with 3d cnns for speech emotion recognition",
      "authors": [
        "J Kim",
        "K Truong",
        "G Englebienne",
        "V Evers"
      ],
      "year": "2017",
      "venue": "2017 Seventh International Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "64",
      "title": "Augmenting generative adversarial networks for speech emotion recognition",
      "authors": [
        "S Latif",
        "M Asim",
        "R Rana",
        "S Khalifa",
        "R Jurdak",
        "B Schuller"
      ],
      "year": "2020",
      "venue": "Augmenting generative adversarial networks for speech emotion recognition",
      "arxiv": "arXiv:2005.08447"
    },
    {
      "citation_id": "65",
      "title": "Semisupervised autoencoders for speech emotion recognition",
      "authors": [
        "J Deng",
        "X Xu",
        "Z Zhang",
        "S Fr",
        "B Schuller"
      ],
      "year": "2017",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "66",
      "title": "Retrieving categorical emotions using a probabilistic framework to define preference learning samples",
      "authors": [
        "R Lotfian",
        "C Busso"
      ],
      "year": "2016",
      "venue": "Retrieving categorical emotions using a probabilistic framework to define preference learning samples"
    },
    {
      "citation_id": "67",
      "title": "Emotion spotting: Discovering regions of evidence in audio-visual emotion expressions",
      "authors": [
        "Y Kim",
        "E Provost"
      ],
      "year": "2016",
      "venue": "Proceedings of the 18th ACM International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "68",
      "title": "Librispeech: an asr corpus based on public domain audio books",
      "authors": [
        "V Panayotov",
        "G Chen",
        "D Povey",
        "S Khudanpur"
      ],
      "year": "2015",
      "venue": "2015 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "69",
      "title": "Building naturalistic emotionally balanced speech corpus by retrieving emotional speech from existing podcast recordings",
      "authors": [
        "R Lotfian",
        "C Busso"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "70",
      "title": "Increasing the reliability of crowdsourcing evaluations using online quality assessment",
      "authors": [
        "A Burmania",
        "S Parthasarathy",
        "C Busso"
      ],
      "year": "2016",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "71",
      "title": "Progressive neural networks for transfer learning in emotion recognition",
      "authors": [
        "J Gideon",
        "S Khorram",
        "Z Aldeneh",
        "D Dimitriadis",
        "E Provost"
      ],
      "year": "2017",
      "venue": "Progressive neural networks for transfer learning in emotion recognition",
      "arxiv": "arXiv:1706.03256"
    },
    {
      "citation_id": "72",
      "title": "A database of german emotional speech",
      "authors": [
        "F Burkhardt",
        "A Paeschke",
        "M Rolfes",
        "W Sendlmeier",
        "B Weiss"
      ],
      "year": "2005",
      "venue": "Interspeech"
    },
    {
      "citation_id": "73",
      "title": "Self supervised adversarial domain adaptation for cross-corpus and cross-language speech emotion recognition",
      "authors": [
        "S Latif",
        "R Rana",
        "S Khalifa",
        "R Jurdak",
        "B Schuller"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "74",
      "title": "Endto-end automatic speech translation of audiobooks",
      "authors": [
        "A Bérard",
        "L Besacier",
        "A Kocabiyikoglu",
        "O Pietquin"
      ],
      "year": "2018",
      "venue": "2018 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "75",
      "title": "Transfer learning using raw waveform sincnet for robust speaker diarization",
      "authors": [
        "H Dubey",
        "A Sangwan",
        "J Hansen"
      ],
      "year": "2019",
      "venue": "ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "76",
      "title": "The diverse environments multi-channel acoustic noise database (demand): A database of multichannel environmental noise recordings",
      "authors": [
        "J Thiemann",
        "N Ito",
        "E Vincent"
      ],
      "year": "2013",
      "venue": "Proceedings of Meetings on Acoustics ICA2013"
    },
    {
      "citation_id": "77",
      "title": "Learning discriminative features from spectrograms using center loss for speech emotion recognition",
      "authors": [
        "D Dai",
        "Z Wu",
        "R Li",
        "X Wu",
        "J Jia",
        "H Meng"
      ],
      "year": "2019",
      "venue": "ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "78",
      "title": "Improving crosscorpus speech emotion recognition with adversarial discriminative domain generalization (addog)",
      "authors": [
        "J Gideon",
        "M Mcinnis",
        "E Provost"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "79",
      "title": "Improving speech emotion recognition with unsupervised representation learning on unlabeled speech",
      "authors": [
        "M Neumann",
        "N Vu"
      ],
      "year": "2019",
      "venue": "ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "80",
      "title": "On enhancing speech emotion recognition using generative adversarial networks",
      "authors": [
        "S Sahu",
        "R Gupta",
        "C Espy-Wilson"
      ],
      "year": "2018",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "81",
      "title": "Robustness to noise for speech emotion classification using cnns and attention mechanisms",
      "authors": [
        "L Wijayasingha",
        "J Stankovic"
      ],
      "year": "2021",
      "venue": "Smart Health"
    },
    {
      "citation_id": "82",
      "title": "Explaining and harnessing adversarial examples",
      "authors": [
        "I Goodfellow",
        "J Shlens",
        "C Szegedy"
      ],
      "year": "2015",
      "venue": "3rd International Conference on Learning Representations, ICLR 2015"
    },
    {
      "citation_id": "83",
      "title": "Adversarial examples in the physical world",
      "authors": [
        "A Kurakin",
        "I Goodfellow",
        "S Bengio"
      ],
      "year": "2017",
      "venue": "5th International Conference on Learning Representations"
    },
    {
      "citation_id": "84",
      "title": "Non-uniform compressive sensing in wireless sensor networks: Feasibility and application",
      "authors": [
        "Y Shen",
        "W Hu",
        "R Rana",
        "C Chou"
      ],
      "year": "2011",
      "venue": "2011 Seventh International Conference on Intelligent Sensors, Sensor Networks and Information Processing"
    }
  ]
}