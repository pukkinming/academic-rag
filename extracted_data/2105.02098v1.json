{
  "paper_id": "2105.02098v1",
  "title": "Does The Goal Matter? Emotion Recognition Tasks Can Change The Social Value Of Facial Mimicry Towards Artificial Agents",
  "published": "2021-05-05T14:58:04Z",
  "authors": [
    "Giulia Perugia",
    "Maike Paetzel-Prüssman",
    "Isabelle Hupont",
    "Giovanna Varni",
    "Mohamed Chetouani",
    "Christopher Edward Peters",
    "Ginevra Castellano"
  ],
  "keywords": [
    "Human-Robot Interaction",
    "Human-Agent Interaction",
    "Affective Computing",
    "Facial Mimicry",
    "Anthropomorphism",
    "Uncanny Valley",
    "Facial Action Coding System"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "In this paper, we present a study aimed at understanding whether the embodiment and humanlikeness of an artificial agent can affect people's spontaneous and instructed mimicry of its facial expressions. The study followed a mixed experimental design and revolved around an emotion recognition task. Participants were randomly assigned to one level of humanlikeness (between-subject variable: humanlike, characterlike, or morph facial texture of the artificial agents) and observed the facial expressions displayed by a human (control) and three artificial agents differing in embodiment (within-subject variable: video-recorded robot, physical robot, and virtual agent). To study both spontaneous and instructed facial mimicry, we divided the experimental sessions into two phases. In the first phase, we asked participants to observe and recognize the emotions displayed by the agents. In the second phase, we asked them to look at the agents' facial expressions, replicate their dynamics as closely as possible, and then identify the observed emotions. In both cases, we assessed participants' facial expressions with an automated Action Unit (AU) intensity detector. Contrary to our hypotheses, our results disclose that the agent that was perceived as the least uncanny, and most anthropomorphic, likable, and co-present, was the one spontaneously mimicked the least. Moreover, they show that instructed facial mimicry negatively predicts spontaneous facial mimicry. Further exploratory analyses revealed that spontaneous facial mimicry appeared when participants were less certain of the emotion they recognized. Hence, we postulate that an emotion recognition goal can flip the social value of facial mimicry as it transforms a likable artificial agent into a distractor. Further work is needed to corroborate this hypothesis. Nevertheless, our findings shed light on the functioning of human-agent and human-robot mimicry in emotion recognition tasks and help us to unravel the relationship between facial mimicry, liking, and rapport.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "The success of artificial agents in areas like healthcare, personal assistance, and education highly depends on whether people perceive them as likable and pleasant to interact with. In the lab, people's perceptions of an artificial agent can be easily measured with questionnaires and interviews. In real-life settings, instead, the artificial agent is on its own and the explicit evaluation of the interaction is not always feasible. In fact, in these contexts, people might skip the proposed surveys or reply carelessly due to lack of time and interest  [9] . A more promising approach in such contexts may be the use of behavioral measures. While behavioral measures are in general extensively used in Human-Agent and Human-Robot Interaction (HAI and HRI), they are seldomly linked to people's self-reported perceptions (e.g., likability and engagement, see  [62, 64] ). In this paper, we focus on facial mimicry -the mirroring of another person's facial expressions  [26]  -as a target behavioral cue. In particular, we are interested in understanding whether humans mimic the facial expressions of the six basic emotions displayed by artificial agents, and how this can be linked to their perceptions of the agents.\n\nFrom psychology, we know that facial mimicry increases with rapport  [32, 71] , but also appears in first acquaintances between individuals as a sign of liking  [8, 43] . Studies on facial mimicry in HAI and HRI have so far mostly focused on whether artificial agents are liked better when given the ability to mimic a human interaction partner  [35, 57, 68] . Hofree et al.  [36]  were among the only researchers who investigated whether human interaction partners mimic the facial expressions of artificial agents as well. In their study, they disclosed that people's mimicry of an android's facial expressions of anger and happiness is connected with their perceptions of the agent's humanlikeness only when the android is co-present. In our study, we extend Hofree et al.'s work  [36]  by (i) including a wider spectrum of artificial agents, (ii) employing an overall less realistic humanoid robot that allows for easy alteration of facial cues (i.e., Furhat), (iii) focusing on all six basic emotions, and (iv) using a computer vision technique in lieu of Electromyography (EMG) to estimate people's facial mimicry. With respect to EMG, computer vision is far less obtrusive and hence more viable for field use.\n\nIn this study, we involve 45 participants in an emotion recognition task with three artificial agents varying in embodiment (i.e., physical Furhat robot, video-recorded Furhat robot, and a virtual agent) and humanlikeness (i.e., humanlike, characterlike, and morph). The emotion recognition task used in our experiment was divided into two phases. In the first phase, participants were asked to observe the facial expressions of the six basic emotions as expressed by the three artificial agents and a video-recorded human (i.e., the control), and pick the correct one from a list. In the second phase, instead, they were asked to observe the same facial expressions in re-shuffled order, mimic their temporal dynamics as closely as possible, and only afterwards recognize them. Based on Kulesza et al.  [43] , this latter phase was carried out under the pretense that intentional mimicry of facial expressions could actually improve participants' emotion recognition. Participants' faces were video-recorded in both stages of the experiment and the activation of the action units (AU) corresponding to the six basic emotions was determined through Hupont and Chetouani's AU intensity detector  [38] . In the first part of the study, we gauged which facial expressions were spontaneously mimicked by participants. In the second part of the study, we focused instead on participants' instructed mimicry, and estimated how accurate participants were in replicating the temporal dynamics of the observed facial expressions.\n\nThe aim of this study is to understand (1) whether an artificial agent's embodiment and humanlikeness can influence people's spontaneous and instructed facial mimicry (as suggested by Hofree et al.  [36]  and Mattheij et al.  [49, 50] ),  (2)  if spontaneous facial mimicry is related to people's perceptions of artificial agents, especially in terms of anthropomorphism, social presence, likability, and uncanniness (perceptual dimensions expected to be influenced by the agent's level of humanlikeness), and (3) whether there is a link between instructed and spontaneous facial mimicry. The overarching ambition of this work is to explore whether spontaneous facial mimicry can be used as an implicit, unconscious cue of liking and rapport in HAI and HRI, and whether instructed facial mimicry can act as its proxy in settings where spontaneous facial mimicry is difficult to gauge. Our work contributes to efforts paving the way towards unobtrusive automatic assessment of facial mimicry in interactions with artificial agents, hence facilitating the measurement of liking and rapport through behavioral cues in the future.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Related Work",
      "text": "Facial mimicry is the spontaneous imitation of another individual's facial expression without explicit instruction to do so  [26] . Within the area of facial mimicry research, emotional mimicry refers to the spontaneous mirroring of a facial expression with inherent emotional meaning, for instance, wincing when observing others in pain  [4]  or frowning at another person's frown. This paper focuses on people's mimicry of the six basic emotions -happiness, sadness, surprise, anger, fear, and disgust  [17]  -as displayed through the facial expressions of artificial and human agents. Within the subsections 2.1, 2.2, and 2.3, we give an account of the different theories on the nature and functioning of spontaneous facial mimicry in human-human interactions (HHI). We then describe the literature on human-agent and human-robot facial mimicry in subsection 2.4, and explain our interest in instructed facial mimicry in subsection 2.5.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Nature Of Spontaneous Facial Mimicry",
      "text": "There are two main theoretical perspectives on the nature and functioning of emotional mimicry: a motor and an emotional perspective. The motor perspective holds that emotional mimicry is an unconscious, unintentional, unemotional, and reflex-like matching of observed facial expressions  [8] . Within this context, the associative sequence learning (ASL) approach posits that mimicry happens by virtue of a learned long-term association between an action stimulus (e.g., a person's smile) and an action response (e.g., the observer's smile;  [33] ), which holds as long as the action stimulus (e.g., the observed facial expression) is similar to other stimuli previously associated with a certain motor action (e.g., the observer's facial expression).\n\nAnother theoretical formalization within the motor perspective is the automatic embodiment account, which postulates that mimicry is the embodied motor simulation of an observed emotion that serves the purpose of emotion recognition  [56] . According to this approach, we mimic another individual's facial expressions to better recognize and differentiate them.\n\nAs opposed to the motor perspective, the emotional perspective sees mimicry as a marker of subtle affective states arising in response to emotional stimuli  [13, 14] . Within this perspective, the facialfeedback hypothesis  [40, 72] , which dates back to Darwin  [11] , posits that \"the sight of a face that is happy, loving, angry, sad, or fearful (...) can cause the viewer to mimic elements of that face and, consequently, to catch the other's emotions\"  [26] . With a slightly different line of thought, the affectmatching account suggests that observing a facial expression triggers a corresponding affective state in the observer, which then generates the mimicking act  [16] . Within the emotional perspective, there is hence no clear consensus yet as to whether the affective state arising from an emotional stimulus precedes or succeeds mimicry.\n\nThe motor and emotional perspectives make somewhat different claims on the outcomes of emotional mimicry  [53] . The motor perspective assumes that facial mimicry is always consistent with the observed facial expression (i.e., emotion-congruent mimicry). For instance, an expression of anger can only trigger a corresponding expression of anger. On the opposite, the emotional perspective suggests that mimicry is related to the action tendencies associated with a stimulus (e.g., competitive and collaborative tasks,  [46] ). Thus an expression of anger can trigger anger but also fear (i.e., valence-congruent mimicry), and the type of emotion triggered depends on the meaning associated with the observed facial expression and the context where mimicry takes place  [21] .",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Evidence Supporting Theoretical Accounts On Spontaneous Facial Mimicry",
      "text": "In general, there is little experimental support for the motor perspective. Available studies almost exclusively focused on facial mimicry of happiness and anger. As Hess and Fischer and Hess et al.  [30, 31]  underline, such studies only confirm that people display a valence-congruent facial expression when exposed to happiness and anger (i.e., smiling to happiness, frowning to anger). However, they do not fully back up emotion-congruent facial mimicry, which is at the core of the motor perspective. With regards to the automatic embodiment account, several studies have investigated whether blocking facial mimicry impairs the correct recognition of emotional facial expressions  [28, 55] . Current evidence supports this position only partially. Indeed, mimicry seems to be crucial for emotion recognition but only when it comes to recognizing ambiguous or subtle facial expressions  [21, 29] .\n\nThere are a number of studies that support the emotional perspective. For instance, Laird and Bresler  [44]  noticed that when people are asked to reproduce facial expressions of fear, anger, sadness, and disgust, they also report experiencing those emotions. Moreover, Ekman et al.  [19]  note that the muscular reproduction of the facial expressions of the six basic emotions activates the Autonomic Nervous System (ANS) in a similar way as to when people actually experience those emotions. Finally, Dimberg et al.  [15]  describe how the facial response system that is responsible for mimicry responds to emotions faster (300-400 ms) than the ANS (1-3 sec.), thus finding support for the affect-matching account. Further support for the emotional perspective was also brought by Moody et al.  [53]  who found that fear priming elicits expressions of fear in response to both fear and anger, thus demonstrating that mimicry is not a purely automatic mirroring of an observed emotion, but has an intrinsic emotional meaning.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "The Social Value Of Spontaneous Facial Mimicry",
      "text": "Regardless of their different views on the nature of facial mimicry, both the motor and the emotional perspective posit that facial mimicry serves a social purpose. In one case (i.e., motor perspective), it serves to recognize and respond to other people's emotions. In the other case (i.e., emotional perspective), it serves the purpose of emotional contagion  [27, 73] , as to say \"the tendency to automatically mimic and synchronize movements, expressions, postures, and vocalizations with those of another person and, consequently, to converge emotionally\"  [26] . The literature suggests that mimicry is indicative of higher liking during first acquaintances  [7, 8, 43] , stronger rapport in already established relationships  [32]  and that it increases when two interaction partners are given the goal to affiliate  [45] . In fact,  [32]  found that watching funny movies with friends elicits more laughs than watching them with strangers. Consistently,  [21]  discovered that dyads of friends mimic each other's smiles of pride more than strangers do.  [30]  and  [6]  propose that mimicry acts as a social regulator as it communicates the intention to bond. Since emotional mimicry is known to be related with interpersonal stance  [66] , social tuning  [5] , bonding  [41] , and rapport  [22, 71, 75] , we consider it an important phenomenon to study in Human-Robot (HRI) and Human-Agent Interaction (HAI). In fact, if facial mimicry was found to work similarly for artificial agents and humans, it could be used as an implicit and unconscious measure of the quality of interaction in HAI and HRI  [63] .",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Spontaneous Facial Mimicry Of Virtual Agents And Social Robots",
      "text": "In face-to-face interactions between humans, acted facial expressions constitute the only possibility of studying spontaneous facial mimicry in a controlled way. However, acted facial expressions can be perceived by humans as being inauthentic and hence might hinder the occurrence of mimicry. For this reason, in psychology, studies on spontaneous facial mimicry have almost exclusively focused on static images or videos of facial expressions, with these latter being sometimes used to simulate live videosessions  [43] . With respect to humans, virtual and robotic agents give the unique possibility to investigate spontaneous mimicry in face-to-face interactions occurring in real-time while preserving control over the experimental setup  [35] . This is because they enable researchers to manipulate only a few facial action units (AU) and control their activation over time. In this sense, the use of virtual and robotic agents not only allows to investigate whether spontaneous facial mimicry occurs or not in specific contexts, but also opens up the possibility to understand whether its temporal dynamics are replicated.\n\nWhile human-agent mimicry has been explored more thoroughly  [22, 35] , studies on human-robot mimicry gained popularity more recently. Such a delay is probably due to the fact that robots' faces were not provided with enough degrees of freedom to accurately reproduce facial expressions until very recently. Most available studies on human-robot and human-agent mimicry focus on endowing agents with the ability to mimic the facial expressions of human interactants and observing how this ability affects people's perceptions and reactions  [35, 57, 68] . Only a few studies investigate people's spontaneous mimicry of an artificial agent's facial expressions. Such studies show similar results to human-human mimicry, with the main difference residing in the lower intensity and slower speed of human-agent and human-robot mimicry. For instance, Mattheij et al.  [49, 50]  found evidence for the spontaneous mimicry of happiness, surprise, and disgust in the context of HAI and Philip et al.  [65]  disclosed that people spontaneously mimic virtual agents' facial expressions of joy, anger, and sadness. They also observed that mimicry is less intense when it is directed to a virtual agent with respect to a human one. Similarly, in HRI, Hofree et al.  [36]  observed that people mimic a video-recorded android (i.e., Hanson's Einstein robot) to a lesser extent than a video-recorded human. Furthermore, they discovered that, while the facial expressions of a video-recorded android are mimicked only when the robot is perceived as highly humanlike, physically co-present androids are mimicked regardless of the perceptions they elicit. Hence, they proposed that it is the robot's co-presence that makes its humanlike appearance highly salient, and in turn elicits spontaneous facial mimicry. Following this line of thought, in the present study, we manipulated the artificial agents' humanlikeness, as well as their embodiment, and attempted to understand whether these influenced spontaneous facial mimicry. We employed all three embodiments used by Hofree et al.  [36]  -a video-recorded human, a video-recorded robot, and a physical robot. Moreover, we added a virtual agent as in Mattheij et al.  [49, 50] . In line with Li  [47] , we considered: (1) the video-recorded robot as artificial, physically embodied, but not co-present; (2) the physical robot as artificial, physically embodied, and co-present; and (3) the video-recorded human as natural, physically embodied, but not co-present. While Li  [47]  differentiates between physical and digital co-presence, in this work we combined the two into one single category of co-presence to distinguish between the two video-recordings that capture behavior of the past and hence do not share the same environment and time with the participant (i.e., video-recorded robot and video-recorded human) from the virtual agent which shares the same environment and time with the participant. Consequently, we categorize the virtual agent as artificial, virtually embodied, and co-present.\n\nIn HHI, Bourgeois and Hess  [6]  showed that the social context in which the interaction takes place has the power to influence emotional mimicry. While happy expressions are mimicked regardless of whether an observed person is an in-group or out-group member, expressions of sadness are mimicked only between in-group members. Likewise, in HRI, Hofree et al.  [37]  showed that participants mimicked a robot's smiles and frowns when cooperating with it, but displayed inverse mimicry (i.e., frowned at the robot's smiles and smiled in response to its frowns), when the context was competitive. To circumvent this problem, in this study, we showed the agents' facial expressions to participants in a non-interactive context inspired by Kulesza et al.  [43] . Similar to Hofree et al.  [36] , in this study, we asked participants to carefully observe the agents' facial expressions. Inspired by Kulesza et al.  [43] , however, we also gave them the goal to recognize the emotion displayed by the agent.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Spontaneous And Instructed Facial Mimicry",
      "text": "Facial mimicry can further be divided into spontaneous and instructed. Spontaneous facial mimicry, which we have discussed so far, occurs unconsciously, without any specific instruction  [26] . Instructed facial mimicry, instead, is deliberate mimicry of facial expressions that occurs consciously as a result of specific instructions  [52, 59] . In their study, Hofree et al.  [36]  used instructed facial mimicry to ensure that the facial expressions of the android they used were visible, feasible to imitate, and that electromyography (EMG) was working properly. Interestingly, they reported similar results for spontaneous and instructed facial mimicry. In fact, similar to spontaneous facial mimicry, the instructed facial mimicry of the videorecorded android was less intense than the one directed to the video-recorded human. This result brought us to hypothesize that instructed facial mimicry might be somehow linked to spontaneous facial mimicry. To deepen our understanding of the relationship between instructed and spontaneous facial mimicry, in this paper, we explore whether spontaneous facial mimicry can be predicted by people's ability to accurately reproduce the dynamics of an agent's facial expressions of the six basic emotions upon instruction to do so. Moreover, we study whether artificial agents' embodiment and level of humanlikeness can affect instructed facial mimicry in a way that is analogous to spontaneous facial mimicry. Should instructed facial mimicry be found to significantly predict spontaneous facial mimicry, it could be used as an explicit cue of people's social tuning with an artificial agent and could act as proxy of spontaneous facial mimicry.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Research Questions And Hypotheses",
      "text": "In this work, we explore the influence of embodiment and humanlikeness on people's spontaneous and instructed mimicry of artificial agents' facial expressions of the six basic emotions. Based on Hofree et al.  [36]  and Mattheij et al.  [49, 50] , we chose three embodiments for this study: a video-recorded robot, a physical robot, and a virtual agent. Furthermore, we added a fourth condition acting as a control in which participants observed the facial expressions of a video-recorded human. To change the artificial agents' level of humanlikenss, we manipulated their facial features to resemble those of a characterlike face, a humanlike face, and a face that includes features from both of them (i.e., a morph). Humanlikeness was chosen as an independent variable in our study not only because Hofree et al.  [36]  found it to be salient for facial mimicry, but also since it is known to influence people's perceptions of an agent's anthropomorphism, social presence, and uncanniness  [54] ,  [42] , which are perceptual dimensions that in turn affect liking and rapport. Our first group of research questions (RQ1a -RQ1c) concerns spontaneous facial mimicry:\n\nRQ1a To what extent does the humanlikeness of artificial agents influence people's spontaneous facial mimicry?\n\nRQ1b To what extent does the embodiment of artificial agents influence people's spontaneous facial mimicry?\n\nRQ1c Does spontaneous facial mimicry differ between artificial and human agents?\n\nOur second group of research questions (RQ2a -RQ2c) revolves around instructed facial mimicry. In previous work  [59] , we investigated how well people were able to reproduce the dynamics of a laughter performed by an artificial agent that they were explicitly instructed to mimic. In this paper we focus on facial expressions of the six basic emotions instead. Here, we aim to understand whether the agents' embodiment and humanlikeness can affect instructed facial mimicry similar to how they affect spontaneous facial mimicry. Therefore, we pose the following research questions:\n\nRQ2a To what extent does the humanlikeness of artificial agents influence people's ability to mimic their facial expressions as accurately as possible when instructed to do so?\n\nRQ2b To what extent does the embodiment of artificial agents influence people's ability to mimic their facial expressions as accurately as possible when instructed to do so?\n\nRQ2c Does instructed facial mimicry differ between artificial and human agents?\n\nThe ultimate aim of our research is to inform the development of implicit and explicit behavioral measures that can extend or replace questionnaire-based investigations of the perception of artificial agents. Previous work has already highlighted that spontaneous facial mimicry signals liking in first acquaintances  [8, 43]  and rapport in established relationships  [21, 32] . Liking and rapport are complex constructs known to be influenced by factors such as the appearance and embodiment of an agent  [60, 61, 64] . In this study, besides understanding the role of embodiment and humanlikeness in facial mimicry, we aim to gain more insights on the relationship between spontaneous facial mimicry and a few of the perceptual dimensions known to influence rapport and liking:\n\nRQ3 To what extent can spontaneous facial mimicry predict the agent's perceived social presence, anthropomorphism, uncanniness, and likability?\n\nFrom the related literature, we know that the occurrence of spontaneous facial mimicry can be an important predictor of the rapport people build with a human or artificial interaction partner. However, due to occlusions of the face and the subtlety of the mimicked facial expressions, it is often difficult to capture and quantify spontaneous facial mimicry in natural settings and more complex interactions. In these contexts, instructed facial mimicry could act as a proxy of spontaneous facial mimicry and could be used in place of a questionnaire as an explicit indirect cue of liking and rapport. Our fourth research question is thus concerned with the relation between instructed and spontaneous facial mimicry:\n\nRQ4 To what extent does instructed facial mimicry predict spontaneous facial mimicry?\n\nBased on related studies performed by Hofree et al.  [36] , Chartrand and Bargh  [8] , Kulesza et al.  [43] , and Hess et al.  [32] , we expected that:\n\n(H1) Physically embodied, co-present, humanlike artificial agents elicit higher spontaneous facial mimicry with respect to virtually embodied, non-co-present, non-humanlike artificial agents.\n\n(H2) Physically embodied, co-present, humanlike artificial agents elicit higher instructed facial mimicry with respect to virtually embodied, non-co-present, non-humanlike artificial agents.\n\n(H3) Spontaneous facial mimicry positively predicts people's evaluations of the agents' anthropomorphism, social presence, and likability, and negatively predicts their perceived uncanniness.\n\n(H4) Instructed facial mimicry positively predicts spontaneous facial mimicry.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Methodology",
      "text": "Our study followed a 3x3 mixed experimental design with:\n\n• Embodiment as within-subject variable with three types of embodiment: a virtual agent, a physical Furhat robot  [2] , and a video-recording of the Furhat robot (cf. Fig.  1 )\n\n• Humanlikeness as between-subject variable with three levels of humanlikeness: humanlike, characterlike and a morph between the humanlike and the characterlike (cf. Fig.  2 )\n\nFurthermore, we included a control condition in which participants observed a video-recorded human (cf.fig.  1 ). This control condition was the same across all levels of the agent's humanlikeness.\n\nThe experimental design was informed by Kulesza et al.  [43]  and consisted of two parts. In the first part, each participant was asked to observe the facial expressions of the agents and identify which of the six basic emotions they displayed (i.e., happiness, sadness, surprise, anger, fear, disgust). In the second part, which occurred after a 5-minute break, participants were explicitly told that the accuracy of mimicry could improve emotion recognition. Consequently, they were instructed to observe the facial expressions corresponding to the six basic emotions performed by the same agents (in randomized order), Each participant observed a set of facial expressions performed by the video-recorded human and the three artificial agents. All three artificial agents had the same level of humanlikeness but differed in their embodiment. Each set of facial expressions was composed of expressions of the six basic emotions performed twice by each agent. Within each set, the order of presentation of the stimuli was randomized, and no two facial expressions of the same type occurred one after the other. The order of presentation of the artificial and human agents was shuffled using Latin Squares. In total, each participant observed 48 facial expressions for each part of the study. Emotional facial expressions were presented in short sequences of 5 seconds including onset, apex and offset, without vocalizations nor head movements (cf. Fig.  3 ).",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Participants",
      "text": "We recruited 46 participants from an international study program in Computer Science at Uppsala University. Participants had at least a high school degree and came from a diverse geographic background (44.4% Swedish). The 46 participants were randomly allocated to the three conditions corresponding to the different levels of humanlikeness of the artificial agents: characterlike (N=15; 11 male; 4 female; 0 other/prefer not to say), humanlike (N=16; 13 male; 3 female; 0 other/prefer not to say), and morph (N=15; 12 male; 3 female; 0 other/prefer not to say). Due to a misunderstanding of the study task, we excluded the data of one male participant from the humanlike condition. The final sample of participants had a mean age of 26.16 years (SD=4.37) and was composed of 10 people identifying themselves a female and 35 as male. None of the participants had previously interacted with the Furhat robot.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Embodiment And Humanlikeness",
      "text": "As a robot, we chose the Furhat platform  [2] . Furhat is a blended robot head consisting of a rigid mask on which a facial texture is projected from within. We chose the Furhat robot for this experiment as its virtual face allowed us to easily alter facial features and design smooth and noiseless facial expressions.\n\nWe designed three different facial textures for the artificial agents. The humanlike face was created from pictures of a real human face using the FaceGen Modeller 1  . The characterlike face was the standard Furhat face with sketched \"drawing-like\" lips and eyebrows. Finally, the morph face was created by blending the humanlike and characterlike skin textures in the Paint.NET digital photo editing package. The three different textures we applied to the artificial agents were selected from a set of 28 faces tested in a pre-study on Amazon Mechanical Turk (AMT). Since initial experiments with the Furhat robot found the face mask without any projection to be perceived as male and dominant  [58] , we limited the set of stimuli to male faces. The same texture we used for the Furhat robot was also utilized to create the virtual agent's face. The video-recorded robot was obtained by recording the physical Furhat. For the human condition, instead, we selected the video-recordings of a male person from the MUG database  [1] .",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Synthesis Of Facial Expressions",
      "text": "The human in the MUG database was video-recorded while performing the facial expressions of the six basic emotions following the Facial Action Coding System (FACS,  [18, 23] ) and an onset-apex-offset temporal scheme. We designed the facial expressions of the artificial agents by replicating the dynamics of the human video recording as closely as possible. Unfortunately, as in Furhat's IrisTK animation system  [70] , some facial Action Units (AUs) are combined and cannot be controlled separately, the facial expressions of the human and those of the artificial agents slightly differed. An expert trained in the FACS ensured that the final set of stimuli for the artificial agents was still following the FACS' guidelines. Furthermore, an online pre-study conducted on AMT found no systematic difference between the artificial synthesis and the human stimulus in terms of emotion recognition.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Experimental Setup",
      "text": "The experimental sessions took place in a private laboratory room at Uppsala University (cf. Fig.  4 ). To grant a feeling of privacy and an even background for the video-recordings, the participant's area was separated from the researcher's area by a blue curtain. Black curtains positioned behind the Furhat robot (FR) and the screen displaying the other agents ensured a good visibility of the agents from the participants' perspective. Uniform lighting for the recordings was guaranteed through a professional lighting system (PLS) composed by two lamps. These were the only light sources in the experiment space. As both Furhat and the screen displaying the agents were sources of light themselves, the dark environment ensured a good visibility of the facial expressions.\n\nThe participant (PR) was sitting in the participants' area at a distance of about 100 cm from the Furhat robot or the screen. This value falls in the personal space of the participants according to Hall  [24] . The agents were thus close enough to the participants to be properly seen, but not too close to elicit an intimidating feeling. The agents were placed on a table at approximately 100 cm from the ground, which was roughly at eye level for the majority of participants. The video-recorded and the virtual agents were presented on a screen in portrait orientation. Their size was calibrated to match the size of Furhat's head. All embodiments were controlled by a desktop computer (M1). An iPad (iP), placed on the table in front of the participant, was used for answering the questionnaires.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "Measures",
      "text": "",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Facial Recordings",
      "text": "To record participants' faces, we used two LOGITECH C920HD PRO webcams (WB) with a 800x600 resolution, operating at 30 fps. The webcams were placed on top of a tripod. One was positioned in front of participants, at approximately 60 cm from them, and slightly on their side to not occlude the stimulus. The second was positioned on the side of the participant (cf. Fig.  4 ). The webcams were connected to a laptop (M2) which was used to start, stop, and control the video-recordings during the experiment. Each webcam recorded the entire experimental session with the exclusion of the break between the spontaneous and instructed mimicry trials. Hence, we obtained two video files per camera, participant and session. The video-recordings of the frontal camera were used to assess participants' mimicry, those of the lateral camera to capture the entire experimental scene.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Questionnaires",
      "text": "Throughout the experiment, four different questionnaires were used. Questionnaire Q1 consisted of a general demographic questionnaire (10 items), the short version of the Big Five personality traits (10 items,  [67] ) and the Interpersonal Reactivity Index (IRI, 21 questions, excluded personal distress, Cronbach's α between .70 and .78 according to Davis et al.  [12] ). This questionnaire gauged the empathy and personality traits of the participants, and hence was not used to answer this paper's research questions.\n\nQuestionnaire Q2 was shown to participants after every facial expression they observed to assess the emotion they recognized in the stimulus. It was composed of the question \"Which of these facial expressions was just displayed?\" with the six basic emotions, \"neutral\" and \"I don't know\" as response options, and the question: \"How certain are you of the selection you made in question 1?\" with a three point Likert scale using the labels: \"Uncertain\", \"Neither nor\", \"Certain\". The response options in the first question were displayed in one of three pre-shuffled orders to prevent a bias towards the first item on the scale. Questionnaire (Q3) was shown after every embodiment in the first part of the experiment (i.e., spontaneous mimicry trial) to measure participants' perceptions of the agents on four dimensions:\n\n• Anthropomorphism (5 items, 5-point Likert scale), sub-scale from the Godspeed questionnaire by Bartneck et al.  [3]  (Cronbach's α = .91 according to Ho and MacDorman  [34] ).\n\n• Social presence (8 items, 5-point Likert scale), excerpt from the social presence questionnaire developed by Harms and Biocca  [25] . Sub-scales: co-presence (2 items, α = .84), Attentional Allocation (2 items, α = .81), Perceived Affective Understanding (2 items, α = .86), Perceived Emotional Interdependence (1 item, α = .85) and Perceived Behavioral Interdependence (1 item, α = .82).\n\n• Uncanniness and Likability (10 items, 5-point Likert scale), excerpt from Rosenthal von Der Pütten and Krämer  [69] , sub-scales likability and perceived threat (Cronbach's α >= 0.82 for both subscales).\n\nThe order of questions and items remained the same across all embodiments. At the end of the experimental session, the experimenter performed a semi-structured interview with the participant. The interview covered potential previous interactions with the Furhat robot, whether participants found aspects in the appearance of one of the characters particularly eerie, and if they had the impression that some of the facial expressions they observed were more difficult to trace back to a specific emotion. This interview was used to gather additional information about the experiment, and was not used to answer any research question present in this paper.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Procedure",
      "text": "After arriving to the lab, participants were informed about the experimental procedure, signed a consent form and answered Q1 on the iPad in front of them.\n\nFor the first part of the experiment, participants were asked to first watch the facial expressions displayed by the four agents, which always started and ended with a beep tone, and then indicate which emotions they corresponded to using the questionnaire Q2 displayed on the iPad. Participants were also explained that, once they finished completing Q2 on the iPad and after a pause of about 2 seconds, the agent would automatically display the next facial expression preceded and followed by another beep tone, and the same procedure would be repeated until they had observed all facial expressions.\n\nAfter participants observed all 12 expressions (2 trials x 6 emotions) for one embodiment, they rated their perception of the observed agent using questionnaire Q3 on the iPad. When neccessary, the experimenter used this lapse of time to switch the physical robot with the screen. Once the participant finished responding to Q3, the stimuli for the subsequent embodiment were shown. Once participants responded to Q3 for the fourth and final agent of the spontaneous mimicry condition, they were given a five minute break and served refreshments.\n\nFor the second part of the experiment, participants were told that research suggests that mimicry increases emotion recognition. Therefore, they were asked to perform the same task once again, but this time by first mimicking the facial expression as accurately as possible and then noting down the emotion. The second part of the experiment followed the same procedure of the first part but the embodiments were re-shuffled in order. As Q3 was omitted for the second part of the experiment, participants had a shorter break between embodiments.\n\nAt the end of the session, the experimenter conducted the short semi-structured interview. This was followed by a debriefing in which the researcher explained the true nature and objective of the experiment. Participants were informed again that they could request the deletion of their data at any point in time.",
      "page_start": 10,
      "page_end": 11
    },
    {
      "section_name": "Mimicry Processing",
      "text": "The strategy to segment the videos differed between spontaneous and instructed facial mimicry. In the first case, we were interested in understanding whether people mimicked the observed facial expressions or not, whereas in the second case, we were interested in understanding how accurate people were in mimicking the dynamics of the observed facial expressions. This difference in focus is motivated by the different expected magnitudes of spontaneous and instructed facial mimicry. While the former is a subtle response that does not necessarily follow the same dynamics of the expression observed, the latter was expected to be a much stronger and accurate response due to its explicitly imitative nature.\n\nFor spontaneous facial mimicry, we annotated the frontal videos of the corresponding trial with the beginning and end of each stimulus in ELAN 5.4. To do so, we used the audible beep tones that marked the start and end of each facial expression of the agents. We then used the minutes obtained from the annotation to automatically cut the original video into shorter snippets using ffmpeg 2  . To properly divide the instructed mimicry episodes, instead, we first manually identified the initial and final mimicry frames for each stimulus by closely examining the participant's AU activation, and then we cut the original video a second before and after these frames. This process ideally led to 96 individual video snippets per participant, 48 for spontaneous and 48 for instructed facial mimicry. Once the data were segmented, we deployed an automatic AU intensity detector to recognize which muscles of the participants' face were activated in each video snippet of the spontaneous and instructed facial mimicry trials (cf. subsection 5.1). Then, in the case of spontaneous facial mimicry, we checked the AU time series to understand whether or not the target AU or combination of AUs amounting to each facial expression was active for a given lapse of time (cf. subsection 5.2). In the case of instructed facial mimicry, instead, we used the AU time series to perform a Cross-Recurrence Quantification Analysis (CRQA,  [73] ) as detailed in Section 5.3.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Detection Of Au Activation",
      "text": "The AU intensity detector used in this work is presented in Hupont and Chetouani  [38]  and follows the pipeline shown in Fig.  5 . In a first step, it segments the face of the person from the whole input image and extracts a set of facial landmarks. Face segmentation is carried out by means of the Viola and Jones' Haar Cascade algorithm  [74] . The landmarks (14 white crosses in Figure  5 ) are extracted with the Intraface library introduced by Xiong and Torre  [76] . On the basis of the facial landmark positions, three rectangular facial Regions of Interest (ROIs) are then defined and features of Histogram Oriented Gradients (HOG,  [10] ) are computed for each one of them. The ROIs used in our pipeline are:\n\n• Frown ROI (used for AU4 model): This ROI is located around the inner eyebrow landmarks, which are also used for alignment purposes.\n\n• Eyes ROI (AU1, AU2 and AU6): This ROI is made up of 8 patches located around the inner eyebrows, the middle eyebrows and the eye landmarks. ROI alignment is performed using inner eye corners. The final descriptor results from the concatenation of the 8 HOG descriptors.\n\n• Mouth ROI (AU12, AU15, AU20, AU25 and AU26): This ROI is bounded by the nose center, the two lip corners and the lower lip. Alignment is done with respect to the lip corner positions.\n\nFinally, the classification of each AU in terms of intensity is performed by an individually pre-trained Support Vector Machine (SVM) model using its corresponding ROI features as input. The SVM models were trained on the large-scale DISFA facial action database  [51] . Each model detects the activation of its corresponding AU in terms of six intensity categories, which are, according to Ekman's taxonomy  [23] : \"N\" (neutral), \"A\" (trace), \"B\" (slight), \"C\" (marked), \"D\" (severe) and \"E\" (maximum). The AU detector achieved an overall Intraclass Correlation Coefficient ICC(3,1) of 0.73, which is within state-of-the-art performances in the task of AU intensity detection.\n\nThe AU intensity time series was low-pass filtered through a centered moving average filter with a window size of 10 samples  (33.3ms ). This filtering was applied to both the spontaneous and instructed facial mimicry time series. Moreover, the duration of time for which each AU was activated was also computed. For instructed facial mimicry, the first and the last 30 samples corresponding to the 1 second buffer left before and after the initial and final mimicry frames were removed in the final time series.",
      "page_start": 11,
      "page_end": 12
    },
    {
      "section_name": "Processing Of Spontaneous Facial Mimicry",
      "text": "To assess spontaneous facial mimicry, we divided the AU time series into two time intervals. The first time interval spanned from 0 to 1000 ms after stimulus onset and encompassed quick mimicry responses occurring at a subperceptual level, which Dimberg et al.  [16]  call Rapid Facial Reactions (RFR). The second time interval ranged from 1000 to 5000 ms after stimulus onset and comprised facial mimicry responses occurring at a more conscious level, which we call Controlled Facial Reactions (CFR).\n\nTo consider a facial expression as mimicked at each time interval (RFR, CFR), we checked whether the AU or combination of AUs corresponding to the target facial expression (cf. Table  1  based on Ekman et al.  [18] ) was active for at least 3 consecutive frames (100 ms). The activation was coded as 0 (not activated) or 1 (activated) and the intensity of the activation was not considered for this analysis as we expected the intensity of spontaneous facial mimicry to be low. We chose the threshold of 100 ms based on Ito et al.  [39] , who defined this as the shortest period of time a muscle can take to move. To perform the statistical analyses, we calculated the percentage of spontaneous facial mimicry for RFR and CFR. This value was obtained per embodiment by dividing the number of trials in which the participant mimicked the facial expressions by the number of valid video snippets for that embodiment. Since in the spontaneous mimicry part of the study, participants were not explicitly asked to mimic the facial expressions they observed, in some snippets their faces were occluded, out of frame, or not recognizable by the AU intensity detector. These snippets were excluded from the final analyses. If more than half of the snippets of a particular embodiment were missing, we also excluded the other valid snippets from the a final analysis. Overall, this led to the exclusion of a total of 465 snippets for RFR (22%) and 394 for CFR (18%), and left us with 1695 valid snippets for RFR, and 1766 for CFR.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Analysis Of Instructed Facial Mimicry",
      "text": "In order to accurately assess the dynamics of facial expressions, we performed a CRQA analysis  [48] . CRQA is a technique enabling a quantitative measure of the graphical patterns occurring in a Cross-Recurrence Plot (CRP, cf. Fig.  6 ). CRP is a plot looking at the times at which the features of a dynamical system recur (i.e., it is close) to features of another dynamical system. In this study, the two dynamical systems were the user and the artificial agents / video-recorded human, and the features were the AU intensities.\n\nA CRP can be displayed as a square / rectangular black and white area spanned by two time series describing two systems. Black points correspond to the times at which the two systems co-visit the same area in the feature space, whereas white points correspond to the times at which each system runs in a different area. A CRP is expressed by the following cross-recurrence matrix (CR) :\n\nwhere f 1 and f 2 ∈ IR d are the d-dimensional time series of the two systems having N and M samples, respectively; is the threshold to claim closeness between two points, Θ(.) is the Heaviside function and . is a norm. In this study, f 1 and f 2 ∈ IR 3 are the time series of the AU intensities of the human and the artificial agents / video-recorded human over N samples. The threshold was set to 2 expressing that there was a match only when the 'distance' between the intensities of corresponding AUs was less than two. The norm used was the Manhattan distance.\n\nCRPs can be analyzed through the Cross-Recurrence Quantification Analysis (CRQA) that enables to extract quantitative information from the black and white patterns appearing in the plot (see  [48]  for a complete survey). Typical patterns are: single isolated points, periodical diagonal lines, and vertical / horizontal lines. These patterns are hints of randomness, periodicity and laminar states of the dynamics of the system. In this study, we focused on the following CRQA measures (  [48] ):",
      "page_start": 12,
      "page_end": 13
    },
    {
      "section_name": "Cross-Recurrence Rate (Crr)",
      "text": "The Cross-Recurrence Rate is defined as:\n\nand measures the density of recurrence points in a CRP. It corresponds to the ratio between the number of the matrix elements shared by the participant and the artificial agents / video-recorded human and the number of available elements (i.e. all the elements in the matrix). Here, cRR represents the overall extent to which the human and the artificial agent / recorded human were activating the same AUs at a similar level. This measure alone, however, even if it is a first measure to address mimicry, does not provide any information about how mimicry unfolds over time. To extract information about that, several other CRQA measures were computed:\n\nAverage diagonal lines length (L) and maximum diagonal line length (L max ) L represents the average length of a recurrent trajectory in a CRP. It is defined as:\n\nwhere l m is the minimal diagonal length to be taken into account, and P (l) is the histogram of the diagonal lines. The minimal diagonal length was set to 8 samples, i.e. around 250 ms  [20] . The value of L expresses how stable a recurrent trajectory is. Here high values of L correspond to long, almost identical portions of AU intensities of the human and the artificial agents over time. Moreover, the length L max of the longest diagonal line in the CRP was computed. A large value of L max shows a slow divergence of the AUs' intensity trajectories.",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "Determinism (Det)",
      "text": "As a fourth and last measure, the determinism was computed. It is defined as:\n\nIt measures the percentage of the cross-recurrence points forming diagonal lines (of at least length l m ) computed with respect to all the cross-recurrence points in the CRP. DET ranges in [0, 1] and it is a hint of the predictability of the system (when DET = 0 the systems is stochastics, when DET = 1 it is periodic). In this study, high values of DET were expected to be found during good mimicry episodes. While participants paid more attention to stay in frame during the instructed mimicry phase, we still had to exclude snippets due to occlusions and errors of the AU intensity detector. If there were only 5 or less valid snippets for a particular embodiment and participant, these were removed from the final analysis. Overall, we excluded a total of 209 snippets (9%) and were left with 1951 valid snippets for the analysis of instructed mimicry. For the statistical analysis, we calculated the average cRR, L max , L avg and DET of each participant across all valid trials associated with one embodiment.",
      "page_start": 13,
      "page_end": 14
    },
    {
      "section_name": "Results",
      "text": "In the remainder of the paper, we use: (1) social presence to refer to the dependent variables copresence, attentional allocation, perceived affective understanding, perceived emotional interdependence, and perceived behavioral interdependence; (2) perception of the agent to refer to the dependent variables anthropomorphism, likability, and perceived threat; (3) emotion recognition to refer to the dependent variables percentage of correctly recognized emotions and average confidence in the recognized emotion; (4) spontaneous facial mimicry to refer to the percentage of spontaneous facial mimicry for rapid facial reactions (RFR) and controlled facial reactions (CFR);  (5)  instructed facial mimicry to refer to the average (avg) cRR, avg L, avg L max , and avg DET .\n\nFor the two manipulation checks (MC1 and MC2) and the preliminary analyses (PA), and for answering RQ1 and RQ2, we performed separate 3x3 repeated measures ANOVAs with humanlikeness as between-subject factor (i.e., humanlike, characterlike, morph), embodiment as within-subject factor (i.e., virtual agent, physical robot, and video-recording of the physical robot) and (i) social presence (MC1), (ii) perception of the agent (MC2), (iii) emotion recognition (PA), (iv) spontaneous facial mimicry (RQ1) and (v) instructed facial mimicry (RQ2) as dependent variables. All p-values that we report in the posthoc analyses are Bonferroni corrected to account for multiple tests.\n\nFor MC2, PA, RQ1, and RQ2, we also ran follow-up 2x3 repeated measures ANOVAs with humanlikeness as a between-subject factor (humanlike, characterlike, morph), artificiality of the agent as a within-subject factor (i.e., artificial agents and human agent), and the same dependent variables. To perform these analyses, we calculated the average value across all three artificial agents on each dependent variable. Social presence (MC1) was excluded from this set of analyses since the video-recorded human did not vary in embodiment like the artificial agents. We kept humanlikeness as a between-subject factor to control for eventual effects of the different levels of humanlikeness of the artificial agents on the dependent variables. However, as this effect is already covered by the 3x3 repeated measures ANOVAs, for the sake of brevity, we do not report these results. All the p-values that we report in the post-hoc analyses are Bonferroni corrected to account for multiple tests.\n\nFinally, for answering RQ3 and RQ4, we performed separate regression analyses using spontaneous facial mimicry as a predictor of social presence and perceptions of the artificial agents (RQ3) and instructed facial mimicry as a predictor of spontaneous facial mimicry (RQ4). As RQ3 specifically focused on artificial agents' facial mimicry, we used only the data from the artificial agents to perform the regression analyses. On the contrary, as RQ4 focused on facial mimicry in general and not specifically on artificial agents' mimicry, we included also the data from the human video in the regression analyses.",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "Manipulation Check And Preliminary Analyses",
      "text": "",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "Manipulation Check: Social Presence Of The Artificial Agents",
      "text": "The results indicated a significant main effect of embodiment on co-presence, affective understanding, and emotional interdependence (cf. Table  2  for the complete results). Furthermore, they showed a significant interaction effect of humanlikeness and embodiment on co-presence.\n\nPost-hoc analyses uncovered that the virtual agent was perceived as significantly more co-present than the video-recorded robot (p = .005, cf. Table  3  for the descriptive statistics), and the physical robot was perceived as significantly more co-present (p = .005) than the video-recorded one. No such difference was observed between the virtual agent and the physical robot (p = 1.00). Moreover, they disclosed that participants perceived their affective understanding of the physical robot to be significantly higher than that of the virtual agent (p = .045), while the virtual agent and the video-recorded robot did not differ in terms of perceived affective understanding (p = .255), and neither did the physical robot and the videorecorded one (p = 1.00). Finally, participants perceived significantly higher emotional interdependence with the physical robot with respect to both the virtual agent (p = .021, cf. Table  3  for the descriptive statistics) and the video-recorded robot (p = .019). No such difference was present between the virtual agent and the video-recorded robot (p = 1.00).\n\nFurther follow-up post-hoc analyses on the interaction effect of humanlikeness and embodiment on co-presence uncovered that, in the characterlike condition, the virtual agent (M = 4.10, SD = .632) and the physical robot (M = 4.27, SD = .729) were perceived as significantly more co-present than the video-recorded robot (M = 3.70, SD = .621, virtual agent: p = .026; physical robot: p = .028), but they did not significantly differ in co-presence from each other (p = 1.00). Likewise, in the morph condition, the virtual agent (M = 3.71, SD = 1.051) was perceived as significantly more co-present than the video-recorded robot (M = 3.07, SD = .938, p = .016), the physical robot (M = 3.53, SD = .930) was perceived as significantly more co-present than the video-recorded one (p = .051), and the virtual agent and the physical robot did not differ from each other (p = .409). Interestingly though, in the humanlike condition (virtual agent: M = 3.79, SD = .777; physical robot: M = 4.04, SD = .746; video-recorded robot: M = 4.00, SD = .734), these differences between artificial agents were not present (virtual agent -physical robot: p = .331; virtual agent -video-recorded robot: p = .083; physical robot -video-recorded robot: p = 1.00).",
      "page_start": 15,
      "page_end": 15
    },
    {
      "section_name": "Manipulation Check: Perception Of The Agents",
      "text": "When checking for differences in the perception of the artificial agents across levels of humanlikeness and embodiments, we found a significant main effect of embodiment on anthropomorphism, perceived threat, and likability (cf. Table  2  for the complete results) and a significant main effect of humanlikeness on anthropomorphism. Bonferroni-corrected post-hoc analyses revealed that the virtual agent and the video-recorded robot were perceived as less anthropomorphic than the physical robot (both p < .001, cf. Table  3  for the descriptive statistics), but the virtual agent and the video-recorded robot did not differ in terms of anthropomorphism between each other (p < .682). Similarly, in terms of likability, the virtual agent was perceived as less likable than the physical robot (p = .001, cf. Table  3  for the descriptive statistics) and the video-recorded robot was perceived as less likable than the physical one (p = .042). However, the virtual agent and the video-recorded robot did not differ from each other (p = .291). Finally, concerning perceived threat, the virtual agent was perceived as more threatening than the video-recorded robot (p = .001, cf. Table  3  for the descriptive statistics), but no such difference was present between the virtual agent and the physical robot (p = .104) and between the video-recorded and the physical robot (p = .822)\n\nWith regards to the main effect of humanlikeness, the post-hoc analyses disclosed that humalike   4  for the descriptive statistics). However, humanlike and characterlike artificial agents (p = .232) and characterlike and morph agents (p = 1.00) did not differ significantly from each other. When running the 2x3 ANOVA focusing on the agents' artificiality, we found out that the videorecorded human was perceived as significantly more anthropomorphic (p < .001), more likable (p < .001), and less threatening (p < .001) than the artificial agents (cf. Table  5  for the results and the descriptive statistics).\n\nDiscussion of Manipulation Check. As specified in section 2.4, the artificial agents and the videorecorded human differed as follows: (i) the physical robot was artificial, physically embodied, and copresent; (ii) the virtual agent was artificial, virtually embodied, and co-present; (iii) the video-recorded robot was artificial, physically embodied, but not co-present; and (iv) the video-recorded human was natural, physically embodied, but not co-present. The manipulation checks that we performed were aligned with these differences. Indeed, the video-recorded robot was perceived as significantly less co-present than the virtual agent and physical robot. Furthermore, the artificial agent that was physically embodied and co-present (i.e., the physical robot) was perceived as easier to understand affectively, more anthropomorphic, more likable, and elicited more emotional understanding than the other artificial agents. Finally, the human agent was perceived as more anthropomorphic, more likable, and less threatening than the artificial agents. As a result, we can state that the manipulation of embodiment worked as expected in this study.\n\nWith regards to the manipulation of humanlikeness, the core dependent variable that we expected to change was anthropomorphism. The characterlike and morph robot did not differ in anthropomorphism and neither did the characterlike and humanlike robot. However, in line with our expectations, the humanlike robot was perceived as more anthropomorphic than the morph robot. As a result, we considered the manipulation of humanlikeness only partially successful. With regards to the manipulation of humanlikeness, it was also very interesting to discover that, when the appearance of the artificial agents was humanlike, the differences in co-presence between the different embodiments ceased to exist. This result seems to suggest that the humanlike appearance has in itself a quality of co-presence that goes beyond the physical instantiation of an artificial agent.",
      "page_start": 16,
      "page_end": 16
    },
    {
      "section_name": "Preliminary Analyses: Emotion Recognition",
      "text": "As a preliminary analysis, we checked whether participants' ability to recognize the emotions displayed by the artificial agents differed across embodiments and levels of humanlikeness. Interestingly, we discov- ered a main effect of humanlikess on the percentage of emotion recognized (cf. Table  2  for the complete results). According to the results, participants' emotion recognition was better when participants observed the characterlike agents with respect to when they observed the humanlike agents (cf. Table  4  for the descriptive statistics). This was true both in the spontaneous mimicry (p = .029) and in the instructed mimicry trials (p = .006, cf. Fig.  7 ). No such differences in emotion recognition were observed between characterlike and morph agents (spontaneous mimicry trial: p = .154; instructed mimicry trial: p = .466) and between morph and humanlike agents across trials (spontaneous mimicry trial: p = 1.00; instructed mimicry trial: p = .218). When it comes to the 2x3 ANOVAs focusing on the agents' artificiality, we found a significant difference between artificial agents and the video-recorded human in terms of emotion recognition only for the instructed mimicry trial (cf. Table  5  for the results and descriptive statistics). In this case, the percentage of emotions correctly recognized was higher for the human with respect to the artificial agents.\n\nDiscussion of Preliminary Analyses. As predicted, the facial expressions of the video-recorded human were easier to recognize in comparison to the facial expressions of the artificial agents. However, somewhat unexpectedly, and partially in conflict with this result, the facial expressions of the characterlike artificial agents were easier to recognize with respect to those of the humanlike artificial agents both for the spontaneous and instructed mimicry trials. We ascribe this results to the stylized appearance of the characterlike agents, which might have made their expressions more readable and recognizable than those of the other agents. Results disclosed a significant main effect of embodiment on spontaneous facial mimicry both for RFR and CFR (cf. Table  2  for the complete results). However, we did not find any significant effect of humanlikeness and embodiment and humanlikeness alone on spontaneous facial mimicry.",
      "page_start": 16,
      "page_end": 17
    },
    {
      "section_name": "Results For Research Questions",
      "text": "Post-hoc analyses with a Bonferroni correction disclosed that for the RFR the video-recorded robot was mimicked significantly more than the virtual agent (p = .003, cf. Table  3  for the descriptive statistics and cf. Fig.  8  for the boxplot) and physical robot (p = .001), and that the physical robot and the virtual agent did not differ in spontaneous facial mimicry from each other (p = 1.00). With regards to CFR, the post-hoc analyses showed that the physical robot was mimicked significantly less than the video-recorded robot (p = .038, cf. Fig.  8  for the boxplot), while the virtual agent and the video-recorded robot did not differ in terms of spontaneous facial mimicry (p = 1.00) and only a trend difference was present between  the virtual agent and the physical robot (p = .068).\n\nWhen taking into account the artificiality of the agent as the within-subject factor, we found a significant main effect of artificiality on spontaneous facial mimicry (cf. Table  5  for the results and the descriptive statistics). In this case, the video-recorded human was spontaneously mimicked significantly more than the artificial agent both for RFR and CFR.\n\nDiscussion of RQ1. These results are somewhat complimentary to those we found for the manipulation checks. Indeed, it seems that the agent that elicited the highest ratings of co-presence, affective understanding, emotional interdependence, anthropomorphism, and likability, namely the physical robot, was also the agent that was spontaneously mimicked the least. If we take the facial mimicry-rapport hypothesis into account, this result is somewhat counterintuitive. Indeed, in line with this hypothesis, the robot eliciting the most favorable relational ratings should have been the one spontaneously mimicked the most. However, if we take the emotion recognition task into account, we can partially explain this result. Recognizing the emotions of another agent is an activity that implies putting some distance between the agent we observe and ourselves. It somewhat entails considering the agent we observe as a stimulus, rather than a relational agent. In this sense, we can hypothesize that an agent that is perceived as more socially present and elicits more positive perceptions is less good as a stimulus, it is more likely to act as a distractor, and hence can hinder the goal of the emotion recognition task. It is interesting to note that, when the agent is a human, this dynamic does not take place and the human is, as foreseeable, spontaneously mimicked more than the artificial agents. We can ascribe this result to the familiarity of the human stimulus. Indeed, the video-recorded human is undoubtedly more positively evaluated than the artificial agents and hence more likely to act as a distractor. However, it is also the stimulus with which participants are the most familiar and whose facial expressions they are more used to recognize.",
      "page_start": 17,
      "page_end": 19
    },
    {
      "section_name": "Influence Of Embodiment And Humanlikeness On Instructed Facial Mimicry [Rq2]",
      "text": "The results of the 3x3 repeated measures ANOVAs did not show any significant effect of embodiment and humanlikeness on instructed facial mimicry (cf. Table  2  for the complete results). Similarly, the results of the 2x3 repeated measures ANOVA did not disclose any significant effect of the agents' artificiality on participants' instructed facial mimicry (cf. Table  5  for the results and the descriptive statistics). Discussion of RQ2. As opposed to Hofree et al.  [36] , in our study, the results of instructed facial mimicry are not congruent with those of spontaneous facial mimicry. Based on our findings, we can state that when facial mimicry is explicitly prompted, the agent's appearance and embodiment cease to have an influence on it. This might be due to the fact that, when facial mimicry transforms itself into a purely imitative act, it loses its social value, and hence those variables that would have likely affected it due to their relational value, such as the agents' level of humanlikeness and their embodiment, do not influence it anymore. This assumption is further reinforced by the fact that people's ability to mimic an agent as closely as possible does not differ also when artificial and human agents are taken into account.",
      "page_start": 19,
      "page_end": 19
    },
    {
      "section_name": "Spontaneous Facial Mimicry As Predictor Of Perceived Social Presence And Perceptions Of Artificial Agents [Rq3]",
      "text": "The results of the regression analyses in Table  6  show that spontaneous facial mimicry for RFR was a negative predictor of co-presence, attentional allocation, and affective understanding, whereas spontaneous facial mimicry for CFR was a negative predictor of attentional allocation and emotional interdependence. Moreover, they showed that spontaneous facial mimicry for RFR was a negative predictor of people's perceptions of the artificial agents' likability and anthropomorphism, and spontaneous facial mimicry for CFR was a negative predictor of participants' perception of the agents' likability (cf. Table  6  for the complete results).\n\nDiscussion of RQ3. These results are in line with those of RQ1 and seem to suggest that, in this study, the more the artificial agents were spontaneously mimicked, the less positive perceptions they elicited, the less socially co-present they were perceived, and the less people people felt emotionally connected with them and capable of understanding their affective states. We assume that this result, which goes against most of the literature focusing on the social function of spontaneous facial mimicry, can be ascribed to the emotion recognition task in which participants were involved. Our hypothesis is that, within an emotion recognition task, spontaneous facial mimicry does not fulfill anymore a social function, but rather serves the purpose of emotion recognition. In this context, all the perceptual dimensions that are normally positively related to spontaneous facial mimicry become negatively related to mimicry as they act as distractors towards the ultimate emotion recognition goal of the task. To verify this assumption, we performed a few additional regression analyses using spontaneous facial mimicry for RFR and CFR as predictors and the percentage of correctly recognized facial expressions and the confidence in the recognized emotion as dependent variables. As we supposed, participants' spontaneous facial mimicry was a significant negative predictor of their certainty of the correctness of the recognized emotion and a trend negative predictor of their emotion recognition performance (cf. Table  6  for the complete results). This indicates that the more participants spontaneously mimicked the artificial agents, the less they were confident in the emotion they recognized. Such a result is particularly important as it corroborates the theory that facial mimicry serves the purpose of emotion recognition, but only when the emotions to recognize are ambiguous  [21, 29] .",
      "page_start": 20,
      "page_end": 20
    },
    {
      "section_name": "Instructed Facial Mimicry As Predictor Of Spontaneous Facial Mimicry [Rq4]",
      "text": "The results of the regression analyses displayed in Table  7  show that the average cRR, L, L max , and DET are all significant negative predictors of spontaneous facial mimicry for RFR but they do not equally predict spontaneous facial mimicry for CFR.\n\nDiscussion RQ4 This result is extremely interesting as it suggests that, in this study, the more closely participants mimicked the facial expressions of the agents when instructed to do so, the less likely they were to spontaneously mimic the agents at an unconscious level of processing. Since we have seen that spontaneous facial mimicry for RFR was a negative predictor of participants' confidence in the recognized emotion (and partially also of their ability to recognize the target emotion), it does not surprise that people that mimic an emotion well under instruction, actually mimic it less at a subperceptual level. Indeed, if people are better able to mimic all the temporal dynamics of a target facial expression, they might also be more capable of recognizing that target emotion. In this sense, as opposed to spontaneous facial mimicry, instructed facial mimicry might signal a better understanding of the emotion. This finding entails that, even though in an emotion recognition task, instructed facial mimicry does not behave similarly to spontaneous facial mimicry, it still maintains a relation with it.",
      "page_start": 20,
      "page_end": 20
    },
    {
      "section_name": "General Discussion",
      "text": "This study investigated how the humanlikeness and embodiment of an artificial agent could influence people's mimicry of its facial expressions. Based on Hofree et al.  [36] , we expected that physically embodied, co-present, and humanlike artificial agents could elicit higher spontaneous and instructed facial mimicry than virtually embodied, non-co-present, and less humanlike ones, and that instructed facial mimicry could positively predict spontaneous facial mimicry. Moreover, based on the link between facial mimicry and rapport, we postulated that spontaneous facial mimicry could positively predict participants' evaluations of the agents' anthropomorphism, social presence, and likability, and negatively predict their perceived uncanniness. Although our manipulation of embodiment and humanlikeness were successful, and the task we chose was taken from the existing literature  [36, 43] , the results we obtained did not meet our expectations (cf. H1-H4 in section 3). We found that: (i) the physically embodied, co-present artificial agent (i.e., the physical robot) was the one that was spontaneously mimicked the least regardless of its humanlikeness (cf. H1); (ii) instructed facial mimicry did not behave congruently to spontaneous facial mimicry (cf. H2); (iii) spontaneous facial mimicry negatively predicted anthropomorphism, social presence, and likability, and did not predict uncanniness (cf. H3); and (iv) instructed facial mimicry negatively predicted spontaneous facial mimicry (cf. H4).\n\nWhile these results were surprising, their consistency led to a hypothesis that some element of the task that was given to the participants hindered the social value of facial mimicry. Following the automatic embodiment account  [56] , we postulated that the task's focus on emotion recognition could have caused a change in the meaning of facial mimicry. Additional analyses confirmed our suspicion. In fact, they indicated that the spontaneous facial mimicry of the artificial agents was a significant negative predictor of participants' confidence in the emotion recognized. This result seems to suggest that, in the context of human-agent and human-robot mimicry, the emotion recognition goal of a task can flip the social value of spontaneous facial mimicry, and transform a physically embodied, co-present artificial agent into a distractor. This may have arisen by chance due to elements of the study design and deserves further exploration and replication. The primary objective of this study was to understand whether spontaneous facial mimicry could be used as a cue of liking and rapport in HAI and HRI, and whether instructed facial mimicry could act as a proxy of spontaneous facial mimicry. Although our findings do not meet our expectations, the fact that they went in the exact opposite direction to our original hypotheses may suggest that, in an emotion recognition task, spontaneous facial mimicry can still be used as a predictor of liking and rapport, and instructed facial mimicry could still function as a predictor of spontaneous facial mimicry, but they need to be envisioned as negative predictors rather than positive ones. Additional work is needed to corroborate these preliminary results, and understand whether context alone (emotion recognition task vs. social interaction) can influence the value of facial mimicry in HAI and HRI in the way we have described.",
      "page_start": 21,
      "page_end": 22
    },
    {
      "section_name": "Limitations & Future Work",
      "text": "One limitation of the current experimental design is the focus on one particular robotic embodiment (i.e., the Furhat robot). While this platform has several advantages, like the easy alteration of facial features and expressions, it is sometimes difficult to discern facial detail clearly. By keeping the robot platform consistent across conditions, we could limit the influence of confounding factors on our results. However, this in turn reduced the strength of the manipulation of humanlikeness and could be the reason why we did not see the agents' anthropomorphism differ between the characterlike and humanlike, and the characterlike and morph conditions. Future studies should hence investigate facial mimicry in emotion recognition tasks carried out with multiple humanoid robots differing in their embodiment and degree of realism to check whether our findings still hold. We also suggest to replicate our study involving a larger and more diverse set of participants, particularly when it comes to academic background and gender.\n\nUnlike most related work (e.g.,  [36] ), in our experiment, we included stimuli covering all six basic emotions  [18] . For the analyses of facial mimicry, however, we combined people's responses to the different emotions together and calculated an average facial mimicry value. It is thus fair to assume that, while comprehensive, our results might not fit all six basic emotions equally. Another element of variation that one might need to control when studying facial mimicry is the observer's belief that an agent's facial expression reflects its subjective emotional state. In future facial mimicry studies, it would be interesting to include additional questionnaires capturing people's belief about the agent's emotional state when performing facial expressions, and their own emotional state before and after the experiment.\n\nSince our study was task-based, non-interactive, and devoid on an emotional context, the acted nature of the agents' facial expressions was particularly clear. Future work should focus on bringing the study of facial mimicry into more interactive and social contexts and assess whether facial mimicry could be used in place of questionnaires to assess people's social attunement with artificial agents. An important pre-condition for using facial mimicry as a behavioral indicator of people's relationship with a robot is a robust and non-intrusive assessment technique of people's facial expressions. While the computer-visionbased approach discussed in this paper has shown promising results, further improvements are necessary to make it more robust with respect to different angles and light conditions. This is especially important if we want to bring the study of facial mimicry to less controlled scenarios.",
      "page_start": 21,
      "page_end": 22
    },
    {
      "section_name": "Conclusion",
      "text": "In the study presented in this paper, we involved participants in an emotion recognition task carried out with artificial agents differing in their embodiment and degree of humanlikeness. In the first phase of the study, we asked participants to observe the artificial agents' facial expressions and attempt to identify the emotions they displayed. In the second phase of the study, instead, we asked participants to observe the agents' facial expressions, mimic them as closely as possible, and then identify them. We used the first part of the study to investigate the frequency of participants' spontaneous facial mimicry, and the second part to investigate the accuracy of their instructed facial mimicry. The aim was to understand whether spontaneous mimicry of artificial agents' facial expressions can be used as a behavioral cue of liking and rapport, and whether instructed facial mimicry could act as a proxy of its spontaneous counterpart. Our results suggest that, in an emotion recognition task, the physical instantiation of an artificial agent, together with its likability and anthropomorphism, intrudes rather than promotes people's spontaneous facial mimicry. Furthermore, results suggest that instructed facial mimicry negatively predicts spontaneous facial mimicry. Since the participants in this study mimicked the facial expressions of the artificial agents more when they were uncertain about the emotion to recognize, one possibility is that, in emotion recognition contexts, facial mimicry serves the purpose of emotion recognition. Even though our results did not support our initial hypotheses, they nevertheless show that spontaneous mimicry can be a behavioral cue of liking and rapport, and instructed facial mimicry a proxy of spontaneous facial mimicry.",
      "page_start": 22,
      "page_end": 23
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Diﬀerent types of embodiments used in the experiment. From left to right: a virtual agent;",
      "page": 7
    },
    {
      "caption": "Figure 2: Levels of humanlikeness used in the experiment. Left: characterlike; right: humanlike; center:",
      "page": 7
    },
    {
      "caption": "Figure 3: Facial expressions of the six basic emotions. From left to right: happiness, sadness, surprise,",
      "page": 8
    },
    {
      "caption": "Figure 4: Left: the experimental setup. Right: A participant in the participant area.",
      "page": 9
    },
    {
      "caption": "Figure 4: ). The webcams were",
      "page": 9
    },
    {
      "caption": "Figure 5: AU intensity detection pipeline. The white crosses represent the facial landmarks extracted",
      "page": 11
    },
    {
      "caption": "Figure 5: In a ﬁrst step, it segments the face of the person from the whole input image",
      "page": 11
    },
    {
      "caption": "Figure 5: ) are extracted with",
      "page": 11
    },
    {
      "caption": "Figure 6: Cross-Recurrence Plot of two diﬀerent participants intentionally mimicking the facial expres-",
      "page": 13
    },
    {
      "caption": "Figure 7: ). No such diﬀerences in emotion recognition were observed",
      "page": 17
    },
    {
      "caption": "Figure 8: for the boxplot) and physical robot (p = .001), and that the physical robot and the virtual",
      "page": 17
    },
    {
      "caption": "Figure 8: for the boxplot), while the virtual agent and the video-recorded robot did not",
      "page": 17
    },
    {
      "caption": "Figure 7: Boxplots showing the eﬀect of level of humanlikeness on the percentage of emotions correctly",
      "page": 18
    },
    {
      "caption": "Figure 8: Boxplots showing the eﬀect of the agent’s embodiment on frequency of spontaneous facial",
      "page": 19
    }
  ],
  "tables": [
    {
      "caption": "Table 3: for the descriptive statistics) and",
      "data": [
        {
          "Social Presence": "Co-presence",
          "F (2, 80)": "7.878",
          "p": ".005",
          "ηp2": ".168",
          "F (2, 40)": "2.719",
          "F (4, 80)": "4.036"
        },
        {
          "Social Presence": "Att. Allocation",
          "F (2, 80)": "2.040",
          "p": ".249",
          "ηp2": ".064",
          "F (2, 40)": ".036",
          "F (4, 80)": "1.377"
        },
        {
          "Social Presence": "Aﬀ. Understand.",
          "F (2, 80)": "3.643",
          "p": ".059",
          "ηp2": ".106",
          "F (2, 40)": "1.115",
          "F (4, 80)": "2.373"
        },
        {
          "Social Presence": "Em.\nInterdep.",
          "F (2, 80)": "5.864",
          "p": ".616",
          "ηp2": ".032",
          "F (2, 40)": "2.157",
          "F (4, 80)": ".668"
        },
        {
          "Social Presence": "Beha.\nInterdep.",
          "F (2, 80)": ".630",
          "p": ".578",
          "ηp2": ".035",
          "F (2, 40)": ".750",
          "F (4, 80)": ".725"
        }
      ],
      "page": 15
    },
    {
      "caption": "Table 3: for the descriptive statistics) and",
      "data": [
        {
          "Instr. Mimicry": "Avg cRR",
          "F (2, 76)": ".097",
          "p": ".538",
          "ηp2": ".040",
          "F (2, 38)": "2.189",
          "F (4, 76)": ".785"
        },
        {
          "Instr. Mimicry": "Avg L",
          "F (2, 76)": ".364",
          "p": ".800",
          "ηp2": ".021",
          "F (2, 38)": ".208",
          "F (4, 76)": ".411"
        },
        {
          "Instr. Mimicry": "Avg Lmax",
          "F (2, 76)": ".477",
          "p": ".878",
          "ηp2": ".015",
          "F (2, 38)": ".293",
          "F (4, 76)": ".298"
        },
        {
          "Instr. Mimicry": "Avg DET",
          "F (2, 76)": ".219",
          "p": ".539",
          "ηp2": ".040",
          "F (2, 38)": ".187",
          "F (4, 76)": ".784"
        }
      ],
      "page": 15
    },
    {
      "caption": "Table 3: Descriptive Statistics of the 3x3 Repeated Measures ANOVAs per Embodiment:",
      "data": [
        {
          "Co-presence": "Att. Allocation",
          "3.87": "4.05",
          ".832": ".837",
          "3.95": "4.20",
          ".844": ".757",
          "3.59": "4.02",
          ".847": ".809"
        },
        {
          "Co-presence": "Aﬀ. Understanding",
          "3.87": "3.17",
          ".832": ".778",
          "3.95": "3.45",
          ".844": ".625",
          "3.59": "3.27",
          ".847": ".658"
        },
        {
          "Co-presence": "Em.\nInterdependence",
          "3.87": "1.79",
          ".832": ".888",
          "3.95": "2.12",
          ".844": "1.051",
          "3.59": "1.70",
          ".847": ".832"
        },
        {
          "Co-presence": "Beha.\nInterdependence",
          "3.87": "2.28",
          ".832": "1.076",
          "3.95": "2.35",
          ".844": "1.066",
          "3.59": "2.21",
          ".847": ".914"
        }
      ],
      "page": 16
    },
    {
      "caption": "Table 4: Descriptive Statistics of the 3x3 Repeated Measures ANOVAs per level of human-",
      "data": [
        {
          "Co-presence": "Att. Allocation",
          "4.02": "4.10",
          ".720": ".734",
          "3.94": "4.12",
          ".722": ".737",
          "3.44": "4.05"
        },
        {
          "Co-presence": "Aﬀ. Understanding",
          "4.02": "3.19",
          ".720": ".550",
          "3.94": "3.48",
          ".722": ".550",
          "3.44": "3.24"
        },
        {
          "Co-presence": "Em.\nInterdependence",
          "4.02": "2.20",
          ".720": ".775",
          "3.94": "1.64",
          ".722": ".775",
          "3.44": "1.74"
        },
        {
          "Co-presence": "Beha.\nInterdependence",
          "4.02": "2.16",
          ".720": ".910",
          "3.94": "2.52",
          ".722": ".909",
          "3.44": "2.17"
        }
      ],
      "page": 17
    },
    {
      "caption": "Table 5: for the results and the",
      "data": [
        {
          "Instr. Mimicry": "Avg cRR",
          "F (1, 39)": ".653",
          "p": ".424",
          "ηp2": ".016",
          "M": "13.29",
          "SD": "10.124"
        },
        {
          "Instr. Mimicry": "Avg L",
          "F (1, 39)": ".039",
          "p": ".844",
          "ηp2": ".001",
          "M": "5.53",
          "SD": "2.697"
        },
        {
          "Instr. Mimicry": "Avg Lmax",
          "F (1, 39)": ".206",
          "p": ".652",
          "ηp2": ".005",
          "M": "6.55",
          "SD": "3.369"
        },
        {
          "Instr. Mimicry": "Avg DET",
          "F (1, 39)": ".249",
          "p": ".621",
          "ηp2": ".006",
          "M": "2.65",
          "SD": "1.638"
        }
      ],
      "page": 18
    },
    {
      "caption": "Table 6: Regression Analyses [RQ3]. Frequency of spontaneous facial mimicry as predictor of Social",
      "data": [
        {
          "Co-presence": "Att. Allocation",
          "-.183": "-.211",
          "-2.271": "-2.626",
          ".025": ".010",
          ".034": ".045",
          "-.100": "-.230",
          "-1.246": "-2.920",
          ".215": ".004",
          ".010": ".053"
        },
        {
          "Co-presence": "Aﬀ. Understanding",
          "-.183": "-.203",
          "-2.271": "-2.527",
          ".025": ".013",
          ".034": ".041",
          "-.100": "-.127",
          "-1.246": "-1.587",
          ".215": ".115",
          ".010": ".016"
        },
        {
          "Co-presence": "Em.\nInterdependence",
          "-.183": "-.077",
          "-2.271": "-.940",
          ".025": ".349",
          ".034": ".006",
          "-.100": "-.204",
          "-1.246": "2.581",
          ".215": ".011",
          ".010": ".042"
        },
        {
          "Co-presence": "Beha.\nInterdependence",
          "-.183": "-.076",
          "-2.271": "-.932",
          ".025": ".353",
          ".034": ".006",
          "-.100": "-.098",
          "-1.246": "-1.224",
          ".215": ".223",
          ".010": "010"
        }
      ],
      "page": 20
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "The MUG facial expression database",
      "authors": [
        "Niki Aifanti",
        "Christos Papachristou",
        "Anastasios Delopoulos"
      ],
      "year": "2010",
      "venue": "IEEE Int. Workshop on Image Analysis for Multimedia Interactive Services"
    },
    {
      "citation_id": "2",
      "title": "Furhat: a backprojected human-like robot head for multiparty human-machine interaction",
      "authors": [
        "Al Samer",
        "Jonas Moubayed",
        "Gabriel Beskow",
        "Björn Skantze",
        "Granström"
      ],
      "year": "2012",
      "venue": "Cognitive Behavioural Systems"
    },
    {
      "citation_id": "3",
      "title": "Measurement instruments for the anthropomorphism, animacy, likeability, perceived intelligence, and perceived safety of robots",
      "authors": [
        "Christoph Bartneck",
        "Dana Kulić",
        "Elizabeth Croft",
        "Susana Zoghbi"
      ],
      "year": "2009",
      "venue": "Int. Journal of Social Robotics"
    },
    {
      "citation_id": "4",
      "title": "i show how you feel\": Motor mimicry as a communicative act",
      "authors": [
        "Janet Bavelas",
        "Alex Black",
        "Charles Lemery",
        "Jennifer Mullett"
      ],
      "year": "1986",
      "venue": "Journal of personality and social psychology"
    },
    {
      "citation_id": "5",
      "title": "Coordinated movement and rapport in teacher-student interactions",
      "authors": [
        "J Frank",
        "Bernieri"
      ],
      "year": "1988",
      "venue": "Journal of Nonverbal behavior"
    },
    {
      "citation_id": "6",
      "title": "The impact of social context on mimicry",
      "authors": [
        "Patrick Bourgeois",
        "Ursula Hess"
      ],
      "year": "2008",
      "venue": "Biological Psychology"
    },
    {
      "citation_id": "7",
      "title": "The effects of robot's facial expressions on children's first impressions of trustworthiness",
      "authors": [
        "Natalia Calvo-Barajas",
        "Giulia Perugia",
        "Ginevra Castellano"
      ],
      "year": "2020",
      "venue": "2020 29th IEEE International Conference on Robot and Human Interactive Communication"
    },
    {
      "citation_id": "8",
      "title": "The chameleon effect: the perception-behavior link and social interaction",
      "authors": [
        "L Tanya",
        "John Chartrand",
        "Bargh"
      ],
      "year": "1999",
      "venue": "Journal of personality and social psychology"
    },
    {
      "citation_id": "9",
      "title": "how was your stay?\": Exploring the use of robots for gathering customer feedback in the hospitality industry",
      "authors": [
        "Jae-Yoon Michael",
        "Maya Chung",
        "Cakmak"
      ],
      "year": "2018",
      "venue": "2018 27th IEEE International Symposium on Robot and Human Interactive Communication"
    },
    {
      "citation_id": "10",
      "title": "Histograms of oriented gradients for human detection",
      "authors": [
        "Navneet Dalal",
        "Bill Triggs"
      ],
      "year": "2005",
      "venue": "IEEE Computer Society Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "11",
      "title": "The expression of the emotions in man and animals",
      "authors": [
        "Charles Darwin",
        "Phillip Prodger"
      ],
      "year": "1998",
      "venue": "The expression of the emotions in man and animals"
    },
    {
      "citation_id": "12",
      "title": "A multidimensional approach to individual differences in empathy",
      "authors": [
        "H Mark",
        "Davis"
      ],
      "year": "1980",
      "venue": "JSAS Catalog of Selected Documents in Psychology"
    },
    {
      "citation_id": "13",
      "title": "For distinguished early career contribution to psychophysiology: award address, 1988: facial electromyography and emotional reactions",
      "authors": [
        "Ulf Dimberg"
      ],
      "year": "1990",
      "venue": "psychophysiology"
    },
    {
      "citation_id": "14",
      "title": "Facial reactions: Rapidly evoked emotional responses",
      "authors": [
        "Ulf Dimberg"
      ],
      "year": "1997",
      "venue": "Journal of Psychophysiology"
    },
    {
      "citation_id": "15",
      "title": "Rapid facial reactions to emotional facial expressions",
      "authors": [
        "Ulf Dimberg",
        "Monika Thunberg"
      ],
      "year": "1998",
      "venue": "Scandinavian journal of psychology"
    },
    {
      "citation_id": "16",
      "title": "Unconscious facial reactions to emotional facial expressions",
      "authors": [
        "Ulf Dimberg",
        "Monika Thunberg",
        "Kurt Elmehed"
      ],
      "year": "2000",
      "venue": "Psychological science"
    },
    {
      "citation_id": "17",
      "title": "What the face reveals: Basic and applied studies of spontaneous expression using the Facial Action Coding System (FACS)",
      "authors": [
        "Paul Ekman",
        "Erika Rosenberg"
      ],
      "year": "1997",
      "venue": "What the face reveals: Basic and applied studies of spontaneous expression using the Facial Action Coding System (FACS)"
    },
    {
      "citation_id": "18",
      "title": "Facial action coding system: Investigator's guide",
      "authors": [
        "Paul Ekman",
        "Wallace Friesen",
        "Joseph Hager"
      ],
      "year": "1978",
      "venue": "Facial action coding system: Investigator's guide"
    },
    {
      "citation_id": "19",
      "title": "Autonomic nervous system activity distinguishes among emotions",
      "authors": [
        "Paul Ekman",
        "Robert Levenson",
        "Wallace Friesen"
      ],
      "year": "1983",
      "venue": "science"
    },
    {
      "citation_id": "20",
      "title": "Automatic facial expression analysis: a survey",
      "authors": [
        "B Fasel",
        "Juergen Luettin"
      ],
      "year": "2003",
      "venue": "Pattern Recognition"
    },
    {
      "citation_id": "21",
      "title": "Emotional mimicry in social context: the case of disgust and pride",
      "authors": [
        "Agneta Fischer",
        "Daniela Becker",
        "Lotte Veenstra"
      ],
      "year": "2012",
      "venue": "Frontiers in psychology"
    },
    {
      "citation_id": "22",
      "title": "Virtual rapport. In Int. Workshop on Intelligent Virtual Agents",
      "authors": [
        "Jonathan Gratch",
        "Anna Okhmatovskaia",
        "Francois Lamothe",
        "Stacy Marsella",
        "Mathieu Morales",
        "Rick Van Der Werf",
        "Louis-Philippe Morency"
      ],
      "year": "2006",
      "venue": "Virtual rapport. In Int. Workshop on Intelligent Virtual Agents"
    },
    {
      "citation_id": "23",
      "title": "Facial Action Coding System. Salt Lake City",
      "authors": [
        "C Joseph",
        "Paul Hager",
        "Wallace Ekman",
        "Friesen"
      ],
      "year": "2002",
      "venue": "Facial Action Coding System. Salt Lake City"
    },
    {
      "citation_id": "24",
      "title": "The Hidden Dimension",
      "authors": [
        "E Hall"
      ],
      "year": "1969",
      "venue": "The Hidden Dimension"
    },
    {
      "citation_id": "25",
      "title": "Internal consistency and reliability of the networked minds measure of social presence",
      "authors": [
        "Chad Harms",
        "Frank Biocca"
      ],
      "year": "2004",
      "venue": "Internal consistency and reliability of the networked minds measure of social presence"
    },
    {
      "citation_id": "26",
      "title": "Primitive emotional contagion",
      "authors": [
        "Elaine Hatfield",
        "John Cacioppo",
        "Richard Rapson"
      ],
      "year": "1992",
      "venue": "Review of personality and social psychology"
    },
    {
      "citation_id": "27",
      "title": "Emotional contagion. Current directions in psychological science",
      "authors": [
        "Elaine Hatfield",
        "John Cacioppo",
        "Richard Rapson"
      ],
      "year": "1993",
      "venue": "Emotional contagion. Current directions in psychological science"
    },
    {
      "citation_id": "28",
      "title": "Face the noise: Embodied responses to nonverbal vocalizations of discrete emotions",
      "authors": [
        "T Skyler",
        "Agneta Hawk",
        "Gerben A Fischer",
        "Van Kleef"
      ],
      "year": "2012",
      "venue": "Journal of personality and social psychology"
    },
    {
      "citation_id": "29",
      "title": "Facial mimicry and emotional contagion to dynamic emotional facial expressions and their influence on decoding accuracy",
      "authors": [
        "Ursula Hess",
        "Sylvie Blairy"
      ],
      "year": "2001",
      "venue": "International journal of psychophysiology"
    },
    {
      "citation_id": "30",
      "title": "Emotional mimicry as social regulation",
      "authors": [
        "Ursula Hess",
        "Agneta Fischer"
      ],
      "year": "2013",
      "venue": "Personality and Social Psychology Review"
    },
    {
      "citation_id": "31",
      "title": "Emotional mimicry: Why and when we mimic emotions",
      "authors": [
        "Ursula Hess",
        "Agneta Fischer"
      ],
      "year": "2014",
      "venue": "Social and Personality Psychology Compass"
    },
    {
      "citation_id": "32",
      "title": "The intensity of facial expression is determined by underlying affective state and social situation",
      "authors": [
        "Ursula Hess",
        "Rainer Banse",
        "Arvid Kappas"
      ],
      "year": "1995",
      "venue": "Journal of personality and social psychology"
    },
    {
      "citation_id": "33",
      "title": "Automatic imitation",
      "authors": [
        "Cecilia Heyes"
      ],
      "year": "2011",
      "venue": "Psychological bulletin"
    },
    {
      "citation_id": "34",
      "title": "Revisiting the uncanny valley theory: Developing and validating an alternative to the godspeed indices",
      "authors": [
        "Chin-Chang Ho",
        "Karl Macdorman"
      ],
      "year": "2010",
      "venue": "Computers in Human Behavior"
    },
    {
      "citation_id": "35",
      "title": "The impact of agent facial mimicry on social behavior in a prisoner's dilemma",
      "authors": [
        "Rens Hoegen",
        "Job Van Der",
        "Gale Schalk",
        "Jonathan Lucas",
        "Gratch"
      ],
      "year": "2018",
      "venue": "Proceedings of the 18th International Conference on Intelligent Virtual Agents"
    },
    {
      "citation_id": "36",
      "title": "Bridging the mechanical and the human mind: spontaneous mimicry of a physically present android",
      "authors": [
        "Galit Hofree",
        "Paul Ruvolo",
        "Marian Bartlett",
        "Piotr Winkielman"
      ],
      "year": "2014",
      "venue": "PloS one"
    },
    {
      "citation_id": "37",
      "title": "Behind the robot's smiles and frowns: In social context, people do not mirror android's expressions but react to their informational value",
      "authors": [
        "Galit Hofree",
        "Paul Ruvolo",
        "Audrey Reinert",
        "Marian Bartlett",
        "Piotr Winkielman"
      ],
      "year": "2018",
      "venue": "Frontiers in neurorobotics"
    },
    {
      "citation_id": "38",
      "title": "Region-based facial representation for real-time action units intensity detection across datasets",
      "authors": [
        "Isabelle Hupont",
        "Mohamed Chetouani"
      ],
      "year": "2019",
      "venue": "Pattern Analysis and Applications"
    },
    {
      "citation_id": "39",
      "title": "Fast force-generation dynamics of human articulatory muscles",
      "authors": [
        "Takayuki Ito",
        "Emi Murano",
        "Hiroaki Gomi"
      ],
      "year": "2004",
      "venue": "Journal of applied physiology"
    },
    {
      "citation_id": "40",
      "title": "Human emotions",
      "authors": [
        "Carroll E Izard"
      ],
      "year": "2013",
      "venue": "Human emotions"
    },
    {
      "citation_id": "41",
      "title": "Understanding and predicting bonding in conversations using thin slices of facial expressions and body language",
      "authors": [
        "Natasha Jaques",
        "Daniel Mcduff",
        "Yoo Lim Kim",
        "Rosalind Picard"
      ],
      "year": "2016",
      "venue": "International Conference on Intelligent Virtual Agents"
    },
    {
      "citation_id": "42",
      "title": "A review of empirical evidence on different uncanny valley hypotheses: support for perceptual mismatch as one road to the valley of eeriness",
      "authors": [
        "Jari Kätsyri",
        "Klaus Förger",
        "Meeri Mäkäräinen",
        "Tapio Takala"
      ],
      "year": "2015",
      "venue": "Frontiers in Psychology"
    },
    {
      "citation_id": "43",
      "title": "The face of the chameleon: The experience of facial mimicry for the mimicker and the mimickee",
      "authors": [
        "Wojciech Marek Kulesza",
        "Aleksandra Cis Lak",
        "Robin Vallacher",
        "Andrzej Nowak",
        "Martyna Czekiel",
        "Sylwia Bedynska"
      ],
      "year": "2015",
      "venue": "The Journal of Social Psychology"
    },
    {
      "citation_id": "44",
      "title": "The process of emotional experience: A self-perception theory",
      "authors": [
        "D James",
        "Charles Laird",
        "Bresler"
      ],
      "year": "1992",
      "venue": "The process of emotional experience: A self-perception theory"
    },
    {
      "citation_id": "45",
      "title": "The chameleon effect as social glue: Evidence for the evolutionary significance of nonconscious mimicry",
      "authors": [
        "Valerie Jessica L Lakin",
        "Clara Jefferis",
        "Tanya Cheng",
        "Chartrand"
      ],
      "year": "2003",
      "venue": "Journal of Nonverbal Behavior"
    },
    {
      "citation_id": "46",
      "title": "Expectations of cooperation and competition and their effects on observers' vicarious emotional responses",
      "authors": [
        "T John",
        "Basil Lanzetta",
        "Englis"
      ],
      "year": "1989",
      "venue": "Journal of personality and social psychology"
    },
    {
      "citation_id": "47",
      "title": "The benefit of being physically present: A survey of experimental works comparing copresent robots, telepresent robots and virtual agents",
      "authors": [
        "Jamy Li"
      ],
      "year": "2015",
      "venue": "Int. Journal of Human-Computer Studies"
    },
    {
      "citation_id": "48",
      "title": "Recurrence plots for the analysis of complex systems",
      "authors": [
        "M Norbert Marwan",
        "Marco Romano",
        "Jürgen Thiel",
        "Kurths"
      ],
      "year": "2007",
      "venue": "Physics Reports"
    },
    {
      "citation_id": "49",
      "title": "Vocal and facial imitation of humans interacting with virtual agents",
      "authors": [
        "Ruud Mattheij",
        "Marie Nilsenova",
        "Eric Postma"
      ],
      "year": "2013",
      "venue": "2013 Humaine Association Conference on Affective Computing and Intelligent Interaction"
    },
    {
      "citation_id": "50",
      "title": "Mirror mirror on the wall",
      "authors": [
        "Ruud Mattheij",
        "Marie Postma-Nilsenová",
        "Eric Postma"
      ],
      "year": "2015",
      "venue": "Journal of Ambient Intelligence and Smart Environments"
    },
    {
      "citation_id": "51",
      "title": "DISFA: A spontaneous facial action intensity database",
      "authors": [
        "Mohammad Mohammad Mavadati",
        "Kevin Mahoor",
        "Philip Bartlett",
        "Jeffrey Trinh",
        "Cohn"
      ],
      "year": "2013",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "52",
      "title": "When the social mirror breaks: deficits in automatic, but not voluntary, mimicry of emotional facial expressions in autism",
      "authors": [
        "Aimee Daniel N Mcintosh",
        "Piotr Reichmann-Decker",
        "Julia Winkielman",
        "Wilbarger"
      ],
      "year": "2006",
      "venue": "Developmental science"
    },
    {
      "citation_id": "53",
      "title": "More than mere mimicry? the influence of emotion on rapid facial reactions to faces",
      "authors": [
        "Eric Moody",
        "Daniel Mcintosh",
        "Laura Mann",
        "Kimberly Weisser"
      ],
      "year": "2007",
      "venue": "Emotion"
    },
    {
      "citation_id": "54",
      "title": "The uncanny valley [from the field]",
      "authors": [
        "Masahiro Mori",
        "Karl Macdorman",
        "Norri Kageki"
      ],
      "year": "2012",
      "venue": "IEEE Robotics & Automation Magazine"
    },
    {
      "citation_id": "55",
      "title": "When did her smile drop? facial mimicry and the influences of emotional state on the detection of change in emotional expression",
      "authors": [
        "Paula Niedenthal",
        "Markus Brauer",
        "Jamin Halberstadt",
        "Åse H Innes-Ker"
      ],
      "year": "2001",
      "venue": "Cognition & Emotion"
    },
    {
      "citation_id": "56",
      "title": "The simulation of smiles (sims) model: Embodied simulation and the meaning of facial expression",
      "authors": [
        "Martial Paula M Niedenthal",
        "Marcus Mermillod",
        "Ursula Maringer",
        "Hess"
      ],
      "year": "2010",
      "venue": "Behavioral brain sciences"
    },
    {
      "citation_id": "57",
      "title": "Achieving affective human-virtual agent communication by enabling virtual agents to imitate positive expressions",
      "authors": [
        "Takashi Numata",
        "Hiroki Sato",
        "Yasuhiro Asa",
        "Takahiko Koike",
        "Kohei Miyata",
        "Eri Nakagawa",
        "Motofumi Sumiya",
        "Norihiro Sadato"
      ],
      "year": "2020",
      "venue": "Scientific reports"
    },
    {
      "citation_id": "58",
      "title": "Congruency matters -How ambiguous gender cues increase a robot's uncanniness",
      "authors": [
        "Maike Paetzel",
        "Christopher Peters",
        "Ingela Nyström",
        "Ginevra Castellano"
      ],
      "year": "2016",
      "venue": "International Conference on Social Robotics"
    },
    {
      "citation_id": "59",
      "title": "Investigating the influence of embodiment on facial mimicry in hri using computer vision-based measures",
      "authors": [
        "Maike Paetzel",
        "Giovanna Varni",
        "Isabelle Hupont",
        "Mohamed Chetouani",
        "Christopher Peters",
        "Ginevra Castellano"
      ],
      "year": "2017",
      "venue": "2017 26th IEEE International Symposium on Robot and Human Interactive Communication"
    },
    {
      "citation_id": "60",
      "title": "The persistence of first impressions: The effect of repeated interactions on the perception of a social robot",
      "authors": [
        "Maike Paetzel",
        "Giulia Perugia",
        "Ginevra Castellano"
      ],
      "year": "2020",
      "venue": "Proceedings of the 2020 ACM/IEEE International Conference on Human-Robot Interaction"
    },
    {
      "citation_id": "61",
      "title": "The influence of robot personality on the development of uncanny feelings",
      "authors": [
        "Maike Paetzel-Prüsmann",
        "Giulia Perugia",
        "Ginevra Castellano"
      ],
      "year": "2021",
      "venue": "Computers in Human Behavior"
    },
    {
      "citation_id": "62",
      "title": "Engage-dem: a model of engagement of people with dementia",
      "authors": [
        "Giulia Perugia",
        "Marta Díaz-Boladeras",
        "Andreu Català-Mallofré",
        "Emilia Barakova",
        "Matthias Rauterberg"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "63",
      "title": "On the role of personality and empathy in human-human, human-agent, and human-robot mimicry",
      "authors": [
        "Giulia Perugia",
        "Maike Paetzel",
        "Ginevra Castellano"
      ],
      "year": "2020",
      "venue": "International Conference on Social Robotics"
    },
    {
      "citation_id": "64",
      "title": "I can see it in your eyes: Gaze as an implicit cue of uncanniness and task performance in repeated interactions with robots",
      "authors": [
        "Giulia Perugia",
        "Maike Paetzel-Prüsmann",
        "Madelene Alanenpää",
        "Ginevra Castellano"
      ],
      "year": "2021",
      "venue": "Frontiers in Robotics and AI"
    },
    {
      "citation_id": "65",
      "title": "Rapid facial reactions in response to facial expressions of emotion displayed by real versus virtual faces. i-Perception",
      "authors": [
        "Leonor Philip",
        "Jean-Claude Martin",
        "Céline Clavel"
      ],
      "year": "2018",
      "venue": "Rapid facial reactions in response to facial expressions of emotion displayed by real versus virtual faces. i-Perception"
    },
    {
      "citation_id": "66",
      "title": "Beyond backchannels: co-construction of dyadic stancce by reciprocal reinforcement of smiles between virtual agents",
      "authors": [
        "Ken Prepin",
        "Magalie Ochs",
        "Catherine Pelachaud"
      ],
      "year": "2013",
      "venue": "Proceedings of the Annual Meeting of the Cognitive Science Society"
    },
    {
      "citation_id": "67",
      "title": "Measuring personality in one minute or less: A 10-item short version of the big five inventory in english and german",
      "authors": [
        "Beatrice Rammstedt",
        "Oliver P John"
      ],
      "year": "2007",
      "venue": "Journal of Research in Personality"
    },
    {
      "citation_id": "68",
      "title": "How anthropomorphism affects empathy toward robots",
      "authors": [
        "Tal-Chen Laurel D Riek",
        "Bhismadev Rabinowitch",
        "Peter Chakrabarti",
        "Robinson"
      ],
      "year": "2009",
      "venue": "ACM/IEEE Int. Conference on Human Robot Interaction"
    },
    {
      "citation_id": "69",
      "title": "How design characteristics of robots determine evaluation and uncanny valley related responses",
      "authors": [
        "Astrid M Rosenthal-Von Der Pütten",
        "Nicole Krämer"
      ],
      "year": "2014",
      "venue": "Computers in Human Behavior"
    },
    {
      "citation_id": "70",
      "title": "IrisTK: a Statechart-based Toolkit for Multi-party Faceto-face Interaction",
      "authors": [
        "Gabriel Skantze",
        "Samer Al"
      ],
      "year": "2012",
      "venue": "Int. Conference on Multimodal Interaction"
    },
    {
      "citation_id": "71",
      "title": "The nature of rapport and its nonverbal correlates",
      "authors": [
        "Linda Tickle-Degnen",
        "Robert Rosenthal"
      ],
      "year": "1990",
      "venue": "Psychological inquiry"
    },
    {
      "citation_id": "72",
      "title": "Affect theory. Approaches to emotion",
      "authors": [
        "Silvan Tomkins"
      ],
      "year": "1984",
      "venue": "Affect theory. Approaches to emotion"
    },
    {
      "citation_id": "73",
      "title": "Computational study of primitive emotional contagion in dyadic interactions",
      "authors": [
        "Giovanna Varni",
        "Isabelle Hupont",
        "Chloe Clavel",
        "Mohamed Chetouani"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "74",
      "title": "Robust real-time face detection",
      "authors": [
        "Paul Viola",
        "J Michael",
        "Jones"
      ],
      "year": "2004",
      "venue": "International Journal of Computer Vision"
    },
    {
      "citation_id": "75",
      "title": "Rapport and facial expression",
      "authors": [
        "Ning Wang",
        "Jonathan Gratch"
      ],
      "year": "2009",
      "venue": "2009 3rd International Conference on Affective Computing and Intelligent Interaction and Workshops"
    },
    {
      "citation_id": "76",
      "title": "Supervised descent method and its applications to face alignment",
      "authors": [
        "Xuehan Xiong",
        "Fernando Torre"
      ],
      "year": "2013",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition"
    }
  ]
}