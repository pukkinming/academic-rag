{
  "paper_id": "2508.05228v1",
  "title": "Cwefs: Brain Volume Conduction Effects Inspired Channel-Wise Eeg Feature Selection For Multi-Dimensional Emotion Recognition",
  "published": "2025-08-07T10:17:59Z",
  "authors": [
    "Xueyuan Xu",
    "Wenjia Dong",
    "Fulin Wei",
    "Li Zhuo"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Due to the intracranial volume conduction effects, highdimensional multi-channel electroencephalography (EEG) features often contain substantial redundant and irrelevant information. This issue not only hinders the extraction of discriminative emotional representations but also compromises the real-time performance. Feature selection has been established as an effective approach to address the challenges while enhancing the transparency and interpretability of emotion recognition models. However, existing EEG feature selection research overlooks the influence of latent EEG feature structures on emotional label correlations and assumes uniform importance across various channels, directly limiting the precise construction of EEG feature selection models for multi-dimensional affective computing. To address these limitations, a novel channel-wise EEG feature selection (CWEFS) method is proposed for multi-dimensional emotion recognition. Specifically, inspired by brain volume conduction effects, CWEFS integrates EEG emotional feature selection into a shared latent structure model designed to construct a consensus latent space across diverse EEG channels. To preserve the local geometric structure, this consensus space is further integrated with the latent semantic analysis of multi-dimensional emotional labels. Additionally, CWEFS incorporates adaptive channel-weight learning to automatically determine the significance of different EEG channels in the emotional feature selection task. The effectiveness of CWEFS was validated using three popular EEG datasets with multi-dimensional emotional labels. Comprehensive experimental results, compared against nineteen feature selection methods, demonstrate that the EEG feature subsets chosen by CWEFS achieve optimal emotion recognition performance across six evaluation metrics.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Introduction",
      "text": "Electroencephalography (EEG) is a non-invasive and portable technology that measures brain activity, enabling rapid responses to various emotional states  (Tang et al. 2023) . Recently, EEG-based emotion recognition has garnered significant attention in multimedia-induced affective computing due to its high temporal resolution and costeffectiveness  (Gong et al. 2023; Wu et al. 2023; Li, Wang, and Lu 2021) . To characterize the non-stationary and non-Figure  1 : (a) Current EEG feature selection methods intend to mine feature-label correlations via a projection subspace of original all EEG features in the emotional label space, which ignores the latent feature structure shared by all channels and all emotional dimensions; (b) To address the issue, a novel brain volume conduction effects inspired EEG feature selection method is proposed to construct a consensus latent space that aligns the multi-channel EEG feature spaces with multi-dimensional emotional label space. linear properties of EEG signals, numerous feature extraction methods have been developed (e.g., differential entropy (DE)  (Duan, Zhu, and Lu 2013) , differential asymmetry (DASM)  (Liu and Sourina 2013) ), facilitating more accurate representations of diverse emotional states.\n\nAdvances in EEG signal acquisition technology have led to an increasing number of electrodes used in emotion recognition, enabling the extraction of larger EEG feature sets  ( Özerdem and Polat 2017; Becker et al. 2017) . However, the limited availability of EEG samples often results in highdimensional feature spaces containing redundant, irrelevant, or noisy information, which can adversely affect the performance of emotion recognition systems  (Wang et al. 2020a) . Feature selection offers a strategy to identify discriminative features while eliminating extraneous ones, preserving the neural information encoded in EEG signals and enhancing arXiv:2508.05228v1 [cs.HC] 7 Aug 2025 the interpretability and transparency of emotion recognition models  (Xu et al. 2023; Jenke, Peer, and Buss 2014) .\n\nEEG feature selection methods could be classified into three categories based on their criteria for evaluating feature subsets and searching for optimal solutions: filter, wrapper, and embedded methods  (Saeys, Inza, and Larrañaga 2007) . Filter methods assess feature relevance using statistical properties of the data, but their performance often remains suboptimal regardless of the underlying learning algorithm  (Zhang et al. 2019b) . Wrapper methods address this limitation by using the performance of a specific classifier as an evaluation criterion, frequently outperforming filter methods. However, they entail extensive computational trials and high costs  (Li et al. 2017) . Recently, embedded methods have emerged as a promising alternative, integrating feature selection directly into the optimization process. Their effectiveness in EEG-based emotion recognition has been demonstrated in several studies  (Liu et al. 2018; Xu et al. 2020 Xu et al. , 2023 Xu et al. , 2024)) .\n\nThe volume conduction effects in EEG refers to the phenomenon where electrical signals generated by neuronal activity propagate through the conductive media of the brain, skull, and scalp, leading to the spatial spread and mixing of potentials recorded at scalp electrodes as a superposition of contributions from multiple neural sources (van den  Broek et al. 1998) . Hence, the volume conduction properties introduce dependencies and independencies among multichannel EEG features, indicating the presence of an intracranial latent structural hierarchy within multi-channel EEG data  (van den Broek et al. 1998; Xu et al. 2020) . However, to guide the feature selection process, current EEG feature selection methods intend to mine feature-label correlations via a projection subspace of original EEG features in the emotion label space. The aforementioned approach ignores the impact of latent EEG feature structure on multi-dimensional emotion label correlations. Additionally, these methods assume that all channels have the same influence on emotional feature selection model construction. Nevertheless, a number of studies have found that electrodes in different brain regions exhibit distinct contributions to emotion modeling  (Tao et al. 2023; Zheng, Zhu, and Lu 2019; Xu et al. 2023) .\n\nTo address these limitations, as illustrated in Fig.  1 , this paper proposes a novel channel-wise EEG feature selection (CWEFS) model for multi-dimensional emotion recognition, leveraging shared latent structure modeling and adaptive channel-weight learning. The model constructs a consensus latent space that aligns the multi-channel EEG feature spaces with multi-dimensional emotion label space, ensuring that similar EEG features are associated with similar emotional labels. Adaptive channel weighting is incorporated to automatically determine the relevance of individual channels during feature selection. Furthermore, graph-based manifold regularization learning is employed to preserve the local geometric relationships within both the EEG channel space and the multi-dimensional emotional label space, enhancing the accuracy of the consensus latent structure.\n\nThe contributions of this work are summarized as follows:",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Related Works",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Notations And Definitions",
      "text": "This section offers a brief overview of the definitions for the norms and symbols employed throughout this work. Vectors are denoted by lowercase boldface letters (e.g., x, y), whereas matrices are represented by uppercase letters (e.g., X, Y ). The transpose operation is denoted by a superscript uppercase T . The operator ⊙ denotes the Hadamard product. The trace of a matrix is denoted as Tr.\n\nThe set X = X (1) , X (2) , ..., X (v) , ..., X (ch) represents the multi-channel EEG feature data, where each element X (v) = [x 1 , x 2 , ..., x dv ] T . Specifically, X (v) ∈ R dv×n . Y ∈ {0, 1} k×n denotes the dimensional emotion label matrix. The matrices U ∈ R n×k , Q (v) ∈ R dv×k , and M ∈ R k×k are the shared common space across all EEG channels, the projection matrix for X (v) , and the coefficient matrix for Y , respectively. The variables d (v) , n, ch, and k denote the number of features for each channel, the number of instances, the number of channels, and the number of label dimensions, respectively.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Eeg Feature Selection Methods",
      "text": "Current feature selection methods for brain-computer interfaces are primarily categorized into three classes: filterbased, wrapper-based, and embedded approaches  (Saeys, Inza, and Larrañaga 2007) . Filter-based methods independently assess feature relevance using statistical or information-theoretic metrics, such as Pearson correlation coefficients (PCC), minimal redundancy maximal relevance (mRMR)  (Wang, Nie, and Lu 2011; Atkinson and Campos 2016) , and information gain  (Chen et al. 2015) . While computationally efficient, these methods may overlook feature combinations optimized for specific classifiers due to their independence from the model-building process  (Zhang et al. 2019b) . Wrapper-based methods guide feature search through classifier performance feedback, exemplified by ant colony optimization and particle swarm optimization algorithms  (Dorigo and Gambardella 1997; He et al. 2020) . However, their high computational complexity limits real-time applications  (Saeys, Inza, and Larrañaga 2007) .\n\nEmbedded methods integrate feature selection into model training, simultaneously achieving feature importance evaluation and model construction by optimizing objective functions, with regularization techniques automatically prioritizing critical features. Among these, least squares regressionbased embedded methods are widely adopted due to their statistical theoretical rigor  (Tang, Alelyani, and Liu 2014) . Several least squares regression-based embedded frameworks have been proposed or implemented for addressing the EEG feature selection issue, including: robust feature selection (RFS)  (Nie et al. 2010) , feature selection with orthogonal regression (FSOR)  (Xu et al. 2020) ,global redundancy minimization in orthogonal regression (GRMOR)  (Xu et al. 2023) , EEG feature selection for multi-dimension emotion recognition (EFSMDER)  (Xu et al. 2024) .",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Problem Formulation",
      "text": "CWEFS comprises shared latent structure learning with adaptive channel-weight and graph regularization learning. Subsequent subsections formally define their mathematical expressions.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Shared Latent Structure Learning With Adaptive Channel-Weight",
      "text": "The conventional non-negative matrix factorization model decomposes a matrix X into two non-negative matrices such that X ≈ QU T , with its cost function formulated as:\n\nwhere U represents the latent structure of the matrix X, and Q denotes the coefficient matrix. Extending this framework, the v-channel EEG feature matrix X (v) and multi-dimensional emotion label matrix Y are respectively factorized as X (v) ≈ Q (v) U T and Y ≈ M U T . By leveraging the premise that neurophysiologically similar EEG features correlate with analogous emotional states, we construct a shared latent structure space through the common matrix U . This unified representation establishes dependencies between EEG features and multidimensional emotion labels, formally expressed as:\n\nBuilding upon empirical evidence  (Tao et al. 2023; Zheng, Zhu, and Lu 2019; Xu et al. 2023 ) that EEG channels exhibit heterogeneous contributions to emotion modeling, we develop an adaptive weighting mechanism to allocate channel-specific coefficients α (v) (v = 1, . . . , ch). Inspired by brain volume conduction effects, to holistically capture cross-channel dependencies, a shared latent feature structure U is maintained across all EEG channels. In summary, Eq. (  2 ) can be changed to:",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Graph Regularization Learning",
      "text": "Drawing upon spectral graph theory  (Jian et al. 2016) , we incorporate dual manifold regularization to maintain consistency of the local geometric structures. Specifically, the term Tr U T L Y U enforces geometric consistency between the shared latent space U and the affective label space Y , while\n\nX U maintains topological fidelity within individual EEG channels. The composite graph regularization term is then formulated as:\n\nwhere the graph Laplacian matrix\n\nHere, S Y represents the affinity graph of Y , and\n\nX . The parameter β denotes a tradeoff coefficient. A heat kernel is used to generate the affinity graphs S Y and S (v) X . The similarity between two instances, x",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "(V)",
      "text": ".i and x",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "(V)",
      "text": ".j , is denoted by the element S (v) X (i, j), which is defined as follows:\n\n(5) The symbol σ denotes the graph construction parameter, while N p x (v) .j represents the set of the q nearest neighbors of the sample x (v) .j . Referring to the study  (Jian et al. 2016) , the value of σ is set to 1.\n\nFurthermore, the similarity between two labels y .i and y .j is denoted by the element S Y (i, j). S Y (i, j), which is defined as follows:\n\ny .i ∈ N q (y .j ) or y .j ∈ N q (y .i ) 0 otherwise (6) where N p (y .j ) denotes the set of the q nearest neighbors of the label y .j .",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "The Final Objective Function Of Cwefs",
      "text": "By integrating Eq. (3) and Eq. (  4 ) and imposing the l 2,1norm regularization on Q (v) , we formulate the channel-wise EEG feature selection framework as: min\n\nwhere λ, β, η, γ, and δ are tradeoff parameters. The architectural workflow of CWEFS is formally depicted in Fig.  2 .",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Optimization Strategy",
      "text": "An alternating optimization scheme is employed to compute closed-form solutions for the variables (Q (v) , U , M , and α (v) ) within the unified framework of Eq. (  7 ). The numerical procedure is implemented through the following steps:\n\nUpdate Q (v) by fixing U , M , and α (v)   Upon fixing U , M , and α (v) , we eliminate extraneous components and formally incorporate a Lagrange multiplier Ψ to enforce the non-negativity constraint Q (v) ≥ 0, yielding:\n\nThe partial derivative of L Q (v) with respect to Q (v) is:\n\n+ Ψ (9) where D (v) denotes a diagonal matrix whose diagonal entries are defined as\n\nUnder the Karush-Kuhn-Tucker (KKT) complementary conditions, Q (v) can be presented as:\n\nUpdate U by fixing Q (v) , M , and α (v)   When Q (v) , M , and α (v) are fixed, the Lagrangian function is constructed by introducing a Lagrange multiplier Θ associated with the constraint U ≥ 0, yielding:\n\nThe partial derivative of L (U ) with respect to U is:\n\nUnder the KKT complementary condition specified by Θ ij U ij = 0, U can be presented as:\n\nUpdate M by fixing Q (v) , U , and α (v)   When Q (v) , U , and α (v) are fixed, the Lagrangian function is constructed by introducing a Lagrange multiplier Φ associated with the constraint M ≥ 0, yielding:\n\nThe partial derivative of L (M ) with respect to M is:\n\n(15) Under the KKT complementary condition specified by Φ ij M ij = 0, M can be presented as:\n\nUpdate α (v) by fixing Q (v) , U , and M When all other variables are held constant, we have:\n\nX U (17) Then, the optimization problem of α (v) is changed to:\n\nThe Lagrangian function is constructed through the introduction of a Lagrange multiplier ξ corresponding to the constraint\n\nThe partial derivative of L α (v) with respect to α (v) is:\n\nSetting the partial derivative of ∂L(α (v) )\n\n∂α (v)   to zero yields:\n\nBy virtue of the constraint ch v=1 α (v) = 1, Eq. (  21 ) can be reformulated in the following form:\n\nAlgorithm 1: Iterative algorithm of CWEFS Input: 1) multi-channel EEG feature data X;\n\n2) multi-dimensional emotional matrix Y ∈ R k×n ; 3) tradeoff parameters λ, β, η, µ, and δ. Output: Return ranked EEG features.\n\n1: Initial Q (v) , U , and M randomly.\n\nUpdate Q (v) via Eq. (  10 ); 4:\n\nUpdate U via Eq. (  13 );\n\n5:\n\nUpdate M via Eq. (  16 ); 6:\n\nUpdate α (v) via Eq. (  22 ); 7: until Convergence; 8: return Q (v) for EEG feature selection. 9: Sort the EEG features by ∥Q (v) i ∥ 2 ; Algorithm 1 details the precise optimization procedure for Eq. (  7 ). The significance of each feature within the emotion recognition task is subsequently quantified via Q (v) . Through this process, the EEG feature subset is chosen.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Experiments Dataset Description",
      "text": "A comprehensive experimental evaluation was performed across three benchmark EEG datasets with multi-dimensional emotional annotations to systematically evaluate the efficacy of CWEFS, encompassing DREAMER  (Katsigiannis and Ramzan 2018) , DEAP  (Koelstra et al. 2011), and HDED (Xu et al. 2023) . The datasets implemented the valence-arousal-dominance (VAD) paradigm to characterize human affective states. Throughout multimedia stimulation protocols, synchronized EEG recordings were acquired. Detailed experimental configuration parameters and acquisition protocols are documented in the original studies  (Katsigiannis and Ramzan 2018; Koelstra et al. 2011; Xu et al. 2023) .",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Eeg Feature Extraction",
      "text": "The experimental pipeline implemented a band-pass filtering operation (1-50 Hz cutoff frequency) to eliminate noise from raw EEG recordings. Ocular and myogenic artifacts were subsequently mitigated through independent component analysis. Notably, the EEG feature extraction process was conducted on entire trial epochs as unified samples. Specifically, trial segmentation was intentionally avoided to preserve the original sample integrity and prevent artificial inflation of the experimental dataset size.\n\nBuilding upon foundational EEG affective computing methodologies  (Jenke, Peer, and Buss 2014; Xu et al. 2020) , thirteen neurophysiological feature types were systematically derived for multi-dimensional emotion recognition: C0 complexity, non-stationary index, DASM, higherorder crossing, spectral entropy, rational asymmetry, shannon entropy, DE, absolute power, the absolute power ratio of the theta band to the beta band, the amplitude of the Hilbert transform of intrinsic mode functions, the instantaneous phase of the Hilbert transform of intrinsic mode functions, and function connectivity. Detailed mathematical formalisms and physiological interpretations of the feature types are documented in  (Duan, Zhu, and Lu 2013; Jenke, Peer, and Buss 2014; Xu et al. 2023) . In summary, the feature dimensions of DREAMER, DEAP, and HDED are 651, 1756, and 7565, respectively.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Experimental Setup",
      "text": "To comprehensively assess the performance of CWEFS in the multi-dimensional affective computing task, nineteen state-of-the-art feature selection methods were included in the comparative analysis. These methods include:\n\n(1) Ten single-label feature selection approaches: ReliefF (Kononenko 1994), PCC  (Ng, Goh, and Low 1997) , CMIM  (Fleuret 2004) , mRMR  (Ding and Peng 2005) , RFS  (Nie et al. 2010) , RPMFS  (Cai, Nie, and Huang 2013) , ESFS  (Chen, Tang, and Li 2017) , FSOR  (Xu et al. 2020) , SDFS  (Wang et al. 2020b) , and GRMOR  (Xu et al. 2023) .\n\n(2) Five multi-label feature selection methods: PMU (Lee and Kim 2013), SCLS  (Lee and Kim 2017) , MFS MCDM  (Hashemi, Dowlatshahi, and Nezamabadi-pour 2020a) , MGFS  (Hashemi, Dowlatshahi, and Nezamabadi-pour 2020b) , and EFSMDER  (Xu et al. 2024) .\n\n(3) Four advanced multi-view multi-label feature selection methods: MSFS  (Zhang et al. 2020) , DHLI  (Hao, Liu, and Gao 2024) , UGRFS  (Hao, Liu, and Gao 2025) , and EF 2 FS  (Hao, Gao, and Hu 2025) .\n\nThe EEG recordings were stratified into dichotomous classes (low/high) according to self-assessed emotion dimensional scores, with an empirically established classification threshold of 5. Multi-label k-nearest neighbors (ML-KNN)  (Zhang and Zhou 2007 ) was adopted as the base classifier, where the neighborhood size and smoothing parameters were configured to k = 10 and s = 1, respectively. The dataset was partitioned through random allocation, with 80% of participants assigned to the training set and the remaining 20% retained as the test set. A cross-subject validation paradigm was employed, complemented by 50 independent trials to eliminate sampling bias. The mean performance metric across all trials served as the definitive evaluation criterion for affective computing efficacy. Six evaluation metrics were employed to assess multi-dimensional emotion recognition performance: (1) label-based metrics: macro-F1 (MA) and micro-F1 (MI); (2) instance-based metrics: average precision (AP), coverage (CV), ranking loss (RL), and hamming loss (HL). The mathematical definitions of the metrics are detailed in  (Zhang et al. 2019a) .",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Performance Comparison",
      "text": "The self-assessed VAD dimensions were stratified into dichotomous classes (high/low), thereby reformulating the     As shown in Fig.  3(a-c ), Fig.  4 (a-c), Fig.  5(a-c ), enhanced multi-dimensional affective computing performance correlates with lower values of three evaluation metrics: HL, CV, and RL. Conversely, as demonstrated in Fig.  3(d-f ), Fig.  4(d-f ), and Fig.  5(d-f ), superior performance is indicated by elevated values of three complementary metrics: AP, MA, and MI. Across all EEG feature selection ratios, CWEFS systematically attains extremal values (minima for HL/CV/RL and maxima for AP/MA/MI), outperforming nineteen benchmark feature selection methods. Collectively, the experimental evidence from Fig.  3 -5 substantiates that EEG feature subsets chosen by CWEFS achieve peak recognition efficacy across all evaluation criteria. Furthermore, the Friedman test was employed to statistically validate significant performance disparities among the twenty methods, as quantified in Table  1 . The null hypothesis was rejected, indicating significant differences in these twenty methods.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Ablation Experiment",
      "text": "To evaluate the contributions of individual modules within the CWEFS framework, we conducted controlled ablation studies. CWEFS comprises three critical components, which were systematically deactivated in sequence. As summarized in Table  2 , the adaptive channel-weight learning module plays a pivotal role in characterizing the varying impacts of individual EEG channels on feature selection model construction. The remaining modules serve to preserve the local geometric structures within both the original EEG feature space and the multi-dimensional affective label space.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Parameter Sensitivity And Convergence Analysis",
      "text": "CWEFS incorporates five tradeoff parameters in its objective function. We systematically evaluate parameter sensitivity through controlled experiments with the following configurations: the values of λ, β, η, γ, and δ are each adjusted  within the set {0.001, 0.01, 0.1, 1, 10, 100, 1000}, while γ is varied within {2, 3, 4, 5, 6, 7, 8}. With four parameters fixed at 0.1 (2 for γ), the remaining parameter is searched within its respective ranges. Owing to space constraints, only the sensitivity results for DEAP are presented. The 3D histogram in Fig.  6  quantifies AP stability. As illustrated in Fig.  6 , AP remains nearly unchanged as other parameters vary, indicating that the performance of CWEFS on DEAP is not highly sensitive to the tradeoff parameters. Furthermore, we validated the convergence properties of the CWEFS algorithm. Fig.  7  shows convergence curves of the objective value with γ = 2 and other parameters fixed at 10. As depicted in Fig.  7 , CWEFS converges rapidly within a small number of iterations, demonstrating the effectiveness of the optimization strategy.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Conclusions",
      "text": "Inspired by brain volume conduction effects, a channelwise EEG feature selection model is proposed for the multidimensional emotion recognition, integrating shared latent structure learning and adaptive channel-weight learning to address two critical challenges: (1) quantifying channelspecific contributions to emotion recognition through dynamic weighting, and (2) discovering a unified latent representation that bridges multi-channel EEG spaces and multidimensional emotional space. CWEFS is optimized using an iterative algorithm that ensures convergence and computational efficiency. Experimental results across three datasets demonstrate that CWEFS effectively selects informative EEG features for the emotion recognition task.",
      "page_start": 7,
      "page_end": 7
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: (a) Current EEG feature selection methods intend",
      "page": 1
    },
    {
      "caption": "Figure 2: The proposed CWEFS framework concludes the following two sections: (a) shared latent structure learning with",
      "page": 4
    },
    {
      "caption": "Figure 2: Optimization Strategy",
      "page": 4
    },
    {
      "caption": "Figure 3: Multi-dimensional emotion recognition perfor-",
      "page": 6
    },
    {
      "caption": "Figure 4: Multi-dimensional emotion recognition perfor-",
      "page": 6
    },
    {
      "caption": "Figure 5: Multi-dimensional emotion recognition perfor-",
      "page": 6
    },
    {
      "caption": "Figure 3: , Fig. 4, and Fig. 5. Horizon-",
      "page": 6
    },
    {
      "caption": "Figure 3: (a-c), Fig. 4(a-c), Fig. 5(a-c), enhanced",
      "page": 7
    },
    {
      "caption": "Figure 4: (d-f), and Fig. 5(d-f), superior performance is indi-",
      "page": 7
    },
    {
      "caption": "Figure 3: -5 substantiates that",
      "page": 7
    },
    {
      "caption": "Figure 6: The parameter sensitivity of CWEFS on DEAP.",
      "page": 7
    },
    {
      "caption": "Figure 7: The convergence of the CWEFS algorithm.",
      "page": 7
    },
    {
      "caption": "Figure 6: quantifies AP stability. As illustrated in",
      "page": 7
    },
    {
      "caption": "Figure 6: , AP remains nearly unchanged as other parameters",
      "page": 7
    },
    {
      "caption": "Figure 7: shows convergence curves of",
      "page": 7
    },
    {
      "caption": "Figure 7: , CWEFS converges rapidly within",
      "page": 7
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "{xxy, zhuoli}@bjut.edu.cn, 23027425@emails.bjut.edu.cn, weifulin@ahu.edu.cn": "Abstract"
        },
        {
          "{xxy, zhuoli}@bjut.edu.cn, 23027425@emails.bjut.edu.cn, weifulin@ahu.edu.cn": "Due\nto\nthe\nintracranial\nvolume\nconduction\neffects,\nhigh-"
        },
        {
          "{xxy, zhuoli}@bjut.edu.cn, 23027425@emails.bjut.edu.cn, weifulin@ahu.edu.cn": "dimensional multi-channel\nelectroencephalography\n(EEG)"
        },
        {
          "{xxy, zhuoli}@bjut.edu.cn, 23027425@emails.bjut.edu.cn, weifulin@ahu.edu.cn": "features often contain substantial redundant and irrelevant in-"
        },
        {
          "{xxy, zhuoli}@bjut.edu.cn, 23027425@emails.bjut.edu.cn, weifulin@ahu.edu.cn": "formation. This issue not only hinders the extraction of dis-"
        },
        {
          "{xxy, zhuoli}@bjut.edu.cn, 23027425@emails.bjut.edu.cn, weifulin@ahu.edu.cn": "criminative emotional representations but also compromises"
        },
        {
          "{xxy, zhuoli}@bjut.edu.cn, 23027425@emails.bjut.edu.cn, weifulin@ahu.edu.cn": "the\nreal-time performance. Feature\nselection has been es-"
        },
        {
          "{xxy, zhuoli}@bjut.edu.cn, 23027425@emails.bjut.edu.cn, weifulin@ahu.edu.cn": "tablished as an effective approach to address the challenges"
        },
        {
          "{xxy, zhuoli}@bjut.edu.cn, 23027425@emails.bjut.edu.cn, weifulin@ahu.edu.cn": "while enhancing the transparency and interpretability of emo-"
        },
        {
          "{xxy, zhuoli}@bjut.edu.cn, 23027425@emails.bjut.edu.cn, weifulin@ahu.edu.cn": "tion recognition models. However, existing EEG feature se-"
        },
        {
          "{xxy, zhuoli}@bjut.edu.cn, 23027425@emails.bjut.edu.cn, weifulin@ahu.edu.cn": "lection research overlooks the influence of\nlatent EEG fea-"
        },
        {
          "{xxy, zhuoli}@bjut.edu.cn, 23027425@emails.bjut.edu.cn, weifulin@ahu.edu.cn": "ture structures on emotional\nlabel correlations and assumes"
        },
        {
          "{xxy, zhuoli}@bjut.edu.cn, 23027425@emails.bjut.edu.cn, weifulin@ahu.edu.cn": "uniform importance across various channels, directly limit-"
        },
        {
          "{xxy, zhuoli}@bjut.edu.cn, 23027425@emails.bjut.edu.cn, weifulin@ahu.edu.cn": "ing the precise construction of EEG feature selection mod-"
        },
        {
          "{xxy, zhuoli}@bjut.edu.cn, 23027425@emails.bjut.edu.cn, weifulin@ahu.edu.cn": "els\nfor multi-dimensional\naffective\ncomputing. To address"
        },
        {
          "{xxy, zhuoli}@bjut.edu.cn, 23027425@emails.bjut.edu.cn, weifulin@ahu.edu.cn": "these limitations, a novel channel-wise EEG feature selection"
        },
        {
          "{xxy, zhuoli}@bjut.edu.cn, 23027425@emails.bjut.edu.cn, weifulin@ahu.edu.cn": "(CWEFS) method is proposed for multi-dimensional emo-"
        },
        {
          "{xxy, zhuoli}@bjut.edu.cn, 23027425@emails.bjut.edu.cn, weifulin@ahu.edu.cn": "tion recognition. Specifically,\ninspired by brain volume con-"
        },
        {
          "{xxy, zhuoli}@bjut.edu.cn, 23027425@emails.bjut.edu.cn, weifulin@ahu.edu.cn": ""
        },
        {
          "{xxy, zhuoli}@bjut.edu.cn, 23027425@emails.bjut.edu.cn, weifulin@ahu.edu.cn": "duction effects, CWEFS integrates EEG emotional\nfeature"
        },
        {
          "{xxy, zhuoli}@bjut.edu.cn, 23027425@emails.bjut.edu.cn, weifulin@ahu.edu.cn": ""
        },
        {
          "{xxy, zhuoli}@bjut.edu.cn, 23027425@emails.bjut.edu.cn, weifulin@ahu.edu.cn": "selection into a\nshared latent\nstructure model designed to"
        },
        {
          "{xxy, zhuoli}@bjut.edu.cn, 23027425@emails.bjut.edu.cn, weifulin@ahu.edu.cn": ""
        },
        {
          "{xxy, zhuoli}@bjut.edu.cn, 23027425@emails.bjut.edu.cn, weifulin@ahu.edu.cn": "construct a consensus latent space across diverse EEG chan-"
        },
        {
          "{xxy, zhuoli}@bjut.edu.cn, 23027425@emails.bjut.edu.cn, weifulin@ahu.edu.cn": ""
        },
        {
          "{xxy, zhuoli}@bjut.edu.cn, 23027425@emails.bjut.edu.cn, weifulin@ahu.edu.cn": "nels. To preserve the local geometric structure, this consensus"
        },
        {
          "{xxy, zhuoli}@bjut.edu.cn, 23027425@emails.bjut.edu.cn, weifulin@ahu.edu.cn": ""
        },
        {
          "{xxy, zhuoli}@bjut.edu.cn, 23027425@emails.bjut.edu.cn, weifulin@ahu.edu.cn": "space is further\nintegrated with the latent semantic analysis"
        },
        {
          "{xxy, zhuoli}@bjut.edu.cn, 23027425@emails.bjut.edu.cn, weifulin@ahu.edu.cn": "of multi-dimensional emotional labels. Additionally, CWEFS"
        },
        {
          "{xxy, zhuoli}@bjut.edu.cn, 23027425@emails.bjut.edu.cn, weifulin@ahu.edu.cn": "incorporates adaptive channel-weight\nlearning to automati-"
        },
        {
          "{xxy, zhuoli}@bjut.edu.cn, 23027425@emails.bjut.edu.cn, weifulin@ahu.edu.cn": "cally determine the significance of different EEG channels"
        },
        {
          "{xxy, zhuoli}@bjut.edu.cn, 23027425@emails.bjut.edu.cn, weifulin@ahu.edu.cn": "in the emotional feature selection task. The effectiveness of"
        },
        {
          "{xxy, zhuoli}@bjut.edu.cn, 23027425@emails.bjut.edu.cn, weifulin@ahu.edu.cn": "CWEFS was validated using three popular EEG datasets with"
        },
        {
          "{xxy, zhuoli}@bjut.edu.cn, 23027425@emails.bjut.edu.cn, weifulin@ahu.edu.cn": "multi-dimensional emotional\nlabels. Comprehensive experi-"
        },
        {
          "{xxy, zhuoli}@bjut.edu.cn, 23027425@emails.bjut.edu.cn, weifulin@ahu.edu.cn": "mental\nresults, compared against nineteen feature selection"
        },
        {
          "{xxy, zhuoli}@bjut.edu.cn, 23027425@emails.bjut.edu.cn, weifulin@ahu.edu.cn": "methods, demonstrate that the EEG feature subsets chosen by"
        },
        {
          "{xxy, zhuoli}@bjut.edu.cn, 23027425@emails.bjut.edu.cn, weifulin@ahu.edu.cn": "CWEFS achieve optimal emotion recognition performance"
        },
        {
          "{xxy, zhuoli}@bjut.edu.cn, 23027425@emails.bjut.edu.cn, weifulin@ahu.edu.cn": ""
        },
        {
          "{xxy, zhuoli}@bjut.edu.cn, 23027425@emails.bjut.edu.cn, weifulin@ahu.edu.cn": "across six evaluation metrics."
        },
        {
          "{xxy, zhuoli}@bjut.edu.cn, 23027425@emails.bjut.edu.cn, weifulin@ahu.edu.cn": ""
        },
        {
          "{xxy, zhuoli}@bjut.edu.cn, 23027425@emails.bjut.edu.cn, weifulin@ahu.edu.cn": ""
        },
        {
          "{xxy, zhuoli}@bjut.edu.cn, 23027425@emails.bjut.edu.cn, weifulin@ahu.edu.cn": ""
        },
        {
          "{xxy, zhuoli}@bjut.edu.cn, 23027425@emails.bjut.edu.cn, weifulin@ahu.edu.cn": "Introduction"
        },
        {
          "{xxy, zhuoli}@bjut.edu.cn, 23027425@emails.bjut.edu.cn, weifulin@ahu.edu.cn": ""
        },
        {
          "{xxy, zhuoli}@bjut.edu.cn, 23027425@emails.bjut.edu.cn, weifulin@ahu.edu.cn": "Electroencephalography\n(EEG)\nis\na\nnon-invasive\nand"
        },
        {
          "{xxy, zhuoli}@bjut.edu.cn, 23027425@emails.bjut.edu.cn, weifulin@ahu.edu.cn": ""
        },
        {
          "{xxy, zhuoli}@bjut.edu.cn, 23027425@emails.bjut.edu.cn, weifulin@ahu.edu.cn": "portable technology that measures brain activity, enabling"
        },
        {
          "{xxy, zhuoli}@bjut.edu.cn, 23027425@emails.bjut.edu.cn, weifulin@ahu.edu.cn": ""
        },
        {
          "{xxy, zhuoli}@bjut.edu.cn, 23027425@emails.bjut.edu.cn, weifulin@ahu.edu.cn": "rapid\nresponses\nto\nvarious\nemotional\nstates\n(Tang\net\nal."
        },
        {
          "{xxy, zhuoli}@bjut.edu.cn, 23027425@emails.bjut.edu.cn, weifulin@ahu.edu.cn": ""
        },
        {
          "{xxy, zhuoli}@bjut.edu.cn, 23027425@emails.bjut.edu.cn, weifulin@ahu.edu.cn": "2023). Recently, EEG-based emotion recognition has gar-"
        },
        {
          "{xxy, zhuoli}@bjut.edu.cn, 23027425@emails.bjut.edu.cn, weifulin@ahu.edu.cn": ""
        },
        {
          "{xxy, zhuoli}@bjut.edu.cn, 23027425@emails.bjut.edu.cn, weifulin@ahu.edu.cn": "nered significant attention in multimedia-induced affective"
        },
        {
          "{xxy, zhuoli}@bjut.edu.cn, 23027425@emails.bjut.edu.cn, weifulin@ahu.edu.cn": ""
        },
        {
          "{xxy, zhuoli}@bjut.edu.cn, 23027425@emails.bjut.edu.cn, weifulin@ahu.edu.cn": "computing due\nto its high temporal\nresolution and cost-"
        },
        {
          "{xxy, zhuoli}@bjut.edu.cn, 23027425@emails.bjut.edu.cn, weifulin@ahu.edu.cn": ""
        },
        {
          "{xxy, zhuoli}@bjut.edu.cn, 23027425@emails.bjut.edu.cn, weifulin@ahu.edu.cn": "effectiveness (Gong et al. 2023; Wu et al. 2023; Li, Wang,"
        },
        {
          "{xxy, zhuoli}@bjut.edu.cn, 23027425@emails.bjut.edu.cn, weifulin@ahu.edu.cn": ""
        },
        {
          "{xxy, zhuoli}@bjut.edu.cn, 23027425@emails.bjut.edu.cn, weifulin@ahu.edu.cn": "and Lu 2021). To characterize the non-stationary and non-"
        },
        {
          "{xxy, zhuoli}@bjut.edu.cn, 23027425@emails.bjut.edu.cn, weifulin@ahu.edu.cn": ""
        },
        {
          "{xxy, zhuoli}@bjut.edu.cn, 23027425@emails.bjut.edu.cn, weifulin@ahu.edu.cn": "Copyright © 2026, Association for the Advancement of Artificial"
        },
        {
          "{xxy, zhuoli}@bjut.edu.cn, 23027425@emails.bjut.edu.cn, weifulin@ahu.edu.cn": "Intelligence (www.aaai.org). All rights reserved."
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "the interpretability and transparency of emotion recognition": "models (Xu et al. 2023; Jenke, Peer, and Buss 2014).",
          "method that\nintegrates the feature selection process into": "a shared latent structure model,\nincorporating an adap-"
        },
        {
          "the interpretability and transparency of emotion recognition": "EEG feature selection methods could be classified into",
          "method that\nintegrates the feature selection process into": "tive channel-weight strategy. This approach could cap-"
        },
        {
          "the interpretability and transparency of emotion recognition": "three categories based on their criteria for evaluating fea-",
          "method that\nintegrates the feature selection process into": "ture the differential\ninfluence of\nindividual EEG chan-"
        },
        {
          "the interpretability and transparency of emotion recognition": "ture subsets and searching for optimal solutions: filter, wrap-",
          "method that\nintegrates the feature selection process into": "nels on emotion recognition while modeling a common"
        },
        {
          "the interpretability and transparency of emotion recognition": "per,\nand embedded methods\n(Saeys,\nInza,\nand Larra˜naga",
          "method that\nintegrates the feature selection process into": "latent representation that aligns multi-channel EEG fea-"
        },
        {
          "the interpretability and transparency of emotion recognition": "2007). Filter methods\nassess\nfeature\nrelevance using sta-",
          "method that\nintegrates the feature selection process into": "tures with multi-dimensional emotion labels."
        },
        {
          "the interpretability and transparency of emotion recognition": "tistical properties of\nthe data, but\ntheir performance often",
          "method that\nintegrates the feature selection process into": ""
        },
        {
          "the interpretability and transparency of emotion recognition": "",
          "method that\nintegrates the feature selection process into": "• To solve the optimization problem inherent\nin CWEFS,"
        },
        {
          "the interpretability and transparency of emotion recognition": "remains\nsuboptimal\nregardless of\nthe underlying learning",
          "method that\nintegrates the feature selection process into": ""
        },
        {
          "the interpretability and transparency of emotion recognition": "",
          "method that\nintegrates the feature selection process into": "a\nstraightforward yet\nefficient\nalternative optimization"
        },
        {
          "the interpretability and transparency of emotion recognition": "algorithm (Zhang et al. 2019b). Wrapper methods address",
          "method that\nintegrates the feature selection process into": ""
        },
        {
          "the interpretability and transparency of emotion recognition": "",
          "method that\nintegrates the feature selection process into": "scheme is developed, ensuring convergence and enabling"
        },
        {
          "the interpretability and transparency of emotion recognition": "this limitation by using the performance of a specific classi-",
          "method that\nintegrates the feature selection process into": ""
        },
        {
          "the interpretability and transparency of emotion recognition": "",
          "method that\nintegrates the feature selection process into": "systematic identification of the optimal solution."
        },
        {
          "the interpretability and transparency of emotion recognition": "fier as an evaluation criterion, frequently outperforming fil-",
          "method that\nintegrates the feature selection process into": ""
        },
        {
          "the interpretability and transparency of emotion recognition": "",
          "method that\nintegrates the feature selection process into": "• To validate\nthe\neffectiveness of CWEFS for\nthe EEG"
        },
        {
          "the interpretability and transparency of emotion recognition": "ter methods. However,\nthey entail extensive computational",
          "method that\nintegrates the feature selection process into": ""
        },
        {
          "the interpretability and transparency of emotion recognition": "",
          "method that\nintegrates the feature selection process into": "based multi-dimensional emotion feature selection task,"
        },
        {
          "the interpretability and transparency of emotion recognition": "trials and high costs (Li et al. 2017). Recently, embedded",
          "method that\nintegrates the feature selection process into": ""
        },
        {
          "the interpretability and transparency of emotion recognition": "",
          "method that\nintegrates the feature selection process into": "three\nbenchmark\ndatasets\n(DREAMER, DEAP,\nand"
        },
        {
          "the interpretability and transparency of emotion recognition": "methods have emerged as a promising alternative,\nintegrat-",
          "method that\nintegrates the feature selection process into": ""
        },
        {
          "the interpretability and transparency of emotion recognition": "",
          "method that\nintegrates the feature selection process into": "HDED) comprising multi-channel EEG recordings and"
        },
        {
          "the interpretability and transparency of emotion recognition": "ing feature selection directly into the optimization process.",
          "method that\nintegrates the feature selection process into": ""
        },
        {
          "the interpretability and transparency of emotion recognition": "",
          "method that\nintegrates the feature selection process into": "multi-dimensional emotion labels were employed. Ex-"
        },
        {
          "the interpretability and transparency of emotion recognition": "Their effectiveness in EEG-based emotion recognition has",
          "method that\nintegrates the feature selection process into": ""
        },
        {
          "the interpretability and transparency of emotion recognition": "",
          "method that\nintegrates the feature selection process into": "perimental results demonstrate that the EEG feature sub-"
        },
        {
          "the interpretability and transparency of emotion recognition": "been demonstrated in several studies (Liu et al. 2018; Xu",
          "method that\nintegrates the feature selection process into": ""
        },
        {
          "the interpretability and transparency of emotion recognition": "",
          "method that\nintegrates the feature selection process into": "sets selected by CWEFS outperform those derived from"
        },
        {
          "the interpretability and transparency of emotion recognition": "et al. 2020, 2023, 2024).",
          "method that\nintegrates the feature selection process into": ""
        },
        {
          "the interpretability and transparency of emotion recognition": "",
          "method that\nintegrates the feature selection process into": "nineteen popular\nfeature\nselection methods,\nachieving"
        },
        {
          "the interpretability and transparency of emotion recognition": "The volume conduction effects in EEG refers to the phe-",
          "method that\nintegrates the feature selection process into": ""
        },
        {
          "the interpretability and transparency of emotion recognition": "",
          "method that\nintegrates the feature selection process into": "superior multi-dimensional emotion recognition perfor-"
        },
        {
          "the interpretability and transparency of emotion recognition": "nomenon where electrical signals generated by neuronal ac-",
          "method that\nintegrates the feature selection process into": ""
        },
        {
          "the interpretability and transparency of emotion recognition": "",
          "method that\nintegrates the feature selection process into": "mance across six evaluation metrics."
        },
        {
          "the interpretability and transparency of emotion recognition": "tivity propagate through the conductive media of the brain,",
          "method that\nintegrates the feature selection process into": ""
        },
        {
          "the interpretability and transparency of emotion recognition": "skull, and scalp,\nleading to the spatial\nspread and mixing",
          "method that\nintegrates the feature selection process into": ""
        },
        {
          "the interpretability and transparency of emotion recognition": "",
          "method that\nintegrates the feature selection process into": "Related Works"
        },
        {
          "the interpretability and transparency of emotion recognition": "of potentials\nrecorded at\nscalp electrodes as a superposi-",
          "method that\nintegrates the feature selection process into": ""
        },
        {
          "the interpretability and transparency of emotion recognition": "tion of contributions from multiple neural sources (van den",
          "method that\nintegrates the feature selection process into": ""
        },
        {
          "the interpretability and transparency of emotion recognition": "",
          "method that\nintegrates the feature selection process into": "Notations and definitions"
        },
        {
          "the interpretability and transparency of emotion recognition": "Broek et al. 1998). Hence, the volume conduction properties",
          "method that\nintegrates the feature selection process into": ""
        },
        {
          "the interpretability and transparency of emotion recognition": "",
          "method that\nintegrates the feature selection process into": "This section offers a brief overview of the definitions for the"
        },
        {
          "the interpretability and transparency of emotion recognition": "introduce dependencies and independencies among multi-",
          "method that\nintegrates the feature selection process into": ""
        },
        {
          "the interpretability and transparency of emotion recognition": "",
          "method that\nintegrates the feature selection process into": "norms and symbols employed throughout\nthis work. Vec-"
        },
        {
          "the interpretability and transparency of emotion recognition": "channel EEG features, indicating the presence of an intracra-",
          "method that\nintegrates the feature selection process into": ""
        },
        {
          "the interpretability and transparency of emotion recognition": "",
          "method that\nintegrates the feature selection process into": "tors are denoted by lowercase boldface letters (e.g., x, y),"
        },
        {
          "the interpretability and transparency of emotion recognition": "nial\nlatent\nstructural hierarchy within multi-channel EEG",
          "method that\nintegrates the feature selection process into": ""
        },
        {
          "the interpretability and transparency of emotion recognition": "",
          "method that\nintegrates the feature selection process into": "whereas matrices are represented by uppercase letters (e.g.,"
        },
        {
          "the interpretability and transparency of emotion recognition": "data (van den Broek et al. 1998; Xu et al. 2020). However, to",
          "method that\nintegrates the feature selection process into": ""
        },
        {
          "the interpretability and transparency of emotion recognition": "",
          "method that\nintegrates the feature selection process into": "X, Y ). The transpose operation is denoted by a superscript"
        },
        {
          "the interpretability and transparency of emotion recognition": "guide the feature selection process, current EEG feature se-",
          "method that\nintegrates the feature selection process into": ""
        },
        {
          "the interpretability and transparency of emotion recognition": "",
          "method that\nintegrates the feature selection process into": "uppercase T . The operator ⊙ denotes the Hadamard product."
        },
        {
          "the interpretability and transparency of emotion recognition": "lection methods intend to mine feature-label correlations via",
          "method that\nintegrates the feature selection process into": ""
        },
        {
          "the interpretability and transparency of emotion recognition": "",
          "method that\nintegrates the feature selection process into": "The trace of a matrix is denoted as Tr."
        },
        {
          "the interpretability and transparency of emotion recognition": "a projection subspace of original EEG features in the emo-",
          "method that\nintegrates the feature selection process into": ""
        },
        {
          "the interpretability and transparency of emotion recognition": "",
          "method that\nintegrates the feature selection process into": "The\nset X = (cid:8)X (1), X (2), ..., X (v), ..., X (ch)(cid:9) repre-"
        },
        {
          "the interpretability and transparency of emotion recognition": "tion label space. The aforementioned approach ignores the",
          "method that\nintegrates the feature selection process into": ""
        },
        {
          "the interpretability and transparency of emotion recognition": "",
          "method that\nintegrates the feature selection process into": "sents\nthe multi-channel EEG feature data, where each el-"
        },
        {
          "the interpretability and transparency of emotion recognition": "impact of latent EEG feature structure on multi-dimensional",
          "method that\nintegrates the feature selection process into": ""
        },
        {
          "the interpretability and transparency of emotion recognition": "",
          "method that\nintegrates the feature selection process into": "=\n∈\nement X (v)\n[x1, x2, ..., xdv ]T . Specifically, X (v)"
        },
        {
          "the interpretability and transparency of emotion recognition": "emotion label correlations. Additionally,\nthese methods as-",
          "method that\nintegrates the feature selection process into": ""
        },
        {
          "the interpretability and transparency of emotion recognition": "",
          "method that\nintegrates the feature selection process into": "Rdv×n. Y\n∈ {0, 1}k×n denotes the dimensional emotion"
        },
        {
          "the interpretability and transparency of emotion recognition": "sume that all channels have the same influence on emotional",
          "method that\nintegrates the feature selection process into": ""
        },
        {
          "the interpretability and transparency of emotion recognition": "",
          "method that\nintegrates the feature selection process into": "label matrix. The matrices U ∈ Rn×k, Q(v) ∈ Rdv×k, and"
        },
        {
          "the interpretability and transparency of emotion recognition": "feature selection model construction. Nevertheless, a num-",
          "method that\nintegrates the feature selection process into": ""
        },
        {
          "the interpretability and transparency of emotion recognition": "",
          "method that\nintegrates the feature selection process into": "M ∈ Rk×k are the shared common space across all EEG"
        },
        {
          "the interpretability and transparency of emotion recognition": "ber of studies have found that electrodes in different brain",
          "method that\nintegrates the feature selection process into": ""
        },
        {
          "the interpretability and transparency of emotion recognition": "",
          "method that\nintegrates the feature selection process into": "channels, the projection matrix for X (v), and the coefficient"
        },
        {
          "the interpretability and transparency of emotion recognition": "regions exhibit distinct contributions to emotion modeling",
          "method that\nintegrates the feature selection process into": ""
        },
        {
          "the interpretability and transparency of emotion recognition": "(Tao et al. 2023; Zheng, Zhu, and Lu 2019; Xu et al. 2023).",
          "method that\nintegrates the feature selection process into": "matrix for Y , respectively. The variables d(v), n, ch, and k"
        },
        {
          "the interpretability and transparency of emotion recognition": "To address these limitations, as illustrated in Fig.1,\nthis",
          "method that\nintegrates the feature selection process into": "denote the number of\nfeatures for each channel,\nthe num-"
        },
        {
          "the interpretability and transparency of emotion recognition": "paper proposes\na novel\nchannel-wise EEG feature\nselec-",
          "method that\nintegrates the feature selection process into": "ber of instances, the number of channels, and the number of"
        },
        {
          "the interpretability and transparency of emotion recognition": "tion (CWEFS) model for multi-dimensional emotion recog-",
          "method that\nintegrates the feature selection process into": "label dimensions, respectively."
        },
        {
          "the interpretability and transparency of emotion recognition": "nition, leveraging shared latent structure modeling and adap-",
          "method that\nintegrates the feature selection process into": ""
        },
        {
          "the interpretability and transparency of emotion recognition": "tive channel-weight\nlearning. The model constructs a con-",
          "method that\nintegrates the feature selection process into": "EEG feature selection methods"
        },
        {
          "the interpretability and transparency of emotion recognition": "sensus latent space that aligns the multi-channel EEG fea-",
          "method that\nintegrates the feature selection process into": ""
        },
        {
          "the interpretability and transparency of emotion recognition": "",
          "method that\nintegrates the feature selection process into": "Current\nfeature\nselection methods\nfor brain-computer\nin-"
        },
        {
          "the interpretability and transparency of emotion recognition": "ture spaces with multi-dimensional emotion label space, en-",
          "method that\nintegrates the feature selection process into": ""
        },
        {
          "the interpretability and transparency of emotion recognition": "",
          "method that\nintegrates the feature selection process into": "terfaces are primarily categorized into three classes: filter-"
        },
        {
          "the interpretability and transparency of emotion recognition": "suring that similar EEG features are associated with simi-",
          "method that\nintegrates the feature selection process into": ""
        },
        {
          "the interpretability and transparency of emotion recognition": "",
          "method that\nintegrates the feature selection process into": "based, wrapper-based,\nand\nembedded\napproaches(Saeys,"
        },
        {
          "the interpretability and transparency of emotion recognition": "lar emotional labels. Adaptive channel weighting is incorpo-",
          "method that\nintegrates the feature selection process into": ""
        },
        {
          "the interpretability and transparency of emotion recognition": "",
          "method that\nintegrates the feature selection process into": "Inza,\nand\nLarra˜naga\n2007).\nFilter-based methods\ninde-"
        },
        {
          "the interpretability and transparency of emotion recognition": "rated to automatically determine the relevance of individual",
          "method that\nintegrates the feature selection process into": ""
        },
        {
          "the interpretability and transparency of emotion recognition": "",
          "method that\nintegrates the feature selection process into": "pendently\nassess\nfeature\nrelevance\nusing\nstatistical\nor"
        },
        {
          "the interpretability and transparency of emotion recognition": "channels during feature selection. Furthermore, graph-based",
          "method that\nintegrates the feature selection process into": ""
        },
        {
          "the interpretability and transparency of emotion recognition": "",
          "method that\nintegrates the feature selection process into": "information-theoretic metrics,\nsuch as Pearson correlation"
        },
        {
          "the interpretability and transparency of emotion recognition": "manifold regularization learning is employed to preserve the",
          "method that\nintegrates the feature selection process into": ""
        },
        {
          "the interpretability and transparency of emotion recognition": "",
          "method that\nintegrates the feature selection process into": "coefficients (PCC), minimal redundancy maximal relevance"
        },
        {
          "the interpretability and transparency of emotion recognition": "local geometric relationships within both the EEG channel",
          "method that\nintegrates the feature selection process into": ""
        },
        {
          "the interpretability and transparency of emotion recognition": "",
          "method that\nintegrates the feature selection process into": "(mRMR)(Wang, Nie,\nand Lu\n2011; Atkinson\nand Cam-"
        },
        {
          "the interpretability and transparency of emotion recognition": "space and the multi-dimensional emotional\nlabel space, en-",
          "method that\nintegrates the feature selection process into": ""
        },
        {
          "the interpretability and transparency of emotion recognition": "",
          "method that\nintegrates the feature selection process into": "pos 2016), and information gain(Chen et al. 2015). While"
        },
        {
          "the interpretability and transparency of emotion recognition": "hancing the accuracy of the consensus latent structure.",
          "method that\nintegrates the feature selection process into": ""
        },
        {
          "the interpretability and transparency of emotion recognition": "",
          "method that\nintegrates the feature selection process into": "computationally efficient,\nthese methods may overlook fea-"
        },
        {
          "the interpretability and transparency of emotion recognition": "The contributions of this work are summarized as follows:",
          "method that\nintegrates the feature selection process into": ""
        },
        {
          "the interpretability and transparency of emotion recognition": "",
          "method that\nintegrates the feature selection process into": "ture combinations optimized for specific classifiers due to"
        },
        {
          "the interpretability and transparency of emotion recognition": "• Inspired by brain volume conduction effects,\nthis study",
          "method that\nintegrates the feature selection process into": "their independence from the model-building process(Zhang"
        },
        {
          "the interpretability and transparency of emotion recognition": "introduces a channel-wise EEG emotion feature selection",
          "method that\nintegrates the feature selection process into": "et al. 2019b). Wrapper-based methods guide feature search"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "through classifier performance feedback, exemplified by ant": "colony optimization and particle swarm optimization algo-",
          "cross-channel dependencies, a shared latent\nfeature struc-": "ture U is maintained across all EEG channels. In summary,"
        },
        {
          "through classifier performance feedback, exemplified by ant": "rithms(Dorigo and Gambardella 1997; He et al. 2020). How-",
          "cross-channel dependencies, a shared latent\nfeature struc-": "Eq. (2) can be changed to:"
        },
        {
          "through classifier performance feedback, exemplified by ant": "ever,\ntheir high computational complexity limits\nreal-time",
          "cross-channel dependencies, a shared latent\nfeature struc-": ""
        },
        {
          "through classifier performance feedback, exemplified by ant": "",
          "cross-channel dependencies, a shared latent\nfeature struc-": "ch"
        },
        {
          "through classifier performance feedback, exemplified by ant": "",
          "cross-channel dependencies, a shared latent\nfeature struc-": ""
        },
        {
          "through classifier performance feedback, exemplified by ant": "",
          "cross-channel dependencies, a shared latent\nfeature struc-": "2\n(cid:16)"
        },
        {
          "through classifier performance feedback, exemplified by ant": "applications(Saeys, Inza, and Larra˜naga 2007).",
          "cross-channel dependencies, a shared latent\nfeature struc-": "(cid:88) v\n(cid:13)(cid:13)\n(cid:13)(cid:13)\n+ λ (cid:13)\nmin\nX (v) − Q(v)U T (cid:13)\nα(v)(cid:17)γ (cid:18)(cid:13)"
        },
        {
          "through classifier performance feedback, exemplified by ant": "Embedded methods integrate feature selection into model",
          "cross-channel dependencies, a shared latent\nfeature struc-": "F\nQ(v),M,"
        },
        {
          "through classifier performance feedback, exemplified by ant": "",
          "cross-channel dependencies, a shared latent\nfeature struc-": "=1"
        },
        {
          "through classifier performance feedback, exemplified by ant": "training, simultaneously achieving feature importance eval-",
          "cross-channel dependencies, a shared latent\nfeature struc-": "α(v),U"
        },
        {
          "through classifier performance feedback, exemplified by ant": "uation and model construction by optimizing objective func-",
          "cross-channel dependencies, a shared latent\nfeature struc-": ""
        },
        {
          "through classifier performance feedback, exemplified by ant": "",
          "cross-channel dependencies, a shared latent\nfeature struc-": "s.t. Q(v) ≥ 0, M ≥ 0, U ≥ 0,"
        },
        {
          "through classifier performance feedback, exemplified by ant": "tions, with regularization techniques automatically prioritiz-",
          "cross-channel dependencies, a shared latent\nfeature struc-": ""
        },
        {
          "through classifier performance feedback, exemplified by ant": "",
          "cross-channel dependencies, a shared latent\nfeature struc-": "ch"
        },
        {
          "through classifier performance feedback, exemplified by ant": "ing critical features. Among these, least squares regression-",
          "cross-channel dependencies, a shared latent\nfeature struc-": ""
        },
        {
          "through classifier performance feedback, exemplified by ant": "",
          "cross-channel dependencies, a shared latent\nfeature struc-": "α(v) = 1, 0 ≤ α(v) ≤ 1, v = 1, . . . , ch"
        },
        {
          "through classifier performance feedback, exemplified by ant": "based embedded methods are widely adopted due to their",
          "cross-channel dependencies, a shared latent\nfeature struc-": "(cid:88) v"
        },
        {
          "through classifier performance feedback, exemplified by ant": "statistical\ntheoretical\nrigor(Tang, Alelyani, and Liu 2014).",
          "cross-channel dependencies, a shared latent\nfeature struc-": "=1"
        },
        {
          "through classifier performance feedback, exemplified by ant": "",
          "cross-channel dependencies, a shared latent\nfeature struc-": "(3)"
        },
        {
          "through classifier performance feedback, exemplified by ant": "Several\nleast\nsquares\nregression-based\nembedded\nframe-",
          "cross-channel dependencies, a shared latent\nfeature struc-": ""
        },
        {
          "through classifier performance feedback, exemplified by ant": "works have been proposed or\nimplemented for addressing",
          "cross-channel dependencies, a shared latent\nfeature struc-": ""
        },
        {
          "through classifier performance feedback, exemplified by ant": "",
          "cross-channel dependencies, a shared latent\nfeature struc-": "Graph regularization learning"
        },
        {
          "through classifier performance feedback, exemplified by ant": "the EEG feature selection issue, including: robust feature se-",
          "cross-channel dependencies, a shared latent\nfeature struc-": ""
        },
        {
          "through classifier performance feedback, exemplified by ant": "",
          "cross-channel dependencies, a shared latent\nfeature struc-": "Drawing upon spectral graph theory (Jian et al. 2016), we"
        },
        {
          "through classifier performance feedback, exemplified by ant": "lection (RFS)(Nie et al. 2010), feature selection with orthog-",
          "cross-channel dependencies, a shared latent\nfeature struc-": ""
        },
        {
          "through classifier performance feedback, exemplified by ant": "",
          "cross-channel dependencies, a shared latent\nfeature struc-": "incorporate dual manifold regularization to maintain consis-"
        },
        {
          "through classifier performance feedback, exemplified by ant": "onal regression (FSOR) (Xu et al. 2020),global redundancy",
          "cross-channel dependencies, a shared latent\nfeature struc-": ""
        },
        {
          "through classifier performance feedback, exemplified by ant": "",
          "cross-channel dependencies, a shared latent\nfeature struc-": "tency of the local geometric structures. Specifically, the term"
        },
        {
          "through classifier performance feedback, exemplified by ant": "minimization in orthogonal regression (GRMOR)(Xu et al.",
          "cross-channel dependencies, a shared latent\nfeature struc-": ""
        },
        {
          "through classifier performance feedback, exemplified by ant": "2023), EEG feature selection for multi-dimension emotion",
          "cross-channel dependencies, a shared latent\nfeature struc-": "Tr (cid:0)U T LY U (cid:1) enforces geometric consistency between the"
        },
        {
          "through classifier performance feedback, exemplified by ant": "",
          "cross-channel dependencies, a shared latent\nfeature struc-": "shared latent space U and the affective label space Y , while"
        },
        {
          "through classifier performance feedback, exemplified by ant": "recognition (EFSMDER)(Xu et al. 2024).",
          "cross-channel dependencies, a shared latent\nfeature struc-": ""
        },
        {
          "through classifier performance feedback, exemplified by ant": "",
          "cross-channel dependencies, a shared latent\nfeature struc-": "(cid:16)\n(cid:17)"
        },
        {
          "through classifier performance feedback, exemplified by ant": "",
          "cross-channel dependencies, a shared latent\nfeature struc-": "Tr\nU T L(v)"
        },
        {
          "through classifier performance feedback, exemplified by ant": "",
          "cross-channel dependencies, a shared latent\nfeature struc-": "maintains topological fidelity within indi-\nX U"
        },
        {
          "through classifier performance feedback, exemplified by ant": "Problem formulation",
          "cross-channel dependencies, a shared latent\nfeature struc-": "vidual EEG channels. The composite graph regularization"
        },
        {
          "through classifier performance feedback, exemplified by ant": "",
          "cross-channel dependencies, a shared latent\nfeature struc-": "term is then formulated as:"
        },
        {
          "through classifier performance feedback, exemplified by ant": "CWEFS comprises\nshared\nlatent\nstructure\nlearning with",
          "cross-channel dependencies, a shared latent\nfeature struc-": ""
        },
        {
          "through classifier performance feedback, exemplified by ant": "",
          "cross-channel dependencies, a shared latent\nfeature struc-": "(cid:16)\n(cid:17)"
        },
        {
          "through classifier performance feedback, exemplified by ant": "adaptive channel-weight and graph regularization learning.",
          "cross-channel dependencies, a shared latent\nfeature struc-": "min\nU T L(v)\ns.t. U ≥ 0\nTr (cid:0)U T LY U (cid:1) + β Tr"
        },
        {
          "through classifier performance feedback, exemplified by ant": "",
          "cross-channel dependencies, a shared latent\nfeature struc-": "(4)\nX U"
        },
        {
          "through classifier performance feedback, exemplified by ant": "Subsequent subsections formally define their mathematical",
          "cross-channel dependencies, a shared latent\nfeature struc-": "U"
        },
        {
          "through classifier performance feedback, exemplified by ant": "expressions.",
          "cross-channel dependencies, a shared latent\nfeature struc-": ""
        },
        {
          "through classifier performance feedback, exemplified by ant": "",
          "cross-channel dependencies, a shared latent\nfeature struc-": "∈ Rn×n for Y\nis\nwhere the graph Laplacian matrix LY"
        },
        {
          "through classifier performance feedback, exemplified by ant": "",
          "cross-channel dependencies, a shared latent\nfeature struc-": "represents\nthe\nindicated as LY\n= GY − SY . Here, SY"
        },
        {
          "through classifier performance feedback, exemplified by ant": "Shared latent structure learning with adaptive",
          "cross-channel dependencies, a shared latent\nfeature struc-": ""
        },
        {
          "through classifier performance feedback, exemplified by ant": "",
          "cross-channel dependencies, a shared latent\nfeature struc-": "denotes\nthe diagonal matrix\naffinity graph of Y , and GY"
        },
        {
          "through classifier performance feedback, exemplified by ant": "channel-weight",
          "cross-channel dependencies, a shared latent\nfeature struc-": "with GY (i, i) = (cid:80)n"
        },
        {
          "through classifier performance feedback, exemplified by ant": "",
          "cross-channel dependencies, a shared latent\nfeature struc-": "j=1 SY (i, j). Similarly, the graph Lapla-"
        },
        {
          "through classifier performance feedback, exemplified by ant": "The conventional non-negative matrix factorization model",
          "cross-channel dependencies, a shared latent\nfeature struc-": "=\ncian matrix L(v)\n∈ Rn×n for X (v)\nis defined by L(v)"
        },
        {
          "through classifier performance feedback, exemplified by ant": "",
          "cross-channel dependencies, a shared latent\nfeature struc-": "X\nX"
        },
        {
          "through classifier performance feedback, exemplified by ant": "decomposes a matrix X into two non-negative matrices such",
          "cross-channel dependencies, a shared latent\nfeature struc-": ""
        },
        {
          "through classifier performance feedback, exemplified by ant": "",
          "cross-channel dependencies, a shared latent\nfeature struc-": "G(v)\nX . The parameter β denotes a tradeoff coefficient.\nX − S(v)"
        },
        {
          "through classifier performance feedback, exemplified by ant": "that X ≈ QU T , with its cost function formulated as:",
          "cross-channel dependencies, a shared latent\nfeature struc-": ""
        },
        {
          "through classifier performance feedback, exemplified by ant": "",
          "cross-channel dependencies, a shared latent\nfeature struc-": "A heat kernel\nis used to generate the affinity graphs SY"
        },
        {
          "through classifier performance feedback, exemplified by ant": "(cid:13)(cid:13)\n2 F\nmin\nX − QU T (cid:13)\ns.t. Q ≥ 0, U ≥ 0",
          "cross-channel dependencies, a shared latent\nfeature struc-": "and\nX . The similarity between two instances, x(v)"
        },
        {
          "through classifier performance feedback, exemplified by ant": "(1)",
          "cross-channel dependencies, a shared latent\nfeature struc-": ""
        },
        {
          "through classifier performance feedback, exemplified by ant": "Q,U",
          "cross-channel dependencies, a shared latent\nfeature struc-": "x(v)\nis denoted by the element S(v)"
        },
        {
          "through classifier performance feedback, exemplified by ant": "",
          "cross-channel dependencies, a shared latent\nfeature struc-": ",\n.j\nX (i, j), which is defined"
        },
        {
          "through classifier performance feedback, exemplified by ant": "",
          "cross-channel dependencies, a shared latent\nfeature struc-": "as follows:"
        },
        {
          "through classifier performance feedback, exemplified by ant": "where U represents the latent structure of the matrix X, and",
          "cross-channel dependencies, a shared latent\nfeature struc-": ""
        },
        {
          "through classifier performance feedback, exemplified by ant": "",
          "cross-channel dependencies, a shared latent\nfeature struc-": "2\n(cid:32)\n(cid:33)"
        },
        {
          "through classifier performance feedback, exemplified by ant": "Q denotes the coefficient matrix.",
          "cross-channel dependencies, a shared latent\nfeature struc-": ""
        },
        {
          "through classifier performance feedback, exemplified by ant": "",
          "cross-channel dependencies, a shared latent\nfeature struc-": "(cid:13)(cid:13)(cid:13)\n(cid:13)(cid:13)(cid:13)\n(cid:16)\n(cid:17)\n.i −x(v)"
        },
        {
          "through classifier performance feedback, exemplified by ant": "",
          "cross-channel dependencies, a shared latent\nfeature struc-": "exp\n−\nx(v)\nx(v)\n∈ Nq"
        },
        {
          "through classifier performance feedback, exemplified by ant": "Extending this\nframework,\nthe v-channel EEG feature",
          "cross-channel dependencies, a shared latent\nfeature struc-": ".i\n.j\nσ2"
        },
        {
          "through classifier performance feedback, exemplified by ant": "matrix X (v)\nand multi-dimensional\nemotion label matrix",
          "cross-channel dependencies, a shared latent\nfeature struc-": " \nS(v)"
        },
        {
          "through classifier performance feedback, exemplified by ant": "",
          "cross-channel dependencies, a shared latent\nfeature struc-": "X (i, j) =\n(cid:16)"
        },
        {
          "through classifier performance feedback, exemplified by ant": "Y\n≈\nQ(v)U T\nare\nrespectively\nfactorized\nas X (v)\nand",
          "cross-channel dependencies, a shared latent\nfeature struc-": "x(v)\n∈ Nq"
        },
        {
          "through classifier performance feedback, exemplified by ant": "",
          "cross-channel dependencies, a shared latent\nfeature struc-": ".j\n.i"
        },
        {
          "through classifier performance feedback, exemplified by ant": "Y ≈ M U T . By leveraging the premise that neurophysiolog-",
          "cross-channel dependencies, a shared latent\nfeature struc-": "0"
        },
        {
          "through classifier performance feedback, exemplified by ant": "",
          "cross-channel dependencies, a shared latent\nfeature struc-": "otherwise"
        },
        {
          "through classifier performance feedback, exemplified by ant": "ically similar EEG features correlate with analogous emo-",
          "cross-channel dependencies, a shared latent\nfeature struc-": ""
        },
        {
          "through classifier performance feedback, exemplified by ant": "",
          "cross-channel dependencies, a shared latent\nfeature struc-": "(5)"
        },
        {
          "through classifier performance feedback, exemplified by ant": "tional\nstates, we construct a shared latent\nstructure space",
          "cross-channel dependencies, a shared latent\nfeature struc-": ""
        },
        {
          "through classifier performance feedback, exemplified by ant": "",
          "cross-channel dependencies, a shared latent\nfeature struc-": "The symbol σ denotes the graph construction parameter,"
        },
        {
          "through classifier performance feedback, exemplified by ant": "through the common matrix U . This unified representation",
          "cross-channel dependencies, a shared latent\nfeature struc-": "(cid:17)\n(cid:16)"
        },
        {
          "through classifier performance feedback, exemplified by ant": "",
          "cross-channel dependencies, a shared latent\nfeature struc-": "x(v)\nrepresents the set of\nthe q nearest neigh-\nwhile Np"
        },
        {
          "through classifier performance feedback, exemplified by ant": "",
          "cross-channel dependencies, a shared latent\nfeature struc-": ".j"
        },
        {
          "through classifier performance feedback, exemplified by ant": "establishes dependencies between EEG features and multi-",
          "cross-channel dependencies, a shared latent\nfeature struc-": ""
        },
        {
          "through classifier performance feedback, exemplified by ant": "dimensional emotion labels, formally expressed as:",
          "cross-channel dependencies, a shared latent\nfeature struc-": "bors of\nthe sample x(v)\n. Referring to the study(Jian et al."
        },
        {
          "through classifier performance feedback, exemplified by ant": "",
          "cross-channel dependencies, a shared latent\nfeature struc-": ".j"
        },
        {
          "through classifier performance feedback, exemplified by ant": "",
          "cross-channel dependencies, a shared latent\nfeature struc-": "2016), the value of σ is set to 1."
        },
        {
          "through classifier performance feedback, exemplified by ant": "2",
          "cross-channel dependencies, a shared latent\nfeature struc-": ""
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Figure 2: The proposed CWEFS framework concludes the following two sections:\n(a) shared latent structure learning with": "adaptive channel-weight; (b) graph regularization learning."
        },
        {
          "Figure 2: The proposed CWEFS framework concludes the following two sections:\n(a) shared latent structure learning with": "The partial derivative of L (cid:0)Q(v)(cid:1) with respect to Q(v) is:\nThe final objective function of CWEFS"
        },
        {
          "Figure 2: The proposed CWEFS framework concludes the following two sections:\n(a) shared latent structure learning with": "By integrating Eq.\n(3) and Eq.\n(4) and imposing the l2,1-"
        },
        {
          "Figure 2: The proposed CWEFS framework concludes the following two sections:\n(a) shared latent structure learning with": "∂L (cid:0)Q(v)(cid:1)\n(cid:16)"
        },
        {
          "Figure 2: The proposed CWEFS framework concludes the following two sections:\n(a) shared latent structure learning with": "norm regularization on Q(v), we formulate the channel-wise\n=\nα(v)(cid:17)γ (cid:16)\n2Q(v)U T U − 2X (v)U + 2δD(v)Q(v)(cid:17)"
        },
        {
          "Figure 2: The proposed CWEFS framework concludes the following two sections:\n(a) shared latent structure learning with": "∂Q(v)\nEEG feature selection framework as:"
        },
        {
          "Figure 2: The proposed CWEFS framework concludes the following two sections:\n(a) shared latent structure learning with": "ch"
        },
        {
          "Figure 2: The proposed CWEFS framework concludes the following two sections:\n(a) shared latent structure learning with": "2\n+ Ψ"
        },
        {
          "Figure 2: The proposed CWEFS framework concludes the following two sections:\n(a) shared latent structure learning with": "(cid:13)(cid:13)\n(cid:13)(cid:13)\n2 F\n(cid:16)\nmin\nα(v)(cid:17)γ (cid:18)(cid:13)\nX (v) − Q(v)U T (cid:13)\n+ λ (cid:13)\n(cid:13)Y − M U T (cid:13)"
        },
        {
          "Figure 2: The proposed CWEFS framework concludes the following two sections:\n(a) shared latent structure learning with": "(cid:88) v\n(9)"
        },
        {
          "Figure 2: The proposed CWEFS framework concludes the following two sections:\n(a) shared latent structure learning with": "F\nQ(v),M,"
        },
        {
          "Figure 2: The proposed CWEFS framework concludes the following two sections:\n(a) shared latent structure learning with": "=1\nwhere D(v) denotes a diagonal matrix whose diagonal en-"
        },
        {
          "Figure 2: The proposed CWEFS framework concludes the following two sections:\n(a) shared latent structure learning with": "α(v),U"
        },
        {
          "Figure 2: The proposed CWEFS framework concludes the following two sections:\n(a) shared latent structure learning with": "1"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "By virtue of the constraint (cid:80)ch": ""
        },
        {
          "By virtue of the constraint (cid:80)ch": "be reformulated in the following form:"
        },
        {
          "By virtue of the constraint (cid:80)ch": "1"
        },
        {
          "By virtue of the constraint (cid:80)ch": "1−γ\n(cid:0)e(v)(cid:1)"
        },
        {
          "By virtue of the constraint (cid:80)ch": ""
        },
        {
          "By virtue of the constraint (cid:80)ch": "α(v) =\n1"
        },
        {
          "By virtue of the constraint (cid:80)ch": ""
        },
        {
          "By virtue of the constraint (cid:80)ch": "1−γ\n(cid:80)ch\n(cid:0)e(v)(cid:1)"
        },
        {
          "By virtue of the constraint (cid:80)ch": "v=1"
        },
        {
          "By virtue of the constraint (cid:80)ch": ""
        },
        {
          "By virtue of the constraint (cid:80)ch": ""
        },
        {
          "By virtue of the constraint (cid:80)ch": "Algorithm 1: Iterative algorithm of CWEFS"
        },
        {
          "By virtue of the constraint (cid:80)ch": ""
        },
        {
          "By virtue of the constraint (cid:80)ch": "Input: 1) multi-channel EEG feature data X;"
        },
        {
          "By virtue of the constraint (cid:80)ch": ""
        },
        {
          "By virtue of the constraint (cid:80)ch": "3) tradeoff parameters λ, β, η, µ, and δ."
        },
        {
          "By virtue of the constraint (cid:80)ch": "Output: Return ranked EEG features."
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Update M by fixing Q(v), U , and α(v)": ""
        },
        {
          "Update M by fixing Q(v), U , and α(v)": "When Q(v), U , and α(v) are fixed,\nthe Lagrangian function"
        },
        {
          "Update M by fixing Q(v), U , and α(v)": "is constructed by introducing a Lagrange multiplier Φ asso-"
        },
        {
          "Update M by fixing Q(v), U , and α(v)": "ciated with the constraint M ≥ 0, yielding:"
        },
        {
          "Update M by fixing Q(v), U , and α(v)": ""
        },
        {
          "Update M by fixing Q(v), U , and α(v)": "ch"
        },
        {
          "Update M by fixing Q(v), U , and α(v)": "(cid:17)\n(cid:16)"
        },
        {
          "Update M by fixing Q(v), U , and α(v)": "(cid:88) v\n2 F\n+ Tr (cid:0)ΦT M (cid:1)\nλ (cid:13)\nL (M ) =\nα(v)(cid:17)γ (cid:16)\n(cid:13)Y − M U T (cid:13)"
        },
        {
          "Update M by fixing Q(v), U , and α(v)": ""
        },
        {
          "Update M by fixing Q(v), U , and α(v)": "=1"
        },
        {
          "Update M by fixing Q(v), U , and α(v)": ""
        },
        {
          "Update M by fixing Q(v), U , and α(v)": "(14)"
        },
        {
          "Update M by fixing Q(v), U , and α(v)": ""
        },
        {
          "Update M by fixing Q(v), U , and α(v)": "The partial derivative of L (M ) with respect to M is:"
        },
        {
          "Update M by fixing Q(v), U , and α(v)": ""
        },
        {
          "Update M by fixing Q(v), U , and α(v)": "ch"
        },
        {
          "Update M by fixing Q(v), U , and α(v)": "(cid:16)"
        },
        {
          "Update M by fixing Q(v), U , and α(v)": "∂L (M )\n=\nα(v)(cid:17)γ (cid:0)2λM U T U − 2λY U (cid:1) + Φ"
        },
        {
          "Update M by fixing Q(v), U , and α(v)": "(cid:88) v"
        },
        {
          "Update M by fixing Q(v), U , and α(v)": "∂M"
        },
        {
          "Update M by fixing Q(v), U , and α(v)": "=1"
        },
        {
          "Update M by fixing Q(v), U , and α(v)": ""
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "2020),\nthirteen neurophysiological\nfeature types were sys-": "tematically derived for multi-dimensional emotion recogni-"
        },
        {
          "2020),\nthirteen neurophysiological\nfeature types were sys-": "tion: C0 complexity, non-stationary index, DASM, higher-"
        },
        {
          "2020),\nthirteen neurophysiological\nfeature types were sys-": "order crossing, spectral entropy,\nrational asymmetry, shan-"
        },
        {
          "2020),\nthirteen neurophysiological\nfeature types were sys-": "non entropy, DE, absolute power,\nthe absolute power\nratio"
        },
        {
          "2020),\nthirteen neurophysiological\nfeature types were sys-": "of\nthe theta band to the beta band,\nthe amplitude of\nthe"
        },
        {
          "2020),\nthirteen neurophysiological\nfeature types were sys-": "Hilbert\ntransform of\nintrinsic mode functions,\nthe instan-"
        },
        {
          "2020),\nthirteen neurophysiological\nfeature types were sys-": "taneous phase of\nthe Hilbert\ntransform of\nintrinsic mode"
        },
        {
          "2020),\nthirteen neurophysiological\nfeature types were sys-": "functions, and function connectivity. Detailed mathematical"
        },
        {
          "2020),\nthirteen neurophysiological\nfeature types were sys-": "formalisms and physiological\ninterpretations of\nthe feature"
        },
        {
          "2020),\nthirteen neurophysiological\nfeature types were sys-": "types are documented in (Duan, Zhu, and Lu 2013; Jenke,"
        },
        {
          "2020),\nthirteen neurophysiological\nfeature types were sys-": "Peer, and Buss 2014; Xu et al. 2023). In summary,\nthe fea-"
        },
        {
          "2020),\nthirteen neurophysiological\nfeature types were sys-": "ture dimensions of DREAMER, DEAP, and HDED are 651,"
        },
        {
          "2020),\nthirteen neurophysiological\nfeature types were sys-": "1756, and 7565, respectively."
        },
        {
          "2020),\nthirteen neurophysiological\nfeature types were sys-": ""
        },
        {
          "2020),\nthirteen neurophysiological\nfeature types were sys-": "Experimental setup"
        },
        {
          "2020),\nthirteen neurophysiological\nfeature types were sys-": ""
        },
        {
          "2020),\nthirteen neurophysiological\nfeature types were sys-": "To comprehensively assess the performance of CWEFS in"
        },
        {
          "2020),\nthirteen neurophysiological\nfeature types were sys-": "the multi-dimensional\naffective\ncomputing task, nineteen"
        },
        {
          "2020),\nthirteen neurophysiological\nfeature types were sys-": "state-of-the-art\nfeature selection methods were included in"
        },
        {
          "2020),\nthirteen neurophysiological\nfeature types were sys-": "the comparative analysis. These methods include:"
        },
        {
          "2020),\nthirteen neurophysiological\nfeature types were sys-": "(1) Ten single-label feature selection approaches: ReliefF"
        },
        {
          "2020),\nthirteen neurophysiological\nfeature types were sys-": "(Kononenko 1994), PCC (Ng, Goh, and Low 1997), CMIM"
        },
        {
          "2020),\nthirteen neurophysiological\nfeature types were sys-": "(Fleuret 2004), mRMR (Ding and Peng 2005), RFS (Nie"
        },
        {
          "2020),\nthirteen neurophysiological\nfeature types were sys-": "et al. 2010), RPMFS (Cai, Nie, and Huang 2013), ESFS"
        },
        {
          "2020),\nthirteen neurophysiological\nfeature types were sys-": "(Chen, Tang, and Li 2017), FSOR (Xu et al. 2020), SDFS"
        },
        {
          "2020),\nthirteen neurophysiological\nfeature types were sys-": "(Wang et al. 2020b), and GRMOR (Xu et al. 2023)."
        },
        {
          "2020),\nthirteen neurophysiological\nfeature types were sys-": "(2) Five multi-label feature selection methods: PMU (Lee"
        },
        {
          "2020),\nthirteen neurophysiological\nfeature types were sys-": "and Kim 2013), SCLS (Lee and Kim 2017), MFS MCDM"
        },
        {
          "2020),\nthirteen neurophysiological\nfeature types were sys-": "(Hashemi,\nDowlatshahi,\nand\nNezamabadi-pour\n2020a),"
        },
        {
          "2020),\nthirteen neurophysiological\nfeature types were sys-": "MGFS\n(Hashemi,\nDowlatshahi,\nand\nNezamabadi-pour"
        },
        {
          "2020),\nthirteen neurophysiological\nfeature types were sys-": "2020b), and EFSMDER (Xu et al. 2024)."
        },
        {
          "2020),\nthirteen neurophysiological\nfeature types were sys-": "(3) Four advanced multi-view multi-label\nfeature selec-"
        },
        {
          "2020),\nthirteen neurophysiological\nfeature types were sys-": ""
        },
        {
          "2020),\nthirteen neurophysiological\nfeature types were sys-": "tion methods: MSFS (Zhang et al. 2020), DHLI (Hao, Liu,"
        },
        {
          "2020),\nthirteen neurophysiological\nfeature types were sys-": ""
        },
        {
          "2020),\nthirteen neurophysiological\nfeature types were sys-": "and Gao 2024), UGRFS (Hao, Liu,\nand Gao 2025),\nand"
        },
        {
          "2020),\nthirteen neurophysiological\nfeature types were sys-": "EF2FS (Hao, Gao, and Hu 2025)."
        },
        {
          "2020),\nthirteen neurophysiological\nfeature types were sys-": "The EEG recordings were\nstratified\ninto\ndichotomous"
        },
        {
          "2020),\nthirteen neurophysiological\nfeature types were sys-": "classes\n(low/high)\naccording to self-assessed emotion di-"
        },
        {
          "2020),\nthirteen neurophysiological\nfeature types were sys-": "mensional scores, with an empirically established classifi-"
        },
        {
          "2020),\nthirteen neurophysiological\nfeature types were sys-": "cation threshold of 5. Multi-label k-nearest neighbors (ML-"
        },
        {
          "2020),\nthirteen neurophysiological\nfeature types were sys-": "KNN) (Zhang and Zhou 2007) was adopted as the base clas-"
        },
        {
          "2020),\nthirteen neurophysiological\nfeature types were sys-": "sifier, where the neighborhood size and smoothing param-"
        },
        {
          "2020),\nthirteen neurophysiological\nfeature types were sys-": "eters were configured to k = 10 and s = 1,\nrespectively."
        },
        {
          "2020),\nthirteen neurophysiological\nfeature types were sys-": "The dataset was partitioned through random allocation, with"
        },
        {
          "2020),\nthirteen neurophysiological\nfeature types were sys-": "80% of participants assigned to the training set and the re-"
        },
        {
          "2020),\nthirteen neurophysiological\nfeature types were sys-": "maining 20% retained as the test set. A cross-subject vali-"
        },
        {
          "2020),\nthirteen neurophysiological\nfeature types were sys-": "dation paradigm was employed, complemented by 50 inde-"
        },
        {
          "2020),\nthirteen neurophysiological\nfeature types were sys-": "pendent\ntrials to eliminate sampling bias. The mean perfor-"
        },
        {
          "2020),\nthirteen neurophysiological\nfeature types were sys-": "mance metric across all\ntrials served as the definitive eval-"
        },
        {
          "2020),\nthirteen neurophysiological\nfeature types were sys-": "uation criterion for affective computing efficacy. Six eval-"
        },
        {
          "2020),\nthirteen neurophysiological\nfeature types were sys-": "uation metrics were employed to assess multi-dimensional"
        },
        {
          "2020),\nthirteen neurophysiological\nfeature types were sys-": "emotion recognition performance:\n(1)\nlabel-based metrics:"
        },
        {
          "2020),\nthirteen neurophysiological\nfeature types were sys-": "macro-F1 (MA) and micro-F1 (MI); (2) instance-based met-"
        },
        {
          "2020),\nthirteen neurophysiological\nfeature types were sys-": "rics: average precision (AP), coverage (CV),\nranking loss"
        },
        {
          "2020),\nthirteen neurophysiological\nfeature types were sys-": "(RL), and hamming loss (HL). The mathematical definitions"
        },
        {
          "2020),\nthirteen neurophysiological\nfeature types were sys-": "of the metrics are detailed in (Zhang et al. 2019a)."
        },
        {
          "2020),\nthirteen neurophysiological\nfeature types were sys-": ""
        },
        {
          "2020),\nthirteen neurophysiological\nfeature types were sys-": "Performance comparison"
        },
        {
          "2020),\nthirteen neurophysiological\nfeature types were sys-": ""
        },
        {
          "2020),\nthirteen neurophysiological\nfeature types were sys-": "The self-assessed VAD dimensions were stratified into di-"
        },
        {
          "2020),\nthirteen neurophysiological\nfeature types were sys-": "chotomous\nclasses\n(high/low),\nthereby\nreformulating\nthe"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table 2: The results of ablation experiments on the perfor-",
      "data": [
        {
          "Evaluation metric": "Ranking loss",
          "FF": "6.919",
          "Critical value": ""
        },
        {
          "Evaluation metric": "Coverage",
          "FF": "7.994",
          "Critical value": ""
        },
        {
          "Evaluation metric": "Hamming loss",
          "FF": "7.754",
          "Critical value": ""
        },
        {
          "Evaluation metric": "Average precision",
          "FF": "9.303",
          "Critical value": "≈ 1.867"
        },
        {
          "Evaluation metric": "Macro-F1",
          "FF": "7.396",
          "Critical value": ""
        },
        {
          "Evaluation metric": "Micro-F1",
          "FF": "7.500",
          "Critical value": ""
        }
      ],
      "page": 7
    },
    {
      "caption": "Table 2: The results of ablation experiments on the perfor-",
      "data": [
        {
          "Table 1: Friedman test results.": ""
        },
        {
          "Table 1: Friedman test results.": ""
        },
        {
          "Table 1: Friedman test results.": "HL ↓"
        },
        {
          "Table 1: Friedman test results.": "0.30"
        },
        {
          "Table 1: Friedman test results.": "0.30"
        },
        {
          "Table 1: Friedman test results.": "0.29"
        },
        {
          "Table 1: Friedman test results.": "0.28"
        },
        {
          "Table 1: Friedman test results.": ""
        }
      ],
      "page": 7
    },
    {
      "caption": "Table 2: The results of ablation experiments on the perfor-",
      "data": [
        {
          "w/o ACWL\n0.30\n0.80\n0.36\n0.83\n0.25\n0.96": "w/o GMRL Y\n0.30\n0.79\n0.37\n0.82\n0.28\n0.94"
        },
        {
          "w/o ACWL\n0.30\n0.80\n0.36\n0.83\n0.25\n0.96": "w/o GMRL X\n0.29\n0.81\n0.36\n0.84\n0.23\n0.97"
        },
        {
          "w/o ACWL\n0.30\n0.80\n0.36\n0.83\n0.25\n0.96": "CWEFS\n0.28\n0.83\n0.35\n0.85\n0.20\n0.98"
        },
        {
          "w/o ACWL\n0.30\n0.80\n0.36\n0.83\n0.25\n0.96": ""
        },
        {
          "w/o ACWL\n0.30\n0.80\n0.36\n0.83\n0.25\n0.96": "Table 2: The results of ablation experiments on the perfor-"
        },
        {
          "w/o ACWL\n0.30\n0.80\n0.36\n0.83\n0.25\n0.96": "mance index average precision (w/o, ACWL, and GMRL de-"
        },
        {
          "w/o ACWL\n0.30\n0.80\n0.36\n0.83\n0.25\n0.96": "note without, adaptive channel-weight\nlearning, and graph-"
        },
        {
          "w/o ACWL\n0.30\n0.80\n0.36\n0.83\n0.25\n0.96": "based manifold regularization learning, respectively)."
        },
        {
          "w/o ACWL\n0.30\n0.80\n0.36\n0.83\n0.25\n0.96": ""
        },
        {
          "w/o ACWL\n0.30\n0.80\n0.36\n0.83\n0.25\n0.96": "As shown in Fig. 3(a-c), Fig. 4(a-c), Fig. 5(a-c), enhanced"
        },
        {
          "w/o ACWL\n0.30\n0.80\n0.36\n0.83\n0.25\n0.96": "multi-dimensional affective computing performance corre-"
        },
        {
          "w/o ACWL\n0.30\n0.80\n0.36\n0.83\n0.25\n0.96": ""
        },
        {
          "w/o ACWL\n0.30\n0.80\n0.36\n0.83\n0.25\n0.96": "lates with lower values of\nthree\nevaluation metrics: HL,"
        },
        {
          "w/o ACWL\n0.30\n0.80\n0.36\n0.83\n0.25\n0.96": "CV,\nand RL. Conversely,\nas demonstrated in Fig. 3(d-f),"
        },
        {
          "w/o ACWL\n0.30\n0.80\n0.36\n0.83\n0.25\n0.96": "Fig. 4(d-f), and Fig. 5(d-f),\nsuperior performance is\nindi-"
        },
        {
          "w/o ACWL\n0.30\n0.80\n0.36\n0.83\n0.25\n0.96": "cated by elevated values of\nthree complementary metrics:"
        },
        {
          "w/o ACWL\n0.30\n0.80\n0.36\n0.83\n0.25\n0.96": "AP, MA, and MI. Across all EEG feature selection ratios,"
        },
        {
          "w/o ACWL\n0.30\n0.80\n0.36\n0.83\n0.25\n0.96": "CWEFS systematically attains extremal values (minima for"
        },
        {
          "w/o ACWL\n0.30\n0.80\n0.36\n0.83\n0.25\n0.96": "HL/CV/RL and maxima\nfor AP/MA/MI),\noutperforming"
        },
        {
          "w/o ACWL\n0.30\n0.80\n0.36\n0.83\n0.25\n0.96": "nineteen benchmark feature selection methods. Collectively,"
        },
        {
          "w/o ACWL\n0.30\n0.80\n0.36\n0.83\n0.25\n0.96": "the experimental evidence from Fig. 3-5 substantiates that"
        },
        {
          "w/o ACWL\n0.30\n0.80\n0.36\n0.83\n0.25\n0.96": "EEG feature subsets chosen by CWEFS achieve peak recog-"
        },
        {
          "w/o ACWL\n0.30\n0.80\n0.36\n0.83\n0.25\n0.96": "nition efficacy across all evaluation criteria. Furthermore,"
        },
        {
          "w/o ACWL\n0.30\n0.80\n0.36\n0.83\n0.25\n0.96": "the Friedman test was employed to statistically validate sig-"
        },
        {
          "w/o ACWL\n0.30\n0.80\n0.36\n0.83\n0.25\n0.96": "nificant performance disparities among the twenty methods,"
        },
        {
          "w/o ACWL\n0.30\n0.80\n0.36\n0.83\n0.25\n0.96": "as quantified in Table 1. The null hypothesis was rejected,"
        },
        {
          "w/o ACWL\n0.30\n0.80\n0.36\n0.83\n0.25\n0.96": "indicating significant differences in these twenty methods."
        },
        {
          "w/o ACWL\n0.30\n0.80\n0.36\n0.83\n0.25\n0.96": ""
        },
        {
          "w/o ACWL\n0.30\n0.80\n0.36\n0.83\n0.25\n0.96": "Ablation experiment"
        },
        {
          "w/o ACWL\n0.30\n0.80\n0.36\n0.83\n0.25\n0.96": ""
        },
        {
          "w/o ACWL\n0.30\n0.80\n0.36\n0.83\n0.25\n0.96": "To evaluate the contributions of\nindividual modules within"
        },
        {
          "w/o ACWL\n0.30\n0.80\n0.36\n0.83\n0.25\n0.96": "the CWEFS framework, we conducted controlled ablation"
        },
        {
          "w/o ACWL\n0.30\n0.80\n0.36\n0.83\n0.25\n0.96": "studies. CWEFS comprises three critical components, which"
        },
        {
          "w/o ACWL\n0.30\n0.80\n0.36\n0.83\n0.25\n0.96": "were\nsystematically deactivated in sequence. As\nsumma-"
        },
        {
          "w/o ACWL\n0.30\n0.80\n0.36\n0.83\n0.25\n0.96": ""
        },
        {
          "w/o ACWL\n0.30\n0.80\n0.36\n0.83\n0.25\n0.96": "rized in Table 2, the adaptive channel-weight learning mod-"
        },
        {
          "w/o ACWL\n0.30\n0.80\n0.36\n0.83\n0.25\n0.96": ""
        },
        {
          "w/o ACWL\n0.30\n0.80\n0.36\n0.83\n0.25\n0.96": "ule plays a pivotal role in characterizing the varying impacts"
        },
        {
          "w/o ACWL\n0.30\n0.80\n0.36\n0.83\n0.25\n0.96": ""
        },
        {
          "w/o ACWL\n0.30\n0.80\n0.36\n0.83\n0.25\n0.96": "of individual EEG channels on feature selection model con-"
        },
        {
          "w/o ACWL\n0.30\n0.80\n0.36\n0.83\n0.25\n0.96": ""
        },
        {
          "w/o ACWL\n0.30\n0.80\n0.36\n0.83\n0.25\n0.96": "struction. The remaining modules serve to preserve the local"
        },
        {
          "w/o ACWL\n0.30\n0.80\n0.36\n0.83\n0.25\n0.96": ""
        },
        {
          "w/o ACWL\n0.30\n0.80\n0.36\n0.83\n0.25\n0.96": "geometric structures within both the original EEG feature"
        },
        {
          "w/o ACWL\n0.30\n0.80\n0.36\n0.83\n0.25\n0.96": ""
        },
        {
          "w/o ACWL\n0.30\n0.80\n0.36\n0.83\n0.25\n0.96": "space and the multi-dimensional affective label space."
        },
        {
          "w/o ACWL\n0.30\n0.80\n0.36\n0.83\n0.25\n0.96": ""
        },
        {
          "w/o ACWL\n0.30\n0.80\n0.36\n0.83\n0.25\n0.96": ""
        },
        {
          "w/o ACWL\n0.30\n0.80\n0.36\n0.83\n0.25\n0.96": "Parameter sensitivity and convergence analysis"
        },
        {
          "w/o ACWL\n0.30\n0.80\n0.36\n0.83\n0.25\n0.96": ""
        },
        {
          "w/o ACWL\n0.30\n0.80\n0.36\n0.83\n0.25\n0.96": "CWEFS incorporates five tradeoff parameters in its objec-"
        },
        {
          "w/o ACWL\n0.30\n0.80\n0.36\n0.83\n0.25\n0.96": "tive function. We systematically evaluate parameter sensitiv-"
        },
        {
          "w/o ACWL\n0.30\n0.80\n0.36\n0.83\n0.25\n0.96": "ity through controlled experiments with the following con-"
        },
        {
          "w/o ACWL\n0.30\n0.80\n0.36\n0.83\n0.25\n0.96": "figurations: the values of λ, β, η, γ, and δ are each adjusted"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "References": "",
          "Hashemi, A.; Dowlatshahi, M. B.; and Nezamabadi-pour, H.": "2020b. MGFS: A multi-label graph-based feature selection"
        },
        {
          "References": "Atkinson, J.; and Campos, D. 2016.\nImproving BCI-based",
          "Hashemi, A.; Dowlatshahi, M. B.; and Nezamabadi-pour, H.": ""
        },
        {
          "References": "",
          "Hashemi, A.; Dowlatshahi, M. B.; and Nezamabadi-pour, H.": "algorithm via PageRank centrality. Expert Systems with Ap-"
        },
        {
          "References": "emotion recognition by combining EEG feature selection",
          "Hashemi, A.; Dowlatshahi, M. B.; and Nezamabadi-pour, H.": ""
        },
        {
          "References": "",
          "Hashemi, A.; Dowlatshahi, M. B.; and Nezamabadi-pour, H.": "plications, 142: 113024."
        },
        {
          "References": "and kernel classifiers. Expert Systems with Applications, 47:",
          "Hashemi, A.; Dowlatshahi, M. B.; and Nezamabadi-pour, H.": ""
        },
        {
          "References": "",
          "Hashemi, A.; Dowlatshahi, M. B.; and Nezamabadi-pour, H.": "He, H.; Tan, Y.; Ying, J.; and Zhang, W. 2020.\nStrengthen"
        },
        {
          "References": "35–41.",
          "Hashemi, A.; Dowlatshahi, M. B.; and Nezamabadi-pour, H.": ""
        },
        {
          "References": "",
          "Hashemi, A.; Dowlatshahi, M. B.; and Nezamabadi-pour, H.": "EEG-based emotion recognition using firefly integrated op-"
        },
        {
          "References": "Becker, H.; Fleureau, J.; Guillotel, P.; Wendling, F.; Merlet,",
          "Hashemi, A.; Dowlatshahi, M. B.; and Nezamabadi-pour, H.": ""
        },
        {
          "References": "",
          "Hashemi, A.; Dowlatshahi, M. B.; and Nezamabadi-pour, H.": "timization algorithm. Applied Soft Computing, 94: 106426."
        },
        {
          "References": "I.; and Albera, L. 2017. Emotion recognition based on high-",
          "Hashemi, A.; Dowlatshahi, M. B.; and Nezamabadi-pour, H.": ""
        },
        {
          "References": "",
          "Hashemi, A.; Dowlatshahi, M. B.; and Nezamabadi-pour, H.": "Jenke, R.; Peer, A.; and Buss, M. 2014. Feature Extraction"
        },
        {
          "References": "resolution EEG recordings and reconstructed brain sources.",
          "Hashemi, A.; Dowlatshahi, M. B.; and Nezamabadi-pour, H.": ""
        },
        {
          "References": "",
          "Hashemi, A.; Dowlatshahi, M. B.; and Nezamabadi-pour, H.": "IEEE\nand Selection for Emotion Recognition from EEG."
        },
        {
          "References": "IEEE Transactions on Affective Computing, 11(2): 244–257.",
          "Hashemi, A.; Dowlatshahi, M. B.; and Nezamabadi-pour, H.": ""
        },
        {
          "References": "",
          "Hashemi, A.; Dowlatshahi, M. B.; and Nezamabadi-pour, H.": "Transactions on Affective Computing, 5(3): 327–339."
        },
        {
          "References": "Cai, X.; Nie, F.; and Huang, H. 2013.\nExact\ntop-k feature",
          "Hashemi, A.; Dowlatshahi, M. B.; and Nezamabadi-pour, H.": ""
        },
        {
          "References": "",
          "Hashemi, A.; Dowlatshahi, M. B.; and Nezamabadi-pour, H.": "Jian, L.; Li, J.; Shu, K.; and Liu, H. 2016. Multi-label\nin-"
        },
        {
          "References": "selection via ℓ 2, 0-norm constraint. In Twenty-third interna-",
          "Hashemi, A.; Dowlatshahi, M. B.; and Nezamabadi-pour, H.": ""
        },
        {
          "References": "",
          "Hashemi, A.; Dowlatshahi, M. B.; and Nezamabadi-pour, H.": "formed feature selection. In Proc. 25th Int. Joint Conf. Artif."
        },
        {
          "References": "tional joint conference on artificial intelligence, 1240–1246.",
          "Hashemi, A.; Dowlatshahi, M. B.; and Nezamabadi-pour, H.": ""
        },
        {
          "References": "",
          "Hashemi, A.; Dowlatshahi, M. B.; and Nezamabadi-pour, H.": "Intell, 1627–1633."
        },
        {
          "References": "Chen, J.; Hu, B.; Moore, P.; Zhang, X.; and Ma, X. 2015.",
          "Hashemi, A.; Dowlatshahi, M. B.; and Nezamabadi-pour, H.": "Katsigiannis, S.; and Ramzan, N. 2018.\nDREAMER: A"
        },
        {
          "References": "Electroencephalogram-based\nemotion\nassessment\nsystem",
          "Hashemi, A.; Dowlatshahi, M. B.; and Nezamabadi-pour, H.": "Database for Emotion Recognition Through EEG and ECG"
        },
        {
          "References": "Applied Soft\nusing ontology and data mining techniques.",
          "Hashemi, A.; Dowlatshahi, M. B.; and Nezamabadi-pour, H.": "Signals\nFrom Wireless Low-cost Off-the-Shelf Devices."
        },
        {
          "References": "Computing, 30: 663–674.",
          "Hashemi, A.; Dowlatshahi, M. B.; and Nezamabadi-pour, H.": "IEEE Journal of Biomedical and Health Informatics, 22(1):"
        },
        {
          "References": "",
          "Hashemi, A.; Dowlatshahi, M. B.; and Nezamabadi-pour, H.": "98–107."
        },
        {
          "References": "Chen, L.; Tang, J.; and Li, B. 2017. Embedded supervised",
          "Hashemi, A.; Dowlatshahi, M. B.; and Nezamabadi-pour, H.": ""
        },
        {
          "References": "feature selection for multi-class data.\nIn Proceedings of the",
          "Hashemi, A.; Dowlatshahi, M. B.; and Nezamabadi-pour, H.": "Koelstra, S.; Muhl, C.; Soleymani, M.; Lee, J.-S.; Yazdani,"
        },
        {
          "References": "2017 SIAM International Conference on Data Mining, 516–",
          "Hashemi, A.; Dowlatshahi, M. B.; and Nezamabadi-pour, H.": "A.; Ebrahimi, T.; Pun, T.; Nijholt, A.; and Patras,\nI. 2011."
        },
        {
          "References": "524. SIAM.",
          "Hashemi, A.; Dowlatshahi, M. B.; and Nezamabadi-pour, H.": "Deap: A database for emotion analysis; using physiological"
        },
        {
          "References": "",
          "Hashemi, A.; Dowlatshahi, M. B.; and Nezamabadi-pour, H.": "signals. IEEE transactions on affective computing, 3(1): 18–"
        },
        {
          "References": "Ding, C.; and Peng, H. 2005. Minimum redundancy feature",
          "Hashemi, A.; Dowlatshahi, M. B.; and Nezamabadi-pour, H.": ""
        },
        {
          "References": "",
          "Hashemi, A.; Dowlatshahi, M. B.; and Nezamabadi-pour, H.": "31."
        },
        {
          "References": "Journal of\nselection from microarray gene expression data.",
          "Hashemi, A.; Dowlatshahi, M. B.; and Nezamabadi-pour, H.": ""
        },
        {
          "References": "",
          "Hashemi, A.; Dowlatshahi, M. B.; and Nezamabadi-pour, H.": "Kononenko, I. 1994. Estimating attributes: analysis and ex-"
        },
        {
          "References": "bioinformatics and computational biology, 3(02): 185–205.",
          "Hashemi, A.; Dowlatshahi, M. B.; and Nezamabadi-pour, H.": ""
        },
        {
          "References": "",
          "Hashemi, A.; Dowlatshahi, M. B.; and Nezamabadi-pour, H.": "tensions of RELIEF.\nIn European conference on machine"
        },
        {
          "References": "Dorigo, M.; and Gambardella, L. M. 1997. Ant colony sys-",
          "Hashemi, A.; Dowlatshahi, M. B.; and Nezamabadi-pour, H.": ""
        },
        {
          "References": "",
          "Hashemi, A.; Dowlatshahi, M. B.; and Nezamabadi-pour, H.": "learning, 171–182. Springer."
        },
        {
          "References": "tem: a cooperative learning approach to the traveling sales-",
          "Hashemi, A.; Dowlatshahi, M. B.; and Nezamabadi-pour, H.": ""
        },
        {
          "References": "",
          "Hashemi, A.; Dowlatshahi, M. B.; and Nezamabadi-pour, H.": "Lee, J.; and Kim, D.-W. 2013. Feature selection for multi-"
        },
        {
          "References": "man problem. IEEE Transactions on evolutionary computa-",
          "Hashemi, A.; Dowlatshahi, M. B.; and Nezamabadi-pour, H.": ""
        },
        {
          "References": "",
          "Hashemi, A.; Dowlatshahi, M. B.; and Nezamabadi-pour, H.": "label classification using multivariate mutual\ninformation."
        },
        {
          "References": "tion, 1(1): 53–66.",
          "Hashemi, A.; Dowlatshahi, M. B.; and Nezamabadi-pour, H.": ""
        },
        {
          "References": "",
          "Hashemi, A.; Dowlatshahi, M. B.; and Nezamabadi-pour, H.": "Pattern Recognition Letters, 34(3): 349–357."
        },
        {
          "References": "Duan, R.; Zhu, J.; and Lu, B. 2013.\nDifferential entropy",
          "Hashemi, A.; Dowlatshahi, M. B.; and Nezamabadi-pour, H.": ""
        },
        {
          "References": "",
          "Hashemi, A.; Dowlatshahi, M. B.; and Nezamabadi-pour, H.": "Lee, J.; and Kim, D.-W. 2017. SCLS: Multi-label feature se-"
        },
        {
          "References": "feature for EEG-based emotion classification.\nIn 2013 6th",
          "Hashemi, A.; Dowlatshahi, M. B.; and Nezamabadi-pour, H.": ""
        },
        {
          "References": "",
          "Hashemi, A.; Dowlatshahi, M. B.; and Nezamabadi-pour, H.": "lection based on scalable criterion for large label set. Pattern"
        },
        {
          "References": "international IEEE/EMBS conference on neural engineering",
          "Hashemi, A.; Dowlatshahi, M. B.; and Nezamabadi-pour, H.": ""
        },
        {
          "References": "",
          "Hashemi, A.; Dowlatshahi, M. B.; and Nezamabadi-pour, H.": "Recognition, 66: 342–352."
        },
        {
          "References": "(NER), 81–84. IEEE.",
          "Hashemi, A.; Dowlatshahi, M. B.; and Nezamabadi-pour, H.": ""
        },
        {
          "References": "",
          "Hashemi, A.; Dowlatshahi, M. B.; and Nezamabadi-pour, H.": "Li, J.; Cheng, K.; Wang, S.; Morstatter, F.; Trevino, R. P.;"
        },
        {
          "References": "Fleuret, F. 2004.\nFast binary feature selection with condi-",
          "Hashemi, A.; Dowlatshahi, M. B.; and Nezamabadi-pour, H.": "Tang, J.; and Liu, H. 2017.\nFeature selection: A data per-"
        },
        {
          "References": "tional mutual information. Journal of Machine learning re-",
          "Hashemi, A.; Dowlatshahi, M. B.; and Nezamabadi-pour, H.": "spective. ACM computing surveys (CSUR), 50(6): 1–45."
        },
        {
          "References": "search, 5(11): 1531–1555.",
          "Hashemi, A.; Dowlatshahi, M. B.; and Nezamabadi-pour, H.": ""
        },
        {
          "References": "",
          "Hashemi, A.; Dowlatshahi, M. B.; and Nezamabadi-pour, H.": "Li, R.; Wang, Y.; and Lu, B.-L. 2021. A multi-domain adap-"
        },
        {
          "References": "Gong,\nP.;\nJia, Z.; Wang,\nP.; Zhou, Y.;\nand Zhang, D.",
          "Hashemi, A.; Dowlatshahi, M. B.; and Nezamabadi-pour, H.": "tive graph convolutional network for EEG-based emotion"
        },
        {
          "References": "2023.\nASTDF-Net: Attention-Based\nSpatial-Temporal",
          "Hashemi, A.; Dowlatshahi, M. B.; and Nezamabadi-pour, H.": "the 29th ACM International\nrecognition.\nIn Proceedings of"
        },
        {
          "References": "Dual-Stream Fusion Network\nfor\nEEG-Based\nEmotion",
          "Hashemi, A.; Dowlatshahi, M. B.; and Nezamabadi-pour, H.": "Conference on Multimedia, 5565–5573."
        },
        {
          "References": "Recognition.\nIn Proceedings of the 31st ACM International",
          "Hashemi, A.; Dowlatshahi, M. B.; and Nezamabadi-pour, H.": "Liu, Y.;\nand Sourina, O. 2013.\nReal-time\nfractal-based"
        },
        {
          "References": "Conference on Multimedia, 883–892.",
          "Hashemi, A.; Dowlatshahi, M. B.; and Nezamabadi-pour, H.": "valence level\nrecognition from EEG.\nIn Transactions on"
        },
        {
          "References": "",
          "Hashemi, A.; Dowlatshahi, M. B.; and Nezamabadi-pour, H.": "computational science XVIII: special issue on Cyberworlds,"
        },
        {
          "References": "Hao, P.; Gao, W.; and Hu, L. 2025. Embedded feature fusion",
          "Hashemi, A.; Dowlatshahi, M. B.; and Nezamabadi-pour, H.": ""
        },
        {
          "References": "",
          "Hashemi, A.; Dowlatshahi, M. B.; and Nezamabadi-pour, H.": "101–120. Springer."
        },
        {
          "References": "for multi-view multi-label feature selection. Pattern Recog-",
          "Hashemi, A.; Dowlatshahi, M. B.; and Nezamabadi-pour, H.": ""
        },
        {
          "References": "",
          "Hashemi, A.; Dowlatshahi, M. B.; and Nezamabadi-pour, H.": "Liu, Z.-T.; Xie, Q.; Wu, M.; Cao, W.-H.; Li, D.-Y.;\nand"
        },
        {
          "References": "nition, 157: 110888.",
          "Hashemi, A.; Dowlatshahi, M. B.; and Nezamabadi-pour, H.": ""
        },
        {
          "References": "",
          "Hashemi, A.; Dowlatshahi, M. B.; and Nezamabadi-pour, H.": "Li, S.-H. 2018. Electroencephalogram emotion recognition"
        },
        {
          "References": "Hao, P.; Liu, K.; and Gao, W. 2024. Double-layer hybrid-",
          "Hashemi, A.; Dowlatshahi, M. B.; and Nezamabadi-pour, H.": ""
        },
        {
          "References": "",
          "Hashemi, A.; Dowlatshahi, M. B.; and Nezamabadi-pour, H.": "based on empirical mode decomposition and optimal feature"
        },
        {
          "References": "label\nidentification feature selection for multi-view multi-",
          "Hashemi, A.; Dowlatshahi, M. B.; and Nezamabadi-pour, H.": ""
        },
        {
          "References": "",
          "Hashemi, A.; Dowlatshahi, M. B.; and Nezamabadi-pour, H.": "IEEE Transactions on Cognitive and Develop-\nselection."
        },
        {
          "References": "the AAAI Conference on\nlabel\nlearning.\nIn Proceedings of",
          "Hashemi, A.; Dowlatshahi, M. B.; and Nezamabadi-pour, H.": ""
        },
        {
          "References": "",
          "Hashemi, A.; Dowlatshahi, M. B.; and Nezamabadi-pour, H.": "mental Systems, 11(4): 517–526."
        },
        {
          "References": "Artificial Intelligence, volume 38, 12295–12303.",
          "Hashemi, A.; Dowlatshahi, M. B.; and Nezamabadi-pour, H.": ""
        },
        {
          "References": "",
          "Hashemi, A.; Dowlatshahi, M. B.; and Nezamabadi-pour, H.": "Ng, H. T.; Goh, W. B.; and Low, K. L. 1997. Feature selec-"
        },
        {
          "References": "Hao, P.; Liu, K.; and Gao, W. 2025.\nUncertainty-Aware",
          "Hashemi, A.; Dowlatshahi, M. B.; and Nezamabadi-pour, H.": ""
        },
        {
          "References": "",
          "Hashemi, A.; Dowlatshahi, M. B.; and Nezamabadi-pour, H.": "tion, perceptron learning, and a usability case study for text"
        },
        {
          "References": "Global-View Reconstruction\nfor Multi-View Multi-Label",
          "Hashemi, A.; Dowlatshahi, M. B.; and Nezamabadi-pour, H.": ""
        },
        {
          "References": "",
          "Hashemi, A.; Dowlatshahi, M. B.; and Nezamabadi-pour, H.": "the 20th annual\ninterna-\ncategorization.\nIn Proceedings of"
        },
        {
          "References": "the AAAI Conference\nFeature Selection.\nIn Proceedings of",
          "Hashemi, A.; Dowlatshahi, M. B.; and Nezamabadi-pour, H.": ""
        },
        {
          "References": "",
          "Hashemi, A.; Dowlatshahi, M. B.; and Nezamabadi-pour, H.": "tional ACM SIGIR conference on Research and development"
        },
        {
          "References": "on Artificial Intelligence, volume 39, 17068–17076.",
          "Hashemi, A.; Dowlatshahi, M. B.; and Nezamabadi-pour, H.": ""
        },
        {
          "References": "",
          "Hashemi, A.; Dowlatshahi, M. B.; and Nezamabadi-pour, H.": "in information retrieval, 67–73."
        },
        {
          "References": "Hashemi, A.; Dowlatshahi, M. B.; and Nezamabadi-pour, H.",
          "Hashemi, A.; Dowlatshahi, M. B.; and Nezamabadi-pour, H.": "Nie, F.; Huang, H.; Cai, X.; and Ding, C. H. 2010. Efficient"
        },
        {
          "References": "2020a. MFS-MCDM: Multi-label\nfeature selection using",
          "Hashemi, A.; Dowlatshahi, M. B.; and Nezamabadi-pour, H.": "and robust feature selection via joint\nl2,1-norms minimiza-"
        },
        {
          "References": "multi-criteria decision making. Knowledge-Based Systems,",
          "Hashemi, A.; Dowlatshahi, M. B.; and Nezamabadi-pour, H.": "tion.\nIn Advances in neural information processing systems,"
        },
        {
          "References": "206: 106365.",
          "Hashemi, A.; Dowlatshahi, M. B.; and Nezamabadi-pour, H.": "1813–1821."
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "¨": "Ozerdem, M. S.; and Polat, H. 2017. Emotion recognition"
        },
        {
          "¨": "based on EEG features in movie clips with channel selec-"
        },
        {
          "¨": "tion. Brain informatics, 4: 241–252."
        },
        {
          "¨": "Saeys, Y.; Inza, I.; and Larra˜naga, P. 2007. A review of fea-"
        },
        {
          "¨": "ture selection techniques in bioinformatics. bioinformatics,"
        },
        {
          "¨": "23(19): 2507–2517."
        },
        {
          "¨": "Tang, J.; Alelyani, S.; and Liu, H. 2014.\nFeature selection"
        },
        {
          "¨": "for classification: A review. Data classification: Algorithms"
        },
        {
          "¨": "and applications, 37."
        },
        {
          "¨": "Tang, X.; Shen, H.; Zhao, S.; Li, N.; and Liu, J. 2023. Flex-"
        },
        {
          "¨": "ible brain–computer\ninterfaces.\nNature Electronics, 6(2):"
        },
        {
          "¨": "109–118."
        },
        {
          "¨": "Tao, W.; Li, C.; Song, R.; Cheng,\nJ.; Liu, Y.; Wan, F.;"
        },
        {
          "¨": "and Chen, X. 2023.\nEEG-based emotion recognition via"
        },
        {
          "¨": "IEEE Transac-\nchannel-wise attention and self attention."
        },
        {
          "¨": "tions on Affective Computing, 14(1): 382–393."
        },
        {
          "¨": "van den Broek, S. P.; Reinders, F.; Donderwinkel, M.; and"
        },
        {
          "¨": "Peters, M. 1998.\nVolume conduction effects in EEG and"
        },
        {
          "¨": "Electroencephalography and clinical neurophysiol-\nMEG."
        },
        {
          "¨": "ogy, 106(6): 522–534."
        },
        {
          "¨": "Wang, F.; Wu, S.; Zhang, W.; Xu, Z.; Zhang, Y.; Wu, C.; and"
        },
        {
          "¨": "Coleman, S. 2020a. Emotion recognition with convolutional"
        },
        {
          "¨": "neural network and EEG-based EFDMs. Neuropsychologia,"
        },
        {
          "¨": "107506."
        },
        {
          "¨": "Wang, X.; Nie, D.; and Lu, B. 2011.\nEEG-based emotion"
        },
        {
          "¨": "recognition using frequency domain features and support"
        },
        {
          "¨": "in-\nvector machines.\nIn International conference on neural"
        },
        {
          "¨": "formation processing, 734–743. Springer."
        },
        {
          "¨": "Wang, Z.; Nie, F.; Tian, L.; Wang, R.; and Li, X. 2020b."
        },
        {
          "¨": "Discriminative feature selection via a structured sparse sub-"
        },
        {
          "¨": "space learning module.\nIn Proc. Twenty-Ninth Int. Joint"
        },
        {
          "¨": "Conf. Artif. Intell., 3009–3015."
        },
        {
          "¨": "Wu, D.; Lu, B.-L.; Hu, B.; and Zeng, Z. 2023.\nAffective"
        },
        {
          "¨": "brain–computer interfaces (abcis): A tutorial. Proceedings"
        },
        {
          "¨": "of the IEEE, 111(10): 1314–1332."
        },
        {
          "¨": "Xu, X.; Jia, T.; Li, Q.; Wei, F.; Ye, L.; and Wu, X. 2023. EEG"
        },
        {
          "¨": "Feature Selection via Global Redundancy Minimization for"
        },
        {
          "¨": "Emotion Recognition. IEEE Transactions on Affective Com-"
        },
        {
          "¨": "puting, 14(1): 421–435."
        },
        {
          "¨": "Xu, X.; Wei, F.; Jia, T.; Zhuo, L.; Zhang, H.; Li, X.; and"
        },
        {
          "¨": "Wu, X. 2024. Embedded EEG Feature Selection for Multi-"
        },
        {
          "¨": "dimension Emotion Recognition via Local and Global Label"
        },
        {
          "¨": "IEEE Transactions on Neural Systems and Re-\nRelevance."
        },
        {
          "¨": "habilitation Engineering, 32: 514–526."
        },
        {
          "¨": "Xu, X.; Wei, F.; Zhu, Z.; Liu, J.; and Wu, X. 2020. Eeg Fea-"
        },
        {
          "¨": "ture Selection Using Orthogonal Regression: Application to"
        },
        {
          "¨": "Emotion Recognition.\nIn ICASSP 2020 - 2020 IEEE Inter-"
        },
        {
          "¨": "national Conference on Acoustics, Speech and Signal Pro-"
        },
        {
          "¨": "cessing (ICASSP), 1239–1243."
        },
        {
          "¨": "Zhang, J.; Lin, Y.; Jiang, M.; Li, S.; Tang, Y.; and Tan, K. C."
        },
        {
          "¨": "2020. Multi-label Feature Selection via Global Relevance"
        },
        {
          "¨": "and Redundancy Optimization.\nIn IJCAI, 2512–2518."
        },
        {
          "¨": "Zhang, J.; Luo, Z.; Li, C.; Zhou, C.; and Li, S. 2019a. Man-"
        },
        {
          "¨": "ifold regularized discriminative feature selection for multi-"
        },
        {
          "¨": "label learning. Pattern Recognition, 95: 136–150."
        }
      ],
      "page": 9
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Improving BCI-based emotion recognition by combining EEG feature selection and kernel classifiers",
      "authors": [
        "J Atkinson",
        "D Campos"
      ],
      "year": "2016",
      "venue": "Expert Systems with Applications"
    },
    {
      "citation_id": "2",
      "title": "Emotion recognition based on highresolution EEG recordings and reconstructed brain sources",
      "authors": [
        "H Becker",
        "J Fleureau",
        "P Guillotel",
        "F Wendling",
        "I Merlet",
        "L Albera"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "3",
      "title": "Exact top-k feature selection via ℓ 2, 0-norm constraint",
      "authors": [
        "X Cai",
        "F Nie",
        "H Huang"
      ],
      "year": "2013",
      "venue": "Twenty-third international joint conference on artificial intelligence"
    },
    {
      "citation_id": "4",
      "title": "Electroencephalogram-based emotion assessment system using ontology and data mining techniques",
      "authors": [
        "J Chen",
        "B Hu",
        "P Moore",
        "X Zhang",
        "X Ma"
      ],
      "year": "2015",
      "venue": "Electroencephalogram-based emotion assessment system using ontology and data mining techniques"
    },
    {
      "citation_id": "5",
      "title": "Embedded supervised feature selection for multi-class data",
      "authors": [
        "L Chen",
        "J Tang",
        "B Li"
      ],
      "year": "2017",
      "venue": "Proceedings of the 2017 SIAM International Conference on Data Mining"
    },
    {
      "citation_id": "6",
      "title": "Minimum redundancy feature selection from microarray gene expression data",
      "authors": [
        "C Ding",
        "H Peng"
      ],
      "year": "2005",
      "venue": "Journal of bioinformatics and computational biology"
    },
    {
      "citation_id": "7",
      "title": "Ant colony system: a cooperative learning approach to the traveling salesman problem",
      "authors": [
        "M Dorigo",
        "L Gambardella"
      ],
      "year": "1997",
      "venue": "IEEE Transactions on evolutionary computation"
    },
    {
      "citation_id": "8",
      "title": "Differential entropy feature for EEG-based emotion classification",
      "authors": [
        "R Duan",
        "J Zhu",
        "B Lu"
      ],
      "year": "2013",
      "venue": "Differential entropy feature for EEG-based emotion classification"
    },
    {
      "citation_id": "9",
      "title": "Fast binary feature selection with conditional mutual information",
      "authors": [
        "F Fleuret"
      ],
      "year": "2004",
      "venue": "Journal of Machine learning research"
    },
    {
      "citation_id": "10",
      "title": "ASTDF-Net: Attention-Based Spatial-Temporal Dual-Stream Fusion Network for EEG-Based Emotion Recognition",
      "authors": [
        "P Gong",
        "Z Jia",
        "P Wang",
        "Y Zhou",
        "D Zhang"
      ],
      "year": "2023",
      "venue": "Proceedings of the 31st ACM International Conference on Multimedia"
    },
    {
      "citation_id": "11",
      "title": "Embedded feature fusion for multi-view multi-label feature selection",
      "authors": [
        "P Hao",
        "W Gao",
        "L Hu"
      ],
      "year": "2025",
      "venue": "Pattern Recognition"
    },
    {
      "citation_id": "12",
      "title": "Double-layer hybridlabel identification feature selection for multi-view multilabel learning",
      "authors": [
        "P Hao",
        "K Liu",
        "W Gao"
      ],
      "year": "2024",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "13",
      "title": "Uncertainty-Aware Global-View Reconstruction for Multi-View Multi-Label Feature Selection",
      "authors": [
        "P Hao",
        "K Liu",
        "W Gao"
      ],
      "year": "2025",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "14",
      "title": "MFS-MCDM: Multi-label feature selection using multi-criteria decision making",
      "authors": [
        "A Hashemi",
        "M Dowlatshahi",
        "H Nezamabadi-Pour"
      ],
      "year": "2020",
      "venue": "Knowledge-Based Systems"
    },
    {
      "citation_id": "15",
      "title": "MGFS: A multi-label graph-based feature selection algorithm via PageRank centrality. Expert Systems with Applications",
      "authors": [
        "A Hashemi",
        "M Dowlatshahi",
        "H Nezamabadi-Pour"
      ],
      "year": "2020",
      "venue": "MGFS: A multi-label graph-based feature selection algorithm via PageRank centrality. Expert Systems with Applications"
    },
    {
      "citation_id": "16",
      "title": "Strengthen EEG-based emotion recognition using firefly integrated optimization algorithm",
      "authors": [
        "H He",
        "Y Tan",
        "J Ying",
        "W Zhang"
      ],
      "year": "2020",
      "venue": "Applied Soft Computing"
    },
    {
      "citation_id": "17",
      "title": "Feature Extraction and Selection for Emotion Recognition from EEG",
      "authors": [
        "R Jenke",
        "A Peer",
        "M Buss"
      ],
      "year": "2014",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "18",
      "title": "Multi-label informed feature selection",
      "authors": [
        "L Jian",
        "J Li",
        "K Shu",
        "H Liu"
      ],
      "year": "2016",
      "venue": "Proc. 25th Int. Joint Conf. Artif. Intell"
    },
    {
      "citation_id": "19",
      "title": "DREAMER: A Database for Emotion Recognition Through EEG and ECG Signals From Wireless Low-cost Off-the-Shelf Devices",
      "authors": [
        "S Katsigiannis",
        "N Ramzan"
      ],
      "year": "2018",
      "venue": "IEEE Journal of Biomedical and Health Informatics"
    },
    {
      "citation_id": "20",
      "title": "Deap: A database for emotion analysis; using physiological signals",
      "authors": [
        "S Koelstra",
        "C Muhl",
        "M Soleymani",
        "J.-S Lee",
        "A Yazdani",
        "T Ebrahimi",
        "T Pun",
        "A Nijholt",
        "I Patras"
      ],
      "year": "1994",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "21",
      "title": "Feature selection for multilabel classification using multivariate mutual information",
      "authors": [
        "J Lee",
        "D.-W Kim"
      ],
      "year": "2013",
      "venue": "Pattern Recognition Letters"
    },
    {
      "citation_id": "22",
      "title": "SCLS: Multi-label feature selection based on scalable criterion for large label set",
      "authors": [
        "J Lee",
        "D.-W Kim"
      ],
      "year": "2017",
      "venue": "Pattern Recognition"
    },
    {
      "citation_id": "23",
      "title": "Feature selection: A data perspective",
      "authors": [
        "J Li",
        "K Cheng",
        "S Wang",
        "F Morstatter",
        "R Trevino",
        "J Tang",
        "H Liu"
      ],
      "year": "2017",
      "venue": "ACM computing surveys (CSUR)"
    },
    {
      "citation_id": "24",
      "title": "A multi-domain adaptive graph convolutional network for EEG-based emotion recognition",
      "authors": [
        "R Li",
        "Y Wang",
        "B.-L Lu"
      ],
      "year": "2021",
      "venue": "Proceedings of the 29th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "25",
      "title": "Real-time fractal-based valence level recognition from EEG",
      "authors": [
        "Y Liu",
        "O Sourina"
      ],
      "year": "2013",
      "venue": "Transactions on computational science XVIII: special issue on Cyberworlds"
    },
    {
      "citation_id": "26",
      "title": "Electroencephalogram emotion recognition based on empirical mode decomposition and optimal feature selection",
      "authors": [
        "Z.-T Liu",
        "Q Xie",
        "M Wu",
        "W.-H Cao",
        "D.-Y Li",
        "S.-H Li"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Cognitive and Developmental Systems"
    },
    {
      "citation_id": "27",
      "title": "Feature selection, perceptron learning, and a usability case study for text categorization",
      "authors": [
        "H Ng",
        "W Goh",
        "K Low"
      ],
      "year": "1997",
      "venue": "Proceedings of the 20th annual international ACM SIGIR conference on Research and development in information retrieval"
    },
    {
      "citation_id": "28",
      "title": "Efficient and robust feature selection via joint l 2,1 -norms minimization",
      "authors": [
        "F Nie",
        "H Huang",
        "X Cai",
        "C Ding"
      ],
      "year": "2010",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "29",
      "title": "Emotion recognition based on EEG features in movie clips with channel selection",
      "authors": [
        "M Özerdem",
        "H Polat"
      ],
      "year": "2017",
      "venue": "Brain informatics"
    },
    {
      "citation_id": "30",
      "title": "A review of feature selection techniques in bioinformatics",
      "authors": [
        "Y Saeys",
        "I Inza",
        "P Larrañaga"
      ],
      "year": "2007",
      "venue": "bioinformatics"
    },
    {
      "citation_id": "31",
      "title": "Feature selection for classification: A review. Data classification: Algorithms and applications",
      "authors": [
        "J Tang",
        "S Alelyani",
        "H Liu"
      ],
      "year": "2014",
      "venue": "Feature selection for classification: A review. Data classification: Algorithms and applications"
    },
    {
      "citation_id": "32",
      "title": "Flexible brain-computer interfaces",
      "authors": [
        "X Tang",
        "H Shen",
        "S Zhao",
        "N Li",
        "J Liu"
      ],
      "year": "2023",
      "venue": "Nature Electronics"
    },
    {
      "citation_id": "33",
      "title": "EEG-based emotion recognition via channel-wise attention and self attention",
      "authors": [
        "W Tao",
        "C Li",
        "R Song",
        "J Cheng",
        "Y Liu",
        "F Wan",
        "X Chen"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "34",
      "title": "Volume conduction effects in EEG and MEG. Electroencephalography and clinical neurophysiology",
      "authors": [
        "S Van Den Broek",
        "F Reinders",
        "M Donderwinkel",
        "M Peters"
      ],
      "year": "1998",
      "venue": "Volume conduction effects in EEG and MEG. Electroencephalography and clinical neurophysiology"
    },
    {
      "citation_id": "35",
      "title": "Emotion recognition with convolutional neural network and EEG-based EFDMs",
      "authors": [
        "F Wang",
        "S Wu",
        "W Zhang",
        "Z Xu",
        "Y Zhang",
        "C Wu",
        "S Coleman"
      ],
      "year": "2020",
      "venue": "Neuropsychologia"
    },
    {
      "citation_id": "36",
      "title": "EEG-based emotion recognition using frequency domain features and support vector machines",
      "authors": [
        "X Wang",
        "D Nie",
        "B Lu"
      ],
      "year": "2011",
      "venue": "International conference on neural information processing"
    },
    {
      "citation_id": "37",
      "title": "Discriminative feature selection via a structured sparse subspace learning module",
      "authors": [
        "Z Wang",
        "F Nie",
        "L Tian",
        "R Wang",
        "X Li"
      ],
      "year": "2020",
      "venue": "Proc. Twenty-Ninth Int. Joint Conf"
    },
    {
      "citation_id": "38",
      "title": "Affective brain-computer interfaces (abcis): A tutorial",
      "authors": [
        "D Wu",
        "B.-L Lu",
        "B Hu",
        "Z Zeng"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE"
    },
    {
      "citation_id": "39",
      "title": "EEG Feature Selection via Global Redundancy Minimization for Emotion Recognition",
      "authors": [
        "X Xu",
        "T Jia",
        "Q Li",
        "F Wei",
        "L Ye",
        "X Wu"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "40",
      "title": "Embedded EEG Feature Selection for Multidimension Emotion Recognition via Local and Global Label Relevance",
      "authors": [
        "X Xu",
        "F Wei",
        "T Jia",
        "L Zhuo",
        "H Zhang",
        "X Li",
        "X Wu"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Neural Systems and Rehabilitation Engineering"
    },
    {
      "citation_id": "41",
      "title": "Eeg Feature Selection Using Orthogonal Regression: Application to Emotion Recognition",
      "authors": [
        "X Xu",
        "F Wei",
        "Z Zhu",
        "J Liu",
        "X Wu"
      ],
      "year": "2020",
      "venue": "ICASSP 2020 -2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "42",
      "title": "Multi-label Feature Selection via Global Relevance and Redundancy Optimization",
      "authors": [
        "J Zhang",
        "Y Lin",
        "M Jiang",
        "S Li",
        "Y Tang",
        "K Tan"
      ],
      "year": "2020",
      "venue": "IJCAI"
    },
    {
      "citation_id": "43",
      "title": "Manifold regularized discriminative feature selection for multilabel learning",
      "authors": [
        "J Zhang",
        "Z Luo",
        "C Li",
        "C Zhou",
        "S Li"
      ],
      "year": "2019",
      "venue": "Pattern Recognition"
    },
    {
      "citation_id": "44",
      "title": "ML-KNN: A lazy learning approach to multi-label learning",
      "authors": [
        "M Zhang",
        "Z Zhou"
      ],
      "year": "2007",
      "venue": "Pattern Recognition"
    },
    {
      "citation_id": "45",
      "title": "Feature selection with multi-view data: A survey",
      "authors": [
        "R Zhang",
        "F Nie",
        "X Li",
        "X Wei"
      ],
      "year": "2019",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "46",
      "title": "Identifying stable patterns over time for emotion recognition from EEG",
      "authors": [
        "W.-L Zheng",
        "J.-Y Zhu",
        "B.-L Lu"
      ],
      "year": "2019",
      "venue": "IEEE transactions on affective computing"
    }
  ]
}