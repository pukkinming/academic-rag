{
  "paper_id": "2310.12851v1",
  "title": "Emodiarize: Speaker Diarization And Emotion Identification From Speech Signals Using Convolutional Neural Networks A Preprint",
  "published": "2023-10-19T16:02:53Z",
  "authors": [
    "Hanan Hamza",
    "Fiza Gafoor",
    "Fathima Sithara",
    "Gayathri Anil",
    "V. S. Anoop"
  ],
  "keywords": [
    "Speech emotion analysis",
    "Speaker diarization",
    "Convolutional neural network",
    "Emotion identification",
    "Machine learning"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "In the era of advanced artificial intelligence and human-computer interaction, identifying emotions in spoken language is paramount. This research explores the integration of deep learning techniques in speech emotion recognition, offering a comprehensive solution to the challenges associated with speaker diarization and emotion identification. It introduces a framework that combines a pre-existing speaker diarization pipeline and an emotion identification model built on a Convolutional Neural Network (CNN) to achieve higher precision. The proposed model was trained on data from five speech emotion datasets, namely, RAVDESS, CREMA-D, SAVEE, TESS, and Movie Clips, out of which the latter is a speech emotion dataset created specifically for this research. The features extracted from each sample include Mel Frequency Cepstral Coefficients (MFCC), Zero Crossing Rate (ZCR), Root Mean Square (RMS), and various data augmentation algorithms like pitch, noise, stretch, and shift. This feature extraction approach aims to enhance prediction accuracy while reducing computational complexity. The proposed model yields an unweighted accuracy of 63%, demonstrating remarkable efficiency in accurately identifying emotional states within speech signals.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Speech Emotion Recognition (SER) is a specialized branch of Natural Language Processing (NLP) that centers around interpreting emotions conveyed through spoken language Al-Dujaili and Ebrahimi-Moghadam  [2023] . The human voice carries a wealth of emotional information in its pitch, intensity, rhythm, and timbre, which makes it a valuable resource for understanding the emotional state of an individual. Emotion recognition from audio samples is a valuable application of machine learning and signal processing, with numerous potential use cases in areas like entertainment, market research, mental health assessment, and more  Wagner et al. [2023] . This process involves analyzing audio data, often in the form of speech or vocal expressions, to determine the emotional state of the speaker. It uses machine learning and deep learning techniques to discern the emotional state of a person from their voice, whether it is in spoken language, song, or other vocalizations de Lope and Graña  [2023] .\n\nThe practical applications of this technology are far-reaching. In entertainment, emotion recognition can transform how we interact with media, making it more immersive and responsive Daneshfar and Jamshidi  [2023] . In marketing and market research, it can provide invaluable insights into consumer sentiment and preferences, enhancing product development and advertising strategies  Kaur and Singh [2023] . Additionally, the field of mental health holds the potential to assist in early diagnosis, monitoring, and treatment, offering a new dimension to patient care Anoop and  Sreelakshmi [2023]   Varghese and Anoop [2022] . However, amidst these promises, we must navigate challenges related to accuracy, privacy, and the ethical implications of emotion recognition technologyAnoop  [2023] . This report aims to navigate this multifaceted landscape, shedding light on the methods, opportunities, and responsibilities inherent in the fascinating world of emotion recognition from audio.\n\nSpeaker diarization is the process of segmenting audio recordings by speaker labels, and it effectively tells us \"Who spoke when?\" in the audio recording  Serafini et al. [2023] . It faces challenges, particularly in situations with multiple speakers talking simultaneously, varying speech patterns, background noise, or low-quality audio recordings  Wu et al. [2023] . It has a wide range of applications, including transcribing multi-speaker conversations accurately, indexing large audio and video archives, and enhancing the performance of speech recognition systems when multiple speakers are involved  Maiti et al. [2023] . Various methods have been employed for speaker diarization over the years. Traditional techniques include Gaussian Mixture Models (GMMs) and Hidden Markov Models (HMMs). More recently, deep learning-based methods, such as recurrent neural networks (RNNs) and convolutional neural networks (CNNs), have shown promise in improving diarization accuracy, especially in complex scenarios  Park et al. [2022] Sreelakshmi and Anoop.\n\nThis paper presents a machine-learning model that performs speaker diarization and emotion identification from speech signals using CNN. The structure of the paper is as follows: Section 2 reviews related research, while Section 3 defines important concepts like CNN and speaker diarization and introduces the relevant datasets. Section 4 elaborates on the proposed approach, followed by a detailed account of the experiments conducted in Section 5. Section 6 then presents the results obtained. Finally, Section 7 presents the conclusion and discusses potential directions for future research.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Related Studies",
      "text": "The significance of Speech Emotion Recognition lies in its capacity to enhance human-computer interactions, provide fresh insights into human behavior, and find applications in various fields, ultimately improving the quality of services and experiences. This importance of SER has piqued significant interest, leading to numerous studies and surveys. This section reviews recent publications and research closely aligned with this study area.  Khalil et al. [2019]  discussed traditional SER methods and their limitations, highlighting the need for more advanced approaches, particularly those involving deep learning. The paper thoroughly examines various deep learning techniques like Deep Boltzmann Machines (DBM), Recurrent Neural Networks (RNN), Convolutional Neural Networks (CNN), and Auto Encoders (AE) and for each of these techniques, provides a detailed analysis of their advantages and disadvantages within the context of SER. This review underscores the growing importance of deep learning in SER research while acknowledging its limitations. A similar study was carried out  Abbaschian et al. [2021] , comparing the existing approaches and databases used in SER. They focused on two main areas: deep learning methods and conventional machine learning techniques, conducting a comprehensive comparison of various practical neural network approaches to speech emotion recognition.  Lieskovská et al. [2021]  discusses the recent incorporation of attention mechanism into DNN architecture to emphasize emotionally significant information. It reviews recent developments in SER and investigates how the different attention mechanisms affect SER performance. The study concludes with a comprehensive comparison of system accuracies using the IEMOCAP benchmark database.\n\nChoosing the right feature extraction techniques in SER is crucial as it determines how effectively relevant information is captured from speech signals.  Gupta et al. [2020]  investigates various such techniques, focusing on prosodic and spectral features like Mel Frequency Cepstral Coefficients (MFCC), Linear Prediction Cepstral Coefficients (LPCC), and Linear Prediction Coefficients (LPC). The study concludes that MFCC is the most effective for recognizing emotional content in speech. Existing classification techniques are applied to identify human emotions, with a detailed comparative analysis based on statistical and mathematical results. The paper also introduces an optimal model, Depthwise Separable Convolutional Neural Network (DSCNN), designed for raw spectrogram data.  Issa et al. [2020]  introduces a novel architecture that directly processes sound data, extracting various acoustic features from audio files, and employs a one-dimensional Convolutional Neural Network (CNN) for emotion identification on publicly available datasets RAVDESS, EMO-DB and IEMOCAP. Using an incremental model refinement, their best-performing model achieves state-of-the-art results in RAVDESS and IEMOCAP, and performs competitively in the EMO-DB dataset.  Anvarjon et al. [2020]  introduces an SER model that utilizes a CNN approach to learn deep frequency features using a specific rectangular filter and modified pooling strategy. The model is trained on frequency features extracted from speech data and tested for emotion prediction. Evaluation on two benchmarks, IEMOCAP and EMO-DB speech datasets, results in high recognition accuracies, demonstrating that the proposed system outperforms state-of-the-art SER systems.\n\nTo address the challenge of inadequate datasets for training deep learning models in SER,  Abdelhamid et al. [2022]  proposed a data augmentation algorithm to enrich the dataset by introducing carefully designed noise fractions. In addition, they introduce an optimized deep learning model that combines a Convolutional Neural Network (CNN) and a Long Short-Term Memory (LSTM) layer to capture both local and long-term correlations in speech samples along with hyperparameters fine-tuned for improved recognition results. The learning rate and label smoothing regularization factor are optimized using a stochastic fractal search-guided whale optimization algorithm. Experimental results on speech emotion datasets IEMOCAP, EMO-DB, RAVDESS, and SAVEE demonstrate the superior performance of the proposed approach, achieving high recognition accuracies. The recognition accuracy of speech emotion recognition systems can also be improved using alternate methods. This is demonstrated in  Koduru et al. [2020]  using different feature extraction algorithms. It emphasizes preprocessing audio samples where noise is removed using filters and employs Mel Frequency Cepstral Coefficients (MFCC), Discrete Wavelet Transform (DWT), pitch, energy, and Zero Crossing Rate (ZCR) algorithms for feature extraction. Global feature selection reduces redundant information, and machine learning classification algorithms are applied to identify emotions. The study validates these algorithms for universal emotions, namely Anger, Happiness, Sadness, and Neutral.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Materials And Methods",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Convolutional Neural Networks (Cnn)",
      "text": "Convolutional Neural Networks (CNNs), often called ConvNets, constitute a prominent category within the domain of deep learning methodologies characterized by their feed-forward architecture. These neural networks have emerged as potent tools, particularly well-suited for the processing of structured grid data. While initially devised for computer vision applications, their utility extends beyond visual tasks to encompass domains like speech analysis and natural language processing. CNNs excel in scenarios involving structured grid data, encompassing many data types, including images and spectrograms. This ability comes from the fact that CNNs are designed to work like our visual system. They are inspired by how our eyes and brains hierarchically see things and process information. The overall architecture of the CNN model is shown in Figure  1 , which illustrates the layered architecture of a fundamental CNN network, providing a comprehensive overview of the network's structural components and their interconnections. The architectural composition of a Convolutional Neural Network (CNN) is foundational to its functionality, comprising several key components:\n\n• Convolutional Layer: Convolutional layers are the building blocks of CNNs. Convolutional operations are used to extract local features out of the input data. Multiple filters make up each layer, which move across the input data to capture spatial hierarchies of features. The convolution operation can be described mathematically as:\n\nIn this equation, 'X' represents the input data, typically a spectrogram, 'W' signifies the weight matrix of the convolutional filter, and 'b' accounts for the bias term. The outcome 'Z[i,j]' denotes the weighted summation of input values at the position (i,j) and is obtained by convolving the filter 'W' with input data 'X' and factoring in the bias 'b.'\n\n• Pooling Layers: Pooling layers, which are frequently interspersed with convolutional layers, are vital for lowering spatial dimensions while preserving important data. Common pooling methods include max-pooling and average-pooling, which downsample feature maps by choosing the maximum or average value within certain geographic areas. • Fully Connected Layers: Fully Connected layers at the CNN architecture's core allow for high-level decisionmaking and global context aggregation. These layers establish connections between every neuron and every neuron in the layer above, enabling the model to understand intricate correlations between characteristics that were retrieved by earlier levels.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Speaker Diarization",
      "text": "Speaker diarization is a vital process in the realm of audio analysis, aimed at segmenting an audio stream containing human speech into coherent portions based on the identity of each speaker. This technique not only enhances the comprehensibility of automated speech transcriptions but also facilitates the organization of the audio stream into distinct speaker segments. Additionally, when integrated with speaker recognition systems, diarization identifies speakers, addressing the fundamental question of \"who spoke when.\" In this context, PyAnnote Audio (available at https://github.com/pyannote/pyannote-audio) emerges as a noteworthy open-source toolkit implemented in Python, meticulously crafted for speaker diarization. Version 2.1 marks a significant evolution, introducing substantial improvements to the default speaker diarization pipeline, which comprises three pivotal stages. Firstly, there's the speaker segmentation phase, which operates on a short sliding window. Next, the toolkit incorporates neural speaker embedding, allowing for the extraction of unique speaker characteristics. Lastly, the process culminates with agglomerative clustering, which effectively groups speaker embeddings, facilitating the identification and separation of individual speakers. This latest version, PyAnnote Audio Speaker Diarization (Version 2.1), represents a substantial advancement in diarization and offers a more refined and effective means of tackling the challenge of \"who spoke when.\" Its enhancements have the potential to greatly benefit various applications, including transcription systems and speaker identification tasks.  Bredin [2023]",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Datasets",
      "text": "The following datasets were used to build the machine-learning model. The first four are publicly available datasets. The final dataset was created specifically for the purpose of this project. The dataset consists of 7,442 original clips from 91 actors from various ethnic backgrounds. These clips are from 48 male and 43 female actors between the ages of 20 and 74 coming from various races and ethnicities (African American, Asian, Caucasian, Hispanic, and Unspecified). Actors spoke from a selection of 12 sentences. The labels were created using the crowdsourcing of 2,443 raters. Each sample in this dataset contains two ratings, one for the emotion category and the other for intensity. The sentences were presented using one of six different emotions (Anger, Disgust, Fear, Happy, Neutral, and Sad) and four different emotion levels (Low, Medium, High, and Unspecified).  Cao et al. [2014]  • TESS (Toronto Emotional Speech Set): This is an acted dataset created primarily to investigate the influence of aging on the capacity to recognize emotions. There is a set of 200 neutral target sentences spoken in the carrier phrase \"Say the word\" by two actresses (aged 26 and 64 years), and recordings were made of the set portraying each of seven emotions (anger, disgust, fear, happiness, pleasant surprise, sadness, and neutral). 56 undergraduate students were asked to identify emotions from phrases to categorize the dataset. Following the identification task, statements with greater than 66% confidence were chosen to be included in the dataset. There are 2,800 data points (audio files) in total. All the audio files are in WAV format.  Dupuis and Pichora-Fuller [2010]  • SAVEE (Survey Audio-Visual Expressed Emotion): This dataset was collected from four native English male speakers (designated as DC, JE, JK, and KL), postgraduate students and researchers at the University of Surrey ranging in age from 27 to 31 years. Anger, contempt, fear, pleasure, sorrow, and surprise have all been classified as distinct emotions in psychology. A neutral category is also included, resulting in records of 7 emotion types. The text material consisted of 15 TIMIT sentences per emotion: 3 common, 2 emotion-specific, and 10 generic lines that were phonetically balanced and distinct for each emotion. To make 30 neutral sentences, the 3 common and 2*6 = 12 emotion-specific phrases were registered as neutral. This resulted in 120 utterances per speaker  Ryumina et al. [2022] .\n\n• Movie clips: The dataset was compiled by randomly selecting audio clips from movies and then annotating them based on the expressed emotions. It has a total of 166 clips. The emotions identified are anger, happy, fear, disgust, sad, surprise, and neutral.\n\n4 Proposed Approach",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Data Augmentation",
      "text": "Data augmentation is a technique used in deep learning to improve the quality of the data being used to train a model. It does so by adding small variations to existing data samples. It helps prevent the models from overfitting and improves the model accuracy Ferreira-Paiva et al.  [2022] . The techniques used here are as follows:\n\n• Noise: Add random noises to the audio recordings • Stretching: Stretch the audio while preserving the audio contents.\n\n• Changing pitch: Randomly change the pitch of the audio",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Feature Extraction",
      "text": "Feature extraction is a crucial step in analyzing and establishing relationships between various elements. Audio data, in its raw form, cannot be directly fed into the models. So it is necessary to convert it into a format that models can understand, and this is where feature extraction comes into play. The audio signal is a 3-dimensional signal in which the three axes represent time, amplitude, and frequency. By using the sample rate and sample data, several transformations can be performed to extract valuable features out of it.\n\n• Zero Crossing Rate: The rate of sign-changes of the signal during the duration of a particular Frame.\n\n• MFCC: Mel Frequency Cepstral Coefficients form a cepstral representation where the frequency bands are not linear but distributed according to the mel scale.\n\n• RMS (Root Mean Square) value: It is the square of the arithmetic mean or the square of the function.  Aggarwal et al. [2022]",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Speaker Diarization",
      "text": "Speaker diarization is the process of segmenting the human-speaking audio stream into separate parts based on the identity of each speaker. By dividing the audio into speaker turns, this method makes computerized speech transcriptions easier to understand. As discussed in Section 3.2 Park et al.\n\n[2022]",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Data Preparation",
      "text": "Splitting the dataset is a common practice in machine learning to assess the performance of the models on unseen data and avoid overfitting. Hence, it enables the development and evaluation of machine learning models. Proper data splitting helps assess a model's generalization performance and ability to make accurate predictions on new and unseen data. It takes several parameters to control how the data should be split. 'X' typically contains the independent variables or features that the model uses to make predictions. 'Y' typically contains the corresponding target values or labels that your model is trying to predict. The random state parameter is important for reproducibility. It sets a seed for the random number generator, ensuring that the same random split will be generated every time you run this code with the same seed. In this case, the random state is 42. The test size parameter determines the proportion of the data allocated to the testing set. In this case, 20% of the data will be used for testing, and the remaining 80% will be used for training. Shuffling is usually set to 'True' to prevent any inherent order or bias in the data from affecting the model's performance.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Model Selection",
      "text": "The emotion expressed in each audio segment obtained after speaker diarization is identified in this step. We employed multiple models, namely LSTM, CNN, and CLSTM, for emotion prediction. After testing these models, it was determined that the CNN model provided the most accurate predictions, and as a result, we selected the same. In this case, a sequential model is employed to construct the CNN. • Sequential Model A sequential model in machine learning provides a structured framework for organizing and applying processing steps in a specific sequence, ensuring that data flows through the model in a predetermined order. The term \"sequential\" refers to the specific way in which layers are stacked in the model, where one layer follows another in a sequential order. Sequential models are commonly used in various neural networks, including CNN. CNNs often have a sequence of layers that perform operations like convolution, pooling, flattening, and fully connected layers. These layers are typically arranged sequentially in the order they are applied to the input data, as explained in Section 3.1",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Model",
      "text": "",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Experiments",
      "text": "This project was done in a system with an Intel core i5 processor having Windows 11. We have executed the code in Visual Studio code with Python environment 3.9. First, we have collected 5 datasets, of which 4 are available on the internet, and the 5th dataset was created especially for this project. Then we integrated the 5 datasets into a single dataset. The next step is data augmentation.\n\n• Data Augmentation: The process in which our training dataset is converted to a new synthetic data sample by adding small perturbations is called Data Augmentation. It is done by applying noise injection, shifting time, changing pitch. It is to make the model invariant to those perturbations and enhance its generalization ability.\n\n(Refer Section 4.2.)\n\n• Feature Extraction: During the features extraction phase, three features -Zero Crossing Rate (ZCR),Mel Frequency Cepstral Coefficient (MFCC), and Root Mean Square Value (RMS) -are extracted. The duration for the feature extraction process is set to 2.5 seconds and the offset to 0.6 seconds to accommodate the varying data lengths. A total of 22 features are extracted from each audio, containing one ZCR value, one RMS value, and 20 MFCC coefficients (Refer Section 4.3)\n\n• Speaker Diarization: Speaker diarization is the process of identifying and separating individual speakers in an audio stream so that, in the automatic speech recognition (ASR) transcript, each speaker's utterances are identified and separated. Speaker diarization is used in scenarios where multiple speakers are involved. Here, a pre-trained speaker diarization pipeline is used to identify and separate two speakers from a given set of audio samples. The integrated dataset is run through the pre-trained model, and the output contains utterances of each speaker separately, along with their start time and end time. The result of the diarization process is represented in the RTTM (Rich Transcription Time Marked) format.\n\n• Splitting the dataset for machine learning: This is a common practice in machine learning to assess the performance of the models on unseen data and avoid overfitting. Hence, it enables the development and evaluation of machine learning models. Proper data splitting helps assess a model's generalization performance and its ability to make accurate predictions on new and unseen data. It takes several parameters to control how the data should be split. 'X' typically contains the independent variables or features that the model uses to make predictions. 'Y' typically contains the corresponding target values or labels that your model is trying to predict. The random state parameter is important for reproducibility. It sets a seed for the random number generator, ensuring that the same random split will be generated every time you run this code with the same seed. In this case, the random state is 42. The test size parameter determines the proportion of the data allocated to the testing set. In this case, 20% of the data will be used for testing, and the remaining 80% will be used for training. Shuffling is usually set to 'True' to prevent any inherent order or bias in the data from affecting the model's performance.\n\n• CNN Model Architecture: We have selected CNN architecture for our model building. We have used a sequential model to implement CNN architecture. A sequential model is a model in which layers are added one after the other. It starts with a convolutional layer with 512 filters, a kernel size of 5, and 'same' padding.\n\nThe activation function used here is ReLU-Rectified Linear unit. It works as follows:\n\n-If input to the function is positive, it outputs that positive value.\n\n-If the input value is negative, then it outputs zero.\n\nA batch normalization is applied after this convolutional layer. it works by normalizing the inputs to a layer in a mini-batch of data, adjusting them to have zero mean and unit variance. Then, there is a max-pooling layer with a pool size of 5 and a stride of 2. This reduces the spatial dimensions of the feature maps. Another set of convolutional layers, batch normalization layers, and max-pooling layers is added. These layers help the model extract and learn hierarchical features from the input data. A dropout layer with a dropout rate 0.2 is added after the second max-pooling layer. Dropout is a regularization technique to prevent overfitting. Again 2 sets of convolutional, batch normalization, and max-pooling layers are added. Also, a dropout layer with a dropout rate of 0.2 is added. this is repeated again. After several convolutional and max-pooling layers, there's a flattened layer that transforms the 2D feature maps into a 1D vector. Two dense (fully connected) layers are added. The first dense layer has 512 units and ReLU activation. Batch normalization is applied after the first dense layer. The second dense layer has 7 units (assuming it's a classification task with 7 classes) and uses the softmax activation function, which is typical for multi-class classification. Then training process is configured. It uses the Adam optimizer, categorical cross-entropy loss (common for multi-class classification), and accuracy as the evaluation metric. Then the model is evaluated.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Results And Discussions",
      "text": "This section discusses the results obtained from the experiment conducted using our proposed framework which is followed by a detailed discussion. The model is better at predicting surprise and irrational emotions. It makes sense  The classification report obtained is given in Table  1 : The model accurately predicts the emotion \"surprise\" with a precision of 0.86, the highest precision value in the classification report. This indicates that most \"surprise\" feelings are correctly predicted. Additionally, the emotion's F1-score and recall value are also 0.86. The accuracy of the model is calculated by:\n\nwhere TP is True Positive, TN is True Negative, FN is False Negative, and FP is False Positive. Here, the accuracy obtained is 63%. The training vs testing loss is plotted in the graph shown in Figure  6  Training loss is initially fairly substantial at epoch 1. This is due to the model's inability to recognize any patterns in the data yet. The training loss gradually diminishes as training goes on (from epochs 2 to 50). This shows that the model is growing more adept at identifying patterns in the training data and at fitting the data more accurately. Similarly, because the model has not yet been exposed to the testing data, the testing loss likewise begins at higher values during epoch 1. However, the testing loss initially diminishes as training continues (from epochs 2 to 50). This is a good indicator because it shows that the model is successfully generalizing to new data. Although there may be minor swings in the testing loss graph, there is a little rising tendency. As a result, the model may become excessively specialized in fitting the training data and may not perform as well on fresh data, raising the possibility of overfitting. The training vs testing accuracy graph is shown in  Emotions are essential to communication because they enable us to express complicated feelings verbally. The goal of our study was to develop a smart system that can recognize these emotions in speech. We created this system by fusing two technologies: one that can distinguish between speakers in audio recordings and another that can foretell each speaker's emotions. This system has several applications. For instance, it can assist businesses in understanding clients' emotions during encounters, allowing them to enhance services in real time. It offers a deeper understanding of customer emotions and goes beyond straightforward satisfaction surveys. Therapists can utilize this approach to more effectively comprehend their patients' feelings during therapy sessions in psychological care, enabling more focused and sympathetic interventions. This could improve the standard of mental health care while also potentially accelerating the healing process for patients. Our experiment demonstrates the intriguing potential of integrating audio separation and emotion prediction to enhance human-technology interaction.  Huang et al. [2014] . Exciting possibilities lie in the future of speaker diarization, which separates and identifies speakers in audio recordings. With modern technology like deep learning, improved accuracy is also to be expected, making it more dependable in varied scenarios.\n\nTeleconferencing and real-time applications like live transcription will function more effectively. Speaker diarization will become more advanced to support several languages, incorporate various media kinds (text, video), and have applications in healthcare, education, and entertainment. It will be essential to address privacy issues, and adjusting to complicated audio situations with multiple speakers is a top priority. The future of this technology will also be shaped by customization for particular circumstances, standard evaluation measures, and integration with language and emotion recognition, making it more accessible and adaptable for various applications.",
      "page_start": 8,
      "page_end": 10
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: , which illustrates the layered architecture of a fundamental CNN network, providing",
      "page": 3
    },
    {
      "caption": "Figure 1: CNN network architecture",
      "page": 4
    },
    {
      "caption": "Figure 2: The overall workflow of the proposed speaker diarization and speech emotion recognition approach",
      "page": 5
    },
    {
      "caption": "Figure 3: Count of emotion",
      "page": 6
    },
    {
      "caption": "Figure 4: Confusion matrix",
      "page": 8
    },
    {
      "caption": "Figure 4: The better the model performs, the darker",
      "page": 8
    },
    {
      "caption": "Figure 5: Precision, recall, f1-score of the proposed approach on different classes",
      "page": 9
    },
    {
      "caption": "Figure 6: Training loss is initially fairly",
      "page": 9
    },
    {
      "caption": "Figure 7: Since the model has not yet acquired the characteristics of the data, the training accuracy is generally low at",
      "page": 9
    },
    {
      "caption": "Figure 6: Graph showing training vs testing loss",
      "page": 10
    },
    {
      "caption": "Figure 7: Graph showing training vs testing accuracy",
      "page": 10
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Precision": "0.73",
          "Recall": "0.74",
          "F1-score": "0.73",
          "Support": "1173"
        },
        {
          "Precision": "0.51",
          "Recall": "0.58",
          "F1-score": "0.55",
          "Support": "1155"
        },
        {
          "Precision": "0.63",
          "Recall": "0.49",
          "F1-score": "0.55",
          "Support": "1157"
        },
        {
          "Precision": "0.60",
          "Recall": "0.57",
          "F1-score": "0.59",
          "Support": "1175"
        },
        {
          "Precision": "0.60",
          "Recall": "0.65",
          "F1-score": "0.62",
          "Support": "1153"
        },
        {
          "Precision": "0.64",
          "Recall": "0.67",
          "F1-score": "0.66",
          "Support": "1207"
        },
        {
          "Precision": "0.86",
          "Recall": "0.86",
          "F1-score": "0.86",
          "Support": "377"
        },
        {
          "Precision": "",
          "Recall": "",
          "F1-score": "0.63",
          "Support": "7397"
        },
        {
          "Precision": "0.65",
          "Recall": "0.65",
          "F1-score": "0.65",
          "Support": "7397"
        },
        {
          "Precision": "0.63",
          "Recall": "0.63",
          "F1-score": "0.63",
          "Support": "7397"
        }
      ],
      "page": 9
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Speech emotion recognition: a comprehensive survey",
      "authors": [
        "Mohammed Jawad",
        "Abbas Ebrahimi-Moghadam"
      ],
      "year": "2023",
      "venue": "Wireless Personal Communications"
    },
    {
      "citation_id": "2",
      "title": "Dawn of the transformer era in speech emotion recognition: closing the valence gap",
      "authors": [
        "Johannes Wagner",
        "Andreas Triantafyllopoulos",
        "Hagen Wierstorf",
        "Maximilian Schmitt",
        "Felix Burkhardt",
        "Florian Eyben",
        "Björn Schuller"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "3",
      "title": "Fatemeh Daneshfar and Mohammad Behdad Jamshidi. An octonion-based nonlinear echo state network for speech emotion recognition in metaverse",
      "authors": [
        "Javier De",
        "Manuel Graña"
      ],
      "year": "2023",
      "venue": "Neural Networks"
    },
    {
      "citation_id": "4",
      "title": "Trends in speech emotion recognition: a comprehensive survey",
      "authors": [
        "Kamaldeep Kaur",
        "Parminder Singh"
      ],
      "year": "2023",
      "venue": "Multimedia Tools and Applications"
    },
    {
      "citation_id": "5",
      "title": "Public discourse and sentiment during mpox outbreak: an analysis using natural language processing",
      "authors": [
        "Vs Anoop",
        "Sreelakshmi"
      ],
      "year": "2023",
      "venue": "Public Health"
    },
    {
      "citation_id": "6",
      "title": "Deep learning-based sentiment analysis on covid-19 news videos",
      "authors": [
        "Milan Varghese",
        "Vs Anoop"
      ],
      "year": "2022",
      "venue": "Proceedings of International Conference on Information Technology and Applications: ICITA 2021"
    },
    {
      "citation_id": "7",
      "title": "Sentiment classification of diabetes-related tweets using transformer-based deep learning approach",
      "authors": [
        "Vs Anoop"
      ],
      "year": "2023",
      "venue": "International Conference on Advances in Computing and Data Sciences"
    },
    {
      "citation_id": "8",
      "title": "An experimental review of speaker diarization methods with application to two-speaker conversational telephone speech recordings",
      "authors": [
        "Luca Serafini",
        "Samuele Cornell",
        "Giovanni Morrone",
        "Enrico Zovato",
        "Alessio Brutti",
        "Stefano Squartini"
      ],
      "year": "2023",
      "venue": "Computer Speech & Language"
    },
    {
      "citation_id": "9",
      "title": "Semi-supervised multi-channel speaker diarization with cross-channel attention",
      "authors": [
        "Shilong Wu",
        "Jun Du",
        "Maokui He",
        "Shutong Niu",
        "Hang Chen",
        "Haitao Tang",
        "Chin-Hui Lee"
      ],
      "year": "2023",
      "venue": "Semi-supervised multi-channel speaker diarization with cross-channel attention",
      "arxiv": "arXiv:2307.08688"
    },
    {
      "citation_id": "10",
      "title": "Eend-ss: Joint end-to-end neural speaker diarization and speech separation for flexible number of speakers",
      "authors": [
        "Soumi Maiti",
        "Yushi Ueda",
        "Shinji Watanabe",
        "Chunlei Zhang",
        "Meng Yu",
        "Shi-Xiong Zhang",
        "Yong Xu"
      ],
      "year": "2023",
      "venue": "2022 IEEE Spoken Language Technology Workshop (SLT)"
    },
    {
      "citation_id": "11",
      "title": "A review of speaker diarization: Recent advances with deep learning",
      "authors": [
        "Jin Tae",
        "Naoyuki Park",
        "Dimitrios Kanda",
        "Dimitriadis",
        "J Kyu",
        "Shinji Han",
        "Shrikanth Watanabe",
        "Narayanan"
      ],
      "year": "2022",
      "venue": "Computer Speech & Language"
    },
    {
      "citation_id": "12",
      "title": "A deep convolutional neural network model for medical data classification from computed tomography images",
      "authors": [
        "S Sreelakshmi",
        "Vs Anoop"
      ],
      "venue": "Expert Systems"
    },
    {
      "citation_id": "13",
      "title": "Speech emotion recognition using deep learning techniques: A review",
      "authors": [
        "Edward Ruhul Amin Khalil",
        "Mohammad Jones",
        "Tariqullah Inayatullah Babar",
        "Mohammad Jan",
        "Thamer Haseeb Zafar",
        "Alhussain"
      ],
      "year": "2019",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "14",
      "title": "Deep learning techniques for speech emotion recognition, from databases to models",
      "authors": [
        "Daniel Babak Joze Abbaschian",
        "Adel Sierra-Sosa",
        "Elmaghraby"
      ],
      "year": "2021",
      "venue": "Sensors"
    },
    {
      "citation_id": "15",
      "title": "A review on speech emotion recognition using deep learning and attention mechanism",
      "authors": [
        "Eva Lieskovská",
        "Maroš Jakubec",
        "Roman Jarina",
        "Michal Chmulík"
      ],
      "year": "2021",
      "venue": "Electronics"
    },
    {
      "citation_id": "16",
      "title": "Emotion recognition of audio/speech data using deep learning approaches",
      "authors": [
        "Vedika Gupta",
        "Stuti Juyal",
        "Pal Gurvinder",
        "Chirag Singh",
        "Nishant Killa",
        "Gupta"
      ],
      "year": "2020",
      "venue": "Journal of Information and Optimization Sciences"
    },
    {
      "citation_id": "17",
      "title": "Speech emotion recognition with deep convolutional neural networks",
      "authors": [
        "Dias Issa",
        "M Fatih Demirci",
        "Adnan Yazici"
      ],
      "year": "2020",
      "venue": "Biomedical Signal Processing and Control"
    },
    {
      "citation_id": "18",
      "title": "Deep-net: A lightweight cnn-based speech emotion recognition system using deep frequency features",
      "authors": [
        "Tursunov Anvarjon",
        "Soonil Mustaqeem",
        "Kwon"
      ],
      "year": "2020",
      "venue": "Sensors"
    },
    {
      "citation_id": "19",
      "title": "Robust speech emotion recognition using cnn+ lstm based on stochastic fractal search optimization algorithm",
      "authors": [
        "Abdelaziz A Abdelhamid",
        "M El-Sayed",
        "Bandar El-Kenawy",
        "Alotaibi",
        "M Ghada",
        "Mahmoud Amer",
        "Abdelhameed Abdelkader",
        "Marwa Ibrahim",
        "Eid Metwally"
      ],
      "year": "2022",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "20",
      "title": "Feature extraction algorithms to improve the speech emotion recognition rate",
      "authors": [
        "Anusha Koduru",
        "Hima Bindu Valiveti",
        "Anil Kumar"
      ],
      "year": "2020",
      "venue": "International Journal of Speech Technology"
    },
    {
      "citation_id": "21",
      "title": "pyannote. audio 2.1 speaker diarization pipeline: principle, benchmark, and recipe",
      "authors": [
        "Hervé Bredin"
      ],
      "year": "2023",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "22",
      "title": "Crema-d: Crowd-sourced emotional multimodal actors dataset",
      "authors": [
        "Houwei Cao",
        "David Cooper",
        "Ruben Michael K Keutmann",
        "Ani Gur",
        "Ragini Nenkova",
        "Verma"
      ],
      "year": "2014",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "23",
      "title": "Toronto emotional speech set (tess)-younger talker_happy",
      "authors": [
        "Kate Dupuis",
        "Kathleen Pichora-Fuller"
      ],
      "year": "2010",
      "venue": "Toronto emotional speech set (tess)-younger talker_happy"
    },
    {
      "citation_id": "24",
      "title": "In search of a robust facial expressions recognition model: A large-scale visual cross-corpus study",
      "authors": [
        "Elena Ryumina",
        "Denis Dresvyanskiy",
        "Alexey Karpov"
      ],
      "year": "2022",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "25",
      "title": "A survey of data augmentation for audio classification",
      "authors": [
        "Lucas Ferreira-Paiva",
        "Elizabeth Alfaro-Espinoza",
        "M Vinicius",
        "Leonardo Almeida",
        "Rodolpho Va Felix",
        "Neves"
      ],
      "venue": "XXIV Brazilian Congress of Automatics (CBA)"
    },
    {
      "citation_id": "26",
      "title": "Abeer Ali Alnuaim, Aseel Alhadlaq, and Heung-No Lee. Two-way feature extraction for speech emotion recognition using deep learning",
      "authors": [
        "Apeksha Aggarwal",
        "Akshat Srivastava",
        "Ajay Agarwal",
        "Nidhi Chahal",
        "Dilbag Singh"
      ],
      "year": "2022",
      "venue": "Sensors"
    },
    {
      "citation_id": "27",
      "title": "Cnn variants for computer vision: History, architecture, application, challenges and future scope",
      "authors": [
        "Dulari Bhatt",
        "Chirag Patel",
        "Hardik Talsania",
        "Jigar Patel",
        "Rasmika Vaghela",
        "Sharnil Pandya",
        "Kirit Modi",
        "Hemant Ghayvat"
      ],
      "year": "2021",
      "venue": "Electronics"
    },
    {
      "citation_id": "28",
      "title": "Human-centered emotion recognition in animated gifs",
      "authors": [
        "Zhengyuan Yang",
        "Yixuan Zhang",
        "Jiebo Luo"
      ],
      "year": "2019",
      "venue": "2019 IEEE International Conference on Multimedia and Expo (ICME)"
    },
    {
      "citation_id": "29",
      "title": "Speech emotion recognition using cnn",
      "authors": [
        "Zhengwei Huang",
        "Ming Dong",
        "Qirong Mao",
        "Yongzhao Zhan"
      ],
      "year": "2014",
      "venue": "Proceedings of the 22nd ACM international conference on Multimedia"
    }
  ]
}