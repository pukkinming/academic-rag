{
  "paper_id": "2504.05368v1",
  "title": "Exploring Local Interpretable Model-Agnostic Explanations For Speech Emotion Recognition With Distribution-Shift",
  "published": "2025-04-07T17:38:21Z",
  "authors": [
    "Maja J. Hjuler",
    "Line H. Clemmensen",
    "Sneha Das"
  ],
  "keywords": [
    "Safe and trustworthy systems",
    "Local Interpretable Model-Agnostic Explanations",
    "Speech Emotion Recognition",
    "Explainable Artificial Intelligence"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "We introduce EmoLIME 1 , a version of local interpretable model-agnostic explanations (LIME) for black-box Speech Emotion Recognition (SER) models. To the best of our knowledge, this is the first attempt to apply LIME in SER. EmoLIME generates high-level interpretable explanations and identifies which specific frequency ranges are most influential in determining emotional states. The approach aids in interpreting complex, high-dimensional embeddings such as those generated by end-to-end speech models. We evaluate EmoLIME, qualitatively, quantitatively, and statistically, across three emotional speech datasets, using classifiers trained on both hand-crafted acoustic features and Wav2Vec 2.0 embeddings. We find that EmoLIME exhibits stronger robustness across different models than across datasets with distribution shifts, highlighting its potential for more consistent explanations in SER tasks within a dataset.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "Transformer models have revolutionized large-scale signal processing, influencing all data modalities  [1, 2, 3] , including speech and audio signals  [4, 5, 6] . While they are versatile across different domains due to their ability to incorporate information and structure in over-parameterized spaces, this also leads to black-box decisions, which is one of their main drawbacks. In other words, it is non-trivial to understand the decision-making process in transformers. In contrast to hand-crafted features, deep features may not represent any physical interpretation, and require alternative explainability techniques to aid the transparency and understanding behind the automated decisions.\n\nExplainable Artificial Intelligence (XAI) is rapidly advancing due to the importance of understanding the decisionmaking process of black-box deep-learning and machine learning models. This is particularly critical in high-stakes Fig.  1 . Functional block diagram of EmoLIME inspired by  [7]  and  [8] .\n\nsectors such as healthcare, law, and education, where the model outcome is as important as how one arrived at it. Transparency and explainability of automated systems are now also necessitated through regulatory mandates and frameworks like the EU AI act and the OECD AI principles, respectively  [9, 10] .\n\nXAI techniques are widely researched and established in computer vision (CV) and Natural Language Processing (NLP). Due to the tangible and physical nature of visual and text data, defining connections between input and output through models, and thereby explaining model predictions, is relatively more intuitive. This is in contrast to speech and audio signals, where XAI methods need to consider what to explain?; this is further influenced by the corresponding speech processing task and its application. Therefore, only a few XAI methods developed for CV and NLP are directly transferable to speech processing. LIME (Local Interpretable Model-Agnostic Explanations) and SHAP (SHapley Additive exPlanations) are state-of-theart XAI methods  [11, 12]  and are model-agnostic, ie: they can be applied to any machine-learning model. Hence, they have also been explored within speech-based classification models; LIME has been adapted to Automatic Speech Recognition (ASR)  [13]  and SHAP has been employed in speech emotion recognition (SER) to evaluate feature importance  [14, 15] . In contrast to gradient-based XAI techniques, LIME has an advantage in explaining waveform-fed models by directly assigning importance to decomposed audio patches rather than single time points  [16] . This makes LIME explanations more aligned with human intuition and easier to interpret since we can relate different elements or segments of the audio to the prediction. SHAP was first proposed as a unified framework for interpreting predictions and it is based on Shapley values from game theory. Some disadvantages of SHAP when compared to LIME include a lack of intuitiveness when working with complex transformed features from deep learning models that do not directly represent any physical characteristics of the audio. If the end-users are non-technical experts, even features like Mel-frequency cepstral coefficients (MFCCs) may not be considered interpretable. Furthermore, the technique can be computationally expensive for high-dimensional datasets and multi-class classification. The hand-crafted feature sets can consist of thousands of acoustic parameters making SHAP infeasible depending on system memory constraints.\n\nIn this work, we present EmoLIME, to explain the predictions of SER classifiers, developed for both hand-crafted and deep features. Due to the relevance of frequency based features in SER (eg: tone, pitch, etc), we primarily focus on spectral decomposition. EmoLIME is developed on LIME by decomposing the audio into equally sized frequency components. This leads to spectral masking in the training of the surrogate model. Explanations are generated by perturbing the input and training a linear sparse surrogate model which assigns weights to each input component. Our main contributions are summarised as follows: 1) We introduce EmoLIME, a LIME technique for interpretable local explanations of black--box SER models. To the best of our knowledge, our work represents the first attempt to apply LIME in SER. 2) We demonstrate EmoLIME on three emotional speech datasets for classifiers trained on hand-crafted and deep features, i.e. embeddings from a general speech model. 3) We investigate the transferability of the explanations across three datasets, with statistical conclusions on the influence of distribution shifts on the explanations. In a recent review  [18] , existing XAI methods for audio models are summarized and the authors emphasize the importance of enhancing their interpretability and trust. XAI methods are split into two categories: generic XAI methods, e.g. Integrated gradients  [19] , LIME  [11] , and SHAP  [12] , and XAI methods specialized for audio models, e.g. LRP  [20]  and DFT-LRP  [21] . Common to methods is they aim to explain complex audio signals and leverage human adeptness at interpreting harmonies, rhythm, and other high-level concepts through listening. SoundLIME (SLIME) proposed in  [7]  extends LIME to music content analysis, specifically to singing voice detection. Furthermore, LIME was proposed for audio classification in AudioLIME  [8] , a system that uses source separation to produce listenable explanations. Recently, an application of LIME to generate faithful audio explanations for COVID-19 detection from recordings of patients' coughs was presented in CoughLIME  [22] . What sets these studies apart is the classification task that LIME is extended to and the type of segmentation applied in the algorithm. While AudioLIME separates the audio into different sources, SLIME and CoughLIME decompose the input data into temporal, frequency, and time-frequency segmentations. The AudioLIME implementation does not generalize to emotional speech data from a single speaker, i.e. a single source. Furthermore, SLIME and CoughLIME generate explanations for binary classifiers, which are not directly applicable to multi-class SER models.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Iii. Method",
      "text": "LIME explains the predictions of any classifier or regressor by treating it as a black-box and approximating it locally with an interpretable model  [11] . Explanations are generated by perturbing the input and training a surrogate model that . The loss function of the surrogate model, g, is a locally weighted square loss, given by:\n\nwhere f is the black-box model and π x (z) is an exponential kernel learned over cosine distance, which accounts for the distance between the perturbed training samples z and the original input x. Hence, input samples z get predictions using f , and we weigh them by the proximity to the input being explained. The implementation of EmoLIME builds on CoughLIME 2  [22]  and the LIME Python module  [23] , and it requires the prediction function to output logits rather than class labels. To accommodate multiple classes, separate prediction functions were defined for each class to perform binary classification and output the class probability. The surrogate model is obtained using Ridge regression as is the default in LimeBase 3 .\n\nTo investigate hand-crafted vs. deep features, two models are included in the analysis; a linear support vector classifier (SVC) trained on ComParE  [24]  features and one trained 2 https://github.com/glam-imperial/CoughLIME 3 https://github.com/marcotcr/lime on embeddings extracted from the last hidden states of a pre-trained Wav2Vec 2.0 (wav2vec2) model  [25] , referred to as ComParE-SVC and wav2vec2-SVC, respectively. Both models are trained on features using Leave-One-Speaker-Out (LOSO) cross-validation on the subsection of the datasets containing the emotions: happiness, anger, sadness, and neutral. Hence, six separate models are trained; one for each combination of the two features and three datasets. The models correctly classified the utterances included in the analysis, hence, the positive class is the correct emotion while the negative class consists of any other emotion. This reasoning aligns well with the One-vs-Rest classification strategy that splits a multi-class classification into one binary classification problem per class.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Iv. Experimental Resources",
      "text": "EmoLIME explanations were generated for ten randomly selected utterances per emotion balanced across speakers in the datasets, as visualized in Fig.  4 . The random seed is kept constant to ensure the input data is perturbed similarly when comparing the models. We used the following datasets in this work: 1) EMODB (Berlin Database of Emotional Speech)  [26]  contains acted emotional speech in German. Ten speakers (five male and five female) participated in the study each producing ten utterances that were a mix of short and longer sentences. In total, the database contains 535 recordings. 2) RAVDESS (Ryerson Audio-Visual Database of Emotional Speech and Song)  [27]  is an audio-visual database containing enacted emotional speech and song from 24 professional actors (12 female and 12 male). The corpus contains 7356 recordings in English with a neutral North American accent. 3) IEMOCAP (The Interactive Emotional Dyadic Motion Capture)  [28]  database  is an acted, multimodal database in English. Ten actors (five male and five female) perform improvisations or scripted scenarios, specifically selected to elicit emotional expressions. The database includes 1277 recorded utterances.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "V. Results And Discussion",
      "text": "The spectral decomposition segment the audio into eight equally sized spectral components in the frequency range between 0 to 8 kHz. Only true predictions are included in the visualizations, hence positive weights correspond to components that yield the model towards predicting the true class. Intuitively, low-pitch speech can be associated with low valence emotions, such as anger and sadness. In contrast, high pitch is usually associated with high valence emotions, such as happiness. For EMODB, this was indeed the observation for the model trained on deep features, but not for the model built on hand-crafted features as exemplified in Figs.  2  and 3 .\n\nWe quantify the average spectral decomposition weights across a selection EMODB, RAVDESS, and IEMOCAP of utterances in Fig.  4 . Although, the fundamental frequency of the human voice lies in the range of 90 to 155 Hz for men and between 165 to 255 Hz for women, research has shown that high-frequency components up to and above 7 kHz play a role in human hearing and perception  [29] . Very high-pitch components do not contribute significantly and are assigned close-to-zero weights by the EmoLIME algorithm.\n\nSome key takeaways from Figure  4  are: (i) Deep features: Low-pitch components (<3 kHz) contribute most to predicting angry and sad emotions. (ii) Deep features: High-pitch components (<3 kHz) tend to account for more in the prediction of high-arousal emotions (happy, angry) compared to low-arousal emotions (sad, neutral). (iii) Hand-crafted features: Spectral weights for very high-pitch components (<4 kHz) are closer to zero when compared to the deep features model, except for sad emotions. (iv) General trend: Indications that the EmoLIME technique is more robust across models than across datasets for the same emotion.\n\nTo statistically test observation (iv) above, we perform a non-parametric Cramer-Test  [30]  for the multivariate twosample problem with the null hypothesis: the two samples come from the same underlying distribution at α = 0.05 significance level. The spectral weight distributions consist of 10 samples and 8 dimensions per emotion, and results are listed in Table  I . The null hypothesis is accepted in 8/12 (67%) possible tests for the same dataset but different models. In comparison, the null hypothesis is accepted in 9/24 (38%) possible tests for the same model but different datasets. This further reinforces our observation that EmoLIME is less robust to distribution shifts.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Vi. Conclusion",
      "text": "Expressing and interpreting emotions is a highly subjective process, and investigating XAI methods for SER forces us to reflect on how humans perceive emotions through speech. It remains a substantial challenge to evaluate XAI techniques on more complex speech tasks owing to the involvement of multiple components within the model such as general language models, the challenge of reliably mapping from speech input to objective ground truths, and the variability due to speakers, language, culture, etc., which is unique to speech signals. We propose EmoLIME, a LIME based XAI method for SER models, and demonstrate that the method can produce explanations that are well-aligned with human intuition. Using EmoLIME, an exploration of average spectral decomposition weights for models based on hand-crafted and deep features was undertaken. The emotional representations learned by the pre-trained model align well with the intuitive connection between pitch and high vs. low valence emotions. To further the development of XAI techniques for SER towards a more comprehensive understanding of model predictions, one could consider incorporating global explanations through gradient-based techniques or SHAP, in addition to the local explanations in EmoLIME.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Functional block diagram of EmoLIME inspired by [7] and [8].",
      "page": 1
    },
    {
      "caption": "Figure 2: Example explanations for the happy expression of a German sentence",
      "page": 2
    },
    {
      "caption": "Figure 3: Explanations for the angry expression of a German sentence from",
      "page": 2
    },
    {
      "caption": "Figure 4: Comparison of spectral decomposition weights for the models based on ComParE (top) vs. deep features (bottom). The weights are computed as the",
      "page": 3
    },
    {
      "caption": "Figure 1: depicts a",
      "page": 3
    },
    {
      "caption": "Figure 4: The random seed is kept",
      "page": 3
    },
    {
      "caption": "Figure 4: Although, the fundamental frequency of",
      "page": 4
    },
    {
      "caption": "Figure 4: are: (i) Deep features:",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Two-sample Cramer test\nComPare: Dataset 1 vs. Dataset 2": "Datasets /\nEmotion"
        },
        {
          "Two-sample Cramer test\nComPare: Dataset 1 vs. Dataset 2": "EDB vs. RV /\nA\nH\nS\nN"
        },
        {
          "Two-sample Cramer test\nComPare: Dataset 1 vs. Dataset 2": "EDB vs. IE /\nA\nH\nS\nN"
        },
        {
          "Two-sample Cramer test\nComPare: Dataset 1 vs. Dataset 2": "RV vs. IE /\nA\nH\nS\nN"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Two-sample Cramer test\nwav2vec2: Dataset 1 vs. Dataset 2": "Datasets /\nEmotion"
        },
        {
          "Two-sample Cramer test\nwav2vec2: Dataset 1 vs. Dataset 2": "EDB vs. RV /\nA\nH\nS\nN"
        },
        {
          "Two-sample Cramer test\nwav2vec2: Dataset 1 vs. Dataset 2": "EDB vs. IE /\nA\nH\nS\nN"
        },
        {
          "Two-sample Cramer test\nwav2vec2: Dataset 1 vs. Dataset 2": "RV vs. IE /\nA\nH\nS\nN"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Two-sample Cramer test\nComPare vs. wav2vec2": "Dataset /\nEmotion"
        },
        {
          "Two-sample Cramer test\nComPare vs. wav2vec2": "EDB /\nA\nH\nS\nN"
        },
        {
          "Two-sample Cramer test\nComPare vs. wav2vec2": "RV /\nA\nH\nS\nN"
        },
        {
          "Two-sample Cramer test\nComPare vs. wav2vec2": "IE /\nA\nH\nS\nN"
        }
      ],
      "page": 4
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "wav2vec 2.0: A framework for self-supervised learning of speech representations",
      "authors": [
        "Alexei Baevski",
        "Henry Zhou",
        "Abdelrahman Mohamed",
        "Michael Auli"
      ],
      "year": "2020",
      "venue": "wav2vec 2.0: A framework for self-supervised learning of speech representations"
    },
    {
      "citation_id": "2",
      "title": "Hubert: Selfsupervised speech representation learning by masked prediction of hidden units",
      "authors": [
        "Wei Hsu",
        "Benjamin Bolte",
        "Hung Yao",
        "Kushal Tsai",
        "Ruslan Lakhotia",
        "Abdelrahman Salakhutdinov",
        "Mohamed"
      ],
      "year": "2021",
      "venue": "Ieee/acm Transactions on Audio Speech and Language Processing"
    },
    {
      "citation_id": "3",
      "title": "Wavlm: Large-scale self-supervised pretraining for full stack speech processing",
      "authors": [
        "Sanyuan Chen",
        "Chengyi Wang",
        "Zhengyang Chen",
        "Yu Wu",
        "Shujie Liu",
        "Zhuo Chen",
        "Jinyu Li",
        "Naoyuki Kanda",
        "Takuya Yoshioka",
        "Xiong Xiao",
        "Jian Wu",
        "Long Zhou",
        "Shuo Ren",
        "Yanmin Qian",
        "Jian Yao Qian",
        "Michael Wu",
        "Furu Zeng",
        "Wei"
      ],
      "year": "2021",
      "venue": "Wavlm: Large-scale self-supervised pretraining for full stack speech processing"
    },
    {
      "citation_id": "4",
      "title": "Emotion recognition from speech using wav2vec 2.0 embeddings",
      "authors": [
        "Leonardo Pepino",
        "Pablo Riera",
        "Luciana Ferrer"
      ],
      "year": "2021",
      "venue": "Emotion recognition from speech using wav2vec 2.0 embeddings"
    },
    {
      "citation_id": "5",
      "title": "Dawn of the transformer era in speech emotion recognition: Closing the valence gap",
      "authors": [
        "Johannes Wagner",
        "Andreas Triantafyllopoulos",
        "Hagen Wierstorf",
        "Maximilian Schmitt",
        "Felix Burkhardt",
        "Florian Eyben",
        "Björn Schuller"
      ],
      "year": "2023",
      "venue": "Ieee Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "6",
      "title": "A multilingual framework based on pre-training model for speech emotion recognition",
      "authors": [
        "Zhaohang Zhang",
        "Xiaohui Zhang",
        "Min Guo",
        "Wei Zhang",
        "Ke Li",
        "Yukai Huang"
      ],
      "year": "2021",
      "venue": "2021 Asia-pacific Signal and Information Processing Association Annual Summit and Conference"
    },
    {
      "citation_id": "7",
      "title": "Local interpretable model-agnostic explanations for music content analysis",
      "authors": [
        "Saumitra Mishra",
        "Bob Sturm",
        "Simon Dixon"
      ],
      "year": "2017",
      "venue": "Proceedings of the 18th International Society for Music Information Retrieval Conference"
    },
    {
      "citation_id": "8",
      "title": "audiolime: Listenable explanations using source separation",
      "authors": [
        "Verena Haunschmid",
        "Ethan Manilow",
        "Gerhard Widmer"
      ],
      "year": "2020",
      "venue": "audiolime: Listenable explanations using source separation"
    },
    {
      "citation_id": "9",
      "title": "Regulation (EU) 2024/1689 of the European Parliament and of the Council on harmonised rules on Artificial Intelligence (AI Act)",
      "year": "2024",
      "venue": "Regulation (EU) 2024/1689 of the European Parliament and of the Council on harmonised rules on Artificial Intelligence (AI Act)"
    },
    {
      "citation_id": "10",
      "title": "Recommendation of the Council on Artificial Intelligence",
      "authors": [
        "Oecd"
      ],
      "year": "2024",
      "venue": "Recommendation of the Council on Artificial Intelligence"
    },
    {
      "citation_id": "11",
      "title": "",
      "authors": [
        "Marco Tulio Ribeiro",
        "Sameer Singh",
        "Carlos Guestrin"
      ],
      "year": "2016",
      "venue": ""
    },
    {
      "citation_id": "12",
      "title": "A unified approach to interpreting model predictions",
      "authors": [
        "Lee Scott",
        "Su-In"
      ],
      "year": "2017",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "13",
      "title": "Can we trust explainable ai methods on asr? an evaluation on phoneme recognition",
      "authors": [
        "Xiaoliang Wu",
        "Peter Bell",
        "Ajitha Rajan"
      ],
      "year": "2024",
      "venue": "Icassp, Ieee International Conference on Acoustics, Speech and Signal Processing -Proceedings"
    },
    {
      "citation_id": "14",
      "title": "Unveiling hidden factors: explainable ai for feature boosting in speech emotion recognition",
      "authors": [
        "Alaa Nfissi",
        "Wassim Bouachir",
        "Nizar Bouguila",
        "Brian Mishara"
      ],
      "year": "2024",
      "venue": "Applied Intelligence"
    },
    {
      "citation_id": "15",
      "title": "Enhancing speech emotion recognition in urdu using bi-gru networks: An in-depth analysis of acoustic features and model interpretability",
      "authors": [
        "Muhammad Adeel",
        "Zhi Yong"
      ],
      "year": "2024",
      "venue": "Proceedings of the Ieee International Conference on Industrial Technology"
    },
    {
      "citation_id": "16",
      "title": "Zero-shot crosslingual speech emotion recognition: A study of loss functions and feature importance",
      "authors": [
        "Sneha Das",
        "Nicole Lonfeldt",
        "Nicklas Leander Lund",
        "Anne Pagsberg",
        "Line Katrine Harder Clemmensen"
      ],
      "year": "2022",
      "venue": "2nd Symposium on Security and Privacy in Speech Communication"
    },
    {
      "citation_id": "17",
      "title": "Explainable artificial intelligence: a systematic review",
      "authors": [
        "Giulia Vilone",
        "Luca Longo"
      ],
      "year": "2020",
      "venue": "Explainable artificial intelligence: a systematic review"
    },
    {
      "citation_id": "18",
      "title": "Audio explainable artificial intelligence: A review",
      "authors": [
        "Alican Akman",
        "Björn Schuller"
      ],
      "year": "2024",
      "venue": "Intelligent Computing"
    },
    {
      "citation_id": "19",
      "title": "Axiomatic attribution for deep networks",
      "authors": [
        "Mukund Sundararajan",
        "Ankur Taly",
        "Qiqi Yan"
      ],
      "year": "2017",
      "venue": "Axiomatic attribution for deep networks"
    },
    {
      "citation_id": "20",
      "title": "Audiomnist: Exploring explainable artificial intelligence for audio analysis on a simple benchmark",
      "authors": [
        "Sören Becker",
        "Johanna Vielhaben",
        "Marcel Ackermann",
        "Klaus-Robert Müller",
        "Sebastian Lapuschkin",
        "Wojciech Samek"
      ],
      "year": "2023",
      "venue": "Audiomnist: Exploring explainable artificial intelligence for audio analysis on a simple benchmark"
    },
    {
      "citation_id": "21",
      "title": "Xai-based comparison of input representations for audio event classification",
      "authors": [
        "Annika Frommholz",
        "Fabian Seipel",
        "Sebastian Lapuschkin",
        "Wojciech Samek",
        "Johanna Vielhaben"
      ],
      "year": "2023",
      "venue": "Xai-based comparison of input representations for audio event classification"
    },
    {
      "citation_id": "22",
      "title": "Coughlime: Sonified explanations for the predictions of covid-19 cough classifiers",
      "authors": [
        "Anne Wullenweber",
        "Alican Akman",
        "Björn Schuller"
      ],
      "year": "2022",
      "venue": "2022 44th Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC)"
    },
    {
      "citation_id": "23",
      "title": "Lime -a flexible, non-lte line excitation and radiation transfer method for millimeter and far-infrared wavelengths",
      "authors": [
        "C Brinch",
        "M Hogerheijde"
      ],
      "year": "2010",
      "venue": "Astronomy & Astrophysics"
    },
    {
      "citation_id": "24",
      "title": "The INTERSPEECH 2016 Computational Paralinguistics Challenge: Deception, Sincerity & Native Language",
      "authors": [
        "Björn Schuller",
        "Stefan Steidl",
        "Anton Batliner",
        "Julia Hirschberg",
        "Judee Burgoon",
        "Alice Baird",
        "Aaron Elkins",
        "Yue Zhang",
        "Eduardo Coutinho",
        "Keelan Evanini"
      ],
      "year": "2016",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "25",
      "title": "Model for dimensional speech emotion recognition based on wav2vec 2.0",
      "authors": [
        "J Wagner"
      ],
      "year": "2022",
      "venue": "Model for dimensional speech emotion recognition based on wav2vec 2.0"
    },
    {
      "citation_id": "26",
      "title": "A database of german emotional speech",
      "authors": [
        "Felix Burkhardt",
        "Astrid Paeschke",
        "M Rolfes",
        "Walter Sendlmeier",
        "Benjamin Weiss"
      ],
      "year": "2005",
      "venue": "A database of german emotional speech"
    },
    {
      "citation_id": "27",
      "title": "The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS)",
      "authors": [
        "Steven Livingstone",
        "Frank Russo"
      ],
      "year": "2018",
      "venue": "The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS)"
    },
    {
      "citation_id": "28",
      "title": "Iemocap: interactive emotional dyadic motion capture database",
      "authors": [
        "Carlos Busso",
        "Murtaza Bulut",
        "Chi-Chun Lee",
        "Abe Kazemzadeh",
        "Emily Mower",
        "Samuel Kim",
        "Jeannette Chang",
        "Sungbok Lee",
        "Shrikanth Narayanan"
      ],
      "year": "2008",
      "venue": "Language Resources and Evaluation"
    },
    {
      "citation_id": "29",
      "title": "Introduction to the special issue on perception and production of sounds in the highfrequency range of human speecha)",
      "authors": [
        "Ewa Jacewicz",
        "Joshua Alexander",
        "Robert Fox"
      ],
      "year": "2023",
      "venue": "Journal of the Acoustical Society of America"
    },
    {
      "citation_id": "30",
      "title": "On a new multivariate two-sample test",
      "authors": [
        "L Baringhaus",
        "C Franz"
      ],
      "year": "2004",
      "venue": "Journal of Multivariate Analysis"
    }
  ]
}