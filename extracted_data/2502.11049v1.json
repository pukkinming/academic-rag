{
  "paper_id": "2502.11049v1",
  "title": "Faces Of Fairness: Examining Bias In Facial Expression Recognition Datasets And Models",
  "published": "2025-02-16T09:23:16Z",
  "authors": [
    "Mohammad Mehdi Hosseini",
    "Ali Pourramezan Fard",
    "Mohammad H. Mahoor"
  ],
  "keywords": [
    "Bias",
    "Fairness",
    "Facial Expression Recognition",
    "Facial Affect Analysis",
    "Bias in FER Datasets",
    "Bias in FER Algorithms"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Building AI systems, including Facial Expression Recognition (FER) , involves two critical aspects: data and model design. Both components significantly influence bias and fairness in FER tasks. Issues related to bias and fairness in FER datasets and models remain underexplored. This study investigates bias sources in FER datasets and models. Four common FER datasets-AffectNet, ExpW, Fer2013, and RAF-DB-are analyzed. The findings demonstrate that AffectNet and ExpW exhibit high generalizability despite data imbalances. Additionally, this research evaluates the bias and fairness of six deep models, including three state-of-the-art convolutional neural network (CNN) models: MobileNet, ResNet, XceptionNet, as well as three transformer-based models: ViT, CLIP, and GPT-4o-mini. Experimental results reveal that while GPT-4o-mini and ViT achieve the highest accuracy scores, they also display the highest levels of bias. These findings underscore the urgent need for developing new methodologies to mitigate bias and ensure fairness in datasets and models, particularly in affective computing applications. See our implementation details at https://github.com/MMHosseini/bias in FER.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Equity In Fer",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Bias Fairness",
      "text": "Evenness Is the data uniformly distributed?",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Richness",
      "text": "Are there enough samples?",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Dominance",
      "text": "Does any group have an advantage? Demographic Parity evaluated using three metrics: Richness  [38] , Evenness  [39] , and Dominance  [40] . Additionally, we investigate inherent biases in the datasets through experiments, as discussed in Sec. 3.1.\n\nIn this study, we evaluate six deep models, commonly used for FER tasks. To this aim, we train three CNN models, i.e., Mo-bileNetV1  [41] , ResNet-50  [42] , XceptionNet  [43] , and the visiontransformer-based ViT  [32]  model, using all the aforementioned datasets. From the two large language-vision models, CLIP  [33]  is fine-tuned using the datasets, while GPT-4o-mini  [34]  is evaluated without training or fine-tuning. We refer to MobileNetV1 as MobileNet and ResNet-50 as ResNet in the paper. To assess the bias of these models, we leverage four key bias metrics: Equalized Odds  [44] , Equal Opportunity  [44] , Demographic Parity  [45] ,  [46] , and Treatment Equality  [47] . These metrics capture aspects of bias and fairness, allowing for a comprehensive evaluation of the six models across multiple attributes. This methodology facilitates a fair and thorough comparison of their respective bias scores.\n\nThe experiments conducted in this study show that AffectNet is the most diverse dataset, while RAF-DB ranks the lowest in diversity. Regarding model performance, despite the high accuracy of the GPT-4o-mini and ViT models, they exhibited the highest bias scores when evaluated using the bias metrics. In contrast, ResNet demonstrated the lowest bias score, making it the fairest model among those tested in this study. Our implementation code and more detail about our experiments is reachable at GitHub.\n\nIn the remainder of this paper, Sec. 2 provides a review of the literature on bias and fairness in both datasets and algorithms. In continue, Sec. 3 outlines the metrics used for evaluating bias, while Sec. 4 discusses bias scores for the datasets and models. Finally, Sec. 5 concludes this bias study in FER.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Literature Review",
      "text": "FER has many applications, such as sensitive biometrics  [48] , where biased data or algorithms can cause irreparable damages. Understanding the sources of bias is essential for effectively addressing its challenges. Bias can originate from both datasets and models. Factors such as data collection and annotation processes, model architecture, objective functions, and evaluation methodologies are among the primary sources of bias. Figure  2  presents a general classification of bias sources. Addressing bias in deep machine learning methods can significantly enhance the reliability and accuracy of FER models. More importantly, developing fair and unbiased deep methods for FER methods potentially ensures equitable treatment across diverse demographic groups  [49] . Thus, dealing with bias in FER has recently raised attention from researchers. Examining data distribution in existing FER datasets  [22] ,  [30] ,  [50] -  [53] , as well as analyzing the algorithms used in the FER models  [21] ,  [27] ,  [28] ,  [54] -  [56]  are the key approaches for studying bias in FER. In the following section, we review recent studies that address bias in FER models and datasets.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Bias In Fer Datasets",
      "text": "Bias in datasets is broadly categorized into representational bias and stereotypical bias  [50] -  [52] . Representational bias focuses mainly on the demographic diversity of data, including age, gender, and race attributes. This bias is evaluated using metrics like Richness, Evenness, and Dominance (for definitions see Sec. 3.1). On the other hand, stereotypical bias investigates the correlation between specific subgroups and their corresponding labels, which can lead to unfair or biased outcomes.\n\nAnother categorization of bias in dataset divided them into intrinsic and extrinsic biases  [57] -  [59] . Intrinsic bias inherent to the process of data collection, i.e., the way data are collected, sampled, and labeled. Demographic attributes, such as age, gender, and race, as well as head-pose, gesture, eye-gaze, and expression intensity are studied as intrinsic biases. On the contrary, extrinsic Fig.  2 . Bias can originate from datasets and models. In datasets, prominent sources of bias include issues in data collection, such as demographic disparities, variations in illumination and lighting conditions, gestures, head poses, and cultural differences in emotional interpretation. In models, key bias sources include architecture design, training parameters, overfitting to specific demographic groups, and the selection of evaluation metrics.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Positional Variations Environmental Conditions",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Demographic Disparities Expression Diversity Annotation Orientation",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Models",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Bias Sources",
      "text": "bias refers to the disparities that could affect the model's performance. For example, lab-controlled datasets can lead to high biases in models, because they are not collected based on the realworld scenarios. Extrinsic bias mainly stems from environmental conditions, such as camera resolution, background noise, illumination, and annotator bias. There are other definitions of bias in dataset. While Cheong et al.  [22]  grouped bias sources as observed and non-observed sources, Terhorst et al.  [60]  classified bias sources into demographic and non-demographic groups. Despite all these different bias categorizations, a strong consensus is that demographic attributes, including age, gender, and race, are the most effective sources of bias in dataset that need to be studied.\n\nIn the following we review the most notable works on this topic. Dominguez et al.  [50]  proposed seventeen representational metrics, where they ranked AffectNet  [35] , ExpW  [5] , RAF-DB  [36] , and Fer2013  [37]  among the most diverse FER datasets. It is notable that their stereotypical metrics showed different results for this four datasets where they did not achieve good scores and rankings. Another research by Li et al.  [30]  studied the intrinsic bias among seven different FER datasets and reported that the main reasons for bias in datasets are cultural differences and data collection conditions. Huber et al.  [53]  studied bias in synthetic facial datasets where they revealed that synthetic datasets follow the same bias patterns as the authentic datasets used for training their corresponding models. Finally, Cheong et al.  [22]  used a directed acyclic graph (DAG) to explore the relation between sources of bias. They introduced gender and age as the strongest sources of bias in emotion labeling. These experiments highlight the importance of dataset analysis in FER tasks, especially demographic attributes and their correlation with the labels for each subgroup. In Sec. 3.1 we review some common metrics for data analysis, and in Sec. 4.1 demonstrate their results.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Bias In Fer Algorithms",
      "text": "The analysis of bias in FER algorithms involves examining how deep models and algorithms impact various demographic groups, aiming to enhance fairness and improve emotion recognition accuracy. Below, we review recent studies addressing algorithmic bias. While some studies tried to reduce the bias of the model's using data compensation, some experiments revealed that there are some inherent biases in the models irrelevant to the data.\n\nTo mitigate the effects of imbalanced data distribution, Yu et al.  [54]  utilized unlabeled data in a semi-supervised learning framework. This framework leveraged unlabeled facial data by assigning pseudo-labels to underrepresented samples in the datasets. As a result, their method achieved a more balanced data distribution and improved the overall accuracy of the FER system. In a related study, Kolahdouzi et al.  [55]  introduced a Hilbert-space kernel to compute the mean distribution of specific demographic groups. To reduce bias, they subtracted a facial image's embedding from the extracted mean, obscuring sensitive attributes such as age, gender, and race. Furthermore, Liang et al.  [27]  investigated the influence of facial attributes on model performance. They categorized these attributes into protected attributes (e.g., gender and race) and unprotected attributes (e.g., pose and lighting). Their findings demonstrated that protected attributes affect model performance, contributing to bias in predictions.\n\nIn contrast, some researchers argue that biases originate from the models. Dooley et al.  [21]  hypothesized that bias stems from the architecture of deep models. They employed a joint CNN architecture with one component emphasizing fairness and another focusing on extracting hyperparameters to mitigate bias. Their results indicated that bias remains inherently tied to model architecture. Abdullah et al.  [56]  conducted experiments focusing on the attention areas allocated to faces by the models. They observed that the attention area assigned to dark-skinned faces was smaller compared to lighter skin tones, revealing an inherent bias in the models. Xu et al.  [28]  compared a baseline model, ResNet-18  [42] , with two disentangled and attribute-aware models. Their experiments demonstrated that data augmentation did not significantly enhance the accuracy of the baseline model. Similarly, Amigo et al.  [49]  proposed a debiasing variational autoencoder to address bias in models. Their findings showed that, despite training on different datasets, models could perpetuate biases. Furthermore, Dominguez et al.  [61]  revealed that stereotypical biases are intrinsic to models, regardless of whether the training data is balanced or imbalanced. These findings underscore the importance of studying bias and fairness in both data and algorithms.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Perspectives",
      "text": "Bias and fairness in facial expression recognition are critical to preventing discrimination, ensuring equitable systems, improving performance across diverse populations, and upholding ethical standards. Real-world FER applications rely on in-the-wild datasets, making it imperative to thoroughly examine commonly used datasets to identify and address bias issues. While some studies have partially investigated these datasets, a more comprehensive exploration of popular FER datasets is needed.\n\nAt the same time, recent advancements in model architectures, such as deep convolutional neural networks, vision transformers, and large vision-language models, have significantly improved FER accuracy  [62] -  [64] . However, despite their broad adoption, these models must be rigorously evaluated for fairness and bias mitigation. This study provides an in-depth analysis of bias in four prominent FER datasets-AffectNet, ExpW, Fer2013, and RAF-DB-and assesses six state-of-the-art models, including Mo-bileNet, ResNet, XceptionNet, ViT, CLIP, and GPT-4o-mini.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Methodology",
      "text": "Bias and fairness are entangled and multifaceted concepts, with various definitions proposed in the literature (see  [65] ). Fairness is often described as the absence of favoritism or unjust privilege toward any group  [9] , while bias refers to prejudice or preferential treatment toward certain cases  [50] . In essence, bias pertains to the data, whereas fairness focuses on the algorithms that process it. Numerous metrics have been introduced to measure both bias and fairness in datasets and algorithms (see  [50] ,  [66] ). In the following sections, we delve into the selected metrics for evaluating datasets and algorithms, aiming to provide a comprehensive understanding of their impact on facial expression recognition.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Data Analysis",
      "text": "In Sec. 2.1, we explored various classifications of bias, including intrinsic vs. extrinsic, observed vs. non-observed, demographic vs. non-demographic, and representational vs. stereotypical categories. Bias can manifest in many ways, such as through camera inconsistencies, background variations, body posture differences, and, most notably, demographic imbalances. Among these, demographic attributes-age, gender, and race-emerge as the most significant contributors to bias in FER datasets. As a result, these attributes, along with label distribution, warrant the most attention in this study to address bias and promote fairness.\n\nBuilding on the taxonomy proposed by Dominguez et al.  [50] , this study analyzes bias in dataset using two major categories of metrics. Representational metrics examine the Richness, Evenness, and Dominance of the dataset, while stereotypical metrics reveal the local and global relationships between attributes.\n\nRichness evaluates the total number of samples within a dataset. More samples lead to a higher Richness score. However, this metric solely considers the quantity of samples, without addressing their distribution across various attributes  [38] .\n\nWe consider R i = N i as the basic Richness equation where R i is Richness of dataset i and N i stands for the number of samples correspond to dataset i. To have a better understanding and scoring, we normalized the Richness score of each dataset, in the range [0, 1], through Eq. 1, where max(N ) is maximum Richness score of all the datasets.\n\nEvenness measures the equity of data distribution across different classes of attributes. It is commonly assessed using the Shannon's Evenness Index  [67] , which quantifies how uniformly the samples are distributed in the datasets.\n\nEq. 2 shows the Shannon's Evenness Index, where 0 ≤ E ≤ 1 represents the Evenness, C refers to the total number of classes, n i denotes the number of samples in class i, and N is total number of samples. E = 1 indicates a perfectly even distribution, whereas E < 1 reflects an uneven data distribution. Dominance measures the extent to which one group holds an advantage over other groups within a dataset. Since this definition gives more score to non-uniform data distribution, we subtracted it from 1, to convert it to the same range and the concept that we are using for the Richness and Evenness metrics. The proposed metric for assessing Dominance is the Simpson's Dominance Index  [40] ,\n\nwhich provides a quantitative evaluation of this phenomenon. In this equation, 0 ≤ D ≤ 1 represents Dominance, C denotes the total number of classes, p i is the proportion of samples in each class i, n i and N indicates the number of samples in a class, and the total number of samples, respectively. In this metric, a value of D = 0 signifies that one class dominates entirely, while D > 0 reflects a more evenly distribution across classes.\n\nGlobal-Stereotypical and Local-Stereotypical metrics examine the correlation between an attribute and a class label. Global-Stereotypical metrics focus on the overall relationship between an attribute and data distribution, such as the connection between age groups and expression labels. Local-Stereotypical metrics delve into more specific relationships, analyzing the connection between a particular attribute value and a class label. For example, the relationship between the attribute value 'age-group = young' and the Happy expression.\n\nTo investigate the local and global stereotypical metrics across different datasets, we extracted the data distribution for each dataset based on expression labels, as well as demographic attributes, age, gender, and race. The mathematical definition of the data distribution is provided in Eq. 4 as follows:\n\nwhere A represents an attribute selected from expression, age, gender, or race, a is any possible value for A, f a is the frequency of a, and N is the total number of samples. In this study, utilizing DeepFace  [68]  definitions, age is classified into four ranges:\n\n, and [Over 54], gender is categorized as Woman or Man, and ultimately, six racial groups of White, Black, Asian, Indian, Latinx, and Middle Eastern, are considered.\n\nIn addition, we assessed the conditional data distribution of each dataset. Given that each expression class is associated with three attributes-age, gender, and race-we explored the data distribution under both single and joint conditions. This was done using the general distribution equation P (A | B) = P (A∩B) P (B) , where the denominator remains constant across all conditions, allowing us to simplify it to P (A | B) = P (A ∩ B). Equations 5, 6, and 7 present the mathematical formulation of this analysis.\n\nIn Eqs. 5, 6, 7, Y shows the expression, while A, B, C correspond to the facial attributes age, gender, and race, with y, a, b, c representing their respective values.\n\nTo evaluate the intrinsic biases of each dataset, we designed an experiment where a model is trained to recognize the datasets. In this experiment, each sample was labeled with its corresponding dataset name, for both the training and validation sets. The model was then trained to predict the dataset of each image. High accuracy in this experiment indicates higher intrinsic biases, such as those related to illumination, background noise, camera resolution, gesture, eye gaze, head pose, hair color, and any other hidden bias in the datasets. In other words, a stronger correlation between any pair of datasets reflects a lower intrinsic bias in those datasets.\n\nWe conducted an additional experiment to evaluate intrinsic biases by implementing a leave-one-dataset-out approach. In this experiment, we trained a deep model for FER using all except one dataset, which was held out for test. By assessing the model's performance on the excluded dataset, we aimed to identify potential biases within the dataset itself while also measuring the model's ability to generalize across different data sources. In this experiment, a lower accuracy suggests bias in the datasets. To ensure a fair comparison, we trained the model on the training sets of all datasets except one, validated its accuracy using their respective validation sets, and tested the bias using the validation set of the excluded (left-out) dataset. If the results of the validation and test experiments show similar accuracies, the model is considered unbiased. If there is a discrepancy, it indicates that the dataset suffers from bias.\n\nTo sum up, we extracted statistical data from each dataset and designed two model-based experiments-the dataset recognition model and the leave-one-dataset-out FER model-to assess both intrinsic and extrinsic biases within the datasets.",
      "page_start": 3,
      "page_end": 5
    },
    {
      "section_name": "Model Analysis",
      "text": "A variety of general metrics have been proposed to evaluate fairness in machine learning models. Verma et al.  [65]  introduced three categories of fairness parameters: statistical measures, similarity-based measures, and causal reasoning. Additionally, Mittal et al.  [66]  presented metrics, such as parity-based metrics, score-based metrics, and facial-analysis-specific metrics.\n\nRecent studies have explored various bias and fairness metrics  [9] ,  [20] ,  [69] . While some researchers introduced their own metrics, most studies have concentrated on widely recognized metrics such as Equalized Odds  [44] , Equal Opportunity  [44] , Demographic Parity  [45] ,  [46] , Treatment Equality  [47] . In this study, we focus on these four metrics and evaluate the bias and fairness of the models. Before delving deeper into these metrics, it is essential to explain the concepts of protected and unprotected groups. Protected groups are privileged against discrimination, while unprotected groups may face exclusion or bias.\n\nEqualized Odds evaluates fairness by examining the positive predicted samples. Specifically, for both protected and unprotected groups, the model should exhibit similar ratios of True Positives (TP) and False Positives (FP). In other words, the likelihood of correctly identifying a positive case or making a False Positive prediction should not depend on the group membership.\n\nwhere y ∈ {0, 1}. Equation 8 states that for an attribute A, a fair model will produce equal ratios of True Positives (TPs) and False Positives (FPs) for both the protected group (A = 0) and unprotected group (A = 1). For example, consider gender as the attribute, where A = f emale. A fair model should yield identical TP and FP ratios for females (A = 0) and males (A = 1). This ensures the model's predictions are not disproportionately influenced by gender, satisfying Equalized Odds.\n\nEqual Opportunity focuses exclusively on the True Positive samples. Specifically, for a protected and an unprotected group, the probability of correctly predicting a positive outcome (Y = 1) should be the same. The key difference between this metric and Equalized Odds is that, while Equalized Odds consider both True Positives (Y = 1) and False Positives (Y = 0), Equal Opportunity is solely concerned with the True Positive (Y = 1).\n\nEq. 9 evaluates the equity in the ratio of TP between the protected (A = 0) and unprotected (A = 1) groups. Demographic Parity, also referred to as statistical parity, examines the independence of predictions from protected attributes. In essence, it evaluates the distribution of positive predictions, irrespective of whether they are TP or FP. This measure expects a uniform distribution of predictions across different groups. Eq. 10 provides a formal definition of Demographic Parity.\n\nFor instance, when considering the attribute race, the number of samples predicted as Ŷ = 1 should be balanced across all racial groups, such as Black, White, and Asian.\n\nTreatment Equality examines the distribution of errors across different demographic groups. Specifically, it ensures that the ratio of False Negatives (FNs) to False Positives (FPs) remains consistent between protected and unprotected groups (see Eq. 11).\n\n) . (11)   For example, the ratio of FN to FP should be consistent across all age groups, such as children, young adults, mature adults, and older individuals. Interestingly, some studies choose to evaluate this relationship by calculating the ratio of FPs to FNs instead.\n\nTo obtain an evaluation score for fairness (bias score) in each model, we applied the four bias metrics to each expression label. Next, we selected the maximum bias across all expressions as the bias score for that metric (see Eq.12). To assess the bias of each attribute, we averaged the bias scores across the different metrics using Eq.13. Finally, the overall bias of each model was calculated by averaging the bias scores of the attributes (Eq. 14).\n\nBias att (met) = M ax(Bias met,att (exp)) ∀exp ∈ EXP , (  12 )\n\nBias att (met) ∀met ∈ M ET , (  13 )\n\nIn Eq. 12, EXP represents all seven basic expressions (Neutral, Happy, Sad, Surprise, Fear, Disgust, and Anger)  [70] . In Eq. 13, K denotes the number of bias metrics, while M ET refers to the individual metrics (Equalized Odds, Equal Opportunity, Demographic Parity, and Treatment Equality). In Eq. 14, N represents the number of attributes, and AT T refers to all attributes.\n\nTo sum up, we employed four well-established bias metrics to evaluate key attributes in facial expression analysis, namely age, gender, and race. Our experiments were conducted on six popular models to assess their fairness. The detailed discussion of the models' bias will follow in Section 4.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Experimental Results",
      "text": "This section explores bias across four well-known FER datasets: AffectNet  [35] , Fer2013  [37] , RAF-DB  [36] , and ExpW  [5] . Additionally, it examines the fairness in six distinct generations of deep models, including three CNN models: MobileNet  [41] , ResNet  [42] , XceptionNet  [43] , as well as three transformer-based models: ViT  [32] , CLIP  [33] , and GPT-4o-mini  [34] .",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Bias In Datasets",
      "text": "Many studies suggest that the primary sources of bias in machine learning models lie in the data itself  [9] -  [11] . Consequently, to investigate bias in FER, it is essential to first examine the datasets. While numerous facial attributes can contribute to bias (see  [60] ), and a variety of biases can be present within the datasets (see  [9] ,  [22] ), our focus is on the three most prominent facial attributes. In the following sections, we provide an overview of the datasets used in our study and detail the preprocessing steps we applied. Additionally, we explore the data distribution, attribute correlations, and model-based analysis.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Datasets",
      "text": "Although numerous FER datasets exist, only a few feature inthe-wild facial images. By considering factors such as popularity, number of expression labels, and annotation methods, we selected four datasets for our study: AffectNet  [35] , Fer2013  [37] , RAF-DB  [36] , and ExpW  [5] . All these datasets include at least six Ekman  [70]  basic expression labels, including Happy, Sad, Surprise, Fear, Disgust, Anger, plus Neutral, manually annotated by annotators.\n\nAffectNet is the largest in-the-wild FER dataset, comprising nearly 1 million images, with half of them manually annotated. It includes eight expressions (seven basic expressions plus Contempt), continuous labels (valence-arousal), and additional metadata. Fer2013 contains approximately 36K images, divided into 29K training samples and 3.5K validation samples. However, its small image size (40×40 pixels) and grayscale color representation pose limitations. RAF-DB provides both single-label and compound-label subsets, with around 30K facial images and facial landmark annotations. The compound-labeling approach in RAF-DB results in 19 expressions, including the seven basic expressions and 12 compound expressions such as Happily-Surprise and Sadly-Angry. Finally, ExpW is a large-scale FER dataset, featuring over 90K manually annotated facial images. We utilize these datasets in our study to train and evaluate our models.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Data Preprocessing",
      "text": "We used four in-the-wild datasets, each with different image sizes. In the first step, we aligned the images by ensuring the same cropping area, image size, and color domain across all the datasets. To achieve a consistent cropping area, we extended the original crop region by 25 to 35 percent in each dimension for all images. We then used the DeepFace model  [71] ,  [72]  to process the extended crop and align the facial images across all datasets. Next, we resized all cropped faces to 224×224. While three datasets are in the RGB color domain, Fer2013 is grayscale. To address this, we replicated the grayscale layer three times to match the RGB format, ensuring uniformity across the data. Additionally, since the ExpW dataset lacks a validation set, we randomly selected 10 percent of its images to serve as the validation set.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Data Diversity And Distribution",
      "text": "After data processing and achieving a unified format, we used the DeepFace model  [68]  to extract facial attributes, including age, gender, and race. Ages were classified into four ranges: [0∼15], [16∼32], [33∼53], and [Over 54]. DeepFace predicts six racial categories: White, Black, Asian, Indian, Latinx, and Middle Eastern. For gender classification, the model outputs either Woman or Man. Throughout the remainder of this paper, we will use these terms to refer to the attribute values.\n\nIn our initial experiment, we assessed the diversity scores of the datasets using the metrics described in Sec. 3.1, namely Richness, Evenness, and Dominance. By applying Eqs. 1, 2, and 3, we derived the diversity scores summarized in Table  1 . This table presents the diversity scores for each dataset, where the average score is a weighted combination of the attributes. Notably, Richness was assigned a weight of 0.25 because it is a dataset-level metric rather than an attribute-specific one, resulting in the same score across all attributes. The overall diversity score was then calculated as the mean of these weighted averages. Our findings indicate that AffectNet is the most diverse dataset, whereas the other datasets scored below 50 on average out of 100.\n\nIn a subsequent experiment, we applied global-and-localstereotypical metrics (Eq. 4) to evaluate data bias. Fig.  3 -a depicts the expression distribution of datasets, highlighting that Neutral and Happy are the most prevalent, whereas Fear and Disgust are As shown in Fig.  3-c , the number of Man samples is approximately 50% more than Woman, though this gender bias is less pronounced compared to other attributes. Fig.  3-d , on the other hand, highlights a notable racial imbalance, with White samples constituting over 50% of the total across all datasets. The distribution of Asian, Black, Latinx, and Middle Eastern samples is more balanced, while Indian samples remain consistently underrepresented. In conclusion, an examination of the data distribution across four essential attributes-age, gender, race, and expression-uncovers notable imbalances that may introduce bias into the model. Addressing these disparities is vital to ensuring fairness in FER systems.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Attributes Correlation",
      "text": "Building upon the global-and-local-stereotypical analysis, we applied Eq. 5 to explore bias across the data samples. Additionally, examining the correlation between different attributes offers another meaningful approach for data analysis. For this purpose, we focused on expressions as the primary attribute and analyzed their correlation with other facial attributes of age, gender, and race. Figure  4  illustrates that 27.7% of the data is labeled Neutral, where 18.1% belong to the [16∼32] age group, and 9.3% fall within [33∼53]. In contrast, less than 0.3% of Neutral label corresponds to the two remaining age groups. Regarding gender distribution, Neutral reveals a significant disparity between Man and Woman samples, with the number of Man being more than double that of Woman. For racial distribution within this expression, 15.5% of the samples are associated with White, while the total representation of the other five racial groups is below 13%.\n\nThe distribution of Happy exhibits a similar bias toward the age groups [16∼32] and [33∼53]. However, no significant gender disparity is observed between Man and Woman. In contrast, bias exists toward White group for this expression. Compared to Happy and Neutral, the bias across attributes is less pronounced for the other expressions, particularly across age groups and racial categories. Among the datasets, ExpW demonstrates the highest bias, while Fer2013 emerges as the least biased.\n\nThe joint attribute correlation, calculated using Eqs. 6 and 7, offers deeper insights into bias in dataset. Fig.  5      5 . The 4D correlation matrix between different attributes of all the datasets is presented, with expressions represented in the rows and the columns divided first by age groups, followed by gender, and finally by race groups. This heat map highlights an uneven data distribution, where a great portion of data are underrepresented. This diagram reveals limited and imbalance diversity in the datasets.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Dataset Generality",
      "text": "Based on the data distribution and correlation analysis, we examined demographic biases within the datasets. While demographic disparity is a primary source of bias, other factors such as illumination, head pose, image quality, background settings, lighting effects, and facial accessories also contribute to bias in dataset  [58] ,  [59] ,  [73] . These sources of bias vary significantly across different datasets.\n\nTo investigate general bias in FER datasets, we conducted an experiment where a model was trained to identify the dataset to which any given image belongs. Each image was labeled with its respective dataset name, and the XceptionNet model  [43]  was trained to predict the dataset label. The results, as shown in Fig.  6 , reveal biases inherent to FER datasets. While some correlation exists between RAF-DB and two other datsets (ExpW and Affect-Net), Fer2013, ExpW, and AffectNet remain uncorrelated. This experiment underscores that differences in data collection methodologies-including image acquisition, processing, cropping, and storage-significantly affect FER datasets. Such biases, stemming from dataset-specific protocols, remain a relatively underexplored drawback in FER research.\n\nAn additional experiment was conducted to further investigate the generality and biases inherent in the datasets. In this experiment, one dataset was excluded, and a facial expression recognition model (XceptionNet) was trained using the remaining datasets. To ensure fairness, the model was first evaluated on the unseen validation sets of the datasets included in training. Subsequently, the generalizability of the excluded dataset was assessed Fig.  6 . The confusion matrix for FER datasets (in percent) reveals that the high diagonal values for Fer2013, ExpW, and AffectNet point to inherent biases within these datasets. Notably, RAF-DB stands out as the only dataset that shows a significant correlation with the others.\n\nby testing the model on its images. Since the excluded dataset was not part of the training process, the results reflect the dataset's bias. Table  2  compares the validation accuracies achieved on the included datasets with the test accuracy for the excluded dataset. Both Validation and Test accuracies in the table represent average per-class accuracies. Interestingly, the Test accuracies for RAF-DB and Fer2013 were higher than their Validation accuracies, while the opposite trend was observed for the ExpW and AffectNet datasets. For instance, when AffectNet was excluded, the Validation accuracy across the validation sets of RAF-DB, Fer2013, and ExpW was 57.0%. However, the Test accuracy on the excluded AffectNet dataset dropped to 45.9%. This result suggests that the challenges presented in the AffectNet dataset are not sufficiently captured by RAF-DB, Fer2013, and ExpW. A similar pattern was observed when the ExpW dataset was excluded. Conversely, for RAF-DB and Fer2013, the Validation accuracies were lower than their respective Test accuracies, demonstrating an inverse trend. Based on these findings, we conclude that the generalizability of the AffectNet and ExpW datasets is higher than that of RAF-DB and Fer2013. This indicates that RAF-DB and Fer2013 exhibit more inherent biases compared to AffectNet and ExpW.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Fairness Of Models",
      "text": "After analyzing bias in the FER data, we further investigate bias and fairness within the models. As mentioned, four bias metrics are used to evaluate six deep models with different architectures. In the following sections, we describe our training methodology and then discuss the accuracy and bias of each model.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Training And Implementation Details",
      "text": "In this experiment, we selected six deep learning architectures, including both CNN-and transformer-based models: MobileNet, ResNet, XceptionNet, ViT, CLIP, and GPT-4o-mini. MobileNet Fig.  7 . Model accuracy for each expression (in percent). In this experiment, models were trained on individual datasets and evaluated on their respective evaluation sets. µ and σ denote the average accuracy and its standard deviation for each dataset, while MEAN and STD represent the overall average accuracy and standard deviation across each model. RAF-DB emerged as the least challenging dataset, whereas ExpW posed the greatest difficulty. Among the models, GPT-4o-mini delivered the highest accuracy, contrasting with the CLIP model, which recorded the lowest performance. was used as a baseline CNN model due to its streamlined feedforward structure without residual connections, making it computationally efficient for mobile and embedded applications. ResNet and XceptionNet, incorporating residual connections, were chosen for their ability to mitigate the vanishing gradient problem and improve model accuracy. Among the transformer-based models, ViT represented a state-of-the-art approach for computer vision tasks, leveraging self-attention mechanisms instead of traditional convolutional operations. CLIP, a large vision-language model designed for multi-modal learning, enabled joint processing of images and text. Lastly, GPT-4o-mini, a lightweight variant of GPT-4, was included for its capability to efficiently handle both NLP and vision tasks.\n\nThe experiment was conducted using Python 3.8.10, Tensor-Flow 2.9.2  [74] , and the Hugging Face  [75]  libraries. Four Nvidia 1080 GPUs with 8 GB and 12 GB of memory were utilized. As a preprocessing step, image dimensions were fixed at 224 × 224. We conducted experiments on each dataset and model separately and also merged the training and validation sets of all datasets for subsequent experiments. Notably, after merging the datasets, we obtained 404,755 training samples and 22,858 validation samples.\n\nIn the first experiment, we fully trained four models, including MobileNet, ResNet, XceptionNet, and ViT, using each dataset and then evaluated their accuracy over their corresponding validation set. In the next experiment, we concatenated training sets, trained all the models, and evaluated them using the concatenated validation set. All four models were trained for 30 epochs with a fixed batch size of 35. The Adam optimizer was used with a learning rate of 10 -3 for MobileNet, ResNet, and XceptionNet, and 10 -5 for the ViT model. The two other Adam parameters were set to beta-1 = 0.9 and beta-2 = 0.999 for all models. Notably, we selected the categorical cross-entropy loss function for our training process.\n\nAside from the ViT model, we also leveraged two other transformer-based models, CLIP and GPT-4o-mini. These two models are large vision-language models capable of handling multiple tasks. Since CLIP is trained on multi-modal datasets and is open-source, we fine-tuned its decision-making head using each training set of different datasets, and their combined version. Then we evaluated its accuracy and fairness on each dataset and the concatenated version. We used the Adam optimizer with a learning rate of 10 -3 , beta-1 of 0.9, and beta-2 of 0.999. Additionally, we selected the categorical cross-entropy loss function for fine-tuning this model over 30 epochs with a batch size of 35.\n\nGPT-4o-mini is another transformer-based vision-language model included in this research. However, since it is not opensource, we did not train or fine-tune it for this specific task. To evaluate this model and compare its accuracy and fairness with the other models, we only evaluated it using the validation set of each dataset and their combined version. We prompted all the facial images of the validation set into this model and asked it to assign one of the seven expressions used in this study. The prompt we used was: \"What is the expression of this person among happy, sad, surprise, fear, disgust, anger, and neutral, in only one word? Try to find one of these expressions, but if your prediction is out of these expressions, select the closest expression to it (among the 7 aforementioned expressions). If you cannot determine any expression, randomly select one of the mentioned expressions. If the predicted expression is another word in the family of the aforementioned expressions (happy, sad, surprise, fear, disgust, anger, neutral) change it to the corresponding word. For example, fearful is equal to fear, disgusted is equal to disgust, angry is equal to anger, surprised is equal to surprise, scared is equal to fear, and so on. If you randomly select an expression, do not need to explain, only generate the expression name. We can leverage this data for comparing their accuracy, and more importantly, models' bias over each group of data, on over different demographic attributes, age, gender, and race.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Facial Expression Recognition",
      "text": "Figure  7  shows the accuracy of each model across the datasets, categorized by expression. The results highlight a noticeable bias toward Neutral and Happy expressions, where they achieved significantly higher accuracy scores compared to the other expressions, in all the models. When comparing the expression distribution in Figure  4  with the accuracy results in Figure  7 , it becomes evident that data imbalance contributes to this bias. Nonetheless, the models displayed an ability to partially compensate for this imbalance. For example, although the ratio of Disgust to Happy data samples was highly skewed ( 2. 2  41.8 = 0.05), the accuracy gap between these two expressions was considerably narrower ( 29.1  85.9 = 0.33), suggesting that the models made efforts to mitigate this bias during training.\n\nFig.  8 . Accuracy of each model across all datasets (in percent). In this experiment, all datasets were used for both training and validation. Notably, no model was able to achieve high scores for the two challenging expressions, Fear and Disgust. A comparison of the results reveals that GPT-4o-mini and ViT models demonstrated the most consistent accuracy scores, while CLIP struggled to achieve satisfactory performance.",
      "page_start": 9,
      "page_end": 10
    },
    {
      "section_name": "Table 3",
      "text": "General accuracy of the models (in percent). Four models-MobileNet, ResNet, XceptionNet, and ViT-were trained on the full training samples of all datasets and evaluated using their respective validation sets. The CLIP model was fine-tuned on the training sets and validated using the validation set, while GPT-4o-mini was evaluated solely on the validation sets, without any training or fine-tuning. decision-making head, could potentially improve its performance. Lastly, among the fully trained models (MobileNet, ResNet, Xcep-tionNet, and ViT), ViT achieved the highest accuracy score, while ResNet exhibited the lowest standard deviation. Based solely on high accuracy and low standard deviation metrics, the models can be ranked from least to most accurate models as follows: GPT-4o-mini, ViT, ResNet, MobileNet, XceptionNet, and CLIP. We will discuss this ranking in the next sections, when we extract the metric-based bias scores.\n\nAnother experiment for FER models involved training each model using the training samples from all datasets and evaluating their performance on all validation sets. Fig.  8  displays the confusion matrix for these general models across all validation sets. At first glance, it is clear that Fear and Disgust are the most challenging expressions for all models, with none achieving more than 50% accuracy in predicting them.\n\nA closer analysis reveals significant confusion among certain expressions. For instance, pairs like Sad-Neutral, Disgust-Neutral, and Disgust-Anger pose considerable challenges, with erroneous predictions in the range of  [18.8-35.3 ] percent, while zero values would be expected. While Fig.  7  showed GPT-4o-mini as the top performer across various datasets, the general model results, summarized in Table  3 , indicate that the ViT model achieved the highest average accuracy in this experiment. Additionally, MobileNet had the lowest standard deviation, indicating more consistency in its predictions. For further details on the experiments, please visit our GitHub.\n\nSo far, we have assessed the models' accuracy to evaluate their robustness against bias stemming from imbalanced data distribution. Our findings indicate that, overall, GPT-4o-mini and ViT are the most accurate models, while CLIP performed poorly. Although these results suggest that GPT-4o-mini and ViT might be the least biased models, subsequent experiments reveal different outcomes. In the next step, we will apply various bias metrics to further analyze the models' biases across different attributes.",
      "page_start": 10,
      "page_end": 11
    },
    {
      "section_name": "Bias Analysis",
      "text": "To investigate bias, we combined the training sets of all four datasets into a unified training set, applying the same approach to the validation sets. Four models-MobileNet, ResNet, Xcep-tionNet, and ViT-were trained on the unified training set, and their bias scores were evaluated using the unified validation set.\n\nAdditionally, the CLIP model was fine-tuned on the training set, and its bias scores were calculated using the validation set. For the GPT-4o-mini model, bias scores were directly quantified using the validation set without any additional training or fine-tuning. This section first explores bias across different attributes and expressions for each model, followed by a comparison of the models' overall bias. It is notable that all the four bias metrics used Eq. 12 for their calculations.   4  and 5 , it becomes evident that Equal Opportunity demonstrates more bias than Equalized Odds. For instance, the bias range for Age in Equalized Odds was [1.7-2.3], while Equal Opportunity showed a higher bias range of  [3-4.6] . Demographic Parity analyzes the likelihood of any group receiving a particular outcome. The data in Table  6  shows that, compared to other bias metrics, Demographic Parity results in lower bias scores. For the Age attribute, the highest bias was observed in the Neutral expression, while Happy, Sad, and Surprise had minimal effects on the Demographic Parity of Age. In contrast, Happy was the most biased expression for the Gender attribute, while Fear and Sad showed the least bias. For the Race attribute, Neutral and Happy were the primary sources of bias, while Fear exhibited the lowest bias across all models. Although the Max column highlights higher biases for the Gender attribute, the Mean column underscores Race as the most biased attribute overall. As with the other bias metrics, Age remained the least biased attribute. The Mean and STD columns illustrate that for the Age attribute, the CLIP model was the most biased model, while for Gender and Race, the ViT model was the most biased model. Overall, considering the Mean bias scores, the ViT model emerged as the most biased, while ResNet demonstrated the least bias.\n\nThe final bias measure we examined is Treatment Equality, which assesses the distribution of misclassified samples across different groups. Table  7  demonstrates that Treatment Equality resulted in higher bias scores. A detailed expression-level analysis shows that Sad, Fear, and Disgust were the main sources of bias for the Age attribute. While Happy and Anger exhibited the highest biases for the Gender attribute, Happy significantly influenced the bias score for Race in three out of the six models. Among the attributes, Race had the highest bias scores, while Age exhibited the lowest. This trend was consistent with our findings in the Equalized Odds and Equal Opportunity metrics. Comparing the Max column in Table  7  with the Max columns in Tables  4, 5 , and 6 shows that Treatment Equality exhibited higher bias scores across all attributes. In a model-wise comparison of Treatment Equality, CLIP showed the lowest bias score, while the highest bias was observed in GPT-4o-mini.\n\nTo sum up, we analyzed the bias of the models for each attribute and expression, followed by a model-based bias comparison. Based on our experiments, the ViT model showed the highest bias in the Equalized Odds and Demographic Parity metrics, while GPT-4o-mini exhibited significant bias in the Equal Opportunity and Treatment Equality metrics. On the other hand, ResNet was the most robust model against bias in three out of four bias metrics.\n\nTo generate a unique mathematical bias score, we utilized Eqs. 13 and 14 to calculate the final bias score for each model. Table  8  presents the bias scores of each model across different attributes and bias metrics. As outlined in Eq. 12, we used the maximum bias score of each model (i.e., the Max column from Tables  4, 5, 6,  and 7 ) as the bias score for each model. In the next step, we calculated the average bias score for each model across all attributes using Eq. 13 (the average of each row in Table  8 ). Finally, the overall bias score for each model was obtained by averaging the bias scores for each attribute by Eq. 14. In conclusion, the results showed that GPT-4o-mini was the most biased model, followed by ViT in second place. Additionally, this table highlights that residual-based models, including ResNet and XceptionNet, were the least biased models. Ultimately, our experiments rank the models from the most biased to the least biased as follows: GPT-4o-mini, ViT, CLIP, MobileNet, XceptionNet, and ResNet. Notably, this research focused solely on the FER task, and the observed bias in the models could be evaluated on different tasks, potentially yielding varying bias scores.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Discussion And Future Works",
      "text": "This research presented a comprehensive analysis of bias and fairness in FER. We examined four in-the-wild datasets (Affect-Net, ExpW, Fer2013, and RAF-DB) and six popular deep models (MobileNet, ResNet, XceptionNet, ViT, CLIP, and GPT-4o-mini). We mainly focused on studying bias in demographic attributes, including age, gender, and race, as well as label distribution.\n\nBased on the experiments conducted in this research, bias was examined in both datasets and models. Initially, bias within the datasets was analyzed using the Richness, Evenness, and Dominance metrics. The results indicated that ExpW is the most biased dataset, whereas AffectNet was identified as the least biased. Additionally, six state-of-the-art models were evaluated using four bias metrics: Equalized Odds, Equal Opportunity, Demographic Parity, and Treatment Equality. The findings revealed that, despite achieving high accuracy, the GPT-4o-mini and ViT models exhibited the highest levels of bias.\n\nThe contributions of this research are pivotal for advancing fairness in FER tasks. Below, we outline several open research directions in FER that can build on this study to address bias and promote fair decision-making:\n\n• Investigating non-demographic sources of bias, such as illumination, background noise, head pose, gestures, eye gaze, and hair color, within datasets and their impact on fairness of models.\n\n• Reporting the bias score of trained models for FER tasks alongside their accuracy, which enables a comprehensive evaluation, ensuring both the effectiveness and fairness of the models.",
      "page_start": 12,
      "page_end": 13
    },
    {
      "section_name": "•",
      "text": "Comparing the biases present in lab-controlled datasets versus in-the-wild datasets, and analyzing their influence on model training and fairness outcomes.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "•",
      "text": "Exploring face generator models as potential sources of bias in future datasets and developing fair data generation methods to address these issues.\n\n• Developing more robust FER models by incorporating fairness constraints into the loss function during training. Future work could examine methods for implementing such constraints and their effects on model accuracy.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "•",
      "text": "Analyzing the evolution of bias throughout the different stages of model development, including pretraining, training, fine-tuning, and domain adaptation.\n\n• Exploring the role of self-supervised learning methods in reducing bias within datasets and evaluating how they can contribute to fair model development.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "•",
      "text": "Evaluating video-based datasets and models, where temporal dynamics between sequences are critical, to mitigate bias in video-based FER tasks.\n\n• Investigating bias in multi-modal models that leverage data from diverse sources such as images, text, video, audio, and physiological signals.\n\nThis research highlighted the significance of considering bias and fairness in FER tasks. Establishing a foundation for enhancing the equity of FER models is essential. These improvements have the potential to benefit critical applications, including human-computer interaction (HCI), mental health monitoring, and surveillance. Future studies will focus on proposing methods to address bias in FER tasks while maintaining high accuracy.",
      "page_start": 13,
      "page_end": 13
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: shows the approach we utilize different metrics to evaluate",
      "page": 1
    },
    {
      "caption": "Figure 1: Our approach focuses on examining equity, including bias and fairness, in FER datasets and models. We leveraged various metrics to assess",
      "page": 2
    },
    {
      "caption": "Figure 2: Bias can originate from datasets and models. In datasets,",
      "page": 2
    },
    {
      "caption": "Figure 3: The data distribution across different datasets shows several trends: a) Happy and Neutral dominate the datasets, while Fear and Disgust",
      "page": 6
    },
    {
      "caption": "Figure 3: -a depicts",
      "page": 6
    },
    {
      "caption": "Figure 4: The 2D correlation matrix illustrates the relationships between different attributes across all the datasets (in percent). The diagram visualizes",
      "page": 7
    },
    {
      "caption": "Figure 3: -c, the number of Man samples is ap-",
      "page": 7
    },
    {
      "caption": "Figure 3: -d, on the",
      "page": 7
    },
    {
      "caption": "Figure 4: illustrates that 27.7% of the data is labeled Neutral,",
      "page": 7
    },
    {
      "caption": "Figure 5: reveals that the",
      "page": 7
    },
    {
      "caption": "Figure 5: The 4D correlation matrix between different attributes of all the datasets is presented, with expressions represented in the rows and the",
      "page": 8
    },
    {
      "caption": "Figure 6: The confusion matrix for FER datasets (in percent) reveals that",
      "page": 8
    },
    {
      "caption": "Figure 7: Model accuracy for each expression (in percent). In this experiment, models were trained on individual datasets and evaluated on their",
      "page": 9
    },
    {
      "caption": "Figure 7: shows the accuracy of each model across the datasets,",
      "page": 9
    },
    {
      "caption": "Figure 4: with the accuracy results in Figure 7, it becomes",
      "page": 9
    },
    {
      "caption": "Figure 8: Accuracy of each model across all datasets (in percent). In this experiment, all datasets were used for both training and validation. Notably, no",
      "page": 10
    },
    {
      "caption": "Figure 7: , demonstrates a correlation between dataset Richness",
      "page": 10
    },
    {
      "caption": "Figure 8: displays the",
      "page": 10
    },
    {
      "caption": "Figure 7: showed GPT-4o-mini as the",
      "page": 10
    }
  ],
  "tables": [
    {
      "caption": "Table 1: sions and 12 compound expressions such as Happily-Surprise",
      "data": [
        {
          "RAF-DB": "Exp\nAge\nGen\nRace",
          "Fer2013": "Exp\nAge\nGen\nRace",
          "ExpW": "Exp\nAge\nGen\nRace",
          "AffectNet": "Exp\nAge\nGen\nRace"
        },
        {
          "RAF-DB": "5.3\n5.3\n5.3\n5.3\n84.2\n52.9\n96.4\n69.7\n76.1\n47.8\n47.5\n60.2\n53.9\n34.0\n48.4\n43.7",
          "Fer2013": "12.4\n12.4\n12.4\n12.4\n93.0\n54.5\n95.7\n58.0\n82.6\n51.0\n47.0\n49.9\n59.6\n36.2\n48.6\n37.0",
          "ExpW": "30.9\n30.9\n30.9\n30.9\n77.1\n54.4\n87.0\n76.6\n72.3\n50.2\n41.2\n65.3\n52.4\n37.4\n45.3\n49.9",
          "AffectNet": "100\n100\n100\n100\n74.0\n52.8\n97.3\n70.8\n69.1\n48.0\n48.2\n60.5\n56.0\n41.9\n56.8\n52.1"
        },
        {
          "RAF-DB": "45",
          "Fer2013": "45.4",
          "ExpW": "46.3",
          "AffectNet": "51.7"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table 4: highlights that the Neutral and Happy expressions were",
      "data": [
        {
          "Neu\nHap\nSad\nSur": "5.0\n0.9\n0.9\n0.3\n1.0\n3.1\n1.1\n8.4\n15.6\n0.7\n2.3\n0.1\n7.4\n2.9\n9.1\n8.0\n6.9\n5.4\n1.9\n8.1\n2.9",
          "Fea Dis Ang Max Mean\nSTD": "5.0\n1.7\n1.5\n15.6\n5.3\n5.1\n9.1\n6.0\n2.5"
        },
        {
          "Neu\nHap\nSad\nSur": "6.5\n0.8\n0.8\n0.1\n1.7\n1.0\n2.2\n7.3\n13.9\n1.0\n2.3\n0.1\n3.1\n4.5\n7.2\n7.6\n7.0\n4.6\n0.8\n4.4\n3.4",
          "Fea Dis Ang Max Mean\nSTD": "6.5\n1.8\n1.9\n13.9\n4.6\n4.3\n7.6\n5.0\n2.2"
        },
        {
          "Neu\nHap\nSad\nSur": "5.1\n0.3\n1.3\n0.4\n2.0\n1.3\n1.2\n8.6\n14.9\n2.6\n2.4\n0.5\n3.4\n3.2\n10.0\n9.5\n6.8\n5.4\n1.6\n4.8\n2.6",
          "Fea Dis Ang Max Mean\nSTD": "5.1\n1.6\n1.5\n14.9\n5.0\n4.6\n10.0\n5.8\n2.9"
        },
        {
          "Neu\nHap\nSad\nSur": "4.6\n0.1\n0.4\n0.5\n0.9\n1.6\n2.8\n11.3\n16.1\n0.1\n2.8\n0.3\n3.1\n5.0\n13.5\n9.9\n9.0\n6.4\n1.2\n3.7\n5.3",
          "Fea Dis Ang Max Mean\nSTD": "4.6\n1.5\n1.5\n16.1\n5.5\n5.5\n13.5\n7.0\n3.8"
        },
        {
          "Neu\nHap\nSad\nSur": "6.5\n1.8\n0.2\n1.1\n2.7\n0.1\n2.9\n10.6\n13.9\n0.0\n2.5\n0.9\n0.1\n6.8\n11.9\n5.9\n7.6\n4.2\n4.6\n1.2\n4.5",
          "Fea Dis Ang Max Mean\nSTD": "6.5\n2.1\n2.0\n13.9\n4.9\n5.1\n11.9\n5.7\n3.0"
        },
        {
          "Neu\nHap\nSad\nSur": "3.6\n0.6\n1.6\n0.6\n1.3\n0.4\n4.4\n8.4\n12.8\n0.1\n3.3\n1.2\n0.6\n9.5\n5.5\n9.5\n5.5\n6.8\n2.3\n2.9\n9.8",
          "Fea Dis Ang Max Mean\nSTD": "4.4\n1.7\n1.4\n12.8\n5.1\n4.6\n9.8\n6.0\n2.7"
        }
      ],
      "page": 11
    },
    {
      "caption": "Table 4: highlights that the Neutral and Happy expressions were",
      "data": [
        {
          "Neu\nHap\nSad\nSur": "4.9\n2.1\n1.1\n0.3\n1.0\n2.0\n1.1\n7.7\n12.7\n2.2\n2.3\n0.1\n5.9\n2.8\n11.6\n9.0\n7.1\n5.9\n2.2\n7.7\n4.7",
          "Fea Dis Ang Max Mean\nSTD": "4.9\n1.7\n1.3\n12.7\n4.8\n3.9\n11.6\n6.8\n2.8"
        },
        {
          "Neu\nHap\nSad\nSur": "6.7\n1.8\n0.7\n0.4\n1.7\n0.6\n2.1\n6.9\n11.6\n0.8\n2.3\n0.5\n2.8\n4.4\n10.1\n6.3\n7.1\n4.7\n1.4\n5.0\n3.4",
          "Fea Dis Ang Max Mean\nSTD": "6.7\n1.9\n2.0\n11.6\n4.1\n3.6\n10.1\n5.4\n2.5"
        },
        {
          "Neu\nHap\nSad\nSur": "5.1\n2.0\n0.9\n0.5\n1.9\n1.0\n1.1\n7.2\n12.2\n2.1\n2.5\n0.9\n3.2\n2.9\n10.2\n7.9\n6.3\n5.7\n1.4\n5.2\n2.8",
          "Fea Dis Ang Max Mean\nSTD": "5.1\n1.7\n1.4\n12.2\n4.4\n3.6\n10.2\n5.6\n2.7"
        },
        {
          "Neu\nHap\nSad\nSur": "4.7\n2.0\n0.5\n0.7\n0.8\n1.4\n2.8\n9.5\n13.4\n0.5\n2.9\n0.4\n2.8\n4.8\n13.1\n9.6\n9.1\n6.0\n1.5\n4.3\n5.2",
          "Fea Dis Ang Max Mean\nSTD": "4.7\n1.8\n1.3\n13.4\n4.8\n4.4\n13.1\n6.9\n3.5"
        },
        {
          "Neu\nHap\nSad\nSur": "7.3\n2.8\n0.1\n0.9\n2.7\n0.1\n2.6\n8.4\n11.0\n1.3\n2.2\n1.2\n0.1\n6.7\n12.5\n7.4\n7.4\n4.9\n4.7\n1.1\n5.6",
          "Fea Dis Ang Max Mean\nSTD": "7.3\n2.3\n2.3\n11.0\n4.4\n3.9\n12.5\n6.2\n3.2"
        },
        {
          "Neu\nHap\nSad\nSur": "3.3\n1.3\n2.2\n0.5\n1.2\n0.6\n4.1\n6.1\n9.4\n1.7\n3.1\n1.2\n1.4\n9.7\n10.3\n8.1\n5.4\n6.2\n2.2\n3.3\n9.7",
          "Fea Dis Ang Max Mean\nSTD": "4.1\n1.8\n1.2\n9.7\n4.6\n3.4\n10.3\n6.4\n2.8"
        }
      ],
      "page": 11
    },
    {
      "caption": "Table 4: highlights that the Neutral and Happy expressions were",
      "data": [
        {
          "Neu\nHap\nSad\nSur\nFea\nDis": "1.1\n1.3\n7.9\n6.6\n5.2\n3.5\n0.3\n1.9\n26.4\n12.4\n14.3\n1.6\n8.7\n6.5\n28.4\n46.8\n38.8\n19.9\n21.7\n11.0\n54.3",
          "Ang Max Mean\nSTD": "7.9\n3.7\n2.7\n26.4\n10.2\n7.9\n54.3\n31.5\n14.4"
        },
        {
          "Neu\nHap\nSad\nSur\nFea\nDis": "8.4\n3.3\n11.0\n11.3\n14.1\n13.6\n5.1\n11.4\n10.4\n6.5\n12.8\n9.3\n1.2\n16.0\n49.7\n28.0\n23.8\n14.2\n44.0\n32.8\n25.4",
          "Ang Max Mean\nSTD": "14.1\n9.5\n3.8\n16.0\n9.6\n4.3\n49.7\n31.1\n11.3"
        },
        {
          "Neu\nHap\nSad\nSur\nFea\nDis": "1.5\n11.4\n0.6\n1.7\n14.5\n17.3\n1.5\n3.0\n22.5\n10.3\n12.3\n1.5\n5.4\n0.2\n35.7\n48.2\n14.5\n24.9\n33.9\n29.8\n19.1",
          "Ang Max Mean\nSTD": "17.3\n6.9\n6.6\n22.5\n7.8\n7.2\n48.2\n29.4\n10.4"
        },
        {
          "Neu\nHap\nSad\nSur\nFea\nDis": "0.7\n3.9\n1.0\n1.6\n3.4\n24.2\n11.5\n7.7\n34.8\n2.8\n4.9\n1.0\n16.0\n16.2\n29.5\n61.7\n44.8\n22.8\n19.5\n31.4\n43.5",
          "Ang Max Mean\nSTD": "24.2\n6.6\n7.9\n34.8\n11.9\n10.8\n61.7\n36.1\n13.6"
        },
        {
          "Neu\nHap\nSad\nSur\nFea\nDis": "6.5\n8.3\n6.3\n2.0\n23.5\n0.0\n1.8\n1.0\n15.2\n4.9\n1.1\n1.7\n3.2\n33.6\n8.8\n23.2\n29.0\n29.1\n26.3\n4.4\n9.4",
          "Ang Max Mean\nSTD": "23.5\n6.9\n7.3\n33.6\n8.6\n11.1\n29.1\n18.5\n9.8"
        },
        {
          "Neu\nHap\nSad\nSur\nFea\nDis": "6.2\n6.3\n15.8\n1.9\n4.2\n14.1\n5.1\n5.3\n8.2\n9.0\n7.7\n1.6\n43.5\n45.3\n48.4\n61.0\n13.5\n46.6\n26.6\n27.0\n39.8",
          "Ang Max Mean\nSTD": "15.8\n7.6\n4.8\n45.3\n17.2\n17.3\n61.0\n37.5\n14.9"
        }
      ],
      "page": 11
    },
    {
      "caption": "Table 4: highlights that the Neutral and Happy expressions were",
      "data": [
        {
          "Neu Hap\nSad\nSur\nFea\nDis": "8.3\n2.0\n4.0\n5.2\n4.6\n0.4\n1.5\n1.8\n11.8\n4.4\n4.4\n1.1\n7.7\n4.6\n3.2\n16.5\n17.9\n20.5\n6.8\n14.4\n4.8",
          "Ang Max Mean\nSTD": "8.3\n3.7\n2.4\n11.8\n5.1\n3.3\n20.5\n12.0\n6.4"
        },
        {
          "Neu Hap\nSad\nSur\nFea\nDis": "9.5\n3.0\n1.8\n4.9\n3.8\n2.3\n0.7\n1.2\n9.5\n0.4\n4.5\n3.8\n2.2\n9.0\n5.6\n14.9\n21.3\n21.2\n8.8\n15.1\n2.9",
          "Ang Max Mean\nSTD": "9.5\n3.7\n2.6\n9.5\n4.3\n3.3\n21.3\n12.8\n6.7"
        },
        {
          "Neu Hap\nSad\nSur\nFea\nDis": "8.5\n4.3\n0.6\n6.8\n6.0\n1.3\n3.1\n1.2\n10.9\n0.8\n4.9\n2.6\n9.0\n8.5\n6.1\n15.7\n16.1\n19.2\n10.4\n11.0\n5.7",
          "Ang Max Mean\nSTD": "8.5\n4.3\n2.6\n10.9\n5.4\n3.7\n19.2\n12.0\n4.7"
        },
        {
          "Neu Hap\nSad\nSur\nFea\nDis": "5.8\n1.6\n4.3\n3.7\n4.4\n0.1\n1.4\n4.3\n11.9\n7.1\n8.3\n3.6\n5.5\n11.0\n4.6\n17.1\n24.9\n25.2\n11.1\n9.6\n10.4",
          "Ang Max Mean\nSTD": "5.8\n3.0\n1.8\n11.9\n7.3\n2.9\n25.2\n14.7\n7.3"
        },
        {
          "Neu Hap\nSad\nSur\nFea\nDis": "7.3\n1.7\n3.1\n9.4\n6.6\n4.4\n0.2\n2.8\n9.0\n2.8\n7.9\n7.4\n7.4\n15.4\n6.5\n12.0\n21.4\n15.4\n27.3\n7.6\n13.0",
          "Ang Max Mean\nSTD": "9.4\n4.6\n3.0\n15.4\n7.5\n3.9\n27.3\n14.7\n6.8"
        },
        {
          "Neu Hap\nSad\nSur\nFea\nDis": "4.5\n0.0\n6.5\n5.8\n0.6\n9.6\n2.7\n1.0\n6.9\n3.0\n10.7\n13.6\n14.9\n17.9\n4.9\n15.3\n11.2\n28.9\n14.9\n11.5\n10.5",
          "Ang Max Mean\nSTD": "9.6\n4.2\n3.1\n17.9\n9.7\n5.8\n28.9\n13.8\n6.9"
        }
      ],
      "page": 11
    },
    {
      "caption": "Table 6: shows that,",
      "data": [
        {
          "Eq-Od\nEq-Op\nDe-Pa\nTr-Eq": "4.9\n8.3\n5.0\n7.9\n12.7\n11.8\n15.6\n26.4\n11.6\n20.5\n9.1\n54.3",
          "Mean": "6.5\n16.6\n23.8",
          "Bias Score": "15.6"
        },
        {
          "Eq-Od\nEq-Op\nDe-Pa\nTr-Eq": "6.7\n9.5\n6.5\n14.1\n11.6\n9.5\n13.9\n16.0\n10.1\n21.3\n7.6\n49.7",
          "Mean": "9.2\n12.7\n22.1",
          "Bias Score": "14.6"
        },
        {
          "Eq-Od\nEq-Op\nDe-Pa\nTr-Eq": "5.1\n8.5\n5.1\n17.3\n12.2\n10.9\n14.9\n22.5\n10.2\n19.2\n10.0\n48.2",
          "Mean": "9.0\n15.1\n21.9",
          "Bias Score": "15.3"
        },
        {
          "Eq-Od\nEq-Op\nDe-Pa\nTr-Eq": "4.7\n5.8\n4.6\n24.2\n13.4\n11.9\n16.1\n34.8\n13.1\n25.2\n13.5\n61.7",
          "Mean": "9.8\n19.0\n28.3",
          "Bias Score": "19.0"
        },
        {
          "Eq-Od\nEq-Op\nDe-Pa\nTr-Eq": "7.3\n9.4\n6.5\n23.5\n11.0\n15.4\n13.9\n33.6\n12.5\n27.3\n11.9\n29.1",
          "Mean": "11.6\n18.4\n20.2",
          "Bias Score": "16.7"
        },
        {
          "Eq-Od\nEq-Op\nDe-Pa\nTr-Eq": "4.1\n9.6\n4.4\n15.8\n9.7\n17.9\n12.8\n45.3\n10.3\n28.9\n9.8\n61.0",
          "Mean": "8.4\n21.4\n27.5",
          "Bias Score": "19.1"
        }
      ],
      "page": 12
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Non-verbal effecting-animal research sheds light on human emotion communication",
      "authors": [
        "A Schirmer",
        "I Croy",
        "K Liebal",
        "S Schweinberger"
      ],
      "venue": "Biological Reviews"
    },
    {
      "citation_id": "2",
      "title": "Emotion, cognition, and behavior",
      "authors": [
        "R Dolan"
      ],
      "year": "2002",
      "venue": "Science"
    },
    {
      "citation_id": "3",
      "title": "Facial expression recognition in mental health monitoring: A review",
      "authors": [
        "F Jiang",
        "X Fu",
        "X Zeng"
      ],
      "year": "2020",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "4",
      "title": "Toward an affect-sensitive multimodal human-computer interaction",
      "authors": [
        "M Pantic",
        "L Rothkrantz"
      ],
      "year": "2003",
      "venue": "Proceedings of the IEEE"
    },
    {
      "citation_id": "5",
      "title": "From facial expression recognition to interpersonal relation prediction",
      "authors": [
        "Z Zhang",
        "P Luo",
        "C Loy",
        "X Tang"
      ],
      "year": "2006",
      "venue": "International Journal of Computer Vision"
    },
    {
      "citation_id": "6",
      "title": "A hybrid optimized learning framework for compound facial emotion recognition",
      "authors": [
        "R Borgalli",
        "S Surve"
      ],
      "year": "2025",
      "venue": "International Conference on Cognitive Computing and Cyber Physical Systems"
    },
    {
      "citation_id": "7",
      "title": "Enhanced facial emotion recognition using vision transformer models",
      "authors": [
        "N Fatima",
        "G Deepika",
        "A Anthonisamy",
        "R Chitra",
        "J Muralidharan",
        "M Alagarsamy",
        "K Ramyasree"
      ],
      "year": "2025",
      "venue": "Journal of Electrical Engineering & Technology"
    },
    {
      "citation_id": "8",
      "title": "Leveraging vision language models for facial expression recognition in driving environment",
      "authors": [
        "I Saadi",
        "A Hadid",
        "D Cunningham",
        "A Taleb-Ahmed",
        "Y Hillali"
      ],
      "venue": "Leveraging vision language models for facial expression recognition in driving environment"
    },
    {
      "citation_id": "9",
      "title": "A survey on bias and fairness in machine learning",
      "authors": [
        "N Mehrabi",
        "F Morstatter",
        "N Saxena",
        "K Lerman",
        "A Galstyan"
      ],
      "year": "2021",
      "venue": "ACM computing surveys (CSUR)"
    },
    {
      "citation_id": "10",
      "title": "Fairness and machine learning: Limitations and opportunities",
      "authors": [
        "S Barocas",
        "M Hardt",
        "A Narayanan"
      ],
      "year": "2023",
      "venue": "Fairness and machine learning: Limitations and opportunities"
    },
    {
      "citation_id": "11",
      "title": "A framework for understanding unintended consequences of machine learning",
      "authors": [
        "H Suresh",
        "J Guttag"
      ],
      "year": "2019",
      "venue": "A framework for understanding unintended consequences of machine learning",
      "arxiv": "arXiv:1901.10002"
    },
    {
      "citation_id": "12",
      "title": "Fairness and bias in artificial intelligence: A brief survey of sources, impacts, and mitigation strategies",
      "authors": [
        "E Ferrara"
      ],
      "venue": "Sci"
    },
    {
      "citation_id": "13",
      "title": "A low-cost stochastic computing-based fuzzy filtering for image noise reduction",
      "authors": [
        "S Estiri",
        "A Jalilvand",
        "S Naderi",
        "M Najafi",
        "M Fazeli"
      ],
      "year": "2022",
      "venue": "2022 IEEE 13th International Green and Sustainable Computing Conference (IGSC)"
    },
    {
      "citation_id": "14",
      "title": "Evolutionary cnn-based architectures with attention mechanisms for enhanced image classification",
      "authors": [
        "A Shams",
        "K Becker",
        "D Becker",
        "S Amirian",
        "K Rasheed"
      ],
      "year": "2024",
      "venue": "Artificial Intelligence: Machine Learning, Convolutional Neural Networks and Large Language Models"
    },
    {
      "citation_id": "15",
      "title": "Lr-net: a blockbased convolutional neural network for low-resolution image classification",
      "authors": [
        "A Ganj",
        "M Ebadpour",
        "M Darvish",
        "H Bahador"
      ],
      "year": "2023",
      "venue": "Iranian Journal of Science and Technology"
    },
    {
      "citation_id": "16",
      "title": "Linking convolutional kernel size to generalization bias in face analysis cnns",
      "authors": [
        "H Liang",
        "J Caro",
        "V Maheshri",
        "A Patel",
        "G Balakrishnan"
      ],
      "year": "2024",
      "venue": "Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision"
    },
    {
      "citation_id": "17",
      "title": "Local convolutions cause an implicit bias towards high frequency adversarial examples",
      "authors": [
        "J Caro",
        "Y Ju",
        "R Pyle",
        "S Dey",
        "W Brendel",
        "F Anselmi",
        "A Patel"
      ],
      "year": "2020",
      "venue": "Local convolutions cause an implicit bias towards high frequency adversarial examples",
      "arxiv": "arXiv:2006.11440"
    },
    {
      "citation_id": "18",
      "title": "Towards a holistic view of bias in machine learning: bridging algorithmic fairness and imbalanced learning",
      "authors": [
        "D Dablain",
        "B Krawczyk",
        "N Chawla"
      ],
      "venue": "Discover Data"
    },
    {
      "citation_id": "19",
      "title": "Large language models: A comprehensive survey on architectures, applications, and challenges",
      "authors": [
        "V Veeramachaneni"
      ],
      "year": "2025",
      "venue": "Advanced Innovations in Computer Programming Languages"
    },
    {
      "citation_id": "20",
      "title": "Bias and unfairness in machine learning models: a systematic review on datasets, tools, fairness metrics, and identification and mitigation methods",
      "authors": [
        "T Pagano",
        "R Loureiro",
        "F Lisboa",
        "R Peixoto",
        "G Guimarães",
        "G Cruz",
        "M Araujo",
        "L Santos",
        "M Cruz",
        "E Oliveira"
      ],
      "year": "2023",
      "venue": "Big data and cognitive computing"
    },
    {
      "citation_id": "21",
      "title": "Rethinking bias mitigation: Fairer architectures make for fairer face recognition",
      "authors": [
        "S Dooley",
        "R Sukthanker",
        "J Dickerson",
        "C White",
        "F Hutter",
        "M Goldblum"
      ],
      "year": "2024",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "22",
      "title": "Causal structure learning of bias for fair affect recognition",
      "authors": [
        "J Cheong",
        "S Kalkan",
        "H Gunes"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision"
    },
    {
      "citation_id": "23",
      "title": "Joint recognition of basic and compound facial expressions by mining latent soft labels",
      "authors": [
        "J Jiang",
        "M Wang",
        "B Xiao",
        "J Hu",
        "W Deng"
      ],
      "year": "2024",
      "venue": "Pattern Recognition"
    },
    {
      "citation_id": "24",
      "title": "Facial mark detection and removal using graph relations and statistics",
      "authors": [
        "M Hosseini",
        "M Jamzad"
      ],
      "year": "2017",
      "venue": "2017 Iranian Conference on Electrical Engineering (ICEE)"
    },
    {
      "citation_id": "25",
      "title": "Robust emotion recognition in context debiasing",
      "authors": [
        "D Yang",
        "K Yang",
        "M Li",
        "S Wang",
        "S Wang",
        "L Zhang"
      ],
      "year": "2024",
      "venue": "Robust emotion recognition in context debiasing",
      "arxiv": "arXiv:2403.05963"
    },
    {
      "citation_id": "26",
      "title": "Exploring facial expression recognition through semi-supervised pretraining and temporal modeling",
      "authors": [
        "J Yu",
        "Z Wei",
        "Z Cai"
      ],
      "year": "2024",
      "venue": "Exploring facial expression recognition through semi-supervised pretraining and temporal modeling",
      "arxiv": "arXiv:2403.11942"
    },
    {
      "citation_id": "27",
      "title": "Benchmarking algorithmic bias in face recognition: An experimental approach using synthetic faces and human evaluation",
      "authors": [
        "H Liang",
        "P Perona",
        "G Balakrishnan"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision"
    },
    {
      "citation_id": "28",
      "title": "Investigating bias and fairness in facial expression recognition",
      "authors": [
        "T Xu",
        "J White",
        "S Kalkan",
        "H Gunes"
      ],
      "year": "2020",
      "venue": "European Conference on Computer Vision"
    },
    {
      "citation_id": "29",
      "title": "Uncovering the bias in facial expressions",
      "authors": [
        "J Deuschel",
        "B Finzel",
        "I Rieger"
      ],
      "year": "2020",
      "venue": "Uncovering the bias in facial expressions",
      "arxiv": "arXiv:2011.11311"
    },
    {
      "citation_id": "30",
      "title": "A deeper look at facial expression dataset bias",
      "authors": [
        "S Li",
        "W Deng"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "31",
      "title": "Understanding and mitigating annotation bias in facial expression recognition",
      "authors": [
        "Y Chen",
        "J Joo"
      ],
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021"
    },
    {
      "citation_id": "32",
      "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
      "authors": [
        "A Dosovitskiy",
        "L Beyer",
        "A Kolesnikov",
        "D Weissenborn",
        "X Zhai",
        "T Unterthiner",
        "M Dehghani",
        "M Minderer",
        "G Heigold",
        "S Gelly"
      ],
      "year": "2005",
      "venue": "An image is worth 16x16 words: Transformers for image recognition at scale",
      "arxiv": "arXiv:2010.11929"
    },
    {
      "citation_id": "33",
      "title": "Learning transferable visual models from natural language supervision",
      "authors": [
        "A Radford",
        "J Kim",
        "C Hallacy",
        "A Ramesh",
        "G Goh",
        "S Agarwal",
        "G Sastry",
        "A Askell",
        "P Mishkin",
        "J Clark"
      ],
      "year": "2021",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "34",
      "title": "Gpt-4 technical report",
      "authors": [
        "J Achiam",
        "S Adler",
        "S Agarwal",
        "L Ahmad",
        "I Akkaya",
        "F Aleman",
        "D Almeida",
        "J Altenschmidt",
        "S Altman",
        "S Anadkat"
      ],
      "year": "2005",
      "venue": "Gpt-4 technical report",
      "arxiv": "arXiv:2303.08774"
    },
    {
      "citation_id": "35",
      "title": "Affectnet: A database for facial expression, valence, and arousal computing in the wild",
      "authors": [
        "A Mollahosseini",
        "B Hasani",
        "M Mahoor"
      ],
      "year": "2006",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "36",
      "title": "Reliable crowdsourcing and deep localitypreserving learning for expression recognition in the wild",
      "authors": [
        "S Li",
        "W Deng",
        "J Du"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "37",
      "title": "Challenges in representation learning: A report on three machine learning contests",
      "authors": [
        "I Goodfellow",
        "D Erhan",
        "P Carrier",
        "A Courville",
        "M Mirza",
        "B Hamner",
        "W Cukierski",
        "Y Tang",
        "D Thaler",
        "D.-H Lee"
      ],
      "year": "2013",
      "venue": "International conference on neural information processing"
    },
    {
      "citation_id": "38",
      "title": "Entropy and diversity",
      "authors": [
        "L Jost"
      ],
      "year": "2006",
      "venue": "Oikos"
    },
    {
      "citation_id": "39",
      "title": "Diversity and evenness: a unifying notation and its consequences",
      "authors": [
        "M Hill"
      ],
      "year": "1973",
      "venue": "Ecology"
    },
    {
      "citation_id": "40",
      "title": "Measurement of diversity",
      "authors": [
        "E Simpon"
      ],
      "year": "1949",
      "venue": "Nature"
    },
    {
      "citation_id": "41",
      "title": "Mobilenets: Efficient convolutional neural networks for mobile vision applications",
      "authors": [
        "A Howard",
        "M Zhu",
        "B Chen",
        "D Kalenichenko",
        "W Wang",
        "T Weyand",
        "M Andreetto",
        "H Adam"
      ],
      "year": "2017",
      "venue": "Mobilenets: Efficient convolutional neural networks for mobile vision applications",
      "arxiv": "arXiv:1704.04861"
    },
    {
      "citation_id": "42",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "K He",
        "X Zhang",
        "S Ren",
        "J Sun"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "43",
      "title": "Xception: Deep learning with depthwise separable convolutions",
      "authors": [
        "F Chollet"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "44",
      "title": "Equality of opportunity in supervised learning",
      "authors": [
        "M Hardt",
        "E Price",
        "N Srebro"
      ],
      "year": "2005",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "45",
      "title": "Fairness through awareness",
      "authors": [
        "C Dwork",
        "M Hardt",
        "T Pitassi",
        "O Reingold",
        "R Zemel"
      ],
      "year": "2012",
      "venue": "Proceedings of the 3rd innovations in theoretical computer science conference"
    },
    {
      "citation_id": "46",
      "title": "Counterfactual fairness",
      "authors": [
        "M Kusner",
        "J Loftus",
        "C Russell",
        "R Silva"
      ],
      "year": "2017",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "47",
      "title": "Fairness in criminal justice risk assessments: The state of the art",
      "authors": [
        "R Berk",
        "H Heidari",
        "S Jabbari",
        "M Kearns",
        "A Roth"
      ],
      "year": "2021",
      "venue": "Sociological Methods & Research"
    },
    {
      "citation_id": "48",
      "title": "Face liveness detection competition (livdet-face)-2024",
      "authors": [
        "L Igene",
        "A Hossain",
        "M Chowdhury",
        "H Rezaie",
        "A Rollins",
        "J Dykes",
        "R Vijaykumar",
        "A Komaty",
        "S Marcel",
        "S Schuckers"
      ],
      "year": "2024",
      "venue": "2024 IEEE International Joint Conference on Biometrics (IJCB)"
    },
    {
      "citation_id": "49",
      "title": "Mitigating algorithmic bias on facial expression recognition",
      "authors": [
        "G Amigo",
        "P Perea",
        "R Marks"
      ],
      "year": "2023",
      "venue": "Mitigating algorithmic bias on facial expression recognition",
      "arxiv": "arXiv:2312.15307"
    },
    {
      "citation_id": "50",
      "title": "Metrics for dataset demographic bias: A case study on facial expression recognition",
      "authors": [
        "I Dominguez-Catena",
        "D Paternain",
        "M Galar"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "51",
      "title": "Gender stereotyping impact in facial expression recognition",
      "authors": [
        "I Dominguez-Catena",
        "M Galar",
        "D Paternain"
      ],
      "year": "2022",
      "venue": "Joint European Conference on Machine Learning and Knowledge Discovery in Databases"
    },
    {
      "citation_id": "52",
      "title": "Assessing demographic bias transfer from dataset to model: A case study in facial expression recognition",
      "authors": [
        "I Dominguez-Catena",
        "D Paternain",
        "M Galar"
      ],
      "year": "2022",
      "venue": "Assessing demographic bias transfer from dataset to model: A case study in facial expression recognition",
      "arxiv": "arXiv:2205.10049"
    },
    {
      "citation_id": "53",
      "title": "Bias and diversity in synthetic-based face recognition",
      "authors": [
        "M Huber",
        "A Luu",
        "F Boutros",
        "A Kuijper",
        "N Damer"
      ],
      "year": "2024",
      "venue": "Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision"
    },
    {
      "citation_id": "54",
      "title": "Exploring large-scale unlabeled faces to enhance facial expression recognition",
      "authors": [
        "J Yu",
        "Z Cai",
        "R Li",
        "G Zhao",
        "G Xie",
        "J Zhu",
        "W Zhu",
        "Q Ling",
        "L Wang",
        "C Wang"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "55",
      "title": "Toward fair facial expression recognition with improved distribution alignment",
      "authors": [
        "M Kolahdouzi",
        "A Etemad"
      ],
      "year": "2023",
      "venue": "Proceedings of the 25th International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "56",
      "title": "Effects of different datasets, models, face-parts on accuracy and performance of intelligent facial expression recognition systems",
      "authors": [
        "S Abdullah",
        "S Zeebaree",
        "M Abdulrazzaq"
      ],
      "year": "2024",
      "venue": "International Journal of Intelligent Systems and Applications in Engineering"
    },
    {
      "citation_id": "57",
      "title": "Disfa: A spontaneous facial action intensity database",
      "authors": [
        "S Mavadati",
        "M Mahoor",
        "K Bartlett",
        "P Trinh",
        "J Cohn"
      ],
      "year": "2013",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "58",
      "title": "Face recognition algorithm bias: Performance differences on images of children and adults",
      "authors": [
        "N Srinivas",
        "K Ricanek",
        "D Michalski",
        "D Bolme",
        "M King"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition workshops"
    },
    {
      "citation_id": "59",
      "title": "Anatomizing bias in facial analysis",
      "authors": [
        "R Singh",
        "P Majumdar",
        "S Mittal",
        "M Vatsa"
      ],
      "year": "2022",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "60",
      "title": "A comprehensive study on face recognition biases beyond demographics",
      "authors": [
        "P Terhörst",
        "J Kolf",
        "M Huber",
        "F Kirchbuchner",
        "N Damer",
        "A Moreno",
        "J Fierrez",
        "A Kuijper"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Technology and Society"
    },
    {
      "citation_id": "61",
      "title": "Less can be more: representational vs. stereotypical gender bias in facial expression recognition",
      "authors": [
        "I Dominguez-Catena",
        "D Paternain",
        "A Jurio",
        "M Galar"
      ],
      "year": "2024",
      "venue": "Progress in Artificial Intelligence"
    },
    {
      "citation_id": "62",
      "title": "Comprehensive comparison between vision transformers and convolutional neural networks for face recognition tasks",
      "authors": [
        "M Rodrigo",
        "C Cuevas",
        "N García"
      ],
      "year": "2024",
      "venue": "Scientific reports"
    },
    {
      "citation_id": "63",
      "title": "Exploring vision language models for facial attribute recognition: Emotion, race, gender, and age",
      "authors": [
        "N Aldahoul",
        "M Tan",
        "H Kasireddy",
        "Y Zaki"
      ],
      "year": "2024",
      "venue": "Exploring vision language models for facial attribute recognition: Emotion, race, gender, and age",
      "arxiv": "arXiv:2410.24148"
    },
    {
      "citation_id": "64",
      "title": "Enhancing zero-shot facial expression recognition by llm knowledge transfer",
      "authors": [
        "Z Zhao",
        "Y Cao",
        "S Gong",
        "I Patras"
      ],
      "year": "2024",
      "venue": "Enhancing zero-shot facial expression recognition by llm knowledge transfer",
      "arxiv": "arXiv:2405.19100"
    },
    {
      "citation_id": "65",
      "title": "Fairness definitions explained",
      "authors": [
        "S Verma",
        "J Rubin"
      ],
      "year": "2018",
      "venue": "Proceedings of the international workshop on software fairness"
    },
    {
      "citation_id": "66",
      "title": "On bias and fairness in deep learning-based facial analysis",
      "authors": [
        "S Mittal",
        "P Majumdar",
        "M Vatsa",
        "R Singh"
      ],
      "year": "2023",
      "venue": "Handbook of Statistics"
    },
    {
      "citation_id": "67",
      "title": "A mathematical theory of communication",
      "authors": [
        "C Shannon"
      ],
      "year": "1948",
      "venue": "The Bell system technical journal"
    },
    {
      "citation_id": "68",
      "title": "Hyperextended lightface: A facial attribute analysis framework",
      "authors": [
        "S Serengil",
        "A Ozpinar"
      ],
      "year": "2021",
      "venue": "2021 International Conference on Engineering and Emerging Technologies (ICEET)",
      "doi": "10.1109/ICEET53442.2021.9659697"
    },
    {
      "citation_id": "69",
      "title": "Fair prediction with disparate impact: A study of bias in recidivism prediction instruments",
      "authors": [
        "A Chouldechova"
      ],
      "year": "2017",
      "venue": "Big data"
    },
    {
      "citation_id": "70",
      "title": "Constants across cultures in the face and emotion",
      "authors": [
        "P Ekman",
        "W Friesen"
      ],
      "year": "1971",
      "venue": "Journal of personality and social psychology"
    },
    {
      "citation_id": "71",
      "title": "A benchmark of facial recognition pipelines and co-usability performances of modules",
      "authors": [
        "S Serengil",
        "A Ozpinar"
      ],
      "year": "2024",
      "venue": "Bilisim Teknolojileri Dergisi"
    },
    {
      "citation_id": "72",
      "title": "Lightface: A hybrid deep face recognition framework",
      "authors": [
        "S Serengil",
        "A Ozpinar"
      ],
      "year": "2020",
      "venue": "2020 Innovations in Intelligent Systems and Applications Conference (ASYU)"
    },
    {
      "citation_id": "73",
      "title": "An analysis of bias in facial image processing: A review of datasets",
      "authors": [
        "A Udefi",
        "S Aina",
        "A Lawal",
        "A Oluwarantie"
      ],
      "year": "2023",
      "venue": "International Journal of Advanced Computer Science and Applications"
    },
    {
      "citation_id": "74",
      "title": "Tensorflow: Large-scale machine learning on heterogeneous distributed systems",
      "authors": [
        "M Abadi",
        "A Agarwal",
        "P Barham",
        "E Brevdo",
        "Z Chen",
        "C Citro",
        "G Corrado",
        "A Davis",
        "J Dean",
        "M Devin"
      ],
      "year": "2016",
      "venue": "Tensorflow: Large-scale machine learning on heterogeneous distributed systems",
      "arxiv": "arXiv:1603.04467"
    },
    {
      "citation_id": "75",
      "title": "Transformers: State-of-the-art natural language processing",
      "authors": [
        "T Wolf",
        "L Debut",
        "V Sanh",
        "J Chaumond",
        "C Delangue",
        "A Moi",
        "P Cistac",
        "T Rault",
        "R Louf",
        "M Funtowicz"
      ],
      "year": "2020",
      "venue": "Proceedings of the 2020 conference on empirical methods in natural language processing: system demonstrations"
    }
  ]
}