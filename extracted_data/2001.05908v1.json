{
  "paper_id": "2001.05908v1",
  "title": "Speech Emotion Recognition Based On Multi-Feature And Multi-Lingual Fusion",
  "published": "2020-01-16T15:53:13Z",
  "authors": [
    "Chunyi Wang"
  ],
  "keywords": [
    "Speech emotion recognition",
    "Feature extraction",
    "Multi-feature fusion",
    "Multi-lingual fusion",
    "Deep neural networks (DNN)"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "A speech emotion recognition algorithm based on multi-feature and Multi-lingual fusion is proposed in order to resolve low recognition accuracy caused by lack of large speech dataset and low robustness of acoustic features in the recognition of speech emotion. First, handcrafted and deep automatic features are extracted from existing data in Chinese and English speech emotions. Then, the various features are fused respectively. Finally, the fused features of different languages are fused again and trained in a classification model. Distinguishing the fused features with the unfused ones, the results manifest that the fused features significantly enhance the accuracy of speech emotion recognition algorithm. The proposed solution is evaluated on the two Chinese corpus and two English corpus, and is shown to provide more accurate predictions compared to original solution. As a result of this study, the multi-feature and Multi-lingual fusion algorithm can significantly improve the speech emotion recognition accuracy when the dataset is small.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Background And Significance Of The Research",
      "text": "In researches involving speech, speech emotion recognition is a step in the right direction. This innovation helps in identifying the emotion being expressed depending on the speech of the speaker, including but not limited to signal processing, feature extraction and pattern recognition. Psychology and medicine had both conducted initial studies concerning speech emotion recognition  [1] , however researchers in cognitive science have not focused on emotional research. By the conclusion of the previous century, MIT Media Lab's Professor Picard introduced emotional computing as a concept  [2] , hence speech emotion recognition emerged as a research hotspot. Initially, the methods involving speech emotion recognition typically utilize an artificial experience as a way of obtaining handcraft features, while employing a traditional machine learning algorithm as a classifier. Such method consumes more time and effort while realizing a low recognition accuracy. Recently, the technology underwent rapid development through the progress of deep learning and the increasing demand on speech emotion recognition. Models based on Recurrent Neural Network (RNN)  [3] , Convolutional Neural Network (CNN)  [4]  and RNN+CNN  [5]  have all significantly improved the accuracy of the speech emotion recognition model.\n\nIndeed, speech is one of the most necessary means of communication among human beings. Its signal is not only comprised of text information with regard to speech, but also abundant emotional information. Different meanings based on various emotions can be rooted from a single sentence. Hence, one can fully comprehend the speaker's intention only through accurately recognizing his speech's emotion. In particular, during interaction between humans and machines, the latter is expected to accurately recognize human emotions. Recently, technologies involving the said interaction have progressively provided convenience to humans.\n\nFor instance, an intelligent customer service can immediately decipher user emotions and thus adapt or regulate its quality of service. Nevertheless, these technologies can address only the simple needs of people in a certain field; performance is usually poor for open domain dialogue where the requirements are much more complicated than before. It is mainly because machines do not employ speech emotion recognition as a function; if it does, the accuracy is typically low. Thus, it is practically significant to further study speech emotion recognition in order to promote the progress of interaction between humans and machines.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "The Speech Emotion Recognition Process",
      "text": "The speech emotion recognition process involves speech signal sampling, preprocessing, feature extraction, classifier training, and output emotion tags. Figure  1  illustrates the process flowchart. The preprocessing process generally involves pre-emphasis, framing and endpoint detection as preparation for feature extraction. Pre-emphasis refers to the compensation of high-frequency components of the input signal, improvement of the high-frequency portion, and improvement of the output signal-noise ratio. Framing is the division of input speech signal into segments; each segment is called a frame, with a speech signal ranging from10 to 30ms. A 10ms overlap exists between each frame. Meanwhile, endpoint detection finds both the starting and ending positions from a recording and separates any invalid speech signal.\n\nThe key to speech emotion recognition is feature extraction process. The quality of the features directly influences the accuracy of classification results. Typically, the feature extraction method designs handcraft features based on acoustic features of speech. Through the progress of neural networks and deep learning, the emergence of automatic extraction methods using deep neural networks is realized.\n\nAs for the training classifier, the input refers to the extracted features as it trains the classification model. The common and traditional classifiers are GMM, SVM, KNN and DNN.\n\nThe expected output is the classification of emotion, which indicates the termination of the process.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Related Work",
      "text": "From the preceding flowchart, some of the factors that influence the accuracy of speech emotion recognition are the size and quality of emotion corpus, the extraction of features, and the selection of classifiers. Hence, three aspects comprise the primary focus of this research: emotion corpus, feature extraction and classifier design. Among the three, only emotion corpus and feature extraction are generally involved in this paper, which will be the basis for this section.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Emotional Corpus",
      "text": "Current models on speech emotion vary depending on the involved datasets, which generally produce a low overall rate of accuracy rate. This is because such data sets are not quite sufficient in scale. Most of the mare comprised of only a few to a dozen hours, which is not a sufficient duration in speech emotion recognition.\n\nIndeed, some scholars have put forward transfer learning or multi-task learning technology to resolve lack of data. The rationale involves the use of data among several datasets to enhance the model's generalization, and to somehow lessen problems resulting from lack of data. Huang et al.  [6]  initiated a shared-hidden-layer multilingual DNN (SHLMDNN), in which different languages are sharing the hidden layer while the output layer's softmax corresponds to those. This model alleviates the error rate by 3%-5% when compared to the single language DNN that is trained only with a certain language data. It is verified that knowledge sharing among languages can enhance speech emotion recognition accuracy using single language. Later, Zhang et al.  [7]  employed the multi-task learning method to assess the effect of corpus, domain and gender on speech emotion recognition. It revealed that by increasing the number of corpora, better performance is expected. The reason for the efficacy of this method is that information involving emotions is typical regardless of the language. A Chinese who lacks comprehension of a foreign language may assess whether a non-Chinese expresses happiness or sadness through the language being spoken.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Feature Extraction",
      "text": "The present acoustic features typically utilized in speech emotion recognition are time-domain features, frequency-domain features, statistical features, deep features and hybrid features. In general, feature extraction depends on the frame involved. A speech signal with a certain duration is divided into frames with an interval ranging from10 to 30ms, while features are then extracted from every frame.\n\nTime-domain feature is the direct process of the time domain waveform. This is the most prevalent feature extraction method since it is relatively easy to think of. It can extract features such as short-time zero crossing rate, short-time energy and pitch frequency  [8] .\n\nFrequency domain feature involves (short-time) Fourier transform wavelet transform and others. Initially, the time-domain signal is transformed into the frequency-domain, from which the feature is extracted. Such features are highly associated with the human perception of speech. Hence, they have apparent acoustic characteristics. These features are usually comprised of formant frequency, linear prediction cepstral coefficient (LPCC), and Mel frequency cepstral coefficients (MFCC)  [9] . They can aptly signify the channel feature  [10]    [11] , indicating noise resistance and good recognition performance  [12] .\n\nThe statistics extracted through a centralized instantaneous data processing refer to statistical features. These features are of an utterance level, while the previous two are frame level. Utterance level feature scan relatively replicate the emotional attributes of speech more deeply  [13]    [14] . Mean value, extreme value, variance, center moment of each order, and origin moment of each order are some of the common statistical features.\n\nDeep features are those extracted by the deep neural network. This generally depicts taking the original speech signal or its spectrum as an input for the deep neural network. RNN  [3] , CNN  [4]  and CNN + RNN  [5]  are some of the common DNNs. These features can automatically extract features while alleviating the complexity of manual design features.\n\nHybrid features involve all the aforementioned features to form a feature set. For instance, the GeMAPS  [15]  feature set has 62 statistical features, calculated from18 time-domain and frequency domain features. Meanwhile, the eGeMAPS is an extension of GeMAPS, with 88 features including 18 time-domain and frequency domain features and 5 spectrum features.\n\n3 Our Approach",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "3.1Multi-Feature Fusion",
      "text": "The combination of several same-level features to come up with a new feature set is referred to as feature fusion. This may involve a combination of time domain features, frequency domain features and statistical features in manually designed low level descriptors (LLDs).In such process, multi-dimensional LLDs are fused  [16] , while associating only some handcraft features. Despite this, some features cannot be manually designed, hence particular limits are expected. This research employs a relatively new feature fusion method which combines handcraft features with DNN-extracted ones. The latter is generated from a VGGish model, a model used by Google to come up with a large-scale audio dataset known as Audio Set  [17] . It can generate up to 128-dimensional features. The feature set in this step is labeled as VGGishs. The time length of each frame is 25ms, with a 15ms overlap in between. The fusion method involves taking the LLD feature in each frame in the VGGishs feature in a 1:4 ratio, assuming that a VGGishs frame corresponds to 96 LLDs frames, then taking 24fromthe 96. Afterward, the acquired LLDs features are fused with VGGishs features to generate 24 new features. The fused features are labeled as LLDs+VGGishs. This now involves both manual features as designed by experts and deep ones from the DNN. With the combination of both advantages, the effectiveness is increased.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Multi-Lingual Fusion",
      "text": "Considering that the speech emotional dataset of a particular language is insufficient to attain model training need, a Multi-lingual fusion model is designed based on transfer learning. Such learning aims to employ knowledge or patterns from a certain domain or task to various associated domains or tasks. Since emotion is usually common among languages, mixing their corpus for training purposes can enable learning from linguistic attributes of different languages and increase model generalization. The main contribution of this paper is that we propose a new fusion method that efficiently utilizes both linguistic data for speech emotion recognition.\n\nChinese and English speech emotion data sets are both utilized in this paper. The fused method first extracts the fused features in both datasets. Afterward, the extracted fused features are reordered and then placed as inputs into the classification model for training. The emotion recognition effect on both speech emotion datasets is then tested. In Figure  4 , local Attention RNN (LA+RNN) with local attention mechanism is the basis for the experiment's classification model  [18] . Improvements are made based on the model, upon which the improved model structure is depicted. From the figure, Attn is the attention parameter vector, while BiLSTM refers to bidirectional long short-term memory network. The original model's single-layer BiLSTMis changed into a double-layer one, while adding 2-layer fully connected layer before BiLSTM.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Model",
      "text": "It is worth mentioning that we make an improvement on the original model. The improvement is on the local attention mechanism. The attention weights of the original model is calculated by the Attn and data in steps of L frame that the L is 1. Our improvement is to set L=2*k+1, k∈Z + . By the experiment, we find that when L=5 , we can get the better result.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Datasets",
      "text": "Four publicly available speech emotion datasets are utilized -the IEMOCAP and SAVEE are English dataset, and the CHEAVD2.0 and CASIA are Chinese dataset.\n\nIEMOCAP  [19]  is an English dataset produced by the University of Southern California. It is obtained from 10 professional actors, with a male and a female designated as a section. It includes 5 sections and 9 emotions, withalmost12 hours of recorded data. From this dataset, 2280 data pieces are extractedrepresenting4 emotions in this research's dataset -happy, angry, sad and neutral.\n\nSAVEE  [20]  is also an English dataset, which consists of recordings from 4 male actors in 7 different emotions, 480 British English utterances in total. The sentences were chosen from the standard TIMIT corpus and phonetically-balanced for each emotion. The data were recorded in a visual media lab with high quality audio-visual equipment, processed and labeled. CHEAVD2.0  [21]  is a dataset produced by the Institute of Automation of the Chinese Academy of Sciences in relation to the 2017 Multimodal Emotional Recognition Challenge (MEC). It contains 7030 data samples extracted from Chinese movies, TV series and other entertainment programs, with the training set, validation set and test set containing 4917, 707, and 1406 samples, respectively. From this dataset, 8 emotions are included: anger, happiness, sadness, worry, anxiety, surprise, disgust and neutral.\n\nCASIA is also a Chinese dataset recorded by the Institute of Automation, Chinese Academy of Sciences. It includes four professional speakers and six kinds of emotions: angry, happy, sad, surprise and neutral, a total of 9600 different pronunciation.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Hyper-Parameters",
      "text": "In the model, the BiLSTM's dimension is set at 100.For optimization, Adam with gradient clipping is employed. The padding length in each speech sentence is adjusted in every batch, referring to the maximum length of speech sentence in a particular batch. In Table  1 , we can find that the accuracy of the handcraft feature combination LLDs in the Cheaved dataset is higher than that of the DNN-extracted feature set VGGishs. But in the other three dataset, the performance of the two feature sets is reversed, with a relatively larger discrepancy in accuracy. This manifests that VGGishs is more suitable for English speech emotion recognition than LLDs. Meanwhile, the attributes of LLDs + VGGishs in both datasets are higher than those of single-type ones. Because LLDs are hand-designed features that involve previous knowledge of acoustic design, while DNN can extract high-level features. With the advantages of both methods, features of speech data can be extracted more comprehensively, thus improving overall recognition effect.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Results",
      "text": "",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Three",
      "text": "Choosing LLDs+VGGishs as the optimum feature combination, the effect of Multi-lingual fusion on speech emotion recognition on different language datasets is then experimented. The results are tabulated in Table  2 . In Table  2 , the LLDs+VGGishs features extracted from Chinese and English data are combined as a training set, then the model on four data are tested. The result has a huge improvement in Savee dataset, and has a slight improvement in other three datasets. The reason for this phenomenon is that Savee dataset is much smaller than the other three. Then we also make the fusion in different dataset in same language, the results in Cheaved and Savee have some improvements, but that in Casia and Iemocap dataset have a bit of drop. The results illustrate that Chinese and English fusion can make a slight improvement compared to the single language. Thus, the Multi-lingual fusion method proposed in this paper can improve speech emotion recognition accuracy.\n\nWe also make the experiment on the improved model by using four dataset as the training set to compare with the original model. The result is shown in Table  3 .\n\nTable  3  From the Table  3 , we can apparently find that the result of improved model is better than the result of original model, especially in the Cheavd dataset, which is about a 4% improvement. This indeed demonstrates the effectiveness of the improved model.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Conclusion",
      "text": "A multi-feature fusion and Multi-lingual fusion speech emotion recognition algorithm is proposed based on the RNN with improved local attention mechanism. Four Chinese and English speech emotion datasets are used in the process. Three sets of contrast experiments are then employed to choose the best feature combination, verify its effect on Multi-lingual fusion and prove the effectiveness of the improved model. The said experiments reveal that the proposed algorithm can effectively enhance speech recognition accuracy. Such algorithm denotes a useful reference in resolving cases in which insufficient speech emotion data are experienced.\n\nThank you also to Jingyan Shi of the Chinese Academy of Sciences for his continuous support, as well as Professor JiepingXu and Professor Shengqi Shao of Renmin University of China for their substantial recommendation. Thank you to the S.-T. Yau High School Science Award for providing middle school students a suitable platform to enhance their interests and innovativeness.",
      "page_start": 8,
      "page_end": 9
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: illustrates the process",
      "page": 2
    },
    {
      "caption": "Figure 1: Process flow chart of speech emotion recognition",
      "page": 2
    },
    {
      "caption": "Figure 2: Multi feature fusion method",
      "page": 4
    },
    {
      "caption": "Figure 3: Experiment flowchart",
      "page": 5
    },
    {
      "caption": "Figure 3: illustrates the experiment flowchart, which principally includes feature extraction,",
      "page": 5
    },
    {
      "caption": "Figure 4: , local Attention RNN (LA+RNN) with local attention mechanism is the basis for the",
      "page": 5
    },
    {
      "caption": "Figure 4: Structure of the classification model",
      "page": 6
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Chunyi Wang": "The Experimental High School Attached to Beijing Normal University"
        },
        {
          "Chunyi Wang": "renying@bjtu.edu.cn"
        },
        {
          "Chunyi Wang": "A\nbstract:  A  speech  emotion  recognition  algorithm  based  on  multi-feature  and  Multi-lingual"
        },
        {
          "Chunyi Wang": "fusion  is  proposed  in  order  to  resolve  low  recognition  accuracy  caused  by  lack  of  large  speech"
        },
        {
          "Chunyi Wang": "dataset  and  low  robustness  of  acoustic  features  in  the  recognition  of  speech  emotion.  First,"
        },
        {
          "Chunyi Wang": "handcrafted and deep automatic features are extracted from existing data in Chinese and English"
        },
        {
          "Chunyi Wang": "speech emotions. Then, the various features are fused respectively. Finally, the fused features of"
        },
        {
          "Chunyi Wang": "different languages are fused again and trained in a classification model. Distinguishing the fused"
        },
        {
          "Chunyi Wang": "features with the unfused ones, the results  manifest that the fused features significantly enhance"
        },
        {
          "Chunyi Wang": "the accuracy of speech emotion recognition algorithm. The proposed solution is evaluated on the"
        },
        {
          "Chunyi Wang": "two Chinese corpus and two English corpus, and is shown to provide  more accurate predictions"
        },
        {
          "Chunyi Wang": "compared to original solution. As a result of this study, the multi-feature and Multi-lingual fusion"
        },
        {
          "Chunyi Wang": "algorithm can significantly improve the speech emotion recognition accuracy when the dataset is"
        },
        {
          "Chunyi Wang": "small."
        },
        {
          "Chunyi Wang": "Keywords:  Speech  emotion  recognition,  Feature  extraction,  Multi-feature  fusion,  Multi-lingual"
        },
        {
          "Chunyi Wang": "fusion, Deep neural networks (DNN)"
        },
        {
          "Chunyi Wang": "1 Introduction"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "For instance, an intelligent customer service can immediately decipher user emotions and thus": "adapt  or  regulate  its  quality  of  service.  Nevertheless,  these  technologies  can  address  only  the"
        },
        {
          "For instance, an intelligent customer service can immediately decipher user emotions and thus": "simple needs of people in a certain field; performance is usually poor for open domain dialogue"
        },
        {
          "For instance, an intelligent customer service can immediately decipher user emotions and thus": "where the requirements are much more complicated than before. It is mainly because machines"
        },
        {
          "For instance, an intelligent customer service can immediately decipher user emotions and thus": "do not employ speech emotion recognition as a function; if it does, the accuracy is typically low."
        },
        {
          "For instance, an intelligent customer service can immediately decipher user emotions and thus": "Thus, it is practically significant to further study speech emotion recognition in order to promote"
        },
        {
          "For instance, an intelligent customer service can immediately decipher user emotions and thus": "the progress of interaction between humans and machines."
        },
        {
          "For instance, an intelligent customer service can immediately decipher user emotions and thus": "1.2 The Speech Emotion Recognition Process"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "emotion corpus, feature extraction and classifier design. Among the three, only emotion corpus": "and feature extraction are generally involved in this paper, which will be the basis for this section."
        },
        {
          "emotion corpus, feature extraction and classifier design. Among the three, only emotion corpus": "2.1 Emotional Corpus"
        },
        {
          "emotion corpus, feature extraction and classifier design. Among the three, only emotion corpus": "Current models on speech emotion vary depending on the involved datasets, which generally"
        },
        {
          "emotion corpus, feature extraction and classifier design. Among the three, only emotion corpus": "produce a low overall rate of accuracy rate. This is because such data sets are not quite sufficient"
        },
        {
          "emotion corpus, feature extraction and classifier design. Among the three, only emotion corpus": "in  scale.  Most  of  the  mare comprised  of  only  a  few to  a  dozen  hours,  which  is  not  a  sufficient"
        },
        {
          "emotion corpus, feature extraction and classifier design. Among the three, only emotion corpus": "duration in speech emotion recognition."
        },
        {
          "emotion corpus, feature extraction and classifier design. Among the three, only emotion corpus": "Indeed, some scholars have put forward transfer learning or multi-task learning technology to"
        },
        {
          "emotion corpus, feature extraction and classifier design. Among the three, only emotion corpus": "resolve lack of data. The rationale involves the use of data among several datasets to enhance"
        },
        {
          "emotion corpus, feature extraction and classifier design. Among the three, only emotion corpus": "the model’s generalization, and to somehow lessen problems resulting from lack of data. Huang"
        },
        {
          "emotion corpus, feature extraction and classifier design. Among the three, only emotion corpus": "et  al.[6] \ninitiated  a  shared-hidden-layer  multilingual  DNN \n(SHLMDNN), \nin  which  different"
        },
        {
          "emotion corpus, feature extraction and classifier design. Among the three, only emotion corpus": "languages  are  sharing  the  hidden  layer  while  the  output  layer's  softmax  corresponds  to  those."
        },
        {
          "emotion corpus, feature extraction and classifier design. Among the three, only emotion corpus": "This model alleviates the error rate by 3%-5% when compared to the single language DNN that is"
        },
        {
          "emotion corpus, feature extraction and classifier design. Among the three, only emotion corpus": "trained only with a certain language data. It is verified that knowledge sharing among languages"
        },
        {
          "emotion corpus, feature extraction and classifier design. Among the three, only emotion corpus": "can enhance speech emotion recognition accuracy using single language. Later, Zhang et al. [7]"
        },
        {
          "emotion corpus, feature extraction and classifier design. Among the three, only emotion corpus": "employed the multi-task learning method to assess the effect of corpus, domain and gender on"
        },
        {
          "emotion corpus, feature extraction and classifier design. Among the three, only emotion corpus": "speech  emotion  recognition. \nIt  revealed  that  by \nincreasing  the  number  of  corpora,  better"
        },
        {
          "emotion corpus, feature extraction and classifier design. Among the three, only emotion corpus": "performance is expected. The reason for the efficacy of this method is that information involving"
        },
        {
          "emotion corpus, feature extraction and classifier design. Among the three, only emotion corpus": "emotions is typical regardless of the language. A Chinese who lacks comprehension of a foreign"
        },
        {
          "emotion corpus, feature extraction and classifier design. Among the three, only emotion corpus": "language  may  assess  whether  a  non-Chinese  expresses  happiness  or  sadness  through  the"
        },
        {
          "emotion corpus, feature extraction and classifier design. Among the three, only emotion corpus": "language being spoken."
        },
        {
          "emotion corpus, feature extraction and classifier design. Among the three, only emotion corpus": "2.2 Feature Extraction"
        },
        {
          "emotion corpus, feature extraction and classifier design. Among the three, only emotion corpus": "The present acoustic features typically utilized in speech emotion recognition are time-domain"
        },
        {
          "emotion corpus, feature extraction and classifier design. Among the three, only emotion corpus": "features,  frequency-domain  features,  statistical  features,  deep  features  and  hybrid  features.  In"
        },
        {
          "emotion corpus, feature extraction and classifier design. Among the three, only emotion corpus": "general,  feature  extraction  depends  on  the  frame \ninvolved.  A  speech  signal  with  a  certain"
        },
        {
          "emotion corpus, feature extraction and classifier design. Among the three, only emotion corpus": "duration is divided into frames with an interval ranging from10 to 30ms, while features are then"
        },
        {
          "emotion corpus, feature extraction and classifier design. Among the three, only emotion corpus": "extracted from every frame."
        },
        {
          "emotion corpus, feature extraction and classifier design. Among the three, only emotion corpus": "Time-domain  feature  is  the  direct  process  of  the  time  domain  waveform.  This  is  the  most"
        },
        {
          "emotion corpus, feature extraction and classifier design. Among the three, only emotion corpus": "prevalent feature extraction method since it is relatively easy to think of. It can extract features"
        },
        {
          "emotion corpus, feature extraction and classifier design. Among the three, only emotion corpus": "such as short-time zero crossing rate, short-time energy and pitch frequency [8]."
        },
        {
          "emotion corpus, feature extraction and classifier design. Among the three, only emotion corpus": "Frequency  domain  feature \ninvolves  (short-time)  Fourier  transform  wavelet  transform  and"
        },
        {
          "emotion corpus, feature extraction and classifier design. Among the three, only emotion corpus": "others.  Initially,  the  time-domain  signal  is  transformed  into  the  frequency-domain,  from  which"
        },
        {
          "emotion corpus, feature extraction and classifier design. Among the three, only emotion corpus": "the  feature  is  extracted.  Such  features  are  highly  associated  with  the  human  perception  of"
        },
        {
          "emotion corpus, feature extraction and classifier design. Among the three, only emotion corpus": "speech. Hence, they have apparent acoustic characteristics. These features are usually comprised"
        },
        {
          "emotion corpus, feature extraction and classifier design. Among the three, only emotion corpus": "of  formant  frequency,  linear  prediction  cepstral  coefficient  (LPCC),  and  Mel  frequency  cepstral"
        },
        {
          "emotion corpus, feature extraction and classifier design. Among the three, only emotion corpus": "coefficients  (MFCC)  [9].  They  can  aptly  signify  the  channel  feature  [10]  [11],  indicating  noise"
        },
        {
          "emotion corpus, feature extraction and classifier design. Among the three, only emotion corpus": "resistance and good recognition performance [12]."
        },
        {
          "emotion corpus, feature extraction and classifier design. Among the three, only emotion corpus": "The statistics extracted through a centralized instantaneous data processing refer to statistical"
        },
        {
          "emotion corpus, feature extraction and classifier design. Among the three, only emotion corpus": "features.  These  features  are  of  an  utterance  level,  while  the  previous  two  are  frame  level."
        },
        {
          "emotion corpus, feature extraction and classifier design. Among the three, only emotion corpus": "Utterance level feature scan relatively replicate the emotional attributes of speech more deeply"
        },
        {
          "emotion corpus, feature extraction and classifier design. Among the three, only emotion corpus": "[13]  [14].  Mean  value,  extreme  value,  variance,  center  moment  of  each  order,  and  origin"
        },
        {
          "emotion corpus, feature extraction and classifier design. Among the three, only emotion corpus": "moment of each order are some of the common statistical features."
        },
        {
          "emotion corpus, feature extraction and classifier design. Among the three, only emotion corpus": "Deep features are those extracted by the deep neural network. This generally depicts taking"
        },
        {
          "emotion corpus, feature extraction and classifier design. Among the three, only emotion corpus": "the original speech signal or its spectrum as an input for the deep neural network. RNN [3], CNN"
        },
        {
          "emotion corpus, feature extraction and classifier design. Among the three, only emotion corpus": "[4] and CNN + RNN [5] are some of the common DNNs. These features can automatically extract"
        },
        {
          "emotion corpus, feature extraction and classifier design. Among the three, only emotion corpus": "features while alleviating the complexity of manual design features."
        },
        {
          "emotion corpus, feature extraction and classifier design. Among the three, only emotion corpus": "Hybrid  features  involve  all  the  aforementioned  features  to  form  a  feature  set.  For  instance,"
        },
        {
          "emotion corpus, feature extraction and classifier design. Among the three, only emotion corpus": "3"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "the  GeMAPS  [15]  feature  set  has  62  statistical  features,  calculated  from18  time-domain  and": "frequency  domain  features.  Meanwhile,  the  eGeMAPS"
        },
        {
          "the  GeMAPS  [15]  feature  set  has  62  statistical  features,  calculated  from18  time-domain  and": "features including 18 time-domain and frequency domain features and 5 spectrum features."
        },
        {
          "the  GeMAPS  [15]  feature  set  has  62  statistical  features,  calculated  from18  time-domain  and": "3 Our Approach"
        },
        {
          "the  GeMAPS  [15]  feature  set  has  62  statistical  features,  calculated  from18  time-domain  and": "3.1Multi-feature Fusion"
        },
        {
          "the  GeMAPS  [15]  feature  set  has  62  statistical  features,  calculated  from18  time-domain  and": ""
        },
        {
          "the  GeMAPS  [15]  feature  set  has  62  statistical  features,  calculated  from18  time-domain  and": "to as feature fusion. This may involve a combination of time domain features, frequency domain"
        },
        {
          "the  GeMAPS  [15]  feature  set  has  62  statistical  features,  calculated  from18  time-domain  and": ""
        },
        {
          "the  GeMAPS  [15]  feature  set  has  62  statistical  features,  calculated  from18  time-domain  and": "multi-dimensional LLDs are fused [16], while associating only some handcraft features. Despite"
        },
        {
          "the  GeMAPS  [15]  feature  set  has  62  statistical  features,  calculated  from18  time-domain  and": "this, some features cannot be manually designed, hence particular limits are expected."
        },
        {
          "the  GeMAPS  [15]  feature  set  has  62  statistical  features,  calculated  from18  time-domain  and": "Figure 2: Multi feature fusion method"
        },
        {
          "the  GeMAPS  [15]  feature  set  has  62  statistical  features,  calculated  from18  time-domain  and": ""
        },
        {
          "the  GeMAPS  [15]  feature  set  has  62  statistical  features,  calculated  from18  time-domain  and": "features with DNN-extracted ones. The latter is generated from a VGGish model, a model used"
        },
        {
          "the  GeMAPS  [15]  feature  set  has  62  statistical  features,  calculated  from18  time-domain  and": "by Google to come up with a large-scale audio dataset known as Audio Set [17]. It can generate"
        },
        {
          "the  GeMAPS  [15]  feature  set  has  62  statistical  features,  calculated  from18  time-domain  and": "up  to  128-dimensional  features.  The  feature  set  in  this  step  is  labeled  as  VGGishs.  The  time"
        },
        {
          "the  GeMAPS  [15]  feature  set  has  62  statistical  features,  calculated  from18  time-domain  and": "length  of  each  frame  is  25ms,  with  a  15ms  overlap  in  between.  The  fusion  method  involves"
        },
        {
          "the  GeMAPS  [15]  feature  set  has  62  statistical  features,  calculated  from18  time-domain  and": "taking  the  LLD  feature  in  each  frame  in  the  VGGishs  feature  in  a  1:4  ratio,  assuming  that  a"
        },
        {
          "the  GeMAPS  [15]  feature  set  has  62  statistical  features,  calculated  from18  time-domain  and": "VGGishs  frame  corresponds  to  96  LLDs  frames,  then  taking  24fromthe  96.  Afterward,  the"
        },
        {
          "the  GeMAPS  [15]  feature  set  has  62  statistical  features,  calculated  from18  time-domain  and": "acquired LLDs features are fused with VGGishs features to generate 24 new features. The fused"
        },
        {
          "the  GeMAPS  [15]  feature  set  has  62  statistical  features,  calculated  from18  time-domain  and": "features  are  labeled  as  LLDs+VGGishs.  This  now  involves  both  manual  features  as  designed  by"
        },
        {
          "the  GeMAPS  [15]  feature  set  has  62  statistical  features,  calculated  from18  time-domain  and": "experts  and  deep  ones \nfrom \nthe  DNN.  With"
        },
        {
          "the  GeMAPS  [15]  feature  set  has  62  statistical  features,  calculated  from18  time-domain  and": "effectiveness is increased."
        },
        {
          "the  GeMAPS  [15]  feature  set  has  62  statistical  features,  calculated  from18  time-domain  and": "3.2 Multi-lingual Fusion"
        },
        {
          "the  GeMAPS  [15]  feature  set  has  62  statistical  features,  calculated  from18  time-domain  and": ""
        },
        {
          "the  GeMAPS  [15]  feature  set  has  62  statistical  features,  calculated  from18  time-domain  and": "model  training  need,  a  Multi-lingual  fusion  model  is  designed  based  on  transfer  learning.  Such"
        },
        {
          "the  GeMAPS  [15]  feature  set  has  62  statistical  features,  calculated  from18  time-domain  and": "4"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "learning  aims  to  employ  knowledge  or  patterns  from  a  certain  domain  or  task  to  various": "associated  domains  or  tasks.  Since  emotion  is  usually  common  among  languages,  mixing  their"
        },
        {
          "learning  aims  to  employ  knowledge  or  patterns  from  a  certain  domain  or  task  to  various": "corpus for training purposes can enable learning from linguistic attributes of different languages"
        },
        {
          "learning  aims  to  employ  knowledge  or  patterns  from  a  certain  domain  or  task  to  various": "and increase model generalization. The main contribution of this paper is that we propose a new"
        },
        {
          "learning  aims  to  employ  knowledge  or  patterns  from  a  certain  domain  or  task  to  various": "Multi-lingual  fusion  method  that  efficiently  utilizes  both \nlinguistic  data  for  speech  emotion"
        },
        {
          "learning  aims  to  employ  knowledge  or  patterns  from  a  certain  domain  or  task  to  various": "recognition."
        },
        {
          "learning  aims  to  employ  knowledge  or  patterns  from  a  certain  domain  or  task  to  various": "Chinese  and  English  speech  emotion  data  sets  are  both  utilized \nin  this  paper.  The  fused"
        },
        {
          "learning  aims  to  employ  knowledge  or  patterns  from  a  certain  domain  or  task  to  various": "method  first  extracts  the  fused  features \nin  both  datasets.  Afterward,  the  extracted  fused"
        },
        {
          "learning  aims  to  employ  knowledge  or  patterns  from  a  certain  domain  or  task  to  various": "features are reordered and then placed as inputs into the classification model for training. The"
        },
        {
          "learning  aims  to  employ  knowledge  or  patterns  from  a  certain  domain  or  task  to  various": "emotion recognition effect on both speech emotion datasets is then tested."
        },
        {
          "learning  aims  to  employ  knowledge  or  patterns  from  a  certain  domain  or  task  to  various": "3.3 Model"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "4.1 Datasets": ""
        },
        {
          "4.1 Datasets": "English dataset, and the CHEAVD2.0 and CASIA are Chinese dataset."
        },
        {
          "4.1 Datasets": ""
        },
        {
          "4.1 Datasets": ""
        },
        {
          "4.1 Datasets": ""
        },
        {
          "4.1 Datasets": ""
        },
        {
          "4.1 Datasets": "sad and neutral."
        },
        {
          "4.1 Datasets": ""
        },
        {
          "4.1 Datasets": "different emotions, 480 British English utterances in total. The sentences were chosen from the"
        },
        {
          "4.1 Datasets": "standard TIMIT corpus and phonetically-balanced for each emotion. The data were recorded in a"
        },
        {
          "4.1 Datasets": "visual media lab with high quality audio-visual equipment, processed and labeled."
        },
        {
          "4.1 Datasets": ""
        },
        {
          "4.1 Datasets": "of  Sciences"
        },
        {
          "4.1 Datasets": ""
        },
        {
          "4.1 Datasets": ""
        },
        {
          "4.1 Datasets": ""
        },
        {
          "4.1 Datasets": "worry, anxiety, surprise, disgust and neutral."
        },
        {
          "4.1 Datasets": ""
        },
        {
          "4.1 Datasets": ""
        },
        {
          "4.1 Datasets": "surprise and neutral, a total of 9600 different pronunciation."
        }
      ],
      "page": 6
    },
    {
      "caption": "Table 1: Table 1: Accuracy of different feature combinations on Four datasets",
      "data": [
        {
          "on speech emotion recognition on different language datasets is then experimented. The results": ""
        },
        {
          "on speech emotion recognition on different language datasets is then experimented. The results": "Table 2: Comparative experiment of multi-lingual fusion"
        },
        {
          "on speech emotion recognition on different language datasets is then experimented. The results": ""
        },
        {
          "on speech emotion recognition on different language datasets is then experimented. The results": "WA"
        },
        {
          "on speech emotion recognition on different language datasets is then experimented. The results": ""
        },
        {
          "on speech emotion recognition on different language datasets is then experimented. The results": "61.9%"
        },
        {
          "on speech emotion recognition on different language datasets is then experimented. The results": "17.8%"
        },
        {
          "on speech emotion recognition on different language datasets is then experimented. The results": "—"
        },
        {
          "on speech emotion recognition on different language datasets is then experimented. The results": "—"
        },
        {
          "on speech emotion recognition on different language datasets is then experimented. The results": "58.8%"
        },
        {
          "on speech emotion recognition on different language datasets is then experimented. The results": "—"
        },
        {
          "on speech emotion recognition on different language datasets is then experimented. The results": "62.8%"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table 2: , the LLDs+VGGishs features extracted from Chinese and English data are combined",
      "data": [
        {
          "In Table 2, the LLDs+VGGishs features extracted from Chinese and English data are combined": "as a training set, then the model on four data are tested. The result has a huge improvement in"
        },
        {
          "In Table 2, the LLDs+VGGishs features extracted from Chinese and English data are combined": "Savee  dataset,  and  has  a  slight"
        },
        {
          "In Table 2, the LLDs+VGGishs features extracted from Chinese and English data are combined": "phenomenon is that Savee dataset is much smaller than the other three. Then we also make the"
        },
        {
          "In Table 2, the LLDs+VGGishs features extracted from Chinese and English data are combined": "in  different  dataset"
        },
        {
          "In Table 2, the LLDs+VGGishs features extracted from Chinese and English data are combined": "improvements,  but  that  in  Casia  and  Iemocap  dataset  have  a  bit  of  drop.  The  results  illustrate"
        },
        {
          "In Table 2, the LLDs+VGGishs features extracted from Chinese and English data are combined": ""
        },
        {
          "In Table 2, the LLDs+VGGishs features extracted from Chinese and English data are combined": "Thus,  the  Multi-lingual  fusion  method  proposed"
        },
        {
          "In Table 2, the LLDs+VGGishs features extracted from Chinese and English data are combined": ""
        },
        {
          "In Table 2, the LLDs+VGGishs features extracted from Chinese and English data are combined": "We also make the experiment on the improved model by using four dataset as the training set"
        },
        {
          "In Table 2, the LLDs+VGGishs features extracted from Chinese and English data are combined": "to compare with the original model. The result is shown in Table 3."
        },
        {
          "In Table 2, the LLDs+VGGishs features extracted from Chinese and English data are combined": "Table 3: Comparison of the improved model with the original model"
        },
        {
          "In Table 2, the LLDs+VGGishs features extracted from Chinese and English data are combined": "Casia"
        },
        {
          "In Table 2, the LLDs+VGGishs features extracted from Chinese and English data are combined": "WA"
        },
        {
          "In Table 2, the LLDs+VGGishs features extracted from Chinese and English data are combined": "62.8%"
        },
        {
          "In Table 2, the LLDs+VGGishs features extracted from Chinese and English data are combined": "63.3%"
        },
        {
          "In Table 2, the LLDs+VGGishs features extracted from Chinese and English data are combined": "From the Table 3, we can apparently find that the result of improved model is better than the"
        },
        {
          "In Table 2, the LLDs+VGGishs features extracted from Chinese and English data are combined": "result of original model, especially in the Cheavd dataset, which is about a 4% improvement. This"
        },
        {
          "In Table 2, the LLDs+VGGishs features extracted from Chinese and English data are combined": "indeed demonstrates the effectiveness of the improved model."
        },
        {
          "In Table 2, the LLDs+VGGishs features extracted from Chinese and English data are combined": ""
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Thank you also to Jingyan Shi of the Chinese Academy of Sciences for his continuous support,": "as well as Professor JiepingXu and Professor Shengqi Shao of Renmin University of China for their"
        },
        {
          "Thank you also to Jingyan Shi of the Chinese Academy of Sciences for his continuous support,": "substantial recommendation. Thank you to the S.-T. Yau High School Science Award for providing"
        },
        {
          "Thank you also to Jingyan Shi of the Chinese Academy of Sciences for his continuous support,": "middle school students a suitable platform to enhance their interests and innovativeness."
        },
        {
          "Thank you also to Jingyan Shi of the Chinese Academy of Sciences for his continuous support,": "References"
        },
        {
          "Thank you also to Jingyan Shi of the Chinese Academy of Sciences for his continuous support,": "[1]Williams C E, Stevens K N. Emotions and Speech: Some Acoustical Correlates [J]. The Journal of"
        },
        {
          "Thank you also to Jingyan Shi of the Chinese Academy of Sciences for his continuous support,": "the Acoustical Society of America, 1972, 52(4):1238-1250."
        },
        {
          "Thank you also to Jingyan Shi of the Chinese Academy of Sciences for his continuous support,": "[2]Picard R W. Affective computing [M]. MIT Press, 1997."
        },
        {
          "Thank you also to Jingyan Shi of the Chinese Academy of Sciences for his continuous support,": "[3]André Stuhlsatz, Meyer C, Eyben F, et al. DNN FOR ACOUSTIC EMOTION RECOGNITION:"
        },
        {
          "Thank you also to Jingyan Shi of the Chinese Academy of Sciences for his continuous support,": "RAISING THE BENCHMARKS[C]// IEEE International Conference on Acoustics. IEEE, 2011."
        },
        {
          "Thank you also to Jingyan Shi of the Chinese Academy of Sciences for his continuous support,": "[4]Bertero D, Fung P. A first look into a Convolutional Neural Network for speech emotion"
        },
        {
          "Thank you also to Jingyan Shi of the Chinese Academy of Sciences for his continuous support,": "detection[C]// IEEE International Conference on Acoustics. IEEE, 2017:5115-5119."
        },
        {
          "Thank you also to Jingyan Shi of the Chinese Academy of Sciences for his continuous support,": "[5]Kim J, Saurous R. Emotion Recognition from Human Speech Using Temporal Information and"
        },
        {
          "Thank you also to Jingyan Shi of the Chinese Academy of Sciences for his continuous support,": "Deep Learning [M]. Interspeech, 2018: 937-940."
        },
        {
          "Thank you also to Jingyan Shi of the Chinese Academy of Sciences for his continuous support,": "[6]Huang J T , Li J , Yu D , et al. Cross-language knowledge transfer using multilingual deep neural"
        },
        {
          "Thank you also to Jingyan Shi of the Chinese Academy of Sciences for his continuous support,": "network with shared hidden layers[C]// Acoustics, Speech and Signal Processing (ICASSP), 2013"
        },
        {
          "Thank you also to Jingyan Shi of the Chinese Academy of Sciences for his continuous support,": "IEEE International Conference on. IEEE, 2013."
        },
        {
          "Thank you also to Jingyan Shi of the Chinese Academy of Sciences for his continuous support,": "[7]Zhang B, Mower Provost E, Essl G. Cross-corpus Acoustic Emotion Recognition with Multi-task"
        },
        {
          "Thank you also to Jingyan Shi of the Chinese Academy of Sciences for his continuous support,": "Learning: Seeking Common Ground while Preserving Differences [J]. IEEE Transactions on"
        },
        {
          "Thank you also to Jingyan Shi of the Chinese Academy of Sciences for his continuous support,": "Affective Computing, 2019, 10:85-99."
        },
        {
          "Thank you also to Jingyan Shi of the Chinese Academy of Sciences for his continuous support,": "[8]Lee C M, Narayanan S S. Toward detecting emotions in spoken dialogs [J]. IEEE Transactions on"
        },
        {
          "Thank you also to Jingyan Shi of the Chinese Academy of Sciences for his continuous support,": "Speech and Audio Processing, 2005, 13(2):293-303."
        },
        {
          "Thank you also to Jingyan Shi of the Chinese Academy of Sciences for his continuous support,": "[9]Davis S B. Comparison of parametric representations for monosyllabic word recognition in"
        },
        {
          "Thank you also to Jingyan Shi of the Chinese Academy of Sciences for his continuous support,": "continuously spoken sentences [J]. IEEE Trans. Acoust. Speech Signal Process. 1980, 28(4):65-74."
        },
        {
          "Thank you also to Jingyan Shi of the Chinese Academy of Sciences for his continuous support,": "[10]Mao X I A, Zhang B, Luo Y I. Speech Emotion Recognition based on a Hybrid of HMM/ANN."
        },
        {
          "Thank you also to Jingyan Shi of the Chinese Academy of Sciences for his continuous support,": "International Journal of Computers, 2007, 4(1):321-324"
        },
        {
          "Thank you also to Jingyan Shi of the Chinese Academy of Sciences for his continuous support,": "[11]Pao T L, Chen Y T, Yeh J H. Emotion recognition from Mandarin speech signals[C]// Chinese"
        },
        {
          "Thank you also to Jingyan Shi of the Chinese Academy of Sciences for his continuous support,": "Spoken Language Processing, 2004 International Symposium on. IEEE, 2005."
        },
        {
          "Thank you also to Jingyan Shi of the Chinese Academy of Sciences for his continuous support,": "[12]Kandali A B, Routray A, Basu T K . Emotion recognition from Assamese speeches using MFCC"
        },
        {
          "Thank you also to Jingyan Shi of the Chinese Academy of Sciences for his continuous support,": "features and GMM classifier[C]// TENCON 2008 - 2008 IEEE Region 10 Conference. IEEE, 2008."
        },
        {
          "Thank you also to Jingyan Shi of the Chinese Academy of Sciences for his continuous support,": "[13]Petrushin V A. Emotion recognition in speech signal: experimental study, development, and"
        },
        {
          "Thank you also to Jingyan Shi of the Chinese Academy of Sciences for his continuous support,": "application [J]. 2000."
        },
        {
          "Thank you also to Jingyan Shi of the Chinese Academy of Sciences for his continuous support,": "[14]Lee C M, Narayanan S, Pieraccini R. Recognition of negative emotions from the speech"
        },
        {
          "Thank you also to Jingyan Shi of the Chinese Academy of Sciences for his continuous support,": "signal[C]// Automatic Speech Recognition and Understanding, 2001. ASRU '01.IEEE Workshop"
        },
        {
          "Thank you also to Jingyan Shi of the Chinese Academy of Sciences for his continuous support,": "on.IEEE, 2001."
        },
        {
          "Thank you also to Jingyan Shi of the Chinese Academy of Sciences for his continuous support,": "[15]EybenF, Scherer K R, Schuller B W, et al. The Geneva Minimalistic Acoustic Parameter Set"
        },
        {
          "Thank you also to Jingyan Shi of the Chinese Academy of Sciences for his continuous support,": "(GeMAPS) for Voice Research and Affective Computing [J]. IEEE Transactions on Affective"
        },
        {
          "Thank you also to Jingyan Shi of the Chinese Academy of Sciences for his continuous support,": "Computing, 2015, 7(2):1-1."
        },
        {
          "Thank you also to Jingyan Shi of the Chinese Academy of Sciences for his continuous support,": "[16]EybenF, Scherer K R, Truong K P, et al. The Geneva Minimalistic Acoustic Parameter Set"
        },
        {
          "Thank you also to Jingyan Shi of the Chinese Academy of Sciences for his continuous support,": "(GeMAPS) for Speech Research and Affective Computing [J]. IEEE Transactions on Affective"
        },
        {
          "Thank you also to Jingyan Shi of the Chinese Academy of Sciences for his continuous support,": "Computing, 2016, 7(2):190-202."
        },
        {
          "Thank you also to Jingyan Shi of the Chinese Academy of Sciences for his continuous support,": "[17]Gemmeke J F, Ellis D P W , Freedman D , et al. Audio Set: An ontology and human-labeled"
        },
        {
          "Thank you also to Jingyan Shi of the Chinese Academy of Sciences for his continuous support,": "dataset for audio events[C]// ICASSP 2017 - 2017 IEEE International Conference on Acoustics,"
        },
        {
          "Thank you also to Jingyan Shi of the Chinese Academy of Sciences for his continuous support,": "Speech and Signal Processing (ICASSP). IEEE, 2017."
        },
        {
          "Thank you also to Jingyan Shi of the Chinese Academy of Sciences for his continuous support,": "[18]Mirsamadi S, BarsoumE , Zhang C . 2017 IEEE International Conference on Acoustics, Speech"
        },
        {
          "Thank you also to Jingyan Shi of the Chinese Academy of Sciences for his continuous support,": "and Signal Processing (ICASSP) - Automatic speech emotion recognition using recurrent neural"
        },
        {
          "Thank you also to Jingyan Shi of the Chinese Academy of Sciences for his continuous support,": "9"
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "networks with local attention[C]// 2017:2227-2231.": "[19]Busso C, Bulut M, Lee C C, et al. IEMOCAP: interactive emotional dyadic motion capture"
        },
        {
          "networks with local attention[C]// 2017:2227-2231.": "database [J]. Language Resources and Evaluation, 2008, 42(4):335-359."
        },
        {
          "networks with local attention[C]// 2017:2227-2231.": "[20] Jackson P, Ul Haq S. Surrey Audio-Visual Expressed Emotion (SAVEE) database, 2011."
        },
        {
          "networks with local attention[C]// 2017:2227-2231.": "[21]Li Y, Tao J, Chao L, et al. CHEAVD: a Chinese natural emotional audio–visual database [J]."
        },
        {
          "networks with local attention[C]// 2017:2227-2231.": "Journal of Ambient Intelligence and Humanized Computing, 2016, 8(6)."
        }
      ],
      "page": 10
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Emotions and Speech: Some Acoustical Correlates [J]",
      "authors": [
        "C Williams",
        "K Stevens"
      ],
      "year": "1972",
      "venue": "The Journal of the Acoustical Society of America"
    },
    {
      "citation_id": "2",
      "title": "",
      "authors": [
        "R Picard"
      ],
      "venue": ""
    },
    {
      "citation_id": "3",
      "title": "DNN FOR ACOUSTIC EMOTION RECOGNITION: RAISING THE BENCHMARKS",
      "authors": [
        "André Stuhlsatz",
        "C Meyer",
        "F Eyben"
      ],
      "year": "2011",
      "venue": "DNN FOR ACOUSTIC EMOTION RECOGNITION: RAISING THE BENCHMARKS"
    },
    {
      "citation_id": "4",
      "title": "A first look into a Convolutional Neural Network for speech emotion detection",
      "authors": [
        "D Bertero",
        "P Fung"
      ],
      "year": "2017",
      "venue": "IEEE"
    },
    {
      "citation_id": "5",
      "title": "Emotion Recognition from Human Speech Using Temporal Information and Deep Learning",
      "authors": [
        "J Kim",
        "R Saurous"
      ],
      "venue": "Emotion Recognition from Human Speech Using Temporal Information and Deep Learning"
    },
    {
      "citation_id": "6",
      "title": "",
      "authors": [
        "Interspeech"
      ],
      "year": "2018",
      "venue": ""
    },
    {
      "citation_id": "7",
      "title": "Cross-language knowledge transfer using multilingual deep neural network with shared hidden layers",
      "authors": [
        "J Huang",
        "J Li",
        "D Yu"
      ],
      "year": "2013",
      "venue": "Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "8",
      "title": "Cross-corpus Acoustic Emotion Recognition with Multi-task Learning: Seeking Common Ground while Preserving Differences [J]",
      "authors": [
        "B Zhang",
        "Mower Provost",
        "E Essl"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "9",
      "title": "Toward detecting emotions in spoken dialogs",
      "authors": [
        "C M Lee",
        "S Narayanan"
      ],
      "year": "2005",
      "venue": "IEEE Transactions on Speech and Audio Processing"
    },
    {
      "citation_id": "10",
      "title": "Comparison of parametric representations for monosyllabic word recognition in continuously spoken sentences [J]",
      "authors": [
        "S Davis"
      ],
      "year": "1980",
      "venue": "IEEE Trans. Acoust. Speech Signal Process"
    },
    {
      "citation_id": "11",
      "title": "Speech Emotion Recognition based on a Hybrid of HMM/ANN",
      "authors": [
        "X I A Mao",
        "B Zhang",
        "Y Luo"
      ],
      "year": "2007",
      "venue": "International Journal of Computers"
    },
    {
      "citation_id": "12",
      "title": "Emotion recognition from Mandarin speech signals",
      "authors": [
        "T Pao",
        "Y T Chen",
        "J Yeh"
      ],
      "year": "2005",
      "venue": "Chinese Spoken Language Processing, 2004 International Symposium on"
    },
    {
      "citation_id": "13",
      "title": "Emotion recognition from Assamese speeches using MFCC features and GMM classifier[C]// TENCON 2008 -2008 IEEE Region 10 Conference",
      "authors": [
        "A Kandali",
        "A Routray",
        "T Basu"
      ],
      "year": "2008",
      "venue": "Emotion recognition from Assamese speeches using MFCC features and GMM classifier[C]// TENCON 2008 -2008 IEEE Region 10 Conference"
    },
    {
      "citation_id": "14",
      "title": "Emotion recognition in speech signal: experimental study, development, and application",
      "authors": [
        "V Petrushin"
      ],
      "year": "2000",
      "venue": "Emotion recognition in speech signal: experimental study, development, and application"
    },
    {
      "citation_id": "15",
      "title": "Recognition of negative emotions from the speech signal",
      "authors": [
        "C M Lee",
        "S Narayanan",
        "R Pieraccini"
      ],
      "year": "2001",
      "venue": "Automatic Speech Recognition and Understanding"
    },
    {
      "citation_id": "16",
      "title": "The Geneva Minimalistic Acoustic Parameter Set (GeMAPS) for Voice Research and Affective Computing [J]",
      "authors": [
        "Eybenf",
        "Scherer K R",
        "B Schuller"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "17",
      "title": "The Geneva Minimalistic Acoustic Parameter Set (GeMAPS) for Speech Research and Affective Computing [J]",
      "authors": [
        "Eybenf",
        "Scherer K R",
        "K P Truong"
      ],
      "year": "2016",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "18",
      "title": "Audio Set: An ontology and human-labeled dataset for audio events",
      "authors": [
        "J Gemmeke",
        "D P W Ellis",
        "D Freedman"
      ],
      "year": "2017",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "19",
      "title": "Automatic speech emotion recognition using recurrent neural networks with local attention",
      "authors": [
        "S Mirsamadi",
        "Barsoume",
        "C Zhang"
      ],
      "year": "2017",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "20",
      "title": "IEMOCAP: interactive emotional dyadic motion capture database [J]. Language Resources and Evaluation",
      "authors": [
        "C Busso",
        "M Bulut",
        "C C Lee"
      ],
      "year": "2008",
      "venue": "IEMOCAP: interactive emotional dyadic motion capture database [J]. Language Resources and Evaluation"
    },
    {
      "citation_id": "21",
      "title": "Audio-Visual Expressed Emotion (SAVEE) database",
      "authors": [
        "P Jackson",
        "Ul Haq",
        "S Surrey"
      ],
      "year": "2011",
      "venue": "Audio-Visual Expressed Emotion (SAVEE) database"
    },
    {
      "citation_id": "22",
      "title": "CHEAVD: a Chinese natural emotional audio-visual database [J]",
      "authors": [
        "Y Li",
        "J Tao",
        "L Chao"
      ],
      "year": "2016",
      "venue": "Journal of Ambient Intelligence and Humanized Computing"
    }
  ]
}