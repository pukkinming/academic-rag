{
  "paper_id": "2007.02038v1",
  "title": "Low Rank Fusion Based Transformers For Multimodal Sequences",
  "published": "2020-07-04T08:05:40Z",
  "authors": [
    "Saurav Sahay",
    "Eda Okur",
    "Shachi H Kumar",
    "Lama Nachman"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Our senses individually work in a coordinated fashion to express our emotional intentions. In this work, we experiment with modeling modality-specific sensory signals to attend to our latent multimodal emotional intentions and vice versa expressed via lowrank multimodal fusion and multimodal transformers. The low-rank factorization of multimodal fusion amongst the modalities helps represent approximate multiplicative latent signal interactions. Motivated by the work of  (Tsai et al., 2019)  and  (Liu et al., 2018) , we present our transformer-based cross-fusion architecture without any over-parameterization of the model. The low-rank fusion helps represent the latent signal interactions while the modality-specific attention helps focus on relevant parts of the signal. We present two methods for the Multimodal Sentiment and Emotion Recognition results on CMU-MOSEI, CMU-MOSI, and IEMOCAP datasets and show that our models have lesser parameters, train faster and perform comparably to many larger fusion-based architectures.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "The field of Emotion Understanding involves computational study of subjective elements such as sentiments, opinions, attitudes, and emotions towards other objects or persons. Subjectivity is an inherent part of emotion understanding that comes from the contextual nature of the natural phenomenon. Defining the metrics and disentangling the objective assessment of the metrics from the subjective signal makes the field quite challenging and exciting. Sentiments and Emotions are attached to the language, audio and visual modalities at different rates of expression and granularity and are useful in deriving social, psychological and behavioral insights about various entities such as movies, products, people or organizations. Emotions are defined as brief organically synchronized evaluations of major events whereas sentiments are considered as more enduring beliefs and dispositions towards objects or persons  (Scherer, 1984) . The field of Emotion Understanding has rich literature with many interesting models of understanding  (Plutchik, 2001; Ekman, 2009; Posner et al., 2005) . Recent studies on tensor-based multimodal fusion explore regularizing tensor representations  (Liang et al., 2019)  and polynomial tensor pooling  (Hou et al., 2019) .\n\nIn this work, we combine ideas from  (Tsai et al., 2019)  and  (Liu et al., 2018)  and explore the use of Transformer  (Vaswani et al., 2017)  based models for both aligned and unaligned signals without extensive over-parameterization of the models by using multiple modality-specific transformers. We utilize Low Rank Matrix Factorization (LMF) based fusion method for representing multimodal fusion of the modality-specific information. Our main contributions can be summarized as follows:\n\n• Recently proposed Multimodal Transformer (MulT) architecture  (Tsai et al., 2019)  uses at least 9 Transformer based models for crossmodal representation of language, audio and visual modalities (3 parallel modality-specific standard Transformers with self-attention and 6 parallel bimodal Transformers with crossmodal attention). These models utilize several parallel unimodal and bimodal transformers and do not capture the full trimodal signal interplay in any single transformer model in the architecture. In contrast, our method uses fewer Transformer based models and fewer parallel models for the same multimodal representation.\n\n• We look at two methods for leveraging the multimodal fusion into the transformer architecture. In one method (LMF-MulT), the fused multimodal signal is reinforced using arXiv:2007.02038v1 [cs.CL] 4 Jul 2020 The ability to use unaligned sequences for modeling is advantageous since we rely on learning based methods instead of using methods that force the signal synchronization (requiring extra timing information) to mimic the coordinated nature of human multimodal language expression. The LMF method aims to capture all unimodal, bimodal and trimodal interactions amongst the modalities via approximate Tensor Fusion method.\n\nWe develop and test our approaches on the CMU-MOSI, CMU-MOSEI, and IEMOCAP datasets as reported in  (Tsai et al., 2019) . CMU Multimodal Opinion Sentiment and Emotion Intensity (CMU-MOSEI)  (Zadeh et al., 2018)  is a large dataset of multimodal sentiment analysis and emotion recognition on YouTube video segments. The dataset contains more than 23,500 sentence utterance videos from more than 1000 online YouTube speakers. The dataset has several interesting properties such as being gender balanced, containing various topics and monologue videos from people with different personality traits. The videos are manually transcribed and properly punctuated. Since the dataset comprises of natural audio-visual opinionated expressions of the speakers, it provides an excellent test-bed for research in emotion and sentiment understanding. The videos are cut into continuous segments and the segments are annotated with 7 point scale sentiment labels and 4 point scale emotion categories corresponding to the Ekman's 6 basic emotion classes  (Ekman, 2002) . The opinionated expressions in the segments contain visual cues, audio variations in signal as well as textual expressions showing various subtle and non-obvious interactions across the modalities for both sentiment and emotion classification. CMU-MOSI  (Zadeh et al., 2016 ) is a smaller dataset (2199 clips) of YouTube videos with sentiment annotations. IEMOCAP  (Busso et al., 2008)  dataset consists of 10K videos with sentiment and emotion labels. We use the same setup as  (Tsai et al., 2019)  with 4 emotions (happy, sad, angry, neutral).\n\nIn",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Model Description",
      "text": "In this section, we describe our models and methods for Low Rank Fusion of the modalities for use with Multimodal Transformers with cross-modal attention.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Low Rank Fusion",
      "text": "LMF is a Tensor Fusion method that models the unimodal, bimodal and trimodal interactions without using an expensive 3-fold Cartesian product  (Zadeh et al., 2017)  from modality-specific embeddings. Instead, the method leverages unimodal features and weights directly to approximate the full multitensor outer product operation. This low-rank matrix factorization operation easily extends to problems where the interaction space (feature space or number of modalities) is very large. We utilize the method as described in  (Liu et al., 2018) . Similar to the prior work, we compress the time-series information of the individual modalities using an LSTM  (Hochreiter and Schmidhuber, 1997)  and extract the hidden state context vector for modalityspecific fusion. We depict the LMF method in Fig 2 similar to the illustration in  (Liu et al., 2018) . This shows how the unimodal tensor sequences are appended with 1s before taking the outer product to",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Multimodal Transformer",
      "text": "We build up on the Transformers  (Vaswani et al., 2017)  based sequence encoding and utilize the ideas from  (Tsai et al., 2019)  for multiple crossmodal attention blocks followed by self-attention for encoding multimodal sequences for classifi- cation. While the earlier work focuses on latent adaptation of one modality to another, we focus on adaptation of the latent multimodal signal itself using single-head cross-modal attention to individual modalities. This helps us reduce the excessive parameterization of the models by using all combinations of modality to modality cross-modal attention for each modality. Instead, we only utilize a linear number of cross-modal attention for each modality and the fused signal representation. We add Temporal Convolutions after the LMF operation to ensure that the input sequences have a sufficient awareness of the neighboring elements. We show the overall architecture of our two proposed models in Fig  3  and",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Experiments",
      "text": "We present our early experiments to evaluate the performance of proposed models on the standard multimodal datasets used by  (Tsai et al., 2019)   1  . We run our models on CMU-MOSI, CMU-MOSEI, and IEMOCAP datasets and present the results for the proposed LMF-MulT and Fusion-Based-CM-Attn-MulT models. Late Fusion (LF) LSTM is a common baseline for all datasets with reported results (pub) together with MulT in  (Tsai et al., 2019) . We include the results we obtain (our run) for the MulT model for a direct comparison 2 . Table 1, Table  2 , and Table  3  show the performance of various models on the sentiment analysis and emotion classification datasets. We do not observe any trend suggesting that our methods can achieve better accuracies or F1-scores than the original MulT method  (Tsai et al., 2019) . However, we do note   that on some occasions, our methods can achieve higher results than the MulT model, in both aligned (see LMF-MulT results for IEMOCAP in Table  3 ) and unaligned (see LMF-MulT results for CMU-MOSEI in Table  2 ) case. We plan to do an exhaustive grid search over the hyper-parameters to understand if our methods can learn to classify the multimodal signal better than the original competitive method. Although the results are comparable, below are the advantages of using our methods:\n\n• Our LMF-MulT model does not use multiple parallel self-attention transformers for the different modalities and it uses least number of transformers compared to the other two models. Given the same training infrastructure and resources, we observe a consistent speedup in training with this method. See Table  4  for average time per epoch in seconds measured with fixed batch sizes for all three models.\n\n• As summarized in Table  5 , we observe that our models use lesser number of trainable parameters compared to the MulT model, and yet achieve similar performance.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Conclusion",
      "text": "In this paper, we present our early investigations towards utilizing Low Rank representations of the multimodal sequences for usage in multimodal transformers with cross-modal attention to the fused signal or the modalities. Our methods build up on the  (Tsai et al., 2019)  work and apply transformers to fused multimodal signal that aim to capture all inter-modal signals via the Low Rank Matrix Factorization  (Liu et al., 2018) . This method is applicable to both aligned and unaligned sequences. Our methods train faster and use fewer parameters to learn classifiers with similar SOTA performance. We are exploring methods to compress the temporal sequences without using the hidden state context vectors from LSTMs that lose the temporal information. We recover the temporal information with a Convolution layer.\n\nWe believe these models can be deployed in low resource settings with further optimizations. We are also interested in using richer features for the audio, text, and the vision pipeline for other use-cases where we can utilize more resources.",
      "page_start": 5,
      "page_end": 5
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Modality-speciﬁc Fused Attention",
      "page": 2
    },
    {
      "caption": "Figure 1: , we illustrate our ideas by showing the",
      "page": 2
    },
    {
      "caption": "Figure 2: Low Rank Matrix Factorization",
      "page": 3
    },
    {
      "caption": "Figure 2: similar to the illustration in (Liu et al., 2018). This",
      "page": 3
    },
    {
      "caption": "Figure 3: Fused Cross-modal Transformer",
      "page": 3
    },
    {
      "caption": "Figure 2: ). As shown,",
      "page": 3
    },
    {
      "caption": "Figure 4: Low Rank Fusion Transformer",
      "page": 3
    },
    {
      "caption": "Figure 3: and Fig 4. In Fig 3, we show the fused multimodal",
      "page": 4
    },
    {
      "caption": "Figure 4: , we show the archi-",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Abstract": "",
          "as brief organically synchronized evaluations of": "major events whereas sentiments are considered as"
        },
        {
          "Abstract": "Our\nsenses\nindividually work\nin\na\ncoordi-",
          "as brief organically synchronized evaluations of": ""
        },
        {
          "Abstract": "",
          "as brief organically synchronized evaluations of": "more enduring beliefs and dispositions towards ob-"
        },
        {
          "Abstract": "nated fashion to express our emotional\ninten-",
          "as brief organically synchronized evaluations of": ""
        },
        {
          "Abstract": "",
          "as brief organically synchronized evaluations of": "jects or persons (Scherer, 1984). The ﬁeld of Emo-"
        },
        {
          "Abstract": "tions.\nIn this work, we experiment with mod-",
          "as brief organically synchronized evaluations of": ""
        },
        {
          "Abstract": "",
          "as brief organically synchronized evaluations of": "tion Understanding has rich literature with many in-"
        },
        {
          "Abstract": "eling modality-speciﬁc sensory signals\nto at-",
          "as brief organically synchronized evaluations of": ""
        },
        {
          "Abstract": "tend to our\nlatent multimodal\nemotional\nin-",
          "as brief organically synchronized evaluations of": "teresting models of understanding (Plutchik, 2001;"
        },
        {
          "Abstract": "tentions\nand\nvice\nversa\nexpressed\nvia\nlow-",
          "as brief organically synchronized evaluations of": "Ekman, 2009; Posner et al., 2005). Recent studies"
        },
        {
          "Abstract": "rank multimodal fusion and multimodal\ntrans-",
          "as brief organically synchronized evaluations of": ""
        },
        {
          "Abstract": "",
          "as brief organically synchronized evaluations of": "on tensor-based multimodal fusion explore regu-"
        },
        {
          "Abstract": "formers. The low-rank factorization of multi-",
          "as brief organically synchronized evaluations of": ""
        },
        {
          "Abstract": "",
          "as brief organically synchronized evaluations of": "larizing tensor representations\n(Liang et al., 2019)"
        },
        {
          "Abstract": "modal fusion amongst the modalities helps rep-",
          "as brief organically synchronized evaluations of": ""
        },
        {
          "Abstract": "",
          "as brief organically synchronized evaluations of": "and polynomial tensor pooling (Hou et al., 2019)."
        },
        {
          "Abstract": "resent approximate multiplicative latent signal",
          "as brief organically synchronized evaluations of": ""
        },
        {
          "Abstract": "",
          "as brief organically synchronized evaluations of": "In this work, we combine ideas from (Tsai et al.,"
        },
        {
          "Abstract": "interactions. Motivated by the work of\n(Tsai",
          "as brief organically synchronized evaluations of": ""
        },
        {
          "Abstract": "",
          "as brief organically synchronized evaluations of": "2019) and (Liu et al., 2018) and explore the use"
        },
        {
          "Abstract": "et al., 2019) and (Liu et al., 2018), we present",
          "as brief organically synchronized evaluations of": ""
        },
        {
          "Abstract": "our\ntransformer-based\ncross-fusion\narchitec-",
          "as brief organically synchronized evaluations of": "of Transformer (Vaswani et al., 2017) based mod-"
        },
        {
          "Abstract": "ture without any over-parameterization of the",
          "as brief organically synchronized evaluations of": "els for both aligned and unaligned signals with-"
        },
        {
          "Abstract": "model.\nThe\nlow-rank\nfusion\nhelps\nrepre-",
          "as brief organically synchronized evaluations of": "out extensive over-parameterization of the models"
        },
        {
          "Abstract": "sent\nthe\nlatent\nsignal\ninteractions while\nthe",
          "as brief organically synchronized evaluations of": ""
        },
        {
          "Abstract": "",
          "as brief organically synchronized evaluations of": "by using multiple modality-speciﬁc transformers."
        },
        {
          "Abstract": "modality-speciﬁc attention helps focus on rel-",
          "as brief organically synchronized evaluations of": ""
        },
        {
          "Abstract": "",
          "as brief organically synchronized evaluations of": "We utilize Low Rank Matrix Factorization (LMF)"
        },
        {
          "Abstract": "evant parts of the signal. We present two meth-",
          "as brief organically synchronized evaluations of": ""
        },
        {
          "Abstract": "",
          "as brief organically synchronized evaluations of": "based fusion method for representing multimodal"
        },
        {
          "Abstract": "ods\nfor\nthe Multimodal Sentiment and Emo-",
          "as brief organically synchronized evaluations of": ""
        },
        {
          "Abstract": "",
          "as brief organically synchronized evaluations of": "fusion of the modality-speciﬁc information. Our"
        },
        {
          "Abstract": "tion Recognition\nresults\non CMU-MOSEI,",
          "as brief organically synchronized evaluations of": ""
        },
        {
          "Abstract": "",
          "as brief organically synchronized evaluations of": "main contributions can be summarized as follows:"
        },
        {
          "Abstract": "CMU-MOSI,\nand\nIEMOCAP\ndatasets\nand",
          "as brief organically synchronized evaluations of": ""
        },
        {
          "Abstract": "show that our models have lesser parameters,",
          "as brief organically synchronized evaluations of": ""
        },
        {
          "Abstract": "",
          "as brief organically synchronized evaluations of": "• Recently proposed Multimodal Transformer"
        },
        {
          "Abstract": "train faster and perform comparably to many",
          "as brief organically synchronized evaluations of": ""
        },
        {
          "Abstract": "",
          "as brief organically synchronized evaluations of": "(MulT) architecture (Tsai et al., 2019) uses at"
        },
        {
          "Abstract": "larger fusion-based architectures.",
          "as brief organically synchronized evaluations of": ""
        },
        {
          "Abstract": "",
          "as brief organically synchronized evaluations of": "least 9 Transformer based models for cross-"
        },
        {
          "Abstract": "Introduction",
          "as brief organically synchronized evaluations of": "modal representation of language, audio and"
        },
        {
          "Abstract": "",
          "as brief organically synchronized evaluations of": "visual modalities (3 parallel modality-speciﬁc"
        },
        {
          "Abstract": "",
          "as brief organically synchronized evaluations of": ""
        },
        {
          "Abstract": "",
          "as brief organically synchronized evaluations of": "standard Transformers with self-attention and"
        },
        {
          "Abstract": "putational study of subjective elements such as sen-",
          "as brief organically synchronized evaluations of": ""
        },
        {
          "Abstract": "",
          "as brief organically synchronized evaluations of": "6 parallel bimodal Transformers with cross-"
        },
        {
          "Abstract": "timents, opinions, attitudes, and emotions towards",
          "as brief organically synchronized evaluations of": ""
        },
        {
          "Abstract": "",
          "as brief organically synchronized evaluations of": "modal attention). These models utilize several"
        },
        {
          "Abstract": "other objects or persons. Subjectivity is an inher-",
          "as brief organically synchronized evaluations of": ""
        },
        {
          "Abstract": "",
          "as brief organically synchronized evaluations of": "parallel unimodal and bimodal transformers"
        },
        {
          "Abstract": "ent part of emotion understanding that comes from",
          "as brief organically synchronized evaluations of": ""
        },
        {
          "Abstract": "",
          "as brief organically synchronized evaluations of": "and do not capture the full\ntrimodal signal"
        },
        {
          "Abstract": "the contextual nature of the natural phenomenon.",
          "as brief organically synchronized evaluations of": ""
        },
        {
          "Abstract": "",
          "as brief organically synchronized evaluations of": "interplay in any single transformer model in"
        },
        {
          "Abstract": "Deﬁning the metrics and disentangling the objec-",
          "as brief organically synchronized evaluations of": ""
        },
        {
          "Abstract": "",
          "as brief organically synchronized evaluations of": "the architecture. In contrast, our method uses"
        },
        {
          "Abstract": "tive assessment of the metrics from the subjective",
          "as brief organically synchronized evaluations of": ""
        },
        {
          "Abstract": "",
          "as brief organically synchronized evaluations of": "fewer Transformer based models and fewer"
        },
        {
          "Abstract": "signal makes the ﬁeld quite challenging and excit-",
          "as brief organically synchronized evaluations of": ""
        },
        {
          "Abstract": "",
          "as brief organically synchronized evaluations of": "parallel models for the same multimodal rep-"
        },
        {
          "Abstract": "ing. Sentiments and Emotions are attached to the",
          "as brief organically synchronized evaluations of": ""
        },
        {
          "Abstract": "",
          "as brief organically synchronized evaluations of": "resentation."
        },
        {
          "Abstract": "language, audio and visual modalities at different",
          "as brief organically synchronized evaluations of": ""
        },
        {
          "Abstract": "rates of expression and granularity and are use-",
          "as brief organically synchronized evaluations of": "• We look at\ntwo methods for\nleveraging the"
        },
        {
          "Abstract": "ful in deriving social, psychological and behavioral",
          "as brief organically synchronized evaluations of": "multimodal\nfusion into the transformer ar-"
        },
        {
          "Abstract": "insights about various entities such as movies, prod-",
          "as brief organically synchronized evaluations of": "chitecture.\nIn one method (LMF-MulT), the"
        },
        {
          "Abstract": "ucts, people or organizations. Emotions are deﬁned",
          "as brief organically synchronized evaluations of": "fused multimodal signal\nis reinforced using"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "train faster and perform comparably to many": ""
        },
        {
          "train faster and perform comparably to many": "larger fusion-based architectures."
        },
        {
          "train faster and perform comparably to many": ""
        },
        {
          "train faster and perform comparably to many": "1\nIntroduction"
        },
        {
          "train faster and perform comparably to many": ""
        },
        {
          "train faster and perform comparably to many": "The ﬁeld of Emotion Understanding involves com-"
        },
        {
          "train faster and perform comparably to many": ""
        },
        {
          "train faster and perform comparably to many": "putational study of subjective elements such as sen-"
        },
        {
          "train faster and perform comparably to many": ""
        },
        {
          "train faster and perform comparably to many": "timents, opinions, attitudes, and emotions towards"
        },
        {
          "train faster and perform comparably to many": ""
        },
        {
          "train faster and perform comparably to many": "other objects or persons. Subjectivity is an inher-"
        },
        {
          "train faster and perform comparably to many": ""
        },
        {
          "train faster and perform comparably to many": "ent part of emotion understanding that comes from"
        },
        {
          "train faster and perform comparably to many": ""
        },
        {
          "train faster and perform comparably to many": "the contextual nature of the natural phenomenon."
        },
        {
          "train faster and perform comparably to many": ""
        },
        {
          "train faster and perform comparably to many": "Deﬁning the metrics and disentangling the objec-"
        },
        {
          "train faster and perform comparably to many": ""
        },
        {
          "train faster and perform comparably to many": "tive assessment of the metrics from the subjective"
        },
        {
          "train faster and perform comparably to many": ""
        },
        {
          "train faster and perform comparably to many": "signal makes the ﬁeld quite challenging and excit-"
        },
        {
          "train faster and perform comparably to many": ""
        },
        {
          "train faster and perform comparably to many": "ing. Sentiments and Emotions are attached to the"
        },
        {
          "train faster and perform comparably to many": ""
        },
        {
          "train faster and perform comparably to many": "language, audio and visual modalities at different"
        },
        {
          "train faster and perform comparably to many": "rates of expression and granularity and are use-"
        },
        {
          "train faster and perform comparably to many": "ful in deriving social, psychological and behavioral"
        },
        {
          "train faster and perform comparably to many": "insights about various entities such as movies, prod-"
        },
        {
          "train faster and perform comparably to many": "ucts, people or organizations. Emotions are deﬁned"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Figure 1: Modality-speciﬁc Fused Attention": "attention from the 3 modalities.\nIn the other"
        },
        {
          "Figure 1: Modality-speciﬁc Fused Attention": "method (Fusion-Based-CM-Attn), the individ-"
        },
        {
          "Figure 1: Modality-speciﬁc Fused Attention": "ual modalities are reinforced in parallel via"
        },
        {
          "Figure 1: Modality-speciﬁc Fused Attention": "the fused signal."
        },
        {
          "Figure 1: Modality-speciﬁc Fused Attention": ""
        },
        {
          "Figure 1: Modality-speciﬁc Fused Attention": "The ability to use unaligned sequences for mod-"
        },
        {
          "Figure 1: Modality-speciﬁc Fused Attention": ""
        },
        {
          "Figure 1: Modality-speciﬁc Fused Attention": "eling is advantageous since we rely on learning"
        },
        {
          "Figure 1: Modality-speciﬁc Fused Attention": ""
        },
        {
          "Figure 1: Modality-speciﬁc Fused Attention": "based methods instead of using methods that force"
        },
        {
          "Figure 1: Modality-speciﬁc Fused Attention": ""
        },
        {
          "Figure 1: Modality-speciﬁc Fused Attention": "the signal synchronization (requiring extra timing"
        },
        {
          "Figure 1: Modality-speciﬁc Fused Attention": ""
        },
        {
          "Figure 1: Modality-speciﬁc Fused Attention": "information) to mimic the coordinated nature of"
        },
        {
          "Figure 1: Modality-speciﬁc Fused Attention": ""
        },
        {
          "Figure 1: Modality-speciﬁc Fused Attention": "human multimodal language expression. The LMF"
        },
        {
          "Figure 1: Modality-speciﬁc Fused Attention": ""
        },
        {
          "Figure 1: Modality-speciﬁc Fused Attention": "method aims to capture all unimodal, bimodal and"
        },
        {
          "Figure 1: Modality-speciﬁc Fused Attention": ""
        },
        {
          "Figure 1: Modality-speciﬁc Fused Attention": "trimodal\ninteractions amongst\nthe modalities via"
        },
        {
          "Figure 1: Modality-speciﬁc Fused Attention": ""
        },
        {
          "Figure 1: Modality-speciﬁc Fused Attention": "approximate Tensor Fusion method."
        },
        {
          "Figure 1: Modality-speciﬁc Fused Attention": ""
        },
        {
          "Figure 1: Modality-speciﬁc Fused Attention": "We develop and test our approaches on the CMU-"
        },
        {
          "Figure 1: Modality-speciﬁc Fused Attention": ""
        },
        {
          "Figure 1: Modality-speciﬁc Fused Attention": "MOSI, CMU-MOSEI,\nand IEMOCAP datasets"
        },
        {
          "Figure 1: Modality-speciﬁc Fused Attention": ""
        },
        {
          "Figure 1: Modality-speciﬁc Fused Attention": "as reported in (Tsai et al., 2019).\nCMU Multi-"
        },
        {
          "Figure 1: Modality-speciﬁc Fused Attention": ""
        },
        {
          "Figure 1: Modality-speciﬁc Fused Attention": "modal Opinion Sentiment and Emotion Intensity"
        },
        {
          "Figure 1: Modality-speciﬁc Fused Attention": ""
        },
        {
          "Figure 1: Modality-speciﬁc Fused Attention": "(CMU-MOSEI)\n(Zadeh et al., 2018)\nis a large"
        },
        {
          "Figure 1: Modality-speciﬁc Fused Attention": ""
        },
        {
          "Figure 1: Modality-speciﬁc Fused Attention": "dataset of multimodal sentiment analysis and emo-"
        },
        {
          "Figure 1: Modality-speciﬁc Fused Attention": ""
        },
        {
          "Figure 1: Modality-speciﬁc Fused Attention": "tion recognition on YouTube video segments. The"
        },
        {
          "Figure 1: Modality-speciﬁc Fused Attention": ""
        },
        {
          "Figure 1: Modality-speciﬁc Fused Attention": "dataset contains more than 23,500 sentence utter-"
        },
        {
          "Figure 1: Modality-speciﬁc Fused Attention": ""
        },
        {
          "Figure 1: Modality-speciﬁc Fused Attention": "ance videos from more than 1000 online YouTube"
        },
        {
          "Figure 1: Modality-speciﬁc Fused Attention": "speakers. The dataset has several interesting prop-"
        },
        {
          "Figure 1: Modality-speciﬁc Fused Attention": "erties such as being gender balanced, containing"
        },
        {
          "Figure 1: Modality-speciﬁc Fused Attention": ""
        },
        {
          "Figure 1: Modality-speciﬁc Fused Attention": "various topics and monologue videos from peo-"
        },
        {
          "Figure 1: Modality-speciﬁc Fused Attention": ""
        },
        {
          "Figure 1: Modality-speciﬁc Fused Attention": "ple with different personality traits. The videos"
        },
        {
          "Figure 1: Modality-speciﬁc Fused Attention": ""
        },
        {
          "Figure 1: Modality-speciﬁc Fused Attention": "are manually transcribed and properly punctuated."
        },
        {
          "Figure 1: Modality-speciﬁc Fused Attention": ""
        },
        {
          "Figure 1: Modality-speciﬁc Fused Attention": "Since the dataset comprises of natural audio-visual"
        },
        {
          "Figure 1: Modality-speciﬁc Fused Attention": "opinionated expressions of the speakers, it provides"
        },
        {
          "Figure 1: Modality-speciﬁc Fused Attention": ""
        },
        {
          "Figure 1: Modality-speciﬁc Fused Attention": "an excellent test-bed for research in emotion and"
        },
        {
          "Figure 1: Modality-speciﬁc Fused Attention": "sentiment understanding. The videos are cut into"
        },
        {
          "Figure 1: Modality-speciﬁc Fused Attention": "continuous segments and the segments are anno-"
        },
        {
          "Figure 1: Modality-speciﬁc Fused Attention": "tated with 7 point\nscale sentiment\nlabels and 4"
        },
        {
          "Figure 1: Modality-speciﬁc Fused Attention": "point scale emotion categories corresponding to the"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Figure 2: Low Rank Matrix Factorization": "Instead,\nthe method leverages unimodal features"
        },
        {
          "Figure 2: Low Rank Matrix Factorization": "and weights directly to approximate the full multi-"
        },
        {
          "Figure 2: Low Rank Matrix Factorization": "tensor outer product operation. This low-rank ma-"
        },
        {
          "Figure 2: Low Rank Matrix Factorization": "trix factorization operation easily extends to prob-"
        },
        {
          "Figure 2: Low Rank Matrix Factorization": "lems where the interaction space (feature space or"
        },
        {
          "Figure 2: Low Rank Matrix Factorization": "number of modalities) is very large. We utilize the"
        },
        {
          "Figure 2: Low Rank Matrix Factorization": "method as described in (Liu et al., 2018).\nSimi-"
        },
        {
          "Figure 2: Low Rank Matrix Factorization": "lar to the prior work, we compress the time-series"
        },
        {
          "Figure 2: Low Rank Matrix Factorization": "information of the individual modalities using an"
        },
        {
          "Figure 2: Low Rank Matrix Factorization": "LSTM (Hochreiter and Schmidhuber, 1997) and"
        },
        {
          "Figure 2: Low Rank Matrix Factorization": "extract the hidden state context vector for modality-"
        },
        {
          "Figure 2: Low Rank Matrix Factorization": ""
        },
        {
          "Figure 2: Low Rank Matrix Factorization": "speciﬁc fusion. We depict the LMF method in Fig 2"
        },
        {
          "Figure 2: Low Rank Matrix Factorization": ""
        },
        {
          "Figure 2: Low Rank Matrix Factorization": "similar to the illustration in (Liu et al., 2018). This"
        },
        {
          "Figure 2: Low Rank Matrix Factorization": ""
        },
        {
          "Figure 2: Low Rank Matrix Factorization": "shows how the unimodal tensor sequences are ap-"
        },
        {
          "Figure 2: Low Rank Matrix Factorization": ""
        },
        {
          "Figure 2: Low Rank Matrix Factorization": "pended with 1s before taking the outer product to"
        },
        {
          "Figure 2: Low Rank Matrix Factorization": ""
        }
      ],
      "page": 3
    },
    {
      "caption": "Table 1: Performance Results for Multimodal Sentiment Analysis on CMU-MOSI dataset with aligned and un-",
      "data": [
        {
          "Metric": "",
          "Acch": "",
          "F1h": "",
          "MAEl": "",
          "Corrh": ""
        },
        {
          "Metric": "LF-LSTM (pub)",
          "Acch": "76.8",
          "F1h": "76.7",
          "MAEl": "1.015",
          "Corrh": "0.625"
        },
        {
          "Metric": "MulT (Tsai et al., 2019) (pub)",
          "Acch": "83.0",
          "F1h": "82.8",
          "MAEl": "0.871",
          "Corrh": "0.698"
        },
        {
          "Metric": "MulT (Tsai et al., 2019) (our run)",
          "Acch": "78.5",
          "F1h": "78.4",
          "MAEl": "0.991",
          "Corrh": "0.676"
        },
        {
          "Metric": "Fusion-Based-CM-Attn-MulT (ours)",
          "Acch": "77.0",
          "F1h": "76.9",
          "MAEl": "1.017",
          "Corrh": "0.636"
        },
        {
          "Metric": "LMF-MulT (ours)",
          "Acch": "77.9",
          "F1h": "77.9",
          "MAEl": "1.016",
          "Corrh": "0.647"
        },
        {
          "Metric": "",
          "Acch": "",
          "F1h": "",
          "MAEl": "",
          "Corrh": ""
        },
        {
          "Metric": "LF-LSTM (pub)",
          "Acch": "77.6",
          "F1h": "77.8",
          "MAEl": "0.988",
          "Corrh": "0.624"
        },
        {
          "Metric": "MulT (Tsai et al., 2019) (pub)",
          "Acch": "81.1",
          "F1h": "81.0",
          "MAEl": "0.889",
          "Corrh": "0.686"
        },
        {
          "Metric": "MulT (Tsai et al., 2019) (our run)",
          "Acch": "80.3",
          "F1h": "80.4",
          "MAEl": "1.008",
          "Corrh": "0.645"
        },
        {
          "Metric": "Fusion-Based-CM-Attn-MulT (ours)",
          "Acch": "76.8",
          "F1h": "76.8",
          "MAEl": "1.003",
          "Corrh": "0.640"
        },
        {
          "Metric": "LMF-MulT (ours)",
          "Acch": "78.5",
          "F1h": "78.5",
          "MAEl": "0.957",
          "Corrh": "0.681"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 1: Performance Results for Multimodal Sentiment Analysis on CMU-MOSI dataset with aligned and un-",
      "data": [
        {
          "Table 2:": "",
          "Performance Results": "aligned and unaligned multimodal sequences.",
          "for Multimodal Sentiment Analysis on larger-scale CMU-MOSEI dataset with": ""
        },
        {
          "Table 2:": "",
          "Performance Results": "cation. While the earlier work focuses on latent",
          "for Multimodal Sentiment Analysis on larger-scale CMU-MOSEI dataset with": "3\nExperiments"
        },
        {
          "Table 2:": "",
          "Performance Results": "adaptation of one modality to another, we focus on",
          "for Multimodal Sentiment Analysis on larger-scale CMU-MOSEI dataset with": ""
        },
        {
          "Table 2:": "",
          "Performance Results": "",
          "for Multimodal Sentiment Analysis on larger-scale CMU-MOSEI dataset with": "We present our early experiments to evaluate the"
        },
        {
          "Table 2:": "",
          "Performance Results": "adaptation of the latent multimodal signal itself us-",
          "for Multimodal Sentiment Analysis on larger-scale CMU-MOSEI dataset with": ""
        },
        {
          "Table 2:": "",
          "Performance Results": "",
          "for Multimodal Sentiment Analysis on larger-scale CMU-MOSEI dataset with": "performance of proposed models on the standard"
        },
        {
          "Table 2:": "",
          "Performance Results": "ing single-head cross-modal attention to individual",
          "for Multimodal Sentiment Analysis on larger-scale CMU-MOSEI dataset with": ""
        },
        {
          "Table 2:": "",
          "Performance Results": "",
          "for Multimodal Sentiment Analysis on larger-scale CMU-MOSEI dataset with": "multimodal datasets used by (Tsai et al., 2019)1."
        },
        {
          "Table 2:": "",
          "Performance Results": "modalities. This helps us reduce the excessive pa-",
          "for Multimodal Sentiment Analysis on larger-scale CMU-MOSEI dataset with": ""
        },
        {
          "Table 2:": "",
          "Performance Results": "",
          "for Multimodal Sentiment Analysis on larger-scale CMU-MOSEI dataset with": "We run our models on CMU-MOSI, CMU-MOSEI,"
        },
        {
          "Table 2:": "",
          "Performance Results": "rameterization of the models by using all combina-",
          "for Multimodal Sentiment Analysis on larger-scale CMU-MOSEI dataset with": ""
        },
        {
          "Table 2:": "",
          "Performance Results": "",
          "for Multimodal Sentiment Analysis on larger-scale CMU-MOSEI dataset with": "and IEMOCAP datasets and present the results for"
        },
        {
          "Table 2:": "",
          "Performance Results": "tions of modality to modality cross-modal attention",
          "for Multimodal Sentiment Analysis on larger-scale CMU-MOSEI dataset with": ""
        },
        {
          "Table 2:": "",
          "Performance Results": "",
          "for Multimodal Sentiment Analysis on larger-scale CMU-MOSEI dataset with": "the proposed LMF-MulT and Fusion-Based-CM-"
        },
        {
          "Table 2:": "",
          "Performance Results": "for each modality. Instead, we only utilize a linear",
          "for Multimodal Sentiment Analysis on larger-scale CMU-MOSEI dataset with": ""
        },
        {
          "Table 2:": "",
          "Performance Results": "",
          "for Multimodal Sentiment Analysis on larger-scale CMU-MOSEI dataset with": "Attn-MulT models.\nLate Fusion (LF) LSTM is"
        },
        {
          "Table 2:": "",
          "Performance Results": "number of cross-modal attention for each modality",
          "for Multimodal Sentiment Analysis on larger-scale CMU-MOSEI dataset with": ""
        },
        {
          "Table 2:": "",
          "Performance Results": "",
          "for Multimodal Sentiment Analysis on larger-scale CMU-MOSEI dataset with": "a common baseline for all datasets with reported"
        },
        {
          "Table 2:": "",
          "Performance Results": "and the fused signal representation. We add Tempo-",
          "for Multimodal Sentiment Analysis on larger-scale CMU-MOSEI dataset with": ""
        },
        {
          "Table 2:": "",
          "Performance Results": "",
          "for Multimodal Sentiment Analysis on larger-scale CMU-MOSEI dataset with": "results (pub)\ntogether with MulT in (Tsai et al.,"
        },
        {
          "Table 2:": "",
          "Performance Results": "ral Convolutions after the LMF operation to ensure",
          "for Multimodal Sentiment Analysis on larger-scale CMU-MOSEI dataset with": ""
        },
        {
          "Table 2:": "",
          "Performance Results": "",
          "for Multimodal Sentiment Analysis on larger-scale CMU-MOSEI dataset with": "2019). We include the results we obtain (our run)"
        },
        {
          "Table 2:": "",
          "Performance Results": "that the input sequences have a sufﬁcient awareness",
          "for Multimodal Sentiment Analysis on larger-scale CMU-MOSEI dataset with": ""
        },
        {
          "Table 2:": "",
          "Performance Results": "",
          "for Multimodal Sentiment Analysis on larger-scale CMU-MOSEI dataset with": "for the MulT model for a direct comparison2. Ta-"
        },
        {
          "Table 2:": "",
          "Performance Results": "of the neighboring elements. We show the overall",
          "for Multimodal Sentiment Analysis on larger-scale CMU-MOSEI dataset with": ""
        },
        {
          "Table 2:": "",
          "Performance Results": "",
          "for Multimodal Sentiment Analysis on larger-scale CMU-MOSEI dataset with": "ble 1, Table 2, and Table 3 show the performance of"
        },
        {
          "Table 2:": "",
          "Performance Results": "architecture of our two proposed models in Fig 3",
          "for Multimodal Sentiment Analysis on larger-scale CMU-MOSEI dataset with": ""
        },
        {
          "Table 2:": "",
          "Performance Results": "",
          "for Multimodal Sentiment Analysis on larger-scale CMU-MOSEI dataset with": "various models on the sentiment analysis and emo-"
        },
        {
          "Table 2:": "",
          "Performance Results": "and Fig 4. In Fig 3, we show the fused multimodal",
          "for Multimodal Sentiment Analysis on larger-scale CMU-MOSEI dataset with": ""
        },
        {
          "Table 2:": "",
          "Performance Results": "",
          "for Multimodal Sentiment Analysis on larger-scale CMU-MOSEI dataset with": "tion classiﬁcation datasets. We do not observe any"
        },
        {
          "Table 2:": "",
          "Performance Results": "signal representation after a temporal convolution",
          "for Multimodal Sentiment Analysis on larger-scale CMU-MOSEI dataset with": ""
        },
        {
          "Table 2:": "",
          "Performance Results": "",
          "for Multimodal Sentiment Analysis on larger-scale CMU-MOSEI dataset with": "trend suggesting that our methods can achieve bet-"
        },
        {
          "Table 2:": "",
          "Performance Results": "to enrich the individual modalities via cross-modal",
          "for Multimodal Sentiment Analysis on larger-scale CMU-MOSEI dataset with": ""
        },
        {
          "Table 2:": "",
          "Performance Results": "",
          "for Multimodal Sentiment Analysis on larger-scale CMU-MOSEI dataset with": "ter accuracies or F1-scores than the original MulT"
        },
        {
          "Table 2:": "",
          "Performance Results": "transformer attention. In Fig 4, we show the archi-",
          "for Multimodal Sentiment Analysis on larger-scale CMU-MOSEI dataset with": ""
        },
        {
          "Table 2:": "",
          "Performance Results": "",
          "for Multimodal Sentiment Analysis on larger-scale CMU-MOSEI dataset with": "method (Tsai et al., 2019). However, we do note"
        },
        {
          "Table 2:": "",
          "Performance Results": "tecture with the least number of Transformer layers",
          "for Multimodal Sentiment Analysis on larger-scale CMU-MOSEI dataset with": ""
        },
        {
          "Table 2:": "where the individual modalities attend to the fused",
          "Performance Results": "",
          "for Multimodal Sentiment Analysis on larger-scale CMU-MOSEI dataset with": ""
        },
        {
          "Table 2:": "",
          "Performance Results": "",
          "for Multimodal Sentiment Analysis on larger-scale CMU-MOSEI dataset with": "1We have built\nthis work up on the code-base released"
        },
        {
          "Table 2:": "convoluted multimodal signal.",
          "Performance Results": "",
          "for Multimodal Sentiment Analysis on larger-scale CMU-MOSEI dataset with": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 3: Performance Results for Multimodal Emotion Recognition on IEMOCAP dataset with aligned and un-",
      "data": [
        {
          "Emotion": "Metric",
          "Happy": "Acch",
          "Sad": "Acch",
          "Angry": "Acch",
          "Neutral": "Acch"
        },
        {
          "Emotion": "",
          "Happy": "(Aligned) IEMOCAP Emotions",
          "Sad": "",
          "Angry": "",
          "Neutral": ""
        },
        {
          "Emotion": "LF-LSTM (pub)",
          "Happy": "85.1",
          "Sad": "78.9",
          "Angry": "84.7",
          "Neutral": "67.1"
        },
        {
          "Emotion": "MulT (Tsai et al., 2019) (pub)",
          "Happy": "90.7",
          "Sad": "86.7",
          "Angry": "87.4",
          "Neutral": "72.4"
        },
        {
          "Emotion": "MulT (Tsai et al., 2019) (our run)",
          "Happy": "86.4",
          "Sad": "82.3",
          "Angry": "85.3",
          "Neutral": "71.2"
        },
        {
          "Emotion": "Fusion-Based-CM-Attn-MulT (ours)",
          "Happy": "85.6",
          "Sad": "83.6",
          "Angry": "84.6",
          "Neutral": "70.4"
        },
        {
          "Emotion": "LMF-MulT (ours)",
          "Happy": "85.3",
          "Sad": "84.1",
          "Angry": "85.7",
          "Neutral": "71.2"
        },
        {
          "Emotion": "",
          "Happy": "(Unaligned) IEMOCAP Emotions",
          "Sad": "",
          "Angry": "",
          "Neutral": ""
        },
        {
          "Emotion": "LF-LSTM (pub)",
          "Happy": "72.5",
          "Sad": "72.9",
          "Angry": "68.6",
          "Neutral": "59.6"
        },
        {
          "Emotion": "MulT (Tsai et al., 2019) (pub)",
          "Happy": "84.8",
          "Sad": "77.7",
          "Angry": "73.9",
          "Neutral": "62.5"
        },
        {
          "Emotion": "MulT (Tsai et al., 2019) (our run)",
          "Happy": "85.6",
          "Sad": "79.4",
          "Angry": "75.8",
          "Neutral": "59.2"
        },
        {
          "Emotion": "Fusion-Based-CM-Attn-MulT (ours)",
          "Happy": "85.6",
          "Sad": "79.4",
          "Angry": "75.8",
          "Neutral": "59.3"
        },
        {
          "Emotion": "LMF-MulT (ours)",
          "Happy": "85.6",
          "Sad": "79.4",
          "Angry": "75.8",
          "Neutral": "59.2"
        },
        {
          "Emotion": "Table 3: Performance Results for Multimodal Emotion Recognition on IEMOCAP dataset with aligned and un-",
          "Happy": "",
          "Sad": "",
          "Angry": "",
          "Neutral": ""
        },
        {
          "Emotion": "aligned multimodal sequences.",
          "Happy": "",
          "Sad": "",
          "Angry": "",
          "Neutral": ""
        }
      ],
      "page": 5
    },
    {
      "caption": "Table 3: Performance Results for Multimodal Emotion Recognition on IEMOCAP dataset with aligned and un-",
      "data": [
        {
          "Table 3: Performance Results for Multimodal Emotion Recognition on IEMOCAP dataset with aligned and un-": "aligned multimodal sequences."
        },
        {
          "Table 3: Performance Results for Multimodal Emotion Recognition on IEMOCAP dataset with aligned and un-": "Dataset"
        },
        {
          "Table 3: Performance Results for Multimodal Emotion Recognition on IEMOCAP dataset with aligned and un-": "Model"
        },
        {
          "Table 3: Performance Results for Multimodal Emotion Recognition on IEMOCAP dataset with aligned and un-": "MulT (Tsai et al., 2019)"
        },
        {
          "Table 3: Performance Results for Multimodal Emotion Recognition on IEMOCAP dataset with aligned and un-": "Fusion-Based-CM-Attn (ours)"
        },
        {
          "Table 3: Performance Results for Multimodal Emotion Recognition on IEMOCAP dataset with aligned and un-": "LMF-MulT (ours)"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table 3: Performance Results for Multimodal Emotion Recognition on IEMOCAP dataset with aligned and un-",
      "data": [
        {
          "LMF-MulT (ours)": "",
          "836121\n855441\n856078": "Table 5: Number of Model Parameters"
        },
        {
          "LMF-MulT (ours)": "",
          "836121\n855441\n856078": "4\nConclusion"
        },
        {
          "LMF-MulT (ours)": "",
          "836121\n855441\n856078": ""
        },
        {
          "LMF-MulT (ours)": "",
          "836121\n855441\n856078": ""
        },
        {
          "LMF-MulT (ours)": "",
          "836121\n855441\n856078": ""
        },
        {
          "LMF-MulT (ours)": "",
          "836121\n855441\n856078": ""
        },
        {
          "LMF-MulT (ours)": "",
          "836121\n855441\n856078": ""
        },
        {
          "LMF-MulT (ours)": "in Table 2) case. We plan to do an ex-",
          "836121\n855441\n856078": ""
        },
        {
          "LMF-MulT (ours)": "",
          "836121\n855441\n856078": "multimodal\nsequences"
        },
        {
          "LMF-MulT (ours)": "",
          "836121\n855441\n856078": ""
        },
        {
          "LMF-MulT (ours)": "",
          "836121\n855441\n856078": "transformers with cross-modal"
        },
        {
          "LMF-MulT (ours)": "",
          "836121\n855441\n856078": ""
        },
        {
          "LMF-MulT (ours)": "",
          "836121\n855441\n856078": "fused\nsignal\nor\nthe modalities."
        },
        {
          "LMF-MulT (ours)": "",
          "836121\n855441\n856078": ""
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "unaligned multimodal\nlanguage sequences.\nIn Pro-": "ceedings of the 57th Annual Meeting of the Associa-"
        },
        {
          "unaligned multimodal\nlanguage sequences.\nIn Pro-": "tion for Computational Linguistics (Volume 1: Long"
        },
        {
          "unaligned multimodal\nlanguage sequences.\nIn Pro-": "Papers), Florence,\nItaly. Association for Computa-"
        },
        {
          "unaligned multimodal\nlanguage sequences.\nIn Pro-": ""
        },
        {
          "unaligned multimodal\nlanguage sequences.\nIn Pro-": "tional Linguistics."
        },
        {
          "unaligned multimodal\nlanguage sequences.\nIn Pro-": ""
        },
        {
          "unaligned multimodal\nlanguage sequences.\nIn Pro-": "Ashish Vaswani, Noam Shazeer, Niki Parmar,\nJakob"
        },
        {
          "unaligned multimodal\nlanguage sequences.\nIn Pro-": ""
        },
        {
          "unaligned multimodal\nlanguage sequences.\nIn Pro-": "Uszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz"
        },
        {
          "unaligned multimodal\nlanguage sequences.\nIn Pro-": ""
        },
        {
          "unaligned multimodal\nlanguage sequences.\nIn Pro-": "Kaiser, and Illia Polosukhin. 2017. Attention is all"
        },
        {
          "unaligned multimodal\nlanguage sequences.\nIn Pro-": ""
        },
        {
          "unaligned multimodal\nlanguage sequences.\nIn Pro-": "you need.\nIn I. Guyon, U. V. Luxburg, S. Bengio,"
        },
        {
          "unaligned multimodal\nlanguage sequences.\nIn Pro-": ""
        },
        {
          "unaligned multimodal\nlanguage sequences.\nIn Pro-": "H. Wallach, R. Fergus, S. Vishwanathan, and R. Gar-"
        },
        {
          "unaligned multimodal\nlanguage sequences.\nIn Pro-": ""
        },
        {
          "unaligned multimodal\nlanguage sequences.\nIn Pro-": "Information Pro-\nnett, editors, Advances in Neural"
        },
        {
          "unaligned multimodal\nlanguage sequences.\nIn Pro-": "cessing Systems 30, pages 5998–6008. Curran Asso-"
        },
        {
          "unaligned multimodal\nlanguage sequences.\nIn Pro-": ""
        },
        {
          "unaligned multimodal\nlanguage sequences.\nIn Pro-": "ciates, Inc."
        },
        {
          "unaligned multimodal\nlanguage sequences.\nIn Pro-": ""
        },
        {
          "unaligned multimodal\nlanguage sequences.\nIn Pro-": "Amir Zadeh, Minghai Chen, Soujanya Poria, Erik Cam-"
        },
        {
          "unaligned multimodal\nlanguage sequences.\nIn Pro-": ""
        },
        {
          "unaligned multimodal\nlanguage sequences.\nIn Pro-": "bria,\nand Louis-Philippe Morency. 2017.\nTensor"
        },
        {
          "unaligned multimodal\nlanguage sequences.\nIn Pro-": ""
        },
        {
          "unaligned multimodal\nlanguage sequences.\nIn Pro-": "fusion network for multimodal\nsentiment analysis."
        },
        {
          "unaligned multimodal\nlanguage sequences.\nIn Pro-": ""
        },
        {
          "unaligned multimodal\nlanguage sequences.\nIn Pro-": "CoRR, abs/1707.07250."
        },
        {
          "unaligned multimodal\nlanguage sequences.\nIn Pro-": ""
        },
        {
          "unaligned multimodal\nlanguage sequences.\nIn Pro-": "Amir Zadeh, Paul Pu Liang, Jon Vanbriesen, Soujanya"
        },
        {
          "unaligned multimodal\nlanguage sequences.\nIn Pro-": ""
        },
        {
          "unaligned multimodal\nlanguage sequences.\nIn Pro-": "Poria, Erik Cambria, Minghai Chen,\nand Louis-"
        },
        {
          "unaligned multimodal\nlanguage sequences.\nIn Pro-": ""
        },
        {
          "unaligned multimodal\nlanguage sequences.\nIn Pro-": "Philippe Morency. 2018. Multimodal language anal-"
        },
        {
          "unaligned multimodal\nlanguage sequences.\nIn Pro-": "ysis\nin\nthe wild:\nCmu-mosei\ndataset\nand\ninter-"
        },
        {
          "unaligned multimodal\nlanguage sequences.\nIn Pro-": ""
        },
        {
          "unaligned multimodal\nlanguage sequences.\nIn Pro-": "pretable dynamic fusion graph.\nIn Association for"
        },
        {
          "unaligned multimodal\nlanguage sequences.\nIn Pro-": ""
        },
        {
          "unaligned multimodal\nlanguage sequences.\nIn Pro-": "Computational Linguistics (ACL)."
        },
        {
          "unaligned multimodal\nlanguage sequences.\nIn Pro-": ""
        },
        {
          "unaligned multimodal\nlanguage sequences.\nIn Pro-": ""
        },
        {
          "unaligned multimodal\nlanguage sequences.\nIn Pro-": "Amir Zadeh, Rowan Zellers, Eli Pincus,\nand Louis-"
        },
        {
          "unaligned multimodal\nlanguage sequences.\nIn Pro-": ""
        },
        {
          "unaligned multimodal\nlanguage sequences.\nIn Pro-": "Philippe Morency. 2016. Multimodal sentiment\nin-"
        },
        {
          "unaligned multimodal\nlanguage sequences.\nIn Pro-": "tensity analysis in videos: Facial gestures and verbal"
        },
        {
          "unaligned multimodal\nlanguage sequences.\nIn Pro-": ""
        },
        {
          "unaligned multimodal\nlanguage sequences.\nIn Pro-": "messages.\nIEEE Intelligent Systems, 31(6):82–88."
        },
        {
          "unaligned multimodal\nlanguage sequences.\nIn Pro-": ""
        }
      ],
      "page": 6
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Iemocap: interactive emotional dyadic motion capture database",
      "authors": [
        "Carlos Busso",
        "Murtaza Bulut",
        "Chi-Chun Lee",
        "Abe Kazemzadeh",
        "Emily Mower",
        "Samuel Kim",
        "Jeannette Chang",
        "Sungbok Lee",
        "Shrikanth Narayanan"
      ],
      "year": "2008",
      "venue": "Language Resources and Evaluation",
      "doi": "10.1007/s10579-008-9076-6"
    },
    {
      "citation_id": "2",
      "title": "Facial action coding system (facs). A Human Face",
      "authors": [
        "Paul Ekman"
      ],
      "year": "2002",
      "venue": "Facial action coding system (facs). A Human Face"
    },
    {
      "citation_id": "3",
      "title": "Telling Lies: Clues to Deceit in the Marketplace, Politics, and Marriage",
      "authors": [
        "Paul Ekman"
      ],
      "year": "2009",
      "venue": "Telling Lies: Clues to Deceit in the Marketplace, Politics, and Marriage"
    },
    {
      "citation_id": "4",
      "title": "Long short-term memory",
      "authors": [
        "Sepp Hochreiter",
        "Jrgen Schmidhuber"
      ],
      "year": "1997",
      "venue": "Neural Computation",
      "doi": "10.1162/neco.1997.9.8.1735"
    },
    {
      "citation_id": "5",
      "title": "Deep multimodal multilinear fusion with high-order polynomial pooling",
      "authors": [
        "Ming Hou",
        "Jiajia Tang",
        "Jianhai Zhang",
        "Wanzeng Kong",
        "Qibin Zhao"
      ],
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "6",
      "title": "Learning representations from imperfect time series data via tensor rank regularization",
      "authors": [
        "Paul Pu Liang",
        "Zhun Liu",
        "Yao-Hung Hubert Tsai",
        "Qibin Zhao",
        "Ruslan Salakhutdinov",
        "Louis-Philippe Morency"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
      "doi": "10.18653/v1/P19-1152"
    },
    {
      "citation_id": "7",
      "title": "Efficient lowrank multimodal fusion with modality-specific factors",
      "authors": [
        "Zhun Liu",
        "Varun Shen",
        "Paul Bharadhwaj Lakshminarasimhan",
        "Amirali Liang",
        "Louis-Philippe Bagher Zadeh",
        "Morency"
      ],
      "year": "2018",
      "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics",
      "doi": "10.18653/v1/p18-1209"
    },
    {
      "citation_id": "8",
      "title": "The nature of emotions human emotions have deep evolutionary roots, a fact that may explain their complexity and provide tools for clinical practice",
      "authors": [
        "Robert Plutchik"
      ],
      "year": "2001",
      "venue": "American Scientist"
    },
    {
      "citation_id": "9",
      "title": "The circumplex model of affect: An integrative approach to affective neuroscience, cognitive development, and psychopathology. Development and psychopathology",
      "authors": [
        "Jonathan Posner",
        "James Russell",
        "Bradley Peterson"
      ],
      "year": "2005",
      "venue": "The circumplex model of affect: An integrative approach to affective neuroscience, cognitive development, and psychopathology. Development and psychopathology"
    },
    {
      "citation_id": "10",
      "title": "Emotion as a multicomponent process: A model and some cross-cultural data",
      "authors": [
        "Klaus Scherer"
      ],
      "year": "1984",
      "venue": "Emotion as a multicomponent process: A model and some cross-cultural data"
    },
    {
      "citation_id": "11",
      "title": "Multimodal transformer for unaligned multimodal language sequences",
      "authors": [
        "Yao-Hung Hubert Tsai",
        "Shaojie Bai",
        "Paul Pu Liang",
        "J Kolter",
        "Louis-Philippe Morency",
        "Ruslan Salakhutdinov"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "12",
      "title": "Attention is all you need",
      "authors": [
        "Ashish Vaswani",
        "Noam Shazeer",
        "Niki Parmar",
        "Jakob Uszkoreit",
        "Llion Jones",
        "Aidan Gomez",
        "Ł Ukasz Kaiser",
        "Illia Polosukhin"
      ],
      "year": "2017",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "13",
      "title": "Tensor fusion network for multimodal sentiment analysis",
      "authors": [
        "Amir Zadeh",
        "Minghai Chen",
        "Soujanya Poria",
        "Erik Cambria",
        "Louis-Philippe Morency"
      ],
      "year": "2017",
      "venue": "Tensor fusion network for multimodal sentiment analysis"
    },
    {
      "citation_id": "14",
      "title": "Multimodal language analysis in the wild: Cmu-mosei dataset and interpretable dynamic fusion graph",
      "authors": [
        "Amir Zadeh",
        "Paul Liang",
        "Jon Vanbriesen",
        "Soujanya Poria",
        "Erik Cambria",
        "Minghai Chen",
        "Louis-Philippe Morency"
      ],
      "year": "2018",
      "venue": "Multimodal language analysis in the wild: Cmu-mosei dataset and interpretable dynamic fusion graph"
    },
    {
      "citation_id": "15",
      "title": "Multimodal sentiment intensity analysis in videos: Facial gestures and verbal messages",
      "authors": [
        "Amir Zadeh",
        "Rowan Zellers",
        "Eli Pincus",
        "Louis-Philippe Morency"
      ],
      "year": "2016",
      "venue": "IEEE Intelligent Systems"
    }
  ]
}