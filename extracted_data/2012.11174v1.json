{
  "paper_id": "2012.11174v1",
  "title": "Unsupervised Cross-Lingual Speech Emotion Recognition Using Domain Adversarial Neural Network",
  "published": "2020-12-21T08:21:11Z",
  "authors": [
    "Xiong Cai",
    "Zhiyong Wu",
    "Kuo Zhong",
    "Bin Su",
    "Dongyang Dai",
    "Helen Meng"
  ],
  "keywords": [
    "speech emotion recognition",
    "domain adversarial learning",
    "cross-lingual",
    "affective representation learning"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "By using deep learning approaches, Speech Emotion Recognition (SER) on a single domain has achieved many excellent results. However, cross-domain SER is still a challenging task due to the distribution shift between source and target domains. In this work, we propose a Domain Adversarial Neural Network (DANN) based approach to mitigate this distribution shift problem for cross-lingual SER. Specifically, we add a language classifier and gradient reversal layer after the feature extractor to force the learned representation both language-independent and emotion-meaningful. Our method is unsupervised, i. e., labels on target language are not required, which makes it easier to apply our method to other languages. Experimental results show the proposed method provides an average absolute improvement of 3.91% over the baseline system for arousal and valence classification task. Furthermore, we find that batch normalization is beneficial to the performance gain of DANN. Therefore we also explore the effect of different ways of data combination for batch normalization.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "With the extensive application of Artificial Intelligence (AI) products in our daily lives, it has become increasingly imperative to design a smarter Human-Computer Speech Interaction (HCSI) system. Speech Emotion Recognition (SER), which aims to infer the emotional state of a speaker from his or her speech  [1] , has been regarded as a crucial component for a more intelligent HCSI system. Existing SER models  [2] [3] [4]  have achieved satisfactory level results when the training and test data are from the same corpus. However, it is still intractable to build a more robust cross-lingual SER system because of the domain shift between corpora of different languages  [5] .\n\nNumerous approaches have been proposed to reduce the domain shift problem for cross-corpus or cross-lingual SER.  [6]  proposes a fine-grained adversarial domain adaptation scheme, which reduces the distribution shift of the same emotion class in different corpora.  [7]  shows that fine-tuning can effectively improve the recognition results. These methods are promising, but additional labeled data are required, which might not be available since their collection is expensive.\n\nA more practical solution is unsupervised domain adaptation which only demands unlabeled data from related domains. A number of previous studies have explored statisticalbased methods to reduce mismatch between domains  [8] [9] [10] [11] [12] . Specifically,  [8, 9]  deploy different level of feature normaliza-tion strategies to minimize the speaker-and-corpus-related effects;  [10] [11] [12]  apply the Maximum Mean Discrepancy (MMD) or Kernel Canonical Correlation Analysis (KCCA) approaches to increase the similarity or correlation of different domains. All these methods reduce the domain shift directly on the original input feature space or its linear transformation space, so the capacity of shift-reduction might be limited. Some other studies  [13] [14] [15]  use variants of autoencoder to learn a concise and common feature representation by incorporating the prior knowledge from unlabeled data into learning. Since the optimization of the autoencoder and emotion classifier is not performed simultaneously, it is not clear whether compressed representations preserve all the emotion information of speech.\n\nRecently, Adversarial Learning (AL), such as Generative Adversarial Network (GAN)  [16]  and Domain Adversarial Neural Network (DANN)  [17] , has become an increasingly popular approach for domain adaptation.  [18]  proposes a GANbased model for cross-lingual SER and demonstrates significant improvements even for the non-mainstream Urdu language.  [19, 20]  use a DANN-based framework to learn a speakerindependent representation and greatly improve the singlecorpus results.  [21]  explores the advantage of DANN for crosscorpus SER on three English corpora. Besides, the DANN techniques have also been widely applied in other speech applications such as automatic speech recognition  [22]  and speaker recognition  [23]  to deal with the domain mismatch problem.\n\nInspired by the success of DANN in domain adaptation tasks, this paper proposes a DANN-based approach to reduce the distribution shift for cross-lingual SER. Specifically, based on the primary emotion classification task, a language classifier with Gradient Reversal Layer (GRL) is added to the model as an auxiliary task to help learn language-independent representations. Unlike the studies mentioned above, our approach reduces the distribution shift in a compressed feature space instead of the original input space, and all the modules are trained jointly rather than separately, which makes the model learn emotion-discriminative and language-independent representations more efficiently. Our contribution is two-fold: First, we introduce the DANN framework for cross-lingual SER and achieve significant performance improvements. Second, our study presents that batch normalization (BN)  [24]  can contribute to improve DANN and explores four different ways of combining data for BN.\n\nThe rest of this paper is organized as follows. The proposed method is described in Section 2. The databases and classification scheme are detailed in Section 3. Experimental setup and results analysis are presented in Section 4. Section 5 finalizes the study with conclusions and future directions.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Methodology",
      "text": "Firstly, we formulate our cross-lingual SER as the following domain adaptation task. We have a source language corpus with emotion labels as source domain, Ds = {(x s i , y s i )} n i=1 , and a target language corpus without emotion labels as target domain, Dt = {x t i } m i=1 , where x s i and x t i ∈ R k×d , y s i ∈ {0, 1} c ; k, d and c are the number of frames in an utterance, the dimension of each feature frame and the number of emotion categories; n and m are the number of samples in Ds and Dt. Our goal is to learn a reliable emotion classifier from the labeled Ds and the unlabeled Dt, which can be generalized well in Dt.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Model Structure",
      "text": "As shown in Figure  1 , the proposed model consists of three modules: encoder (G f ), emotion classifier (Ge) and language classifier (G l ). The encoder structure is mainly adopted from  [7] , except that we add a batch normalization (BN) layer for the stability of training a DANN model. A 1D convolution layer with ReLU activation takes Mel spectral features as input to capture emotion-related patterns. Then, a max pooling layer with a large stride follows to select the most salient features. Next, an attentive vector a f is extracted from the outputs of max pooling by the following attention formulas:\n\nwhere fi is the i-th feature vector of the output f of max pooling layer and v is a trainable vector as a global attention query. The motivation behind using this attention mechanism is that emotion-related information is distributed differently over the utterance. This global attention query v can be used to learn to capture these important emotion patterns. Finally, the attention vector a f is appended to the end of the output f of max pooling along the time dimension, and then all these feature vectors are flattened into a fixed-length vector as the input of the following BN layer. As the final representation, the output of the BN layer f is fed into emotion classifier (only source domain data) and language classifier (both source and target domain data). As for classifiers, a single dense layer with softmax activation and two output units are used for both classifiers. Besides, a Gradient Reversal Layer (GRL) is inserted between BN layer and language classifier to achieve the goal of adversarial training.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Adversarial Training",
      "text": "DANN  [17]  is a fairly elegant neural network framework for unsupervised domain adaptation, where unlabeled target domain data can be efficiently utilized to reduce the variations between the source and target domains. Specifically, there are two tasks: a primary target task (e. g., emotion classification) and an auxiliary domain classification task (e. g., language classification). Both tasks share the feature extractor and a GRL is introduced between feature extractor and domain classifier. GRL is a layer without trainable parameters and works as a \"pseudo-function\" R(x) defined as the following formulas: where I is an identity matrix and β is a hyper parameter controlling the scale of reversal gradient signal. By using GRL, the trainable parameters before and after GRL are updated in the opposite direction, namely, adversarial training. As for the classifiers of the two tasks, parameters are updated to minimize their respective errors. As for the feature extractor, parameters are updated to minimize the error of primary task while maximizing the error of domain classification task, where the latter is implemented by GRL. Therefore, the learned feature could be meaningful for the primary task and indistinguishable for the domain classifier. In terms of our cross-lingual SER, the primary task is emotion classification and the auxiliary task is language classification. Our goal is to learn a representation that retains discriminative information for emotion and reduces variations for languages. Therefore, the learned feature extractor and emotion classifier can be directly applied to target language data. We use the cross-entropy loss as the training objective for both emotion and language classifiers:\n\nwhere D = Ds ∪ Dt; Le and L l are the losses for emotion and language classifiers respectively; θ f , θe and θ l represent the trainable parameters for G f , Ge and G l respectively; y is the one-hot encoding for emotion labels from source domain and y l is the one-hot encoding for language labels distinguishing source and target languages. Then, we define the total loss as the weighted sum of the above two losses and directly minimize it for training:\n\nwhere α plays a trade-off for the two losses. Due to the existence of GRL, minimizing the total loss L will actually lead to the following way of parameter update:\n\nConcretely, θe and θ l are updated for minimizing Le and L l respectively, and θ f is updated for minimizing Le while maximizing L l simultaneously. λ and β are the learning rate and the gradient reversal scale of GRL. After training, a feature representation rich in emotional information and indistinguishable from languages will be obtained from the encoder output.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Databases",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Iemocap",
      "text": "IEMOCAP  [25]  is an audiovisual database of English dyadic conversations performed by ten professional actors. There are two types of conversations: the scripted ones and the improvised ones (given a certain scenario and topic). This corpus contains a total of 10,039 utterances, where audio, video, text and motion-capture recordings are available. The categorical emotion label and 5-point scales on the dimensions valence, arousal, and dominance (1 -low/negative, 5 -high/positive) are annotated by at least 2 raters. In our study, only audio modal data and dimension label of valence and arousal are used.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Recola",
      "text": "RECOLA  [26]  is a multimodal database of French dyadic conversations. Participants express emotions spontaneously during a collaborative video conference. Four different modal data of audio, video, electrocardiogram (ECG) and electrodermal activity (EDA) are recorded continuously and synchronously. Continuous valence and arousal labels in the range[-1, 1] are measured by 6 annotators at frame level. Since our goal is to predict emotion on utterance level, the mean value across all frames of an utterance and all annotators are calculated as the final label.\n\nFreely available 1,308 audio utterances from 23 speakers are used in our study.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Classification Scheme And Input Features",
      "text": "In this work, we focus on a binary classification task of valence (negative/positive) and arousal (low/high). In order to obtain binary training labels, we use the same annotation mapping scheme as in  [7] . For IEMOCAP, the two ranges [1, 2.5] and (2.5, 5] are categorized as low/negative and high/positive respectively. Similarly, the corresponding two ranges are [-1, 0] and (0, 1] for RECOLA. In terms of input features, 26 log-Mel filter-banks are extracted frame-wise from a single utterance with frame size of 25ms and frame shift of 10ms. The log-Mel feature has a fixed length of 750 frames. The shorter one is padded with the minimum for each dimension in an utterance and the longer one is truncated to 750 frames in the middle.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Experiments",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Experimental Setup",
      "text": "We use the following configurations for model training. 200 filters with kernel size 10 and stride 3 are used for the 1D convolution layer. The size and stride of max pooling are both set to 30. Adam  [27]  optimizer and exponential decay learning rate with initial rate 1e-3, decay rate 0.93 for every epoch, and final rate 5e-5 are used to optimize parameters. For the regularization, dropout with rate 0.7 as suggested in  [28]  is used for the output of encoder; l1 and l2 regularization with the weight 5e-3 are used for training RECOLA and IEMOCAP respectively. We train the models for 50 epochs with a batch size of 32, and 30% of data from test set is used as the development set for early stopping. The logMel features are normalized with zero mean and unit variance for each database. All experiments are run five times with different random seeds, and the unweighted average recall (UAR) is chosen as our evaluation criterion.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Experimental Results",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Performance Of The Proposed Model",
      "text": "In this section, we compare three trained models: our proposed model (our), the baseline model (base), and the mono-lingual model (mono). The base and mono model use the structure which consists of the same encoder and emotion classifier only as in Figure  1 . The mono model is trained and tested on the same database, where 70% samples are used for training, 25% for testing and 5% for early stopping. It provides us with an idea about the best achievable results within each database. We use Rec and Iem to represent the RECOLA and IEMOCAP database, and Rec2Iem means training on Rec and testing on Iem and vice versa. Table  1  reports UAR (%) results with standard deviations in parentheses for the three models. Comparing the results of base and our, our proposed model outperforms the baseline model in all experiments and achieves an average improvement of 3.91%. This result presents that the proposed approach can effectively reduce variations between different languages while retaining the information related to emotions. Therefore, the emotion classifier can benefit from the learned language-independent representation to improve results in target language. To illustrate this, we use Principal Component Analysis (PCA) to project the learned feature representation, i. e., the output of encoder, into 2D space.    3 , we plot the training curves of the domain classification loss (blue line) and UAR (green line), and the emotion classification UAR (red line) in development set. It can be seen that, at the early stage of training, the domain loss increases and decreases alternately, and the domain UAR changes oppositely than it, which suggests that the adversarial training itself works well. Moreover, the emotion development set UAR has a similar trend with domain loss, which means that the emotion classifier gets better results when the domain classifier has a higher loss, i. e., the more languageindiscriminative the features are, the better performance of emotion classification will be. These visual results further indicate the effectiveness of the proposed method for cross-lingual SER.\n\nComparing the results of base and mono in Table  1 , the performance of naive cross-lingual SER (baseline) is 8.11% lower on average than the mono-lingual SER. This result consistent with  [5, 29]  indicates that the distribution shift between different languages will seriously harness the predictive ability of SER.   In addition, another clear conclusion can be obtained that the prediction of arousal is easier than valence regardless of crosslingual or mono-lingual tasks. Similar results can be found in  [5, [30] [31] [32] . This is mainly because acoustic features such as energy, pitch and speed are related to arousal  [1] , but for valence, there is no consensus on how acoustic features correlate with it, and it is more speaker-dependent  [28] .",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Impact Of Batch Normalization On Performance",
      "text": "In this section, we first study the effect of BN on widening the performance gap between our DANN-based model and baseline model. As shown in Table  2 , our proposed model is only 2.61% higher than baseline on average when BN is not used, which is much lower than the gap of 3.91% as in Table  1  with BN used. A possible reason for this result is that the adversarial training of our model is not as stable as baseline, while BN can help to make the distribution of the representation layer more stable  [24] . Therefore, better results can be achieved after using the BN layer in our adversarial training model. Based on the conclusion reached above, we further explore four different ways of combining data for BN. For the training of DANN model, both source and target data need to be fed into the model. They can be first combined into one mini-batch and then fed into the model, or each of them occupy a min-batch and fed into the model alternatively. For the first data feeding method, three ways of combining data for BN are performed as follows: perform BN on the whole batch, namely BN1, which is used for above experiments, where the first half batch (source half) is fed to the emotion classifier (Ge) and the whole batch is fed to the language classifier (G l ); perform BN on the source half batch and whole batch respectively, namely BN2; perform BN on the source half batch and target half batch respectively, namely BN3. For the second data feeding method, BN is performed on the whole batch from each domain, namely BN4.\n\nThe evaluation results of the above four types of BN are shown in Table  3 . On the one hand, the average results of all BN1-3 are higher than BN4. This proves that it is better to combine data from both source and target domains in one batch than batch them separately. Therefore, it is important for the training of DANN to ensure the guiding gradient signal comes from both source and target domains at each training step. On the other hand, we can also find the average results of both BN1-2 are better than BN3. The main difference between BN1-2 and BN3 is whether the input features for G l is performed BN on the whole batch (BN1-2) or on the source and target half separately (BN3). This result presents that it is more suitable to feed the language classifier with features performed BN on the entire batch. Besides, it is also worth noting that when training on the smaller database of RECOLA (1,308 utterances), the results of all four settings don't show significant difference. Therefore, this study empirically suggests that BN1 or BN2 is a more recommended way for BN of features, when training the DANN model on a larger corpus.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Conclusions",
      "text": "In this paper, we propose a DANN-based approach for crosslingual SER. Our method works in a completely unsupervised way, where unlabeled target language data is required only. Experimental results show that our method enables the model to focus on the emotion related information, while ignoring the variations between different languages. Moreover, we explore the impact of batch normalization on training DANN models and suggest two practically optimal ways of data combination for batch normalization. For further work, we plan to add more corpora from other languages for the cross-lingual SER task.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: , the proposed model consists of three",
      "page": 2
    },
    {
      "caption": "Figure 1: The proposed model structure. BN and GRL are Batch",
      "page": 2
    },
    {
      "caption": "Figure 1: The mono model is trained and tested on the",
      "page": 3
    },
    {
      "caption": "Figure 2: , regarding language labels, feature",
      "page": 3
    },
    {
      "caption": "Figure 2: (b) right) are",
      "page": 3
    },
    {
      "caption": "Figure 3: , we plot the training curves",
      "page": 3
    },
    {
      "caption": "Figure 2: PCA plot of the learned feature representation with",
      "page": 4
    },
    {
      "caption": "Figure 3: Domain(language) loss, domain UAR, and emotion",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "The Chinese University of Hong Kong, Shatin, N.T., Hong Kong SAR, China": ""
        },
        {
          "The Chinese University of Hong Kong, Shatin, N.T., Hong Kong SAR, China": "tion strategies to minimize the speaker-and-corpus-related ef-"
        },
        {
          "The Chinese University of Hong Kong, Shatin, N.T., Hong Kong SAR, China": "fects; [10–12] apply the Maximum Mean Discrepancy (MMD)"
        },
        {
          "The Chinese University of Hong Kong, Shatin, N.T., Hong Kong SAR, China": ""
        },
        {
          "The Chinese University of Hong Kong, Shatin, N.T., Hong Kong SAR, China": "or Kernel Canonical Correlation Analysis (KCCA) approaches"
        },
        {
          "The Chinese University of Hong Kong, Shatin, N.T., Hong Kong SAR, China": ""
        },
        {
          "The Chinese University of Hong Kong, Shatin, N.T., Hong Kong SAR, China": "to increase the similarity or correlation of different domains."
        },
        {
          "The Chinese University of Hong Kong, Shatin, N.T., Hong Kong SAR, China": ""
        },
        {
          "The Chinese University of Hong Kong, Shatin, N.T., Hong Kong SAR, China": "All these methods reduce the domain shift directly on the orig-"
        },
        {
          "The Chinese University of Hong Kong, Shatin, N.T., Hong Kong SAR, China": ""
        },
        {
          "The Chinese University of Hong Kong, Shatin, N.T., Hong Kong SAR, China": "inal\ninput\nfeature space or\nits linear\ntransformation space,\nso"
        },
        {
          "The Chinese University of Hong Kong, Shatin, N.T., Hong Kong SAR, China": ""
        },
        {
          "The Chinese University of Hong Kong, Shatin, N.T., Hong Kong SAR, China": "the capacity of shift-reduction might be limited.\nSome other"
        },
        {
          "The Chinese University of Hong Kong, Shatin, N.T., Hong Kong SAR, China": ""
        },
        {
          "The Chinese University of Hong Kong, Shatin, N.T., Hong Kong SAR, China": "studies [13–15] use variants of autoencoder to learn a concise"
        },
        {
          "The Chinese University of Hong Kong, Shatin, N.T., Hong Kong SAR, China": ""
        },
        {
          "The Chinese University of Hong Kong, Shatin, N.T., Hong Kong SAR, China": "and common feature representation by incorporating the prior"
        },
        {
          "The Chinese University of Hong Kong, Shatin, N.T., Hong Kong SAR, China": ""
        },
        {
          "The Chinese University of Hong Kong, Shatin, N.T., Hong Kong SAR, China": "knowledge from unlabeled data into learning.\nSince the opti-"
        },
        {
          "The Chinese University of Hong Kong, Shatin, N.T., Hong Kong SAR, China": ""
        },
        {
          "The Chinese University of Hong Kong, Shatin, N.T., Hong Kong SAR, China": "mization of\nthe autoencoder and emotion classiﬁer\nis not per-"
        },
        {
          "The Chinese University of Hong Kong, Shatin, N.T., Hong Kong SAR, China": ""
        },
        {
          "The Chinese University of Hong Kong, Shatin, N.T., Hong Kong SAR, China": "formed simultaneously, it is not clear whether compressed rep-"
        },
        {
          "The Chinese University of Hong Kong, Shatin, N.T., Hong Kong SAR, China": ""
        },
        {
          "The Chinese University of Hong Kong, Shatin, N.T., Hong Kong SAR, China": "resentations preserve all the emotion information of speech."
        },
        {
          "The Chinese University of Hong Kong, Shatin, N.T., Hong Kong SAR, China": ""
        },
        {
          "The Chinese University of Hong Kong, Shatin, N.T., Hong Kong SAR, China": "Recently, Adversarial Learning (AL), such as Generative"
        },
        {
          "The Chinese University of Hong Kong, Shatin, N.T., Hong Kong SAR, China": ""
        },
        {
          "The Chinese University of Hong Kong, Shatin, N.T., Hong Kong SAR, China": "Adversarial Network (GAN) [16] and Domain Adversarial Neu-"
        },
        {
          "The Chinese University of Hong Kong, Shatin, N.T., Hong Kong SAR, China": ""
        },
        {
          "The Chinese University of Hong Kong, Shatin, N.T., Hong Kong SAR, China": "ral Network (DANN)\n[17], has become an increasingly pop-"
        },
        {
          "The Chinese University of Hong Kong, Shatin, N.T., Hong Kong SAR, China": ""
        },
        {
          "The Chinese University of Hong Kong, Shatin, N.T., Hong Kong SAR, China": "ular approach for domain adaptation.\n[18] proposes a GAN-"
        },
        {
          "The Chinese University of Hong Kong, Shatin, N.T., Hong Kong SAR, China": ""
        },
        {
          "The Chinese University of Hong Kong, Shatin, N.T., Hong Kong SAR, China": "based model for cross-lingual SER and demonstrates signiﬁcant"
        },
        {
          "The Chinese University of Hong Kong, Shatin, N.T., Hong Kong SAR, China": ""
        },
        {
          "The Chinese University of Hong Kong, Shatin, N.T., Hong Kong SAR, China": "improvements\neven for\nthe non-mainstream Urdu language."
        },
        {
          "The Chinese University of Hong Kong, Shatin, N.T., Hong Kong SAR, China": ""
        },
        {
          "The Chinese University of Hong Kong, Shatin, N.T., Hong Kong SAR, China": "[19, 20] use\na DANN-based framework to learn a\nspeaker-"
        },
        {
          "The Chinese University of Hong Kong, Shatin, N.T., Hong Kong SAR, China": ""
        },
        {
          "The Chinese University of Hong Kong, Shatin, N.T., Hong Kong SAR, China": "independent\nrepresentation\nand\ngreatly\nimprove\nthe\nsingle-"
        },
        {
          "The Chinese University of Hong Kong, Shatin, N.T., Hong Kong SAR, China": ""
        },
        {
          "The Chinese University of Hong Kong, Shatin, N.T., Hong Kong SAR, China": "corpus results. [21] explores the advantage of DANN for cross-"
        },
        {
          "The Chinese University of Hong Kong, Shatin, N.T., Hong Kong SAR, China": "corpus SER on three English corpora. Besides, the DANN tech-"
        },
        {
          "The Chinese University of Hong Kong, Shatin, N.T., Hong Kong SAR, China": ""
        },
        {
          "The Chinese University of Hong Kong, Shatin, N.T., Hong Kong SAR, China": "niques have also been widely applied in other speech applica-"
        },
        {
          "The Chinese University of Hong Kong, Shatin, N.T., Hong Kong SAR, China": "tions\nsuch as automatic speech recognition [22] and speaker"
        },
        {
          "The Chinese University of Hong Kong, Shatin, N.T., Hong Kong SAR, China": ""
        },
        {
          "The Chinese University of Hong Kong, Shatin, N.T., Hong Kong SAR, China": "recognition [23] to deal with the domain mismatch problem."
        },
        {
          "The Chinese University of Hong Kong, Shatin, N.T., Hong Kong SAR, China": ""
        },
        {
          "The Chinese University of Hong Kong, Shatin, N.T., Hong Kong SAR, China": "Inspired by the success of DANN in domain adaptation"
        },
        {
          "The Chinese University of Hong Kong, Shatin, N.T., Hong Kong SAR, China": "tasks,\nthis paper proposes a DANN-based approach to reduce"
        },
        {
          "The Chinese University of Hong Kong, Shatin, N.T., Hong Kong SAR, China": "the distribution shift for cross-lingual SER. Speciﬁcally, based"
        },
        {
          "The Chinese University of Hong Kong, Shatin, N.T., Hong Kong SAR, China": "on the primary emotion classiﬁcation task, a language classi-"
        },
        {
          "The Chinese University of Hong Kong, Shatin, N.T., Hong Kong SAR, China": "ﬁer with Gradient Reversal Layer (GRL) is added to the model"
        },
        {
          "The Chinese University of Hong Kong, Shatin, N.T., Hong Kong SAR, China": "as an auxiliary task to help learn language-independent repre-"
        },
        {
          "The Chinese University of Hong Kong, Shatin, N.T., Hong Kong SAR, China": "sentations. Unlike the studies mentioned above, our approach"
        },
        {
          "The Chinese University of Hong Kong, Shatin, N.T., Hong Kong SAR, China": "reduces\nthe distribution shift\nin a\ncompressed feature\nspace"
        },
        {
          "The Chinese University of Hong Kong, Shatin, N.T., Hong Kong SAR, China": "instead of\nthe original\ninput\nspace,\nand all\nthe modules are"
        },
        {
          "The Chinese University of Hong Kong, Shatin, N.T., Hong Kong SAR, China": "trained jointly rather\nthan separately, which makes the model"
        },
        {
          "The Chinese University of Hong Kong, Shatin, N.T., Hong Kong SAR, China": "learn emotion-discriminative and language-independent\nrepre-"
        },
        {
          "The Chinese University of Hong Kong, Shatin, N.T., Hong Kong SAR, China": "sentations more efﬁciently. Our contribution is two-fold: First,"
        },
        {
          "The Chinese University of Hong Kong, Shatin, N.T., Hong Kong SAR, China": "we introduce the DANN framework for cross-lingual SER and"
        },
        {
          "The Chinese University of Hong Kong, Shatin, N.T., Hong Kong SAR, China": "achieve signiﬁcant performance improvements.\nSecond, our"
        },
        {
          "The Chinese University of Hong Kong, Shatin, N.T., Hong Kong SAR, China": "study presents\nthat batch normalization (BN)\n[24]\ncan con-"
        },
        {
          "The Chinese University of Hong Kong, Shatin, N.T., Hong Kong SAR, China": "tribute to improve DANN and explores four different ways of"
        },
        {
          "The Chinese University of Hong Kong, Shatin, N.T., Hong Kong SAR, China": "combining data for BN."
        },
        {
          "The Chinese University of Hong Kong, Shatin, N.T., Hong Kong SAR, China": "The rest of this paper is organized as follows. The proposed"
        },
        {
          "The Chinese University of Hong Kong, Shatin, N.T., Hong Kong SAR, China": "method is described in Section 2. The databases and classiﬁca-"
        },
        {
          "The Chinese University of Hong Kong, Shatin, N.T., Hong Kong SAR, China": "tion scheme are detailed in Section 3. Experimental setup and"
        },
        {
          "The Chinese University of Hong Kong, Shatin, N.T., Hong Kong SAR, China": "results analysis are presented in Section 4. Section 5 ﬁnalizes"
        },
        {
          "The Chinese University of Hong Kong, Shatin, N.T., Hong Kong SAR, China": "the study with conclusions and future directions."
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "emotion classifier": "encoder"
        },
        {
          "emotion classifier": "Loss E"
        },
        {
          "emotion classifier": ""
        },
        {
          "emotion classifier": "+"
        },
        {
          "emotion classifier": "Loss L"
        },
        {
          "emotion classifier": ""
        },
        {
          "emotion classifier": "GRL\nBN\nConv-1D\nPooling\nAttention"
        },
        {
          "emotion classifier": "adversarial"
        },
        {
          "emotion classifier": "language classifier"
        },
        {
          "emotion classifier": ""
        },
        {
          "emotion classifier": "Figure 1: The proposed model structure. BN and GRL are Batch"
        },
        {
          "emotion classifier": ""
        },
        {
          "emotion classifier": "Normalization, and Gradient Reversal Layer respectively."
        },
        {
          "emotion classifier": ""
        },
        {
          "emotion classifier": "where I is an identity matrix and β is a hyper parameter con-"
        },
        {
          "emotion classifier": ""
        },
        {
          "emotion classifier": "trolling the scale of\nreversal gradient signal.\nBy using GRL,"
        },
        {
          "emotion classifier": "the trainable parameters before and after GRL are updated in"
        },
        {
          "emotion classifier": "the opposite direction, namely, adversarial training. As for the"
        },
        {
          "emotion classifier": "classiﬁers of the two tasks, parameters are updated to minimize"
        },
        {
          "emotion classifier": "their respective errors. As for the feature extractor, parameters"
        },
        {
          "emotion classifier": "are updated to minimize the error of primary task while maxi-"
        },
        {
          "emotion classifier": "mizing the error of domain classiﬁcation task, where the latter"
        },
        {
          "emotion classifier": "is implemented by GRL. Therefore,\nthe learned feature could"
        },
        {
          "emotion classifier": "be meaningful\nfor\nthe primary task and indistinguishable for"
        },
        {
          "emotion classifier": "the domain classiﬁer.\nIn terms of our cross-lingual SER,\nthe"
        },
        {
          "emotion classifier": "primary task is emotion classiﬁcation and the auxiliary task is"
        },
        {
          "emotion classifier": "language classiﬁcation. Our goal is to learn a representation that"
        },
        {
          "emotion classifier": "retains discriminative information for emotion and reduces vari-"
        },
        {
          "emotion classifier": ""
        },
        {
          "emotion classifier": "ations for\nlanguages.\nTherefore,\nthe learned feature extractor"
        },
        {
          "emotion classifier": "and emotion classiﬁer can be directly applied to target language"
        },
        {
          "emotion classifier": "data."
        },
        {
          "emotion classifier": ""
        },
        {
          "emotion classifier": "We use the cross-entropy loss as the training objective for"
        },
        {
          "emotion classifier": "both emotion and language classiﬁers:"
        },
        {
          "emotion classifier": ""
        },
        {
          "emotion classifier": "(cid:88)"
        },
        {
          "emotion classifier": "1 n\n(5)\nLe(θf , θe) = −\nyT log Ge(Gf (x; θf ); θe)"
        },
        {
          "emotion classifier": ""
        },
        {
          "emotion classifier": "(x,y)∈Ds"
        },
        {
          "emotion classifier": "1"
        },
        {
          "emotion classifier": "yT\n(6)\nLl(θf , θl) = −\nlog Gl(Gf (x; θf ); θl)"
        },
        {
          "emotion classifier": "(cid:88) x\nn + m"
        },
        {
          "emotion classifier": "∈D"
        },
        {
          "emotion classifier": ""
        },
        {
          "emotion classifier": "where D = Ds ∪ Dt; Le and Ll are the losses for emotion and"
        },
        {
          "emotion classifier": "represent\nthe\nlanguage classiﬁers respectively; θf , θe and θl"
        },
        {
          "emotion classifier": "respectively; y is the\ntrainable parameters for Gf , Ge and Gl"
        },
        {
          "emotion classifier": "one-hot encoding for emotion labels from source domain and"
        },
        {
          "emotion classifier": "is the one-hot encoding for\nlanguage labels distinguishing\nyl"
        },
        {
          "emotion classifier": "source and target languages."
        },
        {
          "emotion classifier": "Then, we deﬁne the total\nloss as the weighted sum of\nthe"
        },
        {
          "emotion classifier": "above two losses and directly minimize it for training:"
        },
        {
          "emotion classifier": ""
        },
        {
          "emotion classifier": "(7)\nL(θf , θe, θl) = α · Le(θf , θe) + (1 − α) · Ll(θf , θl)"
        },
        {
          "emotion classifier": ""
        },
        {
          "emotion classifier": "where α plays a trade-off for the two losses. Due to the exis-"
        },
        {
          "emotion classifier": "tence of GRL, minimizing the total\nloss L will actually lead to"
        },
        {
          "emotion classifier": "the following way of parameter update:"
        },
        {
          "emotion classifier": ""
        },
        {
          "emotion classifier": "∂Le"
        },
        {
          "emotion classifier": "(8)\nθe ← θe − λ · α"
        },
        {
          "emotion classifier": ""
        },
        {
          "emotion classifier": "∂θe"
        },
        {
          "emotion classifier": ""
        },
        {
          "emotion classifier": "∂Ll"
        },
        {
          "emotion classifier": "(9)\nθl ← θl − λ · (1 − α)"
        },
        {
          "emotion classifier": "∂θl"
        },
        {
          "emotion classifier": ""
        },
        {
          "emotion classifier": "(cid:18)\n(cid:19)"
        },
        {
          "emotion classifier": "∂Le\n∂Ll"
        },
        {
          "emotion classifier": "α\n− (1 − α) · β\n(10)\nθf ← θf − λ ·"
        },
        {
          "emotion classifier": "∂θf\n∂θf"
        },
        {
          "emotion classifier": ""
        },
        {
          "emotion classifier": ""
        },
        {
          "emotion classifier": "Concretely, θe and θl are updated for minimizing Le and Ll"
        },
        {
          "emotion classifier": ""
        },
        {
          "emotion classifier": "is updated for minimizing Le while maxi-\nrespectively, and θf"
        },
        {
          "emotion classifier": "mizing Ll simultaneously. λ and β are the learning rate and the"
        },
        {
          "emotion classifier": ""
        },
        {
          "emotion classifier": "gradient\nreversal scale of GRL. After\ntraining, a feature rep-"
        },
        {
          "emotion classifier": "resentation rich in emotional information and indistinguishable"
        },
        {
          "emotion classifier": ""
        },
        {
          "emotion classifier": "from languages will be obtained from the encoder output."
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "3. Databases": "",
          "4.2. Experimental results": "4.2.1. Performance of the proposed model"
        },
        {
          "3. Databases": "3.1.\nIEMOCAP",
          "4.2. Experimental results": ""
        },
        {
          "3. Databases": "",
          "4.2. Experimental results": "In this section, we compare three trained models: our proposed"
        },
        {
          "3. Databases": "IEMOCAP [25]\nis an audiovisual database of English dyadic",
          "4.2. Experimental results": ""
        },
        {
          "3. Databases": "",
          "4.2. Experimental results": "model (our),\nthe baseline model (base), and the mono-lingual"
        },
        {
          "3. Databases": "conversations performed by ten professional actors. There are",
          "4.2. Experimental results": ""
        },
        {
          "3. Databases": "",
          "4.2. Experimental results": "model\n(mono).\nThe base and mono model use the structure"
        },
        {
          "3. Databases": "two types of conversations:\nthe scripted ones and the impro-",
          "4.2. Experimental results": ""
        },
        {
          "3. Databases": "",
          "4.2. Experimental results": "which consists of the same encoder and emotion classiﬁer only"
        },
        {
          "3. Databases": "vised ones\n(given a certain scenario and topic).\nThis corpus",
          "4.2. Experimental results": ""
        },
        {
          "3. Databases": "",
          "4.2. Experimental results": "as in Figure 1.\nThe mono model\nis trained and tested on the"
        },
        {
          "3. Databases": "contains a total of 10,039 utterances, where audio, video,\ntext",
          "4.2. Experimental results": ""
        },
        {
          "3. Databases": "",
          "4.2. Experimental results": "same database, where 70% samples are used for training, 25%"
        },
        {
          "3. Databases": "and motion-capture recordings are available.\nThe categorical",
          "4.2. Experimental results": ""
        },
        {
          "3. Databases": "",
          "4.2. Experimental results": "for\ntesting and 5% for early stopping.\nIt provides us with an"
        },
        {
          "3. Databases": "emotion label and 5-point\nscales on the dimensions valence,",
          "4.2. Experimental results": ""
        },
        {
          "3. Databases": "",
          "4.2. Experimental results": "idea about the best achievable results within each database. We"
        },
        {
          "3. Databases": "arousal, and dominance (1 - low/negative, 5 - high/positive) are",
          "4.2. Experimental results": ""
        },
        {
          "3. Databases": "",
          "4.2. Experimental results": "use Rec and Iem to represent\nthe RECOLA and IEMOCAP"
        },
        {
          "3. Databases": "annotated by at\nleast 2 raters.\nIn our study, only audio modal",
          "4.2. Experimental results": ""
        },
        {
          "3. Databases": "",
          "4.2. Experimental results": "database, and Rec2Iem means training on Rec and testing on"
        },
        {
          "3. Databases": "data and dimension label of valence and arousal are used.",
          "4.2. Experimental results": ""
        },
        {
          "3. Databases": "",
          "4.2. Experimental results": "Iem and vice versa."
        },
        {
          "3. Databases": "",
          "4.2. Experimental results": "Table 1 reports UAR (%) results with standard deviations in"
        },
        {
          "3. Databases": "3.2. RECOLA",
          "4.2. Experimental results": "parentheses for the three models. Comparing the results of base"
        },
        {
          "3. Databases": "",
          "4.2. Experimental results": "and our, our proposed model outperforms the baseline model in"
        },
        {
          "3. Databases": "RECOLA [26] is a multimodal database of French dyadic con-",
          "4.2. Experimental results": "all experiments and achieves an average improvement of 3.91%."
        },
        {
          "3. Databases": "versations. Participants express emotions spontaneously during",
          "4.2. Experimental results": "This result presents that\nthe proposed approach can effectively"
        },
        {
          "3. Databases": "a collaborative video conference. Four different modal data of",
          "4.2. Experimental results": "reduce variations between different\nlanguages while retaining"
        },
        {
          "3. Databases": "audio, video, electrocardiogram (ECG) and electrodermal activ-",
          "4.2. Experimental results": "the information related to emotions.\nTherefore,\nthe emotion"
        },
        {
          "3. Databases": "ity (EDA) are recorded continuously and synchronously. Con-",
          "4.2. Experimental results": "classiﬁer\ncan beneﬁt\nfrom the\nlearned language-independent"
        },
        {
          "3. Databases": "tinuous valence and arousal\nlabels in the range[-1, 1] are mea-",
          "4.2. Experimental results": "representation to improve\nresults\nin target\nlanguage.\nTo il-"
        },
        {
          "3. Databases": "sured by 6 annotators at frame level. Since our goal is to predict",
          "4.2. Experimental results": "lustrate this, we use Principal Component Analysis (PCA)\nto"
        },
        {
          "3. Databases": "emotion on utterance level, the mean value across all frames of",
          "4.2. Experimental results": "project the learned feature representation, i. e., the output of en-"
        },
        {
          "3. Databases": "an utterance and all annotators are calculated as the ﬁnal label.",
          "4.2. Experimental results": "coder, into 2D space."
        },
        {
          "3. Databases": "Freely available 1,308 audio utterances from 23 speakers are",
          "4.2. Experimental results": ""
        },
        {
          "3. Databases": "",
          "4.2. Experimental results": "Table 1: UAR (%) for baseline and proposed method."
        },
        {
          "3. Databases": "used in our study.",
          "4.2. Experimental results": ""
        },
        {
          "3. Databases": "",
          "4.2. Experimental results": "Rec2Iem\nIem2Rec"
        },
        {
          "3. Databases": "3.3. Classiﬁcation scheme and input features",
          "4.2. Experimental results": ""
        },
        {
          "3. Databases": "",
          "4.2. Experimental results": "model\narousal\nvalence\narousal\nvalence\naverage"
        },
        {
          "3. Databases": "In this work, we focus on a binary classiﬁcation task of va-",
          "4.2. Experimental results": "62.49(2.96)\n54.15(0.51)\n60.73(0.45)\n58.11(0.51)\n58.87\nbase"
        },
        {
          "3. Databases": "",
          "4.2. Experimental results": "71.99(0.33) 54.54(0.77) 63.18(0.32) 61.43(1.38) 62.78\nour"
        },
        {
          "3. Databases": "lence (negative/positive) and arousal\n(low/high).\nIn order\nto",
          "4.2. Experimental results": ""
        },
        {
          "3. Databases": "",
          "4.2. Experimental results": "75.55(0.78)\n63.20(2.23)\n66.28(1.71)\n62.90(1.12)\n66.98\nmono1"
        },
        {
          "3. Databases": "obtain binary training labels, we use the same annotation map-",
          "4.2. Experimental results": ""
        },
        {
          "3. Databases": "ping scheme as in [7]. For IEMOCAP,\nthe two ranges [1, 2.5]",
          "4.2. Experimental results": ""
        },
        {
          "3. Databases": "",
          "4.2. Experimental results": "As shown in Figure 2,\nregarding language labels,\nfeature"
        },
        {
          "3. Databases": "and (2.5, 5] are categorized as low/negative and high/positive",
          "4.2. Experimental results": ""
        },
        {
          "3. Databases": "",
          "4.2. Experimental results": "representations learned by our adversarial\ntraining model (Fig-"
        },
        {
          "3. Databases": "respectively.\nSimilarly,\nthe corresponding two ranges are [-1,",
          "4.2. Experimental results": ""
        },
        {
          "3. Databases": "",
          "4.2. Experimental results": "ure 2(b)\nleft) are more evenly mixed and therefore more in-"
        },
        {
          "3. Databases": "0] and (0, 1] for RECOLA.\nIn terms of input features, 26 log-",
          "4.2. Experimental results": ""
        },
        {
          "3. Databases": "",
          "4.2. Experimental results": "distinguishable than the ones learned by baseline model\n(Fig-"
        },
        {
          "3. Databases": "Mel ﬁlter-banks are extracted frame-wise from a single utter-",
          "4.2. Experimental results": ""
        },
        {
          "3. Databases": "",
          "4.2. Experimental results": "ure 2(a)\nleft); while, as for emotion labels,\nfeature represen-"
        },
        {
          "3. Databases": "ance with frame size of 25ms and frame shift of 10ms. The log-",
          "4.2. Experimental results": ""
        },
        {
          "3. Databases": "",
          "4.2. Experimental results": "tations learned by our proposed model\n(Figure 2(b)\nright) are"
        },
        {
          "3. Databases": "Mel feature has a ﬁxed length of 750 frames. The shorter one",
          "4.2. Experimental results": ""
        },
        {
          "3. Databases": "",
          "4.2. Experimental results": "more separable than the ones learned by baseline model (Fig-"
        },
        {
          "3. Databases": "is padded with the minimum for each dimension in an utterance",
          "4.2. Experimental results": ""
        },
        {
          "3. Databases": "",
          "4.2. Experimental results": "ure 2(a) right). Besides, in Figure 3, we plot the training curves"
        },
        {
          "3. Databases": "and the longer one is truncated to 750 frames in the middle.",
          "4.2. Experimental results": ""
        },
        {
          "3. Databases": "",
          "4.2. Experimental results": "of\nthe domain classiﬁcation loss\n(blue line) and UAR (green"
        },
        {
          "3. Databases": "",
          "4.2. Experimental results": "line), and the emotion classiﬁcation UAR (red line)\nin devel-"
        },
        {
          "3. Databases": "4. Experiments",
          "4.2. Experimental results": "opment set.\nIt can be seen that, at\nthe early stage of\ntraining,"
        },
        {
          "3. Databases": "",
          "4.2. Experimental results": "the domain loss increases and decreases alternately, and the do-"
        },
        {
          "3. Databases": "4.1. Experimental setup",
          "4.2. Experimental results": "main UAR changes oppositely than it, which suggests that\nthe"
        },
        {
          "3. Databases": "",
          "4.2. Experimental results": "adversarial\ntraining itself works well. Moreover,\nthe emotion"
        },
        {
          "3. Databases": "We use the following conﬁgurations for model\ntraining.\n200",
          "4.2. Experimental results": "development\nset UAR has a similar\ntrend with domain loss,"
        },
        {
          "3. Databases": "ﬁlters with kernel size 10 and stride 3 are used for the 1D con-",
          "4.2. Experimental results": "which means that the emotion classiﬁer gets better results when"
        },
        {
          "3. Databases": "volution layer. The size and stride of max pooling are both set",
          "4.2. Experimental results": "the domain classiﬁer has a higher loss, i. e., the more language-"
        },
        {
          "3. Databases": "to 30. Adam [27] optimizer and exponential decay learning rate",
          "4.2. Experimental results": "indiscriminative the features are, the better performance of emo-"
        },
        {
          "3. Databases": "with initial rate 1e-3, decay rate 0.93 for every epoch, and ﬁnal",
          "4.2. Experimental results": "tion classiﬁcation will be. These visual results further indicate"
        },
        {
          "3. Databases": "rate 5e-5 are used to optimize parameters. For the regulariza-",
          "4.2. Experimental results": "the effectiveness of the proposed method for cross-lingual SER."
        },
        {
          "3. Databases": "tion, dropout with rate 0.7 as suggested in [28] is used for the",
          "4.2. Experimental results": ""
        },
        {
          "3. Databases": "output of encoder;\nl1 and l2 regularization with the weight 5e-",
          "4.2. Experimental results": ""
        },
        {
          "3. Databases": "",
          "4.2. Experimental results": "Comparing the results of base and mono in Table 1, the per-"
        },
        {
          "3. Databases": "3 are used for training RECOLA and IEMOCAP respectively.",
          "4.2. Experimental results": ""
        },
        {
          "3. Databases": "",
          "4.2. Experimental results": "formance of naive cross-lingual SER (baseline) is 8.11% lower"
        },
        {
          "3. Databases": "We train the models for 50 epochs with a batch size of 32, and",
          "4.2. Experimental results": ""
        },
        {
          "3. Databases": "",
          "4.2. Experimental results": "on average than the mono-lingual SER. This result consistent"
        },
        {
          "3. Databases": "30% of data from test set\nis used as the development set\nfor",
          "4.2. Experimental results": ""
        },
        {
          "3. Databases": "",
          "4.2. Experimental results": "with [5,29] indicates that the distribution shift between different"
        },
        {
          "3. Databases": "early stopping. The logMel features are normalized with zero",
          "4.2. Experimental results": ""
        },
        {
          "3. Databases": "",
          "4.2. Experimental results": "languages will seriously harness the predictive ability of SER."
        },
        {
          "3. Databases": "mean and unit variance for each database. All experiments are",
          "4.2. Experimental results": ""
        },
        {
          "3. Databases": "run ﬁve times with different random seeds, and the unweighted",
          "4.2. Experimental results": "1mono is trained and tested on Iem for Rec2Iem setup, and Rec for"
        },
        {
          "3. Databases": "average recall (UAR) is chosen as our evaluation criterion.",
          "4.2. Experimental results": "Iem2Rec setup"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "both source and target domains at each training step. On the": ""
        },
        {
          "both source and target domains at each training step. On the": "other hand, we can also ﬁnd the average results of both BN1-2"
        },
        {
          "both source and target domains at each training step. On the": "are better than BN3. The main difference between BN1-2 and"
        },
        {
          "both source and target domains at each training step. On the": "is performed BN on\nBN3 is whether the input features for Gl"
        },
        {
          "both source and target domains at each training step. On the": ""
        },
        {
          "both source and target domains at each training step. On the": "the whole batch (BN1-2) or on the source and target half sepa-"
        },
        {
          "both source and target domains at each training step. On the": "rately (BN3). This result presents that it is more suitable to feed"
        },
        {
          "both source and target domains at each training step. On the": "the language classiﬁer with features performed BN on the entire"
        },
        {
          "both source and target domains at each training step. On the": "batch. Besides, it is also worth noting that when training on the"
        },
        {
          "both source and target domains at each training step. On the": "smaller database of RECOLA (1,308 utterances),\nthe results of"
        },
        {
          "both source and target domains at each training step. On the": ""
        },
        {
          "both source and target domains at each training step. On the": "all\nfour settings don’t show signiﬁcant difference.\nTherefore,"
        },
        {
          "both source and target domains at each training step. On the": ""
        },
        {
          "both source and target domains at each training step. On the": "this study empirically suggests that BN1 or BN2 is a more rec-"
        },
        {
          "both source and target domains at each training step. On the": ""
        },
        {
          "both source and target domains at each training step. On the": "ommended way for BN of\nfeatures, when training the DANN"
        },
        {
          "both source and target domains at each training step. On the": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Table 2: UAR (%) for no batch normalization.": ""
        },
        {
          "Table 2: UAR (%) for no batch normalization.": ""
        },
        {
          "Table 2: UAR (%) for no batch normalization.": ""
        },
        {
          "Table 2: UAR (%) for no batch normalization.": "Rec2Iem\nIem2Rec"
        },
        {
          "Table 2: UAR (%) for no batch normalization.": ""
        },
        {
          "Table 2: UAR (%) for no batch normalization.": "model\narousal\nvalence\narousal\nvalence\naverage"
        },
        {
          "Table 2: UAR (%) for no batch normalization.": ""
        },
        {
          "Table 2: UAR (%) for no batch normalization.": "58.04(1.90)\n52.34(0.79)\n58.69(0.95)\n53.79(0.87)\n55.71\nbase"
        },
        {
          "Table 2: UAR (%) for no batch normalization.": ""
        },
        {
          "Table 2: UAR (%) for no batch normalization.": "63.62(1.74) 52.94(0.36) 59.76(0.77) 57.81(2.07) 58.32\nour"
        },
        {
          "Table 2: UAR (%) for no batch normalization.": ""
        },
        {
          "Table 2: UAR (%) for no batch normalization.": ""
        },
        {
          "Table 2: UAR (%) for no batch normalization.": ""
        },
        {
          "Table 2: UAR (%) for no batch normalization.": "method, three ways of combining data for BN are performed as"
        },
        {
          "Table 2: UAR (%) for no batch normalization.": "follows: perform BN on the whole batch, namely BN1, which"
        },
        {
          "Table 2: UAR (%) for no batch normalization.": ""
        },
        {
          "Table 2: UAR (%) for no batch normalization.": "is used for above experiments, where the ﬁrst half batch (source"
        },
        {
          "Table 2: UAR (%) for no batch normalization.": "half) is fed to the emotion classiﬁer (Ge) and the whole batch"
        },
        {
          "Table 2: UAR (%) for no batch normalization.": ""
        },
        {
          "Table 2: UAR (%) for no batch normalization.": "is fed to the language classiﬁer (Gl); perform BN on the source"
        },
        {
          "Table 2: UAR (%) for no batch normalization.": ""
        },
        {
          "Table 2: UAR (%) for no batch normalization.": "half batch and whole batch respectively, namely BN2; perform"
        },
        {
          "Table 2: UAR (%) for no batch normalization.": ""
        },
        {
          "Table 2: UAR (%) for no batch normalization.": "BN on the source half batch and target half batch respectively,"
        },
        {
          "Table 2: UAR (%) for no batch normalization.": "namely BN3. For the second data feeding method, BN is per-"
        },
        {
          "Table 2: UAR (%) for no batch normalization.": "formed on the whole batch from each domain, namely BN4."
        },
        {
          "Table 2: UAR (%) for no batch normalization.": ""
        },
        {
          "Table 2: UAR (%) for no batch normalization.": "The evaluation results of\nthe above four\ntypes of BN are"
        },
        {
          "Table 2: UAR (%) for no batch normalization.": "shown in Table 3. On the one hand,\nthe average results of all"
        },
        {
          "Table 2: UAR (%) for no batch normalization.": "BN1-3 are higher than BN4. This proves that it is better to com-"
        },
        {
          "Table 2: UAR (%) for no batch normalization.": ""
        },
        {
          "Table 2: UAR (%) for no batch normalization.": "bine data from both source and target domains in one batch than"
        },
        {
          "Table 2: UAR (%) for no batch normalization.": "batch them separately. Therefore,\nit\nis important for the train-"
        },
        {
          "Table 2: UAR (%) for no batch normalization.": ""
        },
        {
          "Table 2: UAR (%) for no batch normalization.": "ing of DANN to ensure the guiding gradient signal comes from"
        },
        {
          "Table 2: UAR (%) for no batch normalization.": ""
        },
        {
          "Table 2: UAR (%) for no batch normalization.": "both source and target domains at each training step. On the"
        },
        {
          "Table 2: UAR (%) for no batch normalization.": ""
        },
        {
          "Table 2: UAR (%) for no batch normalization.": "other hand, we can also ﬁnd the average results of both BN1-2"
        },
        {
          "Table 2: UAR (%) for no batch normalization.": "are better than BN3. The main difference between BN1-2 and"
        },
        {
          "Table 2: UAR (%) for no batch normalization.": "is performed BN on\nBN3 is whether the input features for Gl"
        },
        {
          "Table 2: UAR (%) for no batch normalization.": ""
        },
        {
          "Table 2: UAR (%) for no batch normalization.": "the whole batch (BN1-2) or on the source and target half sepa-"
        },
        {
          "Table 2: UAR (%) for no batch normalization.": "rately (BN3). This result presents that it is more suitable to feed"
        },
        {
          "Table 2: UAR (%) for no batch normalization.": "the language classiﬁer with features performed BN on the entire"
        },
        {
          "Table 2: UAR (%) for no batch normalization.": "batch. Besides, it is also worth noting that when training on the"
        },
        {
          "Table 2: UAR (%) for no batch normalization.": "smaller database of RECOLA (1,308 utterances),\nthe results of"
        },
        {
          "Table 2: UAR (%) for no batch normalization.": ""
        },
        {
          "Table 2: UAR (%) for no batch normalization.": "all\nfour settings don’t show signiﬁcant difference.\nTherefore,"
        },
        {
          "Table 2: UAR (%) for no batch normalization.": ""
        },
        {
          "Table 2: UAR (%) for no batch normalization.": "this study empirically suggests that BN1 or BN2 is a more rec-"
        },
        {
          "Table 2: UAR (%) for no batch normalization.": ""
        },
        {
          "Table 2: UAR (%) for no batch normalization.": "ommended way for BN of\nfeatures, when training the DANN"
        },
        {
          "Table 2: UAR (%) for no batch normalization.": "model on a larger corpus."
        },
        {
          "Table 2: UAR (%) for no batch normalization.": "Table 3: UAR (%) for four different ways of data combination"
        },
        {
          "Table 2: UAR (%) for no batch normalization.": "for batch normalization."
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "7. References": "",
          "[17] Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle,": "F.\nLaviolette, M. Marchand,\nand V.\nLempitsky,\n“Domain-"
        },
        {
          "7. References": "[1] M. El Ayadi, M. S. Kamel, and F. Karray, “Survey on speech emo-",
          "[17] Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle,": ""
        },
        {
          "7. References": "",
          "[17] Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle,": "adversarial training of neural networks,” The Journal of Machine"
        },
        {
          "7. References": "tion recognition: Features, classiﬁcation schemes, and databases,”",
          "[17] Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle,": ""
        },
        {
          "7. References": "",
          "[17] Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle,": "Learning Research, vol. 17, no. 1, pp. 2096–2030, 2016."
        },
        {
          "7. References": "Pattern Recognition, vol. 44, no. 3, pp. 572–587, 2011.",
          "[17] Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle,": ""
        },
        {
          "7. References": "",
          "[17] Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle,": "[18]\nS. Latif,\nJ. Qadir, and M. Bilal, “Unsupervised adversarial do-"
        },
        {
          "7. References": "[2] R. Li, Z. Wu, J. Jia, Y. Bu, S. Zhao, and H. Meng, “Towards dis-",
          "[17] Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle,": ""
        },
        {
          "7. References": "",
          "[17] Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle,": "main adaptation for cross-lingual speech emotion recognition,” in"
        },
        {
          "7. References": "criminative representation learning for speech emotion recogni-",
          "[17] Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle,": ""
        },
        {
          "7. References": "",
          "[17] Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle,": "International Conference on Affective Computing and Intelligent"
        },
        {
          "7. References": "tion,” in International Joint Conference on Artiﬁcial Intelligence",
          "[17] Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle,": ""
        },
        {
          "7. References": "",
          "[17] Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle,": "Interaction (ACII), 2019, pp. 732–737."
        },
        {
          "7. References": "(IJCAI), 2019, pp. 5060–5066.",
          "[17] Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle,": ""
        },
        {
          "7. References": "",
          "[17] Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle,": "[19] M. Tu, Y. Tang, J. Huang, X. He, and B. Zhou, “Towards adversar-"
        },
        {
          "7. References": "[3] M. A. Jalal, R. K. Moore, and T. Hain, “Spatio-temporal context",
          "[17] Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle,": "ial learning of speaker-invariant representation for speech emotion"
        },
        {
          "7. References": "modelling for speech emotion classiﬁcation,” in IEEE Automatic",
          "[17] Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle,": "recognition,” arXiv preprint arXiv:1903.09606, 2019."
        },
        {
          "7. References": "Speech Recognition and Understanding Workshop (ASRU), 2019,",
          "[17] Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle,": ""
        },
        {
          "7. References": "",
          "[17] Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle,": "[20]\nZ. Lian, J. Tao, B. Liu, and J. Huang, “Domain adversarial learn-"
        },
        {
          "7. References": "pp. 853–859.",
          "[17] Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle,": ""
        },
        {
          "7. References": "",
          "[17] Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle,": "ing for emotion recognition,” arXiv preprint arXiv:1910.13807,"
        },
        {
          "7. References": "[4]\nP. Yenigalla, A. Kumar, S. Tripathi, C. Singh, S. Kar, and J. Vepa,",
          "[17] Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle,": "2019."
        },
        {
          "7. References": "“Speech emotion recognition using spectrogram & phoneme em-",
          "[17] Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle,": ""
        },
        {
          "7. References": "",
          "[17] Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle,": "[21] M. Abdelwahab and C. Busso, “Domain adversarial for acoustic"
        },
        {
          "7. References": "bedding,” in Conference of the International Speech Communica-",
          "[17] Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle,": ""
        },
        {
          "7. References": "",
          "[17] Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle,": "emotion recognition,” IEEE/ACM Transactions on Audio, Speech,"
        },
        {
          "7. References": "tion Association (INTERSPEECH), 2018, pp. 3688–3692.",
          "[17] Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle,": ""
        },
        {
          "7. References": "",
          "[17] Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle,": "and Language Processing, vol. 26, no. 12, pp. 2423–2435, 2018."
        },
        {
          "7. References": "[5]\nS. M. Feraru, D. Schuller et al., “Cross-language acoustic emo-",
          "[17] Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle,": ""
        },
        {
          "7. References": "",
          "[17] Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle,": "[22] G. Saon, G. Kurata, T. Sercu, K. Audhkhasi, S. Thomas, D. Dim-"
        },
        {
          "7. References": "tion recognition: An overview and some tendencies,” in Interna-",
          "[17] Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle,": ""
        },
        {
          "7. References": "",
          "[17] Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle,": "itriadis, X. Cui,\nB. Ramabhadran, M.\nPicheny,\nL.-L.\nLim,"
        },
        {
          "7. References": "tional Conference on Affective Computing and Intelligent Interac-",
          "[17] Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle,": ""
        },
        {
          "7. References": "",
          "[17] Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle,": "B. Roomi, and P. Hall, “English conversational\ntelephone speech"
        },
        {
          "7. References": "tion (ACII), 2015, pp. 125–131.",
          "[17] Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle,": ""
        },
        {
          "7. References": "",
          "[17] Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle,": "recognition by humans and machines,” in Conference of the Inter-"
        },
        {
          "7. References": "[6] H. Zhou and K. Chen,\n“Transferable positive/negative\nspeech",
          "[17] Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle,": "national Speech Communication Association (INTERSPEECH),"
        },
        {
          "7. References": "emotion recognition via\nclass-wise\nadversarial domain adapta-",
          "[17] Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle,": "2017, pp. 132–136."
        },
        {
          "7. References": "tion,” in IEEE International Conference on Acoustics, Speech and",
          "[17] Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle,": ""
        },
        {
          "7. References": "",
          "[17] Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle,": "[23] Y. Tu, M.-W. Mak, and J.-T. Chien, “Variational domain adversar-"
        },
        {
          "7. References": "Signal Processing (ICASSP), 2019, pp. 3732–3736.",
          "[17] Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle,": ""
        },
        {
          "7. References": "",
          "[17] Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle,": "ial\nlearning for speaker veriﬁcation,” in Conference of"
        },
        {
          "7. References": "",
          "[17] Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle,": "national Speech Communication Association (INTERSPEECH),"
        },
        {
          "7. References": "[7] M. Neumann and N. g. Thang Vu, “Cross-lingual and multilin-",
          "[17] Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle,": ""
        },
        {
          "7. References": "",
          "[17] Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle,": "2019, pp. 4315–4319."
        },
        {
          "7. References": "gual speech emotion recognition on English and French,” in IEEE",
          "[17] Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle,": ""
        },
        {
          "7. References": "International Conference on Acoustics, Speech and Signal Pro-",
          "[17] Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle,": "[24]\nS. Ioffe and C. Szegedy, “Batch normalization: Accelerating deep"
        },
        {
          "7. References": "cessing (ICASSP), 2018, pp. 5769–5773.",
          "[17] Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle,": "network training by reducing internal covariate shift,” in Interna-"
        },
        {
          "7. References": "",
          "[17] Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle,": "tional Conference on Machine Learning (ICML), 2015, pp. 448–"
        },
        {
          "7. References": "[8] B. Schuller, B. Vlasenko, F. Eyben, M. Wollmer, A. Stuhlsatz,",
          "[17] Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle,": ""
        },
        {
          "7. References": "",
          "[17] Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle,": "456."
        },
        {
          "7. References": "A. Wendemuth, and G. Rigoll, “Cross-corpus acoustic emotion",
          "[17] Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle,": ""
        },
        {
          "7. References": "recognition: Variances and strategies,” IEEE Transactions on Af-",
          "[17] Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle,": "[25] C. Busso, M. Bulut, C.-C. Lee, A. Kazemzadeh, E. Mower,"
        },
        {
          "7. References": "fective Computing, vol. 1, no. 2, pp. 119–131, 2010.",
          "[17] Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle,": "S. Kim, J. N. Chang, S. Lee, and S. S. Narayanan, “IEMOCAP:"
        },
        {
          "7. References": "",
          "[17] Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle,": "Interactive emotional dyadic motion capture database,” Language"
        },
        {
          "7. References": "[9] H. Kaya and A. A. Karpov,\n“Efﬁcient and effective strategies",
          "[17] Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle,": ""
        },
        {
          "7. References": "",
          "[17] Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle,": "Resources and Evaluation, vol. 42, no. 4, p. 335, 2008."
        },
        {
          "7. References": "for cross-corpus acoustic emotion recognition,” Neurocomputing,",
          "[17] Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle,": ""
        },
        {
          "7. References": "vol. 275, pp. 1028–1034, 2018.",
          "[17] Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle,": "[26]\nF. Ringeval, A. Sonderegger,\nJ. Sauer, and D. Lalanne, “Intro-"
        },
        {
          "7. References": "",
          "[17] Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle,": "ducing the RECOLA multimodal corpus of remote collaborative"
        },
        {
          "7. References": "[10] Y. Zong, W. Zheng, T. Zhang,\nand X. Huang,\n“Cross-corpus",
          "[17] Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle,": ""
        },
        {
          "7. References": "",
          "[17] Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle,": "and affective interactions,” in IEEE International Conference and"
        },
        {
          "7. References": "speech\nemotion\nrecognition\nbased\non\ndomain-adaptive\nleast-",
          "[17] Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle,": ""
        },
        {
          "7. References": "",
          "[17] Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle,": "Workshops on Automatic Face and Gesture Recognition (FG),"
        },
        {
          "7. References": "squares\nregression,”\nIEEE Signal Processing Letters,\nvol. 23,",
          "[17] Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle,": ""
        },
        {
          "7. References": "",
          "[17] Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle,": "2013, pp. 1–8."
        },
        {
          "7. References": "no. 5, pp. 585–589, 2016.",
          "[17] Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle,": ""
        },
        {
          "7. References": "",
          "[17] Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle,": "[27] D. P. Kingma and J. Ba, “Adam: A method for stochastic opti-"
        },
        {
          "7. References": "[11]\nP. Song, W. Zheng, S. Ou, X. Zhang, Y. Jin, J. Liu, and Y. Yu,",
          "[17] Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle,": ""
        },
        {
          "7. References": "",
          "[17] Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle,": "mization,” in International Conference on Learning Representa-"
        },
        {
          "7. References": "“Cross-corpus speech emotion recognition based on transfer non-",
          "[17] Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle,": ""
        },
        {
          "7. References": "",
          "[17] Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle,": "tions (ICLR), 2015."
        },
        {
          "7. References": "negative matrix factorization,” Speech Communication, vol. 83,",
          "[17] Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle,": ""
        },
        {
          "7. References": "",
          "[17] Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle,": "[28] K. Sridhar, S. Parthasarathy, and C. Busso, “Role of\nregulariza-"
        },
        {
          "7. References": "pp. 34–41, 2016.",
          "[17] Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle,": ""
        },
        {
          "7. References": "",
          "[17] Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle,": "tion in the prediction of valence\nfrom speech.”\nin Conference"
        },
        {
          "7. References": "[12] H. Sagha,\nJ. Deng, M. Gavryukova,\nJ. Han,\nand B. Schuller,",
          "[17] Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle,": "of the International Speech Communication Association (INTER-"
        },
        {
          "7. References": "“Cross lingual speech emotion recognition using canonical cor-",
          "[17] Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle,": "SPEECH), 2018, pp. 941–945."
        },
        {
          "7. References": "relation analysis on principal component subspace,” in IEEE in-",
          "[17] Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle,": ""
        },
        {
          "7. References": "",
          "[17] Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle,": "[29] B. Desplanques and K. Demuynck, “Cross-lingual speech emo-"
        },
        {
          "7. References": "ternational conference on acoustics, speech and signal processing",
          "[17] Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle,": ""
        },
        {
          "7. References": "",
          "[17] Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle,": "of\ntion\nrecognition\nthrough\nfactor\nanalysis,”\nin Conference"
        },
        {
          "7. References": "(ICASSP), 2016, pp. 5800–5804.",
          "[17] Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle,": ""
        },
        {
          "7. References": "",
          "[17] Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle,": "the\nInternational Speech Communication Association (INTER-"
        },
        {
          "7. References": "[13]\nJ. Deng, Z. Zhang, F. Eyben, and B. Schuller, “Autoencoder-based",
          "[17] Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle,": "SPEECH), 2018, pp. 3648–3652."
        },
        {
          "7. References": "unsupervised domain adaptation for speech emotion recognition,”",
          "[17] Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle,": ""
        },
        {
          "7. References": "",
          "[17] Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle,": "[30]\nS. Wu, T. H. Falk, and W.-Y. Chan, “Automatic speech emotion"
        },
        {
          "7. References": "IEEE Signal Processing Letters, vol. 21, no. 9, pp. 1068–1072,",
          "[17] Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle,": ""
        },
        {
          "7. References": "",
          "[17] Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle,": "recognition using modulation spectral features,” Speech Commu-"
        },
        {
          "7. References": "2014.",
          "[17] Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle,": ""
        },
        {
          "7. References": "",
          "[17] Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle,": "nication, vol. 53, no. 5, pp. 768–785, 2011."
        },
        {
          "7. References": "[14]\nJ. Deng, R. Xia, Z. Zhang, Y. Liu, and B. Schuller, “Introducing",
          "[17] Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle,": ""
        },
        {
          "7. References": "",
          "[17] Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle,": "[31]\nS. Parthasarathy, R. Cowie,\nand C. Busso,\n“Using agreement"
        },
        {
          "7. References": "shared-hidden-layer autoencoders for\ntransfer\nlearning and their",
          "[17] Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle,": ""
        },
        {
          "7. References": "",
          "[17] Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle,": "on direction of change to build rank-based emotion classiﬁers,”"
        },
        {
          "7. References": "application in acoustic emotion recognition,”\nin IEEE Interna-",
          "[17] Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle,": ""
        },
        {
          "7. References": "",
          "[17] Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle,": "IEEE/ACM Transactions on Audio, Speech, and Language Pro-"
        },
        {
          "7. References": "tional Conference on Acoustics, Speech and Signal Processing",
          "[17] Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle,": ""
        },
        {
          "7. References": "",
          "[17] Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle,": "cessing, vol. 24, no. 11, pp. 2108–2121, 2016."
        },
        {
          "7. References": "(ICASSP), 2014, pp. 4818–4822.",
          "[17] Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle,": ""
        },
        {
          "7. References": "",
          "[17] Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle,": "[32] G. Trigeorgis, F. Ringeval, R. Brueckner, E. Marchi, M. A. Nico-"
        },
        {
          "7. References": "[15]\nJ. Deng, X. Xu, Z. Zhang, S. Fr¨uhholz, and B. Schuller, “Uni-",
          "[17] Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle,": ""
        },
        {
          "7. References": "",
          "[17] Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle,": "laou, B. Schuller, and S. Zafeiriou, “Adieu features? End-to-end"
        },
        {
          "7. References": "versum autoencoder-based domain adaptation for speech emotion",
          "[17] Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle,": ""
        },
        {
          "7. References": "",
          "[17] Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle,": "speech emotion recognition using a deep convolutional recurrent"
        },
        {
          "7. References": "recognition,” IEEE Signal Processing Letters, vol. 24, no. 4, pp.",
          "[17] Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle,": ""
        },
        {
          "7. References": "",
          "[17] Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle,": "network,” in IEEE international conference on acoustics, speech"
        },
        {
          "7. References": "500–504, 2017.",
          "[17] Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle,": ""
        },
        {
          "7. References": "",
          "[17] Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle,": "and signal processing (ICASSP), 2016, pp. 5200–5204."
        },
        {
          "7. References": "[16]\nI. Goodfellow,\nJ. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-",
          "[17] Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle,": ""
        },
        {
          "7. References": "Farley, S. Ozair, A. Courville, and Y. Bengio, “Generative adver-",
          "[17] Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle,": ""
        },
        {
          "7. References": "Information Processing Sys-\nsarial nets,” in Advances in Neural",
          "[17] Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle,": ""
        },
        {
          "7. References": "tems, 2014, pp. 2672–2680.",
          "[17] Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle,": ""
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Survey on speech emotion recognition: Features, classification schemes, and databases",
      "authors": [
        "M Ayadi",
        "M Kamel",
        "F Karray"
      ],
      "year": "2011",
      "venue": "Pattern Recognition"
    },
    {
      "citation_id": "2",
      "title": "Towards discriminative representation learning for speech emotion recognition",
      "authors": [
        "R Li",
        "Z Wu",
        "J Jia",
        "Y Bu",
        "S Zhao",
        "H Meng"
      ],
      "year": "2019",
      "venue": "International Joint Conference on Artificial Intelligence (IJCAI)"
    },
    {
      "citation_id": "3",
      "title": "Spatio-temporal context modelling for speech emotion classification",
      "authors": [
        "M Jalal",
        "R Moore",
        "T Hain"
      ],
      "year": "2019",
      "venue": "IEEE Automatic Speech Recognition and Understanding Workshop"
    },
    {
      "citation_id": "4",
      "title": "Speech emotion recognition using spectrogram & phoneme embedding",
      "authors": [
        "P Yenigalla",
        "A Kumar",
        "S Tripathi",
        "C Singh",
        "S Kar",
        "J Vepa"
      ],
      "year": "2018",
      "venue": "Conference of the International Speech Communication Association (INTERSPEECH)"
    },
    {
      "citation_id": "5",
      "title": "Cross-language acoustic emotion recognition: An overview and some tendencies",
      "authors": [
        "S Feraru",
        "D Schuller"
      ],
      "year": "2015",
      "venue": "International Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "6",
      "title": "Transferable positive/negative speech emotion recognition via class-wise adversarial domain adaptation",
      "authors": [
        "H Zhou",
        "K Chen"
      ],
      "year": "2019",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "7",
      "title": "Cross-lingual and multilingual speech emotion recognition on English and French",
      "authors": [
        "M Neumann",
        "N Vu"
      ],
      "year": "2018",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "8",
      "title": "Cross-corpus acoustic emotion recognition: Variances and strategies",
      "authors": [
        "B Schuller",
        "B Vlasenko",
        "F Eyben",
        "M Wollmer",
        "A Stuhlsatz",
        "A Wendemuth",
        "G Rigoll"
      ],
      "year": "2010",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "9",
      "title": "Efficient and effective strategies for cross-corpus acoustic emotion recognition",
      "authors": [
        "H Kaya",
        "A Karpov"
      ],
      "year": "2018",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "10",
      "title": "Cross-corpus speech emotion recognition based on domain-adaptive leastsquares regression",
      "authors": [
        "Y Zong",
        "W Zheng",
        "T Zhang",
        "X Huang"
      ],
      "year": "2016",
      "venue": "IEEE Signal Processing Letters"
    },
    {
      "citation_id": "11",
      "title": "Cross-corpus speech emotion recognition based on transfer nonnegative matrix factorization",
      "authors": [
        "P Song",
        "W Zheng",
        "S Ou",
        "X Zhang",
        "Y Jin",
        "J Liu",
        "Y Yu"
      ],
      "year": "2016",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "12",
      "title": "Cross lingual speech emotion recognition using canonical correlation analysis on principal component subspace",
      "authors": [
        "H Sagha",
        "J Deng",
        "M Gavryukova",
        "J Han",
        "B Schuller"
      ],
      "year": "2016",
      "venue": "IEEE international conference on acoustics, speech and signal processing"
    },
    {
      "citation_id": "13",
      "title": "Autoencoder-based unsupervised domain adaptation for speech emotion recognition",
      "authors": [
        "J Deng",
        "Z Zhang",
        "F Eyben",
        "B Schuller"
      ],
      "year": "2014",
      "venue": "IEEE Signal Processing Letters"
    },
    {
      "citation_id": "14",
      "title": "Introducing shared-hidden-layer autoencoders for transfer learning and their application in acoustic emotion recognition",
      "authors": [
        "J Deng",
        "R Xia",
        "Z Zhang",
        "Y Liu",
        "B Schuller"
      ],
      "year": "2014",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "15",
      "title": "Universum autoencoder-based domain adaptation for speech emotion recognition",
      "authors": [
        "J Deng",
        "X Xu",
        "Z Zhang",
        "S Frühholz",
        "B Schuller"
      ],
      "year": "2017",
      "venue": "IEEE Signal Processing Letters"
    },
    {
      "citation_id": "16",
      "title": "Generative adversarial nets",
      "authors": [
        "I Goodfellow",
        "J Pouget-Abadie",
        "M Mirza",
        "B Xu",
        "D Warde-Farley",
        "S Ozair",
        "A Courville",
        "Y Bengio"
      ],
      "year": "2014",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "17",
      "title": "Domainadversarial training of neural networks",
      "authors": [
        "Y Ganin",
        "E Ustinova",
        "H Ajakan",
        "P Germain",
        "H Larochelle",
        "F Laviolette",
        "M Marchand",
        "V Lempitsky"
      ],
      "year": "2016",
      "venue": "The Journal of Machine Learning Research"
    },
    {
      "citation_id": "18",
      "title": "Unsupervised adversarial domain adaptation for cross-lingual speech emotion recognition",
      "authors": [
        "S Latif",
        "J Qadir",
        "M Bilal"
      ],
      "year": "2019",
      "venue": "International Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "19",
      "title": "Towards adversarial learning of speaker-invariant representation for speech emotion recognition",
      "authors": [
        "M Tu",
        "Y Tang",
        "J Huang",
        "X He",
        "B Zhou"
      ],
      "year": "2019",
      "venue": "Towards adversarial learning of speaker-invariant representation for speech emotion recognition",
      "arxiv": "arXiv:1903.09606"
    },
    {
      "citation_id": "20",
      "title": "Domain adversarial learning for emotion recognition",
      "authors": [
        "Z Lian",
        "J Tao",
        "B Liu",
        "J Huang"
      ],
      "year": "2019",
      "venue": "Domain adversarial learning for emotion recognition",
      "arxiv": "arXiv:1910.13807"
    },
    {
      "citation_id": "21",
      "title": "Domain adversarial for acoustic emotion recognition",
      "authors": [
        "M Abdelwahab",
        "C Busso"
      ],
      "year": "2018",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "22",
      "title": "English conversational telephone speech recognition by humans and machines",
      "authors": [
        "G Saon",
        "G Kurata",
        "T Sercu",
        "K Audhkhasi",
        "S Thomas",
        "D Dimitriadis",
        "X Cui",
        "B Ramabhadran",
        "M Picheny",
        "L.-L Lim",
        "B Roomi",
        "P Hall"
      ],
      "year": "2017",
      "venue": "Conference of the International Speech Communication Association"
    },
    {
      "citation_id": "23",
      "title": "Variational domain adversarial learning for speaker verification",
      "authors": [
        "Y Tu",
        "M.-W Mak",
        "J.-T Chien"
      ],
      "year": "2019",
      "venue": "Conference of the International Speech Communication Association (INTERSPEECH)"
    },
    {
      "citation_id": "24",
      "title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift",
      "authors": [
        "S Ioffe",
        "C Szegedy"
      ],
      "year": "2015",
      "venue": "International Conference on Machine Learning (ICML)"
    },
    {
      "citation_id": "25",
      "title": "IEMOCAP: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language Resources and Evaluation"
    },
    {
      "citation_id": "26",
      "title": "Introducing the RECOLA multimodal corpus of remote collaborative and affective interactions",
      "authors": [
        "F Ringeval",
        "A Sonderegger",
        "J Sauer",
        "D Lalanne"
      ],
      "year": "2013",
      "venue": "IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG)"
    },
    {
      "citation_id": "27",
      "title": "Adam: A method for stochastic optimization",
      "authors": [
        "D Kingma",
        "J Ba"
      ],
      "year": "2015",
      "venue": "International Conference on Learning Representations (ICLR)"
    },
    {
      "citation_id": "28",
      "title": "Role of regularization in the prediction of valence from speech",
      "authors": [
        "K Sridhar",
        "S Parthasarathy",
        "C Busso"
      ],
      "year": "2018",
      "venue": "Conference of the International Speech Communication Association (INTER-SPEECH)"
    },
    {
      "citation_id": "29",
      "title": "Cross-lingual speech emotion recognition through factor analysis",
      "authors": [
        "B Desplanques",
        "K Demuynck"
      ],
      "year": "2018",
      "venue": "Conference of the International Speech Communication Association (INTER-SPEECH)"
    },
    {
      "citation_id": "30",
      "title": "Automatic speech emotion recognition using modulation spectral features",
      "authors": [
        "S Wu",
        "T Falk",
        "W.-Y Chan"
      ],
      "year": "2011",
      "venue": "Automatic speech emotion recognition using modulation spectral features"
    },
    {
      "citation_id": "31",
      "title": "Using agreement on direction of change to build rank-based emotion classifiers",
      "authors": [
        "S Parthasarathy",
        "R Cowie",
        "C Busso"
      ],
      "year": "2016",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "32",
      "title": "Adieu features? End-to-end speech emotion recognition using a deep convolutional recurrent network",
      "authors": [
        "G Trigeorgis",
        "F Ringeval",
        "R Brueckner",
        "E Marchi",
        "M Nicolaou",
        "B Schuller",
        "S Zafeiriou"
      ],
      "year": "2016",
      "venue": "IEEE international conference on acoustics, speech and signal processing"
    }
  ]
}