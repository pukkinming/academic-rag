{
  "paper_id": "2012.04581v2",
  "title": "Meranet: Facial Micro-Expression Recognition Using 3D Residual Attention Network",
  "published": "2020-12-07T16:41:42Z",
  "authors": [
    "Viswanatha Reddy Gajjala",
    "Sai Prasanna Teja Reddy",
    "Snehasis Mukherjee",
    "Shiv Ram Dubey"
  ],
  "keywords": [
    "Channel Attention",
    "Spatio-Temporal Attention",
    "3D ResNet",
    "Micro Expression Recognition"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Micro-expression has emerged as a promising modality in affective computing due to its high objectivity in emotion detection. Despite the higher recognition accuracy provided by the deep learning models, there are still significant scope for improvements in micro-expression recognition techniques. The presence of microexpressions in small-local regions of the face, as well as the limited size of available databases, continue to limit the accuracy in recognizing micro-expressions. In this work, we propose a facial microexpression recognition model using 3D residual attention network named MERANet to tackle such challenges. The proposed model takes advantage of spatial-temporal attention and channel attention together, to learn deeper fine-grained subtle features for classification of emotions. Further, the proposed model encompasses both spatial and temporal information simultaneously using the 3D kernels and residual connections. Moreover, the channel features and spatio-temporal features are re-calibrated using the channel and spatio-temporal attentions, respectively in each residual module. Our attention mechanism enables the model to learn to focus on different facial areas of interest. The experiments are conducted on benchmark facial micro-expression datasets. A superior performance is observed as compared to the state-of-the-art for facial micro-expression recognition on benchmark data. \n CCS CONCEPTS â€¢ Computing methodologies â†’ Activity recognition and understanding.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "In recent years, facial expression recognition using deep learning techniques have gained lot of popularity among the research community because of its potential applications across various fields, such as psychology, marketing, security, etc. However, recognizing true human emotions from facial expressions often becomes unreliable due to the performer's deliberate control on the expressions, depending on the social circumstances. Hence, micro-expression recognition has evolved as an attempt to capture the true emotion of a performer  [3] . Micro-expressions are involuntarily exposed short term expressions that appear during an active suppression of facial expression to hide the true emotion. Micro-expressions are very short spanned (fraction of a second; usually around 0.5 seconds), which makes the task of capturing the relevant features for recognizing emotion as challenging  [4] .\n\nEfforts have been made to apply Convolutional Neural Networks (CNNs) for micro-expression recognition  [25, 34] . Although CNN models are able to automatically capture useful features from the videos, but they are often unable to capture the minute textural changes on various parts of the face, observed during microexpressions. To capture the minute textural information, facial landmarks are used for feature extraction  [11] . However, even the temporal features play an important role in recognising facial microexpressions.\n\nA few efforts have also been made in capturing temporal cues from optical flow, for recognizing micro-expressions  [5, 17, 18] . However, features based on optical flow are frequently hampered by unnecessary motion information from background pixels. Another line of thought for capturing temporal features suggests applying a Long Short Term Memory (LSTM) on the spatial features  [12] [13] [14] . A few 3D CNN based models can also be found in the literature, to capture temporal cues  [19, 30] . The existing approaches of applying temporal features often fails to capture the minute texture information from micro-expression videos, where the expression continues for the very short span of time. Attention based models often show efficacy in extracting the minute texture information  [15, 24] . However, attention mechanism is still not properly explored for micro-expression recognition task.  Wang et al.  proposed a spatial attention model for micro-expression recognition  [36] .  Wang et al.  proposed an attention mechanism for recognizing micro-expression using just the apex frame from the micro-expression video. The model completely ignores the temporal information between the frames. Whereas, the proposed model considers both spatial and temporal information simultaneously by applying 3D Conv operations while recalibrating features using attention mechanism and residual connections to learn more fine grained information. To capture the temporal information from the short span of video, where the micro-expression takes place, we propose a 3D channel and spatio-temporal attentions based model called as MERANet. Due to the utilization of the 3D Spatial attention and Channel attention, the proposed model is able to ignore the background information while focusing on the region of interest precisely which is consistent across the frames of the video and responsible for the given micro expression. Fig.  1  represents the overall idea of the proposed MERANet model. The main contributions of this paper are as follows:\n\nâ€¢ A 3D residual attention is introduced in this paper for recognizing subtle features related to micro-expressions by attention guided re-calibration of features. â€¢ In order to prioritize the important channels and spatiotemporal features, the channel attention and spatio-temporal attention blocks are used as part of the residual module. â€¢ The increase in number of parameters and FLOPs in the proposed model is negligible as compared to the vanilla model.\n\nâ€¢ Extensive experiments are performed to show the impact of the proposed 3D residual attention model on micro-expression recognition. We show the saliency map to visualize the effect of the proposed approach in extracting features from faces during expression.\n\nNext we provide a survey of literature for facial micro-expression recognition.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Literature Review",
      "text": "The proposed method aims to recognize the micro-expressions using attention models. This section provides a discussion on the literature related to micro-expression recognition, followed by a discussion on how attention models are applied to solve various problems in Computer Vision.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Facial Micro-Expression Recognition",
      "text": "The facial micro-expression recognition methods can be categorized into two categories: handcrafted feature based methods and deep learning based methods.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "2.1.1",
      "text": "Handcrafted Feature based Methods. The traditional approaches for micro-expression recognition were mostly focused on superficial attributes involving complex calculations. Polikovsky  [28]  proposed a three step algorithm that divides the facial region into twelve parts to obtain facial cubes. They computed 3D orientation histogram descriptor to capture correlations across the frames for each facial cube. They classified the expression based on the normalized descriptor vectors. Shreve et al.  [32]  utilized the facial strain caused by the non-rigid motion of facial muscles during an expression. The magnitude of strain was found using optical flow over different regions of the face. Pfister et al.  [27]  used temporal interpolation model to address variable video length problem. They introduced spatiotemporal local texture descriptors to generate feature vector and finally, various kinds of classifiers have been tried to classify the micro-expression. Huang et al.  [9]  proposed a spatiotemporal completed local quantized pattern that extracts sign, magnitude and orientation components from an expression and constructed a codebook for each component that holds more dynamic pattern representations. Liu et al.  [20]  proposed a method that generates a compact feature vector by finding main directional mean optical flow using optical flow field over different regions of interest of the face. The final feature vector is fed into an SVM to classify micro-expressions. Zhao et al.  [44]  proposed a computationally compact feature using Local Binary Pattern -Three Orthogonal Planes (LBP-TOP) which can efficiently extract co-occurring features from neighboring points. Lu et al.  [22]  proposed a fusion based feature that finds differentials of optical flow over horizontal and vertical components. The fused feature vector is fed into an SVM classifier for classification.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Deep",
      "text": "Learning based Methods. In most of the computer vision applications, the deep learning techniques outperform the handcrafted feature based methods due to its increased feature learning capability from the data  [16] . Patel et al.  [25]  proposed a deep learning model for micro-expression recognition, where they used transfer learning to initialize the CNN with weights of a CNN trained on largely available macro-expression data to avoid overfitting. Takalkar implemented a variety of data augmentation techniques to generate synthetic data, resembling different environments to deal with lack of training data. Finally a CNN is applied to classify micro-expressions  [34] . Qiuyu et al.  [18]  proposed a three step algorithm where a multi-task CNN was used to find the facial landmarks. Deep CNN was used to calibrate the optical flow features from different facial regions. Finally, an SVM was employed to classify the micro-expressions. Liong et al.  [5]  collected the optical flow feature for each micro-expression and used it as input to a CNN for classification. Li et al.  [17]  proposed a 3D flow based CNN that uses gray scale images along with components of optical flow in different directions as input to classify micro-expressions. Wang et al.  [36]  proposed an image based residual model that uses microattention units and transfer learning to classify micro-expressions. The model fails to consider temporal information across the frames, which is important for micro-expression videos. Several proposals can be found in the literature  [12] [13] [14]  that used two step architecture where, a CNN is applied on each frame of the micro-expression video to generate spatial features and then a Long Short Term Memory (LSTM) based Recurrent Neural Network (RNN) is used to learn the temporal inter-dependencies between different spatial features from previous step. Reddy et al.  [30]  proposed a 3D CNN based model that tries to capture both spatial and temporal information in a micro-expression video simultaneous using 3D kernels. Liong et al.  [19]  proposed a shallow triple stream 3D CNN that uses optical strain, horizontal and vertical flows from the onset and apex frames to classify micro-expressions.\n\nThe existing methods focus neither on the minute spatial changes on face regions during micro-expressions nor on the temporal cues, as the micro-expressions last for a small span of time. A spatial attention mechanism can help in capturing the minute spatial changes in facial regions during micro-expression. In addition to that, a temporal attention can help in focusing on the smaller span of the micro-expression.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Attention Mechanism In Literature",
      "text": "The process of human perceptual analysis is based on the attention mechanism  [24] . Human vision obtains the target area within the scene, that needs to be focused on, and pays more attention on the target area to obtain more detailed information about the scene that helps to capture the visual structure of the target better  [15] . Attention mechanism is used to find the region of interest within an image and to highlight the representation of the region of interest  [10] . The two main aspects of attention mechanism in deep learning are (a) to obtain the meaningful channels for the respective input feature and (b) to pay attention to the most informative channels to obtain the salient locations.\n\nRecently, several studies attempted to apply attention mechanism to enhance the performance of CNNs in a range of visual tasks (especially the tasks where objects play vital roles), such as image classification, image localization and video understanding  [1, 33] . Wang et al.  [37]  proposed a Residual Attention Network (RAN) which uses a trunk-and-mask module to incorporate an attention mechanism. By re-weighting the feature map, the network not only performs well but is also robust to input noise. Hu et al.  [8]  introduced a Squeeze-and-Excitation module to exploit the inter-channel relationship. Adding to this, Woo et al.  [38]  introduced spatial attention to further improve the image classification performance. In  [26] , Peng et al. designed an attention branch that helps to spotlight finger micro-gesture and reduces the noise introduced from the background and wrist. The spatial attention has improved the recognition accuracy by 3%. However the attention block proposed in the study of  [26] , increased the parameters by about 2ğ‘€. Similarly, for micro-expression recognition, an attention mechanism helps the model to focus on the face and important facial regions, and suppress the irrelevant facial areas and background.\n\nThe existing attention based mechanisms have not yet utilized the 3D channel and spatio-temporal attentions, which are important in the context of facial micro-expression recognition. In this work, we design a novel and effective 3D residual attention mechanism to simultaneously learn fine and subtle features along spatial, temporal and channel dimensions. The proposed attention modules, including channel attention and spatio-temporal attention, add just a few parameters to the base model, so that the increase in computation overhead is negligible. Next we illustrate the proposed method for micro-expression recognition.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Proposed Meranet Model For Facial Micro-Expression Recognition",
      "text": "Given a facial micro-expression video, our objective is to obtain a good representation in terms of their emotional score. We propose a deep learning model, named MERANet, for micro-expression recognition using 3D residual attention network. The proposed residual attention (RA) network is constructed by stacking up multiple RA blocks. The RA block comprises of two major sub-modules: (a) A Spatio-Temporal Attention module to extract the spatio-temporal importance map and (b) A Channel Attention module to extract more discriminative features by prioritizing the important channels.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Spatio-Temporal Attention Block",
      "text": "We compute the spatio-temporal attention map by utilizing the spatial-temporal relationship among the features. The spatio-temporal attention focuses on where we need to pay more attention in the input feature volume.\n\nIn order to compute the spatio-temporal attention without increasing the model parameters, we first apply average-pooling and max-pooling operations along the channel axis and aggregate them by concatenating to generate an efficient feature descriptor\n\n, where ğ‘‡ â€² , ğ‘Š â€² and ğ» â€² represent the temporal dimension, width, and height respectively, of the generated feature descriptor. The pooling operation is employed along the channel axis as it is effective in highlighting the informative regions  [42] . The average pooling across the channels is defined as,\n\nand the max pooling across the channels is defined as,\n\nwhere ğ‘¡ âˆˆ {1, 2, ...,ğ‘‡ â€² }, â„ âˆˆ {1, 2, ..., ğ» â€² }, and ğ‘¤ âˆˆ {1, 2, ...,ğ‘Š â€² }, ğ¹ âˆˆ R ğ¶ â€² Ã—ğ‘‡ â€² Ã—ğ‘Š â€² Ã—ğ» â€² corresponds to the input feature volume and ğ¶ â€² is the number of channels in the input feature (ğ¹ ).\n\nIn order to limit the module complexity, a single convolution operation is performed on the feature descriptor to produce a more refined feature map followed by a sigmoid activation function. The final generated attention importance map (A ğ‘ ğ‘¡ ) learns the importance score for each spatial-temporal location. The A ğ‘ ğ‘¡ is defined as,\n\nwhere F ğ‘ ğ‘¡ is given as,\n\nand ğ¶ğ‘œğ‘›ğ‘ğ‘ğ‘¡ (F ğ‘ ğ‘¡ ğ‘ğ‘£ğ‘” ; F ğ‘ ğ‘¡ ğ‘šğ‘ğ‘¥ ) âˆˆ R 2Ã—ğ‘‡ â€² Ã—ğ‘Š â€² Ã—ğ» â€² is the concatenated features of average pooling and max pooling across the channel dimension,\n\nis the generated attention importance map, ğœ denotes the sigmoid function and ğ‘Š 5Ã—5Ã—5 corresponds to a convolution operation with the kernel of size 5 with same padding. Note that all the values in the importance map are in the range 0 to 1, thus mimicking the probability of being important. Basically, A ğ‘ ğ‘¡ encodes that which spatio-temporal information to emphasize or suppress. The block diagram of the spatio-temporal attention module is shown in Fig.  2 . Element-wise multiplication is employed between the input feature volume and the spatio-temporal importance map (A ğ‘ ğ‘¡ ) to re-weight each pixel value and obtain the refined feature map. The re-calibrated feature activation (ğ¹ ğ‘ ğ‘¡ ) is defined as,\n\nwhere ğ¹ ğ‘ and ğ¹ ğ‘ ğ‘¡ ğ‘ are the ğ‘ ğ‘¡â„ channel of input and re-calibrated output using spatio-temporal attention and âŠ— denotes the elementwise multiplication.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Channel Attention Block",
      "text": "The spatio-temporal attention captures the weight for each spatiotemporal feature volume. However, it is not able to distinguish the weight among the different channels of feature volume. Thus, we also propose a channel attention block in the residual framework. The channel features in the CNNs are exploited extensively to extract more discriminative features. We compute a channel attention map by utilizing the inter-channel relationship of features. The channel attention map focuses on finding the meaningful channels for the respective input feature volume. Integrating a channel attention map helps to improve the learning ability of the model by re-calibrating the channels of the input feature volume with increased or decreased weight. In order to capture the channel attention effectively for a feature volume (F\n\n), first we squeeze the spatial and temporal dimensions of the input feature volume by applying average pooling and max pooling and then we combine the squeezed features by concatenating them. The average pooling over spatial and temporal dimensions is given as,\n\nand the max pooling over spatial and temporal dimensions are given as,\n\nwhere\n\n, ğ‘ âˆˆ {1, 2, ..., ğ¶ â€² }, and ğ¶ â€² is the number of channels.\n\nThe channel descriptors F ğ‘â„ ğ‘šğ‘ğ‘¥ and F ğ‘â„ ğ‘ğ‘£ğ‘” are then fed to a shared sub-network to fully capture the inter-dependencies. We propose two design choices for the shared sub-network. One variant is with the convolutional layers and other with the multi-layer perceptron layers (MLP). Our experimental results show that the shared network with convolutions is better than the MLP. The detailed analysis will follow in experimental results in Section 4. The shared convolutional network consists of a convolution layer with the channel reduction mechanism (through 1 Ã— 1 Ã— 1 convolution) to reduce the computation overhead. The ReLU activation is used to introduce the non-linearity followed by a dimension increasing layer to match the input channel dimension (through 1 Ã— 1 Ã— 1 convolution).\n\nThe final channel attention map (A ğ‘â„ ) is computed as,\n\nwhere ğœ and ğœ‘ correspond to the Sigmoid and ReLU activation functions, respectively, ğ‘Š ğ‘  and ğ‘Š ğ‘’ are the convolution operations in the shared network with ğ‘Š ğ‘  (.) âˆˆ R ğ¶ â€² ğ‘Ÿ Ã—1Ã—1Ã—1 and ğ‘Š ğ‘’ (.) âˆˆ R ğ¶ â€² Ã—1Ã—1Ã—1 , ğ‘Ÿ is the channel reduction ratio in the shared sub-network, and A ğ‘â„ âˆˆ R ğ¶ â€² Ã—1Ã—1Ã—1 . The reduction ratio(ğ‘Ÿ ) is set to 16 in this paper.\n\nNote that all the values in the channel importance map are in the range 0 to 1. Thus, it represents the probability of being important for each channel. The block diagram of the channel attention module is shown in Fig.  3 .\n\nChannel-wise multiplication is employed between the input feature volume and the channel importance map (A ğ‘ ) to obtain the re-calibrated channels of the output feature map. The re-calibrated feature activation (ğ¹ â€² ğ‘â„ ) is computed as, Figure  4 : The proposed 3D RA module by integrating the 3D convolutions, channel attention and spatio-temporal attention blocks. Here '+' stands for element wise addition, 'X' stands for element wise multiplication and the output corresponds to a recalibrated residual feature map that goes through channel and spatial attention sequentially.\n\nfor âˆ€ğ‘ âˆˆ {1, 2, ..., ğ¶ â€² }, âˆ€ğ‘¡ âˆˆ {1, 2, ...,ğ‘‡ â€² }, âˆ€â„ âˆˆ {1, 2, ..., ğ» â€² }, and âˆ€ğ‘¤ âˆˆ {1, 2, ...,ğ‘Š â€² }, where ğ¹ â€² and ğ¹ â€² ğ‘â„ are the input and re-calibrated output using channel attention, respectively.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Residual Attention Module",
      "text": "In this paper, we propose a 3D RA module by utilizing the spatiotemporal and channel attention blocks. In contrast to usual residual block, we introduce the attention blocks into the residual module. The residual attention module comprises of two 3D convolution operations, channel attention, spatial-temporal attention, residual connection followed by an activation function. For simplicity, we ignore the batch-norm and activation layers in the block diagram.\n\nThe block diagram for the proposed 3D RA module is shown in Fig.  4 . Suppose I âˆˆ R ğ¶Ã—ğ‘‡ Ã—ğ‘Š Ã—ğ» is the input feature volume to the proposed residual attention module and I ğ‘ğ‘œğ‘›ğ‘£ âˆˆ R ğ¶ â€² Ã—ğ‘‡ â€² Ã—ğ‘Š â€² Ã—ğ» â€² is the generated intermediate feature volume after two convolution blocks. We apply the channel attention operation over I ğ‘ğ‘œğ‘›ğ‘£ to generate the channel attention map A ğ‘â„ as defined in Eq. (  8 ) which uses the average pooling and max pooling over spatial and temporal dimensions as given in Eq. (  6 ) and  (7) , respectively. The re-calibrated intermediate feature after channel attention is given using Eq. (  9 ) as,\n\nNext, we apply the spatio-temporal attention operation over I ğ‘â„ to generate the spatio-temporal attention map A ğ‘ ğ‘¡ as defined in Eq.\n\n(3) which uses the average pooling and max pooling across channel dimension as given in Eq. (  1 ) and (  2 ), respectively. The re-calibrated intermediate feature after spatio-temporal attention is given using Eq. (  5 ) as,\n\nThe channel attention excites the important channels separately, whereas the spatio-temporal attention excites the important spatiotemporal features. The last step in the proposed residual module is to add the generated re-calibrated features with the original input of this block. The final output of the proposed residual module\n\nwhere ğœ‘ is the ReLU activation function and ğ‘‘ is a down-sampling function to change the dimension of input from ğ¶ Ã— ğ‘‡ Ã— ğ‘Š Ã— ğ» to ğ¶ â€² Ã— ğ‘‡ â€² Ã— ğ‘Š â€² Ã— ğ» â€² . The down-sampling function is used in the residual path to match the dimensions of the features I ğ‘ ğ‘¡ and I. (a) If the dimensions of the I ğ‘ ğ‘¡ and I are same then the down-sampling will be a simple identity mapping with no additional parameters added. (b) If the dimensions of the I ğ‘ ğ‘¡ and I are different then the down-sampling function will use 1 Ã— 1 Ã— 1 convolution with the stride 2 to match the dimensions.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Network Architecture",
      "text": "The residual network (ResNet) and its variants are one of the most successful architectures for image classification  [7]  [39]  [43] . A basic residual module consists of two convolutional layers in a sequence. The first convolutional layer is followed by a batch normalization and a ReLU activation, whereas the second convolution layer is followed by a batch normalization. The residual module adds the output of the second convolution layer to the input which is followed by a ReLU activation function. The ResNet uses the residual connections that enforces the network to learn the difference of transformation instead of complete transformation. It facilitates better gradient flow during the back-propagation and helps in the optimization of the network. Thus, facilitating the training of very deep networks feasible. Due to the residual property, the ResNet abates the vanishing gradient problem. We also generate the proposed MERANet architecture by following the ResNet architecture in this work. The MERANet-18 is constructed by stacking up the multiple 3D residual attention modules. We use the identity connections and zero padding in the residual paths of the residual attention module to avoid increasing the number of parameters of the model. The 3D residual attention modules are used as the basic building blocks of the proposed MERANet model. We add a stem network to get the reasonable size feature map similar to the ResNet. We also employ a fully connected layer at the end to classify the emotion score. The detailed architecture is summarized in supplementary.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Experimental Settings",
      "text": "In this section, first we provide a brief description of four benchmark micro-expression datasets used for the experiments, followed by the pre-processing details and the training settings.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Micro-Expression Datasets Used",
      "text": "We have used four benchmark micro-expression datasets to evaluate the proposed model, namely CASMEI, CASMEII, CAS(ME) 2 and SAMM.\n\nCASME I Dataset  [41] : CASME I dataset is divided into two parts: CASME A and CASME B, which are constructed under two different conditions. CASME A contains images with 1280 Ã— 720 resolution under natural light settings. Whereas, CASME B contains images with resolution of 640 Ã— 480 under the artificial light settings. Both settings have a temporal resolution of 60 fps. Altogether, CASME dataset contains 195 micro-expressions belonging to seven different classes. For this experiment, we have chosen three classes, including disgust (34 samples), repression (36 samples), and tense (45 samples) based on the higher number of samples in these classes.\n\nCASME II Dataset  [40] : CASME II is a spontaneous micro-expressions dataset built under well-controlled laboratory environment. The samples have a higher face resolution at 280 Ã— 340 pixels and a relatively high temporal resolution (200 fps). The database has five micro-expression categories. The number of samples for each category is distributed unequally (there are 60 samples for disgust and only 7 samples for sadness). Since the data is not uniformly distributed across the classes to obtain consistent results, we have used three classes, including disgust (63 samples), happy (32 samples), and surprise (28 samples) based on the higher number of samples in these classes. CAS(ME) 2 Dataset  [29] : CAS(ME) 2 dataset comprises of both macro-expressions and micro-expressions. This dataset is constructed by showing 9 elicitation videos to 22 participants under artificial light settings. The elicitation videos contain three emotions (happy, angry, and disgust) evoking videos. CAS(ME) 2 contains the samples with 640 Ã— 480 resolution at 30 frames per second. In our experiment, we have only used micro-expressions of CAS(ME) 2 dataset. The used dataset contains three expressions, namely happy (18 samples), angry (22 samples), and disgust (17 samples).\n\nSAMM Dataset  [2] : SAMM dataset comprises of 159 samples with 2040 Ã— 1088 resolution at 200 frames per second. The database has seven micro-expression categories. The number of samples for each category is distributed unequally (there are 57 samples for anger and just 6 samples for sadness). Since the data is not evenly distributed across the classes to obtain consistent results, we have used the following three classes: anger (57 samples), happiness (26 samples), and other (26 samples). We have not included rest of the 4 classes as the number of samples in each of these classes is too less (â‰¤ 15) to train a deep learning system.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Pre-Processing",
      "text": "In the experiments, first, we select the apex frame in the video corresponding to the micro-expression in order to generate a training sample. As the data is captured at different frame rates per second, we have assigned temporal depth for SAMM as 64, CASMEI and CASMEII as 32 and CAS(ME) 2 as 16. Also, we have considered samples having atleast 16 frames for CASMEI and CASMEII in this experimentation. A 16-frame clip is generated around the selected apex frame. If the video is shorter than 16 frames, then we replicate the edge frames as many times as necessary. The temporal dimension is selected based on the average number of frames available in a video clip. We spatially resize the sample to 112 Ã— 112ğ‘ğ‘–ğ‘¥ğ‘’ğ‘™ğ‘ . The size of each sample is 3 channels Ã— 16 frames Ã— 112 pixels Ã— 112 pixels, and each sample is horizontally flipped with 0.5 probability. Mean subtraction is applied for each sample by subtracting the mean values from the sample for each channel. Each sample is normalized by the standard deviation.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Training Settings",
      "text": "We have performed 75%-25% train-test split for CASME I and CASME II and 80%-20% train-test split for CAS(ME) 2 due to less samples. Among the splits 80% of data is used as a training set and the rest is used as a validation set. Stratified sampling is used for the validation split to maintain consistency across the results. The train-validation split-up is performed once and then the same training and validation sets are used for all the experiments.\n\nThe weights of both convolutional and fully-connected layers are initialized with the Xavier initialization proposed in  [6] . We set the parameter to random values uniformly drawn from [-ğ‘Ÿğ‘£, ğ‘Ÿğ‘£], where ğ‘Ÿğ‘£ is defined as, ğ‘Ÿğ‘£ = âˆšï¸ƒ 6 (ğ‘‘ ğ‘–ğ‘› +ğ‘‘ ğ‘œğ‘¢ğ‘¡ ) where ğ‘‘ ğ‘–ğ‘› and ğ‘‘ ğ‘œğ‘¢ğ‘¡ are the size of input and output channels, respectively. All biases are initialized to 0. For batch normalization layers, weights are initialized to 1.\n\nThe optimizer proposed in  [23]  is used for training. We use Categorical Cross Entropy as the loss function. Each model is trained for 100 epochs with a batch size of 8. The learning rate is initialized to 0.1. Learning rate adjustment is one of the crucial steps while training. We use the cosine annealing strategy proposed in  [21] . The cosine annealing decreases the learning rate from the initial value to 0 by following the cosine function. The intuition behind using annealing is that it helps to traverse quickly from the initial parameters to a range of good parameter values with a smaller learning rate. Thus, we can explore the deeper and narrower parts of the loss function, which potentially improves the training progress by avoiding the divergence.\n\nWe avoid the overfitting problem in two ways, 1) by using the Horizontal flip data augmentation and 2) by incorporating the warm-up scheduling along with the cosine annealing for ADAM Table  1 : The performance of the proposed MERANet model in terms of the facial micro-expression recognition accuracy as compared to the existing state-of-the-art models. Note that the results of existing methods are taken from the respective papers. The HCM and DLM denote the hand crafted method and deep learning method, respectively. Method CASME I CASME II CAS(ME) 2 Year Method Type STCLQP  [9]  57.31 % 58.39 % -2016 HCM FMBH  [22]  61.  33       [23] , which helps significantly in the convergence. Next we discuss the results obtained by experimenting on the proposed method.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Results And Discussion",
      "text": "In this section, first we present the experimental results comparison, then the impact of kernel size on spatio-temporal attention module and the visualization of impact of attention through saliency maps.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Experimental Results",
      "text": "Experiments are conducted by training both the 3D residual network and the proposed 3D residual attention network models. The results are compared in terms of the facial micro-expression recognition accuracy. The performance of the proposed residual attention network is compared with the state-of-the-art hand-crafted methods (HCM) and deep learning methods (DLM) as shown in Table  1 . The results are computed over three benchmark facial microexpression datasets, including CASME I, CASME II, and CAS(ME) 2 in Table  1 . Note that the results of competing methods are taken from the corresponding papers. Table  1  depicts that the proposed MERANet model outperforms the existing hand crafted and deep learning models with a significant margin. As expected, the channel attention and the spatio-temporal attention blocks help the residual network to learn better micro-level spatio-temporal features leading to the enhanced decision making for facial micro-expression recognition.\n\nThe total number of parameters for the original 3D ResNet model is 3, 31, 67, 811 which, for the proposed 3D MERANet model, is increased to 3, 35, 47, 552. The proposed model uses 1.72 GFLOPs whereas original model uses 1.61 GFLOPs. The number of parameters increased for the proposed model by 3, 79, 741, and FLOPs increased by 0.11G, which is quite small compared to the original 3D ResNet model. So, the overall overhead added by the proposed attention module is negligible in terms of both parameters and computation. Table  2  represents the results in terms of the Accuracy, Precision, Recall and F1 measures for four benchmark datasets, including ğ¶ğ´ğ‘†ğ‘€ğ¸ğ¼ , ğ¶ğ´ğ‘†ğ‘€ğ¸ğ¼ğ¼ , ğ¶ğ´ğ‘† (ğ‘€ğ¸) 2 , and ğ‘†ğ´ğ‘€ğ‘€. Promising results are obtained using the proposed MERANet model for microexpression recognition. The quantitative results provided in Table  1  and Table  2  show the efficacy of the proposed attention modules. Table  3  shows the confusion matrix for the proposed method on the SAMM dataset.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Impact Of Shared Network In Channel Attention",
      "text": "We now compare the efficacy of shared CNN and shared MLP in channel attention. For this study, we use the ğ¶ğ´ğ‘† (ğ‘€ğ¸) 2 dataset. Inorder to fully capture the interrelationship, the channel descriptor of average pooling F ğ‘â„ ğ‘ğ‘£ğ‘” and max pooling F ğ‘â„ ğ‘šğ‘ğ‘¥ are fed to a shared  Both the design choices are trained under the same settings. We observe that the residual attention model with shared CNN in the channel attention module work well compared to the model with the shared MLP in the shared network. Shared CNN works on the sliding window concept (which focuses on local regions of interest), whereas shared MLP works on the global feature connections. Due to the aforementioned property, shared CNN goes hand in hand with the attention module by improving the learning capability of the model. The loss curves for both the design choices are plotted in Fig.  5 . From the validation loss curve in Fig.  5 , we observe that the CN-SCNN outperforms the CN-SMLP at every epoch. Model trained with the CN-SCNN module converges faster compared to the model trained with the CN-SMLP. In conclusion, we infer that the shared CNN in the channel attention module is a better design choice compared to the shared MLP.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Impact Of Kernel Size On Spatio-Temporal Attention",
      "text": "The performance of spatial-temporal attention block is mainly dependant on the applied convolution operation. In general, convolution layer performance depends on the size of the filter used. The kernel size plays a prominent role in extracting the subtle features. So, we have experimented with different kernel sizes, i.e., 3, 5, and 7 to find the optimal kernel size that boosts the model learning capability. The validation curve is shown in Fig.  6  having the plots corresponding to each kernel size. It is observed that the smaller kernel size (i.e., small receptive field) doesn't help the model to learn better. In contrast, higher kernel size (ğ‘˜ = 7) increases the computation head, yet it fails to generalize better. From Fig.  6 , we also infer that a reasonable receptive field is needed for deciding important regions. In the comparison of different convolution kernel sizes, we find the kernel size of 5 helps the model to learn better micro-expression representation.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Impact Of Attention Through Saliency Map",
      "text": "In this subsection, we empirically show the effectiveness of our proposed model. For this study, we use all the three datasets. In order to further understand the behavior of the proposed model, we have used the feature visualization technique proposed in  [31] . These feature visualizations help us to find which spatial locations trigger the classification outcome, thus providing the insights about the model's understanding of the emotions. The feature visualizations are given in Fig.  7 . According to  [4] , an emotion depicting disgust on a subject's face has facial muscle movements centered around the nose of the subject. Similarly, a happy emotion has facial muscle movements centered around the lip region. From Fig.  7 , we infer that the MERANet focuses on the face regions completely ignoring the background. Furthermore, it gives more weight to facial regions corresponding to an emotion. The saliency maps of 3D ResNet model seem to be very arbitrary by considering the noisy regions in the decision making process. Whereas, the saliency maps for the proposed MERANet model clearly matches the intuition with the general procedure of finding facial expressions and more convincing.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Conclusion",
      "text": "We have introduced a 3D residual attention based MERANet model for micro-expression recognition. The proposed 3D residual attention module utilizes the benefits of channel attention as well as spatio-temporal attention. These attention blocks help the proposed model to focus over the spatio-temporal features leading to the facial micro-expression recognition. We have conducted the extensive experiments with the proposed model over four benchmark datasets and compared with the state-of-the-art models. The proposed MERANet model outperforms the existing models with a negligible increase in the number of parameters. We also found that the shared CNN is better suitable with channel attention. However, the kernel size of 5 is better suitable with spatio-temporal attention.\n\nThe saliency map of the proposed MERANet model is very close to the human perception of the emotions in terms of the facial features contribution towards the identification of the emotions.",
      "page_start": 9,
      "page_end": 9
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: An overview of the proposed MERANet model. In",
      "page": 1
    },
    {
      "caption": "Figure 1: represents the overall idea of the",
      "page": 2
    },
    {
      "caption": "Figure 2: The proposed Spatial-Temporal Attention block.",
      "page": 3
    },
    {
      "caption": "Figure 2: Element-wise multiplication is employed",
      "page": 4
    },
    {
      "caption": "Figure 3: Channel-wise multiplication is employed between the input fea-",
      "page": 4
    },
    {
      "caption": "Figure 3: The proposed Channel Attention block. We shrink the spatial dimensions of the input feature map to 1 x 1 x 1 (height,",
      "page": 5
    },
    {
      "caption": "Figure 4: The proposed 3D RA module by integrating the 3D convolutions, channel attention and spatio-temporal attention",
      "page": 5
    },
    {
      "caption": "Figure 4: Suppose I âˆˆRğ¶Ã—ğ‘‡Ã—ğ‘ŠÃ—ğ»is the input feature volume to the",
      "page": 5
    },
    {
      "caption": "Figure 5: Validation loss curves for the shared network",
      "page": 7
    },
    {
      "caption": "Figure 6: The impact of kernel size on the spatio-temporal",
      "page": 8
    },
    {
      "caption": "Figure 5: From the validation loss curve in Fig. 5, we observe that",
      "page": 8
    },
    {
      "caption": "Figure 7: Comparison between the proposed MERANet (At-",
      "page": 8
    },
    {
      "caption": "Figure 6: having the plots",
      "page": 8
    },
    {
      "caption": "Figure 7: According to [4], an emotion depicting disgust",
      "page": 9
    },
    {
      "caption": "Figure 7: , we infer",
      "page": 9
    }
  ],
  "tables": [
    {
      "caption": "Table 1: Theresultsarecomputedoverthreebenchmarkfacialmicro-",
      "data": [
        {
          "Method": "STCLQP [9]",
          "CASME I": "57.31 %",
          "CASME II": "58.39 %",
          "2\nCAS(ME)(cid:98)": "-",
          "Year": "2016",
          "Method Type": "HCM"
        },
        {
          "Method": "FMBH [22]",
          "CASME I": "61.33 %",
          "CASME II": "69.11 %",
          "2\nCAS(ME)(cid:98)": "73.67 %",
          "Year": "2018",
          "Method Type": "HCM"
        },
        {
          "Method": "CNN + Optical Flow [18]",
          "CASME I": "56.60 %",
          "CASME II": "56.94 %",
          "2\nCAS(ME)(cid:98)": "-",
          "Year": "2018",
          "Method Type": "DLM"
        },
        {
          "Method": "3D Flow CNN [17]",
          "CASME I": "54.44 %",
          "CASME II": "59.11 %",
          "2\nCAS(ME)(cid:98)": "-",
          "Year": "2018",
          "Method Type": "DLM"
        },
        {
          "Method": "LEARNet [35]",
          "CASME I": "80.62 %",
          "CASME II": "76.57 %",
          "2\nCAS(ME)(cid:98)": "76.57 %",
          "Year": "2019",
          "Method Type": "DLM"
        },
        {
          "Method": "MicroExpSTCNN [30]",
          "CASME I": "-",
          "CASME II": "-",
          "2\nCAS(ME)(cid:98)": "87.80 %",
          "Year": "2019",
          "Method Type": "DLM"
        },
        {
          "Method": "MicroAttention [36]",
          "CASME I": "-",
          "CASME II": "65.90 %",
          "2\nCAS(ME)(cid:98)": "-",
          "Year": "2020",
          "Method Type": "DLM"
        },
        {
          "Method": "3D Residual Network (Ours)",
          "CASME I": "81.0%",
          "CASME II": "83.3 %",
          "2\nCAS(ME)(cid:98)": "83.3 %",
          "Year": "â€”",
          "Method Type": "DLM"
        },
        {
          "Method": "MERANet (Proposed Model)",
          "CASME I": "90.5 %",
          "CASME II": "93.5 %",
          "2\nCAS(ME)(cid:98)": "91.7 %",
          "Year": "â€”",
          "Method Type": "DLM"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table 1: Theresultsarecomputedoverthreebenchmarkfacialmicro-",
      "data": [
        {
          "Dataset": "CASME I",
          "Accuracy": "90.5%",
          "Precision": "0.917",
          "Recall": "0.907",
          "F1": "0.907"
        },
        {
          "Dataset": "CASME II",
          "Accuracy": "93.5%",
          "Precision": "0.94",
          "Recall": "0.94",
          "F1": "0.933"
        },
        {
          "Dataset": "2\nCAS(ME)(cid:98)",
          "Accuracy": "91.7%",
          "Precision": "0.93",
          "Recall": "0.917",
          "F1": "0.917"
        },
        {
          "Dataset": "SAMM",
          "Accuracy": "85.7%",
          "Precision": "0.823",
          "Recall": "0.799",
          "F1": "0.81"
        }
      ],
      "page": 7
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Look and think twice: Capturing top-down visual attention with feedback convolutional neural networks",
      "authors": [
        "Chunshui Cao",
        "Xianming Liu",
        "Yi Yang",
        "Yinan Yu",
        "Jiang Wang",
        "Zilei Wang",
        "Yongzhen Huang",
        "Liang Wang",
        "Chang Huang",
        "Wei Xu"
      ],
      "year": "2015",
      "venue": "Proceedings"
    },
    {
      "citation_id": "2",
      "title": "SAMM: A Spontaneous Micro-Facial Movement Dataset",
      "authors": [
        "Adrian Davison",
        "Cliff Lansley",
        "Nicholas Costen",
        "Kevin Tan",
        "Moi Hoon"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Affective Computing",
      "doi": "10.1109/TAFFC.2016.2573832"
    },
    {
      "citation_id": "3",
      "title": "Lie catching and microexpressions",
      "authors": [
        "Paul Ekman"
      ],
      "year": "2009",
      "venue": "The philosophy of deception"
    },
    {
      "citation_id": "4",
      "title": "What the face reveals: Basic and applied studies of spontaneous expression using the Facial Action Coding System (FACS)",
      "authors": [
        "Rosenberg Ekman"
      ],
      "year": "1997",
      "venue": "What the face reveals: Basic and applied studies of spontaneous expression using the Facial Action Coding System (FACS)"
    },
    {
      "citation_id": "5",
      "title": "Off-apexnet on micro-expression recognition system",
      "authors": [
        "Sze-Teng Ys Gan",
        "Wei-Chuen Liong",
        "Yen-Chang Yau",
        "Lit-Ken Huang",
        "Tan"
      ],
      "year": "2019",
      "venue": "Signal Processing: Image Communication"
    },
    {
      "citation_id": "6",
      "title": "Understanding the difficulty of training deep feedforward neural networks",
      "authors": [
        "Xavier Glorot",
        "Yoshua Bengio"
      ],
      "year": "2010",
      "venue": "Proceedings of the thirteenth international conference on artificial intelligence and statistics"
    },
    {
      "citation_id": "7",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "Kaiming He",
        "Xiangyu Zhang",
        "Shaoqing Ren",
        "Jian Sun"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "8",
      "title": "Squeeze-and-excitation networks",
      "authors": [
        "Jie Hu",
        "Li Shen",
        "Gang Sun"
      ],
      "year": "2018",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "9",
      "title": "Spontaneous facial micro-expression analysis using spatiotemporal completed local quantized patterns",
      "authors": [
        "Xiaohua Huang",
        "Guoying Zhao",
        "Xiaopeng Hong",
        "Wenming Zheng",
        "Matti PietikÃ¤inen"
      ],
      "year": "2016",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "10",
      "title": "Computational modelling of visual attention",
      "authors": [
        "Laurent Itti",
        "Christof Koch"
      ],
      "year": "2001",
      "venue": "Nature reviews neuroscience"
    },
    {
      "citation_id": "11",
      "title": "Automatic Hidden Sadness Detection Using Micro-Expressions",
      "authors": [
        "Grobova Jelena",
        "Colovic Milica",
        "Marjanovic Marina",
        "Njegus Angelina",
        "Demire Hasan",
        "Anbarjafari Gholamreza"
      ],
      "year": "2017",
      "venue": "12th IEEE International Conference on Automatic Face and Gesture Recognition"
    },
    {
      "citation_id": "12",
      "title": "Enriched long-term recurrent convolutional network for facial micro-expression recognition",
      "authors": [
        "Huai-Qian",
        "John Khor",
        "See",
        "Chung Raphael",
        "Weiyao Phan",
        "Lin"
      ],
      "year": "2018",
      "venue": "2018 13th IEEE International Conference on Automatic Face & Gesture Recognition"
    },
    {
      "citation_id": "13",
      "title": "Multiobjective based spatio-temporal feature representation learning robust to expression intensity variations for facial expression recognition",
      "authors": [
        "Dae Hoe Kim",
        "Wissam Baddar",
        "Jinhyeok Jang",
        "Yong Ro"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "14",
      "title": "Micro-Expression Recognition with Expression-State Constrained Spatio-Temporal Feature Representations",
      "authors": [
        "Dae Hoe Kim",
        "Wissam Baddar",
        "Yong Ro"
      ],
      "year": "2016",
      "venue": "Proceedings of the 2016 ACM on Multimedia Conference"
    },
    {
      "citation_id": "15",
      "title": "Learning to combine foveal glimpses with a third-order Boltzmann machine",
      "authors": [
        "Hugo Larochelle",
        "Geoffrey Hinton"
      ],
      "year": "2010",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "16",
      "title": "Deep learning",
      "authors": [
        "Yann Lecun",
        "Yoshua Bengio",
        "Geoffrey Hinton"
      ],
      "year": "2015",
      "venue": "Nature"
    },
    {
      "citation_id": "17",
      "title": "Micro-expression recognition based on 3D flow convolutional neural network",
      "authors": [
        "Jing Li",
        "Yandan Wang",
        "John See",
        "Wenbin Liu"
      ],
      "year": "2019",
      "venue": "Pattern Analysis and Applications"
    },
    {
      "citation_id": "18",
      "title": "Micro-expression analysis by fusing deep convolutional neural network and optical flow",
      "authors": [
        "Qiuyu Li",
        "Jun Yu",
        "Toru Kurihara",
        "Shu Zhan"
      ],
      "year": "2018",
      "venue": "2018 5th International Conference on Control, Decision and Information Technologies (CoDIT)"
    },
    {
      "citation_id": "19",
      "title": "Shallow triple stream three-dimensional cnn (ststnet) for micro-expression recognition",
      "authors": [
        "Sze-Teng Liong",
        "John Gan",
        "See",
        "Huai-Qian",
        "Yen-Chang Khor",
        "Huang"
      ],
      "year": "2019",
      "venue": "Gesture Recognition"
    },
    {
      "citation_id": "20",
      "title": "A main directional mean optical flow feature for spontaneous micro-expression recognition",
      "authors": [
        "Yong-Jin Liu",
        "Jin-Kai Zhang",
        "Wen-Jing Yan",
        "Su-Jing Wang",
        "Guoying Zhao",
        "Xiaolan Fu"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "21",
      "title": "Sgdr: Stochastic gradient descent with warm restarts",
      "authors": [
        "Ilya Loshchilov",
        "Frank Hutter"
      ],
      "year": "2016",
      "venue": "Sgdr: Stochastic gradient descent with warm restarts",
      "arxiv": "arXiv:1608.03983"
    },
    {
      "citation_id": "22",
      "title": "Motion descriptors for microexpression recognition",
      "authors": [
        "Hua Lu",
        "Kidiyo Kpalma",
        "Joseph Ronsin"
      ],
      "year": "2018",
      "venue": "Signal Processing: Image Communication"
    },
    {
      "citation_id": "23",
      "title": "On the adequacy of untuned warmup for adaptive optimization",
      "authors": [
        "Jerry Ma",
        "Denis Yarats"
      ],
      "year": "2019",
      "venue": "On the adequacy of untuned warmup for adaptive optimization",
      "arxiv": "arXiv:1910.04209"
    },
    {
      "citation_id": "24",
      "title": "Recurrent models of visual attention",
      "authors": [
        "Volodymyr Mnih",
        "Nicolas Heess",
        "Alex Graves"
      ],
      "year": "2014",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "25",
      "title": "Selective deep features for micro-expression recognition",
      "authors": [
        "Devangini Patel",
        "Xiaopeng Hong",
        "Guoying Zhao"
      ],
      "year": "2016",
      "venue": "2016 23rd international conference on pattern recognition (ICPR)"
    },
    {
      "citation_id": "26",
      "title": "Attention Based Residual Network for Micro-Gesture Recognition",
      "authors": [
        "Min Peng",
        "Chongyang Wang",
        "Tong Chen"
      ],
      "year": "2018",
      "venue": "2018 13th IEEE International Conference on Automatic Face & Gesture Recognition"
    },
    {
      "citation_id": "27",
      "title": "Recognising spontaneous facial micro-expressions",
      "authors": [
        "Tomas Pfister",
        "Xiaobai Li",
        "Guoying Zhao",
        "Matti PietikÃ¤inen"
      ],
      "year": "2011",
      "venue": "Recognising spontaneous facial micro-expressions"
    },
    {
      "citation_id": "28",
      "title": "Facial microexpressions recognition using high speed camera and 3D-gradient descriptor",
      "authors": [
        "Senya Polikovsky",
        "Yoshinari Kameda",
        "Yuichi Ohta"
      ],
      "year": "2009",
      "venue": "3rd International Conference on Imaging for Crime Detection and Prevention"
    },
    {
      "citation_id": "29",
      "title": "CAS (ME) 2: A Database of Spontaneous Macro-expressions and Micro-expressions",
      "authors": [
        "Fangbing Qu",
        "Su-Jing Wang",
        "Wen-Jing Yan",
        "Xiaolan Fu"
      ],
      "year": "2016",
      "venue": "International Conference on Human-Computer Interaction"
    },
    {
      "citation_id": "30",
      "title": "Spontaneous facial micro-expression recognition using 3D spatiotemporal convolutional neural networks",
      "authors": [
        "Sai Prasanna",
        "Teja Reddy",
        "Surya Teja Karri",
        "Ram Dubey",
        "Snehasis Mukherjee"
      ],
      "year": "2019",
      "venue": "2019 International Joint Conference on Neural Networks (IJCNN)"
    },
    {
      "citation_id": "31",
      "title": "Grad-cam: Visual explanations from deep networks via gradient-based localization",
      "authors": [
        "Michael Ramprasaath R Selvaraju",
        "Abhishek Cogswell",
        "Ramakrishna Das",
        "Devi Vedantam",
        "Dhruv Parikh",
        "Batra"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE interna"
    },
    {
      "citation_id": "32",
      "title": "Macro-and micro-expression spotting in long videos using spatio-temporal strain",
      "authors": [
        "Matthew Shreve",
        "Sridhar Godavarthy",
        "Dmitry Goldgof",
        "Sudeep Sarkar"
      ],
      "year": "2011",
      "venue": "IEEE International Conference on Face and Gesture"
    },
    {
      "citation_id": "33",
      "title": "Recurrent spatial transformer networks",
      "authors": [
        "Kaae SÃ¸ren",
        "Casper SÃ¸nderby",
        "Lars SÃ¸nderby",
        "Ole MaalÃ¸e",
        "Winther"
      ],
      "year": "2015",
      "venue": "Recurrent spatial transformer networks",
      "arxiv": "arXiv:1509.05329"
    },
    {
      "citation_id": "34",
      "title": "Image based facial micro-expression recognition using deep learning on small datasets",
      "authors": [
        "A Madhumita",
        "Min Takalkar",
        "Xu"
      ],
      "year": "2017",
      "venue": "2017 international conference on digital image computing: techniques and applications (DICTA)"
    },
    {
      "citation_id": "35",
      "title": "LEARNet: Dynamic imaging network for micro expression recognition",
      "authors": [
        "Monu Verma",
        "Santosh Vipparthi",
        "Girdhari Singh",
        "Subrahmanyam Murala"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Image Processing"
    },
    {
      "citation_id": "36",
      "title": "Micro-attention for micro-expression recognition",
      "authors": [
        "Chongyang Wang",
        "Min Peng",
        "Tao Bi",
        "Tong Chen"
      ],
      "year": "2020",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "37",
      "title": "Residual attention network for image classification",
      "authors": [
        "Fei Wang",
        "Mengqing Jiang",
        "Chen Qian",
        "Shuo Yang",
        "Cheng Li",
        "Honggang Zhang",
        "Xiaogang Wang",
        "Xiaoou Tang"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "38",
      "title": "Cbam: Convolutional block attention module",
      "authors": [
        "Sanghyun Woo",
        "Jongchan Park"
      ],
      "year": "2018",
      "venue": "Proceedings of the European conference on computer vision (ECCV)"
    },
    {
      "citation_id": "39",
      "title": "Aggregated residual transformations for deep neural networks",
      "authors": [
        "Saining Xie",
        "Ross Girshick",
        "Piotr DollÃ¡r",
        "Zhuowen Tu",
        "Kaiming He"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "40",
      "title": "CASME II: An improved spontaneous microexpression database and the baseline evaluation",
      "authors": [
        "Wen-Jing Yan",
        "Xiaobai Li",
        "Su-Jing Wang",
        "Guoying Zhao",
        "Yong-Jin Liu",
        "Yu-Hsin Chen",
        "Xiaolan Fu"
      ],
      "year": "2014",
      "venue": "PloS one"
    },
    {
      "citation_id": "41",
      "title": "CASME database: a dataset of spontaneous micro-expressions collected from neutralized faces",
      "authors": [
        "Wen-Jing Yan",
        "Qi Wu",
        "Yong-Jin Liu",
        "Su-Jing Wang",
        "Xiaolan Fu"
      ],
      "year": "2013",
      "venue": "2013 10th IEEE international conference and workshops on automatic face and gesture recognition (FG)"
    },
    {
      "citation_id": "42",
      "title": "Paying more attention to attention: Improving the performance of convolutional neural networks via attention transfer",
      "authors": [
        "Sergey Zagoruyko",
        "Nikos Komodakis"
      ],
      "year": "2016",
      "venue": "Paying more attention to attention: Improving the performance of convolutional neural networks via attention transfer",
      "arxiv": "arXiv:1612.03928"
    },
    {
      "citation_id": "43",
      "title": "Wide residual networks",
      "authors": [
        "Sergey Zagoruyko",
        "Nikos Komodakis"
      ],
      "year": "2016",
      "venue": "Wide residual networks",
      "arxiv": "arXiv:1605.07146"
    },
    {
      "citation_id": "44",
      "title": "Dynamic texture recognition using local binary patterns with an application to facial expressions",
      "authors": [
        "Guoying Zhao",
        "Matti Pietikainen"
      ],
      "year": "2007",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    }
  ]
}