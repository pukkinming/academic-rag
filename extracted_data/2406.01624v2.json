{
  "paper_id": "2406.01624v2",
  "title": "Unveiling Hidden Factors: Explainable Ai For Feature Boosting In Speech Emotion Recognition",
  "published": "2024-06-01T00:39:55Z",
  "authors": [
    "Alaa Nfissi",
    "Wassim Bouachir",
    "Nizar Bouguila",
    "Brian Mishara"
  ],
  "keywords": [
    "Speech emotion recognition",
    "Feature boosting",
    "Shapley values",
    "Explainable AI",
    "Machine learning"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Speech emotion recognition (SER) has gained significant attention due to its several application fields, such as mental health, education, and humancomputer interaction. However, the accuracy of SER systems is hindered by high-dimensional feature sets that may contain irrelevant and redundant information. To overcome this challenge, this study proposes an iterative feature boosting approach for SER that emphasizes feature relevance and explainability to enhance machine learning model performance. Our approach involves meticulous feature selection and analysis to build efficient SER systems. In addressing our main problem through model explainability, we employ a feature evaluation loop with Shapley values to iteratively refine feature sets. This process strikes a balance between model performance and transparency, which enables a comprehensive understanding of the model's predictions. The proposed approach offers several advantages, including the identification and removal of irrelevant and redundant features, leading to a more effective model. Additionally, it promotes explainability, facilitating comprehension of the model's predictions 1",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "The incorporation of emotional aspects into the development of artificial intelligence (AI) has been an area of focus in research for many years. Efforts to incorporate emotion in AI have led to a better understanding of the physiological processes underlying emotions through research in neurophysiological emotional processing  [1] . Emotions are integral to evolutionary and cultural adaptations, influencing human cognition, behavior, and social interactions  [2] . They can manifest as either positive or negative in different scenarios, influenced by the specific context in which they arise. This variation in emotional valence is shaped by the distinct situational elements that trigger these emotional responses  [3] .\n\nThe rise of robotic agents has made it increasingly important to introduce artificial emotionality in social robotics  [4] , as empirical studies have demonstrated improved collaboration and efficiency in task-oriented robotics when artificial agents express emotions to humans  [5] [6] . This integration of emotion in AI development holds practical applications, including the ability to monitor user states and mitigate risks through emotion recognition AI. Emotional AI has the potential to enhance psychological interventions in real-time, offering critical support in maintaining homeostatic balance and providing tailored care and rehabilitation for individuals with developmental disorders  [7] .\n\nPsychological research divides emotion modeling into two primary frameworks: discrete categorization, which aligns emotions with a set of fundamental states, and multidimensional approaches, which delve into the qualitative aspects of emotional experiences. Discrete models view emotions as specific archetypal states, whereas multidimensional models analyze emotions in terms of their subjective valence and functional outcomes  [8] . The simplicity of emotion representations can make it challenging to distinguish states with similar characteristics  [9] . Observable behavioral responses, such as body posture, facial expression, and prosody variation, provide input for models aiming to understand and predict emotional states  [2] . Speech emotion recognition (SER) involves the application of advanced machine learning techniques to analyze and classify emotions from the various frequencies and features present in speech signals  [10] . It has pivotal applications across various domains, including enhancing human-computer interactions, advancing affective computing technologies, and contributing significantly to the detection and diagnosis of mental health conditions, as it aids in the accurate interpretation and analysis of emotional states in speech  [11] .\n\nDespite the exploration of numerous machine learning techniques such as support vector machines, hidden Markov models, and deep neural networks for SER, the identification of the most effective feature representations continues to be a significant challenge, given the extensive range of available features and their diverse types  [10] . A significant obstacle in SER is the training of models on expansive datasets with a wide range of feature representations, where the precise relevance of these features to SER tasks is not always clearly understood. This challenge is further compounded by the limitations of available datasets in terms of size and diversity, hindering the effective training and optimization of machine learning models  [12] . Consequently, these predefined feature sets often result in high-dimensional data, making it challenging for models to effectively learn emotion-related patterns.\n\nIn the realm of SER, transparency and explainability hold paramount importance, particularly in safety-critical applications. Transparent and understandable SER systems facilitate error identification, correction, and the prevention of potentially dangerous situations  [13] . Furthermore, the presence of explainable emotion recognition systems is crucial for producing results that are trustworthy, easily interpretable, and capable of validation. As a result, research in Explainable AI (XAI) strives to enhance transparency and accountability in AI systems, addressing concerns surrounding biases in decision-making processes  [14]    [15] . Within the context of SER, the exploration of XAI techniques contributes to the development of more transparent and interpretable models in order to foster trust and enhance the overall utility of SER systems.\n\nTo address the aforementioned challenges, we propose a comprehensive framework, based on supervised machine learning, that emphasizes feature extraction and selection. Additionally, we integrate an explainability module employing SHapley Additive exPlanations (SHAP)  [16] , to enhance the performance and interpretability of SER systems. It consists of three main modules: 1) a feature boosting module for feature extraction and selection, 2) a supervised classification module for emotion recognition, and 3) an explainability module that explains the model's predictions and evaluates feature contributions using SHAP. The explainability module serves also as a feedback mechanism, continuously refining and boosting the feature set in the first module at each iteration. As far as we know, this is the first study to include model explainability in an SER framework.\n\nOur main contributions can be summarized as follows:\n\n• We introduce a novel machine learning approach for SER that prioritizes feature selection through iterative feature boosting, enhancing emotions sparsity via a variation ratio, thus, the model's performance by identifying the most relevant features for emotion recognition.\n\n• Our approach incorporates an explainability component that utilizes the SHAP technique to provide transparency and insights into the feature boosting process using a feedback loop. This allows for a better understanding of the contribution of each feature to the final decision. • We conduct an experimental evaluation of our proposed method by comparing it to both human-level performance and state-of-the-art algorithms. This evaluation demonstrates the effectiveness of our approach.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Background",
      "text": "Previous works on SER have predominantly employed supervised learning methods, which can be categorized into traditional machine learning and deep learning approaches. These methods rely on various handcrafted features and feature selection techniques to classify emotions in speech signals  [17] [18] [19] [20] [21] . However, SER is a complex task that comes with several challenges. One of the crucial challenges is effectively representing speech, considering the distinctive patterns that differentiate emotions and the temporal dynamics of emotion expression. Consequently, handling high-dimensional speech data and selecting relevant features become significant obstacles in SER. Extracting a multitude of features without a thoughtful selection process can have detrimental consequences for model performance. This indiscriminate approach may result in overfitting, where the model becomes too specialized in fitting the training data, leading to poor generalization to new data. Overfitting occurs because the excessive features introduce noise and complexity into the model. Conversely, this approach might also lead to subpar performance since it lacks the required sparsity, preventing the model from creating a more generalized and representative understanding of emotions. In essence, a balanced and well-considered feature selection process is crucial to avoid these pitfalls and ensure that the model effectively captures informative and discriminative characteristics of emotions  [17, 22, 23] .",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Handcrafted Feature-Based Approaches",
      "text": "In  [17] , authors proposed a method for emotion classification using Mel Frequency Cepstral Coefficients (MFCCs) as features and Support Vector Machines (SVMs) as the classification model. The authors explored the impact of different feature combinations, such as adding pitch and energy features, on classification performance. This study demonstrated the effectiveness of using MFCCs and SVMs for SER, highlighting the importance of feature selection in enhancing classification performance. However, one aspect that could be further improved in the study is the limited exploration of other features. While the authors investigated the impact of adding pitch and energy features to the MFCCs, there may be other relevant features that could contribute to improved classification performance. The study could have benefited from a more comprehensive exploration of feature combinations to ensure that the selected features truly capture the diverse aspects of emotional expression in speech.\n\nTwo-way feature extraction approach for SER has been introduced in  [19] . In the first way, authors directly extracted features from audio data using mel-scale related features, which were then dimensionally-reduced using Principal Component Analysis (PCA). The reduced features were fed into a Deep Neural Network (DNN) for classification. The authors observed that PCA helped reduce overfitting and improved DNN training. In the second way, they used the 2D representation of spectrograms for classification, employing the VGG16 CNN model. This approach eliminated the need for feature engineering or selection as it directly utilized the spectrograms as inputs to the model. However, the study does not provide detailed analysis or comparison of the impact of different dimensionalities on classification performance as it briefly mentions the use of feature selection and dimensionality reduction techniques, while the rationale behind the choice of these techniques and especially their impact on classification performance are not adequately discussed.\n\nIn  [22]  a joint learning framework was proposed for feature selection and emotion classification using the robust discriminative sparse regression (RDSR) approach. This method aimed to select the most discriminative feature subset from the highdimensional feature set by introducing a feature selection regularization constraint. The authors employed sparse regression to enhance model robustness to outliers and noise. The selected features were then fed into a classification model for emotion prediction. Their experiments demonstrated the superiority of the RDSR approach in terms of classification accuracy and feature selection compared to other state-of-theart methods. While the study focuses on improving classification accuracy and feature selection, it neglects the aspect of interpretability. It mentions also the use of a feature selection regularization constraint, but it lacks an in-depth explanation of its impact on the feature selection process.\n\nTo address the sequential nature of speech data, a continuous hidden Markov model (CHMM) was proposed for SER in  [23] . The model extracted a 33-dimensional feature parameter based on the temporal sequence of speech signals. PCA was then applied to reduce the dimensionality of the feature set. The results showed that the PCA-CHMM model outperformed a standard HMM model that used the entire feature set, demonstrating the effectiveness of dimensionality reduction in improving emotion recognition performance. The study centers on comparing the PCA-CHMM model with a standard HMM model, but it falls short in conducting a comprehensive comparison with contemporary SER models and algorithms. Additionally, the study mentions the use of PCA for dimensionality reduction without offering detailed explanations or justifications for this approach. Moreover, while the study aims to enhance emotion recognition performance, it neglects the aspect of interpretability.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Learnable Feature-Based Approaches",
      "text": "Deep learning models have also been applied to SER, leveraging their ability to learn complex features from data. Authors in  [21]  proposed a hybrid end-to-end (E2E) deep learning model combining 1D-CNN and Gated Recurrent Unit (GRU) for feature extraction and classification. The 1D-CNN component extracted spatial features from the input data, while the GRU component captured the time-distributed features and added a time aggregation layer. This model was designed to learn relevant features from raw waveform speech signal and classify emotions in a single E2E process, eliminating the need for handcrafted features. The use of 1D-CNN and GRU components provided a powerful tool for learning complex features in sequential data, which is important for SER as it often depends on the temporal dynamics of speech. While the proposed hybrid E2E model shows promise, the study could benefit from providing more insight into the interpretability of the model's learned features. Understanding the discriminative factors and patterns that contribute to emotion recognition would enhance the trust and applicability of the approach.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Other Approaches",
      "text": "Authors in  [24]  integrated traditional handcrafted features with advanced deep learning techniques. This method begins with the extraction of key emotional features from speech data, utilizing conventional handcrafted features known for their effectiveness in capturing the emotional nuances in speech. To augment this, the study incorporates deep CNNs (DCNN), leveraging their capacity for automatic feature extraction from complex datasets. A critical aspect of this methodology is the application of a correlation-based feature selection technique, designed to identify and retain the most discriminative features for SER, thereby enhancing classification accuracy. This approach is further evaluated using a variety of machine learning classifiers, including SVM, random forests, and neural networks. In  [25] , the performance of CNNs was investigated on five types of spectral features for classifying an increasing number of emotional categories. They conducted a systematic evaluation of CNN performance on an increasing number of emotions, ranging from binary to eight categories. The authors introduced additional classifications beyond binary or all classes and also proposed a new use of 1D convolution for multiple classes. The study provided insights into CNN performance on an increasing number of emotions and introduced a novel approach using 1D convolution for multi-class classification. In  [26] , two modifications were proposed to the extraction of MFCCs by using magnitude spectrum instead of energy spectrum and excluding discrete cosine transform while extracting Mel Frequency Magnitude Coefficient. The authors tested these modifications alongside conventional spectral features and evaluated their impact on SER.\n\nPrior studies have emphasized the significance of extracting effective acoustic characteristics to accurately capture different emotional aspects of speech in SER. However, these studies have predominantly relied on pre-defined features without thoroughly examining their relevance for SER or their potential to enhance performance. Certain deep learning-based approaches have attempted to address the issue of feature selection implicitly through 1D convolutions. Meanwhile, other supervised learning methods have sought to tackle the challenge of high dimensionality by applying PCA to compute principal components. In contrast, our proposed approach for supervised SER places a strong emphasis on feature importance and model explainability throughout the entire framework. We aim to explore the relevance of different acoustic features for SER and prioritize the extraction and selection of the most pertinent features. Additionally, we introduce an interpretable machine learning model that provides insights into how the model makes predictions. By explicitly addressing the feature selection challenge in SER and highlighting the importance of explainability in machine learning models, our approach offers a novel perspective in this domain. In the field of SER, the effectiveness of any predictive model is intrinsically tied to the delicate balance and quality of its feature set. Our research is driven by critical questions that confront core challenges in SER: How can we optimize the selection of features to enhance the model's sensitivity to emotional nuances in speech? What strategies can we employ to avoid the pitfalls of overfitting and underfitting, ensuring our model's robustness and adaptability to new, unseen data? Furthermore, we investigate the role of feature diversity in capturing the broad spectrum of human emotions conveyed through speech. This exploration is not just about achieving high accuracy in emotion classification but also about understanding the underlying patterns that govern emotional expression in human speech. To this end, we delve into the intricacies of voice signal characteristics, seeking to identify and utilize those features that are most indicative of emotional states. Another dimension of our research addresses the transparency of SER models. The interpretability of our model's predictions is paramount, providing insights into the decision-making process and helping to identify the most influential features. This approach aligns with the growing emphasis on explainable AI, where understanding the 'why' behind a model's predictions is as vital as the predictions themselves.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Overview",
      "text": "Our approach comprises three interrelated components that synergistically improve SER, as illustrated in Fig.  1 . The first component is a feature boosting module, which extracts an initial feature set with potential relevance for emotion recognition. This module identifies the most effective feature combinations that best separate emotion classes through a sparsity criterion and then reduces the dimensionality of the resulting feature set while retaining the majority of the relevant information. The second component formulates the SER task as a supervised classification problem, utilizing a classification module to construct a model that can categorize speech samples based on the extracted features. Finally, the third component incorporates an explainability module that analyzes the classification decisions. By leveraging Shapley values, this module gains insights into the feature boosting process and evaluates the significance of features in the decision-making process. One significant advantage of utilizing SHAP over other approaches such as LIME  [27]  for model explanations is the type of explanation they offer. While LIME is model-agnostic, which primarily offers local explanations tailored to individual predictions, SHAP is a model-specific method offering global explanations that illuminate the model's behavior across the entire dataset. This holistic perspective is invaluable for understanding how the model considers various features and interactions, identifying dataset-level feature importance, and revealing broader patterns and tendencies. When prioritizing transparency, interpretability, and the ability to assess a model's overall performance, SHAP's provision of global explanations makes it a preferable choice.\n\nFurthermore, our approach incorporates an iterative feedback mechanism that facilitates information exchange between the third component, the explainability module, and the first component, the feature boosting module. This iterative process allows for the continuous refinement and enhancement of the retained features. By analyzing the classification decisions and evaluating the importance of features through the explainability module, we can identify the most relevant features and discard the less relevant ones. This iterative feedback loop ensures that the feature set becomes increasingly optimized over time. Our integrative approach optimizes feature selection, enables supervised classification, and provides interpretability by analyzing classification decisions, thereby improving the overall performance of our SER approach.\n\nIn addressing these questions, our framework introduces a comprehensive methodological approach that encompasses three integral components: Feature Boosting Module, Classification Module, and Explainability Module. Each module is meticulously designed to tackle specific aspects of the SER challenge. The Feature Boosting Module focuses on extracting and refining a feature set that captures the essence of emotional states in speech. The Classification Module then takes these features and applies them in a supervised learning context to categorize emotional expressions accurately. Finally, the Explainability Module provides a window into the model's inner workings, offering insights into how and why certain features play a pivotal role in emotion recognition. This integrative approach not only aims to enhance the accuracy and efficiency of emotion classification but also ensures that the process is transparent and understandable.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Method",
      "text": "",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Feature Boosting Module",
      "text": "In the first step, we calculate a preliminary feature set representation, including pitch, energy, and rhythm-related characteristics, which we assume are meaningful for the SER task. We also calculate statistical characteristics such as the mean, median, standard deviation, minimum, and maximum based on previous works  [28]  [29], resulting in a set of n initial features.\n\nIn order to improve both the performance and interpretability of our technique, we employ an initial feature selection process. This process starts by selecting a set of m distinct feature combinations of p features from the initial feature set. Then, to determine the most informative and discriminative feature combinations that enhance the separation between emotion categories, we introduce the Variance Ratio Criterion (VRC) as a sparsity criterion. The VRC is designed for datasets with multiple emotion categories, denoted by E, and utilizes a dataset D containing N data samples represented as d i . This criterion evaluates the similarity of a speech sample to its own emotion category (cohesion) compared to other emotion categories (separation), as formulated in eq. 1:\n\nwhere, n e represents the number of data points in the e th emotion category, and c e is the centroid of the data points in that category. The global centroid of all data points is denoted as c. The VRC provides a measure of how well-separated and dense the emotion classes are, with higher values indicating better separation.\n\nTo determine the significance of each feature combination, we compute the difference between the VRC of the feature combination (V RC f ci ) and the VRC of the previous overall feature set (V RC all ). This difference, denoted as σ i in eq. 2, quantifies the improvement in separation achieved by the specific i th feature combination.\n\nTo select the relevant feature combinations, we compare each σ i value to a threshold α as calculated in eq. 3. Specifically, we rank the m combinations in decreasing order of their σ i values and retain the top q combinations that satisfy the condition σ i ≥ α, with\n\nwhere m p is the number of combinations with σ j ≥ 0, and ϵ is a parameter we introduce to control the retention of feature combinations based on their explained variance ratio. By using the VRC and the significance measure σ i , we can identify the feature combinations that contribute the most to the separation and the density of emotion categories, leading to improved emotion recognition performance. We then apply PCA to reduce the dimensionality of each selected feature combination and eliminate noise. This is achieved by transforming the feature combinations using an eigenvector matrix (A i ) and a corresponding eigenvalue vector (λ i ). The calculation of A i and λ i for each combination i follows the equations specified as 4 and 5. By applying this transformation, we effectively reduce the dimensionality of the feature space while preserving the most informative characteristics.\n\nwhere, a ijk is the k th element of the j th eigenvector of the i th feature combination and λ ij is the eigenvalue associated with the j th principal component of the i th feature combination. Each column of the matrix A i represents the j th principal component (P C ij ) of the i th feature combination, capturing specific data information and determining the dimension (r i ) of the reduced subset, as shown in eq. 6:\n\nwhere, X ik is the k th feature of the i th combination. Therefore, we can determine which features contribute the most to each principal component, which helps us identify the best combination of features representing information in our dataset.\n\nWe evaluate the percentage of variance explained by the j th principal component of the i th combination (P C ij ) using eq. 7:\n\nwhere, EV ij represents the percentage of variance explained by the j th principal component of the i th combination, and λ ij is the eigenvalue and amount of variance explained by P C ij . Finally, we construct a new feature set consisting of the principal components (P C ij ) of the selected feature combinations. This module encompasses several steps, including computing a preliminary feature set, selecting feature combinations, ranking them based on improvement in separation, and constructing a new feature set with selected principal components (refer to algorithm 1-3a). It effectively reduces dimensionality while preserving informative features for classification. The process is iteratively enhanced through the feedback loop of the explainability module, as explained in section 3.3.3.",
      "page_start": 8,
      "page_end": 10
    },
    {
      "section_name": "Classification Module",
      "text": "In the proposed SER framework, the classification module is integral, where extracted features are used to discern emotions in speech. This module encompasses a diverse set of M candidate classification models, as detailed in section 4.1.2, each chosen for its proficiency in handling complex emotional data from speech and selected for their adeptness in previous studies processing high-dimensional feature spaces inherent in SER tasks.\n\nThe methodology involves inputting the optimized feature set, refined by the feature boosting module, into these models. This step is crucial as it ensures the models operate with features most indicative of emotional states. The training phase for each model involves learning the correlations between these features and the emotional labels within the training dataset portion. This is followed by a validation phase on a separate unseen dataset segment, a crucial step to assess the models' generalizability and accuracy.\n\nPerformance assessment is conducted through various metrics as detailed in 4.1.4, providing a comprehensive evaluation of each model's classification efficacy. Subsequent to performance evaluation, the module engages in a detailed optimization process. This phase involves hyperparameter tuning and model refinements, utilizing grid search and cross-validation techniques to enhance the predictive performance of each model.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Algorithm 1 Speech Emotion Recognition (Ser) Algorithm",
      "text": "Input:\n\n• Speech dataset with labeled emotional states • Select m optimal combinations of p features.\n\n• Compute σ i and select top q combinations.\n\n• Apply PCA to reduce dimensionality and eliminate noise.\n\n• Construct a new feature set with the selected principal components. b) Train and evaluate M candidate classification models:\n\n• Split the dataset into training and validation sets.\n\n• For i = 1 to M : -Train the i th model on a new feature set.\n\n-Evaluate performance on the validation set. • Select the best-performing model. c) Analyze classification decisions using the explainability module:\n\n• Calculate Shapley values to determine the contribution of each principal component. • Identify the most important principal components and feature combinations. • Identify the most important features from the previous feature set. d) Update the previous feature set by removing less important features. e) Check for convergence by comparing with previous iterations. 4) Train optimal SER model on the entire dataset using retained features. 5) Output trained SER model and retained features. End.\n\nAfter optimizing the models, a comparative analysis is conducted to select the most effective model. This selection is based on various criteria, including not only the accuracy but also the model's adaptability to diverse speech patterns. The finalized model is then integrated with the explainability module, employing Shapley values to illuminate the decision-making processes within the model. This integration is vital, as it provides insights into feature contributions and informs further refinements.\n\nThe classification module, thus, is not merely a collection of machine learning algorithms; it represents a structured process. This module embodies a fusion of classification methodologies and domain-specific optimizations, ensuring that the SER system achieves a good performance. It is worth noting that this approach is not limited to SER but can be adapted to various classification tasks, allowing for the evaluation of alternative candidate models in different domains. It provides a valuable means to select the most appropriate model that maximizes performance with the boosted features, contributing to more accurate and reliable classification outcomes.",
      "page_start": 11,
      "page_end": 12
    },
    {
      "section_name": "Explainability Module",
      "text": "The explainability module incorporates XAI capabilities into the SER system. This helps not only to create a system that is transparent and understandable in terms of prediction and decision-making but also to provide the feature boosting module with the necessary insights for enhanced data representation. To achieve this, we use the Shapley explanation values to explain the model's predictions. Shapley values allow us to understand the contribution of each P C ij in the resulting feature set to a model's prediction. By using these values, we can identify which combination's principal components are most important for SER.\n\nThe interpretability of a model's predictions is crucial for both validation and practical application. This is where the integration of Shapley's values becomes indispensable. Shapley values offer a robust, mathematically grounded method to quantify the contribution of each feature within a complex, multivariate SER model. Each feature in a speech sample contributes to the overall emotional classification, but the extent and nature of this contribution can be elusive in high-dimensional spaces typical of SER models  [30] .\n\nShapley values address this by distributing the 'payout' (i.e., the prediction output) among the features, based on their marginal contribution to the prediction across all possible combinations of features. This method aligns with the cooperative game theory, where each feature is considered a 'player' in the game of classification  [31] . By employing this approach, we can dissect the model's decision-making process, revealing how each feature influences specific emotion classifications -whether a certain tone of voice is pivotal in identifying sadness, or if a particular speech rhythm is key to detecting excitement.\n\nIn SER, this granular insight is invaluable. It allows for a nuanced understanding of the feature interactions within complex emotional spectra, guiding the refinement of feature engineering and selection processes  [32] . Moreover, in scenarios where SER models need to be transparent and their decisions interpretable -such as in usercentric applications or clinical settings -Shapley values provide a scientifically rigorous explanation. They enable us to present a clear, quantifiable rationale behind each prediction, improving the credibility and utility of SER systems  [33] .\n\nThe contribution of each P C ij in the resulting feature set is denoted as ϕ P Cij and formulated in eq. 8:\n\nwhere, q is the number of combinations, S is a subset of principal component indices, J i is the number of principal components used from each combination i with J i ≥ 1, z is the input vector, and f (z) is the output of the classification model for input z. The first summation term (f ij (z s∪ij )) computes the expected output of the model when P C ij is included in the subset, while the second term (f (z s )) computes the expected output of the model when P C ij is excluded from the subset of principal components S. The difference between these two terms represents the contribution of P C ij to the output, which is the marginal contribution of P C ij to coalition S. This value is used to conduct a feature importance analysis, which provides insight into how the model works and what factors are most important.\n\nThe explainability module tackles one of the most pressing questions in SER: How does the model arrive at its conclusions? By implementing Shapley values, this module demystifies the model's decision-making process, elucidating the significance of each feature in the prediction. This not only enhances our understanding of the model's functionality but also informs ongoing refinement efforts, creating a feedback loop that continuously improves the feature set's effectiveness.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Feedback Mechanism",
      "text": "The feedback mechanism within our method allows us to refine the feature selection process and enhance the performance and interpretability of the SER system. It operates iteratively to identify the most relevant principal components that effectively capture essential information for emotion recognition. The contribution of features at iteration t to each relevant principal component is carefully evaluated to determine their significance in both the principal components and the overall classification decision process. As a result, features at iteration t that do not significantly contribute to the system's performance are eliminated in the following iteration t + 1, which is done progressively at each iteration until convergence is achieved. This iterative approach enables us to continuously improve the accuracy and interpretability of the SER system by selectively retaining the most informative features and discarding less important ones. The detailed steps of this iterative feedback mechanism can be found in section 3 of algorithm 1.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Experiments And Results",
      "text": "",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Experimental Design",
      "text": "",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Datasets",
      "text": "In the development of SER systems, the selection of speech datasets is crucial, encompassing three primary categories: actor-based, induced, and natural emotional datasets. The fidelity of SER models largely hinges on the authenticity of these datasets, with natural ones offering genuine emotional representations. However, challenges such as data accessibility and legal concerns often accompany the use of natural datasets, underscoring the need for careful dataset selection in SER model development  [10]    [34] .\n\nEmotional speech datasets vary across several properties such as language, database type, number of emotions, data type, number of speakers, number of samples, number of utterances, and dataset aim  [35] . Among all the datasets, actor-based ones account for the majority of research datasets. The most commonly used language is English, followed by Chinese and German. These datasets mostly comprise of neutral, sad, happy, and angry emotions.\n\nIn the current study, we are using the EMO-DB  [36]  dataset, which includes anxiety/fear, sadness, disgust, happiness, anger, neutral, and boredom emotions for our analysis. Additionally, we use the same emotion categories in our analysis of the TESS  [37]  dataset, with the exception of boredom being replaced by surprise. We also expand our experimental investigation with two more datasets, RAVDESS  [38]  and SAVEE  [39] . These datasets help us evaluate the performance of our approach accurately as they are widely used in the SER field.\n\nEMO-DB: The Berlin Emotional Speech Database is a collection of 535 German language recordings made by 10 professional actors, 5 male, and 5 female, simulating 7 emotional states: anger, boredom, disgust, anxiety/fear, happiness, sadness, and neutral. The recordings comprise 10 short texts, read in a neutral tone and then in different emotional states. The texts were chosen to be neutral in terms of emotional content to avoid potential emotional carryover effects. Each text is approximately 5 seconds long. The recordings were made in a soundproof studio using a high-quality microphone and recording equipment. The database has been widely used in emotion recognition research and is freely available for non-commercial use.\n\nTESS: The Toronto Emotional Speech Set is a dataset that comprises 2800 audio recordings. These recordings were made by two actresses, aged 26 and 64, respectively. The actresses were asked to express 200 target phrases in the context of the phrase \"Say the word .\" The dataset includes seven different emotional states, namely, anger, disgust, pleasant surprise, fear, sadness, happiness, and neutral. Each of the seven emotional categories is represented by 400 recordings.\n\nRAVDESS: The Ryerson Audio-Visual Database of Emotional Speech and Song dataset is notable for its wide range of emotions, including happiness, sadness, anger, fear, surprise, and disgust, expressed in speech. It consists of 24 professional actors (12 male, 12 female) who perform each emotion, resulting in a total of 7356 audiovisual recordings. This diversity makes RAVDESS an ideal resource for analyzing the nuances of emotional expression in speech.\n\nSAVEE: The Surrey Audio-Visual Expressed Emotion dataset is distinguished by its focus on male voices, with recordings from 4 male actors enacting different emotions such as anger, disgust, fear, happiness, sadness, surprise, and neutral. It contains 480 utterances, which are valuable for gender-specific emotion recognition studies. The dataset's concentration on a smaller set of actors provides a more controlled environment for analyzing the subtleties of male emotional expression in speech.",
      "page_start": 13,
      "page_end": 14
    },
    {
      "section_name": "Experimental Setup",
      "text": "To ensure accurate processing and analysis of audio signals in our SER system, we first establish a sampling rate of 16 KHz in a mono-channel format, aligning with the requirements of most SER algorithms. This step ensures consistency and compatibility with subsequent processing stages. Next, we extract an initial feature set comprising n = 90 features that are deemed relevant to SER. These features are then normalized and serve as the starting point for our subsequent feature boosting module.\n\nTo partition our dataset effectively, we employ stratified random sampling  [40] , maintaining class distribution across three distinct groups: training, validation, and testing. This division ensures representative subsets for training and evaluating our models, with 80% allocated for training, 10% for validation, and the remaining 10% reserved as unseen data for testing.\n\nMoving forward, our feature boosting module becomes instrumental in enhancing the feature representation for SER. It iteratively leverages insights from the explainability module and employs a VRC-PCA driven technique to construct a new dataset. This process involves identifying optimal feature combinations that best differentiate emotion categories and capture the most relevant information, reducing dimensionality while preserving crucial discriminative features. The explainability insights guide this selection process, ensuring that the chosen features align with interpretability and contribute significantly to the model's decision-making process. The iterative nature of this module allows us to continuously refine the feature set until convergence is achieved. As a result, we create an improved dataset that reflects the boosted features, offering an enhanced representation of the discriminative patterns underlying emotions. This boosted dataset serves as the foundation for subsequent modules of our method, facilitating improved performance and interpretability in the classification task.\n\nWe proceed by training M = 14 different machine learning models, employing 10-fold cross-validation to assess their performance without overfitting. This technique allows us to evaluate the models' effectiveness and identify the optimal one. Our selection of machine learning models includes Random Forest (RF), Extra Trees classifier (ET), Light Gradient Boosting Machine (LGBM), Linear Discriminant Analysis (LDA), Decision Tree (DT), Quadratic Discriminant Analysis (QDA), Gradient Boosting Classifier (GBC), Logistic Regression (LR), Support Vector Machine (SVM), Naive Bayes (NB), Ridge (Ridge), K-Nearest Neighbors (KNN), Dummy classifier (Dummy), and Adaptive Boosting (ADA). Once the best-performing models have been identified, we employ the grid search technique to fine-tune their hyperparameters. By exhaustively exploring the parameter space, we seek the optimal combination of hyperparameters for each model. This step enhances the robustness and overall performance of the models.\n\nFinally, we evaluate the performance of the final models using the dedicated testing set, ensuring unbiased assessment on unseen data. During the training phase, every segment is employed to predict a single emotion. To ensure the accuracy of our results, we repeat each test 10 times with different random seeds before reporting the average outcome. Additionally, our explainability module incorporates the SHAP approach to evaluate feature importance in the predictions made by the optimal model. This Class-wise feature importance -Extra Tree classifier (EMO-DB): The x-axis of Fig.  2  shows the mean of the absolute values of Shapley values, which indicates the average impact on the model's output magnitude. A higher value suggests a more important feature. The y-axis represents principal components obtained by applying PCA to the optimal combinations of selected features that meet the significance measure (σ i ) threshold. In Fig.  3 , the y-axis represents the initial feature importance to the model's decision.\n\nanalysis enables us to gain insights into the decision-making process of the model and identify the most influential features for emotion determination. The importance of these features is iteratively communicated back to the feature boosting module, allowing for the refinement of the feature set in subsequent iterations. This iterative process ensures that the most relevant features are retained, while less important ones are discarded, leading to an increasingly optimized feature representation for SER.",
      "page_start": 15,
      "page_end": 16
    },
    {
      "section_name": "Significance Of Model Explainability",
      "text": "In our study focusing on SER, we place a significant emphasis on integrating XAI techniques into our system. The core idea behind this integration is not only to enhance the performance of our models but also to make them transparent and interpretable.\n\nIn identifying emotions from speech, it's imperative to understand the 'why' behind the model's predictions. This understanding is key to ensuring the decisions made by our system are both reliable and trustworthy.\n\nTo achieve this level of clarity and insight, we use two main strategies: feature importance evaluation and the application of SHAP. Feature importance evaluation is a process where we identify which features in the model are most influential. This could be features like pitch, tone, or speed of speech. Understanding which of these The SHAP approach complements this by providing a deeper dive into how each of these important features contributes to the final outcome of the model. SHAP acts as a tool that helps us break down the model's decision process, showing us the contribution of each feature to the prediction. This detailed breakdown is crucial, as it offers a comprehensive understanding of the model's inner workings.\n\nA key aspect of our approach is the integration of a feature boosting module. This module is adept at selecting the most effective feature combinations for emotion recognition. It works hand-in-hand with the explainability module and the classification module, forming a trio that ensures the features we select are not just relevant but also contribute to making the model more understandable. This is crucial because it means our model isn't just a black box; it's a system whose decisions can be traced and understood.\n\nOur approach's effectiveness is demonstrated through visualizations in Figs.  2  and 3 , where we show how each feature contributes to predicting different emotions. For instance, in Fig.  2 , we label principal components in a way that allows us to easily track which features are most influential in emotion classification. The principal components are labeled as P C {combination index}{P C index} , where P C ij refers to the j th principal component of the i th feature combination. This labeling is instrumental in identifying key components and understanding their role in the model's predictions.\n\nAnalyzing Fig.  2 , we can identify the principal components that have the highest impact on the model's output. Additionally, Fig.  3  helps us determine the contribution of each initial feature to the final decision. Fig.  4  takes this a step further by providing a biplot (e.g. 2 nd feature combination). This visualization helps us see not only the principal components that are most impactful but also how the initial features contribute to these components. This level of detail is important as it guides us in iteratively eliminating less relevant features and refining our model. By continuously identifying and focusing on the most relevant features, we enhance the accuracy of our SER system. In summary, the integration of feature boosting and a thorough analysis of feature contributions are pivotal in our SER framework. It enables us to identify the most relevant features for emotion recognition, thereby enhancing the accuracy and reliability of our classification decisions. More importantly, it offers a window into our model's thought process, ensuring that each decision made by our system is transparent and understandable.",
      "page_start": 16,
      "page_end": 17
    },
    {
      "section_name": "Evaluation Metrics",
      "text": "For performance evaluation, we use accuracy, recall, precision, and F1-Score, which attempts to establish a compromise between precision and recall metrics  [30] . By using these metrics, we can measure the effectiveness of our approach and compare its performance with other state-of-the-art methods.",
      "page_start": 18,
      "page_end": 18
    },
    {
      "section_name": "Results",
      "text": "",
      "page_start": 18,
      "page_end": 18
    },
    {
      "section_name": "Feature Boosting",
      "text": "In our study, we apply a threshold on σ i , by fixing the value of ϵ, this allows us to select only the feature combinations that best separate emotion classes with optimal density resulting in 23 optimal combinations for TESS dataset and 25 for EMO-DB dataset. When applying PCA, on TESS dataset, one of these combinations captures 84.56% of the total explained variance in the first two principal components and 98.22% in the first four. On EMO-DB dataset, the explained variance by the first two principal components of the optimal feature set to represent the information in the data is 76.8% and 99.36% by the first five principal components, making it the best feature set to represent the information in the data (e.g. see Table  1 ). This reflects the amount of information explained by the principal components of the retained feature combinations.\n\nTo visualize the relationship between the optimal features and principal components, we use a biplot as shown in Fig.  5 , which displays the data points on a 2D scatter plot based on the values of the first two principal components of the optimal feature combination data. The biplot also shows the directions and lengths of arrows representing the optimal features in the transformed space. The direction of the arrow indicates the sign of the contribution, while the length indicates the magnitude of the The magnitude of the arrows indicates the significance of each feature within the dataset, while the angle between the arrows signifies the correlation between the features. Fig.  6  Cumulative explained variance of EMO-DB optimal feature combination contribution. This allows us to understand how the optimal features are related to the principal components and how they contribute to the overall variance in the data.\n\nFurthermore, we add the cumulative explained variance in Fig.  6  to the biplot, which shows the percentage of the total variance in the optimal feature combination data explained by each principal component. This helps us determine the optimal number of principal components to retain when performing PCA on the selected feature combinations of each dataset for a convenient information representation.",
      "page_start": 19,
      "page_end": 19
    },
    {
      "section_name": "Importance Of Feature Boosting And Model Explainability On Tess Dataset:",
      "text": "Table  2  presents the performance of the selected machine learning models on the initially computed features for the SER task using TESS dataset. The ET classifier  achieves the highest accuracy, recall, precision, and F1-score of 95.8%. The LGBM also performs well with 95% accuracy. RF and KNN have a relatively high accuracy of 94.6% and 94.4%, respectively. These results suggest that the initially computed features contain valuable information for the SER task. The confusion matrix in Fig.  7  shows that the ET classifier performs well overall, correctly predicting the diagonal elements of each class. However, there are some misclassifications, indicating that some classes share acoustic similarities. Table  3  compares the performance of the same models using boosted features with explainability module feedback loop. The ET classifier achieves the highest accuracy  and F1-score of 99.4%. The LGBM also performs well with an accuracy and F1-score of 99%. RF has a relatively high accuracy of 98.9%. Some models perform poorly and lose effectiveness when feature boosting and model explainability are used, indicating that they may be less suitable for SER applications than the well-performing models.\n\nIn conclusion, the ET classifier and LGBM are the best-performing models for TESS dataset, achieving high accuracy and F1-score. The confusion matrix shown in Fig.  8  displays the performance of the ET classifier, which performs well overall with a high number of correct predictions on the diagonal elements for each emotion. For instance, all actual \"angry\" classes are correctly predicted as \"angry\" and all actual \"disgust\" classes are correctly predicted as \"disgust\". However, there are still some misclassifications, such as 0.9% of \"happy\" being predicted as \"surprise\", 0.92% of \"neutral\" being predicted as \"sad\", and 2.78% of \"surprise\" being predicted as \"happy\". This suggests that \"happy\" and \"neutral\" share some characteristics with \"surprise\" and \"sad\", respectively.\n\nTo optimize model performance, hyperparameter tuning is performed by finding the best values for the hyperparameters that control the model's complexity and generalization performance, as is the case with our compared models. We use the random grid search technique to achieve this, training the model using a range of hyperparameter values and evaluating the performance of each model using cross-validation sets. The best set of hyperparameters is then selected based on the model's performance on the validation set.",
      "page_start": 20,
      "page_end": 20
    },
    {
      "section_name": "On Emo-Db Dataset:",
      "text": "Table  4  compares the performance of the selected machine learning models on the initially computed features for the SER task using the EMO-DB dataset. The ET classifier outperforms other models with the highest accuracy, recall, precision, and F1-score of 75.3%. The LGBM model achieves the second-highest accuracy of 64.6%, followed by LR with an accuracy of 63.1%. The RF and GBC models have a slightly lower accuracy of 62.8% and 62.2%, respectively. The LDA model has an accuracy of 60.7%. The performance of the remaining models, including Ridge, KNN, SVM, QDA, DT, ADA, and Dummy, are lower with accuracy scores ranging from 23.5% to 56.3%. These results indicate that the designed features have a moderate impact on the SER task using the EMO-DB dataset, and the ET model achieves the best overall performance among the compared models.\n\nThe confusion matrix in Fig.  9  represents the performance of the ET classifier on EMO-DB. Looking at the diagonal values, we can see that the model performs well on some emotions such as \"sadness\" achieving 88.57% accuracy, and \"anxiety/fear\" and \"happiness\" achieving 86.36% accuracy for both. However, the model performs  less with other emotions such as \"disgust\", \"anger\", \"neutral\" and \"boredom\" achieving 83.33%, 41.67%, 68.42%, and 70.59% accuracy respectively. There are also some off-diagonal values, indicating misclassifications. For example, the model seems to frequently misclassify \"boredom\" as \"anger\" or \"happiness\" (11.76% of the instances each), which could indicate some similarity between these emotions. Additionally, the model has difficulty distinguishing between \"anger\" and both \"sadness\" and \"disgust\", with it being misclassified as \"sadness\" in 25% of the instances and as \"disgust\" in 33% of the instances. While the model performs well on some emotions, it slightly struggles with others and there is room for improvement. Table  5  shows the performance of the selected machine learning models on the constructed dataset using feature boosting and model explainability feedback loop, based on the EMO-DB dataset. The best-performing model is the ET model, which achieves an accuracy of 88.3%, recall of 86.8%, precision of 88.6%, and F1 score of 87.4%. The second-best model is the LGBM model, which achieves an accuracy of 73.8%, recall of 69.4%, precision of 74%, and F1 score of 70.5%. The other models, such as LDA, RF, GBC, Ridge, DT, ADA, NB, KNN, Dummy, QDA, SVM, and LR, achieve lower performance than the top two models, with accuracy scores ranging from 62% to as low as 11.6%. The worst-performing models based on all the metrics are LR and SVM models, with accuracy scores of 11.6% and 13.1%, respectively. Overall, the results suggest that the ET and LGBM models are the optimal options for the EMO-DB dataset.\n\nThis confusion matrix in Fig.  10  shows the performance of ET model in classifying different emotions in the EMO-DB dataset. Compared to the previous confusion matrix in Fig.  9 , this one shows an improvement in overall accuracy and precision, as well as some changes in the patterns of misclassifications. For example, the model is improved in classifying \"sadness\" correctly, with 94.29% accuracy, but there is still some confusion between \"neutral\" and \"sadness\" emotions, with 10.53% of \"neutral\" emotions being misclassified as \"sadness\". Similarly, the model is improved in classifying \"disgust\" correctly, with 94.44% accuracy, but there is still some confusion between \"neutral\" and \"disgust\" emotions, with 5.56% of \"neutral\" emotions being misclassified as \"disgust\". The model is also improved in classifying \"anger\" correctly, with 75% accuracy, but there is still some confusion between \"anger\" and \"disgust\", with 16.67% of \"anger\" being misclassified as \"disgust\". We assume that these missclassifications",
      "page_start": 21,
      "page_end": 21
    },
    {
      "section_name": "Compared Methods",
      "text": "",
      "page_start": 21,
      "page_end": 21
    },
    {
      "section_name": "Tess Dataset",
      "text": "Test Accuracy (%) F1-score (%) Aggarwal et al.  [19]  97.6 97 Praseetha et al.  [20]  95.8 NA Choudhary et al.  [41]  97.1 96 Iqbal et al.  [17]  97 NA Kapoor et al.  [42]  97.5 97.4 Krishnan et al.  [18]  93.3 NA Stawicki et al.  [43]  96.5 96.7 Dupuis et al. (HLP)  [44]  82 NA Our method 99.4 99.4\n\nare due to a remarkable acoustic similarity between the mutually missclassified emotions. However, the model shows a considerable improvement in correctly classifying most of the emotions when using our approach. In summary, our experiments demonstrate the significant improvements achieved by integrating feature boosting and model explainability techniques into our SER system. Through dimensionality reduction, noise removal, and redundancy elimination, the models benefit from a more informative representation of the data, leading to enhanced performance and robustness. The explainability module plays a crucial role in identifying key feature combinations that drive the classification process. This iterative feature selection process further enhances the representation of the speech signal and improves the generalization performance of the models. By incorporating feature boosting and model explainability techniques, we achieve a comprehensive and refined approach that maximizes the utilization of relevant information while minimizing noise and redundancy, resulting in a more effective SER system.\n\nOur results highlight the effectiveness of the ET classifier and LGBM for the SER task, as they achieved high accuracy and F1-score. While the initially computed features are valuable for representing speech signals and capturing emotional content, the boosted features outperform them in distinguishing between emotions. The observed misclassifications in the confusion matrices indicate that some emotions may share similar acoustic features, posing challenges in differentiation. However, the feedback mechanism between feature boosting and model explainability proves effective in enhancing model performance and robustness.\n\nAdditionally, hyperparameter tuning techniques, such as random grid search, optimize model performance by finding the best values for controlling model complexity and generalization. This further enhances the robustness and performance of the SER system. Overall, our study demonstrates the value of integrating feature boosting and model explainability in developing an advanced and reliable SER system capable of accurately recognizing emotions in speech signals.",
      "page_start": 22,
      "page_end": 25
    },
    {
      "section_name": "Comparison With Sota Methods",
      "text": "On TESS dataset:",
      "page_start": 25,
      "page_end": 25
    },
    {
      "section_name": "Compared Methods",
      "text": "",
      "page_start": 25,
      "page_end": 25
    },
    {
      "section_name": "Emo-Db Dataset",
      "text": "Test Accuracy (%) F1-score (%) Pham et al.  [25]  76.4 NA Ancilin et al.  [26]  81.5 NA Singh et al.  [45]  86.36 NA Seo et al.  [46]  86.92 86.7 Stawicki et al.  [43]  85.8 85.6 Mustaqeem et al.  [47]  85.57 85 Our method 88.3 87.4\n\nTo further validate our findings, our proposed method's performance on the TESS dataset is compared to other state-of-the-art methods, as shown in Table  6 . We use two main evaluation approaches for our method.\n\nFirst, we compare our method against human-level performance (HLP) on the TESS dataset, as evaluated in  [44] , where authors used 56 human annotators to recognize emotions. Second, we compare our method against machine learning-based SER methods. As previously discussed, SERs typically involve two main stages: feature extraction and classification. Many of the compared methods in the literature used MFCC for feature extraction, such as  [17] ,  [20] , and  [41] , while some others used spectrograms combined with Empirical Mode Decomposition (EMD)  [18]  or PCA  [19] . For classification, some methods employed traditional machine learning techniques such as SVM  [17] , Latent Dirichlet Allocation  [18]  or decision bireducts  [43]  as an extension of decision reducts in rough set theory, offering a rule-based classification. While others used deep neural networks  [19] ,  [41] ,  [20] , and  [42] .\n\nOur proposed method achieves an accuracy of 99.4% and an F1-score of 99.4%, which are the highest scores among all the compared methods. For HLP, an accuracy of 82% is achieved on the TESS dataset. While for the machine learning based method, an accuracy of 97.6% and an F1-score of 97% are achieved by  [19] ,  [42]  achieved an accuracy of 97.5% and an F1-score of 97.4%, and  [41]  achieved an accuracy of 97.1% and an F1-score of 96%.  [20] ,  [17] , and  [18]  achieved accuracies of 95.8%, 97%, and 93.3%, respectively.\n\nIn summary, our proposed method outperforms all the compared state-of-the-art machine learning-based SER methods and it also achieves a performance that exceeds human-level performance (HLP) on the TESS dataset.",
      "page_start": 26,
      "page_end": 26
    },
    {
      "section_name": "On Emo-Db Dataset:",
      "text": "To compare our proposed method against state-of-the-art approaches, we select recent studies that focus on SER using machine learning techniques. The first study,  [25] , focuses on deep learning for SER using CNNs on the EMO-DB dataset. The authors use different spectral features for acoustic signal collections and obtain unweighted average accuracy values of 99.3% and 76.4% on the two-class and sevenclass EMO-DB datasets, respectively. The second approach presented in  [26]  proposes an improved method for SER using the Mel frequency magnitude coefficient as the feature. The authors test the proposed method on several databases, including EMO-DB, and report an accuracy of 81.5% on the EMO-DB dataset using multiclass SVM as the classifier. Authors in  [45]  leveraged optimized feature selection and classifier tuning to significantly enhance language-independent SER accuracy achieving an accuracy of 86.36%. In  [47] , a framework that fuses key sequence segment selection, CNNbased deep feature extraction, and Bi-LSTM-driven temporal information learning is proposed, achieving an accuracy of 85.57% and an F1-score of 85%. The compared methods are presented in Table  7 , where the best results on the EMO-DB dataset are highlighted in bold font. According to these results, our proposed method outperforms these methods with an accuracy of 88.3% and an F1-score of 87.4%.",
      "page_start": 26,
      "page_end": 27
    },
    {
      "section_name": "4.2.4",
      "text": "Performance and comparative analysis with SOTA methods across additional datasets: RAVDESS, SAVEE\n\nThe additional datasets utilized in the comparative analysis of SER methods each provide a further unique landscape for evaluating the effectiveness of these technologies, enriched by their diverse emotional content and specific characteristics. Each dataset, with its distinct set of emotional expressions and contexts, forms a comprehensive base for evaluating and advancing SER methods. The variety in the types of emotions, the modes of expression, and the demographic diversity of the actors across these datasets offer a rich, multifaceted perspective crucial for robust SER technology development.\n\nIn the comparative analysis of our method with other state-of-the-art methods in the field of SER across different datasets, we observe notable trends and performances. Our method showcases a leading performance on the RAVDESS dataset with accuracy and F1-score both at 87.3%, outperforming other techniques such as  [24]  at 81.3%, and  [48]  at 79.41%. Notably, some methods like  [45]  yielded a lower accuracy of 64.15%, as shown in Table  8 . This variation in performance highlights the efficacy of our approach in handling the complexities of the RAVDESS dataset. Our approach again leads on the SAVEE dataset, as we can see in Table  9 , demonstrating a high performance with an accuracy of 85.4% and an F1-score of 85.5%, outperforming  [24]  which achieved 82.1% accuracy and 82.3% F1-score, and  [45]  at 77.38% accuracy. This trend further validates the effectiveness of our method in accurately recognizing emotions from speech, as this consistently high performance across different datasets underlines the robustness and adaptability of our method.\n\nAcross all datasets, our method consistently achieved better performance, underscoring its effectiveness in SER tasks. This superior performance across various datasets indicates a robust and adaptable approach, capable of handling different emotional contexts and speech nuances effectively. The comparison with other methods, ranging from traditional machine learning techniques to advanced deep learning models with different feature selection approaches, reveals the importance of carefully boosting the features used in SER.\n\nUpon detailed examination of the confusion matrices for the RAVDESS (Fig.  11 ) and SAVEE (Fig.  12 ) datasets, it's clear that the model demonstrates remarkable proficiency in SER. These matrices, crucial for understanding the model's accuracy in classifying distinct emotions, reveal a consistent pattern of high true positive rates across various emotional states, with a notably low frequency of false positives and false negatives. In RAVDESS dataset, which features a wide range of emotions and complex interactive dialogues, the model adeptly identifies and categorizes emotions with DCNN with various classifiers 81.3 NA Er et al.  [48]  Acoustic and deep features, SVM 79.41 NA Singh et al.  [45]  Feature and classifier optimization 64.15 NA Kanwal et al.  [49]  Clustering-based genetic algorithm 82.5 NA Radoi et al.  [50]  End-to-end neural network TA-AVN 78.7 NA Ezz-Eldin et al.  [51]  Hybrid CNN and feedforward DNN 80.6 81.1 Xu et al.  [52]  Attention-based ACNN model 76.18 NA Ancilin & Milton  [26]  Mel frequency magnitude coefficient 64.31 NA Seo et al.  [46]  Visual precision, as indicated by the densely populated diagonals in the confusion matrices. Similarly, in SAVEE dataset, despite its unique challenges, the model maintains high accuracy levels. The minor misclassifications observed are typically between emotionally similar categories, such as 'Happy' and 'Surprise', which is a common challenge in SER. However, the overall high correct classification rates across these diverse datasets underscore the model's capability to discern and interpret nuanced emotional expressions in speech. This consistent performance not only attests to the model's reliability but also its adaptability to varied emotional datasets, making it a highly effective tool for SER.",
      "page_start": 27,
      "page_end": 27
    },
    {
      "section_name": "Statistical Evaluation",
      "text": "To rigorously evaluate the performance of our method relative to the best alternative method across several emotional speech datasets as shown in Fig.  13 , we employ independent two-sample t-tests. This statistical method is designed to compare the means of two independent groups (in this case, the performance metrics of two different machine learning models) to ascertain if the observed differences are statistically significant. The t-statistic quantifies the difference between the mean performance metrics relative to the sample variability, providing a basis for assessing the likelihood that such differences arose under the null hypothesis (i.e., no true difference in means). The p-value, derived from the t-statistic and degrees of freedom, offers a measure of the probability of observing the data (or more extreme) if the null hypothesis were true. A conventional alpha level of (α = 0.05) is used to assess statistical significance, where p-values below this threshold indicate strong evidence against the null hypothesis, suggesting a significant difference in performance metrics. The summarized results, as presented in the statistical evaluation Table  10 , elucidate the comparative efficacy of our method against the best alternative method with respect to accuracy and F1-score across the four datasets: TESS, EMO-DB, RAVDESS, and SAVEE. The TESS dataset exhibits pronounced differences in both accuracy and F1-score, with t-statistics of 6.62 and 8.82, respectively, and corresponding p-values significantly below the 0.05 threshold. This strongly suggests that our method not only surpasses the best alternative method in terms of overall accuracy but also maintains a superior balance between precision and recall, as indicated by the F1-score. The test on the EMO-DB dataset demonstrates a notable improvement in accuracy (t-statistic: 5.11; p-value: 0.0003) and a significant difference in F1-score (t-statistic: 2.59; p-value: 0.0244), underscoring the effectiveness of our method in recognizing emotional cues within speech with greater reliability and balance. On the RAVDESS dataset, the test shows statistically significant enhancements for both evaluated metrics (Accuracy t-statistic: 3.55, F1-score t-statistic: 3.73), with p-values indicating that these improvements are unlikely to be due to chance. This reinforces the consistency of our method's performance across varied emotional speech contexts. For the SAVEE dataset, the results indicate substantial improvements in both accuracy and F1-score (t-statistics: 7.76 and 7.52, respectively), with exceedingly low p-values, highlighting the robustness of our method in processing and classifying emotional speech with high accuracy and balanced precision-recall performance.\n\nThe statistical analysis robustly supports the conclusion that our method significantly outperforms the best alternative method across all evaluated emotional speech datasets. The consistent observation of statistically significant differences in both accuracy and F1-scores, as validated by the t-tests, provides compelling evidence of our method's superior performance. This analysis underscores the effectiveness of our approach for diverse speech emotion recognition tasks.",
      "page_start": 28,
      "page_end": 30
    },
    {
      "section_name": "Discussion",
      "text": "This work addresses the task of SER and extracting the most relevant features for accurately detecting emotions in speech, a challenge that lacks consensus in the existing literature. The significance of our research lies in bridging this knowledge gap and providing valuable insights into feature selection for SER. We contribute to the field by exploring and identifying features that play a vital role in detecting and distinguishing emotional states in speech, while also emphasizing their interpretability.\n\nOur analysis focuses on identifying features with high discriminative power and informativeness for differentiating between emotional categories. Through a rigorous feature selection process, we aim to identify the most relevant features for SER. Our study highlights the significance of key features, such as MFCCs, which effectively capture the spectral characteristics of speech and have been widely used in speech analysis tasks. Additionally, pitch or fundamental frequency (F0) features emerge as valuable for SER, as variations in pitch convey important emotional cues. Analyzing pitch-related features, such as pitch contour, range, and dynamics, provides valuable information for emotion classification. We also find that energy and intensity measures play a significant role in capturing emotional intensity and arousal, reflecting the overall energy distribution and loudness of speech. Temporal features, including speech rate and duration, demonstrate relevance in capturing temporal patterns and dynamics of emotional speech.\n\nIt is important to note that we are aware of our study's limitation, which is its testing solely on acted datasets. While the results obtained are promising, validating our approach on real-world scenarios is crucial for generalizability. Such validation would provide a comprehensive assessment of the effectiveness of our feature boosting approach in different contexts and with various speech samples, uncovering any dataset-specific biases or limitations. Real-world scenarios present additional challenges, including varying recording conditions, speaker characteristics, and noise levels, which can impact the performance of the SER system and the relevance of selected features.\n\nHowever, our research contributes to the development of a standardized feature set for SER, such as the ones discussed earlier, by presenting a comprehensive analysis of the feature selection process and highlighting the rationale behind specific feature choices. This standardized feature set serves as a foundation for future research in the field, enabling researchers to focus on these key features when designing and implementing robust SER systems. Ultimately, this standardized feature set enhances the accuracy and effectiveness of trustworthy emotion detection in real-world applications.",
      "page_start": 30,
      "page_end": 31
    },
    {
      "section_name": "Conclusion",
      "text": "This study introduces a novel supervised Speech Emotion Recognition (SER) method based on iterative boosting of designed voice features and their statistical characteristics. The incorporation of feature boosting and explainability is emphasized as crucial for improving the accuracy of SER systems. The proposed method comprises three main modules: the feature boosting module, classification module, and explainability module. Notably, this study uniquely integrates an iterative mechanism based on Explainable Artificial Intelligence (XAI) into the SER framework, which enhances and guides the feature selection process and promotes system interpretability. The presented comprehensive approach strives to strike a balance between leveraging advanced machine learning techniques and addressing the need for transparency and comprehensibility. Experimental results on TESS, EMO-DB, RAVDESS, and SAVEE datasets demonstrate the superiority of the proposed method over state-of-the-art SER methods. Furthermore, the performance surpasses human-level performance (HLP) on the TESS dataset, further validating the significance of the proposed approach.\n\nIn conclusion, this study offers a comprehensive and effective SER approach that highlights the importance of feature boosting and explainability. The method outperforms existing approaches and showcases the potential of incorporating XAI techniques into SER frameworks. Future research directions involve exploring feature boosting within deep learning frameworks, as well as generalizing and evaluating the proposed approach for other classification problems with high dimensionality and feature relevance challenges.",
      "page_start": 31,
      "page_end": 32
    },
    {
      "section_name": "Declarations",
      "text": "• Funding: The authors did not receive support from any organization for the submitted work. DB is available at  [36] , the TESS is available at  [37] , the RAVDESS dataset is available at  [38] , and the SAVEE dataset is available at  [39] . • Code availability: The source code of this paper is publicly available via this https://github.com/alaaNfissi/Unveiling-Hidden-Factors-Explainable-AI-for-Feature-Boosting-in-Speech-Emotion-Recognition.",
      "page_start": 32,
      "page_end": 32
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The proposed method with its main modules: a) Fature boosting module, b) Classification",
      "page": 7
    },
    {
      "caption": "Figure 1: The first component is a feature boosting module, which",
      "page": 7
    },
    {
      "caption": "Figure 2: Boosted features importance to the",
      "page": 16
    },
    {
      "caption": "Figure 3: , the y-axis",
      "page": 16
    },
    {
      "caption": "Figure 4: Biplot of the 2nd Feature combination (EMO-DB)",
      "page": 17
    },
    {
      "caption": "Figure 2: , we label principal components in a way that allows us to easily track",
      "page": 17
    },
    {
      "caption": "Figure 2: , we can identify the principal components that have the highest",
      "page": 17
    },
    {
      "caption": "Figure 3: helps us determine the contribu-",
      "page": 17
    },
    {
      "caption": "Figure 4: takes this a step further by",
      "page": 17
    },
    {
      "caption": "Figure 5: , which displays the data points on a 2D",
      "page": 18
    },
    {
      "caption": "Figure 5: Biplot of EMO-DB optimal feature combination",
      "page": 19
    },
    {
      "caption": "Figure 6: Cumulative explained variance of EMO-DB optimal feature combination",
      "page": 19
    },
    {
      "caption": "Figure 6: to the biplot,",
      "page": 19
    },
    {
      "caption": "Figure 7: Extra Tree classifier confusion matrix without feature boosting and model explainability",
      "page": 20
    },
    {
      "caption": "Figure 7: shows that the ET classifier performs well overall, correctly predicting the diagonal",
      "page": 20
    },
    {
      "caption": "Figure 8: Extra Tree classifier confusion matrix with feature boosting and model explainability feedback",
      "page": 21
    },
    {
      "caption": "Figure 8: displays the performance of the ET classifier, which performs well overall with a high",
      "page": 21
    },
    {
      "caption": "Figure 9: represents the performance of the ET classifier on",
      "page": 22
    },
    {
      "caption": "Figure 9: Extra Tree classifier confusion matrix without feature boosting and model explainability",
      "page": 23
    },
    {
      "caption": "Figure 10: Extra Tree classifier confusion matrix with feature boosting and model explainability",
      "page": 24
    },
    {
      "caption": "Figure 10: shows the performance of ET model in classify-",
      "page": 24
    },
    {
      "caption": "Figure 9: , this one shows an improvement in overall accuracy and precision,",
      "page": 24
    },
    {
      "caption": "Figure 12: ) datasets, it’s clear that the model demonstrates remarkable",
      "page": 27
    },
    {
      "caption": "Figure 13: , we employ",
      "page": 28
    },
    {
      "caption": "Figure 11: Confusion matrix on RAVDESS dataset",
      "page": 29
    },
    {
      "caption": "Figure 12: Confusion matrix on SAVEE dataset",
      "page": 29
    },
    {
      "caption": "Figure 13: ”Our Method” vs. ”Best Alternative Method” accuracy and F1-score comparison for all",
      "page": 29
    }
  ],
  "tables": [
    {
      "caption": "Table 1: Features comprised in EMO-DB optimal combination",
      "page": 18
    },
    {
      "caption": "Table 1: ). This reflects the",
      "page": 18
    },
    {
      "caption": "Table 2: presents the performance of the selected machine learning models on the",
      "page": 19
    },
    {
      "caption": "Table 2: Compared models on all initially computed features",
      "page": 20
    },
    {
      "caption": "Table 3: compares the performance of the same models using boosted features with",
      "page": 20
    },
    {
      "caption": "Table 3: Compared models on the constructed dataset using",
      "page": 21
    },
    {
      "caption": "Table 4: Compared models on all initially computed features",
      "page": 22
    },
    {
      "caption": "Table 4: compares the performance of the selected machine learning models on the",
      "page": 22
    },
    {
      "caption": "Table 5: Compared models on the constructed dataset using",
      "page": 23
    },
    {
      "caption": "Table 5: shows the performance of the selected machine learning models on the",
      "page": 24
    },
    {
      "caption": "Table 6: Compared methods performance on TESS dataset: best results",
      "page": 25
    },
    {
      "caption": "Table 7: Compared methods performance on EMO-DB dataset: best",
      "page": 26
    },
    {
      "caption": "Table 7: , where the best results on the EMO-DB dataset are",
      "page": 27
    },
    {
      "caption": "Table 8: This variation in performance highlights the efficacy of our approach",
      "page": 27
    },
    {
      "caption": "Table 9: , demonstrating a high performance with",
      "page": 27
    },
    {
      "caption": "Table 8: Compared methods performance on RAVDESS dataset: best results are in bold font",
      "page": 28
    },
    {
      "caption": "Table 9: Compared methods performance on SAVEE dataset: best results are in bold font",
      "page": 28
    },
    {
      "caption": "Table 10: , elucidate the comparative",
      "page": 29
    },
    {
      "caption": "Table 10: Statistical evaluation of ”Our Method” vs. ”Best Alternative",
      "page": 30
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "An overview of emotion in artificial intelligence",
      "authors": [
        "G Assunção",
        "B Patrão",
        "M Castelo-Branco",
        "P Menezes"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Artificial Intelligence"
    },
    {
      "citation_id": "2",
      "title": "Emotion, rationality, and social identity: a theoretical-methodological proposal for a cognitive approach",
      "authors": [
        "P Totaro"
      ],
      "year": "2021",
      "venue": "Cognitive Processing"
    },
    {
      "citation_id": "3",
      "title": "Beyond valence: A meta-analysis of discrete emotions in firm-customer encounters",
      "authors": [
        "A.-M Kranzbühler",
        "A Zerres",
        "M Kleijnen",
        "P Verlegh"
      ],
      "year": "2020",
      "venue": "Journal of the Academy of Marketing Science"
    },
    {
      "citation_id": "4",
      "title": "Social robots on a global stage: establishing a role for culture during human-robot interaction",
      "authors": [
        "V Lim",
        "M Rooksby",
        "E Cross"
      ],
      "year": "2021",
      "venue": "International Journal of Social Robotics"
    },
    {
      "citation_id": "5",
      "title": "it was not your fault\"-emotional awareness improves collaborative robots",
      "authors": [
        "M Shayganfar",
        "C Rich",
        "C Sidner",
        "B Hylák"
      ],
      "year": "2019",
      "venue": "2019 IEEE International Conference on Humanized Computing and Communication (HCC)"
    },
    {
      "citation_id": "6",
      "title": "Would you help a sad robot? influence of robots' emotional expressions on human-multi-robot collaboration",
      "authors": [
        "S Zhou",
        "L Tian"
      ],
      "year": "2020",
      "venue": "2020 29th IEEE International Conference on Robot and Human Interactive Communication"
    },
    {
      "citation_id": "7",
      "title": "Using artificial intelligence to enhance ongoing psychological interventions for emotional problems in real-or close to real-time: a systematic review",
      "authors": [
        "P Gual-Montolio",
        "I Jaén",
        "V Martínez-Borba",
        "D Castilla",
        "C Suso-Ribera"
      ],
      "year": "2022",
      "venue": "International Journal of Environmental Research and Public Health"
    },
    {
      "citation_id": "8",
      "title": "Feel good or do good? a valencefunction framework for understanding emotions. Current Directions in Psychological",
      "authors": [
        "S Cohen-Chen",
        "R Pliskin",
        "A Goldenberg"
      ],
      "year": "2020",
      "venue": "Science"
    },
    {
      "citation_id": "9",
      "title": "An argument for basic emotions",
      "authors": [
        "P Ekman"
      ],
      "year": "1992",
      "venue": "Cognition & emotion"
    },
    {
      "citation_id": "10",
      "title": "A comparison of machine learning algorithms and feature sets for automatic vocal emotion recognition in speech",
      "authors": [
        "C Dogdu",
        "T Kessler",
        "D Schneider",
        "M Shadaydeh",
        "S Schweinberger"
      ],
      "year": "2022",
      "venue": "Sensors"
    },
    {
      "citation_id": "11",
      "title": "Human-computer interaction with a real-time speech emotion recognition with ensembling techniques 1d convolution neural network and attention",
      "authors": [
        "W Alsabhan"
      ],
      "year": "2023",
      "venue": "Sensors"
    },
    {
      "citation_id": "12",
      "title": "Robust speech emotion recognition using cnn+ lstm based on stochastic fractal search optimization algorithm",
      "authors": [
        "A Abdelhamid",
        "E.-S El-Kenawy",
        "B Alotaibi",
        "G Amer",
        "M Abdelkader",
        "A Ibrahim",
        "M Eid"
      ],
      "year": "2022",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "13",
      "title": "Recent advances in trustworthy explainable artificial intelligence: Status, challenges, and perspectives",
      "authors": [
        "A Rawal",
        "J Mccoy",
        "D Rawat",
        "B Sadler",
        "R Amant"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Artificial Intelligence"
    },
    {
      "citation_id": "14",
      "title": "Explainable ai: Interpreting, explaining and visualizing deep learning",
      "authors": [
        "W Samek",
        "G Montavon",
        "A Vedaldi",
        "L Hansen",
        "K Müller"
      ],
      "year": "2019",
      "venue": "Explainable ai: Interpreting, explaining and visualizing deep learning"
    },
    {
      "citation_id": "15",
      "title": "A multidisciplinary survey and framework for design and evaluation of explainable ai systems",
      "authors": [
        "S Mohseni",
        "N Zarei",
        "E Ragan"
      ],
      "year": "2021",
      "venue": "ACM Transactions on Interactive Intelligent Systems (TiiS)"
    },
    {
      "citation_id": "16",
      "title": "The many shapley values for model explanation",
      "authors": [
        "M Sundararajan",
        "A Najmi"
      ],
      "year": "2020",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "17",
      "title": "Mfcc and machine learning based speech emotion recognition over tess and iemocap datasets",
      "authors": [
        "M Iqbal"
      ],
      "year": "2020",
      "venue": "Foundation University Journal of Engineering and Applied Sciences"
    },
    {
      "citation_id": "18",
      "title": "Emotion classification from speech signal based on empirical mode decomposition and non-linear features",
      "authors": [
        "P Krishnan",
        "A Joseph Raj",
        "V Rajangam"
      ],
      "year": "2021",
      "venue": "Complex & Intelligent Systems"
    },
    {
      "citation_id": "19",
      "title": "Two-way feature extraction for speech emotion recognition using deep learning",
      "authors": [
        "A Aggarwal",
        "A Srivastava",
        "A Agarwal",
        "N Chahal",
        "D Singh",
        "A Alnuaim",
        "A Alhadlaq",
        "H.-N Lee"
      ],
      "year": "2022",
      "venue": "Sensors"
    },
    {
      "citation_id": "20",
      "title": "Deep learning models for speech emotion recognition",
      "authors": [
        "V Praseetha",
        "S Vadivel"
      ],
      "year": "2018",
      "venue": "Journal of Computer Science"
    },
    {
      "citation_id": "21",
      "title": "Cnn-n-gru: end-to-end speech emotion recognition from raw waveform signal using cnns and gated recurrent unit networks",
      "authors": [
        "A Nfissi",
        "W Bouachir",
        "N Bouguila",
        "B Mishara"
      ],
      "year": "2022",
      "venue": "Proceedings of the 21st IEEE International Conference on Machine Learning and Applications"
    },
    {
      "citation_id": "22",
      "title": "Speech emotion recognition based on robust discriminative sparse regression",
      "authors": [
        "P Song",
        "W Zheng",
        "Y Yu",
        "S Ou"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Cognitive and Developmental Systems"
    },
    {
      "citation_id": "23",
      "title": "Speech emotion recognition based on pca and chmm",
      "authors": [
        "X Ke",
        "B Cao",
        "J Bai",
        "Q Yu",
        "D Yang"
      ],
      "year": "2019",
      "venue": "IEEE 8th Joint International Information Technology and Artificial Intelligence Conference (ITAIC)"
    },
    {
      "citation_id": "24",
      "title": "Impact of feature selection algorithm on speech emotion recognition using deep convolutional neural network",
      "authors": [
        "M Farooq",
        "F Hussain",
        "N Baloch",
        "F Raja",
        "H Yu",
        "Y Zikria"
      ],
      "year": "2020",
      "venue": "Sensors"
    },
    {
      "citation_id": "25",
      "title": "Emotion recognition using speech data with convolutional neural network",
      "authors": [
        "M Pham",
        "F Noori",
        "J Torresen"
      ],
      "year": "2021",
      "venue": "2021 IEEE 2nd International Conference on Signal, Control and Communication (SCC)"
    },
    {
      "citation_id": "26",
      "title": "Improved speech emotion recognition with mel frequency magnitude coefficient",
      "authors": [
        "J Ancilin",
        "A Milton"
      ],
      "year": "2021",
      "venue": "Applied Acoustics"
    },
    {
      "citation_id": "27",
      "title": "explaining the predictions of any classifier",
      "authors": [
        "M Ribeiro",
        "S Singh",
        "C Guestrin"
      ],
      "year": "2016",
      "venue": "Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining"
    },
    {
      "citation_id": "28",
      "title": "On the speech properties and feature extraction methods in speech emotion recognition",
      "authors": [
        "J Kacur",
        "B Puterka",
        "J Pavlovicova",
        "M Oravec"
      ],
      "year": "2021",
      "venue": "Sensors"
    },
    {
      "citation_id": "29",
      "title": "Feature extraction algorithms to improve the speech emotion recognition rate",
      "authors": [
        "A Koduru",
        "H Valiveti",
        "A Budati"
      ],
      "year": "2020",
      "venue": "International Journal of Speech Technology"
    },
    {
      "citation_id": "30",
      "title": "Classification assessment methods. Applied computing and informatics",
      "authors": [
        "A Tharwat"
      ],
      "year": "2020",
      "venue": "Classification assessment methods. Applied computing and informatics"
    },
    {
      "citation_id": "31",
      "title": "A unified approach to interpreting model predictions",
      "authors": [
        "S Lundberg",
        "S.-I Lee"
      ],
      "year": "2017",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "32",
      "title": "Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead",
      "authors": [
        "C Rudin"
      ],
      "year": "2019",
      "venue": "Nature machine intelligence"
    },
    {
      "citation_id": "33",
      "title": "Definitions, methods, and applications in interpretable machine learning",
      "authors": [
        "W Murdoch",
        "C Singh",
        "K Kumbier",
        "R Abbasi-Asl",
        "B Yu"
      ],
      "year": "2019",
      "venue": "Proceedings of the National Academy of Sciences"
    },
    {
      "citation_id": "34",
      "title": "Fusion of mel and gammatone frequency cepstral coefficients for speech emotion recognition using deep c-rnn",
      "authors": [
        "U Kumaran",
        "S Radha Rammohan",
        "S Nagarajan",
        "A Prathik"
      ],
      "year": "2021",
      "venue": "International Journal of Speech Technology"
    },
    {
      "citation_id": "35",
      "title": "A systematic literature review of speech emotion recognition approaches",
      "authors": [
        "Y Singh",
        "S Goel"
      ],
      "year": "2022",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "36",
      "title": "A database of german emotional speech",
      "authors": [
        "F Burkhardt",
        "A Paeschke",
        "M Rolfes",
        "W Sendlmeier",
        "B Weiss"
      ],
      "year": "2005",
      "venue": "Interspeech"
    },
    {
      "citation_id": "37",
      "title": "Toronto emotional speech set (tess)",
      "authors": [
        "K Dupuis",
        "M Pichora-Fuller"
      ],
      "year": "2010",
      "venue": "Scholars Portal Dataverse"
    },
    {
      "citation_id": "38",
      "title": "The ryerson audio-visual database of emotional speech and song (ravdess): A dynamic, multimodal set of facial and vocal expressions in north american english",
      "authors": [
        "S Livingstone",
        "F Russo"
      ],
      "year": "2018",
      "venue": "PloS one"
    },
    {
      "citation_id": "39",
      "title": "Surrey audio-visual expressed emotion (savee) database",
      "authors": [
        "P Jackson",
        "S Haq"
      ],
      "year": "2014",
      "venue": "Surrey audio-visual expressed emotion (savee) database"
    },
    {
      "citation_id": "40",
      "title": "A comparison of resampling methods for remote sensing classification and accuracy assessment",
      "authors": [
        "M Lyons",
        "D Keith",
        "S Phinn",
        "T Mason",
        "J Elith"
      ],
      "year": "2018",
      "venue": "Remote Sensing of Environment"
    },
    {
      "citation_id": "41",
      "title": "Speech emotion based sentiment recognition using deep neural networks",
      "authors": [
        "R Choudhary",
        "G Meena",
        "K Mohbey"
      ],
      "year": "2022",
      "venue": "Journal of Physics: Conference Series"
    },
    {
      "citation_id": "42",
      "title": "Fusing traditionally extracted features with deep learned features from the speech spectrogram for anger and stress detection using convolution neural network",
      "authors": [
        "S Kapoor",
        "T Kumar"
      ],
      "year": "2022",
      "venue": "Multimedia Tools and Applications"
    },
    {
      "citation_id": "43",
      "title": "Ensembles of classifiers based on decision bireducts",
      "authors": [
        "S Stawicki"
      ],
      "year": "2024",
      "venue": "Ensembles of classifiers based on decision bireducts"
    },
    {
      "citation_id": "44",
      "title": "Recognition of emotional speech for younger and older talkers: Behavioural findings from the toronto emotional speech set",
      "authors": [
        "K Dupuis",
        "M Pichora-Fuller"
      ],
      "year": "2011",
      "venue": "Canadian Acoustics"
    },
    {
      "citation_id": "45",
      "title": "An efficient language-independent acoustic emotion classification system",
      "authors": [
        "R Singh",
        "H Puri",
        "N Aggarwal",
        "V Gupta"
      ],
      "year": "2020",
      "venue": "Arabian Journal for Science and Engineering"
    },
    {
      "citation_id": "46",
      "title": "Fusing visual attention cnn and bag of visual words for crosscorpus speech emotion recognition",
      "authors": [
        "M Seo",
        "M Kim"
      ],
      "year": "2020",
      "venue": "Sensors"
    },
    {
      "citation_id": "47",
      "title": "Clustering-based speech emotion recognition by incorporating learned features and deep bilstm",
      "authors": [
        "M Sajjad",
        "S Kwon"
      ],
      "year": "2020",
      "venue": "IEEE access"
    },
    {
      "citation_id": "48",
      "title": "A novel approach for classification of speech emotions based on deep and acoustic features",
      "authors": [
        "M Er"
      ],
      "year": "2020",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "49",
      "title": "Speech emotion recognition using clustering based gaoptimized feature set",
      "authors": [
        "S Kanwal",
        "S Asghar"
      ],
      "year": "2021",
      "venue": "IEEE access"
    },
    {
      "citation_id": "50",
      "title": "An end-to-end emotion recognition framework based on temporal aggregation of multimodal information",
      "authors": [
        "A Radoi",
        "A Birhala",
        "N.-C Ristea",
        "L.-C Dutu"
      ],
      "year": "2021",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "51",
      "title": "Efficient feature-aware hybrid model of deep learning architectures for speech emotion recognition",
      "authors": [
        "M Ezz-Eldin",
        "A Khalaf",
        "H Hamed",
        "A Hussein"
      ],
      "year": "2021",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "52",
      "title": "Head fusion: Improving the accuracy and robustness of speech emotion recognition on the iemocap and ravdess dataset",
      "authors": [
        "M Xu",
        "F Zhang",
        "W Zhang"
      ],
      "year": "2021",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "53",
      "title": "Utterance level feature aggregation with deep metric learning for speech emotion recognition",
      "authors": [
        "B Mocanu",
        "R Tapu",
        "T Zaharia"
      ],
      "year": "2021",
      "venue": "Sensors"
    },
    {
      "citation_id": "54",
      "title": "Recognizing semi-natural and spontaneous speech emotions using deep neural networks",
      "authors": [
        "A Amjad",
        "L Khan",
        "N Ashraf",
        "M Mahmood",
        "H.-T Chang"
      ],
      "year": "2022",
      "venue": "IEEE Access"
    }
  ]
}