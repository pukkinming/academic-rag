{
  "paper_id": "2211.08880v1",
  "title": "Temporal-Spatial Representation Learning Transformer For Eeg-Based Emotion Recognition",
  "published": "2022-11-16T12:38:00Z",
  "authors": [
    "Zhe Wang",
    "Yongxiong Wang",
    "Chuanfei Hu",
    "Zhong Yin",
    "Yu Song"
  ],
  "keywords": [
    "Transformers",
    "Emotion recognition",
    "Self-attention mechanism",
    "EEG"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Both the temporal dynamics and spatial correlations of Electroencephalogram (EEG), which contain discriminative emotion information, are essential for the emotion recognition. However, some redundant information within the EEG signals would degrade the performance. Specifically, the subjects reach prospective intense emotions for only a fraction of the stimulus duration. Besides, it is a challenge to extract discriminative features from the complex spatial correlations among a number of electrodes. To deal with the problems, we propose a transformer-based model to robustly capture temporal dynamics and spatial correlations of EEG. Especially, temporal feature extractors which share the weight among all the EEG channels are designed to adaptively extract dynamic context information from raw signals. Furthermore, multi-head self-attention mechanism within the transformers could adaptively localize the vital EEG fragments and emphasize the essential brain regions which contribute to the performance. To verify the effectiveness of the proposed method, we conduct the experiments on two public datasets, DEAP and MAHNOB-HCI. The results demonstrate that the proposed method achieves outstanding performance on arousal and valence classification.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Emotion recognition has attracted more attention in the last decade, because it is essential to the rapidly developed human-computer interaction (HCI). Physiological signal, facial expression, and speech are the common modalities in the emotion recognition. Among the physiological signals, EEG could measure the amygdala activities which are closely related to the emotions  [1] . Besides, EEG-based emotion recognition can be widely used in service robot  [2] , medical diagnose  [3] , game entertainment  [4] , and so on.\n\nThe conventional methods mainly extract handcrafted features of EEG, and then the features are fed to the deep learning model to recognize the emotional state. Rayatdoost et al.  [5]  transform the Power Spectral Density (PSD) features into topographic maps and feed maps into the Convolutional Neural Network (CNN). Gao et al.  [6]  design a dense CNN to learn the Differential Entropy (DE) features. Khare et al.  [7]  exploit the smoothed pseudo-Wigner-Ville distribution to obtain the temporal-spectral representation which is learned by the CNN. These methods motivate the development of EEG-based emotion recognition, but the handcrafted features would potentially lose the detailed EEG fluctuation information which is beneficial to the performance. For example, Î² band (13-30 Hz) and Î³ band (30-47 Hz) are the common choices in the PSD and DE extraction. The fairly long band-width would cause the dynamic temporal information loss.\n\nTo address this problem, some researchers focus on the end-to-end model to deal with the raw EEG signals. Alhagry et al.  [8]  utilize a Long Short-Term Memory (LSTM) network to capture temporal dynamics. Ding et al.  [9]  propose a 1D-CNN based model which combines the spatial-temporal feature extraction and classification. Bethge et al.  [10]  adopt DeepConvNet architecture as the private encoders to extract latent features which are fed to a shared classifier. These methods could extract discriminative EEG features by capturing the temporal dynamics and spatial correlations among the electrodes.\n\nRaw EEG signal provides detailed information which is beneficial to discriminate the emotion, but the highdimensional input would cause the feature redundancy. One reason might be that only some specific plots in the stimuli materials can induce subject's prospective intense emotions rather than the whole duration  [11] . In addition, extract discriminative features through the complex spatial correlations among a number of electrodes is also a challenge problem in EEG feature learning.\n\nRecently, transformer-based models have been proposed for different EEG tasks. The multi-head self-attention within the transformer emphasizes the contributive features so that the model could effectively capture dynamic EEG context. Phan et al.  [12]  propose the SleepTransformer to recognize the sleep staging, and achieve outstanding performance by intra-epoch and inter-epoch context learning. Sun et al.  [13]  propose a hybrid model which combines the transformer with CNN for motor imagery classification. These researches demonstrate that transformer-based model could robustly extract discriminative EEG features.\n\nIn this work, we propose a transformer-based end-to-end model, denoted as Temporal-Spatial EEG Representation Transformer (TSERT), to deal with the feature redundancy of EEG and extract discriminative features by capturing temporal dynamics and spatial correlations. The model includes two major modules: weight-shared temporal feature extractors and a hierarchical spatial encoder. Different from the handcrafted feature extraction, the temporal feature extractors are weight-shared transformer encoders which adaptively emphasize the contributive representations from raw EEG signals of each channel. Next, the latent representations are fed to a hierarchical spatial encoder which robustly captures the EEG spatial correlations from the electrode level to the brain-region level. And we have preliminarily explored this module in the previous work  [14] .\n\nFinally, the arousal and valence predictions are obtained by TSERT. The contributions of our work can be concluded as follows: 1) We propose a novel transformer-based architecture to learn the discriminative temporal-spatial representations. The endto-end inputs preserve the detailed information of EEG, and multi-head self-attention could alleviate the feature redundancy.\n\n2) The weight-shared temporal feature extractors are inspired by the fact that handcrafted feature extraction adopts the same rule among the EEG channels. And it could adaptively extract the essential information without domain knowledge.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Methodology",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Overview Of The Tsert",
      "text": "As shown in Fig.  1 , the TSERT contains two crucial parts: temporal feature extraction and hierarchical spatial learning. In the temporal feature extraction, raw EEG signals of each channel are equally divided into several slices. Next, EEG slices are considered as the patches to the transformer encoder and fed into temporal feature extractors which share the weights among all the channels. Then the latent features could be obtained by the extractors, and they are divided into different feature sets according to the region classification of the cortex. In the hierarchical spatial learning, the feature sets are separately fed to the corresponding electrode-level transformer encoders. And the obtained representations of different brain regions are served as the patches to a brainregion-level spatial encoder in order to learn the global spatial information. Finally, a sigmoid classifier is utilized for the valence and arousal prediction.   where ğ‘€ğ‘†ğ´(â€¢) denotes MSA operation and ğ‘€ğ¿ğ‘ƒ(â€¢) denotes MLP operation the ğ¿ ğ‘‡ is the number of transformer blocks and the final output is ğ‘ ğ¿ ğ‘‡ âˆˆ â„ (ğ¾+1)Ã—ğ· ğ‘‡ . We take the average of ğ‘ ğ¿ ğ‘‡ as the latent feature ğ‘ Ì… ğ¿ ğ‘‡ âˆˆ â„ ğ· ğ‘‡ of the current channel.\n\nFinally the obtained latent features of all the channels are concatenated as ğ‘ ğ‘‡ âˆˆ â„ ğ‘Ã—ğ· ğ‘‡ .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Hierarchical Spatial Learning",
      "text": "Before the spatial learning the latent feature ğ‘ ğ‘‡ are divided into nine subsets according to nine brain regions (Prefrontal frontal and so on) which has been preliminarily explored in our previous work  [14] . In the electrode-level spatial learning the linear embedding and positional embedding can be represented as follows:\n\nwhere ğ‘€ is the number of the electrodes within the brain region ğ‘ 0 ğ¸ âˆˆ â„ (ğ‘€+1)Ã—ğ· ğ¸ is the transformer input ğ· ğ¸ is the patch dimension ğ‘Š ğ¸ âˆˆ â„ ğ· ğ‘‡ Ã—ğ· ğ¸ is the weight ğ‘‹ ğ¸ ğ‘ğ‘™ğ‘  âˆˆ â„ ğ· ğ¸ is the class token and ğ¸ ğ¸ ğ‘ğ‘œğ‘  âˆˆ â„ (ğ‘€+1)Ã—ğ· ğ¸ is the positional embedding. Similar to the aforementioned the input of brainregion-level spatial learning could be written as: ğ‘ 0 ğµ = [ğ‘‹ ğµ ğ‘ğ‘™ğ‘  , ğ‘‹ 1 ğ‘Š ğµ , â€¦ , ğ‘‹ 9 ğ‘Š ğµ ] + ğ¸ ğµ ğ‘ğ‘œğ‘  , ğ‘ 0 ğµ âˆˆ â„ (9+1)Ã—ğ· ğµ Besides the operation within the transformer is also similar to the temporal feature extraction. The numbers of the block in the electrode-level and brain-region-level are ğ¿ ğ¸ and ğ¿ ğµ respectively. And the outputs are ğ‘ ğ¿ ğ¸ âˆˆ â„ (ğ‘€+1)Ã—ğ· ğ¸ and ğ‘ ğ¿ ğµ âˆˆ â„ (9+1)Ã—ğ· ğµ . The output patch according to the class token ğ‘ ğ¿ ğµ 0 âˆˆ â„ ğ· ğµ is utilized to predict the emotion. The binary prediction of arousal or valence is obtained as follows:\n\nğ‘¦ Ì‚= ğœ(ğ‘Š ğ‘‚ ğ‘ ğ¿ ğµ 0 )\n\nwhere ğ‘¦ Ì‚ is the prediction and ğœ(â€¢) denotes the sigmoid function.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Experiments And Results",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Dataset And Pre-Processing",
      "text": "In this work, the DEAP  [15]  and MAHNOB-HCI  [16]  are chosen as the benchmark emotion datasets. Firstly, the 32channel EEG signals are down-sampled from 512Hz to 128 Hz. Next, a 4-45 Hz band pass filter and independent component analysis are adopted to remove the artifacts. Finally, we adopt a 6-second-long sliding window without overlap to segment the EEG data. Each segment within a trial is considered as a sample.\n\nOn the other hand, the self-assessment value of 1-4 is considered as 'low class' the low arousal (LA) and low valence (LV). And the self-assessment value of 6-9 is considered as 'high class' the high arousal (HA) and high valence (HV). Therefore, we transform the emotion recognition problem into two binary classifications (LA vs. HA, and LV vs. HV).",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Experiment Setup",
      "text": "In the input settings, we set the ğ‘‘ as 768 (6s ï‚´ 128Hz) ğ¾ as 6 and ğ‘‘ â€² as 128. And the hyperparameters of TSERT are also necessary to determine. In the module of temporal feature extraction the ğ· ğ‘‡ and ğ¿ ğ‘‡ are set as 64 and 1 respectively. In the module of hierarchical spatial learning the ğ· ğ¸ ğ· ğµ ğ¿ ğ¸ and ğ¿ ğµ are set as 32 64 2 and 2 respectively. In this work all the networks are implemented by Pytorch with a NVIDIA GeForce RTX 3090 GPU. We adopt the Adam optimizer with cosine learning decay learning rate is 10 -4 batch size is 512 and epoch is 80 with early stopping.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Results",
      "text": "To validate the effectiveness of TSERT we design several variants. The details of these variants are as follows:\n\n(1) SERT: Compared with TSERT the temporal feature extractors are removed from TSERT.\n\n(2) TERT: Compared with TSERT the hierarchical spatial learning module is replaced by an output layer.  (3) STERT: Different from TSERT it firstly learns the spatial information, and then it captures the temporal context of EEG.\n\nThe weights of the transformer encoders within hierarchical spatial learning are shared. And one transformer encoder is adopted to capture the temporal context among the slices. (  4 ) TSERT (PSD input): The raw EEG inputs are replaced by PSD features.\n\n(5) EEGNet  [17] : It is a classic end-to-end EEG representation model, and it also captures the temporal and spatial information. We reimplement it for the comparison.\n\nIn this work, we adopt leave-one-out cross-validation to evaluate the classification performance. Specifically, testing set is the EEG data from one subject, and training set is the EEG data from the others. This process will recurrent until each subject's data has been set as testing set once. And overall performance is the average of all the folds.\n\nThe experiment results are shown in Table  1 , we adopt the accuracy (denoted as Acc) and F1-Score to evaluate the performance. The proposed TSERT has achieved the best performances among all the models. It achieves the accuracy of 68.87% and 67.59% at arousal and valence level on the DEAP, and 70.02% and 69.32% on the MAHNOB-HCI.\n\nCompared with SERT and TERT, the TSERT have boosted the accuracy more than 3% on the DEAP dataset, and 5% on the MAHNOB-HCI. This could validate the effectiveness of temporal feature extraction module and hierarchical spatial learning module. Compared with EEGNet, the TSERT also achieved the accuracy improvement (about 6% on the and MAHNOB-HCI dataset). It demonstrates that the self-attention within the TSERT could obtain more discriminative representations. And the TSERT is also superior to the STERT (about 3% on the DEAP dataset, and about 5% on the MAHNOB-HCI dataset). The reason might be that raw EEG signals are non-linear and nonstationary, and it is difficult for the model to directly learn the spatial information. Nevertheless, STERT still outperforms the SERT and TERT. It indicates that learning discriminative representation from both temporal and spatial domain is better than the method only from single domain. Besides, we also compare the impact of different input forms on performance of TSERT. According to the results, we find that the performance of raw EEG signals surpasses the PSD features. It indicates that the TSERT could take full advantage of the detailed dynamic temporal information which is beneficial to discriminate the emotions. Furthermore, we compare the results with the recent works using cross-subject strategy in Table  2 . The results indicates that it is more difficult to design a cross-subject EEG recognition model. The reason might be the inconsistent data distribution among the different subjects. Overall, TSERT has achieved outstanding performance among the recent works. And it also outperforms the HSLT which is proposed in our previous work.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Conclusion",
      "text": "In this work, we propose a transformer-based end-to-end model, TSERT, to learn the discriminative representations from both temporal and spatial domain for EEG-based emotion recognition. Overall, the TSERT has achieved the outstanding performance on two public datasets. Especially, weight-shared temporal feature extractors outperform the handcrafted feature extraction. It could be attributed to the superior ability of multi-head self-attention in sequence learning. Specifically, the shared temporal feature extractor could adaptively represent the contributive temporal context, and alleviate feature redundancy. In the future work, we will adopt transfer learning method to optimize the TSERT in the cross-subject emotion recognition.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The overview of TSERT. â€˜*â€™ denotes that temporal feature extractors are weight-shared among all channels. â€˜#â€™ denotes",
      "page": 2
    },
    {
      "caption": "Figure 2: The structure of transformer encoder",
      "page": 3
    }
  ],
  "tables": [
    {
      "caption": "Table 1: The results of TSERT and its variants (%)",
      "data": [
        {
          "Model": "",
          "DEAP": "Arousal \nAcc \nF1-Score",
          "MAHNOB-HCI": "Arousal \nAcc \nF1-Score"
        },
        {
          "Model": "SERT \nTERT \nEEGNet \nSTERT \nTSERT (PSD input) \nProposed TSERT",
          "DEAP": "64.98 \n63.31 \n65.02 \n63.56 \n61.93 \n59.14 \n65.83 \n62.63 \n66.76 \n64.42 \n68.87 \n66.76",
          "MAHNOB-HCI": "65.02 \n62.69 \n64.80 \n62.67 \n62.17 \n59.53 \n65.39 \n63.10 \n67.21 \n64.43 \n70.02 \n66.62"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 1: The results of TSERT and its variants (%)",
      "data": [
        {
          "References": "Li et al. [18]  \n(2018)",
          "Model": "SVM",
          "Acc (%)": "A: - V: 59.06 (D)"
        },
        {
          "References": "Pandey et al [19]  \n(2019)",
          "Model": "Deep Neural Network",
          "Acc (%)": "A: 61.25 V: 62.50 (D)"
        },
        {
          "References": "Hagad et al. [20] \n(2021)",
          "Model": "BiVDANN",
          "Acc (%)": "V: 63.52 (D)"
        },
        {
          "References": "Yin et al. [21] \n(2020)",
          "Model": "Locally-robust feature \nselection & LSSVM",
          "Acc (%)": "A: 65.10 V: 67.97 (D) \nA: 67.43 V: 70.90 (M)"
        },
        {
          "References": "Zhang et al. [22] \n(2020)",
          "Model": "Shared-subspace \nfeature elimination",
          "Acc (%)": "A: 65.21 V: 66.35 (D) \nA: 65.20 V: 65.37 (M)"
        },
        {
          "References": "Ding et al. [9] \n(2022)",
          "Model": "Multi-scale 1D-CNN",
          "Acc (%)": "A: 61.57 V: 62.33 (D) \nA: 60.61 V:61.27 (M)"
        },
        {
          "References": "Wang et al. [14] \n(2022)",
          "Model": "HSLT",
          "Acc (%)": "A: 65.75 V: 66.51 (D) \nA: 66.20 V: 66.63 (M)"
        },
        {
          "References": "Ours",
          "Model": "TSERT",
          "Acc (%)": "A: 68.87 V: 67.59 (D) \nA: 70.02 V: 69.32 (M)"
        }
      ],
      "page": 4
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Categorical and dimensional affect analysis in continuous input: Current trends and future directions",
      "authors": [
        "H Gunes",
        "B Schuller"
      ],
      "year": "2013",
      "venue": "Image and Vision Computing"
    },
    {
      "citation_id": "3",
      "title": "Affective Robot Story-Telling Human-Robot Interaction: Exploratory Real-Time Emotion Estimation Analysis Using Facial Expressions and Physiological Signals",
      "authors": [
        "M Val-Calvo",
        "J Alvarez-Sanchez",
        "J Ferrandez-Vicente",
        "E Fernandez"
      ],
      "year": "2020",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "4",
      "title": "An EEG-Based Brain Computer Interface for Emotion Recognition and Its Application in Patients with Disorder of Consciousness",
      "authors": [
        "H Huang",
        "Q Xie",
        "J Pan",
        "Y He",
        "Z Wen",
        "R Yu",
        "Y Li"
      ],
      "year": "2021",
      "venue": "IEEE Transaction on Affective Computing"
    },
    {
      "citation_id": "5",
      "title": "Recognizing Emotional States with Wearables While Playing a Serious Game",
      "authors": [
        "D Diaz-Romero",
        "A RincÃ³ N",
        "A Miguel-Cruz",
        "N Yee",
        "E Stroulia"
      ],
      "year": "2021",
      "venue": "IEEE Transaction on Instrumentation and Measurement"
    },
    {
      "citation_id": "6",
      "title": "Cross-corpus EEG-based emotion recognition",
      "authors": [
        "S Rayatdoost",
        "M Soleymani"
      ],
      "year": "2018",
      "venue": "2018 IEEE International workshop on Machine Learning for Signal Processing"
    },
    {
      "citation_id": "7",
      "title": "A Channel-Fused Dense Convolutional Network for EEG-Based Emotion Recognition",
      "authors": [
        "Z Gao",
        "X Wang",
        "Y Yang",
        "Y Li",
        "K Ma",
        "G Chen"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Cognitive and Developmental Systems"
    },
    {
      "citation_id": "8",
      "title": "Time-Frequency Representation and Convolutional Neural Network-Based Emotion Recognition",
      "authors": [
        "S Khare",
        "V Bajaj"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Neural Networks and Learning Systems"
    },
    {
      "citation_id": "9",
      "title": "Emotion Recognition based on EEG using LSTM Recurrent Neural Network",
      "authors": [
        "Alhagry Aly"
      ],
      "year": "2017",
      "venue": "International Journal of Advanced Computer Science and Applications"
    },
    {
      "citation_id": "10",
      "title": "TSception: Capturing Temporal Dynamics and Spatial Asymmetry from EEG for Emotion Recognition",
      "authors": [
        "Y Ding",
        "N Robinson",
        "S Zhang",
        "Q Zeng",
        "C Guan"
      ],
      "year": "2022",
      "venue": "IEEE Transaction on Affective Computing"
    },
    {
      "citation_id": "11",
      "title": "Domain-Invariant Representation Learning from EEG with Private Encoders",
      "authors": [
        "D Bethge",
        "P Hallgarten",
        "T Grosse-Puppendahl",
        "M Kari",
        "R Mikut",
        "A Schmidt",
        "O Ã–zdenizci"
      ],
      "year": "2022",
      "venue": "2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "12",
      "title": "EEG-based Emotion Recognition with Emotion Localization via Hierarchical Self-Attention",
      "authors": [
        "Y Zhang",
        "H Liu",
        "D Zhang",
        "X Chen",
        "T Qin",
        "Q Zheng"
      ],
      "year": "2022",
      "venue": "IEEE Transaction on Affective Computing"
    },
    {
      "citation_id": "13",
      "title": "SleepTransformer: Automatic Sleep Staging With Interpretability and Uncertainty Quantification",
      "authors": [
        "H Phan",
        "K Mikkelsen",
        "O ChÃ© N",
        "P Koch",
        "A Mertins",
        "M Vos"
      ],
      "year": "2022",
      "venue": "IEEE Transaction on Biomedical Engineering"
    },
    {
      "citation_id": "14",
      "title": "EEG Classification with Transformer-Based Models",
      "authors": [
        "J Sun",
        "J Xie",
        "H Zhou"
      ],
      "year": "2021",
      "venue": "IEEE 3rd Global Conference on Life Sciences and Technologies (LifeTech)"
    },
    {
      "citation_id": "15",
      "title": "Transformers for EEG-Based Emotion Recognition: A Hierarchical Spatial Information Learning Model",
      "authors": [
        "Wang Wang",
        "C Hu",
        "Z Yin",
        "Y Song"
      ],
      "year": "2022",
      "venue": "IEEE Sensors Journal"
    },
    {
      "citation_id": "16",
      "title": "DEAP: A database for emotion analysis using physiological signals",
      "authors": [
        "S Koelstra",
        "C Muhl",
        "M Soleymani",
        "J Lee",
        "A Yazdani",
        "T Ebrahimi",
        "T Pun",
        "A Nijholt",
        "I Patras"
      ],
      "year": "2012",
      "venue": "IEEE Transaction on Affective Computing"
    },
    {
      "citation_id": "17",
      "title": "A Multimodal Database for Affect Recognition and Implicit Tagging",
      "authors": [
        "M Soleymani",
        "J Lichtenauer",
        "T Pun",
        "M Pantic"
      ],
      "year": "2012",
      "venue": "IEEE Transaction on Affective Computing"
    },
    {
      "citation_id": "18",
      "title": "EEGNet: a compact convolutional neural network for EEG-based brain-computer interfaces",
      "authors": [
        "V Lawhern",
        "A Solon",
        "N Waytowich",
        "S Gordon",
        "C Hung",
        "B Lance"
      ],
      "year": "2018",
      "venue": "Journal of Neural Engineering"
    },
    {
      "citation_id": "19",
      "title": "Exploring EEG features in cross-subject emotion recognition",
      "authors": [
        "Li Song",
        "P Zhang",
        "Y Hou",
        "B Hu"
      ],
      "year": "2018",
      "venue": "Frontiers in Neuroscience"
    },
    {
      "citation_id": "20",
      "title": "Subject independent emotion recognition from EEG using VMD and deep learning",
      "authors": [
        "P Pandey",
        "K Seeja"
      ],
      "year": "2022",
      "venue": "Journal of King Saud University -Computer and Information Sciences"
    },
    {
      "citation_id": "21",
      "title": "Learning Subject-Generalized Topographical EEG Embeddings Using Deep Variational Autoencoders and Domain-Adversarial Regularization",
      "authors": [
        "J Hagad",
        "T Kimura",
        "K Fukui",
        "M Numao"
      ],
      "year": "2021",
      "venue": "Sensors"
    },
    {
      "citation_id": "22",
      "title": "Locally robust EEG feature selection for individual-independent emotion recognition",
      "authors": [
        "Z Yin",
        "L Liu",
        "J Chen",
        "B Zhao",
        "Y Wang"
      ],
      "year": "2020",
      "venue": "Expert Systems with Applications"
    },
    {
      "citation_id": "23",
      "title": "Selecting transferrable neurophysiological features for inter-individual emotion recognition via a shared-subspace feature elimination approach",
      "authors": [
        "W Zhang Z. Yin",
        "Z Sun",
        "Y Tian",
        "Y Wang"
      ],
      "year": "2020",
      "venue": "Computers in Biology and Medicine"
    }
  ]
}