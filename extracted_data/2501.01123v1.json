{
  "paper_id": "2501.01123v1",
  "title": "Ted: Turn Emphasis With Dialogue Feature Attention For Emotion Recognition In Conversation",
  "published": "2025-01-02T07:44:48Z",
  "authors": [
    "Junya Ono",
    "Hiromi Wakaki"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Emotion recognition in conversation (ERC) has been attracting attention by methods for modeling multi-turn contexts. The multi-turn input to a pretraining model implicitly assumes that the current turn and other turns are distinguished during the training process by inserting special tokens into the input sequence. This paper proposes a priority-based attention method to distinguish each turn explicitly by adding dialogue features into the attention mechanism, called Turn Emphasis with Dialogue (TED). It has a priority for each turn according to turn position and speaker information as dialogue features. It takes multi-head self-attention between turn-based vectors for multi-turn input and adjusts attention scores with the dialogue features. We evaluate TED on four typical benchmarks. The experimental results demonstrate that TED has high overall performance in all datasets and achieves state-of-the-art performance on IEMOCAP with numerous turns.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Emotion recognition in conversation (ERC) has been discussed for over decade  (Yang et al., 2007; Devlin et al., 2019) . In ERC, an emotion label is estimated for the current turn from multiple turns' worth of utterances and speaker information. Emotion understanding such as ERC has the potential to be used in chatbots, medical situations, and call centers.\n\nThe methods of ERC are often used on the basis of past and future contexts, external commonsense knowledge, and speaker information  (Majumder et al., 2019; Ghosal et al., 2020; Zhu et al., 2021; Shen et al., 2021) . Especially, studies that involve modeling multiple turns have grown in number since the appearance of ERC datasets that embody different perspectives including multiple speakers and topics  (Li et al., 2017; Zahiri and Choi, 2017; Poria et al., 2018) .\n\nFigure  1 : Concept of our method. This shows the difference in the multi-turn input in related methods. Our method prioritizes each turn to distinguish the turns The mainstream multi-turn methods can exploit past and future turns for more contexts through their use of recurrent neural networks (RNNs), graph neural networks (GNNs), and Transformer  (Vaswani et al., 2017) . These methods often input the token sequence for only the current turn into a pretrained model, such as BERT  (Devlin et al., 2019)  and RoBERTa  (Liu et al., 2019) . They cannot use the context other than the current turn for single-turn input. Therefore, the methods of multiturn input have been recently proposed by inserting special tokens into the input sequence as shown in Figure  1    (Li et al., 2020a; Gu et al., 2020; Lee and Choi, 2021; Kim and Vossen, 2021) . These methods can obtain deeper contexts by adding the utterances in multiple turns into the input sequence. On the other hand, they design to distinguish each turn by including information of turn position and speaker as special tokens. However, this multiturn input implicitly expects to be distinguished between the current turn and other turns to be modeled in the process of machine learning.\n\nWe introduce a concept to distinguish explicitly between each turn and to control a degree of the distinction while making the most of dialogue features. As one of the methods, 1 shows our concept to clarify the distinction by prioritizing each turn and maximizing the priority of the current turn in comparison with related methods  (Li et al., 2020a; Kim and Vossen, 2021) . This concept uses special tokens of  [TURN]  and  [SEP]  as turn breaks and delimiter of the current turn, respectively. Moreover, it adjusts relationship between the current turn and other turns by using priority factor β t calculated by dialogue features of turn position and speaker information; t indicates position of a turn; SPK indicates Speaker information.\n\nOur proposal, a priority-based attention method, called \"Turn Emphasis with Dialogue (TED)\", uses the above dialogue features. First, TED creates a multi-turn input sequence by concatenating all turns with special tokens and also creates a turnbased vector by averaging the token-based vectors at the same positions of the utterance in a turn, called \"Turn-Based Encoding (TBE)\". Second, it establishes multi-head self-attention (MHSA) between the turn-based vectors of TBE, called \"Turn-Based MHSA (TBM)\". Finally, it adds a dialogue layer to TBM to adjust attention scores for three types of all turns, the same speaker turns at the same as a current turn, or the listener turns. Figure  2  shows an example of making adjustments with an attention factor β t ; t indicates the turn number. The comprehensive results in four typical benchmarks demonstrate that TED has high performance in all datasets and especially archives state-of-theart performance in IEMOCAP with a lot of turns.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work",
      "text": "ERC has recently attracted attention because it can handle more complex contexts using multiple turns. Numerous powerful approaches have been proposed on the basis of RNNs, GNNs, and Transformers. Many use a pretrained model, such as BERT and RoBERTa, to make sequence representations corresponding to input tokens.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Multi-Turn Models",
      "text": "Here, we review recent models based on three different neural networks.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Rnn-Based Models",
      "text": "ICON  (Hazarika et al., 2018a)  and CMN  (Hazarika et al., 2018b)  have gated recurrent unit (GRU) and memory networks. HiGRU  (Jiao et al., 2019)  has two GRUs for the utterance and conversation. BiF-AGRU  (Jiao et al., 2020)  has a hierarchical memory network with an attention GRU for historical utterances. DialogueRNN  (Majumder et al., 2019)  utilizes the global state as a context and the party state for individual speakers by incorporating bidirectional GRUs for emotional dynamics. COSMIC  (Ghosal et al., 2020)  has a similar structure to Dia-logueRNN but with added external commonsense knowledge. BiERU  (Li et al., 2020b)  devised an efficient and party-ignorant framework by using a bi-recurrent unit. CESTa  (Wang et al., 2020)  handles the global context by using Transformer and individual speakers by using BiLSTM-CRF. DialogueCRN  (Hu et al., 2021)  has contextual reasoning networks that have long short-term memory (LSTM) to understand situations and speaker context.\n\nGNN-based models DialogGCN  (Ghosal et al., 2019)  handles the dependency and positional relationship of speakers as a graph structure. RGAT  (Ishiwatari et al., 2020)  has a similar strategy to DialogGCN but with added positional encodings. ConGCN  (Zhang et al., 2019)  builds an entire dataset including utterances and speakers as a large graph. SumAggGIN  (Sheng et al., 2020)  has two stages of summarization and aggregation graphs for capturing emotional fluctuation. DAG-ERC  (Shen et al., 2021)  models the flow between long distance and nearby contexts. TUCORE-GCN  (Lee and Choi, 2021)  constructs a graph of relational information in a dialogue with four types of nodes and three types of edges. Transformer-based models KET  (Zhong et al., 2019)  incorporates hierarchical self-attention with commonsense knowledge. DialogueTRM  (Mao et al., 2020)  uses a hierarchical Transformer and multi-grained network for multi-modal fusion. Hi-Trans  (Li et al., 2020a)  uses two hierarchical transformers, one for low-level representations from BERT, the other a high level turn-by-turn Transformer structure. DialogXL  (Shen et al., 2020)  deals with long-term memory and four types of dialog-aware self-attention based on XLNet  (Yang et al., 2019) . TODKAT  (Zhu et al., 2021)  integrates topic representations and commonsense knowledge into an encoder-decoder structure. EmoBERTa  (Kim and Vossen, 2021)  inputs one sentence including past and future turns with special tokens into RoBERTa.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Input Encoding",
      "text": "Various formats have recently been used for expressing the multi-turn input by using utterances and speaker information in turns. To encode the multi-turn input sequence, one method adds speaker information in the form of a special token and adds segment, positional, and speaker embeddings to the utterance embedding  (Gu et al., 2020; Li et al., 2017) . Another method replaces the speaker information with a real name and adds separator tokens to the front and back of the current turn  (Kim and Vossen, 2021) . EmoBERTa, which is an example of the latter method, achieves high accuracy with only the token-based vector of the head token <s> and RoBERTa without a special multi-turn network. We believe that a richer context can be obtained from multi-turn utterances.\n\nOur model, TED, is inspired by these methods, but it does not include speaker information in the input sequence. Instead, we apply speaker information to the attention mechanism.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Methodology",
      "text": "The following three subsections describe TED as a classification problem using a pretrained model and multi-turn contexts.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Turn-Based Encoding",
      "text": "Here, we introduce the turn-based encoding model (TBE), which has two components, i.e. input encoding and creation of the mean vector of utterance to encode multi-turn utterances. Figure  3  shows the structure of TBE.\n\nInput Encoding First, TBE creates token-based vectors by concatenating multi-turn utterances including past and future turns with special tokens (called \"Concat Utterance with Special Token\", or CUST). As indicated in Figure  3 , CUST outputs a token sequence including past and future turns as follows:\n\nwhere x denotes a token in an utterance; x c = i x c i denotes the token sequence of an utterance at the current turn;\n\ndenote the list of the token sequences in past and future turns, respectively; t T and t S denote special tokens [TURN] and [SEP], respectively. CUST outputs a token sequence by adding [TURN] at the end of each turn to distinguish turn breaks and by adding [SEP] to the front and back of the current turn to distinguish it from the other turns. [SEP] is also added to the end of a sequence when pretraining. By comparison, HiTrans uses a CLS token at the beginning of each turn to distinguish between turns. However, the CLS token is often used as a context vector in a downstream task as the first token of BERT. Therefore, we add a new token  [TURN] , which means only turn breaks.\n\nThe \"pretrained model\" (PTM) in Figure  3  is a pretrained model such as RoBERTa that outputs a list H of token-based vectors per turn for all turns as follows:\n\nwhere h c denotes the list of token-based vectors in the current turn, where the range of tokens exactly matches the positions of the input sequence in the current turn among the outputs of PTM. Note that the vectors corresponding to the BOS and end EOS tokens are omitted.\n\nMean Vector of Utterance Second, TBE creates turn-based vectors in order to average the tokenbased vectors corresponding to the utterance positions in each turn.\n\nThe list of the turn-based vectors H in all m turns is defined as\n\nwhere H c in Figure  3  is the average of the h c . The label with the highest probability P among L labels is chosen as the \"Emotion label\" in Figure  3 :\n\nwhere F is a linear function; s c denotes speaker information at the current turn;\n\ndenote the list of speaker information of past and future turns; y l denotes the l th emotion label.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Turn-Based Multi-Head Self-Attention",
      "text": "We use the multi-head self-attention (MHSA) of Transformer  (Vaswani et al., 2017)  between turns to obtain more context based on TBE. Figure  4  shows our turn-based MHSA (TBM) model. The output of MHSA is added to the original input; then, the layer norm is taken in the same way as in the vanilla Transformer. TUCORE-GCN uses MHSA masked with a turnbased window size and performs attention on a token-by-token basis. HiTrans also establishes token-based MHSA between CLS tokens, which is added to the input at the beginning of each turn. Our model differs from TUCORE-GCN in that it is MHSA with turn-based vectors of positions that exactly match the tokens of utterances.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Turn Emphasis With Dialogue (Ted)",
      "text": "We propose \"Turn Emphasis with Dialogue (TED)\" to emphasis the current turn with two dialogue features, called \"priority\" and \"speaker\", as shown Figure  2 . We add a dialogue layer to TBM to adjust the attention scores with the turn priority and speaker information, as shown in Figure  5 .\n\nThe multi-head self-attention in Figure  5  has the same structure as the vanilla Transformer. α j is the attention score matrix for all m turns at the j th head of MHSA as follows:\n\nwhere α t j denotes the attention score vector for the t th turn after the softmax function.\n\nMHSA outputs attentional vectors by a concatenation with each head as follows:\n\nwhere H denotes the list of turn-based vectors for all m turns, W V j denote learning parameters to scale the dimension of the model and the head, respectively, V j denotes the list of scaled vectors for all turns, and A t j denotes the attentional vector using the above attention scores.\n\nSo far, all turns are treated equally. Therefore, to emphasize the current turn, we construct a new dialogue layer, as follows. Let α t ′ j be the weighted score vector produced by the dialogue feature attention in the last (N th ) layer in Figure  5 .\n\nwhere β t denotes the attention factor at the t th turn, which is calculated by the following two points.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Turn Priority",
      "text": "The current turn is emphasized with the priority shown in Figure  2 . The dialogue feature attention adjusts the priority in accordance with the following normal distribution.\n\nwhere γ t denotes either no priority (i.e., Constant) or a variable priority based on a normal distribution (N ormdist), γ denotes the maximum coefficient, and t c and σ respectively denote the current turn and standard deviation. In case of N ormdist, Formula (8) shows that the current turn is emphasized by the turn priority; the farther away the turn is from the current turn t c , the smaller the value of γ t (i.e., N ormdist) becomes.\n\nSpeaker Information speaker IDs are applied to the attention mechanism. The dialogue feature attention adjusts the attention factor for the same speaker or listener as follows:\n\nwhere S c and S t denote the speaker IDs of the current and t th turn, respectively. Formula (  9 ) is used to weight the turns that have the same speaker as the current turn and Formula (  10 ) is used to weight the listener turns.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Experiment Setting",
      "text": "We evaluate TED on four ERC datasets. The datasets include speaker IDs for every turn. The training, development, and test data split follow the related work, such as COSMIC  (Ghosal et al., 2020) .",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Dataset",
      "text": "Table  1  shows the statistics of the datasets. IEMO-CAP  (Busso et al., 2008 ) is a multimodal dataset including text transcriptions of two speakers and it is annotated with six emotions. MELD  (Poria et al., 2018 ) is a multimodal dataset created from a TV show, Friends. It includes 260 speakers and is labeled with seven emotions. EmoryNLP (Zahiri and Choi, 2017) uses the same data source as MELD, i.e., the TV show Friends, and it annotates different utterances as compared with MELD by seven emotions. The utterances are from 225 speakers. DailyDialog  (Li et al., 2017)  contains utterances of two speakers communicating on various topics related to daily life. It is annotated with seven emotions.\n\nIEMOCAP is different from the other datasets in that its standard deviation (Std.) of the number of turns in one dialogue is 16.8, while the Std. of the others range from 3.99-5.79. This means it is possible to use more surrounding contexts for IEMOCAP. Regarding the speaker IDs of MELD and EmoryNLP, we assign new IDs to each dialogue to avoid the low frequency of appearance due to the large number of speakers",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Training And Evaluation Setting",
      "text": "We use RoBERTa-large of the Hugging Face library  (Wolf et al., 2020) . Each experiment consists of five trials and the average values of the trials are reported so that the initial values of parameters do not affect the judgments about the models. Moreover, each trial has different seeded fixed values and ran under the same computer specifications in order to maintain reproducibility.\n\nAs in the related work, we calculate micro average F1 (Micro F1) excluding the majority neutral and macro average F1 (Macro F1) on all labels for DailyDialog. On the other datasets, we calculate the weighted average f1 (W-Avg f1) and micro F1 for all labels. The evaluation scores on the test data are calculated for the model of the epoch with the highest score in the development data.\n\nAll experiments are subject to the same conditional schedule of the learning rate and early stopping. The learning rate is multiplied by a fixed value (0.8) when the evaluation score of the development data does not increase at the end of the epoch. Early stopping forcibly terminates the learning when the best score are not updated within 5 epochs Regarding the model parameters, we use two attention layers and four heads. In Formula (8), σ is the standard deviation of the number of turns in all dialogues as shown in Table  1  and γ is a hyperparameter for the turn priority; it was set to 1.5, 2, 3, and 5.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Results And Analysis",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Overall Performance",
      "text": "Table  2  compares the performance of TED and the other latest models listed in Section 2. TED had good overall performance, while the other models had good performance on certain datasets. Moreover, TED had an advantage in terms of the dialogue features on datasets with a lot of turns, such as IEMOCAP.\n\nIn Formula (  7 ), the attention score vector is a function of • all turns, turns that have the same speaker as in the current turn, or listener turn.\n\n• a decay factor determined by turn priority (No decay (i.e., Constant) or a Normal distribution centered on a current turn (i.e., N ormdist).\n\nIn terms of the context turn in CUST, we target the past only or both the past and the future. We describe the above combination of parameters of the results in Table  2 . The IEMOCAP column in Table  2  shows results for past, Listener, and Normdist; the MELD column those for past, Listener, and Constant; the EmoryNLP column those for both the past and the future, All turns, and Normdist, and the DailyDialog those for both the past and the future, Listener, and Constant.\n\nThe results suggest that TED strengthens the relationships between turns and accelerates the turn emphasis by specially treating the current turn.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Detailed Performance",
      "text": "Here, we examine the results of TED in more detail by analyzing the effect of different parameters in the dialogue feature attention. In Table  3 , the \"Dialogue Feature\" column indicates the combination of two dialog feature items; A, S, and L indicate the target turns to be weighted by all turns (A), the same speaker turns (S), and listener turns (L), respectively; \"C\" indicates no decay factor (i.e., Constant), while \"N\" indicates the normal distribution in Formula (8).\n\nThe results for the different context turns in Table 3 vary depending on the dataset. MELD and EmoryNLP have similar data statistics, as shown in Table  1 . Nevertheless, TED's performance was higher in the context turn of the past only for MELD but was higher in the context turn of both the past and future for EmoryNLP.\n\nThe dialogue feature attention provided a slight improvement compared with that of TBM, which does not use the dialogue features. Varying the combinations of features yielded no significant differences in the results.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Effect Of Input Encoding And Mean",
      "text": "Vector of Utterance (TBE)\n\nTable  4  compares the TBE models that have only input encoding part or both the input encoding part and mean vector of utterance part with a baseline that does not have either part. The \"Context turn\" column indicates whether to include past, future, or both kinds of turn in CUST; \"no use\" indicates only the current turn for the baseline.\n\nThe results show the effectiveness of the past turns and TBE. The future turns do not contribute to a performance improvement. We consider that the current state is influenced by the past states in a chain reaction, while the current state is not directly affected by the future states.\n\nThe performance on IEMOCAP is significantly improved in the case of past turns. The performance of EmoBERTa shows the same tendency as TBE on IEMOCAP.\n\nThese results suggest that the turn-based vector of TBE is effective.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Comparison With Related Work Using Speaker Information",
      "text": "TBM performs MHSA on turn-based vectors and does not use speaker information, as stated in Section 3.2. Herein, we applied speaker information to TBM in two ways.\n\nThe first way involved adding special tokens corresponding to speaker IDs to the input sequence, as is done in TUCORE-GCN. For instance, when the speaker IDs are 0 and 1, the speaker token becomes [SPK0] or [SPK1], respectively. Note that while EmoBERTa uses real names as speaker IDs, we added speaker tokens to use the same IDs as the original data as accurate as possible.\n\nThe other way was to attend only to the turns of the target speakers. This is called local attention, and it was originally applied to an encoder-decoder  (Luong et al., 2015) .\n\nIn Table  5 , the \"Speaker token\" column shows results for speaker IDs, while the \"Speaker attention\" and \"Listener attention\" columns show results for the attention on turns with the same speaker as the current one and on listener turns. The performance of TBM was significantly higher than that of TBE. However, neither way of adding speaker information showed any advantage. On the contrary, the addition of speaker tokens caused a noticeable loss in performance.\n\nThe results also show that it is better to attend to all turns than to limit the attention to certain turns. Moreover, DialogXL reported that global attention contributed more than speaker or listener attention. Accordingly, the results suggests that the high performance of TBM is based on global attention.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Variants Of Tbm Structure",
      "text": "The MHSA of TBM is the same as that of the vanilla Transformer. Because Transformer-based  models use positional encoding (PE) and a feed forward network (FFN), we decided to examine the effect of adding them to TBM in the same way as in the vanilla Transformer as shown in Figure  6 .\n\nTable  6  shows the effect of adding these Transformer components and the network size depending on the number of layers and heads. The \"TBM\" row contains results for only MHSA, while the \"Positional Encoding\" and \"Feed Forward Networks\" rows show results for TBM with these layers. The \"# layers\" and \"# heads\" columns respectively indicate the number of layers of the above components and the number of heads of MHSA. Note that CUST used only the past turns.\n\nThe results show that there is no advantage to adding PE and FFN, which are token-based components. Therefore, the results show that TBM works better with turn-based components than with turnbased and token-based components together. Regarding the network parameters, smaller layers and heads are effective. In other words, the complexity of the parameters had no effect on the performance of the turn-based mechanism.\n\nThe improvement on IEMOCAP was remarkable, but that on MELD was slight. Regarding this result, more turns would lead to more context and higher performance. We conclude that, compared with the state-of-the-art methods, TBM, which uses turn-based MHSA between turn-based vectors based on TBE, captures more complicated contexts in the multiple turns.\n\nFor modeling multi-turn contexts of conversation, we presented TED, which emphasizes the current turn and explicitly distinguishes each turn by adding the dialogue features into the attention mechanism. The results in four ERC datasets show that TED had good overall performance, while the other models had good performance on certain datasets. Moreover, TED had an advantage in terms of the dialogue features on datasets with a lot of turns, such as IEMOCAP. Further experiments demonstrated the effectiveness of TED's key components; TBE, TBM, and the dialogue features of the turn position and speaker information. Our priority factor for the distinction of the turns has made it possible to emphasize the emotional target turn and can adapt to diverse datasets by controlling the dialogue features.\n\nIn Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 1571-1582, Online. Association for Computational Linguistics.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "A Attention Score (Supplementary)",
      "text": "The attention score α t j of Formula (  5 ) is as follows:\n\nwhere d K denotes the head dimension to correct the inner product. It is obtained by dividing the model dimension by the number of J heads. W Q j and W K j denote learning parameters for the query and key process, respectively. Q t j and K j take a linear transformation to d K from the turn-based vector at the t th turn for the query process and the list of the scaled vectors for the key process in all turns, respectively. The superscript T denotes transpose.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "B Experimental Environment",
      "text": "",
      "page_start": 10,
      "page_end": 10
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Concept of our method. This shows the dif-",
      "page": 1
    },
    {
      "caption": "Figure 1: (Li et al., 2020a; Gu et al., 2020; Lee",
      "page": 1
    },
    {
      "caption": "Figure 2: Example of ERC and dialogue features. TED",
      "page": 2
    },
    {
      "caption": "Figure 2: shows an example of making adjustments with",
      "page": 2
    },
    {
      "caption": "Figure 3: Turn-based encoding (TBE) model. CUST",
      "page": 3
    },
    {
      "caption": "Figure 3: , CUST outputs a",
      "page": 3
    },
    {
      "caption": "Figure 4: Turn-based MHSA (TBM) model. TBM es-",
      "page": 4
    },
    {
      "caption": "Figure 3: is the average of the hc.",
      "page": 4
    },
    {
      "caption": "Figure 3: P = (Pl,··· ,Pl,··· ,PL)",
      "page": 4
    },
    {
      "caption": "Figure 5: The proposed model, TED, has a dialogue",
      "page": 4
    },
    {
      "caption": "Figure 4: shows our turn-based MHSA (TBM) model. The",
      "page": 4
    },
    {
      "caption": "Figure 2: We add a dialogue layer to TBM to ad-",
      "page": 4
    },
    {
      "caption": "Figure 5: The multi-head self-attention in Figure 5 has the",
      "page": 4
    },
    {
      "caption": "Figure 2: The dialogue",
      "page": 5
    },
    {
      "caption": "Figure 6: Variants of the TBM structure. A positional",
      "page": 8
    },
    {
      "caption": "Figure 6: Table 6 shows the effect of adding these Trans-",
      "page": 8
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Abstract": "Emotion recognition in conversation (ERC) has"
        },
        {
          "Abstract": "been attracting attention by methods for model-"
        },
        {
          "Abstract": "ing multi-turn contexts. The multi-turn input to"
        },
        {
          "Abstract": "a pretraining model implicitly assumes that the"
        },
        {
          "Abstract": "current turn and other turns are distinguished"
        },
        {
          "Abstract": "during the training process by inserting spe-"
        },
        {
          "Abstract": "cial tokens into the input sequence. This paper"
        },
        {
          "Abstract": "proposes a priority-based attention method to"
        },
        {
          "Abstract": "distinguish each turn explicitly by adding di-"
        },
        {
          "Abstract": "alogue features into the attention mechanism,"
        },
        {
          "Abstract": "called Turn Emphasis with Dialogue (TED). It"
        },
        {
          "Abstract": "has a priority for each turn according to turn po-"
        },
        {
          "Abstract": "sition and speaker information as dialogue fea-"
        },
        {
          "Abstract": "tures. It takes multi-head self-attention between"
        },
        {
          "Abstract": "turn-based vectors for multi-turn input and ad-"
        },
        {
          "Abstract": "justs attention scores with the dialogue features."
        },
        {
          "Abstract": "We evaluate TED on four typical benchmarks."
        },
        {
          "Abstract": "The experimental results demonstrate that TED"
        },
        {
          "Abstract": ""
        },
        {
          "Abstract": "has high overall performance in all datasets"
        },
        {
          "Abstract": ""
        },
        {
          "Abstract": "and achieves state-of-the-art performance on"
        },
        {
          "Abstract": ""
        },
        {
          "Abstract": "IEMOCAP with numerous turns."
        },
        {
          "Abstract": ""
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "their use of\nrecurrent neural networks\n(RNNs),": ""
        },
        {
          "their use of\nrecurrent neural networks\n(RNNs),": "graph neural networks (GNNs), and Transformer"
        },
        {
          "their use of\nrecurrent neural networks\n(RNNs),": "(Vaswani et al., 2017). These methods often input"
        },
        {
          "their use of\nrecurrent neural networks\n(RNNs),": ""
        },
        {
          "their use of\nrecurrent neural networks\n(RNNs),": "the token sequence for only the current\nturn into"
        },
        {
          "their use of\nrecurrent neural networks\n(RNNs),": "a pretrained model, such as BERT (Devlin et al.,"
        },
        {
          "their use of\nrecurrent neural networks\n(RNNs),": "2019) and RoBERTa (Liu et al., 2019). They can-"
        },
        {
          "their use of\nrecurrent neural networks\n(RNNs),": "not use the context other than the current turn for"
        },
        {
          "their use of\nrecurrent neural networks\n(RNNs),": "single-turn input. Therefore, the methods of multi-"
        },
        {
          "their use of\nrecurrent neural networks\n(RNNs),": "turn input have been recently proposed by inserting"
        },
        {
          "their use of\nrecurrent neural networks\n(RNNs),": "special\ntokens into the input sequence as shown"
        },
        {
          "their use of\nrecurrent neural networks\n(RNNs),": "in Figure 1 (Li et al., 2020a; Gu et al., 2020; Lee"
        },
        {
          "their use of\nrecurrent neural networks\n(RNNs),": "and Choi, 2021; Kim and Vossen, 2021). These"
        },
        {
          "their use of\nrecurrent neural networks\n(RNNs),": "methods can obtain deeper contexts by adding the"
        },
        {
          "their use of\nrecurrent neural networks\n(RNNs),": ""
        },
        {
          "their use of\nrecurrent neural networks\n(RNNs),": "utterances in multiple turns into the input sequence."
        },
        {
          "their use of\nrecurrent neural networks\n(RNNs),": ""
        },
        {
          "their use of\nrecurrent neural networks\n(RNNs),": "On the other hand, they design to distinguish each"
        },
        {
          "their use of\nrecurrent neural networks\n(RNNs),": ""
        },
        {
          "their use of\nrecurrent neural networks\n(RNNs),": "turn by including information of turn position and"
        },
        {
          "their use of\nrecurrent neural networks\n(RNNs),": ""
        },
        {
          "their use of\nrecurrent neural networks\n(RNNs),": "speaker as special\ntokens. However,\nthis multi-"
        },
        {
          "their use of\nrecurrent neural networks\n(RNNs),": ""
        },
        {
          "their use of\nrecurrent neural networks\n(RNNs),": "turn input\nimplicitly expects to be distinguished"
        },
        {
          "their use of\nrecurrent neural networks\n(RNNs),": ""
        },
        {
          "their use of\nrecurrent neural networks\n(RNNs),": "between the current turn and other turns to be mod-"
        },
        {
          "their use of\nrecurrent neural networks\n(RNNs),": ""
        },
        {
          "their use of\nrecurrent neural networks\n(RNNs),": "eled in the process of machine learning."
        },
        {
          "their use of\nrecurrent neural networks\n(RNNs),": ""
        },
        {
          "their use of\nrecurrent neural networks\n(RNNs),": "We introduce a concept to distinguish explicitly"
        },
        {
          "their use of\nrecurrent neural networks\n(RNNs),": "between each turn and to control a degree of the"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "2\nRelated work": "ERC has\nrecently attracted attention because it"
        },
        {
          "2\nRelated work": "can handle more complex contexts using multi-"
        },
        {
          "2\nRelated work": "ple turns. Numerous powerful approaches have"
        },
        {
          "2\nRelated work": "been proposed on the basis of RNNs, GNNs, and"
        },
        {
          "2\nRelated work": "Transformers. Many use a pretrained model, such"
        },
        {
          "2\nRelated work": "as BERT and RoBERTa, to make sequence repre-"
        },
        {
          "2\nRelated work": "sentations corresponding to input tokens."
        },
        {
          "2\nRelated work": "2.1\nMulti-turn models"
        },
        {
          "2\nRelated work": ""
        },
        {
          "2\nRelated work": "Here, we review recent models based on three dif-"
        },
        {
          "2\nRelated work": ""
        },
        {
          "2\nRelated work": "ferent neural networks."
        },
        {
          "2\nRelated work": ""
        },
        {
          "2\nRelated work": ""
        },
        {
          "2\nRelated work": "RNN-based models\nICON (Hazarika\net\nal.,"
        },
        {
          "2\nRelated work": "2018a) and CMN (Hazarika et al., 2018b) have"
        },
        {
          "2\nRelated work": "gated recurrent unit (GRU) and memory networks."
        },
        {
          "2\nRelated work": "HiGRU (Jiao et\nal., 2019) has\ntwo GRUs\nfor"
        },
        {
          "2\nRelated work": "the utterance and conversation. BiF-AGRU (Jiao"
        },
        {
          "2\nRelated work": ""
        },
        {
          "2\nRelated work": "et al., 2020) has a hierarchical memory network"
        },
        {
          "2\nRelated work": ""
        },
        {
          "2\nRelated work": "with an attention GRU for historical utterances."
        },
        {
          "2\nRelated work": ""
        },
        {
          "2\nRelated work": "DialogueRNN (Majumder et al., 2019) utilizes"
        },
        {
          "2\nRelated work": ""
        },
        {
          "2\nRelated work": "the global state as a context and the party state"
        },
        {
          "2\nRelated work": ""
        },
        {
          "2\nRelated work": "for individual speakers by incorporating bidirec-"
        },
        {
          "2\nRelated work": ""
        },
        {
          "2\nRelated work": "tional GRUs for emotional dynamics. COSMIC"
        },
        {
          "2\nRelated work": ""
        },
        {
          "2\nRelated work": "(Ghosal et al., 2020) has a similar structure to Dia-"
        },
        {
          "2\nRelated work": ""
        },
        {
          "2\nRelated work": "logueRNN but with added external commonsense"
        },
        {
          "2\nRelated work": ""
        },
        {
          "2\nRelated work": "knowledge. BiERU (Li et al., 2020b) devised an"
        },
        {
          "2\nRelated work": ""
        },
        {
          "2\nRelated work": "efficient and party-ignorant\nframework by using"
        },
        {
          "2\nRelated work": ""
        },
        {
          "2\nRelated work": "a bi-recurrent unit.\nCESTa (Wang et al., 2020)"
        },
        {
          "2\nRelated work": ""
        },
        {
          "2\nRelated work": "handles the global context by using Transformer"
        },
        {
          "2\nRelated work": ""
        },
        {
          "2\nRelated work": "and individual speakers by using BiLSTM-CRF."
        },
        {
          "2\nRelated work": ""
        },
        {
          "2\nRelated work": "DialogueCRN (Hu et al., 2021) has contextual rea-"
        },
        {
          "2\nRelated work": ""
        },
        {
          "2\nRelated work": "soning networks that have long short-term memory"
        },
        {
          "2\nRelated work": ""
        },
        {
          "2\nRelated work": "(LSTM) to understand situations and speaker con-"
        },
        {
          "2\nRelated work": ""
        },
        {
          "2\nRelated work": "text."
        },
        {
          "2\nRelated work": ""
        },
        {
          "2\nRelated work": "GNN-based models\nDialogGCN (Ghosal et al.,"
        },
        {
          "2\nRelated work": "2019) handles the dependency and positional rela-"
        },
        {
          "2\nRelated work": "tionship of speakers as a graph structure. RGAT"
        },
        {
          "2\nRelated work": "(Ishiwatari et al., 2020) has a similar strategy to"
        },
        {
          "2\nRelated work": "DialogGCN but with added positional encodings."
        },
        {
          "2\nRelated work": "ConGCN (Zhang et al., 2019) builds an entire"
        },
        {
          "2\nRelated work": "dataset including utterances and speakers as a large"
        },
        {
          "2\nRelated work": "graph. SumAggGIN (Sheng et al., 2020) has two"
        },
        {
          "2\nRelated work": "stages of summarization and aggregation graphs"
        },
        {
          "2\nRelated work": "for capturing emotional fluctuation. DAG-ERC"
        },
        {
          "2\nRelated work": "(Shen et al., 2021) models the flow between long"
        },
        {
          "2\nRelated work": "distance and nearby contexts. TUCORE-GCN (Lee"
        },
        {
          "2\nRelated work": "and Choi, 2021) constructs a graph of relational in-"
        },
        {
          "2\nRelated work": "formation in a dialogue with four types of nodes"
        },
        {
          "2\nRelated work": "and three types of edges."
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "the head token <s> and RoBERTa without a spe-": "cial multi-turn network. We believe that a richer"
        },
        {
          "the head token <s> and RoBERTa without a spe-": "context can be obtained from multi-turn utterances."
        },
        {
          "the head token <s> and RoBERTa without a spe-": "Our model, TED, is inspired by these methods,"
        },
        {
          "the head token <s> and RoBERTa without a spe-": "but it does not include speaker information in the"
        },
        {
          "the head token <s> and RoBERTa without a spe-": "input sequence. Instead, we apply speaker informa-"
        },
        {
          "the head token <s> and RoBERTa without a spe-": "tion to the attention mechanism."
        },
        {
          "the head token <s> and RoBERTa without a spe-": "3\nMethodology"
        },
        {
          "the head token <s> and RoBERTa without a spe-": "The following three subsections describe TED as"
        },
        {
          "the head token <s> and RoBERTa without a spe-": "a classification problem using a pretrained model"
        },
        {
          "the head token <s> and RoBERTa without a spe-": "and multi-turn contexts."
        },
        {
          "the head token <s> and RoBERTa without a spe-": "3.1\nTurn-Based Encoding"
        },
        {
          "the head token <s> and RoBERTa without a spe-": ""
        },
        {
          "the head token <s> and RoBERTa without a spe-": "Here, we introduce the turn-based encoding model"
        },
        {
          "the head token <s> and RoBERTa without a spe-": ""
        },
        {
          "the head token <s> and RoBERTa without a spe-": "(TBE), which has two components, i.e.\ninput en-"
        },
        {
          "the head token <s> and RoBERTa without a spe-": ""
        },
        {
          "the head token <s> and RoBERTa without a spe-": "coding and creation of the mean vector of utterance"
        },
        {
          "the head token <s> and RoBERTa without a spe-": ""
        },
        {
          "the head token <s> and RoBERTa without a spe-": "to encode multi-turn utterances. Figure 3 shows"
        },
        {
          "the head token <s> and RoBERTa without a spe-": "the structure of TBE."
        },
        {
          "the head token <s> and RoBERTa without a spe-": ""
        },
        {
          "the head token <s> and RoBERTa without a spe-": "Input Encoding\nFirst, TBE creates token-based"
        },
        {
          "the head token <s> and RoBERTa without a spe-": ""
        },
        {
          "the head token <s> and RoBERTa without a spe-": "vectors by concatenating multi-turn utterances in-"
        },
        {
          "the head token <s> and RoBERTa without a spe-": ""
        },
        {
          "the head token <s> and RoBERTa without a spe-": "cluding past and future turns with special\ntokens"
        },
        {
          "the head token <s> and RoBERTa without a spe-": ""
        },
        {
          "the head token <s> and RoBERTa without a spe-": "(called “Concat Utterance with Special Token”, or"
        },
        {
          "the head token <s> and RoBERTa without a spe-": ""
        },
        {
          "the head token <s> and RoBERTa without a spe-": "CUST). As indicated in Figure 3, CUST outputs a"
        },
        {
          "the head token <s> and RoBERTa without a spe-": ""
        },
        {
          "the head token <s> and RoBERTa without a spe-": "token sequence including past and future turns as"
        },
        {
          "the head token <s> and RoBERTa without a spe-": ""
        },
        {
          "the head token <s> and RoBERTa without a spe-": "follows:"
        },
        {
          "the head token <s> and RoBERTa without a spe-": ""
        },
        {
          "the head token <s> and RoBERTa without a spe-": "(cid:16)\n(cid:17)"
        },
        {
          "the head token <s> and RoBERTa without a spe-": ""
        },
        {
          "the head token <s> and RoBERTa without a spe-": "CU ST\n:= Concat\n(1)\nxc, X p, X f , tT , ts"
        },
        {
          "the head token <s> and RoBERTa without a spe-": ""
        },
        {
          "the head token <s> and RoBERTa without a spe-": ""
        },
        {
          "the head token <s> and RoBERTa without a spe-": "where x denotes a token in an utterance; xc ="
        },
        {
          "the head token <s> and RoBERTa without a spe-": "(cid:80)"
        },
        {
          "the head token <s> and RoBERTa without a spe-": "i denotes the token sequence of an utterance\ni xc"
        },
        {
          "the head token <s> and RoBERTa without a spe-": "(cid:1)"
        },
        {
          "the head token <s> and RoBERTa without a spe-": ",· · ·, (cid:80)\nat the current turn; X p = (cid:0)(cid:80)"
        },
        {
          "the head token <s> and RoBERTa without a spe-": "i xc−1\ni x1"
        },
        {
          "the head token <s> and RoBERTa without a spe-": ""
        },
        {
          "the head token <s> and RoBERTa without a spe-": "= (cid:0)(cid:80)\n,· · ·, (cid:80)\n(cid:1) denote"
        },
        {
          "the head token <s> and RoBERTa without a spe-": "and X f\nthe\ni xm\ni xc+1"
        },
        {
          "the head token <s> and RoBERTa without a spe-": ""
        },
        {
          "the head token <s> and RoBERTa without a spe-": "list of\nthe\ntoken sequences\nin past\nand future"
        },
        {
          "the head token <s> and RoBERTa without a spe-": ""
        },
        {
          "the head token <s> and RoBERTa without a spe-": "turns, respectively; tT and tS denote special tokens"
        },
        {
          "the head token <s> and RoBERTa without a spe-": ""
        },
        {
          "the head token <s> and RoBERTa without a spe-": "[TURN] and [SEP], respectively. CUST outputs a"
        },
        {
          "the head token <s> and RoBERTa without a spe-": "token sequence by adding [TURN] at\nthe end of"
        },
        {
          "the head token <s> and RoBERTa without a spe-": ""
        },
        {
          "the head token <s> and RoBERTa without a spe-": "each turn to distinguish turn breaks and by adding"
        },
        {
          "the head token <s> and RoBERTa without a spe-": "[SEP] to the front and back of the current turn to"
        },
        {
          "the head token <s> and RoBERTa without a spe-": "distinguish it from the other turns.\n[SEP] is also"
        },
        {
          "the head token <s> and RoBERTa without a spe-": "added to the end of a sequence when pretraining."
        },
        {
          "the head token <s> and RoBERTa without a spe-": "By comparison, HiTrans uses a CLS token at the"
        },
        {
          "the head token <s> and RoBERTa without a spe-": "beginning of each turn to distinguish between turns."
        },
        {
          "the head token <s> and RoBERTa without a spe-": "However, the CLS token is often used as a context"
        },
        {
          "the head token <s> and RoBERTa without a spe-": "vector in a downstream task as the first\ntoken of"
        },
        {
          "the head token <s> and RoBERTa without a spe-": "BERT. Therefore, we add a new token [TURN],"
        },
        {
          "the head token <s> and RoBERTa without a spe-": "which means only turn breaks."
        },
        {
          "the head token <s> and RoBERTa without a spe-": "The “pretrained model” (PTM) in Figure 3 is a"
        },
        {
          "the head token <s> and RoBERTa without a spe-": "pretrained model such as RoBERTa that outputs a"
        },
        {
          "the head token <s> and RoBERTa without a spe-": "list H of token-based vectors per turn for all turns"
        },
        {
          "the head token <s> and RoBERTa without a spe-": "as follows:"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Figure 4: Turn-based MHSA (TBM) model. TBM es-": "tablishes MHSA between turn-based vectors to obtain"
        },
        {
          "Figure 4: Turn-based MHSA (TBM) model. TBM es-": "more contexts based on TBE."
        },
        {
          "Figure 4: Turn-based MHSA (TBM) model. TBM es-": ""
        },
        {
          "Figure 4: Turn-based MHSA (TBM) model. TBM es-": ""
        },
        {
          "Figure 4: Turn-based MHSA (TBM) model. TBM es-": ""
        },
        {
          "Figure 4: Turn-based MHSA (TBM) model. TBM es-": ""
        },
        {
          "Figure 4: Turn-based MHSA (TBM) model. TBM es-": ",··· ,hc\n,··· ,hm(cid:1)"
        },
        {
          "Figure 4: Turn-based MHSA (TBM) model. TBM es-": "(2)"
        },
        {
          "Figure 4: Turn-based MHSA (TBM) model. TBM es-": "where hc = (cid:0)hc"
        },
        {
          "Figure 4: Turn-based MHSA (TBM) model. TBM es-": "1,··· ,hc\ni,··· ,hc\nn"
        },
        {
          "Figure 4: Turn-based MHSA (TBM) model. TBM es-": ""
        },
        {
          "Figure 4: Turn-based MHSA (TBM) model. TBM es-": "where hc denotes the list of token-based vectors in"
        },
        {
          "Figure 4: Turn-based MHSA (TBM) model. TBM es-": ""
        },
        {
          "Figure 4: Turn-based MHSA (TBM) model. TBM es-": "the current turn, where the range of tokens exactly"
        },
        {
          "Figure 4: Turn-based MHSA (TBM) model. TBM es-": ""
        },
        {
          "Figure 4: Turn-based MHSA (TBM) model. TBM es-": "matches the positions of the input sequence in the"
        },
        {
          "Figure 4: Turn-based MHSA (TBM) model. TBM es-": ""
        },
        {
          "Figure 4: Turn-based MHSA (TBM) model. TBM es-": "current turn among the outputs of PTM. Note that"
        },
        {
          "Figure 4: Turn-based MHSA (TBM) model. TBM es-": ""
        },
        {
          "Figure 4: Turn-based MHSA (TBM) model. TBM es-": "the vectors corresponding to the BOS and end EOS"
        },
        {
          "Figure 4: Turn-based MHSA (TBM) model. TBM es-": ""
        },
        {
          "Figure 4: Turn-based MHSA (TBM) model. TBM es-": "tokens are omitted."
        },
        {
          "Figure 4: Turn-based MHSA (TBM) model. TBM es-": ""
        },
        {
          "Figure 4: Turn-based MHSA (TBM) model. TBM es-": "Mean Vector of Utterance\nSecond, TBE creates"
        },
        {
          "Figure 4: Turn-based MHSA (TBM) model. TBM es-": ""
        },
        {
          "Figure 4: Turn-based MHSA (TBM) model. TBM es-": "turn-based vectors in order to average the token-"
        },
        {
          "Figure 4: Turn-based MHSA (TBM) model. TBM es-": ""
        },
        {
          "Figure 4: Turn-based MHSA (TBM) model. TBM es-": "based vectors corresponding to the utterance posi-"
        },
        {
          "Figure 4: Turn-based MHSA (TBM) model. TBM es-": ""
        },
        {
          "Figure 4: Turn-based MHSA (TBM) model. TBM es-": "tions in each turn."
        },
        {
          "Figure 4: Turn-based MHSA (TBM) model. TBM es-": ""
        },
        {
          "Figure 4: Turn-based MHSA (TBM) model. TBM es-": "The list of\nthe turn-based vectors (cid:102)H in all m"
        },
        {
          "Figure 4: Turn-based MHSA (TBM) model. TBM es-": ""
        },
        {
          "Figure 4: Turn-based MHSA (TBM) model. TBM es-": "turns is defined as"
        },
        {
          "Figure 4: Turn-based MHSA (TBM) model. TBM es-": ""
        },
        {
          "Figure 4: Turn-based MHSA (TBM) model. TBM es-": "c\nm(cid:17)\n(cid:16)"
        },
        {
          "Figure 4: Turn-based MHSA (TBM) model. TBM es-": "H 1"
        },
        {
          "Figure 4: Turn-based MHSA (TBM) model. TBM es-": ",..., (cid:102)H\n,..., (cid:102)H"
        },
        {
          "Figure 4: Turn-based MHSA (TBM) model. TBM es-": ""
        },
        {
          "Figure 4: Turn-based MHSA (TBM) model. TBM es-": "(3)"
        },
        {
          "Figure 4: Turn-based MHSA (TBM) model. TBM es-": "where (cid:101)H c = M ean(hc)"
        },
        {
          "Figure 4: Turn-based MHSA (TBM) model. TBM es-": ""
        },
        {
          "Figure 4: Turn-based MHSA (TBM) model. TBM es-": ""
        },
        {
          "Figure 4: Turn-based MHSA (TBM) model. TBM es-": "H c\nwhere\nin Figure 3 is\nthe average of\nthe hc."
        },
        {
          "Figure 4: Turn-based MHSA (TBM) model. TBM es-": ""
        },
        {
          "Figure 4: Turn-based MHSA (TBM) model. TBM es-": "The label with the highest probability P among"
        },
        {
          "Figure 4: Turn-based MHSA (TBM) model. TBM es-": ""
        },
        {
          "Figure 4: Turn-based MHSA (TBM) model. TBM es-": "L labels is chosen as the “Emotion label” in Figure"
        },
        {
          "Figure 4: Turn-based MHSA (TBM) model. TBM es-": ""
        },
        {
          "Figure 4: Turn-based MHSA (TBM) model. TBM es-": "3:"
        },
        {
          "Figure 4: Turn-based MHSA (TBM) model. TBM es-": ""
        },
        {
          "Figure 4: Turn-based MHSA (TBM) model. TBM es-": "P = (Pl,··· ,Pl,··· ,PL)"
        },
        {
          "Figure 4: Turn-based MHSA (TBM) model. TBM es-": "(cid:110)\n(cid:16)"
        },
        {
          "Figure 4: Turn-based MHSA (TBM) model. TBM es-": "= Sof tmax\nF\nH c(cid:17)(cid:111)"
        },
        {
          "Figure 4: Turn-based MHSA (TBM) model. TBM es-": "(4)"
        },
        {
          "Figure 4: Turn-based MHSA (TBM) model. TBM es-": ""
        },
        {
          "Figure 4: Turn-based MHSA (TBM) model. TBM es-": "(cid:16)"
        },
        {
          "Figure 4: Turn-based MHSA (TBM) model. TBM es-": "where Pl = P\nyl|xc, X p, X f , sc, Sp, Sf (cid:17)"
        },
        {
          "Figure 4: Turn-based MHSA (TBM) model. TBM es-": ""
        },
        {
          "Figure 4: Turn-based MHSA (TBM) model. TBM es-": "where F is a linear function; sc denotes speaker in-"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "to scale the dimension of the model and the head,": "",
          "(cid:40)": "γt\n,\nSt\n̸= Sc"
        },
        {
          "to scale the dimension of the model and the head,": "respectively, Vj denotes the list of scaled vectors",
          "(cid:40)": "βt =\n(10)"
        },
        {
          "to scale the dimension of the model and the head,": "",
          "(cid:40)": "1 ,\notherwise"
        },
        {
          "to scale the dimension of the model and the head,": "for all turns, and A t\ndenotes the attentional vector",
          "(cid:40)": ""
        },
        {
          "to scale the dimension of the model and the head,": "j",
          "(cid:40)": ""
        },
        {
          "to scale the dimension of the model and the head,": "using the above attention scores.",
          "(cid:40)": ""
        },
        {
          "to scale the dimension of the model and the head,": "",
          "(cid:40)": "where Sc and St denote the speaker IDs of the"
        },
        {
          "to scale the dimension of the model and the head,": "So far, all turns are treated equally. Therefore, to",
          "(cid:40)": ""
        },
        {
          "to scale the dimension of the model and the head,": "",
          "(cid:40)": "current and tth turn, respectively. Formula (9) is"
        },
        {
          "to scale the dimension of the model and the head,": "emphasize the current turn, we construct a new dia-",
          "(cid:40)": ""
        },
        {
          "to scale the dimension of the model and the head,": "",
          "(cid:40)": "used to weight the turns that have the same speaker"
        },
        {
          "to scale the dimension of the model and the head,": "be the weighted\nlogue layer, as follows. Let (cid:101)α t ′",
          "(cid:40)": "as the current\nturn and Formula (10)\nis used to"
        },
        {
          "to scale the dimension of the model and the head,": "score vector produced by the dialogue feature at-",
          "(cid:40)": ""
        },
        {
          "to scale the dimension of the model and the head,": "",
          "(cid:40)": "weight the listener turns."
        },
        {
          "to scale the dimension of the model and the head,": "tention in the last (N th) layer in Figure 5.",
          "(cid:40)": ""
        },
        {
          "to scale the dimension of the model and the head,": "",
          "(cid:40)": "4\nExperiment setting"
        },
        {
          "to scale the dimension of the model and the head,": "(cid:17)\n(cid:16)",
          "(cid:40)": ""
        },
        {
          "to scale the dimension of the model and the head,": "βt exp\nα t",
          "(cid:40)": ""
        },
        {
          "to scale the dimension of the model and the head,": "j",
          "(cid:40)": ""
        },
        {
          "to scale the dimension of the model and the head,": "",
          "(cid:40)": "We evaluate TED on four ERC datasets.\nThe"
        },
        {
          "to scale the dimension of the model and the head,": "α t ′\n=\n(7)",
          "(cid:40)": ""
        },
        {
          "to scale the dimension of the model and the head,": "j\n(cid:16)\n(cid:17)",
          "(cid:40)": ""
        },
        {
          "to scale the dimension of the model and the head,": "(cid:101)",
          "(cid:40)": ""
        },
        {
          "to scale the dimension of the model and the head,": "(cid:80)",
          "(cid:40)": "datasets include speaker IDs for every turn. The"
        },
        {
          "to scale the dimension of the model and the head,": "α k",
          "(cid:40)": ""
        },
        {
          "to scale the dimension of the model and the head,": "k βk exp\nj",
          "(cid:40)": ""
        },
        {
          "to scale the dimension of the model and the head,": "",
          "(cid:40)": "training, development, and test data split\nfollow"
        },
        {
          "to scale the dimension of the model and the head,": "where βt denotes the attention factor at\nthe tth",
          "(cid:40)": "the related work, such as COSMIC (Ghosal et al.,"
        },
        {
          "to scale the dimension of the model and the head,": "turn, which is calculated by the following two",
          "(cid:40)": "2020)."
        },
        {
          "to scale the dimension of the model and the head,": "points.",
          "(cid:40)": ""
        },
        {
          "to scale the dimension of the model and the head,": "",
          "(cid:40)": "4.1\nDataset"
        },
        {
          "to scale the dimension of the model and the head,": "Turn priority\nThe current\nturn is emphasized",
          "(cid:40)": ""
        },
        {
          "to scale the dimension of the model and the head,": "",
          "(cid:40)": "Table 1 shows the statistics of the datasets. IEMO-"
        },
        {
          "to scale the dimension of the model and the head,": "with the priority shown in Figure 2. The dialogue",
          "(cid:40)": ""
        },
        {
          "to scale the dimension of the model and the head,": "",
          "(cid:40)": "CAP (Busso et al., 2008) is a multimodal dataset"
        },
        {
          "to scale the dimension of the model and the head,": "feature attention adjusts the priority in accordance",
          "(cid:40)": ""
        },
        {
          "to scale the dimension of the model and the head,": "",
          "(cid:40)": "including text transcriptions of two speakers and it"
        },
        {
          "to scale the dimension of the model and the head,": "with the following normal distribution.",
          "(cid:40)": ""
        },
        {
          "to scale the dimension of the model and the head,": "",
          "(cid:40)": "is annotated with six emotions."
        },
        {
          "to scale the dimension of the model and the head,": "",
          "(cid:40)": "MELD (Poria et al., 2018) is a multimodal dataset"
        },
        {
          "to scale the dimension of the model and the head,": "γ\nConstant",
          "(cid:40)": ""
        },
        {
          "to scale the dimension of the model and the head,": "",
          "(cid:40)": "created from a TV show, Friends. It includes 260"
        },
        {
          "to scale the dimension of the model and the head,": " \n(cid:26)\n(cid:27)\nβt\n:= γt =",
          "(cid:40)": ""
        },
        {
          "to scale the dimension of the model and the head,": "(t − tc)2",
          "(cid:40)": ""
        },
        {
          "to scale the dimension of the model and the head,": "",
          "(cid:40)": "speakers and is labeled with seven emotions."
        },
        {
          "to scale the dimension of the model and the head,": "1 + γ exp\n−\nN ormdist",
          "(cid:40)": ""
        },
        {
          "to scale the dimension of the model and the head,": "2σ2",
          "(cid:40)": ""
        },
        {
          "to scale the dimension of the model and the head,": "",
          "(cid:40)": "EmoryNLP (Zahiri and Choi, 2017) uses the same"
        },
        {
          "to scale the dimension of the model and the head,": "(8)",
          "(cid:40)": ""
        },
        {
          "to scale the dimension of the model and the head,": "",
          "(cid:40)": "data source as MELD, i.e.,\nthe TV show Friends,"
        },
        {
          "to scale the dimension of the model and the head,": "γt\nwhere\ndenotes\neither\nno\npriority\n(i.e.,",
          "(cid:40)": ""
        },
        {
          "to scale the dimension of the model and the head,": "",
          "(cid:40)": "and it annotates different utterances as compared"
        },
        {
          "to scale the dimension of the model and the head,": "Constant) or a variable priority based on a nor-",
          "(cid:40)": ""
        },
        {
          "to scale the dimension of the model and the head,": "",
          "(cid:40)": "with MELD by seven emotions. The utterances are"
        },
        {
          "to scale the dimension of the model and the head,": "mal distribution (N ormdist), γ denotes the maxi-",
          "(cid:40)": ""
        },
        {
          "to scale the dimension of the model and the head,": "",
          "(cid:40)": "from 225 speakers."
        },
        {
          "to scale the dimension of the model and the head,": "mum coefficient, and tc and σ respectively denote",
          "(cid:40)": ""
        },
        {
          "to scale the dimension of the model and the head,": "",
          "(cid:40)": "DailyDialog (Li et al., 2017) contains utterances"
        },
        {
          "to scale the dimension of the model and the head,": "the current turn and standard deviation. In case of",
          "(cid:40)": ""
        },
        {
          "to scale the dimension of the model and the head,": "",
          "(cid:40)": "of two speakers communicating on various topics"
        },
        {
          "to scale the dimension of the model and the head,": "N ormdist, Formula (8) shows that the current turn",
          "(cid:40)": ""
        },
        {
          "to scale the dimension of the model and the head,": "",
          "(cid:40)": "related to daily life.\nIt\nis annotated with seven"
        },
        {
          "to scale the dimension of the model and the head,": "is emphasized by the turn priority; the farther away",
          "(cid:40)": ""
        },
        {
          "to scale the dimension of the model and the head,": "",
          "(cid:40)": "emotions."
        },
        {
          "to scale the dimension of the model and the head,": "the turn is from the current turn tc, the smaller the",
          "(cid:40)": ""
        },
        {
          "to scale the dimension of the model and the head,": "",
          "(cid:40)": "IEMOCAP is different from the other datasets"
        },
        {
          "to scale the dimension of the model and the head,": "value of γt (i.e., N ormdist) becomes.",
          "(cid:40)": ""
        },
        {
          "to scale the dimension of the model and the head,": "",
          "(cid:40)": "in that its standard deviation (Std.) of the number"
        },
        {
          "to scale the dimension of the model and the head,": "Speaker Information\nspeaker IDs are applied",
          "(cid:40)": "of turns in one dialogue is 16.8, while the Std. of"
        },
        {
          "to scale the dimension of the model and the head,": "to the attention mechanism. The dialogue feature",
          "(cid:40)": "the others range from 3.99-5.79.\nThis means it"
        },
        {
          "to scale the dimension of the model and the head,": "attention adjusts the attention factor for the same",
          "(cid:40)": "is possible to use more surrounding contexts for"
        },
        {
          "to scale the dimension of the model and the head,": "speaker or listener as follows:",
          "(cid:40)": "IEMOCAP. Regarding the speaker IDs of MELD"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table 1: and γ is a",
      "data": [
        {
          "IEMOCAP": "",
          "MELD": "",
          "EmoryNLP": "",
          "DailyDialog": ""
        },
        {
          "IEMOCAP": "W-Avg F1",
          "MELD": "W-Avg F1",
          "EmoryNLP": "W-Avg F1",
          "DailyDialog": "Micro F1"
        },
        {
          "IEMOCAP": "64.50",
          "MELD": "61.94",
          "EmoryNLP": "36.75",
          "DailyDialog": "-"
        },
        {
          "IEMOCAP": "65.94",
          "MELD": "62.41",
          "EmoryNLP": "34.73",
          "DailyDialog": "-"
        },
        {
          "IEMOCAP": "65.28",
          "MELD": "65.21",
          "EmoryNLP": "38.11",
          "DailyDialog": "51.05"
        },
        {
          "IEMOCAP": "67.10",
          "MELD": "58.36",
          "EmoryNLP": "-",
          "DailyDialog": "-"
        },
        {
          "IEMOCAP": "68.03",
          "MELD": "63.65",
          "EmoryNLP": "39.02",
          "DailyDialog": "-"
        },
        {
          "IEMOCAP": "68.57",
          "MELD": "65.61",
          "EmoryNLP": "-",
          "DailyDialog": "-"
        },
        {
          "IEMOCAP": "61.33",
          "MELD": "68.23",
          "EmoryNLP": "43.12",
          "DailyDialog": "52.56"
        },
        {
          "IEMOCAP": "-",
          "MELD": "65.36",
          "EmoryNLP": "39.24",
          "DailyDialog": "-"
        },
        {
          "IEMOCAP": "68.63",
          "MELD": "66.35",
          "EmoryNLP": "39.07",
          "DailyDialog": "55.71"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table 1: and γ is a",
      "data": [
        {
          "average values of\nthe trials are reported so that": ""
        },
        {
          "average values of\nthe trials are reported so that": "the initial values of parameters do not affect\nthe"
        },
        {
          "average values of\nthe trials are reported so that": ""
        },
        {
          "average values of\nthe trials are reported so that": "judgments about the models. Moreover, each trial"
        },
        {
          "average values of\nthe trials are reported so that": ""
        },
        {
          "average values of\nthe trials are reported so that": "has different seeded fixed values and ran under the"
        },
        {
          "average values of\nthe trials are reported so that": "same computer specifications in order to maintain"
        },
        {
          "average values of\nthe trials are reported so that": "reproducibility."
        },
        {
          "average values of\nthe trials are reported so that": "As in the related work, we calculate micro aver-"
        },
        {
          "average values of\nthe trials are reported so that": ""
        },
        {
          "average values of\nthe trials are reported so that": "age F1 (Micro F1) excluding the majority neutral"
        },
        {
          "average values of\nthe trials are reported so that": ""
        },
        {
          "average values of\nthe trials are reported so that": "and macro average F1 (Macro F1) on all labels for"
        },
        {
          "average values of\nthe trials are reported so that": ""
        },
        {
          "average values of\nthe trials are reported so that": "DailyDialog. On the other datasets, we calculate"
        },
        {
          "average values of\nthe trials are reported so that": ""
        },
        {
          "average values of\nthe trials are reported so that": "the weighted average f1 (W-Avg f1) and micro F1"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table 1: and γ is a",
      "data": [
        {
          "68.23\nTODKAT\n61.33\n62.60": "TUCORE-GCN\n-\n-\n65.36",
          "43.12\n64.75\n42.68\n58.47\n52.56": "-\n39.24\n-\n61.91\n-"
        },
        {
          "68.23\nTODKAT\n61.33\n62.60": "TED (Ours)\n68.63\n68.82\n66.35",
          "43.12\n64.75\n42.68\n58.47\n52.56": "67.25\n43.33\n55.71\n39.07\n62.43"
        },
        {
          "68.23\nTODKAT\n61.33\n62.60": "Table 2: Overall comparison with state-of-the-art models. TED had good overall performance and achieved the",
          "43.12\n64.75\n42.68\n58.47\n52.56": ""
        },
        {
          "68.23\nTODKAT\n61.33\n62.60": "state-of-the-art performance for IEMOCAP with a lot of turns.",
          "43.12\n64.75\n42.68\n58.47\n52.56": ""
        },
        {
          "68.23\nTODKAT\n61.33\n62.60": "and EmoryNLP, we assign new IDs to each dia-",
          "43.12\n64.75\n42.68\n58.47\n52.56": "5\nResults and Analysis"
        },
        {
          "68.23\nTODKAT\n61.33\n62.60": "logue to avoid the low frequency of appearance",
          "43.12\n64.75\n42.68\n58.47\n52.56": ""
        },
        {
          "68.23\nTODKAT\n61.33\n62.60": "",
          "43.12\n64.75\n42.68\n58.47\n52.56": "5.1\nOverall Performance"
        },
        {
          "68.23\nTODKAT\n61.33\n62.60": "due to the large number of speakers",
          "43.12\n64.75\n42.68\n58.47\n52.56": ""
        },
        {
          "68.23\nTODKAT\n61.33\n62.60": "",
          "43.12\n64.75\n42.68\n58.47\n52.56": "Table 2 compares the performance of TED and the"
        },
        {
          "68.23\nTODKAT\n61.33\n62.60": "4.2\nTraining and evaluation setting",
          "43.12\n64.75\n42.68\n58.47\n52.56": "other latest models listed in Section 2. TED had"
        },
        {
          "68.23\nTODKAT\n61.33\n62.60": "",
          "43.12\n64.75\n42.68\n58.47\n52.56": "good overall performance, while the other models"
        },
        {
          "68.23\nTODKAT\n61.33\n62.60": "We use RoBERTa-large of the Hugging Face library",
          "43.12\n64.75\n42.68\n58.47\n52.56": ""
        },
        {
          "68.23\nTODKAT\n61.33\n62.60": "",
          "43.12\n64.75\n42.68\n58.47\n52.56": "had good performance on certain datasets. More-"
        },
        {
          "68.23\nTODKAT\n61.33\n62.60": "(Wolf et al., 2020).",
          "43.12\n64.75\n42.68\n58.47\n52.56": ""
        },
        {
          "68.23\nTODKAT\n61.33\n62.60": "",
          "43.12\n64.75\n42.68\n58.47\n52.56": "over, TED had an advantage in terms of the dia-"
        },
        {
          "68.23\nTODKAT\n61.33\n62.60": "Each experiment consists of five trials and the",
          "43.12\n64.75\n42.68\n58.47\n52.56": ""
        },
        {
          "68.23\nTODKAT\n61.33\n62.60": "",
          "43.12\n64.75\n42.68\n58.47\n52.56": "logue features on datasets with a lot of turns, such"
        },
        {
          "68.23\nTODKAT\n61.33\n62.60": "average values of\nthe trials are reported so that",
          "43.12\n64.75\n42.68\n58.47\n52.56": ""
        },
        {
          "68.23\nTODKAT\n61.33\n62.60": "",
          "43.12\n64.75\n42.68\n58.47\n52.56": "as IEMOCAP."
        },
        {
          "68.23\nTODKAT\n61.33\n62.60": "the initial values of parameters do not affect\nthe",
          "43.12\n64.75\n42.68\n58.47\n52.56": ""
        },
        {
          "68.23\nTODKAT\n61.33\n62.60": "",
          "43.12\n64.75\n42.68\n58.47\n52.56": "In Formula (7),\nthe attention score vector is a"
        },
        {
          "68.23\nTODKAT\n61.33\n62.60": "judgments about the models. Moreover, each trial",
          "43.12\n64.75\n42.68\n58.47\n52.56": ""
        },
        {
          "68.23\nTODKAT\n61.33\n62.60": "",
          "43.12\n64.75\n42.68\n58.47\n52.56": "function of"
        },
        {
          "68.23\nTODKAT\n61.33\n62.60": "has different seeded fixed values and ran under the",
          "43.12\n64.75\n42.68\n58.47\n52.56": ""
        },
        {
          "68.23\nTODKAT\n61.33\n62.60": "same computer specifications in order to maintain",
          "43.12\n64.75\n42.68\n58.47\n52.56": "•\nall turns, turns that have the same speaker as"
        },
        {
          "68.23\nTODKAT\n61.33\n62.60": "reproducibility.",
          "43.12\n64.75\n42.68\n58.47\n52.56": "in the current turn, or listener turn."
        },
        {
          "68.23\nTODKAT\n61.33\n62.60": "As in the related work, we calculate micro aver-",
          "43.12\n64.75\n42.68\n58.47\n52.56": ""
        },
        {
          "68.23\nTODKAT\n61.33\n62.60": "",
          "43.12\n64.75\n42.68\n58.47\n52.56": "•\na decay factor determined by turn priority"
        },
        {
          "68.23\nTODKAT\n61.33\n62.60": "age F1 (Micro F1) excluding the majority neutral",
          "43.12\n64.75\n42.68\n58.47\n52.56": ""
        },
        {
          "68.23\nTODKAT\n61.33\n62.60": "",
          "43.12\n64.75\n42.68\n58.47\n52.56": "(No\ndecay\n(i.e., Constant)\nor\na Normal"
        },
        {
          "68.23\nTODKAT\n61.33\n62.60": "and macro average F1 (Macro F1) on all labels for",
          "43.12\n64.75\n42.68\n58.47\n52.56": ""
        },
        {
          "68.23\nTODKAT\n61.33\n62.60": "",
          "43.12\n64.75\n42.68\n58.47\n52.56": "distribution centered on a current\nturn (i.e.,"
        },
        {
          "68.23\nTODKAT\n61.33\n62.60": "DailyDialog. On the other datasets, we calculate",
          "43.12\n64.75\n42.68\n58.47\n52.56": ""
        },
        {
          "68.23\nTODKAT\n61.33\n62.60": "",
          "43.12\n64.75\n42.68\n58.47\n52.56": "N ormdist)."
        },
        {
          "68.23\nTODKAT\n61.33\n62.60": "the weighted average f1 (W-Avg f1) and micro F1",
          "43.12\n64.75\n42.68\n58.47\n52.56": ""
        },
        {
          "68.23\nTODKAT\n61.33\n62.60": "for all labels. The evaluation scores on the test data",
          "43.12\n64.75\n42.68\n58.47\n52.56": ""
        },
        {
          "68.23\nTODKAT\n61.33\n62.60": "",
          "43.12\n64.75\n42.68\n58.47\n52.56": "In terms of the context turn in CUST, we target"
        },
        {
          "68.23\nTODKAT\n61.33\n62.60": "are calculated for the model of the epoch with the",
          "43.12\n64.75\n42.68\n58.47\n52.56": ""
        },
        {
          "68.23\nTODKAT\n61.33\n62.60": "",
          "43.12\n64.75\n42.68\n58.47\n52.56": "the past only or both the past and the future. We de-"
        },
        {
          "68.23\nTODKAT\n61.33\n62.60": "highest score in the development data.",
          "43.12\n64.75\n42.68\n58.47\n52.56": ""
        },
        {
          "68.23\nTODKAT\n61.33\n62.60": "",
          "43.12\n64.75\n42.68\n58.47\n52.56": "scribe the above combination of parameters of the"
        },
        {
          "68.23\nTODKAT\n61.33\n62.60": "All experiments are subject\nto the same condi-",
          "43.12\n64.75\n42.68\n58.47\n52.56": "results in Table 2. The IEMOCAP column in Table"
        },
        {
          "68.23\nTODKAT\n61.33\n62.60": "tional schedule of the learning rate and early stop-",
          "43.12\n64.75\n42.68\n58.47\n52.56": "2 shows results for past, Listener, and Normdist;"
        },
        {
          "68.23\nTODKAT\n61.33\n62.60": "ping.\nThe learning rate is multiplied by a fixed",
          "43.12\n64.75\n42.68\n58.47\n52.56": "the MELD column those for past, Listener, and"
        },
        {
          "68.23\nTODKAT\n61.33\n62.60": "value (0.8) when the evaluation score of the devel-",
          "43.12\n64.75\n42.68\n58.47\n52.56": "Constant;\nthe EmoryNLP column those for both"
        },
        {
          "68.23\nTODKAT\n61.33\n62.60": "opment data does not\nincrease at\nthe end of\nthe",
          "43.12\n64.75\n42.68\n58.47\n52.56": "the past and the future, All\nturns, and Normdist,"
        },
        {
          "68.23\nTODKAT\n61.33\n62.60": "epoch. Early stopping forcibly terminates the learn-",
          "43.12\n64.75\n42.68\n58.47\n52.56": "and the DailyDialog those for both the past and the"
        },
        {
          "68.23\nTODKAT\n61.33\n62.60": "ing when the best score are not updated within 5",
          "43.12\n64.75\n42.68\n58.47\n52.56": "future, Listener, and Constant."
        },
        {
          "68.23\nTODKAT\n61.33\n62.60": "epochs",
          "43.12\n64.75\n42.68\n58.47\n52.56": "The results suggest\nthat TED strengthens the"
        },
        {
          "68.23\nTODKAT\n61.33\n62.60": "Regarding the model parameters, we use two",
          "43.12\n64.75\n42.68\n58.47\n52.56": "relationships between turns and accelerates the turn"
        },
        {
          "68.23\nTODKAT\n61.33\n62.60": "attention layers and four heads.\nIn Formula (8),",
          "43.12\n64.75\n42.68\n58.47\n52.56": "emphasis by specially treating the current turn."
        },
        {
          "68.23\nTODKAT\n61.33\n62.60": "σ is the standard deviation of the number of turns",
          "43.12\n64.75\n42.68\n58.47\n52.56": ""
        },
        {
          "68.23\nTODKAT\n61.33\n62.60": "",
          "43.12\n64.75\n42.68\n58.47\n52.56": "5.2\nDetailed Performance"
        },
        {
          "68.23\nTODKAT\n61.33\n62.60": "in all dialogues as shown in Table 1 and γ is a",
          "43.12\n64.75\n42.68\n58.47\n52.56": ""
        },
        {
          "68.23\nTODKAT\n61.33\n62.60": "hyperparameter for the turn priority; it was set to",
          "43.12\n64.75\n42.68\n58.47\n52.56": "Here, we examine the results of TED in more detail"
        },
        {
          "68.23\nTODKAT\n61.33\n62.60": "1.5, 2, 3, and 5.",
          "43.12\n64.75\n42.68\n58.47\n52.56": "by analyzing the effect of different parameters in"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table 5: Comparison with related methods using",
      "data": [
        {
          "IE": "",
          "ME": "",
          "EN": "",
          "DD": "",
          "Context": "",
          "IEMOCAP": "",
          "MELD": ""
        },
        {
          "IE": "W-Avg",
          "ME": "W-Avg",
          "EN": "W-Avg",
          "DD": "Micro",
          "Context": "turn",
          "IEMOCAP": "W-Avg F1",
          "MELD": "W-Avg F1"
        },
        {
          "IE": "",
          "ME": "",
          "EN": "",
          "DD": "",
          "Context": "",
          "IEMOCAP": "",
          "MELD": ""
        },
        {
          "IE": "F1",
          "ME": "F1",
          "EN": "F1",
          "DD": "F1",
          "Context": "",
          "IEMOCAP": "",
          "MELD": ""
        },
        {
          "IE": "",
          "ME": "",
          "EN": "",
          "DD": "",
          "Context": "",
          "IEMOCAP": "68.63",
          "MELD": "66.35"
        },
        {
          "IE": "68.50",
          "ME": "65.87",
          "EN": "38.09",
          "DD": "61.52",
          "Context": "",
          "IEMOCAP": "",
          "MELD": ""
        },
        {
          "IE": "",
          "ME": "",
          "EN": "",
          "DD": "",
          "Context": "",
          "IEMOCAP": "66.70",
          "MELD": "65.66"
        },
        {
          "IE": "68.56",
          "ME": "66.05",
          "EN": "38.60",
          "DD": "61.09",
          "Context": "",
          "IEMOCAP": "",
          "MELD": ""
        },
        {
          "IE": "",
          "ME": "",
          "EN": "",
          "DD": "",
          "Context": "",
          "IEMOCAP": "68.50",
          "MELD": "65.87"
        },
        {
          "IE": "",
          "ME": "",
          "EN": "",
          "DD": "",
          "Context": "past",
          "IEMOCAP": "",
          "MELD": ""
        },
        {
          "IE": "68.48",
          "ME": "65.94",
          "EN": "38.78",
          "DD": "61.20",
          "Context": "",
          "IEMOCAP": "",
          "MELD": ""
        },
        {
          "IE": "",
          "ME": "",
          "EN": "",
          "DD": "",
          "Context": "",
          "IEMOCAP": "67.44",
          "MELD": "65.50"
        },
        {
          "IE": "",
          "ME": "",
          "EN": "",
          "DD": "",
          "Context": "",
          "IEMOCAP": "",
          "MELD": ""
        },
        {
          "IE": "68.59",
          "ME": "66.25",
          "EN": "38.56",
          "DD": "61.10",
          "Context": "",
          "IEMOCAP": "",
          "MELD": ""
        },
        {
          "IE": "",
          "ME": "",
          "EN": "",
          "DD": "",
          "Context": "",
          "IEMOCAP": "68.40",
          "MELD": "65.51"
        },
        {
          "IE": "68.12",
          "ME": "66.35",
          "EN": "38.64",
          "DD": "61.08",
          "Context": "",
          "IEMOCAP": "",
          "MELD": ""
        },
        {
          "IE": "",
          "ME": "",
          "EN": "",
          "DD": "",
          "Context": "",
          "IEMOCAP": "68.40",
          "MELD": "65.85"
        },
        {
          "IE": "68.63",
          "ME": "66.10",
          "EN": "38.59",
          "DD": "61.18",
          "Context": "",
          "IEMOCAP": "",
          "MELD": ""
        },
        {
          "IE": "",
          "ME": "",
          "EN": "",
          "DD": "",
          "Context": "",
          "IEMOCAP": "68.13",
          "MELD": "66.11"
        },
        {
          "IE": "67.62",
          "ME": "65.60",
          "EN": "38.93",
          "DD": "61.99",
          "Context": "",
          "IEMOCAP": "",
          "MELD": ""
        },
        {
          "IE": "",
          "ME": "",
          "EN": "",
          "DD": "",
          "Context": "",
          "IEMOCAP": "66.62",
          "MELD": "65.52"
        },
        {
          "IE": "68.13",
          "ME": "65.96",
          "EN": "39.07",
          "DD": "62.16",
          "Context": "past",
          "IEMOCAP": "",
          "MELD": ""
        },
        {
          "IE": "",
          "ME": "",
          "EN": "",
          "DD": "",
          "Context": "",
          "IEMOCAP": "67.62",
          "MELD": "65.69"
        },
        {
          "IE": "",
          "ME": "",
          "EN": "",
          "DD": "",
          "Context": "+",
          "IEMOCAP": "",
          "MELD": ""
        },
        {
          "IE": "67.77",
          "ME": "66.11",
          "EN": "38.96",
          "DD": "62.39",
          "Context": "",
          "IEMOCAP": "",
          "MELD": ""
        },
        {
          "IE": "",
          "ME": "",
          "EN": "",
          "DD": "",
          "Context": "",
          "IEMOCAP": "66.67",
          "MELD": "65.25"
        },
        {
          "IE": "",
          "ME": "",
          "EN": "",
          "DD": "",
          "Context": "future",
          "IEMOCAP": "",
          "MELD": ""
        },
        {
          "IE": "68.02",
          "ME": "65.89",
          "EN": "38.64",
          "DD": "62.20",
          "Context": "",
          "IEMOCAP": "",
          "MELD": ""
        },
        {
          "IE": "",
          "ME": "",
          "EN": "",
          "DD": "",
          "Context": "",
          "IEMOCAP": "67.58",
          "MELD": "66.18"
        },
        {
          "IE": "68.02",
          "ME": "65.86",
          "EN": "38.87",
          "DD": "62.43",
          "Context": "",
          "IEMOCAP": "",
          "MELD": ""
        },
        {
          "IE": "",
          "ME": "",
          "EN": "",
          "DD": "",
          "Context": "",
          "IEMOCAP": "67.43",
          "MELD": "65.57"
        },
        {
          "IE": "68.13",
          "ME": "66.03",
          "EN": "38.89",
          "DD": "62.20",
          "Context": "",
          "IEMOCAP": "",
          "MELD": ""
        },
        {
          "IE": "",
          "ME": "",
          "EN": "",
          "DD": "",
          "Context": "Table\n5:",
          "IEMOCAP": "related methods",
          "MELD": "using"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table 5: Comparison with related methods using",
      "data": [
        {
          "the past and future for EmoryNLP.": ""
        },
        {
          "the past and future for EmoryNLP.": ""
        },
        {
          "the past and future for EmoryNLP.": "The dialogue feature attention provided a slight"
        },
        {
          "the past and future for EmoryNLP.": "improvement compared with that of TBM, which"
        },
        {
          "the past and future for EmoryNLP.": "does not use the dialogue features. Varying the"
        },
        {
          "the past and future for EmoryNLP.": ""
        },
        {
          "the past and future for EmoryNLP.": "combinations of features yielded no significant dif-"
        },
        {
          "the past and future for EmoryNLP.": ""
        },
        {
          "the past and future for EmoryNLP.": ""
        },
        {
          "the past and future for EmoryNLP.": "ferences in the results."
        },
        {
          "the past and future for EmoryNLP.": ""
        },
        {
          "the past and future for EmoryNLP.": ""
        },
        {
          "the past and future for EmoryNLP.": ""
        },
        {
          "the past and future for EmoryNLP.": ""
        },
        {
          "the past and future for EmoryNLP.": "5.3\nEffect of Input Encoding and Mean"
        },
        {
          "the past and future for EmoryNLP.": ""
        },
        {
          "the past and future for EmoryNLP.": "Vector of Utterance (TBE)"
        },
        {
          "the past and future for EmoryNLP.": ""
        },
        {
          "the past and future for EmoryNLP.": ""
        },
        {
          "the past and future for EmoryNLP.": ""
        },
        {
          "the past and future for EmoryNLP.": "Table 4 compares the TBE models that have only"
        },
        {
          "the past and future for EmoryNLP.": ""
        },
        {
          "the past and future for EmoryNLP.": ""
        },
        {
          "the past and future for EmoryNLP.": "input encoding part or both the input encoding part"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table 5: Comparison with related methods using",
      "data": [
        {
          "+ Speaker token\n66.67": "future",
          "65.25": ""
        },
        {
          "+ Speaker token\n66.67": "",
          "65.25": ""
        },
        {
          "+ Speaker token\n66.67": "+ Speaker attention\n67.58",
          "65.25": "66.18"
        },
        {
          "+ Speaker token\n66.67": "",
          "65.25": ""
        },
        {
          "+ Speaker token\n66.67": "+ Listener attention\n67.43",
          "65.25": "65.57"
        },
        {
          "+ Speaker token\n66.67": "",
          "65.25": ""
        },
        {
          "+ Speaker token\n66.67": "Table\n5:\nComparison with\nrelated methods",
          "65.25": "using"
        },
        {
          "+ Speaker token\n66.67": "speaker",
          "65.25": "information. Neither adding token (Speaker"
        },
        {
          "+ Speaker token\n66.67": "token) nor limiting the attentional",
          "65.25": "turns (Speaker and"
        },
        {
          "+ Speaker token\n66.67": "Listener attention) has an advantage.",
          "65.25": ""
        },
        {
          "+ Speaker token\n66.67": "",
          "65.25": ""
        },
        {
          "+ Speaker token\n66.67": "",
          "65.25": ""
        },
        {
          "+ Speaker token\n66.67": "the past and future for EmoryNLP.",
          "65.25": ""
        },
        {
          "+ Speaker token\n66.67": "",
          "65.25": ""
        },
        {
          "+ Speaker token\n66.67": "",
          "65.25": ""
        },
        {
          "+ Speaker token\n66.67": "",
          "65.25": "The dialogue feature attention provided a slight"
        },
        {
          "+ Speaker token\n66.67": "improvement compared with that of TBM, which",
          "65.25": ""
        },
        {
          "+ Speaker token\n66.67": "does not use the dialogue features. Varying the",
          "65.25": ""
        },
        {
          "+ Speaker token\n66.67": "",
          "65.25": ""
        },
        {
          "+ Speaker token\n66.67": "",
          "65.25": "combinations of features yielded no significant dif-"
        },
        {
          "+ Speaker token\n66.67": "",
          "65.25": ""
        },
        {
          "+ Speaker token\n66.67": "",
          "65.25": ""
        },
        {
          "+ Speaker token\n66.67": "ferences in the results.",
          "65.25": ""
        },
        {
          "+ Speaker token\n66.67": "",
          "65.25": ""
        },
        {
          "+ Speaker token\n66.67": "",
          "65.25": ""
        },
        {
          "+ Speaker token\n66.67": "",
          "65.25": ""
        },
        {
          "+ Speaker token\n66.67": "",
          "65.25": ""
        },
        {
          "+ Speaker token\n66.67": "5.3\nEffect of Input Encoding and Mean",
          "65.25": ""
        },
        {
          "+ Speaker token\n66.67": "",
          "65.25": ""
        },
        {
          "+ Speaker token\n66.67": "Vector of Utterance (TBE)",
          "65.25": ""
        },
        {
          "+ Speaker token\n66.67": "",
          "65.25": ""
        },
        {
          "+ Speaker token\n66.67": "",
          "65.25": ""
        },
        {
          "+ Speaker token\n66.67": "",
          "65.25": ""
        },
        {
          "+ Speaker token\n66.67": "Table 4 compares the TBE models that have only",
          "65.25": ""
        },
        {
          "+ Speaker token\n66.67": "",
          "65.25": ""
        },
        {
          "+ Speaker token\n66.67": "",
          "65.25": ""
        },
        {
          "+ Speaker token\n66.67": "input encoding part or both the input encoding part",
          "65.25": ""
        },
        {
          "+ Speaker token\n66.67": "and mean vector of utterance part with a baseline",
          "65.25": ""
        },
        {
          "+ Speaker token\n66.67": "",
          "65.25": ""
        },
        {
          "+ Speaker token\n66.67": "",
          "65.25": "that does not have either part. The “Context turn“"
        },
        {
          "+ Speaker token\n66.67": "",
          "65.25": ""
        },
        {
          "+ Speaker token\n66.67": "",
          "65.25": "column indicates whether to include past, future,"
        },
        {
          "+ Speaker token\n66.67": "or both kinds of turn in CUST; “no use“ indicates",
          "65.25": ""
        },
        {
          "+ Speaker token\n66.67": "only the current turn for the baseline.",
          "65.25": ""
        },
        {
          "+ Speaker token\n66.67": "",
          "65.25": ""
        },
        {
          "+ Speaker token\n66.67": "The results show the effectiveness of",
          "65.25": "the past"
        },
        {
          "+ Speaker token\n66.67": "",
          "65.25": ""
        },
        {
          "+ Speaker token\n66.67": "turns and TBE. The future turns do not contribute",
          "65.25": ""
        },
        {
          "+ Speaker token\n66.67": "",
          "65.25": ""
        },
        {
          "+ Speaker token\n66.67": "to a performance improvement. We consider that",
          "65.25": ""
        },
        {
          "+ Speaker token\n66.67": "",
          "65.25": ""
        },
        {
          "+ Speaker token\n66.67": "the current state is influenced by the past states in a",
          "65.25": ""
        },
        {
          "+ Speaker token\n66.67": "",
          "65.25": ""
        },
        {
          "+ Speaker token\n66.67": "",
          "65.25": "chain reaction, while the current state is not directly"
        },
        {
          "+ Speaker token\n66.67": "",
          "65.25": ""
        },
        {
          "+ Speaker token\n66.67": "affected by the future states.",
          "65.25": ""
        },
        {
          "+ Speaker token\n66.67": "",
          "65.25": ""
        },
        {
          "+ Speaker token\n66.67": "",
          "65.25": "The performance on IEMOCAP is significantly"
        },
        {
          "+ Speaker token\n66.67": "",
          "65.25": ""
        },
        {
          "+ Speaker token\n66.67": "improved in the case of past\nturns.",
          "65.25": "The perfor-"
        },
        {
          "+ Speaker token\n66.67": "",
          "65.25": ""
        },
        {
          "+ Speaker token\n66.67": "mance of EmoBERTa shows the same tendency as",
          "65.25": ""
        },
        {
          "+ Speaker token\n66.67": "",
          "65.25": ""
        },
        {
          "+ Speaker token\n66.67": "TBE on IEMOCAP.",
          "65.25": ""
        },
        {
          "+ Speaker token\n66.67": "",
          "65.25": ""
        },
        {
          "+ Speaker token\n66.67": "",
          "65.25": "These results suggest that the turn-based vector"
        },
        {
          "+ Speaker token\n66.67": "of TBE is effective.",
          "65.25": ""
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "IEMOCAP\nMELD\nTransformer\n# layers": "Structure\n, heads\nW-Avg F1\nW-Avg F1"
        },
        {
          "IEMOCAP\nMELD\nTransformer\n# layers": "68.50\n65.87\n2, 4"
        },
        {
          "IEMOCAP\nMELD\nTransformer\n# layers": "TBM"
        },
        {
          "IEMOCAP\nMELD\nTransformer\n# layers": "4, 8\n67.92\n65.37"
        },
        {
          "IEMOCAP\nMELD\nTransformer\n# layers": "(MHSA)"
        },
        {
          "IEMOCAP\nMELD\nTransformer\n# layers": "6, 16\n67.63\n65.54"
        },
        {
          "IEMOCAP\nMELD\nTransformer\n# layers": "2, 4\n67.97\n65.86"
        },
        {
          "IEMOCAP\nMELD\nTransformer\n# layers": "+ Positional"
        },
        {
          "IEMOCAP\nMELD\nTransformer\n# layers": "4, 8\n67.64\n65.27"
        },
        {
          "IEMOCAP\nMELD\nTransformer\n# layers": "Encoding"
        },
        {
          "IEMOCAP\nMELD\nTransformer\n# layers": "6, 16\n66.93\n65.50"
        },
        {
          "IEMOCAP\nMELD\nTransformer\n# layers": ""
        },
        {
          "IEMOCAP\nMELD\nTransformer\n# layers": "2, 4\n67.64\n65.49"
        },
        {
          "IEMOCAP\nMELD\nTransformer\n# layers": "+ Feed Forward"
        },
        {
          "IEMOCAP\nMELD\nTransformer\n# layers": "4, 8\n67.47\n65.54"
        },
        {
          "IEMOCAP\nMELD\nTransformer\n# layers": "Network"
        },
        {
          "IEMOCAP\nMELD\nTransformer\n# layers": ""
        },
        {
          "IEMOCAP\nMELD\nTransformer\n# layers": "6, 16\n67.66\n65.74"
        },
        {
          "IEMOCAP\nMELD\nTransformer\n# layers": "Table 6: Comparison of TBM model and TBM with the"
        },
        {
          "IEMOCAP\nMELD\nTransformer\n# layers": ""
        },
        {
          "IEMOCAP\nMELD\nTransformer\n# layers": "additional Transformer components. Adding PE and"
        },
        {
          "IEMOCAP\nMELD\nTransformer\n# layers": ""
        },
        {
          "IEMOCAP\nMELD\nTransformer\n# layers": "FFN did not give any improvements. This means that"
        },
        {
          "IEMOCAP\nMELD\nTransformer\n# layers": "MHSA contributed to the high performance of TED."
        },
        {
          "IEMOCAP\nMELD\nTransformer\n# layers": ""
        },
        {
          "IEMOCAP\nMELD\nTransformer\n# layers": ""
        },
        {
          "IEMOCAP\nMELD\nTransformer\n# layers": ""
        },
        {
          "IEMOCAP\nMELD\nTransformer\n# layers": ""
        },
        {
          "IEMOCAP\nMELD\nTransformer\n# layers": "models use positional encoding (PE) and a feed"
        },
        {
          "IEMOCAP\nMELD\nTransformer\n# layers": ""
        },
        {
          "IEMOCAP\nMELD\nTransformer\n# layers": "forward network (FFN), we decided to examine the"
        },
        {
          "IEMOCAP\nMELD\nTransformer\n# layers": ""
        },
        {
          "IEMOCAP\nMELD\nTransformer\n# layers": "effect of adding them to TBM in the same way as"
        },
        {
          "IEMOCAP\nMELD\nTransformer\n# layers": ""
        },
        {
          "IEMOCAP\nMELD\nTransformer\n# layers": "in the vanilla Transformer as shown in Figure 6."
        },
        {
          "IEMOCAP\nMELD\nTransformer\n# layers": ""
        },
        {
          "IEMOCAP\nMELD\nTransformer\n# layers": ""
        },
        {
          "IEMOCAP\nMELD\nTransformer\n# layers": "Table 6 shows the effect of adding these Trans-"
        },
        {
          "IEMOCAP\nMELD\nTransformer\n# layers": ""
        },
        {
          "IEMOCAP\nMELD\nTransformer\n# layers": "former components and the network size depend-"
        },
        {
          "IEMOCAP\nMELD\nTransformer\n# layers": ""
        },
        {
          "IEMOCAP\nMELD\nTransformer\n# layers": "ing on the number of layers and heads. The “TBM“"
        },
        {
          "IEMOCAP\nMELD\nTransformer\n# layers": ""
        },
        {
          "IEMOCAP\nMELD\nTransformer\n# layers": "row contains results for only MHSA, while the “Po-"
        },
        {
          "IEMOCAP\nMELD\nTransformer\n# layers": ""
        },
        {
          "IEMOCAP\nMELD\nTransformer\n# layers": "sitional Encoding“ and “Feed Forward Networks“"
        },
        {
          "IEMOCAP\nMELD\nTransformer\n# layers": ""
        },
        {
          "IEMOCAP\nMELD\nTransformer\n# layers": "rows show results for TBM with these layers. The"
        },
        {
          "IEMOCAP\nMELD\nTransformer\n# layers": ""
        },
        {
          "IEMOCAP\nMELD\nTransformer\n# layers": "“# layers“ and “# heads“ columns respectively in-"
        },
        {
          "IEMOCAP\nMELD\nTransformer\n# layers": ""
        },
        {
          "IEMOCAP\nMELD\nTransformer\n# layers": "dicate the number of layers of the above compo-"
        },
        {
          "IEMOCAP\nMELD\nTransformer\n# layers": ""
        },
        {
          "IEMOCAP\nMELD\nTransformer\n# layers": "nents and the number of heads of MHSA. Note that"
        },
        {
          "IEMOCAP\nMELD\nTransformer\n# layers": ""
        },
        {
          "IEMOCAP\nMELD\nTransformer\n# layers": "CUST used only the past turns."
        },
        {
          "IEMOCAP\nMELD\nTransformer\n# layers": ""
        },
        {
          "IEMOCAP\nMELD\nTransformer\n# layers": ""
        },
        {
          "IEMOCAP\nMELD\nTransformer\n# layers": "The results show that\nthere is no advantage to"
        },
        {
          "IEMOCAP\nMELD\nTransformer\n# layers": ""
        },
        {
          "IEMOCAP\nMELD\nTransformer\n# layers": "adding PE and FFN, which are token-based compo-"
        },
        {
          "IEMOCAP\nMELD\nTransformer\n# layers": ""
        },
        {
          "IEMOCAP\nMELD\nTransformer\n# layers": "nents. Therefore, the results show that TBM works"
        },
        {
          "IEMOCAP\nMELD\nTransformer\n# layers": ""
        },
        {
          "IEMOCAP\nMELD\nTransformer\n# layers": "better with turn-based components than with turn-"
        },
        {
          "IEMOCAP\nMELD\nTransformer\n# layers": ""
        },
        {
          "IEMOCAP\nMELD\nTransformer\n# layers": "based and token-based components together. Re-"
        },
        {
          "IEMOCAP\nMELD\nTransformer\n# layers": ""
        },
        {
          "IEMOCAP\nMELD\nTransformer\n# layers": "garding the network parameters, smaller layers and"
        },
        {
          "IEMOCAP\nMELD\nTransformer\n# layers": ""
        },
        {
          "IEMOCAP\nMELD\nTransformer\n# layers": "heads are effective. In other words, the complexity"
        },
        {
          "IEMOCAP\nMELD\nTransformer\n# layers": ""
        },
        {
          "IEMOCAP\nMELD\nTransformer\n# layers": "of the parameters had no effect on the performance"
        },
        {
          "IEMOCAP\nMELD\nTransformer\n# layers": ""
        },
        {
          "IEMOCAP\nMELD\nTransformer\n# layers": "of the turn-based mechanism."
        },
        {
          "IEMOCAP\nMELD\nTransformer\n# layers": ""
        },
        {
          "IEMOCAP\nMELD\nTransformer\n# layers": "The improvement on IEMOCAP was remark-"
        },
        {
          "IEMOCAP\nMELD\nTransformer\n# layers": "able, but\nthat on MELD was slight.\nRegarding"
        },
        {
          "IEMOCAP\nMELD\nTransformer\n# layers": "this result, more turns would lead to more con-"
        },
        {
          "IEMOCAP\nMELD\nTransformer\n# layers": "text and higher performance. We conclude that,"
        },
        {
          "IEMOCAP\nMELD\nTransformer\n# layers": "compared with the state-of-the-art methods, TBM,"
        },
        {
          "IEMOCAP\nMELD\nTransformer\n# layers": ""
        },
        {
          "IEMOCAP\nMELD\nTransformer\n# layers": "which uses turn-based MHSA between turn-based"
        },
        {
          "IEMOCAP\nMELD\nTransformer\n# layers": "vectors based on TBE, captures more complicated"
        },
        {
          "IEMOCAP\nMELD\nTransformer\n# layers": "contexts in the multiple turns."
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "6\nConclusion": "",
          "aware BERT for multi-turn response selection in": "retrieval-based chatbots. CoRR, abs/2004.03588."
        },
        {
          "6\nConclusion": "For modeling multi-turn contexts of conversation,",
          "aware BERT for multi-turn response selection in": ""
        },
        {
          "6\nConclusion": "",
          "aware BERT for multi-turn response selection in": "Devamanyu Hazarika, Soujanya Poria, Rada Mihal-"
        },
        {
          "6\nConclusion": "we presented TED, which emphasizes\nthe\ncur-",
          "aware BERT for multi-turn response selection in": ""
        },
        {
          "6\nConclusion": "",
          "aware BERT for multi-turn response selection in": "cea, Erik Cambria, and Roger Zimmermann. 2018a."
        },
        {
          "6\nConclusion": "rent\nturn and explicitly distinguishes each turn",
          "aware BERT for multi-turn response selection in": ""
        },
        {
          "6\nConclusion": "",
          "aware BERT for multi-turn response selection in": "ICON: Interactive conversational memory network"
        },
        {
          "6\nConclusion": "by adding the dialogue features into the attention",
          "aware BERT for multi-turn response selection in": ""
        },
        {
          "6\nConclusion": "",
          "aware BERT for multi-turn response selection in": "for multimodal emotion detection.\nIn Proceedings of"
        },
        {
          "6\nConclusion": "mechanism. The results in four ERC datasets show",
          "aware BERT for multi-turn response selection in": "the 2018 Conference on Empirical Methods in Nat-"
        },
        {
          "6\nConclusion": "",
          "aware BERT for multi-turn response selection in": "ural Language Processing, pages 2594–2604, Brus-"
        },
        {
          "6\nConclusion": "that TED had good overall performance, while",
          "aware BERT for multi-turn response selection in": ""
        },
        {
          "6\nConclusion": "",
          "aware BERT for multi-turn response selection in": "sels, Belgium. Association for Computational Lin-"
        },
        {
          "6\nConclusion": "the other models had good performance on cer-",
          "aware BERT for multi-turn response selection in": ""
        },
        {
          "6\nConclusion": "",
          "aware BERT for multi-turn response selection in": "guistics."
        },
        {
          "6\nConclusion": "tain datasets. Moreover, TED had an advantage in",
          "aware BERT for multi-turn response selection in": ""
        },
        {
          "6\nConclusion": "terms of the dialogue features on datasets with a lot",
          "aware BERT for multi-turn response selection in": "Devamanyu Hazarika, Soujanya Poria, Amir Zadeh,"
        },
        {
          "6\nConclusion": "",
          "aware BERT for multi-turn response selection in": "Erik Cambria, Louis-Philippe Morency, and Roger"
        },
        {
          "6\nConclusion": "of turns, such as IEMOCAP. Further experiments",
          "aware BERT for multi-turn response selection in": ""
        },
        {
          "6\nConclusion": "",
          "aware BERT for multi-turn response selection in": "Zimmermann. 2018b. Conversational memory net-"
        },
        {
          "6\nConclusion": "demonstrated the effectiveness of TED’s key com-",
          "aware BERT for multi-turn response selection in": ""
        },
        {
          "6\nConclusion": "",
          "aware BERT for multi-turn response selection in": "work for emotion recognition in dyadic dialogue"
        },
        {
          "6\nConclusion": "ponents; TBE, TBM, and the dialogue features of",
          "aware BERT for multi-turn response selection in": ""
        },
        {
          "6\nConclusion": "",
          "aware BERT for multi-turn response selection in": "the 2018 Conference of\nvideos.\nIn Proceedings of"
        },
        {
          "6\nConclusion": "the turn position and speaker information. Our pri-",
          "aware BERT for multi-turn response selection in": "the North American Chapter of the Association for"
        },
        {
          "6\nConclusion": "",
          "aware BERT for multi-turn response selection in": "Computational Linguistics: Human Language Tech-"
        },
        {
          "6\nConclusion": "ority factor for the distinction of the turns has made",
          "aware BERT for multi-turn response selection in": ""
        },
        {
          "6\nConclusion": "",
          "aware BERT for multi-turn response selection in": "nologies, Volume 1 (Long Papers), pages 2122–2132,"
        },
        {
          "6\nConclusion": "it possible to emphasize the emotional target turn",
          "aware BERT for multi-turn response selection in": ""
        },
        {
          "6\nConclusion": "",
          "aware BERT for multi-turn response selection in": "New Orleans, Louisiana. Association for Computa-"
        },
        {
          "6\nConclusion": "and can adapt to diverse datasets by controlling the",
          "aware BERT for multi-turn response selection in": ""
        },
        {
          "6\nConclusion": "",
          "aware BERT for multi-turn response selection in": "tional Linguistics."
        },
        {
          "6\nConclusion": "dialogue features.",
          "aware BERT for multi-turn response selection in": ""
        },
        {
          "6\nConclusion": "",
          "aware BERT for multi-turn response selection in": "Dou Hu, Lingwei Wei, and Xiaoyong Huai. 2021. Dia-"
        },
        {
          "6\nConclusion": "",
          "aware BERT for multi-turn response selection in": "loguecrn: Contextual reasoning networks for emotion"
        },
        {
          "6\nConclusion": "",
          "aware BERT for multi-turn response selection in": "recognition in conversations. CoRR, abs/2106.01978."
        },
        {
          "6\nConclusion": "References",
          "aware BERT for multi-turn response selection in": ""
        },
        {
          "6\nConclusion": "Carlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe",
          "aware BERT for multi-turn response selection in": "Taichi\nIshiwatari, Yuki Yasuda, Taro Miyazaki, and"
        },
        {
          "6\nConclusion": "Kazemzadeh, Emily Mower, Samuel Kim,\nJean-",
          "aware BERT for multi-turn response selection in": "Jun Goto. 2020. Relation-aware graph attention net-"
        },
        {
          "6\nConclusion": "nette N Chang,\nSungbok Lee,\nand Shrikanth S",
          "aware BERT for multi-turn response selection in": "works with relational position encodings for emotion"
        },
        {
          "6\nConclusion": "Narayanan. 2008.\nIemocap:\nInteractive emotional",
          "aware BERT for multi-turn response selection in": "recognition in conversations.\nIn Proceedings of the"
        },
        {
          "6\nConclusion": "dyadic motion capture database. Language resources",
          "aware BERT for multi-turn response selection in": "2020 Conference on Empirical Methods in Natural"
        },
        {
          "6\nConclusion": "and evaluation, 42(4):335.",
          "aware BERT for multi-turn response selection in": "Language Processing (EMNLP), pages 7360–7370,"
        },
        {
          "6\nConclusion": "",
          "aware BERT for multi-turn response selection in": "Online. Association for Computational Linguistics."
        },
        {
          "6\nConclusion": "Jacob Devlin, Ming-Wei Chang, Kenton Lee,\nand",
          "aware BERT for multi-turn response selection in": ""
        },
        {
          "6\nConclusion": "Kristina Toutanova. 2019.\nBERT: Pre-training of",
          "aware BERT for multi-turn response selection in": "Wenxiang Jiao, Michael Lyu, and Irwin King. 2020."
        },
        {
          "6\nConclusion": "deep bidirectional transformers for language under-",
          "aware BERT for multi-turn response selection in": "Real-time emotion recognition via attention gated"
        },
        {
          "6\nConclusion": "standing.\nIn Proceedings of the 2019 Conference of",
          "aware BERT for multi-turn response selection in": "Proceedings\nof\nhierarchical memory\nnetwork."
        },
        {
          "6\nConclusion": "the North American Chapter of the Association for",
          "aware BERT for multi-turn response selection in": "the AAAI Conference\non Artificial\nIntelligence,"
        },
        {
          "6\nConclusion": "Computational Linguistics: Human Language Tech-",
          "aware BERT for multi-turn response selection in": "34(05):8002–8009."
        },
        {
          "6\nConclusion": "nologies, Volume 1 (Long and Short Papers), pages",
          "aware BERT for multi-turn response selection in": ""
        },
        {
          "6\nConclusion": "",
          "aware BERT for multi-turn response selection in": "Wenxiang\nJiao,\nHaiqin Yang,\nIrwin King,\nand"
        },
        {
          "6\nConclusion": "4171–4186, Minneapolis, Minnesota. Association for",
          "aware BERT for multi-turn response selection in": ""
        },
        {
          "6\nConclusion": "",
          "aware BERT for multi-turn response selection in": "Michael R. Lyu. 2019. HiGRU: Hierarchical gated re-"
        },
        {
          "6\nConclusion": "Computational Linguistics.",
          "aware BERT for multi-turn response selection in": ""
        },
        {
          "6\nConclusion": "",
          "aware BERT for multi-turn response selection in": "current units for utterance-level emotion recognition."
        },
        {
          "6\nConclusion": "Deepanway Ghosal, Navonil Majumder, Alexander Gel-",
          "aware BERT for multi-turn response selection in": "In Proceedings of the 2019 Conference of the North"
        },
        {
          "6\nConclusion": "bukh, Rada Mihalcea,\nand Soujanya Poria. 2020.",
          "aware BERT for multi-turn response selection in": "American Chapter of the Association for Computa-"
        },
        {
          "6\nConclusion": "COSMIC: COmmonSense knowledge for eMotion",
          "aware BERT for multi-turn response selection in": "tional Linguistics: Human Language Technologies,"
        },
        {
          "6\nConclusion": "identification in conversations.\nIn Findings of the As-",
          "aware BERT for multi-turn response selection in": "Volume 1 (Long and Short Papers), pages 397–406,"
        },
        {
          "6\nConclusion": "sociation for Computational Linguistics: EMNLP",
          "aware BERT for multi-turn response selection in": "Minneapolis, Minnesota. Association for Computa-"
        },
        {
          "6\nConclusion": "2020, pages 2470–2481, Online. Association for",
          "aware BERT for multi-turn response selection in": "tional Linguistics."
        },
        {
          "6\nConclusion": "Computational Linguistics.",
          "aware BERT for multi-turn response selection in": ""
        },
        {
          "6\nConclusion": "",
          "aware BERT for multi-turn response selection in": "Taewoon Kim and Piek Vossen. 2021.\nEmoberta:"
        },
        {
          "6\nConclusion": "Deepanway Ghosal, Navonil Majumder, Soujanya Poria,",
          "aware BERT for multi-turn response selection in": "Speaker-aware emotion recognition in conversation"
        },
        {
          "6\nConclusion": "Niyati Chhaya, and Alexander Gelbukh. 2019. Di-",
          "aware BERT for multi-turn response selection in": "with roberta. CoRR, abs/2108.12009."
        },
        {
          "6\nConclusion": "alogueGCN: A graph convolutional neural network",
          "aware BERT for multi-turn response selection in": ""
        },
        {
          "6\nConclusion": "for emotion recognition in conversation.\nIn Proceed-",
          "aware BERT for multi-turn response selection in": "Bongseok Lee and Yong Suk Choi. 2021. Graph based"
        },
        {
          "6\nConclusion": "ings of the 2019 Conference on Empirical Methods",
          "aware BERT for multi-turn response selection in": "network with contextualized representations of turns"
        },
        {
          "6\nConclusion": "in Natural Language Processing and the 9th Inter-",
          "aware BERT for multi-turn response selection in": "in dialogue. CoRR, abs/2109.04008."
        },
        {
          "6\nConclusion": "national Joint Conference on Natural Language Pro-",
          "aware BERT for multi-turn response selection in": ""
        },
        {
          "6\nConclusion": "",
          "aware BERT for multi-turn response selection in": "Jingye Li, Donghong Ji, Fei Li, Meishan Zhang, and"
        },
        {
          "6\nConclusion": "cessing (EMNLP-IJCNLP), pages 154–164, Hong",
          "aware BERT for multi-turn response selection in": ""
        },
        {
          "6\nConclusion": "",
          "aware BERT for multi-turn response selection in": "Yijiang Liu. 2020a. HiTrans: A transformer-based"
        },
        {
          "6\nConclusion": "Kong, China. Association for Computational Lin-",
          "aware BERT for multi-turn response selection in": ""
        },
        {
          "6\nConclusion": "",
          "aware BERT for multi-turn response selection in": "context- and speaker-sensitive model for emotion de-"
        },
        {
          "6\nConclusion": "guistics.",
          "aware BERT for multi-turn response selection in": ""
        },
        {
          "6\nConclusion": "",
          "aware BERT for multi-turn response selection in": "tection in conversations.\nIn Proceedings of the 28th"
        },
        {
          "6\nConclusion": "Jia-Chen Gu, Tianda Li, Quan Liu, Xiaodan Zhu, Zhen-",
          "aware BERT for multi-turn response selection in": "International Conference on Computational Linguis-"
        },
        {
          "6\nConclusion": "Hua Ling, Zhiming Su, and Si Wei. 2020. Speaker-",
          "aware BERT for multi-turn response selection in": "tics, pages 4190–4200, Barcelona, Spain (Online)."
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Dou Hu, Lingwei Wei, and Xiaoyong Huai. 2021. Dia-": "loguecrn: Contextual reasoning networks for emotion"
        },
        {
          "Dou Hu, Lingwei Wei, and Xiaoyong Huai. 2021. Dia-": "recognition in conversations. CoRR, abs/2106.01978."
        },
        {
          "Dou Hu, Lingwei Wei, and Xiaoyong Huai. 2021. Dia-": ""
        },
        {
          "Dou Hu, Lingwei Wei, and Xiaoyong Huai. 2021. Dia-": "Taichi\nIshiwatari, Yuki Yasuda, Taro Miyazaki, and"
        },
        {
          "Dou Hu, Lingwei Wei, and Xiaoyong Huai. 2021. Dia-": "Jun Goto. 2020. Relation-aware graph attention net-"
        },
        {
          "Dou Hu, Lingwei Wei, and Xiaoyong Huai. 2021. Dia-": "works with relational position encodings for emotion"
        },
        {
          "Dou Hu, Lingwei Wei, and Xiaoyong Huai. 2021. Dia-": "recognition in conversations.\nIn Proceedings of the"
        },
        {
          "Dou Hu, Lingwei Wei, and Xiaoyong Huai. 2021. Dia-": "2020 Conference on Empirical Methods in Natural"
        },
        {
          "Dou Hu, Lingwei Wei, and Xiaoyong Huai. 2021. Dia-": "Language Processing (EMNLP), pages 7360–7370,"
        },
        {
          "Dou Hu, Lingwei Wei, and Xiaoyong Huai. 2021. Dia-": "Online. Association for Computational Linguistics."
        },
        {
          "Dou Hu, Lingwei Wei, and Xiaoyong Huai. 2021. Dia-": ""
        },
        {
          "Dou Hu, Lingwei Wei, and Xiaoyong Huai. 2021. Dia-": "Wenxiang Jiao, Michael Lyu, and Irwin King. 2020."
        },
        {
          "Dou Hu, Lingwei Wei, and Xiaoyong Huai. 2021. Dia-": "Real-time emotion recognition via attention gated"
        },
        {
          "Dou Hu, Lingwei Wei, and Xiaoyong Huai. 2021. Dia-": "Proceedings\nof\nhierarchical memory\nnetwork."
        },
        {
          "Dou Hu, Lingwei Wei, and Xiaoyong Huai. 2021. Dia-": "the AAAI Conference\non Artificial\nIntelligence,"
        },
        {
          "Dou Hu, Lingwei Wei, and Xiaoyong Huai. 2021. Dia-": "34(05):8002–8009."
        },
        {
          "Dou Hu, Lingwei Wei, and Xiaoyong Huai. 2021. Dia-": ""
        },
        {
          "Dou Hu, Lingwei Wei, and Xiaoyong Huai. 2021. Dia-": "Wenxiang\nJiao,\nHaiqin Yang,\nIrwin King,\nand"
        },
        {
          "Dou Hu, Lingwei Wei, and Xiaoyong Huai. 2021. Dia-": ""
        },
        {
          "Dou Hu, Lingwei Wei, and Xiaoyong Huai. 2021. Dia-": "Michael R. Lyu. 2019. HiGRU: Hierarchical gated re-"
        },
        {
          "Dou Hu, Lingwei Wei, and Xiaoyong Huai. 2021. Dia-": ""
        },
        {
          "Dou Hu, Lingwei Wei, and Xiaoyong Huai. 2021. Dia-": "current units for utterance-level emotion recognition."
        },
        {
          "Dou Hu, Lingwei Wei, and Xiaoyong Huai. 2021. Dia-": "In Proceedings of the 2019 Conference of the North"
        },
        {
          "Dou Hu, Lingwei Wei, and Xiaoyong Huai. 2021. Dia-": "American Chapter of the Association for Computa-"
        },
        {
          "Dou Hu, Lingwei Wei, and Xiaoyong Huai. 2021. Dia-": "tional Linguistics: Human Language Technologies,"
        },
        {
          "Dou Hu, Lingwei Wei, and Xiaoyong Huai. 2021. Dia-": "Volume 1 (Long and Short Papers), pages 397–406,"
        },
        {
          "Dou Hu, Lingwei Wei, and Xiaoyong Huai. 2021. Dia-": "Minneapolis, Minnesota. Association for Computa-"
        },
        {
          "Dou Hu, Lingwei Wei, and Xiaoyong Huai. 2021. Dia-": "tional Linguistics."
        },
        {
          "Dou Hu, Lingwei Wei, and Xiaoyong Huai. 2021. Dia-": ""
        },
        {
          "Dou Hu, Lingwei Wei, and Xiaoyong Huai. 2021. Dia-": "Taewoon Kim and Piek Vossen. 2021.\nEmoberta:"
        },
        {
          "Dou Hu, Lingwei Wei, and Xiaoyong Huai. 2021. Dia-": "Speaker-aware emotion recognition in conversation"
        },
        {
          "Dou Hu, Lingwei Wei, and Xiaoyong Huai. 2021. Dia-": "with roberta. CoRR, abs/2108.12009."
        },
        {
          "Dou Hu, Lingwei Wei, and Xiaoyong Huai. 2021. Dia-": ""
        },
        {
          "Dou Hu, Lingwei Wei, and Xiaoyong Huai. 2021. Dia-": "Bongseok Lee and Yong Suk Choi. 2021. Graph based"
        },
        {
          "Dou Hu, Lingwei Wei, and Xiaoyong Huai. 2021. Dia-": "network with contextualized representations of turns"
        },
        {
          "Dou Hu, Lingwei Wei, and Xiaoyong Huai. 2021. Dia-": "in dialogue. CoRR, abs/2109.04008."
        },
        {
          "Dou Hu, Lingwei Wei, and Xiaoyong Huai. 2021. Dia-": ""
        },
        {
          "Dou Hu, Lingwei Wei, and Xiaoyong Huai. 2021. Dia-": "Jingye Li, Donghong Ji, Fei Li, Meishan Zhang, and"
        },
        {
          "Dou Hu, Lingwei Wei, and Xiaoyong Huai. 2021. Dia-": ""
        },
        {
          "Dou Hu, Lingwei Wei, and Xiaoyong Huai. 2021. Dia-": "Yijiang Liu. 2020a. HiTrans: A transformer-based"
        },
        {
          "Dou Hu, Lingwei Wei, and Xiaoyong Huai. 2021. Dia-": ""
        },
        {
          "Dou Hu, Lingwei Wei, and Xiaoyong Huai. 2021. Dia-": "context- and speaker-sensitive model for emotion de-"
        },
        {
          "Dou Hu, Lingwei Wei, and Xiaoyong Huai. 2021. Dia-": ""
        },
        {
          "Dou Hu, Lingwei Wei, and Xiaoyong Huai. 2021. Dia-": "tection in conversations.\nIn Proceedings of the 28th"
        },
        {
          "Dou Hu, Lingwei Wei, and Xiaoyong Huai. 2021. Dia-": "International Conference on Computational Linguis-"
        },
        {
          "Dou Hu, Lingwei Wei, and Xiaoyong Huai. 2021. Dia-": "tics, pages 4190–4200, Barcelona, Spain (Online)."
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "International Committee on Computational Linguis-": "tics.",
          "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob": "Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz"
        },
        {
          "International Committee on Computational Linguis-": "",
          "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob": "Kaiser, and Illia Polosukhin. 2017. Attention is all"
        },
        {
          "International Committee on Computational Linguis-": "Wei Li, Wei Shao, Shaoxiong Ji, and Erik Cambria.",
          "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob": "you need. CoRR, abs/1706.03762."
        },
        {
          "International Committee on Computational Linguis-": "2020b.\nBieru: Bidirectional emotional\nrecurrent",
          "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob": ""
        },
        {
          "International Committee on Computational Linguis-": "unit\nfor conversational sentiment analysis. CoRR,",
          "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob": "Yan Wang, Jiayu Zhang, Jun Ma, Shaojun Wang, and"
        },
        {
          "International Committee on Computational Linguis-": "abs/2006.00492.",
          "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob": "Jing Xiao. 2020. Contextualized emotion recognition"
        },
        {
          "International Committee on Computational Linguis-": "",
          "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob": "in conversation as sequence tagging.\nIn Proceedings"
        },
        {
          "International Committee on Computational Linguis-": "Yanran Li, Hui Su, Xiaoyu Shen, Wenjie Li, Ziqiang",
          "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob": "of\nthe 21th Annual Meeting of\nthe Special Interest"
        },
        {
          "International Committee on Computational Linguis-": "Cao, and Shuzi Niu. 2017.\nDailydialog: A man-",
          "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob": "Group on Discourse and Dialogue, pages 186–195,"
        },
        {
          "International Committee on Computational Linguis-": "ually labelled multi-turn dialogue dataset.\nCoRR,",
          "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob": "1st virtual meeting. Association for Computational"
        },
        {
          "International Committee on Computational Linguis-": "abs/1710.03957.",
          "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob": "Linguistics."
        },
        {
          "International Committee on Computational Linguis-": "",
          "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob": "Thomas Wolf, Lysandre Debut, Victor Sanh,\nJulien"
        },
        {
          "International Committee on Computational Linguis-": "Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-",
          "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob": ""
        },
        {
          "International Committee on Computational Linguis-": "",
          "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob": "Chaumond, Clement Delangue, Anthony Moi, Pier-"
        },
        {
          "International Committee on Computational Linguis-": "dar\nJoshi, Danqi Chen, Omer Levy, Mike Lewis,",
          "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob": ""
        },
        {
          "International Committee on Computational Linguis-": "",
          "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob": "ric Cistac, Tim Rault, Remi Louf, Morgan Funtow-"
        },
        {
          "International Committee on Computational Linguis-": "Luke Zettlemoyer,\nand Veselin\nStoyanov.\n2019.",
          "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob": ""
        },
        {
          "International Committee on Computational Linguis-": "",
          "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob": "icz, Joe Davison, Sam Shleifer, Patrick von Platen,"
        },
        {
          "International Committee on Computational Linguis-": "Roberta: A robustly optimized BERT pretraining",
          "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob": ""
        },
        {
          "International Committee on Computational Linguis-": "",
          "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob": "Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu,"
        },
        {
          "International Committee on Computational Linguis-": "approach. CoRR, abs/1907.11692.",
          "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob": ""
        },
        {
          "International Committee on Computational Linguis-": "",
          "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob": "Teven Le Scao, Sylvain Gugger, Mariama Drame,"
        },
        {
          "International Committee on Computational Linguis-": "Minh-Thang\nLuong,\nHieu\nPham,\nand\nChristo-",
          "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob": "Quentin Lhoest, and Alexander Rush. 2020. Trans-"
        },
        {
          "International Committee on Computational Linguis-": "pher D. Manning. 2015.\nEffective approaches to",
          "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob": "formers: State-of-the-art natural language processing."
        },
        {
          "International Committee on Computational Linguis-": "attention-based neural machine translation. CoRR,",
          "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob": "In Proceedings of the 2020 Conference on Empirical"
        },
        {
          "International Committee on Computational Linguis-": "abs/1508.04025.",
          "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob": "Methods in Natural Language Processing: System"
        },
        {
          "International Committee on Computational Linguis-": "",
          "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob": "Demonstrations, pages 38–45, Online. Association"
        },
        {
          "International Committee on Computational Linguis-": "Navonil Majumder, Soujanya Poria, Devamanyu Haz-",
          "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob": "for Computational Linguistics."
        },
        {
          "International Committee on Computational Linguis-": "arika, Rada Mihalcea, Alexander Gelbukh, and Erik",
          "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob": ""
        },
        {
          "International Committee on Computational Linguis-": "",
          "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob": "Changhua Yang, Kevin Hsin-Yih Lin, and Hsin-Hsi"
        },
        {
          "International Committee on Computational Linguis-": "Cambria. 2019. Dialoguernn: An attentive rnn for",
          "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob": ""
        },
        {
          "International Committee on Computational Linguis-": "",
          "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob": "Chen. 2007. Emotion classification using web blog"
        },
        {
          "International Committee on Computational Linguis-": "Proceedings\nemotion detection in conversations.",
          "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob": ""
        },
        {
          "International Committee on Computational Linguis-": "",
          "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob": "corpora.\nIn 2007 IEEE / WIC / ACM International"
        },
        {
          "International Committee on Computational Linguis-": "of\nthe AAAI Conference on Artificial\nIntelligence,",
          "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob": ""
        },
        {
          "International Committee on Computational Linguis-": "",
          "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob": "Conference on Web Intelligence, WI 2007, 2-5 Novem-"
        },
        {
          "International Committee on Computational Linguis-": "33(01):6818–6825.",
          "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob": ""
        },
        {
          "International Committee on Computational Linguis-": "",
          "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob": "ber 2007, Silicon Valley, CA, USA, Main Conference"
        },
        {
          "International Committee on Computational Linguis-": "Yuzhao Mao, Qi Sun, Guang Liu, Xiaojie Wang,",
          "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob": "Proceedings, pages 275–278. IEEE Computer Soci-"
        },
        {
          "International Committee on Computational Linguis-": "Weiguo Gao, Xuan Li,\nand Jianping Shen. 2020.",
          "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob": "ety."
        },
        {
          "International Committee on Computational Linguis-": "Dialoguetrm: Exploring the intra- and inter-modal",
          "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob": ""
        },
        {
          "International Committee on Computational Linguis-": "",
          "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob": "Zhilin Yang, Zihang Dai, Yiming Yang, Jaime G. Car-"
        },
        {
          "International Committee on Computational Linguis-": "emotional behaviors\nin the conversation.\nCoRR,",
          "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob": ""
        },
        {
          "International Committee on Computational Linguis-": "",
          "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob": "bonell, Ruslan Salakhutdinov, and Quoc V. Le. 2019."
        },
        {
          "International Committee on Computational Linguis-": "abs/2010.07637.",
          "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob": ""
        },
        {
          "International Committee on Computational Linguis-": "",
          "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob": "Xlnet: Generalized autoregressive pretraining for lan-"
        },
        {
          "International Committee on Computational Linguis-": "",
          "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob": "guage understanding. CoRR, abs/1906.08237."
        },
        {
          "International Committee on Computational Linguis-": "Soujanya Poria, Devamanyu Hazarika, Navonil Ma-",
          "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob": ""
        },
        {
          "International Committee on Computational Linguis-": "jumder, Gautam Naik, Erik Cambria, and Rada Mi-",
          "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob": ""
        },
        {
          "International Committee on Computational Linguis-": "",
          "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob": "Sayyed M. Zahiri and Jinho D. Choi. 2017. Emotion"
        },
        {
          "International Committee on Computational Linguis-": "halcea. 2018. MELD: A multimodal multi-party",
          "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob": ""
        },
        {
          "International Committee on Computational Linguis-": "",
          "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob": "detection on TV show transcripts with sequence-"
        },
        {
          "International Committee on Computational Linguis-": "dataset\nfor\nemotion recognition in conversations.",
          "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob": ""
        },
        {
          "International Committee on Computational Linguis-": "",
          "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob": "based\nconvolutional\nneural\nnetworks.\nCoRR,"
        },
        {
          "International Committee on Computational Linguis-": "CoRR, abs/1810.02508.",
          "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob": ""
        },
        {
          "International Committee on Computational Linguis-": "",
          "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob": "abs/1708.04299."
        },
        {
          "International Committee on Computational Linguis-": "Weizhou Shen,\nJunqing Chen, Xiaojun Quan,\nand",
          "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob": ""
        },
        {
          "International Committee on Computational Linguis-": "",
          "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob": "Dong Zhang, Liangqing Wu, Changlong Sun, Shoushan"
        },
        {
          "International Committee on Computational Linguis-": "Zhixian Xie. 2020. Dialogxl: All-in-one xlnet for",
          "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob": ""
        },
        {
          "International Committee on Computational Linguis-": "",
          "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob": "Li, Qiaoming Zhu, and Guodong Zhou. 2019. Mod-"
        },
        {
          "International Committee on Computational Linguis-": "multi-party conversation emotion recognition. CoRR,",
          "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob": ""
        },
        {
          "International Committee on Computational Linguis-": "",
          "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob": "eling both context- and speaker-sensitive dependence"
        },
        {
          "International Committee on Computational Linguis-": "abs/2012.08695.",
          "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob": ""
        },
        {
          "International Committee on Computational Linguis-": "",
          "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob": "for emotion detection in multi-speaker conversations."
        },
        {
          "International Committee on Computational Linguis-": "",
          "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob": "the Twenty-Eighth International\nIn Proceedings of"
        },
        {
          "International Committee on Computational Linguis-": "Weizhou Shen, Siyue Wu, Yunyi Yang, and Xiaojun",
          "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob": ""
        },
        {
          "International Committee on Computational Linguis-": "",
          "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob": "Joint Conference on Artificial Intelligence, IJCAI-19,"
        },
        {
          "International Committee on Computational Linguis-": "Quan. 2021.\nDirected acyclic graph network for",
          "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob": ""
        },
        {
          "International Committee on Computational Linguis-": "",
          "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob": "pages 5415–5421. International Joint Conferences on"
        },
        {
          "International Committee on Computational Linguis-": "conversational emotion recognition.\nIn Proceedings",
          "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob": ""
        },
        {
          "International Committee on Computational Linguis-": "",
          "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob": "Artificial Intelligence Organization."
        },
        {
          "International Committee on Computational Linguis-": "of\nthe 59th Annual Meeting of\nthe Association for",
          "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob": ""
        },
        {
          "International Committee on Computational Linguis-": "Computational Linguistics and the 11th International",
          "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob": "Peixiang Zhong, Di Wang, and Chunyan Miao. 2019."
        },
        {
          "International Committee on Computational Linguis-": "Joint Conference on Natural Language Processing",
          "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob": "Knowledge-enriched transformer\nfor\nemotion de-"
        },
        {
          "International Committee on Computational Linguis-": "(Volume 1: Long Papers), pages 1551–1560, Online.",
          "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob": "tection in textual conversations.\nIn Proceedings"
        },
        {
          "International Committee on Computational Linguis-": "Association for Computational Linguistics.",
          "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob": "of\nthe 2019 Conference on Empirical Methods in"
        },
        {
          "International Committee on Computational Linguis-": "",
          "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob": "Natural Language Processing and the 9th Interna-"
        },
        {
          "International Committee on Computational Linguis-": "Dongming Sheng, Dong Wang, Ying Shen, Haitao",
          "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob": ""
        },
        {
          "International Committee on Computational Linguis-": "",
          "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob": "tional Joint Conference on Natural Language Pro-"
        },
        {
          "International Committee on Computational Linguis-": "Zheng, and Haozhuang Liu. 2020. Summarize before",
          "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob": ""
        },
        {
          "International Committee on Computational Linguis-": "",
          "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob": "cessing (EMNLP-IJCNLP), pages 165–176, Hong"
        },
        {
          "International Committee on Computational Linguis-": "aggregate: A global-to-local heterogeneous graph in-",
          "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob": ""
        },
        {
          "International Committee on Computational Linguis-": "",
          "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob": "Kong, China. Association for Computational Lin-"
        },
        {
          "International Committee on Computational Linguis-": "ference network for conversational emotion recogni-",
          "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob": ""
        },
        {
          "International Committee on Computational Linguis-": "",
          "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob": "guistics."
        },
        {
          "International Committee on Computational Linguis-": "tion.\nIn Proceedings of the 28th International Con-",
          "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob": ""
        },
        {
          "International Committee on Computational Linguis-": "ference on Computational Linguistics, pages 4153–",
          "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob": "Lixing Zhu, Gabriele Pergola, Lin Gui, Deyu Zhou,"
        },
        {
          "International Committee on Computational Linguis-": "4163, Barcelona, Spain (Online). International Com-",
          "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob": "and Yulan He. 2021. Topic-driven and knowledge-"
        },
        {
          "International Committee on Computational Linguis-": "mittee on Computational Linguistics.",
          "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob": "aware transformer\nfor dialogue emotion detection."
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "the 59th Annual Meeting of\nIn Proceedings of": "Association for Computational Linguistics and the",
          "the": "",
          "B": "",
          "Experimental environment": ""
        },
        {
          "the 59th Annual Meeting of\nIn Proceedings of": "11th International Joint Conference on Natural Lan-",
          "the": "",
          "B": "",
          "Experimental environment": ""
        },
        {
          "the 59th Annual Meeting of\nIn Proceedings of": "",
          "the": "",
          "B": "",
          "Experimental environment": "Item"
        },
        {
          "the 59th Annual Meeting of\nIn Proceedings of": "guage Processing (Volume 1: Long Papers), pages",
          "the": "",
          "B": "",
          "Experimental environment": ""
        },
        {
          "the 59th Annual Meeting of\nIn Proceedings of": "1571–1582, Online. Association for Computational",
          "the": "",
          "B": "",
          "Experimental environment": "OS"
        },
        {
          "the 59th Annual Meeting of\nIn Proceedings of": "Linguistics.",
          "the": "",
          "B": "",
          "Experimental environment": ""
        },
        {
          "the 59th Annual Meeting of\nIn Proceedings of": "",
          "the": "",
          "B": "",
          "Experimental environment": "CUDA Version"
        },
        {
          "the 59th Annual Meeting of\nIn Proceedings of": "",
          "the": "",
          "B": "",
          "Experimental environment": "GPU Card"
        },
        {
          "the 59th Annual Meeting of\nIn Proceedings of": "A\nAttention score (supplementary)",
          "the": "",
          "B": "",
          "Experimental environment": ""
        },
        {
          "the 59th Annual Meeting of\nIn Proceedings of": "",
          "the": "",
          "B": "",
          "Experimental environment": "GPU RAM Size"
        },
        {
          "the 59th Annual Meeting of\nIn Proceedings of": "The attention score α t",
          "the": "",
          "B": "",
          "Experimental environment": ""
        },
        {
          "the 59th Annual Meeting of\nIn Proceedings of": "",
          "the": "j of Formula (5) is as follows:",
          "B": "",
          "Experimental environment": ""
        },
        {
          "the 59th Annual Meeting of\nIn Proceedings of": "",
          "the": "",
          "B": "",
          "Experimental environment": "CPU"
        },
        {
          "the 59th Annual Meeting of\nIn Proceedings of": "",
          "the": "",
          "B": "",
          "Experimental environment": ""
        },
        {
          "the 59th Annual Meeting of\nIn Proceedings of": "√",
          "the": "",
          "B": "",
          "Experimental environment": "Deep Learning Framework"
        },
        {
          "the 59th Annual Meeting of\nIn Proceedings of": "dK\nj = Q t\nk KT",
          "the": "",
          "B": "",
          "Experimental environment": "PyTorch versio"
        },
        {
          "the 59th Annual Meeting of\nIn Proceedings of": "",
          "the": "(11)",
          "B": "",
          "Experimental environment": ""
        },
        {
          "the 59th Annual Meeting of\nIn Proceedings of": "where Q t",
          "the": "",
          "B": "",
          "Experimental environment": "Python version"
        },
        {
          "the 59th Annual Meeting of\nIn Proceedings of": ", Kj (cid:102)H W K\nj = (cid:101)H t W Q",
          "the": "",
          "B": "",
          "Experimental environment": ""
        }
      ],
      "page": 11
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "Carlos Busso",
        "Murtaza Bulut",
        "Chi-Chun Lee",
        "Abe Kazemzadeh",
        "Emily Mower",
        "Samuel Kim",
        "Jeannette Chang",
        "Sungbok Lee",
        "Shrikanth S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation",
      "doi": "10.1007/s10579-008-9076-6"
    },
    {
      "citation_id": "2",
      "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "Jacob Devlin",
        "Ming-Wei Chang",
        "Kenton Lee",
        "Kristina Toutanova"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
      "doi": "10.18653/v1/N19-1423"
    },
    {
      "citation_id": "3",
      "title": "COSMIC: COmmonSense knowledge for eMotion identification in conversations",
      "authors": [
        "Deepanway Ghosal",
        "Navonil Majumder"
      ],
      "year": "2020",
      "venue": "Findings of the Association for Computational Linguistics: EMNLP 2020",
      "doi": "10.18653/v1/2020.findings-emnlp.224"
    },
    {
      "citation_id": "4",
      "title": "Di-alogueGCN: A graph convolutional neural network for emotion recognition in conversation",
      "authors": [
        "Deepanway Ghosal",
        "Navonil Majumder",
        "Soujanya Poria",
        "Niyati Chhaya",
        "Alexander Gelbukh"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
      "doi": "10.18653/v1/D19-1015"
    },
    {
      "citation_id": "5",
      "title": "Speaker-aware BERT for multi-turn response selection in retrieval-based chatbots",
      "authors": [
        "Jia-Chen Gu",
        "Tianda Li",
        "Quan Liu",
        "Xiaodan Zhu",
        "Zhen-Hua Ling",
        "Zhiming Su",
        "Si Wei"
      ],
      "year": "2020",
      "venue": "Speaker-aware BERT for multi-turn response selection in retrieval-based chatbots"
    },
    {
      "citation_id": "6",
      "title": "2018a. ICON: Interactive conversational memory network for multimodal emotion detection",
      "authors": [
        "Devamanyu Hazarika",
        "Soujanya Poria",
        "Rada Mihalcea",
        "Erik Cambria",
        "Roger Zimmermann"
      ],
      "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
      "doi": "10.18653/v1/D18-1280"
    },
    {
      "citation_id": "7",
      "title": "Conversational memory network for emotion recognition in dyadic dialogue videos",
      "authors": [
        "Devamanyu Hazarika",
        "Soujanya Poria",
        "Amir Zadeh",
        "Erik Cambria",
        "Louis-Philippe Morency",
        "Roger Zimmermann"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
      "doi": "10.18653/v1/N18-1193"
    },
    {
      "citation_id": "8",
      "title": "Dialoguecrn: Contextual reasoning networks for emotion recognition in conversations",
      "authors": [
        "Dou Hu",
        "Lingwei Wei",
        "Xiaoyong Huai"
      ],
      "year": "2021",
      "venue": "Dialoguecrn: Contextual reasoning networks for emotion recognition in conversations"
    },
    {
      "citation_id": "9",
      "title": "Relation-aware graph attention networks with relational position encodings for emotion recognition in conversations",
      "authors": [
        "Taichi Ishiwatari",
        "Yuki Yasuda",
        "Taro Miyazaki",
        "Jun Goto"
      ],
      "year": "2020",
      "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
      "doi": "10.18653/v1/2020.emnlp-main.597"
    },
    {
      "citation_id": "10",
      "title": "Real-time emotion recognition via attention gated hierarchical memory network",
      "authors": [
        "Wenxiang Jiao",
        "Michael Lyu",
        "Irwin King"
      ],
      "year": "2020",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence",
      "doi": "10.1609/aaai.v34i05.6309"
    },
    {
      "citation_id": "11",
      "title": "HiGRU: Hierarchical gated recurrent units for utterance-level emotion recognition",
      "authors": [
        "Wenxiang Jiao",
        "Haiqin Yang",
        "Irwin King",
        "Michael Lyu"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
      "doi": "10.18653/v1/N19-1037"
    },
    {
      "citation_id": "12",
      "title": "Emoberta: Speaker-aware emotion recognition in conversation with roberta",
      "authors": [
        "Taewoon Kim",
        "Piek Vossen"
      ],
      "year": "2021",
      "venue": "Emoberta: Speaker-aware emotion recognition in conversation with roberta"
    },
    {
      "citation_id": "13",
      "title": "Graph based network with contextualized representations of turns in dialogue",
      "authors": [
        "Bongseok Lee",
        "Yong Choi"
      ],
      "year": "2021",
      "venue": "Graph based network with contextualized representations of turns in dialogue"
    },
    {
      "citation_id": "14",
      "title": "HiTrans: A transformer-based context-and speaker-sensitive model for emotion detection in conversations",
      "authors": [
        "Jingye Li",
        "Donghong Ji",
        "Fei Li",
        "Meishan Zhang",
        "Yijiang Liu"
      ],
      "year": "2020",
      "venue": "Proceedings of the 28th International Conference on Computational Linguistics",
      "doi": "10.18653/v1/2020.coling-main.370"
    },
    {
      "citation_id": "15",
      "title": "Shaoxiong Ji, and Erik Cambria. 2020b. Bieru: Bidirectional emotional recurrent unit for conversational sentiment analysis",
      "authors": [
        "Wei Li",
        "Wei Shao"
      ],
      "venue": "Shaoxiong Ji, and Erik Cambria. 2020b. Bieru: Bidirectional emotional recurrent unit for conversational sentiment analysis"
    },
    {
      "citation_id": "16",
      "title": "Dailydialog: A manually labelled multi-turn dialogue dataset",
      "authors": [
        "Yanran Li",
        "Hui Su",
        "Xiaoyu Shen",
        "Wenjie Li",
        "Ziqiang Cao",
        "Shuzi Niu"
      ],
      "year": "2017",
      "venue": "Dailydialog: A manually labelled multi-turn dialogue dataset"
    },
    {
      "citation_id": "17",
      "title": "Roberta: A robustly optimized BERT pretraining approach",
      "authors": [
        "Yinhan Liu",
        "Myle Ott",
        "Naman Goyal",
        "Jingfei Du",
        "Mandar Joshi",
        "Danqi Chen",
        "Omer Levy",
        "Mike Lewis",
        "Luke Zettlemoyer",
        "Veselin Stoyanov"
      ],
      "year": "2019",
      "venue": "Roberta: A robustly optimized BERT pretraining approach"
    },
    {
      "citation_id": "18",
      "title": "Effective approaches to attention-based neural machine translation",
      "authors": [
        "Minh-Thang Luong",
        "Hieu Pham",
        "Christopher Manning"
      ],
      "year": "2015",
      "venue": "Effective approaches to attention-based neural machine translation"
    },
    {
      "citation_id": "19",
      "title": "Dialoguernn: An attentive rnn for emotion detection in conversations",
      "authors": [
        "Navonil Majumder",
        "Soujanya Poria",
        "Devamanyu Hazarika",
        "Rada Mihalcea",
        "Alexander Gelbukh",
        "Erik Cambria"
      ],
      "year": "2019",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence",
      "doi": "10.1609/aaai.v33i01.33016818"
    },
    {
      "citation_id": "20",
      "title": "Dialoguetrm: Exploring the intra-and inter-modal emotional behaviors in the conversation",
      "authors": [
        "Yuzhao Mao",
        "Qi Sun",
        "Guang Liu",
        "Xiaojie Wang",
        "Weiguo Gao",
        "Xuan Li",
        "Jianping Shen"
      ],
      "year": "2020",
      "venue": "Dialoguetrm: Exploring the intra-and inter-modal emotional behaviors in the conversation"
    },
    {
      "citation_id": "21",
      "title": "MELD: A multimodal multi-party dataset for emotion recognition in conversations",
      "authors": [
        "Soujanya Poria",
        "Devamanyu Hazarika"
      ],
      "year": "2018",
      "venue": "MELD: A multimodal multi-party dataset for emotion recognition in conversations"
    },
    {
      "citation_id": "22",
      "title": "Dialogxl: All-in-one xlnet for multi-party conversation emotion recognition",
      "authors": [
        "Weizhou Shen",
        "Junqing Chen",
        "Xiaojun Quan",
        "Zhixian Xie"
      ],
      "year": "2020",
      "venue": "Dialogxl: All-in-one xlnet for multi-party conversation emotion recognition"
    },
    {
      "citation_id": "23",
      "title": "Directed acyclic graph network for conversational emotion recognition",
      "authors": [
        "Weizhou Shen",
        "Siyue Wu",
        "Yunyi Yang",
        "Xiaojun Quan"
      ],
      "year": "2021",
      "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing",
      "doi": "10.18653/v1/2021.acl-long.123"
    },
    {
      "citation_id": "24",
      "title": "Summarize before aggregate: A global-to-local heterogeneous graph inference network for conversational emotion recognition",
      "authors": [
        "Dongming Sheng",
        "Dong Wang",
        "Ying Shen",
        "Haitao Zheng",
        "Haozhuang Liu"
      ],
      "year": "2020",
      "venue": "Proceedings of the 28th International Conference on Computational Linguistics",
      "doi": "10.18653/v1/2020.coling-main.367"
    },
    {
      "citation_id": "25",
      "title": "Attention is all you need",
      "authors": [
        "Ashish Vaswani",
        "Noam Shazeer",
        "Niki Parmar",
        "Jakob Uszkoreit",
        "Llion Jones",
        "Aidan Gomez",
        "Lukasz Kaiser",
        "Illia Polosukhin"
      ],
      "year": "2017",
      "venue": "Attention is all you need"
    },
    {
      "citation_id": "26",
      "title": "Contextualized emotion recognition in conversation as sequence tagging",
      "authors": [
        "Yan Wang",
        "Jiayu Zhang",
        "Jun Ma",
        "Shaojun Wang",
        "Jing Xiao"
      ],
      "year": "2020",
      "venue": "Proceedings of the 21th Annual Meeting of the Special Interest Group on Discourse and Dialogue",
      "doi": "10.18653/v1/2020.sigdial-1.23"
    },
    {
      "citation_id": "27",
      "title": "Transformers: State-of-the-art natural language processing",
      "authors": [
        "Thomas Wolf",
        "Lysandre Debut",
        "Victor Sanh",
        "Julien Chaumond",
        "Clement Delangue",
        "Anthony Moi",
        "Pierric Cistac",
        "Tim Rault",
        "Remi Louf",
        "Morgan Funtowicz",
        "Joe Davison",
        "Sam Shleifer",
        "Clara Patrick Von Platen",
        "Yacine Ma",
        "Julien Jernite",
        "Canwen Plu",
        "Teven Xu",
        "Sylvain Le Scao",
        "Mariama Gugger",
        "Quentin Drame",
        "Alexander Lhoest",
        "Rush"
      ],
      "year": "2020",
      "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations",
      "doi": "10.18653/v1/2020.emnlp-demos.6"
    },
    {
      "citation_id": "28",
      "title": "Emotion classification using web blog corpora",
      "authors": [
        "Changhua Yang",
        "Kevin Hsin-Yih Lin",
        "Hsin-Hsi Chen"
      ],
      "year": "2007",
      "venue": "2007 IEEE / WIC / ACM International Conference on Web Intelligence",
      "doi": "10.1109/WI.2007.51"
    },
    {
      "citation_id": "29",
      "title": "Xlnet: Generalized autoregressive pretraining for language understanding",
      "authors": [
        "Zhilin Yang",
        "Zihang Dai",
        "Yiming Yang",
        "Jaime Carbonell",
        "Ruslan Salakhutdinov",
        "V Quoc",
        "Le"
      ],
      "year": "2019",
      "venue": "Xlnet: Generalized autoregressive pretraining for language understanding"
    },
    {
      "citation_id": "30",
      "title": "Emotion detection on TV show transcripts with sequencebased convolutional neural networks",
      "authors": [
        "M Sayyed",
        "Jinho Zahiri",
        "Choi"
      ],
      "year": "2017",
      "venue": "Emotion detection on TV show transcripts with sequencebased convolutional neural networks"
    },
    {
      "citation_id": "31",
      "title": "Modeling both context-and speaker-sensitive dependence for emotion detection in multi-speaker conversations",
      "authors": [
        "Dong Zhang",
        "Liangqing Wu",
        "Changlong Sun",
        "Shoushan Li",
        "Qiaoming Zhu",
        "Guodong Zhou"
      ],
      "year": "2019",
      "venue": "Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI-19",
      "doi": "10.24963/ijcai.2019/752"
    },
    {
      "citation_id": "32",
      "title": "Knowledge-enriched transformer for emotion detection in textual conversations",
      "authors": [
        "Peixiang Zhong",
        "Di Wang",
        "Chunyan Miao"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
      "doi": "10.18653/v1/D19-1016"
    },
    {
      "citation_id": "33",
      "title": "Topic-driven and knowledgeaware transformer for dialogue emotion detection",
      "authors": [
        "Lixing Zhu",
        "Gabriele Pergola",
        "Lin Gui",
        "Deyu Zhou",
        "Yulan He"
      ],
      "year": "2021",
      "venue": "Topic-driven and knowledgeaware transformer for dialogue emotion detection",
      "doi": "10.18653/v1/2021.acl-long.125"
    }
  ]
}