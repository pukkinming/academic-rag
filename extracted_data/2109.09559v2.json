{
  "paper_id": "2109.09559v2",
  "title": "Contrastive Learning Of Subject-Invariant Eeg Representations For Cross-Subject Emotion Recognition",
  "published": "2021-09-20T14:13:45Z",
  "authors": [
    "Xinke Shen",
    "Xianggen Liu",
    "Xin Hu",
    "Dan Zhang",
    "Sen Song"
  ],
  "keywords": [
    "EEG",
    "emotion recognition",
    "brain-computer interface",
    "cross-subject",
    "contrastive learning Shen",
    "X.",
    "Liu",
    "X.",
    "Hu",
    "X.",
    "Zhang",
    "D.",
    "& Song",
    "S.",
    "Contrastive Learning of Subject-Invariant EEG Representations for Cross-Subject Emotion Recognition. IEEE Transactions on Affective Computing",
    "2022. doi: 10.1109/TAFFC.2022.3164516"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "EEG signals have been reported to be informative and reliable for emotion recognition in recent years. However, the inter-subject variability of emotion-related EEG signals still poses a great challenge for the practical applications of EEG-based emotion recognition. Inspired by recent neuroscience studies on inter-subject correlation, we proposed a Contrastive Learning method for Inter-Subject Alignment (CLISA) to tackle the cross-subject emotion recognition problem. Contrastive learning was employed to minimize the inter-subject differences by maximizing the similarity in EEG signal representations across subjects when they received the same emotional stimuli in contrast to different ones. Specifically, a convolutional neural network was applied to learn inter-subject aligned spatiotemporal representations from EEG time series in contrastive learning. The aligned representations were subsequently used to extract differential entropy features for emotion classification. CLISA achieved stateof-the-art cross-subject emotion recognition performance on our THU-EP dataset with 80 subjects and the publicly available SEED dataset with 15 subjects. It could generalize to unseen subjects or unseen emotional stimuli in testing. Furthermore, the spatiotemporal representations learned by CLISA could provide insights into the neural mechanisms of human emotion processing.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "lectroencephalogram (EEG) based emotion recognition has gained increasing interest in recent years  [1] ,  [2] . Unlike behavioral techniques that record facial expressions, body gestures, voice, etc.  [3] , EEG provides a more direct and objective measurement of human emotional responses that cannot be easily disguised or consciously restrained  [1] ,  [4] ,  [5] . Compared to other neuroimaging techniques such as functional magnetic imaging (fMRI) and magnetoencephalography (MEG), EEG is advantageous for its portability and cost-effectiveness in real-world applications  [4] ,  [5] .\n\nExtensive studies have investigated emotion-related EEG representations  [6] ,  [7] ,  [8] . Differential entropy (DE) features, which are equivalent to the logarithm energy spectrum in a specific frequency band  [9] ,  [10] , have been widely used in the state-of-the-art emotion recognition methods  [11] ,  [12] ,  [13] . Beta-and Gamma-band DE features in temporal regions were found to be closely related to emotion  [14] ,  [15] . Beyond localized DE features, researchers found that modeling the relationships among EEG electrodes was critical for emotion recognition. In this direction, graph neural networks (GNNs)  [13] ,  [16] ,  [17]  and long short-term memory (LSTM)  [11] ,  [12]  were proposed to extract spatial relationships of DE features among different EEG channels. Network-based features like phase locking values  [18]  and microstate parameters  [19]  were also proposed to extract the coactivation patterns of different brain regions directly. Furthermore, convolutional neural networks (CNNs) and attention mechanisms were utilized to learn the emotion-related EEG representations in an end-to-end manner  [20] ,  [21] ,  [22] . These methods employed the prominent representational power of deep neural networks to avoid human-crafted feature extraction.\n\nHowever, most studies have mainly focused on intrasubject emotion recognition  [5] . To reach satisfactory recognition performance, researchers needed to collect sufficient data (generally half an hour to more than one hour) within one subject to learn subject-dependent EEG representations for emotion recognition  [14] ,  [23] ,  [24] . This laborious and time-consuming training procedure has become a major bottleneck for the practical use of EEG-based emotion recognition. Therefore, developing emotion recognition methods with good cross-subject generalizability is desirable for realistic applications, especially in the cases of new users  [25] ,  [26] .\n\nThe substantial inter-subject variabilities of emotion-related EEG activities posed great challenge for cross-subject emotion recognition  [27] ,  [28] . For example, the cross-subject emotion recognition accuracy on the widely used SEED dataset  [9] ,  [14]  could be as low as 58% with a generic multiple layer perceptron classifier. In comparison, the intra-subject emotion recognition could achieve high accuracy of 96% with the same classifier  [29] . The substantial drop in the emotion recognition performance from the intra-subject to the cross-subject scenario could be explained by the well-acknowledged individual difference in EEGbased emotion representations due to factors such as individualized experience and dispositional characteristics  [30] ,  [31] ,  [32] ,  [33] . Nonetheless, subject-invariance of emotion representation has also been well documented in the field of psychology and neuroscience  [15] ,  [34] ,  [35] ,  [36] ,  [37] . Previous fMRI and EEG studies have identified distinct and stable neural representations for different emotions across subjects  [15] ,  [34] ,  [35] ,  [36] ,  [37] , suggesting the possibility of developing cross-subject emotion recognition algorithms.\n\nTo address the issue of inter-subject variability, researchers have applied domain adaptation (DA) and domain generalization (DG) methods to cross-subject emotion recognition  [26] ,  [38] . The DA methods aim to minimize the discrepancy between data distributions of the source domain (i.e., the training subject) and the target domain (i.e., the testing subject). These methods have to access data from the target domain during the training process to measure the data discrepancy. As an example of the DA methods, domain-adversarial neural networks (DANNs) leverage adversarial training to align the EEG representation of the source domain and the target domain  [39] . It was adopted by multiple state-of-the-art cross-subject emotion recognition models  [11] ,  [12] ,  [13]  and could improve the accuracy from approximately 60% to more than 80% on the SEED dataset. The DG methods, on the other hand, find the domain-invariant representations from the source domains. In contrast to DA, it does not need to access the testing subjects' data, so it is preferred in realworld applications. An adversarial domain generalization method recently achieved comparable results with the DA methods on the SEED dataset  [26] . The success of these methods indicated the possibility of finding subject-invariant EEG representations for emotion recognition. However, the cross-subject emotion recognition methods to date have been developed mainly from the machine learning perspective, with very limited consideration of the neuroscientific basis of human emotion processing.\n\nThe emerging neuroscience studies on inter-subject correlation (ISC) could offer a new perspective for exploring subject-invariant emotion representations and developing cross-subject emotion recognition methods  [40] ,  [41] ,  [42] ,  [43] ,  [44] ,  [45] ,  [46] . The ISC approach, originally proposed to investigate the perception of naturalistic visual scenes, focuses on the synchronization of neural activities (e.g., EEG) between subjects when perceiving the same stimuli. Taking this inter-subject perspective, the temporal, spatial, and spectral patterns of ISC could reveal the neural mechanisms of information processing for naturalistic stimuli such as movies and narrative speech  [40] ,  [41] ,  [43] . In the field of emotion, several pioneering studies have shown that the ISC of EEG signals among a group of subjects watching the same emotional videos could reflect their group-level preference, arousal, valence, etc.  [4] ,  [41] ,  [42] . These findings suggest that the stimulus-specific EEG responses shared across individuals could carry valuable information for discriminating different emotional states. More importantly, the effectiveness of the shared EEG responses provides critical neuroscience evidence in favor of constructing subject-invariant emotion representations. Nonetheless, it remains elusive how to extract the subjectinvariant emotion representations effectively and make them generalizable to new subjects and new stimuli.\n\nIn this work, we propose a data-driven approach that performs Contrastive Learning for Inter-Subject Alignment (CLISA). Inspired from the neuroscientific observations of ISC, CLISA is grounded on the assumption that the neural activities of the subjects are in a similar state when they receive the same segment of emotional stimuli (i.e., the emotional videos in our study). Based on this fundamental idea, we propose to learn a subject-invariant space for EEG signals by aligning the representations underlying similar mental activities. Specifically, our CLISA framework contains two phases, i.e., the contrastive learning procedure and the prediction procedure. In the contrastive learning procedure, a convolutional neural network (CNN)-based encoder learns invariant and predictive spatiotemporal representations of EEG signals. It maximizes the similarities of the representations in response to identical emotional stimuli (positive pairs) while minimizing the similarities between signals corresponding to different stimuli (negative pairs). In the prediction procedure, a classifier together with the trained encoder takes EEG signals as inputs to identify human emotions. Considering that the contrastive pairs can be easily constructed from EEG datasets  [14] ,  [23] ,  [24]  and they are of great number in the contrastive learning procedure, the learned representations are expected to be informative and generalizable in the prediction procedure.\n\nThe advantages of CLISA are three-fold: 1) CLISA virtually and vastly increases the training data for the learning of EEG representations by contrastive learning. 2) CLISA can generalize to new subjects without requiring extensive data from them, thus enhancing the practicality of emotion recognition systems. 3) Benefiting from the contrastive learning strategy and the considerably larger amount of training samples, the learned representations are not only invariant to subjects but also generalizable to different stimuli. That is to say, the representation is not specific to the particular stimuli used in training. Instead, it is a general representation for emotion processing.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Related Work",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Eeg-Based Cross-Subject Emotion Recognition",
      "text": "Domain adaptation (DA) has been demonstrated as an effective technology in cross-subject emotion recognition. It aims to deal with the domain shift problem during training and testing. For the classical DA methods, Zheng & Lu  [47]  compared the performance of transfer component analysis (TCA)  [48] , kernel principal component analysis (KPCA)  [49] , transductive parameter transfer (TPT), etc. on the SEED dataset  [50] . TPT achieved the best accuracy of 76.3%, with a considerable improvement of 19.6% over the generic classifier with no DA. Further, Chai et al.  [51]  proposed an adaptive subspace feature matching (ASFM) strategy. Its essential component is a subspace alignment (SA) algorithm  [52] , which linearly transforms the PCA subspace of the training subject's data to be aligned with that of the testing subject's data.\n\nIn addition to these classical DA methods, DA with deep learning has also been utilized in cross-subject emotion recognition. In particular, Chai et al.  [53]  proposed an auto-encoder architecture to reduce the discrepancy of training and testing subjects in a learned latent space. Domain-adversarial neural networks (DANNs) employed a domain classifier and a gradient reversal layer to enforce the network to learn domain-indiscriminate representations across the source domain and the target domain  [39] . Based on this strategy, researchers further proposed the bihemispheres domain-adversarial neural network (Bi-DANN)  [11]  and regularized graph neural network (RGNN)  [13]  to facilitate cross-subject emotion recognition. BiDANN contained two local domain classifiers for each hemisphere and a global domain classifier to learn subjectinvariant emotion representations. RGNN introduced a node-wise domain-adversarial training in graph neural networks, which was better than graph-level domain-adversarial training. In addition to DANN, Li et al.  [29]  proposed a joint domain adaptation model to align the marginal and conditional distribution of the data simultaneously. These methods have improved the performance of cross-subject emotion recognition to around or above 85% on the SEED dataset, demonstrating their superiority to most of the classical DA methods. However, all the methods above had to access extensive data from the testing subjects for domain-adversarial training. Similar to intrasubject emotion recognition, half-an-hour to one-hour data from a new subject (although no requirements for emotional labels) were needed in general, which still hindered the application of emotion recognition methods in the real world.\n\nIn recent years, domain generalization (DG) methods have been applied to cross-subject emotion recognition to alleviate the reliance on data from testing subjects. DG models are trained to extract the domain-invariant representations across multiple training subjects, and they are ready to be applied to new subjects without access to their data. For example, Ma et al.  [26]  built a domain residual network to learn domain-shared weights and domain-specific weights with domain-adversarial training. Then the domain-shared weights were used to classify emotions for unseen subjects. Besides, Zhao et al.  [54]  proposed an autoencoder architecture to learn domain-shared encoders and classifiers, which showed generalizability to a new subject using his/her data within only one minute for calibration. These methods have achieved similar performance with DA methods on the SEED dataset.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Contrastive Learning",
      "text": "Contrastive learning is a kind of self-supervised learning algorithm that learns to discriminate whether pairs of data are similar or not. It has achieved state-of-the-art performance in various fields such as computer vision  [55] , natural language processing (NLP)  [56] , and bioinformatics  [57] ,  [58] . Contrastive learning can be generally divided into two types: 1) context-instance contrast, or global-local contrast, such as assigning a sentence to its paragraph and associating strides to a zebra, and 2) instance-instance contrast, like identifying a transformed image with the original one  [59] . After the pretext contrastive learning, the model can generate better data representations for downstream tasks.\n\nFor EEG, a few studies have used contrastive learning methods to learn data representations from relatively large datasets and then applied the pretrained model to downstream tasks. Mohsenvand et al.  [60]  enforced the model to learn similarities between different views of augmented samples from the same original data, similar to the popular SimCLR framework (a simple framework for contrastive learning of visual representations)  [55] . The samples were augmented by temporal masking, linear scaling, Gaussian noise adding, etc. The model pretrained on the combination of three datasets performed well on multiple downstream tasks: sleep stage classification, clinical abnormal detection, and emotion recognition. Banville et al.  [61]  used temporal context prediction and contrastive predictive coding  [62]  to learn data representations on two clinical datasets. They demonstrated that the pretrained model could extract latent structures of age effects, gender effects, and pathological effects. These methods generally adopted the contrastive learning framework from computer vision or natural language processing directly. Benefitted from the contrastive learning strategy and large datasets in pretraining, they could denoise the data or learn the temporal dependencies of the data. In this work, we proposed a contrastive learning strategy specifically designed for crosssubject generalization. It learns the similarity between samples from different subjects when they were presented with the same stimuli. Our method does not need extensive external data but creates large self-supervised labels based on the inter-subject aligned experimental design.\n\nAs our contrastive learning strategy was inspired by inter-subject correlation (ISC) studies, we will introduce the recent neuroscience findings of ISC here. The ISC studies have focused on the inter-subject consistent neural activities in response to the same naturalistic stimuli  [40] ,  [41] ,  [42] ,  [43]  or in the same social interaction scenarios  [44] ,  [45] . They provided insights into how human perceptions and emotions can be shared and why human communications can be effective from the neuroscience perspective. In a keynote fMRI study, Hasson et al.  [43]  found voxel-byvoxel synchronization across subjects when they were watching the same movie, revealing that different brains tend to respond similarly to the same naturalistic stimuli. Later, Dmochowski et al. discovered that the level of ISC in EEG signals could reflect people's emotionally laden attention to the movie  [42]  and predict the preference for the video clips in a large population  [41] . Ding et al.  [4]  further reported that EEG-based ISC could effectively predict realtime emotional experiences of arousal and valence. The prediction performance increased as a function of the number of subjects (i.e., brains) included in the analysis. These findings supported that the invariant response across individuals could be informative about various mental states, serving as solid theoretical supports for our contrastive learning strategy. However, due to the complexity of neural signals like EEGs, current ISC analysis has emphasized that the subjects should be positioned in the same sensory environment (and preferably engaged in similar tasks) to facilitate the computation of ISCs  [46] ,  [63] . How to extract inter-subject invariant representations that can generalize to new subjects or new stimuli has rarely been explored.\n\nFortunately, the paradigms adopted by the mainstream emotion EEG datasets, such as SEED  [14] , DEAP  [23] , and DREAMER  [24] , have made them well suited for our contrastive learning method. Specifically, the data were generally collected from a group of subjects undergoing the same task with the same emotional stimuli. Therefore, contrasting the EEG signals in response to the same or different emotional stimuli from two subjects would be helpful to learn the EEG representations for emotion processing that are shared across subjects. If the learned representation could capture the subject-invariant emotional EEG responses, it would be expected to generalize well to unseen subjects and unseen emotional stimuli to predict the emotional states.\n\nLast but not least, the contrastive learning framework would benefit from increasing the number of subjects. The number of contrasts could increase quadratically with the increasing number of subjects. However, to our knowledge, the number of subjects usually ranged from 10 to 40 in the publicly available emotional EEG datasets  [14] ,  [23] ,  [24] . In the present study, in addition to the popular SEED dataset with 15 subjects, a new dataset THU-EP (the Tsing-Hua University Emotional Profile dataset  [64] ) with 80 subjects was included (see Section 4.1 for more details), which was expected better to evaluate the effectiveness of the proposed CLISA method.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "The Contrastive Learning Method For Inter-Subject Alignment (Clisa)",
      "text": "This section presents our Contrastive Learning method for Inter-Subject Alignment (CLISA) (Fig.  1 ). We first introduce the contrastive learning procedure and then explain the prediction procedure of CLISA. In the contrastive learning procedure, we utilize a base encoder and a projector with convolutional layers to align the data representations across individuals. In the prediction procedure, we use the learned representations to identify the emotion labels of the EEG signals.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "The Contrastive Learning Procedure",
      "text": "In the contrastive learning procedure, CLISA contains four components: a data sampler, a base encoder, a projector, and a contrastive loss function. First, the data sampler generates a minibatch containing several pairs of EEG segments for training. Next, the base encoder processes these segments using a spatial convolution operator and a temporal convolution operator, aiming to transform the data from individual subjects to inter-subject aligned representations. Then the projector maps the representations to another latent space for computing their similarities (Fig.  1 ).\n\nTogether, the parameters of the base encoder and projector are optimized to minimize contrastive loss.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "A. The Data Sampler",
      "text": "In our contrastive learning strategy, the model learns to distinguish whether two series of EEG signals correspond to the same stimuli (i.e., the same segment of the video). To achieve this goal, we design a data sampler to prepare the inputs in a minibatch manner. In the EEG datasets, the data from one subject consist of N trials. In each trial, the subject watched one emotional video. To obtain a minibatch, we firstly randomly extract one EEG sample from each trial of subject A (the time length of one sample is smaller than that of one trial), resulting in N samples. We denote them as , , … , ( ∈ ℝ × , is the number of EEG channels and T is the number of time points in one EEG sample). Then we extract N samples of EEG signals from another subject B similarly. We denote them as , , … , . The",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "B. The Base Encoder",
      "text": "The base encoder takes the EEG signals or as inputs to generate aligned representations of the EEG data of different subjects. It adopts two one-dimensional convolutional operations to perform signal transformations (Fig.  1 , upper-left). For simplicity, we omit the corner markers of or in the following sections. 1) Spatial convolution As one EEG channel can pick up neural activities from multiple sources and the same source activity can influence signals of multiple EEG channels  [65] , we use a spatial convolution to transform the signals to a latent space for identifying plausible source activities. The spatial convolution is formulated as\n\nwhere ∈ ℝ × is the weights of the k-th one-dimensional spatial convolution filter. There are filters in total. ( ) ∈ ℝ × is the extracted representation by the spatial convolution. Each row of ( ) is a latent signal identified by a linear combination of the original signals ∈ ℝ × . We denote the overall spatial convolution operation as ( ) = Conv ( ) for simplicity. 2) Temporal convolution Another property of the EEG signals is the dynamic changes over time  [66] ,  [67] . Therefore, we further apply a temporal convolution to learn the temporal patterns of the EEG signals:\n\nwhere ∈ ℝ represents the filter weights of the temporal convolution. The filter length is , and the number of filters is .\n\n• ( )   , :\n\nrepresents the dot product of the vectors and ( ) , :\n\n. ∈ ℝ × × is the representation extracted by the temporal convolution. The input ( ) is padded to ensure the output is still of length T on the temporal dimension. A temporal convolution filter can usually extract representations in a specific frequency band of the EEG signals. We denote the overall temporal convolution operation as = Conv ( ( ) ).",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "C. The Projector",
      "text": "A nonlinear projector is utilized between the base encoder and the final contrastive loss (Fig.  1 , upper-right). This idea is inspired by the SimCLR framework  [55] , in which a nonlinear projector can help the base encoder learn better representations for downstream prediction tasks. As the use of structure units is a common practice in neural network designs  [68] ,  [69] , we employ a similar convolutional unit in the projector as that in the base encoder. Specifically, we but is a depthwise convolution with the filter size of 1 × . Here, the input is regarded as having feature maps, each of size × ⌊ / ⌋. In the depthwise convolution, each feature map of is processed by spatial convolution filters, so there are filters in total. ∈ ℝ × ×⌊ / ⌋ is the output of the spatial convolution.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "3) Temporal Convolution",
      "text": "We further apply a depthwise temporal convolution to further identify the temporal patterns:\n\nThe function Conv shares similar computations with Conv but is a depthwise convolution with the filter size as . There are temporal convolution filters for each feature map of , resulting in × = feature\n\nThe contrastive loss aims to maximize the similarity of two pieces of EEG signals in a positive pair. Similar to the SimCLR framework  [55] , we adopt the normalized temperature-scaled cross-entropy loss computed by\n\nwhere [ ] ∈ {0, 1} is an indicator function. It is set to 1 iff ≠ . By minimizing the loss function, the model will increase the similarity between and in contrast to all other possible sample pairs involving . Finally, the total loss of the minibatch is = ∑ + ∑ (8) The overall training algorithm of the contrastive learning procedure is summarized in Algorithm 1.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "The Prediction Procedure",
      "text": "In the prediction procedure, we use the trained base encoder to align the representations from different subjects and then extract predictive features from these representations for emotion recognition (Fig.  1 ).\n\nHere, we denote the data in the prediction procedure as { } and their labels as { }. The label is a categorical variable. For example, if there are three emotional categories, can take three values: 0, 1, or 2. We need to predict the emotion category for each sample ∈ ℝ × . CLISA extracts the aligned representations from the input by the trained base encoder:\n\nwhere ∈ ℝ × × can be regarded as signals with frequency components, and each component has latent dimensions with a time length of T'. We assume that possesses a better representation than for cross-subject emotion recognition. Note that T' can be different from the time length T used in contrastive learning, as the convolution operators can receive data of various lengths.\n\nConsidering the limited amount of data in EEG emotion recognition, machine learning models tend to overfit the high-dimensional representations. To obtain low-dimensional relevant representations for emotion recognition, we extract the widely-used differential entropy (DE) features  [9]  from . DE is defined as\n\nwhere is the variance of the signal and ∈ ℝ × . DE measures the complexity of the time series. It is equivalent to the logarithmic energy spectrum in a specific frequency band  [10] . We call the DE features as \"trained DE features\" as it is extracted from the output of the trained base encoder. The trained DE features from consecutive samples within one trial of one subject are concatenated across time and smoothed with a linear dynamical system (LDS) model as in Zheng & Lu's work  [14] . The smoothed features are reshaped into a one-dimensional vector ∈ ℝ as the classifier's input. Here, we use a three-layer multilayer perceptron (MLP) as the classifier. We denote the transformation of the classifier as = (\n\n).\n\n(11) The MLP is optimized with a minibatch stochastic gradient descent algorithm and cross-entropy loss to learn the mapping from the DE features { } to the emotion labels { }. The algorithm of the prediction procedure is also summarized in Algorithm 1. Calculate loss by (  6 )-(  8 ).",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "8:",
      "text": "θ ← θ -α , θ ← θ -α .",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Experiments",
      "text": "In our computational experiments, the proposed CLISA method was implemented and evaluated on two datasets: a new dataset THU-EP with 80 subjects  [64]  and the widely-used SEED dataset  [9] ,  [14] . The THU-EP dataset is expected to be a good benchmark for testing cross-subject emotion recognition models for its relatively larger number of subjects than most publicly available datasets. This section will first introduce the THU-EP dataset in more detail and the SEED dataset in brief, then describe the data preprocessing procedure and the implementation details of our model. Finally, we will introduce the approach of performance evaluation, performance comparison, and spatiotemporal pattern analysis.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "The Thu-Ep Dataset",
      "text": "Subjects. Eighty college students (50 females, mean age = 20.16 years, ranging from 17 to 24 years) were recruited into the study. Informed consent was obtained from all subjects. The study was approved by the Ethics Committee of Tsinghua University.\n\nStimuli. In the experiments, 28 emotional video clips were used as the stimuli. There were 12 video clips for eliciting four negative emotions (i.e., anger, disgust, fear, and sadness, three clips for each category), 12 video clips for eliciting four positive emotions (i.e., amusement, joy, inspiration, and tenderness, three clips for each category), and four video clips with neutral emotion. Therefore, nine emotion categories were included in total. These emotional categories were expected to cover the daily experienced emotion to a large extent  [73] ,  [74] ,  [75] ,  [76] . The video clips were selected from the published emotional video datasets  [73] ,  [77] ,  [78] . The duration of the videos is 67 seconds on average, ranging from 34 to 129 seconds.\n\nExperimental procedure. Each subject watched the video clips in seven blocks. Each block contained four trials. The subjects watched one video clip for each trial and rated their emotional states afterward. The subject reported their emotional states on 12 emotion items (i.e., anger, disgust, fear, sadness, amusement, joy, inspiration, and tenderness, as well as arousal, valence, familiarity, and liking) on a scale of 0 to 7. The four video clips in one block had the same valence (i.e., positive, negative, or neutral) to avoid possible influence between consecutive video clips with different valence. The subjects were asked to solve 20 arithmetic problems between two blocks to prevent carry-over effects of valence across blocks  [79] .\n\nEEG recording. EEG signals were recorded using a 32channel wireless EEG system (NeuSen.W32, Neuracle, China) placed according to the international 10-20 system: Fp1/2, Fz, F3/4, F7/8, FC1/2, FC5/6, Cz, C3/4, T3/4, A1/2 (left and right mastoids), CP1/2, CP5/6, T5/6, Pz, P3/4, PO3/4, Oz, O1/2. The sampling rate was 250 Hz. The EEG signals were referenced to CPz with a forehead ground at AFz. Electrode impedances were kept below 10 kOhm for all electrodes throughout the experiment.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "The Seed Dataset",
      "text": "The SEED dataset is a widely used benchmark to evaluate emotion recognition algorithms  [9] ,  [14] . It contains the EEG data collected from 15 subjects (8 females, mean age = 23.27 years, std of age = 2.37 years). Each of them watched 15 film clips. These film clips elicited three kinds of emotion, including positive, neutral, and negative (five film clips for each emotion). Each film clip was selected to elicit a single desired target emotion. The duration of each film clip is 226 seconds on average, ranging from 185 seconds to 265 seconds. Each subject was required to carry out the experiments for three sessions. There was a one-week or longer time interval between two sessions. For each session, the subjects watched one film clip for each trial, resulting in 15 trials. EEG signals were recorded using an ESI NeuroScan System2 with 62 channels placed according to the international 10-20 system with a sampling rate of 1000 Hz.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Data Preprocessing",
      "text": "Data preprocessing was conducted with Fieldtrip  [80]  and NoiseTools  [81]  toolkits in Matlab.\n\nFor the THU-EP dataset, we first applied a bandpass filter from 0.05 to 47 Hz. Then independent component analysis (ICA) was applied to remove possible artifacts due to eye movements, muscle movements, or other environmental noise. We used the Infomax algorithm  [82]  in Fieldtrip for ICA, which minimizes the mutual information of different components with natural gradient approach  [83] . The ICA algorithm received continuous data of one subject (without extracting data epochs) as inputs. Other settings were left as their defaults in Fieldtrip. A conservative criterion was applied to the ICA-based artifact rejection procedure: only the independent components (ICs) showing intense and persistent noise were removed. One or two ICs were removed per subject. After that, we implemented an automatic denoising procedure with NoiseTools to fix the data for spatially or temporally localized noises on a single-trial basis: We first interpolated the noisy channels by their three closest channels. If the proportion of outliers from one channel exceeded 30% of time points, the channel was defined as a noisy channel. The outliers were defined as those whose absolute values exceeded three times the median absolute value in the particular trial. After that, we fixed the remaining outliers with a threshold of 100 uV, which means if the difference of absolute values between two consecutive time points of one channel exceeded 100 uV, we replaced the value of the latter time point with that of the previous one. On average, 0.13 channels were interpolated, with a maximum of 3 channels per trial. 10.8% of all trials had at least one channel interpolated. The purpose of having the ICA-based and NoiseTools-based procedure was to keep (and fix) rather than reject the data, which was important for the present contrastive learning framework.\n\nAfter the automatic denoising procedure, we applied a bandpass filter from 4 to 47 Hz and re-referenced the data to the common average. We used the last 30 seconds of each trial to ensure the elicited emotions were coherent and intense enough  [23] ,  [33] ,  [73] . The EEG data from one subject was rejected due to strong power-line noise in all the EEG channels.\n\nFor the SEED dataset, the publicly available data have been downsampled to 200 Hz and filtered from 0 to 75 Hz by the data provider. The same denoising procedure was applied to the SEED dataset as the THU-EP dataset. Then we applied a bandpass filter from 4 to 47 Hz and re-referenced the data to the common average.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Implementation Details",
      "text": "In the base encoder for the THU-EP dataset: the filter size of the spatial convolution was set as the number of EEG channels (M=30); the temporal convolution filter length was set to 60, as a 60th-order finite impulse response filter is expected to provide the necessary support for extracting EEG signals in a specific frequency band; the number of spatial convolution filters ( ) and temporal convolution filters ( ) were both set to 16, which was expected to be sufficient to extract enough information for emotion-related neural representations.\n\nIn the projector for the THU-EP dataset: the spatial convolution filter size was set to 16 to match the spatial dimension of its input (i.e., the output of the base encoder, =16); the temporal convolution filter length was set to 6, which was expected to extract the temporal patterns of the averaged features sufficiently; the kernel length of the average pooling (prior to the spatial and temporal filters) was empirically set to 30; the parameter that controls the number of spatial or temporal filters in the projector was empirically set to 2.\n\nFor the SEED dataset, most of the hyperparameters were set the same as for THU-EP, except that 1) the temporal filter lengths and the average pooling's kernel length were proportional to those for the THU-EP dataset according to their sampling frequencies (200 Hz for SEED and 250 Hz for THU-EP) and 2) the spatial filter size in the base encoder was set to match the number of EEG channels in SEED (M=62).\n\nThe hyperparameters of the implemented CLISA method for the two datasets are shown in Table  1 . The temporal filter sizes in both the base encoder and the projector were further manipulated, and the models with the chosen parameters were able to obtain the top performances (Table S1, S2).",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "Table 1 Hyperparameter Settings Of The Model Architecture",
      "text": "",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Hyperparameters",
      "text": "",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Thu-Ep Seed Base Encoder",
      "text": "The spatial filter size M 30 62\n\nThe temporal filter size P 1  60 48  The number of spatial filters K 1  16  16\n\nThe number of temporal filters K 2  16 16  Projector Kernel length of average pooling S  30 24  The spatial filter size K 1  16  16\n\nThe temporal filter size P 2  6 4  The number of spatial filters CK 2  32 32  The number of temporal filters C 2 K 2  64  64\n\nThe time length of the samples in contrastive learning was determined by a tradeoff between the training samples' number and adequate sample length. On the one hand, the sample needed to be long enough for the contrastive learning model to capture the underlying emotion effectively. On the other hand, the longer sample's length resulted in an insufficient number of training data. Based on this consideration, the time length of one sample was set to 5 seconds (with a time step of 2 seconds) for the THU-EP dataset and 30 seconds (with a time step of 15 seconds) for the SEED dataset in contrastive learning. As the length of one trial is 30 seconds in the THU-EP dataset, the sample number from one trial is ⌊(30-5)/2⌋+1=13. The length of one trial ranges from 185 seconds to 265 seconds on the SEED dataset, so the sample number from one trial ranges from 11 to 16 (13 on average).\n\nIn the contrastive learning procedure, we used stratified normalization  [84]  during training. In stratified normalization, we concatenated the same channel of different samples from one subject in the minibatch together and conducted z-score normalization. The stratified normalization was applied to inputs of the base encoder, outputs of average pooling, and outputs of the temporal convolution in the projector. For optimization of the contrastive learning model, we trained the model for 100 epochs with early stopping (maximal tolerance of 30 epochs without validation accuracy increase). We used an Adam optimizer  [85]  with a cosine annealing learning rate scheduler and a three-time warm restart  [86] . The initial learning rate was set to 0.0007, and the weight decay was set to 0.015 empirically.\n\nIn the prediction procedure, the input sample length was set to 1 second as in many previous studies  [12] ,  [14] ,  [15] . For the extracted DE features, we conducted adaptive feature normalization, which adapted the mean and variance in z-score normalization online as new data come in  [87] ,  [88] ,  [89] . Specifically, the initial mean and variance were defined as the training data's mean ( ̅ ) and variance (\n\n). Then, with new data of the testing subject coming in, the mean and variance were updated as the weighted summation of ̅ (or ) and current available testing data's mean ̅ : (or variance, : ). The weights for ̅ and decayed exponentially with the input of the testing data, with a decay rate set as 0.99 empirically.\n\nFor the MLP classifier in the prediction procedure, there were two hidden layers with 30 units for each. Rectified linear units (ReLUs)  [90]  were used between every two layers. We used cross-entropy loss and an Adam optimizer to optimize the parameters. The learning rate was set as 0.0005 empirically, and the weight decay was selected from 0.005, 0.011, 0.025, 0.056, and 0.125 by cross-validation. The batch size was set as 256 empirically. We trained the model for 100 epochs with early stopping.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Performance Evaluation",
      "text": "We implemented two tasks to evaluate the model. The first was the generic cross-subject emotion recognition task. In this task, we used 10-fold cross-subject cross-validation for the THU-EP dataset and leave-one-subject-out cross-validation for the SEED dataset. The other was a more challenging task, which we call the generalizability test. In this task, the model needed not only to be generalized to new subjects but also to be generalized to new stimuli. This task could test whether the model learned real subject-invariant emotional representations rather than overfitting the existing stimuli. In particular, we used 2/3 of the trials from the training subjects in training and used the other 1/3 of the trials from the testing subjects in testing. Thus, the stimuli in testing had never been accessed by the model in training.\n\nThe training and testing subjects partition was the same as in the generic cross-subject emotion recognition task.\n\nFor the generic cross-subject emotion recognition task on the THU-EP dataset, we implemented the following two versions with respect to the number of emotion classes: 1) a basic version of binary classification for negative and positive emotional states and 2) a more challenging version of the nine-class emotional classification, including eight emotional classes mentioned above and neutral emotion. In the basic version, the samples from 12 trials that elicited anger, disgust, fear, and sadness were all labeled as negative emotions, while the samples from 12 trials eliciting amusement, joy, inspiration, and tenderness were labeled as positive emotions. The neutral emotion category was not included in the basic version due to an unbalanced number of trials (only four trials for eliciting neutral emotion). Regarding the SEED dataset, we implemented a three-class emotion classification for negative, neutral, and positive.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "Performance Comparison",
      "text": "To investigate the effectiveness of our contrastive learning method, we compared it with several competing emotion recognition methods, namely differential entropy (DE) features with MLP classifier (denoted by DE+MLP), subspace alignment (SA)  [52] , correlated component analysis (Cor-rCA)  [42] ,  [91] , and SeqCLR  [60] . DE+MLP was a simple baseline with no inter-subject alignment. SA and CorrCA conducted inter-subject alignment other than contrastive learning. SeqCLR implemented an alternative contrastive learning strategy by data augmentation. Among them, SA  [28] ,  [51]  and SeqCLR  [60]  have reached state-of-the-art performance in cross-subject emotion recognition.\n\nIn the DE+MLP baseline, we extracted DE features directly from preprocessed EEG data, normalized them adaptively, smoothed, and fed them into an MLP. The DE features were extracted from four frequency bands: theta (4-8 Hz), alpha (8-13 Hz), beta  (13) (14) (15) (16) (17) (18) (19) (20) (21) (22) (23) (24) (25) (26) (27) (28) (29) (30) , and gamma  (30) (31) (32) (33) (34) (35) (36) (37) (38) (39) (40) (41) (42) (43) (44) (45) (46) (47) . The hyperparameters of the classifier were the same as the proposed method.\n\nSubspace alignment (SA) utilizes a linear transformation to align the source and target data in PCA subspace. For the implementation of SA, we projected the data of training subjects to be aligned with that of each testing subject in the PCA subspace. In the subspace, the spatial dimension of the data was reduced to 16, which was identical to the number of spatial convolution filters in our model. After the projection, we filtered the data into 16 frequency bands equally spaced between the data's frequency limits ([4 Hz, 47 Hz]). DE features were extracted from each frequency band. This process resulted in 256-dimension (16×16, the same as in our method) features. Then we normalized, smoothed the data, and submitted them to the classifier.\n\nCorrelated component analysis (CorrCA) maximizes the correlation of latent components extracted from multiple EEG records. This method has been utilized to reveal subject-invariant brain responses to emotion  [91] . For the implementation of CorrCA, we identified the linear transformation of each subject that maximizes the ratio of between-subject to within-subject covariance. Similar to SA, we also retained 16 components and decomposed them into 16 frequency bands.\n\nSeqCLR is a contrastive learning method for EEG classification  [60] . It learned the similarity between augmented samples with the original ones and achieved state-of-theart performance on the SEED dataset. We implemented five augmentation strategies Mohsenvand et al.  [60]  proposed, including amplitude scale, time shift, zero-masking, additive Gaussian noise, and band-stop filter. The parameters of augmentation were the same as the original paper. To make a fair comparison, we used the same architectures of the base encoder and the projector as our model here. The other hyperparameters and pipelines were the same as our method.\n\nPaired t-tests were performed for the performance comparison between different methods. To account for multiple comparisons, we conducted Bonferroni correction and reported the corrected p-values.",
      "page_start": 9,
      "page_end": 10
    },
    {
      "section_name": "Spatiotemporal Pattern Analysis",
      "text": "We used the integrated gradients  [92]  method to identify important features for the MLP classifier and examined the spatiotemporal characteristics of these features. The integrated gradients method accumulates the gradient of prediction outputs along the straight line from a reference input (with all entries as the minimum of the actual input) to the actual input. The importance index for each feature was defined as its corresponding accumulated gradients. To obtain the most powerful predictive capacity, we first identified the best training epoch for the contrastive learning procedure and the prediction procedure with crossvalidation. Then we trained the models with those specific epochs using all data. The integrated gradient method was applied to the trained MLP classifier to derive the features with large importance indices for each emotion category.\n\nFor each important DE feature in MLP, we further identified its corresponding spatial and temporal filters in the base encoder. We obtained the spatial activation pattern from the spatial filter by: = Σ ( = 1,2, … ,16), where Σ is the mean covariance matrix of the EEG time series across subjects  [93] . Then we source localized the spatial activation pattern with Brainstorm  [94]  and OpenMEEG  [95]  toolboxes in Matlab. The forward model was estimated by a three-layer (scalp, inner skull, outer skull) symmetric boundary element method based on ICBM152 brain template  [96] . It generated 15002 fixed-orientation dipoles oriented normally to the cortex. A leadfield matrix linking the dipole activities to the EEG signals was obtained by the boundary element method. Then the linear inverse kernel that maps the EEG signals to the source activities was estimated by the sLORETA algorithm  [97] .",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Results",
      "text": "",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Emotion Recognition Performance On The Thu-Ep Dataset",
      "text": "Our model achieved a binary classification accuracy of 71.9±8.8% for discriminating the positive and negative emotional states on the THU-EP dataset (Table  2 , Table  S3 ). Compared with the control model (i.e., DE+MLP) that extracted DE features directly, CLISA presented a significant improvement of 6.7% (t(78)=4.19, Bonferroni corrected p<0.001). This comparison indicated that the representations learned by our contrastive learning method were more powerful than the simple DE features for cross-subject emotion recognition. CLISA also significantly outperformed other inter-subject alignment methods, including SA (65.5±9.7%, t(78)=5.01, corrected p<0.001), CorrCA (64.5±11.0%, t(78)=5.25, corrected p<0.001), and SeqCLR (64.0±9.8%, t(78)=6.41, corrected p<0.001), demonstrating the superiority of our contrastive learning strategy. The area under the receiver operating characteristic (ROC) curve further illustrated the effectiveness of the proposed method, as it presented noticeably better predictive power under all thresholds than the other competing baselines (Fig.  3 ). To illustrate the effects of contrastive learning, we visualized the original features and pretrained features of five example subjects by t-sne embedding (Fig.  4 ). The original DE features extracted from EEG signals were scattered in t-sne embedded space separately for different subjects (Fig.  4a ). In contrast, the trained DE features produced by CLISA were merged together, and different emotion categories remained separable (Fig.  4b ), indicating that our model could alleviate subject discrepancy effectively without loss of emotional separability, thus facilitating the cross-subject emotion recognition.\n\nThe performance of the proposed CLISA method benefitted substantially from the increasing number of training subjects in contrastive learning (Fig.  5 ). To investigate the effects of training subjects' numbers, we randomly selected a subset of training subjects with different subject numbers  (8, 16, 24, 32, 40, 48, 56, 64, or 72)  in the contrastive learning procedure. In the prediction procedure, we used all training subjects, which ensured the difference is only induced by the number of subjects in contrastive learning. Besides, we also implemented a baseline with no contrastive learning, i.e., we randomly initialized the base encoder and extracted DE features from its output. The performances of CLISA under different numbers of training subjects were illustrated in Fig.  5 . We observed that the performance of CLISA rises considerably with the increasing number of training subjects. Therefore, we could expect the CLISA method to learn better subject-invariant emotion representations with more subjects for contrastive learning.\n\nIn the generalizability test concerning new stimuli for the testing subjects (introduced in Section 4.5), our model achieved a classification accuracy of 63.4±17.1% (Table  3 , Table  S3 ), which was higher than all baseline models (the highest: 60.7±19.7%, although the improvement was not   significant, corrected ps>0.05). This result indicated that the model did not just overfit or memorize the stimuli (i.e., the videos) that it had already seen in contrastive learning. We used 16 trials of the training subjects and the other eight trials of the testing subjects in the generalizability test.\n\nIn the nine-class emotional classification task, CLISA significantly surpassed all the other competing methods by an improvement of 10.2% (Table  4 , Table  S4 , corrected ps<0.001 for all comparisons). The consistent improvement of CLISA over SA, CorrCA, and SeqCLR on these tasks shows the superiority of our method to other linear transformation methods or other contrastive learning strategies.\n\nBased on the prediction results of CLISA, we further analyzed the confusion matrix of different emotion categories (Fig.  6 ). We observed that the disgust emotion could be identified with high accuracy, indicating its clear neural representation and high inter-subject consistency. Among positive emotions, amusement and tenderness could be identified best.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Emotion Recognition Performance On The Seed Dataset",
      "text": "The effectiveness of our method was also evaluated on the widely-used SEED dataset. CLISA obtained an accuracy of 86.4±6.4% on the three-class emotion classification task (Table  5 , Table  S5 ).   [26] ,  [54] . It further demonstrated the effectiveness of our contrastive learning strategy compared to domain-adversarial strategies. The confusion matrix of our model is shown in Fig.  7 . The model tended to confuse negative emotion with the other two emotions, especially with neutral emotion. The classification of the positive emotion was more accurate. These results were similar to previous studies  [11] ,  [13] ,  [14] . The comparison methods DE+MLP, SA, CorrCA, and SeqCLR were implemented in this paper. The results of DResNet and PPDA were reported in previous studies.\n\nIn the generalizability test on the SEED dataset, our model achieved the highest classification accuracy as 77.4±13.4% (Table  6 , Table  S5 ), although the improvement was not statistically significant (corrected ps>0.05). This result further validated the generalizability of our model to new stimuli (i.e., the videos) that it had never seen.   We used the first nine trials of the training subjects and the last six trials of the testing subjects in the generalizability test.",
      "page_start": 11,
      "page_end": 12
    },
    {
      "section_name": "Spatiotemporal Patterns For Emotion Recognition",
      "text": "This section presents the inter-subject aligned spatiotemporal representations extracted by CLISA. We analyzed the two most important features with the largest importance indices (See Section 4.7 for the definition) for each emotion category. On the SEED dataset, we identified spatial activations mainly in anterior temporal regions for negative emotion, with frequency responses at around 12-21 Hz and 4 Hz (Fig.  8 , Fig.  S1 ). For positive emotion, the spatial activations focused on the temporal regions, with higher frequency responses of more than 25 Hz. The neutral state has frontal and temporal activations at around 4-8 Hz and left temporal activations at high frequencies of more than 25 Hz.\n\nFor binary classification on the THU-EP dataset, the spatial activations of the most important feature reside mainly in right occipital and right temporal regions, for both positive and negative emotions (Figs.  S2, S3 ). The corresponding frequency response of this pattern was 4-8 Hz for positive emotion and less than 4 Hz for negative emotion. The second important features for positive and negative emotions also have similar spatial activation patterns located in bilateral temporal regions. The corresponding temporal response was around 21-38 Hz for positive emotion and 17-21 Hz for negative emotion. The activation in temporal regions with high-frequency responses for positive emotion was similar to that on the SEED dataset.\n\nFor nine-class emotion classification, the model",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Discussion",
      "text": "In comparison to existing cross-subject emotion recognition methods, our CLISA model has several prominent features. Firstly, the proposed contrastive learning strategy utilized the temporal alignment information of the data, i.e., which two pieces of data correspond to the same video segment. Thus, the contrastive learning strategy can match different subjects' data on a finer scale. As DResNet, PPDA, and other mainstream domain adaptation methods  [38]  were mostly based on domain classifiers, they can only roughly match the overall data distributions from different subjects. Secondly, CLISA implemented subject alignment with EEG time series as inputs, while DResNet, PPDA, and most existing cross-subject methods  [38]    adaptive feature normalization. In comparison, most domain adaptation methods require extensive data from the testing subjects for adaptation  [38] . Therefore, CLISA can enhance the practicality of an emotion recognition system. The spatial and temporal convolution architecture in CLISA offers the possibility to analyze activated sources and responsive frequencies for each emotion category. The spatial activations in temporal regions with high frequency responses (>20 Hz) were important for positive emotion on both the THU-EP and the SEED datasets. This result is in line with most previous studies  [14] ,  [15] ,  [23] . The spatial activations for negative emotion were mainly in more anterior regions on the SEED dataset and more posterior regions on the THU-EP dataset. The discrepancy could be due to context-specific responses in each dataset that requires further investigations.\n\nWhen we compare the important patterns for positive emotion and negative emotion, spatial activations generally have considerable overlap. It supports the affective workspace hypothesis that positive and negative emotion processing involves the same valence-general brain regions  [98] , which is consistent with a meta-analysis of 397 fMRI studies  [99] . At the same time, the frequency responses for different affective states were distinct on both datasets. It indicates different dynamic processing mechanisms for positive and negative emotions, which fMRI studies can hardly reveal.\n\nFurthermore, discrete emotion categories (in nine-class emotion classification) showed both emotion-specific spatial and temporal patterns. On the one hand, it suggests that from the perspective of EEG spatial activities, discrete emotion categories have more distinct neural representations than the affective dimension (i.e., valence). Different network activities underlying discrete emotion categories have also been well documented in fMRI studies  [35] ,  [100] . On the other hand, the temporal patterns are distinguishable for both discrete emotion categories and the valence dimension. Further studies could more thoroughly investigate the dynamic neural activities underlying emotion with EEG or MEG.\n\nThis study has some limitations that should be noted. Firstly, the proposed CLISA model was validated with EEG data of young adults (mean age = 20.16 years for the THU-EP dataset and mean age = 23.27 years for the SEED dataset). As age has been known to play an important role in emotion processing  [101] ,  [102] , further studies are necessary to include subjects covering different age ranges for a more generalized model. Secondly, while the base encoder's architecture could be neurophysiologically meaningful, the possible neurophysiological implications for the projector were limited. Further studies are expected to develop a more neurophysiologically inspired projector, which could improve the interpretability of the whole network architecture. Finally, since CLISA aimed to differentiate different emotion categories, the spatiotemporal patterns that are shared across emotion categories might not be sufficiently revealed in this study. Further studies should be designed to identify both shared and distinguishable spatiotemporal patterns for emotion categories to better understand the neural mechanisms of emotion processing.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Conclusions",
      "text": "In this study, we proposed a contrastive learning method for inter-subject alignment, which is inspired by the intersubject correlation studies in neuroscience. The proposed method achieved comparable or better performance in comparison to state-of-the-art methods on two datasets. On the THU-EP dataset, it obtained a binary classification accuracy of 71.9±8.8% and a nine-class classification accuracy of 45.7±11.8%. On the SEED dataset, it achieved a three-class classification accuracy of 86.4±6.4%. Moreover, we validated the model could generalize to unseen emotional stimuli better than other comparison methods. By visualizing important spatial and temporal filters in the model, we also demonstrated its potential for providing insights into the neural substrates of emotion.   The temporal filter length in the projector was set as 6 here. The temporal filter length in the base encoder was set as 60 here.",
      "page_start": 13,
      "page_end": 20
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: ). We first",
      "page": 4
    },
    {
      "caption": "Figure 1: The illustration of the Contrastive Learning method for Inter-subject Alignment (CLISA). In the figure, “Sub” stands for “subject” and “Conv”",
      "page": 4
    },
    {
      "caption": "Figure 2: ). In this minibatch,",
      "page": 5
    },
    {
      "caption": "Figure 1: , upper-right). This idea",
      "page": 5
    },
    {
      "caption": "Figure 2: The illustration of the data sampler. In a minibatch, given one",
      "page": 5
    },
    {
      "caption": "Figure 4: ). The original",
      "page": 10
    },
    {
      "caption": "Figure 4: a). In contrast, the trained DE features produced by",
      "page": 10
    },
    {
      "caption": "Figure 4: b), indicating that our",
      "page": 10
    },
    {
      "caption": "Figure 5: ). To investigate the",
      "page": 10
    },
    {
      "caption": "Figure 5: We observed that the performance of",
      "page": 10
    },
    {
      "caption": "Figure 4: (a) t-sne results of original differential entropy (DE) features. (b)",
      "page": 10
    },
    {
      "caption": "Figure 3: The receiver operating characteristic (ROC) curve in the binary",
      "page": 10
    },
    {
      "caption": "Figure 5: The validation accuracy increases with the number of training",
      "page": 10
    },
    {
      "caption": "Figure 6: ). We observed that the disgust emotion could be",
      "page": 11
    },
    {
      "caption": "Figure 7: The model tended to confuse neg-",
      "page": 11
    },
    {
      "caption": "Figure 6: The confusion matrix for the nine-class emotional classification",
      "page": 11
    },
    {
      "caption": "Figure 7: The confusion matrix for the SEED dataset.",
      "page": 11
    },
    {
      "caption": "Figure 8: , Fig. S1). For positive emotion, the spatial activations fo-",
      "page": 12
    },
    {
      "caption": "Figure 8: Spatiotemporal characteristics of the important features for emotion classification on the SEED dataset. We visualize the spatial acti-",
      "page": 12
    }
  ],
  "tables": [
    {
      "caption": "Table 1: . The tem-",
      "page": 8
    },
    {
      "caption": "Table 1: HYPERPARAMETER SETTINGS OF THE MODEL ARCHITECTURE",
      "page": 8
    },
    {
      "caption": "Table 2: , Table S3).",
      "page": 10
    },
    {
      "caption": "Table 2: THE BINARY CLASSIFICATION ACCURACIES OF DIFFERENT",
      "page": 10
    },
    {
      "caption": "Table 3: THE GENERALIZABILITY TEST ON THE THU-EP DATASET",
      "page": 11
    },
    {
      "caption": "Table 4: , Table S4, corrected",
      "page": 11
    },
    {
      "caption": "Table 4: THE NINE-CLASS CLASSIFICATION ACCURACIES OF DIFFER-",
      "page": 11
    },
    {
      "caption": "Table 5: , Table S5). Similar to the THU-EP dataset, CLISA",
      "page": 11
    },
    {
      "caption": "Table 5: THE CLASSIFICATION ACCURACIES OF DIFFERENT METHODS",
      "page": 11
    },
    {
      "caption": "Table 6: , Table S5), although the improvement",
      "page": 11
    },
    {
      "caption": "Table 6: THE GENERALIZABILITY TEST ON THE SEED DATASET",
      "page": 12
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "A Review of Emotion Recognition Using Physiological Signals",
      "authors": [
        "L Shu",
        "J Xie",
        "M Yang",
        "Z Li",
        "Z Li",
        "D Liao",
        "X Xu",
        "X Yang"
      ],
      "year": "2018",
      "venue": "Sensors"
    },
    {
      "citation_id": "2",
      "title": "Emotions Recognition Using EEG Signals: A Survey",
      "authors": [
        "S Alarcao",
        "M Fonseca"
      ],
      "year": "2017",
      "venue": "IEEE Trans. Affect. Comput"
    },
    {
      "citation_id": "3",
      "title": "A Survey of Affective Brain Computer Interfaces: Principles, State-of-the-Art, and Challenges",
      "authors": [
        "C Mühl",
        "B Allison",
        "A Nijholt",
        "G Chanel"
      ],
      "year": "2014",
      "venue": "Brain-Comput. Interfaces"
    },
    {
      "citation_id": "4",
      "title": "Inter-Brain EEG Feature Extraction and Analysis for Continuous Implicit Emotion Tagging During Video Watching",
      "authors": [
        "Y Ding",
        "X Hu",
        "Z Xia",
        "Y.-J Liu",
        "D Zhang"
      ],
      "year": "2018",
      "venue": "IEEE Trans. Affect. Comput"
    },
    {
      "citation_id": "5",
      "title": "Ten Challenges for EEG-Based Affective Computing",
      "authors": [
        "X Hu",
        "J Chen",
        "F Wang",
        "D Zhang"
      ],
      "year": "2019",
      "venue": "Brain sci. adv"
    },
    {
      "citation_id": "6",
      "title": "Feature extraction and selection for emotion recognition from EEG",
      "authors": [
        "R Jenke",
        "A Peer",
        "M Buss"
      ],
      "year": "2014",
      "venue": "IEEE Trans. Affect. Comput"
    },
    {
      "citation_id": "7",
      "title": "Emotion recognition from multi-channel eeg via deep forest",
      "authors": [
        "J Cheng",
        "M Chen",
        "C Li",
        "Y Liu",
        "R Song",
        "A Liu",
        "X Chen"
      ],
      "year": "2020",
      "venue": "IEEE J. Biomed. Health Inform"
    },
    {
      "citation_id": "8",
      "title": "Multichannel EEG-based emotion recognition in the presence of noisy labels",
      "authors": [
        "C Li",
        "Y Hou",
        "R Song",
        "J Cheng",
        "Y Liu",
        "X Chen"
      ],
      "year": "2022",
      "venue": "Information Sciences"
    },
    {
      "citation_id": "9",
      "title": "Differential Entropy Feature for EEG-Based Emotion Classification",
      "authors": [
        "R.-N Duan",
        "J.-Y Zhu",
        "B.-L Lu"
      ],
      "year": "2013",
      "venue": "Int. IEEE/EMBS Conf. Neural Eng. (NER)"
    },
    {
      "citation_id": "10",
      "title": "Differential Entropy Feature for EEG-Based Vigilance Estimation",
      "authors": [
        "L Shi",
        "Y Jiao",
        "B Lu"
      ],
      "year": "2013",
      "venue": "Conf. Proc"
    },
    {
      "citation_id": "11",
      "title": "A Bi-Hemisphere Domain Adversarial Neural Network Model for EEG Emotion Recognition",
      "authors": [
        "Y Li",
        "W Zheng",
        "Y Zong",
        "Z Cui",
        "T Zhang",
        "X Zhou"
      ],
      "year": "2018",
      "venue": "IEEE Trans. Affect. Comput"
    },
    {
      "citation_id": "12",
      "title": "A Novel Bi-Hemispheric Discrepancy Model for EEG Emotion Recognition",
      "authors": [
        "Y Li",
        "L Wang",
        "W Zheng",
        "Y Zong",
        "L Qi",
        "Z Cui",
        "T Zhang",
        "T Song"
      ],
      "year": "2021",
      "venue": "IEEE Trans. Cogn. Develop"
    },
    {
      "citation_id": "13",
      "title": "EEG-Based Emotion Recognition Using Regularized Graph Neural Networks",
      "authors": [
        "P Zhong",
        "D Wang",
        "C Miao"
      ],
      "year": "2020",
      "venue": "IEEE Trans. Affect. Comput",
      "doi": "10.1109/TAFFC.2020.2994159"
    },
    {
      "citation_id": "14",
      "title": "Investigating Critical Frequency Bands and Channels for EEG-Based Emotion Recognition with Deep Neural Networks",
      "authors": [
        "W.-L Zheng",
        "B.-L Lu"
      ],
      "year": "2015",
      "venue": "IEEE Trans. Cogn. Develop. Syst"
    },
    {
      "citation_id": "15",
      "title": "Identifying Stable Patterns over Time for Emotion Recognition from EEG",
      "authors": [
        "W.-L Zheng",
        "J.-Y Zhu",
        "B.-L Lu"
      ],
      "year": "2017",
      "venue": "IEEE Trans. Affect. Comput"
    },
    {
      "citation_id": "16",
      "title": "EEG Emotion Recognition Using Dynamical Graph Convolutional Neural Networks",
      "authors": [
        "T Song",
        "W Zheng",
        "P Song",
        "Z Cui"
      ],
      "year": "2018",
      "venue": "IEEE Trans. Affect. Comput"
    },
    {
      "citation_id": "17",
      "title": "Sparsedgcnn: Recognizing Emotion from Multichannel EEG Signals",
      "authors": [
        "G Zhang",
        "M Yu",
        "Y.-J Liu",
        "G Zhao",
        "D Zhang",
        "W Zheng"
      ],
      "year": "2021",
      "venue": "IEEE Trans. Affect. Comput",
      "doi": "10.1109/TAFFC.2021.3051332"
    },
    {
      "citation_id": "18",
      "title": "EEG Based Emotion Recognition by Combining Functional Connectivity Network and Local Activations",
      "authors": [
        "P Li",
        "H Liu",
        "Y Si",
        "C Li",
        "F Li",
        "X Zhu",
        "X Huang",
        "Y Zeng",
        "D Yao",
        "Y Zhang"
      ],
      "year": "2019",
      "venue": "IEEE Trans. Biomed. Eng"
    },
    {
      "citation_id": "19",
      "title": "Exploring EEG Microstates for Affective Computing: Decoding Valence and Arousal Experiences During Video Watching",
      "authors": [
        "X Shen",
        "X Hu",
        "S Liu",
        "S Song",
        "D Zhang"
      ],
      "year": "2020",
      "venue": "Conf. Proc"
    },
    {
      "citation_id": "20",
      "title": "EEGbased emotion recognition using an end-to-end regional-asymmetric convolution-al neural network",
      "authors": [
        "H Cui",
        "A Liu",
        "X Zhang",
        "X Chen",
        "K Wang",
        "X Chen"
      ],
      "year": "2020",
      "venue": "Knowl. Based. Syst"
    },
    {
      "citation_id": "21",
      "title": "EEG-based emotion recognition via channel-wise attention and self attention",
      "authors": [
        "W Tao",
        "C Li",
        "R Song",
        "J Cheng",
        "Y Liu",
        "F Wan",
        "X Chen"
      ],
      "year": "2020",
      "venue": "IEEE Trans. Affect. Comput",
      "doi": "10.1109/TAFFC.2020.3025777"
    },
    {
      "citation_id": "22",
      "title": "EEGbased Emotion Recognition via Neural Architecture Search",
      "authors": [
        "C Li",
        "Z Zhang",
        "R Song",
        "J Cheng",
        "Y Liu",
        "X Chen"
      ],
      "year": "2021",
      "venue": "IEEE Trans. Affect. Comput",
      "doi": "10.1109/TAFFC.2021.3130387"
    },
    {
      "citation_id": "23",
      "title": "DEAP: A Database for Emotion Analysis Using Physiological Signals",
      "authors": [
        "S Koelstra",
        "C Muhl",
        "M Soleymani",
        "J Lee",
        "A Yazdani",
        "T Ebrahimi",
        "T Pun",
        "A Nijholt",
        "I Patras"
      ],
      "year": "2012",
      "venue": "IEEE Trans. Affect. Comput"
    },
    {
      "citation_id": "24",
      "title": "Dreamer: A Database for Emotion Recognition through EEG and ECG Signals from Wireless Low-Cost Off-the-Shelf Devices",
      "authors": [
        "S Katsigiannis",
        "N Ramzan"
      ],
      "year": "2017",
      "venue": "IEEE J. Biomed. Health Inform"
    },
    {
      "citation_id": "25",
      "title": "Multisource Transfer Learning for Cross-Subject EEG Emotion Recognition",
      "authors": [
        "J Li",
        "S Qiu",
        "Y Shen",
        "C Liu",
        "H He"
      ],
      "year": "2019",
      "venue": "IEEE Trans. Cybern"
    },
    {
      "citation_id": "26",
      "title": "Reducing the Subject Variability of EEG Signals with Adversarial Domain Generalization",
      "authors": [
        "B.-Q Ma",
        "H Li",
        "W.-L Zheng",
        "B.-L Lu"
      ],
      "year": "2019",
      "venue": "Int. Conf. on Neural Inf. Process. (ICONIP)"
    },
    {
      "citation_id": "27",
      "title": "Transfer Learning in Brain-Computer Interfaces",
      "authors": [
        "V Jayaram",
        "M Alamgir",
        "Y Altun",
        "B Scholkopf",
        "M Grosse-Wentrup"
      ],
      "year": "2016",
      "venue": "IEEE Comput. Intell. Mag"
    },
    {
      "citation_id": "28",
      "title": "Domain Adaptation for Cross-Subject Emotion Recognition by Subject Clustering",
      "authors": [
        "J Liu",
        "X Shen",
        "S Song",
        "D Zhang"
      ],
      "year": "2021",
      "venue": "Int. IEEE/EMBS Conf. Neural Eng. (NER)"
    },
    {
      "citation_id": "29",
      "title": "Domain Adaptation for EEG Emotion Recognition Based on Latent Representation Similarity",
      "authors": [
        "J Li",
        "S Qiu",
        "C Du",
        "Y Wang",
        "H He"
      ],
      "year": "2019",
      "venue": "IEEE Trans. Cogn. Develop"
    },
    {
      "citation_id": "30",
      "title": "Individual Differences in Emotion Processing",
      "authors": [
        "S Hamann",
        "T Canli"
      ],
      "year": "2004",
      "venue": "Curr. Opin. Neurobiol"
    },
    {
      "citation_id": "31",
      "title": "Emotion Analysis for Personality Inference from EEG Signals",
      "authors": [
        "G Zhao",
        "Y Ge",
        "B Shen",
        "X Wei",
        "H Wang"
      ],
      "year": "2017",
      "venue": "IEEE Trans. Affect. Comput"
    },
    {
      "citation_id": "32",
      "title": "Quantitative Personality Predictions from a Brief EEG Recording",
      "authors": [
        "W Li",
        "C Wu",
        "X Hu",
        "J Chen",
        "S Fu",
        "F Wang",
        "D Zhang"
      ],
      "year": "2020",
      "venue": "IEEE Trans. Affect. Comput",
      "doi": "10.1109/TAFFC.2020.3008775"
    },
    {
      "citation_id": "33",
      "title": "EEG Responses to Emotional Videos Can Quantitatively Predict Big-Five Personality Traits",
      "authors": [
        "W Li",
        "X Hu",
        "X Long",
        "L Tang",
        "J Chen",
        "F Wang",
        "D Zhang"
      ],
      "year": "2020",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "34",
      "title": "Neural Systems for Recognizing Emotion",
      "authors": [
        "R Adolphs"
      ],
      "year": "2002",
      "venue": "Curr. Opin. Neurobiol"
    },
    {
      "citation_id": "35",
      "title": "Decoding the Nature of Emotion in the Brain",
      "authors": [
        "P Kragel",
        "K Labar"
      ],
      "year": "2016",
      "venue": "Trends Cogn. Sci"
    },
    {
      "citation_id": "36",
      "title": "Measures of Emotion: A Review",
      "authors": [
        "I Mauss",
        "M Robinson"
      ],
      "year": "2009",
      "venue": "Cogn. Emot"
    },
    {
      "citation_id": "37",
      "title": "Mapping Discrete and Dimensional Emotions onto the Brain: Controversies and Consensus",
      "authors": [
        "S Hamann"
      ],
      "year": "2012",
      "venue": "Trends Cogn. Sci"
    },
    {
      "citation_id": "38",
      "title": "Transfer Learning for EEG-Based Brain-Computer Interfaces: A Review of Progress Made since 2016",
      "authors": [
        "D Wu",
        "Y Xu",
        "B.-L Lu"
      ],
      "year": "2020",
      "venue": "IEEE Trans. Cogn. Develop"
    },
    {
      "citation_id": "39",
      "title": "Domain-Adversarial Training of Neural Networks",
      "authors": [
        "Y Ganin",
        "E Ustinova",
        "H Ajakan",
        "P Germain",
        "H Larochelle",
        "F Laviolette",
        "M Marchand",
        "V Lempitsky"
      ],
      "year": "2016",
      "venue": "J. Mach. Learn. Res"
    },
    {
      "citation_id": "40",
      "title": "Cortical Response Similarities Predict Which Audiovisual Clips Individuals Viewed, but Are Unrelated to Clip Preference",
      "authors": [
        "D Bridwell",
        "C Roth",
        "C Gupta",
        "V Calhoun"
      ],
      "year": "2015",
      "venue": "PloS one"
    },
    {
      "citation_id": "41",
      "title": "Audience Preferences Are Predicted by Temporal Reliability of Neural Processing",
      "authors": [
        "J Dmochowski",
        "M Bezdek",
        "B Abelson",
        "J Johnson",
        "E Schumacher",
        "L Parra"
      ],
      "year": "2014",
      "venue": "Nat. Commun"
    },
    {
      "citation_id": "42",
      "title": "Correlated Components of Ongoing EEG Point to Emotionally Laden Attention -a Possible Marker of Engagement?",
      "authors": [
        "J Dmochowski",
        "S Paul",
        "D Joao",
        "L Parra"
      ],
      "year": "2012",
      "venue": "Front. Hum. Neurosci"
    },
    {
      "citation_id": "43",
      "title": "Intersubject Synchronization of Cortical Activity During Natural Vision",
      "authors": [
        "U Hasson",
        "Y Nir",
        "I Levy",
        "G Fuhrmann",
        "R Malach"
      ],
      "year": "2004",
      "venue": "Science"
    },
    {
      "citation_id": "44",
      "title": "Inter-Brain Synchronization During Coordination of Speech Rhythm in Human-to-Human Social Interaction",
      "authors": [
        "M Kawasaki",
        "Y Yamada",
        "Y Ushiku",
        "E Miyauchi",
        "Y Yamaguchi"
      ],
      "year": "2013",
      "venue": "Sci. Rep"
    },
    {
      "citation_id": "45",
      "title": "Inter-Brain Amplitude Correlation Differentiates Cooperation from Competition in a Motion-Sensing Sports Game",
      "authors": [
        "H Liu",
        "C Zhao",
        "F Wang",
        "D Zhang"
      ],
      "year": "2021",
      "venue": "Soc. Cogn. Affect. Neurosci"
    },
    {
      "citation_id": "46",
      "title": "Computational EEG Analysis for Hyperscanning and Social Neuroscience",
      "authors": [
        "D Zhang"
      ],
      "year": "2018",
      "venue": "Computational EEG Analysis"
    },
    {
      "citation_id": "47",
      "title": "Personalizing EEG-Based Affective Models with Transfer Learning",
      "authors": [
        "W.-L Zheng",
        "B.-L Lu"
      ],
      "year": "2016",
      "venue": "Int. Jt. Conf. Artif. Intell. (IJCAI)"
    },
    {
      "citation_id": "48",
      "title": "Domain Adaptation Via Transfer Component Analysis",
      "authors": [
        "S Pan",
        "I Tsang",
        "J Kwok",
        "Q Yang"
      ],
      "year": "2010",
      "venue": "IEEE Trans. Neural Networks"
    },
    {
      "citation_id": "49",
      "title": "Nonlinear Component Analysis as a Kernel Eigenvalue Problem",
      "authors": [
        "B Scholkopf",
        "A Smola",
        "K Muller"
      ],
      "year": "1998",
      "venue": "Neural Comput"
    },
    {
      "citation_id": "50",
      "title": "We Are Not All Equal: Personalizing Models for Facial Expression Analysis with Transductive Parameter Transfer",
      "authors": [
        "E Sangineto",
        "G Zen",
        "E Ricci",
        "N Sebe"
      ],
      "year": "2014",
      "venue": "Proc. ACM Int. Conf. Multimed. (MM)"
    },
    {
      "citation_id": "51",
      "title": "A Fast, Efficient Domain Adaptation Technique for Cross-Domain Electroencephalography (EEG)-Based Emotion Recognition",
      "authors": [
        "X Chai",
        "Q Wang",
        "Y Zhao",
        "Y Li",
        "D Liu",
        "X Liu",
        "O Bai"
      ],
      "year": "2017",
      "venue": "Sensors"
    },
    {
      "citation_id": "52",
      "title": "Unsupervised Visual Domain Adaptation Using Subspace Alignment",
      "authors": [
        "B Fernando",
        "A Habrard",
        "M Sebban",
        "T Tuytelaars"
      ],
      "year": "2014",
      "venue": "IEEE Int. Conf. on Comput. Vis. (ICCV)"
    },
    {
      "citation_id": "53",
      "title": "Unsupervised Domain Adaptation Techniques Based on Auto-Encoder for Non-Stationary EEG-Based Emotion Recognition",
      "authors": [
        "X Chai",
        "Q Wang",
        "Y Zhao",
        "X Liu",
        "O Bai",
        "Y Li"
      ],
      "year": "2016",
      "venue": "Comput. Biol. Med"
    },
    {
      "citation_id": "54",
      "title": "Plug-and-Play Domain Adaptation for Cross-Subject EEG-Based Emotion Recognition",
      "authors": [
        "L.-M Zhao",
        "X Yan",
        "B.-L Lu"
      ],
      "venue": "Proc. AAAI Conf. Artif. Intell. (AAAI)"
    },
    {
      "citation_id": "55",
      "title": "A Simple Framework for Contrastive Learning of Visual Representations",
      "authors": [
        "T Chen",
        "S Kornblith",
        "M Norouzi",
        "G Hinton"
      ],
      "year": "2020",
      "venue": "Int. Conf. Mach. Learn. (ICML)"
    },
    {
      "citation_id": "56",
      "title": "Bert: Pre-Training of Deep Bidirectional Transformers for Language Understanding",
      "authors": [
        "J Devlin",
        "M.-W Chang",
        "K Lee",
        "K Toutanova"
      ],
      "year": "2019",
      "venue": "Proc. Conf. North American Chapter of the Association for Computational Linguistics (NAACL)"
    },
    {
      "citation_id": "57",
      "title": "An Effective Self-Supervised Framework for Learning Expressive Molecular Global Representations to Drug Discovery",
      "authors": [
        "P Li",
        "J Wang",
        "Y Qiao",
        "H Chen",
        "Y Yu",
        "X Yao",
        "P Gao",
        "G Xie",
        "S Song"
      ],
      "year": "2021",
      "venue": "Brief. Bioinformatics"
    },
    {
      "citation_id": "58",
      "title": "Deep Geometric Representations for Modeling Effects of Mutations on Protein-Protein Binding Affinity",
      "authors": [
        "X Liu",
        "Y Luo",
        "P Li",
        "S Song",
        "J Peng"
      ],
      "year": "2021",
      "venue": "PLoS Comput. Biol"
    },
    {
      "citation_id": "59",
      "title": "Self-Supervised Learning: Generative or Contrastive",
      "authors": [
        "X Liu",
        "F Zhang",
        "Z Hou",
        "L Mian",
        "Z Wang",
        "J Zhang",
        "J Tang"
      ],
      "year": "2021",
      "venue": "IEEE Trans. Knowl. Data Eng"
    },
    {
      "citation_id": "60",
      "title": "Contrastive Representation Learning for Electroencephalogram Classification",
      "authors": [
        "M Mohsenvand",
        "M Izadi",
        "P Maes"
      ],
      "year": "2020",
      "venue": "Machine Learning for Health"
    },
    {
      "citation_id": "61",
      "title": "Uncovering the Structure of Clinical EEG Signals with Self-Supervised Learning",
      "authors": [
        "H Banville",
        "O Chehab",
        "A Hyvrinen",
        "D Engemann",
        "A Gramfort"
      ],
      "venue": "J. Neural Eng"
    },
    {
      "citation_id": "62",
      "title": "Representation Learning with Contrastive Predictive Coding",
      "authors": [
        "A Oord",
        "Y Li",
        "O Vinyals"
      ],
      "year": "2018",
      "venue": "Representation Learning with Contrastive Predictive Coding",
      "arxiv": "arXiv:1807.03748"
    },
    {
      "citation_id": "63",
      "title": "Measuring Shared Responses across Subjects Using Intersubject Correlation",
      "authors": [
        "S Nastase",
        "V Gazzola",
        "U Hasson",
        "C Keysers"
      ],
      "year": "2019",
      "venue": "Soc. Cogn. Affect. Neurosci"
    },
    {
      "citation_id": "64",
      "title": "Similar Brains Blend Emotion in Similar Ways: Neural Representations of Individual Difference in Emotion Profiles",
      "authors": [
        "X Hu",
        "F Wang",
        "D Zhang"
      ],
      "year": "2021",
      "venue": "NeuroImage"
    },
    {
      "citation_id": "65",
      "title": "Electric Fields of the Brain: The Neurophysics of EEG",
      "authors": [
        "P Nunez",
        "R Srinivasan"
      ],
      "year": "2006",
      "venue": "Electric Fields of the Brain: The Neurophysics of EEG"
    },
    {
      "citation_id": "66",
      "title": "Topographic Time-Frequency Decomposition of the EEG",
      "authors": [
        "T König",
        "F Marti-Lopez",
        "P Valdes-Sosa"
      ],
      "year": "2001",
      "venue": "NeuroImage"
    },
    {
      "citation_id": "67",
      "title": "Detection of Time-, Frequency-and Direction-Resolved Communication within Brain Networks",
      "authors": [
        "B Crouch",
        "L Sommerlade",
        "P Veselcic",
        "G Riedel",
        "B Schelter",
        "B Platt"
      ],
      "year": "2018",
      "venue": "Sci. Rep"
    },
    {
      "citation_id": "68",
      "title": "Deep Residual Learning for Image Recognition",
      "authors": [
        "K He",
        "X Zhang",
        "S Ren",
        "J Sun"
      ],
      "year": "2016",
      "venue": "Proc. IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit. (CVPR)"
    },
    {
      "citation_id": "69",
      "title": "Densely Connected Convolutional Networks",
      "authors": [
        "G Huang",
        "Z Liu",
        "L Van Der Maaten",
        "K Weinberger"
      ],
      "year": "2017",
      "venue": "Proc. IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit. (CVPR)"
    },
    {
      "citation_id": "70",
      "title": "EEGNet: A Compact Convolutional Network for EEG-Based Brain-Computer Interfaces",
      "authors": [
        "V Lawhern",
        "A Solon",
        "N Waytowich",
        "S Gordon",
        "C Hung",
        "B Lance"
      ],
      "year": "2016",
      "venue": "J. Neural Eng"
    },
    {
      "citation_id": "71",
      "title": "Fast and Accurate Deep Network Learning by Exponential Linear Units (Elus)",
      "authors": [
        "D.-A Clevert",
        "T Unterthiner",
        "S Hochreiter"
      ],
      "year": "2016",
      "venue": "Int. Conf. Learn. Represent. (ICLR)"
    },
    {
      "citation_id": "72",
      "title": "Xception: Deep Learning with Depthwise Separable Convolutions",
      "authors": [
        "F Chollet"
      ],
      "year": "2017",
      "venue": "Proc. IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit. (CVPR)"
    },
    {
      "citation_id": "73",
      "title": "EEG Correlates of Ten Positive Emotions",
      "authors": [
        "X Hu",
        "J Yu",
        "M Song",
        "C Yu",
        "F Wang",
        "P Sun",
        "D Wang",
        "D Zhang"
      ],
      "year": "2017",
      "venue": "Front. Hum. Neurosci"
    },
    {
      "citation_id": "74",
      "title": "Fnirs Evidence for Recognizably Different Positive Emotions",
      "authors": [
        "X Hu",
        "C Zhuang",
        "F Wang",
        "Y.-J Liu",
        "C.-H Im",
        "D Zhang"
      ],
      "year": "2019",
      "venue": "Front. Hum. Neurosci"
    },
    {
      "citation_id": "75",
      "title": "Humor Styles and Personality: A Meta-Analysis of the Relation between Humor Styles and the Big Five Personality Traits",
      "authors": [
        "A Mendiburo -Seguel",
        "D Páez",
        "F Martínez -Sánchez"
      ],
      "year": "2015",
      "venue": "Scand. J. Psychol"
    },
    {
      "citation_id": "76",
      "title": "The Nature of Emotion: Fundamental Questions",
      "authors": [
        "P Ekman",
        "R Davidson"
      ],
      "year": "1994",
      "venue": "The Nature of Emotion: Fundamental Questions"
    },
    {
      "citation_id": "77",
      "title": "Assessing the Effectiveness of a Large Database of Emotion-Eliciting Films: A New Tool for Emotion Researchers",
      "authors": [
        "A Schaefer",
        "F Nils",
        "X Sanchez",
        "P Philippot"
      ],
      "year": "2010",
      "venue": "Cogn. Emot"
    },
    {
      "citation_id": "78",
      "title": "Real-Time Movie-Induced Discrete Emotion Recognition from EEG Signals",
      "authors": [
        "Y.-J Liu",
        "M Yu",
        "G Zhao",
        "J Song",
        "Y Ge",
        "Y Shi"
      ],
      "year": "2017",
      "venue": "IEEE Trans. Affect. Comput"
    },
    {
      "citation_id": "79",
      "title": "Clearing the Mind: A Working Memory Model of Distraction from Negative Mood",
      "authors": [
        "L Van Dillen",
        "S Koole"
      ],
      "year": "2007",
      "venue": "Emotion"
    },
    {
      "citation_id": "80",
      "title": "FieldTrip: Open Source Software for Advanced Analysis of MEG, EEG, and Invasive Electrophysiological Data",
      "authors": [
        "R Oostenveld",
        "P Fries",
        "E Maris",
        "J.-M Schoffelen"
      ],
      "year": "2011",
      "venue": "FieldTrip: Open Source Software for Advanced Analysis of MEG, EEG, and Invasive Electrophysiological Data"
    },
    {
      "citation_id": "81",
      "title": "Robust Detrending, Rereferencing, Outlier Detection, and Inpainting for Multichannel Data",
      "authors": [
        "A De Cheveigné",
        "D Arzounian"
      ],
      "year": "2018",
      "venue": "NeuroImage"
    },
    {
      "citation_id": "82",
      "title": "An information-maximization approach to blind separation and blind deconvolution",
      "authors": [
        "A Bell",
        "T Sejnowski"
      ],
      "year": "1995",
      "venue": "Neural Comput"
    },
    {
      "citation_id": "83",
      "title": "A new learning algorithm for blind signal separation",
      "authors": [
        "S Amari",
        "A Cichocki",
        "H Yang"
      ],
      "year": "1995",
      "venue": "Adv. Neural Inf. Process. Syst. (Neu-rIPS)"
    },
    {
      "citation_id": "84",
      "title": "Cross-Subject EEG-Based Emotion Recognition through Neural Networks with Stratified Normalization",
      "authors": [
        "J Fdez",
        "N Guttenberg",
        "O Witkowski",
        "A Pasquali"
      ],
      "year": "2021",
      "venue": "Front. Neurosci"
    },
    {
      "citation_id": "85",
      "title": "Adam: A Method for Stochastic Optimization",
      "authors": [
        "D Kingma",
        "J Ba"
      ],
      "year": "2015",
      "venue": "Int. Conf. Learn. Represent. (ICLR)"
    },
    {
      "citation_id": "86",
      "title": "Sgdr: Stochastic Gradient Descent with Warm Restarts",
      "authors": [
        "I Loshchilov",
        "F Hutter"
      ],
      "year": "2016",
      "venue": "Int. Conf. Learn. Represent. (ICLR)"
    },
    {
      "citation_id": "87",
      "title": "Improved Epileptic Seizure Detection Combining Dynamic Feature Normalization with EEG Novelty Detection",
      "authors": [
        "J Bogaarts",
        "D Hilkman",
        "E Gommer",
        "V Van Kranen-Mastenbroek",
        "J Reulen"
      ],
      "year": "2016",
      "venue": "Med. Biol. Eng. Comput"
    },
    {
      "citation_id": "88",
      "title": "Dynamic Feature Scaling for Online Learning of Binary Classifiers",
      "authors": [
        "D Bollegala"
      ],
      "year": "2017",
      "venue": "Knowl. Based. Syst"
    },
    {
      "citation_id": "89",
      "title": "Adaptive Normalization: A Novel Data Normalization Approach for Non-Stationary Time Series",
      "authors": [
        "E Ogasawara",
        "L Martinez",
        "D Oliveira",
        "G Zimbrão",
        "G Pappa",
        "M Mattoso"
      ],
      "year": "2010",
      "venue": "Int. Jt. Conf. Neural Netw. (IJCNN)"
    },
    {
      "citation_id": "90",
      "title": "Rectified Linear Units Improve Restricted Boltzmann Machines",
      "authors": [
        "V Nair",
        "G Hinton"
      ],
      "year": "2010",
      "venue": "Int. Conf. Mach. Learn. (ICML)"
    },
    {
      "citation_id": "91",
      "title": "Correlated Components Analysis -Extracting Reliable Dimensions in Multivariate Data",
      "authors": [
        "L Parra"
      ],
      "venue": "Correlated Components Analysis -Extracting Reliable Dimensions in Multivariate Data",
      "arxiv": "arXiv:1801.08881"
    },
    {
      "citation_id": "92",
      "title": "Axiomatic attribution for deep networks",
      "authors": [
        "M Sundararajan",
        "A Taly",
        "Q Yan"
      ],
      "year": "2017",
      "venue": "Int. Conf. Mach. Learn. (ICML)"
    },
    {
      "citation_id": "93",
      "title": "On the Interpretation of Weight Vectors of Linear Models in Multivariate Neuroimaging",
      "authors": [
        "S Haufe",
        "F Meinecke",
        "K Görgen",
        "S Dähne",
        "J.-D Haynes",
        "B Blankertz",
        "F Bießmann"
      ],
      "year": "2014",
      "venue": "Neuroimage"
    },
    {
      "citation_id": "94",
      "title": "Brainstorm: A User-Friendly Application for MEG/EEG Analysis",
      "authors": [
        "F Tadel",
        "S Baillet",
        "J Mosher",
        "D Pantazis",
        "R Leahy"
      ],
      "year": "2011",
      "venue": "Comput. Intell. Neurosci"
    },
    {
      "citation_id": "95",
      "title": "Open-MEEG: Opensource Software for Quasistatic Bioelectromagnetics",
      "authors": [
        "A Gramfort",
        "T Papadopoulo",
        "E Olivi",
        "M Clerc"
      ],
      "year": "2010",
      "venue": "Open-MEEG: Opensource Software for Quasistatic Bioelectromagnetics"
    },
    {
      "citation_id": "96",
      "title": "Symmetric Atlasing and Model Based Segmentation: An Application to the Hippocampus in Older Adults",
      "authors": [
        "G Grabner",
        "A Janke",
        "M Budge",
        "D Smith",
        "J Pruessner",
        "D Collins"
      ],
      "year": "2006",
      "venue": "Med. Image Comput. Comput. Assist. Interv. (MICCAI)"
    },
    {
      "citation_id": "97",
      "title": "Standardized Low-Resolution Brain Electromagnetic Tomography (Sloreta): Technical Details",
      "authors": [
        "R Pascual-Marqui"
      ],
      "year": "2002",
      "venue": "Methods Find. Exp. Clin. Pharmacol"
    },
    {
      "citation_id": "98",
      "title": "Affect as a Psychological Primitive",
      "authors": [
        "L Barrett",
        "E Bliss-Moreau"
      ],
      "year": "2009",
      "venue": "Adv. Exp. Soc. Psychol"
    },
    {
      "citation_id": "99",
      "title": "The Brain Basis of Positive and Negative Affect: Evidence from a Meta-Analysis of the Human Neuroimaging Literature",
      "authors": [
        "K Lindquist",
        "A Satpute",
        "T Wager",
        "J Weber",
        "L Barrett"
      ],
      "year": "2016",
      "venue": "Cereb. Cortex"
    },
    {
      "citation_id": "100",
      "title": "A Bayesian Model of Category-Specific Emotional Brain Responses",
      "authors": [
        "T Wager",
        "J Kang",
        "T Johnson",
        "T Nichols",
        "A Satpute",
        "L Barrett"
      ],
      "year": "2015",
      "venue": "PLoS Comput. Biol"
    },
    {
      "citation_id": "101",
      "title": "Emotion and Aging: Evidence from Brain and Behavior",
      "authors": [
        "N Ebner",
        "H Fischer"
      ],
      "year": "2014",
      "venue": "Front. Psychol"
    },
    {
      "citation_id": "102",
      "title": "Evaluative Ratings and Attention across the Life Span: Emotional Arousal and Gender",
      "authors": [
        "V Ferrari",
        "N Bruno",
        "R Chattat",
        "M Codispoti"
      ],
      "year": "2017",
      "venue": "Cogn. Emot"
    }
  ]
}