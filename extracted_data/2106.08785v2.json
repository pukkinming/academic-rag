{
  "paper_id": "2106.08785v2",
  "title": "Seover: Sentence-Level Emotion Orientation Vector Based Conversation Emotion Recognition Model",
  "published": "2021-06-16T13:44:03Z",
  "authors": [
    "Zaijing Li",
    "Fengxiao Tang",
    "Tieyu Sun",
    "Yusen Zhu",
    "Ming Zhao"
  ],
  "keywords": [
    "Conversation emotion recognition",
    "Pre-trained language model",
    "Emotion vector"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "In this paper, we propose a new expression paradigm of sentence-level emotion orientation vector to model the potential correlation of emotions between sentence vectors. Based on it, we design an emotion recognition model referred to as SEOVER, which extracts the sentence-level emotion orientation vectors from the pre-trained language model and jointly learns from the dialogue sentiment analysis model and extracted sentence-level emotion orientation vectors to identify the speaker's emotional orientation during the conversation. We conduct experiments on two benchmark datasets and compare them with the five baseline models. The experimental results show that our model has better performance on all data sets.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Conversation emotion recognition (CER) refers to the process of identifying the speaker's emotions through text, audio, and visual information in the process of two or more persons' conversations. Nowadays, conversation emotion recognition tasks are widely used in social media such as Twitter and Facebook.\n\nRecent work of conversation emotion recognition mainly focuses on speaker identification and conversation relationship modeling  (Majumder et al. 2019 ;  Ghosal et al. 2019 ). However, these proposals use CNN  (Kim,2014)  to encode the utterances, which can not express the grammatical and semantic features of the utterance well and lead to inaccurate emotion identification. To mitigate this issue, some works try to employ the BERT  (Devlin et al. 2019 ) model with improved semantic features extraction ability to encode the sentences (Yuzhao  Mao et al. 2020 ; Jiangnan  Li et al. 2020 ). The proposal achieves good results, however, the proposed BERT-based CER does not fully extract the correlation of emotional tendency between sentences especially when the emotion turns in sudden. As shown in Figure  1 , we encode the first sentence of the first dialogue of the MELD dataset with CNN and BERT models respectively to obtain a 600-dimensional vector, and normalize the data of each dimension so that its value is a continuous number within (0,1). Then, a hot zone map is leveraged to show the difference between the feature maps encoded with the two models. It is obvious that trends of the feature maps are totally opposite. It seems that those encoding methods and corresponding feature maps lose some important features of the sentences and can't represent the emotional tendency of the utterance well. So, we propose a new utterance representation vector referred to as the sentence-level emotion orientation vector (SEOV) to further represent the potential emotion correlation of sentences. The SEOV represents the emotional intensity of sentence encoding vector with its \"size\", and models the emotion tendency between vectors with its \"direction\", which is as shown in Figure  2 .\n\nBased on the proposed SEOV, we further propose a sentence-level emotion orientation vector based conversation emotion recognition model (SEOVER) to encode and decode the utterances of dialogue, and get the emotions of each speaker. In the model, we employ an improved transformer called as transformeremo to extract the SEOV and then jointly use the SEOV and dialogue sentiment analysis model (DSAM) to obtain the contextual semantic information. With continuous fine-tune, we finally get the speaker's emotion classification result. Fig.  2 . The schematic diagram of SEOV, where the x-axis represents the dimension of the vector, y-axis represents the value of each dimension of the vector, and z-axis represents different sentence-level emotion orientation vectors. There is a correlation between different emotion vectors, indicating the \"direction\" of the SEOV.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Methodology",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Problem Definition",
      "text": "Given a conversation U : U 1 , U 2 , U 3 , ...U n , N is the number of conversations, the speaker's utterance is represented by the function P t (U i ), where t is the t-th speaker. Our task is to input each utterance U i and to get its correct classification result in the emotional label set L: l 1 , l 2 , l 3 , ...l m , where M is the number of types of emotional label.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Model",
      "text": "As shown in the Figure  3 , our model is divided into three parts: Sentence-level encoder, emotion-level encoder and context modeling. Sentence-level Encoder: Since the pre-trained language model can't directly encode the dialogue, we at first split the conversation U n into a series of single sentences :s 1 , s 2 , s 3 ...s n . Then, we input them into an improved pre-trained language model called transformer-emo model, which can map the representation of utterance to the sentence vectors Q:\n\nwhere Q is a series of sentence vectors :q 1 , q 2 , q 3 ...q n , the length of each sentence vector is set as d (d is set to 786 in our experiment).  transformer can obtain more syntactic and semantic information. In order to better adapting to our model, we improve the transformer model by adjusting the output form (referred to as transformer-emo) to obtain the sentence vectors of the utterance.\n\nEmotion-level Encoder: As for sentence vectors, the length of the sentence itself, semantic information, symbols, etc. are the \"size\" of the vector, and its classification attributes are the \"direction\" of the vector. The sentence representation obtained by encoding the sentence in the existing method is not a \"vector\" in the true sense, because it only contains the \"size\" without the \"direction\". So we propose a new expression paradigm, SEOV, which represents the emotional intensity of sentence vector with its \"size\", and models the emotion tendency between emotion vectors with its \"direction\". We map the vector q from the k-dim space to the k*-dim space to obtain the emotion vector q * , to achieve emotions representation:\n\nwhere w 1 , w 2 , . . . , w k * is the weight parameter.\n\nIn theory, when the spatial dimension after the mapping is equal to the number of categories, we can regard it as the classification result. The elements in the emotion vector q * represent the probabilities of classified emotions in each sentence.\n\nThen we merge the original vector q and the obtained emotion vector q * to obtain the sentence-level emotion orientation vector e:\n\nThe e is called as SEOV, in which, the emotion's direction is represented in the collection of SEOVs as shown in Fig.  2 . With the final DSAM, the \"direction\" of the emotion tendencies can be efficiently extracted and achieves better emotion recognition performance. Context Modeling: Finally, we reassemble SEOVs into dialogue lists, and input them into the DSAM. The existing DSAMs can distinguish the speaker and obtain the speaker state, so it can fully obtain the context information and context. We choose DialogueRNN, DialogueGCN, and bc-LSTM models as the benchmark version of the dialogue emotion analysis models to compare the test results.\n\n3 Experimental Setting BERT is a pre-trained language model that can be fine-tuned to achieve good results in downstream tasks. In this article, we use BERT to classify the sentiment of a single sentence text and compare it with our model.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Results And Analysis",
      "text": "We compare our model with all baselines on the MELD and IEMOCAP datasets and get the experimental results in Tables 1. As expected, our proposal outperforms other baseline models.\n\nTable  1 . Experimental results(F1 score) on the IEMOCAP dataset and MELD dataset. SEOVER-RNN represents the result of using DialogueRNN as the fine-tuning model, SEOVER-GCN represents the result of using DialogueGCN as the fine-tuning model, and SEOVER-LSTM represents the result of using bc-LSTM as the fine-tuning model.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Model",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Comparison With Baseline Models",
      "text": "Compared with the state-of-the-art conversation emotion recognition models, our proposal achieves better performance. This is cause by two reasons. Firstly, our model can obtain more syntactic and semantic features of utterances' presentation by using transformer-emo which is a transformer based pretraining model. Secondly, the proposed SEOV can efficiently map the emotion orientations between sentence vectors.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Ablation Study",
      "text": "In order to study the influence of the fusion of SEOV on the experimental results, we remove the emotion vectors in the SEOVER to compare the emotion recognition performance with benchmark model of DialogueRNN on the MELD dataset. As shown in Table  2 , unsurprisingly, the accuracy and F1 score of the model without emotion vectors are much lower. The comparison illustrates the importance of the emotion tendency encoding in SEOV for the conversation emotion recognition.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Error Analysis",
      "text": "We analyze the confusion matrix on the MELD data set. As shown in Table  3 , assuming \"sad\" and \"joy\" are opposite emotions, the count of the misjudge between \"sad\" an \"joy\" is relatively small. On the other hand, the \"sad\" and \"angry\" are a pair of adjacent emotions. The misjudge count of this pair adjacent is relatively high. It is not difficult to find that the fusion of emotion vectors is a double-edged sword, which improves the classification accuracy of flipped emotions, but slightly reduced the classification accuracy of adjacent emotions. Therefore, in future research, we will focus on improving the performance of the adjacent emotions classification.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Conclusion",
      "text": "In this work, we propose a new paradigm of sentence-level emotion orientation vector(SEOV) to assist the emotion recognition, which solves the discourse representation information loss problem of conventional methods in the CER. Then, we designed a conversation emotion recognition model based on the SEOV called as SEOVER. It uses the Transformer-emo model to encode sentence vectors and emotion vectors containing emotion tendency information and fuses them as SEOV. Then, the SEOV is leveraged as input to active the final dialogue emotion analysis model to classify the speaker's emotions. We conducted comparative experiments of the proposal with several benchmarks on both the MELD and IEMOCAP dataset. The experimental results prove that the proposed SEOVER outperforms the state-of-the-art methods.",
      "page_start": 7,
      "page_end": 7
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: , we encode the ﬁrst sentence of the ﬁrst dialogue",
      "page": 2
    },
    {
      "caption": "Figure 1: (a) Comparison of the value of each dimension of the sentence vector get by",
      "page": 2
    },
    {
      "caption": "Figure 2: Based on the proposed SEOV, we further propose a sentence-level emotion",
      "page": 2
    },
    {
      "caption": "Figure 2: The schematic diagram of SEOV, where the x-axis represents the dimension",
      "page": 3
    },
    {
      "caption": "Figure 3: , our model is divided into three parts: Sentence-level",
      "page": 3
    },
    {
      "caption": "Figure 3: Our model’s structure.",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table 2: , unsurprisingly, the accuracy and F1 score of the",
      "data": [
        {
          "Model": "",
          "IEMOCAP": "Happy",
          "MELD": ""
        },
        {
          "Model": "DialogueRNN\nDialogueGCN\nDialogXL\nTRMSM\nbc-LSTM\nBERT",
          "IEMOCAP": "33.18\n42.75\n-\n50.22\n43.40\n-",
          "MELD": "55.90\n-\n62.41\n62.36\n55.90\n60.34"
        },
        {
          "Model": "SEOVER-RNN\nSEOVER-GCN",
          "IEMOCAP": "69.47\n53.85\nSEOVER-LSTM 70.38 85.65",
          "MELD": "65.66\n-\n63.82"
        }
      ],
      "page": 6
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Neural machine translation by jointly learning to align and translate",
      "authors": [
        "D Bahdanau",
        "K Cho",
        "Y Bengio"
      ],
      "year": "2014",
      "venue": "Computer Science"
    },
    {
      "citation_id": "2",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "3",
      "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "J Devlin",
        "M Chang",
        "K Lee",
        "K Toutanova"
      ],
      "year": "2019",
      "venue": "Bert: Pre-training of deep bidirectional transformers for language understanding"
    },
    {
      "citation_id": "4",
      "title": "Cosmic: Commonsense knowledge for emotion identification in conversations",
      "authors": [
        "D Ghosal",
        "N Majumder",
        "A Gelbukh",
        "R Mihalcea",
        "S Poria"
      ],
      "year": "2020",
      "venue": "EMNLP"
    },
    {
      "citation_id": "5",
      "title": "Dialoguegcn: A graph convolutional neural network for emotion recognition in conversation",
      "authors": [
        "D Ghosal",
        "N Majumder",
        "S Poria",
        "N Chhaya",
        "A Gelbukh"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing"
    },
    {
      "citation_id": "6",
      "title": "Icon: interactive conversational memory network for multimodal emotion detection",
      "authors": [
        "D Hazarika",
        "S Poria",
        "R Mihalcea",
        "E Cambria",
        "R Zimmermann"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 conference on empirical methods in natural language processing"
    },
    {
      "citation_id": "7",
      "title": "Conversational memory network for emotion recognition in dyadic dialogue videos",
      "authors": [
        "D Hazarika",
        "S Poria",
        "A Zadeh",
        "E Cambria",
        "L Morency",
        "R Zimmermann"
      ],
      "year": "2018",
      "venue": "Proceedings of the conference Association for Computational Linguistics North American Chapter Meeting"
    },
    {
      "citation_id": "8",
      "title": "Conversational transfer learning for emotion recognition",
      "authors": [
        "D Hazarika",
        "S Poria",
        "R Zimmermann",
        "R Mihalcea"
      ],
      "year": "2021",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "9",
      "title": "Convolutional neural networks for sentence classification",
      "authors": [
        "Y Kim"
      ],
      "year": "2014",
      "venue": "Convolutional neural networks for sentence classification",
      "arxiv": "arXiv:1408.5882"
    },
    {
      "citation_id": "10",
      "title": "A hierarchical transformer with speaker modeling for emotion recognition in conversation",
      "authors": [
        "J Li",
        "Z Lin",
        "P Fu",
        "Q Si",
        "W Wang"
      ],
      "year": "2020",
      "venue": "A hierarchical transformer with speaker modeling for emotion recognition in conversation",
      "arxiv": "arXiv:2012.14781"
    },
    {
      "citation_id": "11",
      "title": "Dialoguernn: An attentive rnn for emotion detection in conversations",
      "authors": [
        "N Majumder",
        "S Poria",
        "D Hazarika",
        "R Mihalcea",
        "A Gelbukh",
        "E Cambria"
      ],
      "year": "2019",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "12",
      "title": "Dialoguetrm: Exploring the intra-and inter-modal emotional behaviors in the conversation",
      "authors": [
        "Y Mao",
        "Q Sun",
        "G Liu",
        "X Wang",
        "W Gao",
        "X Li",
        "J Shen"
      ],
      "year": "2020",
      "venue": "Dialoguetrm: Exploring the intra-and inter-modal emotional behaviors in the conversation",
      "arxiv": "arXiv:2010.07637"
    },
    {
      "citation_id": "13",
      "title": "Context-dependent sentiment analysis in user-generated videos",
      "authors": [
        "S Poria",
        "E Cambria",
        "D Hazarika",
        "N Majumder",
        "A Zadeh",
        "L Morency"
      ],
      "year": "2017",
      "venue": "Proceedings of the 55th annual meeting of the association for computational linguistics"
    },
    {
      "citation_id": "14",
      "title": "Meld: A multimodal multi-party dataset for emotion recognition in conversations",
      "authors": [
        "S Poria",
        "D Hazarika",
        "N Majumder",
        "G Naik",
        "E Cambria",
        "R Mihalcea"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "15",
      "title": "Dialogxl: All-in-one xlnet for multi-party conversation emotion recognition",
      "authors": [
        "W Shen",
        "J Chen",
        "X Quan",
        "Z Xie"
      ],
      "year": "2020",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "16",
      "title": "Attention is all you need",
      "authors": [
        "A Vaswani",
        "N Shazeer",
        "N Parmar",
        "J Uszkoreit",
        "L Jones",
        "A Gomez",
        "L Kaiser",
        "I Polosukhin"
      ],
      "year": "2017",
      "venue": "Attention is all you need"
    }
  ]
}