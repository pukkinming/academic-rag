{
  "paper_id": "2205.09791v1",
  "title": "A Peek At Peak Emotion Recognition",
  "published": "2022-05-19T18:23:24Z",
  "authors": [
    "Tzvi Michelson",
    "Hillel Aviezer",
    "Shmuel Peleg"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Despite much progress in the field of facial expression recognition, little attention has been paid to the recognition of peak emotion. Aviezer et al.  [1]  showed that humans have trouble discerning between positive and negative peak emotions. In this work we analyze how deep learning fares on this challenge. We find that (i) despite using very small datasets, features extracted from deep learning models can achieve results significantly better than humans. (ii) We find that deep learning models, even when trained only on datasets tagged by humans, still outperform humans in this task.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "Aviezer et al.  [1]  discovered that the human ability to distinguish between positive and negative facial expressions in day-to-day situations, does not generalize well to peak (extreme) emotions, such as sports players winning or losing a point. When the emotions are extreme (peak emotions), positive and negative emotions share characteristics such that the distinction becomes significantly harder. In their work  [1] , they gather pictures of tennis players winning or losing a point and show the pictures to human evaluators. In one of their settings they show images of only the face, with the body cropped out. In another they show images of only the body, with the face cropped out. In the third setting they show the images as they were taken. They discovered that human raters succeed in distinguishing between winning and losing a point only when they can see the body of the player. However, the peak facial expressions the players display are harder to read. This finding is supported by brain imaging work which finds that the same areas in the brain such as: the insula, striatum, orbitofrontal cortex, and amygdala are activated when experiencing both positive and negative emotions  [2] ,  [3] ,  [4] ,  [5] . In this work we study whether neural networks can succeed in correctly classifying the images of the tennis players' faces, despite humans failing. We find that with very little training, features extracted from neural network models are able to predict the classification of the face at a significantly higher accuracy then humans.\n\nThis work is different from most works in the field of Facial Emotion Recognition (FER) in a number of ways:\n\n• First of all, most efforts in the field of FER have relied upon datasets with human annotations  [6] . The common datasets include either a categorical classification to a specific emotion (angry, sad, happy etc.) or a score on 2 or 3 dimensions: valence, arousal and sometimes domination. The weakness of these datasets is that they rely on human annotations and are thus limited by them.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Related Work",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "A. Automatic Facial Expression Recognition",
      "text": "The field of automatic FER can be generally split into 2 main categories: discrete and continuous. The first people to identify discrete, universal emotions were Ekman and Friesen  [7]  who identified 5 discrete emotion: happiness, sadness, anger, surprise, disgust and fear. They claimed these emotions had universal facial expression, consistent across cultures. Contempt was later added as another emotion with universal facial expressions  [8] . Although recent research has cast doubt on the universality of the the expressions  [9] , most works still use this framework for the FER task. A number of datasets have been gathered and manually annotated with these emotions. Examples of such datasets include:\n\n• CK+ [10] -contains multiple series of images in which the face changes from neutral to expressive\n\n• FER2013  [11]  -contains 35,000 individual images each tagged with one of seven emotions\n\n• AffectNet  [12]  -contains over a million images obtained by querying search engines for emotion-related tags, 450,000 of the images have manually annotated labels for eight basic emotions and other less well known datasets.\n\nThe second category is continuous affect recognition. In this framework facial expressions are placed on a 2-dimensional grid where one axis represents arousal (relaxed vs. aroused) and the other valence (pleasant vs. unpleasant)  [13] . Datasets annotated according to this model include: SEWA  [14]  which contains over 2000 minutes of hand annotated audio-visual data and Facial Affect \"in-the-wild\"  [15]  which gathered data from 500 youtube videos and manually annotated them with regard to valence and arousal.\n\nBoth of these categories were originally analyzed using classical methods such as: Local Binary Patterns (LBP)  [16] , LBP on three orthogonal planes  [17] , non-negative matrix factorization (NMF)  [18]  and sparse learning  [19] . Since 2013 emotion recognition competitions have collected sufficient training data from challenging real-world scenarios, which implicitly promote the transition of FER from lab-controlled to in-the-wild settings. Additionally, deep learning methods provided the tools to utilize these datasets and have achieved state-of-the-art results  [20] ,  [21] . Their methods involve using a model pretrained on VGGFace or Imagenet and then fine tuned with FER specific datasets. The backbone model is usually convolutional and is heavily influenced by state of the art Facial Recognition models  [6] .",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "B. Pose And Gesture Emotion Recognition",
      "text": "Pose and Gesture Emotion Recognition is significantly less studied then FER  [22] . Here, in a similar fashion to FER, emotions are either defined categorically where the goal is to classify the pose to the correct emotion, or continuously, where every pose is given a score on a few continuous scales. Unlike FER, very little work has been done to use deep learning for Pose and Gesture Emotion Recognition  [22] . Also, the datasets here are less standardized, and most works use data they gathered themselves.\n\nUsing classical methods, Saha et al.  [23]  investigated gestures reflecting five basic human emotional states from skeletal geometrical features. They compared a variety of machine learning classifiers and obtained best results using ensemble trees. They gathered their data by stimulating the desired emotion in their study participants while recording the motions and gestures with a Microsoft Kinect (depth sensor). Fourati and Pelachaud  [24]  proposed a different approach to analyse the emotional meaning of movements by using a range of features from the whole body. They described movement on an anatomical, directional and posture level. Using these descriptors, they used a Random Forest classifier to classify the movements to one of eight emotional states expressed by actors in various day-to-day actions such as walking, sitting etc. Moving on to deep methods, Kosti et al.  [25]  presented a method for emotion recognition based on images containing people in non-controlled environments. They trained a two low-rank filter CNN that jointly analysed the person and the whole scene to recognize the emotional state. The analysed images depicted people annotated with 26 emotional categories as well as the continuous dimensions: valence, arousal, and dominance. In their research they emphasized the importance of context for recognizing people's emotions in images. A few works have used Body Pose in multimodal affect recognition together with FER  [26]  or audio-based affect recognition  [27] .",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Iii. Data",
      "text": "The data used in this paper consists of 176 images of players in tennis matches. 88 of the images depict a player winning a point and 88 depict a player losing a point. The images were obtained using a google image search and the queries \"reacting to a winning point\" or \"reacting to a losing point\", crossed with \"tennis\". The gender distribution for the winning images is 41 male vs 43 female, and for the losing images is 45 male vs 39 female. Although some of the tennis players were more popular then others there are 71 unique tennis players losing a point and 75 unique tennis players winning a point.  For every model, the table describes it's architecture, the data it was pretrained on and the ROC AUC achieved on our data, using 5-fold cross validation. In the case of the FER models we examined the options of (i) using the output of the models directly i.e. happy is winning and sad is losing. This option is called \"FER Model -Predictions\". (ii) Extracting features on which to train a classifier, this option is called \"FER Models -Features\".\n\nRemoving the tennis players appearing more then once did not significantly affect the results of the analysis.\n\nIn the original analysis by Aviezer et al.  [1]  the faces were cropped by hand, in our work we used an automatic face detector  [28]  since deep learning models require a rectangular image to function properly. Images chosen by journalists to depict losing or winning a point typically contain the image representing the peak emotion  [1] . This has been confirmed with a manual analysis of randomly sampled professional tennis matches from the finals of Wimbledon and Australia open (full matches were captured from YouTube)  [1] . The focus was put on the most critical emotional peak of the events: the immediate response to winning the final match points. The faces of the winners/losers were essentially neutral immediately before the final serve of the game. However, upon winning/losing the match, a spike in facial movement was observed. The facial expressions of the winners replicated the type of reactions found in the image search. Importantly, these intense expressions reached their peak less than 1000ms after the onset of the winning/losing. The rapid elicitation of the expression immediately after the emotional event confirms that these expressions indeed reflect a peak experience.\n\nIt was found  [1]  that when examining images in this dataset humans had a very difficult time distinguishing between faces of winning and losing players, in cases where only the faces were shown to them (ROC Auc 0.52). At the same time, when only the body of the player was shown to the participants they achieved significantly better results (ROC Auc 0.93). In the next section we will describe our proposed methods to automatically analyze the players' expressions.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Iv. Modality I -Face",
      "text": "Due to lack of data it was impossible to train a model from scratch, so we made extensive use of pretrained models. In all cases features were extracted from the pretrained model and a simple Machine Learning (ML) classifier was trained on the extracted features with heavy regularization. Also, in all cases which involved training the evaluation was done using 5-fold cross validation to calculate the ROC Auc. Previous research has shown that embedded in the faces there are clues to the quality of the emotion  [29] , here we attempt to see how extensive these clues are and how well neural networks can utilize them.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "A. Fr -Face Recognition Models",
      "text": "In this stage we used 3 different face recognition models each trained on VGGFace2  [30] . The VGGFace2 dataset contains 3.31 million images of 9131 subjects, with an average of 362.6 images per subject. Images were downloaded from Fig.  2 : Images of the tennis players including body pose. The cropped faces can be found in figure  1 . In this scenario it is significantly easier to discern which images are of losing players and which are of winning players.\n\nGoogle Image Search and have large variations in pose, age, illumination, ethnicity and profession (e.g. actors, athletes, politicians etc.). The models we used are a VGG16  [31]  from which we extracted a 512-d feature vector, a Resnet50  [32]  from which we extracted a 2048-d feature vector and a SEnet50  [33]  from which we extracted a 2048-d feature vector. The SENet50 is a variation of the Resnet which includes squeeze-excite blocks. These blocks allow global information to impact the learning after every convolutional block. After experimenting with a number of classifiers we trained a Logistic Regression classifier on the extracted vectors and used 5-fold cross validation to calculate ROC AUC. The results can be found in Table  I . We used face recognition models since they have been shown to extract good representations of the face  [34]  and have been used in a variety of other tasks which require such representations  [35] ,  [34] .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "B. Fer -Facial Emotion Recognition Models",
      "text": "Here, we experimented with networks which were essentially trained for the precise task we are trying to perform -FER. We use a VGG13 network trained by Barsoum et al.  [36]  and a Resnet18 network trained by Wen et al.  [37] . The networks are trained on 4 different datasets: FER+  [38] , Affectnet7  [12] , Affectnet8  [12]  and rafdb  [39] . We performed our experiments in 2 parts.\n\n1) In the first part we took the output of the network before the softmax, for the emotions happy and sad and inputted just the two scores into the softmax function. Thus, we could essentially see what are the raw predictions of the network for this face. This part achieved results better then human performance when using the FER+ dataset  [38] . This is of interest since the dataset FER+  [36]  was tagged by humans, meaning the network is essentially learning to approximate the human evaluation function, and yet still when predicting on peak emotion the network outperforms humans. 2) In the second part we extracted a feature vector from the network and trained a classifier on the feature vector, we experimented with a number of classifiers and found that usually Logistic Regression works best. A summary of the results can be found in Table  I .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "C. Implementation Details",
      "text": "In all cases where the features were extracted from a network they were then fed into a classical ML classifier. We experimented with the following classifiers: Random Forest, Logistic regression, K-nearest neighbors and SVM. A randomized grid search was run over the following hyperparameters: Regularization type (L2, L1 or elasticnet, for Logistic Regression), Regularization Coefficient, Kernel (polynomial or linear, for SVM), tree depth (for random forest) and number of neighbors (for K-nearest neighbors). The same methodology was used for body pose as well.\n\nV. MODALITY II -BODY POSE Fig.  3 : Visualizations of full body pose coordinates vs. only hand coordinates.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "A. Full Body Pose",
      "text": "An additional input that was studied by Aviezer et al.  [1]  is the body pose. In their experiments subjects were able to distinguish between winners and losers using body pose alone. Additionally, they found that the combination of facial expressions with body pose deteriorated performance slightly.\n\nIn our experiments we found, that models are less successful then humans. We used 2 different pre-trained models  [40] ,  [41] ,  [42]  which extracted 3D coordinates for the main body keypoints and then we trained a classifier on these keypoints to predict whether they came from a winning player or a  losing player. The first model we used is OpenPose  [40] ,  [41] . OpenPose works in two steps. In step 1 it identifies different body points within the image, as well as the fields connecting the body points. In the second step it follows a greedy inference process for connecting the keypoints through said fields. The second model we use, Detectron2, uses He et al.'s mask RCNN  [43]  to output a one hot image size filter for every keypoint of every person detected in the image, and uses a whole human detector to connect the keypoints. The 2D coordinates outputted by Detectron2 are then fed to another network which parses them and coverts them to 3D coordinates. A K-nearest neighbors classifier worked best for these features. Our results can be found in Table  II .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "B. Hand Pose",
      "text": "A surprising weakness of most models which extract body pose is that their analysis is limited to major body parts i.e. arms, legs, head, shoulders and hips while the fingers and hand are ignored. A visualization of this can be seen in the left side of Figure  3 . When examining our images we noticed that winning players often fist pumped. This additional information is intuitively utilized by human viewers but can not be used by our model which is unaware of the finger locations. Thus we used Google's MediaPipe  [44]  framework to extract hand keypoints and then trained a classifier on these keypoints to predict whether the player lost or won a point. Our results show that indeed the hand pose provide more information then the general body pose, and in fact almost all the information which can be extracted from the body pose allready exists in the hand pose. The quantitative results can be seen in Table  II .",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Vi. Combining Different Modalities",
      "text": "To estimate the total ability of models in this task we experimented with the combination of different modalities including Facial Recognition + Facial Emotion Recognition + Body Pose + Hand Pose. The combination was generally done by averaging across the best performing results from previous stages as this achieved better results then training a classifier on the concatenated features. As can be seen in Table  II  we found that most of the information models can capture from the body and hand pose is allready captured from the face. It is noteworthy that in the case of body and hand emotion recognition we were unable to find models trained specifically for this task and as such unable to extract domainrelevant features. This is likely the cause of the gap between the humans' and the models' performance.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "A. Alignment",
      "text": "To further analyze our model we tested it's alignment. What we were trying to understand is whether the model is more correct when it is more confident in it's prediction. To test this, we calculate the ROC Auc scores while only taking the samples where the model's confidence is above a certain threshold. We run i from 50 to 75, and only take the samples with a score greater then i (high confidence in a positive score) or lower the 100 -i (high confidence in a negative score):\n\nf or i in range(50, 75) :\n\nFor each iteration we calculate the ROC Auc score and plot them. We do not go above 75% confidence since beyond that point there are not enough samples for the ROC Auc score to be meaningful. We find our model to be well calibrated, where if you only take predictions where the model is sure of itself, the ROC Auc score increases to 0.91. A graph of these findings can be found in Figure  5 .",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Vii. Qualitative Comparison",
      "text": "Examining images where our model made errors vs. images on which humans made errors can supply insight into attributes of the function the model used. Figure  4  provides examples of cases of both losing and winning players where: (1) humans classify incorrectly while our model classifies correctly, (2) our model as well as humans classify correctly and (3) humans classify correctly while our model classifies incorrectly. To get a taste of why the model makes mistakes we will examine the two images in the figure. In the top right corner of the figure we have a case where the player is sticking out his tongue and the model made an error. This image is one of only 3 in the dataset where the player is sticking out his tongue, and sticking out the tongue is generally associated with positive and not negative emotions  [45] . As such that it is easy to understand why pretrained networks would mistake this expression for a positive one. The bottom right image presents a female tennis player wearing a hat while looking up with an intense expression on her face. Her mouth is open, her eyes squeezed shut and her brow is furrowed. It is clear that this is an intense expression, however it is less clear if the expression is positive or negative. In this case the player lost a point, the human classified her correctly while our model mistook her for winning a point. However, if you examine the top left image in figure  4  the description provided above fits her identically. All the way from the female player wearing a hat, until the furrowed brow. Here, our model classifies her correctly as having won a point while human annotators classify her as having lost a point. We were not able to see a qualitative difference between the two images so perhaps there are indeed expressions which are inherently ambivalent.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Viii. Conclusion",
      "text": "We find, that models can perform better then humans on tasks when provided with sufficient relevant training data. We also find that even when models are trained on data tagged by humans, they can still outperform the human taggers, likely by learning patterns which for some reason humans do not.",
      "page_start": 7,
      "page_end": 7
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Can you tell in which images the player won a point and in which he / she lost a point?",
      "page": 1
    },
    {
      "caption": "Figure 2: Images of the tennis players including body pose. The cropped faces can be found in ﬁgure 1. In this scenario it is",
      "page": 4
    },
    {
      "caption": "Figure 3: Visualizations of full body pose coordinates vs. only",
      "page": 4
    },
    {
      "caption": "Figure 3: When examining our images we noticed that",
      "page": 5
    },
    {
      "caption": "Figure 5: Y axis describes ROC Auc score, X axis de-",
      "page": 5
    },
    {
      "caption": "Figure 4: Examples of images in six different categories. Top row is images of players who just scored (won) a point. Bottom",
      "page": 6
    },
    {
      "caption": "Figure 5: VII. QUALITATIVE COMPARISON",
      "page": 6
    },
    {
      "caption": "Figure 4: provides examples of",
      "page": 6
    }
  ],
  "tables": [],
  "citations": [
    {
      "citation_id": "1",
      "title": "Body cues, not facial expressions, discriminate between intense positive and negative emotions",
      "authors": [
        "H Aviezer",
        "Y Trope",
        "A Todorov"
      ],
      "year": "2012",
      "venue": "Science"
    },
    {
      "citation_id": "2",
      "title": "A common neurobiology for pain and pleasure",
      "authors": [
        "S Leknes",
        "I Tracey"
      ],
      "year": "2008",
      "venue": "Nature reviews neuroscience"
    },
    {
      "citation_id": "3",
      "title": "Nonlinear amygdala response to face trustworthiness: contributions of high and low spatial frequency information",
      "authors": [
        "C Said",
        "S Baron",
        "A Todorov"
      ],
      "year": "2009",
      "venue": "Journal of cognitive neuroscience"
    },
    {
      "citation_id": "4",
      "title": "Impaired recognition of emotion in facial expressions following bilateral damage to the human amygdala",
      "authors": [
        "R Adolphs",
        "D Tranel",
        "H Damasio",
        "A Damasio"
      ],
      "year": "1994",
      "venue": "Nature"
    },
    {
      "citation_id": "5",
      "title": "Amygdala response to happy faces as a function of extraversion",
      "authors": [
        "T Canli",
        "H Sivers",
        "S Whitfield",
        "I Gotlib",
        "J Gabrieli"
      ],
      "year": "2002",
      "venue": "Science"
    },
    {
      "citation_id": "6",
      "title": "Deep facial expression recognition: A survey",
      "authors": [
        "S Li",
        "W Deng"
      ],
      "year": "2020",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "7",
      "title": "Constants across cultures in the face and emotion",
      "authors": [
        "P Ekman",
        "W Friesen"
      ],
      "year": "1971",
      "venue": "Journal of personality and social psychology"
    },
    {
      "citation_id": "8",
      "title": "More evidence for the universality of a contempt expression",
      "authors": [
        "D Matsumoto"
      ],
      "year": "1992",
      "venue": "Motivation and Emotion"
    },
    {
      "citation_id": "9",
      "title": "Facial expressions of emotion are not culturally universal",
      "authors": [
        "R Jack",
        "O Garrod",
        "H Yu",
        "R Caldara",
        "P Schyns"
      ],
      "year": "2012",
      "venue": "Proceedings of the National Academy of Sciences"
    },
    {
      "citation_id": "10",
      "title": "The extended cohn-kanade dataset (ck+): A complete dataset for action unit and emotion-specified expression",
      "authors": [
        "P Lucey",
        "J Cohn",
        "T Kanade",
        "J Saragih",
        "Z Ambadar",
        "I Matthews"
      ],
      "year": "2010",
      "venue": "2010 ieee computer society conference on computer vision and pattern recognition-workshops"
    },
    {
      "citation_id": "11",
      "title": "Challenges in representation learning: A report on three machine learning contests",
      "authors": [
        "I Goodfellow",
        "D Erhan",
        "P Carrier",
        "A Courville",
        "M Mirza",
        "B Hamner",
        "W Cukierski",
        "Y Tang",
        "D Thaler",
        "D.-H Lee"
      ],
      "year": "2013",
      "venue": "International conference on neural information processing"
    },
    {
      "citation_id": "12",
      "title": "Affectnet: A database for facial expression, valence, and arousal computing in the wild",
      "authors": [
        "A Mollahosseini",
        "B Hasani",
        "M Mahoor"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "13",
      "title": "Categorical and dimensional affect analysis in continuous input: Current trends and future directions",
      "authors": [
        "H Gunes",
        "B Schuller"
      ],
      "year": "2013",
      "venue": "Image and Vision Computing"
    },
    {
      "citation_id": "14",
      "title": "Sewa db: A rich database for audio-visual emotion and sentiment research in the wild",
      "authors": [
        "J Kossaifi",
        "R Walecki",
        "Y Panagakis",
        "J Shen",
        "M Schmitt",
        "F Ringeval",
        "J Han",
        "V Pandit",
        "A Toisoul",
        "B Schuller"
      ],
      "year": "2019",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "15",
      "title": "Facial affect\"in-the-wild",
      "authors": [
        "S Zafeiriou",
        "A Papaioannou",
        "I Kotsia",
        "M Nicolaou",
        "G Zhao"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops"
    },
    {
      "citation_id": "16",
      "title": "Facial expression recognition based on local binary patterns: A comprehensive study",
      "authors": [
        "C Shan",
        "S Gong",
        "P Mcowan"
      ],
      "year": "2009",
      "venue": "Image and vision Computing"
    },
    {
      "citation_id": "17",
      "title": "Dynamic texture recognition using local binary patterns with an application to facial expressions",
      "authors": [
        "G Zhao",
        "M Pietikainen"
      ],
      "year": "2007",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "18",
      "title": "Graph-preserving sparse nonnegative matrix factorization with application to facial expression recognition",
      "authors": [
        "R Zhi",
        "M Flierl",
        "Q Ruan",
        "W Kleijn"
      ],
      "year": "2010",
      "venue": "IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)"
    },
    {
      "citation_id": "19",
      "title": "Learning active facial patches for expression analysis",
      "authors": [
        "L Zhong",
        "Q Liu",
        "P Yang",
        "B Liu",
        "J Huang",
        "D Metaxas"
      ],
      "year": "2012",
      "venue": "2012 IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "20",
      "title": "Video-based emotion recognition in the wild using deep transfer learning and score fusion",
      "authors": [
        "H Kaya",
        "F Gürpınar",
        "A Salah"
      ],
      "year": "2017",
      "venue": "Image and Vision Computing"
    },
    {
      "citation_id": "21",
      "title": "Convolutional neural networks pretrained on large face recognition datasets for emotion classification from video",
      "authors": [
        "B Knyazev",
        "R Shvetsov",
        "N Efremova",
        "A Kuharenko"
      ],
      "year": "2017",
      "venue": "Convolutional neural networks pretrained on large face recognition datasets for emotion classification from video",
      "arxiv": "arXiv:1711.04598"
    },
    {
      "citation_id": "22",
      "title": "Survey on emotional body gesture recognition",
      "authors": [
        "F Noroozi",
        "D Kaminska",
        "C Corneanu",
        "T Sapinski",
        "S Escalera",
        "G Anbarjafari"
      ],
      "year": "2018",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "23",
      "title": "A study on emotion recognition from body gestures using kinect sensor",
      "authors": [
        "S Saha",
        "S Datta",
        "A Konar",
        "R Janarthanan"
      ],
      "year": "2014",
      "venue": "2014 International Conference on Communication and Signal Processing"
    },
    {
      "citation_id": "24",
      "title": "Multi-level classification of emotional body expression",
      "authors": [
        "N Fourati",
        "C Pelachaud"
      ],
      "year": "2015",
      "venue": "2015 11th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG)"
    },
    {
      "citation_id": "25",
      "title": "Emotion recognition in context",
      "authors": [
        "R Kosti",
        "J Alvarez",
        "A Recasens",
        "A Lapedriza"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "26",
      "title": "Bi-modal emotion recognition from expressive face and body gestures",
      "authors": [
        "H Gunes",
        "M Piccardi"
      ],
      "year": "2007",
      "venue": "Journal of Network and Computer Applications"
    },
    {
      "citation_id": "27",
      "title": "Emotion recognition based on human gesture and speech information using rt middleware",
      "authors": [
        "H Vu",
        "Y Yamazaki",
        "F Dong",
        "K Hirota"
      ],
      "year": "2011",
      "venue": "2011 IEEE International Conference on Fuzzy Systems"
    },
    {
      "citation_id": "28",
      "title": "Joint face detection and alignment using multitask cascaded convolutional networks",
      "authors": [
        "K Zhang",
        "Z Zhang",
        "Z Li",
        "Y Qiao"
      ],
      "year": "2016",
      "venue": "IEEE signal processing letters"
    },
    {
      "citation_id": "29",
      "title": "Thrill of victory or agony of defeat? perceivers fail to utilize information in facial movements",
      "authors": [
        "H Aviezer",
        "D Messinger",
        "S Zangvil",
        "W Mattson",
        "D Gangi",
        "A Todorov"
      ],
      "year": "2015",
      "venue": "Emotion"
    },
    {
      "citation_id": "30",
      "title": "Vggface2: A dataset for recognising faces across pose and age",
      "authors": [
        "Q Cao",
        "L Shen",
        "W Xie",
        "O Parkhi",
        "A Zisserman"
      ],
      "year": "2018",
      "venue": "2018 13th IEEE international conference on automatic face & gesture recognition"
    },
    {
      "citation_id": "31",
      "title": "Very deep convolutional networks for large-scale image recognition",
      "authors": [
        "K Simonyan",
        "A Zisserman"
      ],
      "year": "2014",
      "venue": "Very deep convolutional networks for large-scale image recognition",
      "arxiv": "arXiv:1409.1556"
    },
    {
      "citation_id": "32",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "K He",
        "X Zhang",
        "S Ren",
        "J Sun"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "33",
      "title": "Squeeze-and-excitation networks",
      "authors": [
        "J Hu",
        "L Shen",
        "G Sun"
      ],
      "year": "2018",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "34",
      "title": "Looking to listen at the cocktail party: A speaker-independent audio-visual model for speech separation",
      "authors": [
        "A Ephrat",
        "I Mosseri",
        "O Lang",
        "T Dekel",
        "K Wilson",
        "A Hassidim",
        "W Freeman",
        "M Rubinstein"
      ],
      "year": "2018",
      "venue": "Looking to listen at the cocktail party: A speaker-independent audio-visual model for speech separation",
      "arxiv": "arXiv:1804.03619"
    },
    {
      "citation_id": "35",
      "title": "Audio-visual evaluation of oratory skills",
      "authors": [
        "T Michelson",
        "S Peleg"
      ],
      "year": "2021",
      "venue": "2021 Third International Conference on Transdisciplinary AI (TransAI)"
    },
    {
      "citation_id": "36",
      "title": "Training deep networks for facial expression recognition with crowd-sourced label distribution",
      "authors": [
        "E Barsoum",
        "C Zhang",
        "C Ferrer",
        "Z Zhang"
      ],
      "year": "2016",
      "venue": "Proceedings of the 18th ACM International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "37",
      "title": "Distract your attention: Multihead cross attention network for facial expression recognition",
      "authors": [
        "Z Wen",
        "W Lin",
        "T Wang",
        "G Xu"
      ],
      "year": "2021",
      "venue": "Distract your attention: Multihead cross attention network for facial expression recognition",
      "arxiv": "arXiv:2109.07270"
    },
    {
      "citation_id": "38",
      "title": "Training deep networks for facial expression recognition with crowd-sourced label distribution",
      "authors": [
        "E Barsoum",
        "C Zhang",
        "C Ferrer",
        "Z Zhang"
      ],
      "year": "2016",
      "venue": "ACM International Conference on Multimodal Interaction (ICMI)"
    },
    {
      "citation_id": "39",
      "title": "Reliable crowdsourcing and deep localitypreserving learning for expression recognition in the wild",
      "authors": [
        "S Li",
        "W Deng",
        "J Du"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "40",
      "title": "Convolutional pose machines",
      "authors": [
        "S.-E Wei",
        "V Ramakrishna",
        "T Kanade",
        "Y Sheikh"
      ],
      "year": "2016",
      "venue": "CVPR"
    },
    {
      "citation_id": "41",
      "title": "Openpose: Realtime multi-person 2d pose estimation using part affinity fields",
      "authors": [
        "Z Cao",
        "G Hidalgo",
        "T Martinez",
        "S Simon",
        "Y Wei",
        "Sheikh"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "42",
      "title": "Detectron2",
      "authors": [
        "Y Wu",
        "A Kirillov",
        "F Massa",
        "W.-Y Lo",
        "R Girshick"
      ],
      "year": "2019",
      "venue": "Detectron2"
    },
    {
      "citation_id": "43",
      "title": "Mask r-cnn",
      "authors": [
        "K He",
        "G Gkioxari",
        "P Dollár",
        "R Girshick"
      ],
      "year": "2017",
      "venue": "Proceedings"
    },
    {
      "citation_id": "44",
      "title": "Mediapipe hands: On-device real-time hand tracking",
      "authors": [
        "F Zhang",
        "V Bazarevsky",
        "A Vakunov",
        "A Tkachenka",
        "G Sung",
        "C.-L Chang",
        "M Grundmann"
      ],
      "year": "2020",
      "venue": "Mediapipe hands: On-device real-time hand tracking",
      "arxiv": "arXiv:2006.10214"
    },
    {
      "citation_id": "45",
      "title": "Positive emotions from brain injury: the emergence of mirth and happiness",
      "authors": [
        "M Mendez",
        "L Parand"
      ],
      "year": "2020",
      "venue": "Case reports in psychiatry"
    }
  ]
}