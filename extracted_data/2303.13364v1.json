{
  "paper_id": "2303.13364v1",
  "title": "Reevaluating Data Partitioning For Emotion Detection In Emowoz",
  "published": "2023-03-15T03:06:13Z",
  "authors": [
    "Moeen Mostafavi",
    "Michael D. Porter"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "This paper focuses on the EmoWoz dataset, an extension of MultiWOZ that provides emotion labels for the dialogues. MultiWOZ was partitioned initially for another purpose, resulting in a distributional shift when considering the new purpose of emotion recognition. The emotion tags in EmoWoz are highly imbalanced and unevenly distributed across the partitions, which causes sub-optimal performance and poor comparison of models. We propose a stratified sampling scheme based on emotion tags to address this issue, improve the dataset's distribution, and reduce dataset shift. We also introduce a special technique to handle conversation (sequential) data with many emotional tags. Using our proposed sampling method, models built upon EmoWoz can perform better, making it a more reliable resource for training conversational agents with emotional intelligence. We recommend that future researchers use this new partitioning to ensure consistent and accurate performance evaluations.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Emotion recognition in task-oriented conversational agents is challenging because it requires the agent to accurately interpret and respond to a user's emotional state in real time. Emotional signals can be complex and difficult to detect accurately, especially in unstructured conversations where users may not express their emotions explicitly. Another challenge is that emotions can be context-dependent, meaning that they may be influenced by the user's past experiences, current environment, and cultural background. Therefore, conversational agents need to interpret these contextual factors accurately to provide appropriate responses sensitive to the user's emotional state.\n\nDespite the challenges, emotion recognition in task-oriented conversational agents is important because it can improve the overall user experience  (Zhang et al., 2020) . By accurately detecting and responding to a user's emotional state, conversational agents can provide more personalized and empathetic interactions, increasing user satisfaction and engagement. Additionally, emotion recognition can help agents identify when a user is experiencing frustration, confusion, or other negative emotions, allowing them to intervene and provide support to prevent user dropout or dissatisfaction  (Andre et al., 2004) . Emotion recognition is critical to developing effective task-oriented conversational agents that can provide a human-like user experience  (Polzin and Waibel, 2000) .\n\nSentiment analysis in task-oriented conversational agents has been addressed in the literature as an essential aspect of natural language processing that can improve the overall user experience  (Shi and Yu, 2018; Saha et al., 2020; Wang et al., 2020) . However, the lack of publicly available data for emotion recognition is a significant limitation for task-oriented conversational agent applications.\n\nMultiWOZ (Multi-Domain Wizard-of-Oz) is a large-scale dataset of human-human written conversations for task-oriented dialogue modeling. The dataset was initially collected for training and evaluating dialogue systems, particularly those designed to assist users with completing specific tasks such as booking a hotel or reserving a table at a restaurant  (Budzianowski et al., 2018) .  Feng et al. (2022)  extended the MultiWOZ dataset by including dialogues between humans and a machine-generated policy, which they named DialMAGE. The resulting dataset was called EmoWOZ. EmoWOZ is a large-scale, manually emotion-annotated corpus of task-oriented dialogues. The corpus contains more than 11K dialogues with more than 83K emotion annotations of user utterances, which makes it the first large-scale open-source corpus of its kind. The authors propose a novel emotion labeling scheme tailored to task-oriented dialogues and demonstrate the usability of this corpus for emotion recognition arXiv:2303.13364v1 [cs.CL] 15 Mar 2023 and state tracking in task-oriented dialogues. The paper highlights that while emotions in chit-chat dialogues have received considerable attention  (Li et al., 2017; Poria et al., 2018; Zahiri and Choi, 2017) , emotions in task-oriented dialogues remain largely unaddressed. They argue that incorporating emotional intelligence can help conversational AI generate more emotionally and semantically appropriate responses, making a better user experience.\n\nIn their study, the authors described their methodology for partitioning the EmoWOZ dataset into training, validation, and testing sets while maintaining the original split of the MultiWOZ dataset. They also divided the DialMAGE dataset into three parts with a ratio of 8:1:1. However, in the following section, we will examine the limitations of their approach to data partitioning and suggest an alternative method.\n\nWe specifically concentrate on the MultiWOZ section of EmoWOZ in this paper as it is widely used in various applications. Therefore, we do not discuss all subsets of the EmoWOZ in detail. Nonetheless, our approach can be readily extended to the entire EmoWOZ dataset.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Data Partitioning",
      "text": "When building a predictive model, we typically split our data into three sets: a training set, a validation set, and a test set. The purpose of the training set is to estimate model parameters, and the purpose of the validation set is to tune the model's hyperparameters and assess its performance. The test set aims to get an unbiased estimate of the model's performance on new, unseen data. It's important to note that the test set should not be used in any part of model fitting or model selection because doing so can lead to overfitting and inaccurate performance estimate. However, after we have trained and validated our models on the training and validation sets, we can use the test set to compare the performance of different models and select the best one. This is where the issue of different distributions between the test and validation sets comes in. If the test set has a different distribution than the validation set, a model that performs well on the validation set may not be the best model for the test set, and vice versa. Therefore, it's essential to ensure that the distributions of the three sets are as similar as possible.\n\nApproximately 2.5% of the MultiWOZ subset in  (Feng et al., 2022)   for resolution. However, this manual annotation was not evenly distributed across all classes, and minority classes were affected more than majority classes. For instance, the abusive class in the test set was completely annotated manually, while in the developing set it was manually labeled in 50% of cases. Also, the Fearful class had a relative frequency three times higher in the training set, as shown in Table  1 .\n\nThree annotators labeled the utterances according to the task. The final label was determined primarily by the majority vote of the annotators. Among all utterances, 72.1% had a complete agreement among the three annotators. A partial agreement was found for 26.4% of the utterances, while for 1.5%, there was no agreement. The paper reports that these instances were resolved manually to address cases where the annotators could not reach an agreement. In a small portion of the data, a label different from the majority vote was chosen.\n\nWe use F1 scores to compare annotators across data partitions to assess inter-annotator agreement. In essence, we measure the effectiveness of annotations by the three annotators across the training, validation, and test sets using the final labels. This phenomenon is known as dataset shift in which there is a difference between the joint distribution of inputs and outputs during the training stage compared to the validation and test stage  (Quinonero-Candela et al., 2008) , leading to a decrease in performance. Dataset Shift is a common problem in machine learning, and it can have significant consequences, such as a decrease in accuracy. In the case of EmoWoz, the dataset Shift arises from the fact that  (Feng et al., 2022)  kept the original partitioning of MultiWOZ, which is not evenly distributed across partitions for this particular task.\n\nIt's worth noting that while the original partitioning of MultiWOZ data is suitable for many tasks related to the development of task-oriented conversational agents, it may not be ideal for emotion detection. Emotion detection requires consideration of different contextual aspects, which may require a new partitioning approach. For instance, in the original partitioning, 60% of messages with the Fearful emotion in the training set were in conversations with the police or hospital. However, in the validation and test sets, none of the conversations with Fearful emotions were related to the police or hospital. This contextual aspect of the conversation plays a crucial role in accurately recognizing emotions.\n\nTo address this issue, we use stratified sampling, which is a sampling technique that ensures that each sub-group in the data is represented proportionally in the sample. In this case, we use stratified sampling to ensure that the training set has a distribution similar to the validation and test sets. Stratified sampling is particularly useful in situations where the distribution of the target variable is imbalanced or varies across sub-groups in the data. In the case of EmoWoz, the distribution of emotions in the training set has differed from that in the validation and test sets, which could have contributed to the dataset shift. We used the Algorithm 1 to get a new partitioning of the data. By using stratified sampling, we have ensured that the emotion distribution in the training set was similar to that in the validation and test sets, which helps to reduce the dataset Shift and improve the model's performance. Table  3  shows the annotator's F1-score after the new partitioning.\n\n3 Case study  Feng et al. (2022)  used Bert, Contextual BERT, DialogueRNN  (Majumder et al., 2019) , and COS-MIC  (Ghosal et al., 2020)  for the baseline methods. Among these methods, BERT did not incorporate the sequential aspects of the conversation, yet it yielded the best Macro F1 scores in most EmoWOZ Data: MultiWoz dataset with emotion labels from EmoWOZ 1. Group the dataset based on their utterance ids and find a list with the emotion sequence in each utterance.\n\n2. Determine the frequency of emotional sequences in the dataset.\n\n3. Make a dictionary called emotion_seq_dict with the emotional sequence as the key and the counts of the sequence in the dataset as the value.\n\n4. Partition the whole dataset into one set called frequent_seq with conversations of more than six emotional list frequencies and another set non_frequent_seq with the rest of the data.\n\n5. For the frequent_seq, do the stratified sampling of conversation based on the emotion sequence and partition it to the training, validation, and test set, with a 80-10-10 split similar to the original split of the data.\n\n6. Use random sampling to partition the non_frequent_seq to the training, validation, and test sets.\n\n7. Find the union of the two partitions to get the partitioning of the whole dataset.\n\nAlgorithm 1: Stratified sampling for emotion recognition in the conversation subsets. To enhance the BERT model, we compute the relative embedding of the message from the chatbot and agent and employ a transformer model to address the sequential aspects of the problem. Hyperparameters for this method using both the original and proposed partitioning are illustrated in Table  4 . Upon examining the results, we can see that in the original partitioning, the hyperparameters corresponding to the top-performing model on the validation set produced the worst model on the test set. This discrepancy could be indicative of data drift, as we previously discussed. This implementation uses five distinct seeds to generate five unique embeddings. We then employed five seeds to construct the transformer model on top of these embeddings. Consequently, we had 25 different models for each parameter in Table  4 .   Train 94.02 52.38 50.97 73.06 34.87 41.73 88.83 56.98 62.27 Val. 93.86 48.58 47.16 71.49 37.50 41.65 88.69 55.85 61.28 Test 93.79 52.08 46.13 72.20 32.26 41.54 88.49 55.45 60.93    To visualize the Macro F1 score for each of these models, we included Figures  1  and 2 . The colors within these figures correspond to the five distinct seeds utilized in the embedding step. Notably, we can observe that only in the proposed partitioning of the data the change in averaged Macro F1 score is similar across the validation and test sets for various hyperparameters. Furthermore, we can observe that for models with equivalent hyperparameters, the change in score across different initial seeds is comparable in both the development and test sets. These observations suggest that no data drift is present in the new partitioning.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Concluding Remarks",
      "text": "After analyzing the original data partitioning, we have identified potential data drift and suggested an alternative approach to address this issue. Our evaluation of the results indicates that the new partitioning approach effectively reduces data drift, as demonstrated by the consistency of Macro F1 scores in both the validation and test sets across different hyperparameters and initial seeds. These findings suggest that the proposed partitioning method is a suitable alternative for researchers working on emotion detection using MultiWOZ data. This work emphasizes the significance of meticulously choosing and employing partitioning methods in the training and assessment of machine learning models.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: This ﬁgure depicts the Macro F1 score for",
      "page": 4
    },
    {
      "caption": "Figure 2: This ﬁgure depicts the Macro F1 score for",
      "page": 5
    }
  ],
  "tables": [
    {
      "caption": "Table 3: Performance of annotators based on F1",
      "data": [
        {
          "Original splits\nStratiﬁed splits": "Val.\nTest\nDif."
        },
        {
          "Original splits\nStratiﬁed splits": "51.4\n48.92\n-2.48\n50.58\n54.03\n3.45\n54.23\n48.96\n5.27\n49.52\n52.35\n2.83\n50.31\n51.77\n1.46\n49.86\n52.27\n2.41"
        }
      ],
      "page": 4
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Endowing spoken language dialogue systems with emotional intelligence",
      "authors": [
        "Elisabeth Andre",
        "Matthias Rehm",
        "Wolfgang Minker",
        "Dirk Bühler"
      ],
      "year": "2004",
      "venue": "Affective Dialogue Systems: Tutorial and Research Workshop"
    },
    {
      "citation_id": "2",
      "title": "Multiwoz-a large-scale multi-domain wizard-of-oz dataset for task-oriented dialogue modelling",
      "authors": [
        "Paweł Budzianowski",
        "Tsung-Hsien Wen",
        "Bo-Hsiang Tseng",
        "Inigo Casanueva",
        "Stefan Ultes",
        "Milica Osman Ramadan",
        "Gašić"
      ],
      "year": "2018",
      "venue": "Multiwoz-a large-scale multi-domain wizard-of-oz dataset for task-oriented dialogue modelling",
      "arxiv": "arXiv:1810.00278"
    },
    {
      "citation_id": "3",
      "title": "Emowoz: A large-scale corpus and labelling scheme for emotion recognition in task-oriented dialogue systems",
      "authors": [
        "Shutong Feng",
        "Nurul Lubis",
        "Christian Geishauser",
        "Hsien-Chin Lin",
        "Michael Heck",
        "Carel Van Niekerk",
        "Milica Gašić"
      ],
      "year": "2022",
      "venue": "Proceedings of the Thirteenth Language Resources and Evaluation Conference"
    },
    {
      "citation_id": "4",
      "title": "Commonsense knowledge for emotion identification in conversations",
      "authors": [
        "Deepanway Ghosal",
        "Navonil Majumder",
        "Alexander Gelbukh",
        "Rada Mihalcea",
        "Soujanya Poria"
      ],
      "year": "2020",
      "venue": "Commonsense knowledge for emotion identification in conversations",
      "arxiv": "arXiv:2010.02795"
    },
    {
      "citation_id": "5",
      "title": "Dailydialog: A manually labelled multi-turn dialogue dataset",
      "authors": [
        "Yanran Li",
        "Hui Su",
        "Xiaoyu Shen",
        "Wenjie Li",
        "Ziqiang Cao",
        "Shuzi Niu"
      ],
      "year": "2017",
      "venue": "Dailydialog: A manually labelled multi-turn dialogue dataset",
      "arxiv": "arXiv:1710.03957"
    },
    {
      "citation_id": "6",
      "title": "and Erik Figure 2: This figure depicts the Macro F1 score for each of the seeds utilized in implementing the sequential extension of the BERT model on the proposed Multiwoz partitioning. For each hyperparameter, both the Macro F1 score in the validation and test sets are plotted in close proximity to one another. Additionally, the colors within the figure represent the five distinct seeds utilized in the embedding step. Cambria",
      "authors": [
        "Navonil Majumder",
        "Soujanya Poria",
        "Devamanyu Hazarika",
        "Rada Mihalcea",
        "Alexander Gelbukh"
      ],
      "year": "2019",
      "venue": "Proceedings of the AAAI conference on artificial intelligence"
    },
    {
      "citation_id": "7",
      "title": "Emotion-sensitive human-computer interfaces",
      "authors": [
        "S Thomas",
        "Alexander Polzin",
        "Waibel"
      ],
      "year": "2000",
      "venue": "ISCA tutorial and research workshop (ITRW) on speech and emotion"
    },
    {
      "citation_id": "8",
      "title": "Navonil Majumder, Gautam Naik, Erik Cambria, and Rada Mihalcea",
      "authors": [
        "Soujanya Poria",
        "Devamanyu Hazarika"
      ],
      "year": "2018",
      "venue": "Meld: A multimodal multi-party dataset for emotion recognition in conversations",
      "arxiv": "arXiv:1810.02508"
    },
    {
      "citation_id": "9",
      "title": "Dataset shift in machine learning",
      "authors": [
        "Joaquin Quinonero-Candela",
        "Masashi Sugiyama",
        "Anton Schwaighofer",
        "Neil Lawrence"
      ],
      "year": "2008",
      "venue": "Dataset shift in machine learning"
    },
    {
      "citation_id": "10",
      "title": "Towards sentiment aided dialogue policy learning for multi-intent conversations using hierarchical reinforcement learning",
      "authors": [
        "Tulika Saha",
        "Sriparna Saha",
        "Pushpak Bhattacharyya"
      ],
      "year": "2020",
      "venue": "PloS one"
    },
    {
      "citation_id": "11",
      "title": "Sentiment adaptive end-to-end dialog systems",
      "authors": [
        "Weiyan Shi",
        "Zhou Yu"
      ],
      "year": "2018",
      "venue": "Sentiment adaptive end-to-end dialog systems",
      "arxiv": "arXiv:1804.10731"
    },
    {
      "citation_id": "12",
      "title": "Sentiment classification in customer service dialogue with topic-aware multitask learning",
      "authors": [
        "Jiancheng Wang",
        "Jingjing Wang",
        "Changlong Sun",
        "Shoushan Li",
        "Xiaozhong Liu",
        "Luo Si",
        "Min Zhang",
        "Guodong Zhou"
      ],
      "year": "2020",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "13",
      "title": "Emotion detection on tv show transcripts with sequencebased convolutional neural networks",
      "authors": [
        "M Sayyed",
        "Jinho D Zahiri",
        "Choi"
      ],
      "year": "2017",
      "venue": "Emotion detection on tv show transcripts with sequencebased convolutional neural networks",
      "arxiv": "arXiv:1708.04299"
    },
    {
      "citation_id": "14",
      "title": "Towards emotion-aware user simulator for task-oriented dialogue",
      "authors": [
        "Rui Zhang",
        "Kai Yin",
        "Li Li"
      ],
      "year": "2020",
      "venue": "Towards emotion-aware user simulator for task-oriented dialogue",
      "arxiv": "arXiv:2011.09696"
    }
  ]
}