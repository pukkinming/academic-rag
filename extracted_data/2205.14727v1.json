{
  "paper_id": "2205.14727v1",
  "title": "Cped: A Large-Scale Chinese Personalized And Emotional Dialogue Dataset For Conversational Ai",
  "published": "2022-05-29T17:45:12Z",
  "authors": [
    "Yirong Chen",
    "Weiquan Fan",
    "Xiaofen Xing",
    "Jianxin Pang",
    "Minlie Huang",
    "Wenjing Han",
    "Qianfeng Tie",
    "Xiangmin Xu"
  ],
  "keywords": [
    "Dialogue system",
    "cognitive processing",
    "conversation generation",
    "data collection"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Human language expression is based on the subjective construal of the situation instead of the objective truth conditions, which means that speakers' personalities and emotions after cognitive processing have an important influence on conversation. However, most existing datasets for conversational AI ignore human personalities and emotions, or only consider part of them. It's difficult for dialogue systems to understand speakers' personalities and emotions although large-scale pretraining language models have been widely used. In order to consider both personalities and emotions in the process of conversation generation, we propose CPED, a large-scale Chinese personalized and emotional dialogue dataset, which consists of multi-source knowledge related to empathy and personal characteristic. These knowledge covers gender, Big Five personality traits, 13 emotions, 19 dialogue acts and 10 scenes. CPED contains more than 12K dialogues of 392 speakers from 40 TV shows. We release the textual dataset with audio features and video features according to the copyright claims, privacy issues, terms of service of video platforms. We provide detailed description of the CPED construction process and introduce three tasks for conversational AI, including personality recognition, emotion recognition in conversations as well as personalized and emotional conversation generation. Finally, we provide baseline systems for these tasks and consider the function of speakers' personalities and emotions on conversation. Our motivation is to propose a dataset to be widely adopted by the NLP community as a new open benchmark for conversational AI research. The full dataset is available 1 .",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "O PEN-DOMAIN conversation systems are of great signif- icance in the application of human-computer interaction, companionship, depression treatment, autism intervention, etc.\n\n[1]-  [3] . Driving dialogue systems to learn expression capabilities from a large-scale dialogue corpus, such as OpenSubtitles  [4] , Ubuntu Dialogue Corpus  [5] , STC  [6] , LCCC  [7] , Open-ViDial  [8] , etc., is considered to be feasible. However, if we expect the dialogue systems to possess a good command of personification capabilities, e.g., emotional expression, personality presentation and empathetic conversation, two critical problems need to be tackled: (i) the lack of long-term stable personalities (e.g., gender, age, and Big Five), and (ii) the lack of dynamic emotions or dialogue acts (DAs) during conversation. To the best of our knowledge, dialogue generation models considering emotion and personality as prior knowledge at the same time are currently scarce since no available dialogue dataset simultaneously provides emotional information and personalities of the speakers.\n\nIn a conversation, the participants' expression depends on not only their linguistic context but also the priori personalities 0000-0000/00$00.00 © 2022 IEEE arXiv:2205.14727v1 [cs.CL] 29 May 2022 and dynamic emotions. For example, in Figure  1 , \"speaker1\" with high neuroticism may easily present an angry state in conversation when saying \"你谁? (who are you?)\". In contrast, \"speaker2\" with high extraversion and low neuroticism, may tend to joke during communication, pretending to be Yu Chunxiao's husband to joke with \"speaker1\". People's personality is imperceptibly affecting their own expression style. In other words, relying solely on supervised learning on textual contexts is insufficient to model this dialogue generation process. Besides, according to the book Cognitive psychology: Applying the science of the mind  [9] , there are also significant differences in Conversation Styles between female and male speakers. On the whole, only providing large-scale text for training conversation generation models can not make them master human cognitive expression patterns.\n\nTherefore, we propose a large-scale Chinese Personalized and Emotional Dialogue dataset (CPED), which includes the personalities of the speakers, dynamic emotions and DAs of the multimodal dialogue contexts. CPED, which contains 12K dialogues and 133K utterances, is collected from 40 popular TV series closely related to daily life, making its distribution of personality or emotion close to the real world. We asked the psychology professional annotators to label the emotions and DAs of the speakers through video, audio and text, which is different from DailyDialog  [10]  and ESTC  [1] . In daily life, speakers may continuously speak in a round of conversation (Figure  1 ) during which the emotional state or DA state may change several times. Therefore, we divided a turn of dialogue into multiple utterances and annotated emotions and DAs multiple times. Furthermore, we considered gender, age and Big Five personality  [11]  as the basic personality traits.\n\nThe contributions of this paper are summarized as follows:\n\n1) We build a multiturn Chinese Personalized and Emotional Dialogue dataset called CPED. To the best of our knowledge, CPED is the first Chinese personalized and emotional dialogue dataset. CPED contains 12K dialogues and 133K utterances with multi-modal context. Therefore, it can be used in both complicated dialogue understanding and human-like conversation generation. 2) CPED has been annotated with 3 character attributes ((name, gender age), Big Five personality traits, 2 types of dynamic emotional information (sentiment and emotion) and DAs. The personality traits and emotions can be used as prior external knowledge for open-domain conversation generation, making the conversation system have a good command of personification capabilities. 3) We propose three tasks for CPED: personality recognition in conversations (PRC), emotion recognition in conversations (ERC), and personalized and emotional conversation (PEC). A set of experiments verify the importance of using personalities and emotions as prior external knowledge for conversation generation.\n\nThe remainder of this paper is organized as follows: Section II discusses the related work; we then describe the construction process and detailed characteristics of CPED in Section III; definition of personality and emotion recognition in conversations, and corresponding baseline experiments are elaborated in Section IV; definition and baseline experiments of personalized and emotional conversation are described in Section V; applications and limitations of CPED are presented in Section VI; finally, Section VII illustrates the conclusion and future work.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Related Work",
      "text": "A. Cognitive Psychology Theory for Conversation a) Personality Theory: Allport proposed the personality traits  [12]  in 1921. Allport claims that personality trait has the ability to dominate individual behavior, and divides personality traits into two categories: common traits and individual traits. Cattell proposed the Sixteen Personality Factor Questionnaire (16PF) in 1949  [13] . Eysenck proposed the structure of personality  [14]  in 1953, and developed the Eysenck Personality Questionnaire (EPQ)  [15]  in 1975. Early lexical studies on personality models  [16] ,  [17]  have proved that the terms used to describe personality traits in English are mainly composed of five dimensions, that is named the five factor personality model.  McCrae & Costa (1997)  established a five factor personality model based on 16PF factor analysis, which are Neuroticism, Extraversion, Openness, Agreeableness, and Conscientiousness  [18] . They also released a NEO Personality Inventory (NEO-PI)  [19]  in 1992 and NEO-PI-R  [18]  in 1997. Typical five factor personality invertories include Hogan Personality Inventory (HPI)  [20]  and Big Five Inventory (BFI)  [21] .  Tellegen & Waller (1987)    [22]  used the method of random stratified sampling to select 400 adjectives for self description, and then did factor analysis to obtain the seven dimensions of personality, and put forward the Big Seven factor model of of personality, which are Positive Emotionality (PEM), Nagetive Valence (NVAL), Positive Valence (PVAL), Negative Emotionality (NEM), Dependability (DEP), Agreeableness (AGR), Conventionality (CONV). Among multifarious personality models, the Big Five(BF, also called OCEAN)  [18] ,  [21]  personality model has been proved to have cross-cultural applicability and has been widely used. In addition,  Marusic & Bratko (1998)  found that different gender has different distribution in each personality dimension of BF  [23] .  Soto et al. (2011)  found that BF personality domains has mean-level age differences  [24] . Therefore, gender, age group and BF of the speakers are taken into account in the annotation label.\n\nb) Emotion Theory: More than 90 definitions of \"emotion\" have been proposed in the past according to Plutchik's research  [25] .  Izard (1991)  divides emotions into three parts: subjective experience, external performance and physiological arousal  [26] . At present, there are two basic representative formats for the classification of emotions: dimensional emotional state (DES) and categorical emotion states (CES)  [27] .  Russell (1980)  defines emotion as two continuous scales: valence and arousal  [28] . The valence-arousal-dominance space (VAD)  [29]  and the pleasure-arousal-dominance space (PAD)  [30]  are the commonly used DES models, which transform the complex emotions into continuous 3D space. However, it is very difficult to annotate the continuous emotional labels, which consumes a lot of time and human resources, especially for the text. The CES models hold that emotions have completely different structures.  Tomkins (1970)  believes that there are eight primary emotions  [31] . In 1971, Ekman & Friesen proposed the Ekman's six basic emotions, which categorize emotions as: happiness, surprise, anger, disgust, fear and sadness  [32] . In 1980, Robert proposed the Plutchik's Emotion Wheel, which consist 8 basic emotions (anger, disgust, fear, sadness, anticipation, joy, surprise and trust)  [33] .  Izard (1991)  put forward that there are 10 basic emotions  [26] .",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "B. Conversation Datasets",
      "text": "In Table  I , we briefly review the available conversation datasets.\n\na) Open-domain Conversation Datasets: There have been various open-domain conversation datasets (Table  I (rows 2-10)) over the past few years. These datasets are usually crawled from blogs, forums, or TV series subtitle sites, e.g. OpenSubtitles  [4] , Cornell Movie Dialog Corpus  [35] , Ubuntu Dialogue Corpus  [5] , Twitter  [34]  and OpenViDial  [8] . In the field of Chinese conversation generation, the corpus is usually crawled from social media, such as STC  [6] , the Douban Conversation Corpus  [36] , LCCC  [7]  and WDC-Dialogue  [37] . Among them, WDC-Dialogue  [37]  has 1.4 billion dialogues so that the pre-training model can be fully trained in the field of open-domain dialogue generation. These datasets do not contain any emotional or personalized annotation information. Therefore, the dialogue generation model (e.g. DialoGPT  [2] , CDialGPT  [7] ) can only learn personalized or emotional expressions through the dialogue context (single-modal or multi-modal) provided by the corpus.\n\nb) Emotional Conversation Datasets: Generally, the emotional perception ability of a dialogue model is defined as the task: emotion recognition in conversations (ERC)  [40]  or emotion reasoning (ER)  [46] . Datasets, e.g., IEMOCAP  [38] , Mastodon  [39] , MELD  [40] , EMOTyDA  [42] , EDA  [50] , MEmoR  [46]  and M 3 ED  [43] , are usually used for the ERC or ER task. These datasets generally have small sizes, with fewer than 10K dialogues, making them unsuitable for conversation generation tasks. Another type of dataset is specifically constructed for emotional conversation generation tasks. For example, DailyDialog  [10]  contains 13K multi-turn dialogues with 102K utterances manually annotated with 7 emotions and 4 DAs. Thus, the dataset is usually used for emotional conversation generation  [51] ,  [52] . EmpatheticDialogues  [41]  provides 25K dialogues with 32 types of emotion labels and 2 roles (speaker and listener) for empathetic conversation. ESTC  [1] , which is annotated with six emotion categories using the Bi-LSTM emotion classifier based on the STC dataset, is used for Chinese emotional conversation generation. Unfortunately, there is no available large-scale Chinese multimodal emotional dialogue dataset for emotional conversation generation so far. c) Personalized Conversation Datasets: There are already some datasets related to personalized conversation (in Table  I (rows 19-24)). For example, PERSONA-CHAT  [44]  crowdsourced a set of 1,155 personas and obtained 10,981 dialogs with 164,356 utterances from Turkers assigned a ran-dom persona that were asked to chat with others. In particular, each persona consists of at least 5 profile sentences, just like a small knowledge base that can provide information, such as \"I am an artist\" or \"I like to shi\". PersonalDialog  [49] , a Chinese personalized conversation dataset, provides 56.25M utterances from 8.47M speakers who are annotated with personality traits, e.g., age, gender, location, interest tags, etc. Specifically, PERSONA-CHAT  [44]  and PersonalDialog  [49]  provide actually character attributes rather than personality traits. FriendsPersona  [47]  is annotated with BF personality traits of speakers, which is used for personality recognition on multiparty dialogues. However, the BF personality traits of speaker in FriendsPersona change in different conversations, which is contradictory to the personality coherence  [53] . PELD  [48]  is proposed for predicting emotion for response using BF personality traits and VAD vector, in which the personality traits are averaged with personality traits of FriendsPersona  [47] . MEmoR  [46] , a recent multimodal emotion reasoning dataset used for the task of multimodal emotion reasoning, provides a multimodal conversation context, 14 fine-grained emotions and 3 types of personalities (16PF, BF and MBTI). MEmoR is mainly used for the task of multimodal emotion reasoning, in which the personalities are used for improving the performance of emotion reasoning. At present, in the field of Chinese conversation, there is a lack of personality related datasets, which hinders the research on personality related tasks, such as personality recognition in conversations.\n\nWith explicit personality and dynamic emotional information, we believe that CPED will provide novel research opportunities and conditions for Chinese open-domain conversation, e.g. personality recognition and emotion recognition on conversations, personalized and emotional conversation.",
      "page_start": 1,
      "page_end": 4
    },
    {
      "section_name": "Iii. Cped Dataset",
      "text": "In this section, we describe the processing stage of constructing the CPED dataset. To construct a Chinese personalized and emotional dialogue dataset, we collected a large number of TV series related to daily life, and asked the crowdworkers to filter the dialogue segments with abundant emotions and personalities. These dialogue segments were annotated in terms of emotions and personalities by 3 full-time staff of psychology major. In the following, we describe each processing stage of constructing CPED dataset: (1) collecting and prepocessing videos; (2) designing the annotation labels;\n\n(3) annotating the emotions and personalities; (4) ensuring annotation quality and re-annotating the overlapping utterance segments.\n\nA. Video Collection and Preprocessing a) Video Source: In the past, Chinese conversation datasets were obtained by crawling textual dialogues from the Internet. It is difficult to obtain multimodal dialogue data and annotate the emotions and personalities based on multimodal contexts. Therefore, we searched for 100 Chinese TV series closely related to daily life and finally selected 40 TV series that had abundant emotional interaction content and sufficient characters with distinctive personalities. b) Dialogue Segment Selection: We built a Windows application and designed a three-step filtering process to reduce the difficulty of video selection and promote the quality of dialogue segments. Each worker was asked to learn the filtering rules and pass an assessment on which they obtained at least a 98% pass rate in the premarking stage. First, each worker was asked to watch the video and mark the start time and end time of each potential dialogue sample through the developed application. Then, whether every potential dialogue sample was suitable for CPED would be confirmed by another worker. Finally, we split the videos into dialogue segments through the video editing tool MoviePy 2  .\n\nc) Subtitle Exaction: For most TV series, subtitles are embedded in videos and need to be transcribed to text using the optical character recognition (OCR) technique. We use the video OCR tool HTWCore 3  to generate the subtitles of each dialogue segment. Thus, we obtain the dialogue segments and their subtitles to annotate the emotions, DAs, and personalities. B. Annotation Scheme a) Annotation Label: In order for the dialogue system to learn emotional expression and personalized expression abilities, we provide multiple types of annotation labels listed in Table  II : sentiments, emotions, personalities (gender, age group and BF), DAs and scenes. We consider \"positive, neutral, and negative\" as the sentiment labels that are the same as MELD  [40] . In general, the emotion labels of conversation datasets are considered from among Ekman's six basic emotions (joy, sad, feared, angry, surprise, and disgusted)  [54] . However, the latest studies, e.g., 32 emotion labels in EmpatheticDialogues  [41]  and 14 emotion labels in MEmoR  [55] , show that more fine-grained emotion annotation can contribute to research on emotional reasoning and empathetic conversation. Considering the diversity of emotional tags and the similarity of different tags, we selected 13 emotion labels referring to EmpatheticDialogues  [41]  and 19 DA labels referring to the SWBD-DAMSL tag-set  [56]  based on the characteristics of Chinese open-domain conversation. In particular, we have added two special labels, \"other-positive\" and \"othernegative\", which allow uncommon emotions to be included. Personality is complex and changeable, and there is no unified  b) Annotation Process: The annotation process is divided into two stages: (1) utterance-level annotation and (2) speakerlevel annotation. First, we ask annotators to label the sentiments, emotions, DAs and scenes of each utterance. Second, when the dialogue samples of a TV series have been annotated, the experts are asked to annotate the gender, age group and Big Five of each character that appears in the dialogue samples. In particular, the Chinese Big Five Inventory-2 (Chinese BFI-2)  [57]  proposed by Zhang et al. is used for calculating the scores of Big Five personalities. Annotators were asked to fill Chinese BFI-2 for each speaker. The normalized average of the final score is used to judge the personality traits (high, unknown or low).",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "C. Annotation Tool",
      "text": "We built two Windows applications for dialogue segment and annotation by using the PyQt 4 tool, as shown in Figure  2  and Figure  3 . In the dialogue segment cutting stage, the annotators click the button \"打开视频 (open video)\", select an original video (about 40min), and then mark the start time and end time of the dialogue segment by repeatedly clicking the buttons \"对话开始 (start of dialogue)\" and \" 对话结束 (end of dialogue)\".\n\nAs shown in Figure  3 , annotators click \"open video\" to open a short dialogue video and the corresponding subtitle file. For each sentence, annotators need to select the sentiment, emotion and dialogue act. Meanwhile, they need to fill in the speaker's 4 https://www.riverbankcomputing.com/software/pyqt name of each sentence and the scene of the whole dialogue sample.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "D. Annotation Quality Control",
      "text": "To guarantee quality, we recruit three psychology experts who have a wealth of prior knowledge and experience for discriminating emotion, DA and personality. We jointly formulated labeling rules and labeling examples and randomly selected 200 samples for 3 rounds of prelabeling, thereby reducing the discrepancy in labeling by discussing and improving the annotation scheme. Following  [40] , experts are required to annotate utterances with multi-modal information that combines video, facial expressions, audio and text, which can help improve the emotional annotation accuracy. Each utterance was annotated by 3 experts, and the majority rule was used to determine the final labels. If the labeling results of the three experts are inconsistent, they needed to reannotate those utterances to find a \"common\" annotation. Finally, samples that still could not be labeled uniformly were discarded. In addition, since some speakers rarely speak, they will be uniformly defined as \"其他 (other)\", of which the gender, age group, and Big Five personality will be annotated as \"unknown\". Finally, we include a total of 11,835 dialogues with multi-source knowledge.  a) Utterance Overlap Processing: Automatic subtitle extraction will be accompanied by utterance overlap, which means that one utterance contains the content of two speakers talking (Table  III ). The statistics indicated that there were 4,613 utterance overlaps identified by annotators during the construction of the entire dataset. These utterance samples were correctly cut into multiple utterances, and the emotions and DAs were respectively reannotated.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "E. Corpus Exploration A) Dataset Split:",
      "text": "We randomly split the CPED dataset into three sets: train, valid and test according to the ratio of 7:1:2. In order to avoid data leakage, the split of the dataset is based on TV series, which ensures that the speakers in the training set will not appear in the valid/test set.\n\nb) Dataset Statistics: Figure  4  presents the distribution of the genders, ages groups, sentiments, emotions and DAs of the CPED dataset. The ratio of males to females is close to 1:1, which makes the distribution of personality and emotion close to the real world. Similar to other conversation datasets, the distribution of emotion and DA labels are unbalanced. Among them, \"neutral\" accounts for 32.4% of all emotions. The   5 , the proportion of high is higher than that of low in Extraversion, Openness, Agreeableness and Conscientiousness, while lower in Neuroticism.\n\nNeu.\n\nExt.\n\nOpe.\n\nAgr.\n\nCon.\n\nHigh Unkown Low",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Iv. Personality And Emotion Recognition In Conversations",
      "text": "We are committed to making the dialogue system acquire cognitive ability like human, including understanding the personalities of the speaker and speaker's current emotion through conversations. Therefore, we research two subtasks respectively: personality recognition in conversations (PRC) and emotion recognition in conversations (ERC).",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "A. Personality Recognition In Conversations (Prc)",
      "text": "a) Task Definition: Given a speaker's conversation with others, it is required to recognize the speaker's personality traits through the conversation record, which includes two scenarios, (1) 1 -1 conversations: the robot recognizes the personality traits of the speaker through the conversation between them (e.g., psychological counseling), (2) 1 -N ...  conversations (see Figure  6 ): the robot listens to the speaker's conversations with other N people and then recognizes the speaker's personality traits (e.g., group chatbot, home service robot). Since 1 -N includes the case of 1 -1, we only discusses PRC in 1 -N conversations. The task of PRC in 1 -N conversations can be formulated as:\n\nwhere P er i = [N eu, Ext, Ope, Agr, Con] is a 5-dimensional vector representing Neuroticism, Extraversion, Openness, Agreeableness, and Conscientiousness. C i,j is the conversations between Speaker i and Speaker j (1 ≤ j ≤ N ). b) Baseline Models: Several benchmack models are provided for PRC task, including 1) BERT s only uses the concatenation of the utterances of the target Speaker i  [47]  as the input to BERT  [58] , while BERT c uses the full conversation C i,j in the natural order. The personality P er i of Speaker i is calculated as follows:\n\nwhere T i,j is U i,j for BERT s and C i,j for BERT c .\n\nis the concatenation of the utterances of the target Speaker i in conversation C i,j . [CLS] is the special token of BERT. M LP k is a multi-layer perceptron. Avgpooling means average pooling. k is the index of personality in different dimensions.\n\n2) BERT c ssenet uses the full conversation text as the input to BERT and a shared SeNet  [59]  as the feature fusion layer of local personality features of different conversations, which can be formulated as:\n\nwhere k ∈  [1, 5]  is the dimension index of personalities.\n\n3) BERT c senet uses the full conversation text as the input to BERT and five independent SeNets as the feature fusion layer of local personality features of different conversations, which can be formulated as:   VI  shows the performance of the baseline models on PRC task. Comparing the performance of BERT s and BERT c , it can be found that BERT s achieves the better performance in Openness (57.93%) and Agreeableness (85.76%) while BERT c achieves the better performance in Neuroticism (55.29%). In the two dimensions of Extraversion and Conscientiousness, the performance of BERT s and BERT c is basically the same. Future work should needs to set up different recognition frameworks for different dimensions of BF, which requires further psychological analysis. Among the four baseline models, BERT c ssenet achieves the best performance in Macro-F1 and average accuracy. Among all the dimensions, the performances on Extraversion and Agreeableness are much higher than others because the two dimensions are more consistent when the speaker communicates with different characters. The future research on PRC in 1-N conversations needs to further consider the demonstrated personalities are different when the speaker talk with different characters.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "B. Emotion Recognition In Conversations (Erc)",
      "text": "a) Task Definition: ERC task focuses on identifying the sentiment-level or emotion-level labels e M of the utterance u M according to the conversation context\n\nwhere u i is the utterance contains several tokens spoken by Speaker(u i ). M is the number of the utterances in the conversation.\n\nb) Baseline Models: In order to provide an effective benchmark, we consider two types of benchmark models: ERC models with current single utterance as input and ERC models with current utterance and dialogue history as input.\n\n1) ERC models with current single utterance u M as input, include TextCNN  [60] , TextRNN  [61] , TextRCNN  [62] , FastText  [63] , BERT  [58] . They use sentence-level language model LM to obtain the representation of u M , and then use M LP to predict the emotion e M as follows:\n\n2) ERC models with both current utterance and dialogue history include bcLSTM  [64] , DialogueRNN  [65] , DialogueGCN  [66] , DialogXL  [67]  and EmoBERTa  [68] .\n\nbcLSTM is a bi-directional contextual LSTM model that has two unidirectional and opposite-direction LSTMs stacked together. DialogueRNN is a classic and efficient algorithm for ERC, which uses three gated recurrent units (GRU) to model the dialogue process, including the global GRU, the party GRU and the emotion GRU. DialogueGCN captures richer contextual information by considering the speaker information of the utterance and the relative positions of the target utterance and the context, which has three stages, consisting of sequential context encoding, speaker-level context encoding and classification. DialogXL used pre-trained language models for ERC, in which the memory-saving utterance recurrence mechanism and dialog-aware self-attention are used. EmoBERTa is a speaker-aware model based on RoBERTa, which prepends speaker names to utterances. We also proposed a baseline model BERT+AVG+MLP based on BERT  [58] , which adds all the speaker names to the special token dictionary and splices the speaker names and utterances sequentially as the input of BERT. The average pooling of hidden-layer output of BERT then inputs to multi-layer perceptron (MLP) to predict the emotion labels.   VII . Considering only the current utterance for emotion recognition, FastText achieves the state-of-the-art performance for negative emotion while poor performance on the emotion classes neutral(24.76) and positive(0.95). Other utterance-level models have the same defects, mainly because the ability of these models to deal with label imbalance (see Figure  4(c) ) is weak. The dialogue-level models can better handle the adverse effects caused by label imbalance. For example, BERT+AVG+MLP achieves the state-of-theart performances in average accuracy and Macro-F1, since it has relatively good performance in three emotional polarities. The emotion consistence and emotion mutation also affect the performance of dialogue-level models. The probability of emotion transition is shown in Figure  7 . The probability of emotion mutation in negative, neutral and positive are 0.225, 0.337 and 0.427 respectively, which makes the ERC task significantly different from other long text emotion recognition tasks. In the future, it is necessary to further study the influence and challenge of emotion consistence and emotion mutation in dialogue to ERC task.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Neg.",
      "text": "Neu. Pos.\n\nNeg.\n\nNeu.\n\nPos.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Contextual",
      "text": "",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "V. Personalized And Emotional Conversation",
      "text": "In this section, we provide several benchmarks for the Personalized and Emotional Conversation (PEC) task on the proposed CPED. Conversation generation models can usually be divided into retrieval-based  [69] ,  [70]  and generative  [2] ,  [3] ,  [71] . As shown in Figure  8 , generative conversation models can be divided into three types: (1) w/o control signal  [2] ,  [72] , (2) implicit embedding  [3] ,  [73] ,  [74] , and (3) explicit fusion  [1] ,  [52] . Generally, the latter two architectures are used for personalized conversation generation or emotional conversation generation.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "A. Task Definition",
      "text": "We research enabling the conversation generation system to generate more anthropomorphic reply content by infusing emotion and personality at the same time. Personalized and Emotional Conversation (PEC) is defined as follows: Given the personalized information (P R1 and P R2 ) of two speakers, their conversation context C, the emotion E K and DA D K of the response to be generated, and the personalized information P K of the responder, the goal is to generate an anthropomorphic response Y .\n\nParticularly, context\n\n)} contains multi-turn conversation content (i.e., utterance U i ), emotion E i of the associated utterance, DA D i of the associated utterance, and personalized information P i of the associated speaker.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "B. Baseline Models",
      "text": "As shown in Figure  8 , we compare several categories of generative models and our method in CPED: a) w/o Control Signal: (1) Seq2Seq  [75] , the classical dialogue generation model we selected, is widely used in conversation generation. (2) Transformer  [76] , the second model that we evaluate, is an encoder-decoder framework based on a self-attention mechanism. The transformer has been widely applied in machine translation  [76] , language modeling  [58] , dialogue generation, etc. (3) GPT  [2]  has recently gradually been used in the field of dialog generation  [2] ,  [7] . Following  [7] , we fine-tune CDial-GPT on the CPED dataset.\n\nb) Implicit Embedding: {emo+da}-GPT is the proposed method inspired by  [3]  that adds word embeddings E w , segmentation embeddings E seq , position embeddings E pos , emotion embeddings E emo and DA embeddings E da together as the input embeddings for GPT:\n\nc) Explicit Fusion: GPT-{per+emo+da} is the proposed method that infuses emotion E K and DA D K of the response to be generated and the personalized information P K of the responder. For the emotion and DA, we constructed the embedding matrix separately to obtain emotion embedding E g and DA embedding D g , respectively. The embedding of personalized information is computed by a two-layer M LP ( * ) to project P K to word embedding space P g as follows:\n\nSubsequently, emotion embedding E g , DA embedding D g and personalization embedding P g are concatenated together and then infused by a M LP ( * ) to generate control vector C g :\n\nWe design a conditional layer to control the text generation:\n\nwhere O is the output of the last hidden layer of the language model (transformer or GPT, etc.). R g denotes the role of the responder, which is the word embedding of \"[speaker1]\" or \"[speaker2]\". is element-wise multiplication. g ∈ [0, 1] denotes the condition weight as follows:\n\nwhere σ( * ) is an activation function (e.g., T anh( * )).",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "C. Implementation Details",
      "text": "We use transformers 5    [77]  and CDial-GPT  6  to implement the baseline model. Emotion and DA labels are added to the dictionary as special characters through the function add special tokens of transformers for {emo+da}-GPT. The dimension of the word embeddings is set to 768, and the input length is ≤ 512 tokens. The dropout rate is set to 0.1, and the total number of training epochs is set to 120. We used the AdamW optimizer with β 1 = 0.9, β 2 = 0.999 and the Noam learning rate scheduler  [76]  with warmup steps = 10000. We conduct experiments on Ubuntu 18.04 with 2 GeForce RTX 2080ti GPUs.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "D. Automatic Evaluation A) Metrics:",
      "text": "The perplexity (PPL) and BLEU  [78]  are used to evaluate the relevance and fluency of the generated responses, respectively. Then, distinct-n (D-1, D-2)  [79]  is applied to evaluate the degree of diversity. Greedy matching (Gre.), embedding average (Avg.)  [80]  and F BERT of BERTscore (BERT.)  [81]  are used to evaluate the semanticlevel relevance of the generated responses and the reference responses.\n\nb) Results: The results in Table  VIII  show that it is better to explicitly infuse the emotions and personalities of the response to be generated into the conversation model than implicitly embed them. Compared to the baseline model GPT, GPT-emo achieves the best PPL (2.59↓), D-1 (0.0132↑) and D-2 (0.0692↑); GPT-{per+emo} achieves the best Gre. (0.0104↑) and Avg. (0.0108↑); and GPT-{per+emo+da} achieves the best BERT. (0.0093↑). The results demonstrate the superiority and effectiveness of explicitly infusing emotions and personalities into open-domain conversation generation.\n\nE. Manual Evaluation a) Metrics: Three individual experts majoring in Chinese language and literature were asked to evaluate the generated responses in terms of content consistency (Con.), emotion correlation (Emo.) and personification capabilities (Per.). Con.\n\ndenotes the consistency of the topic and content according to the conversation context. Emo. denotes the emotional relevance and rationality of the response generated by the dialogue system. Per. denotes the personification capabilities of the dialogue system and is applied to measure the humanlike expression ability. The rating scale is (0, 1, 2), where 0 means the worst and 2 means the best. b) Results: Two hundred dialogues were randomly sampled from the test set of CPED for manual evaluation. Fleiss' kappa  [82]  is calculated to measure the inter-rater consistency for Con., Emo., and Per., which are 0.658, 0.632 and 0.646, indicating substantial annotation agreement respectively. Table  VIII  shows the results of the manual evaluation in terms of content, emotion and personification. We observe that GPT-{per+emo+da} achieves the best Con. (0.272↑) and the best Per. (0.477↑) compared with GPT while GPT-{per+emo} achieves the best Emo. (0.335↑). This demonstrates that \"explicit fusion\" can effectively benefit the conversation generation model to generate more anthropomorphic responses. Furthermore, explicitly specifying the emotion and personality of the responses will improve the emotional expression ability and personality expression ability of the dialogue system.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "F. Case Study",
      "text": "In Table IX, we present an example of the answers generated by the baseline models to give insight into whether the emotion and personality of the generated responses are expressed appropriately. The table shows that GPT-{per+emo+da} can generate highly anthropomorphic responses (e.g., 你 想 得 美。(When pigs fly!)) with appropriate emotion and personality while the GPT could not express the emotion \"anger\" with the generated response \"我还是想要你的。(I still want yours.)\". In other words, when the emotion and DA of a response are generated and the personalities of the responder are explicitly infused into the conversation generation model, the model can perform with a high personification level and suitable emotional expression.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Vi. Applications And Limitation Of Cped",
      "text": "",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "A. Applications",
      "text": "CPED allows evaluation of both conversational cognitive tasks and conversation generation tasks, e.g. speaker modeling, personality recognition in conversations, emotion recognition in conversations, DA recognition in conversations, emotion prediction for response, emotional conversation generation, personalized conversation generation, empathetic conversation etc. By being multimodal, CPED can also be applied in multimodal personality or emotion recognition, multimodal conversation generation. It will play a positive role in promoting the development of cognitive intelligence.\n\nB. Ethical Considerations a) Data and Privacy: All the dialogue materials are based on TV dramas (publicly available source: Tencent Video 7  , Youku Video 8  , iQiyi Video 9  ) in which the names of the characters are all fictitious. Correspondingly, the personalities are also marked from the performance of the characters in the TV dramas. The video and audio clips are licensed under Copyright Law of the People's Republic of China. According to the privacy issues, copyright claims, terms of service of Tencent Video, Youku Video and iQiyi Video, we only release the textual dataset with audio features and video features.\n\nb) Difference between television conversation and natural conversation: Similar to FriendsPersona  [47]  and MELD  [40] , CPED is derived from TV shows. According to the book Television Dialogue: The sitcom Friends vs natural conversation  [83] , television conversations and natural conversations are basically the same in terms of linguistic features. However, television conversations tend to present a limited set of scenarios, interaction types and topic categories. To this end, when we select TV series, we try to cover different scenes of daily life as much as possible (see Table  II ). In addition, due to the entertainment characteristics of TV shows, screenwriters often use expletive, slang, appellation and other language means to achieve humorous effects, in order to make the language in TV shows more authentic and attractive. Therefore, the emotion distribution of television conversations c) Potential bias and Ethical Risk: We realize that if the model learns anthropomorphic expression ability, it may also learn the negative expressions or dangerous expressions brought about by personality. Negative responses represent those responses that make the emotions of both sides of the conversation develop in a worse direction. Dangerous responses represent those types of responses that involve suicide, abetting others to commit suicide, intimidation, etc. As shown in Table X, we randomly selected 200 samples from the test set and counted the proportions of negative responses and dangerous responses. It is foreseeable that by improving the personification level of the dialogue generation model, it is also possible for the dialogue model to learn those risk responses. When using the CPED dataset, users should consider how to reduce the possibility of risk responses from the dialogue system while improving the level of personification of the dialogue system.",
      "page_start": 11,
      "page_end": 12
    },
    {
      "section_name": "Vii. Conclusion And Future Work",
      "text": "In this paper, we proposed a challenging dataset CPED for conversational AI, a large-scale Chinese personalized and emotional dialogue dataset containing more than 11K dialogues with 392 speakers from 40 TV shows. CPED contains abundant prior information about emotions, personalities, dialog acts and other items. We introduce three challenging tasks for conversational AI research in CPED, including personality recognition, emotion recognition in conversations as well as personalized and emotional conversation generation. The evaluation results of the baseline models are initial but indicative. In Chinese conversations, the tasks of personality recognition and emotion recognition need to be further with linguistic characteristics and psychological knowledge, e.g. differences in different personality dimensions, differences of the demonstrated personalities or emotions in conversations between speakers and different characters, linguistic characteristics of different sentiment polarities and etc. For personalized and emotional conversation, explicitly infusing emotions, personalities and dialog acts of the response to be generated can improve the personification level and emotional expression of a dialogue system. We believe that CPED can help researchers study both the cognitive processing in conversations and the personalized and emotional conversation (PEC) task. Based on the abundant emotions, personalities, and multimodal contexts of CPED, future work can explore the following: (i) modeling or recognition of speakers' personality and emotion, (ii) prediction of responded emotion and personality, (iii) personalized and emotional conversation generation using multimodal contexts, (iv) pretrained PEC model for empathetic conversation or mental health support, etc.",
      "page_start": 12,
      "page_end": 12
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Example from CPED dataset. The dialogue consists of quadruples",
      "page": 1
    },
    {
      "caption": "Figure 1: , ”speaker1”",
      "page": 2
    },
    {
      "caption": "Figure 1: ) during which the emotional state or DA state may",
      "page": 2
    },
    {
      "caption": "Figure 2: Tools for dialogue segment selection.",
      "page": 4
    },
    {
      "caption": "Figure 2: and Figure 3. In the dialogue segment cutting stage, the",
      "page": 5
    },
    {
      "caption": "Figure 3: , annotators click ”open video” to open",
      "page": 5
    },
    {
      "caption": "Figure 3: Conversation annotation application.",
      "page": 5
    },
    {
      "caption": "Figure 4: Distribution of Gender, Age Group, Sentiment, Emotion and DA in CPED Dataset.",
      "page": 6
    },
    {
      "caption": "Figure 4: presents the distribution",
      "page": 6
    },
    {
      "caption": "Figure 5: Distribution statistics of Big Five (Neu.: Neuroticism, Ext.: Extraver-",
      "page": 7
    },
    {
      "caption": "Figure 6: Personality recognition in 1 −N conversations.",
      "page": 7
    },
    {
      "caption": "Figure 6: ): the robot listens to the speaker’s",
      "page": 7
    },
    {
      "caption": "Figure 4: (c)) is weak. The dialogue-level models can",
      "page": 8
    },
    {
      "caption": "Figure 7: The probability of",
      "page": 8
    },
    {
      "caption": "Figure 7: Probability distribution of emotion transition from context (maximum",
      "page": 9
    },
    {
      "caption": "Figure 8: , generative conversation",
      "page": 9
    },
    {
      "caption": "Figure 8: , we compare several categories of",
      "page": 9
    },
    {
      "caption": "Figure 8: The generic framework of PEC. Three type of generative dialogue generation model are devised. External signal represents emotion, personality, DA",
      "page": 10
    },
    {
      "caption": "Figure 9: According to the statistics, most",
      "page": 15
    },
    {
      "caption": "Figure 9: Relation between the Emotions and DAs.",
      "page": 15
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "0.775\n0.239\n0.216": "0.152"
        },
        {
          "0.775\n0.239\n0.216": "0.073"
        }
      ],
      "page": 9
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Emotional Chatting Machine: Emotional conversation generation with internal and external memory",
      "authors": [
        "H Zhou",
        "M Huang",
        "T Zhang",
        "X Zhu",
        "B Liu"
      ],
      "year": "2018",
      "venue": "Thirty-Second AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "2",
      "title": "DIALOGPT : Large-scale generative pre-training for conversational response generation",
      "authors": [
        "Y Zhang",
        "S Sun",
        "M Galley",
        "Y.-C Chen",
        "C Brockett",
        "X Gao",
        "J Gao",
        "J Liu",
        "B Dolan"
      ],
      "year": "2020",
      "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations"
    },
    {
      "citation_id": "3",
      "title": "A pre-training based personalized dialogue generation model with persona-sparse data",
      "authors": [
        "Y Zheng",
        "R Zhang",
        "X Mao",
        "M Huang"
      ],
      "year": "2020",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "4",
      "title": "News from opus -a collection of multilingual parallel corpora with tools and interfaces",
      "authors": [
        "J Tiedemann"
      ],
      "year": "2009",
      "venue": "Recent Advances in Natural Language Processing V: Selected papers from RANLP 2007"
    },
    {
      "citation_id": "5",
      "title": "The Ubuntu dialogue corpus: A large dataset for research in unstructured multi-turn dialogue systems",
      "authors": [
        "R Lowe",
        "N Pow",
        "I Serban",
        "J Pineau"
      ],
      "year": "2015",
      "venue": "Proceedings of the 16th Annual Meeting of the Special Interest Group on Discourse and Dialogue"
    },
    {
      "citation_id": "6",
      "title": "Neural responding machine for short-text conversation",
      "authors": [
        "L Shang",
        "Z Lu",
        "H Li"
      ],
      "year": "2015",
      "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing"
    },
    {
      "citation_id": "7",
      "title": "A large-scale chinese short-text conversation dataset",
      "authors": [
        "Y Wang",
        "P Ke",
        "Y Zheng",
        "K Huang",
        "Y Jiang",
        "X Zhu",
        "M Huang"
      ],
      "year": "2020",
      "venue": "CCF International Conference on Natural Language Processing and Chinese Computing(NLPCC2020"
    },
    {
      "citation_id": "8",
      "title": "OpenViDial: A large-scale, open-domain dialogue dataset with visual contexts",
      "authors": [
        "Y Meng",
        "S Wang",
        "Q Han",
        "X Sun",
        "F Wu",
        "R Yan",
        "J Li"
      ],
      "year": "2020",
      "venue": "OpenViDial: A large-scale, open-domain dialogue dataset with visual contexts"
    },
    {
      "citation_id": "9",
      "title": "Cognitive psychology: Applying the science of the mind",
      "authors": [
        "B Robinson-Riegler",
        "G Robinson-Riegler"
      ],
      "year": "2016",
      "venue": "Cognitive psychology: Applying the science of the mind"
    },
    {
      "citation_id": "10",
      "title": "DailyDialog: A manually labelled multi-turn dialogue dataset",
      "authors": [
        "Y Li",
        "H Su",
        "X Shen",
        "W Li",
        "Z Cao",
        "S Niu"
      ],
      "year": "2017",
      "venue": "Proceedings of the Eighth International Joint Conference on Natural Language Processing"
    },
    {
      "citation_id": "11",
      "title": "The big five personality dimensions and job performance: A meta-analysis",
      "authors": [
        "M Barrick",
        "M Mount"
      ],
      "year": "1991",
      "venue": "Personnel Psychology",
      "doi": "10.1111/j.1744-6570.1991.tb00688.x"
    },
    {
      "citation_id": "12",
      "title": "Personality traits: Their classification and measurement",
      "authors": [
        "F Allport",
        "G Allport"
      ],
      "year": "1921",
      "venue": "The Journal of Abnormal Psychology and Social Psychology",
      "doi": "10.1037/h0069790"
    },
    {
      "citation_id": "13",
      "title": "Personality structure and the new fifth edition of the 16pf",
      "authors": [
        "R Cattell",
        "H Cattell"
      ],
      "year": "1995",
      "venue": "Educational and Psychological Measurement",
      "doi": "10.1177/0013164495055006002"
    },
    {
      "citation_id": "14",
      "title": "The structure of human personality",
      "authors": [
        "H Eysenck"
      ],
      "year": "1953",
      "venue": "The structure of human personality"
    },
    {
      "citation_id": "15",
      "title": "Manual of the eysenck personality questionnaire",
      "authors": [
        "H Eysenck",
        "S Eysenck"
      ],
      "year": "1984",
      "venue": "Journal of Personality Assessment",
      "doi": "10.1037/t05462-000"
    },
    {
      "citation_id": "16",
      "title": "Recurrent personality factors based on trait ratings",
      "authors": [
        "E Tupes"
      ],
      "year": "1992",
      "venue": "Journal of personality",
      "doi": "10.1111/j.1467-6494.1992.tb00973.x"
    },
    {
      "citation_id": "17",
      "title": "Toward an adequate taxonomy of personality attributes: Replicated factor structure in peer nomination personality ratings",
      "authors": [
        "W Norman"
      ],
      "year": "1963",
      "venue": "The journal of abnormal and social psychology",
      "doi": "10.1037/h0040291"
    },
    {
      "citation_id": "18",
      "title": "Personality trait structure as a human universal",
      "authors": [
        "R Mccrae",
        "P Costa"
      ],
      "year": "1997",
      "venue": "American psychologist",
      "doi": "10.1037/0003-066X.52.5.509"
    },
    {
      "citation_id": "19",
      "title": "Four ways five factors are basic",
      "authors": [
        "P Costa",
        "R Mccrae"
      ],
      "year": "1992",
      "venue": "Personality and individual differences",
      "doi": "10.1016/0191-8869(92)90236-I"
    },
    {
      "citation_id": "20",
      "title": "Hogan personality inventory manual",
      "authors": [
        "R Hogan"
      ],
      "year": "1992",
      "venue": "Hogan Assessment Systems",
      "doi": "10.1037/t02029-000"
    },
    {
      "citation_id": "21",
      "title": "The\" big five\" factor taxonomy: Dimensions of personality in the natural language and in questionnaires",
      "authors": [
        "O John"
      ],
      "year": "1990",
      "venue": "Handbook of personality: Theory and research"
    },
    {
      "citation_id": "22",
      "title": "Re-examining basic dimensions of natural language trait descriptors",
      "authors": [
        "A Tellegen",
        "N Waller"
      ],
      "year": "1987",
      "venue": "Re-examining basic dimensions of natural language trait descriptors"
    },
    {
      "citation_id": "23",
      "title": "Relations of masculinity and femininity with personality dimensions of the five-factor model",
      "authors": [
        "I Marusic",
        "D Bratko"
      ],
      "year": "1998",
      "venue": "Sex roles",
      "doi": "10.1023/A:1018708410947"
    },
    {
      "citation_id": "24",
      "title": "Age differences in personality traits from 10 to 65: Big five domains and facets in a large cross-sectional sample",
      "authors": [
        "C Soto",
        "O John",
        "S Gosling",
        "J Potter"
      ],
      "year": "2011",
      "venue": "Journal of personality and social psychology",
      "doi": "10.1037/a0021717"
    },
    {
      "citation_id": "25",
      "title": "The nature of emotions: Human emotions have deep evolutionary roots, a fact that may explain their complexity and provide tools for clinical practice",
      "authors": [
        "R Plutchik"
      ],
      "year": "2001",
      "venue": "American scientist"
    },
    {
      "citation_id": "26",
      "title": "The psychology of emotions",
      "authors": [
        "C Izard"
      ],
      "year": "1991",
      "venue": "The psychology of emotions"
    },
    {
      "citation_id": "27",
      "title": "Emotion recognition from multiple modalities: Fundamentals and methodologies",
      "authors": [
        "S Zhao",
        "G Jia",
        "J Yang",
        "G Ding",
        "K Keutzer"
      ],
      "year": "2021",
      "venue": "IEEE Signal Processing Magazine",
      "doi": "10.1109/MSP.2021.3106895"
    },
    {
      "citation_id": "28",
      "title": "A circumplex model of affect",
      "authors": [
        "J Russell"
      ],
      "year": "1980",
      "venue": "Journal of personality and social psychology",
      "doi": "10.1037/h0077714"
    },
    {
      "citation_id": "29",
      "title": "Three dimensions of emotion",
      "authors": [
        "H Schlosberg"
      ],
      "year": "1954",
      "venue": "Psychological review",
      "doi": "10.1037/h0054570"
    },
    {
      "citation_id": "30",
      "title": "Pleasure-arousal-dominance: A general framework for describing and measuring individual differences in temperament",
      "authors": [
        "A Mehrabian"
      ],
      "year": "1996",
      "venue": "Current Psychology",
      "doi": "10.1007/BF02686918"
    },
    {
      "citation_id": "31",
      "title": "Affects as primary motivational system",
      "authors": [
        "S Tomkins"
      ],
      "year": "1970",
      "venue": "Affects as primary motivational system"
    },
    {
      "citation_id": "32",
      "title": "Constants across cultures in the face and emotion",
      "authors": [
        "P Ekman",
        "W Friesen"
      ],
      "year": "1971",
      "venue": "Journal of personality and social psychology",
      "doi": "10.1037/h0030377"
    },
    {
      "citation_id": "33",
      "title": "Emotion: Theory, research, and experience",
      "authors": [
        "R Plutchik",
        "H Kellerman"
      ],
      "year": "1980",
      "venue": "Theories of Emotion",
      "doi": "10.1016/B978-0-12-558701-3.50001-6"
    },
    {
      "citation_id": "34",
      "title": "A neural network approach to context-sensitive generation of conversational responses",
      "authors": [
        "A Sordoni",
        "M Galley",
        "M Auli",
        "C Brockett",
        "Y Ji",
        "M Mitchell",
        "J.-Y Nie",
        "J Gao",
        "B Dolan"
      ],
      "year": "2015",
      "venue": "Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies"
    },
    {
      "citation_id": "35",
      "title": "Chameleons in imagined conversations: A new approach to understanding coordination of linguistic style in dialogs",
      "authors": [
        "C Danescu-Niculescu-Mizil",
        "L Lee"
      ],
      "year": "2011",
      "venue": "Proceedings of the 2nd Workshop on Cognitive Modeling and Computational Linguistics"
    },
    {
      "citation_id": "36",
      "title": "Sequential matching network: A new architecture for multi-turn response selection in retrieval-based chatbots",
      "authors": [
        "Y Wu",
        "W Wu",
        "C Xing",
        "M Zhou",
        "Z Li"
      ],
      "year": "2017",
      "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "37",
      "title": "EVA: an open-domain chinese dialogue system with large-scale generative pre-training",
      "authors": [
        "H Zhou",
        "P Ke",
        "Z Zhang",
        "Y Gu",
        "Y Zheng",
        "C Zheng",
        "Y Wang",
        "C Wu",
        "H Sun",
        "X Yang",
        "B Wen",
        "X Zhu",
        "M Huang",
        "J Tang"
      ],
      "year": "2021",
      "venue": "CoRR"
    },
    {
      "citation_id": "38",
      "title": "IEMOCAP: interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language Resources and Evaluation",
      "doi": "10.1007/s10579-008-9076-6"
    },
    {
      "citation_id": "39",
      "title": "Multi-task dialog act and sentiment recognition on mastodon",
      "authors": [
        "C Cerisara",
        "S Jafaritazehjani",
        "A Oluokun",
        "H Le"
      ],
      "year": "2018",
      "venue": "CoRR"
    },
    {
      "citation_id": "40",
      "title": "MELD: A multimodal multi-party dataset for emotion recognition in conversations",
      "authors": [
        "S Poria",
        "D Hazarika",
        "N Majumder",
        "G Naik",
        "E Cambria",
        "R Mihalcea"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "41",
      "title": "Towards empathetic open-domain conversation models: A new benchmark and dataset",
      "authors": [
        "H Rashkin",
        "E Smith",
        "M Li",
        "Y.-L Boureau"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "42",
      "title": "Towards emotionaided multi-modal dialogue act classification",
      "authors": [
        "T Saha",
        "A Patra",
        "S Saha",
        "P Bhattacharyya"
      ],
      "year": "2020",
      "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "43",
      "title": "M3ED: Multi-modal multi-scene multi-label emotional dialogue database",
      "authors": [
        "J Zhao",
        "T Zhang",
        "J Hu",
        "Y Liu",
        "Q Jin",
        "X Wang",
        "H Li"
      ],
      "year": "2022",
      "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "44",
      "title": "Personalizing dialogue agents: I have a dog, do you have pets too",
      "authors": [
        "S Zhang",
        "E Dinan",
        "J Urbanek",
        "A Szlam",
        "D Kiela",
        "J Weston"
      ],
      "year": "2018",
      "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "45",
      "title": "Towards personabased empathetic conversational models",
      "authors": [
        "P Zhong",
        "C Zhang",
        "H Wang",
        "Y Liu",
        "C Miao"
      ],
      "year": "2020",
      "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)"
    },
    {
      "citation_id": "46",
      "title": "Memor: A dataset for multimodal emotion reasoning in videos",
      "authors": [
        "G Shen",
        "X Wang",
        "X Duan",
        "H Li",
        "W Zhu"
      ],
      "year": "2020",
      "venue": "Proceedings of the 28th ACM International Conference on Multimedia, ser. MM '20",
      "doi": "10.1145/3394171.3413909"
    },
    {
      "citation_id": "47",
      "title": "Automatic text-based personality recognition on monologues and multiparty dialogues using attentive networks and contextual embeddings (student abstract)",
      "authors": [
        "H Jiang",
        "X Zhang",
        "J Choi"
      ],
      "year": "2020",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence",
      "doi": "10.1609/aaai.v34i10.7182"
    },
    {
      "citation_id": "48",
      "title": "Automatically select emotion for response via personality-affected emotion transition",
      "authors": [
        "Z Wen",
        "J Cao",
        "R Yang",
        "S Liu",
        "J Shen"
      ],
      "year": "2021",
      "venue": "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021"
    },
    {
      "citation_id": "49",
      "title": "Personalized dialogue generation with diversified traits",
      "authors": [
        "Y Zheng",
        "G Chen",
        "M Huang",
        "S Liu",
        "X Zhu"
      ],
      "year": "2020",
      "venue": "Personalized dialogue generation with diversified traits"
    },
    {
      "citation_id": "50",
      "title": "EDA: Enriching emotional dialogue acts using an ensemble of neural annotators",
      "authors": [
        "C Bothe",
        "C Weber",
        "S Magg",
        "S Wermter"
      ],
      "year": "2020",
      "venue": "LREC"
    },
    {
      "citation_id": "51",
      "title": "An affect-rich neural conversational model with biased attention and weighted cross-entropy loss",
      "authors": [
        "P Zhong",
        "D Wang",
        "C Miao"
      ],
      "year": "2019",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence",
      "doi": "10.1609/aaai.v33i01.33017492"
    },
    {
      "citation_id": "52",
      "title": "Infusing multi-source knowledge with heterogeneous graph neural network for emotional conversation generation",
      "authors": [
        "Y Liang",
        "F Meng",
        "Y Zhang",
        "Y Chen",
        "J Xu",
        "J Zhou"
      ],
      "year": "2021",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "53",
      "title": "The development of personality traits in adulthood",
      "authors": [
        "B Roberts",
        "D Wood",
        "A Caspi"
      ],
      "year": "2008",
      "venue": "The development of personality traits in adulthood"
    },
    {
      "citation_id": "54",
      "title": "Universals and cultural differences in the judgments of facial expressions of emotion",
      "authors": [
        "P Ekman",
        "W Friesen",
        "M O\"sullivan",
        "A Chan",
        "E Al"
      ],
      "year": "1987",
      "venue": "Journal of Personality & Social Psychology"
    },
    {
      "citation_id": "55",
      "title": "Memor: A dataset for multimodal emotion reasoning in videos",
      "authors": [
        "G Shen",
        "X Wang",
        "X Duan",
        "H Li",
        "W Zhu"
      ],
      "year": "2020",
      "venue": "Proceedings of the 28th ACM international conference on Multimedia",
      "doi": "10.1145/3394171.3413909"
    },
    {
      "citation_id": "56",
      "title": "Switchboard SWBD-DAMSL shallow-discourse-function annotation coders manual, draft 13",
      "authors": [
        "D Jurafsky",
        "E Shriberg",
        "D Biasca"
      ],
      "year": "1997",
      "venue": "Switchboard SWBD-DAMSL shallow-discourse-function annotation coders manual, draft 13"
    },
    {
      "citation_id": "57",
      "title": "The big five inventory-2 in china: A comprehensive psychometric evaluation in four diverse samples",
      "authors": [
        "B Zhang",
        "Y Li",
        "J Li",
        "J Luo",
        "Y Ye",
        "L Yin",
        "Z Chen",
        "C Soto",
        "O John"
      ],
      "year": "2021",
      "venue": "Assessment",
      "doi": "10.1177/10731911211008245"
    },
    {
      "citation_id": "58",
      "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "J Devlin",
        "M.-W Chang",
        "K Lee",
        "K Toutanova"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies"
    },
    {
      "citation_id": "59",
      "title": "Squeeze-and-excitation networks",
      "authors": [
        "J Hu",
        "L Shen",
        "G Sun"
      ],
      "year": "2018",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "60",
      "title": "Convolutional neural networks for sentence classification",
      "authors": [
        "Y Kim"
      ],
      "year": "2014",
      "venue": "CoRR"
    },
    {
      "citation_id": "61",
      "title": "Recurrent neural network for text classification with multi-task learning",
      "authors": [
        "P Liu",
        "X Qiu",
        "X Huang"
      ],
      "year": "2016",
      "venue": "Proceedings of the Twenty-Fifth International Joint Conference on Artificial Intelligence, ser. IJCAI'16",
      "doi": "10.5555/3060832.3061023"
    },
    {
      "citation_id": "62",
      "title": "Recurrent convolutional neural networks for text classification",
      "authors": [
        "S Lai",
        "L Xu",
        "K Liu",
        "J Zhao"
      ],
      "year": "2015",
      "venue": "Twenty-ninth AAAI conference on artificial intelligence"
    },
    {
      "citation_id": "63",
      "title": "Bag of tricks for efficient text classification",
      "authors": [
        "A Joulin",
        "E Grave",
        "P Bojanowski",
        "T Mikolov"
      ],
      "year": "2017",
      "venue": "Proceedings of the 15th Conference of the European Chapter"
    },
    {
      "citation_id": "64",
      "title": "Context-dependent sentiment analysis in user-generated videos",
      "authors": [
        "S Poria",
        "E Cambria",
        "D Hazarika",
        "N Majumder",
        "A Zadeh",
        "L.-P Morency"
      ],
      "year": "2017",
      "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "65",
      "title": "Dialoguernn: An attentive rnn for emotion detection in conversations",
      "authors": [
        "N Majumder",
        "S Poria",
        "D Hazarika",
        "R Mihalcea",
        "A Gelbukh",
        "E Cambria"
      ],
      "year": "2019",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence",
      "doi": "10.1609/aaai.v33i01.33016818"
    },
    {
      "citation_id": "66",
      "title": "DialogueGCN: A graph convolutional neural network for emotion recognition in conversation",
      "authors": [
        "D Ghosal",
        "N Majumder",
        "S Poria",
        "N Chhaya",
        "A Gelbukh"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)"
    },
    {
      "citation_id": "67",
      "title": "Dialogxl: All-inone xlnet for multi-party conversation emotion recognition",
      "authors": [
        "W Shen",
        "J Chen",
        "X Quan",
        "Z Xie"
      ],
      "year": "2021",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "68",
      "title": "Emoberta: Speaker-aware emotion recognition in conversation with roberta",
      "authors": [
        "T Kim",
        "P Vossen"
      ],
      "year": "2021",
      "venue": "CoRR"
    },
    {
      "citation_id": "69",
      "title": "Learning to respond with deep neural networks for retrieval-based human-computer conversation system",
      "authors": [
        "R Yan",
        "Y Song",
        "H Wu"
      ],
      "year": "2016",
      "venue": "Proceedings of the 39th International ACM SIGIR Conference on Research and Development in Information Retrieval, ser. SIGIR '16",
      "doi": "10.1145/2911451.2911542"
    },
    {
      "citation_id": "70",
      "title": "Speaker-aware bert for multi-turn response selection in retrieval-based chatbots",
      "authors": [
        "J.-C Gu",
        "T Li",
        "Q Liu",
        "Z.-H Ling",
        "Z Su",
        "S Wei",
        "X Zhu"
      ],
      "year": "2020",
      "venue": "Proceedings of the 29th ACM International Conference on Information & Knowledge Management, ser. CIKM '20",
      "doi": "10.1145/3340531.3412330"
    },
    {
      "citation_id": "71",
      "title": "A neural network approach to context-sensitive generation of conversational responses",
      "authors": [
        "A Sordoni",
        "M Galley",
        "M Auli",
        "C Brockett",
        "Y Ji",
        "M Mitchell",
        "J.-Y Nie",
        "J Gao",
        "B Dolan"
      ],
      "year": "2015",
      "venue": "Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies"
    },
    {
      "citation_id": "72",
      "title": "An auto-encoder matching model for learning utterance-level semantic dependency in dialogue generation",
      "authors": [
        "L Luo",
        "J Xu",
        "J Lin",
        "Q Zeng",
        "X Sun"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "73",
      "title": "Emptransfo: A multi-head transformer architecture for creating empathetic dialog systems",
      "authors": [
        "R Zandie",
        "M Mahoor"
      ],
      "year": "2020",
      "venue": "The Thirty-Third International Flairs Conference"
    },
    {
      "citation_id": "74",
      "title": "CoMAE: A multi-factor hierarchical framework for empathetic response generation",
      "authors": [
        "C Zheng",
        "Y Liu",
        "W Chen",
        "Y Leng",
        "M Huang"
      ],
      "year": "2021",
      "venue": "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021"
    },
    {
      "citation_id": "75",
      "title": "Sequence to sequence learning with neural networks",
      "authors": [
        "I Sutskever",
        "O Vinyals",
        "Q Le"
      ],
      "year": "2014",
      "venue": "Proceedings of the 27th International Conference on Neural Information Processing Systems"
    },
    {
      "citation_id": "76",
      "title": "Attention is all you need",
      "authors": [
        "A Vaswani",
        "N Shazeer",
        "N Parmar",
        "J Uszkoreit",
        "L Jones",
        "A Gomez",
        "L Kaiser",
        "I Polosukhin"
      ],
      "year": "2017",
      "venue": "Proceedings of the 31st International Conference on Neural Information Processing Systems, ser. NIPS'17",
      "doi": "10.5555/3295222.3295349"
    },
    {
      "citation_id": "77",
      "title": "Transformers: State-ofthe-art natural language processing",
      "authors": [
        "T Wolf",
        "L Debut",
        "V Sanh",
        "J Chaumond",
        "C Delangue",
        "A Moi",
        "P Cistac",
        "T Rault",
        "R Louf",
        "M Funtowicz",
        "J Davison",
        "S Shleifer",
        "P Von Platen",
        "C Ma",
        "Y Jernite",
        "J Plu",
        "C Xu",
        "T Scao",
        "S Gugger",
        "M Drame",
        "Q Lhoest",
        "A Rush"
      ],
      "year": "2020",
      "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations"
    },
    {
      "citation_id": "78",
      "title": "Bleu: a method for automatic evaluation of machine translation",
      "authors": [
        "K Papineni",
        "S Roukos",
        "T Ward",
        "W.-J Zhu"
      ],
      "year": "2002",
      "venue": "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "79",
      "title": "A diversity-promoting objective function for neural conversation models",
      "authors": [
        "J Li",
        "M Galley",
        "C Brockett",
        "J Gao",
        "B Dolan"
      ],
      "year": "2016",
      "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies"
    },
    {
      "citation_id": "80",
      "title": "How NOT to evaluate your dialogue system: An empirical study of unsupervised evaluation metrics for dialogue response generation",
      "authors": [
        "C.-W Liu",
        "R Lowe",
        "I Serban",
        "M Noseworthy",
        "L Charlin",
        "J Pineau"
      ],
      "year": "2016",
      "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "81",
      "title": "Bertscore: Evaluating text generation with bert",
      "authors": [
        "T Zhang",
        "V Kishore",
        "F Wu",
        "K Weinberger",
        "Y Artzi"
      ],
      "year": "2020",
      "venue": "International Conference on Learning Representations"
    },
    {
      "citation_id": "82",
      "title": "Measuring nominal scale agreement among many raters",
      "authors": [
        "J Fleiss"
      ],
      "year": "1971",
      "venue": "Psychological Bulletin"
    },
    {
      "citation_id": "83",
      "title": "Television dialogue: The sitcom friends vs. natural conversation",
      "authors": [
        "P Quaglio"
      ],
      "year": "2009",
      "venue": "Television dialogue: The sitcom friends vs. natural conversation",
      "doi": "10.1075/scl.36"
    }
  ]
}