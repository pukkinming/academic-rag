{
  "paper_id": "2407.15458v4",
  "title": "Emo-Codec: An In-Depth Look At Emotion Preservation Capacity Of Legacy And Neural Codec Models With Subjective And Objective Evaluations",
  "published": "2024-07-22T08:14:16Z",
  "authors": [
    "Wenze Ren",
    "Yi-Cheng Lin",
    "Huang-Cheng Chou",
    "Haibin Wu",
    "Yi-Chiao Wu",
    "Chi-Chun Lee",
    "Hung-yi Lee",
    "Yu Tsao"
  ],
  "keywords": [
    "Nerual codec",
    "Legacy codec",
    "speech unit",
    "speech emotion recognition"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "The neural codec model reduces speech data transmission delay and serves as the foundational tokenizer for speech language models (speech LMs). Preserving emotional information in codecs is crucial for effective communication and context understanding. However, there is a lack of studies on emotion loss in existing codecs. This paper evaluates neural and legacy codecs using subjective and objective methods on emotion datasets like IEMOCAP. Our study identifies which codecs best preserve emotional information under various bitrate scenarios. We found that training codec models with both English and Chinese data had limited success in retaining emotional information in Chinese. Additionally, resynthesizing speech through these codecs degrades the performance of speech emotion recognition (SER), particularly for emotions like sadness, depression, fear, and disgust. Human listening tests confirmed these findings. This work guides future speech technology developments to ensure new codecs maintain the integrity of emotional information in speech.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Audio codecs are first introduced to compress speech signals to a limited number of bits for low-latency communication. They typically include both an encoder and a decoder to reconstruct the compressed codes back into speech signals. Given their success, researchers are exploring how the advanced capacity of LMs, which have demonstrated remarkable performance in text modeling and surpassing human abilities across various tasks, can integrate speech perception capacity.  [1] [2] [3] . Recent advancements in speech LMs use codec codes as discrete tokens.  [4] [5] [6] [7] [8] [9] [10] [11] [12]  What distinguishes speech LMs is their capacity to extract richer information, such as emotion, from spoken language  [13, 14] . They capture content information and delve into the nuances of speaker identity and emotion, aspects that text alone cannot fully grasp.\n\nDue to the wide application of codec models for reducing communication latency and serving as tokenizers for speech LMs, the codecs should preserve the signal's integrity and substantial emotional information. For example, when an individual communicates with a virtual assistant through voice commands, the emotional information embedded within the speech signal can provide valuable context for the assistant to deliver more empathetic and tailored responses  [15] . However, the speech codec used in the communication pipeline inadvertently distorts or discards critical emotional cues * Equal first contribution during compression. In that case, the assistant may struggle to perceive the user's emotional state accurately, leading to less effective interactions. Therefore, preserving emotion in speech signals is essential for the effectiveness of speech LMs.\n\nMany advanced neural codec models have been developed using different techniques  [16] . However, existing evaluations on these codecs predominantly focus on signal-level metrics, overlooking crucial paralinguistic elements like emotion  [17] . While Codec-SUPERB  [18]  endeavors to compare emotion preservation in neural codec reconstructed speech, its assessment is confined to a single small dataset, single language, and a specific case-study model. Similarly, Siegert et al.  [19]  investigate the intelligibility of codec-compressed emotional speech but overlook the evaluation of affective content in speech. Previous works compared the speech distorted by legacy codec compression algorithms and evaluated speech emotion recognition (SER) performance by human perception  [20]  and automatic methods such as Gaussian mixture model  [21] [22] [23]  or Support Vector Machine (SVM)  [24] . However, these studies are confined to legacy codecs, which exhibit inferior performance compared to neural codecs. They also utilize evaluation models that are less precise than advanced SER models.\n\nThere is an urgent need for a comprehensive analysis to compare the ability of codec models in different languages to maintain sentiment across different SER systems under staged or real-world, multilingual, and multi-speaker dataset conditions. We consider various factors of codecs, such as bitrates, pretraining dataset languages, and architecture. Different evaluations are conducted to comprehensively assess these factors and their effects on the accuracy of the pretrained SER systems and the emotional perception of humans. Our goal is to provide a path for the community to refer for a new design of codecs. Our study comprehensively evaluates the efficacy of 14 neural network codecs and 3 legacy codecs in retaining emotional information across 15 different SER models on six datasets, revealing their potential to enhance affective computing in real-world applications. The main flow of Emo-Codec is shown in the Fig.  1 .\n\nIn conclusion, our work contributes the following valuable findings:\n\n• Emo-Codec provides comprehensive performance benchmarks for fourteen codec models in six emotion datasets, highlighting their capacity to preserve emotional information. • Descript Audio Codec (DAC) series  [25]  consistently outperforms other codecs in SER at equivalent bit rates. Additionally, AcademiCodec  [26]  and SpeechTokenizer  [27]  show considerable performance under low-bit rate scenarios.  Then, we inference the testset of the original audio, and the audio resynthesized with codecs. We calculate the F1 score difference of emotion recognition to get the objective emotion loss. We used the original audio files and the resynthesized audio files from the codec model to do the human subjective listening test for emotion recognition.\n\npreserving Chinese emotional information compared with codec models trained with only English data.\n\n• Of all the emotions investigated, some negative valence emotions, sadness, depression, fear, and disgust, exhibit a higher performance drop than other emotions.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Method",
      "text": "This section first overviews our rationales for the large-scale evaluation and then details each part.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Rationales Of Our Evaluation",
      "text": "We evaluate both neural and legacy codecs to ensure a comprehensive analysis. The legacy codecs serve as established benchmarks, enabling us to gauge the advancements made by neural codecs. For neural codecs, we select models based on their functionality and design principles. These codecs encompass a range of innovative architectural designs and sophisticated methodologies, ensuring optimal performance and versatility in processing various types of speech data. Specifically, we target models explicitly tailored for speech language model (LM) tokenization. Additionally, we consider models trained on mixed Chinese and English data such as Funcodec, to ensure comprehensive linguistic coverage, accounting for the nuances and emotional variances inherent in both languages. We evaluate all codec models at similar bitrates to ensure a fair comparison.\n\nFor SER models, regarding Table  1 , we use the representation from  self-supervised learning (SSL) models to train SER models because the SSL paradigm has reached state-of-the-art performance on SER tasks  [28, 29] . We adopt various emotion datasets to increase the diversity of language, dataset collection methods (real world, improvised act, scripted act), and speaker.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Codec Models",
      "text": "We carefully select six cutting-edge, high-fidelity neural codec models for our comparative analysis, as shown in Table  2 . Encodec  [30]  serves as our baseline. We use the 2, 4, 8, 16, and 32 layer settings to compare with other codecs under similar bit rates. Build- ing upon Encodec, AudioDec  [31]  introduces a novel approach employing group convolution to accelerate and streamline operations.\n\nAcademiCodec  [26]  uses group-residual vector quantization to decrease the amount of codebook usage while keeping comparable performance. We use the universal version of the model. Fun-Codec  [32]  proposed a frequency domain codec, which achieves a comparable performance with lower computation and parameter complexity. Additionally, SpeechTokenizer  [27]  introduces a unified speech tokenizer tailored for speech LMs, integrating HuBERT units as semantic teachers in the first layer of RVQ. Descript Audio Codec (DAC)  [25]  leverages advanced Snake activation from BigVGAN  [33]  and utilizes a novel complex STFT discriminator at multiple time scales to further enhance audio fidelity. We use the 16k and 24k models. Lastly, SoundStream  [34]  use of RVQ in its encoders allows for a more efficient and compact representation of the audio signal, resulting in higher quality reconstruction at a lower bitrate. * To provide a more comprehensive comparison of the ability to retain emotional traits. we also chose three legacy codecs for comparisons: the MP3  [35] , which is widely used for its efficient compression and good sound quality at different bitrates; the Opus  [36] , known for its adaptability to a wide range of audios and its low latency; and the AAC  [37] , which offers a high level of fidelity and is commonly used for streaming and broadcasting. These legacy codecs can help us benchmark neural codecs against established standards.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Speech Emotion Recognition Dataset",
      "text": "To assess the codecs, we use six public datasets partitioned by EMO-SUPERB  [38] . Those datasets are classified according to their source (acted or real-world) and language (Chinese or English) shown in Table  3 . Datasets have two annotation scenarios: Primary (P) asks each annotator to choose only one emotion during annotation, while Secondary (S) allows annotators to choose multiple emotions for a clip.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Experimental Setup",
      "text": "In contrast to prior SER approaches that relied on single target prediction  [39, 40] , we employ a distribution-like representation to model the multi-dimensional complexity of emotion, as suggested by  [41] . Furthermore, to enhance the performance of our SER model, we incorporate label smoothing  [42]  into the emotion distribution, effectively regularizing the classifier layer with a smoothing parameter set to 0.05.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Speech Emotion Recognition Models",
      "text": "The model architecture is the same as the S3PRL  [43]  toolkit, consisting of a CNN-Self Attention network. It includes three Conv1d layers, followed by a self-attention pooling layer, and two linear layers. We use class-balanced cross-entropy  [44]  to mitigate the effect of imbalance labels in emotion. We use a fixed learning rate of 10 -4 and AdamW optimizer  [45]  to train SER models until the loss of the development set stops decreasing for 5 epochs.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Evaluation Metrics",
      "text": "We use the macro-F1 score  [46]  as the evaluation metric for SER models. Since the output of SER models are probability distributions, an emotion prediction is successful if its probability surpasses a threshold 1  n for n-class SER models, following  [41, 47] .",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Human Subjective Evaluation",
      "text": "We are concerned with how human emotional perception of the synthesized sound differs from the original sound, so we conduct human subjective listening tests. We chose the audio samples from the well-known emotion database, the IEMOCAP. We randomly selected 45 pieces of audio and used various codecs to resynthesize the audio to conduct our subjective evaluation. We selected three codecs-Encodec, DAC, and the legacy codec Opus-using 6k and 24k sample rates. The annotators of the IEMOCAP dataset originally saw both video and audio to label emotions, but in our study, they only listened to the audio. This difference might affect emotion perception. Therefore, we also asked all annotators to label the emotions of the original audio for a fair comparison. We hire annotators from the Prolific platform to annotate the resynthesized audio. Each audio is annotated by 5 male and 5 female annotators. We require annotators from the US and have more than 90% of the acceptance rate. Each annotator answers three questions for each audio: (1) Choose one or more emotion(s) of the speaker from the same pre-defined options as the IEMOCAP. (2) What is the quality of the speech for the purposes of everyday speech communication on a scale of 1 to 5? (3) What is the quality of the speech based on the level of distortion of the speech on a scale of 1 to 5?\n\nAfter collecting annotations, we reported averaged values for speech quality and distortion factors. In terms of emotion, we calculate the distributional label for each sample, and we use the same way to use the threshold to binary the label for measuring the macro-F1 score. We take the label of the original audio as ground truth and calculate the accuracy, which is defined subjective emotion loss.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Result And Discussion",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Effect Of Bitrate On Human/Machine Emotion Recognition",
      "text": "Fig.  3  shows the emotion retention capacity for codecs at different bitrates on the IEMOCAP dataset. In most codecs, both neural and legacy, there is a consistent trend: the emotion retention capacity is significantly enhanced as the bitrate of the codec increases. It highlights the direct impact of bitrate on the amount of voice information that can be transmitted and retained. A higher bitrate facilitates the retention of more detailed emotional information.\n\nWhile lower bitrates usually impair the retention of emotional information, particular codecs have shown a remarkable emotion retention capacity even at low bitrates, such as the SpeechTokenizer and Academicodec series. This indicates that these codecs are particularly effective at maintaining emotional information's integrity despite lower bitrates' limitations. Among all codecs, Soundstream was not good at retaining emotional information, showing the worst emotion recognition performance than all the other codecs. When comparing legacy codecs (represented by MP3, Opus, AAC) with neural codecs, legacy codecs typically show significant improvements in emotional information retention at higher bitrates, e.g. the Opus series codecs perform well at higher bitrates, approaching baseline performance, the MP3 series codecs show varying performance, with some models (e.g. MP3 24k) maintaining good emotional information retention even at higher bitrates. In contrast, others (e.g. MP3 6k) do not perform well at low bitrates, with the AAC series of legacy codecs in a similar situation.\n\nIn contrast, the neural codecs in the DAC family consistently outperform legacy codecs at the same bit rate, highlighting their advanced design and efficiency in retaining emotional information. DAC's superior performance might be attributed to two tricks: snake activation function, and balanced data sampling during training. It also performs well at 6kbps, suggesting that neural codecs are better suited to retaining the integrity of emotional information in bitrateconstrained environments.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Trend Across English Datasets",
      "text": "This section discusses the emotional information retention capacity of codec models in three additional English datasets (CREMA-D, IMPROV, and PODCAST). This analysis extends the performance of codecs in the IEMOCAP dataset from Section 3, highlighting consistent trends and codec efficacy on emotional information retention capacity.\n\nAccording to Fig.  4 , higher bitrates generally improve the retention of emotional information in English datasets. Neural codecs, particularly those in the DAC family, excel at preserving emotions even at lower bitrates (6 kbps), outperforming other codecs. The Funcodec series matches baseline performance and achieves the highest score on the PODCAST dataset. Neural codecs like Speech-Tokenizer and Academicdec also perform well at low bitrates, demonstrating their advanced design and efficiency in retaining emotional nuances. In contrast, legacy codecs such as Opus and MP3 show improvement at higher bitrates but still lag behind neural codecs in preserving emotional information. Overall, codecs with higher bitrates generally enhance emotional capacity, with neural codecs consistently outperforming legacy ones across all bitrates.\n\nLegacy codecs fall short compared to the top-performing neural codecs, highlighting the importance of neural codecs for retaining emotional information in bitrate-constrained environments. Similar to the English Dataset, higher bitrate codecs typically retain emotional information better in the Chinese dataset; the DAC series still delivers excellent performance compared to codecs at the same bitrate; neural codecs like SpeechTokenizer and Academicodec also maintain good performance at lower bitrate; legacy codecs were consistently weaker than neural codecs.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Variability Across Chinese Datasets",
      "text": "While the overall trend is consistent, there are some differences when comparing in detail the performance of some codecs on English and Chinese datasets.\n\nIn the case of the Encodec series, a significant increase in emotional information retention with increasing bitrate can be observed in the English dataset. However, in the Chinese dataset, this trend slows down as the bitrate increases and even decreases in the BIIC-PODCAST dataset. Compared with English, the emotional information in Chinese have different phonetic and tonal characteristics, and the neural codec may not have been specifically optimised for these differences, resulting in a slow increase or even a decrease in the efficiency of emotional information retention with increasing bitrate.\n\nAlso, specific codecs are trained on English-only datasets, such as funcodec en libritts, while on the contrary, funcodec zh en, trained on a mixture of English and Chinese datasets. The insight received from Fig.  6  is that the neural codec model trained on the Chinese dataset BIIC-PODCAST dataset with a mixture of English and Chinese is slightly better at preserving Chinese emotional information than the neural codec model trained on the Englishonly dataset. However, the funcodec zh en series did not beat the funcodec en libritts series on the Chinese dataset NNIME. Such an observation motivates the study of more codec model-trained datasets to accommodate the generic representation of different emotional information across languages.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Specific Emotion Losses",
      "text": "From Table  4 , speech resynthesized through a codec may significantly lose certain emotional information, particularly for emotions challenging for speech emotion recognition (SER) models, such as Table  4 : SER F1 score drop of resynthesized emotional speech on different emotions relative to F1 score of the original speech, in %. Positive values imply an increase in SER performance. Emotions include depression (P), frustration (T), anger (A) sadness (S), disgust (D), excitement (E), fear (F), neutral (N), surprise (U), and happiness (H). The row Ori., represents the performance of original audio in F1 score.   from the codec has a pronounced impact. Despite the low absolute scores, the considerable percentage drop underscores the codec's inability to retain emotional information of fear, which is already a challenge for the model's detection. In addition to fear, other emotions such as depression and sadness also experience degraded SER performance when speech is resynthesized through these codecs. This highlights the shortcomings of codecs in retaining complex and subtle emotional information. While codecs are proficient in compressing and resynthesizing speech, they struggle to preserve the nuanced emotional cues necessary for accurate emotion recognition, particularly for more complex emotions. This demonstrates the necessity for further optimization and enhancement of codec models to maintain the emotional integrity of resynthesized speech better.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Human Subjective Listening Tests",
      "text": "Fig.  7  shows the Mean Opinion Score (MOS) of original audio and audio resynthesized by codecs. The original audio has the highest MOS. Codecs with a 24k bitrate experience a slight drop in MOS but still maintain scores close to the original audio. In contrast, codecs with a 6k bitrate show a significant drop in MOS, except for DAC 16k, which maintains a high MOS. Opus 6k has the lowest MOS among our evaluations. These observations align with our findings on the objective emotion preservation capacity of these codecs. Fig.  8  illustrates macro-F1 scores calculated by the emotional ratings from workers on the original audios and resynthesized audios. We assume the ground truth is the labels of the original audio, and the resynthesized ones are predicted labels. We define the score as subjective emotion retention capacity. Based on the results, the DAC 24K has the highest emotion retention capacity, but the Encodec 24K has the lowest performance. The findings align with the objective evaluation in Fig.  3 .",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Limitation",
      "text": "Existing neural codec models are predominantly trained by English and Chinese datasets. However, there are thousands of spoken languages around the world. Whether existing codec models can be generalized to other languages and how to train codecs that preserve multilingual emotion information are issues that have not been solved.\n\nWhile subjective human evaluations provided valuable insights, the pool of annotators was limited to a specific demographic. Future studies should aim to include a more diverse group of evaluators to account for varying perceptions of emotions across different age groups, cultures, and backgrounds.\n\nDue to limited scope, this work does not consider how the interactivity and context of a conversation might influence the effectiveness of emotion preservation. Future research should explore how codecs perform in interactive dialogue systems where context and conversational history play crucial roles in emotion recognition and response generation.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Conclusion",
      "text": "This work provides extensive insights into the capacity of emotional information preservation of neural codec models. We provide a different perspective to evaluate the performance of codecs. We find that codecs with higher bitrate preserve more emotional information. DAC performs best among all neural codecs, while AcademiCodec and SpeechTokenizer preserve considerable emotional information under a limited bit rate, and our subjective evaluation aligns with the findings. Legacy codecs perform worse than neural codecs at lowbitrate scenarios. Furthermore, training codecs with Chinese and English data might have a limited effect on Chinese Emotion recognition compared with codecs trained with English data. We find that the resynthesis of speech by neural codecs degrades the information in emotions such as sadness, depression, fear, and disgust. Our future work will train codec models to preserve emotional information across diverse linguistic contexts.",
      "page_start": 6,
      "page_end": 6
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: In conclusion, our work contributes the following valuable find-",
      "page": 1
    },
    {
      "caption": "Figure 1: The pipeline of Emo-Codec. The process starts by training the SSL model for emotion recognition using only the original audio.",
      "page": 2
    },
    {
      "caption": "Figure 3: shows the emotion retention capacity for codecs at different",
      "page": 3
    },
    {
      "caption": "Figure 2: Legends for all codecs throughout this paper",
      "page": 4
    },
    {
      "caption": "Figure 3: Emotion recognition performance in macro-F1 score on dif-",
      "page": 4
    },
    {
      "caption": "Figure 4: , higher bitrates generally improve the re-",
      "page": 4
    },
    {
      "caption": "Figure 5: illustrate the emotion preservation capacity of various codecs",
      "page": 4
    },
    {
      "caption": "Figure 6: is that the neural codec model trained on the",
      "page": 4
    },
    {
      "caption": "Figure 4: Emotion recognition performance (macro-F1) on different codec with (a) CREMA-D (b) IMPORV (c) PODCAST dataset. The red",
      "page": 5
    },
    {
      "caption": "Figure 5: Emotion recognition performance (macro-F1) on different codec with (a) NNIME (b) BIIC-PODCAST Chinese datasets. The red",
      "page": 5
    },
    {
      "caption": "Figure 6: SER macro-F1 scores of resynthesized emotional speech",
      "page": 6
    },
    {
      "caption": "Figure 7: Speech subjective quality evaluation (MOS score) based on",
      "page": 6
    },
    {
      "caption": "Figure 8: Subjective emotion retention capacity (macro-F1 score) of",
      "page": 6
    },
    {
      "caption": "Figure 7: shows the Mean Opinion Score (MOS) of original audio and",
      "page": 6
    },
    {
      "caption": "Figure 8: illustrates macro-F1 scores calculated by the emotional",
      "page": 6
    },
    {
      "caption": "Figure 3: 5. LIMITATION",
      "page": 6
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "ID": "U",
          "Codec": "AudioDec",
          "Codec Configuration": "symAD libritts 24000 hop300",
          "kBPS": "6.4",
          "sr": "24"
        },
        {
          "ID": "C",
          "Codec": "AcademiCodec",
          "Codec Configuration": "large universal",
          "kBPS": "2",
          "sr": "16"
        },
        {
          "ID": "S",
          "Codec": "SpeechTokenizer",
          "Codec Configuration": "hubert avg",
          "kBPS": "4",
          "sr": "16"
        },
        {
          "ID": "D1\nD2",
          "Codec": "DAC",
          "Codec Configuration": "DAC 16k\nDAC 24k",
          "kBPS": "6\n24",
          "sr": "16\n24"
        },
        {
          "ID": "E1\nE2\nE3\nE4\nE5",
          "Codec": "Encodec",
          "Codec Configuration": "Encodec 24k",
          "kBPS": "1.5\n3\n6\n12\n24",
          "sr": "24\n24\n24\n24\n24"
        },
        {
          "ID": "F1\nF2\nF3\nF4",
          "Codec": "Funcodec",
          "Codec Configuration": "en libritts 16k nq32ds320\nen libritts 16k nq32ds640\nzh en 16k nq32ds320\nzh en 16k nq32ds640",
          "kBPS": "16\n8\n16\n8",
          "sr": "16\n16\n16\n16"
        },
        {
          "ID": "N",
          "Codec": "Soundstream",
          "Codec Configuration": "Soundstream",
          "kBPS": "6",
          "sr": "16"
        },
        {
          "ID": "M1\nM2\nM3",
          "Codec": "MP3",
          "Codec Configuration": "-\n-\n-",
          "kBPS": "6\n24\n192",
          "sr": "-\n-\n-"
        },
        {
          "ID": "O1\nO2",
          "Codec": "Opus",
          "Codec Configuration": "-\n-",
          "kBPS": "6\n24",
          "sr": "-\n-"
        },
        {
          "ID": "A1\nA2\nA3",
          "Codec": "AAC",
          "Codec Configuration": "-\n-\n-",
          "kBPS": "6\n24\n192",
          "sr": "-\n-\n-"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "0.209\n0.212\n0.208\n0.209\n0.208": "0.196\n0.197\n0.196\n0.194\n0.194"
        },
        {
          "0.209\n0.212\n0.208\n0.209\n0.208": "0.209\n0.206\n0.203\n0.207\n0.204"
        },
        {
          "0.209\n0.212\n0.208\n0.209\n0.208": "0.188\n0.188\n0.187\n0.185\n0.190"
        },
        {
          "0.209\n0.212\n0.208\n0.209\n0.208": "0.197\n0.196\n0.196\n0.198\n0.197"
        },
        {
          "0.209\n0.212\n0.208\n0.209\n0.208": "0.178\n0.180\n0.177\n0.180\n0.178"
        },
        {
          "0.209\n0.212\n0.208\n0.209\n0.208": "0.156\n0.174\n0.175\n0.173\n0.174"
        },
        {
          "0.209\n0.212\n0.208\n0.209\n0.208": "0.188\n0.181\n0.182\n0.180\n0.181"
        },
        {
          "0.209\n0.212\n0.208\n0.209\n0.208": "0.175\n0.176\n0.174\n0.175\n0.176"
        },
        {
          "0.209\n0.212\n0.208\n0.209\n0.208": "0.187\n0.187\n0.184\n0.184\n0.184"
        },
        {
          "0.209\n0.212\n0.208\n0.209\n0.208": "0.193\n0.190\n0.186\n0.188\n0.187"
        },
        {
          "0.209\n0.212\n0.208\n0.209\n0.208": "0.185\n0.182\n0.176\n0.182\n0.176"
        },
        {
          "0.209\n0.212\n0.208\n0.209\n0.208": "0.172\n0.171\n0.168\n0.170\n0.168\n0.181\n0.179\n0.179\n0.178\n0.180\n0.186\n0.186\n0.183\n0.186\n0.185"
        }
      ],
      "page": 6
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models",
      "authors": [
        "Hugo Touvron"
      ],
      "year": "2023",
      "venue": "Llama 2: Open Foundation and Fine-Tuned Chat Models"
    },
    {
      "citation_id": "3",
      "title": "PaLM 2 Technical Report",
      "authors": [
        "Rohan Anil"
      ],
      "year": "2023",
      "venue": "PaLM 2 Technical Report"
    },
    {
      "citation_id": "4",
      "title": "Stanford Alpaca: An Instruction-following LLaMA model",
      "authors": [
        "Rohan Taori"
      ],
      "year": "2023",
      "venue": "Stanford Alpaca: An Instruction-following LLaMA model"
    },
    {
      "citation_id": "5",
      "title": "Anygpt: Unified multimodal llm with discrete sequence modeling",
      "authors": [
        "Jun Zhan"
      ],
      "year": "2024",
      "venue": "Anygpt: Unified multimodal llm with discrete sequence modeling",
      "arxiv": "arXiv:2402.12226"
    },
    {
      "citation_id": "6",
      "title": "Speechx: Neural codec language model as a versatile speech transformer",
      "authors": [
        "Xiaofei Wang"
      ],
      "year": "2023",
      "venue": "Speechx: Neural codec language model as a versatile speech transformer",
      "arxiv": "arXiv:2308.06873"
    },
    {
      "citation_id": "7",
      "title": "Viola: Unified codec language models for speech recognition, synthesis, and translation",
      "authors": [
        "Tianrui Wang"
      ],
      "year": "2023",
      "venue": "Viola: Unified codec language models for speech recognition, synthesis, and translation",
      "arxiv": "arXiv:2305.16107"
    },
    {
      "citation_id": "8",
      "title": "Speak foreign languages with your own voice: Cross-lingual neural codec language modeling",
      "authors": [
        "Ziqiang Zhang"
      ],
      "year": "2023",
      "venue": "Speak foreign languages with your own voice: Cross-lingual neural codec language modeling",
      "arxiv": "arXiv:2303.03926"
    },
    {
      "citation_id": "9",
      "title": "Neural codec language models are zero-shot text to speech synthesizers",
      "authors": [
        "Chengyi Wang"
      ],
      "year": "2023",
      "venue": "Neural codec language models are zero-shot text to speech synthesizers",
      "arxiv": "arXiv:2301.02111"
    },
    {
      "citation_id": "10",
      "title": "Uniaudio: An audio foundation model toward universal audio generation",
      "authors": [
        "Dongchao Yang"
      ],
      "year": "2023",
      "venue": "Uniaudio: An audio foundation model toward universal audio generation",
      "arxiv": "arXiv:2310.00704"
    },
    {
      "citation_id": "11",
      "title": "SoundStorm: Efficient Parallel Audio Generation",
      "authors": [
        "Zalán Borsos"
      ],
      "year": "2023",
      "venue": "SoundStorm: Efficient Parallel Audio Generation",
      "arxiv": "arXiv:2305.09636"
    },
    {
      "citation_id": "12",
      "title": "LAURAGPT: LISTEN, ATTEND, UNDERSTAND, AND REGENERATE AUDIO WITH GPT",
      "authors": [
        "Jiaming Wang"
      ],
      "year": "2024",
      "venue": "LAURAGPT: LISTEN, ATTEND, UNDERSTAND, AND REGENERATE AUDIO WITH GPT"
    },
    {
      "citation_id": "13",
      "title": "Towards General-Purpose Text-Instruction-Guided Voice Conversion",
      "authors": [
        "Chun-Yi Kuan"
      ],
      "venue": "2023 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)"
    },
    {
      "citation_id": "14",
      "title": "Dynamic-SUPERB: Towards A Dynamic, Collaborative, and Comprehensive Instruction-Tuning Benchmark for Speech",
      "authors": [
        "Chien Yu Huang"
      ],
      "year": "2023",
      "venue": "Dynamic-SUPERB: Towards A Dynamic, Collaborative, and Comprehensive Instruction-Tuning Benchmark for Speech"
    },
    {
      "citation_id": "15",
      "title": "Qwen Technical Report",
      "authors": [
        "Jinze Bai"
      ],
      "year": "2023",
      "venue": "Qwen Technical Report"
    },
    {
      "citation_id": "16",
      "title": "DESCo: Detecting Emotions from Smart Commands",
      "authors": [
        "Sunanda Guha",
        "Razib Iqbal"
      ],
      "venue": "2022 IEEE 46th Annual Computers, Software, and Applications Conference (COMPSAC)"
    },
    {
      "citation_id": "17",
      "title": "Towards audio language modeling -an overview",
      "authors": [
        "Haibin Wu"
      ],
      "year": "2024",
      "venue": "Towards audio language modeling -an overview"
    },
    {
      "citation_id": "18",
      "title": "Warp-Q: Quality Prediction for Generative Neural Speech Codecs",
      "authors": [
        "A Wissam",
        "Jan Jassim",
        "Michael Skoglund",
        "Andrew Chinen",
        "Hines"
      ],
      "venue": "ICASSP 2021 -2021 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "19",
      "title": "Codec-SUPERB: An In-Depth Analysis of Sound Codec Models",
      "authors": [
        "Haibin Wu"
      ],
      "year": "2024",
      "venue": "Codec-SUPERB: An In-Depth Analysis of Sound Codec Models"
    },
    {
      "citation_id": "20",
      "title": "Emotion Intelligibility within Codec-Compressed and Reduced Bandwidth Speech",
      "authors": [
        "Ingo Siegert"
      ],
      "year": "2016",
      "venue": "Speech Communication; 12. ITG Symposium"
    },
    {
      "citation_id": "21",
      "title": "HIGH ON EMO-TION \"? HOW AUDIO CODECS INTERFERE WITH THE PERCEIVED CHARISMA AND EMOTIONAL STATES OF MEN AND WOMEN",
      "authors": [
        "Oliver Niebuhr",
        "Ingo Siegert"
      ],
      "year": "2022",
      "venue": "Konferenz Elektronische Sprachsignalverarbeitung"
    },
    {
      "citation_id": "22",
      "title": "Effects of band reduction and coding on speech emotion recognition",
      "authors": [
        "Abas Albahri",
        "Margaret Lech"
      ],
      "year": "2016",
      "venue": "2016 10th International Conference on Signal Processing and Communication Systems (ICSPCS)"
    },
    {
      "citation_id": "23",
      "title": "Automatic emotion recognition in compressed speech using acoustic and non-linear features",
      "authors": [
        "N García"
      ],
      "year": "2015",
      "venue": "2015 20th Symposium on Signal Processing, Images and Computer Vision (STSIVA)"
    },
    {
      "citation_id": "24",
      "title": "Effect of speech compression on the automatic recognition of emotions",
      "authors": [
        "Abas Albahri"
      ],
      "year": "2016",
      "venue": "International Journal of Signal Processing Systems"
    },
    {
      "citation_id": "25",
      "title": "Audio Compression and its Impact on Emotion Recognition in Affective Computing",
      "authors": [
        "Alicia Flores"
      ],
      "year": "2017",
      "venue": "Studientexte zur Sprachkommunikation: Elektronische Sprachsignalverarbeitung"
    },
    {
      "citation_id": "26",
      "title": "High-fidelity audio compression with improved RVQGAN",
      "authors": [
        "Rithesh Kumar"
      ],
      "year": "2024",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "27",
      "title": "Hifi-codec: Group-residual vector quantization for high fidelity audio codec",
      "authors": [
        "Dongchao Yang"
      ],
      "year": "2023",
      "venue": "Hifi-codec: Group-residual vector quantization for high fidelity audio codec",
      "arxiv": "arXiv:2305.02765"
    },
    {
      "citation_id": "28",
      "title": "Speechtokenizer: Unified speech tokenizer for speech large language models",
      "authors": [
        "Xin Zhang"
      ],
      "year": "2023",
      "venue": "Speechtokenizer: Unified speech tokenizer for speech large language models",
      "arxiv": "arXiv:2308.16692"
    },
    {
      "citation_id": "29",
      "title": "Dawn of the Transformer Era in Speech Emotion Recognition: Closing the Valence Gap",
      "authors": [
        "J Wagner"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Pattern Analysis & Machine Intelligence"
    },
    {
      "citation_id": "30",
      "title": "Speech Emotion Recognition Using Self-Supervised Features",
      "authors": [
        "Edmilson Morais"
      ],
      "venue": "ICASSP 2022 -2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "31",
      "title": "High Fidelity Neural Audio Compression",
      "authors": [
        "Alexandre Défossez"
      ],
      "year": "2022",
      "venue": "High Fidelity Neural Audio Compression",
      "arxiv": "arXiv:2210.13438"
    },
    {
      "citation_id": "32",
      "title": "Audiodec: An Open-Source Streaming High-Fidelity Neural Audio Codec",
      "authors": [
        "Yi-Chiao Wu"
      ],
      "venue": "ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "33",
      "title": "FunCodec: A Fundamental, Reproducible and Integrable Open-source Toolkit for Neural Speech Codec",
      "authors": [
        "Zhihao Du"
      ],
      "year": "2023",
      "venue": "FunCodec: A Fundamental, Reproducible and Integrable Open-source Toolkit for Neural Speech Codec",
      "arxiv": "arXiv:2309.07405"
    },
    {
      "citation_id": "34",
      "title": "Bigvgan: A universal neural vocoder with large-scale training",
      "authors": [
        "Sang-Gil Lee",
        "Wei Ping",
        "Boris Ginsburg",
        "Bryan Catanzaro",
        "Sungroh Yoon"
      ],
      "year": "2022",
      "venue": "Bigvgan: A universal neural vocoder with large-scale training",
      "arxiv": "arXiv:2206.04658"
    },
    {
      "citation_id": "35",
      "title": "Soundstream: An endto-end neural audio codec",
      "authors": [
        "Neil Zeghidour",
        "Alejandro Luebs",
        "Ahmed Omran",
        "Jan Skoglund",
        "Marco Tagliasacchi"
      ],
      "year": "2021",
      "venue": "Soundstream: An endto-end neural audio codec"
    },
    {
      "citation_id": "36",
      "title": "Iso-mpeg-1 audio: A generic standard for coding of high-: Quality digital audio",
      "authors": [
        "Karlheinz Brandenburg",
        "Gerhard Stoll"
      ],
      "year": "1994",
      "venue": "Iso-mpeg-1 audio: A generic standard for coding of high-: Quality digital audio"
    },
    {
      "citation_id": "37",
      "title": "High-quality, low-delay music coding in the opus codec",
      "authors": [
        "Jean-Marc Valin",
        "Gregory Maxwell",
        "Timothy Terriberry",
        "Koen Vos"
      ],
      "year": "2016",
      "venue": "High-quality, low-delay music coding in the opus codec"
    },
    {
      "citation_id": "38",
      "title": "Iso/iec mpeg-2 advanced audio coding",
      "authors": [
        "Marina Bosi",
        "Karlheinz Brandenburg",
        "Schuyler Quackenbush",
        "Louis Fielder",
        "Kenzo Akagiri",
        "Hendrik Fuchs",
        "Martin Dietz"
      ],
      "year": "1997",
      "venue": "Journal of The Audio Engineering Society"
    },
    {
      "citation_id": "39",
      "title": "EMO-SUPERB: An In-depth Look at Speech Emotion Recognition",
      "authors": [
        "Haibin Wu"
      ],
      "year": "2024",
      "venue": "EMO-SUPERB: An In-depth Look at Speech Emotion Recognition",
      "arxiv": "arXiv:2402.13018"
    },
    {
      "citation_id": "40",
      "title": "Emotion recognition using a hierarchical binary decision tree approach",
      "authors": [
        "Chi-Chun Lee"
      ],
      "year": "2011",
      "venue": "Sensing Emotion and Affect -Facing Realism in Speech Processing"
    },
    {
      "citation_id": "41",
      "title": "Every Rating Matters: Joint Learning of Subjective Labels and Individual Annotators for Speech Emotion Classification",
      "authors": [
        "H.-C Chou",
        "C.-C Lee"
      ],
      "year": "2019",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "42",
      "title": "Minority views matter: Evaluating speech emotion classifiers with human subjective annotations by an all-inclusive aggregation rule",
      "authors": [
        "H.-C Chou"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "43",
      "title": "Rethinking the Inception Architecture for Computer Vision",
      "authors": [
        "Christian Szegedy"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "44",
      "title": "SUPERB: Speech Processing Universal PERformance Benchmark",
      "authors": [
        "Yang Shu Wen"
      ],
      "year": "2021",
      "venue": "Proc. Interspeech 2021"
    },
    {
      "citation_id": "45",
      "title": "Class-Balanced Loss Based on Effective Number of Samples",
      "authors": [
        "Y Cui"
      ],
      "year": "2019",
      "venue": "2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "46",
      "title": "Decoupled Weight Decay Regularization",
      "authors": [
        "Ilya Loshchilov",
        "Frank Hutter"
      ],
      "year": "2019",
      "venue": "International Conference on Learning Representations"
    },
    {
      "citation_id": "47",
      "title": "Macro f1 and macro f1",
      "authors": [
        "Juri Opitz",
        "Sebastian Burst"
      ],
      "year": "2019",
      "venue": "Macro f1 and macro f1",
      "arxiv": "arXiv:1911.03347"
    },
    {
      "citation_id": "48",
      "title": "No Sample Left Behind: Towards a Comprehensive Evaluation of Speech Emotion Recognition Systems",
      "authors": [
        "Pablo Riera"
      ],
      "year": "2019",
      "venue": "Proc. SMM19, Workshop on Speech, Music and Mind"
    },
    {
      "citation_id": "49",
      "title": "Itu-t p.835, subjective test methodology for evaluating speech communication systems that include noise suppression algorithm",
      "authors": [
        "Itu"
      ],
      "year": "2003",
      "venue": "Itu-t p.835, subjective test methodology for evaluating speech communication systems that include noise suppression algorithm"
    }
  ]
}