{
  "paper_id": "2405.17024v1",
  "title": "Beware Of Overestimated Decoding Performance Arising From Temporal Autocorrelations In Electroencephalogram Signals",
  "published": "2024-05-27T10:25:03Z",
  "authors": [
    "Xiran Xu",
    "Bo Wang",
    "Boda Xiao",
    "Yadong Niu",
    "Yiwen Wang",
    "Xihong Wu",
    "Jing Chen"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Researchers have reported high decoding accuracy (>95%) using non-invasive Electroencephalogram (EEG) signals for brain-computer interface (BCI) decoding tasks like image decoding, emotion recognition, auditory spatial attention detection, etc. Since these EEG data were usually collected with well-designed paradigms in labs, the reliability and robustness of the corresponding decoding methods were doubted by some researchers, and they argued that such decoding accuracy was overestimated due to the inherent temporal autocorrelation of EEG signals. However, the coupling between the stimulus-driven neural responses and the EEG temporal autocorrelations makes it difficult to confirm whether this overestimation exists in truth. Furthermore, the underlying pitfalls behind overestimated decoding accuracy have not been fully explained due to a lack of appropriate formulation. In this work, we formulate the pitfall in various EEG decoding tasks in a unified framework. EEG data were recorded from watermelons to remove stimulus-driven neural responses. Labels were assigned to continuous EEG according to the experimental design for EEG recording of several typical datasets, and then the decoding methods were conducted. The results showed the label can be successfully decoded as long as continuous EEG data with the same label were split into training and test sets. Further analysis indicated that high accuracy of various BCI decoding tasks could be achieved by associating labels with EEG intrinsic temporal autocorrelation features. These results underscore the importance of choosing the right experimental designs and data splits in BCI decoding tasks to prevent inflated accuracies due to EEG temporal autocorrelation.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction And Related Work",
      "text": "A brain-computer interface (BCI) is a type of human-machine interaction that bridges a pathway from the brain to external devices  [1] . Electroencephalogram (EEG) has emerged as a valuable tool for BCI because of its high time resolution, low cost, and good portability  [2] , and algorithms of neural decoding from EEG signals play a role in its practical applications. Recently, deep learning methods have been developed widely for various EEG decoding tasks, and high decoding accuracy was reported. For example, in the task of decoding image classes with EEG recordings, when subjects were required to watch images of different classes, a decoding accuracy of 82.90% was reported for the 40-way classification by Spampinato et al.  [3] . With their EEG dataset, subsequent studies reported a higher decoding accuracy (98.30%,  [4] ), high performance on image retrieval, and even image generation from EEG  [5] ,  [6] ,  [7] .\n\nHowever, it remains unclear what kind of EEG features are learned by the DNN-based models. Some researchers have posited that the high decoding accuracy on the image-evoked EEG dataset was attributed to the block-design paradigm during EEG recording  [8]  [9]  [10] , in which 50 images with the same class label were presented to the subject continuously in one block, and the 40 imageclasses were presented as 40 separate blocks. Due to the existence of temporal autocorrelation of EEG signals, i.e., the temporally nearby data is more similar than the temporally distal  [11] ,  [12] ,  [13] ,  [14] , the models could learn the block-related features rather than the image-related.\n\nTo verify their concerns, Li et al.  [8]  recorded EEG with two experimental designs: block design and rapid-event design. For the rapid-event design, images across the 40 classes were presented alternately and randomly. When the same DNN model was used, it was found that the decoding accuracy was close to Spampinato et al.  [3]  with the block-design EEG data, but it was dramatically decreased to the chance-level (2.50%) with the rapid-event design data. Subsequent work also confirmed the low decoding accuracy for EEG recorded with rapid-event design  [9]    [10] . However, Palazzo et al.  [15]  proposed that temporal autocorrelations only play a marginal role in EEG decoding tasks because they found that EEG data recorded during rest periods (temporal proximity to adjacent blocks) could not be successfully classified as the preceding block label or the succeeding block label.\n\nThey also argued that the rapid-event design seemed to weaken the image-related neural responses due to the possible cognitive load and fatigue effect compared to the block design. Some researchers  [15] ,  [16] ,  [17] ,  [18]  pointed out that block design is essential because humans tend to react more consistently and respond faster when conditions are presented in blocks  [19] ,  [20] . Wilson et al.  [18]  advised that classification work that decodes from block design datasets is the most suitable approach until advances are made to reduce noise.\n\nAlthough the pitfall of overestimated decoding accuracy has been mainly discussed in image decoding tasks, we noticed that similar pitfalls might also exist in various EEG decoding tasks such as in auditory spatial attention detection (ASAD) tasks  [21] ,  [22] ,  [23]    [24] , which involves decoding the subjects auditory attention locus from neural data, and in emotion recognition task  [25]  [26]  [27] , which involves recognizing the subjects emotion type from neural data. Researchers have also found that splitting a continuous EEG from a specific experimental condition into training and test sets would bring higher decoding accuracy in epilepsy detection tasks  [28] , motor imagery decoding tasks  [29] , and so on. All those high decoding accuracy works share the common characteristic: continuously recorded EEG data of a specific class (condition) label are divided into training and test sets (see the top-left of Figure  1 ).\n\nAlthough some studies have mentioned the overestimated decoding accuracy and tried to remind the possible pitfall  [8] ,  [30] , it is difficult to discriminate the influence of the inherent temporal autocorrelation in EEG signals due to the coupling of stimuli-driven neural responses and the temporal autocorrelations. More importantly, due to the lack of an effective formalization, there is not an adequate explanation of how models utilize temporal autocorrelation features for decoding. Furthermore, their concerns only focused on one specific decoding task, and the results and conclusions cannot be generalized to general BCI decoding tasks.\n\nIn this work, the pitfall of various EEG decoding tasks was formulated with a unified framework.\n\nTo completely decouple the temporal autocorrelation features from stimuli-driven neural responses, EEG data were collected from 10 watermelons in this work to construct \"Watermelon EEG\". This method is known as phantom EEG in previous studies  [31] ,  [32] ,  [33] ,  [34] ,  [35] ,  [36] , and the EEG data exclude stimulus-driven neural responses while reserving the temporal autocorrelation features.\n\nFor comparison, a human EEG dataset was also adopted. The watermelon EEG and human EEG were reorganized into three classic neural decoding EEG datasets following their EEG experimental paradigm: image classification (CVPR,  [3] ), emotion recognition (DEAP,  [37] ), and auditory spatial attention decoding (KUL,  [38] ), resulting in six EEG datasets. A sample CNN-based decoding model was used to complete the decoding tasks with the corresponding EEG dataset, and the experimental results revealed that:\n\n1. When the pitfall was formulated with a unique framework, and the temporal autocorrelation was defined as domain features, high decoding accuracy of various BCI decoding tasks could be achieved by associating labels with EEG intrinsic temporal autocorrelation features.",
      "page_start": 2,
      "page_end": 4
    },
    {
      "section_name": "The Pitfall Exists Not Only In Classification But Also Widely In Eeg-Image Joint Training",
      "text": "without explicit labels and even image generation.\n\n3. Splitting a continuous EEG with the same class label into training and test sets should never be used in future BCI decoding works.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Method",
      "text": "The section is organized by: the pitfall is formulated in Subsection 2.1, and the datasets used are introduced in Subsection 2.2. Then, the methods to finish different classification tasks are introduced in Subsection 2.3, and joint training and image generation from EEG are introduced in Subsection 2.4. Some implementation details and statistical analysis method are described in Subsection 2.5.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Problem Formulation",
      "text": "In some BCI works on domain generalization  [39] , all EEG data from a dataset  [40]  or from a subject  [41]  are usually regarded as a domain to emphasize EEG pattern distribution differences between datasets or subjects. Adopted from this concept, we regard a period of continuous EEG data with the same class label as a domain. In some BCI works  [3] ,  [4] ,  [21] ,  [22] ,  [23] [24]  [25]  [26] [27] , researches segment the EEG data from the same domain into samples and further split the samples into training and test data (as shown in Figure  1a ) and complete decoding task, such as classification, retrieval and generation (as shown in Figure  1b ). In these cases, the models used in these works would learn the coupled features containing the class-related feature and domain feature (as shown in the middle of the Figure  1c ). The underlying assumption of these works is that the domain feature plays only a margin role in EEG decoding tasks as shown in the left of the Figure  1c . However, we assumed that the domain feature contributes to the high decoding accuracy as shown in the right of the Figure  1c , which is the pitfall we mentioned in Section 1.\n\nTo validate our assumption, we need to formulate the pitfall. Denote ğ·ğ· as the domain set, and each domain ğ‘‘ğ‘‘ğœ–ğœ–ğ·ğ· contains many samples. We use ğ‘†ğ‘† ğ‘‘ğ‘‘ to denote the sample set of the domain ğ‘‘ğ‘‘.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "The Notation ğ‘¥ğ‘¥ ğ‘–ğ‘–",
      "text": "ğ‘‘ğ‘‘ represents the ğ‘–ğ‘–-th sample (e.g., a 0.5-second EEG data corresponding to watching a specific image) of domain ğ‘‘ğ‘‘, which is associated with class ğ‘¦ğ‘¦ ğ‘–ğ‘– ğ‘‘ğ‘‘ (e.g., the class label panda of the watched image). Considering the temporal autocorrelation of the EEG data, the domain features of data within the same domain are more similar, while the domain features of data in different domains are more distinct.\n\nFor EEG decoding tasks, we assume the data is generated from a two-stage process. First, each domain is modeled as a latent factor ğ‘§ğ‘§ sampled from some meta domain distribution ğ‘ğ‘(â€¢). Second, each data sample x is sampled from a sample distribution conditioned on the domain z and class y:\n\nGiven the sample ğ‘¥ğ‘¥, the aim of a specific EEG decoding task is to uncover its true class label using the posterior ğ‘ğ‘(ğ‘¦ğ‘¦|ğ‘¥ğ‘¥). The quantity can be factorized by the domain factor ğ‘§ğ‘§ as,\n\nWhen we use the Watermelon EEG dataset or use a dataset that is completely unrelated to the current task (e.g., decoding images from an auditory EEG dataset), the class-related feature has none possibility to exist in EEG samples. In this condition, ğ‘ğ‘(ğ‘¦ğ‘¦|ğ‘¥ğ‘¥, ğ‘§ğ‘§) = ğ‘ğ‘(ğ‘¦ğ‘¦|ğ‘§ğ‘§) and the equation (  2 ) can be modified as:\n\nThe assumption of this work is that the model could also deduce ğ‘ğ‘(ğ‘¦ğ‘¦|ğ‘¥ğ‘¥) by learning ğ‘ğ‘(ğ‘¦ğ‘¦|ğ‘§ğ‘§)\n\nand ğ‘ğ‘(ğ‘§ğ‘§|ğ‘¥ğ‘¥) even there is none class-related feature exists. In other words, we assumed that it could also achieve high decoding accuracy on different EEG decoding tasks when using the Watermelons EEG dataset.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Dataset",
      "text": "Watermelon EEG Dataset Ten watermelons were selected as subjects. EEG data were recorded with a NeuroScan SynAmps2 system (Compumedics Limited, Victoria, Australia), using a 64channel Ag/AgCl electrodes cap with a 10/20 layout. An additional electrode was placed on the lower part of the watermelon as the physiological reference, and the forehead served as the ground site (see Appendix A.1 for photography). The inter-electrode impedances were maintained under 20 kOhm.\n\nData were recorded at a sampling rate of 1000 Hz. EEG recordings for each watermelon lasted for more than 1 hour to ensure sufficient data for the decoding task. We refer to the dataset consisting of EEG recordings of 10 watermelons as the Watermelon EEG Dataset.\n\nSparrKULee Dataset SparrKULee dataset  [42]  is a speech-evoked EEG dataset from the KU Leuven University containing 64-channel EEG recordings from 85 participants, each of whom listened to 90-150 minutes of natural speech. We used this dataset because EEG recordings were longer than 1 hour to ensure a sufficient amount of data for each subject. To match the number of subjects in the Watermelon EEG Dataset, EEG data from 10 subjects (ID: Sub7-Sub16) from the SparrKULee Dataset were used.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Dataset Reorganization And Dataset Segmentation",
      "text": "The term \"reorganization\" refers to segmenting continuous EEG into samples and assigning each sample a class label and a domain label according to the referenced experimental design. Here, we follow the experimental designs of three classical published EEG datasets to reorganize the Watermelon EEG Dataset and SparrKULee\n\nDataset. These three datasets were collected respectively for image decoding, emotion recognition, and ASAD tasks.\n\nFor the image decoding task, we referred to the experimental design of the CVPR dataset  [3] .",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Classification Tasks",
      "text": "Model To demonstrate that domain features are strong and easy to be learned by the network, we used a simple CNN (or some parts of this CNN) to complete all classification tasks mentioned in",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Joint Training And Image Generation",
      "text": "To demonstrate that the model can utilize domain features to accomplish retrieval and generation besides classification, EEG-image joint training and image generation on WM-CVPR and SK-CVPR were conducted.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Joint Training",
      "text": "In the EEG-image joint training, a pre-trained image encoder was typically utilized to extract image representation, while an EEG encoder was employed to extract EEG features to align with the image representation. During the decoding process, a retrieval task was applied.\n\nSpecifically, given a test EEG sample and a collection of images containing the target and the nontarget. The image representation was reconstructed from the EEG with the EEG encoder. The similarity between the reconstructed image representation and all candidate image representations in the collection is calculated. The decoded output image is selected based on the ranking of these similarities. Usually, the Top-k accuracy and normalized Rank accuracy are used as evaluation metrics.\n\nIn this work, the simple CNN described in Subsection 2.3 is used as an EEG encoder. The detailed implementation can be found in Appendix A.3.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Image Generation",
      "text": "The image generation aims to generate images seen by the subjects from their EEG data. This task commonly uses a two-stage process: EEG encoding and image generation. In the EEG encoding stage, a model is built to encode EEG data into a latent representation. In the image generation stage, a pre-trained image generator is used. The generator is fine-tuned with EEG representation and corresponding images. In this work, the EEG data are first encoded into image representation with a simple CNN described in Subsection 2.3. Following previous work  [43] , a latent diffusion model conditioned on image representation was used. The metric of n-way top-k accuracy was used for evaluating the semantic correctness of generated images  [44] . The detailed implementation can be found in Appendix A.4.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Implement Details",
      "text": "The neural networks were implemented with the Pytorch and trained on a single highperformance computing node with 8 A800 GPU. For the classification task, the AdamW  [45]  optimizer was employed to minimize the cross-entropy loss function with a learning rate of 10 -3 .\n\nFor the joint training and image generation, the AdamW optimizer was used with a learning rate of 10 -3 and 5 Ã— 10 -4 for each task respectively. More details can be found in our codes. All the experiments mentioned in this work were trained within the subjects (i.e., models were trained for each subject respectively) except special annotation (unseen subject decoding results were only presented in Appendix A.5). For statistical analysis, the one-sample t-test was used to check whether the reported results were significantly higher than the chance level. Bonferroni correction was used to adjust the p-value. A p-value of 0.05 or lower was considered statistically significant.\n\n3 Results",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Classification Tasks",
      "text": "The results shown in Table  1  present that classification accuracy in domain label classification and class label classification are all significantly above the chance level. This shows that the domain feature can be extracted effectively with a simple CNN, and the label class can be decoded from the extracted domain features or from EEG directly. In contrast, the decoding accuracy drops to the chance level when using the splitting strategy \"leave-domains-out\", further supporting domain feature-induced high decoding accuracy. The standard error of the mean calculated over the subjects level is reported for accuracy in this work.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Joint Training And Image Generation",
      "text": "For EEG-image joint training, Table  3  displays the accuracy for the retravel task on the test set.\n\nThe table shows that, for both types of loss functions, decoding accuracy is far above the chance level, demonstrating that the model can utilize domain features to align EEG with image features. Table  3  Result for joint training on WM-CVPR and SK-CVPR with a loss function of cosine similarity (CS)\n\nor InfoNCE.  For image generation, Table  4  displays the n-way top-k accuracy for the generated images on the WM-CVPR and SK-CVPR datasets. The metrics are significantly above the chance level, indicating that the generated images have correct semantics. Figure  3  shows some generated images on the WM-CVPR dataset. As shown in the figure, the model can exactly generate the correct images.\n\nThe results on EEG-image joint training and image generation show that in addition to classification tasks, retrieval, and generation can also achieve high performance by leveraging domain features shared by the test and training sets.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Discussion",
      "text": "",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Relying On The Domain Features For Eeg Decoding",
      "text": "While many works on EEG decoding have reported high-performance results, we proposed that some of these high-performance may rely on temporal autocorrelation of EEG data. The pitfall may involve different EEG decoding tasks. To clarify this pitfall, the concept of domain was adopted to describe the temporal autocorrelation of a continuous EEG with the same label. EEG data were collected from watermelon as the phantom to exclude the contribution of stimuli-driven neural responses to decoding results. The results showed that a simple CNN network could well learn domain features from EEG data and could associate class labels with domain features.\n\nTo avoid the pitfalls, a feasible approach is to adopt a reasonable data-splitting strategy to avoid training and test sets sharing the common domain features, i.e., a leave-domains-out splitting strategy.\n\nFor instance, a leave-subjects-out data-splitting strategy can be adopted, which entails designating the data from certain participants for training and data from others for testing. Alternatively, for datasets that do not follow a block design, a leave-trials-out strategy may be applied. Prior research has consistently demonstrated that employing a leave-subjects-out splitting strategy precipitates a notable decline in decoding performance  [46] . In some cases, it has been reported that decoding accuracy dropped to the chance level  [8] ,  [47] . The prevalent interpretation is that inter-individual variability  [46]  hampers the generalizability across different subjects. However, we posit that the observed decrement in decoding accuracy is attributable to model overfitting to domain features.\n\nAlthough the leave-subjects-out splitting strategy is designed to prevent the leakage of domain features, the presence of these domain features in the training set can still lead the model to inadvertently exploit them to differentiate between categories during the training phase. The methods and results further support the conclusion can be found in Appendix A.5.\n\nPalazzo et al.  [15]  proposed that the EEG temporal correlation related to baseline drift could be alleviated by high-pass filtering. However, our further experiment proved that the domain feature still exists and that high decoding accuracy could be achieved in any frequency band (see Appendix A.6).\n\nWe argue that the focus should not be exclusively on the elimination of EEG autocorrelation through filtering. Instead, greater emphasis should be placed on the experimental paradigms of EEG recording and the strategies employed for dataset splitting. By addressing these aspects, we can proactively prevent the overestimated decoding accuracy arising from EEG temporal autocorrelations.\n\nIt is worth noting that we do not want to create an illusion that all BCI works utilize EEG temporal autocorrelation features for decoding. In fact, there are many works that do not rely on EEG temporal autocorrelation features for decoding in image decoding  [48] ,  [49] ,  [50]  emotion recognition  [51] , sleep detection  [40] ,  [41]  and ASAD  [52] . These works demonstrated the feasibility of various BCI tasks.",
      "page_start": 15,
      "page_end": 16
    },
    {
      "section_name": "Potential Sources Of Domain Features",
      "text": "In this work, we have demonstrated the existence of EEG temporal autocorrelation in the watermelon EEG, which consists of no neural activities, and in the human EEG data. Li et al.  [8]  believed the model decodes by utilizing the baseline drift in the CVPR dataset. They found that when the EEG data is filtered with a bandpass filter, the decoding accuracy dropped greatly. Palazzo et al.\n\n[15] also claimed that temporal autocorrelation was strong only in low frequency. However, we have demonstrated in Appendix A.4 that the domain feature still exists and that high decoding accuracy can be achieved in any frequency band. In addition to baseline drift, some neuroscience researches have shown that temporal autocorrelation existed in neural oscillation, which could be reflected in EEG in various frequency bands. This is referred to as Long-Range Temporal Correlations (LRTC) in neuroscience research  [11] ,  [12] ,  [13] ,  [14] . Linkenkaer-Hansen et al.  [13]  first calculated the LRTC in resting-state EEG data. They found that spontaneous alpha, mu, and beta oscillations result in significant LRTC for at least several hundred seconds during resting conditions. Subsequent neuroscience research further demonstrated that significant LRTC exists in the theta  [11]  and gamma  [12]  bands. While baseline drift can be removed through filtering, the frequency range of the LRTC overlaps with the frequency range of stimuli-driven neural responses, making it impossible to remove this domain feature through filtering. Temporal correlation analysis on human EEG in the SparrKULee Dataset showed the existence of strong LRTC in all frequency bands, and the LRTC in a narrowband is sufficient to complete the corresponding decoding task. The methods and results\n\nfurther support the conclusion can be found in Appendix A.7.",
      "page_start": 16,
      "page_end": 17
    },
    {
      "section_name": "Limitation And Future Work",
      "text": "Although direct evidence of overestimated decoding accuracy attributable to domain feature across various brain-computer interface (BCI) tasks have been provided in the current work, no solution has been proposed to mitigate overfitting to domain features in the training set. Some works have already used domain adaptation  [2] ,  [53] ,  [54]  or domain generalization  [40] ,  [41]  method to improve decoding accuracy under leave-subjects-out data splitting in BCI tasks. This may also help alleviate the adverse effects of domain features on decoding tasks. It is also noteworthy to highlight the remarkable efficacy of large-scale EEG model in various BCI decoding tasks  [55] ,  [56] ,  [57] .\n\nGiven that domain features are pervasive in extensive EEG datasets and do not necessitate manually annotated labels, self-supervised pre-trained large EEG models may be especially adept at discerning and neutralizing domain features, thereby facilitating more robust and generalizable decoding performance.",
      "page_start": 17,
      "page_end": 17
    },
    {
      "section_name": "Conclusion",
      "text": "In this work, the \"overestimated decoding accuracy pitfall\" in various EEG decoding tasks is formulated in a unified framework by adopting the concept of \"domain\". Some typical EEG decoding",
      "page_start": 18,
      "page_end": 18
    },
    {
      "section_name": "A.4 Detailed Implementation Of Image Generation",
      "text": "We take an approach similar to previous works  [44]  (https://github.com/bbaaii/DreamDiffusion).\n\nWe used a CLIP image encoder to extract image representation and trained an EEG encoder with cosine similarity loss to reconstruct image representation from EEG. This process is the same as Consistent with previous work  [44] , we evaluate the semantic correctness of the generated images using N-way Top-1 and Top-5 accuracy classification tasks. Specifically, given a generated image input, a pre-trained ImageNet1K classifier is used to output a classification logit probability among 1000 classes. Among the 1000 classes, N-1 random classes and the correct class are selected, and the Top-1 and Top-5 classification accuracy are calculated. To avoid randomness, this operation is repeated 50 times for each generated image, with the average value taken as the accuracy.",
      "page_start": 19,
      "page_end": 25
    },
    {
      "section_name": "A.5 Leave-Subjects-Out Data Splitting Strategy",
      "text": "In this subsection, we employed the leave-subjects-out data splitting strategy. This refers to using data from a subset of subjects for training, while data from the remaining subjects are used for testing.\n\nWithin the training data, there are two further data splitting strategies: leave-samples-out and leavesubjects-out. The former involves randomly dividing all samples of the training data into training and validation sets, whereas the latter uses data from a subset of subjects for the training set, with the remaining subjects data allocated for the validation set. Table  5  presents the decoding accuracy for six datasets (i.e., WM-CVPR, WM-DEAP, WM-KUL, SK-CVPR, SK-DEAP, and SK-KUL). It can be observed that when the leave-samples-out splitting strategy was used within the training data, both the training and validation sets achieved very high decoding accuracy, but the accuracy only reached the chance level on the test set. Such results are similar to those reported by  [46] ,  [8] ,  [47] , which corroborates the argument that while the leave-subjects-out approach may avert the domain features leakage, it cannot prevent overfitting of the domain features during the training stage, as discussed in Subsection 4.1. Moreover, when the leave-subjects-out data splitting strategy was used within the training dataset, the validation set performance was only at chance level despite high accuracy on the training set. This further demonstrates that decoding that relies on domain features cannot be generalized to practical application scenarios.\n\neach of their ten trials.  As demonstrated in Figure  5 , EEG data from both Watermelon and SparrKULee datasets show significant LRTC across multiple frequency bands. For the EEG data from the Watermelon dataset, significant bands of LRTC are primarily distributed in the low-frequency range (<8 Hz) and around 50 Hz, with these correlations spanning over 500 seconds. This indicates that baseline drifts and line noise contribute to the temporal correlation observed in the Watermelon dataset. For the EEG data from the SparrKULee dataset, LRTCs are significant across the entire frequency range. Similarly, LTRCs are most prominent at low frequencies (<5 Hz) and around 50 Hz, consistent with the findings from the Watermelon dataset. Notably, for SparrKULee dataset, there is also a significant presence of LTRC around 10 Hz, which aligns with previous research findings  [13] , suggesting the temporal correlation of alpha oscillations in human subjects.",
      "page_start": 25,
      "page_end": 31
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Overestimated decoding performance in BCI works. (a) Continuous EEG data in a certain",
      "page": 5
    },
    {
      "caption": "Figure 1: a) and complete decoding task, such as classification,",
      "page": 5
    },
    {
      "caption": "Figure 1: b). In these cases, the models used in these works would",
      "page": 5
    },
    {
      "caption": "Figure 1: c). The underlying assumption of these works is that the domain feature plays",
      "page": 6
    },
    {
      "caption": "Figure 1: c. However, we assumed",
      "page": 6
    },
    {
      "caption": "Figure 1: c, which is the pitfall we mentioned in Section 1.",
      "page": 6
    },
    {
      "caption": "Figure 2: a, 8 distinct clusters exist, each",
      "page": 12
    },
    {
      "caption": "Figure 2: b, 8 distinct clusters also exist, with four corresponding to",
      "page": 13
    },
    {
      "caption": "Figure 2: t-SNE plot for (a) domain label classification on WM-KUL dataset, (b) end-to-end class label",
      "page": 13
    },
    {
      "caption": "Figure 2: c shows the t-",
      "page": 13
    },
    {
      "caption": "Figure 3: EEG-generated image from a typical watermelon subject, where the first column of each panel",
      "page": 14
    },
    {
      "caption": "Figure 3: shows some generated images",
      "page": 14
    },
    {
      "caption": "Figure 4: Photos of watermelons used in the experiment. Each watermelonâ€™s ID is marked on the",
      "page": 23
    },
    {
      "caption": "Figure 5: shows the results of the autocorrelation analysis for the Watermelon",
      "page": 30
    },
    {
      "caption": "Figure 5: Autocorrelation analysis result on (a) Watermelon and (b) SparrKULee datasets.",
      "page": 30
    },
    {
      "caption": "Figure 5: , EEG data from both Watermelon and SparrKULee datasets show",
      "page": 30
    }
  ],
  "tables": [
    {
      "caption": "Table 1: present that classification accuracy in domain label classification",
      "page": 12
    },
    {
      "caption": "Table 1: Classification accuracy (%) on the six datasets. DLC is for domain label classification. TLC-DF is for",
      "page": 12
    },
    {
      "caption": "Table 2: It can be observed",
      "page": 13
    },
    {
      "caption": "Table 2: Zero-shot EEG classification accuracy (%) on WM-CVPR and SK-CVPR datasets.",
      "page": 13
    },
    {
      "caption": "Table 3: displays the accuracy for the retravel task on the test set.",
      "page": 13
    },
    {
      "caption": "Table 3: Result for joint training on WM-CVPR and SK-CVPR with a loss function of cosine similarity (CS)",
      "page": 13
    },
    {
      "caption": "Table 3: Accuracy (%) for joint training on WM-CVPR and SK-CVPR with a loss function of cosine similarity",
      "page": 14
    },
    {
      "caption": "Table 4: Accuracy (%) for semantic correctness. The repeated times N was set to 50.",
      "page": 14
    },
    {
      "caption": "Table 4: displays the n-way top-k accuracy for the generated images on",
      "page": 14
    },
    {
      "caption": "Table 5: presents the decoding accuracy for",
      "page": 26
    },
    {
      "caption": "Table 5: Decoding accuracy for the six datasets on training, validation and test set. Leave-subjects-out data",
      "page": 26
    },
    {
      "caption": "Table 6: Decoding accuracy (%) using different EEG bands for domain label classification (DLC-EEG)",
      "page": 27
    },
    {
      "caption": "Table 7: Decoding accuracy (%) using different EEG bands for class label classification from domain",
      "page": 28
    },
    {
      "caption": "Table 8: Decoding accuracy (%) using different EEG bands for class label classification directly from EEG",
      "page": 28
    },
    {
      "caption": "Table 9: Decoding accuracy (%) using different EEG bands for class label classification directly from EEG",
      "page": 29
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "MAtt: A Manifold Attention Network for EEG Decoding",
      "authors": [
        "Y.-T Pan",
        "J.-L Chou",
        "C.-S Wei"
      ],
      "year": "2022",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "2",
      "title": "SPD domain-specific batch normalization to crack interpretable unsupervised domain adaptation in EEG",
      "authors": [
        "R Kobler",
        "J Hirayama",
        "Q Zhao",
        "M Kawanabe"
      ],
      "year": "2022",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "3",
      "title": "Deep Learning Human Mind for Automated Visual Classification",
      "authors": [
        "C Spampinato",
        "S Palazzo",
        "I Kavasidis",
        "D Giordano",
        "N Souly",
        "M Shah"
      ],
      "year": "2017",
      "venue": "2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
      "doi": "10.1109/CVPR.2017.479"
    },
    {
      "citation_id": "4",
      "title": "Learning Robust Deep Visual Representations from EEG Brain Recordings",
      "authors": [
        "P Singh",
        "D Dalal",
        "G Vashishtha",
        "K Miyapuram",
        "S Raman"
      ],
      "year": "2024",
      "venue": "Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision"
    },
    {
      "citation_id": "5",
      "title": "Brain2Image: Converting Brain Signals into Images",
      "authors": [
        "I Kavasidis",
        "S Palazzo",
        "C Spampinato",
        "D Giordano",
        "M Shah"
      ],
      "year": "2017",
      "venue": "Proceedings of the 25th ACM international conference on Multimedia, in MM '17",
      "doi": "10.1145/3123266.3127907"
    },
    {
      "citation_id": "6",
      "title": "Generative Adversarial Networks Conditioned by Brain Signals",
      "authors": [
        "S Palazzo",
        "C Spampinato",
        "I Kavasidis",
        "D Giordano",
        "M Shah"
      ],
      "year": "2017",
      "venue": "2017 IEEE International Conference on Computer Vision (ICCV)",
      "doi": "10.1109/ICCV.2017.369"
    },
    {
      "citation_id": "7",
      "title": "ThoughtViz: Visualizing Human Thoughts Using Generative Adversarial Network",
      "authors": [
        "P Tirupattur",
        "Y Rawat",
        "C Spampinato",
        "M Shah"
      ],
      "year": "2018",
      "venue": "Proceedings of the 26th ACM international conference on Multimedia, in MM '18",
      "doi": "10.1145/3240508.3240641"
    },
    {
      "citation_id": "8",
      "title": "The Perils and Pitfalls of Block Design for EEG Classification Experiments",
      "authors": [
        "R Li"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
      "doi": "10.1109/TPAMI.2020.2973153"
    },
    {
      "citation_id": "9",
      "title": "Object classification from randomized EEG trials",
      "authors": [
        "H Ahmed",
        "R Wilbur",
        "H Bharadwaj",
        "J Siskind"
      ],
      "year": "2021",
      "venue": "2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)",
      "doi": "10.1109/CVPR46437.2021.00384"
    },
    {
      "citation_id": "10",
      "title": "Still an Ineffective Method With Supertrials/ERPs-Comments on 'Decoding Brain Representations by Multimodal Learning of Neural Activity and Visual Features",
      "authors": [
        "H Bharadwaj",
        "R Wilbur",
        "J Siskind"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
      "doi": "10.1109/TPAMI.2023.3292062"
    },
    {
      "citation_id": "11",
      "title": "Human EEG shows long-range temporal correlations of oscillation amplitude in Theta, Alpha and Beta bands across a wide age range",
      "authors": [
        "L Berthouze",
        "L James",
        "S Farmer"
      ],
      "year": "2010",
      "venue": "Clinical Neurophysiology",
      "doi": "10.1016/j.clinph.2010.02.163"
    },
    {
      "citation_id": "12",
      "title": "Strong long-range temporal correlations of beta/gamma oscillations are associated with poor sustained visual attention performance",
      "authors": [
        "M Irrmischer",
        "S.-S Poil",
        "H Mansvelder",
        "F Intra",
        "K Linkenkaer-Hansen"
      ],
      "year": "2018",
      "venue": "European Journal of Neuroscience",
      "doi": "10.1111/ejn.13672"
    },
    {
      "citation_id": "13",
      "title": "Long-Range Temporal Correlations and Scaling Behavior in Human Brain Oscillations",
      "authors": [
        "K Linkenkaer-Hansen",
        "V Nikouline",
        "J Palva",
        "R Ilmoniemi"
      ],
      "year": "2001",
      "venue": "J. Neurosci",
      "doi": "10.1523/JNEUROSCI.21-04-01370.2001"
    },
    {
      "citation_id": "14",
      "title": "Long-range temporal correlations in alpha and beta oscillations: effect of arousal level and test-retest reliability",
      "authors": [
        "V Nikulin",
        "T Brismar"
      ],
      "year": "2004",
      "venue": "Clinical Neurophysiology",
      "doi": "10.1016/j.clinph.2004.03.019"
    },
    {
      "citation_id": "15",
      "title": "Correct block-design experiments mitigate temporal correlation bias in EEG classification",
      "authors": [
        "S Palazzo",
        "C Spampinato",
        "J Schmidt",
        "I Kavasidis",
        "D Giordano",
        "M Shah"
      ],
      "year": "2020",
      "venue": "arXiv"
    },
    {
      "citation_id": "16",
      "title": "Understanding action concepts from videos and brain activity through subjects' consensus",
      "authors": [
        "J Cavazza"
      ],
      "year": "2022",
      "venue": "Sci Rep",
      "doi": "10.1038/s41598-022-23067-2"
    },
    {
      "citation_id": "17",
      "title": "EEG-based Image Feature Extraction for Visual Classification using Deep Learning",
      "authors": [
        "A Mishra",
        "N Raj",
        "G Bajwa"
      ],
      "year": "2022",
      "venue": "arXiv",
      "doi": "10.48550/arXiv.2209.13090"
    },
    {
      "citation_id": "18",
      "title": "Feasibility of decoding visual information from EEG",
      "authors": [
        "H Wilson",
        "X Chen",
        "M Golbabaee",
        "M Proulx",
        "E O'neill"
      ],
      "year": "2023",
      "venue": "Brain-Computer Interfaces",
      "doi": "10.1080/2326263X.2023.2287719"
    },
    {
      "citation_id": "19",
      "title": "Consider the context: Blocked versus interleaved presentation of antisaccade trials",
      "authors": [
        "L Ethridge",
        "S Brahmbhatt",
        "Y Gao",
        "J Mcdowell",
        "B Clementz"
      ],
      "year": "2009",
      "venue": "Psychophysiology",
      "doi": "10.1111/j.1469-8986.2009.00834.x"
    },
    {
      "citation_id": "20",
      "title": "Do different attention capture paradigms measure different types of capture?",
      "authors": [
        "N Roque",
        "T Wright",
        "W Boot"
      ],
      "year": "2014",
      "venue": "Atten Percept Psychophys",
      "doi": "10.3758/s13414-016-1117-"
    },
    {
      "citation_id": "21",
      "title": "STAnet: A Spatiotemporal Attention Network for Decoding Auditory Spatial Attention From EEG",
      "authors": [
        "E Su",
        "S Cai",
        "L Xie",
        "H Li",
        "T Schultz"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Biomedical Engineering",
      "doi": "10.1109/TBME.2022.3140246"
    },
    {
      "citation_id": "22",
      "title": "XAnet: Cross-Attention Between EEG of Left and Right Brain for Auditory Attention Decoding",
      "authors": [
        "S Pahuja",
        "S Cai",
        "T Schultz",
        "H Li"
      ],
      "year": "2023",
      "venue": "2023 11th International IEEE/EMBS Conference on Neural Engineering (NER)",
      "doi": "10.1109/NER52421.2023.10123792"
    },
    {
      "citation_id": "23",
      "title": "A DenseNet-Based Method for Decoding Auditory Spatial Attention with EEG",
      "authors": [
        "X Xu",
        "B Wang",
        "Y Yan",
        "X Wu",
        "J Chen"
      ],
      "year": "2024",
      "venue": "ICASSP 2024 -2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
      "doi": "10.1109/ICASSP48485.2024.10448013"
    },
    {
      "citation_id": "24",
      "title": "DBPNet: Dual-Branch Parallel Network with Temporal-Frequency Fusion for Auditory Attention Detection",
      "authors": [
        "Q Ni",
        "H Zhang",
        "C Fan",
        "S Pei",
        "C Zhou",
        "Z Lv"
      ],
      "venue": "International Joint Conference on Artificial Intelligence (IJCAI 2024)"
    },
    {
      "citation_id": "25",
      "title": "Decoding emotion with phase-amplitude fusion features of EEG functional connectivity network",
      "authors": [
        "L Hu",
        "C Tan",
        "J Xu",
        "R Qiao",
        "Y Hu",
        "Y Tian"
      ],
      "year": "2024",
      "venue": "Neural Networks",
      "doi": "10.1016/j.neunet.2024.106148"
    },
    {
      "citation_id": "26",
      "title": "EEG decoding for musical emotion with functional connectivity features",
      "authors": [
        "J Xu",
        "W Qian",
        "L Hu",
        "G Liao",
        "Y Tian"
      ],
      "year": "2024",
      "venue": "Biomedical Signal Processing and Control",
      "doi": "10.1016/j.bspc.2023.105744"
    },
    {
      "citation_id": "27",
      "title": "Beyond Mimicking Under-Represented Emotions: Deep Data Augmentation with Emotional Subspace Constraints for EEG-Based Emotion Recognition",
      "authors": [
        "Z Zhang",
        "S Zhong",
        "Y Liu"
      ],
      "year": "2024",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence",
      "doi": "10.1609/aaai.v38i9.28891"
    },
    {
      "citation_id": "28",
      "title": "Data leakage in deep learning studies of translational EEG",
      "authors": [
        "G Brookshire"
      ],
      "year": "2024",
      "venue": "Front. Neurosci",
      "doi": "10.3389/fnins.2024.1373515"
    },
    {
      "citation_id": "29",
      "title": "Deep learning techniques for classification of electroencephalogram (EEG) motor imagery (MI) signals: a review",
      "authors": [
        "H Altaheri"
      ],
      "year": "2023",
      "venue": "Neural Computing and Applications",
      "doi": "10.1007/s00521-021-06352-5"
    },
    {
      "citation_id": "30",
      "title": "What are we really decoding? Unveiling biases in EEG-based decoding of the spatial focus of auditory attention",
      "authors": [
        "I Rotaru",
        "S Geirnaert",
        "N Heintz",
        "I De Ryck",
        "A Bertrand",
        "T Francart"
      ],
      "year": "2024",
      "venue": "J. Neural Eng",
      "doi": "10.1088/1741-2552/ad2214"
    },
    {
      "citation_id": "31",
      "title": "RF Heating of Gold Cup and Conductive Plastic Electrodes during Simultaneous EEG and MRI",
      "authors": [
        "M Balasubramanian",
        "W Wells",
        "J Ives",
        "P Britz",
        "R Mulkern",
        "D Orbach"
      ],
      "year": "2017",
      "venue": "The Neurodiagnostic Journal",
      "doi": "10.1080/21646821.2017.1256722"
    },
    {
      "citation_id": "32",
      "title": "Safety and data quality of EEG recorded simultaneously with multi-band fMRI",
      "authors": [
        "M Egan",
        "R Larsen",
        "J Wirsich",
        "B Sutton",
        "S Sadaghiani"
      ],
      "year": "2021",
      "venue": "PLOS ONE",
      "doi": "10.1371/journal.pone.0238485"
    },
    {
      "citation_id": "33",
      "title": "A quantitative physical model of the TMSinduced discharge artifacts in EEG",
      "authors": [
        "D Freche",
        "J Naim-Feil",
        "A Peled",
        "N Levit-Binnun",
        "E Moses"
      ],
      "year": "2018",
      "venue": "PLOS Computational Biology",
      "doi": "10.1371/journal.pcbi.1006177"
    },
    {
      "citation_id": "34",
      "title": "The Effect of Stimulus Parameters on TMS-EEG Muscle Artifacts",
      "authors": [
        "T Mutanen",
        "H MÃ¤ki",
        "R Ilmoniemi"
      ],
      "year": "2013",
      "venue": "Brain Stimulation",
      "doi": "10.1016/j.brs.2012.07.005"
    },
    {
      "citation_id": "35",
      "title": "Simultaneously recorded EEG-fMRI: Removal of gradient artifacts by subtraction of head movement related average artifact waveforms",
      "authors": [
        "L Sun",
        "H Hinrichs"
      ],
      "year": "2009",
      "venue": "Human Brain Mapping",
      "doi": "10.1002/hbm.20758"
    },
    {
      "citation_id": "36",
      "title": "Effects of mobile phone electromagnetic fields on brain waves in healthy volunteers",
      "authors": [
        "J Van Der Meer",
        "Y Eisma",
        "R Meester",
        "M Jacobs",
        "A Nederveen"
      ],
      "year": "2023",
      "venue": "Sci Rep",
      "doi": "10.1038/s41598-023-48561-z"
    },
    {
      "citation_id": "37",
      "title": "DEAP: A Database for Emotion Analysis ;Using Physiological Signals",
      "authors": [
        "S Koelstra"
      ],
      "year": "2012",
      "venue": "IEEE Transactions on Affective Computing",
      "doi": "10.1109/T-AFFC.2011.15"
    },
    {
      "citation_id": "38",
      "title": "Auditory Attention Detection Dataset KULeuven",
      "authors": [
        "N Das",
        "T Francart",
        "A Bertrand"
      ],
      "year": "2020",
      "venue": "Zenodo",
      "doi": "10.5281/zenodo.3997352"
    },
    {
      "citation_id": "39",
      "title": "Generalizing to Unseen Domains: A Survey on Domain Generalization",
      "authors": [
        "J Wang"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Knowledge and Data Engineering",
      "doi": "10.1109/TKDE.2022.3178128"
    },
    {
      "citation_id": "40",
      "title": "Generalizable Sleep Staging via Multi-Level Domain Alignment",
      "authors": [
        "J Wang",
        "S Zhao",
        "H Jiang",
        "S Li",
        "T Li",
        "G Pan"
      ],
      "year": "2024",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence",
      "doi": "10.1609/aaai.v38i1.27779"
    },
    {
      "citation_id": "41",
      "title": "ManyDG: Many-domain Generalization for Healthcare Applications",
      "authors": [
        "C Yang",
        "M Westover",
        "J Sun"
      ],
      "year": "2023",
      "venue": "The Eleventh International Conference on Learning Representations"
    },
    {
      "citation_id": "42",
      "title": "SparrKULee: A Speech-evoked Auditory Response Repository of the KU Leuven, containing EEG of 85 participants",
      "authors": [
        "B Accou",
        "L Bollens",
        "M Gillis",
        "W Verheijen",
        "H Hamme",
        "T Francart"
      ],
      "year": "2023",
      "venue": "SparrKULee: A Speech-evoked Auditory Response Repository of the KU Leuven, containing EEG of 85 participants",
      "doi": "10.1101/2023.07.24.550310"
    },
    {
      "citation_id": "43",
      "title": "Seeing beyond the brain: Conditional diffusion model with sparse masked modeling for vision decoding",
      "authors": [
        "Z Chen",
        "J Qing",
        "T Xiang",
        "W Yue",
        "J Zhou"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "44",
      "title": "Dreamdiffusion: Generating high-quality images from brain eeg signals",
      "authors": [
        "Y Bai",
        "X Wang",
        "Y Cao",
        "Y Ge",
        "C Yuan",
        "Y Shan"
      ],
      "year": "2023",
      "venue": "Dreamdiffusion: Generating high-quality images from brain eeg signals",
      "arxiv": "arXiv:2306.16934"
    },
    {
      "citation_id": "45",
      "title": "Decoupled weight decay regularization",
      "authors": [
        "I Loshchilov",
        "F Hutter"
      ],
      "year": "2017",
      "venue": "Decoupled weight decay regularization",
      "arxiv": "arXiv:1711.05101"
    },
    {
      "citation_id": "46",
      "title": "Contrastive Learning of Subject-Invariant EEG Representations for Cross-Subject Emotion Recognition",
      "authors": [
        "X Shen",
        "X Liu",
        "X Hu",
        "D Zhang",
        "S Song"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Affective Computing",
      "doi": "10.1109/TAFFC.2022.3164516"
    },
    {
      "citation_id": "47",
      "title": "Extracting the Auditory Attention in a Dual-Speaker Scenario From EEG Using a Joint CNN-LSTM Model",
      "authors": [
        "I Kuruvila",
        "J Muncke",
        "E Fischer",
        "U Hoppe"
      ],
      "year": "2021",
      "venue": "Front. Physiol",
      "doi": "10.3389/fphys.2021.700655"
    },
    {
      "citation_id": "48",
      "title": "Decoding Visual Neural Representations by Multimodal Learning of Brain-Visual-Linguistic Features",
      "authors": [
        "C Du",
        "K Fu",
        "J Li",
        "H He"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
      "doi": "10.1109/TPAMI.2023.3263181"
    },
    {
      "citation_id": "49",
      "title": "Decoding Natural Images from EEG for Object Recognition",
      "authors": [
        "Y Song",
        "B Liu",
        "X Li",
        "N Shi",
        "Y Wang",
        "X Gao"
      ],
      "year": "2024",
      "venue": "The Twelfth International Conference on Learning Representations"
    },
    {
      "citation_id": "50",
      "title": "Self-supervised cross-modal visual retrieval from brain activities",
      "authors": [
        "Z Ye",
        "L Yao",
        "Y Zhang",
        "S Gustin"
      ],
      "year": "2024",
      "venue": "Pattern Recognition",
      "doi": "10.1016/j.patcog.2023.109915"
    },
    {
      "citation_id": "51",
      "title": "DMMR: Cross-Subject Domain Generalization for EEG-Based Emotion Recognition via Denoising Mixed Mutual Reconstruction",
      "authors": [
        "Y Wang",
        "B Zhang",
        "Y Tang"
      ],
      "year": "2024",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence",
      "doi": "10.1609/aaai.v38i1.27819"
    },
    {
      "citation_id": "52",
      "title": "EEG-based detection of the locus of auditory attention with convolutional neural networks",
      "authors": [
        "S Vandecappelle",
        "L Deckers",
        "N Das",
        "A Ansari",
        "A Bertrand",
        "T Francart"
      ],
      "year": "2021",
      "venue": "eLife",
      "doi": "10.7554/eLife.56481"
    },
    {
      "citation_id": "53",
      "title": "Convolution Monge Mapping Normalization for learning on sleep data",
      "authors": [
        "T Gnassounou",
        "R Flamary",
        "A Gramfort"
      ],
      "year": "2023",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "54",
      "title": "Improving EEGbased decoding of the locus of auditory attention through domain adaptation*",
      "authors": [
        "J Wilroth",
        "B Bernhardsson",
        "F Heskebeck",
        "M Skoglund",
        "C Bergeling",
        "E Alickovic"
      ],
      "year": "2023",
      "venue": "J. Neural Eng",
      "doi": "10.1088/1741-2552/ad0e7b"
    },
    {
      "citation_id": "55",
      "title": "Large Brain Model for Learning Generic Representations with Tremendous EEG Data in BCI",
      "authors": [
        "W Jiang",
        "L.-M Zhao",
        "B.-L Lu"
      ],
      "year": "2024",
      "venue": "ICLR 2024 Conference"
    },
    {
      "citation_id": "56",
      "title": "Biot: Biosignal transformer for cross-data learning in the wild",
      "authors": [
        "C Yang",
        "M Westover",
        "J Sun"
      ],
      "year": "2023",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "57",
      "title": "Learning Topology-Agnostic EEG Representations with Geometry-Aware Modeling",
      "authors": [
        "K Yi",
        "Y Wang",
        "K Ren",
        "D Li"
      ],
      "year": "2023",
      "venue": "Advances in Neural Information Processing Systems"
    }
  ]
}