{
  "paper_id": "2403.14048v1",
  "title": "The Neurips 2023 Machine Learning For Audio Workshop: Affective Audio Benchmarks And Novel Data",
  "published": "2024-03-21T00:13:59Z",
  "authors": [
    "Alice Baird",
    "Rachel Manzelli",
    "Panagiotis Tzirakis",
    "Chris Gagne",
    "Haoqi Li",
    "Sadie Allen",
    "Sander Dieleman",
    "Brian Kulis",
    "Shrikanth S. Narayanan",
    "Alan Cowen"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "The NeurIPS 2023 Machine Learning for Audio Workshop brings together machine learning (ML) experts from various audio domains. There are several valuable audio-driven ML tasks, from speech emotion recognition to audio event detection, but the community is sparse compared to other ML areas, e.g., computer vision or natural language processing. A major limitation with audio is the available data; with audio being a time-dependent modality, high-quality data collection is time-consuming and costly, making it challenging for academic groups to apply their often state-of-the-art strategies to a larger, more generalizable dataset. In this short white paper, to encourage researchers with limited access to largedatasets, the organizers first outline several open-source datasets that are available to the community, and for the duration of the workshop are making several propriety datasets available. Namely, three vocal datasets, HUME-PROSODY, HUME-VOCALBURST, an acted emotional speech dataset MODULATE-SONATA, and an in-game streamer dataset MODULATE-STREAM. We outline the current baselines on these datasets but encourage researchers from across audio to utilize them outside of the initial baseline tasks.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Working with audio data in machine learning presents unique challenges compared to fields like computer vision. Despite the importance of various key problems in the audio domain, such as textto-speech, voice recognition, source separation, and synthesis, it has received considerably lower attention. However, there has been a recent renaissance in audio research, particularly in the field of synthesis, with the release of several influential papers in the last year  [1] [2] [3] [4] [5] [6] . The relative scarcity of prior research and this recent boom serves as the primary motivations behind organizing the 2023 NeurIPS Machine Learning for Audio (MLA) Workshop.\n\nThe workshop covers a vast scope of audio-related tasks, including but not limited to speech modeling, speech generation, music generation, denoising of speech and music, data augmentation, acoustic event classification, transcription, source separation, and even multimodal modelling involving audio.\n\nThere are prestigious competitions like the Detection and Classification of Acoustic Scenes and Events (DCASE) focusing on audio-driven machine learning  [7] , and numerous large-scale audio datasets with high-level labeling available in the literature, including Audioset  [8] , Urban80k  [9] , and Librispeech  [10] , along with unique concepts such as bird identification  [11] , and music genre detection  [12] . Despite the availability of such datasets, there still exists a scarcity of openly accessible large-scale datasets particularly tailored for more specialized domains, such as human-computer interaction and human behavior analysis. The lack of specialized datasets presents a considerable hurdle for developing systems that understand more nuanced human characteristics.\n\nTo address this issue and to support authors submitting their work to MLA, the organizers have proactively provided four datasets to researchers participating in the workshop, namely, the HUME-PROSODY, HUME-VOCALBURST, MODULATE-STREAM and MODULATE-SONATA. These datasets offer a broad spectrum of human states, including both labeled and unlabeled data. Researchers are given unrestricted access to use these datasets for their submissions, providing them with the opportunity to tailor their strategies to new data situations and investigate innovative solutions. Moreover, in addition to utilizing the datasets for applied research, the workshop encourages researchers to test their approaches against established benchmarks on baseline tasks outlined in previous machine learning competitions  [13, 14] . This paper aims to provide a comprehensive description of the four large-scale datasets made available in conjunction with this workshop (see Section 2). Additionally, for the datasets where applicable, we present benchmark results achieved thus far (see Section 3).",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Workshop Audio Datasets",
      "text": "Researchers have harnessed an array of audio datasets to train models in various disciplines, including automatic speech recognition and speech diarization, among others. However, there exists a conspicuous scarcity of large-scale datasets specifically designed for speech emotion recognition and the study of people's responses to events. To bridge this gap, our workshop presents substantial datasets derived from individuals interacting with video games, making them ideally suited for investigating human responses to dynamic events. We further provide labeled datasets for emotion recognition, focusing on speech prosody and vocal bursts. These resources offer an invaluable benchmark for the evaluation and enhancement of unsupervised models, marking a significant advancement in the field of speech emotion recognition research. In what follows, we present four datasets: HUME-PROSODY (Sec. 2.1), HUME-VOCALBURST (Sec. 2.2), MODULATE-SONATA (Sec. 2.3), and MODULATE-STREAM (Sec. 2.4)",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "The Hume Expressive Prosodic Speech Dataset (Hume-Prosody)",
      "text": "HUME-PROSODY represents a subset of a comprehensive dataset containing emotionally rated spoken utterances with diverse prosody. This subset comprises 41 hours, 48 minutes, and 55 seconds of audio data collected from 1,004 speakers, aged between 20 to 66 years old. The data was collected across three countries with distinct cultures: the United States, South Africa, and Venezuela. Notably, the data was recorded \"in-the-wild\", meaning it was captured in uncontrolled recording conditions using the speakers' own microphones.\n\nThe foundation of this dataset consists of more than 5,000 \"seed\" samples, which encompass various emotional expressions. These seed samples were gathered from openly available datasets such as MELD  [15]  and VENEC  [16] [17] [18] . The seeds include a mix of 'same' sentences, such as over 500 instances of the phrase \"Let me tell you something\"  [16] , where the prosody plays a significant role in conveying meaning, and 'different' sentences, each with varying words and semantics, where the prosody's functional load is relatively lower.\n\nEach audio sample in HUME-PROSODY is associated with intensity labels for ten different expressed emotions, ranging from 1 to 100. The complete Hume-Prosody dataset comprises 48 emotional expression dimensions, based on the semantic-space model for emotion  [19] . However, for this particular subset, which was introduced in this year's Computational Paralinguistic Challenge (Com-ParE)  [20] , nine emotional classes were selected due to their more balanced distribution across the valence-arousal space. These classes include 'Anger,' 'Boredom,' 'Calmness,' 'Concentration,' 'Determination,' 'Excitement,' 'Interest,' 'Sadness,' and 'Tiredness.'\n\nTo create HUME-PROSODY, participants were recruited through various crowdsourcing platforms like Amazon Mechanical Turk, Clickworker, Prolific, Microworkers, and RapidWorker. They were instructed to mimic a seed vocal burst they heard and use their computer microphone to record themselves imitating the seed sentence with similar prosody to the original recording. Each participant completed 30 trials per survey, and they could complete multiple versions of the survey. The study received informed consent from all participants, and its design and procedure were approved by Heartland IRB.\n\nThe intensity ratings for each emotion were normalized to a scale ranging from 0 to 1. For baseline experiments, the audio files were normalized to -3 decibels and converted to 16 kHz, 16-bit, mono format (the raw unprocessed audio is also provided, captured at 48 kHz). No additional processing was applied to the files, making the data amenable to various tasks. Subsequently, the data was divided into training, validation, and test sets, ensuring speaker independence, see 1 for an overview. The HUME-VOCALBURST dataset is a vast collection of emotional non-linguistic vocalizations, also known as vocal bursts. It includes audio data totaling 36 hours, 47 minutes, and 04 seconds (HH:MM:SS) from 1,702 speakers aged between 20 and 39 years. The dataset was compiled in four countries with diverse cultures: China, South Africa, the U.S., and Venezuela. In these recordings, individuals imitated expressive seed samples, showcasing various emotions. The speakers' vocalizations were recorded in the comfort of their homes using their microphones.\n\nEach vocal burst in the dataset has been rated by an average of 85.2 raters for the intensity of 10 different expressed emotions. These emotions are Amusement, Awe, Awkwardness, Distress, Excitement, Fear, Horror, Sadness, Surprise, and Triumph, each rated on a scale from 1 to 100. The intensity ratings for each emotion were scaled to a range of 0 to 1.\n\nFor the baseline experiments, the audio files were standardized by normalizing them to -3 decibels and then converted to a 16 kHz, 16-bit, mono format. Additionally, the dataset also provides participants with the original, unprocessed audio, which was captured at 48 kHz. To ensure fair evaluation, the data was divided into training, validation, and test sets, taking into account speaker independence and maintaining a balance across different emotion classes. (Refer to Table  2  for the details of the data partitioning.) mp3 file contains the full audio recording of the actor speaking from a script of emotional sentences, in a particular style (role). The audio files contain the full script alongside a text file of the emotional sentence spoken at that specific timestamp.\n\nThere are 25 unique emotion classes in the dataset, including: 'adoration' , 'amusement' , 'anger' , 'awe' , 'confusion' , 'contempt' , 'contentment' , 'desire' , 'disappointment' , 'disgust' , 'distress' , 'elation' , 'embarrassment' , 'fear' , 'hype' , 'interest' , 'pain' , 'realization' , 'relief ' , 'sadness' , 'seductionecstasy' , 'surprisenegative' , 'surprisepositive' , 'sympathy' , 'triumph'.\n\nThe 15 roles performed by the actors include anime voice archetypes (AVA), well-known actors, and fantasy characters. Specifically, these are: 'AVABubblyAndSweet' , 'AVANasallyAndMidpitched' , 'AVAChild' , 'BatmanImpression' , 'Em-maWatsonImpression' , 'JudyDenchImpression' , 'KeanuImpression' , 'KristenBellImpression' , 'MatureAndSmokey' , 'Nobility' , 'Outlander' , 'Pirate' , 'ScarlettJohansonImpression' , 'Small-Companion' , 'Spellcaster' .\n\nThe script of emotional sentences read by the actors is provided in Appendix 5.1. For each emotional class there are between 2 to 6 sentences.\n\nFor the purposes of benchmarking, we segment the long-form audio files following the emotion labels. This results in 756 total audio samples, with an average of 29.52 seconds in length, and between 29-32 segments for each class. Proceeding this we have prepared a speaker-independent partition split (see Table  3  for further details), based on the individual segments of audio.\n\nThe audio was recorded using predominantly Neummann U87, SM7B, and Blue Yeti microphones, with some additional unknown condenser microphones used by actors who recorded their sessions remotely.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "The Modulate Streamer Dataset (Modulate-Stream)",
      "text": "MODULATE-STREAM is an audio-only dataset of over 7 000 hours of publicly available gaming streams. The dataset is in total is 379GB in size, and contains 1 880 260 audio files saved in opus format for space efficiency.\n\nMetadata is contained in a separate TSV file, and contains the following attributes for each audio file:\n\n[ c l i p _ n a m e , p l a y e r _ i d , s e s s i o n _ i d , c l i p _ d u r a t i o n _ m s e c , t r a n s c r i p t , num_words , g a m e _ m e t a d a t a ] Each attribute is described as follows:\n\n• clip_name: the audio filepath.\n\n• player_id: an anonymized ID of the speaker. There are 940 unique players in this dataset.\n\n• session_id: an anonymized ID of the stream. There are 2 839 unique streams in this dataset.\n\n• clip_duration_msec: the clip duration in milliseconds.\n\n• transcript: the approximate transcription of the audio contained within the clip, estimated by a wav2vec-based STT model. • num_words: the number of words in the approximated transcription, with an average value of 20 for this dataset.\n\n• game_metadata: the game the speaker in the clip is playing in their stream (or if not a game, the general topic of the stream), if provided. There are 110 unique topics/games in this dataset.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Current Baselines, And Machine Learning Tasks",
      "text": "For the HUME-PROSODY and HUME-VOCALBURST datasets, various benchmarks are available, encompassing a variety of machine learning tasks. For MODULATE-SONATA we provide a simple baseline for speech emotion recognition to validate the efficacy of the datasets target domain. Due to the size of MODULATE-STREAM we do not provide any baseline results, however we have provided speaker-independent partition splits which anyone using the data are welcome to utilize.\n\nIn this section, we will provide an overview of the original baselines established by the workshop organizers for each dataset. Additionally, whenever applicable, we will showcase the best results achieved through contributions to previous competitions held where this data was provided. As HUME-PROSODY and HUME-VOCALBURST were provided as part of previously held competitions, the test sets for each of these will remain blind, and those wishing to evaluate for the tasks described herein should send predictions to competitions@hume.ai.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Hume-Prosody Description Of Tasks",
      "text": "The HUME-PROSODY have been assigned only one task, which is publicly available with detailed information provided by the ComParE challenge organizers  [20] . You can find the baseline code for this task at https://github.com/EIHW/ComParE2023.\n\nThis specific task for the HUME-PROSODY is known as the Emotion Share Sub-Challenge, which involves a multi-label regression task. It requires participants to predict the proportion or 'share' of nine different emotions based on the ratings given by multiple raters for the 'seed' sample.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Hume-Prosody Initial Baseline Results",
      "text": "For this baseline, embeddings were extracted from Wav2Vec2, fine-tuned to the MSP-Podcast dataset  [21] , to establish an initial baseline for the dataset. Next, a Support Vector Regressor was trained and evaluated, with the cost parameter C of the SVM optimized based on performance on the Dev set. After this optimization process, a final model was trained using the concatenated training and Dev sets for evaluation on the Test partition. The competition is at this time still on-going and so we would advise the authors to check the literature relating to the ComParE 2022 challenge for any updated benchmarks.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Hume-Vocalburst Description Of Tasks",
      "text": "The HUME-VOCALBURST was first introduced during the ICML Expressive Vocal Burst (ExVo) Challenge  [13] , highlighting three key tasks with an emphasis on audio machine learning. Subsequently, it was presented at the Affective Vocal Burst (A-VB) Workshop at the 2022 ACII conference  [14] , where four additional tasks were defined with a focus on affective computing. In this section we will briefly describe each but please see the reference papers for further information.\n\nExVo Multi-Task Learning: Participants in this track will train multi-task models to predict 10 emotions, the speaker's age, and native country using vocal bursts. The baseline performance metric, is a combined metric (see equation 1) based on mean Concordance Correlation Coefficient (CCC) for emotions, Mean Absolute Error (MAE) for age, and Unweighted Average Recall (UAR) for native country, all of which will determine the final standings.\n\nExVo Emotion Generation: This track requires teams to train generative models to produce vocal bursts for 10 distinct emotions. For evaluation in the competition combine metric was presented which included the quantitative methods of Fréchet Inception Distance (FID)  [22] :\n\nAs well as a qualitative approach based on human ratings (HEEP):\n\nwhere T corresponds to the vectorized target matrix, a dummy matrix of size N (number of generated vocal bursts) by 10 (emotions), with ones for targeted emotions and zeros for non-targeted emotions, and H corresponds to the vectorized rating matrix, a matrix of size N × 10 with entries corresponding to the average human intensity ratings of each generated vocal burst.\n\nThe two are then combined as S GEN , and compute the mean between the inverted FID distance, and the HEEP score for each emotion (e); this is defined as\n\nPlease note the human evaluation was provided by the organizer for the ExVo competition only.\n\nExVo Few-Shot Emotion Recognition: In this innovative track, teams will recognize emotional vocalizations using few-shot learning, emphasizing personalization by considering factors like pitch and frequency of the speaker's voice. Two labeled samples per speaker will be used for personalization.\n\nA-VB High-Dimensional Emotion: Participants will predict the intensity of 10 specific emotions through a multi-output regression approach, evaluating their performance using the mean CCC across all emotions.\n\nA-VB Two-Dimensional Emotion: Focused on predicting arousal and valence from the circumplex model of affect  [23] , participants will approach this task as a regression problem, reporting performance using the mean CCC across both dimensions.\n\nA-VB Cross-Cultural Emotion: This unique track introduces a 10-dimensional emotion intensity regression task for four different countries, challenging participants to predict the intensity of 40 emotions. The evaluation will use the mean CCC across all 40 emotions.\n\nA-VB Expressive Burst-Type: For this classification task, participants will aim to classify eight types of expressive vocal bursts (e.g., Laugh, Cry, Scream). Performance will be assessed using the Unweighted Average Recall (UAR), serving as an accuracy measure.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Hume-Vocalburst Initial Baseline Results",
      "text": "For HUME-VOCALBURST there are several baselines set for the various tasks described. For both competitions the baseline code is provided at https://github.com/HumeAI/competitions.\n\nFor each competitions a variety of approaches were presented, including feature-driven  [24] , endto-end  [25]  Long-short-term memory-based architectures, and Generative Adversarial Networks for generation.\n\nTable  6 : Results for the 2022 ICML ExVo Workshop  [13] . Reporting best results for ExVo Multi-Task, ExVo Generation, and ExVo FewShot Tasks.\n\n(a) Development and baseline test scores for ExVo-MultiTask, reporting best for each as given in  [13] .\n\nDevelopment",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Hume-Vocalburst Latest Benchmark Results",
      "text": "As the competitions related to HUME-VOCALBURST we can provide the full list of results here for each, where applicable citing the relevant literature for the approach.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Modulate-Sonata Description Of Tasks",
      "text": "Given the high quality of the audio, and the scenario being acted, we suggest that the MODULATE-SONATA data be used for generation or as a benchmark for evaluation of speech emotion recognition tasks. With this in mind, we provide an initial speech emotion classification task utilizing all classes. --.668 HCAI  [37]  --.685 HCCL  [38]  --.724 Anonymous  [39]  --.730 EIHW  [40]  --.736 Organizers  [14]  Two CCC .508 SclabCNU  [36]  --.620 TeamEP-ITS  [35]  --.629 HCCL  [38]  --.685 EIHW  [40]  --.707 Organizers  [14]  Culture CCC .440 TeamEP-ITS  [35]  --.520 HCAI  [37]  --.526 SclabCNU  [36]  --.550 HCCL  [38]  --.602 EIHW  [40]  --.620 Organizers  [14]  Type UAR .417 TeamEP-ITS  [35]  --.490 SclabCNU  [36]  --.497 EIHW  [40]  --.562 Team-AVB  [41]  --.519 HCAI  [37]  --.586",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Modulate-Sonata Initial Baseline Results",
      "text": "For MODULATE-SONATA we provide an initial baseline for the partitions provided. We extracted HuBERT and Wav2Vec2 embeddings utilizing the default parameters from each of the audio files, and took the mean across the time axis. For a classifier we utilize the sklearn Logistic Regression module, and optimize the value for C={0.001, 0.01, 0.1, 1} on the validation set, and retrain with the model with the validation and training set concatenated using on the best value for C, evaluated on the test set.\n\nFrom the results in Table  10  we can see that Wav2Vec2 and HuBERT embeddings perform similarly for the emotion recognition task, and fusing these embeddings shows further improvement. In all cases, the strong performance across the data indicates the validity of this data for both speech emotion recognition and generation type tasks.",
      "page_start": 9,
      "page_end": 9
    }
  ],
  "figures": [],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Abstract": ""
        },
        {
          "Abstract": "chine learning (ML) experts from various audio domains. There are several valu-"
        },
        {
          "Abstract": "able audio-driven ML tasks, from speech emotion recognition to audio event de-"
        },
        {
          "Abstract": ""
        },
        {
          "Abstract": "vision or natural language processing. A major limitation with audio is the avail-"
        },
        {
          "Abstract": "able data; with audio being a time-dependent modality, high-quality data collec-"
        },
        {
          "Abstract": ""
        },
        {
          "Abstract": "apply their often state-of-the-art strategies to a larger, more generalizable dataset."
        },
        {
          "Abstract": "to encourage researchers with limited access to large-"
        },
        {
          "Abstract": ""
        },
        {
          "Abstract": "to the community, and for the duration of the workshop are making several pro-"
        },
        {
          "Abstract": "priety datasets available. Namely, three vocal datasets, HUME-PROSODY, HUME-"
        },
        {
          "Abstract": ""
        },
        {
          "Abstract": ""
        },
        {
          "Abstract": "on these datasets but encourage researchers from across audio to utilize them out-"
        },
        {
          "Abstract": ""
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "side of the initial baseline tasks.": "1\nIntroduction"
        },
        {
          "side of the initial baseline tasks.": "Working with audio data in machine learning presents unique challenges compared to ﬁelds like"
        },
        {
          "side of the initial baseline tasks.": "computer vision. Despite the importance of various key problems in the audio domain, such as text-"
        },
        {
          "side of the initial baseline tasks.": "to-speech, voice recognition, source separation, and synthesis,\nit has received considerably lower"
        },
        {
          "side of the initial baseline tasks.": "attention. However, there has been a recent renaissance in audio research, particularly in the ﬁeld of"
        },
        {
          "side of the initial baseline tasks.": "synthesis, with the release of several inﬂuential papers in the last year [1–6]. The relative scarcity"
        },
        {
          "side of the initial baseline tasks.": "of prior research and this recent boom serves as the primary motivations behind organizing the 2023"
        },
        {
          "side of the initial baseline tasks.": "NeurIPS Machine Learning for Audio (MLA) Workshop."
        },
        {
          "side of the initial baseline tasks.": "The workshop covers a vast scope of audio-related tasks, including but not limited to speech model-"
        },
        {
          "side of the initial baseline tasks.": "ing, speech generation, music generation, denoising of speech and music, data augmentation, acous-"
        },
        {
          "side of the initial baseline tasks.": "tic event classiﬁcation,\ntranscription, source separation, and even multimodal modelling involving"
        },
        {
          "side of the initial baseline tasks.": "audio."
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "There are prestigious competitions like the Detection and Classiﬁcation of Acoustic Scenes and": "Events (DCASE) focusing on audio-driven machine learning [7], and numerous large-scale audio"
        },
        {
          "There are prestigious competitions like the Detection and Classiﬁcation of Acoustic Scenes and": "datasets with high-level\nlabeling available in the literature,\nincluding Audioset [8], Urban80k [9],"
        },
        {
          "There are prestigious competitions like the Detection and Classiﬁcation of Acoustic Scenes and": "and Librispeech [10], along with unique concepts such as bird identiﬁcation [11], and music genre"
        },
        {
          "There are prestigious competitions like the Detection and Classiﬁcation of Acoustic Scenes and": "detection [12]. Despite the availability of such datasets, there still exists a scarcity of openly accessi-"
        },
        {
          "There are prestigious competitions like the Detection and Classiﬁcation of Acoustic Scenes and": "ble large-scale datasets particularly tailored for more specialized domains, such as human-computer"
        },
        {
          "There are prestigious competitions like the Detection and Classiﬁcation of Acoustic Scenes and": "interaction and human behavior analysis. The lack of specialized datasets presents a considerable"
        },
        {
          "There are prestigious competitions like the Detection and Classiﬁcation of Acoustic Scenes and": "hurdle for developing systems that understand more nuanced human characteristics."
        },
        {
          "There are prestigious competitions like the Detection and Classiﬁcation of Acoustic Scenes and": "To\naddress\nthis\nissue\nand\nto\nsupport\nauthors\nsubmitting\ntheir work\nto MLA,\nthe\norganizers"
        },
        {
          "There are prestigious competitions like the Detection and Classiﬁcation of Acoustic Scenes and": "have proactively provided four datasets\nto researchers participating in the workshop,\nnamely,"
        },
        {
          "There are prestigious competitions like the Detection and Classiﬁcation of Acoustic Scenes and": "the HUME-PROSODY, HUME-VOCALBURST, MODULATE-STREAM and MODULATE-SONATA."
        },
        {
          "There are prestigious competitions like the Detection and Classiﬁcation of Acoustic Scenes and": "These datasets offer a broad spectrum of human states,\nincluding both labeled and unlabeled data."
        },
        {
          "There are prestigious competitions like the Detection and Classiﬁcation of Acoustic Scenes and": "Researchers are given unrestricted access to use these datasets for their submissions, providing them"
        },
        {
          "There are prestigious competitions like the Detection and Classiﬁcation of Acoustic Scenes and": "with the opportunity to tailor their strategies to new data situations and investigate innovative solu-"
        },
        {
          "There are prestigious competitions like the Detection and Classiﬁcation of Acoustic Scenes and": "tions. Moreover, in addition to utilizing the datasets for applied research, the workshop encourages"
        },
        {
          "There are prestigious competitions like the Detection and Classiﬁcation of Acoustic Scenes and": "researchers to test\ntheir approaches against established benchmarks on baseline tasks outlined in"
        },
        {
          "There are prestigious competitions like the Detection and Classiﬁcation of Acoustic Scenes and": "previous machine learning competitions [13, 14]."
        },
        {
          "There are prestigious competitions like the Detection and Classiﬁcation of Acoustic Scenes and": "This paper aims to provide a comprehensive description of the four large-scale datasets made avail-"
        },
        {
          "There are prestigious competitions like the Detection and Classiﬁcation of Acoustic Scenes and": "able in conjunction with this workshop (see Section 2). Additionally, for the datasets where applica-"
        },
        {
          "There are prestigious competitions like the Detection and Classiﬁcation of Acoustic Scenes and": "ble, we present benchmark results achieved thus far (see Section 3)."
        },
        {
          "There are prestigious competitions like the Detection and Classiﬁcation of Acoustic Scenes and": "2 Workshop Audio Datasets"
        },
        {
          "There are prestigious competitions like the Detection and Classiﬁcation of Acoustic Scenes and": "Researchers have harnessed an array of audio datasets to train models in various disciplines,\nin-"
        },
        {
          "There are prestigious competitions like the Detection and Classiﬁcation of Acoustic Scenes and": "cluding automatic speech recognition and speech diarization, among others. However, there exists"
        },
        {
          "There are prestigious competitions like the Detection and Classiﬁcation of Acoustic Scenes and": "a conspicuous scarcity of large-scale datasets speciﬁcally designed for speech emotion recognition"
        },
        {
          "There are prestigious competitions like the Detection and Classiﬁcation of Acoustic Scenes and": "and the study of people’s responses to events. To bridge this gap, our workshop presents substan-"
        },
        {
          "There are prestigious competitions like the Detection and Classiﬁcation of Acoustic Scenes and": "tial datasets derived from individuals interacting with video games, making them ideally suited for"
        },
        {
          "There are prestigious competitions like the Detection and Classiﬁcation of Acoustic Scenes and": "investigating human responses to dynamic events. We further provide labeled datasets for emo-"
        },
        {
          "There are prestigious competitions like the Detection and Classiﬁcation of Acoustic Scenes and": "tion recognition, focusing on speech prosody and vocal bursts. These resources offer an invaluable"
        },
        {
          "There are prestigious competitions like the Detection and Classiﬁcation of Acoustic Scenes and": "benchmark for the evaluation and enhancement of unsupervised models, marking a signiﬁcant ad-"
        },
        {
          "There are prestigious competitions like the Detection and Classiﬁcation of Acoustic Scenes and": "vancement\nin the ﬁeld of speech emotion recognition research.\nIn what follows, we present four"
        },
        {
          "There are prestigious competitions like the Detection and Classiﬁcation of Acoustic Scenes and": "datasets: HUME-PROSODY (Sec.\n2.1), HUME-VOCALBURST (Sec.\n2.2), MODULATE-SONATA"
        },
        {
          "There are prestigious competitions like the Detection and Classiﬁcation of Acoustic Scenes and": "(Sec. 2.3), and MODULATE-STREAM (Sec. 2.4)"
        },
        {
          "There are prestigious competitions like the Detection and Classiﬁcation of Acoustic Scenes and": "2.1\nThe Hume Expressive Prosodic Speech Dataset (HUME-PROSODY)"
        },
        {
          "There are prestigious competitions like the Detection and Classiﬁcation of Acoustic Scenes and": "HUME-PROSODY represents a subset of a comprehensive dataset containing emotionally rated spo-"
        },
        {
          "There are prestigious competitions like the Detection and Classiﬁcation of Acoustic Scenes and": "ken utterances with diverse prosody. This subset comprises 41 hours, 48 minutes, and 55 seconds"
        },
        {
          "There are prestigious competitions like the Detection and Classiﬁcation of Acoustic Scenes and": "of audio data collected from 1,004 speakers, aged between 20 to 66 years old. The data was col-"
        },
        {
          "There are prestigious competitions like the Detection and Classiﬁcation of Acoustic Scenes and": "lected across three countries with distinct cultures:\nthe United States, South Africa, and Venezuela."
        },
        {
          "There are prestigious competitions like the Detection and Classiﬁcation of Acoustic Scenes and": "Notably,\nthe data was recorded \"in-the-wild\", meaning it was captured in uncontrolled recording"
        },
        {
          "There are prestigious competitions like the Detection and Classiﬁcation of Acoustic Scenes and": "conditions using the speakers’ own microphones."
        },
        {
          "There are prestigious competitions like the Detection and Classiﬁcation of Acoustic Scenes and": "The foundation of this dataset consists of more than 5,000 \"seed\" samples, which encompass various"
        },
        {
          "There are prestigious competitions like the Detection and Classiﬁcation of Acoustic Scenes and": "emotional expressions. These seed samples were gathered from openly available datasets such as"
        },
        {
          "There are prestigious competitions like the Detection and Classiﬁcation of Acoustic Scenes and": "MELD [15] and VENEC [16–18]. The seeds include a mix of ’same’ sentences, such as over 500"
        },
        {
          "There are prestigious competitions like the Detection and Classiﬁcation of Acoustic Scenes and": "instances of the phrase \"Let me tell you something\" [16], where the prosody plays a signiﬁcant role"
        },
        {
          "There are prestigious competitions like the Detection and Classiﬁcation of Acoustic Scenes and": "in conveying meaning, and ’different’ sentences, each with varying words and semantics, where the"
        },
        {
          "There are prestigious competitions like the Detection and Classiﬁcation of Acoustic Scenes and": "prosody’s functional load is relatively lower."
        },
        {
          "There are prestigious competitions like the Detection and Classiﬁcation of Acoustic Scenes and": "Each audio sample in HUME-PROSODY is associated with intensity labels for ten different expressed"
        },
        {
          "There are prestigious competitions like the Detection and Classiﬁcation of Acoustic Scenes and": "emotions, ranging from 1 to 100. The complete Hume-Prosody dataset comprises 48 emotional ex-"
        },
        {
          "There are prestigious competitions like the Detection and Classiﬁcation of Acoustic Scenes and": "pression dimensions, based on the semantic-space model for emotion [19]. However, for this par-"
        },
        {
          "There are prestigious competitions like the Detection and Classiﬁcation of Acoustic Scenes and": "ticular subset, which was introduced in this year’s Computational Paralinguistic Challenge (Com-"
        },
        {
          "There are prestigious competitions like the Detection and Classiﬁcation of Acoustic Scenes and": "ParE) [20], nine emotional classes were selected due to their more balanced distribution across the"
        },
        {
          "There are prestigious competitions like the Detection and Classiﬁcation of Acoustic Scenes and": "valence-arousal space. These classes include ’Anger,’ ’Boredom,’ ’Calmness,’ ’Concentration,’ ’De-"
        },
        {
          "There are prestigious competitions like the Detection and Classiﬁcation of Acoustic Scenes and": "termination,’ ’Excitement,’ ’Interest,’ ’Sadness,’ and ’Tiredness.’"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "To create HUME-PROSODY, participants were recruited through various crowdsourcing platforms": "like Amazon Mechanical Turk, Clickworker, Proliﬁc, Microworkers, and RapidWorker. They were"
        },
        {
          "To create HUME-PROSODY, participants were recruited through various crowdsourcing platforms": ""
        },
        {
          "To create HUME-PROSODY, participants were recruited through various crowdsourcing platforms": "selves imitating the seed sentence with similar prosody to the original recording. Each participant"
        },
        {
          "To create HUME-PROSODY, participants were recruited through various crowdsourcing platforms": "completed 30 trials per survey, and they could complete multiple versions of the survey. The study"
        },
        {
          "To create HUME-PROSODY, participants were recruited through various crowdsourcing platforms": "received informed consent"
        },
        {
          "To create HUME-PROSODY, participants were recruited through various crowdsourcing platforms": "Heartland IRB."
        },
        {
          "To create HUME-PROSODY, participants were recruited through various crowdsourcing platforms": "The intensity ratings for each emotion were normalized to a scale ranging from 0 to 1. For baseline"
        },
        {
          "To create HUME-PROSODY, participants were recruited through various crowdsourcing platforms": "experiments, the audio ﬁles were normalized to -3 decibels and converted to 16 kHz, 16-bit, mono"
        },
        {
          "To create HUME-PROSODY, participants were recruited through various crowdsourcing platforms": "format (the raw unprocessed audio is also provided, captured at 48 kHz). No additional processing"
        },
        {
          "To create HUME-PROSODY, participants were recruited through various crowdsourcing platforms": "was applied to the ﬁles, making the data amenable to various tasks."
        },
        {
          "To create HUME-PROSODY, participants were recruited through various crowdsourcing platforms": ""
        },
        {
          "To create HUME-PROSODY, participants were recruited through various crowdsourcing platforms": "Table 1: Summary of the HUME-PROSODY dataset, ﬁrst presented at ComParE 2023 [20]. Including"
        },
        {
          "To create HUME-PROSODY, participants were recruited through various crowdsourcing platforms": "number of samples, speakers, and distribution of identiﬁed gender. Test split remains blind."
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "[ c l i p _ n a m e ,": "num_words ,",
          "p l a y e r _ i d ,\ns e s s i o n _ i d ,": "g a m e _ m e t a d a t a ]",
          "c l i p _ d u r a t i o n _ m s e c ,": "",
          "t r a n s c r i p t": "",
          ",": ""
        },
        {
          "[ c l i p _ n a m e ,": "Each attribute is described as follows:",
          "p l a y e r _ i d ,\ns e s s i o n _ i d ,": "",
          "c l i p _ d u r a t i o n _ m s e c ,": "",
          "t r a n s c r i p t": "",
          ",": ""
        },
        {
          "[ c l i p _ n a m e ,": "",
          "p l a y e r _ i d ,\ns e s s i o n _ i d ,": "• clip_name: the audio ﬁlepath.",
          "c l i p _ d u r a t i o n _ m s e c ,": "",
          "t r a n s c r i p t": "",
          ",": ""
        },
        {
          "[ c l i p _ n a m e ,": "",
          "p l a y e r _ i d ,\ns e s s i o n _ i d ,": "• player_id: an anonymized ID of the speaker. There are 940 unique players in this dataset.",
          "c l i p _ d u r a t i o n _ m s e c ,": "",
          "t r a n s c r i p t": "",
          ",": ""
        },
        {
          "[ c l i p _ n a m e ,": "• session_id:",
          "p l a y e r _ i d ,\ns e s s i o n _ i d ,": "an anonymized ID of",
          "c l i p _ d u r a t i o n _ m s e c ,": "the stream.",
          "t r a n s c r i p t": "There are 2 839 unique streams in this",
          ",": ""
        },
        {
          "[ c l i p _ n a m e ,": "dataset.",
          "p l a y e r _ i d ,\ns e s s i o n _ i d ,": "",
          "c l i p _ d u r a t i o n _ m s e c ,": "",
          "t r a n s c r i p t": "",
          ",": ""
        },
        {
          "[ c l i p _ n a m e ,": "",
          "p l a y e r _ i d ,\ns e s s i o n _ i d ,": "• clip_duration_msec: the clip duration in milliseconds.",
          "c l i p _ d u r a t i o n _ m s e c ,": "",
          "t r a n s c r i p t": "",
          ",": ""
        },
        {
          "[ c l i p _ n a m e ,": "• transcript:",
          "p l a y e r _ i d ,\ns e s s i o n _ i d ,": "the approximate transcription of",
          "c l i p _ d u r a t i o n _ m s e c ,": "",
          "t r a n s c r i p t": "the audio contained within the clip, esti-",
          ",": ""
        },
        {
          "[ c l i p _ n a m e ,": "",
          "p l a y e r _ i d ,\ns e s s i o n _ i d ,": "mated by a wav2vec-based STT model.",
          "c l i p _ d u r a t i o n _ m s e c ,": "",
          "t r a n s c r i p t": "",
          ",": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 4: Summary of the MODULATE-STREAM data. Includingnumberof samples and speakers",
      "data": [
        {
          "3": "For",
          "Current Baselines, and Machine Learning Tasks": "the HUME-PROSODY and HUME-VOCALBURST datasets, various benchmarks are available,"
        },
        {
          "3": "encompassing a variety of machine learning tasks. For MODULATE-SONATA we provide a simple",
          "Current Baselines, and Machine Learning Tasks": ""
        },
        {
          "3": "baseline for speech emotion recognition to validate the efﬁcacy of the datasets target domain. Due to",
          "Current Baselines, and Machine Learning Tasks": ""
        },
        {
          "3": "the size of MODULATE-STREAM we do not provide any baseline results, however we have provided",
          "Current Baselines, and Machine Learning Tasks": ""
        },
        {
          "3": "speaker-independent partition splits which anyone using the data are welcome to utilize.",
          "Current Baselines, and Machine Learning Tasks": ""
        },
        {
          "3": "In this section, we will provide an overview of the original baselines established by the workshop",
          "Current Baselines, and Machine Learning Tasks": ""
        },
        {
          "3": "organizers for each dataset. Additionally, whenever applicable, we will showcase the best results",
          "Current Baselines, and Machine Learning Tasks": ""
        },
        {
          "3": "achieved through contributions to previous competitions held where this data was provided. As",
          "Current Baselines, and Machine Learning Tasks": ""
        },
        {
          "3": "",
          "Current Baselines, and Machine Learning Tasks": "HUME-PROSODY and HUME-VOCALBURST were provided as part of previously held competitions,"
        },
        {
          "3": "the test sets for each of these will remain blind, and those wishing to evaluate for the tasks described",
          "Current Baselines, and Machine Learning Tasks": ""
        },
        {
          "3": "herein should send predictions to competitions@hume.ai.",
          "Current Baselines, and Machine Learning Tasks": ""
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "3.3\nHUME-VOCALBURST Description of Tasks": "The HUME-VOCALBURST was ﬁrst\nintroduced during the ICML Expressive Vocal Burst\n(ExVo)"
        },
        {
          "3.3\nHUME-VOCALBURST Description of Tasks": "Challenge [13], highlighting three key tasks with an emphasis on audio machine learning. Subse-"
        },
        {
          "3.3\nHUME-VOCALBURST Description of Tasks": "quently,\nit was presented at\nthe Affective Vocal Burst (A-VB) Workshop at\nthe 2022 ACII confer-"
        },
        {
          "3.3\nHUME-VOCALBURST Description of Tasks": "ence [14], where four additional\ntasks were deﬁned with a focus on affective computing.\nIn this"
        },
        {
          "3.3\nHUME-VOCALBURST Description of Tasks": "section we will brieﬂy describe each but please see the reference papers for further information."
        },
        {
          "3.3\nHUME-VOCALBURST Description of Tasks": "ExVo Multi-Task Learning: Participants in this track will\ntrain multi-task models to predict 10"
        },
        {
          "3.3\nHUME-VOCALBURST Description of Tasks": "emotions, the speaker’s age, and native country using vocal bursts. The baseline performance metric,"
        },
        {
          "3.3\nHUME-VOCALBURST Description of Tasks": "is a combined metric (see equation\n1) based on mean Concordance Correlation Coefﬁcient (CCC)"
        },
        {
          "3.3\nHUME-VOCALBURST Description of Tasks": "for emotions, Mean Absolute Error\n(MAE)\nfor age, and Unweighted Average Recall\n(UAR)\nfor"
        },
        {
          "3.3\nHUME-VOCALBURST Description of Tasks": "native country, all of which will determine the ﬁnal standings."
        }
      ],
      "page": 6
    },
    {
      "caption": "Table 6: Resultsforthe2022ICML ExVoWorkshop[13]. ReportingbestresultsforExVoMulti-",
      "data": [
        {
          "3.4\nHUME-VOCALBURST Initial Baseline Results": "For HUME-VOCALBURST there are several baselines set for the various tasks described. For both"
        },
        {
          "3.4\nHUME-VOCALBURST Initial Baseline Results": "competitions the baseline code is provided at https://github.com/HumeAI/competitions."
        },
        {
          "3.4\nHUME-VOCALBURST Initial Baseline Results": "For each competitions a variety of approaches were presented,"
        },
        {
          "3.4\nHUME-VOCALBURST Initial Baseline Results": "to-end [25] Long-short-term memory-based architectures, and Generative Adversarial Networks for"
        },
        {
          "3.4\nHUME-VOCALBURST Initial Baseline Results": "generation."
        },
        {
          "3.4\nHUME-VOCALBURST Initial Baseline Results": "Table 6: Results for the 2022 ICML ExVo Workshop [13]. Reporting best results for ExVo Multi-"
        },
        {
          "3.4\nHUME-VOCALBURST Initial Baseline Results": "Task, ExVo Generation, and ExVo FewShot Tasks."
        }
      ],
      "page": 7
    },
    {
      "caption": "Table 8: Latest benchmarks for HUME-VOCALBURST based on the tasks presented in the 2022",
      "data": [
        {
          "Table 8: Latest benchmarks for HUME-VOCALBURST based on the tasks presented in the 2022": ""
        },
        {
          "Table 8: Latest benchmarks for HUME-VOCALBURST based on the tasks presented in the 2022": ""
        },
        {
          "Table 8: Latest benchmarks for HUME-VOCALBURST based on the tasks presented in the 2022": "Task"
        },
        {
          "Table 8: Latest benchmarks for HUME-VOCALBURST based on the tasks presented in the 2022": "MultiTask"
        },
        {
          "Table 8: Latest benchmarks for HUME-VOCALBURST based on the tasks presented in the 2022": "–"
        },
        {
          "Table 8: Latest benchmarks for HUME-VOCALBURST based on the tasks presented in the 2022": "–"
        },
        {
          "Table 8: Latest benchmarks for HUME-VOCALBURST based on the tasks presented in the 2022": "–"
        },
        {
          "Table 8: Latest benchmarks for HUME-VOCALBURST based on the tasks presented in the 2022": "–"
        },
        {
          "Table 8: Latest benchmarks for HUME-VOCALBURST based on the tasks presented in the 2022": "–"
        },
        {
          "Table 8: Latest benchmarks for HUME-VOCALBURST based on the tasks presented in the 2022": "–"
        },
        {
          "Table 8: Latest benchmarks for HUME-VOCALBURST based on the tasks presented in the 2022": "Generate"
        },
        {
          "Table 8: Latest benchmarks for HUME-VOCALBURST based on the tasks presented in the 2022": "–"
        },
        {
          "Table 8: Latest benchmarks for HUME-VOCALBURST based on the tasks presented in the 2022": "–"
        },
        {
          "Table 8: Latest benchmarks for HUME-VOCALBURST based on the tasks presented in the 2022": "FewShot"
        },
        {
          "Table 8: Latest benchmarks for HUME-VOCALBURST based on the tasks presented in the 2022": "–"
        },
        {
          "Table 8: Latest benchmarks for HUME-VOCALBURST based on the tasks presented in the 2022": "–"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table 8: Latest benchmarks for HUME-VOCALBURST based on the tasks presented in the 2022",
      "data": [
        {
          "Table 9: Latest benchmarks for HUME-VOCALBURST based on the tasks presented in the 2022": ""
        },
        {
          "Table 9: Latest benchmarks for HUME-VOCALBURST based on the tasks presented in the 2022": ""
        },
        {
          "Table 9: Latest benchmarks for HUME-VOCALBURST based on the tasks presented in the 2022": "Task"
        },
        {
          "Table 9: Latest benchmarks for HUME-VOCALBURST based on the tasks presented in the 2022": "High"
        },
        {
          "Table 9: Latest benchmarks for HUME-VOCALBURST based on the tasks presented in the 2022": "–"
        },
        {
          "Table 9: Latest benchmarks for HUME-VOCALBURST based on the tasks presented in the 2022": "–"
        },
        {
          "Table 9: Latest benchmarks for HUME-VOCALBURST based on the tasks presented in the 2022": "–"
        },
        {
          "Table 9: Latest benchmarks for HUME-VOCALBURST based on the tasks presented in the 2022": "–"
        },
        {
          "Table 9: Latest benchmarks for HUME-VOCALBURST based on the tasks presented in the 2022": "–"
        },
        {
          "Table 9: Latest benchmarks for HUME-VOCALBURST based on the tasks presented in the 2022": "–"
        },
        {
          "Table 9: Latest benchmarks for HUME-VOCALBURST based on the tasks presented in the 2022": "Two"
        },
        {
          "Table 9: Latest benchmarks for HUME-VOCALBURST based on the tasks presented in the 2022": "–"
        },
        {
          "Table 9: Latest benchmarks for HUME-VOCALBURST based on the tasks presented in the 2022": "–"
        },
        {
          "Table 9: Latest benchmarks for HUME-VOCALBURST based on the tasks presented in the 2022": "–"
        },
        {
          "Table 9: Latest benchmarks for HUME-VOCALBURST based on the tasks presented in the 2022": "–"
        },
        {
          "Table 9: Latest benchmarks for HUME-VOCALBURST based on the tasks presented in the 2022": "Culture"
        },
        {
          "Table 9: Latest benchmarks for HUME-VOCALBURST based on the tasks presented in the 2022": "–"
        },
        {
          "Table 9: Latest benchmarks for HUME-VOCALBURST based on the tasks presented in the 2022": "–"
        },
        {
          "Table 9: Latest benchmarks for HUME-VOCALBURST based on the tasks presented in the 2022": "–"
        },
        {
          "Table 9: Latest benchmarks for HUME-VOCALBURST based on the tasks presented in the 2022": "–"
        },
        {
          "Table 9: Latest benchmarks for HUME-VOCALBURST based on the tasks presented in the 2022": "–"
        },
        {
          "Table 9: Latest benchmarks for HUME-VOCALBURST based on the tasks presented in the 2022": "Type"
        },
        {
          "Table 9: Latest benchmarks for HUME-VOCALBURST based on the tasks presented in the 2022": "–"
        },
        {
          "Table 9: Latest benchmarks for HUME-VOCALBURST based on the tasks presented in the 2022": "–"
        },
        {
          "Table 9: Latest benchmarks for HUME-VOCALBURST based on the tasks presented in the 2022": "–"
        },
        {
          "Table 9: Latest benchmarks for HUME-VOCALBURST based on the tasks presented in the 2022": "–"
        },
        {
          "Table 9: Latest benchmarks for HUME-VOCALBURST based on the tasks presented in the 2022": "–"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "3.7\nMODULATE-SONATA Initial Baseline Results": "For MODULATE-SONATA we provide an initial baseline for the partitions provided. We extracted"
        },
        {
          "3.7\nMODULATE-SONATA Initial Baseline Results": ""
        },
        {
          "3.7\nMODULATE-SONATA Initial Baseline Results": "and took the mean across the time axis. For a classiﬁer we utilize the sklearn Logistic Regression"
        },
        {
          "3.7\nMODULATE-SONATA Initial Baseline Results": "module, and optimize the value for C={0.001, 0.01, 0.1, 1} on the validation set, and retrain with"
        },
        {
          "3.7\nMODULATE-SONATA Initial Baseline Results": "the model with the validation and training set concatenated using on the best value for C, evaluated"
        },
        {
          "3.7\nMODULATE-SONATA Initial Baseline Results": "on the test set."
        },
        {
          "3.7\nMODULATE-SONATA Initial Baseline Results": "From the results in Table 10 we can see that Wav2Vec2 and HuBERT embeddings perform similarly"
        },
        {
          "3.7\nMODULATE-SONATA Initial Baseline Results": "for the emotion recognition task, and fusing these embeddings shows further improvement."
        },
        {
          "3.7\nMODULATE-SONATA Initial Baseline Results": "cases,\nthe strong performance across the data indicates the validity of"
        },
        {
          "3.7\nMODULATE-SONATA Initial Baseline Results": "emotion recognition and generation type tasks."
        },
        {
          "3.7\nMODULATE-SONATA Initial Baseline Results": "Table 10: MODULATE-SONATA validation and test sets performance, for 25 class, speech emotion"
        },
        {
          "3.7\nMODULATE-SONATA Initial Baseline Results": "recognition reporting Unweighted Average Recall (UAR), chance level 0.04."
        },
        {
          "3.7\nMODULATE-SONATA Initial Baseline Results": "Embedding\nValidation\nTest"
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "in audio-driven machine learning,\nleading to the development of more robust models, advancing": "areas including human behavior understanding and human-computer-interaction amongst other."
        },
        {
          "in audio-driven machine learning,\nleading to the development of more robust models, advancing": "References"
        },
        {
          "in audio-driven machine learning,\nleading to the development of more robust models, advancing": "[1] Zalán Borsos, Raphaël Marinier, Damien Vincent, Eugene Kharitonov, Olivier Pietquin, Matt"
        },
        {
          "in audio-driven machine learning,\nleading to the development of more robust models, advancing": "Shariﬁ, Olivier Teboul, David Grangier, Marco Tagliasacchi, and Neil Zeghidour. Audiolm: a"
        },
        {
          "in audio-driven machine learning,\nleading to the development of more robust models, advancing": "language modeling approach to audio generation, 2022."
        },
        {
          "in audio-driven machine learning,\nleading to the development of more robust models, advancing": "[2] Andrea Agostinelli, Timo I. Denk, Zalán Borsos, Jesse Engel, Mauro Verzetti, Antoine Cail-"
        },
        {
          "in audio-driven machine learning,\nleading to the development of more robust models, advancing": "lon, Qingqing Huang, Aren Jansen, Adam Roberts, Marco Tagliasacchi, Matt Shariﬁ, Neil"
        },
        {
          "in audio-driven machine learning,\nleading to the development of more robust models, advancing": "Zeghidour, and Christian Frank. Musiclm: Generating music from text, 2023."
        },
        {
          "in audio-driven machine learning,\nleading to the development of more robust models, advancing": "[3] Flavio Schneider, Zhijing Jin, and Bernhard Schölkopf. Moûsai: Text-to-music generation"
        },
        {
          "in audio-driven machine learning,\nleading to the development of more robust models, advancing": "with long-context latent diffusion, 2023."
        },
        {
          "in audio-driven machine learning,\nleading to the development of more robust models, advancing": "[4] Haohe Liu, Zehua Chen, Yi Yuan, Xinhao Mei, Xubo Liu, Danilo Mandic, Wenwu Wang, and"
        },
        {
          "in audio-driven machine learning,\nleading to the development of more robust models, advancing": "Mark D. Plumbley. Audioldm: Text-to-audio generation with latent diffusion models, 2023."
        },
        {
          "in audio-driven machine learning,\nleading to the development of more robust models, advancing": "[5] Kinyugo Maina. Msanii: High ﬁdelity music synthesis on a shoestring budget, 2023."
        },
        {
          "in audio-driven machine learning,\nleading to the development of more robust models, advancing": "[6] Chengyi Wang, Sanyuan Chen, Yu Wu, Ziqiang Zhang, Long Zhou, Shujie Liu, Zhuo Chen,"
        },
        {
          "in audio-driven machine learning,\nleading to the development of more robust models, advancing": "Yanqing Liu, Huaming Wang, Jinyu Li, et al. Neural codec language models are zero-shot text"
        },
        {
          "in audio-driven machine learning,\nleading to the development of more robust models, advancing": "to speech synthesizers. arXiv preprint arXiv:2301.02111, 2023."
        },
        {
          "in audio-driven machine learning,\nleading to the development of more robust models, advancing": "[7] Annamaria Mesaros, Toni Heittola, Aleksandr Diment, Benjamin Elizalde, Ankit Shah, Em-"
        },
        {
          "in audio-driven machine learning,\nleading to the development of more robust models, advancing": "manuel Vincent, Bhiksha Raj, and Tuomas Virtanen.\nDcase 2017 challenge setup: Tasks,"
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "[8]\nJort F Gemmeke, Daniel PW Ellis, Dylan Freedman, Aren Jansen, Wade Lawrence, R Chan-": "ning Moore, Manoj Plakal, and Marvin Ritter. Audio set: An ontology and human-labeled"
        },
        {
          "[8]\nJort F Gemmeke, Daniel PW Ellis, Dylan Freedman, Aren Jansen, Wade Lawrence, R Chan-": "dataset\nfor audio events.\nIn 2017 IEEE International Conference on Acoustics, Speech and"
        },
        {
          "[8]\nJort F Gemmeke, Daniel PW Ellis, Dylan Freedman, Aren Jansen, Wade Lawrence, R Chan-": "Signal Processing (ICASSP), pages 776–780. IEEE, 2017."
        },
        {
          "[8]\nJort F Gemmeke, Daniel PW Ellis, Dylan Freedman, Aren Jansen, Wade Lawrence, R Chan-": "[9]\nJustin Salamon, Christopher Jacoby, and Juan Pablo Bello. A dataset and taxonomy for urban"
        },
        {
          "[8]\nJort F Gemmeke, Daniel PW Ellis, Dylan Freedman, Aren Jansen, Wade Lawrence, R Chan-": "sound research.\nIn Proceedings of\nthe 22nd ACM international conference on Multimedia,"
        },
        {
          "[8]\nJort F Gemmeke, Daniel PW Ellis, Dylan Freedman, Aren Jansen, Wade Lawrence, R Chan-": "pages 1041–1044, 2014."
        },
        {
          "[8]\nJort F Gemmeke, Daniel PW Ellis, Dylan Freedman, Aren Jansen, Wade Lawrence, R Chan-": "[10] Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur.\nLibrispeech:\nan"
        },
        {
          "[8]\nJort F Gemmeke, Daniel PW Ellis, Dylan Freedman, Aren Jansen, Wade Lawrence, R Chan-": "asr corpus based on public domain audio books.\nIn 2015 IEEE international conference on"
        },
        {
          "[8]\nJort F Gemmeke, Daniel PW Ellis, Dylan Freedman, Aren Jansen, Wade Lawrence, R Chan-": "acoustics, speech and signal processing (ICASSP), pages 5206–5210. IEEE, 2015."
        },
        {
          "[8]\nJort F Gemmeke, Daniel PW Ellis, Dylan Freedman, Aren Jansen, Wade Lawrence, R Chan-": "[11] Stefan Kahl, Amanda Navine, Tom Denton, Holger Klinck, Patrick Hart, Hervé Glotin, Hervé"
        },
        {
          "[8]\nJort F Gemmeke, Daniel PW Ellis, Dylan Freedman, Aren Jansen, Wade Lawrence, R Chan-": "Goëau, Willem-Pier Vellinga, Robert Planqué, and Alexis Joly. Overview of birdclef 2022:"
        },
        {
          "[8]\nJort F Gemmeke, Daniel PW Ellis, Dylan Freedman, Aren Jansen, Wade Lawrence, R Chan-": "Endangered bird species recognition in soundscape recordings. Working Notes of CLEF, 2022."
        },
        {
          "[8]\nJort F Gemmeke, Daniel PW Ellis, Dylan Freedman, Aren Jansen, Wade Lawrence, R Chan-": "[12] Bob L Sturm. An analysis of the gtzan music genre dataset.\nIn Proceedings of the second in-"
        },
        {
          "[8]\nJort F Gemmeke, Daniel PW Ellis, Dylan Freedman, Aren Jansen, Wade Lawrence, R Chan-": "ternational ACM workshop on Music information retrieval with user-centered and multimodal"
        },
        {
          "[8]\nJort F Gemmeke, Daniel PW Ellis, Dylan Freedman, Aren Jansen, Wade Lawrence, R Chan-": "strategies, pages 7–12, 2012."
        },
        {
          "[8]\nJort F Gemmeke, Daniel PW Ellis, Dylan Freedman, Aren Jansen, Wade Lawrence, R Chan-": "[13] Alice Baird, Panagiotis Tzirakis, Gauthier Gidel, Marco Jiralerspong, Eilif B Muller, Kory"
        },
        {
          "[8]\nJort F Gemmeke, Daniel PW Ellis, Dylan Freedman, Aren Jansen, Wade Lawrence, R Chan-": "Mathewson, Björn Schuller, Erik Cambria, Dacher Keltner, and Alan Cowen. The icml 2022"
        },
        {
          "[8]\nJort F Gemmeke, Daniel PW Ellis, Dylan Freedman, Aren Jansen, Wade Lawrence, R Chan-": "expressive vocalizations workshop and competition: Recognizing, generating, and personaliz-"
        },
        {
          "[8]\nJort F Gemmeke, Daniel PW Ellis, Dylan Freedman, Aren Jansen, Wade Lawrence, R Chan-": "ing vocal bursts. arXiv preprint arXiv:2205.01780, 2022."
        },
        {
          "[8]\nJort F Gemmeke, Daniel PW Ellis, Dylan Freedman, Aren Jansen, Wade Lawrence, R Chan-": "[14] Alice Baird, Panagiotis Tzirakis, Jeffrey A Brooks, Chris B Gregory, Björn Schuller, Anton"
        },
        {
          "[8]\nJort F Gemmeke, Daniel PW Ellis, Dylan Freedman, Aren Jansen, Wade Lawrence, R Chan-": "Batliner, Dacher Keltner, and Alan Cowen. The acii 2022 affective vocal bursts workshop &"
        },
        {
          "[8]\nJort F Gemmeke, Daniel PW Ellis, Dylan Freedman, Aren Jansen, Wade Lawrence, R Chan-": "competition.\nIn 2022 10th International Conference on Affective Computing and Intelligent"
        },
        {
          "[8]\nJort F Gemmeke, Daniel PW Ellis, Dylan Freedman, Aren Jansen, Wade Lawrence, R Chan-": "Interaction Workshops and Demos (ACIIW), pages 1–5. IEEE, 2022."
        },
        {
          "[8]\nJort F Gemmeke, Daniel PW Ellis, Dylan Freedman, Aren Jansen, Wade Lawrence, R Chan-": "[15] Soujanya Poria, Devamanyu Hazarika, Navonil Majumder, Gautam Naik, Erik Cambria, and"
        },
        {
          "[8]\nJort F Gemmeke, Daniel PW Ellis, Dylan Freedman, Aren Jansen, Wade Lawrence, R Chan-": "Rada Mihalcea. Meld: A multimodal multi-party dataset for emotion recognition in conversa-"
        },
        {
          "[8]\nJort F Gemmeke, Daniel PW Ellis, Dylan Freedman, Aren Jansen, Wade Lawrence, R Chan-": "tions. preprint arXiv:1810.02508, 2018."
        },
        {
          "[8]\nJort F Gemmeke, Daniel PW Ellis, Dylan Freedman, Aren Jansen, Wade Lawrence, R Chan-": "[16] Petri Laukka, Hillary Anger Elfenbein, Wanda Chui, Nutankumar S. Thingujam, Frederick K."
        },
        {
          "[8]\nJort F Gemmeke, Daniel PW Ellis, Dylan Freedman, Aren Jansen, Wade Lawrence, R Chan-": "Iraki, Thomas Rockstuhl, and Jean Althoff. Presenting the VENEC corpus: Development of a"
        },
        {
          "[8]\nJort F Gemmeke, Daniel PW Ellis, Dylan Freedman, Aren Jansen, Wade Lawrence, R Chan-": "cross-cultural corpus of vocal emotion expressions and a novel method of annotating emotion"
        },
        {
          "[8]\nJort F Gemmeke, Daniel PW Ellis, Dylan Freedman, Aren Jansen, Wade Lawrence, R Chan-": "appraisals.\nIn Proc. LREC 2010 Workshop on Corpora for Research on Emotion and Affect,"
        },
        {
          "[8]\nJort F Gemmeke, Daniel PW Ellis, Dylan Freedman, Aren Jansen, Wade Lawrence, R Chan-": "pages 53–57. LREC, Marrakesh, Morocco, 2010."
        },
        {
          "[8]\nJort F Gemmeke, Daniel PW Ellis, Dylan Freedman, Aren Jansen, Wade Lawrence, R Chan-": "[17] Hillary Anger Elfenbein, Petri Laukka, Jean Althoff, Wanda Chui, Frederick K Iraki, Thomas"
        },
        {
          "[8]\nJort F Gemmeke, Daniel PW Ellis, Dylan Freedman, Aren Jansen, Wade Lawrence, R Chan-": "Rockstuhl, and Nutankumar S. Thingujam. What do we hear\nin the voice?\nan open-ended"
        },
        {
          "[8]\nJort F Gemmeke, Daniel PW Ellis, Dylan Freedman, Aren Jansen, Wade Lawrence, R Chan-": "judgment study of emotional speech prosody.\nPersonality and Social Psychology Bulletin,"
        },
        {
          "[8]\nJort F Gemmeke, Daniel PW Ellis, Dylan Freedman, Aren Jansen, Wade Lawrence, R Chan-": "48(7):1087–1104, 2022."
        },
        {
          "[8]\nJort F Gemmeke, Daniel PW Ellis, Dylan Freedman, Aren Jansen, Wade Lawrence, R Chan-": "[18] Petri Laukka, Hillary Anger Elfenbein, Nutankumar S Thingujam, Thomas Rockstuhl, Fred-"
        },
        {
          "[8]\nJort F Gemmeke, Daniel PW Ellis, Dylan Freedman, Aren Jansen, Wade Lawrence, R Chan-": "erick K Iraki, Wanda Chui, and Jean Althoff. The expression and recognition of emotions in"
        },
        {
          "[8]\nJort F Gemmeke, Daniel PW Ellis, Dylan Freedman, Aren Jansen, Wade Lawrence, R Chan-": "Journal of\nthe voice across ﬁve nations: A lens model analysis based on acoustic features."
        },
        {
          "[8]\nJort F Gemmeke, Daniel PW Ellis, Dylan Freedman, Aren Jansen, Wade Lawrence, R Chan-": "personality and social psychology, 111(5):686, 2016."
        },
        {
          "[8]\nJort F Gemmeke, Daniel PW Ellis, Dylan Freedman, Aren Jansen, Wade Lawrence, R Chan-": "[19] Alan S Cowen and Dacher Keltner.\nSemantic space theory: A computational approach to"
        },
        {
          "[8]\nJort F Gemmeke, Daniel PW Ellis, Dylan Freedman, Aren Jansen, Wade Lawrence, R Chan-": "emotion. Trends in Cognitive Sciences, 25(2):124–136, 2021."
        },
        {
          "[8]\nJort F Gemmeke, Daniel PW Ellis, Dylan Freedman, Aren Jansen, Wade Lawrence, R Chan-": "[20] Björn W Schuller, Anton Batliner, Shahin Amiriparian, Alexander Barnhill, Maurice Gerczuk,"
        },
        {
          "[8]\nJort F Gemmeke, Daniel PW Ellis, Dylan Freedman, Aren Jansen, Wade Lawrence, R Chan-": "Andreas Triantafyllopoulos, Alice Baird, Panagiotis Tzirakis, Chris Gagne, Alan S Cowen,"
        },
        {
          "[8]\nJort F Gemmeke, Daniel PW Ellis, Dylan Freedman, Aren Jansen, Wade Lawrence, R Chan-": "et al.\nThe acm multimedia 2023 computational paralinguistics challenge: Emotion share &"
        },
        {
          "[8]\nJort F Gemmeke, Daniel PW Ellis, Dylan Freedman, Aren Jansen, Wade Lawrence, R Chan-": "requests. arXiv preprint arXiv:2304.14882, 2023."
        },
        {
          "[8]\nJort F Gemmeke, Daniel PW Ellis, Dylan Freedman, Aren Jansen, Wade Lawrence, R Chan-": "[21] Reza Lotﬁan and Carlos Busso. Building naturalistic emotionally balanced speech corpus by"
        },
        {
          "[8]\nJort F Gemmeke, Daniel PW Ellis, Dylan Freedman, Aren Jansen, Wade Lawrence, R Chan-": "retrieving emotional speech from existing podcast recordings. IEEE Transactions on Affective"
        },
        {
          "[8]\nJort F Gemmeke, Daniel PW Ellis, Dylan Freedman, Aren Jansen, Wade Lawrence, R Chan-": "Computing, 10(4):471–483, 2019."
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "[22] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.": "Gans trained by a two time-scale update rule converge to a local nash equilibrium. Advances"
        },
        {
          "[22] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.": "in neural information processing systems, 2017."
        },
        {
          "[22] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.": "[23]\nJames A Russell. A circumplex model of affect. Journal of personality and social psychology,"
        },
        {
          "[22] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.": "39(6):1161, 1980."
        },
        {
          "[22] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.": "[24] Florian Eyben, Martin Wöllmer, and Björn Schuller. Opensmile:\nthe munich versatile and"
        },
        {
          "[22] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.": "the 18th ACM International Conference on\nfast open-source audio feature extractor.\nIn Proc."
        },
        {
          "[22] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.": "Multimedia, pages 1459–1462, 2010."
        },
        {
          "[22] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.": "[25] Panagiotis Tzirakis, George Trigeorgis, Mihalis A Nicolaou, Björn W Schuller, and Stefanos"
        },
        {
          "[22] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.": "IEEE\nZafeiriou.\nEnd-to-end multimodal emotion recognition using deep neural networks."
        },
        {
          "[22] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.": "Journal of Selected Topics in Signal Processing, 11(8):1301–1309, 2017."
        },
        {
          "[22] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.": "[26] Bagus Tris Atmaja, Zanjabila, and Akira Sasou.\nJointly predicting emotion, age, and country"
        },
        {
          "[22] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.": "using pre-trained acoustic embedding, 2022."
        },
        {
          "[22] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.": "[27] Tilak Purohit, Imen Ben Mahmoud, Bogdan Vlasenko, and Mathew Magimai. Doss. Compar-"
        },
        {
          "[22] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.": "ing supervised and self-supervised embedding for exvo multi-task learning track, 2022."
        },
        {
          "[22] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.": "[28] Xin Jing, Meishu Song, Andreas Triantafyllopoulos, Zijiang Yang, and Björn W. Schuller. Re-"
        },
        {
          "[22] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.": "dundancy reduction twins network: A training framework for multi-output emotion regression,"
        },
        {
          "[22] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.": "2022."
        },
        {
          "[22] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.": "[29]\nJosh Belanich, Krishna Somandepalli, Brian Eoff, and Brendan Jou. Multitask vocal burst"
        },
        {
          "[22] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.": "modeling with resnets and pre-trained paralinguistic conformers, 2022."
        },
        {
          "[22] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.": "[30] Roshan Sharma, Tyler Vuong, Mark Lindsey, Hira Dhamyal, Rita Singh, and Bhiksha Raj."
        },
        {
          "[22] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.": "Self-supervision and learnable strfs for age, emotion, and country prediction, 2022."
        },
        {
          "[22] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.": "[31] Atijit Anuchitanukul and Lucia Specia.\nBurst2vec: An adversarial multi-task approach for"
        },
        {
          "[22] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.": "predicting emotion, age, and origin from vocal bursts, 2022."
        },
        {
          "[22] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.": "[32] Chin-Cheng Hsu.\nSynthesizing personalized non-speech vocalization from discrete speech"
        },
        {
          "[22] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.": "representations, 2022."
        },
        {
          "[22] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.": "[33] Marco Jiralerspong and Gauthier Gidel. Generating diverse vocal bursts with stylegan2 and"
        },
        {
          "[22] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.": "mel-spectrograms, 2022."
        },
        {
          "[22] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.": "[34] Detai Xin, Shinnosuke Takamichi, and Hiroshi Saruwatari. Exploring the effectiveness of self-"
        },
        {
          "[22] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.": "supervised learning and classiﬁer chains in emotion recognition of nonverbal vocalizations,"
        },
        {
          "[22] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.": "2022."
        },
        {
          "[22] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.": "[35] Bagus Tris Atmaja and Akira Sasou. Predicting affective vocal bursts with ﬁnetuned wav2vec"
        },
        {
          "[22] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.": "2.0, 2022."
        },
        {
          "[22] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.": "[36] Dang-Khanh Nguyen, Sudarshan Pant, Ngoc-Huynh Ho, Guee-Sang Lee, Soo-Huyng Kim,"
        },
        {
          "[22] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.": "and Hyung-Jeong Yang. Fine-tuning wav2vec for vocal-burst emotion recognition, 2022."
        },
        {
          "[22] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.": "[37] Tobias Hallmen, Silvan Mertes, Dominik Schiller, and Elisabeth André. An efﬁcient multitask"
        },
        {
          "[22] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.": "learning architecture for affective vocal burst analysis, 2022."
        },
        {
          "[22] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.": "[38]\nJinchao Li, Xixin Wu, Kaitao Song, Dongsheng Li, Xunying Liu, and Helen Meng. A hierar-"
        },
        {
          "[22] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.": "chical regression chain framework for affective vocal burst recognition, 2023."
        },
        {
          "[22] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.": "[39] Dang-Linh Trinh, Minh-Cong Vo, and Guee-Sang Lee.\nSelf-relation attention and temporal"
        },
        {
          "[22] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.": "awareness for emotion recognition via vocal burst, 2022."
        },
        {
          "[22] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.": "[40] Vincent Karas, Andreas Triantafyllopoulos, Meishu Song,\nand Björn W. Schuller.\nSelf-"
        },
        {
          "[22] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.": "supervised attention networks and uncertainty loss weighting for multi-task emotion recog-"
        },
        {
          "[22] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.": "nition on vocal bursts, 2022."
        },
        {
          "[22] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.": "[41] Muhammad Shehram Shah Syed, Zaﬁ Sherhan Syed, and Abbas Syed. Classiﬁcation of vo-"
        },
        {
          "[22] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.": "cal bursts for acii 2022 a-vb-type competition using convolutional neural networks and deep"
        },
        {
          "[22] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.": "acoustic embeddings, 2022."
        }
      ],
      "page": 11
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "5\nAppendix": "5.1\nMODULATE-SONATA Script"
        },
        {
          "5\nAppendix": "Amusement"
        },
        {
          "5\nAppendix": "“I’m the Juggernaut, bitch.” “Want to know how I got these scars?...He turns to me and he says: Why."
        },
        {
          "5\nAppendix": "So. Serious. Let’s put a smile on that face!” “In the face of overwhelming odds, I’m left with only"
        },
        {
          "5\nAppendix": "one option. I’m gonna have to science the shit outta this.”"
        },
        {
          "5\nAppendix": "Embarrassment"
        },
        {
          "5\nAppendix": "“I didn’t mean to...just...just look away please!” “Are you kidding? I sounded like a total buffoon!"
        },
        {
          "5\nAppendix": "I’m sure she’ll never even want to talk to me again.”"
        },
        {
          "5\nAppendix": "Sadness"
        },
        {
          "5\nAppendix": "“They cursed us. Murderer they called us. The cursed us, and drove us away. And we wept, Precious,"
        },
        {
          "5\nAppendix": "we wept to be so alone. And we only wish to catch ﬁsh so juicy sweet. And we forgot the taste of"
        },
        {
          "5\nAppendix": "bread...the sound of trees...the softness of the wind. We even forgot our own name. My precious.”"
        },
        {
          "5\nAppendix": "“Sometimes I wish I had never met you. Because then I could go to sleep at night not knowing there"
        },
        {
          "5\nAppendix": "was someone like you out there.” “We’re going to be okay. You can rest now.”"
        },
        {
          "5\nAppendix": "Elation"
        },
        {
          "5\nAppendix": "“I love the smell of napalm in the morning.” “Carpe diem. Seize the day, boys. Make your lives"
        },
        {
          "5\nAppendix": "extraordinary!” “Whew! That’s the stuff!”"
        },
        {
          "5\nAppendix": "Triumph"
        },
        {
          "5\nAppendix": "“Are you not entertained! Are you not entertained! Is this not why you are here?!” “King Kong ain’t"
        },
        {
          "5\nAppendix": "got shit on me!” “We will not go quietly into the night! We will not vanish without a ﬁght! We’re"
        },
        {
          "5\nAppendix": "going to live on! We’re going to survive! Today we celebrate...our Independence Day!”"
        },
        {
          "5\nAppendix": "Contempt"
        },
        {
          "5\nAppendix": "“My name is Maximus Decimus Meridius, commander of the Armies of the North, General of the"
        },
        {
          "5\nAppendix": "Felix Legions and loyal servant\nto the true emperor, Marcus Aurelius.\nFather to a murdered son."
        },
        {
          "5\nAppendix": "Husband to a murdered wife. And I will have my vengeance,\nin this life or\nthe next.” “You can"
        },
        {
          "5\nAppendix": "torture us and bomb us and burn our districts to the ground. But do you see that? Fire is catching."
        },
        {
          "5\nAppendix": "And if we burn...you burn with us.” “I’m surrounded by idiots.”"
        },
        {
          "5\nAppendix": "Disgust"
        },
        {
          "5\nAppendix": "“Get your stinking paws off me, you damned dirty ape.” “I can hardly forbear hurling things at him.”"
        },
        {
          "5\nAppendix": "Disappointment"
        },
        {
          "5\nAppendix": "“Why is the rum gone? Why is the rum...always gone?” “Well\nlook at you now; you just got your"
        },
        {
          "5\nAppendix": "asses whipped, by a bunch of goddamn nerds. I’m sure your father is rolling his his grave.” “I’m not"
        },
        {
          "5\nAppendix": "angry with you. I’m just...disappointed.”"
        },
        {
          "5\nAppendix": "Anger"
        },
        {
          "5\nAppendix": "“You talkin’ to me? You talking to ME? ARE YOU TALKING TO ME? THEY CALL ME MR."
        },
        {
          "5\nAppendix": "PIG!” “Badges? We ain’t got no badges! We don’t need no badges!\nI don’t have to show you"
        },
        {
          "5\nAppendix": "any stinking badges!” “Enough is enough!\nI have had it with these motherfucking snakes on this"
        },
        {
          "5\nAppendix": "motherfucking plane!”"
        }
      ],
      "page": 12
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Adoration": "“When I drift off, I will dream about you.\nIt’s always you.” “Because I still wake up every morn-"
        },
        {
          "Adoration": "ing...and the ﬁrst thing I want to do is see your face.”"
        },
        {
          "Adoration": "Sympathy"
        },
        {
          "Adoration": "“Ye best start believin’ in ghost stories, Miss Turner. Yer in one.” “Oh yes,\nthe past can hurt. But"
        },
        {
          "Adoration": "you can either run from it, or learn from it.” “If I told you what happens...it won’t happen.”"
        },
        {
          "Adoration": "Distress"
        },
        {
          "Adoration": "“You don’t understand!\nI coulda had class.\nI coulda been a contender.\nI could’ve been somebody,"
        },
        {
          "Adoration": "instead of a bum, which is what I am.” “I don’t know where I am.\nI don’t know what’s going no."
        },
        {
          "Adoration": "I think I lost somebody but I..I can’t remember...” “Gentlemen, you can’t ﬁght in here! This is the"
        },
        {
          "Adoration": "War Room!” “Westballz doesn’t even try to win. He just styles on you as hard as possible, and he’s"
        },
        {
          "Adoration": "so good at that, he just wins.”"
        },
        {
          "Adoration": "Pain"
        },
        {
          "Adoration": "“I would have followed you...my brother...my captain...my king.” “I don’t care!” Harry yelled,"
        },
        {
          "Adoration": "snatching up a lunascope and throwing it\ninto the ﬁreplace.\n“I’ve had enough,\nI’ve seen enough,"
        },
        {
          "Adoration": "I want it to end, I don’t care anymore!” “But anyways, I just - ouch! Oh, sonnoffabitch, right on the"
        },
        {
          "Adoration": "corner of the table, ooh, that one stings...”"
        },
        {
          "Adoration": "Hype"
        },
        {
          "Adoration": "“This is where we ﬁght! This is where they die!” “Leeeeroy Jenkins!”"
        },
        {
          "Adoration": "Seduction / Ecstasy"
        },
        {
          "Adoration": "“Mm...this chocolate is\nrich...and velvety...and so very delicious...” “I bet you would enjoy that"
        },
        {
          "Adoration": "luscious dragonfruit very much, wouldn’t you..” “Yes..\nI want\nthe cupcakes...\nI want all of\nthe"
        },
        {
          "Adoration": "cupcakes.” “Your appetite...\nis very impressive....” “The yogurt!\nIt ﬂows like water.\nIt’s soo good."
        },
        {
          "Adoration": "I’ll have what I’m having!”"
        },
        {
          "Adoration": "Desire"
        },
        {
          "Adoration": "“It is mine, I tell you. My own. My precious....yes, my precious...” “If I could just see her again...hear"
        },
        {
          "Adoration": "her again...touch her again...maybe I could die happy.”"
        },
        {
          "Adoration": "Contentment"
        },
        {
          "Adoration": "“Mama always said life was like a box of chocolates. You never know what you’re gonna get.”"
        },
        {
          "Adoration": "“Elementary, my dear Watson.” “Get busy living, or get busy dying.” “No thanks,\nI’m perfectly"
        },
        {
          "Adoration": "happy sitting here with the sun on my face reading a good book for as long as the weather will\nlet"
        },
        {
          "Adoration": "me.”"
        },
        {
          "Adoration": "Relief"
        }
      ],
      "page": 13
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Interest": "“Hope.\nIt is the only thing stronger than fear. A little hope is effective. A lot of hope is dangerous."
        },
        {
          "Interest": "A spark is ﬁne, as long as it’s contained.” “What do you mean, you have a plan? Care to share?”"
        },
        {
          "Interest": "Confusion"
        },
        {
          "Interest": "“The plot thickens, as they say. Why, by the way? Is it a soup metaphor?” “What is this? A center"
        },
        {
          "Interest": "for ants?”"
        },
        {
          "Interest": "Awe"
        },
        {
          "Interest": "“Toto, I’ve got a feeling we’re not\nin Kansas anymore.” “You’re gonna need a bigger boat.” “Yoo!"
        },
        {
          "Interest": "Did he just walk up...slowly...and down smash?!”"
        },
        {
          "Interest": "Surprise (positive)"
        },
        {
          "Interest": "“Cinderella story. Outta nowhere. A former greenskeeper, now, about to become the Masters cham-"
        },
        {
          "Interest": "pion.\nIt\nlooks like a mirac - It’s in the hole!\nIt’s in the hole!\nIt’s in the hole!” “And the crowd’s"
        },
        {
          "Interest": "cheering for a four stock...this could be it...THIS IS IT!”"
        },
        {
          "Interest": "Surprise (negative)"
        },
        {
          "Interest": "“You drilled a hole in the dentist?!” “What\nthe - what\nthe hell has been going on here? What, did"
        },
        {
          "Interest": "you think I wouldn’t notice?”"
        },
        {
          "Interest": "Fear"
        },
        {
          "Interest": "“I’m not going near there! Sunnyside is a place of ruin and despair, ruled by an evil bear who smells"
        },
        {
          "Interest": "of strawberries!” “You want me to ﬁght? No way, man, he’s gonna kill me. He’s gonna kill me and"
        },
        {
          "Interest": "my whole family. I’m out, you hear me, I’m out!”"
        }
      ],
      "page": 14
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Audiolm: a language modeling approach to audio generation",
      "authors": [
        "Zalán Borsos",
        "Raphaël Marinier",
        "Damien Vincent",
        "Eugene Kharitonov",
        "Olivier Pietquin",
        "Matt Sharifi",
        "Olivier Teboul",
        "David Grangier",
        "Marco Tagliasacchi",
        "Neil Zeghidour"
      ],
      "year": "2022",
      "venue": "Audiolm: a language modeling approach to audio generation"
    },
    {
      "citation_id": "2",
      "title": "Musiclm: Generating music from text",
      "authors": [
        "Andrea Agostinelli",
        "I Timo",
        "Zalán Denk",
        "Jesse Borsos",
        "Mauro Engel",
        "Antoine Verzetti",
        "Qingqing Caillon",
        "Aren Huang",
        "Adam Jansen",
        "Marco Roberts",
        "Matt Tagliasacchi",
        "Neil Sharifi",
        "Christian Zeghidour",
        "Frank"
      ],
      "year": "2023",
      "venue": "Musiclm: Generating music from text"
    },
    {
      "citation_id": "3",
      "title": "Moûsai: Text-to-music generation with long-context latent diffusion",
      "authors": [
        "Flavio Schneider",
        "Zhijing Jin",
        "Bernhard Schölkopf"
      ],
      "year": "2023",
      "venue": "Moûsai: Text-to-music generation with long-context latent diffusion"
    },
    {
      "citation_id": "4",
      "title": "Audioldm: Text-to-audio generation with latent diffusion models",
      "authors": [
        "Haohe Liu",
        "Zehua Chen",
        "Yi Yuan",
        "Xinhao Mei",
        "Xubo Liu",
        "Danilo Mandic",
        "Wenwu Wang",
        "Mark Plumbley"
      ],
      "year": "2023",
      "venue": "Audioldm: Text-to-audio generation with latent diffusion models"
    },
    {
      "citation_id": "5",
      "title": "High fidelity music synthesis on a shoestring budget",
      "authors": [
        "Kinyugo Maina",
        "Msanii"
      ],
      "year": "2023",
      "venue": "High fidelity music synthesis on a shoestring budget"
    },
    {
      "citation_id": "6",
      "title": "Neural codec language models are zero-shot text to speech synthesizers",
      "authors": [
        "Chengyi Wang",
        "Sanyuan Chen",
        "Yu Wu",
        "Ziqiang Zhang",
        "Long Zhou",
        "Shujie Liu",
        "Zhuo Chen",
        "Yanqing Liu",
        "Huaming Wang",
        "Jinyu Li"
      ],
      "year": "2023",
      "venue": "Neural codec language models are zero-shot text to speech synthesizers",
      "arxiv": "arXiv:2301.02111"
    },
    {
      "citation_id": "7",
      "title": "Dcase 2017 challenge setup: Tasks, datasets and baseline system",
      "authors": [
        "Annamaria Mesaros",
        "Toni Heittola",
        "Aleksandr Diment",
        "Benjamin Elizalde",
        "Ankit Shah",
        "Emmanuel Vincent"
      ],
      "year": "2017",
      "venue": "DCASE 2017-Workshop on Detection and Classification of Acoustic Scenes and Events"
    },
    {
      "citation_id": "8",
      "title": "Audio set: An ontology and human-labeled dataset for audio events",
      "authors": [
        "Jort F Gemmeke",
        "P Daniel",
        "Dylan Ellis",
        "Aren Freedman",
        "Wade Jansen",
        "R Channing Lawrence",
        "Manoj Moore",
        "Marvin Plakal",
        "Ritter"
      ],
      "year": "2017",
      "venue": "2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "9",
      "title": "A dataset and taxonomy for urban sound research",
      "authors": [
        "Justin Salamon",
        "Christopher Jacoby",
        "Juan Bello"
      ],
      "year": "2014",
      "venue": "Proceedings of the 22nd ACM international conference on Multimedia"
    },
    {
      "citation_id": "10",
      "title": "Librispeech: an asr corpus based on public domain audio books",
      "authors": [
        "Vassil Panayotov",
        "Guoguo Chen",
        "Daniel Povey",
        "Sanjeev Khudanpur"
      ],
      "year": "2015",
      "venue": "2015 IEEE international conference on acoustics, speech and signal processing (ICASSP)"
    },
    {
      "citation_id": "11",
      "title": "Overview of birdclef 2022: Endangered bird species recognition in soundscape recordings",
      "authors": [
        "Stefan Kahl",
        "Amanda Navine",
        "Tom Denton",
        "Holger Klinck",
        "Patrick Hart",
        "Hervé Glotin",
        "Hervé Goëau",
        "Willem-Pier Vellinga",
        "Robert Planqué",
        "Alexis Joly"
      ],
      "year": "2022",
      "venue": "Working Notes of CLEF"
    },
    {
      "citation_id": "12",
      "title": "An analysis of the gtzan music genre dataset",
      "authors": [
        "L Bob",
        "Sturm"
      ],
      "year": "2012",
      "venue": "Proceedings of the second international ACM workshop on Music information retrieval with user-centered and multimodal strategies"
    },
    {
      "citation_id": "13",
      "title": "The icml 2022 expressive vocalizations workshop and competition: Recognizing, generating, and personalizing vocal bursts",
      "authors": [
        "Alice Baird",
        "Panagiotis Tzirakis",
        "Gauthier Gidel",
        "Marco Jiralerspong",
        "B Eilif",
        "Kory Muller",
        "Björn Mathewson",
        "Erik Schuller",
        "Dacher Cambria",
        "Alan Keltner",
        "Cowen"
      ],
      "year": "2022",
      "venue": "The icml 2022 expressive vocalizations workshop and competition: Recognizing, generating, and personalizing vocal bursts",
      "arxiv": "arXiv:2205.01780"
    },
    {
      "citation_id": "14",
      "title": "The acii 2022 affective vocal bursts workshop & competition",
      "authors": [
        "Alice Baird",
        "Panagiotis Tzirakis",
        "Jeffrey Brooks",
        "Chris Gregory",
        "Björn Schuller",
        "Anton Batliner",
        "Dacher Keltner",
        "Alan Cowen"
      ],
      "year": "2022",
      "venue": "2022 10th International Conference on Affective Computing and Intelligent Interaction Workshops and Demos (ACIIW)"
    },
    {
      "citation_id": "15",
      "title": "Gautam Naik, Erik Cambria, and Rada Mihalcea. Meld: A multimodal multi-party dataset for emotion recognition in conversations",
      "authors": [
        "Soujanya Poria",
        "Devamanyu Hazarika",
        "Navonil Majumder"
      ],
      "year": "2018",
      "venue": "Gautam Naik, Erik Cambria, and Rada Mihalcea. Meld: A multimodal multi-party dataset for emotion recognition in conversations",
      "arxiv": "arXiv:1810.02508"
    },
    {
      "citation_id": "16",
      "title": "Presenting the VENEC corpus: Development of a cross-cultural corpus of vocal emotion expressions and a novel method of annotating emotion appraisals",
      "authors": [
        "Petri Laukka",
        "Hillary Anger Elfenbein",
        "Wanda Chui",
        "Nutankumar Thingujam",
        "Frederick Iraki",
        "Thomas Rockstuhl",
        "Jean Althoff"
      ],
      "year": "2010",
      "venue": "Proc. LREC 2010 Workshop on Corpora for Research on Emotion and Affect"
    },
    {
      "citation_id": "17",
      "title": "What do we hear in the voice? an open-ended judgment study of emotional speech prosody",
      "authors": [
        "Hillary Anger Elfenbein",
        "Petri Laukka",
        "Jean Althoff",
        "Wanda Chui",
        "Frederick Iraki",
        "Thomas Rockstuhl",
        "Nutankumar Thingujam"
      ],
      "year": "2022",
      "venue": "Personality and Social Psychology Bulletin"
    },
    {
      "citation_id": "18",
      "title": "The expression and recognition of emotions in the voice across five nations: A lens model analysis based on acoustic features",
      "authors": [
        "Petri Laukka",
        "Hillary Anger Elfenbein",
        "S Nutankumar",
        "Thomas Thingujam",
        "Frederick Rockstuhl",
        "Wanda Iraki",
        "Jean Chui",
        "Althoff"
      ],
      "year": "2016",
      "venue": "Journal of personality and social psychology"
    },
    {
      "citation_id": "19",
      "title": "Semantic space theory: A computational approach to emotion",
      "authors": [
        "Alan Cowen",
        "Dacher Keltner"
      ],
      "year": "2021",
      "venue": "Trends in Cognitive Sciences"
    },
    {
      "citation_id": "20",
      "title": "The acm multimedia 2023 computational paralinguistics challenge: Emotion share & requests",
      "authors": [
        "W Björn",
        "Anton Schuller",
        "Shahin Batliner",
        "Alexander Amiriparian",
        "Maurice Barnhill",
        "Andreas Gerczuk",
        "Alice Triantafyllopoulos",
        "Panagiotis Baird",
        "Chris Tzirakis",
        "Alan Gagne",
        "Cowen"
      ],
      "year": "2023",
      "venue": "The acm multimedia 2023 computational paralinguistics challenge: Emotion share & requests",
      "arxiv": "arXiv:2304.14882"
    },
    {
      "citation_id": "21",
      "title": "Building naturalistic emotionally balanced speech corpus by retrieving emotional speech from existing podcast recordings",
      "authors": [
        "Reza Lotfian",
        "Carlos Busso"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "22",
      "title": "Gans trained by a two time-scale update rule converge to a local nash equilibrium",
      "authors": [
        "Martin Heusel",
        "Hubert Ramsauer",
        "Thomas Unterthiner",
        "Bernhard Nessler",
        "Sepp Hochreiter"
      ],
      "year": "2017",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "23",
      "title": "A circumplex model of affect",
      "authors": [
        "Russell James"
      ],
      "year": "1980",
      "venue": "Journal of personality and social psychology"
    },
    {
      "citation_id": "24",
      "title": "Opensmile: the munich versatile and fast open-source audio feature extractor",
      "authors": [
        "Florian Eyben",
        "Martin Wöllmer",
        "Björn Schuller"
      ],
      "year": "2010",
      "venue": "Proc. the 18th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "25",
      "title": "End-to-end multimodal emotion recognition using deep neural networks",
      "authors": [
        "Panagiotis Tzirakis",
        "George Trigeorgis",
        "A Mihalis",
        "Björn Nicolaou",
        "Stefanos Schuller",
        "Zafeiriou"
      ],
      "year": "2017",
      "venue": "IEEE Journal of Selected Topics in Signal Processing"
    },
    {
      "citation_id": "26",
      "title": "Jointly predicting emotion, age, and country using pre-trained acoustic embedding",
      "authors": [
        "Zanjabila Bagus Tris Atmaja",
        "Akira Sasou"
      ],
      "year": "2022",
      "venue": "Jointly predicting emotion, age, and country using pre-trained acoustic embedding"
    },
    {
      "citation_id": "27",
      "title": "Comparing supervised and self-supervised embedding for exvo multi-task learning track",
      "authors": [
        "Tilak Purohit",
        "Imen Ben Mahmoud",
        "Bogdan Vlasenko",
        "Mathew Magimai",
        "Doss"
      ],
      "year": "2022",
      "venue": "Comparing supervised and self-supervised embedding for exvo multi-task learning track"
    },
    {
      "citation_id": "28",
      "title": "Redundancy reduction twins network: A training framework for multi-output emotion regression",
      "authors": [
        "Xin Jing",
        "Meishu Song",
        "Andreas Triantafyllopoulos",
        "Zijiang Yang",
        "Björn Schuller"
      ],
      "year": "2022",
      "venue": "Redundancy reduction twins network: A training framework for multi-output emotion regression"
    },
    {
      "citation_id": "29",
      "title": "Multitask vocal burst modeling with resnets and pre-trained paralinguistic conformers",
      "authors": [
        "Josh Belanich",
        "Krishna Somandepalli",
        "Brian Eoff",
        "Brendan Jou"
      ],
      "year": "2022",
      "venue": "Multitask vocal burst modeling with resnets and pre-trained paralinguistic conformers"
    },
    {
      "citation_id": "30",
      "title": "Self-supervision and learnable strfs for age, emotion, and country prediction",
      "authors": [
        "Roshan Sharma",
        "Tyler Vuong",
        "Mark Lindsey",
        "Hira Dhamyal",
        "Rita Singh",
        "Bhiksha Raj"
      ],
      "year": "2022",
      "venue": "Self-supervision and learnable strfs for age, emotion, and country prediction"
    },
    {
      "citation_id": "31",
      "title": "Burst2vec: An adversarial multi-task approach for predicting emotion, age, and origin from vocal bursts",
      "authors": [
        "Atijit Anuchitanukul",
        "Lucia Specia"
      ],
      "year": "2022",
      "venue": "Burst2vec: An adversarial multi-task approach for predicting emotion, age, and origin from vocal bursts"
    },
    {
      "citation_id": "32",
      "title": "Synthesizing personalized non-speech vocalization from discrete speech representations",
      "authors": [
        "Chin-Cheng Hsu"
      ],
      "year": "2022",
      "venue": "Synthesizing personalized non-speech vocalization from discrete speech representations"
    },
    {
      "citation_id": "33",
      "title": "Generating diverse vocal bursts with stylegan2 and mel-spectrograms",
      "authors": [
        "Marco Jiralerspong",
        "Gauthier Gidel"
      ],
      "year": "2022",
      "venue": "Generating diverse vocal bursts with stylegan2 and mel-spectrograms"
    },
    {
      "citation_id": "34",
      "title": "Exploring the effectiveness of selfsupervised learning and classifier chains in emotion recognition of nonverbal vocalizations",
      "authors": [
        "Shinnosuke Detai Xin",
        "Hiroshi Takamichi",
        "Saruwatari"
      ],
      "year": "2022",
      "venue": "Exploring the effectiveness of selfsupervised learning and classifier chains in emotion recognition of nonverbal vocalizations"
    },
    {
      "citation_id": "35",
      "title": "Predicting affective vocal bursts with finetuned wav",
      "authors": [
        "Bagus Tris",
        "Akira Sasou"
      ],
      "year": "2022",
      "venue": "Predicting affective vocal bursts with finetuned wav"
    },
    {
      "citation_id": "36",
      "title": "Fine-tuning wav2vec for vocal-burst emotion recognition",
      "authors": [
        "Dang-Khanh Nguyen",
        "Sudarshan Pant",
        "Ngoc-Huynh Ho",
        "Guee-Sang Lee",
        "Soo-Huyng Kim",
        "Hyung-Jeong Yang"
      ],
      "year": "2022",
      "venue": "Fine-tuning wav2vec for vocal-burst emotion recognition"
    },
    {
      "citation_id": "37",
      "title": "An efficient multitask learning architecture for affective vocal burst analysis",
      "authors": [
        "Tobias Hallmen",
        "Silvan Mertes",
        "Dominik Schiller",
        "Elisabeth André"
      ],
      "year": "2022",
      "venue": "An efficient multitask learning architecture for affective vocal burst analysis"
    },
    {
      "citation_id": "38",
      "title": "A hierarchical regression chain framework for affective vocal burst recognition",
      "authors": [
        "Jinchao Li",
        "Xixin Wu",
        "Kaitao Song",
        "Dongsheng Li",
        "Xunying Liu",
        "Helen Meng"
      ],
      "year": "2023",
      "venue": "A hierarchical regression chain framework for affective vocal burst recognition"
    },
    {
      "citation_id": "39",
      "title": "Self-relation attention and temporal awareness for emotion recognition via vocal burst",
      "authors": [
        "Dang-Linh Trinh",
        "Minh-Cong Vo",
        "Guee-Sang Lee"
      ],
      "year": "2022",
      "venue": "Self-relation attention and temporal awareness for emotion recognition via vocal burst"
    },
    {
      "citation_id": "40",
      "title": "Selfsupervised attention networks and uncertainty loss weighting for multi-task emotion recognition on vocal bursts",
      "authors": [
        "Vincent Karas",
        "Andreas Triantafyllopoulos",
        "Meishu Song",
        "Björn Schuller"
      ],
      "year": "2022",
      "venue": "Selfsupervised attention networks and uncertainty loss weighting for multi-task emotion recognition on vocal bursts"
    },
    {
      "citation_id": "41",
      "title": "Classification of vocal bursts for acii 2022 a-vb-type competition using convolutional neural networks and deep acoustic embeddings",
      "authors": [
        "Muhammad Shehram",
        "Shah Syed",
        "Zafi Sherhan Syed",
        "Abbas Syed"
      ],
      "year": "2022",
      "venue": "Classification of vocal bursts for acii 2022 a-vb-type competition using convolutional neural networks and deep acoustic embeddings"
    }
  ]
}