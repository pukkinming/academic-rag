{
  "paper_id": "2404.18514v1",
  "title": "A Systematic Evaluation Of Adversarial Attacks Against Speech Emotion Recognition Models",
  "published": "2024-04-29T09:00:32Z",
  "authors": [
    "Nicolas Facchinetti",
    "Federico Simonetta",
    "Stavros Ntalampiras"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Speech emotion recognition (SER) is constantly gaining attention in recent years due to its potential applications in diverse fields and thanks to the possibility offered by deep learning technologies. However, recent studies have shown that deep learning models can be vulnerable to adversarial attacks. In this paper, we systematically assess this problem by examining the impact of various adversarial white-box and black-box attacks on different languages and genders within the context of SER. We first propose a suitable methodology for audio data processing, feature extraction, and CNN-LSTM architecture. The observed outcomes highlighted the significant vulnerability of CNN-LSTM models to adversarial examples (AEs). In fact, all the considered adversarial attacks are able to significantly reduce the performance of the constructed models. Furthermore, when assessing the efficacy of the attacks, minor differences were noted between the languages analyzed as well as between male and female speech. In summary, this work contributes to the understanding of the robustness of CNN-LSTM models, particularly in SER scenarios, and the impact of AEs. Interestingly, our findings serve as a baseline for a) developing more robust algorithms for SER, b) designing more effective attacks, c) investigating possible defenses, d) improved understanding of the vocal differences between different languages and genders, and e) overall, enhancing our comprehension of the SER task.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "The exploration of automatic emotional state detection from vocal expressions has drawn considerable attention in the contemporary era, primarily due to its potential applicability in a broad spectrum of fields such as human-computer interaction, psychology, entertainment, and education  [1] [2] [3] . The introduction of deep learning techniques has markedly improved the performance of Speech Emotion Recognition (SER) models, fostering the development of numerous applications for public use  [4] . However, recent studies have underscored the vulnerability of deep learning models to Adversarial Examples (AEs) -carefully crafted input samples designed to mislead the model into producing erroneous predictions  [5] . This susceptibility could trigger serious consequences in an SER context, especially in applications that are integral to safety. Evaluating the robustness of SER models is crucial, given their prospective use in areas such as affective computing, human-robot interaction, and mental health monitoring. By assessing and enhancing the resilience of these models, researchers can aid in the creation of reliable and trustworthy tools for these vital real-world applications.\n\nAdversarial attacks represent a substantial menace to SER systems, resulting in the erroneous interpretation of a speaker's emotional condition, which might lead to severe implications. The following are five instances that illustrate the aforementioned risk:\n\n• In the context of customer service, an adversarial assault on an SER model might misread a customer's irritation as joy, culminating in an unsuitable response and possibly a discontented customer.\n\n• In the setting of mental health diagnosis, an adversarial incursion on an SER model utilized to identify depression could lead it to falsely categorize a patient as being in good health, resulting in a faulty diagnosis and insufficient treatment.\n\n• In the realm of entertainment, an adversarial onslaught on an SER model employed to modulate a virtual assistant's tone might result in responses that are incongruous with the user's expectations, leading to bewilderment and annoyance.\n\n• In a security-oriented scenario, an adversarial intrusion on an SER model used for detecting deceit during a police inquiry might lead to the misclassification of a suspect's truthful declarations as falsehoods, resulting in unwarranted allegations and potential erroneous detentions.\n\n• In the situation of a job interview, an adversarial strike on an SER model employed to assess a candidate's emotional aptitude might lead to the misinterpretation of a candidate's anxiety as hostility, culminating in an incorrect evaluation and potentially overlooking a competent candidate.\n\nIn recent years a novel branch of scientific research has been studying the impact of AEs on SER tasks. Research in this area primarily aims to gauge the resilience of SER systems against various forms of attacks, and to devise methodologies to enhance model performance in the face of such threats  [6] . Despite these efforts, the subject matter remains largely unexplored, necessitating further investigation to gain a more comprehensive understanding of how different attack techniques could potentially impact system performance. The outcomes of such research could, for instance, highlight specific types of attacks that are exceptionally proficient at misleading the model, or indicate that the model exhibits greater robustness against attacks that alter certain input data features. Furthermore, the study may reveal that a specific language or gender is more susceptible to these attacks.\n\nAlthough the impact of adversarial examples (AEs) on image models has been extensively studied, their application to speech emotion recognition (SER) models is lacking research. Furthermore, it is not possible to draw the same conclusions since the input data of image classification and SER models are only superficially similar. In fact, while the success of convolutional neural networks (CNNs) and Transformer-based architectures has been extended to the audio processing domain, the input samples used in the audio domain differ from those in the image domain in two main aspects: a) they often consist of sparse matrices, with most entries close to zero, and b) they are not easily segmentable, meaning that the same sound source (i.e., object) is spread across the matrix and is not contiguous as in the case of image segmentation. We believe that these differences warrant further investigation into the use of AEs in SER tasks.\n\nThis paper endeavours to address this gap in the existing body of knowledge by scrutinising the effects of multiple adversarial attacks on various languages and genders in the SER context. It is of paramount importance to assess the robustness of emotion recognition models, comprehend their limitations, and create more resilient algorithms. This process could entail evaluating the model's precision in the face of AEs that have been manipulated to trick the model into generating erroneous predictions. Within this framework, the primary contributions of this research are to:\n\n• conduct an exhaustive analysis of the susceptibility of Convolutional Neural Network-Long Short-Term Memory (CNN-LSTM) models to AE in SER;\n\n• compare the performance of diverse attack categories;\n\n• investigate potential disparities in the attack across three distinct languages and between male and female vocal samples.\n\nFollowing an initial exploration of the scientific literature pertaining to principal techniques and methodologies in SER and Adversarial Machine Learning, an optimal neural network model was identified to address the problem at hand. Given that there is no single model architecture that performs well across multiple languages in the literature  [7, 8] , we designed a model consisting of a fusion of CNN and LSTM, and is trained using log Mel-spectrograms derived from audio samples embodying diverse emotional states. The current paper focuses on multiple languages and scrutinizes the impacts of various adversarial attacks on speech data from both genders. To this end, three distinct datasets are utilized: EmoDB  [9]  for German, EMOVO  [10]  for Italian, and Ravdess  [11]  for English. A conscious decision was made to design and educate our unique model to ensure maximum flexibility during experimentation, rather than depending on pre-existing pre-trained models that may not yield satisfactory outcomes across all languages.\n\nInterestingly, a multitude of attacks were employed with the objective of assessing their influence on the established models. A broad spectrum of varied attack methodologies  [12]  was assessed, and, when feasible, diverse parameter sets were employed. The white-box attacks included were Fast Gradient Sign Method (FGSM)  [13] , Basic Iterative Method (BIM)  [14] , Deep-Fool  [15] , Jacobian-based Saliency Map Attack (JSMA)  [16] , and Carlini & Wagner (C&W)  [17] .\n\nFor the black-box attacks, PixelAttack  [18, 19] , and BoundaryAttack  [20]  were utilized in our experimentation. Following comprehensive experimentation, we present detailed results that evalu-ate the efficacy of SER models when subjected to various attacks, taking into account language and gender factors. The execution of all the experiments depicted in this article can be found at https://github.com/LIMUNIMI/thesis_adversarial_ml_audio.",
      "page_start": 1,
      "page_end": 7
    },
    {
      "section_name": "Analysis Of The Literature",
      "text": "Speech emotion recognition SER represents the computational challenge of discerning a speaker's emotional state by examining the acoustic properties of their speech signal  [21] . Emotions, being a crucial component of human communication, are manifested through various speech facets including pitch, tempo, intensity, and spectral features. Despite the complexity and diversity in emotional expression through speech, recent breakthroughs in machine learning and deep learning methodologies have propelled substantial advancements in this domain, thereby stimulating active research interest in SER.\n\nSER is a potent instrument, with its utility extending to a range of fields including virtual assistant development, emotion detection in customer service, and mental health surveillance. This paper provides a concise review of some pioneering studies and their respective application areas.\n\nAmong the earliest researches, Nakatsu et al.  [22]  explored the application of SER in an interactive movie system. This system not only allowed viewers to watch the narrative but also to engage with it, employing emotion recognition to facilitate spontaneous interactions among computer characters.\n\nIn a different context, Petrushin et al.  [23]  concentrated on identifying emotional states in telephone call center dialogues. Here, understanding the caller's mental state proved beneficial in decision support systems for tasks such as prioritizing voice messages, assigning suitable agents for responses, or categorizing voice mails based on the emotions expressed by the caller. Their study revealed anger as the most identifiable emotion. Further, France et al.  [24]  used acoustical characteristics as markers of depression and suicide risk, aiding therapists in comprehending their patients' concealed emotions and overall mental state. Lastly, Schuller et al.  [25]  proposed a method that combined acoustic features and linguistic information in the automotive industry to enhance car ride safety by monitoring the driver's mental state. Remarkably, this approach could trigger safety measures, potentially preventing accidents.\n\nThe research paper by  [21]  categorizes datasets for SER into three distinct types, based on the method of sample collection. These are simulated, semi-natural, and natural speech datasets. Among these, the simulated datasets are the most prevalent. They are synthesized by trained speakers who articulate the same text, each time embodying a different emotion. Such datasets typically portray a standard set of emotions and, due to their acted nature, exhibit less noise and realism in comparison to datasets of natural speech  [21] . The datasets utilized in the present study are all of the simulated kind.\n\nOne of the most frequently used datasets for SER tasks is the Berlin Database of Emotional Speech (EMO-DB)  [9] . This dataset features ten actors, evenly split by gender, who simulate a range of emotions while uttering ten German sentences of varying lengths. Specifically, seven emotions (neutral, anger, fear, joy, sadness, disgust, and boredom) are represented across approximately 800 sentences, including 700 primary samples and some secondary versions. The recordings were conducted in an anechoic chamber, and the resultant material was subjected to an automated listening test. Each sentence was assessed by a panel of 20 listeners. Additionally, electroglottograms are provided to facilitate more precise extraction of prosodic and voice quality features.\n\nThe Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS)  [11]  is a comprehensive, gender-balanced dataset. It comprises emotional speech and song recordings from 24 professional North American actors (12 female, 12 male), each contributing 104 sentences. This results in a total of 7356 speech and 3036 song samples, including both facial and vocal expressions. The speech component encapsulates a spectrum of emotions such as calm, happiness, sadness, anger, fear, surprise, and disgust, whereas the song component encompasses calm, happiness, sadness, anger, and fear. Each emotional expression is represented at two levels of intensity: normal and strong, complemented by a neutral expression. The validation process for RAVDESS involved two stages, with an initial group of 247 raters from North America followed by another group of 72 participants.\n\nThe EMOVO  [10]  dataset, on the other hand, is the inaugural emotional corpus tailored for the Italian language. It consists of recordings from six actors (three males and three females), each delivering 14 sentences that simulate seven emotions: disgust, fear, anger, joy, surprise, sadness, and neutral. These recordings, made using professional equipment in the Fondazione Ugo Bordoni laboratories, total 588, with each actor contributing approximately 10 minutes of material. This culminates in an overall database duration of one hour. The validation of the EMOVO dataset involved two distinct groups of 24 individuals, achieving an overall recognition accuracy of 80%.\n\nIn the realm of existing SER models, the Convolutional Neural Network-Long Short-Term Memory (CNN-LSTM) models have, in recent years, consistently exhibited remarkable effectiveness, achieving unparalleled results as evidenced in multiple studies  [7, 8, [26] [27] [28] . These models, characterized by their deep architecture, possess the ability to independently extract high-quality features from the data. Consequently, we have chosen to employ log-Mel spectrograms, a decision informed by their proven compatibility with this particular architecture in previous research  [7, 8, [28] [29] [30] [31] [32] .\n\nAdversarial Machine Learning The concept of \"Adversarial machine learning\" encompasses a collection of methodologies devised for instigating malevolent attacks by manipulating models using accessible information. Typically, Machine Learning (ML) models are constructed based on a particular train/test set derived from an identical statistical distribution  [33] . However, upon deployment, the model may be subjected to interference from an attacker who manipulates the system's operation by introducing meticulously designed input data. Such input, termed as adversarial example (AE)  [34] , comprises legitimate inputs modified by the addition of minimal, often undetectable, perturbations. These perturbations are designed to deceive the system, thereby altering the anticipated outcomes by exploiting certain susceptibilities, all while being accurately classified by a human observer.\n\nThe susceptibility of numerous ML models, including neural networks, to attacks instigated by minor modifications to the model's input during testing, is a significant concern. Biggio et al.  [35]  underscored this point by illustrating the dependency of an ML model's success on its robustness against adversarial data. Their exemplar was a malware detection system for PDF files, which relied on a differentiable discriminant function.\n\nThe research explored two distinct scenarios. The first scenario involved an attacker possessing comprehensive knowledge of the target classifier, including the feature space, the model type, and the trained model. Conversely, in the second scenario, the attacker's knowledge was limited. The employed attack strategy hinged on the gradient descent walk of the classifier's discriminant function g(x), which was assumed to be differentiable, or an approximation thereof. The findings highlighted that both support vector machines (SVM) and neural networks could be successfully evaded, even when the adversary's understanding of the system was minimal.\n\nRegarding contemporary state-of-the-art deep neural networks, which demonstrate remarkable generalization in classification tasks, one would anticipate robustness against minor perturbations of the input signal. However, Szegedy et al.  [34]  discovered that even a negligible yet carefully tailored perturbation of an input image could alter the network's prediction. Intriguingly, the error rate induced by these meticulously crafted examples surpassed that of examples perturbed with Gaussian noise, even though the average distortion was less.\n\nAdversarial ML and SER models The pioneering scheme for generating AE in the context of linguistic applications was introduced by  [36] . In their work, the authors focused on three paralinguistic tasks, including SER. Rather than applying perturbations to specific acoustic features, they opted to directly manipulate the raw waveform of an audio recording. The dataset utilized for their study was IEMOCAP, and the models of choice were WaveRNN and WaveCNN  [36] , both of which are considered to be state-of-the-art.\n\nIn the task of emotion recognition, both models demonstrated similar performance levels: Wa-veRNN achieved an accuracy rate of 84%, while WaveCNN slightly surpassed it with an accuracy of 85%. The authors employed the Fast Gradient Sign Method (FGSM)  [13] , detailed further in Section 2.4, as their chosen attack strategy. This method was applied twice, using various values of ϵ. When the perturbation factor was set to 0.015, there was a significant increase in the emotion recognition error rate for both models: from 16% to 48% for WaveRNN, and from 15% to 42.5% for WaveCNN. Notably, these rates approached an upper bound error rate of 50% when only two classes were considered. The authors also observed that the AEs generated could be profitably transferred from WaveCNN to WaveRNN.\n\nAn important observation made by the authors was that the perturbations introduced through their approach were not only smaller, but also more effective than those achievable through an attack at the Mel Frequency Cepstral Coefficients (MFCC) feature level.\n\nThe inaugural black-box adversarial attack on SER systems is put forward by the authors in  [37] . This attack, currently recognized as the Real-World Noise (RWN) attack, subtly manipulates the speech signal by incorporating minute, indiscernible noise. Upon experimental evaluation, the classification error rates were found to be 56.87 and 66.87 for the FAU-AIBO and IEMOCAP datasets, ates between adversarial and genuine examples. Interestingly, they discovered that the inclusion of a random noise layer did not benefit SER, a finding that contrasts with its impact on images.\n\nThe authors in  [31]  present a methodology for enhancing the resilience of a CNN based SER system against adversarial assaults. They employ the FGSM to generate adversarial instances from log Mel-spectrograms extracted from the DEMoS dataset. The robustness of three distinct models, namely a four-layer CNN, a ResNet model, and a VGG model, is assessed against these adversarial instances. The experimental results reveal a significant decline in the Unweighted Average Recall (UAR) of the models, from 0.8 to 0.2, with an increase in the ϵ parameter of FGSM.\n\nTo address this vulnerability, the authors propose three robustness-enhancing strategies. The initial strategy involves a data augmentation approach, which incorporates FGSM-generated instances into the training set. Subsequently, two adversarial training methodologies are suggested: vanilla and similarity-based. The former method proves effective in enhancing performance on authentic data compared to the conventional training approach, whereas the latter demonstrates superior efficacy in defending against adversarial attacks.\n\nThe necessity for pre-processing steps is evident when dealing with audio-type inputs, as it is crucial to extract certain features  [38] . Unlike CNN that can directly operate on image pixels, this context requires initial extraction of signal features such as MFCCs or a log Mel-spectrogram  [38] . The complexity increases when considering white-box attacks like Fast Gradient Sign Method (FGSM), which utilize the gradient of the targeted audio relative to the input to calculate the optimal perturbation. Although the backpropagation method is efficiently applicable in image recognition due to the differentiability of all layers, the scenario becomes intricate for SER systems. The complexity arises from the commonly extracted features, such as the introduction of non-linearity during the computation of MFCCs and the significant non-linearity in the output due to the usage of numerous Long Short-Term Memory (LSTM) units  [37, 39, 40] . In light of empirical studies within this context, iterative methods have demonstrated superior effectiveness compared to single-step approaches  [40] .\n\nTo the best of our knowledge, this constitutes the inaugural comprehensive examination of AEs impact on SER systems, encompassing a broad spectrum of elements, from the nature of the attacks to the spoken language and the gender of the speaker. In the spirit of ensuring complete reproducibility of our methodology and findings, the implementation has been made publicly accessible at https://github.com/LIMUNIMI/thesis_adversarial_ml_audio.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Materials And Methods",
      "text": "This section delineates the methodologies employed within the proposed framework for audio pattern recognition, which is dedicated to processing audio data and training SER models. It also provides a brief overview of the attacks utilized in the study.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Experimental Design",
      "text": "In order to assess adversarial attacks for Speech Emotion Recognition (SER) models, a specific pipeline was developed. The objective was to observe the impact of different attacks on different languages, while also controlling for the influence of speaker characteristics such as sex.\n\nThree datasets were used, each representing a different language: RAVDESS (English), EmoDB (German), and EMOVO (Italian). To ensure consistency in the evaluation of attack effectiveness, the same model was kept fixed throughout the experiments. Since there is currently no existing model capable of performing SER in multiple languages, a custom model was developed for this study. The data underwent a rigorous cleaning and preprocessing to obtain log-Mel spectrograms.\n\nLabels that exhibited high correlation or were heavily under or over-represented in the datasets were removed. To increase the amount of training data, data augmentation techniques were applied.\n\nA total of seven different CNN-LSTM architectures were designed and evaluated. The first model (M0) was used to determine the most effective normalization procedure, which was found to be simple standardization. This normalization procedure was then applied to all the remaining tests.\n\nThe seven architectures were compared, and the best performing multi-language model structure (M1) was identified. Further fine-tuning was conducted on the number of LSTM units and other hyper-parameters of this model.\n\nFinally, the obtained model and datasets were utilized to assess the impact of adversarial attacks from the ART library.\n\nOverall, this comprehensive methodology allowed for a thorough evaluation of adversarial attacks on SER models, taking into consideration different languages and controlling for speaker characteristics. The described workflow is shown in Figure  1 .",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "Datasets",
      "text": "The EmoDB and EMOVO databases share a similar quantity of samples, in contrast to the substantially larger Ravdess database, which is approximately 2.5 times greater in size. A notable variance is observed in the number of actors across these databases, as previously discussed in Section 1.1.\n\nIn terms of audio duration distributions, the mean and 25, 50, and 75 percentiles demonstrate comparable statistics across the databases. However, significant discrepancies are evident in the maximum duration and inter-file duration. Given our objective to assess the impact of various attacks on SER systems through comparative analysis across different languages and genders, it is essential to maintain uniform training parameterization across all architectures, inclusive of sample duration. When examining the labels, it is observed that EmoDB and EMOVO incorporate 7 emotions, whereas Ravdess includes 8. Further details concerning the extracted metadata can be found in Section 6.1.\n\nThe datasets under consideration presented several challenges, which are addressed in the subsequent experimental setup. These challenges include: a) heterogeneity in the duration and sample rate of audio files, b) inconsistency in the number of classes across datasets, c) scarcity of data.\n\nPre-processing In order to obtain homogeneity of the samples, we applied some basic processing to the data. First, the datasets' samples are uniformly resampled at a frequency of 16,000 Hz and the silence at the commencement and termination of each signal is eliminated. Successively, segments with a duration less than 3 seconds are looped, while those exceeding this length are segmented into continuous, non-overlapping 3-second intervals. This process of silence trimming is reiterated on the resulting segments to eliminate superfluous portions. Segments falling short of the 3-second standard are looped until they attain the requisite length. We finally computed the log-Mel spectrograms of the obtained audio excerpts, a choice motivated by prior research  [8] . Log-Mel spectrograms were computed out using 128 Mel bands, a Fast Fourier Transformation window length of 368, and a hop size of 184. These parameters, corresponding to 23 ms and 11.5 ms respectively, were set in line with a sampling rate of 16000 Hz, as suggested by  [41] . The spectrogram obtained was subsequently transformed to a logarithmic scale, leading to a 128x261 matrix saved for subsequent analysis. For illustrative purposes, an exemplar is provided in Figure  2 .\n\nRegarding the classes available in the datasets, since we are not interested in finding the optimal SER model, we chose to retain only five labels per dataset. We first computed log-Mel spectrograms of the audio excerpts obtained after segmentation, then, we used PCA and T-SNE to spot classes that were highly correlated. Moreover, other labels were discarded to improve the balance of the datasets. Specifically, the fear and angry labels were omitted from EmoDB. As detailed in Section 6.1, the angry label was the most prevalent, and its removal facilitated a more balanced dataset.\n\nFor EMOVO, the excluded labels were sad and angry, while for Ravdess, the calm, neutral, and angry labels were discarded. Similarly to the angry label in EmoDB, the neutral label in Ravdess was disproportionately represented and its exclusion rectified this imbalance.\n\nWe approached the problem of the scarcity of data using data augmentation methods. While GANs have shown encouraging results in SER tasks  [42] [43] [44] , our approach is grounded in the application of less computationally intensive techniques. Data augmentation has the potential to significantly enhance the precision of a classifier and facilitate better model generalization to unobserved data, as the model becomes more resilient to the deformations applied  [45] . According to  [45] , viable augmentation options encompass time stretching and pitch shifting. We employed an acceleration factor of [0.75, 0.9, 1.1, 1.25] for time stretching. For pitch shifting, each step was configured to correspond to a semitone, with the values [-3, -1.5, +1.5, +3] being considered.\n\nThe entire datasets are subjected to these processing, yielding eight supplementary augmented samples. To maintain a uniform duration of 3 seconds across all samples, identical procedures of division and repetition are subsequently applied to these newly generated samples.\n\nNormalization Once we obtained the augmented datasets, we were ready for continuing with subsequent phase as described in Section 2.1. However, one remaining step for preparing the data for neural models was the normalization of the log-Mel spectrograms. In literature, various methods are adopted without a single methodology being more successful than the others. We were therefore interested in testing them. The objective of the normalization procedure is to transform the input values into a range suitable for neural learning, typically either [0, 1] or [-1, +1].\n\nIn the initial stages, four distinct transformations of the features are contemplated:\n\n• Original: Utilizes the log Mel-spectrogram matrices devoid of any normalization, functioning as a baseline for assessing the practical benefits of the subsequent transformations.\n\n• NormSum: Implements normalization by dividing each element of the matrix by the aggregate of all elements, thereby ensuring uniform sound energy across all samples. However, the values of each cell are significantly small and approach zero.\n\n• NormMaxGlobal: This method normalizes by dividing each matrix element by the maximum value across the dataset, ensuring all values fall within the [0, 1] range while preserving the original proportions between the cells across varying spectrograms.\n\n• NormMaxLocal: Normalization is achieved by scaling each element in a matrix through division by the maximum value within that specific spectrogram. This process ensures each matrix has a minimum value of 0 and a maximum value of 1, but the original proportions between cells are not preserved.\n\nBeyond the aforementioned transformations, we opted to standardize each previously delineated version. This decision was informed by preliminary experiments that indicated a notable instability in the model's learning process, evidenced by significant fluctuations in the loss function across epochs. In this standardization phase, we initially transformed the matrices into arrays, subsequently standardized these, and then reshaped the data to its original form.\n\nFor assessing the best normalization strategy, we developed a rudimentary CNN-LSTM, designated as M0, which demonstrated notable potential in preliminary investigations -see Section 2.3.\n\nThe accuracy and its corresponding standard deviation, derived from each processing variant and dataset, are presented in Table  1  as per the definition provided in Section 3.\n\nThe assessment indicates that the use of NormMaxGlobal in the context of EmoDB and Original Standardized for EMOVO and Ravdess, leads to enhanced accuracies as reflected in Table  1 .\n\nConsistently, the standardized variant yields more reliable results, corroborating the preliminary experimental observations. Additionally, the loss function exhibits fewer temporal variations, fostering a steadier learning trajectory.\n\nAlthough NormMaxGlobal exhibited superior performance with respect to accuracy and standard deviation on the EmoDB dataset, we opted to utilize Original Standardized consistently across all three instances, which maximizes the accuracy across the datasets on average. The standardization was applied to the entire datasets after the above-described pre-processing steps.",
      "page_start": 8,
      "page_end": 10
    },
    {
      "section_name": "Models",
      "text": "Multi-dataset architecture We developed 7 model architectures to search for the an optimal model across the three datasets that could be used for a fair assessment of the attacks.\n\nThe base model M0 incorporates three Conv2D layers, exhibiting an increase in filters  (16, 32, 64 ) and a decrease in square kernel size, in a manner akin to the models presented in  [46, 47] . The activation function employed is ReLU, as suggested by  [47] . Following each convolutional layer is a MaxPooling layer, with a pool size that varies ((from  (4, 4)  initially to (2,2) subsequently)) and strides (initially 2, later 1), mirroring the approach in  [46, 47] . To mitigate overfitting, expedite training, and facilitate the adoption of elevated learning rates, a BatchNormalization layer is positioned post the initial convolution  [48] . The output from the CNN is flattened and transferred to an LSTM layer comprising three internal units, configured with an internal dropout of 0.2. The final output layer is a Dense layer with five units, employing a Softmax activation function to generate the probability for the five labels present in each dataset. A comprehensive delineation of the architecture is provided in Section 6.2.\n\nWe then designed other 6 models, each exhibiting a progressive increase in parameter count and complexity. These models bear the nomenclature M1, M2, M3, M4, M5, and M6. The networks are sequentially arranged based on their parameter quantity, with slight variations in both CNN structure and LSTM internal unit count, while maintaining a foundational structure akin to M0.\n\nIn-depth information regarding the architectures is provided in Section 6.2. Every convolutional layer uniformly employs the same kernel size, with the exception of models M3 and M6 that utilize more intricate architectures. The pooling operations exhibit a similar pattern, save for the reduced pool size in the initial pooling layer of models M3 and M4. In the case of M5, there is a doubling of the filter count for each layer, whereas model M6 opts for a quartet over a trio. As for the LSTM aspect, all models function in a unidirectional manner, barring M1 which operates bidirectionally. Models M2, M4, and M6 incorporate six units, in contrast to the remaining models which utilize three.\n\nThe models undergo training with a batch size of 32 across 50 epochs, utilizing the Adam optimization algorithm with a learning rate of 0.001, in line with  [49] 's approach to a similar CNN-LSTM architecture. The categorical crossentropy serves as the loss function. To mitigate overfitting, an EarlyStopping callback is introduced with a tolerance of 10 epochs, which observes the validation loss. Failing to observe an improvement over 10 epochs prompts the restoration of the weights corresponding to the optimal validation loss. Moreover, a ReduceLROnPlateau callback is implemented to decrease the learning rate upon observing no improvement in validation loss over six epochs. This strategy aims to prevent the model from straying from the ideal solution due to an excessive learning rate, while also promoting convergence by adopting smaller steps towards the cost function's optimal solution with a diminished learning rate. The underlying theory is that as the model nears a suboptimal solution with the current learning rate, it oscillates around the global minimum. Reducing the rate allows for smaller steps towards the cost function's optimal solution. The validation loss serves as the metric for this callback, and the learning rate is reduced by a factor of 0.1 when there is no improvement in the validation loss for six epochs.\n\nIn the realm of SER tasks, a prevalent evaluation strategy is the Leave-One-Speaker-Out (LOSO) cross-validation method, which designates each unique speaker as a test set. Nonetheless, this research deviates from the norm, opting for a conventional train/validation/test partition due to the inconsistency in the number of actors across datasets, which inevitably yields diverse test sets.\n\nSuch disparity could potentially skew performance assessments when juxtaposing attacks on varying languages and genders. In particular, the data was apportioned into three splits, with proportions of 64%, 16%, and 20% respectively. To ensure a more reliable estimation of the performance metrics, this procedure was reiterated thrice, each time employing a distinct random seed.\n\nThe architectures and datasets under consideration were evaluated using identical train/validation/test splits, consistent with the methodology employed in the preceding stage. The mean accuracy and corresponding standard deviation for each split, across all architectures and datasets, are tabulated in Table  2 .\n\nThe M1 architecture emerges as the most potent, demonstrating satisfactory performance across all three datasets. Compared to M0, the accuracy exhibits a substantial enhancement: a gain of +0.12 points for EmoDB, +0.11 for EMOVO, and +0.23 for Ravdess. Concurrently, a notable reduction in the standard deviation signifies a robust generalization capability. The bidirectional configuration's efficacy is also evident, as it surpasses M2-an architecture with identical CNN and output shape post-LSTM, but with six unidirectional units.\n\nFurthermore, the convergence across all splits for each dataset is a shared characteristic of both M1 and M2, a trait not observed in other configurations. A noteworthy observation is the inverse relationship between model complexity and accuracy, implying an optimal parameter count in M1\n\nfor the given training set size. Intriguingly, the application of more intricate or deeper networks does not confer any advantage for Ravdess, despite its larger number of examples. Once more, EmoDB consistently outperforms, reinforcing the notion that it represents a less complex task.\n\nConsequently, it is cogent to persist in the optimization of hyperparameters for M1, with its architecture succinctly revisited in Section 6.2.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Hyper-Parameter Optimization",
      "text": "The final stage in defining the model architecture involves hyperparameter tuning to augment the model's performance on the respective datasets. The hyperparameter optimization was split into two parts: first, we optimized the number of LSTM units, then the remaining hyper-parameters that did not impact the overall model architecture. For this, we used the Hyperband tuner  [50]  with categorical cross-entropy loss.\n\nPrior studies indicated an enhancement in performance upon augmenting the LSTM layer's output quantity, as evidenced by the 3 units in M0 and the 6 units in both M1 and M2. Consequently, additional investigations were undertaken on the M1 model to ascertain the ideal quantity of LSTM units. In particular, we examined multiple bidirectional configurations, encompassing 4, 8, 16, 32, 64, 128, 256, 512, and 1024 units.\n\nTypically, the model's accuracy is enhanced by augmenting the quantity of LSTM units, although this association does not consistently apply to the final four values. To illustrate, the configurations yielding the most superior outcomes, in descending order, are as follows:\n\n• For EmoDB, the top four configurations encompass 512, 256, 128, and 1024 units;\n\n• For EMOVO, the top four configurations comprise 512, 256, 1024, and 128 units;\n\n• For Ravdess, the top four configurations consist of 1024, 256, 512, and 128 units.\n\nConsequently, the LSTM layer was equipped with 256 units due to the following reasons:\n\n1. It consistently delivered the second highest performance across all three scenarios;\n\n2. The performance disparity between the top configuration and this one is negligible;\n\n3. It engenders a more streamlined model, thereby mitigating the potential for overfitting.\n\nFor additional insights pertaining to the tuning of model M1, please refer to section 6.3.\n\nSince the number of internal units in the LSTM layer has considerably increased, an additional Dropout layer was added after the Flatten layer to prevent possible overfitting. Figure  3  summarizes the final architecture of the model.\n\nThe second part of the hyper-parameter optimization consisted of the fine-tuning of the dropout layer's probability, the internal dropout probability of the BLSTM layer, the initial learning rate for the Adam optimizer, and the batch size.\n\nThe tuning of the latter two parameters, namely the learning rate and batch size, is of paramount importance to avert overfitting. These parameters significantly influence the model's performance, convergence, and stability. They are intricately linked to the problem at hand, the input data, and the model's architecture.\n\nThe hyperparameter tuning process has notably enhanced the performance, particularly for larger datasets like EMOVO and Ravdess. For instance, the loss function showed significant reductions:\n\n• In the EmoDB dataset, the loss minimally decreased from 0.321270 to 0.312610, which is a reduction of 0.00866 or 2.69%;\n\n• In the EMOVO dataset, the loss substantially decreased from 0.428224 to 0.344954, which is a reduction of 0.08327 or 19.44%;\n\n• In the Ravdess dataset, the loss remarkably decreased from 0.366705 to 0.281702, which is a reduction of 0.08501 or 23.18%.\n\nThis enhancement in performance through hyperparameter tuning has particularly allowed for a more effective utilization of datasets with a larger volume of data. Additional details regarding the optimization process can be referred to in Section 6.3.\n\nThe final accuracies of the models are presented in Table  3 . Fast Gradient Sign Method (FGSM)  [13] , a well-known and straightforward adversarial threat generation technique, primarily exploits the L ∞ distance metric. The method manipulates machine learning models by introducing minimal perturbations to the input data, thereby inducing the model to generate erroneous predictions. Specifically designed to exploit the learning mechanism of neural networks, FGSM utilizes the gradient of the loss with respect to the input data to augment the input data in a way that maximizes the loss. This is achieved by adding a noise vector, derived from the sign of the gradient, to the input data  [51] . The following equation typically illustrates the generation of an adversarial example via this method:",
      "page_start": 12,
      "page_end": 14
    },
    {
      "section_name": "Attack Algorithms",
      "text": "Given an input X and its corresponding label y, the loss function J(•), and a noise magnitude parameter ϵ, the authors demonstrated the potential for successful adversarial attacks. The parameter ϵ is carefully chosen to balance two conflicting requirements: it must be sufficiently small to render the perturbations imperceptible to humans, yet large enough to mislead the model into making erroneous predictions. The efficacy of such attacks is contingent upon the model's gradient strength, the magnitude of the added noise, and the model's complexity.\n\nThe authors' experiments on the ImageNet dataset using the GoogLeNet CNN  [13] , revealed the ease with which highly effective adversarial examples could be generated. The AEs, created using the fast method, maintained a similar accuracy level until ϵ = 32. Beyond this point, the accuracy gradually declined to nearly zero as ϵ increased to 128  [52] . This phenomenon can be attributed to the fact that the Fast Gradient Sign Method (FGSM) adds noise scaled by ϵ to each image. Consequently, utilizing higher ϵ values effectively obliterates the image content, rendering it unrecognizable to humans.\n\nBasic Iterative Method (BIM)  [14] . This method, an extension of FGSM utilizing the L ∞ distance metric, employs multiple iterations with a minimal step size. The BIM attack commences with an initial input image, following which the adversary calculates the gradient of the model's loss function relative to this image. Subsequently, the image is incrementally adjusted in the direction of this gradient. This iterative procedure continues either for a predetermined number of iterations or until the attainment of the desired output. Through continuous input modifications based on the model's gradient, the adversary can gradually manipulate the image to induce an erroneous prediction from the model. Specifically, during each iteration, the pixel values of the input image are confined to ensure their location within the ϵ-neighbourhood of the original image.\n\nThe recursive function used to produce an AE from an input image X is the following:\n\nThe function Clip X,ϵ (X ′ ) performs per-pixel clipping on the image X ′ , resulting in an L∞ ϵneighbourhood of the original image X. The label associated with X is represented by y, while the loss function and step size are denoted by J() and ϵ respectively. The authors empirically determined optimal values, setting α = 1 which modifies each pixel value by 1 at every step. The number of iterations was chosen heuristically as min(ϵ + 4, 1.25ϵ), a balance ensuring that the AE would reach the boundary of the L∞ ϵ-neighbourhood while maintaining a manageable computational cost for experiments.\n\nThe experimental results  [14]  reveal that the iterative method induces subtler perturbations compared to Fast Gradient Sign Method (FGSM), maintaining the integrity of the image even at high ϵ values. Concurrently, it confounds the classifier at a higher rate. Specifically, the Basic Iterative Method (BIM) generates superior AE for ϵ < 48. Beyond this threshold, however, its performance plateaus and no further improvements are observed.\n\nDeepFool (DF)  [15]  is an adversarial attack method that crafts AEs by iteratively computing the minimal L 2 perturbations. This iterative process seeks to ascertain the shortest distance from the original input to the decision boundary of the threat model. The underlying premise of the DeepFool technique is the local approximation of highly nonlinear deep neural networks (NNs) by linear decision boundaries. This assumption allows the authors to analytically formulate the optimal solution to this simplified problem and subsequently construct the AE. Given that NNs are not strictly linear, the algorithm incrementally moves towards the derived solution, repeating the process until a genuine AE is discovered. The DeepFool algorithm utilizes the resulting gradient to determine the optimal direction and magnitude of the perturbations necessary to induce a misclassification by the model. For any differentiable classifier, DeepFool presumes that f is linear around x ′ t and iteratively computes the perturbation r t : arg min\n\nThe algorithm, in every iteration, computes the gradient of the decision function relative to the input data. Subsequently, it determines the least perturbation necessary to transition the input data point beyond the decision boundary into the subsequent class. This procedure is iteratively executed until the model misclassifies the input data point.\n\nIt has been demonstrated by the authors that the proposed technique effectively deceives advanced image recognition systems. Furthermore, the perturbation instigated by DeepFool is found to be less than that of FGSM across multiple benchmark datasets  [15] .\n\nJacobian Saliency Map Attack (JSMA)  [16]  is an effective adversarial methodology employing a greedy algorithm. This approach leverages L 0 distances to generate AEs by iteratively modifying individual pixels. The gradient of the loss, with respect to every input component, is exploited to identify key pixels and their corresponding perturbations. This is facilitated by a saliency map, which pinpoints the input features of significance to the adversary's objectives. The saliency map is derived from the forward derivative (Jacobian) of the function that a Deep Neural Network (DNN) has learned. The procedure of the JSMA attack can be encapsulated as follows:\n\n• Calculate the Jacobian matrix pertaining to the model's output with respect to the input image;\n\n• Generate the saliency map;\n\n• Select a target class, denoted as l, for the AE;\n\n• From the saliency map, pinpoint the most influential features that augment the likelihood of class l while simultaneously diminishing the likelihood of the original class;\n\n• Adjust the aforementioned features by a specified parameter θ, to create an AE that is erroneously classified as l;\n\n• Iterate over steps 2-5 until the AE is successfully produced.\n\nThe JSMA is a notably potent technique for the creation of AEs with minimal perturbations, which are often challenging to identify. The original work by Papernot et al.  [16]  provides a comprehensive explanation of the precise formulation employed, which we recommend for interested readers.\n\nThe authors demonstrate that this algorithm can consistently generate samples that, while perceived as correctly classified by human subjects, are misclassified by a Deep Neural Network (DNN) towards specific targets. This is achieved with an impressive adversarial success rate of 97% and an average modification of merely 4.02% of the input features per sample  [16] . Despite its effectiveness, the JSMA technique may not always be the most efficient choice due to its computational cost. Furthermore, its performance may vary compared to other attack methods under certain circumstances.\n\nOptimization-Based Attacks by Carlini and Wagner (C&W)  [17] . This robust method, capable of generating AE measured in L 0 , L 2 , and L ∞ norms, is an optimization-based approach.\n\nIt modifies the objective and the primary constraint of the AE generation optimization problem, as initially proposed in  [34] . However, the constraint under consideration possesses a highly nonlinear nature, which prompts the authors to recast it in a form more conducive to optimization. Consequently, the optimization problem is reformulated by integrating the constraint within the objective, as shown below:\n\nIn the context of the chosen distance metric ||.|| p with L p norm and a suitably selected constant parameter c > 0, while maintaining the second constraint from  [34]  unaltered, equation 4 yields more potent AEs when tackled with gradient descent in comparison to the FGSM.\n\nThe implementation nuances diverge based on the employed metric as follows:\n\n• The L 2 norm implementation incorporates multiple initial points for the gradient descent to mitigate the chances of the algorithm landing in unfavorable local minima.\n\n• As stated by Carlini and Wagner  [17] , the L 0 metric is non-differentiable, necessitating the use of an iterative algorithm. This algorithm identifies the least significant pixels in each iteration (utilizing the L 2 attack) and subsequently fixes them. Upon identifying a minimal subset of pixels, it is employed to generate an AE. This approach bears resemblance to the JSMA, with the exception that while JSMA expands a set of alterable pixels, the C&W L 2 method reduces the pixel set.\n\n• In line with Carlini and Wagner  [17] , the L ∞ metric lacks full differentiability and conventional gradient descent yields subpar results. This issue is circumvented with the use of an iterative algorithm, replacing the ||δ|| p term in the objective formulation with a penalty term τ = 1. This revised objective is re-evaluated at each iteration and provided δ < τ , the latter is reduced by a factor of 0.9, allowing transition to the subsequent iteration; otherwise, the search is terminated.\n\nThe full details of the implementation are pretty complex and can be found in  [17] .\n\nThe experimental results presented by the authors demonstrate that each distance metric employed in the attacks yields AEs that are closer in comparison to those obtained from previous state-ofthe-art attacks  [17] . The L 0 and L 2 attacks produce AEs that exhibit 2× to 10× lower distortion compared to the best attacks reported in the literature, boasting a success probability of 100%.\n\nAlthough the L ∞ attacks yield AEs of similar quality to other studies, they outperform in terms of successful attack rates. Moreover, the effectiveness of the proposed techniques is such that their performance improves with increasing task complexity, a condition under which other methods typically deteriorate. The C&W attacks consistently achieve a 100% success rate on naturally trained DNNs across various datasets including MNIST, CIFAR-10, and ImageNet. Additionally, these attacks can successfully compromise defensive distilled models, a feat that DeepFool fails to accomplish in its search for adversarial samples  [53] .\n\nPixel Attack (PA)  [18, 19] . This method, predicated on differential evolution (DE), fabricates AE by perturbing a single  [18]  or a limited number  [19]  of pixels, utilizing either the L 0 or L ∞ metric. The attack delineated in these studies concentrates on a small number of pixels without restricting the intensity of modification. This attack aims to derive an AE x ′ from an original sample\n\nx via the computation of a minimal perturbation δ. This results in x ′ = x + δ, f (x) ̸ = f (x ′ ), where f () represents the model's output. Consequently, the goal of the associated optimization problem is  [19] :\n\nwhere th denotes a pre-specified threshold parameter that governs the maximum count of alterable pixels, while g(•) c signifies the confidence associated with the correct class c, such that f (x) = arg max g(x). The perturbations are encapsulated within arrays, referred to as candidate solutions, and are subjected to optimization through the process of differential evolution  [18] .\n\nEach candidate solution comprises a constant number of perturbations where every perturbation alters a single pixel. This alteration is represented as a quintuple that includes the (x, y) coordinates and the RGB values associated with the perturbation. Upon generation, each candidate solution is pitted against its respective parent, based on the population index, and the victor persists into the subsequent iteration.\n\nThe potential evolution strategies include differential evolution and Covariance Matrix Adaptation. As previously discussed, the attack methodology is designed around two distance metrics  [19] ,\n\nwhich are detailed as follows:\n\n• Threshold Attack: Leveraging the L ∞ metric, this attack can enact slight perturbations across all pixels. It is constrained by the optimization of ||δ|| ∞ ≤ th, which allows the algorithm to search within the same space as the input, given that the variables can be any variation of the input, with a maximum limit of th.\n\n• Few-Pixel Attack: Utilizing the L 0 metric, this attack can strongly perturb selected pixels. It is a variation of the original One-Pixel Attack  [18] , and it optimizes the constraint ||δ|| 0 ≤ th.\n\nIn this scenario, the search space for the variables is reduced, as it is a combination of pixel values (dependent on channels 'c' in the image) and position (two values X, Y) for all of the th pixels.\n\nThe experimental findings reveal a significant disparity in robustness when faced with L 0 and L ∞ norm attacks. The attack's effectiveness is remarkably high, even with exceedingly low threshold th values, requiring only a slight perturbation to successfully generate AE  [19] .\n\nBoundary Attack (BA)  [20] . The BA is an iterative adversarial attack that operates without a gradient. It commences from a substantial adversarial perturbation and subsequently endeavors to minimize the L 2 distance between the original and perturbed examples, while maintaining the adversarial nature. Specifically, the attack begins with an image of the target class and alternates steps between moving the image along the decision boundary (maintaining its adversarial status)\n\nand steps moving towards the original image to discover incrementally smaller perturbations. The direction of the steepest ascent on the boundary surface, which is the direction where the model output alters most rapidly, is identified at each iteration by the attacker. The fundamental concept of the algorithm revolves around executing a rejection sampling with an appropriate proposal distribution P to identify incrementally smaller perturbations. During the k-th step, the aim is to draw a perturbation η k from a maximum entropy distribution while adhering to the subsequent constraints:\n\n1. The perturbed instance, denoted as õ, is confined within the original input domain: õk-1 The original formulation presents substantial complexity due to the challenges in sampling from the given distribution. In light of this, a more straightforward heuristic is employed as follows:\n\n• Utilization of a Gaussian distribution N (0, 1), which is independent and identically distributed;\n\n• Perturbed samples undergo rescaling and clipping to ensure the satisfaction of constraints (  1 ) and (2);\n\n• The parameter η k is projected onto a sphere centred at o, such that d(o, õk-1 +η k ) = d(o, õk-1 , thereby maintaining constraint  (1) . This is referred to as the orthogonal perturbation step;\n\n• A modest progression is made towards the original image, ensuring that constraints (1) and\n\n(3) are upheld.\n\nThe algorithm, as elucidated earlier, pivots on two significant parameters: the cumulative perturbation length δ and the step length ϵ in the direction of o. The intricacies of parameter modification are complex, hence, for a comprehensive understanding, the reader is directed to the original study  [20] .\n\nEmpirical evidence substantiates the efficiency of the Boundary attack as a robust black-box attack.\n\nIts capability to locate AEs with minimal perturbations, which are challenging for human detection, underscores its potency. Moreover, its superiority over other gradient-based attacks is evidenced by its ability to bypass the computationally intensive task of gradient computation with respect to the input.",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "Results And Discussion",
      "text": "The ensuing discourse delineates the outcomes of the attacks under consideration, as previously introduced in Section 2.4. The ART library  [54]  provided the implementations for all the methodologies under scrutiny, facilitating the examination of a multitude of configuration parameters to comprehensively probe the potential of each technique. To maintain conciseness, the specifics of each attack's results are relegated to the Supplementary Material, where the experiments with diverse parameter values are documented. The primary focus of this section, however, remains the comparative analysis of the optimized attacks.\n\nThe evaluation metrics employed include the model's accuracy over the manipulated samples, the average perturbation introduced to the AEs vis-a-vis the original samples, and the time required for processing. The first two metrics, also utilized in all ART library 1  examples, serve as the primary measures for gauging the performance disparities between attacks and thus, were chosen as the principal metrics for assessing the efficacy of the methods implemented. The training and evaluation of the models were conducted using an identical distribution of data for each split as mentioned in the preceding section, specifically 64/16/20% for training, validation, and testing, respectively. Detailed insights regarding the structure of the test for each dataset can be referred to in Section 6.4. The performance of these models, evaluated in terms of accuracy on the comprehensive test set, as well as on the subsets of male and female emotional speech, is presented in Table  3 .\n\nInitially, a comparative evaluation is conducted predicated on the metrics under consideration. Subsequently, a comprehensive analysis is carried out, factoring in the inherent properties of the diverse attack techniques.\n\nIn our data presentation, we focus solely on the attack configuration that results in the lowest accuracy parameter, indicative of optimal performance within an adversarial context. This approach is maintained even in light of performance variances across genders under different configurations.\n\nIn instances where multiple configurations yield identical outcomes, our selection is guided by the configuration that produces the minimum average perturbation. Table  4  encapsulates the optimal configurations for each dataset and attack, serving as a comparative reference in subsequent chapters.\n\nIn general, the optimal configuration remains consistent across the entire test set, for both male and female samples. However, a few exceptions have been noted. For the EMOVO dataset, the FGSM attack outperforms others on the entire data and female samples when eps = 0.5. Yet, for male samples, equivalent accuracy is achieved when eps = 0.25, albeit with reduced perturbation.\n\nSimilarly, the JSMA attack on the EMOVO dataset provides superior results on the entire data and female samples when theta = +1. However, male samples exhibit improved accuracy with theta = -1. Differently, for the Ravdess dataset, the JSMA attack proves most effective on the entire data and male samples when theta = +1, while female samples show enhanced accuracy with theta = -0.5.",
      "page_start": 19,
      "page_end": 20
    },
    {
      "section_name": "Performance Comparison",
      "text": "In this section, we evaluate the outcomes, taking into account each performance metric separately.  The precision of the models is assessed using the AEs derived from the instigated attacks, as detailed in Table  4 . The resultant accuracy metrics are presented in Table  5  and illustrated in Figure  5 .",
      "page_start": 20,
      "page_end": 20
    },
    {
      "section_name": "Accuracy In The Experiments, We Computed The Accuracy As",
      "text": "The analysis of the results reveals a substantial impact of all attack variants, including the rudimentary FGSM, on the models' performance. Among these, PixelAttack exhibits a distinct efficacy, setting itself apart from the rest. Its effectiveness is particularly pronounced for EMOVO and Ravdess, whereas it shows less impact on EmoDB. Despite this discrepancy, the minimal perturbations it introduces to the samples, as elaborated in the succeeding section, underscore its impressive performance. This idiosyncratic behavior may be attributed to the specific configurations employed, characterized by low thresholds denoted by th. Across all three datasets, the JSMA attack emerges as the most potent, reducing the accuracy to approximately 1-2%.\n\nThe performance of all attacks on the EmoDB dataset is reasonable. The simple yet effective FGSM reduces the accuracy to approximately 10%, while BIM, DeepFool, and C&W yield similar results at around 6.5%. Interestingly, BoundaryAttack, a black-box attack operating with less information, outperforms the latter, suggesting that a successful attack can be executed with minimal or no information about the target.\n\nIn the EMOVO dataset, FGSM exhibits an unusual efficiency, ranking second and surpassing its iterative BIM variant. The BIM-DeepFool-C&W trio performs similarly, achieving around 8% accuracy, on par with BoundaryAttack.\n\nIn the case of Ravdess, BIM-DeepFool-C&W again generates similar results, this time around 5.6%, whereas BoundaryAttack does not maintain the same level of performance as observed in the other datasets.\n\nIn the comparative analysis of the three languages, the aggregate results exhibit remarkable similarity. The arithmetic mean of accuracies derived from white-box attacks by AEs is the lowest for EmoDB, registering at 0.063, followed by Ravdess at 0.067, and lastly, EMOVO at 0.068. This allows us to infer with confidence that German, Italian, and English demonstrate equivalent susceptibility in the context of a SER task. Further exploration of this subject will be undertaken in the succeeding discourse.\n\nTurning our attention to the variance between male and female samples, white-box attacks proved to be more successful on male subjects in 9 out of 15 instances. In the case of EMOVO, this trend is discernible in 4 out of 5 instances, a noteworthy observation given the higher accuracy for males compared to females in the original dataset. Likewise, for Ravdess, male samples proved more susceptible in 3 instances. Conversely, for EmoDB, women were more impacted by 3 out of 5 attacks. The disparity is rather pronounced in certain attacks, with the accuracy differing by approximately 0.04 (excluding PixelAttack), while in other instances, the variation is around 0.01.\n\nIn summary, JSMA emerges as the most potent assault, substantially undermining the performance of models across all languages. The trio of BIM, DeepFool, and C&W exhibits a consistent performance across all cases. Even with its unassuming complexity, FGSM has demonstrated its efficacy across all three datasets, with a notable impact on EMOVO. Moreover, despite its black-box characteristics, BoundaryAttack achieves commendable results in two out of the three cases. We have also evidenced how PixelAttack, by altering merely a handful of pixels, can induce significant deviations in a model's behavior.\n\nThe susceptibility of diverse languages to white-box attacks exhibits no marked disparities. The languages under scrutiny, namely German, Italian, and English, all demonstrate susceptibility to adversarial incursions. Furthermore, the analysis reveals that AEs derived from male audio samples typically yield higher efficacy, notably within the context of the EMOVO and Ravdess datasets.\n\nThe augmentation of datasets was accomplished through the use of pitch shifting and time stretching techniques, inducing deformations to the input samples. Despite this, the deformations were not adequate to guarantee a robust defense against all forms of AEs attacks. These observations underscore the pronounced susceptibility of the SER task to such attacks when addressed using a CNN-LSTM model trained on log Mel-spectrograms.\n\nPerturbation We now turn our attention to the average perturbation induced by the attacks in the creation of the AEs. The data presented in Table  6  pertain to the highest accuracy outcomes derived from the parameter configurations encapsulated in Table  4  and depicted in Figure  6 . For further inspection, we also provide samples of spectrograms perturbed in Figures  9 10 11 12 13 14 .\n\nThe preceding section discussed the configuration of PixelAttack, which manipulates only a restricted set of pixels, thereby leading to a markedly diminished mean perturbation. Apart from PixelAttack, the most commendable performance is delivered by JSMA. Intriguingly, the configuration yielding the lowest accuracy for this attack is not the one introducing the minimal perturbation, but rather corresponds to theta = 0.5. Further details are provided in Section 6.4, which elucidates that each theta value corresponds to an average perturbation that is lower than that of all instances presented in Table  6 . In the most unfavorable scenario, i.e., when theta = -1, the outcomes are akin to those obtained with C&W using the L2 distance, albeit surpassing the results of the other attacks.\n\nThe triplet comprising BIM, DeepFool, and C&W, previously discussed for their comparable accuracy, demonstrates substantial discrepancies when it comes to the magnitude of introduced perturbations. Notably, an order of magnitude difference is observed between attacks for each dataset.\n\nIn ascending order of perturbation magnitude, the attacks are C&W, BIM, and DeepFool.\n\nUpon application to EmoDB, the optimal configuration of DeepFool, irrespective of gender, does not necessarily yield the least amount of noise introduced, albeit the discrepancy is negligible. In the case of Ravdess, the least perturbation is attained with a solitary iteration of the algorithm, the difference being a mere 0.02. Comprehensive details for both instances are available in Section 6.4.\n\nRemarkably, despite its nature as a black-box attack, BoundaryAttack exhibits performance on par with, and occasionally surpassing that of DeepFool.\n\nThe outcomes of the evaluated algorithms are generally consistent across various languages, with the exception of FGSM. This divergence can be attributed to performance fluctuations corresponding to different eps values, which dictate the attack step size. As anticipated, a decrease in eps values results in reduced perturbation levels. Based on the configurations outlined in Table  4 , the perturbation levels span from optimal to suboptimal for Ravdess, EMOVO, and EmoDB respectively.\n\nThe influence of the speaker's gender on all adversarial attacks is generally insignificant, with the notable exception of DeepFool, which exhibits the most substantial disparities.\n\nTo summarize, the Jacobian-based Saliency Map Attack (JSMA) emerges as the most efficient attack mechanism, given its proficiency in inducing perturbations in the sample data. Its effective-ness is further underscored by the resultant decrease in accuracies, thereby solidifying its position as the preeminent attack strategy.\n\nAs evidenced in tables 30 and 31, DeepFool is characterized by the introduction of substantial noise. However, neither the accuracy nor the noise level exhibits noticeable enhancement with an escalation in the iteration count. The PixelAttack method, on the other hand, substantiates the feasibility of misleading the model through the alteration of a minimal number of pixels in the log Mel-spectrogram.\n\nAs Figures 9-14 exemplify, the resulting spectrograms are mainly degraded because of an almost uniform decrease of the amplitude of the spectrograms, resulting in sparse outlier points. When listening to such samples, the user is usually able to detect the attacked audio due to a noticeable decrease in volume. The GitHub repository contains examples of such audio.",
      "page_start": 20,
      "page_end": 23
    },
    {
      "section_name": "Execution Time",
      "text": "In line with the evaluation carried out for precision and average disturbance, our attention now turns to the duration required by the under-consideration attack algorithms for the creation of AEs. The trial runs were conducted on a workstation of the HP Z4 G4 series, equipped with an i9-9820X CPU, a Nvidia TITAN V GPU possessing 12 GB of RAM, and a CPU RAM of 64 GB. The findings depicted in Figure  4  derive from the most effective precision outcomes garnered from the parameter setups delineated in Table  4 .\n\nThe FGSM, as anticipated, outperforms all other attacks in terms of speed, even while introducing substantial noise. This makes it an effective and efficient approach for generating AEs rapidly.\n\nAlthough none of the selected configurations prove to be the fastest in terms of execution time, the disparities across various experiments are trivial, merely amounting to fractional seconds.\n\nFocusing on the BIM-DeepFool-C&W trio, which we reiterate achieves comparable accuracy outcomes, noticeable variations are evident in terms of execution time aside from perturbation.\n\nThe C&W attack is the most time-consuming and requires a substantial duration to yield results.\n\nThe scenario remains unchanged even when the L 2 distance is taken into account: the ensuing durations are akin to those of PixelAttack with th = 1, as highlighted in Section 6.4.\n\nContrarily, DeepFool is capable of generating AEs promptly, although the perturbation induced is substantially high, as previously discussed.\n\nBIM necessitates marginally extended durations compared to DeepFool, yet significantly less than C&W, positioning it as an optimal choice for creating high-quality AEs with minimal noise and in a reasonable timeframe. Although the configurations utilized are not the quickest, the disparities are inconsequential, akin to FGSM.\n\nAs previously noted, JSMA presents a high degree of effectiveness in impairing performance and introducing perturbation, albeit it requires a longer duration to generate samples in comparison to FGSM, BIM, and DeepFool. Nevertheless, its execution time remains considerably less than that of C&W. It is important to highlight that the parameter configurations selected to minimize accuracy concurrently result in reduced execution times. Despite the outcome for EMOVO not being strictly superior, the difference in execution time is a mere second.\n\nContrary to the majority of white-box attacks, the two black-box attacks necessitate a longer duration, except for C&W which is the slowest technique overall. This is anticipated due to their limited insight into the internal workings of the target model.\n\nFor PixelAttack, it is pertinent to mention, as elaborated further in Section 6.4, that increasing the number of modifiable pixels reduces the computation time. As previously stated, it is important to consider that experimenting with larger values may lead to a decrease in both the model's accuracy and the time required.",
      "page_start": 23,
      "page_end": 24
    },
    {
      "section_name": "Comparison Between Characteristics",
      "text": "In this section, a critical examination of the metrics under consideration is undertaken, with subsequent conclusions drawn from the intrinsic properties of each methodology and potential variances within the data.",
      "page_start": 24,
      "page_end": 24
    },
    {
      "section_name": "Variations In Distance Metrics",
      "text": "It is imperative to understand that diverse attack types strive to minimize the disparity between the original sample, employing a range of distance metrics. A summary of the distance metrics utilized by the implemented ART is presented in Table  7 .\n\nDrawing definitive conclusions about potential disparities among distances and attacks utilizing the same distance metric is a complex task. Notably, JSMA and PixelAttack employ the L 0 distance, yet their effectiveness varies significantly. JSMA proves to be the most efficient, whereas PixelAttack is less effective. The gathered data indicates that these attacks introduce minimal perturbations, as they aim to reduce the quantity of altered pixels, a characteristic inherent to the L 0 distance.\n\nConsidering the L 2 category, it encompasses DeepFool and one variant of C&W. The interpretation of results becomes intricate here as the two attacks yield considerably divergent outcomes. Despite C&W achieving superior accuracy (tripling the score on EmoDB), the perturbation it introduces is remarkably lower (by three orders of magnitude). However, the generation of AEs via C&W is significantly more time-consuming, taking thousands of times longer than DeepFool. Consequently, it is challenging to extract consistent patterns from the executed experiments. The only conclusive remark is that the outcomes derived using the L 2 distance are profoundly influenced by the attack's intrinsic logic.\n\nLastly, the L ∞ category, which encompasses the most substantial number of attacks, is considered. Despite all attacks striving to minimize the maximum discrepancy between the original and manipulated examples, the results exhibit significant variations. The accuracy is comparable in all instances (with the exception of FGSM and BoundaryAttack on Ravdess, which demonstrate notably superior results), yet the average perturbation and execution times differ considerably among various cases.\n\nHence, the overall behavior of these techniques is primarily determined by their internal mechanisms.\n\nThe task of discerning a universal pattern through the juxtaposition of dissimilarities among attack groups utilizing identical metrics presents a considerable challenge. The primary source of variability in algorithms arises from the inherent methodology, rather than the minimized distance.\n\nThe complexity is further heightened when attempting to compare attacks employing disparate metrics.\n\nA preliminary inference drawn from the acquired results suggests that L 0 attacks appear to induce fewer perturbations compared to attacks that deploy other distance metrics. Yet, this inference warrants additional scrutiny, considering the fact that our testing was confined to merely two algorithms within this category, and PixelAttack was set up differently compared to the rest of the techniques.\n\nIterative and normal versions BIM and FGSM, two techniques with a comparative relationship, given BIM is an extension of FGSM, exhibit differing performance under varying conditions. Upon examination of their accuracy results in light of all parameters considered, as presented in Tables  23  and 26  in Section 6.4, it is notable that the eps parameter appears to exert no influence on BIM's performance. This is evidenced by the consistent accuracy across different eps values.\n\nConversely, FGSM's accuracy fluctuates with the parameter, presenting a unique trend for each language. This suggests that BIM, with its independence from identifying the optimal configuration for deceiving the model, may be more advantageous.\n\nThe choice of datasets also influences the performance, as demonstrated in Table  5 . BIM was observed to be more proficient in diminishing accuracy in the EmoDB and Ravdess datasets, while FGSM demonstrated slightly superior performance in the EMOVO dataset.\n\nWith regards to the perturbation introduced, an increase in eps results in heightened noise for both attacks, as illustrated in Tables 24 and 28 in Section 6.4. This is an anticipated outcome, given that the eps parameter signifies the maximum perturbation an attacker can introduce. Nevertheless, BIM generates more refined AEs with lower perturbation values.\n\nAlthough BIM requires a significantly longer duration than FGSM to yield results, the attack time of 51 seconds for the largest dataset, Ravdess, is deemed acceptable, as indicated in Tables  29  and 25  in Section 6.4.",
      "page_start": 24,
      "page_end": 24
    },
    {
      "section_name": "White And Black Box Attacks",
      "text": "In this study, we executed a series of five white-box attacks and two black-box attacks. Despite the disparity in their quantities, a comparative analysis of these two categories is still significant. This significance stems from the distinct configuration of PixelAttack, which leads to considerably dissimilar results compared to other forms of attacks. Moreover, a closer inspection of the internal procedures utilized by these algorithms allows us to perceive this comparison in the context of gradient-based (white-box) and gradient-free (black-box) attacks, and the varying degrees of access to the model's information.",
      "page_start": 25,
      "page_end": 25
    },
    {
      "section_name": "Contrary To Intuitive Expectations, Black-Box Attacks Do Not Necessarily Underperform Due To",
      "text": "their limited access to model information. This assertion is substantiated by the data presented in Table  5 . For instance, the BoundaryAttack surpasses almost all white-box attacks in terms of performance for the EmoDB and EMOVO datasets. Nevertheless, this does not hold true for the Ravdess dataset, where its performance is significantly inferior to all gradient-based attacks.\n\nUpon evaluating the accuracy achieved by white-box attacks, it is evident that the results are comparable and consistently effective across different datasets. On the other hand, black-box techniques demonstrate wider variances. This indicates that gradient-based methods could potentially be language-independent, or at the very least, more so than population-based (PixelAttack) or decisionbased (BoundaryAttack) methods, and are capable of working efficiently with log Mel-spectrograms.\n\nFrom the average perturbation data presented in Table  6 , it is evident that there are substantial variances in the performance of BoundaryAttack across different datasets. A similar trend is also discernible in the case of FGSM, where the selection of the eps value for accuracy minimization is of considerable significance, and DeepFool, where the noise level is even more pronounced. In contrast, PixelAttack consistently produces AEs with a comparable, low level of noise, given its configuration to alter only a minimal number of pixels.\n\nOn analyzing the perturbation variations across different languages, it is noted that black-box attacks introduce a lesser degree of perturbation for EmoDB, while the perturbation is more pronounced for white-box attacks. A more detailed discussion on this observation will be presented in the subsequent section.\n\nAs indicated in Table  44 , the execution time for black-box attacks is typically longer, owing to the limited information available about the targeted model.\n\nTo summarize, despite the paucity of information about the victim model, black-box attacks can sometimes outperform their white-box counterparts by generating AEs with superior performance and lower disruption. However, our experimental results suggest that, on average, black-box attacks necessitate a longer execution time.",
      "page_start": 25,
      "page_end": 26
    },
    {
      "section_name": "Differences Between Languages",
      "text": "The trained models exhibit proficient performance across the three languages under consideration, as demonstrated in Table  3 . Ravdess yields the highest accuracy on the original data, registering at 0.912. This is closely followed by EmoDB and EMOVO, with respective accuracies of 0.909 and 0.872.\n\nIn section 3.1, we posited that the vulnerability to attacks across all three languages is relatively uniform. To elucidate this further, Figure  5  presents the accuracy across the three datasets for the most effective attack configurations, as detailed in Table  45 , applied to the entire test set.\n\nWhile the discrepancies between the achieved values are generally insubstantial, it is noteworthy that black-box attacks exhibit more significant variations. Nevertheless, these observations enable us to derive some intriguing insights.\n\nThe model trained on the EMOVO dataset, despite exhibiting the lowest accuracy on the original data, outperforms the other models in terms of resistance to AEs. Specifically, it achieves superior performance when subjected to 4 out of 7 attack methods, thus suggesting a diminished impact of the attacks on this model. Consequently, it can be deduced that the model trained on Italian samples exhibits a marginally higher resilience.\n\nIn contrast, models trained on the Ravdess dataset present a different scenario. These models, while attaining the highest accuracy on the original data, demonstrate a drop in performance when exposed to AEs, thereby making them the least resistant in 4 out of the 7 cases. This is particularly alarming given that the Ravdess model was trained with a larger dataset and over a greater number of epochs, factors that would typically contribute to increased robustness. Crucially, this underscores the fact that the resilience of a model to AEs is not solely contingent on the volume of the training data, but also its quality.\n\nAn analogous analysis can be conducted on the injected perturbations. The accuracy of the most effective attack configurations across the three datasets, as presented in Table  46 , is depicted in Figure  6  for the entire test set.\n\nThe EmoDB dataset is subjected to the most substantial perturbations in five out of all the attacks, implying that the majority of the implemented attacks have introduced the maximum level of noise. Notwithstanding the elevated perturbation, the attacks executed on EmoDB do not necessarily yield the lowest accuracies among the languages, as corroborated by Table  45 . This observation infers that the scrutinized techniques instigate an increased level of noise, which does not unequivocally translate into superior-performing AEs.\n\nAs previously alluded to, EmoDB exhibits a higher degree of perturbation across all white-box attacks, yet it manifests less interference under black-box attacks in contrast to the other two languages. This could infer that the efficacy of gradient-based methodologies in generating adversarial examples might be diminished when applied to the German language.\n\nIn terms of average perturbation, both EMOVO and Ravdess demonstrate analogous scores, with the latter predominantly impacted by the outcomes of black-box attacks. By synthesizing the data from tables 45 and 46 in Section 6.4, it can be inferred that the assaults on Ravdess are both effective (evidenced by low accuracy) and efficient (indicated by minimal noise introduction). This insinuates that the English language might be more susceptible to AEs, and reinforces the notion that a model's robustness does not necessarily equate to its resistance against such attacks. EMOVO, on the other hand, registers the lowest average perturbation and, as anticipated, the highest accuracy score.\n\nIn summary, the present analysis demonstrates that AEs based on log Mel-spectrogram, when fed to a CNN-LSTM, can significantly degrade the performance of a SER model, regardless of the language considered. Although the performance differences among languages are relatively small, the experiments provide valuable insights.\n\nOur findings indicate a heightened susceptibility of the English language to the discussed attacks, evidenced by its diminished accuracy notwithstanding its superior performance on the pristine data. Moreover, the observed mean perturbation is relatively insignificant, implying the generation of high-quality AEs.\n\nConversely, the Italian language demonstrates a greater degree of resilience to the same attacks, as inferred from its marginally superior accuracy and diminished perturbation.\n\nThe German language, however, presents a scenario that lies intermediate to the aforesaid languages. It exhibits an increased vulnerability specifically to gradient-based attacks, given that the perturbation introduced in these instances surpasses that noted for the other languages.\n\nDifferences between genders Table  3  illustrates that the trained models distinguish between male and female samples with negligible variations in the EmoDB original data. However, a pronounced discrepancy is discernible in the EMOVO dataset. Conversely, the models exhibit insignificant fluctuations in the classification of male and female samples in the Ravdess dataset.\n\nAlthough gender disparities are generally nuanced, Table  7  provides a more detailed insight by presenting the accuracy exclusively for male/female samples across each dataset. Additional details are elaborated in Section 6.4.\n\nA salient observation is that the utilization of PA yields significant disparities between the two genders. This phenomenon could be attributed to the specific configuration employed. However, it implies that for marginal deviations from the original samples, both German and English languages exhibit increased resilience towards female AEs, conversely, the Italian language demonstrates greater resistance against male AEs. Our preceding analyses, delineated in Section 3.1, revealed a distinct pattern of white-box attacks exerting a greater impact on male subjects in 9 out of 15 instances -additional details can be found in BoundaryAttack. This pattern was not exclusive to the aforementioned cases but was also observed in 4 out of 5 instances within the EMOVO dataset, despite the higher initial data accuracy of male subjects compared to their female counterparts. In the case of the Ravdess dataset, the AEs proved to be more effective in 3 instances concerning male subjects, who, interestingly, exhibited marginally lower initial data accuracy. However, in the EmoDB dataset, a contrasting trend was observed. Here, 3 out of 5 attacks were more potent on female subjects, who had initially achieved higher accuracy scores on the original data compared to male subjects. In summary, the data suggests that the gender with superior initial data accuracy is more susceptible to attacks in two out of the three datasets analyzed. Consequently, this leads to diminished accuracy on AEs relative to the other gender. This indicates that the model's resilience to gradient-based attacks on the best-performing gender cannot be reliably predicted solely based on the performance of the original data. inal data accuracy. In contrast, for EmoDB, 3 out of 5 attacks were found to be more influential on female subjects, who had higher accuracy scores on the original data than male subjects. Overall, these results suggest that, for two out of three datasets, the gender with higher original data accuracy is more vulnerable to attacks, resulting in lower accuracy on AEs than the other gender. Thus, relying on the original data's performance may not provide a reliable indicator of the model's susceptibility to gradient-based attacks on the best-performing gender\n\nThe outcomes of the black-box attack scenario present an equitable distribution, with males outperforming on the EmoDB, while females demonstrate superior attack efficacy on EMOVO. In the case of Ravdess, each gender triumphs in one attack.\n\nIn addition, the findings corroborate those delineated in Table  45 , which pertain to the most effective attacks on the datasets. This consistency in performance is observed even when the dataset is bifurcated into male and female categories. The minor variations in accuracy between the two genders do not significantly impact the overall efficacy of the attacks against the comprehensive AE dataset. Notably, Ravdess comprises the majority of subsets with diminished accuracies, two attacks excel on EmoDB, while EMOVO records a single instance of superior performance with FGSM.\n\nIn a similar vein, Figure  8  provides perturbations exclusively for male and female samples within each dataset. Further elaboration on this topic can be located in Section 6.4.\n\nAn initial cursory examination suggests an insubstantial distinction between genders. Yet, a more meticulous analysis of the white-box results uncovers that in 11 out of 15 scenarios, male AEs manifest a diminished level of perturbation compared to female AEs. In addition, in 7 out of these 11 instances, males also demonstrate a reduced accuracy rate than females, as illustrated in Table  47  in Section 6.4. From these observations, it can be deduced that male AEs typically yield superior quality in terms of both the degradation of model performance and the magnitude of induced perturbation.\n\nIn the context of black-box attacks, males registered a lower accuracy in 4 out of 6 situations, although their accuracy was only inferior to females in a single case.\n\nUpon evaluating individual attacks, it is discernible that male speech consistently manifests diminished perturbation across all instances for the FGSM, BIM, JSMA, and BA attacks. Conversely, for female speech, this phenomenon is solely observed with the C&W attack.\n\nIn summary, the findings suggest that there are negligible differences in performance between males and females in the majority of instances. Nevertheless, upon a more detailed examination, males seem to hold a minor edge in terms of efficacy (lower accuracy) and quality (reduced induced noise), particularly in the context of white-box attacks.",
      "page_start": 26,
      "page_end": 29
    },
    {
      "section_name": "Conclusion",
      "text": "The scientific community has been increasingly focusing on adversarial machine learning in recent years. Despite the surge in the development and application of new techniques for SER, the susceptibility of these methods to various forms of attacks has not been sufficiently explored. This paper aims to fill this research gap by evaluating the robustness of SER systems against AEs. Our study scrutinizes three languages-German, Italian, and English-sourced from distinct datasets (EmoDB, EMOVO, and Ravdess, respectively) to discern potential disparities among them. Furthermore, we have incorporated a gender-based perspective into our analysis, investigating the differential impacts of adversarial attacks on male and female speech. Importantly, the implementation of the presented experimental set-up is publicly available at https://github.com/LIMUNIMI/thesis_adversarial_ ml_audio ensuring full reproducibility of the achieved results.\n\nWe devised a pipeline to standardize the samples across the three languages and extract log Melspectrograms. Our methodology involved augmenting the datasets using pitch shifting and time stretching techniques, while maintaining a maximum sample duration of 3 seconds. Specifically, we generated eight distinct versions of the processed data, each differing in the data normalization method applied. The outcomes of our experiments were highly encouraging, demonstrating that the Convolutional Neural Network -Long Short-Term Memory (CNN-LSTM) models performed optimally and consistently when standardized log Mel-spectrograms were used, across all datasets.\n\nTo address the SER task, we established a uniform CNN-LSTM architecture across all datasets, thereby ensuring methodological consistency for attack comparisons. Through rigorous experimentation with diverse configurations of the neural network, optimal performance was achieved with a modestly sized CNN and 256 bidirectional LSTM units. Subsequent hyperparameter tuning further refined the performance for each dataset. This design strategy yielded high accuracy results on the EmoDB, EMOVO, and Ravdess test sets, with respective accuracies of 90.92%, 89.52%, and 91.76%. These findings underscore the efficacy of employing a CNN-LSTM network trained on log Mel-spectrograms for the SER task while being in line with the state of the art.\n\nUpon completion of the model development phase, we assessed the vulnerability of the resultant models to the previously mentioned attacks, under varying parameter configurations. Our empirical investigation revealed a substantial susceptibility of the SER task to AEs. Each examined attack method, including the relatively straightforward FGSM or PixelAttack, which was designed to alter a minimal number of features, successfully misled the network's predictions. In light of these findings, it is evident that the CNN-LSTM model did not exhibit resilience against any of the employed attack techniques. Consequently, we advocate for the exploration of more robust models or alternative training data to enhance system robustness.\n\nOur research findings indicate that amongst the multitude of attacks considered, Jacobian-based Saliency Map Attack (JSMA) surfaced as the most potent. The optimal configuration of its parameters led to a significant dip in accuracy rates, resulting in 1.31 2.23 and 1.73 for EmoDB, EMOVO, and Ravdess datasets, respectively. Furthermore, JSMA introduces only a minuscule degree of perturbation into the AEs. It is second only to PixelAttack, which is specifically tailored to alter a minimal number of pixels per spectrogram, in achieving the least perturbation.\n\nThe comparative analysis between white-box and black-box methodologies revealed that black-box techniques exhibit superior performance and minimal perturbation in two out of the three cases, specifically with BoundaryAttack. This is notwithstanding their limited access to information about the targeted model. Using BoundaryAttack, we recorded a significant drop in the accuracy of EmoDB, EMOVO, and Ravdess to 4.  54 7.6 and 20.38  respectively. This observation is alarming as it implies that attackers can potentially achieve remarkable results without any understanding of the model's internal operation, simply by scrutinizing its output.\n\nWhen we evaluated the impacts of the attacks across the three languages, no substantial difference in performance was observed. However, the results suggest that English appears to be the most susceptible, while Italian displays the highest resistance.\n\nThe comparative analysis between male and female samples revealed only negligible variations.\n\nA meticulous examination of the results, however, indicates a slight superiority of male samples, particularly in white-box attack scenarios, where they exhibited marginally lesser accuracy and perturbation.\n\nTo encapsulate, we have introduced a reliable and efficacious approach for the training of a deep neural network for SER, which has been corroborated on three distinct languages. Our exploratory study on the model's susceptibility to various adversarial attacks has unveiled substantial vulnerabilities to all examined techniques, even revealing critical deficiencies in the face of black-box attacks.\n\nThe empirical trials showed that the proposed method does not exhibit considerable disparities in attack performance across different languages or gender samples, but only minor variances.",
      "page_start": 29,
      "page_end": 31
    },
    {
      "section_name": "Distance Metric",
      "text": "Table  7 : Distance metric used by the tested attacks.",
      "page_start": 32,
      "page_end": 32
    },
    {
      "section_name": "Supplementary Material",
      "text": "In this section, all the supplementary materials have been placed to integrate the presented results.",
      "page_start": 47,
      "page_end": 47
    },
    {
      "section_name": "Dataset Processing",
      "text": "The summary statistics regarding the duration of the samples are presented in Table  8 , the number of examples for each label in the three datasets is resumed in detail in Table  9 , and Table  10   Regarding the hyperparameter process, the considered parameters and their values are:\n\n• Probability of the Dropout layer: [0, 0.3, 0.6];\n\n• Probability of internal dropout of the BLSTM layer: [0, 0.2, 0.4];\n\n• Initial learning rate of Adam optimizer: [0.01, 0.001, 0.0001];\n\n• Batch size:  [8, 16, 32, 64, 128] .\n\nThe best values found by the algorithm for the three datasets are reported in Table  20 .\n\nOther than the added dropout probability, the parameter configuration for EmoDB is curiously identical to the original, which probably explains why the results were better than the other datasets.\n\nThe intuition to add the Dropout layer proved to be correct, as all models produced better results with dropout probabilities other than 0. Surprisingly, Ravdess required greater regularization by preferring higher dropout probabilities, despite having the largest amount of data. it needs a slower training process to converge to the optimum, while Ravdess prefers larger batches.\n\nTo evaluate the effectiveness of the hyperparameter tuning process, we repeated the training using the same train/validation/test sets as in the previous paragraph but with the parameters found during tuning. The results are presented in Table  21 . The hyperparameter tuning process has thus enabled a significant increase in performance and made it possible to better leverage datasets with more data, such as EMOVO and Ravdess.\n\nThe early stopping callback is triggered in all cases before reaching 50 epochs, and the learning rate reduction callback is triggered when the validation loss increases to stabilize and improve the training procedure. Specifically, the training stops:\n\n• For EmoDB at epoch 33 (previously 20)\n\n• For EMOVO at epoch 35 (previously 20)\n\n• For Ravdess at epoch  40 (previously 18)  This phenomenon is particularly pronounced for Ravdess, where the number of epochs more than doubles compared to the base case. This proves that a correct setting of the parameters leads to better exploitation of the greater amount of data available, resulting in higher performance than the previous cases.",
      "page_start": 47,
      "page_end": 48
    },
    {
      "section_name": "Attack Deployment Results",
      "text": "This section presents the results obtained by the different attack techniques along with their configuration parameters 2 . The performance is tracked for males, females, and the entire test set and parable across various eps values, and there appears to be no correlation between these two values.\n\nOverall, the BIM attack produces very effective AEs, leading to a high number of misclassifications in the models. It works well across various languages and genders, with a low eps value resulting in minimal perturbations. The execution times are also reasonable, indicating that lower eps values and more epochs could be tried to produce equally good AEs with even lower degrees of perturbation.\n\nDeepFool The DeepFool attack is an iterative approach aimed at finding the minimum perturbation required to cause misclassification in the model. The most straightforward parameter to adjust is max iter, which defines the maximum number of iterations, and the values considered are  [1, 5, 20] .\n\nOther parameters include:\n\n• nb grads = 5, which denotes the number of class gradients (top nb grads w.r.t. prediction) to compute. This is set to 5 to consider all output classes rather than just the most probable.\n\n• epsilon = 1e -05 is the overshoot used to aid convergence and prevent stagnation. cluded that using 5 iterations is sufficient to find the minimum perturbation in all datasets, as the accuracy results are the same as when using 20 iterations. However, when using only one iteration, the best results are achieved with EmoDB, followed by EMOVO, and then Ravdess. Increasing the number of iterations reduces the difference between different languages, and the examples in Ravdess cause the most misclassification, followed by EmoDB and EMOVO. In the best cases, female samples are more vulnerable to attack in EmoDB and Ravdess, while for EMOVO, male samples have lower accuracy.\n\nThe mean perturbation is reported in Table  31 . Using more iterations in EMOVO and EmoDB leads to the discovery of AEs with less perturbation, which they are more efficient in causing misclassification compared to using only 1 iteration. However, in the case of Ravdess, even with only 1 iteration, the perturbation is lower than in other cases, although the differences are minimal, as in the other two cases. This suggests that AEs are already effective with just 1 iteration, and with more effort, it is possible to achieve more efficient threats with a similar degree of perturbation. clearly achieved with a single iteration. However, for cases with 5 and 20 iterations, both versions take approximately the same amount of time for all datasets. This suggests that they perform the same number of iterations, which is ≤ 5, because the accuracy is identical with both 5 and 20 iterations, as shown in Table  26 . Specifically, based on the time required for one iteration, we can estimate that:\n\n• EmoDB and EMOVO require around 2 iterations;\n\n• Ravdess requires around 3 iterations.\n\nIn general, we can conclude that DeepFool is capable of producing reasonably good AEs with minimal time requirements. Specifically, for the studied context, a small number of iterations is sufficient to reach the minimal perturbation. When iterations are sufficient, AEs can significantly degrade the model's performance, but at the cost of introducing a relatively high degree of perturbation. The algorithm works well for all languages and produces essentially the same results for both male and female voices.\n\nJSMA is configured with different theta values, which define the amount of perturbation introduced to each modified feature per step, in the range of [-1, -0.5, 0.5, 1]. Additionally, gamma is bation for EmoDB and Ravdess, regardless of the theta value used. The situation for EMOVO is more complex, with men experiencing less perturbation for positive values of theta and more perturbation for negative values. The difference in perturbation between males and females is generally less pronounced for positive theta values, and more pronounced for negative values.\n\nOverall, we are very satisfied with the level of mean perturbation introduced, especially considering the low levels of accuracy obtained. While introducing only a slight perturbation, this attack results in a considerably high degree of degradation in the model's performance.\n\nIn Table  35 , we provide information on the running time taken by the JSMA implementation used in this analysis. Compared to the attacks presented earlier, this attack takes significantly more time Dataset theta -1 theta -0.5 theta 0. to execute, but it yields better outcomes. Interestingly, when comparing the time taken for different thetas, we observe that the best results are achieved when theta = +1 (the difference between 0.5 and 1 for EMOVO is very small). It is worth noting that this value is also associated with the highest misclassification rate. Moreover, we find it intriguing that the worst results, in terms of accuracy and perturbation, are linked to the values that take longer to execute the algorithm.\n\nTo summarize, this method is highly effective in generating AEs with minimal perturbation but significantly high misclassification rates; so we can say that using mechanisms such as saliency maps to identify features that strongly impact model results is suitable for SER tasks. However, this approach requires a greater computational effort in terms of generating examples. Based on our experiments, it is crucial to select an appropriate theta value to achieve optimal results in terms of misclassification, perturbation, and Execution time.\n\nC&W attack can be conducted with different distance metrics. However, due to a possible bug in the implementation of the L 0 version, I only tested the algorithm using L 2 and L ∞ metrics. To ensure a fair comparison of the results, I set the parameters for the two versions to be as similar as possible, taking into account the default values provided in the ART documentation.\n\nFor the L 2 attack, the following parameters were used:\n\n• conf idence = 0: this parameter controls the confidence of the AEs. Higher values produce examples that are farther away from the original input but are classified with higher confidence as the target class.\n\n• learning rate = 0.01: this is the initial learning rate for the attack algorithm. Smaller values produce better results but are slower to converge.",
      "page_start": 53,
      "page_end": 61
    }
  ],
  "figures": [
    {
      "caption": "Figure 2: Regarding the classes available in the datasets, since we are not interested in finding the optimal",
      "page": 9
    },
    {
      "caption": "Figure 3: summarizes",
      "page": 13
    },
    {
      "caption": "Figure 5: The analysis of the results reveals a substantial impact of all attack variants, including the",
      "page": 20
    },
    {
      "caption": "Figure 4: derive from the most effective precision outcomes garnered",
      "page": 23
    },
    {
      "caption": "Figure 5: presents the accuracy across the three datasets for the",
      "page": 26
    },
    {
      "caption": "Figure 6: for the entire test set.",
      "page": 27
    },
    {
      "caption": "Figure 8: provides perturbations exclusively for male and female samples within",
      "page": 29
    },
    {
      "caption": "Figure 1: Flowchart of the proposed methodology to conduct the experiment.",
      "page": 36
    },
    {
      "caption": "Figure 2: Example of split and repeat process on log Mel-spectrograms.",
      "page": 37
    },
    {
      "caption": "Figure 3: Architecture of the optimized CNN-LSTM model.",
      "page": 38
    },
    {
      "caption": "Figure 4: Time (s) required to generate the AEs for the various attacks and datasets for the best-",
      "page": 41
    },
    {
      "caption": "Figure 5: Accuracy obtained by the most effective configuration of each attack. Additional data can",
      "page": 42
    },
    {
      "caption": "Figure 6: Perturbations injected by the most effective configuration of each attack. Additional data",
      "page": 42
    },
    {
      "caption": "Figure 7: Accuracy obtained by the most effective configuration of each attack for male/female data",
      "page": 43
    },
    {
      "caption": "Figure 8: Perturbation injected by the most effective configuration of each attack for male/female",
      "page": 43
    },
    {
      "caption": "Figure 9: Example of male samples from EmoDB. (a) standardized original sample and (b) its",
      "page": 44
    },
    {
      "caption": "Figure 10: Example of female samples from EmoDB. (a) standardized original sample and (b) its",
      "page": 44
    },
    {
      "caption": "Figure 11: Example of male samples from EMOVO. (a) standardized original sample and (b) its",
      "page": 45
    },
    {
      "caption": "Figure 12: Example of female samples from EMOVO. (a) standardized original sample and (b)",
      "page": 45
    },
    {
      "caption": "Figure 13: Example of male samples from Ravdess. (a) standardized original sample and (b) its",
      "page": 46
    },
    {
      "caption": "Figure 14: Example of female samples from Ravdess. (a) standardized original sample and (b) its",
      "page": 46
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Original": "NormSum",
          "0.68 ± 0.04": "0.46 ± 0.16",
          "0.35 ± 0.13": "0.29 ± 0.10",
          "0.45 ± 0.1": "0.21 ± 0",
          "0.50 ± 0.09": "0.32 ± 0.09"
        },
        {
          "Original": "NormMaxGlobal",
          "0.68 ± 0.04": "0.79 ± 0",
          "0.35 ± 0.13": "0.36 ± 0.20",
          "0.45 ± 0.1": "0.21 ± 0",
          "0.50 ± 0.09": "0.45 ± 0.07"
        },
        {
          "Original": "NormMaxLocal",
          "0.68 ± 0.04": "0.63 ± 0.07",
          "0.35 ± 0.13": "0.48 ± 0.02",
          "0.45 ± 0.1": "0.60 ± 0.03",
          "0.50 ± 0.09": "0.57 ± 0.04"
        },
        {
          "Original": "Original standardized",
          "0.68 ± 0.04": "0.71 ± 0.02",
          "0.35 ± 0.13": "0.60 ± 0.07",
          "0.45 ± 0.1": "0.62 ± 0.01",
          "0.50 ± 0.09": "0.64 ± 0.04"
        },
        {
          "Original": "NormSum standardized",
          "0.68 ± 0.04": "0.71 ± 0.01",
          "0.35 ± 0.13": "0.35 ± 0.19",
          "0.45 ± 0.1": "0.21 ± 0",
          "0.50 ± 0.09": "0.42 ± 0.07"
        },
        {
          "Original": "NormMaxGlobal standardized",
          "0.68 ± 0.04": "0.68 ± 0.02",
          "0.35 ± 0.13": "0.24 ± 0.03",
          "0.45 ± 0.1": "0.21 ± 0",
          "0.50 ± 0.09": "0.38 ± 0.02"
        },
        {
          "Original": "NormMaxLocal standardized",
          "0.68 ± 0.04": "0.68 ± 0.02",
          "0.35 ± 0.13": "0.42 ± 0.15",
          "0.45 ± 0.1": "0.58 ± 0.01",
          "0.50 ± 0.09": "0.56 ± 0.06"
        }
      ],
      "page": 36
    },
    {
      "caption": "Table 2: Mean accuracy and standard deviation over the three splits for each dataset and model",
      "data": [
        {
          "M0": "M1",
          "0.71 ± 0.05": "0.83 ± 0.00",
          "0.59 ± 0.03": "0.70 ± 0.03",
          "0.53 ± 0.11": "0.76 ± 0.01",
          "0.61 ± 0.06": "0.76 ± 0.01"
        },
        {
          "M0": "M2",
          "0.71 ± 0.05": "0.78 ± 0.01",
          "0.59 ± 0.03": "0.68 ± 0.03",
          "0.53 ± 0.11": "0.70 ± 0.02",
          "0.61 ± 0.06": "0.72 ± 0.02"
        },
        {
          "M0": "M3",
          "0.71 ± 0.05": "0.68 ± 0.05",
          "0.59 ± 0.03": "0.39 ± 0.14",
          "0.53 ± 0.11": "0.39 ± 0.13",
          "0.61 ± 0.06": "0.49 ± 0.10"
        },
        {
          "M0": "M4",
          "0.71 ± 0.05": "0.76 ± 0.01",
          "0.59 ± 0.03": "0.66 ± 0.03",
          "0.53 ± 0.11": "0.45 ± 0.15",
          "0.61 ± 0.06": "0.62 ± 0.06"
        },
        {
          "M0": "M5",
          "0.71 ± 0.05": "0.58 ± 0.05",
          "0.59 ± 0.03": "0.25 ± 0.03",
          "0.53 ± 0.11": "0.26 ± 0.04",
          "0.61 ± 0.06": "0.36 ± 0.04"
        },
        {
          "M0": "M6",
          "0.71 ± 0.05": "0.32 ± 0.06",
          "0.59 ± 0.03": "0.24 ± 0.03",
          "0.53 ± 0.11": "0.23 ± 0.03",
          "0.61 ± 0.06": "0.26 ± 0.04"
        }
      ],
      "page": 37
    },
    {
      "caption": "Table 3: Accuracy on the original test set of the models that will be attacked.",
      "data": [
        {
          "FGSM": "BIM",
          "eps = 1.25": "eps = 0.25",
          "eps = 0.5": "eps = 0.25",
          "eps = 0.25": "eps = 0.25"
        },
        {
          "FGSM": "DeepFool",
          "eps = 1.25": "iter = 5",
          "eps = 0.5": "iter = 5",
          "eps = 0.25": "iter = 5"
        },
        {
          "FGSM": "JSMA",
          "eps = 1.25": "theta = +1",
          "eps = 0.5": "theta = +1",
          "eps = 0.25": "theta = +1"
        },
        {
          "FGSM": "C&W",
          "eps = 1.25": "metric = L∞",
          "eps = 0.5": "metric = L∞",
          "eps = 0.25": "metric = L∞"
        }
      ],
      "page": 38
    },
    {
      "caption": "Table 5: Accuracy obtained w.r.t all considered datasets with the original test data and with the",
      "data": [
        {
          "FGSM": "BIM",
          "EmoDB": "EmoDB",
          "0.109": "0.067",
          "0.121": "0.065",
          "0.089": "0.070"
        },
        {
          "FGSM": "DeepFool",
          "EmoDB": "EmoDB",
          "0.109": "0.061",
          "0.121": "0.061",
          "0.089": "0.070"
        },
        {
          "FGSM": "JSMA",
          "EmoDB": "EmoDB",
          "0.109": "0.013",
          "0.121": "0.010",
          "0.089": "0.019"
        },
        {
          "FGSM": "C&W",
          "EmoDB": "EmoDB",
          "0.109": "0.069",
          "0.121": "0.076",
          "0.089": "0.065"
        },
        {
          "FGSM": "PixelAttack",
          "EmoDB": "EmoDB",
          "0.109": "0.547",
          "0.121": "0.603",
          "0.089": "0.454"
        },
        {
          "FGSM": "BoundaryAttack",
          "EmoDB": "EmoDB",
          "0.109": "0.045",
          "0.121": "0.057",
          "0.089": "0.038"
        },
        {
          "FGSM": "Original",
          "EmoDB": "EmoDB",
          "0.109": "0.909",
          "0.121": "0.918",
          "0.089": "0.895"
        }
      ],
      "page": 39
    },
    {
      "caption": "Table 5: Accuracy obtained w.r.t all considered datasets with the original test data and with the",
      "data": [
        {
          "FGSM": "BIM",
          "EMOVO": "EMOVO",
          "0.070": "0.076",
          "0.078": "0.088",
          "0.061": "0.064"
        },
        {
          "FGSM": "DeepFool",
          "EMOVO": "EMOVO",
          "0.070": "0.086",
          "0.078": "0.104",
          "0.061": "0.068"
        },
        {
          "FGSM": "JSMA",
          "EMOVO": "EMOVO",
          "0.070": "0.022",
          "0.078": "0.024",
          "0.061": "0.020"
        },
        {
          "FGSM": "C&W",
          "EMOVO": "EMOVO",
          "0.070": "0.085",
          "0.078": "0.068",
          "0.061": "0.102"
        },
        {
          "FGSM": "PixelAttack",
          "EMOVO": "EMOVO",
          "0.070": "0.364",
          "0.078": "0.339",
          "0.061": "0.389"
        },
        {
          "FGSM": "BoundaryAttack",
          "EMOVO": "EMOVO",
          "0.070": "0.076",
          "0.078": "0.070",
          "0.061": "0.082"
        },
        {
          "FGSM": "Original",
          "EMOVO": "EMOVO",
          "0.070": "0.872",
          "0.078": "0.852",
          "0.061": "0.893"
        }
      ],
      "page": 39
    },
    {
      "caption": "Table 5: Accuracy obtained w.r.t all considered datasets with the original test data and with the",
      "data": [
        {
          "FGSM": "BIM",
          "Ravdess": "Ravdess",
          "0.146": "0.059",
          "0.164": "0.060",
          "0.127": "0.057"
        },
        {
          "FGSM": "DeepFool",
          "Ravdess": "Ravdess",
          "0.146": "0.052",
          "0.164": "0.049",
          "0.127": "0.056"
        },
        {
          "FGSM": "JSMA",
          "Ravdess": "Ravdess",
          "0.146": "0.017",
          "0.164": "0.019",
          "0.127": "0.016"
        },
        {
          "FGSM": "C&W",
          "Ravdess": "Ravdess",
          "0.146": "0.060",
          "0.164": "0.058",
          "0.127": "0.062"
        },
        {
          "FGSM": "PixelAttack",
          "Ravdess": "Ravdess",
          "0.146": "0.322",
          "0.164": "0.419",
          "0.127": "0.225"
        },
        {
          "FGSM": "BoundaryAttack",
          "Ravdess": "Ravdess",
          "0.146": "0.204",
          "0.164": "0.202",
          "0.127": "0.205"
        },
        {
          "FGSM": "Original",
          "Ravdess": "Ravdess",
          "0.146": "0.911",
          "0.164": "0.911",
          "0.127": "0.911"
        }
      ],
      "page": 39
    },
    {
      "caption": "Table 6: Mean perturbation injected by the best-performing configuration for each attack w.r.t all",
      "data": [
        {
          "FGSM": "BIM",
          "EmoDB": "EmoDB",
          "1.070": "0.159",
          "1.068": "0.158"
        },
        {
          "FGSM": "DeepFool",
          "EmoDB": "EmoDB",
          "1.070": "1.917",
          "1.068": "1.858"
        },
        {
          "FGSM": "JSMA",
          "EmoDB": "EmoDB",
          "1.070": "0.004",
          "1.068": "0.003"
        },
        {
          "FGSM": "C&W",
          "EmoDB": "EmoDB",
          "1.070": "0.050",
          "1.068": "0.054"
        },
        {
          "FGSM": "PixelAttack",
          "EmoDB": "EmoDB",
          "1.070": "4.92e-4",
          "1.068": "6-6e-4"
        },
        {
          "FGSM": "BoundaryAttack",
          "EmoDB": "EmoDB",
          "1.070": "0.775",
          "1.068": "0.746"
        }
      ],
      "page": 40
    },
    {
      "caption": "Table 6: Mean perturbation injected by the best-performing configuration for each attack w.r.t all",
      "data": [
        {
          "FGSM": "BIM",
          "EMOVO": "EMOVO",
          "0.436": "0.154",
          "0.427": "0.153"
        },
        {
          "FGSM": "DeepFool",
          "EMOVO": "EMOVO",
          "0.436": "1.020",
          "0.427": "1.192"
        },
        {
          "FGSM": "JSMA",
          "EMOVO": "EMOVO",
          "0.436": "0.002",
          "0.427": "0.002"
        },
        {
          "FGSM": "C&W",
          "EMOVO": "EMOVO",
          "0.436": "0.035",
          "0.427": "0.035"
        },
        {
          "FGSM": "PixelAttack",
          "EMOVO": "EMOVO",
          "0.436": "8-03e-4",
          "0.427": "7.48e-4"
        },
        {
          "FGSM": "FGSM",
          "EMOVO": "Ravdess",
          "0.436": "0.204",
          "0.427": "0.192"
        },
        {
          "FGSM": "BIM",
          "EMOVO": "Ravdess",
          "0.436": "0.149",
          "0.427": "0.146"
        },
        {
          "FGSM": "DeepFool",
          "EMOVO": "Ravdess",
          "0.436": "1.411",
          "0.427": "1.243"
        },
        {
          "FGSM": "JSMA",
          "EMOVO": "Ravdess",
          "0.436": "0.003",
          "0.427": "0.002"
        },
        {
          "FGSM": "C&W",
          "EMOVO": "Ravdess",
          "0.436": "0.025",
          "0.427": "0.029"
        }
      ],
      "page": 40
    },
    {
      "caption": "Table 7: Distance metric used by the tested attacks.",
      "data": [
        {
          "FGSM": "BIM",
          "✓": "✓"
        },
        {
          "FGSM": "DeepFool",
          "✓": ""
        },
        {
          "FGSM": "JSMA",
          "✓": ""
        },
        {
          "FGSM": "C&W",
          "✓": "✓"
        }
      ],
      "page": 41
    },
    {
      "caption": "Table 8: Descriptive statistics of duration in seconds about the length of the 3 datasets.",
      "data": [
        {
          "Mean": "Std",
          "2.78": "1.028",
          "3.121": "1.357",
          "3.701": "0.337"
        },
        {
          "Mean": "Min",
          "2.78": "1.226",
          "3.121": "1.291",
          "3.701": "2.936"
        },
        {
          "Mean": "25%",
          "2.78": "2.027",
          "3.121": "2.133",
          "3.701": "3.47"
        },
        {
          "Mean": "50%",
          "2.78": "2.59",
          "3.121": "2.773",
          "3.701": "3.67"
        },
        {
          "Mean": "75%",
          "2.78": "3.308",
          "3.121": "3.84",
          "3.701": "3.871"
        },
        {
          "Mean": "Max",
          "2.78": "8.978",
          "3.121": "13.995",
          "3.701": "5.272"
        }
      ],
      "page": 47
    },
    {
      "caption": "Table 8: Descriptive statistics of duration in seconds about the length of the 3 datasets.",
      "data": [
        {
          "angry": "bored",
          "127": "81",
          "84": "NaN",
          "192": "NaN"
        },
        {
          "angry": "neutral",
          "127": "79",
          "84": "84",
          "192": "96"
        },
        {
          "angry": "happy",
          "127": "71",
          "84": "84",
          "192": "192"
        },
        {
          "angry": "fear",
          "127": "69",
          "84": "84",
          "192": "192"
        },
        {
          "angry": "sad",
          "127": "62",
          "84": "84",
          "192": "192"
        },
        {
          "angry": "disgust",
          "127": "46",
          "84": "84",
          "192": "192"
        },
        {
          "angry": "surprised",
          "127": "NaN",
          "84": "84",
          "192": "192"
        },
        {
          "angry": "calm",
          "127": "NaN",
          "84": "NaN",
          "192": "192"
        }
      ],
      "page": 47
    },
    {
      "caption": "Table 11: Architecture of the CNN-LSTM base model M0",
      "data": [
        {
          "Input": "Reshape",
          "(261, 128, 1)": "(9, 29, 128, 1)"
        },
        {
          "Input": "Conv",
          "(261, 128, 1)": "(9, 25, 124, 16)"
        },
        {
          "Input": "BatchNorm",
          "(261, 128, 1)": ""
        },
        {
          "Input": "Pool",
          "(261, 128, 1)": "(9, 11, 61, 16)"
        },
        {
          "Input": "Conv",
          "(261, 128, 1)": "(9, 9, 59, 32)"
        },
        {
          "Input": "Pool",
          "(261, 128, 1)": "(9, 4, 29, 32)"
        },
        {
          "Input": "Conv",
          "(261, 128, 1)": "(9, 2, 27, 64)"
        },
        {
          "Input": "Pool",
          "(261, 128, 1)": "(9, 1, 26, 64)"
        },
        {
          "Input": "Flatten",
          "(261, 128, 1)": "(9, 1664)"
        },
        {
          "Input": "LSTM",
          "(261, 128, 1)": "(3)"
        },
        {
          "Input": "Dense",
          "(261, 128, 1)": "(5)"
        }
      ],
      "page": 48
    },
    {
      "caption": "Table 12: Architecture of the CNN-LSTM model M1",
      "data": [
        {
          "Input": "Reshape",
          "(261, 128, 1)": "(9, 29, 128, 1)"
        },
        {
          "Input": "Conv",
          "(261, 128, 1)": "(9, 25, 124, 16)"
        },
        {
          "Input": "BatchNorm",
          "(261, 128, 1)": ""
        },
        {
          "Input": "Pool",
          "(261, 128, 1)": "(9, 11, 61, 16)"
        },
        {
          "Input": "Conv",
          "(261, 128, 1)": "(9, 9, 59, 32)"
        },
        {
          "Input": "Pool",
          "(261, 128, 1)": "(9, 4, 29, 32)"
        },
        {
          "Input": "Conv",
          "(261, 128, 1)": "(9, 2, 27, 64)"
        },
        {
          "Input": "Pool",
          "(261, 128, 1)": "(9, 1, 26, 64)"
        },
        {
          "Input": "Flatten",
          "(261, 128, 1)": "(9, 1664)"
        },
        {
          "Input": "BLSTM",
          "(261, 128, 1)": "(6)"
        },
        {
          "Input": "Dense",
          "(261, 128, 1)": "(5)"
        }
      ],
      "page": 49
    },
    {
      "caption": "Table 12: Architecture of the CNN-LSTM model M1",
      "data": [
        {
          "Input": "Reshape",
          "(261, 128, 1)": "(9, 29, 128, 1)"
        },
        {
          "Input": "Conv",
          "(261, 128, 1)": "(9, 25, 124, 16)"
        },
        {
          "Input": "BatchNorm",
          "(261, 128, 1)": ""
        },
        {
          "Input": "Pool",
          "(261, 128, 1)": "(9, 11, 61, 16)"
        },
        {
          "Input": "Conv",
          "(261, 128, 1)": "(9, 9, 59, 32)"
        },
        {
          "Input": "Pool",
          "(261, 128, 1)": "(9, 4, 29, 32)"
        },
        {
          "Input": "Conv",
          "(261, 128, 1)": "(9, 2, 27, 64)"
        },
        {
          "Input": "Pool",
          "(261, 128, 1)": "(9, 1, 26, 64)"
        },
        {
          "Input": "Flatten",
          "(261, 128, 1)": "(9, 1664)"
        },
        {
          "Input": "LSTM",
          "(261, 128, 1)": "(6)"
        },
        {
          "Input": "Dense",
          "(261, 128, 1)": "(5)"
        }
      ],
      "page": 49
    },
    {
      "caption": "Table 14: Architecture of the CNN-LSTM model M3",
      "data": [
        {
          "Input": "Reshape",
          "(261, 128, 1)": "(9, 29, 128, 1)"
        },
        {
          "Input": "Conv",
          "(261, 128, 1)": "(9, 27, 126, 16)"
        },
        {
          "Input": "BatchNorm",
          "(261, 128, 1)": ""
        },
        {
          "Input": "Pool",
          "(261, 128, 1)": "(9, 13, 63, 16)"
        },
        {
          "Input": "Conv",
          "(261, 128, 1)": "(9, 11, 61, 32)"
        },
        {
          "Input": "Pool",
          "(261, 128, 1)": "(9, 5, 30, 32)"
        },
        {
          "Input": "Conv",
          "(261, 128, 1)": "(9, 3, 28, 64)"
        },
        {
          "Input": "Pool",
          "(261, 128, 1)": "(9, 2, 27, 64)"
        },
        {
          "Input": "Flatten",
          "(261, 128, 1)": "(9, 3456)"
        },
        {
          "Input": "LSTM",
          "(261, 128, 1)": "(3)"
        },
        {
          "Input": "Dense",
          "(261, 128, 1)": "(5)"
        }
      ],
      "page": 50
    },
    {
      "caption": "Table 14: Architecture of the CNN-LSTM model M3",
      "data": [
        {
          "Input": "Reshape",
          "(261, 128, 1)": "(9, 29, 128, 1)"
        },
        {
          "Input": "Conv",
          "(261, 128, 1)": "(9, 25, 124, 16)"
        },
        {
          "Input": "BatchNorm",
          "(261, 128, 1)": ""
        },
        {
          "Input": "Pool",
          "(261, 128, 1)": "(9, 12, 62, 16)"
        },
        {
          "Input": "Conv",
          "(261, 128, 1)": "(9, 10, 60, 32)"
        },
        {
          "Input": "Pool",
          "(261, 128, 1)": "(9, 5, 30, 32)"
        },
        {
          "Input": "Conv",
          "(261, 128, 1)": "(9, 3, 28, 64)"
        },
        {
          "Input": "Pool",
          "(261, 128, 1)": "(9, 1, 2, 27, 64)"
        },
        {
          "Input": "Flatten",
          "(261, 128, 1)": "(9, 3456)"
        },
        {
          "Input": "LSTM",
          "(261, 128, 1)": "(6)"
        },
        {
          "Input": "Dense",
          "(261, 128, 1)": "(5)"
        }
      ],
      "page": 50
    },
    {
      "caption": "Table 16: Architecture of the CNN-LSTM model M5",
      "data": [
        {
          "Input": "Reshape",
          "(261, 128, 1)": "(9, 29, 128, 1)"
        },
        {
          "Input": "Conv",
          "(261, 128, 1)": "(9, 25, 124, 32)"
        },
        {
          "Input": "BatchNorm",
          "(261, 128, 1)": ""
        },
        {
          "Input": "Pool",
          "(261, 128, 1)": "(9, 11, 61, 32)"
        },
        {
          "Input": "Conv",
          "(261, 128, 1)": "(9, 9, 59, 64)"
        },
        {
          "Input": "Pool",
          "(261, 128, 1)": "(9, 4, 29, 64)"
        },
        {
          "Input": "Conv",
          "(261, 128, 1)": "(9, 2, 27, 128)"
        },
        {
          "Input": "Pool",
          "(261, 128, 1)": "(9, 1, 26, 128)"
        },
        {
          "Input": "Flatten",
          "(261, 128, 1)": "(9, 3328)"
        },
        {
          "Input": "LSTM",
          "(261, 128, 1)": "(3)"
        },
        {
          "Input": "Dense",
          "(261, 128, 1)": "(5)"
        }
      ],
      "page": 51
    },
    {
      "caption": "Table 16: Architecture of the CNN-LSTM model M5",
      "data": [
        {
          "Input": "Reshape",
          "(261, 128, 1)": "(9, 29, 128, 1)"
        },
        {
          "Input": "Conv",
          "(261, 128, 1)": "(9, 25, 124, 16)"
        },
        {
          "Input": "BatchNorm",
          "(261, 128, 1)": ""
        },
        {
          "Input": "Pool",
          "(261, 128, 1)": "(9, 12, 62, 16)"
        },
        {
          "Input": "Conv",
          "(261, 128, 1)": "(9, 8, 58, 32)"
        },
        {
          "Input": "Pool",
          "(261, 128, 1)": "(9, 7, 57, 32)"
        },
        {
          "Input": "Conv",
          "(261, 128, 1)": "(9, 5, 55, 64)"
        },
        {
          "Input": "Pool",
          "(261, 128, 1)": "(9, 4, 54, 64)"
        },
        {
          "Input": "Conv",
          "(261, 128, 1)": "(9, 2, 52, 128)"
        },
        {
          "Input": "Pool",
          "(261, 128, 1)": "(9, 1, 51, 128)"
        },
        {
          "Input": "Flatten",
          "(261, 128, 1)": "(9, 6528)"
        },
        {
          "Input": "LSTM",
          "(261, 128, 1)": "(3)"
        },
        {
          "Input": "Dense",
          "(261, 128, 1)": "(5)"
        }
      ],
      "page": 51
    },
    {
      "caption": "Table 18: Validation loss with respect to the number of LSTM bidirectional units for each dataset",
      "data": [
        {
          "4": "8",
          "0.85": "0.63",
          "1.29": "1.02",
          "1.27": "0.92"
        },
        {
          "4": "16",
          "0.85": "0.45",
          "1.29": "0.78",
          "1.27": "0.79"
        },
        {
          "4": "32",
          "0.85": "0.4",
          "1.29": "0.71",
          "1.27": "0.6"
        },
        {
          "4": "64",
          "0.85": "0.33",
          "1.29": "0.56",
          "1.27": "0.44"
        },
        {
          "4": "128",
          "0.85": "0.3",
          "1.29": "0.52",
          "1.27": "0.38"
        },
        {
          "4": "256",
          "0.85": "0.29",
          "1.29": "0.48",
          "1.27": "0.33"
        },
        {
          "4": "512",
          "0.85": "0.28",
          "1.29": "0.48",
          "1.27": "0.33"
        },
        {
          "4": "1024",
          "0.85": "0.32",
          "1.29": "0.48",
          "1.27": "0.3"
        }
      ],
      "page": 52
    },
    {
      "caption": "Table 31: Perturbation of DeepFool attack",
      "data": [
        {
          "EmoDB": "EMOVO",
          "4.991": "5.629",
          "7.520": "8.101",
          "7.356": "8.345"
        },
        {
          "EmoDB": "Ravdess",
          "4.991": "10.657",
          "7.520": "22.576",
          "7.356": "22.584"
        },
        {
          "EmoDB": "EmoDB",
          "4.991": "4.991",
          "7.520": "7.520",
          "7.356": "7.356"
        },
        {
          "EmoDB": "EMOVO",
          "4.991": "5.629",
          "7.520": "8.101",
          "7.356": "8.345"
        },
        {
          "EmoDB": "Ravdess",
          "4.991": "10.657",
          "7.520": "22.576",
          "7.356": "22.584"
        },
        {
          "EmoDB": "EmoDB",
          "4.991": "4.991",
          "7.520": "7.520",
          "7.356": "7.356"
        },
        {
          "EmoDB": "EMOVO",
          "4.991": "5.629",
          "7.520": "8.101",
          "7.356": "8.345"
        },
        {
          "EmoDB": "Ravdess",
          "4.991": "10.657",
          "7.520": "22.576",
          "7.356": "22.584"
        }
      ],
      "page": 58
    },
    {
      "caption": "Table 43: Time in seconds of BoundaryAttack attack",
      "data": [
        {
          "FGSM": "BIM",
          "1.272": "24.962",
          "1.536": "28.622",
          "3.150": "51.782"
        },
        {
          "FGSM": "DeepFool",
          "1.272": "7.520",
          "1.536": "8.101",
          "3.150": "22.576"
        },
        {
          "FGSM": "JSMA",
          "1.272": "225.941",
          "1.536": "136.015",
          "3.150": "374.433"
        },
        {
          "FGSM": "C&W",
          "1.272": "4230.925",
          "1.536": "5158.850",
          "3.150": "9882.848"
        }
      ],
      "page": 66
    },
    {
      "caption": "Table 45: Accuracy obtained by the most effective configuration of each attack. In brackets is",
      "data": [
        {
          "FGSM": "BIM",
          "0.109 (2)": "0.067 (2)",
          "0.070 (1)": "0.076 (3)",
          "0.146 (3)": "0.059 (1)"
        },
        {
          "FGSM": "DeepFool",
          "0.109 (2)": "0.061 (2)",
          "0.070 (1)": "0.086 (3)",
          "0.146 (3)": "0.052 (1)"
        },
        {
          "FGSM": "JSMA",
          "0.109 (2)": "0.013 (1)",
          "0.070 (1)": "0.022 (3)",
          "0.146 (3)": "0.017 (2)"
        },
        {
          "FGSM": "C&W",
          "0.109 (2)": "0.069 (2)",
          "0.070 (1)": "0.085 (3)",
          "0.146 (3)": "0.060 (1)"
        }
      ],
      "page": 67
    },
    {
      "caption": "Table 45: Accuracy obtained by the most effective configuration of each attack. In brackets is",
      "data": [
        {
          "FGSM": "BIM",
          "1.070 (3)": "0.159 (3)",
          "0.436 (2)": "0.153 (2)",
          "0.198 (1)": "0.147 (1)"
        },
        {
          "FGSM": "DeepFool",
          "1.070 (3)": "1.894 (3)",
          "0.436 (2)": "1.105 (1)",
          "0.198 (1)": "1.327 (2)"
        },
        {
          "FGSM": "JSMA",
          "1.070 (3)": "0.003 (3)",
          "0.436 (2)": "0.002 (1)",
          "0.198 (1)": "0.002 (2)"
        },
        {
          "FGSM": "C&W",
          "1.070 (3)": "0.053 (3)",
          "0.436 (2)": "0.035 (2)",
          "0.198 (1)": "0.027 (1)"
        }
      ],
      "page": 67
    },
    {
      "caption": "Table 45: Accuracy obtained by the most effective configuration of each attack. In brackets is",
      "data": [
        {
          "FGSM": "BIM",
          "0.121": "0.065",
          "0.089": "0.070",
          "0.078": "0.088",
          "0.061": "0.064",
          "0.164": "0.060",
          "0.127": "0.057"
        },
        {
          "FGSM": "DF",
          "0.121": "0.061",
          "0.089": "0.070",
          "0.078": "0.104",
          "0.061": "0.068",
          "0.164": "0.049",
          "0.127": "0.056"
        },
        {
          "FGSM": "JSMA",
          "0.121": "0.010",
          "0.089": "0.019",
          "0.078": "0.024",
          "0.061": "0.020",
          "0.164": "0.019",
          "0.127": "0.016"
        },
        {
          "FGSM": "C&W",
          "0.121": "0.076",
          "0.089": "0.065",
          "0.078": "0.068",
          "0.061": "0.102",
          "0.164": "0.058",
          "0.127": "0.063"
        }
      ],
      "page": 67
    },
    {
      "caption": "Table 48: Comparison of the perturbation introduced between male/female samples of the datasets",
      "data": [
        {
          "FGSM": "BIM",
          "1.070": "0.159",
          "1.068": "0.158",
          "0.436": "0.154",
          "0.427": "0.153",
          "0.204": "0.149",
          "0.192": "0.146"
        },
        {
          "FGSM": "DF",
          "1.070": "1.917",
          "1.068": "1.858",
          "0.436": "1.020",
          "0.427": "1.192",
          "0.204": "1.411",
          "0.192": "1.243"
        },
        {
          "FGSM": "JSMA",
          "1.070": "0.00372",
          "1.068": "0.00302",
          "0.436": "0.00220",
          "0.427": "0.00197",
          "0.204": "0.00294",
          "0.192": "0.00190"
        },
        {
          "FGSM": "C&W",
          "1.070": "0.050",
          "1.068": "0.054",
          "0.436": "0.035",
          "0.427": "0.035",
          "0.204": "0.025",
          "0.192": "0.029"
        }
      ],
      "page": 68
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Italian Speech Emotion Recognition",
      "authors": [
        "I Mantegazza",
        "S Ntalampiras"
      ],
      "venue": "2023 24th International Conference on Digital Signal Processing",
      "doi": "10.1109/DSP58604.2023.10167766"
    },
    {
      "citation_id": "2",
      "title": "Speech emotion recognition via learning analogies",
      "authors": [
        "S Ntalampiras"
      ],
      "year": "2021",
      "venue": "Pattern Recognition Letters"
    },
    {
      "citation_id": "3",
      "title": "Toward Language-Agnostic Speech Emotion Recognition",
      "authors": [
        "S Ntalampiras"
      ],
      "year": "2020",
      "venue": "Journal of the Audio Engineering Society"
    },
    {
      "citation_id": "4",
      "title": "Aware Speech Emotion Recognition in Multiple Languages",
      "authors": [
        "M Nicolini",
        "Ntalampiras Gender"
      ],
      "venue": "Pattern Recognition Applications and Methods",
      "doi": "10.1007/978-3-031-54726-3_7"
    },
    {
      "citation_id": "5",
      "title": "Adversarial Attacks Against Audio Surveillance Systems",
      "authors": [
        "S Ntalampiras"
      ],
      "venue": "2022 30th European Signal Processing Conference (EUSIPCO)",
      "doi": "10.23919/EUSIPCO55093.2022.9909635"
    },
    {
      "citation_id": "6",
      "title": "Adversarial Attacks Against Acoustic Monitoring of Industrial Machines",
      "authors": [
        "S Ntalampiras"
      ],
      "year": "2023",
      "venue": "IEEE Internet of Things Journal"
    },
    {
      "citation_id": "7",
      "title": "Speech emotion recognition from 3D log-mel spectrograms with deep learning network",
      "authors": [
        "H Meng",
        "T Yan",
        "F Yuan"
      ],
      "year": "2019",
      "venue": "IEEE access"
    },
    {
      "citation_id": "8",
      "title": "Speech emotion recognition using deep 1D & 2D CNN LSTM networks",
      "authors": [
        "J Zhao",
        "X Mao"
      ],
      "year": "2019",
      "venue": "Biomedical signal processing and control"
    },
    {
      "citation_id": "9",
      "title": "A database of German emotional speech",
      "authors": [
        "F Burkhardt",
        "A Paeschke",
        "M Rolfes",
        "W Sendlmeier",
        "B Weiss"
      ],
      "year": "2005",
      "venue": "Interspeech",
      "doi": "10.21437/interspeech.2005-446"
    },
    {
      "citation_id": "10",
      "title": "EMOVO Corpus: an Italian Emotional Speech Database",
      "authors": [
        "G Costantini",
        "I Iaderola",
        "A Paoloni",
        "M Todisco"
      ],
      "year": "2014",
      "venue": "Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC'14)"
    },
    {
      "citation_id": "11",
      "title": "The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS): A dynamic, multimodal set of facial and vocal expressions in North American English",
      "authors": [
        "S Livingstone",
        "F Russo"
      ],
      "year": "2018",
      "venue": "PloS one"
    },
    {
      "citation_id": "12",
      "title": "A brief survey of adversarial machine learning and defense strategies",
      "authors": [
        "Z Akhtar",
        "D Dasgupta"
      ],
      "venue": "A brief survey of adversarial machine learning and defense strategies"
    },
    {
      "citation_id": "13",
      "title": "Explaining and harnessing adversarial examples",
      "authors": [
        "I Goodfellow",
        "J Shlens",
        "C Szegedy"
      ],
      "year": "2014",
      "venue": "Explaining and harnessing adversarial examples",
      "doi": "10.48550/arXiv.1412.6572",
      "arxiv": "arXiv:1412.6572"
    },
    {
      "citation_id": "14",
      "title": "Adversarial examples in the physical world",
      "authors": [
        "A Kurakin",
        "I Goodfellow"
      ],
      "year": "2018",
      "venue": "Artificial intelligence safety and security"
    },
    {
      "citation_id": "15",
      "title": "Deepfool: a simple and accurate method to fool deep neural networks",
      "authors": [
        "S Moosavi-Dezfooli",
        "A Fawzi"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "16",
      "title": "The limitations of deep learning in adversarial settings",
      "authors": [
        "N Papernot",
        "P Mcdaniel",
        "S Jha",
        "M Fredrikson",
        "Z Celik",
        "A Swami"
      ],
      "year": "2016",
      "venue": "IEEE"
    },
    {
      "citation_id": "17",
      "title": "Towards evaluating the robustness of neural networks",
      "authors": [
        "N Carlini",
        "D Wagner"
      ],
      "year": "2017",
      "venue": "2017 ieee symposium on security and privacy (sp)"
    },
    {
      "citation_id": "18",
      "title": "One pixel attack for fooling deep neural networks",
      "authors": [
        "J Su",
        "D Vargas",
        "K Sakurai"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Evolutionary Computation"
    },
    {
      "citation_id": "19",
      "title": "Adversarial Robustness Assessment: Why both L 0 and L i nf ty Attacks Are Necessary",
      "authors": [
        "S Kotyan",
        "D Vargas"
      ],
      "year": "2019",
      "venue": "Adversarial Robustness Assessment: Why both L 0 and L i nf ty Attacks Are Necessary",
      "doi": "10.48550/arXiv.1906.06026"
    },
    {
      "citation_id": "20",
      "title": "Decision-based adversarial attacks: Reliable attacks against black-box machine learning models",
      "authors": [
        "W Brendel",
        "J Rauber"
      ],
      "year": "2017",
      "venue": "Decision-based adversarial attacks: Reliable attacks against black-box machine learning models",
      "doi": ".org/10.48550/arXiv.1712.04248"
    },
    {
      "citation_id": "21",
      "title": "Deep learning techniques for speech emotion recognition, from databases to models",
      "authors": [
        "B Abbaschian",
        "D Sierra-Sosa"
      ],
      "year": "2021",
      "venue": "Sensors"
    },
    {
      "citation_id": "22",
      "title": "Emotion recognition and its application to computer agents with spontaneous interactive capabilities",
      "authors": [
        "R Nakatsu",
        "J Nicholson"
      ],
      "year": "1999",
      "venue": "Proceedings of the seventh ACM international conference on Multimedia (Part 1)",
      "doi": "10.1145/319463.319641"
    },
    {
      "citation_id": "23",
      "title": "Emotion in speech: Recognition and application to call centers",
      "authors": [
        "V Petrushin"
      ],
      "year": "1999",
      "venue": "Proceedings of artificial neural networks in engineering"
    },
    {
      "citation_id": "24",
      "title": "Acoustical properties of speech as indicators of depression and suicidal risk",
      "authors": [
        "D France",
        "R Shiavi",
        "S Silverman",
        "M Silverman",
        "M Wilkes"
      ],
      "year": "2000",
      "venue": "IEEE transactions on Biomedical Engineering"
    },
    {
      "citation_id": "25",
      "title": "Speech emotion recognition combining acoustic features and linguistic information in a hybrid support vector machine-belief network architecture",
      "authors": [
        "B Schuller",
        "G Rigoll"
      ],
      "year": "2004",
      "venue": "IEEE international conference on acoustics, speech, and signal processing"
    },
    {
      "citation_id": "26",
      "title": "Adieu features? End-to-end speech emotion recognition using a deep convolutional recurrent network",
      "authors": [
        "G Trigeorgis",
        "F Ringeval",
        "R Brueckner"
      ],
      "year": "2016",
      "venue": "2016 IEEE International Conference on Acoustics, Speech and Signal Processing",
      "doi": "10.1109/ICASSP.2016.7472669"
    },
    {
      "citation_id": "27",
      "title": "Direct modelling of speech emotion from raw speech",
      "authors": [
        "S Latif",
        "R Rana",
        "S Khalifa",
        "R Jurdak",
        "J Epps"
      ],
      "venue": "Direct modelling of speech emotion from raw speech",
      "doi": "10.21437/interspeech.2019-32522019"
    },
    {
      "citation_id": "28",
      "title": "Cnn+ lstm architecture for speech emotion recognition with data augmentation",
      "authors": [
        "C Etienne",
        "G Fidanza",
        "A Petrovskii",
        "L Devillers",
        "B Schmauch"
      ],
      "venue": "Cnn+ lstm architecture for speech emotion recognition with data augmentation",
      "doi": "10.21437/smm.2018-52018"
    },
    {
      "citation_id": "29",
      "title": "Deep learning for audio signal processing",
      "authors": [
        "H Purwins",
        "B Li",
        "T Virtanen",
        "J Schlüter",
        "S Chang",
        "T Sainath"
      ],
      "year": "2019",
      "venue": "IEEE Journal of Selected Topics in Signal Processing"
    },
    {
      "citation_id": "30",
      "title": "Deep learning techniques for speech emotion recognition: A review",
      "authors": [
        "S Pandey",
        "H Shekhawat",
        "S Prasanna"
      ],
      "year": "2019",
      "venue": "29th International Conference Radioelektronika (RADIOELEK-TRONIKA)",
      "doi": "10.1109/radioelek.2019.8733432"
    },
    {
      "citation_id": "31",
      "title": "Generating and protecting against adversarial attacks for deep speech-based emotion recognition models",
      "authors": [
        "Z Ren",
        "A Baird",
        "J Han",
        "Z Zhang",
        "B Schuller"
      ],
      "year": "2020",
      "venue": "ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing",
      "doi": "10.1109/icassp40776.2020.9054087"
    },
    {
      "citation_id": "32",
      "title": "Robust Federated Learning Against Adversarial Attacks for Speech Emotion Recognition",
      "authors": [
        "Y Chang",
        "S Laridi",
        "Z Ren",
        "G Palmer",
        "B Schuller",
        "M Fisichella"
      ],
      "year": "2022",
      "venue": "Robust Federated Learning Against Adversarial Attacks for Speech Emotion Recognition",
      "doi": "10.48550/arxiv.2203.04696"
    },
    {
      "citation_id": "33",
      "title": "Few-Shot Learning Network for Out-of-Distribution Image Classification",
      "authors": [
        "I Osman",
        "M Shehata"
      ],
      "venue": "IEEE Transactions on Artificial Intelligence"
    },
    {
      "citation_id": "34",
      "title": "Intriguing properties of neural networks",
      "authors": [
        "C Szegedy",
        "W Zaremba",
        "I Sutskever"
      ],
      "year": "2013",
      "venue": "Intriguing properties of neural networks",
      "doi": "10.48550/arXiv.1312.6199"
    },
    {
      "citation_id": "35",
      "title": "Evasion attacks against machine learning at test time",
      "authors": [
        "B Biggio",
        "I Corona",
        "D Maiorca"
      ],
      "year": "2013",
      "venue": "Joint European conference on machine learning and knowledge discovery in databases"
    },
    {
      "citation_id": "36",
      "title": "Crafting adversarial examples for speech paralinguistics applications",
      "authors": [
        "Y Gong",
        "C Poellabauer"
      ],
      "year": "2017",
      "venue": "Crafting adversarial examples for speech paralinguistics applications",
      "doi": "10.1145/3306195.3306196"
    },
    {
      "citation_id": "37",
      "title": "Adversarial machine learning and speech emotion recognition: Utilizing generative adversarial networks for robustness",
      "authors": [
        "S Latif",
        "R Rana",
        "J Qadir"
      ],
      "year": "2018",
      "venue": "Adversarial machine learning and speech emotion recognition: Utilizing generative adversarial networks for robustness",
      "doi": "10.48550/arXiv.1811.11402"
    },
    {
      "citation_id": "38",
      "title": "Deep Learning for Audio Signal Processing",
      "authors": [
        "H Purwins",
        "B Li",
        "T Virtanen",
        "J Schlüter",
        "S Chang",
        "T Sainath"
      ],
      "year": "2019",
      "venue": "IEEE Journal of Selected Topics in Signal Processing"
    },
    {
      "citation_id": "39",
      "title": "Targeted adversarial examples for black box audio systems",
      "authors": [
        "R Taori",
        "A Kamsetty",
        "B Chu",
        "N Vemuri"
      ],
      "year": "2019",
      "venue": "2019 IEEE security and privacy workshops (SPW)"
    },
    {
      "citation_id": "40",
      "title": "Audio adversarial examples: Targeted attacks on speech-to-text",
      "authors": [
        "N Carlini",
        "D Wagner"
      ],
      "year": "2018",
      "venue": "2018 IEEE Security and Privacy Workshops (SPW)",
      "doi": "10.1109/spw.2018.00009"
    },
    {
      "citation_id": "41",
      "title": "Data Augmentation Using GANs for Speech Emotion Recognition",
      "authors": [
        "A Chatziagapi",
        "G Paraskevopoulos",
        "D Sgouropoulos"
      ],
      "year": "2019",
      "venue": "Interspeech"
    },
    {
      "citation_id": "42",
      "title": "On enhancing speech emotion recognition using generative adversarial networks",
      "authors": [
        "S Sahu",
        "R Gupta",
        "C Espy-Wilson"
      ],
      "venue": "On enhancing speech emotion recognition using generative adversarial networks",
      "doi": "10.21437/interspeech.2018-18832018"
    },
    {
      "citation_id": "43",
      "title": "Augmenting generative adversarial networks for speech emotion recognition",
      "authors": [
        "S Latif",
        "M Asim",
        "R Rana",
        "S Khalifa",
        "R Jurdak",
        "B Schuller"
      ],
      "venue": "Augmenting generative adversarial networks for speech emotion recognition",
      "doi": "10.21437/interspeech.2020-31942020"
    },
    {
      "citation_id": "44",
      "title": "Deep convolutional neural networks and data augmentation for environmental sound classification",
      "authors": [
        "J Salamon",
        "J Bello"
      ],
      "year": "2017",
      "venue": "IEEE Signal processing letters"
    },
    {
      "citation_id": "45",
      "title": "Going deeper with convolutions",
      "authors": [
        "C Szegedy",
        "W Liu",
        "Y Jia"
      ],
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "46",
      "title": "Very deep convolutional networks for large-scale image recognition",
      "authors": [
        "K Simonyan",
        "A Zisserman"
      ],
      "year": "2014",
      "venue": "Very deep convolutional networks for large-scale image recognition",
      "doi": "10.48550/arXiv.1409.1556"
    },
    {
      "citation_id": "47",
      "title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift",
      "authors": [
        "S Ioffe",
        "C Szegedy"
      ],
      "year": "2015",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "48",
      "title": "Application of stacked convolutional and long short-term memory network for accurate identification of CAD ECG signals",
      "authors": [
        "J Tan",
        "Y Hagiwara",
        "W Pang"
      ],
      "year": "2018",
      "venue": "Computers in biology and medicine"
    },
    {
      "citation_id": "49",
      "title": "A novel banditbased approach to hyperparameter optimization",
      "authors": [
        "L Li",
        "K Jamieson",
        "G Desalvo",
        "A Rostamizadeh",
        "Talwalkar Hyperband"
      ],
      "year": "2017",
      "venue": "The Journal of Machine Learning Research"
    },
    {
      "citation_id": "50",
      "title": "Very Deep Convolutional Networks for Large-Scale Image Recognition",
      "authors": [
        "K Simonyan",
        "A ; Zisserman",
        "Y Bengio",
        "Y Lecun"
      ],
      "year": "2015",
      "venue": "3rd International Conference on Learning Representations, ICLR 2015"
    },
    {
      "citation_id": "51",
      "title": "Adversarial machine learning at scale",
      "authors": [
        "A Kurakin",
        "I Goodfellow",
        "S Bengio"
      ],
      "year": "2016",
      "venue": "Adversarial machine learning at scale",
      "doi": "10.48550/arXiv.1611.01236"
    },
    {
      "citation_id": "52",
      "title": "Adversarial attacks and defenses in deep learning",
      "authors": [
        "K Ren",
        "T Zheng",
        "Z Qin",
        "X Liu"
      ],
      "year": "2020",
      "venue": "Engineering"
    },
    {
      "citation_id": "53",
      "title": "Adversarial Robustness Toolbox v1.2.0",
      "authors": [
        "M Nicolae",
        "M Sinn",
        "M Tran"
      ],
      "year": "2018",
      "venue": "CoRR"
    }
  ]
}