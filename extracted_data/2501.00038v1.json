{
  "paper_id": "2501.00038v1",
  "title": "Sound-Based Recognition Of Touch Gestures And Emotions For Enhanced Human-Robot Interaction",
  "published": "2024-12-24T09:51:00Z",
  "authors": [
    "Yuanbo Hou",
    "Qiaoqiao Ren",
    "Wenwu Wang",
    "Dick Botteldooren"
  ],
  "keywords": [
    "Affective computing",
    "emotion classification",
    "touch gestures",
    "humanoid robots",
    "multi-temporal resolution CNN"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Emotion recognition and touch gesture decoding are crucial for advancing human-robot interaction (HRI), especially in social environments where emotional cues and tactile perception play important roles. However, many humanoid robots, such as Pepper, Nao, and Furhat, lack full-body tactile skin, limiting their ability to engage in touch-based emotional and gesture interactions. In addition, vision-based emotion recognition methods usually face strict GDPR compliance challenges due to the need to collect personal facial data. To address these limitations and avoid privacy issues, this paper studies the potential of using the sounds produced by touching during HRI to recognise tactile gestures and classify emotions along the arousal and valence dimensions. Using a dataset of tactile gestures and emotional interactions from 28 participants with the humanoid robot Pepper, we design an audio-only lightweight touch gesture and emotion recognition model with only 0.24M parameters, 0.94MB model size, and 0.7G FLOPs. Experimental results show that the proposed soundbased touch gesture and emotion recognition model effectively recognises the arousal and valence states of different emotions, as well as various tactile gestures, when the input audio length varies. The proposed model is low-latency and achieves similar results as well-known pretrained audio neural networks (PANNs), but with much smaller FLOPs, parameters, and model size.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "Robots' perception of tactile gestures and emotions is integral to the development of advanced human-robot interaction (HRI), especially in social environments, where understanding and responding to emotional cues is critical for meaningful interactions  [1] -  [3] . Moreover, real-time emotion states and touch gesture recognition can significantly improve the naturalness and effectiveness of HRI  [4]    [5] .\n\nPrevious work  [6]  has attempted to decode touch gestures from tactile signals, but a major limitation is that many robots lack skin that can sense touch throughout the body. Most research on humanoid robotics values touch less than vision or audio signals. Robots like Pepper  [7] , NAO  [8] , and Furhat  [9]  do not have extensive tactile sensors covering the entire body  [10] . This limitation hinders the development of HRI systems that decode gestures and emotions based on tactile perception. To this end, tactile sensors embedded in the surface of robots are used to decode touch gestures based on pressure patterns, thus providing a direct tactile-based interaction method  [11] . However, tactile sensors  [12]  can be intrusive and are often limited by the need for different body contacts.\n\nIn addition to tactile-based gesture recognition, most gesture decoding studies adopt vision-based methods, which use depth cameras  [13]  or optical sensors  [14]  to capture human gestures and emotions. These vision-based models perform well in controlled environments. However, they often face significant challenges in dynamic real-world environments  [15] , e.g., occlusion of certain parts of the face/body or changes in lighting conditions can seriously affect the effectiveness of the vision-based models  [16] . Moreover, the vision-based models usually require relatively high computational resources, and collection of training data. However, it is difficult to avoid issues related to privacy and general data protection regulation (GDPR) during data collection. These limitations restrict the scalability and practicality of vision-based methods.\n\nTo recognize gestures and emotions, researchers have also explored auditory signals and demonstrated the feasibility of sensing touch gestures on the surface of mobile devices based on acoustic signals  [17] . Auditory stimuli are viewed to be potent triggers of affective responses, and specific sounds, e.g., tapping sounds, can affect people's emotional states and behaviours  [18] . Moreover, speech-based emotion recognitions have been successfully applied to adjust the behaviour of social and educational robots to adapt to current social emotions. Furthermore, the recognition of emotions such as happiness, sadness, and anger has been widely carried out using features such as the rhythm, pitch, and intensity of speech  [19] -  [21] .\n\nEmotions are typically analysed along two independent dimensions (arousal and valence), like in Russell's model  [22]    [23] , and neuroimaging studies  [24]  [25] support these two-dimensional representations. Hence, the arousal-valence dimensional model (AVDM) is more commonly used for sound-related affective computing, e.g., soundscape studies  [26] [27] , than the discrete emotion model for single discrete entities  [28]    [29] . Hence, this paper uses AVDM to analyse touch-related emotions.\n\nThis paper makes the first attempt to recognise touch gestures and emotions based on sounds produced during HRI. Although previous studies have shown the feasibility of sensing touch gestures based on acoustic signals, they did not further study the decoding of different gestures from participants. Whereas most of the previous auditory-based human emotion recognition related to robots is based on speech signals, this paper utilises the non-speech sounds produced during HRI.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Background",
      "text": "Pepper is an interactive robot developed by SoftBank Robotics  [7] , as shown in Fig.  1 ; more technical specifications, please see here  1  . In the data collection experiment, participants are asked to express 6 gestures (hold, pat, poke, tickle, tap, rub) and 10 emotions by touching Pepper's left forearm with spontaneous movements. The sounds produced by these touch movements are recorded by a microphone (the black device in Fig.  2 ) to form the sound dataset in this paper.\n\nAccording to the Circumplex Model  [23] , the distribution of the 10 emotions involved in arousal-valence dimensions is shown in Fig.  3 . Happiness and surprise occupy the high arousal, positive valence quadrant (Q1); anger, fear and disgust are located in the high arousal, negative valence quadrant (Q2); sadness and confusion are located in the low arousal, negative valence quadrant (Q3); comfort and calming belong to the low arousal, positive valence quadrant (Q4); the neutral emotion, attention, is located at the origin (Q0).",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Iii. Proposed Method",
      "text": "Our ultimate goal is to develop a sound-based model that can be embedded into a robot to perceive touch gestures and emotions. To this end, this section first analyzes the limited availability of computing resources in the robot brain system and then designs a lightweight model tailored for the robot.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "A. Robot'S Brain System Computing Resources",
      "text": "Pepper's Brain system consists of an Intel ATOM® E3845 processor and 4 GB of DDR3 RAM. The documentation 2  provided by Intel shows that E3845 processor's Floating Point of Operations (FLOPs) is 11.46G per second. To deploy a model in Pepper's brain system, the model's FLOPs should not be greater than 11.46G; otherwise, it cannot be run on Pepper. Considering this restriction, the model to be deployed should meet the following requirements: 1) lightweight with few parameters; 2) small FLOPs and low processing latency; 3) able to process varying-length audio clips. Next, we will use these three points as guidelines to design the model.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "B. The Proposed Lightweight Model: Mtrcnn",
      "text": "Sounds caused by various touch gestures and emotions may have different durations, e.g., the sound of an angry hit and the rust of a calm touch. Thus, we propose a multitemporal resolution convolutional neural network (MTRCNN) in Fig.  4 . The convolution (Conv) part of MTRCNN consists of 3 branches with Conv kernel sizes of (3, 3),  (5, 5) , and (7, 7), respectively. Different kernel sizes extract representations with different resolutions. To obtain a larger Conv receptive field size (RFS) with fewer parameters, the dilated Conv  [30]  is used. Moreover, to avoid the gridding artifacts  [31]  of the dilated Conv, hybrid dilated Conv scheme  [30]  is adopted, so the number of filters and the dilation rate of 3 Conv layers of each branch are  [16, 32, 64]    Taking the branch with the largest kernel (7, 7) as an example, according to the convolution RFS calculation formula,\n\nwhere R i is the i-th Conv layer's RFS relative to the input feature map, R 0 = 1, the Conv step size stride defaults to 1, and k is the Conv kernel size. If there is no pooling operation, according to Eq. (  1 ), the 1st Conv layer's RFS on the time axis is R 1 = 7. For dilated Conv, the formula for the RFS is\n\nwhere r is the dilation rate. For the 2nd Conv layer with dilation rate (2, 1), on the time axis, R 2 is 20. For the 3rd Conv layer, R 3 is 38 on the time axis. That is, without pooling, MTRCNN requires that the length of the input be at least 38 frames. With a frame hop of 10ms, the corresponding length of the input clip is at least 0.38s. Identifying emotions or gestures within 0.38 seconds is challenging, even for humans. Moreover, if pooling is not used, the number of model parameters and the computation load will increase. After comprehensive trade-offs, we add pooling operations to these Conv layers, resulting in a minimum input audio length of 1.10s.\n\nAfter the three Conv layers, the representations are fed into the following 64D embedding layer to learn embeddings with different resolutions. Then, the 3 branches' embeddings are concatenated and fed into the 192-dimensional (64 * 3) multiresolution embedding fusion layer to fuse information from different temporal resolutions. The fused embeddings are fed to the last classification layers. For the resulting MTRCNN model, the FLOPs is 0.708G, the number of parameters is 0.24M , the model size is 0.94M B, and it can process input audio clips of any length of at least 1.10s. These fit well with the design guidelines in Section (Sec.) III-A.\n\nThe tasks in subsequent experiments are all single-label multi-classification tasks, so the activation functions for the arousal (Aro), valence (Val), Aro-Val, and gesture classification layers are all Softmax, the loss function is cross entropy  [32] . For more details, please see the homepage (https://github.com/Yuanbo2020/MTRCNN).",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Iv. Experiments And Results",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "A. Dataset, Experiments Setup, And Metrics",
      "text": "We conducted data collection experiments to record touch sounds for gesture and touch sounds for emotion  [33] . As stated in Sec. II, participants first expressed gestures 10 emotions independently by touching Pepper's arm, and then expressed 6 touch gestures; the sounds generated by movements during touch are recorded as the dataset. Participants complete 3 rounds of interaction, each lasting 10s. Finally, there are 84 (28 × 3) 10s audio clips for each gesture and emotion. For touch gesture classification, the number of samples in training, validation, and test sets is 366, 42, and 84, respectively. For emotion classification, the number of samples in training, validation, and test sets is 660, 80, and 100, respectively.\n\nThe Mel-filter with 64 banks is used as the acoustic feature, with a Hamming window of 32ms and an overlap of 10ms  [34] . Dropout and normalization are used to prevent model overfitting  [35] . A batch size of 32 and Adam optimizer  [36]  with a learning rate of 1e-3 are used to minimize loss. Models are trained on a GPU card Tesla T4 for 100 epochs, and 10 times without a fixed seed to obtain the mean result of 10 runs. Accuracy (Acc) is used to evaluate the classification results. Dataset, code, and models are available on the homepage.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "B. Results And Analysis",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "This Part Analyzes The Performance Of The Proposed Mtr-Cnn By The Following Research Question (Rq).",
      "text": "RQ1: Is it feasible to recognise touch gestures and distinguish emotions' arousal and valence based on audio alone?\n\nTable  I  shows the results of MTRCNN for classifying 6 gestures, as well as arousal and valence of emotions. Arousal is usually classified as low, neutral, and high. Valence is classified as negative, neutral, and positive. The AVDM of arousal-valence joint classification has four quadrants and an origin, so the arousal-valence  [23]  joint classification has five categories: Q1, Q2, Q3, Q4, and Q0, as shown in Fig.  3 . In Table  I , MTRCNN performs better on touch gesture classification than emotion classification. This may be because gestures usually contain clear and consistent patterns, such as regular sounds when tapping and snapping sounds when patting, so the model can capture sounds produced by specific movements and rhythms to identify touch gestures effectively. However, due to the variety of ways in which different participants express emotions and different perceptions of the same type of emotions  [37] , e.g., a calm emotion expressed by some may appear sad to others. This makes Aro-Val-based emotion classification, especially relying on sounds produced when touching, challenging.\n\nIn Table  I , MTRCNN has a higher classification accuracy on the arousal dimension than the valence dimension. Arousal denotes the intensity of emotion, which is usually conveyed via direct physical cues, e.g., pressure, frequency, and speed, which allows MTRCNN to grab these cues in sounds to efficiently identify the class of arousal. Valence reflects emotion's positive or negative nature, which is more subtle and contextdependent  [38] , making it challenging to distinguish it based solely on touch actions and the sounds caused by it. RQ2: What is the shortest effective audio length required for touch-sound-based gesture and emotion recognition models?\n\nTable  I  shows the performance of MTRCNN trained with full 10s audio clips. Here, we further explore the minimum audio length required for MTRCNN to effectively recognise gestures and emotion states. The input audio length can be regarded as a hyperparameter of the model. To avoid information leakage, Table  II  shows the results of this hyperparameter on the validation set. As mentioned in Sec. III-B, the proposed MTRCNN can handle audio clips with varying lengths with a minimum length of 1.10s, so the input audio length range in Table  II  is  [1.10, 10] .\n\nTable  II  shows that the accuracy of touch gesture classification increases with the input audio length and peaks at 6s. The gesture classification results closest to the 6s result are at 5s and 7s, respectively, so we conduct statistical analysis on these similar results. The Shapiro-Wilk test  [39]  shows that the data follow a normal distribution. Then, the paired t-test  [40]  is used, and the statistics show that the gesture classification results based on 6s clips are significantly better than those of 5s The above analyses show that MTRCNN effectively recognises different touch gestures within 6s and decodes emotions within 7s. Hence, input lengths of 6s and 7s will be used as default settings of MTRCNN for touch gesture classification and Aro-Val classification of emotions, respectively. RQ3: What are the most challenging touch gestures and the emotions' dimensions to distinguish based on sounds?\n\nIn Fig.  5  (a), MTRCNN performs better in identifying high arousal than low and neutral states. This implies that high arousal associated with touch sounds is easier to distinguish. Fig.  5 (c ) implies that touch-based emotions in positive valence are more distinguishable than those in negative valence. In the Aro-Val space in Fig.  5  (b), MTRCNN can better distinguish emotions in Q2 (high arousal, negative valence) and Q4 (low arousal, positive valence) than those in Q1 and Q3. This is interesting because it suggests that these combinations of arousal and valence may be more consistently expressed by specific touch gestures conveyed by participants. The high and low arousal in Fig.  5  (a), as well as positive and negative valence in Fig.  5 (c ), are rarely misclassified as neutral, implying that non-neutral emotions are less likely to be confused with neutral emotion due to their different tactile cues.\n\nFor gesture recognition, as shown in Fig.  5  (d), MTRCNN accurately identifies gestures such as pat, tap, and hold. These gestures usually have unique tactile profiles that are easy to identify. Like, pat may involve an easily recognizable repetitive rhythmic pattern  [41] , while tap and hold are simple, discrete movements with clear tactile features. In addition, rub is only misclassified as hold. This may be due to the subtle differences and overlapping tactile sensations between the two gestures, making their sounds similar, especially when the strength or speed of rub is not obvious. RQ4: How does the proposed model perform compared to other typical sound-related models?\n\nTable  III  compares models' performance on the same server (CPU: AMD EPYC 7352, GPU: Tesla T4 16GB). CNN-Transformer consists of 3 convolutional layers with (3 × 3) kernels, a Transformer encoder, and the final classification layers. YAMNet and MobileNetV2  [42]  are classic CNN-based networks. PANNs  [34]  have shown excellent performance on AudioSet  [43]  and audio pattern-related tasks. Therefore, #4 and #5 explore the performance of PANNs with and without pretrained weights on large-scale AudioSet  [43] , respectively. The #5 with pretrained weights (PreW) significantly out-",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "V. Conclusion",
      "text": "This paper explores the feasibility of identifying touch gestures and the emotions leading to them based on sounds produced by movements during touch, which fills the gap in HRI that lacks touch-related sounds to decode touch gestures and emotions. The proposed sound-based touch gesture and emotion recognition model can effectively recognize the arousal and valence states of different emotions, as well as various tactile gestures, when the input audio length varies from at least about 2 seconds to the optimal 6 to 7 seconds. Moreover, its lightweight, low-parameter, and low-latency processing characteristics make it ideal for real-time applications on robots such as Pepper. Future work will package the proposed model into an application and deploy it on Pepper.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: ; more technical specifications,",
      "page": 1
    },
    {
      "caption": "Figure 1: The robot Pepper’s physical information1.",
      "page": 2
    },
    {
      "caption": "Figure 2: ) to form the sound dataset in this paper.",
      "page": 2
    },
    {
      "caption": "Figure 3: Happiness and surprise occupy the high",
      "page": 2
    },
    {
      "caption": "Figure 2: The participant interacts with the robot Pepper.",
      "page": 2
    },
    {
      "caption": "Figure 3: Circumplex Model [23] with 10 emotions in this paper.",
      "page": 2
    },
    {
      "caption": "Figure 4: The convolution (Conv) part of MTRCNN consists",
      "page": 2
    },
    {
      "caption": "Figure 4: The proposed lightweight multi-temporal resolution",
      "page": 2
    },
    {
      "caption": "Figure 3: TABLE I: Test set classification results of the model with 10s",
      "page": 3
    },
    {
      "caption": "Figure 5: (a), MTRCNN performs better in identifying high",
      "page": 4
    },
    {
      "caption": "Figure 5: (c) implies that touch-based emotions in positive valence",
      "page": 4
    },
    {
      "caption": "Figure 5: (b), MTRCNN can better distinguish",
      "page": 4
    },
    {
      "caption": "Figure 5: (a), as well as positive and negative valence",
      "page": 4
    },
    {
      "caption": "Figure 5: (c), are rarely misclassified as neutral, implying",
      "page": 4
    },
    {
      "caption": "Figure 5: (d), MTRCNN",
      "page": 4
    },
    {
      "caption": "Figure 5: Normalized confusion matrix on the test set.",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "#": "",
          "Model": "",
          "Param.\n(M)": "",
          "Size\n(MB)": "",
          "(G)": "",
          "FLOPs Inference\ntime (s)": "",
          "Accuracy": "Aro-Val"
        },
        {
          "#": "1\n2\n4\n5",
          "Model": "CNN-Trans.\nYAMNet\n3 MobileNetV2\nPANNs\nPANNs PreW 79.68",
          "Param.\n(M)": "1.58\n3.21\n2.23\n79.68",
          "Size\n(MB)": "6.02\n12.30\n8.74\n304.1\n304.1",
          "(G)": "0.266\n0.728\n0.351\n11.96\n11.96",
          "FLOPs Inference\ntime (s)": "0.006\n0.008\n0.007\n0.012\n0.012",
          "Accuracy": "28.87±4.47 60.60±6.51\n29.03±2.77 61.19±5.78\n45.63±6.22 71.90±5.56\n49.20±3.02 76.43±4.48\n55.83±4.84 83.33±3.12"
        },
        {
          "#": "6",
          "Model": "MTRCNN",
          "Param.\n(M)": "0.24",
          "Size\n(MB)": "0.94",
          "(G)": "0.708",
          "FLOPs Inference\ntime (s)": "0.007",
          "Accuracy": "54.73±3.29 84.17±3.89"
        }
      ],
      "page": 4
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Recent advancements in multimodal human-robot interaction",
      "authors": [
        "H Su",
        "W Qi",
        "J Chen",
        "C Yang",
        "J Sandoval",
        "M Laribi"
      ],
      "year": "2023",
      "venue": "Frontiers in Neurorobotics"
    },
    {
      "citation_id": "2",
      "title": "Emotion recognition for human-robot interaction: Recent advances and future perspectives",
      "authors": [
        "M Spezialetti",
        "G Placidi",
        "S Rossi"
      ],
      "year": "2020",
      "venue": "Frontiers in Robotics and AI"
    },
    {
      "citation_id": "3",
      "title": "Affect display recognition through tactile and visual stimuli in a social robot",
      "authors": [
        "S Marques-Villarroya",
        "J Gamboa-Montero",
        "C Jumela-Yedra",
        "J Castillo",
        "M Salichs"
      ],
      "year": "2022",
      "venue": "International Conference on Social Robotics"
    },
    {
      "citation_id": "4",
      "title": "Humans interacting with multi-robot systems: a natural affect-based approach",
      "authors": [
        "V Villani",
        "B Capelli",
        "C Secchi",
        "C Fantuzzi",
        "L Sabattini"
      ],
      "year": "2020",
      "venue": "Autonomous Robots"
    },
    {
      "citation_id": "5",
      "title": "Gestural behavioral implementation on a humanoid robotic platform for effective social interaction",
      "authors": [
        "L Brown",
        "A Howard"
      ],
      "year": "2014",
      "venue": "IEEE International Symposium on Robot and Human Interactive Communication"
    },
    {
      "citation_id": "6",
      "title": "A review of surface haptics: Enabling tactile effects on touch surfaces",
      "authors": [
        "C Basdogan",
        "F Giraud",
        "V Levesque",
        "S Choi"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on haptics"
    },
    {
      "citation_id": "7",
      "title": "Pepper: The first machine of its kind",
      "authors": [
        "A Pandey",
        "R Gelin",
        "A Robot"
      ],
      "year": "2018",
      "venue": "IEEE Robotics & Automation Magazine"
    },
    {
      "citation_id": "8",
      "title": "Humanoid robot nao: Review of control and motion exploration",
      "authors": [
        "S Shamsuddin",
        "L Ismail",
        "H Yussof",
        "N Zahari",
        "S Bahari",
        "H Hashim",
        "A Jaffar"
      ],
      "year": "2011",
      "venue": "IEEE international conference on Control System, Computing and Engineering"
    },
    {
      "citation_id": "9",
      "title": "Furhat: a back-projected human-like robot head for multiparty human-machine interaction",
      "authors": [
        "S Al Moubayed",
        "J Beskow",
        "G Skantze"
      ],
      "year": "2012",
      "venue": "Cognitive Behavioural Systems"
    },
    {
      "citation_id": "10",
      "title": "Tactile sensing-from humans to humanoids",
      "authors": [
        "R Dahiya",
        "G Metta",
        "M Valle",
        "G Sandini"
      ],
      "year": "2009",
      "venue": "IEEE Transactions on robotics"
    },
    {
      "citation_id": "11",
      "title": "Gelsight: High-resolution robot tactile sensors for estimating geometry and force",
      "authors": [
        "W Yuan",
        "S Dong",
        "E Adelson"
      ],
      "year": "2017",
      "venue": "Sensors"
    },
    {
      "citation_id": "12",
      "title": "Tactile sensors: A review",
      "authors": [
        "M Meribout",
        "N Takele",
        "O Derege",
        "N Rifiki",
        "M El",
        "V Tiwari",
        "J Zhong"
      ],
      "year": "2024",
      "venue": "Measurement"
    },
    {
      "citation_id": "13",
      "title": "Depth camera based hand gesture recognition and its applications in human-computer-interaction",
      "authors": [
        "Z Ren",
        "J Meng",
        "J Yuan"
      ],
      "year": "2011",
      "venue": "International conference on information, communications & signal processing"
    },
    {
      "citation_id": "14",
      "title": "Vision-based hand gesture recognition for human-robot collaboration: a survey",
      "authors": [
        "Z Xia",
        "Q Lei",
        "Y Yang",
        "H Zhang",
        "Y He",
        "W Wang",
        "M Huang"
      ],
      "year": "2019",
      "venue": "International Conference on Control, Automation and Robotics"
    },
    {
      "citation_id": "15",
      "title": "Real-time human pose recognition in parts from single depth images",
      "authors": [
        "J Shotton",
        "A Fitzgibbon",
        "M Cook",
        "T Sharp",
        "M Finocchio",
        "R Moore",
        "A Kipman",
        "A Blake"
      ],
      "year": "2011",
      "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "16",
      "title": "Survey on emotional body gesture recognition",
      "authors": [
        "F Noroozi",
        "C Corneanu",
        "D Kamińska",
        "T Sapiński",
        "S Escalera",
        "G Anbarjafari"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on affective computing"
    },
    {
      "citation_id": "17",
      "title": "Vskin: Sensing touch gestures on surfaces of mobile devices using acoustic signals",
      "authors": [
        "K Sun",
        "T Zhao",
        "W Wang",
        "L Xie"
      ],
      "year": "2018",
      "venue": "Proceedings of Annual International Conference on Mobile Computing and Networking"
    },
    {
      "citation_id": "18",
      "title": "Sonification of surface tapping changes behavior, surface perception, and emotion",
      "authors": [
        "A Tajadura-Jiménez",
        "N Bianchi-Berthouze",
        "E Furfaro",
        "F Bevilacqua"
      ],
      "year": "2015",
      "venue": "IEEE MultiMedia"
    },
    {
      "citation_id": "19",
      "title": "Emotion recognition in human-computer interaction",
      "authors": [
        "R Cowie",
        "E Douglas-Cowie",
        "N Tsapatsoulis",
        "G Votsis",
        "S Kollias"
      ],
      "year": "2001",
      "venue": "IEEE Signal processing magazine"
    },
    {
      "citation_id": "20",
      "title": "Openear-introducing the munich open-source emotion and affect recognition toolkit",
      "authors": [
        "F Eyben",
        "M Wöllmer",
        "B Schuller"
      ],
      "year": "2009",
      "venue": "International conference on affective computing and intelligent interaction and workshops"
    },
    {
      "citation_id": "21",
      "title": "A scoping review of the literature on prosodic elements related to emotional speech in human-robot interaction",
      "authors": [
        "N Gasteiger",
        "J Lim",
        "M Hellou",
        "B Macdonald",
        "H Ahn"
      ],
      "year": "2024",
      "venue": "International Journal of Social Robotics"
    },
    {
      "citation_id": "22",
      "title": "A cross-cultural study of a circumplex model of affect",
      "authors": [
        "J Russell",
        "M Lewicka",
        "T Niit"
      ],
      "year": "1989",
      "venue": "Journal of personality and social psychology"
    },
    {
      "citation_id": "23",
      "title": "A circumplex model of affect",
      "authors": [
        "J Russell"
      ],
      "year": "1980",
      "venue": "Journal of personality and social psychology"
    },
    {
      "citation_id": "24",
      "title": "Emotional valence and arousal affect reading in an interactive way: neuroimaging evidence for an approach-withdrawal framework",
      "authors": [
        "F Citron",
        "M Gray",
        "H Critchley",
        "B Weekes",
        "E Ferstl"
      ],
      "year": "2014",
      "venue": "Neuropsychologia"
    },
    {
      "citation_id": "25",
      "title": "Neural systems subserving valence and arousal during the experience of induced emotions",
      "authors": [
        "T Colibazzi",
        "J Posner",
        "Z Wang",
        "D Gorman",
        "A Gerber",
        "S Yu"
      ],
      "year": "2010",
      "venue": "Emotion"
    },
    {
      "citation_id": "26",
      "title": "Understanding urban and natural soundscapes",
      "authors": [
        "D Botteldooren",
        "C Lavandier",
        "A Preis",
        "D Dubois",
        "I Aspuru"
      ],
      "year": "2011",
      "venue": "Forum Acusticum 2011"
    },
    {
      "citation_id": "27",
      "title": "Soundscape for european cities and landscape: understanding and exchanging",
      "authors": [
        "D Botteldooren",
        "T Andringa",
        "I Aspuru",
        "L Brown",
        "D Dubois",
        "C Guastavino",
        "C Lavandier",
        "M Nilsson",
        "A Preis"
      ],
      "year": "2013",
      "venue": "COST TD0804 Final conference: Soundscape of European cities and landscapes"
    },
    {
      "citation_id": "28",
      "title": "Ai-based soundscape analysis: Jointly identifying sound sources and predicting annoyance",
      "authors": [
        "Y Hou",
        "Q Ren",
        "H Zhang",
        "A Mitchell",
        "F Aletta",
        "J Kang",
        "D Botteldooren"
      ],
      "year": "2023",
      "venue": "The Journal of the Acoustical Society of America (JASA)"
    },
    {
      "citation_id": "29",
      "title": "Soundscape captioning using sound affective quality network and large language model",
      "authors": [
        "Y Hou",
        "Q Ren",
        "A Mitchell",
        "W Wang",
        "J Kang",
        "T Belpaeme",
        "D Botteldooren"
      ],
      "year": "2024",
      "venue": "Soundscape captioning using sound affective quality network and large language model",
      "arxiv": "arXiv:2406.05914"
    },
    {
      "citation_id": "30",
      "title": "Understanding convolution for semantic segmentation",
      "authors": [
        "P Wang",
        "P Chen",
        "Y Yuan",
        "D Liu",
        "Z Huang",
        "X Hou",
        "G Cottrell"
      ],
      "year": "2018",
      "venue": "IEEE"
    },
    {
      "citation_id": "31",
      "title": "Hierarchical shrinkage multiscale network for hyperspectral image classification with hierarchical feature fusion",
      "authors": [
        "H Gao",
        "Z Chen",
        "C Li"
      ],
      "year": "2021",
      "venue": "IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing"
    },
    {
      "citation_id": "32",
      "title": "Cooperative scene-event modelling for acoustic scene classification",
      "authors": [
        "Y Hou",
        "B Kang",
        "A Mitchell",
        "W Wang",
        "J Kang",
        "D Botteldooren"
      ],
      "year": "2024",
      "venue": "IEEE/ACM Transactions on audio, speech, and language processing"
    },
    {
      "citation_id": "33",
      "title": "Conveying emotions to robots through touch and sound",
      "authors": [
        "Q Ren",
        "R Proesmans",
        "F Bossuyt",
        "J Vanfleteren",
        "F Wyffels",
        "T Belpaeme"
      ],
      "year": "2024",
      "venue": "Conveying emotions to robots through touch and sound",
      "arxiv": "arXiv:2412.03300"
    },
    {
      "citation_id": "34",
      "title": "PANNs: Large-scale pretrained audio neural networks for audio pattern recognition",
      "authors": [
        "Q Kong",
        "Y Cao",
        "T Iqbal",
        "Y Wang",
        "W Wang",
        "M Plumbley"
      ],
      "year": "2020",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "35",
      "title": "Dropout: a simple way to prevent neural networks from overfitting",
      "authors": [
        "N Srivastava",
        "G Hinton",
        "A Krizhevsky",
        "I Sutskever"
      ],
      "year": "2014",
      "venue": "Journal of Machine Learning Research"
    },
    {
      "citation_id": "36",
      "title": "Adam: A method for stochastic optimization",
      "authors": [
        "D Kingma",
        "J Ba"
      ],
      "year": "2015",
      "venue": "Proc. of ICLR"
    },
    {
      "citation_id": "37",
      "title": "Emotion and perception: The role of affective information",
      "authors": [
        "J Zadra",
        "G Clore"
      ],
      "year": "2011",
      "venue": "Wiley interdisciplinary reviews: cognitive science"
    },
    {
      "citation_id": "38",
      "title": "The effect of context on choice and value",
      "authors": [
        "B Martino"
      ],
      "year": "2012",
      "venue": "Neuroscience of Preference and Choice"
    },
    {
      "citation_id": "39",
      "title": "Comparisons of various types of normality tests",
      "authors": [
        "B Yap",
        "C Sim"
      ],
      "year": "2011",
      "venue": "Journal of Statistical Computation and Simulation"
    },
    {
      "citation_id": "40",
      "title": "Continuous variable analyses: t-test, mann-whitney, wilcoxin rank",
      "authors": [
        "M Riina",
        "C Stambaugh",
        "N Stambaugh",
        "K Huber"
      ],
      "year": "2023",
      "venue": "Continuous variable analyses: t-test, mann-whitney, wilcoxin rank"
    },
    {
      "citation_id": "41",
      "title": "On rhythmic and discrete movements: reflections, definitions and implications for motor control",
      "authors": [
        "N Hogan",
        "D Sternad"
      ],
      "year": "2007",
      "venue": "Experimental brain research"
    },
    {
      "citation_id": "42",
      "title": "Mobilenetv2: Inverted residuals and linear bottlenecks",
      "authors": [
        "M Sandler",
        "A Howard",
        "M Zhu",
        "A Zhmoginov",
        "L Chen"
      ],
      "year": "2018",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "43",
      "title": "AudioSet: An ontology and human-labeled dataset for audio events",
      "authors": [
        "J Gemmeke",
        "D Ellis",
        "D Freedman",
        "A Jansen",
        "W Lawrence",
        "R Moore"
      ],
      "year": "2017",
      "venue": "IEEE international conference on acoustics, speech and signal processing (ICASSP)"
    }
  ]
}