{
  "paper_id": "2507.14698v2",
  "title": "Spatial-Temporal Transformer With Curriculum Learning For Eeg-Based Emotion Recognition",
  "published": "2025-07-19T17:23:38Z",
  "authors": [
    "Xuetao Lin",
    "Tianhao Peng",
    "Peihong Dai",
    "Yu Liang",
    "Wenjun Wu"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "EEG-based emotion recognition plays an important role in developing adaptive brain-computer communication systems, yet faces two fundamental challenges in practical implementations: (1) effective integration of non-stationary spatial-temporal neural patterns, (2) robust adaptation to dynamic emotional intensity variations in real-world scenarios. This paper proposes STT-CL, a novel framework integrating spatial-temporal transformers with curriculum learning. Our method introduces two core components: a spatial encoder that models inter-channel relationships and a temporal encoder that captures multi-scale dependencies through windowed attention mechanisms, enabling simultaneous extraction of spatial correlations and temporal dynamics from EEG signals. Complementing this architecture, an intensity-aware curriculum learning strategy progressively guides training from high-intensity to low-intensity emotional states through dynamic sample scheduling based on a dual difficulty assessment. Comprehensive experiments on three benchmark datasets demonstrate state-of-theart performance across various emotional intensity levels, with ablation studies confirming the necessity of both architectural components and the curriculum learning mechanism.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "Emotion recognition constitutes a fundamental component of brain-inspired human-computer interaction systems  [1] . Electroencephalography (EEG), a non-invasive technique for measuring brain activity with high temporal resolution, has emerged as a critical modality for emotion recognition due to its ability to directly capture neural dynamics objectively  [2] . Compared to non-physiological modalities such as facial expressions and speech, EEG provides a more direct reflection of the underlying cognitive and emotional processes, making it a preferred way for fine-grained emotion recognition  [3] .\n\nEEG-based emotion recognition is challenging due to the non-stationary, noisy, and high-dimensional nature of EEG signals  [4] ,  [5] . Recent advancements in deep learning have significantly improved EEG-based emotion recognition by enabling more efficient modeling of spatial and temporal dependencies in EEG features  [6] ,  [7] . Early methods like convolutional neural networks (CNNs)  [8]  and graph-based approaches  [9] -  [12]  such as DGCNN  [13]  have proven effective in extracting spatial features, while models like ATDD-LSTM  [14]  and Transformer-based architectures  [15] ,  [16]  focus on capturing temporal dependencies  [17] . However, these methods often treat spatial and temporal features independently, limiting their ability to fully model the joint spatiotemporal dynamics critical for emotion recognition.\n\nIn addition to the spatial and temporal complexities of EEG signals, emotional intensity variability presents a significant challenge in emotion recognition. Emotional intensity, defined as the strength of an emotional state, strongly influences the neural patterns observed in EEG signals  [18] . High-intensity emotions are often easier to classify due to their distinct neural activations, whereas low-intensity emotions produce subtle patterns that are more challenging to detect  [19] . Most existing frameworks assume that emotional states remain stable during experimental trials, failing to account for the dynamic fluctuations in intensity observed in real-world scenarios  [20] . Several approaches have been proposed to address emotional intensity variability. These include using generative models like Variational Autoencoders (VAEs)  [21] ,  [22]  and Generative Adversarial Networks (GANs)  [23] ,  [24]  to augment datasets, and employing contrastive learning to align feature representations across intensity levels  [25] . While effective for data imbalance and representation learning, these methods lack a structured, progressive mechanism for capturing the hierarchical nature of emotional intensity variations.\n\nIn summary, despite recent advancements in deep learning and EEG-based emotion recognition, two critical challenges remain unresolved:\n\n(1) EEG signals exhibit complex temporal and spatial dependencies that are difficult to model jointly. While existing methods focus on either spatial (e.g. CNNs, GNNs) or temporal features (e.g. LSTMs, Transformers), they often fail to capture the full spatiotemporal dynamics essential for understanding emotional states comprehensively.\n\n(2) Emotional intensity fluctuates over time, most existing models assume emotional stability and lack mechanisms to adapt to these intensity variations, resulting in reduced robustness in real-world scenarios.\n\nTo address these challenges, we propose the Spatial-Temporal Transformer with Curriculum Learning (STT-CL) for EEG-Based Emotion Recognition, a novel framework designed to capture the spatiotemporal dynamics of EEG signals while systematically addressing emotional intensity variability.\n\nThe contributions of STT-CL can be summarized as follows:\n\n1) We introduces a dual-attention mechanism to simultaneously model inter-channel spatial relationships and long-range temporal dependencies, addressing the limitations of existing Transformer-based models.\n\n2) The proposed STT-CL implements a curriculum learning framework that dynamically adapts to emotional intensity strata through sample difficulty scheduling, enhancing classification robustness intensity levels.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "3) Extensive Experiments On Eeg Datasets Demonstrate",
      "text": "the effectiveness of STT-CL, showcasing its ability to handle emotional intensity variability and capture spatial-temporal dynamics.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Ii. Related Work A. Spatial Temporal Modeling Methods",
      "text": "EEGNet  [8]  employs depthwise and separable convolutions to extract spatial and temporal features efficiently but struggles to model long-range dependencies critical for complex emotional states. FBCCNN  [24]  and Tsception  [26]  improve spatial and temporal feature extraction, respectively, but fail to jointly model spatiotemporal relationships in EEG data. Graph-based methods like DGCNN  [13]  and RGNN  [27]  capture inter-channel spatial correlations by modeling EEG signals as graphs. However, these methods lack explicit mechanisms to account for temporal dynamics. Hybrid models, such as ECLGCNN  [28] , combine graphbased spatial modeling with LSTMs for temporal feature extraction but rely on predefined adjacency matrices limiting adaptability to dynamic EEG patterns. Transformer-based architectures have emerged as promising alternatives for modeling EEG data. HSLT  [15]  extracts hierarchical spatial features, while EEG Conformer  [16]  combines convolutional layers for local spatial patterns with Transformer layers for long-range temporal modeling. However, these models often treat spatial and temporal features separately, limiting their ability to fully capture the joint spatiotemporal dynamics essential for emotion recognition.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "B. Emotional Intensity Modeling Methods",
      "text": "Emotional intensity variability poses a significant challenge in EEG-based emotion recognition. To address this, feature augmentation techniques such as VAE  [21] ,  [22]  and GAN  [24] ,  [29]  generate synthetic low-intensity samples, improving dataset balance and classifier robustness. However, these methods are computationally expensive and sensitive to hyperparameter tuning, limiting their scalability  [30] . Contrastive learning approaches, such as the spatiotemporal framework by Shen et al.  [25] , align feature representations across intensity levels to enhance generalization. Yet, they lack hierarchical learning mechanisms to handle the progressive complexity of emotional intensity variations.\n\nCurriculum learning  [31]  provides a systematic solution that plan the training process of models in various domains  [32] -  [35] . Yang et al.  [36]  applied CL to text-based emotion recognition by prioritizing high-confidence samples, improving model stability. Zoumpourlis et al.  [37]  used CL to enhance cross-subject generalization in motor imagery decoding by progressively increasing task complexity. Feng et al.  [38]  extended CL to EEG-to-text generation, introducing ambiguous features progressively. Inspired by these works, we adapt CL to EEG-based emotion recognition, leveraging emotional intensity as a natural criterion to guide training and effectively learn subtle low-intensity patterns.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Iii. Motivation",
      "text": "In EEG-based emotion recognition, current experimental paradigms assign identical emotional labels to all EEG segments extracted from the same stimulus-evoked trial, implicitly assuming uniform emotional intensity levels across co-labeled segments. This fundamental assumption inherently contradicts real-world neurodynamics where affective intensity fluctuates temporally, even within single trials. To bridge this experimental-neurophysiological gap, we implement curriculum learning with dynamic sample scheduling guided by evolving intensity estimates. Specially, we operationalize emotional intensity through a neurocomputational lens as sample-wise discrimination difficulty, quantified by a composite metric combining instantaneous prediction loss and cumulative error history. This dual-aspect quantification naturally captures both transient neural activation patterns and persistent learning challenges inherent to low-intensity states. The framework progressively exposes the model to graded intensity strata-from prototypical high-discriminability to ambiguous low-intensity patterns-through neural-plausible adaptation mechanisms. This strategic progression enhances robustness to affective fluctuations while maintaining methodological rigor through data-driven intensity quantification.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Iv. Methodology",
      "text": "In this section, we present the proposed STT-CL framework for EEG-based emotion recognition. STT-CL is designed to address two major challenges: (1) joint modeling of spatial and temporal dependencies in EEG signals, and (2) handling the variability in emotional intensity through progressive learning. As shown in Fig.  1 , the framework consists of three major components: (a) the data processing, (b) the Spatial-Temporal Transformer, and (c) the curriculum learning training framework.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "A. Data Processing",
      "text": "EEG signals are recorded using electrodes placed on the scalp, each recording is typically represented as a matrix X ∈ R C×T , where C is the number of channels, and T is the number of time steps. However, EEG signals are noisy and high-dimensional, requiring robust preprocessing and feature extraction to improve downstream model performance.   c ) is an illustration of our curriculum learning process, where training samples are dynamically organized from \"easy\" to \"hard\" based on their difficulty levels. As training progresses, probability density function evolves from left-skewed (prioritizing easy samples) to right-skewed (emphasizing hard samples) to progressively handle increasingly complex emotional states. Here, Acc Seg i denotes the cumulative accuracy of segment i, and Loss Seg i represents the cross-entropy loss of sample i.\n\nOur processing pipeline follows prior work  [39]  and begins with spatial normalization through common average re-referencing to mitigate channel-specific biases. Next, a Butterworth bandpass filter (0.5-48 Hz) is applied for spectral purification, suppressing low-frequency drifts (<0.5 Hz) and high-frequency muscle artifacts (>48 Hz). Independent Component Analysis (ICA), with automated ocular artifact detection  [40] , is subsequently used to isolate and remove physiological contaminants while preserving neural oscillatory patterns. This cascade processing ensures signal integrity while maintaining the temporal resolution essential for emotion-related neural dynamics.\n\nAfter preprocessing, the continuous EEG signal is divided into continuous windows, allowing for the capture of distinct temporal segments while retaining important contextual stability across the signal. Each window undergoes spectral decomposition via Welch's method to compute Differential Entropy (DE) features across five frequency bands: δ (0.5-4 Hz), θ (4-8 Hz), α (8-13 Hz), β (13-30 Hz), and γ (30-48 Hz). The DE values are calculated as:\n\nwhere σ 2 represents signal variance within each sub-band. The resulting feature matrix for each segment is denoted as Seg ∈ R T ×C×B , where T represents the number of time windows in one segment, and B denotes the number of frequency bands.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "B. Spatial-Temporal Transformer",
      "text": "The Spatial-Temporal Transformer is the core component of STT-CL, designed to extract joint spatial and temporal features from EEG signals. STT consists of two key modules: the Spatial Encoder and the Temporal Encoder. The Spatial Encoder processes the spatial correlations across channels, while the Temporal Encoder captures the temporal dependencies within each EEG channel.\n\n1) Spatial Encoder: The Spatial Encoder models interchannel dependencies by analyzing the dynamics across electrodes. Giving the input Seg ∈ R T ×C×B , we first reorganize it into X Spatial ∈ R C×(T •B) , then pass it through the Spatial Encoder. The encoder applies multi-head selfattention to discover functional connectivity patterns:\n\nwhere Q = X Spatial W q , K = X Spatial W k , and V = X Spatial W v are learnable projections. This attention mechanism automatically identifies salient spatial correlations through end-to-end training, eliminating the need for predefined neurophysiological constraints.\n\nThen, the processed features undergo refinement through:\n\nthat capture inter-channel relationships and preserve relevant temporal information.\n\n2) Temporal Encoder: Building upon the spatial representations, the Temporal Encoder captures multi-scale temporal dependencies through a windowed attention mechanism. Given the spatial embeddings H s ∈ R C×(T •dc) , we reorganize it to obtain temporal sequences X t ∈ R T ×(C•dc) . The hybrid attention architecture enables simultaneous modeling of local and global temporal dynamics:\n\nwhere M win ∈ {0, 1} T ×T is a sliding window to prioritize proximal time steps while retaining capacity for long-range interactions. This design captures both transient neural oscillations and sustained temporal patterns inherent in emotional responses.\n\nThe processed features are then transformed through:\n\nyielding embeddings H t ∈ R T ×dt that capture both spatial and temporal patterns across the EEG signal, as they are derived from the Spatial Encoder and the Temporal Encoder.\n\n3) Classification and Output: The spatial-temporal embeddings H t undergo hierarchical transformation through a cascaded classifier architecture to generate emotion probability distributions. The computational process is formally defined as:\n\nwhere the Flatten(•) operation reshapes the spatial-temporal embedding tensor H t ∈ R C×dt into a vectorized representation h flat ∈ R C•dt , preserving spatial-temporal interactions while enabling subsequent linear projections. The weight matrix W 1 ∈ R (C•dt)×d h implements a non-linear mapping through Gaussian Error Linear Unit (GELU) activation, projecting the flattened features into a latent semantic space of dimension d h . Finally, W 2 ∈ R d h ×K establishes discriminative boundaries in this latent space to generate class logits for K emotion categories, which are normalized through softmax activation to obtain probability distributions.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "C. Curriculum Learning Framework",
      "text": "To handle the variability of emotional intensity, STT-CL uses a curriculum learning framework that dynamically adjusts the training process based on sample difficulty. The CL framework consists of two key components: difficulty evaluation and dynamic training subset selection.The detailed process of curriculum learning is summarized in Algorithm 1.\n\n1) Difficulty Evaluation: The difficulty of each EEG segment is estimated through a composite metric combining both instantaneous loss and historical performance. This evaluation is conducted over the entire training dataset at each epoch for two purposes:\n\nHistorical Error Rate  (7)  where L (k) i is the instantaneous cross-entropy loss at current epoch k, I(•) is the indicator function, and β balances the two components. This dual-metric approach captures two temporal dimensions of learning dynamics: the current loss term directly reflects the sample's instantaneous prediction difficulty, while the error rate term evaluates longterm classification patterns through accumulated historical accuracy statistics. The parameter β acts as an adaptive gatekeeper-lower values prioritize recent performance fluctuations for rapid adaptation, whereas higher values emphasize systematic learning bottlenecks. By integrating both temporal granularity and longitudinal consistency, the metric robustly identifies samples that require sustained learning effort, avoiding overemphasis on ephemeral anomalies while progressively exposing the model to progressively complex emotional states.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "2) Dynamic Training Subset Selection:",
      "text": "The sample selection process employs a shifting probability distribution to implement curriculum-driven learning.The core idea consists of two synergistically coordinated strategies:\n\nThe peak migration operator governs sample selection probabilities through an adaptive Gaussian kernel:\n\nwhere the dynamic center parameter µ k controls the difficulty focus window, and the bandwidth parameter σ k regulates sample diversity. During early training phases, µ k is initialized to emphasize prototypical samples with easy samples while σ k maintains wide exploration bandwidth. As training progresses, µ k systematically shifts toward higher difficulty regions while σ k undergoes controlled narrowing, creating a smooth curriculum transition from high-intensity focus on discriminative patterns, through moderate exploration of ambiguous samples, to low-intensity refinement of subtle emotional expressions.\n\nComplementing this spatial adaptation, the subset size scheduler regulates data exposure intensity through monotonic linear expansion:\n\nThe expansion coefficient α k follows a linear progression from partial to complete dataset utilization, gradually enlarging the training subset throughout the learning process. This controlled scaling mechanism ensures continuous difficulty escalation by systematically introducing more challenging samples proportionally to the model's growing competency. Following probabilistic selection, the chosen subset S k undergoes difficulty-based sorting where samples are arranged in ascending order of their d i values, establishing explicit curriculum sequencing within each training epoch.\n\nThe compound effect of these mechanisms emulates expert-guided learning processes in neural systems -initially building strong priors through focused repetition, then refining discrimination capabilities via controlled challenge escalation. This computational curriculum mirrors the natural progression of human skill acquisition, where foundational competence in core patterns precedes sophisticated handling of edge cases and subtle variations through progressive complexity exposure.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "V. Experiments",
      "text": "This section presents the evaluation of STT-CL on EEGbased emotion recognition tasks using three benchmark datasets. We first describe the datasets, baseline models, and experimental setup. Following this, we compare the performance of STT-CL against state-of-the-art methods and provide detailed analyses.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "A. Datasets",
      "text": "We evaluate the proposed STT-CL on three widely used EEG-based emotion recognition datasets: SEED Series (SEED  [39]  and SEED-IV  [41] ) : The SEED dataset consists of EEG signals from 15 subjects, each participating in three sessions with 15 emotion-inducing trials per session (positive, neutral, and negative). SEED-IV extends this dataset with four emotion labels (happiness, sadness, fear, and neutral) and 24 trials per session. Both datasets were recorded with 62 EEG channels.\n\nDEAP  [42] : This dataset contains EEG signals from 32 participants while they watched 40 one-minute music videos, rated on arousal, valence, dominance, and liking. EEG signals were recorded using 32 EEG channels and downsampled to 128 Hz for analysis.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "B. Compared Methods",
      "text": "This section introduces the methods used for comparison in our study, including spatial-temporal modeling and emotional intensity modeling baselines.\n\nSpatial-Temporal modeling baselines: CNN-based methods include EEGNet  [8]  with depthwise convolutions and Tsception  [26]  using multi-scale temporal kernels. Graphbased approaches contain DGCNN  [13]  for dynamic connectivity learning and RGNN  [27]  with neurophysiological constraints. Transformer architectures feature HSLT  [15]  employing hierarchical attention and EEG Conformer  [16]  combining CNNs with transformers.\n\nEmotional intensity modeling baselines: Generative approaches comprise FBCCNN  [24]  synthesizing low-intensity samples via GANs and STNet  [29]  utilizing adversarial segment interpolation. For curriculum learning, we implement three paradigms under Bengio's framework  [31] : 1) Self-Paced CL  [43] ,  [44]  prioritizes high-confidence samples, 2) Competence-CL  [45]  schedules training via model competency metrics, and 3) Dynamic-CL  [46]  dynamically adjusts sample probabilities based on instance hardness.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "C. Experimental Setting",
      "text": "We follow the experimental protocols from LibEER  [47]  and TorchEEG  [48] , employing a 5-fold cross-validation scheme with the cross-trial setting. In this setup, each subject's trials are divided into five folds, with training and testing conducted on different trials, ensuring effective evaluation of the model's performance across distinct emotional states. For SEED and SEED-IV, we perform threeclass and four-class classification respectively according to their labels, while for DEAP, we derive four classes by dichotomizing arousal and valence into high and low levels. We use classification accuracy and F1-score as the evaluation metrics.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "D. Performance Comparison",
      "text": "As presented in Table  I , our proposed STT-CL framework consistently outperforms existing state-of-the-art (SOTA) methods across three benchmark datasets, establishing new performance benchmarks in EEG-based emotion recognition.\n\nOn the SEED dataset, STT-CL achieves an accuracy of 83.84% (+2.21%) and an F1-score of 78.24% (+4.58%) over the previous SOTA, FBCCNN. This strong performance  Furthermore, STT-CL demonstrates the superiority of its intensity-based curriculum over several alternative CL strategies. It surpasses the self-paced (STT-SPCL) and competence-based (STT-CCL) variants by 2.77% and 1.39% in accuracy on SEED and DEAP, respectively. Our model also outperforms the dynamic curriculum (STT-DCL) on SEED-IV by 0.82% in accuracy and 2.25% in F1-score. These results validate that our design is crucial for effectively modeling emotional dynamics.\n\nThree key characteristics emerge from the experimental validation: First, the framework demonstrates enhanced robustness with standard deviations lower than previous methods. Second, the performance gains are consistent across datasets, highlighting the method's adaptability. Third, the improvements hold across both accuracy and F1-score metrics, suggesting balanced enhancement in both overall prediction correctness and class-wise performance.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "E. Ablation Study",
      "text": "Table  II  validates the necessity of each component in STT-CL via three variants: spatial-only (ST), temporal-only (TT), and their CL-enhanced counterparts. The quantitative analysis reveals three critical findings:\n\n(1) Decomposing the model reveals complementary spatiotemporal contributions. On SEED, removing the spatial encoder (TT variant) reduces accuracy by 5.21%. Conversely, ablating the temporal encoder (ST variant) causes a more severe performance drop of 9.82% in accuracy. This empirically confirms that temporal dynamics constitute fundamental emotional signatures, while spatial patterns provide discriminative refinement.\n\n(2) The CL extension provides a consistent performance uplift. When applied to the TT baseline, CL achieves a 3.43% accuracy improvement on SEED (TT-CL), demonstrating its standalone efficacy. Its full integration into the STT architecture yields a 3.53% accuracy enhancement, underscoring CL's critical role in handling intensity variations through progressive learning.\n\n(3) The analysis reveals an asymmetric dependency on CL. The temporal component exhibits a stronger reliance on CL guidance, with its accuracy improving by 3.43% (TT vs. TT-CL). In contrast, the spatial component shows a smaller  gain of 2.15% (ST vs. ST-CL). This suggests that temporal dynamics, being more sensitive to emotional intensity, benefit more from the structured learning that CL provides.\n\nThese findings establish three conclusions: First, spatiotemporal co-modeling is essential for comprehensive neural representation. Second, curriculum-based learning effectively bridges the gap to real-world emotional variability. Finally, component synergy, not isolated optimization, drives state-of-the-art performance.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "F. Hyper-Parameter Analysis",
      "text": "In this section, we analyze the impact of key hyperparameters, including the number of layers in the Transformer Encoders and the time window size of one segment, on the performance of STT-CL using the SEED-IV dataset. Following prior work  [39] , we set the segment length to 3 seconds.\n\nThe results of the encoder layers' analysis are shown in Fig.  2 . As shown, increasing the number of layers improves both accuracy and F1-score up to 5 layers, where the highest performance is achieved. Then, increasing the number of layers beyond 5 (to 6 and 10 layers) results in a slight decrease in performance, indicating diminishing returns from additional layers. This suggests that 5 layers strike a good balance between model complexity and performance.\n\nWe also analyze the effect of different time window sizes, while keeping the number of layers fixed at 5. As shown in Fig.  3 , performance improves as the time window increases, with the best results achieved at 0.5 seconds. Smaller time windows (0.1s and 0.2s) result in lower performance, likely due to insufficient temporal resolution.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "G. Visualization",
      "text": "In this section, We use t-SNE to visualize feature distributions for the SEED-IV dataset with and without the",
      "page_start": 7,
      "page_end": 7
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: , the framework",
      "page": 2
    },
    {
      "caption": "Figure 1: Overview of the proposed STT-CL. (a) Raw EEG signals undergo preprocessing, feature extraction, and reorganization",
      "page": 3
    },
    {
      "caption": "Figure 2: Hyper-parameter analysis: Encoder layers.",
      "page": 7
    },
    {
      "caption": "Figure 3: Hyper-parameter analysis: Time window.",
      "page": 7
    },
    {
      "caption": "Figure 2: As shown, increasing the number of layers improves",
      "page": 7
    },
    {
      "caption": "Figure 3: , performance improves as the time window increases,",
      "page": 7
    },
    {
      "caption": "Figure 4: t-SNE visualization of STT-CL for feature learning.",
      "page": 7
    }
  ],
  "tables": [],
  "citations": [
    {
      "citation_id": "1",
      "title": "Gender-sensitive eeg channel selection for emotion recognition using enhanced genetic algorithm",
      "authors": [
        "D.-T Duan",
        "B Sun",
        "Q Yang",
        "W Zhong",
        "L Ye",
        "Q Zhang",
        "J Zhang"
      ],
      "year": "2023",
      "venue": "2023 IEEE International Conference on Systems, Man, and Cybernetics (SMC)"
    },
    {
      "citation_id": "2",
      "title": "Fetcheeg: a hybrid approach combining feature extraction and temporal-channel joint attention for eeg-based emotion classification",
      "authors": [
        "Y Liang",
        "C Zhang",
        "S An",
        "Z Wang",
        "K Shi",
        "T Peng",
        "Y Ma",
        "X Xie",
        "J He",
        "K Zheng"
      ],
      "year": "2024",
      "venue": "Journal of Neural Engineering"
    },
    {
      "citation_id": "3",
      "title": "Eeg based emotion recognition: A tutorial and review",
      "authors": [
        "X Li",
        "Y Zhang",
        "P Tiwari",
        "D Song",
        "B Hu",
        "M Yang",
        "Z Zhao",
        "N Kumar",
        "P Marttinen"
      ],
      "year": "2023",
      "venue": "ACM Computing Surveys"
    },
    {
      "citation_id": "4",
      "title": "Research progress of eeg-based emotion recognition: A survey",
      "authors": [
        "Y Wang",
        "B Zhang",
        "L Di"
      ],
      "year": "2024",
      "venue": "ACM Computing Surveys"
    },
    {
      "citation_id": "5",
      "title": "Goat: a novel global-local optimized graph transformer framework for predicting student performance in collaborative learning",
      "authors": [
        "T Peng",
        "Q Yue",
        "Y Liang",
        "J Ren",
        "J Luo",
        "H Yuan",
        "W Wu"
      ],
      "year": "2025",
      "venue": "Scientific Reports"
    },
    {
      "citation_id": "6",
      "title": "Help-dkt: an interpretable cognitive model of how students learn programming based on deep knowledge tracing",
      "authors": [
        "Y Liang",
        "T Peng",
        "Y Pu",
        "W Wu"
      ],
      "year": "2022",
      "venue": "Scientific Reports"
    },
    {
      "citation_id": "7",
      "title": "Pre-trained molecular language models with random functional group masking",
      "authors": [
        "T Peng",
        "Y Li",
        "X Li",
        "J Bian",
        "Z Xie",
        "N Sui",
        "S Mumtaz",
        "Y Xu",
        "L Kong",
        "H Xiong"
      ],
      "year": "2024",
      "venue": "Pre-trained molecular language models with random functional group masking",
      "arxiv": "arXiv:2411.01401"
    },
    {
      "citation_id": "8",
      "title": "Eegnet: A compact convolutional network for eeg-based brain-computer interfaces",
      "authors": [
        "V Lawhern",
        "A Solon",
        "N Waytowich",
        "S Gordon",
        "C Hung",
        "B Lance"
      ],
      "year": "2018",
      "venue": "Journal of Neural Engineering"
    },
    {
      "citation_id": "9",
      "title": "Clgt: A graph transformer for student performance prediction in collaborative learning",
      "authors": [
        "T Peng",
        "Y Liang",
        "W Wu",
        "J Ren",
        "Z Pengrui",
        "Y Pu"
      ],
      "year": "2023",
      "venue": "Proceedings of the AAAI conference on artificial intelligence"
    },
    {
      "citation_id": "10",
      "title": "Graphrare: Reinforcement learning enhanced graph neural network with relative entropy",
      "authors": [
        "T Peng",
        "W Wu",
        "H Yuan",
        "Z Bao",
        "Z Pengru",
        "X Yu",
        "X Lin",
        "Y Liang",
        "Y Pu"
      ],
      "year": "2024",
      "venue": "2024 IEEE 40th International Conference on Data Engineering (ICDE)"
    },
    {
      "citation_id": "11",
      "title": "Sola-gcl: Subgraphoriented learnable augmentation method for graph contrastive learning",
      "authors": [
        "T Peng",
        "X Li",
        "H Yuan",
        "Y Li",
        "H Xiong"
      ],
      "year": "2025",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "12",
      "title": "Microegrcl: An edge-attention-based graph neural network approach for root cause localization in microservice systems",
      "authors": [
        "R Chen",
        "J Ren",
        "L Wang",
        "Y Pu",
        "K Yang",
        "W Wu"
      ],
      "year": "2022",
      "venue": "International Conference on Service-Oriented Computing"
    },
    {
      "citation_id": "13",
      "title": "Eeg emotion recognition using dynamical graph convolutional neural networks",
      "authors": [
        "T Song",
        "W Zheng",
        "P Song",
        "Z Cui"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "14",
      "title": "An efficient lstm network for emotion recognition from multichannel eeg signals",
      "authors": [
        "X Du",
        "C Ma",
        "G Zhang",
        "J Li",
        "Y.-K Lai",
        "G Zhao",
        "X Deng",
        "Y.-J Liu",
        "H Wang"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "15",
      "title": "Transformers for eegbased emotion recognition: A hierarchical spatial information learning model",
      "authors": [
        "Z Wang",
        "Y Wang",
        "C Hu",
        "Z Yin",
        "Y Song"
      ],
      "year": "2022",
      "venue": "IEEE Sensors Journal"
    },
    {
      "citation_id": "16",
      "title": "Eeg conformer: Convolutional transformer for eeg decoding and visualization",
      "authors": [
        "Y Song",
        "Q Zheng",
        "B Liu",
        "X Gao"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Neural Systems and Rehabilitation Engineering"
    },
    {
      "citation_id": "17",
      "title": "Tagrec: Temporal-aware graph contrastive learning with theoretical augmentation for sequential recommendation",
      "authors": [
        "T Peng",
        "H Yuan",
        "Y Zhang",
        "Y Li",
        "P Dai",
        "Q Wang",
        "S Wang",
        "W Wu"
      ],
      "year": "2025",
      "venue": "IEEE Transactions on Knowledge and Data Engineering"
    },
    {
      "citation_id": "18",
      "title": "The emerging field of emotion regulation: An integrative review",
      "authors": [
        "J Gross"
      ],
      "year": "1998",
      "venue": "Review of general psychology"
    },
    {
      "citation_id": "19",
      "title": "A circumplex model of affect",
      "authors": [
        "J Russell"
      ],
      "year": "1980",
      "venue": "Journal of personality and social psychology"
    },
    {
      "citation_id": "20",
      "title": "Approaches, applications, and challenges in physiological emotion recognition-a tutorial overview",
      "authors": [
        "Y Can",
        "B Mahesh",
        "E André"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE"
    },
    {
      "citation_id": "21",
      "title": "Dual-encoder vae-gan with spatiotemporal features for emotional eeg data augmentation",
      "authors": [
        "C Tian",
        "Y Ma",
        "J Cammon",
        "F Fang",
        "Y Zhang",
        "M Meng"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Neural Systems and Rehabilitation Engineering"
    },
    {
      "citation_id": "22",
      "title": "Graph variational auto-encoder for deriving eeg-based graph embedding",
      "authors": [
        "T Behrouzi",
        "D Hatzinakos"
      ],
      "year": "2022",
      "venue": "Pattern Recognition"
    },
    {
      "citation_id": "23",
      "title": "Ganser: A self-supervised data augmentation framework for eeg-based emotion recognition",
      "authors": [
        "Z Zhang",
        "S.-H Zhong",
        "Y Liu"
      ],
      "year": "2021",
      "venue": "Ganser: A self-supervised data augmentation framework for eeg-based emotion recognition"
    },
    {
      "citation_id": "24",
      "title": "Emotion recognition based on eeg using generative adversarial nets and convolutional neural network",
      "authors": [
        "B Pan",
        "W Zheng"
      ],
      "year": "2021",
      "venue": "computational and Mathematical Methods in Medicine"
    },
    {
      "citation_id": "25",
      "title": "Contrastive learning of shared spatiotemporal eeg representations across individuals for naturalistic neuroscience",
      "authors": [
        "X Shen",
        "L Tao",
        "X Chen",
        "S Song",
        "Q Liu",
        "D Zhang"
      ],
      "year": "2024",
      "venue": "Contrastive learning of shared spatiotemporal eeg representations across individuals for naturalistic neuroscience",
      "arxiv": "arXiv:2402.14213"
    },
    {
      "citation_id": "26",
      "title": "Tsception: Capturing temporal dynamics and spatial asymmetry from eeg for emotion recognition",
      "authors": [
        "Y Ding",
        "N Robinson",
        "S Zhang",
        "Q Zeng",
        "C Guan"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "27",
      "title": "Eeg-based emotion recognition using regularized graph neural networks",
      "authors": [
        "P Zhong",
        "D Wang",
        "C Miao"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "28",
      "title": "Eeg emotion recognition using fusion model of graph convolutional neural networks and lstm",
      "authors": [
        "Y Yin",
        "X Zheng",
        "B Hu",
        "Y Zhang",
        "X Cui"
      ],
      "year": "2021",
      "venue": "Applied Soft Computing"
    },
    {
      "citation_id": "29",
      "title": "Ganser: A self-supervised data augmentation framework for eeg-based emotion recognition",
      "authors": [
        "Z Zhang",
        "Y Liu",
        "S.-H Zhong"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "30",
      "title": "Data augmentation for deep neural networks model in eeg classification task: a review",
      "authors": [
        "C He",
        "J Liu",
        "Y Zhu",
        "W Du"
      ],
      "year": "2021",
      "venue": "Frontiers in Human Neuroscience"
    },
    {
      "citation_id": "31",
      "title": "Curriculum learning",
      "authors": [
        "Y Bengio",
        "J Louradour",
        "R Collobert",
        "J Weston"
      ],
      "year": "2009",
      "venue": "Proceedings of the 26th annual international conference on machine learning"
    },
    {
      "citation_id": "32",
      "title": "Adaptaug: Adaptive data augmentation framework for multi-agent reinforcement learning",
      "authors": [
        "X Yu",
        "Y Tian",
        "L Wang",
        "P Feng",
        "W Wu",
        "R Shi"
      ],
      "year": "2024",
      "venue": "2024 IEEE International Conference on Robotics and Automation (ICRA)"
    },
    {
      "citation_id": "33",
      "title": "Leveraging partial symmetry for multi-agent reinforcement learning",
      "authors": [
        "X Yu",
        "R Shi",
        "P Feng",
        "Y Tian",
        "S Li",
        "S Liao",
        "W Wu"
      ],
      "year": "2024",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "34",
      "title": "Safe and efficient multi-agent collision avoidance with physics-informed reinforcement learning",
      "authors": [
        "P Feng",
        "R Shi",
        "S Wang",
        "J Liang",
        "X Yu",
        "S Li",
        "W Wu"
      ],
      "year": "2024",
      "venue": "IEEE Robotics and Automation Letters"
    },
    {
      "citation_id": "35",
      "title": "Hierarchical consensus-based multi-agent reinforcement learning for multi-robot cooperation tasks",
      "authors": [
        "P Feng",
        "J Liang",
        "S Wang",
        "X Yu",
        "X Ji",
        "Y Chen",
        "K Zhang",
        "R Shi",
        "W Wu"
      ],
      "year": "2024",
      "venue": "2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)"
    },
    {
      "citation_id": "36",
      "title": "Hybrid curriculum learning for emotion recognition in conversation",
      "authors": [
        "L Yang",
        "Y Shen",
        "Y Mao",
        "L Cai"
      ],
      "year": "2022",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "37",
      "title": "Motor imagery decoding using ensemble curriculum learning and collaborative training",
      "authors": [
        "G Zoumpourlis",
        "I Patras"
      ],
      "year": "2024",
      "venue": "2024 12th International Winter Conference on Brain-Computer Interface (BCI)"
    },
    {
      "citation_id": "38",
      "title": "Semantic-aware contrastive learning for electroencephalography-to-text generation with curriculum learning",
      "authors": [
        "X Feng",
        "X Feng",
        "B Qin"
      ],
      "year": "2023",
      "venue": "Semantic-aware contrastive learning for electroencephalography-to-text generation with curriculum learning",
      "arxiv": "arXiv:2301.09237"
    },
    {
      "citation_id": "39",
      "title": "Investigating critical frequency bands and channels for eeg-based emotion recognition with deep neural networks",
      "authors": [
        "W.-L Zheng",
        "B.-L Lu"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on autonomous mental development"
    },
    {
      "citation_id": "40",
      "title": "Iclabel: An automated electroencephalographic independent component classifier, dataset, and website",
      "authors": [
        "L Pion-Tonachini",
        "K Kreutz-Delgado",
        "S Makeig"
      ],
      "year": "2019",
      "venue": "NeuroImage"
    },
    {
      "citation_id": "41",
      "title": "Emotionmeter: A multimodal framework for recognizing human emotions",
      "authors": [
        "W.-L Zheng",
        "W Liu",
        "Y Lu",
        "B.-L Lu",
        "A Cichocki"
      ],
      "year": "2018",
      "venue": "IEEE transactions on cybernetics"
    },
    {
      "citation_id": "42",
      "title": "Deap: A database for emotion analysis; using physiological signals",
      "authors": [
        "S Koelstra",
        "C Muhl",
        "M Soleymani",
        "J.-S Lee",
        "A Yazdani",
        "T Ebrahimi",
        "T Pun",
        "A Nijholt",
        "I Patras"
      ],
      "year": "2011",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "43",
      "title": "Self-paced learning for latent variable models",
      "authors": [
        "M Kumar",
        "B Packer",
        "D Koller"
      ],
      "year": "2010",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "44",
      "title": "Selfpaced curriculum learning",
      "authors": [
        "L Jiang",
        "D Meng",
        "Q Zhao",
        "S Shan",
        "A Hauptmann"
      ],
      "year": "2015",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "45",
      "title": "Competence-based curriculum learning for neural machine translation",
      "authors": [
        "E Platanios",
        "O Stretcu",
        "G Neubig",
        "B Poczos",
        "T Mitchell"
      ],
      "year": "2019",
      "venue": "Competence-based curriculum learning for neural machine translation",
      "arxiv": "arXiv:1903.09848"
    },
    {
      "citation_id": "46",
      "title": "Curriculum learning by dynamic instance hardness",
      "authors": [
        "T Zhou",
        "S Wang",
        "J Bilmes"
      ],
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "47",
      "title": "Libeer: A comprehensive benchmark and algorithm library for eeg-based emotion recognition",
      "authors": [
        "H Liu",
        "S Yang",
        "Y Zhang",
        "M Wang",
        "F Gong",
        "C Xie",
        "G Liu",
        "Z Liu",
        "Y.-J Liu",
        "B.-L Lu"
      ],
      "year": "2024",
      "venue": "Libeer: A comprehensive benchmark and algorithm library for eeg-based emotion recognition",
      "arxiv": "arXiv:2410.09767"
    },
    {
      "citation_id": "48",
      "title": "Torcheegemo: A deep learning toolbox towards eeg-based emotion recognition",
      "authors": [
        "Z Zhang",
        "S.-H Zhong",
        "Y Liu"
      ],
      "year": "2024",
      "venue": "Expert Systems with Applications"
    },
    {
      "citation_id": "49",
      "title": "Gcb-net: Graph con-volutional broad network and its application in emotion recognition",
      "authors": [
        "T Zhang",
        "X Wang",
        "X Xu",
        "C Chen"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    }
  ]
}