{
  "paper_id": "2210.14716v1",
  "title": "Pretrained Audio Neural Networks For Speech Emotion Recognition In Portuguese",
  "published": "2022-10-26T13:48:51Z",
  "authors": [
    "Marcelo Matheus Gauy",
    "Marcelo Finger"
  ],
  "keywords": [
    "Speech emotion recognition",
    "Pretrained audio neural networks",
    "Transfer learning",
    "Transformers"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "The goal of speech emotion recognition (SER) is to identify the emotional aspects of speech. The SER challenge for Brazilian Portuguese speech was proposed with short snippets of Portuguese which are classified as neutral, non-neutral female and non-neutral male according to paralinguistic elements (laughing, crying, etc). This dataset contains about 50 minutes of Brazilian Portuguese speech. As the dataset leans on the small side, we investigate whether a combination of transfer learning and data augmentation techniques can produce positive results. Thus, by combining a data augmentation technique called SpecAugment, with the use of Pretrained Audio Neural Networks (PANNs) for transfer learning we are able to obtain interesting results. The PANNs (CNN6, CNN10 and CNN14) are pretrained on a large dataset called AudioSet containing more than 5000 hours of audio. They were finetuned on the SER dataset and the best performing model (CNN10) on the validation set was submitted to the challenge, achieving an ùêπ 1 score of 0.73 up from 0.54 from the baselines provided by the challenge. Moreover, we also tested the use of Transformer neural architecture, pretrained on about 600 hours of Brazilian Portuguese audio data. Transformers, as well as more complex models of PANNs (CNN14), fail to generalize to the test set in the SER dataset and do not beat the baseline. Considering the limitation of the dataset sizes, currently the best approach for SER is using PANNs (specifically, CNN6 and CNN10).",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Speech emotion recognition (SER) aims at identifying the emotional aspects of speech independently from the actual semantic content. SER can be used to identify the emotions of humans, e.g., when using mobile phones, an ability that may become crucial in improving human-machine interactions in the future  [1] . Several efforts to acquire speech data classified with different emotional labels have been undertaken  [2, 3, 4] . These datasets are typically small in size, even for languages such as English. In order to tackle these datasets, the use of transfer learning and data augmentation techniques may be instrumental.\n\nTransfer learning is the method of training a network on a particular problem where there is an abundance of data, with the goal of using the acquired knowledge to obtain better performance on a related problem with limited data available. Transfer learning has been effectively used in many fields of deep learning such as computer vision  [5]  and language modelling  [6] . Data augmentation is the method of increasing the amount of data available by slightly modifying copies of the data. This can be done, for example, by masking parts of the input or by adding Gaussian noise to it.\n\nIn this paper, we use transfer learning and data augmentation techniques to study SER in Brazilian Portuguese speech. We participate in the shared task SER challenge, a challenge for Brazilian Portuguese speech emotion recognition. This challenge made available a labeled dataset of 625 audio files as training set for SER. Moreover, a dataset of 308 files was made available as the test set. The training and test datasets consisted of short snippets of Brazilian Portuguese speech, usually less than 15 ùë† long, labeled neutral, non-neutral female and non-neutral male (non-neutral for audios containing laughs, cries, etc).\n\nFor transfer learning, we employ Pretrained audio neural network (PANN)  [7] , which are convolutional neural networks trained on a large dataset of audios (AudioSet  [8] ), consisting of 1.9 million audio clips distributed across 527 sound classes. By using the pretrained models made available by the developers, and finetuning on the SER dataset for Brazilian Portuguese speech, we are able to beat the proposed baselines of prosodic features and wav2vec features. We achieve (via CNN10) F1-score of 0.73, up from 0.54 from the baselines. During finetuning, we employ a data augmentation technique called SpecAugment  [9] .\n\nWe also tested the use of Transformer neural networks, pretrained on a large amount of Brazilian Portuguese audio data  [10] . However, we find that, with the current amount of available data for SER, Transformers do not generalize their training performance to the validation and test sets. This holds even while using most common techniques to prevent overfitting. The same behaviour was also observed for more complex PANNs, such as CNN14.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Related Work",
      "text": "There is a large literature on SER in English  [11, 12, 13, 14, 15, 16, 17, 18] . Moreover, there are a lot of small datasets for SER in English, such as, RAVDESS  [2] , SAVEE  [3]  and IEMOCAP  [4] . To the best of our knowledge, the SER dataset for Brazilian Portuguese speech is the only available dataset on the language. In addition, English datasets are usually classified in a different set of labels. RAVDESS  [2] , for example, has the classes of calm, happy, angry, sad, fearful, surprise and disgust. This contrasts with the classes of neutral, non-neutral female and non-neutral male present in the SER dataset for Brazilian Portuguese speech. As such, comparisons of our work with the state of the art in English language are not really possible. Nevertheless, the authors of  [18] , the most recent work, obtain an average recall on RAVDESS of 84.3 percent using wav2vec 2.0  [19] . On IEMOCAP, they obtain an average recall of 67.2 percent, also using wav2vec 2.0.\n\nTransfer learning is a very common technique in situations where the dataset available is small in size. It has been effectively employed in computer vision  [5, 20] , language modelling  [6, 21]  and audio tasks  [7, 22, 18] . In the original PANN paper  [7] , authors propose several convolutional neural networks pretrained on AudioSet which can be finetuned on other smaller datasets. In  [18]  the authors use wav2vec 2.0 pretrained on Librispeech and finetuned on either RAVDESS or IEMOCAP for speech emotion recognition. Finally, in  [22]  the authors provide a comprehensive review on transfer learning methods used for speech and language processing tasks.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Methodology",
      "text": "The code for this paper can be found at GitHub. Below we describe the dataset and architectures used.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Ser Dataset",
      "text": "To perform SER on Brazilian Portuguese speech, we use the training dataset (CORAA SER version 1.0) provided for the challenge. This dataset was built from the C-ORAL-BRASIL I corpus  [23] , with 625 audio files, typically less than 15ùë†-long, containing informal spontaneous Brazilian Portuguese speech. These audio files are labeled neutral, non-neutral female, non-neutral male. An audio is labeled non-neutral male if it is a male speaker and it contains paralinguistic elements in the speech (such as laughing, crying, etc). Similarly, an audio is labeled non-neutral female if it is a female speaker and the speech contains such paralinguistic elements. We split the official training dataset into training (80%), validation (10%) and test sets (10%). The split was done in an arbitrary way to ensure that the three datasets were balanced (i.e. contained relatively the same proportion of neutral, non-neutral female and non-neutral male files). The training dataset consisted of 500 files, the validation dataset consisted of 63 files and the test set of 62 files. The results we report are for the validation and test set performance.\n\nAs the official test dataset made available did not have labels, we have labeled it ourselves, out of curiosity and to enable more consistent tests of the performance of the networks. While the labels may not be perfect, they provide a close enough picture, so the performance of the models can be measured as an average over multiple experiments (as we were observing high variance). As such, we also provide results for the official test set with our unofficial labels. We stress that we did not use the test set labels for any form of model or parameter selection.\n\nLastly, the PANNs we use have been trained on the AudioSet  [8]  dataset containing more than 5000 hours of audio distributed across 527 classes.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Pann Architectures",
      "text": "Table  1  describes the three architectures we use. They are named CNN6, CNN10 and CNN14 after the 6-layer, 10-layer and 14-layer CNNs they represent. These are the same\n\nCNN network architectures used in  [7] . We take their pretrained models on AudioSet  [8]  to allow us to obtain better generalization performances on the SER dataset.\n\nThe audios are preprocessed in the following way: the audios are first resampled to 32ùëòùêªùëß. After that, we apply short-time Fourier transform  [24]  (with a window size of 1024 frames and hop size of 320 frames) to the standard time-domain waveforms to obtain spectrograms. Then, Mel filter banks are applied to spectrograms, followed by a logarithm operation to obtain log Mel spectrograms. These preprocessing steps are commonly done when using CNNs for audio  [25, 26] .\n\nAs described in Table  1 , the CNN architectures used are composed of convolutional layers with kernel 5 √ó 5 for CNN6, and 3 √ó 3 for CNN10 and CNN14. Each convolutional layer is followed by batch normalization  [27]  and ReLU non-linearity  [28]  is used to allow for better training convergence. Each such convolutional block is present 4 times in CNN6 and, in between, an average pooling 2 √ó 2 layer is applied (average pooling is observed to be better than max pooling  [29] ). In CNN10 and CNN14, the convolutional blocks are always used in pairs before an average pooling layer is applied. CNN10 contains 8 such convolutional blocks (4 pairs) and CNN14 contains 12 such convolutional blocks (6 pairs). All networks have a penultimate fully connected layer to add extra representation ability, as well as a final 527 units fully connected layer where a sigmoid is applied to obtain the probabilities for each class. In Table  1 , the first line describes the input of the networks, that is, ùëõ frames of Log Mel Spectrogram with 64 mel bins for each frame. Each subsequent line represents a layer of the networks. The numbers following the @ sign represent the quantity of 5 √ó 5 or 3 √ó 3 feature maps used.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Transformer Encoder Architecture",
      "text": "In addition to experimenting with the PANNs, we also attempt to extract good performances from Transformers. The Transformer architecture we use is equivalent to the Transformer Encoder architecture from  [30] . That is, we use a three-layer Transformer with multi-head self attention. Each encoder layer is composed of two sub-layers. The first is a multi-head self-attention network and the second is a fully connected feed-forward layer. Each sub-layer has a residual connection followed by layer normalization  [31] . The encoder layers and sub-layers produce outputs of dimension ùëë (in experiments ùëë is either 128 or 512). The fully connected feed forward network within each encoder layer has an inner dimension of 4ùëë. We feed the Transformer Encoders the MFCC-gram of the audios, with each token fed to the Transformer corresponding to a frame of the MFCC-gram  [32] . We name these Transformers, the MFCC-gram Transformers  [32] . We use sinusoidal positional encoding so the Transformer has access to the order of the sequence fed  [30, 33] . The input frames are projected linearly to a hidden layer of dimension ùëë, as direct addition of acoustic features to positional encoding may lead to training failure  [33] .\n\nTypically, Transformers undergo two training phases: pretraining and finetuning. In the pretraining phase, we make use of a technique called Time alteration  [33]  to pretrain the Transformer in about 600 hours of Brazilian Portuguese audio data (in other words, we use pretrained models from  [10] ). Time alteration is a technique that masks random spans of frames of the MFCC-gram similarly to how time masking functions in SpecAugment (described in subsection 3.4). During pretraining, the model is trained to reconstruct the masked frames. For Brazilian Portuguese audio data, we use the corpora of NURC-S√£o Paulo  [34] , NURC-Recife  [35] , ALIP  [36] , SP2010  [37]  and Programa Certas Palavras  [38] . In the experiments, we also show the performance of Transformers which do not undergo pretraining, that is, which we initialize at random and do finetuning directly. We name those Transformers the Baseline MFCC-gram Transformers. After pretraining, the Transformers are finetuned on the SER dataset.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Data Augmentation: Specaugment",
      "text": "The SER training dataset used for the challenge leans on the small side and contains about 50 minutes of audio. To mitigate the potential overfitting effects of a small training dataset, we perform a common audio data augmentation technique called SpecAugment  [9]  on the Mel spectrogram (or MFCC-gram) of the audio files before feeding it to the network's layers. SpecAugment consists in masking random spans of consecutive segments of the spectrogram of the audios. Masking can be done along the time dimension (that is, on spans of consecutive frames), or along the frequency dimension (that is, on spans of consecutive frequency channels). Following  [7] , time masking is done by selecting a uniform length ‚Ñì (chosen between 0 and 64) and a uniform frame start ùë° (chosen between 0 and ùëá -‚Ñì, where ùëá is the total number of frames of the audio) and proceeding to mask the frames from ùë° to ùë° + ‚Ñì -1. We mask two such blocks of consecutive frames. Frequency masking is similar to time masking but done along the frequency dimension. So, a random uniform length ‚Ñì is chosen (between 0 and 8) and a uniform frequency band ùëì is chosen (between 0 and ùêπ -‚Ñì where ùêπ is the total number of Mel frequency bins). The frequency bands from ùëì to ùëì + ‚Ñì -1 are masked to zero. As with time masking, we mask two such blocks of consecutive frequency bands.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Results And Discussion",
      "text": "We will check the performance of the three proposed PANNs (CNN6, CNN10 and CNN14) on the SER training and test datasets. In order to take advantage of the large pretraining done on the AudioSet  [8]  dataset, we will use the pretrained models of CNN6, CNN10 and CNN14 made available by the authors of  [7] . These can be found in Zenodo. These pretrained models will be finetuned on the SER training dataset in order to achieve better performance than the baseline.\n\nMoreover, to showcase the massive level of transfer learning that is happening via the pretrained models, we will show the performance of the three networks (CNN6, CNN10 and CNN14) without the use of a pretrained model, that is, initializing their weights at random and not making use of the AudioSet  [8]  pretraining. We call these three models the Baseline CNN6, the Baseline CNN10 and the Baseline CNN14.\n\nLastly, we show the performance of three Transformers models. We analyze MFCCgram Transformers pretrained on about 600 hours of Brazilian Portuguese audio data, as well as, Baseline MFCC-gram Transformers (without pretraining) containing 512 and 128 units per Encoder layer.\n\nAs mentioned before, the SER training dataset is split into a training (80%), validation (10%) and test sets (10%). In Table  2 , we report the ùêπ 1 score performance of the nine models in the validation and test datasets as well as in the official dataset (which was labeled by us). The results in the table are averaged across 25 experiments, to better control the generally high ùêπ 1 score variance between different experiments. Each experiment consisted of training the model for 100 epochs for CNNs and 20 epochs for Transformers 1  in the training set and the best validation performance model (checked after each epoch) was saved and later analyzed on the test set and official test set. The batch size used was 16 and the learning rate was 10 -4 for the CNNs and we use a warmup learning rate schedule according to the formula ùëë -0.5 √ó ùëöùëñùëõ(ùë†ùë°ùëíùëùùëõùë¢ùëöùëèùëíùëü -0.5 , ùë†ùë°ùëíùëùùëõùë¢ùëöùëèùëíùëü √ó ùë§ùëéùëüùëöùë¢ùëùùë†ùë°ùëíùëùùë† -1.5 ) for the Transformers as is standard  [6] . We use ùë§ùëéùëüùëöùë¢ùëùùë†ùë°ùëíùëùùë† = 4000.\n\nAs can be seen on Table  2 , the best results in the test set were attained by the CNN6 (0.62 ùêπ 1 score). Moreover, it seems that the test set built by us was inherently harder than the official test set. In the official test set, the best result was obtained by CNN10 (0.74 ùêπ 1 score), in line with it achieving also the best results on the validation set.\n\nWe observe that CNN14's performance was significantly worse both on validation and test. However, representation ability wise it is the most powerful of the PANN models. It is likely that the SER dataset being so small meant CNN14 suffered from overfitting.\n\nWe also experienced overfitting issues when attempting MFCC-gram Transformers based models. There, using pretraining techniques did not yield better performance. This is likely because the pretraining data contained primarily voice, without laughs or cries, so the important markers were not present in pretrained data. Moreover, no common technique (such as dropout  [39] , L1 or L2 regularization  [40] , data augmentation techniques as SpecAugment  [9]  and Mixup  [41] ) to prevent overfitting yielded good results. It seems that the reduced size of the SER dataset is currently hindering performance in more complex networks, so a likely way of dramatically improving results would be to increase the size of the available dataset.\n\nLastly, note that the three baseline PANN models are far away from beating the baselines provided by the challenge. There is noticeable transfer learning benefit in using the pretrained models on AudioSet  [8] . This large difference illustrates again the fact that the SER dataset is so small (50 minutes of audio) and that these networks suffer to generalize on it.\n\nWe have sent for evaluation in the challenge, the model which attained best test performance (a CNN6 which officially reported 0.66 ùêπ 1-score) and the model which attained best validation performance (a CNN10 which officially reported 0.73 ùêπ 1-score). Moreover, out of curiosity, we show the confusion matrix of the CNN10 model sent for evaluation in Table  3 . Observe that the model classifies the vast majority of neutral and non-neutral females files correctly. Most of the errors are done classifying non-neutral male files (often wrongly classified as neutral).",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Conclusion",
      "text": "In this paper, we have effectively used transfer learning to beat the proposed baselines in the shared task SER challenge in Brazilian Portuguese speech. By using, the PANNs CNN6 and CNN10, we have attained ùêπ 1 score of 0.73 up from 0.54 from the baselines. We have also observed that more complex networks, such as CNN14 and Transformers, while being in theory more capable of attaining better performances, suffer from overfitting. As such, we determine that probably the best way of improving results is by increasing the size of the training set.\n\nFuture work could involve increasing the size of the training set so that Transformers and CNN14 generalize their training performances to the test set. In addition, pretraining Transformers with audio data containing specifically laughs, cries and so on may prove useful. Moreover, other data augmentation techniques could be used which might provide additional benefit in terms of preventing overfitting.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Table 2",
      "text": "The mean and standard deviation of the ùêπ 1 score is shown in the table below for the nine models (CNN6, CNN10 and CNN14 and their respective baseline version, i.e., their versions without pretraining on AudioSet  [8] , as well as MFCC-gram Transformers with and without pretraining and a smaller version of MFCC-gram Transformers). The results shown are for the validation set, the test set and the official test set. Labels for the official test set were created by us.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Model",
      "text": "",
      "page_start": 8,
      "page_end": 8
    }
  ],
  "figures": [],
  "tables": [
    {
      "caption": "Table 1: PANN architectures. We describe the layers of CNN6, CNN10 and CNN14.",
      "data": [
        {
          "CNN6\nCNN10\nCNN14": "Log Mel Spectrogram n frames √ó 64 mel bins"
        },
        {
          "CNN6\nCNN10\nCNN14": "(Ô∏Å 5√ó5@64\n)Ô∏Å\nBN,ReLU"
        },
        {
          "CNN6\nCNN10\nCNN14": "Avg Pooling 2√ó2"
        },
        {
          "CNN6\nCNN10\nCNN14": "(Ô∏Å 5√ó5@128\n)Ô∏Å\nBN,ReLU"
        },
        {
          "CNN6\nCNN10\nCNN14": "Avg Pooling 2√ó2"
        },
        {
          "CNN6\nCNN10\nCNN14": "(Ô∏Å 5√ó5@256\n)Ô∏Å\nBN,ReLU"
        },
        {
          "CNN6\nCNN10\nCNN14": "Avg Pooling 2√ó2"
        },
        {
          "CNN6\nCNN10\nCNN14": "(Ô∏Å 5√ó5@512\n)Ô∏Å\nBN,ReLU"
        },
        {
          "CNN6\nCNN10\nCNN14": "Global Avg Pool-\ning"
        },
        {
          "CNN6\nCNN10\nCNN14": "FC 512, ReLU"
        },
        {
          "CNN6\nCNN10\nCNN14": "FC 527, Sigmoid"
        },
        {
          "CNN6\nCNN10\nCNN14": ""
        },
        {
          "CNN6\nCNN10\nCNN14": ""
        },
        {
          "CNN6\nCNN10\nCNN14": ""
        },
        {
          "CNN6\nCNN10\nCNN14": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 2: The mean and standard deviation of the ùêπ1 score is shown in the table below for the nine models",
      "data": [
        {
          "Model": "Baseline\nCNN6",
          "F 1 score Validation per-\nformance": "0.45 ¬± 0.06",
          "F 1\nscore\nTest\nperfor-\nmance": "0.36 ¬± 0.05",
          "F 1\nscore Official\ntest\nperformance": "0.33 ¬± 0.03"
        },
        {
          "Model": "Baseline\nCNN10",
          "F 1 score Validation per-\nformance": "0.58 ¬± 0.06",
          "F 1\nscore\nTest\nperfor-\nmance": "0.41 ¬± 0.09",
          "F 1\nscore Official\ntest\nperformance": "0.42 ¬± 0.05"
        },
        {
          "Model": "Baseline\nCNN14",
          "F 1 score Validation per-\nformance": "0.38 ¬± 0.06",
          "F 1\nscore\nTest\nperfor-\nmance": "0.33 ¬± 0.04",
          "F 1\nscore Official\ntest\nperformance": "0.32 ¬± 0.03"
        },
        {
          "Model": "CNN6",
          "F 1 score Validation per-\nformance": "0.78 ¬± 0.05",
          "F 1\nscore\nTest\nperfor-\nmance": "0.62 ¬± 0.06",
          "F 1\nscore Official\ntest\nperformance": "0.69 ¬± 0.04"
        },
        {
          "Model": "CNN10",
          "F 1 score Validation per-\nformance": "0.80 ¬± 0.06",
          "F 1\nscore\nTest\nperfor-\nmance": "0.57 ¬± 0.06",
          "F 1\nscore Official\ntest\nperformance": "0.74 ¬± 0.04"
        },
        {
          "Model": "CNN14",
          "F 1 score Validation per-\nformance": "0.61 ¬± 0.11",
          "F 1\nscore\nTest\nperfor-\nmance": "0.54 ¬± 0.06",
          "F 1\nscore Official\ntest\nperformance": "0.52 ¬± 0.10"
        },
        {
          "Model": "MFCC-gram\nTransformers\n512 units",
          "F 1 score Validation per-\nformance": "0.50 ¬± 0.04",
          "F 1\nscore\nTest\nperfor-\nmance": "0.36 ¬± 0.06",
          "F 1\nscore Official\ntest\nperformance": "0.38 ¬± 0.03"
        },
        {
          "Model": "Baseline\nMFCC-gram\nTransformers\n512 units",
          "F 1 score Validation per-\nformance": "0.57 ¬± 0.04",
          "F 1\nscore\nTest\nperfor-\nmance": "0.43 ¬± 0.08",
          "F 1\nscore Official\ntest\nperformance": "0.43 ¬± 0.06"
        },
        {
          "Model": "Baseline\nMFCC-gram\nTransformers\n128 units",
          "F 1 score Validation per-\nformance": "0.60 ¬± 0.05",
          "F 1\nscore\nTest\nperfor-\nmance": "0.45 ¬± 0.07",
          "F 1\nscore Official\ntest\nperformance": "0.44 ¬± 0.04"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table 2: The mean and standard deviation of the ùêπ1 score is shown in the table below for the nine models",
      "data": [
        {
          "Confusion Ma-\ntrix": "Neutral",
          "predicted neutral": "244",
          "predicted\nnon-neutral\nmale": "2",
          "predicted non-neutral\nfe-\nmale": "5"
        },
        {
          "Confusion Ma-\ntrix": "Non-neutral\nmale",
          "predicted neutral": "14",
          "predicted\nnon-neutral\nmale": "8",
          "predicted non-neutral\nfe-\nmale": "2"
        },
        {
          "Confusion Ma-\ntrix": "Non-neutral fe-\nmale",
          "predicted neutral": "6",
          "predicted\nnon-neutral\nmale": "1",
          "predicted non-neutral\nfe-\nmale": "26"
        }
      ],
      "page": 8
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Endowing spoken language dialogue systems with emotional intelligence",
      "authors": [
        "E Andre",
        "M Rehm",
        "W Minker",
        "D B√ºhler"
      ],
      "year": "2004",
      "venue": "Tutorial and Research Workshop on Affective Dialogue Systems"
    },
    {
      "citation_id": "2",
      "title": "The ryerson audio-visual database of emotional speech and song (ravdess): A dynamic, multimodal set of facial and vocal expressions in north american english",
      "authors": [
        "S Livingstone",
        "F Russo"
      ],
      "year": "2018",
      "venue": "PloS one"
    },
    {
      "citation_id": "3",
      "title": "Machine Audition: Principles, Algorithms and Systems: Principles, Algorithms and Systems",
      "authors": [
        "W Wang"
      ],
      "year": "2010",
      "venue": "Machine Audition: Principles, Algorithms and Systems: Principles, Algorithms and Systems"
    },
    {
      "citation_id": "4",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "5",
      "title": "Deep learning for computer vision: A brief review, Computational intelligence and neuroscience",
      "authors": [
        "A Voulodimos",
        "N Doulamis",
        "A Doulamis",
        "E Protopapadakis"
      ],
      "year": "2018",
      "venue": "Deep learning for computer vision: A brief review, Computational intelligence and neuroscience"
    },
    {
      "citation_id": "6",
      "title": "Pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "J Devlin",
        "M.-W Chang",
        "K Lee",
        "K Toutanova"
      ],
      "year": "2018",
      "venue": "Pre-training of deep bidirectional transformers for language understanding",
      "arxiv": "arXiv:1810.04805"
    },
    {
      "citation_id": "7",
      "title": "Panns: Largescale pretrained audio neural networks for audio pattern recognition",
      "authors": [
        "Q Kong",
        "Y Cao",
        "T Iqbal",
        "Y Wang",
        "W Wang",
        "M Plumbley"
      ],
      "year": "2020",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "8",
      "title": "Audio set: An ontology and human-labeled dataset for audio events",
      "authors": [
        "J Gemmeke",
        "D Ellis",
        "D Freedman",
        "A Jansen",
        "W Lawrence",
        "R Moore",
        "M Plakal",
        "M Ritter"
      ],
      "year": "2017",
      "venue": "2017 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "9",
      "title": "A simple data augmentation method for automatic speech recognition",
      "authors": [
        "D Park",
        "W Chan",
        "Y Zhang",
        "C.-C Chiu",
        "B Zoph",
        "E Cubuk",
        "Q Le"
      ],
      "year": "2019",
      "venue": "A simple data augmentation method for automatic speech recognition",
      "arxiv": "arXiv:1904.08779"
    },
    {
      "citation_id": "10",
      "title": "Acoustic models for brazilian portuguese speech based on neural transformers, IN PREPARATION",
      "authors": [
        "M Gauy",
        "M Finger"
      ],
      "year": "2022",
      "venue": "Acoustic models for brazilian portuguese speech based on neural transformers, IN PREPARATION"
    },
    {
      "citation_id": "11",
      "title": "Real-time speech emotion recognition using a pre-trained image classification network: Effects of bandwidth reduction and companding",
      "authors": [
        "M Lech",
        "M Stolar",
        "C Best",
        "R Bolia"
      ],
      "year": "2020",
      "venue": "Frontiers in Computer Science"
    },
    {
      "citation_id": "12",
      "title": "Speech emotion recognition using multi-hop attention mechanism",
      "authors": [
        "S Yoon",
        "S Byun",
        "S Dey",
        "K Jung"
      ],
      "year": "2019",
      "venue": "ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "13",
      "title": "Learning alignment for multimodal emotion recognition from speech",
      "authors": [
        "H Xu",
        "H Zhang",
        "K Han",
        "Y Wang",
        "Y Peng",
        "X Li"
      ],
      "year": "2019",
      "venue": "Learning alignment for multimodal emotion recognition from speech",
      "arxiv": "arXiv:1909.05645"
    },
    {
      "citation_id": "14",
      "title": "Multimodal speech emotion recognition using audio and text",
      "authors": [
        "S Yoon",
        "S Byun",
        "K Jung"
      ],
      "year": "2018",
      "venue": "2018 IEEE Spoken Language Technology Workshop (SLT)"
    },
    {
      "citation_id": "15",
      "title": "Efficient emotion recognition from speech using deep learning on spectrograms",
      "authors": [
        "A Satt",
        "S Rozenberg",
        "R Hoory"
      ],
      "year": "2017",
      "venue": "Efficient emotion recognition from speech using deep learning on spectrograms"
    },
    {
      "citation_id": "16",
      "title": "Speech emotion recognition with deep convolutional neural networks",
      "authors": [
        "D Issa",
        "M Demirci",
        "A Yazici"
      ],
      "year": "2020",
      "venue": "Biomedical Signal Processing and Control"
    },
    {
      "citation_id": "17",
      "title": "Efficient speech emotion recognition using multiscale cnn and attention",
      "authors": [
        "Z Peng",
        "Y Lu",
        "S Pan",
        "Y Liu"
      ],
      "year": "2021",
      "venue": "ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "18",
      "title": "Emotion recognition from speech using wav2vec 2.0 embeddings",
      "authors": [
        "L Pepino",
        "P Riera",
        "L Ferrer"
      ],
      "year": "2021",
      "venue": "Emotion recognition from speech using wav2vec 2.0 embeddings",
      "arxiv": "arXiv:2104.03502"
    },
    {
      "citation_id": "19",
      "title": "wav2vec 2.0: A framework for selfsupervised learning of speech representations",
      "authors": [
        "A Baevski",
        "Y Zhou",
        "A Mohamed",
        "M Auli"
      ],
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "20",
      "title": "Imagenet classification with deep convolutional neural networks",
      "authors": [
        "A Krizhevsky",
        "I Sutskever",
        "G Hinton"
      ],
      "year": "2012",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "21",
      "title": "Language models are few-shot learners",
      "authors": [
        "T Brown",
        "B Mann",
        "N Ryder",
        "M Subbiah",
        "J Kaplan",
        "P Dhariwal",
        "A Neelakantan",
        "P Shyam",
        "G Sastry",
        "A Askell"
      ],
      "year": "2020",
      "venue": "Language models are few-shot learners",
      "arxiv": "arXiv:2005.14165"
    },
    {
      "citation_id": "22",
      "title": "Transfer learning for speech and language processing",
      "authors": [
        "D Wang",
        "T Zheng"
      ],
      "year": "2015",
      "venue": "Transfer learning for speech and language processing"
    },
    {
      "citation_id": "23",
      "title": "Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA)",
      "year": "2015",
      "venue": "Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA)"
    },
    {
      "citation_id": "24",
      "title": "The c-oral-brasil i: reference corpus for informal spoken brazilian portuguese",
      "authors": [
        "T Raso",
        "H Mello"
      ],
      "year": "2012",
      "venue": "International Conference on Computational Processing of the Portuguese Language"
    },
    {
      "citation_id": "25",
      "title": "The fast fourier transform",
      "authors": [
        "E Brigham",
        "R Morrow"
      ],
      "year": "1967",
      "venue": "IEEE spectrum"
    },
    {
      "citation_id": "26",
      "title": "Automatic tagging using deep convolutional neural networks",
      "authors": [
        "K Choi",
        "G Fazekas",
        "M Sandler"
      ],
      "year": "2016",
      "venue": "Automatic tagging using deep convolutional neural networks",
      "arxiv": "arXiv:1606.00298"
    },
    {
      "citation_id": "27",
      "title": "Weakly labelled audioset tagging with attention neural networks",
      "authors": [
        "Q Kong",
        "C Yu",
        "Y Xu",
        "T Iqbal",
        "W Wang",
        "M Plumbley"
      ],
      "year": "2019",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "28",
      "title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift",
      "authors": [
        "S Ioffe",
        "C Szegedy"
      ],
      "year": "2015",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "29",
      "title": "Rectified linear units improve restricted boltzmann machines",
      "authors": [
        "V Nair",
        "G Hinton"
      ],
      "year": "2010",
      "venue": "Rectified linear units improve restricted boltzmann machines"
    },
    {
      "citation_id": "30",
      "title": "Cross-task learning for audio tagging, sound event detection and spatial localization",
      "authors": [
        "Q Kong",
        "Y Cao",
        "T Iqbal",
        "Y Xu",
        "W Wang",
        "M Plumbley"
      ],
      "year": "2019",
      "venue": "Dcase 2019 baseline systems",
      "arxiv": "arXiv:1904.03476"
    },
    {
      "citation_id": "31",
      "title": "Attention is all you need",
      "authors": [
        "A Vaswani",
        "N Shazeer",
        "N Parmar",
        "J Uszkoreit",
        "L Jones",
        "A Gomez",
        "≈Å Kaiser",
        "I Polosukhin"
      ],
      "year": "2017",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "32",
      "title": "Layer normalization",
      "authors": [
        "J Ba",
        "J Kiros",
        "G Hinton"
      ],
      "year": "2016",
      "venue": "Layer normalization",
      "arxiv": "arXiv:1607.06450"
    },
    {
      "citation_id": "33",
      "title": "Audio mfcc-gram transformers for respiratory insufficiency detection in covid-19",
      "authors": [
        "M Gauy",
        "M Finger"
      ],
      "venue": "STIL 2021 ("
    },
    {
      "citation_id": "34",
      "title": "Mockingjay: Unsupervised speech representation learning with deep bidirectional transformer encoders",
      "authors": [
        "A Liu",
        "S -W. Yang",
        "P.-H Chi",
        "P.-C Hsu",
        "H.-Y Lee"
      ],
      "year": "2020",
      "venue": "ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "35",
      "title": "A linguagem falada culta na cidade de s√£o paulo: materiais para seu estudo",
      "authors": [
        "A Castilho",
        "D Pretti"
      ],
      "year": "1986",
      "venue": "A linguagem falada culta na cidade de s√£o paulo: materiais para seu estudo"
    },
    {
      "citation_id": "36",
      "title": "Nurc digital um protocolo para a digitaliza√ß√£o, anota√ß√£o, arquivamento e dissemina√ß√£o do material do projeto da norma urbana lingu√≠stica culta (nurc), CHIMERA: Revista de Corpus de",
      "authors": [
        "M Oliviera"
      ],
      "year": "2016",
      "venue": "Lenguas Romances y Estudios Ling√º√≠sticos"
    },
    {
      "citation_id": "37",
      "title": "Projeto alip (amostra lingu√≠stica do interior paulista) e banco de dados iboruna: 10 anos de contribui√ß√£o com a descri√ß√£o do portugu√™s brasileiro",
      "authors": [
        "S Gon√ßalves"
      ],
      "year": "1978",
      "venue": "Projeto alip (amostra lingu√≠stica do interior paulista) e banco de dados iboruna: 10 anos de contribui√ß√£o com a descri√ß√£o do portugu√™s brasileiro"
    },
    {
      "citation_id": "38",
      "title": "Projeto sp2010: Amostra da fala paulistana",
      "authors": [
        "R Mendes"
      ],
      "year": "2013",
      "venue": "Projeto sp2010: Amostra da fala paulistana"
    },
    {
      "citation_id": "39",
      "title": "Acervo Certas Palavras-Cat√°logo",
      "authors": [
        "C Teixeira"
      ],
      "year": "1981",
      "venue": "Acervo Certas Palavras-Cat√°logo"
    },
    {
      "citation_id": "40",
      "title": "Dropout: a simple way to prevent neural networks from overfitting",
      "authors": [
        "N Srivastava",
        "G Hinton",
        "A Krizhevsky",
        "I Sutskever",
        "R Salakhutdinov"
      ],
      "year": "2014",
      "venue": "The journal of machine learning research"
    },
    {
      "citation_id": "41",
      "title": "Deep learning",
      "authors": [
        "I Goodfellow",
        "Y Bengio",
        "A Courville"
      ],
      "year": "2016",
      "venue": "Deep learning"
    },
    {
      "citation_id": "42",
      "title": "mixup: Beyond empirical risk minimization",
      "authors": [
        "H Zhang",
        "M Cisse",
        "Y Dauphin",
        "D Lopez-Paz"
      ],
      "year": "2017",
      "venue": "mixup: Beyond empirical risk minimization",
      "arxiv": "arXiv:1710.09412"
    }
  ]
}