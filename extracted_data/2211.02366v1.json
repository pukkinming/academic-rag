{
  "paper_id": "2211.02366v1",
  "title": "U N P U B L I S H E D W O R K I N G D R A F T . Speaker Vgg Cct: Cross-Corpus Speech Emotion Recognition With Speaker Embedding And Vision Transformers",
  "published": "2022-11-04T10:49:44Z",
  "authors": [
    "A. Arezzo",
    "S. Berretti"
  ],
  "keywords": [
    "Speech emotion recognition",
    "spectrograms",
    "visual transformers",
    "compact convolutional transformers",
    "speaker embedding",
    "crosscorpus test"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "In recent years, Speech Emotion Recognition (SER) has been investigated mainly transforming the speech signal into spectrograms that are then classified using Convolutional Neural Networks pretrained on generic images and fine tuned with spectrograms. In this paper, we start from the general idea above and develop a new learning solution for SER, which is based on Compact Convolutional Transformers (CCTs) combined with a speaker embedding. With CCTs, the learning power of Vision Transformers (ViT) is combined with a diminished need for large volume of data as made possible by the convolution. This is important in SER, where large corpora of data are usually not available. The speaker embedding allows the network to extract an identity representation of the speaker, which is then integrated by means of a self-attention mechanism with the features that the CCT extracts from the spectrogram. Overall, the solution is capable of operating in real-time showing promising results in a cross-corpus scenario, where training and test datasets are kept separate. Experiments have been performed on several benchmarks in a cross-corpus setting as rarely used in the literature, with results that are comparable or superior to those obtained with state-of-the-art network architectures. Our code is available at https://github.com/JabuMlDev/Speaker-VGG-CCT \n CCS CONCEPTS ‚Ä¢ Information systems ‚Üí Speech / audio search; ‚Ä¢ Computing methodologies ‚Üí Speech recognition.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "The human-machine communication reached truly remarkable achievements so far. For example, an impressive progress has been achieved in the conversion of human speech signals in text, with a growing variety of voice assistants now available on the market for disparate uses that go from home automation to the automotive one. Despite of these advances, an obstacle that does not allow the man-machine interaction to be completely indistinguishable from what occurs between two people is that machines still are not ready to fully understand the emotion of the interlocutor. This constitutes a major limitation given the importance that emotions play in dialogues between two human beings.\n\nMotivated by these premises, recognizing the emotion of a speaker from her/his audio track is a task which is receiving an increasing attention. In particular, recent developments in the field of realtime SER exploited the advances in deep learning and transformed SER to an image classification task  [17] . In these works, the audio track is usually transformed into a spectrogram image, which is then classified using Convolutional Neural Networks (CNN) or a concatenation of CNNs and Long-Short Term Memory (LSTM) networks  [26] . However, there are several unsolved problems that still make this task very challenging such as the lack of usable data in the desired language, the difficulty in obtaining from an audio track features that, at the same time, can be extracted in real-time and characterize the emotion, and the dependence between the representation of the emotion and the subject that expresses it. Further, most of the studies focused on the in-corpus scenario, while the more realistic cross-corpus protocol, where training and test data are completely separated has been rarely tested with.\n\nDeveloping on the above considerations, in this paper we aimed to overcome some of the aforementioned difficulties, and developed an approach capable of predicting, in real-time, the emotion of a person starting from her/his speech track. One main contribution of this work is that of using Vision Transformers (ViTs)  [9]  in place of traditional CNN for SER. ViTs proved their effectiveness in vision related tasks when used alone or in combination with CNNs, achieving performance that can surpass CNNs in many image classification tasks  [8, 37] , but they did not find yet application to SER. To this end, we used Compact Convolutional Transformers (CCTs)  [12] , which represent one variant of the ViT that is simpler to train when example data are scarce. Subsequently, we extended this model to the Speaker VGG CCT that exploits the ViT self-attention mechanism in an architecture that makes use of a representation of the speakers (speaker embedding) that partially compensates for the fact that each person potentially expresses emotion in a different way. As a further contribution, we target a cross-corpus scenario. To validate the proposed approach, we performed a comprehensive set of experiments on the most commonly used speech datasets. First, we presented results for the challenging cross-corpus classification protocol and compared our method with state-of-the-art CNN image classification architectures fine-tuned for SER. Then, we also performed an in-corpus experiment to make our approach comparable with state-of-the-art solutions.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Related Work",
      "text": "Voice signals with associated different emotions are characterized by changes in frequency and intensity of their waveform  [2] . Therefore, several works focused on capturing those variations using different discriminating acoustic features. Until few years ago, naive (i.e., hand-crafted) features were proposed in various studies  [11] . These features can be categorized on the basis of various aspects such as the domain of extraction, time or frequency, timing, local or global, type, continuous or quality of the voice, etc., using classifiers that go from Hidden Markov Models (HMMs) to Support Vector Machines (SVMs) and decision trees  [16, 27, 30] . Several challenges were also organized to understand the better features to use  [31, 32, 36] . More recently, deep learning (DL) solutions that are driven from the data were proposed  [20] . Among different approaches, spectrogram-based solutions emerged as of particular interest for the fact spectrograms can be extracted quickly for realtime solutions, and can be also used as direct input to CNN and LSTM models pre-trained for the image classification task. This has the further advantage of exploiting the huge amount of available image data compared to the scarcity of speech tracks.\n\nIn the following, we focus on DL methods that used spectrograms as input data (a summary on spectrogram generation from audio tracks is given in Appendix A).\n\nDeep learning. One of the first works that experimented a DL approach for SER was proposed in  [1] . In this work, spectrograms were used as features of audio data by comparing two CNN solutions: in the first one, a network was trained from scratch or with randomly initialized weights while, in the second one, a fine-tuning operation was performed on an Alexnet  [15]  pre-trained on the Imagenet dataset  [28, 29]  for the image classification task. In  [33] , the first work was presented that applied CNNs for real-time SER. This study compared two approaches based on the extraction of RGB spectrograms of the audio data, and on fine-tuning an AlexNet taken as a reference CNN. In the first architecture, named AlexNet-SVM, a pre-trained CNN was used to extract from the spectrograms, the features generated by its second fully connected layer, which are then given as input to an SVM classifier. In the second approach, instead, a fine-tuning was applied to the AlexNet (FTAlexNet) to take advantage of what the model learned to detect during pretraining, while adapting, at the same time, the weights of the last layers to the domain represented by spectrograms. The FTAlexNet shown better performance. This work was deepened in  [17] , where it is illustrated the performance decline of the models proposed, while varying the sampling frequency used to acquire the signals.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Ser Cross Corpus.",
      "text": "In SER, few papers focused on cross-corpus evaluation, which is referred to as the case where datasets used for training (one or more) are different from those used in the test. This corresponds to a more realistic application scenario than that used by in-corpus evaluations. In these studies, increasing the data available for training was used to improve the performance. This was obtained either using data augmentation in the time or frequency domain, or forming a training set by the union of different datasets that can also contain examples in different languages.\n\nIn  [26] , a CNN, an LSTM, and an architecture given by their combination were used in a cross-corpus scenario to predict the emotions from spectrograms given as input representation of the audio tracks. In order to evaluate the performance, the IEMOCAP  [4] , EMOVO  [7] , EmoDB  [3] , EPST  [19] , RAVDESS  [21] , SAVEE  [34]  and TESS  [10]  datasets were used in a three-class classification task, where each emotion label is mapped into the positive, negative or neutral class on the basis of its membership in the target domain. In a first experiment, the models were trained on IEMOCAP, the largest dataset among those considered, and evaluated on each of the remaining sets. In a second one, the models were in turn trained using ùëõ-1 datasets and tested on the remaining one. This allowed authors to evaluate the enhancement deriving from a cross-corpus training. The accuracy ranged from 30% to 50% for the first experiment, while in the second one the accuracy was around 50% in all cases but Emo-DB and SAVEE, where it reached 69.7% and 72.7%, respectively. The best performing model was obtained by concatenating the CNN and LSTM, that resulted slightly better than the CNN model, and much better than the LSTM one.\n\nAttention in SER. One current trend in DL is that of empowering the proposed architectures with attention mechanisms. An overview of DL-based methods in SER, with specific analysis of variants that use attention mechanisms is given in  [20] .\n\nIn  [23] , the authors proposed a self-attention mechanism to reduce the dependence of the emotion on the speaker who expresses it. To this end, a particular architecture known as ACRNN, was presented for the first time in SER in  [6] . This receives in input 3D spectrograms generated by calculating the Mel spectrograms of each audio track and their respective first and second derivatives. Such 3D-representations were computed by a 3D CNN based on the application of 3D convolutions. The outputs of this network were subsequently processed by a Bidirectional LSTM (BLSTM). The output of this module, which represents a sequence with the temporal dependencies extracted from the first part of the architecture is then processed by a level of self-attention that returns the descriptor used by a fully-connected layer to classify the emotion. The architecture was tested with an in-corpus protocol on the Att-HACK  [22]  and IEMOCAP datasets, and compared with the basic model given by the ACRNN. Promising results were reported with the accuracy passing, respectively, on Att-HACK and IEMO-CAP, from 66.4% and 86.4% for ACRNN, to 88.9% and 96.1% for the proposed architecture.\n\nIn  [35] , a self-attention mechanism embedded into a Capsule Network, and a Transfer Learning based Mixed Task Net (CTLMTNet) were proposed to deal with both the single corpus and cross-corpus SER tasks simultaneously. This work shares the self-attention idea with our proposal but without using ViT and speaker embedding. Moreover, in their cross-corpus tests only one dataset was used for training, while we trained our proposed network on the union of different datasets.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Proposed Approach",
      "text": "Following some recent works  [18] , we propose a real-time SER method that first transforms the speech signal to a spectrogram image, then trains a neural network architecture to classify the spectrograms. Inspired by recent results on image analysis, we propose a Vision Transformer (ViT)  [14]  based architecture. We did not find in the literature any previous attempt of using ViTs for SER.\n\nOne reason for this is that in the image domain such architecture showed superior image classification results with respect to stateof-the-art ResNet-based models  [13]  when trained on very large amount of data. Since in SER there is not the same data abundance as in the image domain, we started from the Compact Convolutional Transformer (CCT)  [12]  that introduces convolution in ViT to make their performance superior to those of convolutional architectures even when trained on smaller amount of data.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Speaker Vgg Cct",
      "text": "In our solution, we used a CCT-14/7x2 model trained on Ima-geNet  [15] , where 14 is the number of filters in the Transformer Encoder, and 2 is the number of convolutional layers each with filters of size 7 √ó 7. These layers are applied before the ViTs layers and receive as an input spectrogram images of size 224 √ó 224 generated from the audio track.\n\nIn order to adapt the CCT-based architecture to SER, we investigated the possibility to better represent the dependence of a speaker emotion and the speaker him/her-self. For example, not all people express a state of happiness or frustration in the same way, causing the spectrograms corresponding to the same spoken text and emotional state to be very different when passing from one subject to another. This can lead to overfitting, where models tend to learn how to classify emotions for subjects in the training dataset, but do not generalize well in testing on spectrograms extracted from speakers that were never seen before. So, our idea here is to enhance the basic CCT model with a descriptor of the speakers by combining the representation through a self-attention module. We got inspiration from the work in  [23] , where a 3D Attention-based Convolutional Recurrent Neural Networks (ACRNN) was trained to classify the speakers (see also Section 2 for more details on  [23] ).\n\nWe propose two architectures based on the VGG-Vox  [24]  speaker embedder. This is basically a CNN based on the Visual Geometry Group (VGG)-M  [5]  network trained for the speaker recognition task starting from the spectrograms of the speech signal of the Vox Celeb dataset  [24] . These were extracted in the same way as for all the models presented in this paper by applying a DFT with a Hamming window of size 25ùëöùë† and stride of 10ùëöùë†. The resulting embedder is a vector with dimension 1024.\n\nThe first model, denoted as Speaker VGG CCT works in a similar way as in  [23] . In this method, the speaker embedding extracted as reported above is combined downstream of the CCT architecture, making it possible to learn the dependence between speaker and emotion. This is illustrated on the left of Figure  1 .\n\nIn the second model, instead, we defined an end-to-end solution, called Speaker VGG CCT end-to-end, where it is exploited the fact that with CCT the self-attention module is already integrated into the classifier. Therefore, the architecture ensures that the speaker embedding is given as input to the Transformers Encoder block so that it directly learns the dependence between speaker and emotion. This solution is illustrated in the middle of Figure  1 . In this case, the speaker embedding is passed directly to the Transformer Encoder block of CCT. Similar to traditional CCTs, the image is processed by a sequence of convolutional blocks that output a set of feature maps. The speaker embedding is then linked with such feature maps and the entire sequence is given as an input to the Transformer Encoder. The output is a set of vectors equal to the number of feature maps plus the one associated with the representation of the speaker. A pooling layer is then used to obtain a single vector in the dimension of the transformer starting from the exit sequence. This output is then used by an MLP network for classification. Therefore, the learning of the dependence between speaker and emotion underlying the speech represented by the spectrogram occurs directly in the self-attention modules internal to the Transformer Encoder. In this way, an additional vector is returned that represents the attention to be given to each patch in interpreting the speaker embedding.\n\nGiven the importance of this speaker embedding vector, with a different meaning than those representing the feature maps, and getting inspiration from the original implementation of ViTs, we defined a further model, called Speaker VGG CCT end-to-end Speaker Token. This model differs from the previous architecture in the classification methodology. In this case, there is no pooling layer,   and the last level discriminates the class associated with the sample directly from the output of the Transformer Encoder associated with the speaker embedding. Given the similarity with the ViTs, in which the input corresponding to the output used to classify was defined as class token, we denoted the speaker embedding provided by the PCA as a speaker token. In this way, we used this particular vector that somehow represents the dependence between the speaker embedding and all the feature maps extracted from the convolutional blocks of the CCT.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Speaker Embedding",
      "text": "Our architecture extracts a representation of the speakers from each voice signal with a VGG-Vox network  [24] , and a subsequent combination of such descriptors with information detected from spectrograms. To evidence this, for each set described in Section 4 and Table  1 , first, the embeddings associated with each audio track were generated, thus resulting into vectors of 1,024 elements extracted from the last fully-connected layer of the VGG-Vox network. Then, Multi-dimensional Scaling (MDS) was used to represent the embeddings in a planar domain as illustrated in Figure  2 . We note the dimension of the speaker embedding vector was larger than the internal size of the used CCTs. In addition, the self-attention module used in each of the proposed architectures requires an equal size of the input features. So, PCA was applied to reduce the speaker embedding size to 384 elements. In this way, the desired size was obtained, while keeping the most discriminatory features among the 1,024. PCA effectively allowed us to reduce the size of the embedding, while keeping its discriminatory capability of the subjects.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Experimental Validation",
      "text": "We performed a comprehensive set of experiments to validate the proposed architectures in a real-time cross-corpus scenario. We selected the ResNet50 network pre-trained on Imagenet as the reference model to compare with. ResNet is a state-of-the-art residual architecture for image classification. The computational cost associated to its 50 layers is similar to that of the used CCT-14/7x2. Datasets. To verify the cross-corpus performance of the proposed methods, we considered some of the most used datasets in SER as summarized in Table  1 . All the sets taken into consideration contain examples for the seven archetypal emotions or for a subset of them, except for IEMOCAP and EMODB, which also include a few examples for the emotional states associated with boredom and excitement. Given this categorization, we restricted SER to four classes, namely, Anger (A), Happiness (H), Sadness (S), and Neutrality (N). In SER, this is often a way to reduce the difficulty of the crosscorpus task, while also reducing the problem to the most important emotions among all the primary ones. Cross-corpus tests for three classes are reported in Appendix B.3.\n\nWe used the larger datasets for training, while testing was performed on the remaining sets. In particular, we considered EmoDB, EMOVO and SAVEE as test sets for cross-corpus validation, which also allowed us to evaluate the general performance for three languages, i.e., German, Italian and English, respectively. We also note all these datasets include speech tracks of emphasized sentences associated to an emotional state reproduced by actors.\n\nData Balancing. In SER, it is quite common that audio data include many examples for certain classes and a smaller number of samples for other labels. This generally leads to the problem of class collapse: a model tends to always predict examples with the labels associated with the classes that are more popular in the training dataset. This also occurs for the datasets used in this work, whose imbalance is shown in Table  1 . It results that only the EMOVO dataset is perfectly balanced for archetypal emotions, while the others are somehow unbalanced. This problem tends to get even worse when aggregating multiple different datasets in a cross-corpus training procedure.\n\nTo address this issue and improve performance, two approaches were considered. In the first one, we implemented an undersampling technique that, for each class, removes a number of randomly selected samples equal to the difference between the samples of the aforementioned label compared to those associated with the minority class. This allows training the models on a perfectly balanced dataset; however, this significantly reduces the data available for training, thus making more severe the scarcity of data and decreasing the generalization capability of the network.\n\nAs an alternative technique, we used a data augmentation procedure. The idea was to first apply this operation to each sample of the training set in order to obtain multiple versions associated with it. Then, for each class with fewer examples, additional samples were added in numbers equal to those missing to reach the cardinality of the majority class. As typical for audio computation, we first considered transformations directly applied in the time domain, specifically designed to increase a voice signal by speeding it up, slowing it down, introducing noise into it, shifting it over time, varying its tone and normalizing it. Another technique widely used for spectrograms derived from audio tracks consists in transforming them by acting directly in frequency. Doing so, it is possible to add vertical or horizontal bars to mask, respectively, certain moments in time rather than particular frequency bands. This allows the models to adapt less to the given training data, thus improving their generalization ability on never observed data. Examples of augmented spectrograms for both the approaches are reported in Figure  4  of the Appendix.\n\nMeasures. The following measures have been used in the evaluation, being ùë° ùëù and ùë° ùëõ the true positive and true negative, respectively, and ùëù and ùëõ positive and negative total:\n\n‚Ä¢ Accuracy: ratio between the number of correct predictions and the total number of predictions; ‚Ä¢ Unweighted Average Recall (UAR): it accounts for unbalancing of the test dataset. For the binary case, it is defined as",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "ùëà ùê¥ùëÖ =",
      "text": "ùë° ùëù ùëù ‚Ä¢ 0.5 + ùë° ùëõ ùëõ ‚Ä¢ 0.5, where 0.5 is replaced with 1",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "ùëêùëôùëéùë†ùë†ùëíùë†",
      "text": "for the multi-class case. Each Recall is weighed in the same way regardless of the number of samples of the respective class. This is considered the metric of reference in SER as the datasets are unbalanced in most of the cases; ‚Ä¢ Macro-F1: measures the quality of results for binary or multi-class classification tasks. It computes an average of the ùêπ 1 measurements associated with the various classes, where each of them is defined as ùêπ 1 = ùë° ùëù ùë° ùëù +1/2(ùëì ùëù +ùëì ùëõ ) . Settings. Each training was performed on a machine with an Nvidia RTX 3060 Ti GPU for a total of 50 epochs with Adam optimizer and a batch size of 32. The learning rate was selected by estimating the best performance obtained from the models on a particular test considered as a reference. Each architecture was trained by varying its learning rate taking as training set all datasets except EmoDB and using the latter as a test. The optimal choice of such parameter was therefore found to be 5.0ùëí -4 for the ResNet50 and 5.0ùëí -5 for the proposed CCT-based models.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Cross-Corpus Results",
      "text": "The two \"basic\" models (i.e., ResNet50 and CCT-14/7x2) were compared for the four-class task with undersampling for balancing. The IEMOCAP dataset was used for training, while tests were performed on EmoDB, SAVEE and EMOVO. We note that IEMOCAP is the largest set among those at our disposal, whereas the test datasets are those with fewer samples and characterizing three languages (i.e., English, German, Italian). Results shown in Table  2  denote a clear superiority of the CCT-based model compared to the ResNet50. Results also confirm the fact widely reported in the literature, that testing the models on a dataset other than the one used for training causes a noticeable performance degradation. We note lower performance is obtained for SAVEE that includes less speaker compared to the others: for example, if one speaker is predicted with more difficulty this can largely affect the results. We also note that IEMOCAP was used for training, though in the literature it is reported to not score good results when tested intra-corpus; we proceeded in this way because it is one of the largest dataset at our disposal and a similar choice was also proposed in  [26] . Table  3  reports the comparison between all the proposed models. We still performed a cross-corpus experiment, with four classes and with undersampling for balancing (left values for each table entry). In this case, we used a cross validation protocol, where for each test dataset the models were trained several times, each time excluding the test set and, in round, selecting as validation RAVDESS and the remaining two datasets of test among EmoDB, EMOVO and SAVEE. IEMOCAP and DEMOS that include more samples among the available datasets were used for training. From the table, it can be noted that all the metrics are generally superior to those obtained by training on IEMOCAP only for ResNet50 and for the CCT model. This highlights the fact, already reported in the literature, that cross-corpus training can bring significant benefits in SER. As for the comparison between the proposed architectures, it is evident that the learning of the dependence between speakers and emotion allows us to improve the performance of the models also in a cross-corpus test. In particular, the end-to-end architectures get the best metrics. Among the proposed models, for EmoDB the one without speaker token (which uses instead the pooling layer to classify) achieves the best results, while for the other two sets it was the other architecture to achieve the best results. On average, by focusing on the two most relevant metrics, the speaker token model has obtained a higher UAR, but with a lower value for Macro-F1.\n\nThe gap with the state-of-the-art is of about 2%. This gap increases by performing the same experiments but with training carried out on balanced datasets with data augmentation, as shown by the values reported on the right of each entry of the table. The best models are those end-to-end, with a gap between the best performing one (Speaker CCT end-to-end) compared to ResNet50",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Intra-Corpus Results",
      "text": "An intra-corpus experiment was performed on the EmoDB dataset, thus making the approach comparable with other solutions in the literature. We used a Leave-One-Speaker-Out (LOSO) protocol, where iteratively a speaker is used for test and models are trained only on the samples of the rest of the subjects.\n\nTable  4  shows the results for each of the proposed architectures obtained by averaging the values detected for all the speakers. The performance shows a different trend with respect to the crosscorpus experiment, with the models using the speaker embeddings showing lower accuracy and Macro-F1. This can be explained by the AlexNet  [17] * 80.5% --*Results taken from the original paper using 5-fold cross validation fact that, to work properly, such architectures need more speakers than the 10 included in EmoDB. In fact, such methods during the training learn the dependence between the representation of the speakers and their emotions. During inference, the models should be able to match the embedding of the tested speaker with the closest one among those in the training set. Therefore, if the speakers observed during the training are too few, such association can be inaccurate. Notably, CCT and all the proposed variants achieved superior performance compared to RenNet50. This is inasmuch relevant as we propose for the first time the use of ViT in SER.\n\nThe table also reports results for the method in  [17] , which is a reference one for the application of CNN in real-time SER. In that study, authors classified the spectrograms using AlexNet pretrained on Imagenet. Despite the reported results were of about 80%, we note they used a 5-fold cross-validation protocol, where the trained models were evaluated on audio produced by speakers already viewed by the network during the training, with a significant loss of generality compared to an evaluation performed with the LOSO protocol.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Conclusions",
      "text": "In this paper, we addressed the SER problem in real-time and considering a concrete application scenario where most of the evaluation is performed cross-corpus. The proposed approach has some innovative aspects. First of all, it is the first to apply a ViT model in SER through the CCT variant, also integrating the learning of dependence between speaker embedding and emotion by means of self-attention on its own or in an end-to-end architecture. Furthermore, looking for solution as general as possible, all proposed methods were tested in a cross-corpus setting as it is rarely the case in SER. Results show evident improvements with respect to ResNet50 taken as state-of-the-art architecture for real-time SER.\n\nIn particular, the proposed end-to-end architecture turned out to be the best performing one.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "A Spectrograms",
      "text": "The spectrogram of an audio signal is generally defined as its own representation that makes explicit the contributions of each frequency for each time frame. The process of extracting the spectrogram from an audio track is summarized in Figure  3 . First, the Fast Fourier Transform (FFT) is applied to the audio signal sampled into the time domain using small time windows; in this way, for each of them the contribution of each of its components in the frequency domain is obtained. Putting together the results obtained on all the windows, a 2D representation of the original signal is derived, where each element on the horizontal axis represents a window and the values on the vertical axis explain the amplitudes of the spectrum. To compensate for non linearity, the Mel spectrograms are used that map each frequency into a corresponding one using the Mel scale. This, to bring the frequencies to a logarithmic scale, transforms low frequencies into a larger range than that used for the high ones, so that the coefficients of the lower frequencies are represented by a larger region on the spectrogram. A logarithmic scale is also used for the amplitude representation that transforms the frequencies from Hertz (hz) to Decibels (db). A final image with three well distributed channels representing the original audio signal is obtained. In our work, to generate the spectrogram images, a Discrete domain Fourier Transform (DFT) was first applied to the audio tracks in the time domain to obtain a frequency representation, which is then mapped to new values using the Mel scale and decibels. Parameters that characterize such process are the factors used for applying the DFT. Following other works in SER, we applied the aforementioned transform with a Hamming window of 25ùëöùë† and a stride of 10ùëöùë†.\n\nThese descriptors are characterized by the bands frequency, represented by their number and the minimum and maximum frequencies, and aspects related to temporal sections, such as the length of each window and the stride that identifies the number of samples of which to shift the frames at each step. Usually, in SER, 128 frequency bands are selected, with a window size of 25ms and a stride of 10ms.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "B Additional Results B.1 Data Augmentation",
      "text": "Figure  4  shows, for the sample spectrograms in (a), five time domain transformations, i.e., noise, normalization, tone variation, shifting, and speed change in (b), and two transformations in the frequency domain, i.e., masks introduced in time and frequency in (c).",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "B.2 Confusion Matrices",
      "text": "Here, we further investigate the relevance of the used metrics. In Section 4, we pointed out the two reference measures for evaluation are the UAR and the Macro-F1 that can be considered as complementary: the UAR indicates the accuracy obtained on average taking into account the unbalancing of the test datasets, while the Macro-F1 measures the quality of the results. Even for a lower UAR value, if the respective Macro-F1 value is higher, the quality of the results, explained as the concentration of the values of the confusion matrix along the main diagonal, will be better.\n\nAn example of this is shown in Figure  5 , where the matrices obtained from the two end-to-end models are shown by testing on EMOVO with the four-class cross corpus training and the balancing procedure for data augmentation. The matrix on the left and on the right refer, respectively, to the model that uses the pooling layer for the extraction of the classification features (Speaker VGG CCT end-to-end), and to the model with speaker token (Speaker VGG CCT end-to-end token).\n\nDespite from the results shown in Table  3 , the best accuracy is obtained with the method that uses the speaker token, the confusion matrices show a more balanced performance between classes in the Speaker VGG CCT end-to-end case. The highest quality, quantitatively expressed by a higher Macro-F1 value is highlighted by a better ability of the model to predict the class neutral, which is poorly evaluated by the other architecture. It is therefore evident how much a greater value of the Macro-F1 is even more indicative of that of the UAR, that is instead most often used in SER. In fact, for application purposes, it would be more useful to have a model that performs well enough for all classes, rather than one capable of perfectly predicting only a subset of them, while providing poor performance on the remaining ones.  Disgust (D), Fearful (F), Sadness (S); positive-Excitement (E), Happiness (H), Surprise (Sr); neutral-Neutral (N). Using three classes further allows reducing the number of classes to discriminate, while increasing, at the same time, the number of examples since none of them is discarded but just remapped to another space. However, since there are many labels associated with the negative class, and a lower number with the positive, and only one with the neutral class, there is a notable increase in the imbalance already present in the used datasets. All this considered, the difficulty in discriminating these features still keeps high despite the lower number of classes.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "B.3 Cross-Corpus Experiments On Three Classes",
      "text": "As evaluation pipeline, we used the same procedure as described in Section 4.1 for the cross-corpus training with four-classes. However, the evaluation was constrained to the EMOVO dataset only. Results reported in Table  5  show the same trend of the tests performed for four-classes, with the end-to-end models tending to have superior performance than the others.\n\nHowever, unlike what was observed with the four-classes task, in this case the balancing technique with data augmentation achieves lower performance than that with undersampling. This is due to the fact that by running data augmentation, the resulting models tend to predict almost always with the negative class for any test example. The cause of this problem is that in this case the imbalance of the training dataset already present for the four-class task increases considerably. Indeed, according to the mapping, many labels of the reference datasets are mapped into the negative class, few in the positive one and only one in the neutral one.\n\nThe effect of this is that the resulting set has thousands more examples for the negative label compared to the other two. Therefore, adding samples to balance the dataset, the models during training observe many generated examples for the positive and neutral classes, while for the negative label the observed signals are all original. As a result, in inference, the models tend to generalize well for the negative class and worse for the other two, with the consequence that most of the speech signals are predicted with the majority class, as shown in Figure  6 . Finally, we note that Table  5  also shows the UAR value taken from  [26] , one of the few works that proposed a CNN architecture for cross-corpus SER. We note that although this value is similar to those obtained from our models, the data used in  [26]  are partially different from ours. In particular, IEMOCAP, EmoDB, RAVDESS and SAVEE are used in  [26]  as in our work, but instead of DEMOS the authors in  [26]  considered the EPST  [19]  dataset.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "B.4 Additional Discussion Of The Results And",
      "text": "Future Perspectives Among all the experiments carried out, we consider those related to the four-classes task, as reported in the paper, as the most relevant ones. In relation to this, the performance analysis obtained with each model highlighted the already known difficulty in training cross-corpus models in SER. In fact, the accuracy of the best models are not yet sufficiently high to allow the implementation of a system that can be of concrete help in daily life. Despite this, the proposed solutions can provide inspiration and indicate possible directions for further investigations. First of all, it was observed that considering the four-class task, the low accuracy is partly caused by the fact that the predictions of the classes relating to happiness  and neutrality are extremely low, compared to high precision for the other two associated with anger and sadness. This could be reduced by balancing the training dataset so to have more samples for the first two labels and fewer examples for the other two. In addition, the introduction of the self-attention module within the architectures offers various application opportunities. In fact, this mechanism allows learning the relevance of a set of vectors when one in particular is interpreted. In this work, this approach has been used to detect the dependence of the speaker embedding with respect to features extracted from a spectrogram in a module of attention that receives only two vectors as input. However, it is evident how using as inputs other representations extracted from the audio track such as the encoding of the corpus to which it belongs to or the genus of the subject who is speaking, it would be possible to add information at a low computational cost.",
      "page_start": 10,
      "page_end": 10
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The proposed Speaker VGG architectures with: (left) CCT; (middle) CCT end-to-end; (right) CCT speaker token.",
      "page": 3
    },
    {
      "caption": "Figure 1: In the second model, instead, we defined an end-to-end solution,",
      "page": 3
    },
    {
      "caption": "Figure 2: Figure 2: Plots illustrate, for each dataset, the speaker em-",
      "page": 4
    },
    {
      "caption": "Figure 4: of the Appendix.",
      "page": 5
    },
    {
      "caption": "Figure 5: in the appendix for a confusion-matrix example using",
      "page": 6
    },
    {
      "caption": "Figure 3: First, the Fast",
      "page": 7
    },
    {
      "caption": "Figure 3: Generation of the spectrogram of an audio signal.",
      "page": 7
    },
    {
      "caption": "Figure 4: shows, for the sample spectrograms in (a), five time domain",
      "page": 7
    },
    {
      "caption": "Figure 5: , where the matrices",
      "page": 7
    },
    {
      "caption": "Figure 4: Examples of spectrograms associated with data augmentation transformations applied to a sample of EmoDB. (a)",
      "page": 8
    },
    {
      "caption": "Figure 5: Confusion matrices referring to cross-corpus ex-",
      "page": 8
    },
    {
      "caption": "Figure 6: Figure 6: Confusion matrices derived from the three-class",
      "page": 8
    }
  ],
  "tables": [
    {
      "caption": "Table 1: Main characteristics of the datasets used in the experiments. Labels: Anger (A), Boredom (B), Disgust (D), Excitement",
      "page": 4
    },
    {
      "caption": "Table 1: , first, the embeddings associated with each audio track",
      "page": 4
    },
    {
      "caption": "Table 1: All the sets taken into consideration",
      "page": 4
    },
    {
      "caption": "Table 1: It results that only",
      "page": 4
    },
    {
      "caption": "Table 2: denote a clear superiority of the CCT-based model compared to the",
      "page": 5
    },
    {
      "caption": "Table 2: Cross-corpus results: train on IEMOCAP; test on",
      "page": 5
    },
    {
      "caption": "Table 3: reports the comparison between all the proposed models.",
      "page": 5
    },
    {
      "caption": "Table 3: Cross-corpus results: training on IEMOCAP and DEMOS; testing on EmoDB, EMOVO and SAVEE each with four classes",
      "page": 6
    },
    {
      "caption": "Table 4: shows the results for each of the proposed architectures",
      "page": 6
    },
    {
      "caption": "Table 4: Intra-corpus results: LOSO protocol on EmoDB us-",
      "page": 6
    },
    {
      "caption": "Table 3: , the best accuracy is",
      "page": 7
    },
    {
      "caption": "Table 5: show the same trend of the tests per-",
      "page": 8
    },
    {
      "caption": "Table 5: also shows the UAR value taken",
      "page": 8
    },
    {
      "caption": "Table 5: Cross-corpus experiments on the Positive, Negative, and Neutral classes, and test on the EMOVO dataset. For each",
      "page": 9
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Speech Emotion Recognition from Spectrograms with Deep Convolutional Neural Network",
      "authors": [
        "Abdul Malik Badshah",
        "Jamil Ahmad",
        "Nasir Rahim",
        "Sung Baik"
      ],
      "year": "2017",
      "venue": "Int. Conf. on Platform Technology and Service (PlatCon)",
      "doi": "10.1109/PlatCon.2017.7883728"
    },
    {
      "citation_id": "2",
      "title": "Prosody as a compensatory strategy in the conversations of people with agrammatism",
      "authors": [
        "Suzanne Beeke",
        "Ray Wilkinson",
        "Jane Maxim"
      ],
      "year": "2009",
      "venue": "Clinical Linguistics & Phonetics"
    },
    {
      "citation_id": "3",
      "title": "A database of German emotional speech",
      "authors": [
        "Felix Burkhardt",
        "Astrid Paeschke",
        "M Rolfes",
        "Walter Sendlmeier",
        "Benjamin Weiss"
      ],
      "year": "2005",
      "venue": "A database of German emotional speech"
    },
    {
      "citation_id": "4",
      "title": "IEMOCAP: interactive emotional dyadic motion capture database",
      "authors": [
        "Carlos Busso",
        "Murtaza Bulut",
        "Chi-Chun Lee",
        "Emily Ebrahim (abe) Kazemzadeh",
        "Samuel Provost",
        "Jeannette Kim",
        "Sungbok Chang",
        "Shrikanth Lee",
        "Narayanan"
      ],
      "year": "2008",
      "venue": "Language Resources and Evaluation"
    },
    {
      "citation_id": "5",
      "title": "Return of the devil in the details: Delving deep into convolutional nets",
      "authors": [
        "Ken Chatfield",
        "Karen Simonyan",
        "Andrea Vedaldi",
        "Andrew Zisserman"
      ],
      "year": "2014",
      "venue": "Return of the devil in the details: Delving deep into convolutional nets",
      "arxiv": "arXiv:1405.3531"
    },
    {
      "citation_id": "6",
      "title": "3-D Convolutional Recurrent Neural Networks With Attention Model for Speech Emotion Recognition",
      "authors": [
        "Mingyi Chen",
        "Xuanji He",
        "Jing Yang",
        "Han Zhang"
      ],
      "year": "2018",
      "venue": "IEEE Signal Processing Letters",
      "doi": "10.1109/LSP.2018.2860246"
    },
    {
      "citation_id": "7",
      "title": "EMOVO Corpus: an Italian Emotional Speech Database",
      "authors": [
        "Giovanni Costantini",
        "Iacopo Iaderola",
        "Andrea Paoloni",
        "Massimiliano Todisco"
      ],
      "year": "2014",
      "venue": "Int. Conf. on Language Resources and Evaluation"
    },
    {
      "citation_id": "8",
      "title": "CoAtNet: Marrying Convolution and Attention for All Data Sizes",
      "authors": [
        "Zihang Dai",
        "Hanxiao Liu",
        "Quoc Le",
        "Mingxing Tan"
      ],
      "year": "2021",
      "venue": "CoAtNet: Marrying Convolution and Attention for All Data Sizes",
      "arxiv": "arXiv:2106.04803"
    },
    {
      "citation_id": "9",
      "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
      "authors": [
        "Alexey Dosovitskiy",
        "Lucas Beyer",
        "Alexander Kolesnikov",
        "Dirk Weissenborn",
        "Xiaohua Zhai",
        "Thomas Unterthiner",
        "Mostafa Dehghani",
        "Matthias Minderer",
        "Georg Heigold",
        "Sylvain Gelly",
        "Jakob Uszkoreit",
        "Neil Houlsby"
      ],
      "year": "2021",
      "venue": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"
    },
    {
      "citation_id": "10",
      "title": "Recognition of emotional speech for younger and older talkers: Behavioural findings from the toronto emotional speech set",
      "authors": [
        "Kate Dupuis",
        "Kathleen Pichora-Fuller"
      ],
      "year": "2011",
      "venue": "Canadian Acoustics"
    },
    {
      "citation_id": "11",
      "title": "Survey on speech emotion recognition: Features, classification schemes, and databases",
      "authors": [
        "Moataz Ayadi",
        "Mohamed Kamel",
        "Fakhri Karray"
      ],
      "year": "2011",
      "venue": "Pattern Recognition"
    },
    {
      "citation_id": "12",
      "title": "Escaping the Big Data Paradigm with Compact Transformers",
      "authors": [
        "Lli Hassani",
        "Steven Walton",
        "Nikhil Shah",
        "Abulikemu Abuduweili",
        "Jiachen Li",
        "Humphrey Shi"
      ],
      "year": "2021",
      "venue": "IEEE/CVF Conf. on Computer Vision and Pattern Recognition Workshops"
    },
    {
      "citation_id": "13",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "Kaiming He",
        "Xiangyu Zhang",
        "Shaoqing Ren",
        "Jian Sun"
      ],
      "year": "2016",
      "venue": "IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "14",
      "title": "An Image is Worth 16x16 Words: Transformers for Image",
      "authors": [
        "Alexander Kolesnikov",
        "Alexey Dosovitskiy",
        "Dirk Weissenborn",
        "Georg Heigold",
        "Jakob Uszkoreit",
        "Lucas Beyer",
        "Matthias Minderer",
        "Mostafa Dehghani",
        "Neil Houlsby",
        "Sylvain Gelly",
        "Thomas Unterthiner",
        "Xiaohua Zhai"
      ],
      "year": "2021",
      "venue": "An Image is Worth 16x16 Words: Transformers for Image"
    },
    {
      "citation_id": "15",
      "title": "ImageNet Classification with Deep Convolutional Neural Networks",
      "authors": [
        "Alex Krizhevsky",
        "Ilya Sutskever",
        "Geoffrey Hinton"
      ],
      "year": "2012",
      "venue": "Advances in Neural Information Processing Systems (NeurIPS)"
    },
    {
      "citation_id": "16",
      "title": "Emotion recognition by speech signals",
      "authors": [
        "Oh-Wook Kwon",
        "Kwokleung Chan",
        "Jiucang Hao",
        "Te-Won Lee"
      ],
      "year": "2003",
      "venue": "Eighth European conference on speech communication and technology"
    },
    {
      "citation_id": "17",
      "title": "Real-Time Speech Emotion Recognition Using a Pre-trained Image Classification Network: Effects of Bandwidth Reduction and Companding",
      "authors": [
        "Margaret Lech",
        "Melissa Stolar",
        "Christopher Best",
        "Robert Bolia"
      ],
      "year": "2020",
      "venue": "Frontiers in Computer Science"
    },
    {
      "citation_id": "18",
      "title": "Real-Time Speech Emotion Recognition Using a Pre-trained Image Classification Network: Effects of Bandwidth Reduction and Companding",
      "authors": [
        "Margaret Lech",
        "Melissa Stolar",
        "Christopher Best",
        "Robert Bolia"
      ],
      "year": "2020",
      "venue": "Frontiers in Computer Science"
    },
    {
      "citation_id": "19",
      "title": "Emotional Prosody Speech and Transcripts. LDC2002S28. Web Download. Philadelphia: Linguistic Data Consortium",
      "authors": [
        "Mark Liberman",
        "Kelly Davis",
        "Murray Grossman",
        "Nii Martey",
        "John Bell"
      ],
      "year": "2002",
      "venue": "Emotional Prosody Speech and Transcripts. LDC2002S28. Web Download. Philadelphia: Linguistic Data Consortium"
    },
    {
      "citation_id": "20",
      "title": "A Review on Speech Emotion Recognition Using Deep Learning and Attention Mechanism",
      "authors": [
        "Eva Lieskovsk√°",
        "Maro≈° Jakubec",
        "Roman Jarina",
        "Michal Chmul√≠k"
      ],
      "year": "2021",
      "venue": "Electronics",
      "doi": "10.3390/electronics10101163"
    },
    {
      "citation_id": "21",
      "title": "The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS): A dynamic, multimodal set of facial and vocal expressions in North American English",
      "authors": [
        "Steven Livingstone",
        "Frank Russo"
      ],
      "year": "2018",
      "venue": "PLOS ONE",
      "doi": "10.1371/journal.pone.0196391"
    },
    {
      "citation_id": "22",
      "title": "Att-HACK: An Expressive Speech Database with Social Attitudes",
      "authors": [
        "' Cl",
        "Le Moine",
        "Nicolas Obin"
      ],
      "year": "2020",
      "venue": "Att-HACK: An Expressive Speech Database with Social Attitudes"
    },
    {
      "citation_id": "23",
      "title": "Nicolas Obin, and Axel Roebel. 2021. Speaker attentive speech emotion recognition",
      "authors": [
        "Cl√©ment Le"
      ],
      "year": "2021",
      "venue": "Nicolas Obin, and Axel Roebel. 2021. Speaker attentive speech emotion recognition",
      "arxiv": "arXiv:2104.07288"
    },
    {
      "citation_id": "24",
      "title": "Voxceleb: a large-scale speaker identification dataset",
      "authors": [
        "Arsha Nagrani",
        "Son Chung",
        "Andrew Zisserman"
      ],
      "year": "2017",
      "venue": "Voxceleb: a large-scale speaker identification dataset",
      "arxiv": "arXiv:1706.08612"
    },
    {
      "citation_id": "25",
      "title": "DEMoS: an Italian emotional speech corpus",
      "authors": [
        "Emilia Parada-Cabaleiro",
        "Giovanni Costantini",
        "Anton Batliner",
        "Maximilian Schmitt",
        "Bj√∂rn Schuller"
      ],
      "year": "2020",
      "venue": "DEMoS: an Italian emotional speech corpus"
    },
    {
      "citation_id": "26",
      "title": "Analysis of Deep Learning Architectures for Cross-Corpus Speech Emotion Recognition",
      "authors": [
        "Jack Parry",
        "Dimitri Palaz",
        "Georgia Clarke",
        "Pauline Lecomte",
        "Rebecca Mead",
        "M Berger",
        "Gregor Hofer"
      ],
      "year": "2019",
      "venue": "Analysis of Deep Learning Architectures for Cross-Corpus Speech Emotion Recognition"
    },
    {
      "citation_id": "27",
      "title": "The production and recognition of emotions in speech: features and algorithms",
      "authors": [
        "Oudeyer Pierre-Yves"
      ],
      "year": "2003",
      "venue": "Applications of Affective Computing in Human-Computer Interaction",
      "doi": "10.1016/S1071-5819(02)00141-6"
    },
    {
      "citation_id": "28",
      "title": "ImageNet Large Scale Visual Recognition Challenge",
      "authors": [
        "Olga Russakovsky",
        "Jia Deng",
        "Hao Su",
        "Jonathan Krause",
        "Sanjeev Satheesh",
        "Sean Ma",
        "Zhiheng Huang",
        "Andrej Karpathy",
        "Aditya Khosla",
        "Michael Bernstein",
        "Alexander Berg",
        "Li Fei-Fei"
      ],
      "year": "2014",
      "venue": "ImageNet Large Scale Visual Recognition Challenge"
    },
    {
      "citation_id": "29",
      "title": "Conference'17",
      "year": "2017",
      "venue": "Conference'17",
      "doi": "10.48550/ARXIV.1409.0575"
    },
    {
      "citation_id": "30",
      "title": "ImageNet Large Scale Visual Recognition Challenge",
      "authors": [
        "Olga Russakovsky",
        "Jia Deng",
        "Hao Su",
        "Jonathan Krause",
        "Sanjeev Satheesh",
        "Sean Ma",
        "Zhiheng Huang",
        "Andrej Karpathy",
        "Aditya Khosla",
        "Michael Bernstein",
        "Alexander Berg",
        "Li Fei-Fei"
      ],
      "year": "2015",
      "venue": "International Journal of Computer Vision (IJCV)",
      "doi": "10.1007/s11263-015-0816-y"
    },
    {
      "citation_id": "31",
      "title": "Speech emotion recognition combining acoustic features and linguistic information in a hybrid support vector machinebelief network architecture",
      "authors": [
        "B Schuller",
        "G Rigoll",
        "M Lang"
      ],
      "year": "2004",
      "venue": "IEEE Int. Conf. on Acoustics, Speech, and Signal Processing",
      "doi": "10.1109/ICASSP.2004.1326051"
    },
    {
      "citation_id": "32",
      "title": "The INTERSPEECH 2009 emotion challenge",
      "authors": [
        "Bj√∂rn Schuller",
        "Stefan Steidl",
        "Anton Batliner"
      ],
      "year": "2009",
      "venue": "Proc. Interspeech",
      "doi": "10.21437/Interspeech.2009-103"
    },
    {
      "citation_id": "33",
      "title": "The INTERSPEECH 2010 paralinguistic challenge",
      "authors": [
        "Bj√∂rn Schuller",
        "Stefan Steidl",
        "Anton Batliner",
        "Felix Burkhardt",
        "Laurence Devillers",
        "Christian M√ºller",
        "Shrikanth Narayanan"
      ],
      "year": "2010",
      "venue": "Proc. Interspeech",
      "doi": "10.21437/Interspeech.2010-739"
    },
    {
      "citation_id": "34",
      "title": "Real time speech emotion recognition using RGB image classification and transfer learning",
      "authors": [
        "Melissa Stolar",
        "Margaret Lech",
        "Robert Bolia",
        "Michael Skinner"
      ],
      "year": "2017",
      "venue": "Int. Conf. on Signal Processing and Communication Systems (ICSPCS)",
      "doi": "10.1109/ICSPCS.2017.8270472"
    },
    {
      "citation_id": "35",
      "title": "Machine Audition: Principles, Algorithms and Systems",
      "authors": [
        "Wenwu Wang"
      ],
      "year": "2010",
      "venue": "Machine Audition: Principles, Algorithms and Systems"
    },
    {
      "citation_id": "36",
      "title": "CTL-MTNet: A Novel CapsNet and Transfer Learning-Based Mixed Task Net for the Single-Corpus and Cross-Corpus Speech Emotion Recognition",
      "authors": [
        "Xin-Cheng Jia-Xin Ye",
        "Yan Luo",
        "Yong Xu",
        "Xuan-Ze Wang",
        "Chang-Li Wu",
        "Kun-Hong Liu"
      ],
      "year": "2022",
      "venue": "CTL-MTNet: A Novel CapsNet and Transfer Learning-Based Mixed Task Net for the Single-Corpus and Cross-Corpus Speech Emotion Recognition",
      "doi": "10.48550/ARXIV.2207.10644"
    },
    {
      "citation_id": "37",
      "title": "On the Acoustics of Emotion in Audio: What Speech, Music, and Sound have in Common",
      "authors": [
        "Felix Weninger",
        "Florian Eyben",
        "Bj√∂rn Schuller",
        "Marcello Mortillaro",
        "Klaus Scherer"
      ],
      "year": "2013",
      "venue": "Frontiers in Psychology"
    },
    {
      "citation_id": "38",
      "title": "CvT: Introducing Convolutions to Vision Transformers",
      "authors": [
        "Haiping Wu",
        "Bin Xiao",
        "Noel Codella",
        "Mengchen Liu",
        "Xiyang Dai",
        "Lu Yuan",
        "Lei Zhang"
      ],
      "year": "2021",
      "venue": "CvT: Introducing Convolutions to Vision Transformers",
      "arxiv": "arXiv:2103.15808"
    }
  ]
}