{
  "paper_id": "2503.08974v1",
  "title": "Beyond Overfitting: Doubly Adaptive Dropout For Generalizable Au Detection",
  "published": "2025-03-12T00:34:43Z",
  "authors": [
    "Yong Li",
    "Yi Ren",
    "Xuesong Niu",
    "Yi Ding",
    "Xiu-Shen Wei",
    "Cuntai Guan"
  ],
  "keywords": [
    "Facial Action Unit Detection",
    "Domain Adaption",
    "Feature Dropout"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Facial Action Units (AUs) are essential for conveying psychological states and emotional expressions. While automatic AU detection systems leveraging deep learning have progressed, they often overfit to specific datasets and individual features, limiting their cross-domain applicability. To overcome these limitations, we propose a doubly adaptive dropout approach for cross-domain AU detection, which enhances the robustness of convolutional feature maps and spatial tokens against domain shifts. This approach includes a Channel Drop Unit (CD-Unit) and a Token Drop Unit (TD-Unit), which work together to reduce domain-specific noise at both the channel and token levels. The CD-Unit preserves domain-agnostic local patterns in feature maps, while the TD-Unit helps the model identify AU relationships generalizable across domains. An auxiliary domain classifier, integrated at each layer, guides the selective omission of domain-sensitive features. To prevent excessive feature dropout, a progressive training strategy is used, allowing for selective exclusion of sensitive features at any model layer. Our method consistently outperforms existing techniques in cross-domain AU detection, as demonstrated by extensive experimental evaluations. Visualizations of attention maps also highlight clear and meaningful patterns related to both individual and combined AUs, further validating the approach's effectiveness.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "Facial expressions constitute a nuanced channel for conveying a wide array of meanings, encompassing emotions, intentions, attitudes, and both mental and physical states  [1] . The Facial Action Coding System (FACS)  [2] , which is firmly rooted in anatomical principles, stands as the most exhaustive approach for encoding and representing facial expressions. FACS offers an explainable and reliable framework for the analysis of human facial expressions. It systematically dissects facial expressions into discrete elements termed Action Units (AUs), which correspond to the activation of specific facial muscles or muscle groups. By identifying and quantifying these AUs, researchers can methodically investigate facial expressions and the emotional states they signify. Presently, automated AU detection has emerged as a burgeoning field within computer vision, holding considerable potential for diverse applications such as human-computer interaction, emotion analysis, and medical pain assessment  [3, 4, 5] .\n\nIn recent times, deep neural networks (DNNs) have been increasingly utilized for within-dataset AU detection, yielding significant advancements  [6, 7] . However, their performance tends to deteriorate noticeably when confronted with new scenarios or evaluated across different datasets  [8, 9, 10, 11] . As demonstrated in recent work  [11] , the AU feature extractor tends to produce indiscriminate features for unseen data from previously unseen domains. Moreover, the process of annotating AUs is inherently challenging due to the subtle facial deformations induced by AUs, which span various local facial regions with varying intensities  [12] . Consequently, manually annotating and constructing AU detection models for each novel scenario entails considerable time and effort. Given the potential disparities across domains and the intricacies associated with manual AU annotation, it becomes impera-tive to devise innovative methodologies. These methodologies should facilitate the adaptation of AU detection models to new scenarios without reliance on annotated datasets specific to these novel environments.\n\nThis manuscript mitigates the challenges posed by crossdomain AU detection through the introduction of a novel methodology termed Doubly adaptive Dropout (AUDD). AUDD is purposefully crafted to remove features that are susceptible to domain discrepancies while simultaneously preserving representations that are both invariant to domain variations and discriminative for AU detection. Traditionally, dropout techniques have been employed as a regularization strategy within fully-connected layers  [13] , and subsequently adapted to enhance the regularization of convolutional features  [14, 15, 16] . Despite the conceptual simplicity of discarding domain-sensitive features, the practical implementation of this strategy poses two significant challenges: (1) How to reliably and autonomously identify features that are sensitive to domain-specific variations? (2) How to develop a dropoutbased framework that is meticulously tailored to enhance the efficacy of cross-domain AU detection?\n\nTo respond the first challenge of autonomously identification of domain-sensitive features, AUDD incorporates a lightweight, trainable domain classifier. This binary classifier is adept at evaluating features derived from Convolutional Neural Networks (CNNs) or transformers to ascertain their relevance for domain classification. The efficacy of this classifier in accurately distinguishing between domains serves as a proxy for gauging the domain-specific sensitivity of the features. Addressing the second challenge, which involves adapting the feature-dropping framework for enhanced crossdomain AU detection, necessitates a sophisticated neural network design. Our approach leverages a hybrid architecture that synergizes an initial CNN layer with subsequent transformer layers, as detailed in  [17] . This strategic combination empowers our AUDD to concurrently mitigate domain-sensitive channels and tokens. Given that each spatial token correlates with a specific facial region reflective of certain AUs, the targeted elimination of tokens extends beyond mere domainspecific feature reduction. It also aids the model in in deducing the underlying connections that are consistent across datasets, enhancing the model's generalisation capabilities. This dualfaceted approach not only can be used to drop the domainsensitive features, but also used to assist the model to implicitly inferring the intrinsic relationships between AUs, thus enhancing cross-domain generalizability.\n\nSpecifically, our proposed AUDD consists of two integral components: (1) Channel Drop Unit (CD-Unit) and (2) Token Drop Unit (TD-Unit). These two components function collaboratively to mitigate domain-specific perturbations, channelwise and token-wise, respectively. The CD-Unit is designed to autonomously identify and then drop the convolutional channels with undesired domain-sensitive attributes. Furthermore, the TD-Unit aims to perceive and preserve local tokens with generalizable features. With the TD-Unit, AUDD is expected to implicitly deduce the inherent AU relationships. It is anticipated to possess significant cross-dataset applicability because each token focuses on specific local areas of the input facial images  [7] . Through the strategic dropping and retention of features, AUDD aspires to achieve a delicate balance between domain adaptability and the preservation of AU-specific information.\n\nFurthermore, previous works have verified that the presence of domain-sensitive features actually exist across both shallow and deep layers of the network  [18] , challenging the conventional dropout practices that predominantly target either highlevel or low-level features. In response, we adopt a progressive training strategy wherein, during each iteration, a CNN block preceding and a transformer block succeeding are chosen at random, followed by the application of a CD-Unit and TD-Unit for adaptive feature dropout. This progressive training strategy ensures a comprehensive attenuation of domainsensitive channels and tokens throughout the network's hierarchy. Concurrently, it inherently safeguards against the excessive exclusion of features, a common pitfall that could precipitate training instability. As illustrated in Fig.  1 , AUDD achieves superior cross-domain AU detection accuracy than DANN  [19]  and the typical baseline methods under various settings. More experimental details and comparisons can be found in Sec. IV.\n\nOur main contributions can be concluded as follows:\n\n• We introduce an innovative doubly adaptive feature selection approach aimed at improving cross-domain facial AU detection. The proposed AUDD method deliberately diminishes domain-specific features to ensure the adaptability of AU detection across various domains. • AUDD seamlessly integrates CD-Unit and TD-Unit, which collaboratively suppress domain-related nuisances while retaining features that are both generalizable and discriminative for AU detection. These units introduce a minimal number of trainable parameters and impose no additional computational load during the inference phase. The remainder of this paper is organized as follows. In Section II we review related work in facial action unit detection, domain adaptation and dropout. In Section III, we present the details of our AUDD method, while in Section IV, we demonstrate its efficacy through experiments.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Related Work",
      "text": "Facial action unit detection and unsupervised domain adaptation. Automatic AU detection has been studied for decades and a number of approaches have been proposed. Given that AUs correspond to subtle alterations in facial muscle regions and skin textures, numerous methods leverage facial landmarks  [20]  or employ adaptive attention mechanisms  [21, 22]  to acquire region-specific representations.\n\nAcknowledging the inherent dependencies and exclusions among different facial action units, some literature suggests explicitly incorporating structural information to enhance the robustness of AU detection. On the other hand, the limited availability of training data for AU detection necessitates exploration into alternative methodologies. Some studies delve into self-supervised learning approaches  [23, 24]  and weaklysupervised methods  [25]  to augment the generalizability of AU detectors. Reference  [26]  introduced a self-supervised learning framework aimed at acquiring pose-invariant AU features by predicting separate displacements for pose and AU between randomly sampled facial frames belonging to the same subject. Similarly, Chang et al.  [23]  proposed a contrastive learning component that capitalizes on inter-area differences to acquire AU-related local representations while preserving intra-area instance discrimination. However, existing AU detection models usually exhibit significant overfitting issue on the training data, resulting in reduced performance when confronted with out-of-distribution or cross-dataset data  [9, 10, 11] . Such obvious AU detection accuracy degradation caused by data distribution discrepancy hinders the applications of current AU detectors, as in reality the various new scenarios usually exhibit different data distributions.\n\nIn this manuscript, we conduct a comparative analysis of the proposed method with typical Unsupervised Domain Adaptation (UDA) techniques, including Domain-Adversarial Neural Network (DANN)  [19] , MCD  [27] . Ganin et al.  [19]  introduced DANN, a method employing an elegant domain classifier that reverses the gradient by multiplying it by a negative scalar during backpropagation to achieve unsupervised domain adaptation. Saito et al.  [27]  proposed Maximum Classifier Discrepancy (MCD), which aims to align the distributions of source and target domains by leveraging task-specific decision boundaries. While these approaches endeavor to align global source and target data distributions to learn domaininvariant representations, the resulting representations do not necessarily ensure the desired class-to-class alignment. Our proposed AUDD bypass this obvious drawback via simultaneously dropping the domain-sensitive features and preserving representations that are discriminative for AU detection.\n\nFurthermore, a recent trend in UDA methods focuses on feature-disentangling-based approaches, e.g., Lee et al.  [28]  proposed a method to disentangle image representations and transfer visual attributes in a latent space for UDA. Chen et al.  [29]  introduced a joint generative and contrastive learning framework for unsupervised person re-identification. Different with these feature-disentangling-based methods, AUDD does not rely on pixel-wise image reconstruction that is quite challenging and can mitigate the domain-sensitive nuisances more flexibly. We will compare AUDD with the general UDA methods and present the results as well as the corresponding analysis in Sec. IV-B.\n\nDropout. Dropout has emerged as a potent algorithm for training robust deep networks, primarily due to its efficacy in mitigating overfitting by discouraging the co-adaptation of feature detectors  [5, 13] . Conceptually, Dropout can be viewed as an approximate model aggregation technique, wherein an exponential number of smaller networks are averaged to form a more robust ensemble.\n\nBased on the vanilla dropout, a plethora of variants have been introduced in the literature, e.g., Morerio et al.  [30]  proposed Curriculum Dropout, which challenges fixed dropout probabilities and employs a time scheduling mechanism to regulate the retention probability of neurons in the network. To alleviate the overfitting issue when finetuning CNN on small datasets, Hou et al.  [14]  proposed Weighted Channel Dropout, a technique specifically designed for the regularization of convolutional layers. Subsequently, structured dropout methods have also garnered attention. Ghiasi et al.  [31]  introduced DropBlock, a structured dropout variant where units in a contiguous region of a convolutional feature map are dropped together. Lee et al.  [32]  proposed Drop to Adapt (DTA), which utilizes adversarial dropout to learn discriminative features by enforcing the cluster assumption. However, none of the above approaches consider to drop feature from hybrid granularities, a critical design aspect incorporated in AUDD. AUDD uniquely integrates channel and token-wise dropout mechanisms for cross-domain action unit (AU) detection, distinguishing it from other methods.\n\nSubsequently, Zeng et al.  [33]  devised Correlation based Dropout (CorrDrop), a structural dropout method that regularizes CNNs by dropping feature units based on feature correlation. Guo et al.  [34]  presented a dropout-based domain generalization framework, aiming to enhance channel robustness to domain shifts. In comparison to existing dropoutbased domain adaptation or domain generalization methods, our proposed AUDD offers a unique advantage by being AUspecific. This specificity allows AUDD to provide more effective regularization effects, specifically targeting and mitigating the severe distribution discrepancy issues between the source and target domains.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Iii. Method",
      "text": "In this paper, we study the problem of cross-domain AU detection. Without loss of generality, suppose we have a source\n\ndenote the i-th image in D s , D t , respectively. N, M denote the total number of samples in D s and D t . y j i ∈ {0, 1} represents annotation w.r.t the j-th AU for i-th sample in D s . 1 means the AU is active, 0 means inactive. Under conventional UDA setting, merely the images in the source dataset D s are annotated with y i . Our target is to maximally align the feature distributions of D s and D t without the access to the annotations in the target domain. We will detail the framework and training process bellow.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "A. Framework Overview",
      "text": "We propose a Doubly Adaptive Dropout method, i.e., AUDD, to empower the simultaneous dropout of domain-sensitive convolutional channels and spatial tokens, thus fostering the retention of features with high generalizability across diverse granularities. The architecture of the AUDD is shown in Fig.  2 .\n\nThe network takes facial images from both source and target datasets as input. These images are then passed through several CNN-based residual blocks, resulting in the generation of feature maps. For these generated feature maps, AUDD employs the CD-Unit to generate corresponding binary masks for automatically dropping the domain-sensitive channels. Subsequently, the convolutional feature maps from the last residual block are reshaped to generate input tokens for subsequent transformer blocks. As depicted in Fig.  2 , AUDD simultaneously employs the TD-Unit to adaptively drop and filter the intermediate tokens. Regarding the CDand TD-Units, the former is randomly interspersed among the residual blocks, and similarly, the TD-Unit is incorporated randomly within the transformer part. These units operate by autonomously generating a binary mask that discerns the significance of respective channels and tokens in the context of cross-domain AU detection.\n\nFor the facial images from the source domain, we exploit the multi-label sigmoid cross-entropy loss for AU detection. It is formulated as\n\nwhere J is the number of AUs. y j denotes the j-th ground truth AU label of the input AU sample. ŷj means the predicted AU score. Below, we will present the detailed neural network structure in AUDD in Sec. III-B, how AUDD learns to drop domain-sensitive features with CD-/TD-Units in Sec. III-C.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "B. Hybrid Cnn-Transformer Architecture",
      "text": "To fully leverage the dropout mechanism for suppressing the domain-specific features while concurrently preserving the generalizable attributes across various granularity, AUDD employs a hybrid neural network architecture, combining Convolutional Neural Networks (CNNs) and Transformer blocks, akin to the structure presented in  [17] . Through the integration of a CNN-based residual block, AUDD is adept at channelwise identification and elimination of domain-specific features, facilitated by the subsequent CD-Unit, wherein each channel potentially encapsulates distinct semantic patterns. In parallel, the transformer-based blocks of the network are used to preserve the generalizable features from an alternative viewpoint, with each token representing localized facial regions. This dual approach enables AUDD to drop the undesired features and deduce the inherent relationships among AUs to enhance its generalizability. The details of the hybrid architecture are as follows. We also show the parameters configuration in Tab. I.\n\n(1) The CNN component comprises four successive residual stages, akin to ResNet-50. These stages are utilized to extract detailed spatial features from the input facial image. Following the methodology outlined in reference  [17] , we replaced the fourth stage of ResNet-50 with an equal number of layers as in stage 3. So the total number of layers are unchanged and we can obtain three residual blocks. The output feature maps X i ∈ R Hi×Wi×Ci for the i-th residual block are then flattened into H i × W i tokens, where each token has a C i -dimensional feature vector. Typically, based on the 224×224×3 input image, the feature maps X i∈1,2,3 for the three residual blocks are encoded as 56×56×256, 28×28×512, and 14×14×1024, respectively. The final feature maps from the last residual block are then convolved with a pointwise convolution, resulting in reshaped feature maps X r ∈ R 14×14×768 .\n\n(2) The transformer part is based on the vanilla ViT-B/16 architecture from  [17] . It comprises 12 transformer encoders, each consisting of alternating layers of multiheaded self-attention (MHSA) and MLP layers. Analogous to the preceding CNN part, the total 12 transformer encoders are evenly partitioned into three successive blocks. The reshaped feature maps X r ∈ R 14×14×768 are then flattened to obtain the input feature vector",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "C. Selective Feature Dropout",
      "text": "With the hybrid neural network architecture, we are capable of dropping the undesired channels or tokens if they are domain-sensitive. As illustrate in Fig.  2 , we insert a CD-Unit between two consecutive residual blocks to automatically filter the channels. In parallel, a TD-Unit can be placed between two consecutive transformer blocks to merely preserve the generalizable tokens. Both CD-Unit and TD-Unit function in two steps: (1) Rate the channels or tokens according to their contribution to the true domain prediction; (2) Adaptively drop the channels or tokens according to both the corresponding score and a Weighted Random Selection algorithm  [35] .\n\nDomain-sensitive score generation. We first calculate the domain-sensitive score to determine which channels or tokens are domain-specific and need to be dropped. The scores are calculated based on the parameters of the domain discriminator F d , which is used for domain classification and consists of a Global Average Pooling (GAP) layer F GAP , a Gradient Reversal Layer (GRL) F GRL , and a Fully-Connected (FC) layer F F C . The GAP layer is used to obtain global information. For the CNN part, the feature of the i-th CNN block Xi ∈ R Hi×Wi×Ci is pooled into a global feature Xgi ∈ R Ci . For the transformer part, the global token of the j-th block xgj ∈ R N j is pooled from its input tokens xgj ∈ R N j×Dj , where N j tokens are of D j dimension. Following  [19] , to avoid the negative impact of domain discriminators on the main network, we insert a GRL layer to truncate the gradients before the FC layer. The loss for domain classification is formulated as:\n\nwhere ps i , pt i demote the predicted probability for the input feature, i.e., X i or x j , belonging to the source or target domain, l i corresponds to the i-th residual or transformer block in AUDD.\n\nWe then calculate the domain-sensitive scores based on the domain discriminator. We assume that the more features contribute to true domain classification, the more these features are likely to contain domain-specific information. Therefore, we calculate the correlation between the input channels/tokens and domain-specific information using the weighted activations of the domain classification. To be specific, for the generated features\n\nand transformer blocks from domain d ∈ {s, t}, we formulate them as a set of K sub-features\n\nto be decided whether should be dropped, where K is the size of the sub feature sets, i.e., the number of channels for the CNN part and the number of tokens for the transformer part. The domain-sensitive score for the j-the sub-feature of input feature f d input is computed as\n\nwhere W 0 j and W 1 j is the layer weight of the F F C of domain discriminator for the source and target domain respectively. Domain-irrelated feature selection. With the domainsensitive score, we can then calculate which channels or tokens should be dropped. Higher domain-sensitive score s i demonstrates the higher correlation between the sub-feature and domain-specific information. Therefore, we should drop these sub features with higher domain-sensitive scores. For the computing efficiency, we follow  [14]  and utilize Weighted Random Selection (WRS) to generate the binary mask for dropout. Specifically, for the i-th channel with s i , we first randomly sample a number r ∈ (0, 1) and compute a key value k i = r 1/si . The N mask sub-features with the largest k i will be masked to 0, and the domain-irrelated feature selection mask is formulated as\n\nwhere argmax({k Optimize θ with L 15: end for Let L li do and L lj do respectively represent the domain classification loss generated by the CD-Unit and the TD-Unit, the full objective function of our proposed AUDD can be formulated as,\n\nwhere α and β means the trade-off factors that control the importance of the domain classification constraint. For binary domain classification, we exploit the GRL operation, i.e., reverse the gradient from the domain classifier by a constant value λ, which will be investigated in Sec. IV-C. The overall training process is illustrated in Alg. 1.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Iv. Experiments",
      "text": "",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "A. Experiment Setup",
      "text": "Data Preparation. We evaluate the proposed AUDD on four publicly FACS datasets, including BP4D  [36] , BP4D+  [37] , DISFA  [38] , GFT  [39] . BP4D encompasses data from 41 individuals (23 females and 18 males), yielding approximately 146,000 image frames annotated with 12 distinct AUs. BP4D+ comprises 140 subjects with a corpus of roughly 198,000 frames, each annotated with the same 12 AUs as BP4D. For the purpose of within-domain assessment, the two datasets were partitioned into three segments based on participant identifiers, facilitating a tripartite crossvalidation scheme. The DISFA dataset, featuring 27 participants, includes close to 130,000 frames, each labeled with 8 AUs and corresponding intensity levels ranging from 0 to 5. Frames exhibiting intensities above the threshold of 1 were designated as positive instances, while the remainder were classified as negative. A subject-independent three-fold cross-validation protocol was employed for evaluation. Lastly, the GFT dataset, documenting naturalistic social interactions among 96 participants divided into 32 triads, presents a unique challenge for AU detection due to the moderate degree of outof-plane head movements. Each participant in the GFT dataset is associated with approximately 1,800 frames, annotated with 10 AUs.\n\nTraining and evaluation protocol. In our cross-domain experiments, we organize four datasets into four pairs (BP4D vs. GFT, BP4D+ vs. GFT, BP4D vs. DISFA, and BP4D+ vs. DISFA), where each pair alternates between serving as the source and target domains. During the training phase, the methodology leverages labeled images from the source domain in conjunction with unlabeled images from the target domain to facilitate unsupervised cross-domain adaptation. The GFT dataset is utilized in accordance with its official partitioning, while the DISFA dataset is divided into three subjectindependent folds to serve the purposes of training, validation, and testing, respectively. For the purpose of evaluation within the contexts of BP4D and DISFA, the analysis concentrates on five mutually shared AUs, specifically AU1, AU2, AU4, AU6, and AU12. Conversely, the evaluation encompassing BP4D and GFT focuses on a broader set of ten shared AUs: AU1, AU2, AU4, AU6, AU10, AU12, AU14, AU15, AU23, and AU24. We use F1 score (F 1 = 2RP R+P ) to measure the AU detection performance for the proposed AUDD as well as the compared methods, where R and P denote recall and precision, respectively. In addition, we calculate the average performance over all AUs (AVE).\n\nImplementation Details. During training, we set the actual weight for α and β as 1 : 1 according to the cross-domain AU detection performance on the validation set. We implemented all the experiments using PyTorch on two RTX 3090 GPUs, each with 24 GB memory. We set the respective training batch size N = 36 for the source and target images, respectively. We use the commonly-used data augmentation strategies, including randomly data cropping, random horizontal flipping. The neural architecture, i.e., ResNet50-Vit-B/16  [17]  was pretrained on ImageNet-21K. We set the learning rate as 0.001 and trained the AUDD model for 50 epochs until convergence for each cross-domain setting. We use the popular thop 1  library to measure the number of parameters and FLOPs of the utilized AU encoder. The AU encoder comprises 97.90 million parameters and requires 16.83 GFLOPs.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "B. Comparison With State-Of-The-Arts",
      "text": "We compare AUDD with the representative unsupervised domain adaptation methods for cross-domain AU detection, including DANN  [19] , MCD  [27] , P-MCD  [11] . We also compare AUDD with three representative feature-decouplingbased UDA methods, including DG-Net++  [40] , DRANet  [28] , GCL  [29] . Notably, DG-Net++ and GCL were originally developed for cross-domain person re-identification but were adapted for cross-domain AU detection based on their feature disentangling principle. They can be naturally used for crossdomain AU detection following their feature disentangling principle. For a fair comparison with AUDD, we re-trained these three models using the FACS datasets with the released codes. Besides, we compare AUDD with the another dropoutbased DA method: Drop to Adapt (DTA)  [32] , which leverages adversarial dropout to learn discriminative features by enforcing the cluster assumption. We also compare AUDD with the self-supervised AU detection method TCAE  [24] , where a pretrained AU detection model is publicly available. Following TCAE  [24] , we train a linear classifier with the pre-trained model and evaluate its generalization capability on the target FACS dataset. We show the quantitative cross-domain AU detection comparison in Tab. II, Tab. III, Tab. IV and Tab. V. In all the tables, we use the \"Source\" and \"Target\" as the indicators for the lower and upper bounds. \"Source\" means directly evaluating the source model on the target domain. \"Target\" means withindomain training and evaluation on the target dataset.\n\nFrom the quantitative results in Tab. II, Tab. III, Tab. IV and Tab. V, we have three observations: (1) Compared with the general domain adaptation methods including DANN  [19] , MCD  [27]  and P-MCD  [11] , our proposed AUDD obtains consistent improvements in the average F1 scores. The observed improvements suggest that the AUDD-learned AU features are more cross-domain generalizable and also indicate the effectiveness the proposed channel-wise and token-wise feature dropping mechanism. (2) Compared with the featuredisentangling-based methods, e.g., TCAE  [24] , DGNet++  [40] , DRANet  [28]  and GCL  [29] , that builds their cross-domain AU detection performance on the quality of the generated faces, our proposed AUDD is more straightforward and learns the generalizable features in a discriminative manner via incorporating the prior AU relation knowledge into the framework.\n\n(3) Compared with DTA  [32] , AUDD consistently illustrates its superiority under various cross-domain settings. As a comparison, AUDD is assumption-free and can be trained straightforwardly without alternating adversarial manner.\n\nOverall, the improvements of AUDD over other compared methods should be reasonable because AUDD is capable of dropping and removing the domain-sensitive features both in the shallow and the deep layers. Besides, it specially considers the intrinsic AUs relation during training. We will explore the benefits of each of the components in AUDD in Sec. IV-C.\n\nTo evaluate the robustness of the proposed AUDD model against domain shifts between the source and target domains, the output from the terminal layer of the backbone network serves as representations of action units (AUs). Subsequently, we assess the feature disparities between the source and target domains. As there are N = 768 tokens, we calculate the average activation values for i-th token w.r.t the source/target domain as:\n\n), where x j,i 3 denotes the i-th token generated by the third transformer block w.r.t the j-th input image. n k means the total images in the source (n s = N ) or target (n t = M ) domain. Then we can obtain the domain discrepancies between the source and target domains as:\n\nHere we exploit the maximal values for numeric normalization. Finally, we visualize the cross-domain discrepancies under different crossdomain settings in Fig.  3 . The comparisons presented in Fig.  3  demonstrate that our proposed AUDD consistently exhibits fewer tokens with noticeable domain discrepancies compared to the baseline method. Moreover, most tokens in AUDD demonstrate lower discrepancies than those in the baseline method. The comparisonS depicted in Fig.  3  suggest that AUDD effectively identifies and removes domain-sensitive     Secondly, we perform extra experiments to investigate the contribution of each component in our proposed AUDD, i.e., CD-Unit, TD-Unit, as well as the vanilla domain adversarial learning method. We show the numeric results and comparisons in Tab. VI. In Tab. VI, the Baseline method means we merely use the vanilla ResNet50-ViT-B/16 backbone. \"Baseline+DA\" means we use multi-layer domain adversarial learning with the backbone. \"CD-Unit\" means we merely exploit progressive channel-wise selective dropout. \"TD-Unit\" means we merely exploit progressive token-wise selective dropout. We have four observations: (1) The introduction of multi-layer domain adversarial learning leads to minor enhancements (as seen when comparing column 2 with column 3). This is understandable, given that the domain discriminator's role is to align the features from source and target domains without taking class labels into account. Consequently, while the features may become domain-invariant, the preservation of class-discriminative properties is not assured. (2) Both CD-Unit and TD-Unit are effective. They both show more obvious improvements than merely adding domain adversarial learning, indicating the importance of domain-sensitive feature dropping. The efficacy of the CD-Unit lies in its ability to eliminate domain-sensitive channel-wise features, each representing a unique pattern, while the TD-Unit aids the model in discerning the inherent relationships among AUs, thus enhancing crossdomain adaptability. (3) The TD-Unit (column 5) exhibits marginal superiority over the CD-Unit (column 4). This finding aligns with insights from  [18] , suggesting that although both shallow and deep network layers harbor domain-relevant information, the deeper layers are more likely to be taskspecific and may harbor a greater amount of domain-specific content. (4) AUDD obtains the best results under various different cross-domain AU detection settings, indicating CD-Unit and TD-Unit collaboratively drop the domain-sensitive features from different aspects.\n\nFeature visualization. We show the feature distributions using our proposed AUDD and the vanilla Baseline method in Fig.  5 . For each sub-figure in Fig.  5 , totally 400 facial images were randomly sampled from the testing set of the  target dataset, with half of the images show activated AU6. The features of the selected facial images are projected into a 2D space by t-SNE. It is obvious that incorporating the CD-Unit will achieve more discriminative AU features in the target dataset, indicating more cross-dataset generalizability. Besides, our proposed AUDD (the rightmost columns) shows the best feature discrimination w.r.t AU6. The comparison in Fig.  5  further verifies the effectiveness of the proposed selective dropout learning paradigm for cross domain facial action unit detection.\n\nSensitivity of hyperparameters. Our investigation focuses on assessing the influence of two crucial hyperparameters: λ and γ. Here, λ represents a coefficient regulating the impact of the Gradient Reversal Layer (GRL) within the domain classifier, while γ specifies the proportion of channels or tokens to be selectively discarded each training iteration. The experimental findings are illustrated in Fig.  6 .\n\nFor λ, the empirical data in Fig.  6  (a) clearly indicates that the AUDD model attains optimal performance at λ = 0.25. This result shows the efficacy of gradient reversal operation in facilitating the model's acquisition of domain-invariant features. However, a performance decline is observed with increasing λ values, suggesting that an excessive emphasis on learning domain-invariant features, due to a higher GRL weight, may detract from the model's ability to discern AUspecific features. Regarding γ, optimal results are achieved with a value of γ = 0.33, as shown in Fig.  6 (b ). This finding implies that a lower γ value may be insufficient to prompt the model to effectively eliminate domain-sensitive features. Conversely, an elevated γ value could be detrimental to performance, as the removal of an excessive number of features might pose substantial challenges to the model's training process.\n\nAttention map investigation. We delve into the analysis of GradCAM-based attention maps for our AUDD model to extend our exploration  [41] . Specifically, we present the attention maps in relation to individual AU as well as combinations thereof in Fig.  7  and 8 , respectively. For ease of illustration, combinational AUs are categorized into distinct groups.\n\nWe have three observations. Initially, the efficacy of the CD-Unit is evident, as it facilitates the model's focus on pertinent facial regions to a certain extent, as illustrated by the comparative analysis between the \"Baseline\" and \"CD-Unit\" in each sub-figure. However, the resulting attention maps from the exclusive use of the CD-Unit fail to meet expectations, suggesting that mere channel dropout in convolutional layers is inadequate. Secondly, the integration of the TD-Unit within the AUDD framework notably enhances the clarity of the attention maps, particularly demonstrated in Fig.  7 . This observation suggests a synergistic operation between the CD-Unit and TD-Unit, further augmenting the model's capability. Thirdly, as depicted in the comparison in Fig.  8 , AUDD significantly improves the attention maps concerning combinational AUs. This finding indicates that integrating the TD-Unit effectively captures AU relationships and enhances their representation, contributing to the overall efficacy of the model.  V.\n\nWithin this paper we have introduced a Doubly adaptive Dropout method tailored specifically for cross-domain AU detection. Our approach integrates a Channel Drop Unit (CD-Unit) and a Token Drop Unit (TD-Unit) to effectively mitigate domain-specific disparities at both the channel and token levels. The CD-Unit is tasked to preserve domain-agnostic local patterns within the convolutional feature maps. Concurrently, the TD-Unit is devised to enable the model to discern the inherent AU relationships that exhibit a high degree of generalizability across different domains. These two units collaborate to eliminate domain-sensitive features at varying levels of granularity, aiming to enhance the model's adaptability and performance in various cross-domain settings. One limitation of AUDD is that it does not incorporate foundational vision models or language-based AU definition to further improve the generalization of cross-domain AU detection. We will explore these in future work.",
      "page_start": 6,
      "page_end": 6
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Our proposed AUDD shows its superiority on a broad range of 8 cross-",
      "page": 1
    },
    {
      "caption": "Figure 2: The network takes facial images from both source and",
      "page": 3
    },
    {
      "caption": "Figure 2: (a) The framework of the proposed AUDD for cross-domain AU detection. AUDD takes as input a hybrid batch of source and target images and",
      "page": 4
    },
    {
      "caption": "Figure 2: , we insert a CD-Unit",
      "page": 5
    },
    {
      "caption": "Figure 3: The comparisons presented in Fig. 3",
      "page": 7
    },
    {
      "caption": "Figure 3: suggest that",
      "page": 7
    },
    {
      "caption": "Figure 3: Domain discrepancy comparisons between AUDD and Baseline",
      "page": 9
    },
    {
      "caption": "Figure 4: Domain classification performance w.r.t. different blocks (Source:",
      "page": 9
    },
    {
      "caption": "Figure 4: Firstly, the vanilla baseline method show quite high",
      "page": 9
    },
    {
      "caption": "Figure 5: For each sub-figure in Fig. 5, totally 400 facial",
      "page": 9
    },
    {
      "caption": "Figure 5: t-SNE visualization of the learned features. AUDD shows the best feature discrimination w.r.t AU6. Red/Blue indicate AU6 exists or not.",
      "page": 10
    },
    {
      "caption": "Figure 6: Performance variations under different choice of λ and γ (Source:",
      "page": 10
    },
    {
      "caption": "Figure 5: further verifies the effectiveness of the proposed",
      "page": 10
    },
    {
      "caption": "Figure 6: For λ, the empirical data in Fig. 6 (a) clearly indicates that",
      "page": 10
    },
    {
      "caption": "Figure 6: (b). This",
      "page": 10
    },
    {
      "caption": "Figure 7: and 8, respectively. For ease of illustration,",
      "page": 10
    },
    {
      "caption": "Figure 7: This observation",
      "page": 10
    },
    {
      "caption": "Figure 8: , AUDD significantly",
      "page": 10
    },
    {
      "caption": "Figure 7: Visualization of attention maps of the baseline and the compared methods. In sub-figures (a) and (c), BP4D is designated as the source dataset and",
      "page": 11
    },
    {
      "caption": "Figure 8: Visualization of attention maps of the baseline and the compared methods. The sub-figure (a) delineates the attention maps with BP4D+ as the source",
      "page": 11
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "[1×1,64;3×3,64;1×1,256]×3": "[1×1,128;3×3,128;1×1,512]×4"
        },
        {
          "[1×1,64;3×3,64;1×1,256]×3": "[1×1,256;3×3,256;1×1,1024]×9"
        },
        {
          "[1×1,64;3×3,64;1×1,256]×3": "1×1,768;Flatten;PositionEmbedding"
        },
        {
          "[1×1,64;3×3,64;1×1,256]×3": "[dim768;head12]×4"
        },
        {
          "[1×1,64;3×3,64;1×1,256]×3": "[dim768;head12]×4"
        },
        {
          "[1×1,64;3×3,64;1×1,256]×3": "[dim768;head12]×4"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "27.7": "10.1\n17.9\n34.3",
          "28.0": "8.1\n18.1\n16.6",
          "52.2": "17.1\n31.5\n52.1",
          "49.5": "33.3\n30.2\n33.5",
          "62.0": "56.8\n38.9\n50.4"
        },
        {
          "27.7": "22.2\n17.9\n33.1\n25.8\n14.4\n37.8",
          "28.0": "37.6\n9.6\n27.0\n34.6\n11.7\n38.1",
          "52.2": "42.1\n38.0\n49.2\n46.0\n21.7\n56.3",
          "49.5": "32.7\n28.2\n33.4\n33.6\n20.6\n53.0",
          "62.0": "61.9\n64.8\n65.7\n69.7\n37.5\n67.6"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "54.1": "27.3\n31.1\n27.2",
          "40.7": "11.5\n24.0\n26.2",
          "46.1": "39.6\n32.6\n32.0",
          "69.7": "49.5\n66.5\n68.3",
          "74.3": "55.7\n54.5\n58.0"
        },
        {
          "54.1": "39.2\n33.6\n45.1\n49.9\n34.1\n56.9",
          "40.7": "44.7\n35.7\n49.7\n48.8\n29.7\n52.4",
          "46.1": "32.8\n41.0\n52.6\n44.1\n30.0\n52.6",
          "69.7": "68.5\n68.6\n60.9\n62.7\n52.3\n74.6",
          "74.3": "57.5\n65.2\n51.1\n46.5\n60.3\n77.4"
        },
        {
          "54.1": "58.3",
          "40.7": "51.1",
          "46.1": "57.0",
          "69.7": "80.8",
          "74.3": "84.7"
        }
      ],
      "page": 8
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Affective computing",
      "authors": [
        "R Picard"
      ],
      "year": "2000",
      "venue": "Affective computing"
    },
    {
      "citation_id": "2",
      "title": "Facial action coding system: a technique for the measurement of facial movement",
      "authors": [
        "E Friesen",
        "P Ekman"
      ],
      "year": "1978",
      "venue": "Palo Alto"
    },
    {
      "citation_id": "3",
      "title": "Occlusion aware facial expression recognition using cnn with attention mechanism",
      "authors": [
        "Y Li",
        "J Zeng",
        "S Shan",
        "X Chen"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Image Processing"
    },
    {
      "citation_id": "4",
      "title": "Automatically detecting pain using facial actions",
      "authors": [
        "P Lucey",
        "J Cohn",
        "S Lucey",
        "I Matthews",
        "S Sridharan",
        "K Prkachin"
      ],
      "year": "2009",
      "venue": "2009 3rd International Conference on Affective Computing and Intelligent Interaction and Workshops"
    },
    {
      "citation_id": "5",
      "title": "Cnn autoencoders and lstm-based reduced order model for student dropout prediction",
      "authors": [
        "K Niu",
        "G Lu",
        "X Peng",
        "Y Zhou",
        "J Zeng",
        "K Zhang"
      ],
      "year": "2023",
      "venue": "Neural Computing an-dApplications"
    },
    {
      "citation_id": "6",
      "title": "Expression-assisted facial action unit recognition under incomplete au annotation",
      "authors": [
        "S Wang",
        "Q Gan",
        "Q Ji"
      ],
      "year": "2017",
      "venue": "Pattern Recognition"
    },
    {
      "citation_id": "7",
      "title": "Meta auxiliary learning for facial action unit detection",
      "authors": [
        "Y Li",
        "S Shan"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "8",
      "title": "A multi-label convolutional neural network approach to cross-domain action unit detection",
      "authors": [
        "S Ghosh",
        "E Laksana",
        "S Scherer",
        "L.-P Morency"
      ],
      "year": "2015",
      "venue": "2015 International Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "9",
      "title": "Cross-domain au detection: Domains, learning approaches, and measures",
      "authors": [
        "I Ertugrul",
        "J Cohn",
        "L Jeni",
        "Z Zhang",
        "L Yin",
        "Q Ji"
      ],
      "year": "2019",
      "venue": "FG"
    },
    {
      "citation_id": "10",
      "title": "Crossing domains for au coding: Perspectives, approaches, and measures",
      "authors": [
        "I Ertugrul",
        "J Cohn",
        "L Jeni",
        "Z Zhang",
        "L Yin",
        "Q Ji"
      ],
      "year": "2020",
      "venue": "IEEE transactions on biometrics, behavior, and identity science"
    },
    {
      "citation_id": "11",
      "title": "Selfsupervised patch localization for cross-domain facial action unit detection",
      "authors": [
        "Y Yin",
        "L Lu",
        "Y Wu",
        "M Soleymani"
      ],
      "year": "2021",
      "venue": "2021 16th IEEE International Conference on Automatic Face and Gesture Recognition (FG 2021)"
    },
    {
      "citation_id": "12",
      "title": "Geoconv: Geodesic guided convolution for facial action unit recognition",
      "authors": [
        "Y Chen",
        "G Song",
        "Z Shao",
        "J Cai",
        "T.-J Cham",
        "J Zheng"
      ],
      "year": "2022",
      "venue": "Pattern Recognition"
    },
    {
      "citation_id": "13",
      "title": "Dropout: a simple way to prevent neural networks from overfitting",
      "authors": [
        "N Srivastava",
        "G Hinton",
        "A Krizhevsky",
        "I Sutskever",
        "R Salakhutdinov"
      ],
      "year": "2014",
      "venue": "The journal of machine learning research"
    },
    {
      "citation_id": "14",
      "title": "Weighted channel dropout for regularization of deep convolutional neural network",
      "authors": [
        "S Hou",
        "Z Wang"
      ],
      "year": "2019",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "15",
      "title": "Channel dropblock: An improved regularization method for fine-grained visual classification",
      "authors": [
        "Y Ding",
        "S Dong",
        "Y Tong",
        "Z Ma",
        "B Xiao",
        "H Ling"
      ],
      "year": "2021",
      "venue": "Channel dropblock: An improved regularization method for fine-grained visual classification"
    },
    {
      "citation_id": "16",
      "title": "Reflash dropout in image super-resolution",
      "authors": [
        "X Kong",
        "X Liu",
        "J Gu",
        "Y Qiao",
        "C Dong"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "17",
      "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
      "authors": [
        "A Dosovitskiy",
        "L Beyer",
        "A Kolesnikov",
        "D Weissenborn",
        "X Zhai",
        "T Unterthiner",
        "M Dehghani",
        "M Minderer",
        "G Heigold",
        "S Gelly"
      ],
      "year": "2020",
      "venue": "International Conference on Learning Representations"
    },
    {
      "citation_id": "18",
      "title": "How transferable are features in deep neural networks?",
      "authors": [
        "J Yosinski",
        "J Clune",
        "Y Bengio",
        "H Lipson"
      ],
      "year": "2014",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "19",
      "title": "Unsupervised domain adaptation by backpropagation",
      "authors": [
        "Y Ganin",
        "V Lempitsky"
      ],
      "year": "2015",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "20",
      "title": "Algrnet: Multi-relational adaptive facial action unit modelling for face representation and relevant recognitions",
      "authors": [
        "X Ge",
        "J Jose",
        "P Wang",
        "A Iyer",
        "X Liu",
        "H Han"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Biometrics, Behavior, and Identity Science"
    },
    {
      "citation_id": "21",
      "title": "Jaa-net: joint facial action unit detection and face alignment via adaptive attention",
      "authors": [
        "Z Shao",
        "Z Liu",
        "J Cai",
        "L Ma"
      ],
      "year": "2021",
      "venue": "International Journal of Computer Vision"
    },
    {
      "citation_id": "22",
      "title": "Action unit detection by exploiting spatial-temporal and labelwise attention with transformer",
      "authors": [
        "L Wang",
        "J Qi",
        "J Cheng",
        "K Suzuki"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "23",
      "title": "Knowledge-driven selfsupervised representation learning for facial action unit recognition",
      "authors": [
        "Y Chang",
        "S Wang"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "24",
      "title": "Self-supervised representation learning from videos for facial action unit detection",
      "authors": [
        "Y Li",
        "J Zeng",
        "S Shan",
        "X Chen"
      ],
      "year": "2019",
      "venue": "CVPR"
    },
    {
      "citation_id": "25",
      "title": "Weakly supervised facial action unit recognition through adversarial training",
      "authors": [
        "G Peng",
        "S Wang"
      ],
      "year": "2018",
      "venue": "CVPR"
    },
    {
      "citation_id": "26",
      "title": "Learning representations for facial actions from unlabeled videos",
      "authors": [
        "Y Li",
        "J Zeng",
        "S Shan"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "27",
      "title": "Maximum classifier discrepancy for unsupervised domain adaptation",
      "authors": [
        "K Saito",
        "K Watanabe",
        "Y Ushiku",
        "T Harada"
      ],
      "year": "2018",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "28",
      "title": "Dranet: Disentangling representation and adaptation networks for unsupervised cross-domain adaptation",
      "authors": [
        "S Lee",
        "S Cho",
        "S Im"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "29",
      "title": "Joint generative and contrastive learning for unsupervised person re-identification",
      "authors": [
        "H Chen",
        "Y Wang",
        "B Lagadec",
        "A Dantcheva",
        "F Bremond"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "30",
      "title": "Curriculum dropout",
      "authors": [
        "P Morerio",
        "J Cavazza",
        "R Volpi",
        "R Vidal",
        "V Murino"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE International Conference on Computer Vision"
    },
    {
      "citation_id": "31",
      "title": "Dropblock: A regularization method for convolutional networks",
      "authors": [
        "G Ghiasi",
        "T.-Y Lin",
        "Q Le"
      ],
      "year": "2018",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "32",
      "title": "Drop to adapt: Learning discriminative features for unsupervised domain adaptation",
      "authors": [
        "S Lee",
        "D Kim",
        "N Kim",
        "S.-G Jeong"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE/CVF international conference on computer vision"
    },
    {
      "citation_id": "33",
      "title": "Corrdrop: Correlation based dropout for convolutional neural networks",
      "authors": [
        "Y Zeng",
        "T Dai",
        "S.-T Xia"
      ],
      "year": "2020",
      "venue": "ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "34",
      "title": "Domaindrop: Suppressing domain-sensitive channels for domain generalization",
      "authors": [
        "J Guo",
        "L Qi",
        "Y Shi"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision"
    },
    {
      "citation_id": "35",
      "title": "Weighted random sampling with a reservoir",
      "authors": [
        "P Efraimidis",
        "P Spirakis"
      ],
      "year": "2006",
      "venue": "Information processing letters"
    },
    {
      "citation_id": "36",
      "title": "Bp4dspontaneous: a high-resolution spontaneous 3d dynamic facial expression database",
      "authors": [
        "X Zhang",
        "L Yin",
        "J Cohn",
        "S Canavan",
        "M Reale",
        "A Horowitz",
        "P Liu",
        "J Girard"
      ],
      "year": "2014",
      "venue": "Image and Vision Computing"
    },
    {
      "citation_id": "37",
      "title": "Multimodal spontaneous emotion corpus for human behavior analysis",
      "authors": [
        "Z Zhang",
        "J Girard",
        "Y Wu",
        "X Zhang",
        "P Liu",
        "U Ciftci",
        "S Canavan",
        "M Reale",
        "A Horowitz",
        "H Yang"
      ],
      "year": "2016",
      "venue": "Proceedings"
    },
    {
      "citation_id": "38",
      "title": "Disfa: A spontaneous facial action intensity database",
      "authors": [
        "S Mavadati",
        "M Mahoor",
        "K Bartlett",
        "P Trinh",
        "J Cohn"
      ],
      "year": "2013",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "39",
      "title": "Sayette group formation task (gft) spontaneous facial expression database",
      "authors": [
        "J Girard",
        "W.-S Chu",
        "L Jeni",
        "J Cohn"
      ],
      "year": "2017",
      "venue": "IEEE international conference on automatic face & gesture recognition"
    },
    {
      "citation_id": "40",
      "title": "Joint disentangling and adaptation for cross-domain person reidentification",
      "authors": [
        "Y Zou",
        "X Yang",
        "Z Yu",
        "B Kumar",
        "J Kautz"
      ],
      "year": "2020",
      "venue": "ECCV"
    },
    {
      "citation_id": "41",
      "title": "Grad-cam: Visual explanations from deep networks via gradient-based localization",
      "authors": [
        "R Selvaraju",
        "M Cogswell",
        "A Das",
        "R Vedantam",
        "D Parikh",
        "D Batra"
      ],
      "year": "2017",
      "venue": "Proceedings"
    }
  ]
}