{
  "paper_id": "2302.02520v1",
  "title": "Pgcn: Pyramidal Graph Convolutional Network For Eeg Emotion Recognition",
  "published": "2023-02-06T01:24:44Z",
  "authors": [
    "Ming Jin",
    "Enwei Zhu",
    "Changde Du",
    "Huiguang He",
    "Jinpeng Li"
  ],
  "keywords": [
    "Emotion Recognition",
    "Electroencephalogram",
    "Graph Convolutional Network",
    "Knowledge-based Modelling computer interaction [1]",
    "mental disease diagnosis and rehabilitation [2]",
    "[3]",
    "transportation [4] and security [5]"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Emotion recognition is essential in the diagnosis and rehabilitation of various mental diseases. In the last decade, electroencephalogram (EEG)-based emotion recognition has been intensively investigated due to its prominative accuracy and reliability, and graph convolutional network (GCN) has become a mainstream model to decode emotions from EEG signals. However, the electrode relationship, especially long-range electrode dependencies across the scalp, may be underutilized by GCNs, although such relationships have been proven to be important in emotion recognition. The small receptive field makes shallow GCNs only aggregate local nodes. On the other hand, stacking too many layers leads to over-smoothing. To solve these problems, we propose the pyramidal graph convolutional network (PGCN), which aggregates features at three levels: local, mesoscopic, and global. First, we construct a vanilla GCN based on the 3D topological relationships of electrodes, which is used to integrate two-order local features; Second, we construct several mesoscopic brain regions based on priori knowledge and employ mesoscopic attention to sequentially calculate the virtual mesoscopic centers to focus on the functional connections of mesoscopic brain regions; Finally, we fuse the node features and their 3D positions to construct a numerical relationship adjacency matrix to integrate structural and functional connections from the global perspective. Experimental results on three public datasets indicate that PGCN enhances the relationship modelling across the scalp and achieves state-of-the-art performance in both subject-dependent and subject-independent scenarios. Meanwhile, PGCN makes an effective trade-off between enhancing network depth and receptive fields while suppressing the ensuing over-smoothing. Our codes are publicly accessible at https://github.com/Jinminbox/PGCN.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Ii. Related Work",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "A. Eeg Emotion Recognition",
      "text": "Since the collected EEG raw data contain artifacts and noise, it is challenging to directly apply EEG raw data for emotion recognition. Pre-processing operations such as filtering, baseline correction, and re-referencing can effectively suppress or eliminate the artifacts and noises. Some work has reported using pre-processed EEG signals for end-to-end emotion recognition. TSception  [17]  uses temporal and spatial convolutional layers to extract the time-frequency characteristics and the difference between the left and right hemispheres for end-to-end emotion recognition. EEGNet  [18]  uses deep separable convolutions to extract Spatio-temporal patterns and is prominent in various BCI tasks.\n\nAlthough the preprocessed EEG data can be directly used for emotion recognition, it is more effective to further extract emotion-related EEG features. After spectral analysis  [19]  and frequency band interception, frequency-domain feature extraction can obtain rhythmic information of neural activity in the brain. Commonly used frequency domain features include power spectral density (PSD) features  [20] , differential entropy (DE) features  [21] , differential asymmetry (DASM), and Rational Asymmetry (RASM), etc.\n\nAfter extracting the EEG features, different models are designed for emotion recognition. Traditional machine learning methods such as support vector machine (SVM)  [22] , and clustering  [23]  have been shown to perform emotion recognition.\n\nDeep learning has become mainstream for processing EEG features with its rapid development in recent years. Zheng et al.  [24]  constructed a deep belief network (DBN) to explore the role of different frequency bands and channels. Yang et al.  [25]  proposes a hierarchical network structure with subnetwork nodes to discriminate human emotions. Since human emotions have continuity in time, recurrent neural network (RNN) or long short-term memory can effectively utilize the temporal correlation of EEG  [26] , ACRNN  [27]  employed CNN to extract the spatial information and applied RNN with extended self-attention to explore the temporal information of EEG feature, Zhang et al.  [28]  introduced CNN and RNN to explore the preserved spatial and temporal information in either a cascade or a parallel manner.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "B. Graph Convolutional Networks",
      "text": "Thanks to the introduction of practical structural information, GCN is considered an effective method for processing non-Euclidean data and has achieved great success in the field of social networks  [29] , knowledge graphs  [30]  and traffic prediction  [31] , and so on.\n\nAn undirected graph can be expressed as G = (V, E), in which E represents the set of edges that connect different nodes in the set of V. By connecting all edge E, an adjacency matrix A ∈ R n×n that characterizes the graph relationship can be constructed, at the same time, by aggregating the feature x ∈ R d of each node, a feature matrix X ∈ R n×d characterizing the features of the nodes on the graph can be constructed, where n denotes the number of nodes and d is the dimension of input features.\n\nThe simplified GCN proposed by Kipf et al.  [29]  effectively simplifies the original complex spectral GCN method  [32] , and the simplified GCN can be expressed as\n\nwhere Ã = A + I and Dii = j Ãij , I ∈ R n×n is the n-dimensional degree matrix, Θ is a trainable weight matrix.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "C. Gcn In Eeg Emotion Recognition",
      "text": "Since the EEG feature is a specific structured non-Euclidean data, GCN-based EEG sentiment recognition has been developed rapidly. DGCNN  [10]  dynamically updates the adjacency matrix characterizing the relationship between nodes by gradient backpropagation to obtain more accurate internode relationships and better emotion recognition. RGNN  [13]  used two regularizers, node-based domain adversarial training, and emotion-aware distribution learning, to optimize the crosssubject emotion recognition effect. Inspired by the neurological knowledge of brain cognitive processes, LGG-net  [33]  proposed local and global graphical filtering layers to learn brain activities within and between different brain functional regions to simulate the complex relationships in human brain cognitive processes, thus achieving the best emotion recognition effect. LR-GCN  [34]  employed self-attention forward updating Laplacian matrices and gradient backpropagation updating adjacency matrices to construct learnable brain electrode relationships jointly. To deal with individual differences and dynamic, uncertain relationships between different EEG regions, (V-IAG)  [14]  proposed the variational instance adaptive graph method and achieved good results.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "D. Emotion And Human Brain",
      "text": "Brain networks have been studied for a long time, with a view to contributing to the understanding of human emotions. Sporns et al.  [35]  proposed that studies related to the human brain's connectome can be conducted at the microscale, mesoscale, and macroscale, corresponding to neurons, neuronal clusters, and brain regions, respectively. However, due to the vast number of neurons in human brain, neuron-level studies are not yet practical, and most of the work focuses on neuronal clusters and larger scales.\n\nThe human brain has significant small-world properties, as demonstrated by the fact that neighboring neurons are more likely to exchange information frequently. He et al.  [15]  used structural image data to construct a 54-node brain network for the first time and observed small-world properties. Many subsequent works have confirmed the small-world properties with different node partitioning methods, and the distribution of node degrees obeyed the power-law distribution with exponential truncation tails  [36] . Hagmann et al.  [16]  used the diffusion spectrum imaging technique to construct a weighted brain structural network and found that the brain network could be divided into six modules, and each module had corresponding core nodes, which were mainly distributed in the frontal, temporal and occipital lobes.\n\nTo further enhance communication efficiency between neurons  [37] , the brain has been found to have significant \"longrange connections.\" Although long-distance connections are more costly in energy and volume than short-distance connections, they significantly reduce the cost of wiring between several indirect short-distance connections, thus providing faster, more direct, and less noisy information transport  [38] .",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Iii. Methodology",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "A. Overview",
      "text": "PGCN builds a pyramidal network that fuses EEG features at different scales layer by layer. The overall architecture of the model is shown in Figure  2 , which can be divided into the following steps. (1) Considering the frequent local connections of brain networks, we construct a sparse structural adjacency matrix based on the spatial distance between electrodes and introduce a GCN to aggregate local features.  (2)  To better distinguish the relevance of different brain regions to emotions, we constructed mesoscopic-scale brain regions based on a priori studies and calculated virtual mesoscopic centers for each brain region to characterize the mesoscopic features. (3) To balance the importance and economics of long-distance connections, we fuse the original nodes with virtual mesoscopic nodes, construct a sparse global graph connectivity network with the help of an attention mechanism, and aggregate global features with graph convolution. (4) Finally, the fused features are fed into 3-layer fully connected network for final emotion recognition tasks.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "B. Local Feature Aggregation",
      "text": "Local feature aggregation focuses on the frequent local connections of the human brain and constructs an effective information transport mechanism to fuse the EEG information of neighboring nodes. To focus on short-range neighboring features, we first constructed sparse graph relations reasoning based on the relative spatial positions of electrodes and made their degrees obey exponentially truncated power distributions  [36]  and kept the adjacency matrix with sufficient sparsity  [13] . After that, we constructed a two-layer GCN to aggregate the features of the adjacency nodes. Finally, we fused the original features with the features after graph convolution to suppress over-smoothing  [39] .\n\n1) Sparse Graph Relation Reasoning: GCN relies on constructing an accurate node relation matrix and calculating the corresponding Laplacian matrix. In the task of EEG emotion recognition, a common way to construct the adjacency matrix of different electrodes i and j is:\n\nwhere N i represents the 2D spatial neighbor of electrode i.\n\nHowever, the method faces some problems: (1) The electrodes of all commercially available brain electrode caps cannot be evenly distributed, and the relationship between electrodes cannot be accurately portrayed by simple numbers 0 and 1. (2) The constructed adjacency matrix will lead to oversparsity, resulting in the loss of some critical connections in the optimization process. (3) Since EEG has very large crosssubject or even cross-session differences, the construction of a fixed adjacency matrix will lead to under-optimization.\n\nInspired by the work of Salvador et al.  [40]  and Zhong et al.  [13] , to better describe the connections between different nodes, we constructed an initial adjacency matrix based on the inverse square of the spatial distance between different nodes:\n\nwhere d ij is 3D distance between node i and j, and δ is the sparsity factor. In the experiment, we found that the best result is obtained when δ is set to 9. We clip A ij greater than 0.1 to maintain the sparsity; and clip A ij less than 1 to reduce the weights of the self-loop and extremely close neighbors.\n\nRefering the work of Jin et al.  [34]  in LR-GCN, we set A as a learnable matrix and update it through gradient backpropagation.\n\n2) GCN for Local Representation Aggregation: After constructing the appropriate adjacency matrix A, we compute the corresponding laplacian matrix L and perform information transport between neighbor nodes with GCN:\n\nwhere H (l) and H (l+1) are the input and output node representations at layer l, respectively; the initial input representations H (0) are the original input features X. W (l) is a learnable weight matrix and σ is an activation function.\n\nIn order to prevent the GCN from over-smoothing, we designed only a two-layer local GCN network and introduced cross-layer connections; the final output of the local feature aggregation is: (1) , H (2)  (5)",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "C. Mesoscopic Feature Aggregation",
      "text": "Different areas of the human cerebral cortex are highly connected and centralized, and some work has been reported dividing the cortex into several emotion-related brain regions  [16] ,  [41] . With reference to priori knowledge of current brain science research, we constructed different mesoscopic brain regions and then calculated the location and feature of the virtual mesoscopic center in each region. Since the features in the region only converge to the virtual mesoscopic center, it can effectively avoid the over-smoothing while increasing the perceptual field.\n\n1) Graph Coarsen for Mesoscopic Regions: To obtain the mesoscopic feature aggregation, we designed two different mesoscopic divisions with different sensory fields, as shown in Figure  3 . Figure  3 (a) shows the first partition with reference to the anatomy of the cerebral cortex. The brain's cortex is generally divided into four lobes, frontal, parietal, temporal, and occipital  [42] , each lobe being responsible for a different task; for example, the occipital lobe is associated with visual processing and interpretation. Firstly, we divided the electrodes into five regions; secondly, as electrodes located in the temporal lobe (FT7, T7, TP7, FT8, T8, TP8) have been reported to be important for emotion recognition, we performed a more detailed secondary division of the electrodes in the temporal lobe  [24] ,  [43] ; finally, inspired by the work of Hagmann et al.  [16] , we adjusted the division of the regions to meet the needs of both structural and functional connectivity.\n\nFigure  3 (b) shows the other valid way to devide the mesoscopic regions based on the brain's two hemispheres. The corpus callosum connects the two hemispheres of the brain, and all human activities are realized through their information interaction. However, the functions of the two halves of the brain differ significantly.\n\n2) Mesoscopic Node Relation: After constructing the mesoscopic brain regions, we focused on functional connectivity within the mesoscopic regions through a self-attention. The input is the set of features within each mesoscopic region, h = h 1 , h 2 , . . . , h N , h ∈ R N ×F , where N is the number of nodes in the mesoscopic region, and F is the number of features in each node. The attention-based connectivity matrix e can be expressed as:\n\nwhere W is a learnable weight matrix and • T is transposition.\n\n3) Virtual Mesoscopic Center: After calculating the attention-based connectivity matrix of the mesoscopic region, we begin to construct the virtual mesoscopic centers. We compute the weight coefficient Λ ∈ R 1×N in each mesoscopic region by performing a row-wise summation for e, where Λ i denotes the functional connection weight of each node to all other nodes. After that, we begin to calculate the features and locations of the virtual mesoscopic region center:\n\nwhere P ∈ R N ×3 is the absolute position matrix of the electrodes.\n\nAfter calculating the regional centers of each mesoscopic region, the features and locations are fused according to different divisions. The fused features and locations in Figure  3  (a) and Figure  3\n\nFinally, we implement virtual node imputation to fuse the virtual mesoscopic centers with the original electrode nodes, and obtained a fused feature and location matrix containing vital local and mesoscopic attributes.\n\nX meso = concat (X local ) T , (M (1) ) T , (M (2) ) T T , P meso = concat P T , (P (1) ) T , (P (2) ) T T .",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "D. Global Feature Aggregation",
      "text": "Although mesoscopic partitions can effectively alleviate the node information cannot transport at longer distances, a more extensive range of mesoscopic regions can easily cause information loss as the number of nodes increases.\n\nAttention mechanisms are very effective in constructing global feature relationships  [44] . In order to assemble longdistance node correlations, we construct an attention-based absolute position-dependent feature correlation adjacency matrix to characterize the possible connections between each node and eliminate the weak connection relationships among them. After constructing the global electrode connectivity relationship, we use graph convolution for global feature aggregation.\n\n1) Global Node Relation: We established node position encoding with their spatial location to better characterize the global node relationships. The full nodes P meso consist of the original electrode nodes P and the virtual mesoscopic nodes P virtual . Since the virtual nodes also have corresponding features, we sum the features of all nodes with the position encoding to obtain the position-enhanced node features as follows:\n\nX enhanced = X meso + embed (P meso ) .\n\n(\n\nAfter that, we compute the relation matrix G ∈ R 6×N ×N using a multi-head self-attention containing 6 heads and fuse G into the final attentional relation matrix A global ∈ R N ×N using a learnable weight vector w ∈ R 1×6 :\n\nThe dense attention matrix empirically exacerbate oversmoothing but we intentionally preserve the top 20% connections in the adjacency matrix to ensure its sparsity.\n\n2) GCN for Global Representation Aggregation: After obtaining the global attention-based adjacency matrix A global , we compute the corresponding laplacian matrix Lglobal and employ it for graph convolution:\n\nwhere O (l) are the input node representations and O (l+1) are the output node representations, the initial input representations O (0) are the original input features X meso . W (l) is a learnable weight matrix and σ is activation function.\n\nAfter that, we concatenate the input mesoscopic features X meso with the global GCN output O (1) to obtain the feature that covers the local, mesoscopic, and global perceptual fields.\n\nFinally, we set X global as the input of a three-layer fully connected emotion recognition network to obtain the final output of the subject's emotions. Details of the model implementation are in Appendix A.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Iv. Experiments",
      "text": "In this section, we evaluate the effectiveness of the proposed PGCN on three well-known emotion recognition database, SEED  [24] , SEED-IV  [45] , and SEED-V  [46] .",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "A. Datasets And Protocol",
      "text": "To comprehensively evaluate the effectiveness of the proposed PCGN, we conducted subject-dependent and subjectindependent experiments on the above three datasets; preprocessing and feature extraction was carried out for subsequent objective model evaluation. In the subject-dependent experiments, the training data and testing data were both from the same subject to evaluate the effectiveness of the model for cross-temporal application to the same subject; in the subjectindependent experiments, the training data and testing data were from different subjects to evaluate the effectiveness of the model for the cross-subject application. A detailed description of the dataset and protocol is in Appendix B. The experimental results for all baselines are extracted from the citations.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "B. Experiment On Seed",
      "text": "Table I presents the subject-dependent emotion recognition accuracy of PGCN and all baselines on the SEED dataset, and the results of all baselines are extracted from the corresponding references. SVM is a traditional machine learning method, DBN and BiDANN-S are deep learning methods, and DGCNN, GCB-net+BLS, RGNN, and V-IAG are GCN-based methods.\n\nEncouragingly, on the subject-dependent emotion recognition task, our PGCN achieves the best emotion recognition results on all-frequency band and theta, beta, and gamma bands and performs marginally worse than the previous best model V-IAG in delta and alpha frequencies. For the first time to our knowledge, the accuracy of emotion recognition has been increased to 96.93%. By comparing the results of emotion recognition in different frequency bands, it can be seen that the model is better at capturing the emotional information carried in high-frequency features to achieve higher accuracy, which is in line with the findings of many previous studies  [13] ,  [24] .\n\nTable  II  shows the results of the PGCN and all baselines models for subject-independent emotion recognition on the SEED dataset. In Table  II , we collected the results of both supervise and transfer learning-based experiments.\n\nThe comparison shows that the proposed PGCN achieves the best results among the supervised learning-based methods, leading the RGNN with the domain adaptive module removed by 2.67%. Since the transfer learning uses additional data from the testing set (without using the labels from the testing set) for model training, making the supervised learning approach significantly behind the transfer learning approach, RGNN is 0.71% ahead of PGCN with the DA module added on. However, it is almost impossible to obtain enough data from the test set for training a transfer learning model in practical online emotion recognition.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "C. Experiment On Seed-Iv",
      "text": "Table  III  shows the results of the proposed PGCN for emotion recognition on SEED-IV. In the subject-dependent experiments, the accuracy of PGCN was 2.87% higher than that of RGNN under the same experimental setting. In addition, we present the subject-dependent results for different subjects in different sessions in Figure  4 . The results show that more than two-thirds of the subjects had an accuracy of more than 80%, and one-third of the accuracy was above 90%. In contrast, the emotion recognition results in session 1 were almost evenly distributed between 0.6 and 1, visually demonstrating the considerable variation across subjects under the same experimental setting.\n\nIn the subject-independent experiments, the PGCN showed an improvement of over 3% relative to all baselines except the RGNN. For RGNN without the domain adaption (DA) module, PGCN also showed a 2.04% improvement, and when the DA module was added to RGNN, PGCN was slightly worse by 0.15%. We found that BiHDM and RGNN improved by 1.56% and 2.19%, respectively, with the addition of the DA module, demonstrating the excellent ability of domain adaptation in reducing distribution diversity between subjects.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "D. Experiment On Seed-V",
      "text": "Table  IV  shows the results of the proposed PGCN for emotion recognition on the SEED-V dataset. All experimental results of baselines are extracted from the corresponding",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "V. Discussion",
      "text": "In this chapter, we demonstrate the role of the various modules of the PGCN with the help of ablation experiments, try to decipher why the PGCN works well. We also visualize the results of the PGCN.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "A. Ablation Study",
      "text": "We disassembled and combined the local, mesoscopic and global modules in the PGCN to demonstrate the effect of each module on emotion recognition and show the results in Table V. In the ablation experiment, we only modified the feature extraction network composed of the local, mesoscopic, and global modules without changing other parts. In Table  V , the backbone represents the most basic and commonly used twolayer GCN network based on 2D electrode adjacency matrix  [33] ,  [14] , and backbone module and local module do not activate at the same time.\n\nThe three modules improve the sentiment recognition accuracy of PGCN from 92.34% to 96.93% on the SEED dataset; and from 75.94% to 82.24% on the SEED-IV dataset; furthermore, it seems each module contributes positively. The following is a more detailed discussion of the data in Table  V .\n\n(1) The introduction of each module individually gives a comprehensive boost to the model, with the local module having a Sharpley value of about 2% on the SEED and SEED-IV dataset, the meso module having a Sharpley value of about 3% -3.5% on the SEED and SEED-IV dataset, and the global module having a Sharpley value of about 1% and 2% on the SEED and SEED-IV dataset. (2) Comparing the baseline modules, the meso module can effectively improve the network performance, with a 2.57% improvement on the SEED dataset and a 4.77% improvement on the SEED-IV dataset. We speculate that the improvement may come from that the meso module can extract discriminative features between nodes at the mesoscopic scale with the reference of a priori knowledge and effectively improve the network by fusing the extracted features with the output of the backbone. (3) Since the meso-removed is a combination of the local and global module, comparing it with the local-only module reveals that introducing the global module on top of the local module only improves the emotion recognition ability by 0.39% on SEED, while it brings no improvement on SEED-IV. We hypothesized that for the GCNbased model, stacking network layers is accompanied by a severe over-smoothing problem, and to verify the conjecture; we conducted a more in-depth experiment in Figure  5 . As a comparison, global-only brings about 0.5% improvement on baseline probably because the GCN in the baseline is not sufficiently trained and optimized.\n\nSince the meso module contains two different scales, the brain region scale, and the hemisphere scale, we ablated the submodules in the meso module and presented the result in Table  VI .\n\nBefore the introduction of the meso module, the stacking of local and global modules allowed the network to have the corresponding local and global perceptions but was accompanied by stagnation or even a decrease in the fitting ability due to the deepening of the GCN. By introducing the 7-region and 2region meso module, the network gains mesoscopic perceptual fields while adding some critical virtual mesoscopic centers, resulting in an improved fitting ability.\n\nIn Table  V  and VI, the hyperparameter is optimized for PGCN, but more optimal choices may exist for other models. However, to control the variables and reduce the parameter tuning, we tried to keep the hyperparameters fixed, but this may allow the ablation experiments to demonstrate more significant improvements than model-by-model optimization.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "B. Why Pgcn Works? Network Architecture Analysis",
      "text": "To further explore why PGCN works, we plot line graphs as the layer number of the GCN network increases and depict it in Figure  5  and Figure  6 . In this case, vanilla GCN presents the most commonly used GCN network based on a 2D electrode adjacency matrix, and for the 6-layer network of PGCN, we added a attention-based global-scale layer.\n\nWe plot the node smoothness curve with an increasing network layer in Figure  5  to visualize the over-smoothing problem. In this case, node smoothness calculates the cosine similarity between the nodes of the output features in each layer. Thanks to the redesigned initialized adjacency matrix, the PGCN has a minor increase in node smoothness than the vanilla GCN at the local layer  [53] . At the mesoscopic layer, with the introduction of virtual nodes, the node smoothness remains almost constant or even decreases while the PGCN gains a larger perceptual field, while the node smoothness of the vanilla GCN continues to increase. At the global layer, the global perception field allows the node smoothness of the PGCN to rapidly increase and surpass that of the vanilla GCN. Figure  6  shows the average prediction accuracy of the networks with different layer numbers. For vanilla GCN networks, as the layer number increases, the recognition accuracy tends to decrease after reaching a maximum of 92.34% at two layers, which is why the majority of current GCNs for emotion recognition tasks have two-layer networks  [33] ,  [14] . In the PGCN, as the number of network layers increases, the recognition accuracy reaches a maximum of 96.93% at the number of layers of five and begins to decline.\n\nComparing vanilla GCN with PGCN, it can be seen that thanks to the introduction of sparse graph relational reasoning and local feature aggregation, PGCN can outperform vanilla GCN by 1.2% on shallow networks (layer ≤ 2). When the network continues to deepen (2 < layer ≤ 4), the better emotion recognition effect brought by the larger receptive field in vanilla GCN is gradually at a disadvantage in the competition with the high feature similarity problem caused by over-smoothing, the effectiveness begins to decline; in the PGCN, thanks to the network's ability to aggregate mesoscopic features guided by a priori knowledge in a highly sparse manner, the features are aggregated to virtual nodes, which gives a larger perceptual field without causing excessive feature   similarity problems for the electrode nodes, and the network performance continues to increase. The over-smoothing problem in the vanilla GCN becomes more pronounced when the network reaches the global perceptual field (layer > 4), which further reduces the model's effectiveness; in the PGCN, by fusing electrode nodes with the virtual nodes and employing a sparse attention-based adjacency matrix, the better information interaction due to increasing the perceptual field still competes with the over-interaction of feature information due to over-smoothing at network layer number 5, and the model's performance continues to improve, but as the network layers continue to stack, over-smoothing prevails, and the model's performance declines.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "C. Representation Visualization",
      "text": "To manifest brain activation in the emotion recognition progress, we selected five subjects in each of the SEED and SEED-IV datasets and displayed their heat map of the learned adjacency matrix diagonal elements in Figure  7 . The diagonal elements of the adjacency matrix provide the most intuitive indication of the weights of the node features in the graph convolution, and the diagonal elements of the adjacency matrix were deflated to between 0 and 1 for presentation.\n\nCombining the results from the two datasets, electrodes in the frontal, temporal and occipital lobes gained greater weight after learning. In particular, on the 3-category SEED dataset, there was significant activation of nodes located in the temporal and occipital lobes and partial activation of nodes located in the frontal lobe. On the 4-category SEED-IV dataset, nodes were significantly activated in the temporal and occipital lobes and more significant activation in the frontal lobes. The difference in activation of the nodes on the two datasets may be caused by a degree of inductive bias for the 3-category and 4-category classification tasks, where the four classification task requires attention to information from the frontal, temporal and occipital lobes simultaneously for accurate emotion recognition.\n\nTo show the connections between nodes, we selected two subjects in each of the SEED and SEED-IV datasets, and plot the top-10 connections of the learned adjacency matrix in Figure  8 . Unlike the heat map plotting, we removed the diagonal elements and plotted the connections.\n\nComparing the different chordograms in Figure  8 , the top-10 connections vary considerably across subjects. For example, for subject1 of the SEED dataset, the primary connections are mainly in the frontal and temporal lobes, with fewer links in the occipital lobe; in contrast, for subject2, the critical connections are located in the temporal and occipital lobes, while the frontal lobe decreases in importance. For the SEED-IV dataset, a similar situation exists. Throughout the two datasets, although there were some variations in critical connectivity between subjects, most critical connections were consistently related to the temporal, frontal, and occipital lobes, which are closely related to emotion and vision when combined with the chord diagrams of the two data sets. (a)  In combination with Figures  7  and 8 , despite the crosssubject and cross-dataset subject-related variability, the features of nodes located in the frontal, temporal and occipital lobes have greater self-loop weights, and not only that but by building strong connections with other nodes, nodes located in the regions can play a leading role in the network, ultimately achieving excellent emotion recognition.",
      "page_start": 9,
      "page_end": 10
    },
    {
      "section_name": "Vi. Conclusion",
      "text": "This paper proposes a pyramidal graph convolution network that improves emotion recognition accuracy by effectively finding a balance between the expanded receptive fields of GCN and the consequent over-smoothing problem. In PGCN, three separate modules are designed to acquire information between electrodes at different scales. The local module mainly concentrates on small-world properties, the mesoscopic module learns connections between different brain regions constructed with priori knowledge, and the global module learns the sparse connections between global nodes. The features learned by the three modules are then effectively integrated to improve the effectiveness of emotion recognition. The proposed PGCN achieves the best results on all three publicly available datasets. Our future work will focus on 1) designing optimized meso structures to reduce over-smoothing further while acquiring discriminatory features; 2) exploring a more effective initial brain adjacency matrix; 3) trying to introduce more priori knowledge to improve the emotion recognition accuracy.",
      "page_start": 10,
      "page_end": 10
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: (a) illustrates the",
      "page": 1
    },
    {
      "caption": "Figure 1: The EEG emotion recognition paradigm and the conceptual design of PGCN. (a) The basic ﬂowchart of EEG emotion recognition, which consists",
      "page": 2
    },
    {
      "caption": "Figure 1: (b), PGCN contains three main components. The ﬁrst part",
      "page": 2
    },
    {
      "caption": "Figure 2: , which can be divided",
      "page": 3
    },
    {
      "caption": "Figure 2: The ﬂowchart of the proposed PGCN. To excavate the electrode relationships, PGCN aggregates multiscale information, i.e., local, mesoscopic and",
      "page": 4
    },
    {
      "caption": "Figure 3: Brain region division based on priori knowledge.",
      "page": 5
    },
    {
      "caption": "Figure 3: (a) shows the ﬁrst partition with reference to the",
      "page": 5
    },
    {
      "caption": "Figure 3: (b) shows the other valid way to devide the meso-",
      "page": 5
    },
    {
      "caption": "Figure 3: (a) and Figure 3 (b) are M(1) ∈R7×F and P(1) ∈R7×3,",
      "page": 5
    },
    {
      "caption": "Figure 4: The results show",
      "page": 6
    },
    {
      "caption": "Figure 4: Subject-dependent emotion recognition accuracy on the SEED-IV",
      "page": 7
    },
    {
      "caption": "Figure 5: As a comparison, global-only brings about 0.5%",
      "page": 8
    },
    {
      "caption": "Figure 5: and Figure 6. In this case, vanilla GCN presents the",
      "page": 8
    },
    {
      "caption": "Figure 5: to visualize the over-smoothing",
      "page": 8
    },
    {
      "caption": "Figure 5: The curve of node smoothness with the increasing number of network",
      "page": 8
    },
    {
      "caption": "Figure 6: shows the average prediction accuracy of the",
      "page": 8
    },
    {
      "caption": "Figure 6: Average prediction accuracy of vanilla GCN and PGCN on the SEED",
      "page": 9
    },
    {
      "caption": "Figure 7: The diagonal",
      "page": 9
    },
    {
      "caption": "Figure 8: Unlike the heat map plotting, we removed the",
      "page": 9
    },
    {
      "caption": "Figure 8: , the top-10",
      "page": 9
    },
    {
      "caption": "Figure 7: Heat map of the learned adjacency matrix diagonal elements for subject-dependent emotion recognition on the (a) SEED and (b) SEED-IV datasets,",
      "page": 10
    },
    {
      "caption": "Figure 8: Top 10 connections of learned adjacency matrix on the (a) SEED",
      "page": 10
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "IEEE": "Abstract—Emotion recognition is essential\nin the diagnosis and"
        },
        {
          "IEEE": "rehabilitation\nof\nvarious mental\ndiseases.\nIn\nthe\nlast\ndecade,"
        },
        {
          "IEEE": ""
        },
        {
          "IEEE": "electroencephalogram (EEG)-based emotion recognition has been"
        },
        {
          "IEEE": ""
        },
        {
          "IEEE": "intensively\ninvestigated\ndue\nto\nits\nprominative\naccuracy\nand"
        },
        {
          "IEEE": ""
        },
        {
          "IEEE": "reliability, and graph convolutional network (GCN) has become"
        },
        {
          "IEEE": "a mainstream model\nto\ndecode\nemotions\nfrom EEG signals."
        },
        {
          "IEEE": "However,\nthe\nelectrode\nrelationship,\nespecially long-range\nelec-"
        },
        {
          "IEEE": "trode dependencies\nacross\nthe\nscalp, may be underutilized by"
        },
        {
          "IEEE": "GCNs,\nalthough\nsuch\nrelationships\nhave\nbeen\nproven\nto\nbe"
        },
        {
          "IEEE": ""
        },
        {
          "IEEE": "important in emotion recognition. The small receptive ﬁeld makes"
        },
        {
          "IEEE": ""
        },
        {
          "IEEE": "shallow GCNs only aggregate\nlocal nodes. On the other hand,"
        },
        {
          "IEEE": ""
        },
        {
          "IEEE": "stacking\ntoo many\nlayers\nleads\nto\nover-smoothing. To\nsolve"
        },
        {
          "IEEE": "these problems, we propose the pyramidal graph convolutional"
        },
        {
          "IEEE": "network\n(PGCN), which\naggregates\nfeatures\nat\nthree\nlevels:"
        },
        {
          "IEEE": "local, mesoscopic, and global. First, we construct a vanilla GCN"
        },
        {
          "IEEE": ""
        },
        {
          "IEEE": "based on the 3D topological relationships of electrodes, which is"
        },
        {
          "IEEE": ""
        },
        {
          "IEEE": "used to integrate two-order local\nfeatures; Second, we construct"
        },
        {
          "IEEE": ""
        },
        {
          "IEEE": "several mesoscopic brain regions based on priori knowledge and"
        },
        {
          "IEEE": "employ mesoscopic attention to sequentially calculate the virtual"
        },
        {
          "IEEE": "mesoscopic\ncenters\nto\nfocus\non\nthe\nfunctional\nconnections\nof"
        },
        {
          "IEEE": "mesoscopic brain regions; Finally, we fuse the node features and"
        },
        {
          "IEEE": ""
        },
        {
          "IEEE": "their 3D positions to construct a numerical relationship adjacency"
        },
        {
          "IEEE": ""
        },
        {
          "IEEE": "matrix to integrate\nstructural and functional\nconnections\nfrom"
        },
        {
          "IEEE": ""
        },
        {
          "IEEE": "the\nglobal\nperspective. Experimental\nresults\non\nthree\npublic"
        },
        {
          "IEEE": "datasets\nindicate\nthat PGCN enhances\nthe\nrelationship mod-"
        },
        {
          "IEEE": "elling across the scalp and achieves state-of-the-art performance"
        },
        {
          "IEEE": "in\nboth\nsubject-dependent\nand\nsubject-independent\nscenarios."
        },
        {
          "IEEE": ""
        },
        {
          "IEEE": "Meanwhile, PGCN makes an effective trade-off between enhanc-"
        },
        {
          "IEEE": ""
        },
        {
          "IEEE": "ing network depth and receptive ﬁelds while\nsuppressing\nthe"
        },
        {
          "IEEE": ""
        },
        {
          "IEEE": "ensuing\nover-smoothing. Our\ncodes\nare\npublicly\naccessible\nat"
        },
        {
          "IEEE": "https://github.com/Jinminbox/PGCN."
        },
        {
          "IEEE": ""
        },
        {
          "IEEE": "Index\nTerms—Emotion Recognition,\nElectroencephalogram,"
        },
        {
          "IEEE": "Graph Convolutional Network, Knowledge-based Modelling"
        },
        {
          "IEEE": ""
        },
        {
          "IEEE": ""
        },
        {
          "IEEE": "I.\nINTRODUCTION"
        },
        {
          "IEEE": ""
        },
        {
          "IEEE": "E MOTION recognition is an important module in human-"
        },
        {
          "IEEE": ""
        },
        {
          "IEEE": "rehabilitation [2],\n[3],\ntransportation [4] and security [5]."
        },
        {
          "IEEE": ""
        },
        {
          "IEEE": "Existing works on emotion recognition fall\ninto two cat-"
        },
        {
          "IEEE": ""
        },
        {
          "IEEE": "egories: The ﬁrst\ncategory\nuses\nlow-cost,\neasily\naccessible"
        },
        {
          "IEEE": ""
        },
        {
          "IEEE": "behavioral\nsignals\nsuch as\nspeech [6], gesture [7] and facial"
        },
        {
          "IEEE": ""
        },
        {
          "IEEE": ""
        },
        {
          "IEEE": "This work was supported in part by National Natural Science Foundation"
        },
        {
          "IEEE": "of China\n(62106248), Zhejiang Provincial Natural Science Foundation\nof"
        },
        {
          "IEEE": "China (LQ20F030013), Ningbo Public Service Technology Foundation, China"
        },
        {
          "IEEE": ""
        },
        {
          "IEEE": "(202002N3181),\nand Medical Scientiﬁc Research Foundation\nof Zhejiang"
        },
        {
          "IEEE": ""
        },
        {
          "IEEE": "Province, China (2021431314)."
        },
        {
          "IEEE": "Ming Jin, Enwei Zhu and Jinpeng Li are with HwaMei Hospital, University"
        },
        {
          "IEEE": "of Chinese Academy of Sciences, Ningbo, Zhejiang Province, China. They are"
        },
        {
          "IEEE": ""
        },
        {
          "IEEE": "also with Ningbo Institute of Life and Health Industry, University of Chinese"
        },
        {
          "IEEE": ""
        },
        {
          "IEEE": "Academy of Sciences, Ningbo, Zhejiang Province, China."
        },
        {
          "IEEE": "Changde Du\nis with Research Center\nfor Brain-Inspired\nIntelligence,"
        },
        {
          "IEEE": "Institute of Automation, Chinese Academy of Sciences, Beijing, China."
        },
        {
          "IEEE": ""
        },
        {
          "IEEE": "Huiguang He\nis with\nthe National Laboratory\nof\nPattern Recognition,"
        },
        {
          "IEEE": ""
        },
        {
          "IEEE": "Institute of Automation, Chinese Academy of Sciences, Beijing, China."
        },
        {
          "IEEE": "Corresponding author: Jinpeng Li\n(E-mail:\nlijinpeng@ucas.ac.cn)"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Add": "Local Feature Aggregation with"
        },
        {
          "Add": "Stimuli Presentation"
        },
        {
          "Add": "vanilla GCN \nEEG recording"
        },
        {
          "Add": "(a)\n(b)"
        },
        {
          "Add": "Fig. 1.\nThe EEG emotion recognition paradigm and the conceptual design of PGCN.\n(a) The basic ﬂowchart of EEG emotion recognition, which consists"
        },
        {
          "Add": "of visual-audio stimuli presentation (to evoke\ncorresponding emotions), EEG signal\nacquisition, pre-processing,\nfeature\nextraction,\nemotion classiﬁcation"
        },
        {
          "Add": "and (sometimes)\nfeedback to the subject.\n(b) There are three main components\nfor\nfeature aggregation in the PGCN. The vanilla GCN extracts\nlocal bias"
        },
        {
          "Add": "information from neighboring nodes,\nthe virtual mesoscopic center aggregates information within brain regions constructed with priori knowledge guidance,"
        },
        {
          "Add": "and the attentional GCN further\nfuses structural and functional connectivity at a global\nlevel."
        },
        {
          "Add": "datasets. Experimental\nresults\ndemonstrate\nthat PGCN\nTo address\nthese\nissues, we propose\na graph-based pyra-"
        },
        {
          "Add": "achieves\nstate-of-the-art\nperformance.\nIn\naddition,\nthe\nmidal network that progressively extends the perceptual ﬁeld"
        },
        {
          "Add": "from local to mesoscopic and global and incorporates the priori\nvisualization results further demonstrate the effectiveness"
        },
        {
          "Add": "knowledge from neuroscience research. As\nshown in Figure\nof\nthe proposed method. The code for\nthe paper will be"
        },
        {
          "Add": "open sourced after publication.\n1 (b), PGCN contains\nthree main components. The ﬁrst part"
        },
        {
          "Add": "focuses on strong local connections between different nodes."
        },
        {
          "Add": "The rest of\nthis paper\nis organized as\nfollows: Section II"
        },
        {
          "Add": "It constructs adjacency matrices based on 3D spatial distances"
        },
        {
          "Add": "gives a brief\nreview of\nrelated work; Section III provides a"
        },
        {
          "Add": "of different electrodes and employs a two-layer GCN to fuse"
        },
        {
          "Add": "detailed interpretation of the proposed PGCN;\nthe experimen-"
        },
        {
          "Add": "structural\nassociations\nbetween\nadjacent\nnodes with\nstrong"
        },
        {
          "Add": "tal results of the PGCN on three emotion recognition datasets"
        },
        {
          "Add": "region speciﬁcity. The second part\nfocuses on the functional"
        },
        {
          "Add": "are presented in Section IV; Section V attempts a more in-"
        },
        {
          "Add": "connections between nodes in speciﬁc brain regions. Different"
        },
        {
          "Add": "depth analysis of\nthe PGCN and their visualization; Section"
        },
        {
          "Add": "mesoscopic brain regions\nare delineated based on the brain"
        },
        {
          "Add": "VI elaborates our conclusions."
        },
        {
          "Add": "research prior [15], [16]. The attention correlation coefﬁcients"
        },
        {
          "Add": "between nodes within each region are calculated, and virtual"
        },
        {
          "Add": "II. RELATED WORK"
        },
        {
          "Add": "mesoscopic nodes\nare generated. The\nthird part\nfocuses on"
        },
        {
          "Add": "A. EEG Emotion Recognition"
        },
        {
          "Add": "possible long-distance dependencies between different nodes"
        },
        {
          "Add": "of\nthe whole brain. We use attention to compute global adja-\nSince\nthe\ncollected EEG raw data\ncontain\nartifacts\nand"
        },
        {
          "Add": "cency matrices that fuse numerical and positional relationships\nnoise,\nit\nis\nchallenging to directly apply EEG raw data\nfor"
        },
        {
          "Add": "between nodes and use GCN to fuse node relationships at\nthe\nemotion\nrecognition. Pre-processing\noperations\nsuch\nas ﬁl-"
        },
        {
          "Add": "whole-brain scale. Finally,\nthe features at different scales are\ntering, baseline correction, and re-referencing can effectively"
        },
        {
          "Add": "fused with the original features, and a 3-layer fully connected\nsuppress\nor\neliminate\nthe\nartifacts\nand\nnoises. Some work"
        },
        {
          "Add": "network is used for ﬁnal emotion recognition.\nhas reported using pre-processed EEG signals for end-to-end"
        },
        {
          "Add": "Compared with previous works\nthat mostly use only 2D\nemotion recognition. TSception [17] uses temporal and spatial"
        },
        {
          "Add": "electrode\nrelations\nfor GCN construction,\nthe PGCN fuses\nconvolutional\nlayers to extract\nthe time-frequency characteris-"
        },
        {
          "Add": "absolute\npositional,\nrelative\npositional,\nand\nnumerical\nrela-\ntics and the difference between the left and right hemispheres"
        },
        {
          "Add": "tionships between nodes\ninto the network while drawing on\nfor end-to-end emotion recognition. EEGNet\n[18] uses deep"
        },
        {
          "Add": "a priori\nstudies\nin emotion-related neuroscience to construct\nseparable convolutions to extract Spatio-temporal patterns and"
        },
        {
          "Add": "virtual mesoscopic centers of brain regions. To achieve this\nis prominent\nin various BCI\ntasks."
        },
        {
          "Add": "goal, we construct\nthe PGCN that aggregates EEG features at\nAlthough the preprocessed EEG data can be directly used"
        },
        {
          "Add": "different\nscales\nfrom local, mesoscopic to global. The main\nfor emotion recognition,\nit\nis more effective to further extract"
        },
        {
          "Add": "contributions of\nthis work is threefold:\nemotion-related EEG features. After\nspectral\nanalysis\n[19]"
        },
        {
          "Add": "and\nfrequency\nband\ninterception,\nfrequency-domain\nfeature\n(1) We exploit\nthe Pyramidal Graph Convolutional Network"
        },
        {
          "Add": "extraction can obtain rhythmic information of neural activity\nthat disposes EEG electrode features at different\nscales."
        },
        {
          "Add": "in\nthe\nbrain. Commonly\nused\nfrequency\ndomain\nfeatures\nPGCN effectively excavates\nthe\ninformation embedded"
        },
        {
          "Add": "include power spectral density (PSD) features [20], differential\nin the node’s structural and functional connections,\nthus"
        },
        {
          "Add": "entropy (DE)\nfeatures\n[21], differential asymmetry (DASM),\nimproving the network’s effectiveness."
        },
        {
          "Add": "and Rational Asymmetry (RASM), etc.\n(2) Based\non\npriori\nknowledge\nin\nemotion-related\nneuro-"
        },
        {
          "Add": "After extracting the EEG features, different models are de-\nscience, we\ndesign\ndifferent mesoscopic\nregions\nand"
        },
        {
          "Add": "signed for emotion recognition. Traditional machine learning\ncalculate\ntheir virtual\ncenters\nto distinguish the\nrole of"
        },
        {
          "Add": "methods such as support vector machine (SVM) [22], and clus-\ndifferent brain regions in emotion recognition tasks."
        },
        {
          "Add": "tering [23] have been shown to perform emotion recognition.\n(3) We\nevaluate\nthe network’s performance on three open"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "Deep learning has become mainstream for processing EEG",
          "3": "and dynamic, uncertain relationships between different EEG"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "features with its rapid development\nin recent years. Zheng et",
          "3": "regions,\n(V-IAG)[14] proposed the variational\ninstance adap-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "al.\n[24] constructed a deep belief network (DBN)\nto explore",
          "3": "tive graph method and achieved good results."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "the role of different\nfrequency bands and channels. Yang et",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "al.\n[25] proposes a hierarchical network structure with sub-",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "3": "D. Emotion and Human Brain"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "network nodes to discriminate human emotions. Since human",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "emotions have\ncontinuity in time,\nrecurrent neural network",
          "3": "Brain\nnetworks\nhave been\nstudied\nfor\na\nlong time, with"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "(RNN) or\nlong short-term memory can effectively utilize the",
          "3": "a view to contributing to the understanding of human emo-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "temporal\ncorrelation\nof EEG [26], ACRNN [27]\nemployed",
          "3": "tions. Sporns et al.\n[35] proposed that\nstudies\nrelated to the"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "CNN to extract\nthe spatial\ninformation and applied RNN with",
          "3": "human brain’s connectome can be conducted at the microscale,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "extended self-attention to explore the temporal\ninformation of",
          "3": "mesoscale,\nand macroscale,\ncorresponding to neurons, neu-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "EEG feature, Zhang et al.\n[28]\nintroduced CNN and RNN",
          "3": "ronal clusters, and brain regions,\nrespectively. However, due"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "to explore the preserved spatial and temporal\ninformation in",
          "3": "to the vast number of neurons\nin human brain, neuron-level"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "either a cascade or a parallel manner.",
          "3": "studies are not yet practical, and most of the work focuses on"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "3": "neuronal clusters and larger scales."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "B. Graph Convolutional Networks",
          "3": "The human brain has signiﬁcant small-world properties, as"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "3": "demonstrated by the fact\nthat neighboring neurons are more"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "Thanks\nto the introduction of practical\nstructural\ninforma-",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "3": "likely to exchange information frequently. He et al. [15] used"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "tion, GCN is considered an effective method for processing",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "3": "structural\nimage data\nto construct\na 54-node brain network"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "non-Euclidean data and has achieved great success in the ﬁeld",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "3": "for\nthe ﬁrst\ntime and observed small-world properties. Many"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "of\nsocial networks\n[29], knowledge graphs\n[30]\nand trafﬁc",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "3": "subsequent works have conﬁrmed the small-world properties"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "prediction [31], and so on.",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "3": "with different node partitioning methods, and the distribution"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "An undirected graph can be expressed as G = (V, E),\nin",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "3": "of node degrees obeyed the power-law distribution with ex-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "which E\nrepresents\nthe\nset\nof\nedges\nthat\nconnect\ndifferent",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "3": "ponential\ntruncation tails [36]. Hagmann et al.\n[16] used the"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "nodes in the set of V. By connecting all edge E, an adjacency",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "3": "diffusion spectrum imaging technique to construct a weighted"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "matrix A ∈ Rn×n\nthat\ncharacterizes\nthe graph relationship",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "3": "brain\nstructural\nnetwork\nand\nfound\nthat\nthe\nbrain\nnetwork"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "can\nbe\nconstructed,\nat\nthe\nsame\ntime,\nby\naggregating\nthe",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "3": "could\nbe\ndivided\ninto\nsix modules,\nand\neach module\nhad"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "feature x ∈ Rd of\neach node,\na\nfeature matrix X ∈ Rn×d",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "3": "corresponding core nodes, which were mainly distributed in"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "characterizing the features of\nthe nodes on the graph can be",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "3": "the frontal,\ntemporal and occipital\nlobes."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "constructed, where n denotes\nthe number of nodes and d is",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "3": "To further enhance communication efﬁciency between neu-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "the dimension of\ninput\nfeatures.",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "3": "rons [37],\nthe brain has been found to have signiﬁcant ”long-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "The simpliﬁed GCN proposed by Kipf et al. [29] effectively",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "3": "range\nconnections.” Although long-distance\nconnections\nare"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "simpliﬁes the original complex spectral GCN method [32], and",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "3": "more\ncostly in energy and volume\nthan short-distance\ncon-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "the simpliﬁed GCN can be expressed as",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "3": "nections,\nthey signiﬁcantly reduce the cost of wiring between"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "(cid:17)",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "2 ˜A ˜D− 1\n2 XΘ\n,\nZ = σ\n(cid:16) ˜D− 1\n(1)",
          "3": "several\nindirect\nshort-distance\nconnections,\nthus\nproviding"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "3": "faster, more direct, and less noisy information transport\n[38]."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "˜\n˜\nA = A + I\nI ∈ Rn×n\nwhere\nand\nis\nthe\nDii = (cid:80)\nAij,",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "j",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "n-dimensional degree matrix, Θ is a trainable weight matrix.",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "3": "III. METHODOLOGY"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "C. GCN in EEG emotion recognition",
          "3": "A. Overview"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "Since the EEG feature is a speciﬁc structured non-Euclidean",
          "3": "PGCN builds a pyramidal network that\nfuses EEG features"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "data, GCN-based EEG sentiment\nrecognition\nhas\nbeen\nde-",
          "3": "at\ndifferent\nscales\nlayer\nby\nlayer. The\noverall\narchitecture"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "veloped rapidly. DGCNN [10] dynamically updates\nthe\nad-",
          "3": "of\nthe model\nis\nshown\nin Figure\n2, which\ncan\nbe divided"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "jacency matrix characterizing the relationship between nodes",
          "3": "into the\nfollowing steps.\n(1) Considering the\nfrequent\nlocal"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "by gradient backpropagation to obtain more\naccurate\ninter-",
          "3": "connections of brain networks, we\nconstruct\na\nsparse\nstruc-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "node relationships and better emotion recognition. RGNN[13]",
          "3": "tural adjacency matrix based on the spatial distance between"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "used two regularizers, node-based domain adversarial training,",
          "3": "electrodes and introduce a GCN to aggregate local features. (2)"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "and emotion-aware distribution learning, to optimize the cross-",
          "3": "To better distinguish the relevance of different brain regions"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "subject emotion recognition effect.\nInspired by the neurolog-",
          "3": "to emotions, we\nconstructed mesoscopic-scale brain regions"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "ical knowledge of brain cognitive processes, LGG-net\n[33]",
          "3": "based on a priori\nstudies\nand calculated virtual mesoscopic"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "proposed local\nand global graphical ﬁltering layers\nto learn",
          "3": "centers\nfor each brain region to characterize the mesoscopic"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "brain activities within and between different brain functional",
          "3": "features.\n(3) To\nbalance\nthe\nimportance\nand\neconomics\nof"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "regions to simulate the complex relationships in human brain",
          "3": "long-distance\nconnections, we\nfuse\nthe\noriginal\nnodes with"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "cognitive processes,\nthus\nachieving the best\nemotion recog-",
          "3": "virtual mesoscopic\nnodes,\nconstruct\na\nsparse\nglobal\ngraph"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "nition effect. LR-GCN [34] employed self-attention forward",
          "3": "connectivity network with the help of an attention mechanism,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "updating Laplacian matrices\nand\ngradient\nbackpropagation",
          "3": "and\naggregate\nglobal\nfeatures with\ngraph\nconvolution.\n(4)"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "updating adjacency matrices to construct\nlearnable brain elec-",
          "3": "Finally,\nthe fused features are fed into 3-layer fully connected"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "trode relationships jointly. To deal with individual differences",
          "3": "network for ﬁnal emotion recognition tasks."
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Feature Fusion\nF": "Multi-node Feature Fusion\nV Virtual Node Imputation\nF"
        },
        {
          "Feature Fusion\nF": "Graph Coarsen According to a Priori Knowledge\nG\nC Attention-based Node Relation"
        },
        {
          "Feature Fusion\nF": "Fig. 2.\nThe ﬂowchart of\nthe proposed PGCN. To excavate the electrode relationships, PGCN aggregates multiscale information,\ni.e.,\nlocal, mesoscopic and"
        },
        {
          "Feature Fusion\nF": "global\nfeatures to conduct\nthe emotion classiﬁcation task. GCNs are used to model\nthe relationships."
        },
        {
          "Feature Fusion\nF": "B. Local Feature Aggregation\nInspired by the work of Salvador et al.\n[40] and Zhong et"
        },
        {
          "Feature Fusion\nF": "al.\n[13],\nto better describe the connections between different"
        },
        {
          "Feature Fusion\nF": "Local\nfeature\naggregation\nfocuses\non\nthe\nfrequent\nlocal"
        },
        {
          "Feature Fusion\nF": "nodes, we constructed an initial adjacency matrix based on the"
        },
        {
          "Feature Fusion\nF": "connections of\nthe human brain and constructs\nan effective"
        },
        {
          "Feature Fusion\nF": "inverse square of the spatial distance between different nodes:"
        },
        {
          "Feature Fusion\nF": "information transport mechanism to fuse the EEG information"
        },
        {
          "Feature Fusion\nF": "of neighboring nodes. To focus on short-range neighboring"
        },
        {
          "Feature Fusion\nF": "1\nif\nAij ≥ 1"
        },
        {
          "Feature Fusion\nF": "features, we ﬁrst constructed sparse graph relations reasoning"
        },
        {
          "Feature Fusion\nF": "δ"
        },
        {
          "Feature Fusion\nF": " \nbased on the relative spatial positions of electrodes and made\nif\n0.1 ≤ Aij ≤ 1\n,"
        },
        {
          "Feature Fusion\nF": "(3)\nAij ="
        },
        {
          "Feature Fusion\nF": "d2"
        },
        {
          "Feature Fusion\nF": "ij\ntheir degrees obey exponentially truncated power distributions"
        },
        {
          "Feature Fusion\nF": "0.1\nif\nAij ≤ 0.1\n[36]\nand kept\nthe\nadjacency matrix with sufﬁcient\nsparsity"
        },
        {
          "Feature Fusion\nF": "[13]. After that, we constructed a two-layer GCN to aggregate"
        },
        {
          "Feature Fusion\nF": "is 3D distance between node i and j, and δ is the\nwhere dij"
        },
        {
          "Feature Fusion\nF": "the\nfeatures\nof\nthe\nadjacency\nnodes. Finally, we\nfused\nthe"
        },
        {
          "Feature Fusion\nF": "sparsity factor. In the experiment, we found that the best result"
        },
        {
          "Feature Fusion\nF": "original\nfeatures with the features after graph convolution to"
        },
        {
          "Feature Fusion\nF": "is obtained when δ is set\nto 9. We clip Aij greater than 0.1 to"
        },
        {
          "Feature Fusion\nF": "suppress over-smoothing [39]."
        },
        {
          "Feature Fusion\nF": "less\nthan 1 to reduce the\nmaintain the sparsity; and clip Aij"
        },
        {
          "Feature Fusion\nF": "1)\nSparse Graph Relation Reasoning: GCN relies on con-"
        },
        {
          "Feature Fusion\nF": "weights of\nthe self-loop and extremely close neighbors."
        },
        {
          "Feature Fusion\nF": "structing an accurate node relation matrix and calculating the"
        },
        {
          "Feature Fusion\nF": "et\nal.\nRefering\nthe work\nof\nJin\n[34]\nin LR-GCN, we"
        },
        {
          "Feature Fusion\nF": "corresponding Laplacian matrix.\nIn the task of EEG emotion"
        },
        {
          "Feature Fusion\nF": "set A as\na\nlearnable matrix and update\nit\nthrough gradient"
        },
        {
          "Feature Fusion\nF": "recognition, a common way to construct\nthe adjacency matrix"
        },
        {
          "Feature Fusion\nF": "backpropagation."
        },
        {
          "Feature Fusion\nF": "of different electrodes i and j is:"
        },
        {
          "Feature Fusion\nF": "2) GCN for Local Representation Aggregation: After con-"
        },
        {
          "Feature Fusion\nF": "structing\nthe\nappropriate\nadjacency matrix A, we\ncompute"
        },
        {
          "Feature Fusion\nF": "(cid:40)"
        },
        {
          "Feature Fusion\nF": "1\nif\nj ∈ Ni"
        },
        {
          "Feature Fusion\nF": "the corresponding laplacian matrix ˆL and perform information"
        },
        {
          "Feature Fusion\nF": ",\n(2)\nAij ="
        },
        {
          "Feature Fusion\nF": "0\nif\nj /∈ Ni\ntransport between neighbor nodes with GCN:"
        },
        {
          "Feature Fusion\nF": "represents the 2D spatial neighbor of electrode i.\nwhere Ni\nH(l+1) = σ\n(cid:16)ˆLH(l)W(l)(cid:17)\n,\n(4)"
        },
        {
          "Feature Fusion\nF": "However,\nthe method faces\nsome problems:\n(1) The elec-"
        },
        {
          "Feature Fusion\nF": "where H(l) and H(l+1) are the input and output node represen-\ntrodes\nof\nall\ncommercially\navailable\nbrain\nelectrode\ncaps"
        },
        {
          "Feature Fusion\nF": "tations at\nlayer l, respectively;\nthe initial\ninput representations\ncannot\nbe\nevenly\ndistributed,\nand\nthe\nrelationship\nbetween"
        },
        {
          "Feature Fusion\nF": "H(0)\nare the original\ninput\nfeatures X. W(l)\nis a learnable\nelectrodes cannot be accurately portrayed by simple numbers 0"
        },
        {
          "Feature Fusion\nF": "weight matrix and σ is an activation function.\nand 1. (2) The constructed adjacency matrix will\nlead to over-"
        },
        {
          "Feature Fusion\nF": "sparsity,\nresulting in the loss of\nsome critical connections\nin\nIn\norder\nto\nprevent\nthe GCN from over-smoothing, we"
        },
        {
          "Feature Fusion\nF": "the optimization process. (3) Since EEG has very large cross-\ndesigned only a two-layer local GCN network and introduced"
        },
        {
          "Feature Fusion\nF": "subject or even cross-session differences,\nthe construction of\ncross-layer connections;\nthe ﬁnal output of\nthe local\nfeature"
        },
        {
          "Feature Fusion\nF": "a ﬁxed adjacency matrix will\nlead to under-optimization.\naggregation is:"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "5": "features in each node. The attention-based connectivity matrix"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "(cid:16)\nX, H(1), H(2)(cid:17)\nXlocal = concat",
          "5": "e can be expressed as:"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "5": "e = LeakyReLU((hW)(hW)T ),\n(6)"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "C. Mesoscopic Feature Aggregation",
          "5": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "5": "where W is a learnable weight matrix and ·T is transposition."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "Different\nareas\nof\nthe\nhuman\ncerebral\ncortex\nare",
          "5": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "5": "3) Virtual\nMesoscopic\nCenter:\nAfter\ncalculating\nthe"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "connected and centralized, and some work has been reported",
          "5": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "5": "attention-based connectivity matrix of\nthe mesoscopic region,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "dividing the cortex into several emotion-related brain regions",
          "5": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "5": "we\nbegin\nto\nconstruct\nthe\nvirtual mesoscopic\ncenters. We"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "[16], [41]. With reference to priori knowledge of current brain",
          "5": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "5": "compute the weight coefﬁcient Λ ∈ R1×N in each mesoscopic"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "science\nresearch, we\nconstructed different mesoscopic brain",
          "5": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "5": "region by performing a row-wise summation for e, where Λi"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "regions\nand then calculated the\nlocation and feature of",
          "5": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "5": "denotes the functional connection weight of each node to all"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "virtual mesoscopic center in each region. Since the features in",
          "5": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "5": "other nodes. After that, we begin to calculate the features and"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "the region only converge to the virtual mesoscopic center,",
          "5": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "5": "locations of\nthe virtual mesoscopic region center:"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "can effectively avoid the over-smoothing while increasing the",
          "5": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "perceptual ﬁeld.",
          "5": "plocate = softmax(Λ)P,"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "in Figure 3.": "",
          "electrodes.": ""
        },
        {
          "in Figure 3.": "",
          "electrodes.": "region,\nthe\nfeatures\nand\nlocations\nare"
        },
        {
          "in Figure 3.": "",
          "electrodes.": ""
        },
        {
          "in Figure 3.": "",
          "electrodes.": ""
        },
        {
          "in Figure 3.": "",
          "electrodes.": "3 (a) and Figure 3 (b) are M(1) ∈ R7×F"
        },
        {
          "in Figure 3.": "F7\nF5",
          "electrodes.": ""
        },
        {
          "in Figure 3.": "",
          "electrodes.": "M(2) ∈ R2×F\nand P(2) ∈ R2×3,\nrespectively."
        },
        {
          "in Figure 3.": "FT7\nFC5",
          "electrodes.": ""
        },
        {
          "in Figure 3.": "",
          "electrodes.": ""
        },
        {
          "in Figure 3.": "T7\nC5",
          "electrodes.": ""
        },
        {
          "in Figure 3.": "",
          "electrodes.": ""
        },
        {
          "in Figure 3.": "CP5\nTP7",
          "electrodes.": ""
        },
        {
          "in Figure 3.": "P5\nP7",
          "electrodes.": ""
        },
        {
          "in Figure 3.": "",
          "electrodes.": ""
        },
        {
          "in Figure 3.": "PO7",
          "electrodes.": "vital\nlocal and mesoscopic attributes."
        },
        {
          "in Figure 3.": "CB1",
          "electrodes.": ""
        },
        {
          "in Figure 3.": "",
          "electrodes.": ""
        },
        {
          "in Figure 3.": "",
          "electrodes.": ""
        },
        {
          "in Figure 3.": "",
          "electrodes.": ""
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "After\nthat, we compute the relation matrix G ∈ R6×N (cid:48)×N (cid:48)",
          "6": "method, DBN and BiDANN-S are deep learning methods, and"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "using a multi-head self-attention containing 6 heads and fuse",
          "6": "DGCNN, GCB-net+BLS, RGNN, and V-IAG are GCN-based"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "G into the ﬁnal attentional relation matrix Aglobal ∈ RN (cid:48)×N (cid:48)",
          "6": "methods."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "using a learnable weight vector w ∈ R1×6:",
          "6": "Encouragingly, on the subject-dependent emotion recogni-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "6": "tion task, our PGCN achieves the best emotion recognition re-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "Aglobal = wG.\n(10)",
          "6": "sults on all-frequency band and theta, beta, and gamma bands"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "6": "and performs marginally worse than the previous best model"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "The\ndense\nattention matrix\nempirically\nexacerbate\nover-",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "6": "V-IAG in delta\nand alpha\nfrequencies. For\nthe ﬁrst\ntime\nto"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "smoothing but we intentionally preserve the top 20% connec-",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "6": "our knowledge,\nthe accuracy of emotion recognition has been"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "tions in the adjacency matrix to ensure its sparsity.",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "6": "increased to 96.93%. By comparing the\nresults of\nemotion"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "2) GCN for Global Representation Aggregation: After ob-",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "6": "recognition in different frequency bands, it can be seen that the"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "taining the global\nattention-based adjacency matrix Aglobal,",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "6": "model\nis better at capturing the emotional\ninformation carried"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "we\ncompute\nthe\ncorresponding laplacian matrix ˆLglobal\nand",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "6": "in high-frequency features to achieve higher accuracy, which"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "employ it\nfor graph convolution:",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "6": "is in line with the ﬁndings of many previous studies [13], [24]."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "6": "Table II\nshows\nthe results of\nthe PGCN and all baselines"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "O(l+1) = σ\n(cid:16)ˆLglobalO(l)W(l)(cid:17)\n,\n(11)",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "6": "models\nfor\nsubject-independent\nemotion\nrecognition\non\nthe"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "6": "SEED dataset.\nIn Table\nII, we\ncollected the\nresults of both"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "where O(l)\nare the input node representations and O(l+1)",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "6": "supervise and transfer\nlearning-based experiments."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "are the output node representations,\nthe initial\ninput represen-",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "6": "The comparison shows\nthat\nthe proposed PGCN achieves"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "tations O(0) are the original\ninput\nfeatures Xmeso. W(l)\nis a",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "6": "the best results among the supervised learning-based methods,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "learnable weight matrix and σ is activation function.",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "6": "leading the RGNN with the domain adaptive module removed"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "After\nthat, we\nconcatenate\nthe\ninput mesoscopic\nfeatures",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "6": "by 2.67%. Since the transfer learning uses additional data from"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "Xmeso with the global GCN output O(1)\nto obtain the feature",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "6": "the testing set\n(without using the labels from the testing set)"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "that covers the local, mesoscopic, and global perceptual ﬁelds.",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "6": "for model\ntraining, making the supervised learning approach"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "(cid:16)",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "Xglobal = concat\nXmeso, O(1)(cid:17)\n(12)",
          "6": "signiﬁcantly\nbehind\nthe\ntransfer\nlearning\napproach, RGNN"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "6": "is\n0.71% ahead\nof PGCN with\nthe DA module\nadded\non."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "6": "However,\nit\nis almost\nimpossible to obtain enough data from"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "Finally, we set Xglobal\nas\nthe input of a three-layer\nfully",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "6": "the test set\nfor\ntraining a transfer\nlearning model\nin practical"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "connected\nemotion\nrecognition\nnetwork\nto\nobtain\nthe ﬁnal",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "6": "online emotion recognition."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "output of\nthe\nsubject’s\nemotions. Details of\nthe model\nim-",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "plementation are in Appendix A.",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "6": "C. Experiment on SEED-IV"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "IV. EXPERIMENTS",
          "6": "Table\nIII\nshows\nthe\nresults\nof\nthe\nproposed\nPGCN for"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "6": "emotion\nrecognition\non SEED-IV.\nIn\nthe\nsubject-dependent"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "In this section, we evaluate the effectiveness of the proposed",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "6": "experiments,\nthe accuracy of PGCN was 2.87% higher\nthan"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "PGCN on\nthree well-known\nemotion\nrecognition\ndatabase,",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "6": "that of RGNN under\nthe\nsame\nexperimental\nsetting.\nIn ad-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "SEED[24], SEED-IV[45], and SEED-V[46].",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "6": "dition, we present\nthe subject-dependent\nresults\nfor different"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "6": "subjects\nin different\nsessions\nin Figure 4. The\nresults\nshow"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "A. Datasets and Protocol",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "6": "that more\nthan\ntwo-thirds\nof\nthe\nsubjects\nhad\nan\naccuracy"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "To comprehensively evaluate the effectiveness of\nthe pro-",
          "6": "of more than 80%, and one-third of\nthe accuracy was above"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "posed PCGN, we\nconducted subject-dependent\nand subject-",
          "6": "90%.\nIn contrast,\nthe emotion recognition results\nin session"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "independent\nexperiments\non\nthe\nabove\nthree\ndatasets;\npre-",
          "6": "1 were almost evenly distributed between 0.6 and 1, visually"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "processing\nand\nfeature\nextraction was\ncarried\nout\nfor\nsub-",
          "6": "demonstrating the considerable variation across subjects under"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "sequent objective model evaluation.\nIn the subject-dependent",
          "6": "the same experimental setting."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "experiments,\nthe training data and testing data were both from",
          "6": "In the subject-independent experiments,\nthe PGCN showed"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "the same subject\nto evaluate the effectiveness of the model for",
          "6": "an improvement of over 3% relative to all baselines except\nthe"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "cross-temporal application to the same subject;\nin the subject-",
          "6": "RGNN. For RGNN without the domain adaption (DA) module,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "independent\nexperiments,\nthe\ntraining data\nand testing data",
          "6": "PGCN also showed a 2.04% improvement, and when the DA"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "were from different subjects to evaluate the effectiveness of the",
          "6": "module was added to RGNN, PGCN was\nslightly worse by"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "model for the cross-subject application. A detailed description",
          "6": "0.15%. We found that BiHDM and RGNN improved by 1.56%"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "of the dataset and protocol is in Appendix B. The experimental",
          "6": "and 2.19%, respectively, with the addition of the DA module,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "results for all baselines are extracted from the citations.",
          "6": "demonstrating the\nexcellent\nability of domain adaptation in"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "6": "reducing distribution diversity between subjects."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "B. Experiment on SEED",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "6": "D. Experiment on SEED-V"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "Table I presents the subject-dependent emotion recognition",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "accuracy of PGCN and all baselines on the SEED dataset,",
          "6": "Table\nIV shows\nthe\nresults\nof\nthe\nproposed\nPGCN for"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "and the\nresults of\nall baselines\nare\nextracted from the\ncor-",
          "6": "emotion recognition on the SEED-V dataset. All experimental"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "responding references. SVM is a traditional machine learning",
          "6": "results\nof\nbaselines\nare\nextracted\nfrom the\ncorresponding"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "TABLE I": ""
        },
        {
          "TABLE I": "α (8–13 Hz)"
        },
        {
          "TABLE I": "66.64 / 14.41"
        },
        {
          "TABLE I": "64.01 / 15.97"
        },
        {
          "TABLE I": "74.43 / 12.16"
        },
        {
          "TABLE I": "81.03 / 11.74"
        },
        {
          "TABLE I": "81.97 / 11.05"
        },
        {
          "TABLE I": "75.33 / 8.85"
        },
        {
          "TABLE I": "84.51 / 9.68"
        },
        {
          "TABLE I": "83.74 / 9.57"
        },
        {
          "TABLE I": ""
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "TABLE III": "SUBJECT-DEPENDENT AND SUBJECT-INDEPENDENT CLASSIFICATION"
        },
        {
          "TABLE III": ""
        },
        {
          "TABLE III": "ACCURACY (MEAN/STD) ON THE SEED-IV DATASET"
        },
        {
          "TABLE III": ""
        },
        {
          "TABLE III": ""
        },
        {
          "TABLE III": "subject-dependent"
        },
        {
          "TABLE III": "56.61 / 20.05"
        },
        {
          "TABLE III": "66.77 / 7.38"
        },
        {
          "TABLE III": "69.88 / 16.29"
        },
        {
          "TABLE III": "70.29 / 12.63"
        },
        {
          "TABLE III": "74.35 / 14.09"
        },
        {
          "TABLE III": "79.37 / 10.54"
        },
        {
          "TABLE III": "72.22 / 14.69"
        },
        {
          "TABLE III": "-\n/\n-"
        },
        {
          "TABLE III": "82.24 / 14.85"
        },
        {
          "TABLE III": ""
        },
        {
          "TABLE III": ""
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "79.62 / 10.53\n83.62 / 6.91": "† calculate the average accuracy based on the results of\ntwo sessions.",
          "83.74 / 9.57\n92.33 / 8.66\n93.05 / 5.78\n96.93 / 5.11": ""
        },
        {
          "79.62 / 10.53\n83.62 / 6.91": "TABLE II",
          "83.74 / 9.57\n92.33 / 8.66\n93.05 / 5.78\n96.93 / 5.11": ""
        },
        {
          "79.62 / 10.53\n83.62 / 6.91": "SUBJECT-INDEPENDENT CLASSIFICATION ACCURACY (MEAN/STD) ON",
          "83.74 / 9.57\n92.33 / 8.66\n93.05 / 5.78\n96.93 / 5.11": ""
        },
        {
          "79.62 / 10.53\n83.62 / 6.91": "THE SEED DATASET",
          "83.74 / 9.57\n92.33 / 8.66\n93.05 / 5.78\n96.93 / 5.11": ""
        },
        {
          "79.62 / 10.53\n83.62 / 6.91": "Method\nall bands",
          "83.74 / 9.57\n92.33 / 8.66\n93.05 / 5.78\n96.93 / 5.11": ""
        },
        {
          "79.62 / 10.53\n83.62 / 6.91": "TCA [48]\n63.64 / 14.88",
          "83.74 / 9.57\n92.33 / 8.66\n93.05 / 5.78\n96.93 / 5.11": ""
        },
        {
          "79.62 / 10.53\n83.62 / 6.91": "DANN [49]\n75.08 / 11.18",
          "83.74 / 9.57\n92.33 / 8.66\n93.05 / 5.78\n96.93 / 5.11": ""
        },
        {
          "79.62 / 10.53\n83.62 / 6.91": "Transfer Learning",
          "83.74 / 9.57\n92.33 / 8.66\n93.05 / 5.78\n96.93 / 5.11": ""
        },
        {
          "79.62 / 10.53\n83.62 / 6.91": "BiDANN-S [47]\n84.14 / 6.87",
          "83.74 / 9.57\n92.33 / 8.66\n93.05 / 5.78\n96.93 / 5.11": ""
        },
        {
          "79.62 / 10.53\n83.62 / 6.91": "RGNN [13]\n85.30 / 6.72",
          "83.74 / 9.57\n92.33 / 8.66\n93.05 / 5.78\n96.93 / 5.11": ""
        },
        {
          "79.62 / 10.53\n83.62 / 6.91": "SVM [22]\n56.73 / 16.29",
          "83.74 / 9.57\n92.33 / 8.66\n93.05 / 5.78\n96.93 / 5.11": ""
        },
        {
          "79.62 / 10.53\n83.62 / 6.91": "DGCNN [10]\n79.95 / 9.02",
          "83.74 / 9.57\n92.33 / 8.66\n93.05 / 5.78\n96.93 / 5.11": ""
        },
        {
          "79.62 / 10.53\n83.62 / 6.91": "Supervised Learning",
          "83.74 / 9.57\n92.33 / 8.66\n93.05 / 5.78\n96.93 / 5.11": ""
        },
        {
          "79.62 / 10.53\n83.62 / 6.91": "RGNN w/o DA [13]\n81.92 / 9.35",
          "83.74 / 9.57\n92.33 / 8.66\n93.05 / 5.78\n96.93 / 5.11": ""
        },
        {
          "79.62 / 10.53\n83.62 / 6.91": "PGCN (ours)\n84.59 / 8.68",
          "83.74 / 9.57\n92.33 / 8.66\n93.05 / 5.78\n96.93 / 5.11": ""
        },
        {
          "79.62 / 10.53\n83.62 / 6.91": "",
          "83.74 / 9.57\n92.33 / 8.66\n93.05 / 5.78\n96.93 / 5.11": "Fig. 4.\nSubject-dependent\nemotion recognition accuracy on the SEED-IV"
        },
        {
          "79.62 / 10.53\n83.62 / 6.91": "",
          "83.74 / 9.57\n92.33 / 8.66\n93.05 / 5.78\n96.93 / 5.11": "dataset."
        },
        {
          "79.62 / 10.53\n83.62 / 6.91": "TABLE III",
          "83.74 / 9.57\n92.33 / 8.66\n93.05 / 5.78\n96.93 / 5.11": ""
        },
        {
          "79.62 / 10.53\n83.62 / 6.91": "SUBJECT-DEPENDENT AND SUBJECT-INDEPENDENT CLASSIFICATION",
          "83.74 / 9.57\n92.33 / 8.66\n93.05 / 5.78\n96.93 / 5.11": ""
        },
        {
          "79.62 / 10.53\n83.62 / 6.91": "",
          "83.74 / 9.57\n92.33 / 8.66\n93.05 / 5.78\n96.93 / 5.11": "TABLE IV"
        },
        {
          "79.62 / 10.53\n83.62 / 6.91": "ACCURACY (MEAN/STD) ON THE SEED-IV DATASET",
          "83.74 / 9.57\n92.33 / 8.66\n93.05 / 5.78\n96.93 / 5.11": ""
        },
        {
          "79.62 / 10.53\n83.62 / 6.91": "",
          "83.74 / 9.57\n92.33 / 8.66\n93.05 / 5.78\n96.93 / 5.11": "SUBJECT-DEPENDENT AND SUBJECT-INDEPENDENT CLASSIFICATION"
        },
        {
          "79.62 / 10.53\n83.62 / 6.91": "",
          "83.74 / 9.57\n92.33 / 8.66\n93.05 / 5.78\n96.93 / 5.11": "ACCURACY (MEAN/STD) ON THE SEED-V DATASET"
        },
        {
          "79.62 / 10.53\n83.62 / 6.91": "Method\nsubject-dependent\nsubject-independent",
          "83.74 / 9.57\n92.33 / 8.66\n93.05 / 5.78\n96.93 / 5.11": ""
        },
        {
          "79.62 / 10.53\n83.62 / 6.91": "SVM [22]\n56.61 / 20.05\n37.99 / 12.52",
          "83.74 / 9.57\n92.33 / 8.66\n93.05 / 5.78\n96.93 / 5.11": "Method\nsubject-dependent\nsubject-independent"
        },
        {
          "79.62 / 10.53\n83.62 / 6.91": "DBN [24]\n66.77 / 7.38\n-\n/\n-",
          "83.74 / 9.57\n92.33 / 8.66\n93.05 / 5.78\n96.93 / 5.11": "SVM [24]\n69.5 / 10.28\n-\n/\n-"
        },
        {
          "79.62 / 10.53\n83.62 / 6.91": "69.88 / 16.29\n52.82 / 9.23\nDGCNN [10]",
          "83.74 / 9.57\n92.33 / 8.66\n93.05 / 5.78\n96.93 / 5.11": "BDAE [51]\n79.7 / 4.76\n-\n/\n-"
        },
        {
          "79.62 / 10.53\n83.62 / 6.91": "BiDANN-S [47]\n70.29 / 12.63\n65.59 / 10.39",
          "83.74 / 9.57\n92.33 / 8.66\n93.05 / 5.78\n96.93 / 5.11": "80.77 / 6.61\n-\n/\n-\nMD-AGCN [52]"
        },
        {
          "79.62 / 10.53\n83.62 / 6.91": "74.35 / 14.09\n69.03 / 8.66\nBiHDM [50]",
          "83.74 / 9.57\n92.33 / 8.66\n93.05 / 5.78\n96.93 / 5.11": "PGCN (ours)\n81.69 / 10.57\n61.78 / 8.59"
        },
        {
          "79.62 / 10.53\n83.62 / 6.91": "RGNN [13]\n79.37 / 10.54\n73.84 / 8.02",
          "83.74 / 9.57\n92.33 / 8.66\n93.05 / 5.78\n96.93 / 5.11": ""
        },
        {
          "79.62 / 10.53\n83.62 / 6.91": "BiHDM w/o DA [50]\n72.22 / 14.69\n67.47 / 8.22",
          "83.74 / 9.57\n92.33 / 8.66\n93.05 / 5.78\n96.93 / 5.11": ""
        },
        {
          "79.62 / 10.53\n83.62 / 6.91": "RGNN w/o DA [13]\n-\n/\n-\n71.65 / 9.43",
          "83.74 / 9.57\n92.33 / 8.66\n93.05 / 5.78\n96.93 / 5.11": "V. DISCUSSION"
        },
        {
          "79.62 / 10.53\n83.62 / 6.91": "PGCN (ours)\n82.24 / 14.85\n73.69 / 7.16",
          "83.74 / 9.57\n92.33 / 8.66\n93.05 / 5.78\n96.93 / 5.11": ""
        },
        {
          "79.62 / 10.53\n83.62 / 6.91": "",
          "83.74 / 9.57\n92.33 / 8.66\n93.05 / 5.78\n96.93 / 5.11": "In\nthis\nchapter, we\ndemonstrate\nthe\nrole\nof\nthe\nvarious"
        },
        {
          "79.62 / 10.53\n83.62 / 6.91": "",
          "83.74 / 9.57\n92.33 / 8.66\n93.05 / 5.78\n96.93 / 5.11": "modules of\nthe PGCN with the help of ablation experiments,"
        },
        {
          "79.62 / 10.53\n83.62 / 6.91": "",
          "83.74 / 9.57\n92.33 / 8.66\n93.05 / 5.78\n96.93 / 5.11": "try to decipher why the PGCN works well. We also visualize"
        },
        {
          "79.62 / 10.53\n83.62 / 6.91": "citations. BDAE [51] enhances emotion recognition with the",
          "83.74 / 9.57\n92.33 / 8.66\n93.05 / 5.78\n96.93 / 5.11": "the results of\nthe PGCN."
        },
        {
          "79.62 / 10.53\n83.62 / 6.91": "help\nof\nhigh-level\nrepresentational\nfeatures\nextracted\nby\na",
          "83.74 / 9.57\n92.33 / 8.66\n93.05 / 5.78\n96.93 / 5.11": ""
        },
        {
          "79.62 / 10.53\n83.62 / 6.91": "bimodal deep auto-encoder, MD-AGCN [52] proposes a multi-",
          "83.74 / 9.57\n92.33 / 8.66\n93.05 / 5.78\n96.93 / 5.11": ""
        },
        {
          "79.62 / 10.53\n83.62 / 6.91": "",
          "83.74 / 9.57\n92.33 / 8.66\n93.05 / 5.78\n96.93 / 5.11": "A. Ablation Study"
        },
        {
          "79.62 / 10.53\n83.62 / 6.91": "domain adaptive graph convolution network that\nincorporates",
          "83.74 / 9.57\n92.33 / 8.66\n93.05 / 5.78\n96.93 / 5.11": ""
        },
        {
          "79.62 / 10.53\n83.62 / 6.91": "frequency and temporal domain knowledge, making full use",
          "83.74 / 9.57\n92.33 / 8.66\n93.05 / 5.78\n96.93 / 5.11": "We disassembled and combined the local, mesoscopic and"
        },
        {
          "79.62 / 10.53\n83.62 / 6.91": "of\nthe complementary information of EEG signals.\nIt can be",
          "83.74 / 9.57\n92.33 / 8.66\n93.05 / 5.78\n96.93 / 5.11": "global modules in the PGCN to demonstrate the effect of each"
        },
        {
          "79.62 / 10.53\n83.62 / 6.91": "found that\nthe PGCN improves 0.92% over\nthe previous best",
          "83.74 / 9.57\n92.33 / 8.66\n93.05 / 5.78\n96.93 / 5.11": "module on emotion recognition and show the results in Table"
        },
        {
          "79.62 / 10.53\n83.62 / 6.91": "MD-AGCN on the subject-dependent\ntask, while the PGCN",
          "83.74 / 9.57\n92.33 / 8.66\n93.05 / 5.78\n96.93 / 5.11": "V.\nIn the ablation experiment, we only modiﬁed the feature"
        },
        {
          "79.62 / 10.53\n83.62 / 6.91": "is able to achieve 61.78% accuracy in emotion recognition on",
          "83.74 / 9.57\n92.33 / 8.66\n93.05 / 5.78\n96.93 / 5.11": "extraction network composed of\nthe\nlocal, mesoscopic,\nand"
        },
        {
          "79.62 / 10.53\n83.62 / 6.91": "the subject-independent\ntask.",
          "83.74 / 9.57\n92.33 / 8.66\n93.05 / 5.78\n96.93 / 5.11": "global modules without changing other parts.\nIn Table V,\nthe"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "mance, with a 2.57% improvement on the SEED dataset": "and a 4.77% improvement on the SEED-IV dataset. We",
          "PGCN to rapidly increase and surpass that of the vanilla GCN.": ""
        },
        {
          "mance, with a 2.57% improvement on the SEED dataset": "speculate that\nthe improvement may come from that\nthe",
          "PGCN to rapidly increase and surpass that of the vanilla GCN.": "Subject 1\nSubject 2"
        },
        {
          "mance, with a 2.57% improvement on the SEED dataset": "meso module can extract discriminative features between",
          "PGCN to rapidly increase and surpass that of the vanilla GCN.": ""
        },
        {
          "mance, with a 2.57% improvement on the SEED dataset": "",
          "PGCN to rapidly increase and surpass that of the vanilla GCN.": "vanilla GCN\nPGCN\nvanilla GCN\nPGCN"
        },
        {
          "mance, with a 2.57% improvement on the SEED dataset": "nodes\nat\nthe mesoscopic\nscale with the\nreference of\na",
          "PGCN to rapidly increase and surpass that of the vanilla GCN.": "90\n90"
        },
        {
          "mance, with a 2.57% improvement on the SEED dataset": "priori\nknowledge\nand\neffectively\nimprove\nthe\nnetwork",
          "PGCN to rapidly increase and surpass that of the vanilla GCN.": "80\n80"
        },
        {
          "mance, with a 2.57% improvement on the SEED dataset": "by fusing the\nextracted features with the output of\nthe",
          "PGCN to rapidly increase and surpass that of the vanilla GCN.": ""
        },
        {
          "mance, with a 2.57% improvement on the SEED dataset": "",
          "PGCN to rapidly increase and surpass that of the vanilla GCN.": "70\n70"
        },
        {
          "mance, with a 2.57% improvement on the SEED dataset": "backbone.",
          "PGCN to rapidly increase and surpass that of the vanilla GCN.": "Node Smoothness(%)"
        },
        {
          "mance, with a 2.57% improvement on the SEED dataset": "",
          "PGCN to rapidly increase and surpass that of the vanilla GCN.": "60\n60"
        },
        {
          "mance, with a 2.57% improvement on the SEED dataset": "Since the meso-removed is a combination of the local and",
          "PGCN to rapidly increase and surpass that of the vanilla GCN.": ""
        },
        {
          "mance, with a 2.57% improvement on the SEED dataset": "global module, comparing it with the local-only module",
          "PGCN to rapidly increase and surpass that of the vanilla GCN.": "50\n50"
        },
        {
          "mance, with a 2.57% improvement on the SEED dataset": "",
          "PGCN to rapidly increase and surpass that of the vanilla GCN.": "Local\nMesoscopic\nGlobal\nLocal\nMesoscopic"
        },
        {
          "mance, with a 2.57% improvement on the SEED dataset": "reveals\nthat\nintroducing\nthe\nglobal module\non\ntop\nof",
          "PGCN to rapidly increase and surpass that of the vanilla GCN.": "40\n40"
        },
        {
          "mance, with a 2.57% improvement on the SEED dataset": "",
          "PGCN to rapidly increase and surpass that of the vanilla GCN.": "1\n2\n3\n4\n5\n1\n2\n3\n4"
        },
        {
          "mance, with a 2.57% improvement on the SEED dataset": "the local module only improves the emotion recognition",
          "PGCN to rapidly increase and surpass that of the vanilla GCN.": ""
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "backbone represents the most basic and commonly used two-",
          "8": "B. Why PGCN works? Network Architecture Analysis"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "layer GCN network based on 2D electrode adjacency matrix",
          "8": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "8": "To further explore why PGCN works, we plot line graphs as"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "[33],\n[14],\nand backbone module\nand local module do not",
          "8": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "8": "the layer number of\nthe GCN network increases and depict\nit"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "activate at\nthe same time.",
          "8": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "8": "in Figure 5 and Figure 6. In this case, vanilla GCN presents the"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "The three modules\nimprove the sentiment\nrecognition ac-",
          "8": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "8": "most commonly used GCN network based on a 2D electrode"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "curacy\nof\nPGCN from 92.34% to\n96.93% on\nthe\nSEED",
          "8": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "8": "adjacency matrix, and for\nthe 6-layer network of PGCN, we"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "dataset; and from 75.94% to 82.24% on the SEED-IV dataset;",
          "8": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "8": "added a attention-based global-scale layer."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "furthermore,\nit seems each module contributes positively. The",
          "8": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "8": "We\nplot\nthe\nnode\nsmoothness\ncurve with\nan\nincreasing"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "following is a more detailed discussion of\nthe data in Table",
          "8": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "8": "network\nlayer\nin Figure\n5\nto\nvisualize\nthe\nover-smoothing"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "V.",
          "8": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "8": "problem.\nIn this case, node smoothness calculates the cosine"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "(1) The\nintroduction\nof\neach module\nindividually\ngives\na",
          "8": "similarity between the nodes of\nthe output\nfeatures\nin each"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "comprehensive boost\nto the model, with the local module",
          "8": "layer. Thanks\nto the redesigned initialized adjacency matrix,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "having a Sharpley value of about 2% on the SEED and",
          "8": "the PGCN has a minor\nincrease in node smoothness than the"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "SEED-IV dataset,\nthe meso module having a Sharpley",
          "8": "vanilla GCN at\nthe local\nlayer\n[53]. At\nthe mesoscopic layer,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "value of about 3% - 3.5% on the SEED and SEED-IV",
          "8": "with the introduction of virtual nodes,\nthe node smoothness"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "dataset, and the global module having a Sharpley value",
          "8": "remains almost constant or even decreases while the PGCN"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "of about 1% and 2% on the SEED and SEED-IV dataset.",
          "8": "gains a larger perceptual ﬁeld, while the node smoothness of"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "(2) Comparing\nthe\nbaseline\nand meso-only modules,\nthe",
          "8": "the vanilla GCN continues\nto increase. At\nthe global\nlayer,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "meso module can effectively improve the network perfor-",
          "8": "the global perception ﬁeld allows the node smoothness of\nthe"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "mance, with a 2.57% improvement on the SEED dataset",
          "8": "PGCN to rapidly increase and surpass that of the vanilla GCN."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "and a 4.77% improvement on the SEED-IV dataset. We",
          "8": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "speculate that\nthe improvement may come from that\nthe",
          "8": "Subject 1\nSubject 2"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "meso module can extract discriminative features between",
          "8": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "8": "vanilla GCN\nPGCN\nvanilla GCN\nPGCN"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "nodes\nat\nthe mesoscopic\nscale with the\nreference of\na",
          "8": "90\n90"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "priori\nknowledge\nand\neffectively\nimprove\nthe\nnetwork",
          "8": "80\n80"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "by fusing the\nextracted features with the output of\nthe",
          "8": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "8": "70\n70"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "backbone.",
          "8": "Node Smoothness(%)"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "8": "60\n60"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "(3)\nSince the meso-removed is a combination of the local and",
          "8": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "global module, comparing it with the local-only module",
          "8": "50\n50"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "8": "Local\nMesoscopic\nGlobal\nLocal\nMesoscopic\nGlobal"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "reveals\nthat\nintroducing\nthe\nglobal module\non\ntop\nof",
          "8": "40\n40"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "8": "1\n2\n3\n4\n5\n1\n2\n3\n4\n5"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "the local module only improves the emotion recognition",
          "8": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "8": "# Model Layer"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "ability by 0.39% on SEED, while it brings no improve-",
          "8": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "ment on SEED-IV. We hypothesized that\nfor\nthe GCN-",
          "8": "Fig. 5. The curve of node smoothness with the increasing number of network"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "8": "layers. Node smoothness calculates the mean of the cosine similarity between"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "based model,\nstacking\nnetwork\nlayers\nis\naccompanied",
          "8": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "8": "the nodes of\nthe output\nfeatures of each layer."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "by a severe over-smoothing problem, and to verify the",
          "8": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "conjecture; we conducted a more in-depth experiment\nin",
          "8": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "8": "Figure\n6\nshows\nthe\naverage\nprediction\naccuracy\nof\nthe"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "Figure 5. As a comparison, global-only brings about 0.5%",
          "8": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "8": "networks with different\nlayer numbers. For vanilla GCN net-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "improvement on baseline probably because the GCN in",
          "8": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "8": "works, as the layer number increases, the recognition accuracy"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "the baseline is not sufﬁciently trained and optimized.",
          "8": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "8": "tends\nto decrease\nafter\nreaching a maximum of 92.34% at"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "Since the meso module contains\ntwo different\nscales,\nthe",
          "8": "two layers, which is why the majority of current GCNs\nfor"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "brain region scale, and the hemisphere scale, we ablated the",
          "8": "emotion recognition tasks have two-layer networks [33], [14]."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "submodules\nin the meso module and presented the result\nin",
          "8": "In the PGCN, as the number of network layers increases,\nthe"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "Table VI.",
          "8": "recognition accuracy reaches\na maximum of 96.93% at\nthe"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "Before the introduction of the meso module,\nthe stacking of",
          "8": "number of\nlayers of ﬁve and begins to decline."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "local and global modules allowed the network to have the cor-",
          "8": "Comparing vanilla GCN with PGCN,\nit\ncan be\nseen that"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "responding local and global perceptions but was accompanied",
          "8": "thanks to the introduction of sparse graph relational reasoning"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "by stagnation or even a decrease in the ﬁtting ability due to",
          "8": "and local\nfeature aggregation, PGCN can outperform vanilla"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "the deepening of the GCN. By introducing the 7-region and 2-",
          "8": "GCN by 1.2% on shallow networks\n(layer ≤ 2). When the"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "region meso module, the network gains mesoscopic perceptual",
          "8": "network\ncontinues\nto\ndeepen\n(2 < layer ≤ 4),\nthe\nbetter"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "ﬁelds while adding some critical virtual mesoscopic centers,",
          "8": "emotion\nrecognition\neffect\nbrought\nby\nthe\nlarger\nreceptive"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "resulting in an improved ﬁtting ability.",
          "8": "ﬁeld\nin\nvanilla GCN is\ngradually\nat\na\ndisadvantage\nin\nthe"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "In Table V and VI,\nthe hyperparameter\nis optimized for",
          "8": "competition with the high feature similarity problem caused"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "PGCN, but more optimal choices may exist for other models.",
          "8": "by over-smoothing,\nthe effectiveness begins to decline;\nin the"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "However,\nto control\nthe variables\nand reduce\nthe parameter",
          "8": "PGCN, thanks to the network’s ability to aggregate mesoscopic"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "tuning, we tried to keep the hyperparameters ﬁxed, but\nthis",
          "8": "features guided by a priori knowledge in a highly sparse man-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "may\nallow the\nablation\nexperiments\nto\ndemonstrate more",
          "8": "ner,\nthe features are aggregated to virtual nodes, which gives"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "signiﬁcant\nimprovements than model-by-model optimization.",
          "8": "a\nlarger\nperceptual ﬁeld without\ncausing\nexcessive\nfeature"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "TABLE V": ""
        },
        {
          "TABLE V": "Mesoscopic"
        },
        {
          "TABLE V": "◦"
        },
        {
          "TABLE V": "◦"
        },
        {
          "TABLE V": "◦"
        },
        {
          "TABLE V": "•"
        },
        {
          "TABLE V": "◦"
        },
        {
          "TABLE V": "•"
        },
        {
          "TABLE V": "•"
        },
        {
          "TABLE V": "•"
        },
        {
          "TABLE V": ""
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "at\nthe same time.": "TABLE VI"
        },
        {
          "at\nthe same time.": "EFFECTIVENESS OF MESO-LAYER ON THE SEED AND SEED-IV DATASET."
        },
        {
          "at\nthe same time.": ""
        },
        {
          "at\nthe same time.": ""
        },
        {
          "at\nthe same time.": "7 regions\n2 regions\nSEED\nSEED-IV"
        },
        {
          "at\nthe same time.": ""
        },
        {
          "at\nthe same time.": "◦\n◦\n94.08 / 7.16\n77.84 / 13.08"
        },
        {
          "at\nthe same time.": ""
        },
        {
          "at\nthe same time.": "•\n◦\n94.74 / 6.66\n80.34 / 14.71"
        },
        {
          "at\nthe same time.": ""
        },
        {
          "at\nthe same time.": "◦\n•\n94.85 / 5.91\n79.77 / 13.16"
        },
        {
          "at\nthe same time.": ""
        },
        {
          "at\nthe same time.": "•\n•\n96.93 / 5.11\n82.24 / 14.85"
        },
        {
          "at\nthe same time.": ""
        },
        {
          "at\nthe same time.": "In each line, • means the module is employed, and ◦ means that"
        },
        {
          "at\nthe same time.": "module is blocked."
        },
        {
          "at\nthe same time.": ""
        },
        {
          "at\nthe same time.": ""
        },
        {
          "at\nthe same time.": "vanilla GCN\nPGCN"
        },
        {
          "at\nthe same time.": ""
        },
        {
          "at\nthe same time.": "100"
        },
        {
          "at\nthe same time.": ""
        },
        {
          "at\nthe same time.": "95"
        },
        {
          "at\nthe same time.": ""
        },
        {
          "at\nthe same time.": "Acc (%)\n90"
        },
        {
          "at\nthe same time.": ""
        },
        {
          "at\nthe same time.": "85"
        },
        {
          "at\nthe same time.": "Local\nMesoscopic\nGlobal"
        },
        {
          "at\nthe same time.": "80\n1\n2\n3\n4\n5\n6"
        },
        {
          "at\nthe same time.": "# Model Layer"
        },
        {
          "at\nthe same time.": ""
        },
        {
          "at\nthe same time.": "Fig. 6. Average prediction accuracy of vanilla GCN and PGCN on the SEED"
        },
        {
          "at\nthe same time.": ""
        },
        {
          "at\nthe same time.": "dataset. As\nthe perceptual ﬁeld increases,\nthe accuracy of\nthe vanilla GCN"
        },
        {
          "at\nthe same time.": "begins to decline after\nthe second layer, and the ﬁtting ability of\nthe PGCN"
        },
        {
          "at\nthe same time.": "continues to improve until\nthe ﬁfth layer, when it begins to decline."
        },
        {
          "at\nthe same time.": ""
        },
        {
          "at\nthe same time.": ""
        },
        {
          "at\nthe same time.": ""
        },
        {
          "at\nthe same time.": "similarity problems for\nthe electrode nodes, and the network"
        },
        {
          "at\nthe same time.": "performance continues to increase. The over-smoothing prob-"
        },
        {
          "at\nthe same time.": "lem in the vanilla GCN becomes more pronounced when the"
        },
        {
          "at\nthe same time.": "network reaches the global perceptual ﬁeld (layer > 4), which"
        },
        {
          "at\nthe same time.": "further\nreduces\nthe model’s\neffectiveness;\nin the PGCN, by"
        },
        {
          "at\nthe same time.": "fusing electrode nodes with the virtual nodes and employing a"
        },
        {
          "at\nthe same time.": "sparse attention-based adjacency matrix, the better information"
        },
        {
          "at\nthe same time.": "interaction due\nto increasing the perceptual ﬁeld still\ncom-"
        },
        {
          "at\nthe same time.": "petes with the over-interaction of\nfeature information due to"
        },
        {
          "at\nthe same time.": "over-smoothing at network layer number 5, and the model’s"
        },
        {
          "at\nthe same time.": "performance continues to improve, but as the network layers"
        },
        {
          "at\nthe same time.": "continue\nto stack, over-smoothing prevails,\nand the model’s"
        },
        {
          "at\nthe same time.": "performance declines."
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "1.0": "0.8"
        },
        {
          "1.0": "0.6"
        },
        {
          "1.0": "0.4"
        },
        {
          "1.0": "0.2"
        },
        {
          "1.0": "0.0"
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "1.0": "0.8"
        },
        {
          "1.0": "0.6"
        },
        {
          "1.0": "0.4"
        },
        {
          "1.0": "0.2"
        },
        {
          "1.0": "0.0"
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "self-weighting.": ""
        },
        {
          "self-weighting.": ""
        },
        {
          "self-weighting.": "FCZ\nFC1"
        },
        {
          "self-weighting.": ""
        },
        {
          "self-weighting.": ""
        },
        {
          "self-weighting.": ""
        },
        {
          "self-weighting.": ""
        },
        {
          "self-weighting.": ""
        },
        {
          "self-weighting.": ""
        },
        {
          "self-weighting.": ""
        },
        {
          "self-weighting.": ""
        },
        {
          "self-weighting.": ""
        },
        {
          "self-weighting.": ""
        },
        {
          "self-weighting.": ""
        },
        {
          "self-weighting.": ""
        },
        {
          "self-weighting.": ""
        },
        {
          "self-weighting.": ""
        },
        {
          "self-weighting.": ""
        },
        {
          "self-weighting.": ""
        },
        {
          "self-weighting.": ""
        },
        {
          "self-weighting.": ""
        },
        {
          "self-weighting.": "P1\nPZ\n(a)"
        },
        {
          "self-weighting.": ""
        },
        {
          "self-weighting.": ""
        },
        {
          "self-weighting.": "FC1"
        },
        {
          "self-weighting.": "FCZ"
        },
        {
          "self-weighting.": ""
        },
        {
          "self-weighting.": ""
        },
        {
          "self-weighting.": ""
        },
        {
          "self-weighting.": ""
        },
        {
          "self-weighting.": ""
        },
        {
          "self-weighting.": ""
        },
        {
          "self-weighting.": ""
        },
        {
          "self-weighting.": ""
        },
        {
          "self-weighting.": ""
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "C4": "C6",
          "C4\nC4\nC4\nFP2\nFP2\nFP2": "C6\nC6\nC6\nFPZ\nFPZ\nFPZ",
          "FP2": "FPZ"
        },
        {
          "C4": "T8",
          "C4\nC4\nC4\nFP2\nFP2\nFP2": "T8\nFP1\nFP1\nT8\nT8\nFP1",
          "FP2": "FP1"
        },
        {
          "C4": "TP7",
          "C4\nC4\nC4\nFP2\nFP2\nFP2": "TP7\nTP7\nTP7\nCB2\nCB2\nCB2",
          "FP2": "CB2"
        },
        {
          "C4": "CP5",
          "C4\nC4\nC4\nFP2\nFP2\nFP2": "O2\nO2\nO2\nCP5\nCP5\nCP5\nOZ\nOZ\nOZ",
          "FP2": "O2\nREFERENCES\nOZ"
        },
        {
          "C4": "CP3",
          "C4\nC4\nC4\nFP2\nFP2\nFP2": "CP3\nCP3\nCP3",
          "FP2": ""
        },
        {
          "C4": "CP1",
          "C4\nC4\nC4\nFP2\nFP2\nFP2": "O1\nO1\nO1\nCP1\nCP1\nCP1",
          "FP2": "O1"
        },
        {
          "C4": "CPZ",
          "C4\nC4\nC4\nFP2\nFP2\nFP2": "CPZ\nCPZ\nCPZ\nB1\nB1\nB1",
          "FP2": "B1"
        },
        {
          "C4": "P\nC",
          "C4\nC4\nC4\nFP2\nFP2\nFP2": "P\nP\nP\nC\nC\nC",
          "FP2": "[1] R. Cowie, E. Douglas-Cowie, N. Tsapatsoulis, G. Votsis, S. Kollias,"
        },
        {
          "C4": "2 CP4 CP6 TP8\n7",
          "C4\nC4\nC4\nFP2\nFP2\nFP2": "PO5PO3POZPO4PO6PO8C\nPO5PO3POZPO4PO6PO8C\n2 CP4 CP6 TP8\nPO5PO3POZPO4PO6PO8C\n7\n7\n7",
          "FP2": "PO5PO3POZPO4PO6PO8C\nW. Fellenz, and J. G. Taylor, “Emotion recognition in human-computer"
        },
        {
          "C4": "P",
          "C4\nC4\nC4\nFP2\nFP2\nFP2": "P\nP\nP",
          "FP2": ""
        },
        {
          "C4": "P5",
          "C4\nC4\nC4\nFP2\nFP2\nFP2": "P3\nP5\nP3",
          "FP2": "interaction,” IEEE Signal processing magazine, vol. 18, no. 1, pp. 32–80,"
        },
        {
          "C4": "",
          "C4\nC4\nC4\nFP2\nFP2\nFP2": "P1\nPZ\nP2\nP4\nP6\nP8\nPO7\nP1\nPZ\nP2\nP4\nP6\nP8\nPO7\nP1\nPZ\nP2\nP4\nP6\nP8\nPO7\nP1\nPZ\nP2\nP4\nP6\nP8\n(b)",
          "FP2": "PO7"
        },
        {
          "C4": "",
          "C4\nC4\nC4\nFP2\nFP2\nFP2": "",
          "FP2": "2001."
        },
        {
          "C4": "",
          "C4\nC4\nC4\nFP2\nFP2\nFP2": "",
          "FP2": "[2] V.\nZotev, A. Mayeli, M. Misaki,\nand\nJ. Bodurka,\n“Emotion\nself-"
        },
        {
          "C4": "",
          "C4\nC4\nC4\nFP2\nFP2\nFP2": "Fig. 8.\nTop 10 connections of\nlearned adjacency matrix on the (a) SEED",
          "FP2": ""
        },
        {
          "C4": "",
          "C4\nC4\nC4\nFP2\nFP2\nFP2": "",
          "FP2": "regulation training in major depressive disorder using simultaneous real-"
        },
        {
          "C4": "",
          "C4\nC4\nC4\nFP2\nFP2\nFP2": "dataset and (b) SEED-IV dataset, the electrodes occupying the most important",
          "FP2": ""
        },
        {
          "C4": "",
          "C4\nC4\nC4\nFP2\nFP2\nFP2": "",
          "FP2": "time\nfmri\nand eeg neurofeedback,” NeuroImage: Clinical, vol. 27, p."
        },
        {
          "C4": "",
          "C4\nC4\nC4\nFP2\nFP2\nFP2": "connections are distributed in the frontal,\ntemporal and occipital\nlobes.",
          "FP2": ""
        },
        {
          "C4": "",
          "C4\nC4\nC4\nFP2\nFP2\nFP2": "",
          "FP2": "102331, 2020."
        },
        {
          "C4": "",
          "C4\nC4\nC4\nFP2\nFP2\nFP2": "",
          "FP2": "[3]\nJ. K. Carpenter, L. A. Andrews, S. M. Witcraft, M. B. Powers,\nJ. A."
        },
        {
          "C4": "",
          "C4\nC4\nC4\nFP2\nFP2\nFP2": "",
          "FP2": "Smits, and S. G. Hofmann, “Cognitive behavioral\ntherapy for anxiety"
        },
        {
          "C4": "",
          "C4\nC4\nC4\nFP2\nFP2\nFP2": "In combination with Figures 7 and 8, despite\nthe\ncross-",
          "FP2": "and related disorders: A meta-analysis of randomized placebo-controlled"
        },
        {
          "C4": "",
          "C4\nC4\nC4\nFP2\nFP2\nFP2": "subject\nand cross-dataset\nsubject-related variability,\nthe\nfea-",
          "FP2": "trials,” Depression and anxiety, vol. 35, no. 6, pp. 502–514, 2018."
        },
        {
          "C4": "",
          "C4\nC4\nC4\nFP2\nFP2\nFP2": "",
          "FP2": "[4]\nE. Q. Wu, P.-Y. Deng, X.-Y. Qu, Z. Tang, W.-M. Zhang, L.-M. Zhu,"
        },
        {
          "C4": "",
          "C4\nC4\nC4\nFP2\nFP2\nFP2": "tures of nodes\nlocated in the frontal,\ntemporal and occipital",
          "FP2": ""
        },
        {
          "C4": "",
          "C4\nC4\nC4\nFP2\nFP2\nFP2": "",
          "FP2": "H. Ren, G.-R. Zhou, and R. S. Sheng, “Detecting fatigue status of pilots"
        },
        {
          "C4": "",
          "C4\nC4\nC4\nFP2\nFP2\nFP2": "lobes have greater self-loop weights, and not only that but by",
          "FP2": "based on deep learning network using eeg signals,” IEEE Transactions"
        },
        {
          "C4": "",
          "C4\nC4\nC4\nFP2\nFP2\nFP2": "building strong connections with other nodes, nodes located in",
          "FP2": "on Cognitive and Developmental Systems, 2020."
        },
        {
          "C4": "",
          "C4\nC4\nC4\nFP2\nFP2\nFP2": "",
          "FP2": "[5] C. D. Katsis, N. Katertsidis, G. Ganiatsas, and D.\nI. Fotiadis, “Toward"
        },
        {
          "C4": "",
          "C4\nC4\nC4\nFP2\nFP2\nFP2": "the regions can play a leading role in the network, ultimately",
          "FP2": ""
        },
        {
          "C4": "",
          "C4\nC4\nC4\nFP2\nFP2\nFP2": "",
          "FP2": "emotion recognition in car-racing drivers: A biosignal processing ap-"
        },
        {
          "C4": "",
          "C4\nC4\nC4\nFP2\nFP2\nFP2": "achieving excellent emotion recognition.",
          "FP2": "proach,” IEEE Transactions on Systems, Man, and Cybernetics-Part A:"
        },
        {
          "C4": "",
          "C4\nC4\nC4\nFP2\nFP2\nFP2": "",
          "FP2": "Systems and Humans, vol. 38, no. 3, pp. 502–512, 2008."
        },
        {
          "C4": "",
          "C4\nC4\nC4\nFP2\nFP2\nFP2": "",
          "FP2": "[6] Y. Gu, S. Chen, and I. Marsic, “Deep mul\ntimodal\nlearning for emotion"
        },
        {
          "C4": "",
          "C4\nC4\nC4\nFP2\nFP2\nFP2": "VI. CONCLUSION",
          "FP2": ""
        },
        {
          "C4": "",
          "C4\nC4\nC4\nFP2\nFP2\nFP2": "",
          "FP2": "recognition in spoken language,” in 2018 IEEE International Conference"
        },
        {
          "C4": "",
          "C4\nC4\nC4\nFP2\nFP2\nFP2": "This paper proposes a pyramidal graph convolution network",
          "FP2": "on Acoustics, Speech and Signal Processing (ICASSP).\nIEEE, 2018,"
        },
        {
          "C4": "",
          "C4\nC4\nC4\nFP2\nFP2\nFP2": "",
          "FP2": "pp. 5079–5083."
        },
        {
          "C4": "",
          "C4\nC4\nC4\nFP2\nFP2\nFP2": "that\nimproves\nemotion\nrecognition\naccuracy\nby\neffectively",
          "FP2": ""
        },
        {
          "C4": "",
          "C4\nC4\nC4\nFP2\nFP2\nFP2": "",
          "FP2": "[7]\nF. Noroozi, D. Kaminska, C. Corneanu, T. Sapinski, S. Escalera, and"
        },
        {
          "C4": "",
          "C4\nC4\nC4\nFP2\nFP2\nFP2": "ﬁnding a balance between the\nexpanded receptive ﬁelds of",
          "FP2": ""
        },
        {
          "C4": "",
          "C4\nC4\nC4\nFP2\nFP2\nFP2": "",
          "FP2": "G. Anbarjafari, “Survey on emotional body gesture recognition,” IEEE"
        },
        {
          "C4": "",
          "C4\nC4\nC4\nFP2\nFP2\nFP2": "GCN and the consequent over-smoothing problem. In PGCN,",
          "FP2": "transactions on affective computing, 2018."
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "[8] N. Zeng, H. Zhang, B.\nSong, W. Liu, Y. Li,\nand A. M. Dobaie,",
          "11": "[32] M. Defferrard, X. Bresson, and P. Vandergheynst, “Convolutional neural"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "“Facial expression recognition via learning deep sparse autoencoders,”",
          "11": "in\nnetworks on graphs with fast\nlocalized spectral ﬁltering,” Advances"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "Neurocomputing, vol. 273, pp. 643–649, 2018.",
          "11": "neural\ninformation processing systems, vol. 29, 2016."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "[9] G. Nam, H. Lee,\nJ.-H. Lee,\nand\nJ.-W. Hur,\n“Disguised\nemotion\nin",
          "11": "[33] Y. Ding, N. Robinson, Q. Zeng, and C. Guan, “Lggnet:\nlearning from"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "alexithymia: subjective difﬁculties in emotion processing and increased",
          "11": "local-global-graph representations\nfor brain-computer\ninterface,” arXiv"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "empathic distress,” Frontiers in Psychiatry, vol. 11, 2020.",
          "11": "preprint arXiv:2105.02786, 2021."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "[10]\nT. Song, W. Zheng, P. Song, and Z. Cui, “Eeg emotion recognition using",
          "11": "[34] M.\nJin, H. Chen, Z. Li,\nand\nJ. Li,\n“Eeg-based\nemotion\nrecognition"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "dynamical graph convolutional neural networks,” IEEE Transactions on",
          "11": "using graph convolutional network with learnable electrode relations,”"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "Affective Computing, vol. 11, no. 3, pp. 532–541, 2018.",
          "11": "the IEEE Engineering\nin 2021 43rd Annual International Conference of"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "[11] K. A. Robbins,\nJ. Touryan, T. Mullen, C. Kothe,\nand N. Bigdely-",
          "11": "in Medicine & Biology Society (EMBC).\nIEEE, 2021, pp. 5953–5957."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "Shamlo,\n“How sensitive\nare\neeg results\nto preprocessing methods:\na",
          "11": "[35] O.\nSporns, G. Tononi,\nand R. K¨otter,\n“The\nhuman\nconnectome:\na"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "benchmarking study,” IEEE transactions on neural systems and rehabil-",
          "11": "structural description of the human brain,” PLoS computational biology,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "itation engineering, vol. 28, no. 5, pp. 1081–1090, 2020.",
          "11": "vol. 1, no. 4, p. e42, 2005."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "[12]\nT. Zhang, X. Wang, X. Xu, and C. P. Chen, “Gcb-net: Graph convolu-",
          "11": "[36] G. Gong, Y. He, L. Concha, C. Lebel, D. W. Gross, A. C. Evans,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "tional broad network and its application in emotion recognition,” IEEE",
          "11": "and C. Beaulieu, “Mapping anatomical connectivity patterns of human"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "Transactions on Affective Computing, 2019.",
          "11": "cerebral\ncortex using in vivo diffusion tensor\nimaging tractography,”"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "[13]\nP. Zhong, D. Wang, and C. Miao, “Eeg-based emotion recognition using",
          "11": "Cerebral cortex, vol. 19, no. 3, pp. 524–536, 2009."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "IEEE Transactions\non Affective\nregularized\ngraph\nneural\nnetworks,”",
          "11": "[37]\nE. Bullmore and O. Sporns, “The economy of brain network organiza-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "Computing, 2020.",
          "11": "tion,” Nature reviews neuroscience, vol. 13, no. 5, pp. 336–349, 2012."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "[14]\nT. Song, S. Liu, W. Zheng, Y. Zong, Z. Cui, Y. Li,\nand X. Zhou,",
          "11": "[38] G. Buzs´aki, C. Geisler, D. A. Henze,\nand X.-J. Wang,\n“Interneuron"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "“Variational\ninstance-adaptive graph for eeg emotion recognition,” IEEE",
          "11": "diversity series: circuit complexity and axon wiring economy of cortical"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "Transactions on Affective Computing, 2021.",
          "11": "in\ninterneurons,” Trends\nneurosciences, vol.\n27, no. 4,\npp. 186–193,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "[15] Y. He, Z. J. Chen, and A. C. Evans, “Small-world anatomical networks",
          "11": "2004."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "in the human brain revealed by cortical\nthickness from mri,” Cerebral",
          "11": "[39] G. Li, M. M¨uller, G. Qian, I. C. D. Perez, A. Abualshour, A. K. Thabet,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "cortex, vol. 17, no. 10, pp. 2407–2419, 2007.",
          "11": "and B. Ghanem, “Deepgcns: Making gcns go as deep as cnns,” IEEE"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "[16]\nP. Hagmann, L. Cammoun, X. Gigandet, R. Meuli, C.\nJ. Honey, V.\nJ.",
          "11": "Transactions on Pattern Analysis and Machine Intelligence, 2021."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "Wedeen, and O. Sporns, “Mapping the structural core of human cerebral",
          "11": "[40] R. Salvador, J. Suckling, M. R. Coleman, J. D. Pickard, D. Menon, and"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "cortex,” PLoS biology, vol. 6, no. 7, p. e159, 2008.",
          "11": "E. Bullmore,\n“Neurophysiological\narchitecture of\nfunctional magnetic"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "[17] Y. Ding, N. Robinson, Q. Zeng, D. Chen, A. A. P. Wai, T.-S. Lee, and",
          "11": "resonance images of human brain,” Cerebral cortex, vol. 15, no. 9, pp."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "C. Guan, “Tsception: a deep learning framework for emotion detection",
          "11": "1332–1342, 2005."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "using eeg,” in 2020 International Joint Conference on Neural Networks",
          "11": "[41] G. E. Bruder, J. W. Stewart, and P. J. McGrath, “Right brain,\nleft brain"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "(IJCNN).\nIEEE, 2020, pp. 1–7.",
          "11": "in depressive disorders: clinical and theoretical\nimplications of behav-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "[18] V. J. Lawhern, A. J. Solon, N. R. Waytowich, S. M. Gordon, C. P. Hung,",
          "11": "ioral, electrophysiological and neuroimaging ﬁndings,” Neuroscience &"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "and B. J. Lance, “Eegnet: a compact convolutional neural network for",
          "11": "Biobehavioral Reviews, vol. 78, pp. 178–191, 2017."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "Journal of neural\neeg-based brain–computer\ninterfaces,”\nengineering,",
          "11": "[42] K. H. Jawabri and S. Sharma, “Physiology, cerebral cortex functions,”"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "vol. 15, no. 5, p. 056013, 2018.",
          "11": "2019."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "time series data:\n[19] M. X. Cohen, Analyzing neural\ntheory and practice.",
          "11": "[43] W.-L. Zheng, J.-Y. Zhu, and B.-L. Lu, “Identifying stable patterns over"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "MIT press, 2014.",
          "11": "time for emotion recognition from eeg,” IEEE Transactions on Affective"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "[20] A. H. Kemp, R. B. Silberstein, S. M. Armstrong,\nand P.\nJ. Nathan,",
          "11": "Computing, vol. 10, no. 3, pp. 417–429, 2017."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "“Gender differences\nin the\ncortical\nelectrophysiological processing of",
          "11": "[44]\nP. Veliˇckovi´c, G. Cucurull, A. Casanova, A. Romero, P. Lio, and Y. Ben-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "visual\nemotional\nstimuli,” NeuroImage, vol. 21, no. 2, pp. 632–646,",
          "11": "gio, “Graph attention networks,” arXiv preprint arXiv:1710.10903, 2017."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "2004.",
          "11": "[45] W.-L. Zheng, W. Liu, Y. Lu, B.-L. Lu, and A. Cichocki, “Emotionmeter:"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "[21] R.-N. Duan,\nJ.-Y. Zhu, and B.-L. Lu, “Differential entropy feature for",
          "11": "IEEE\nA multimodal\nframework\nfor\nrecognizing\nhuman\nemotions,”"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "eeg-based emotion classiﬁcation,” in 2013 6th International IEEE/EMBS",
          "11": "transactions on cybernetics, vol. 49, no. 3, pp. 1110–1122, 2018."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "Conference on Neural Engineering (NER).\nIEEE, 2013, pp. 81–84.",
          "11": "[46] W. Liu, J.-L. Qiu, W.-L. Zheng, and B.-L. Lu, “Comparing recognition"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "[22]\nJ. A. Suykens and J. Vandewalle, “Least squares support vector machine",
          "11": "performance\nand robustness of multimodal deep learning models\nfor"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "classiﬁers,” Neural processing letters, vol. 9, no. 3, pp. 293–300, 1999.",
          "11": "multimodal emotion recognition,” IEEE Transactions on Cognitive and"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "[23]\nZ. Liang, S. Oba, and S. Ishii, “An unsupervised eeg decoding system for",
          "11": "Developmental Systems, 2021."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "human emotion recognition,” Neural Networks, vol. 116, pp. 257–268,",
          "11": "[47] Y. Li, W. Zheng, Y. Zong, Z. Cui, T. Zhang,\nand X. Zhou,\n“A bi-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "2019.",
          "11": "hemisphere domain adversarial neural network model\nfor eeg emotion"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "[24] W.-L. Zheng and B.-L. Lu, “Investigating critical\nfrequency bands and",
          "11": "recognition,” IEEE Transactions on Affective Computing, 2018."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "channels for eeg-based emotion recognition with deep neural networks,”",
          "11": "[48]\nS. J. Pan,\nI. W. Tsang, J. T. Kwok, and Q. Yang, “Domain adaptation"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "IEEE Transactions on Autonomous Mental Development, vol. 7, no. 3,",
          "11": "via transfer component analysis,” IEEE transactions on neural networks,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "pp. 162–175, 2015.",
          "11": "vol. 22, no. 2, pp. 199–210, 2010."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "[25] Y. Yang, Q.\nJ. Wu, W.-L. Zheng,\nand B.-L. Lu,\n“Eeg-based emotion",
          "11": "[49] Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle, F. Lavi-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "recognition using hierarchical network with subnetwork nodes,” IEEE",
          "11": "olette, M. Marchand, and V. Lempitsky, “Domain-adversarial\ntraining"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "Transactions on Cognitive and Developmental Systems, vol. 10, no. 2,",
          "11": "of neural networks,” The journal of machine learning research, vol. 17,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "pp. 408–419, 2017.",
          "11": "no. 1, pp. 2096–2030, 2016."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "[26]\nS. Alhagry, A. A. Fahmy, and R. A. El-Khoribi, “Emotion recognition",
          "11": "[50] Y. Li, L. Wang, W. Zheng, Y. Zong, L. Qi, Z. Cui, T. Zhang,\nand"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "based on eeg using lstm recurrent neural network,” Emotion, vol. 8,",
          "11": "T. Song, “A novel bi-hemispheric discrepancy model\nfor eeg emotion"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "no. 10, pp. 355–358, 2017.",
          "11": "recognition,” IEEE Transactions on Cognitive and Developmental Sys-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "[27] W. Tao, C. Li, R. Song, J. Cheng, Y. Liu, F. Wan, and X. Chen, “Eeg-",
          "11": "tems, vol. 13, no. 2, pp. 354–367, 2020."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "based emotion recognition via channel-wise attention and self attention,”",
          "11": "[51]\nL.-M. Zhao, R. Li, W.-L. Zheng, and B.-L. Lu, “Classiﬁcation of ﬁve"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "IEEE Transactions on Affective Computing, 2020.",
          "11": "emotions from eeg and eye movement signals: complementary represen-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "[28] D. Zhang, L. Yao, K. Chen, S. Wang, X. Chang, and Y. Liu, “Making",
          "11": "tation properties,” in 2019 9th International IEEE/EMBS Conference on"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "sense of spatio-temporal preserving representations for eeg-based human",
          "11": "Neural Engineering (NER).\nIEEE, 2019, pp. 611–614."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "intention recognition,” IEEE transactions on cybernetics, vol. 50, no. 7,",
          "11": "[52] R. Li, Y. Wang,\nand B.-L. Lu,\n“A multi-domain adaptive graph con-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "pp. 3033–3044, 2019.",
          "11": "volutional network for eeg-based emotion recognition,” in Proceedings"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "[29]\nT. N. Kipf and M. Welling, “Semi-supervised classiﬁcation with graph",
          "11": "of\nthe 29th ACM International Conference on Multimedia, 2021, pp."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "convolutional networks,” arXiv preprint arXiv:1609.02907, 2016.",
          "11": "5565–5573."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "[30]\nT. Hamaguchi, H. Oiwa, M. Shimbo, and Y. Matsumoto, “Knowledge",
          "11": "[53] D. Chen, Y. Lin, W. Li, P. Li,\nJ. Zhou, and X. Sun, “Measuring and"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "transfer\nfor\nout-of-knowledge-base\nentities: A graph\nneural\nnetwork",
          "11": "relieving the over-smoothing problem for graph neural networks from the"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "approach,” arXiv preprint arXiv:1706.05674, 2017.",
          "11": "the AAAI Conference on Artiﬁcial\ntopological view,” in Proceedings of"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "[31]\nL. Zhao, Y. Song, C. Zhang, Y. Liu, P. Wang, T. Lin, M. Deng, and H. Li,",
          "11": "Intelligence, vol. 34, no. 04, 2020, pp. 3438–3445."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "“T-gcn: A temporal graph convolutional network for\ntrafﬁc prediction,”",
          "11": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "IEEE Transactions on Intelligent Transportation Systems, vol. 21, no. 9,",
          "11": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "pp. 3848–3858, 2019.",
          "11": ""
        }
      ],
      "page": 11
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "APPENDIX A"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "This paper uses PyTorch to build PGCN and deploy it on a"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "2080TI GPU. The local module contains two layers of GCN"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "networks with a size of 62 × 5 for\nthe input and 62 × 30"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "for\nthe output;\nthe mesoscopic module contains two layers of"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "networks with an output size of 71 × 30;\nthe Global network"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "contains\none\nlayer\nof GCNs with\nan\noutput\nsize\nof\n71 ×"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "70;\nthe emotion recognition network contains a 3-layer\nfully"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "connected network. For the optimization of the model, we used"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "the AdamW optimizer and warm-up, setting the learning rate"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "to 1e-2 and the batch size to 64. We evaluated the model using"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "the average accuracy (ACC) and standard deviation (STD)."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "APPENDIX B"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "1)\nSEED Dataset: The SEED dataset collected EEG emo-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "tional data from 15 subjects (seven males and eight\nfemales)"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "who watched movie clips with three different emotional\nten-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "dencies: negative, positive, and neutral. Three sessions of EEG"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "data were\ncollected\nfrom each\nsubject,\neach\ncontaining\n15"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "movie clips of different emotions. Each movie clip corresponds"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "to a trial, and each trial contained a 5-second hint, a 4-minute"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "movie\nclip,\na\n45-second\nself-assessment,\nand\na\n15-second"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "break."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "In the subject-dependent experiments, we followed the ex-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "perimental setup of [24], [10], [47], for each subject, we used"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "the ﬁrst nine trials of the same session as the training set and"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "the last six trials as the testing set, and calculated the mean and"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "standard deviation for all subjects on two sessions [24],\n[13],"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "[14]. In the subject-independent experiments, we followed the"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "experimental\nsetup\nin\n[10],\n[47]\nand\nimplemented\na\nleave-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "one-subject-out cross-validation, and calculated the mean and"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "standard deviation of all subjects on all\nthree sessions."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "2) SEED-IV Dataset: The SEED-IV dataset collected EEG"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "emotional data from 15 subjects (7 males and eight\nfemales)"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "who watched movie\nclips with\nfour\nemotional\ntendencies:"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "neutral, sad,\nfear, and happy. Each subject watched 24 movie"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "clips of\nthree sessions, and each movie clip corresponding to"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "a trial, and each trial\nlasted around 2 minutes."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "In\nthe\nsubject-dependent\nexperiments, we\nfollowed\nthe"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "experimental setup of [50], [13], and for each subject, to ensure"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "data\nbalance, we ﬁrst\nset\naside\nthe\nlast\ntwo\ntrials\nof\neach"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "emotion data in each session as the testing set and use the re-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "maining 16 trials as the training set. In the subject-independent"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "experiments, we followed the experimental setup of [50], [13]"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "and implemented cross-validation with one subject left behind."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "In evaluating the results, we calculated the average accuracy"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "of all subjects in the three sessions."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "3) SEED-V Dataset: The SEED-V dataset collected EEG"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "data from 16 subjects\n(six males and ten females) and con-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "tained ﬁve emotional\ntendencies: happy, disgust, neutral, fear,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "and sad. Each subject watched 15 movie clips in three sessions."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "In the subject-dependent experiments,\nfollowing the previ-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "ous experimental settings [51],\n[52], we used the data of\nthe"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "ﬁrst\nten trials as the training set, and the last ﬁve trials as the"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "testing set. For\nthe subject-independent experiments, we also"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "followed the same leave-one-out experimental approach. We"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "calculated the accuracy and standard deviation of all subjects"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "on the three sessions on both two experiments."
        }
      ],
      "page": 12
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Emotion recognition in human-computer interaction",
      "authors": [
        "R Cowie",
        "E Douglas-Cowie",
        "N Tsapatsoulis",
        "G Votsis",
        "S Kollias",
        "W Fellenz",
        "J Taylor"
      ],
      "year": "2001",
      "venue": "IEEE Signal processing magazine"
    },
    {
      "citation_id": "2",
      "title": "Emotion selfregulation training in major depressive disorder using simultaneous realtime fmri and eeg neurofeedback",
      "authors": [
        "V Zotev",
        "A Mayeli",
        "M Misaki",
        "J Bodurka"
      ],
      "year": "2020",
      "venue": "NeuroImage: Clinical"
    },
    {
      "citation_id": "3",
      "title": "Cognitive behavioral therapy for anxiety and related disorders: A meta-analysis of randomized placebo-controlled trials",
      "authors": [
        "J Carpenter",
        "L Andrews",
        "S Witcraft",
        "M Powers",
        "J Smits",
        "S Hofmann"
      ],
      "year": "2018",
      "venue": "Depression and anxiety"
    },
    {
      "citation_id": "4",
      "title": "Detecting fatigue status of pilots based on deep learning network using eeg signals",
      "authors": [
        "E Wu",
        "P.-Y Deng",
        "X.-Y Qu",
        "Z Tang",
        "W.-M Zhang",
        "L.-M Zhu",
        "H Ren",
        "G.-R Zhou",
        "R Sheng"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Cognitive and Developmental Systems"
    },
    {
      "citation_id": "5",
      "title": "Toward emotion recognition in car-racing drivers: A biosignal processing approach",
      "authors": [
        "C Katsis",
        "N Katertsidis",
        "G Ganiatsas",
        "D Fotiadis"
      ],
      "year": "2008",
      "venue": "IEEE Transactions on Systems, Man, and Cybernetics-Part A: Systems and Humans"
    },
    {
      "citation_id": "6",
      "title": "Deep mul timodal learning for emotion recognition in spoken language",
      "authors": [
        "Y Gu",
        "S Chen",
        "I Marsic"
      ],
      "year": "2018",
      "venue": "2018 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "7",
      "title": "Survey on emotional body gesture recognition",
      "authors": [
        "F Noroozi",
        "D Kaminska",
        "C Corneanu",
        "T Sapinski",
        "S Escalera",
        "G Anbarjafari"
      ],
      "year": "2018",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "8",
      "title": "Facial expression recognition via learning deep sparse autoencoders",
      "authors": [
        "N Zeng",
        "H Zhang",
        "B Song",
        "W Liu",
        "Y Li",
        "A Dobaie"
      ],
      "year": "2018",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "9",
      "title": "Disguised emotion in alexithymia: subjective difficulties in emotion processing and increased empathic distress",
      "authors": [
        "G Nam",
        "H Lee",
        "J.-H Lee",
        "J.-W Hur"
      ],
      "year": "2020",
      "venue": "Frontiers in Psychiatry"
    },
    {
      "citation_id": "10",
      "title": "Eeg emotion recognition using dynamical graph convolutional neural networks",
      "authors": [
        "T Song",
        "W Zheng",
        "P Song",
        "Z Cui"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "11",
      "title": "How sensitive are eeg results to preprocessing methods: a benchmarking study",
      "authors": [
        "K Robbins",
        "J Touryan",
        "T Mullen",
        "C Kothe",
        "N Bigdely-Shamlo"
      ],
      "year": "2020",
      "venue": "IEEE transactions on neural systems and rehabilitation engineering"
    },
    {
      "citation_id": "12",
      "title": "Gcb-net: Graph convolutional broad network and its application in emotion recognition",
      "authors": [
        "T Zhang",
        "X Wang",
        "X Xu",
        "C Chen"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "13",
      "title": "Eeg-based emotion recognition using regularized graph neural networks",
      "authors": [
        "P Zhong",
        "D Wang",
        "C Miao"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "14",
      "title": "Variational instance-adaptive graph for eeg emotion recognition",
      "authors": [
        "T Song",
        "S Liu",
        "W Zheng",
        "Y Zong",
        "Z Cui",
        "Y Li",
        "X Zhou"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "15",
      "title": "Small-world anatomical networks in the human brain revealed by cortical thickness from mri",
      "authors": [
        "Y He",
        "Z Chen",
        "A Evans"
      ],
      "year": "2007",
      "venue": "Cerebral cortex"
    },
    {
      "citation_id": "16",
      "title": "Mapping the structural core of human cerebral cortex",
      "authors": [
        "P Hagmann",
        "L Cammoun",
        "X Gigandet",
        "R Meuli",
        "C Honey",
        "V Wedeen",
        "O Sporns"
      ],
      "year": "2008",
      "venue": "PLoS biology"
    },
    {
      "citation_id": "17",
      "title": "Tsception: a deep learning framework for emotion detection using eeg",
      "authors": [
        "Y Ding",
        "N Robinson",
        "Q Zeng",
        "D Chen",
        "A Wai",
        "T.-S Lee",
        "C Guan"
      ],
      "year": "2020",
      "venue": "2020 International Joint Conference on Neural Networks (IJCNN)"
    },
    {
      "citation_id": "18",
      "title": "Eegnet: a compact convolutional neural network for eeg-based brain-computer interfaces",
      "authors": [
        "V Lawhern",
        "A Solon",
        "N Waytowich",
        "S Gordon",
        "C Hung",
        "B Lance"
      ],
      "year": "2018",
      "venue": "Journal of neural engineering"
    },
    {
      "citation_id": "19",
      "title": "Analyzing neural time series data: theory and practice",
      "authors": [
        "M Cohen"
      ],
      "year": "2014",
      "venue": "Analyzing neural time series data: theory and practice"
    },
    {
      "citation_id": "20",
      "title": "Gender differences in the cortical electrophysiological processing of visual emotional stimuli",
      "authors": [
        "A Kemp",
        "R Silberstein",
        "S Armstrong",
        "P Nathan"
      ],
      "year": "2004",
      "venue": "NeuroImage"
    },
    {
      "citation_id": "21",
      "title": "Differential entropy feature for eeg-based emotion classification",
      "authors": [
        "R.-N Duan",
        "J.-Y Zhu",
        "B.-L Lu"
      ],
      "year": "2013",
      "venue": "2013 6th International IEEE/EMBS Conference on Neural Engineering (NER)"
    },
    {
      "citation_id": "22",
      "title": "Least squares support vector machine classifiers",
      "authors": [
        "J Suykens",
        "J Vandewalle"
      ],
      "year": "1999",
      "venue": "Neural processing letters"
    },
    {
      "citation_id": "23",
      "title": "An unsupervised eeg decoding system for human emotion recognition",
      "authors": [
        "Z Liang",
        "S Oba",
        "S Ishii"
      ],
      "year": "2019",
      "venue": "Neural Networks"
    },
    {
      "citation_id": "24",
      "title": "Investigating critical frequency bands and channels for eeg-based emotion recognition with deep neural networks",
      "authors": [
        "W.-L Zheng",
        "B.-L Lu"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on Autonomous Mental Development"
    },
    {
      "citation_id": "25",
      "title": "Eeg-based emotion recognition using hierarchical network with subnetwork nodes",
      "authors": [
        "Y Yang",
        "Q Wu",
        "W.-L Zheng",
        "B.-L Lu"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Cognitive and Developmental Systems"
    },
    {
      "citation_id": "26",
      "title": "Emotion recognition based on eeg using lstm recurrent neural network",
      "authors": [
        "S Alhagry",
        "A Fahmy",
        "R El-Khoribi"
      ],
      "year": "2017",
      "venue": "Emotion"
    },
    {
      "citation_id": "27",
      "title": "Eegbased emotion recognition via channel-wise attention and self attention",
      "authors": [
        "W Tao",
        "C Li",
        "R Song",
        "J Cheng",
        "Y Liu",
        "F Wan",
        "X Chen"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "28",
      "title": "Making sense of spatio-temporal preserving representations for eeg-based human intention recognition",
      "authors": [
        "D Zhang",
        "L Yao",
        "K Chen",
        "S Wang",
        "X Chang",
        "Y Liu"
      ],
      "year": "2019",
      "venue": "IEEE transactions on cybernetics"
    },
    {
      "citation_id": "29",
      "title": "Semi-supervised classification with graph convolutional networks",
      "authors": [
        "T Kipf",
        "M Welling"
      ],
      "year": "2016",
      "venue": "Semi-supervised classification with graph convolutional networks",
      "arxiv": "arXiv:1609.02907"
    },
    {
      "citation_id": "30",
      "title": "Knowledge transfer for out-of-knowledge-base entities: A graph neural network approach",
      "authors": [
        "T Hamaguchi",
        "H Oiwa",
        "M Shimbo",
        "Y Matsumoto"
      ],
      "year": "2017",
      "venue": "Knowledge transfer for out-of-knowledge-base entities: A graph neural network approach",
      "arxiv": "arXiv:1706.05674"
    },
    {
      "citation_id": "31",
      "title": "T-gcn: A temporal graph convolutional network for traffic prediction",
      "authors": [
        "L Zhao",
        "Y Song",
        "C Zhang",
        "Y Liu",
        "P Wang",
        "T Lin",
        "M Deng",
        "H Li"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Intelligent Transportation Systems"
    },
    {
      "citation_id": "32",
      "title": "Convolutional neural networks on graphs with fast localized spectral filtering",
      "authors": [
        "M Defferrard",
        "X Bresson",
        "P Vandergheynst"
      ],
      "year": "2016",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "33",
      "title": "Lggnet: learning from local-global-graph representations for brain-computer interface",
      "authors": [
        "Y Ding",
        "N Robinson",
        "Q Zeng",
        "C Guan"
      ],
      "year": "2021",
      "venue": "Lggnet: learning from local-global-graph representations for brain-computer interface",
      "arxiv": "arXiv:2105.02786"
    },
    {
      "citation_id": "34",
      "title": "Eeg-based emotion recognition using graph convolutional network with learnable electrode relations",
      "authors": [
        "M Jin",
        "H Chen",
        "Z Li",
        "J Li"
      ],
      "year": "2021",
      "venue": "2021 43rd Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC)"
    },
    {
      "citation_id": "35",
      "title": "The human connectome: a structural description of the human brain",
      "authors": [
        "O Sporns",
        "G Tononi",
        "R Kötter"
      ],
      "year": "2005",
      "venue": "PLoS computational biology"
    },
    {
      "citation_id": "36",
      "title": "Mapping anatomical connectivity patterns of human cerebral cortex using in vivo diffusion tensor imaging tractography",
      "authors": [
        "G Gong",
        "Y He",
        "L Concha",
        "C Lebel",
        "D Gross",
        "A Evans",
        "C Beaulieu"
      ],
      "year": "2009",
      "venue": "Cerebral cortex"
    },
    {
      "citation_id": "37",
      "title": "The economy of brain network organization",
      "authors": [
        "E Bullmore",
        "O Sporns"
      ],
      "year": "2012",
      "venue": "Nature reviews neuroscience"
    },
    {
      "citation_id": "38",
      "title": "Interneuron diversity series: circuit complexity and axon wiring economy of cortical interneurons",
      "authors": [
        "G Buzsáki",
        "C Geisler",
        "D Henze",
        "X.-J Wang"
      ],
      "year": "2004",
      "venue": "Trends in neurosciences"
    },
    {
      "citation_id": "39",
      "title": "Deepgcns: Making gcns go as deep as cnns",
      "authors": [
        "G Li",
        "M Müller",
        "G Qian",
        "I Perez",
        "A Abualshour",
        "A Thabet",
        "B Ghanem"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "40",
      "title": "Neurophysiological architecture of functional magnetic resonance images of human brain",
      "authors": [
        "R Salvador",
        "J Suckling",
        "M Coleman",
        "J Pickard",
        "D Menon",
        "E Bullmore"
      ],
      "year": "2005",
      "venue": "Cerebral cortex"
    },
    {
      "citation_id": "41",
      "title": "Right brain, left brain in depressive disorders: clinical and theoretical implications of behavioral, electrophysiological and neuroimaging findings",
      "authors": [
        "G Bruder",
        "J Stewart",
        "P Mcgrath"
      ],
      "year": "2017",
      "venue": "Neuroscience & Biobehavioral Reviews"
    },
    {
      "citation_id": "42",
      "title": "Physiology, cerebral cortex functions",
      "authors": [
        "K Jawabri",
        "S Sharma"
      ],
      "year": "2019",
      "venue": "Physiology, cerebral cortex functions"
    },
    {
      "citation_id": "43",
      "title": "Identifying stable patterns over time for emotion recognition from eeg",
      "authors": [
        "W.-L Zheng",
        "J.-Y Zhu",
        "B.-L Lu"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "44",
      "title": "Graph attention networks",
      "authors": [
        "P Veličković",
        "G Cucurull",
        "A Casanova",
        "A Romero",
        "P Lio",
        "Y Bengio"
      ],
      "year": "2017",
      "venue": "Graph attention networks",
      "arxiv": "arXiv:1710.10903"
    },
    {
      "citation_id": "45",
      "title": "Emotionmeter: A multimodal framework for recognizing human emotions",
      "authors": [
        "W.-L Zheng",
        "W Liu",
        "Y Lu",
        "B.-L Lu",
        "A Cichocki"
      ],
      "year": "2018",
      "venue": "IEEE transactions on cybernetics"
    },
    {
      "citation_id": "46",
      "title": "Comparing recognition performance and robustness of multimodal deep learning models for multimodal emotion recognition",
      "authors": [
        "W Liu",
        "J.-L Qiu",
        "W.-L Zheng",
        "B.-L Lu"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Cognitive and Developmental Systems"
    },
    {
      "citation_id": "47",
      "title": "A bihemisphere domain adversarial neural network model for eeg emotion recognition",
      "authors": [
        "Y Li",
        "W Zheng",
        "Y Zong",
        "Z Cui",
        "T Zhang",
        "X Zhou"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "48",
      "title": "Domain adaptation via transfer component analysis",
      "authors": [
        "S Pan",
        "I Tsang",
        "J Kwok",
        "Q Yang"
      ],
      "year": "2010",
      "venue": "IEEE transactions on neural networks"
    },
    {
      "citation_id": "49",
      "title": "Domain-adversarial training of neural networks",
      "authors": [
        "Y Ganin",
        "E Ustinova",
        "H Ajakan",
        "P Germain",
        "H Larochelle",
        "F Laviolette",
        "M Marchand",
        "V Lempitsky"
      ],
      "year": "2016",
      "venue": "The journal of machine learning research"
    },
    {
      "citation_id": "50",
      "title": "A novel bi-hemispheric discrepancy model for eeg emotion recognition",
      "authors": [
        "Y Li",
        "L Wang",
        "W Zheng",
        "Y Zong",
        "L Qi",
        "Z Cui",
        "T Zhang",
        "T Song"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Cognitive and Developmental Systems"
    },
    {
      "citation_id": "51",
      "title": "Classification of five emotions from eeg and eye movement signals: complementary representation properties",
      "authors": [
        "L.-M Zhao",
        "R Li",
        "W.-L Zheng",
        "B.-L Lu"
      ],
      "year": "2019",
      "venue": "Neural Engineering"
    },
    {
      "citation_id": "52",
      "title": "A multi-domain adaptive graph convolutional network for eeg-based emotion recognition",
      "authors": [
        "R Li",
        "Y Wang",
        "B.-L Lu"
      ],
      "year": "2021",
      "venue": "Proceedings of the 29th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "53",
      "title": "Measuring and relieving the over-smoothing problem for graph neural networks from the topological view",
      "authors": [
        "D Chen",
        "Y Lin",
        "W Li",
        "P Li",
        "J Zhou",
        "X Sun"
      ],
      "year": "2020",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    }
  ]
}