{
  "paper_id": "2106.02810v2",
  "title": "An Attribute-Aligned Strategy For Learning Speech Representation",
  "published": "2021-06-05T06:19:14Z",
  "authors": [
    "Yu-Lin Huang",
    "Bo-Hao Su",
    "Y. -W. Peter Hong",
    "Chi-Chun Lee"
  ],
  "keywords": [
    "speech representation",
    "layered dropout",
    "privacy",
    "fair",
    "attribute alignment"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Advancement in speech technology has brought convenience to our life. However, the concern is on the rise as speech signal contains multiple personal attributes, which would lead to either sensitive information leakage or bias toward decision. In this work, we propose an attribute-aligned learning strategy to derive speech representation that can flexibly address these issues by attribute-selection mechanism. Specifically, we propose a layered-representation variational autoencoder (LR-VAE), which factorizes speech representation into attributesensitive nodes, to derive an identity-free representation for speech emotion recognition (SER), and an emotionless representation for speaker verification (SV). Our proposed method achieves competitive performances on identity-free SER and a better performance on emotionless SV, comparing to the current state-of-the-art method of using adversarial learning applied on a large emotion corpora, the MSP-Podcast. Also, our proposed learning strategy reduces the model and training process needed to achieve multiple privacy-preserving tasks.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Speech technology has rapidly proliferated and integrated deeply into our daily life  [1, 2, 3, 4, 5] . While these applications bring convenience to our life, several growing concerns have gained attention and need to be addressed with care. The first is privacy due to concerns of sensitive information leakage: for example, users may not expect to disclose their identity information while using a speech emotion recognition (SER) system; on the other hand, users may not wish to share their emotional condition when being assessed by a speaker recognition (SR) system. Moreover, the collective social norm would create unwanted and often detrimental self-exaggerated issues around equality, e.g., unfair biases toward gender types  [6]  or race  [7] , when using data-driven approaches for speech technology. Speech is an informative signal which contains personal sensitive attributes by nature; hence developing appropriate methods either to protect privacy information, such as identity and emotion, or to mitigate the undesired biases, like gender and race, is critical in the current era.\n\nRecently, several works in speech processing have started to address these issues using privacy-aware representation learning. For example, Srivastava et al. used adversarial representation learning on automatic speech recognition (ASR) to protect speaker identity  [8] , Alouf et al. used CycleGAN-based method to generate emotion-less synthesized speech for voice assistant to hide personal affect  [9] , Jaiswal et al. used adversarial learning to generate gender-invariant representation for identify-free emotion recognition  [10] , and Xia et al. applied adversarial learning to mitigate racial bias in hate speech detection  [11] . While current state-of-the-art methods concentrate on using adversarial learning, this strategy suffers from several shortcomings. Adversarial method address privacy issues by learning a speech signal space with no targeted sensitive attributes as measured by its ability in fooling a well-trained discriminator that is in charge of classifying sensitive information, e.g., gender and speaker identity. This attribute invariant learning strategy lacks a flexible mechanism to adapt to different criterion of privacy preserving; for example, in some tasks only the \"gender\" attribute may need to be protected while some other tasks would require the \"speaker identity\" to be private. For different scenario of interest, one would have to re-train the adversarial network over again.\n\nIn this work, instead of taking a 'per-attribute' adversarial invariant learning approach, we formulate the problem as devising a learning strategy that would result in attribute-aligned speech representation. The core idea centers on conceptualizing that speech contains a mixture of attributes,  [12, 13] , e.g., gender, age, emotion and semantics, etc. By factorizing the entangled information of speech signal into independent attributes with proper attribute-alignment, we can protect particular sensitive information by attribute selection, i.e., masking targeted sensitive attributes, to minimize either privacy-leakage or biased decision. In this paper, we evaluate this idea by targeting two sensitive attributes in speech, i.e., emotion and identity, and our aim is to show that this approach can flexibly achieve privacy-preserving applications by eliminating identity contents in SER or emotion contents in SV at ease. We propose a framework of flexible attribute masking for speech, inspired by the fair representation learning  [14] . We aim to learn a layered disentangled speech representation with a backbone of variational autoencoder (VAE)  [15, 16] . We specifically propose a layered dropout strategy in a multi-task framework to achieve attribute-alignment, i.e., forces the latent to align in an emotion-related to identity-related order. To further clean up the aligned representation knowing that these two attributes are highly correlated  [17, 18] , we add adversarial reversal layer to each task-specific branch. Our strategy provides flexibility in either identity masking or emotion masking to come up with an identity-free latent for privacy-preserving SER or emotionless latent for privacy-preserving SV with a unified learning framework. In this work, we evaluate our method on MSP-Podcast  [19]  for SER and SV tasks using three types of feature, and achieve competitive results on SER (emobase: 52.41% weighted f-score, 41.14% EER), and an improvement on SV (netvlad: 34.35% weighted f-score, 10.91% EER; xvector: 34.23% weighted f-score, 9.63% EER), compared to the state-of-the-art adversarial learning method.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "2. Methodology",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "2.1. Dataset Description",
      "text": "In this study, we focus on two main tasks, emotion recognition and speaker verification. To evaluate the performance of these two tasks, a large corpus with emotional labels and multiple speakers is needed. Hence, we use the MSP-Podcast database  [19] , which includes over 1,000 podcast recordings. Each podcast is segmented into speaking turns, where segments with music, overlapped speech, telephone quality speech and background noise are discarded.\n\nIn this work, we use data with 5 categorical emotions: neutral, angry, sad, happy and disgust as in  [18] . We used the standard splits in Release 1.4 for training, development, and testing, which includes 610 speakers in train set, 30 speakers in development set, and 50 speakers in test set, where each set of speakers are disjoint. The distribution of the 5 emotion classes are: angry: 8.81%, happiness: 27.10%, neutral: 53.05%, sad: 3.95%, disgust: 7.09%.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Feature Extraction",
      "text": "In this work, we use three different input features for the two tasks: emobase2010, netvlad embedding, and x-vector embedding to verify the effectiveness of our proposed method. First, we use emobase2010, which is a commonly used feature for SER, as input. It is a 1582 dimensional feature including pitch, loudness, mfcc and spectral, etc. We extract emobase2010 using openSMILE toolkit  [2] . Further, we extract embeddings commonly used in state-of-the-art speaker verification task, i.e., netvlad  [3]  and x-vector  [18] . The netvlad embedding is extracted using the released pre-trained model  [3] , while the xvector embedding is obtained by training on the Voxceleb2  [20]  using the structure mentioned in  [18] .",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Layered Representation Variational Autoencoder",
      "text": "We propose a layered-representation variational autoencoder (LR-VAE) to factorize the entangled dimensions contained in speech and arrange these dimensions in an emotion-related to identity-related order. LR-VAE contains two main components, i.e., disentangled representation and layered dropout. We will first describe VAE, i.e., a well-known structure for disentangled learning. Then, we will further detail our layered dropout with adversarial multitask learning to obtain attribute-aligned speech representation.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Variational Autoencoder (Vae)",
      "text": "In this work, we use disentangled representation learning via VAE to derive a latent node-wise independent representation. VAE model aims to learn the marginal likelihood of a data x, with the objective function:\n\nwhere DKL(||) stands for the non-negative Kullback-Leibler divergence. The KL-divergence term encourages the posterior distribution to be close to an isotropic Gaussian to achieve disentanglement purpose.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Layered Dropout With Adversarial Multitask Learning",
      "text": "In this work, we propose a strategy of layered dropout with a multitask-learning architecture. Multitask learning aims to include both emotion and speaker identity information into the latent codes. Layered dropout is further utilized to force these attributes to align toward both ends of the latent codes resulting in a layered representation. Also, adversarial branches, i.e., gradient reversal layer, are used to additionally 'purify' this attributealigned representation.\n\nDropout is a well-known regularization method in deep learning to prevent neural networks from overfitting  [21] . Layered dropout works in a similar manner but with a different purpose. We propose to use this as a learning mechanism to make each dimension of the latent codes carry different importance to the designated task. In our work, the two tasks are defined as the emotion recognition and the speaker verification. We design a dropout rate function making the probability of dropping decreases (or increases) monotonically for each node of the input layer. This effectively forces the target task's discriminatory information to concentrate on nodes with lower dropout rates.\n\nLet x denotes the input vector with N dimensions of a layer of a neural network, we define a vector with decreasing preserving rates p for task of emotion recognition (increasing preserving rates for speaker verification), where 0 ≤ pi ≤ 1 for i ∈ {0, . . . , N -1}. With layered dropout, the input vector x of the feed-forward operation is replaced by vector x, generated by: mi ∼ Bernoulli(pi) (2)\n\nHere, * denotes an element-wise product. m acts as a mask before the vector x is fed into the layer. For a dimension mi in the vector m, it's an independent Bernoulli random variable with probability pi being 1, which means to preserve the i th node, and 0 means to drop the i th node. While testing, W This layered dropout mechanism alters the dropout rates being applied on both sides of the representation before an emotion (identity) classifier, the latent codes form an aligned emotion-to-identity order from top end to bottom end during the optimization step. Furthermore, we add an auxiliary mechanism of adversarial branches with gradient reversal layers  [22]  during multitask learning. The goal is to learn cleaner factorized identity-free (emotion-free) representation. After having an attribute-aligned representation, we simply need to mask the dimension representing the particular attribute of interest. For example, to protect identity information in SER, we can simply mask the nodes that have high emotion preservation rates and low speaker identity preservation rates, and vice versa for in SV. Notice that our attribute aligned strategy provides a mechanism to select \"what to protect\" with a single unified learning, which is more efficient than the adversarial method that requires re-training in different scenarios.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Experiments",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Experiment Setup",
      "text": "The structure of our VAE model is as follows: multi-layer perceptron (MLP) is applied for encoder and decoder. Additionally, fully connected layer is applied to model the mean and log variance of the latent code for the encoder. For multi-task learning, two MLP classifiers are trained for emotion recognition and speaker identification, and two MLP discriminators are trained for adversarial learning by applying gradient reversal layers  [22] . We set the learning rate as 5e -4 , and the batch size as 128. Moreover, we add a regularization of 1e -6 to all weights and biases to stabilize the training process. Rectified Linear Unit (ReLU) is chosen as the activation function. We train the model using Adam optimizer, with L obj as the objective, defined as:\n\nwhere LV AE represents the reconstruction error and KL divergence loss as defined in equation 1, while Lemo and L id represents the cross-entropy loss for emotion recognition and speaker identification; L emo-adv and L id-adv represents the adversarial loss for emotion recognition and speaker identification. Notice that for speaker verification (SV) task, models are trained to predict speaker identity in the training set, i.e., speaker identification, to learn identity-related information during training; while during evaluation, the hidden layer embedding is extracted and apply to speaker verification system. We evaluate the performance of SER using weighted fscore (WFS), following the experiment setup in  [18] , and eval-uate the performance of speaker verification by equal error rate (EER). For each feature set, we train a factorized layered representation encoder based on training set, select model using validation set, and test performance on test set. Assuming attackers have access to the training set with encoded representation and labels of speakers. Our goal is to generate a representation such that for the encoded representations with particular sensitive attributes masked, neither attackers nor hosts are able to identify the sensitive attributes while the main task performance is maintained.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Baseline Methods",
      "text": "The following are the baseline methods of different learning strategies that we use to compare with LR-VAE. Notice that privacy-preserving (PP) on LR-VAE are done by masking the dimension of particular sensitive attributes in the latent codes. DNN: A model conducted by fully connected layers to obtain the baseline performance on SER and SV for each feature. VAE: A vanilla VAE trained by multi-task learning on SER and SV tasks. A-VAE: A VAE trained for single task (SER or SV) with adversarial learning (reverse gradient) on the other task (SV or SER). LR-VAE (w/o adv): A model similar to our proposed LR-VAE, but trained without adding the adversarial branch.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Result And Analysis",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Sensitive Attribute Protection",
      "text": "Note that all the comparison are presented in absolute points in this section. For privacy-preserving speech emotion recognition (PP-SER), we aim to protect user's identity information while preserving the emotion recognition performance. As shown in the PP-SER columns in table 1, our proposed LR-VAE achieves the better privacy preserving performance on xvector and netvlad and similar result on emobase comparing to A-VAE. It shows that our proposed method is able to obtain a competitive emotion recognition performance (0.32% WFS higher on emobase, 1.01% WFS lower on netvlad, and 2.14% WFS higher on x-vector), with better improvements in protecting speaker identity (only 0.35% EER worse on emobase, 8.85% EER better on netvlad, and 3.55% EER better on xvector).\n\nOn the other hand, to achieve emotion-protected speaker verification (PP-SV), we aim to reduce users' emotional information in the speech while preserving the speaker verification performance. As shown in PP-SV columns in table 1, our proposed LR-VAE achieves the best emotion protection performance on all three features comparing to A-VAE. It shows that our proposed method could better maintain the speaker verifica- For the first row (protect identity), the masking process starts in a bottom up order, while for the second row (protect emotion), we start masking from the top group.\n\ntion performance (4.46% EER better on emobase, 4.34% EER better on netvlad, and 3.93% EER better on x-vector), while achieving state-of-the-art emotion-related attributes protection (0.39% WFS better on emobase, 4.57% WFS better on netvlad, and 0.10% WFS worse on x-vector) We first study the baseline DNN results shown in the column, DNN, in table 1. The promising performance show that regardless of features, it contains both emotion and identity information. It reinforces the current concerns that speech contains many personal attributes that users may not want to reveal. Then, we compare the DNN results to VAE results shown in the column, VAE origin, in table 1. We do see that there is a slight performance drop in emotion recognition potentially due to the information loss caused by kl-divergence loss in VAE training for factorization, which is a trade-off between disentanglement and reconstruction. This factorization VAE is however a key backbone in achieving our attribute-aligned representation.\n\nTo study how adversarial branches work in our framework, we compare LR-VAE results to LR-VAE(w/o adv). It shows that without adversarial learning in explicitly purifying the emotion-related (identity-related) dimension to identityfree (emotion-free), the representation learned is not \"clean\" enough. Hence, while LR-VAE(w/o adv) also achieves competitive results on main tasks, the sensitive attribute-preserving results are usually worse. This also demonstrates that the emotion-related (identity-related) attributes may contains identity (emotion) information if not explicitly cleaned.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Analysis Of Aligned Attributes",
      "text": "In this section, we further discuss the effectiveness of layered dropout that align attribute-specific information to both ends of the latent codes. We conduct an experiment with the following procedure: we encode the input features into latent codes; next, we divide the latent dimensions into 32 groups; then, for each step, we mask one additional group of latent codes, and train two models, one for emotion recognition, and the other for speaker verification. We compare the performance curve of LR-VAE and A-VAE to observe how layered dropout influence the discriminatory power of the chosen latent code dimension.\n\nWe first study the privacy-preserving speech emotion recognition task. The results are shown in the upper row of figure  2 . In this experiment, we start masking from the bottom of the latent code, which contains more identity-related attributes, to the top of the latent code (more emotion-related attributes). As the procedure moves on, the speaker verification performance steadily decreases (EER increases) until the masking process reaches the middle part of the latent code, where it results in a high EER indicating the point where we achieve an identity-free representation. We can also see that EER curves of LR-VAE and A-VAE intersects, which shows that the masked LR-VAE latent can better eliminate the identity-related attributes. On the other hand, we also observe that the emotion recognition performance slightly decreases toward the ending portion of masking process due to a significant reduction in the node dimension, though A-VAE has an even earlier performance drop.\n\nNext, we study the emotion-protection speaker verification task. The results are shown in the lower row of figure  2 . In this part of experiment, we start masking from the top of the latent code, similar to the previous procedure, but in a reverse order. As the progress moves on, the emotion recognition performance steadily decreases (weighted f-score decreases), and finally reaches to a similar result comparing to A-VAE. On the other hand, we can see that the speaker verification performance of LR-VAE is better preserved comparing to A-VAE, i.e., the EER curve of LR-VAE is lower in the beginning and increases slower comparing to A-VAE.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Conclusions And Future Works",
      "text": "In this paper, we propose a novel disentangled layered speech representation learning that can flexibly preserve sensitive attribute in a unified single training architecture. Compared with other methods, our method achieves a competitive performance on identity-free SER and an improvement on emotionless SV. Also, we show that our proposed method help in pushing the emotion and identity information toward the both ends of the latent codes, and this strategy provides a flexible mechanism to select the target sensitive attributes to protect. Moreover, our attribute aligned learning strategy reduce the training and memory cost as we require only single process and single model to achieve competitive privacy-preserving results on SER and SV against adversarial training, which requires training twice and two models.\n\nIn the future, we will generalize our attribute aligned representation from two specific task to general multi-attributes scenarios. We could utilize the middle portion of the latent codes to capture other information about the speaker, e.g., gender, personality, semantics, etc, in order to provide a more complete profile on this factorized speech representation. Moreover, as the disentanglement achieved by kl divergence loss causes information loss, different factorization methods may be applied to enhance our representation capacity.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: An illustration of our proposed method for attribute-aligned representation learning. It includes three blocks: representation",
      "page": 2
    },
    {
      "caption": "Figure 2: The performance curves in the masking experiment where the y-axis for SER results are weighted f-scores, and EER for SV.",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "huang8592301@gapp.nthu.edu.tw,\nborrissu@gapp.nthu.edu.tw,": "ywhong@ee.nthu.edu.tw,\ncclee@ee.nthu.edu.tw"
        },
        {
          "huang8592301@gapp.nthu.edu.tw,\nborrissu@gapp.nthu.edu.tw,": "emotion recognition [10],\nand Xia\net\nal.\napplied adversarial\nAbstract"
        },
        {
          "huang8592301@gapp.nthu.edu.tw,\nborrissu@gapp.nthu.edu.tw,": "learning to mitigate racial bias in hate speech detection [11]."
        },
        {
          "huang8592301@gapp.nthu.edu.tw,\nborrissu@gapp.nthu.edu.tw,": "Advancement in speech technology has brought convenience to"
        },
        {
          "huang8592301@gapp.nthu.edu.tw,\nborrissu@gapp.nthu.edu.tw,": "While current state-of-the-art methods concentrate on using ad-"
        },
        {
          "huang8592301@gapp.nthu.edu.tw,\nborrissu@gapp.nthu.edu.tw,": "our\nlife.\nHowever,\nthe concern is on the rise as\nspeech sig-"
        },
        {
          "huang8592301@gapp.nthu.edu.tw,\nborrissu@gapp.nthu.edu.tw,": "versarial\nlearning,\nthis strategy suffers from several shortcom-"
        },
        {
          "huang8592301@gapp.nthu.edu.tw,\nborrissu@gapp.nthu.edu.tw,": "nal contains multiple personal attributes, which would lead to"
        },
        {
          "huang8592301@gapp.nthu.edu.tw,\nborrissu@gapp.nthu.edu.tw,": "ings. Adversarial method address privacy issues by learning a"
        },
        {
          "huang8592301@gapp.nthu.edu.tw,\nborrissu@gapp.nthu.edu.tw,": "either\nsensitive information leakage or bias\ntoward decision."
        },
        {
          "huang8592301@gapp.nthu.edu.tw,\nborrissu@gapp.nthu.edu.tw,": "speech signal space with no targeted sensitive attributes as mea-"
        },
        {
          "huang8592301@gapp.nthu.edu.tw,\nborrissu@gapp.nthu.edu.tw,": "In this work, we propose an attribute-aligned learning strat-"
        },
        {
          "huang8592301@gapp.nthu.edu.tw,\nborrissu@gapp.nthu.edu.tw,": "sured by its ability in fooling a well-trained discriminator that is"
        },
        {
          "huang8592301@gapp.nthu.edu.tw,\nborrissu@gapp.nthu.edu.tw,": "egy to derive speech representation that can ﬂexibly address"
        },
        {
          "huang8592301@gapp.nthu.edu.tw,\nborrissu@gapp.nthu.edu.tw,": "in charge of classifying sensitive information, e.g., gender and"
        },
        {
          "huang8592301@gapp.nthu.edu.tw,\nborrissu@gapp.nthu.edu.tw,": "these issues by attribute-selection mechanism. Speciﬁcally, we"
        },
        {
          "huang8592301@gapp.nthu.edu.tw,\nborrissu@gapp.nthu.edu.tw,": "speaker identity. This attribute invariant learning strategy lacks"
        },
        {
          "huang8592301@gapp.nthu.edu.tw,\nborrissu@gapp.nthu.edu.tw,": "propose a layered-representation variational autoencoder (LR-"
        },
        {
          "huang8592301@gapp.nthu.edu.tw,\nborrissu@gapp.nthu.edu.tw,": "a ﬂexible mechanism to adapt\nto different criterion of privacy"
        },
        {
          "huang8592301@gapp.nthu.edu.tw,\nborrissu@gapp.nthu.edu.tw,": "VAE), which factorizes\nspeech representation into attribute-"
        },
        {
          "huang8592301@gapp.nthu.edu.tw,\nborrissu@gapp.nthu.edu.tw,": "preserving;\nfor example,\nin some tasks only the “gender” at-"
        },
        {
          "huang8592301@gapp.nthu.edu.tw,\nborrissu@gapp.nthu.edu.tw,": "sensitive nodes,\nto derive\nan identity-free\nrepresentation for"
        },
        {
          "huang8592301@gapp.nthu.edu.tw,\nborrissu@gapp.nthu.edu.tw,": "tribute may need to be protected while some other tasks would"
        },
        {
          "huang8592301@gapp.nthu.edu.tw,\nborrissu@gapp.nthu.edu.tw,": "speech emotion recognition (SER), and an emotionless repre-"
        },
        {
          "huang8592301@gapp.nthu.edu.tw,\nborrissu@gapp.nthu.edu.tw,": "require the “speaker identity” to be private. For different sce-"
        },
        {
          "huang8592301@gapp.nthu.edu.tw,\nborrissu@gapp.nthu.edu.tw,": "sentation for speaker veriﬁcation (SV). Our proposed method"
        },
        {
          "huang8592301@gapp.nthu.edu.tw,\nborrissu@gapp.nthu.edu.tw,": "nario of interest, one would have to re-train the adversarial net-"
        },
        {
          "huang8592301@gapp.nthu.edu.tw,\nborrissu@gapp.nthu.edu.tw,": "achieves competitive performances on identity-free SER and a"
        },
        {
          "huang8592301@gapp.nthu.edu.tw,\nborrissu@gapp.nthu.edu.tw,": "work over again."
        },
        {
          "huang8592301@gapp.nthu.edu.tw,\nborrissu@gapp.nthu.edu.tw,": "better performance on emotionless SV, comparing to the current"
        },
        {
          "huang8592301@gapp.nthu.edu.tw,\nborrissu@gapp.nthu.edu.tw,": "In this work,\ninstead of taking a ‘per-attribute’ adversarial"
        },
        {
          "huang8592301@gapp.nthu.edu.tw,\nborrissu@gapp.nthu.edu.tw,": "state-of-the-art method of using adversarial learning applied on"
        },
        {
          "huang8592301@gapp.nthu.edu.tw,\nborrissu@gapp.nthu.edu.tw,": "invariant\nlearning approach, we formulate the problem as de-"
        },
        {
          "huang8592301@gapp.nthu.edu.tw,\nborrissu@gapp.nthu.edu.tw,": "a large emotion corpora, the MSP-Podcast. Also, our proposed"
        },
        {
          "huang8592301@gapp.nthu.edu.tw,\nborrissu@gapp.nthu.edu.tw,": "vising a learning strategy that would result\nin attribute-aligned"
        },
        {
          "huang8592301@gapp.nthu.edu.tw,\nborrissu@gapp.nthu.edu.tw,": "learning strategy reduces the model and training process needed"
        },
        {
          "huang8592301@gapp.nthu.edu.tw,\nborrissu@gapp.nthu.edu.tw,": "speech representation. The core idea centers on conceptualiz-"
        },
        {
          "huang8592301@gapp.nthu.edu.tw,\nborrissu@gapp.nthu.edu.tw,": "to achieve multiple privacy-preserving tasks."
        },
        {
          "huang8592301@gapp.nthu.edu.tw,\nborrissu@gapp.nthu.edu.tw,": "ing that speech contains a mixture of attributes,\n[12, 13], e.g.,"
        },
        {
          "huang8592301@gapp.nthu.edu.tw,\nborrissu@gapp.nthu.edu.tw,": "Index Terms: speech representation,\nlayered dropout, privacy,"
        },
        {
          "huang8592301@gapp.nthu.edu.tw,\nborrissu@gapp.nthu.edu.tw,": "gender, age, emotion and semantics, etc. By factorizing the en-"
        },
        {
          "huang8592301@gapp.nthu.edu.tw,\nborrissu@gapp.nthu.edu.tw,": "fair, attribute alignment"
        },
        {
          "huang8592301@gapp.nthu.edu.tw,\nborrissu@gapp.nthu.edu.tw,": "tangled information of speech signal into independent attributes"
        },
        {
          "huang8592301@gapp.nthu.edu.tw,\nborrissu@gapp.nthu.edu.tw,": "with proper attribute-alignment, we can protect particular sen-"
        },
        {
          "huang8592301@gapp.nthu.edu.tw,\nborrissu@gapp.nthu.edu.tw,": "1.\nIntroduction"
        },
        {
          "huang8592301@gapp.nthu.edu.tw,\nborrissu@gapp.nthu.edu.tw,": "sitive information by attribute selection,\ni.e., masking targeted"
        },
        {
          "huang8592301@gapp.nthu.edu.tw,\nborrissu@gapp.nthu.edu.tw,": "sensitive attributes,\nto minimize either privacy-leakage or bi-"
        },
        {
          "huang8592301@gapp.nthu.edu.tw,\nborrissu@gapp.nthu.edu.tw,": "Speech\ntechnology\nhas\nrapidly\nproliferated\nand\nintegrated"
        },
        {
          "huang8592301@gapp.nthu.edu.tw,\nborrissu@gapp.nthu.edu.tw,": "ased decision.\nIn this paper, we evaluate this idea by target-"
        },
        {
          "huang8592301@gapp.nthu.edu.tw,\nborrissu@gapp.nthu.edu.tw,": "deeply into our daily life [1, 2, 3, 4, 5]. While these applica-"
        },
        {
          "huang8592301@gapp.nthu.edu.tw,\nborrissu@gapp.nthu.edu.tw,": "ing two sensitive attributes in speech, i.e., emotion and identity,"
        },
        {
          "huang8592301@gapp.nthu.edu.tw,\nborrissu@gapp.nthu.edu.tw,": "tions bring convenience to our\nlife,\nseveral growing concerns"
        },
        {
          "huang8592301@gapp.nthu.edu.tw,\nborrissu@gapp.nthu.edu.tw,": "and our aim is to show that\nthis approach can ﬂexibly achieve"
        },
        {
          "huang8592301@gapp.nthu.edu.tw,\nborrissu@gapp.nthu.edu.tw,": "have gained attention and need to be addressed with care. The"
        },
        {
          "huang8592301@gapp.nthu.edu.tw,\nborrissu@gapp.nthu.edu.tw,": "privacy-preserving applications by eliminating identity contents"
        },
        {
          "huang8592301@gapp.nthu.edu.tw,\nborrissu@gapp.nthu.edu.tw,": "ﬁrst\nis privacy due to concerns of sensitive information leak-"
        },
        {
          "huang8592301@gapp.nthu.edu.tw,\nborrissu@gapp.nthu.edu.tw,": "in SER or emotion contents in SV at ease."
        },
        {
          "huang8592301@gapp.nthu.edu.tw,\nborrissu@gapp.nthu.edu.tw,": "age: for example, users may not expect to disclose their identity"
        },
        {
          "huang8592301@gapp.nthu.edu.tw,\nborrissu@gapp.nthu.edu.tw,": "information while using a speech emotion recognition (SER)\nWe propose a framework of ﬂexible attribute masking for"
        },
        {
          "huang8592301@gapp.nthu.edu.tw,\nborrissu@gapp.nthu.edu.tw,": "system; on the other hand, users may not wish to share their\nspeech,\ninspired by the fair\nrepresentation learning [14]. We"
        },
        {
          "huang8592301@gapp.nthu.edu.tw,\nborrissu@gapp.nthu.edu.tw,": "emotional condition when being assessed by a speaker recogni-\naim to learn a layered disentangled speech representation with"
        },
        {
          "huang8592301@gapp.nthu.edu.tw,\nborrissu@gapp.nthu.edu.tw,": "tion (SR) system. Moreover,\nthe collective social norm would\na backbone of variational autoencoder\n(VAE)\n[15, 16]. We"
        },
        {
          "huang8592301@gapp.nthu.edu.tw,\nborrissu@gapp.nthu.edu.tw,": "create unwanted and often detrimental self-exaggerated issues\nspeciﬁcally propose a layered dropout strategy in a multi-task"
        },
        {
          "huang8592301@gapp.nthu.edu.tw,\nborrissu@gapp.nthu.edu.tw,": "around equality, e.g., unfair biases toward gender types [6] or\nframework to achieve attribute-alignment,\ni.e.,\nforces\nthe la-"
        },
        {
          "huang8592301@gapp.nthu.edu.tw,\nborrissu@gapp.nthu.edu.tw,": "race [7], when using data-driven approaches for speech tech-\ntent\nto align in an emotion-related to identity-related order. To"
        },
        {
          "huang8592301@gapp.nthu.edu.tw,\nborrissu@gapp.nthu.edu.tw,": "nology.\nSpeech is an informative signal which contains per-\nfurther clean up the aligned representation knowing that\nthese"
        },
        {
          "huang8592301@gapp.nthu.edu.tw,\nborrissu@gapp.nthu.edu.tw,": "sonal sensitive attributes by nature; hence developing appropri-\ntwo attributes are highly correlated [17, 18], we add adversarial"
        },
        {
          "huang8592301@gapp.nthu.edu.tw,\nborrissu@gapp.nthu.edu.tw,": "ate methods either to protect privacy information, such as iden-\nreversal\nlayer\nto each task-speciﬁc branch. Our strategy pro-"
        },
        {
          "huang8592301@gapp.nthu.edu.tw,\nborrissu@gapp.nthu.edu.tw,": "tity and emotion, or to mitigate the undesired biases, like gender\nvides ﬂexibility in either identity masking or emotion masking"
        },
        {
          "huang8592301@gapp.nthu.edu.tw,\nborrissu@gapp.nthu.edu.tw,": "and race, is critical in the current era.\nto come up with an identity-free latent\nfor privacy-preserving"
        },
        {
          "huang8592301@gapp.nthu.edu.tw,\nborrissu@gapp.nthu.edu.tw,": "SER or emotionless latent for privacy-preserving SV with a uni-\nRecently, several works in speech processing have started to"
        },
        {
          "huang8592301@gapp.nthu.edu.tw,\nborrissu@gapp.nthu.edu.tw,": "ﬁed learning framework.\nIn this work, we evaluate our method\naddress these issues using privacy-aware representation learn-"
        },
        {
          "huang8592301@gapp.nthu.edu.tw,\nborrissu@gapp.nthu.edu.tw,": "on MSP-Podcast [19] for SER and SV tasks using three types\ning. For example, Srivastava et al. used adversarial representa-"
        },
        {
          "huang8592301@gapp.nthu.edu.tw,\nborrissu@gapp.nthu.edu.tw,": "of\nfeature, and achieve competitive results on SER (emobase:\ntion learning on automatic speech recognition (ASR) to protect"
        },
        {
          "huang8592301@gapp.nthu.edu.tw,\nborrissu@gapp.nthu.edu.tw,": "52.41% weighted f-score, 41.14% EER), and an improvement\nspeaker identity [8], Alouf et al. used CycleGAN-based method"
        },
        {
          "huang8592301@gapp.nthu.edu.tw,\nborrissu@gapp.nthu.edu.tw,": "on SV (netvlad:\n34.35% weighted f-score, 10.91% EER; x-\nto generate emotion-less synthesized speech for voice assistant"
        },
        {
          "huang8592301@gapp.nthu.edu.tw,\nborrissu@gapp.nthu.edu.tw,": "vector:\n34.23% weighted f-score, 9.63% EER), compared to\nto hide personal affect [9], Jaiswal et al. used adversarial learn-"
        },
        {
          "huang8592301@gapp.nthu.edu.tw,\nborrissu@gapp.nthu.edu.tw,": "the state-of-the-art adversarial learning method.\ning to generate gender-invariant representation for identify-free"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Figure 1: An illustration of our proposed method for attribute-aligned representation learning. It includes three blocks: representation": ""
        },
        {
          "Figure 1: An illustration of our proposed method for attribute-aligned representation learning. It includes three blocks: representation": ""
        },
        {
          "Figure 1: An illustration of our proposed method for attribute-aligned representation learning. It includes three blocks: representation": "2.3.1. Variational Autoencoder (VAE)"
        },
        {
          "Figure 1: An illustration of our proposed method for attribute-aligned representation learning. It includes three blocks: representation": "In this work, we use disentangled representation learning via"
        },
        {
          "Figure 1: An illustration of our proposed method for attribute-aligned representation learning. It includes three blocks: representation": ""
        },
        {
          "Figure 1: An illustration of our proposed method for attribute-aligned representation learning. It includes three blocks: representation": "VAE to derive a latent node-wise independent\nrepresentation."
        },
        {
          "Figure 1: An illustration of our proposed method for attribute-aligned representation learning. It includes three blocks: representation": "VAE model aims to learn the marginal\nlikelihood of a data x,"
        },
        {
          "Figure 1: An illustration of our proposed method for attribute-aligned representation learning. It includes three blocks: representation": ""
        },
        {
          "Figure 1: An illustration of our proposed method for attribute-aligned representation learning. It includes three blocks: representation": "with the objective function:"
        },
        {
          "Figure 1: An illustration of our proposed method for attribute-aligned representation learning. It includes three blocks: representation": ""
        },
        {
          "Figure 1: An illustration of our proposed method for attribute-aligned representation learning. It includes three blocks: representation": ""
        },
        {
          "Figure 1: An illustration of our proposed method for attribute-aligned representation learning. It includes three blocks: representation": "(1)\nLV AE = Eq(z|x)[log p(x|z)] − DKL(q(z|x)||p(z))"
        },
        {
          "Figure 1: An illustration of our proposed method for attribute-aligned representation learning. It includes three blocks: representation": ""
        },
        {
          "Figure 1: An illustration of our proposed method for attribute-aligned representation learning. It includes three blocks: representation": "stands\nfor\nthe\nnon-negative Kullback-\nwhere DKL(||)"
        },
        {
          "Figure 1: An illustration of our proposed method for attribute-aligned representation learning. It includes three blocks: representation": "Leibler divergence.\nThe KL-divergence term encourages\nthe"
        },
        {
          "Figure 1: An illustration of our proposed method for attribute-aligned representation learning. It includes three blocks: representation": "posterior distribution to be close to an isotropic Gaussian to"
        },
        {
          "Figure 1: An illustration of our proposed method for attribute-aligned representation learning. It includes three blocks: representation": "achieve disentanglement purpose."
        },
        {
          "Figure 1: An illustration of our proposed method for attribute-aligned representation learning. It includes three blocks: representation": ""
        },
        {
          "Figure 1: An illustration of our proposed method for attribute-aligned representation learning. It includes three blocks: representation": "2.3.2.\nLayered Dropout with Adversarial Multitask Learning"
        },
        {
          "Figure 1: An illustration of our proposed method for attribute-aligned representation learning. It includes three blocks: representation": ""
        },
        {
          "Figure 1: An illustration of our proposed method for attribute-aligned representation learning. It includes three blocks: representation": "In this work, we propose a strategy of\nlayered dropout with a"
        },
        {
          "Figure 1: An illustration of our proposed method for attribute-aligned representation learning. It includes three blocks: representation": "multitask-learning architecture. Multitask learning aims to in-"
        },
        {
          "Figure 1: An illustration of our proposed method for attribute-aligned representation learning. It includes three blocks: representation": "clude both emotion and speaker identity information into the la-"
        },
        {
          "Figure 1: An illustration of our proposed method for attribute-aligned representation learning. It includes three blocks: representation": "tent codes. Layered dropout is further utilized to force these at-"
        },
        {
          "Figure 1: An illustration of our proposed method for attribute-aligned representation learning. It includes three blocks: representation": "tributes to align toward both ends of the latent codes resulting in"
        },
        {
          "Figure 1: An illustration of our proposed method for attribute-aligned representation learning. It includes three blocks: representation": "a layered representation. Also, adversarial branches, i.e., gradi-"
        },
        {
          "Figure 1: An illustration of our proposed method for attribute-aligned representation learning. It includes three blocks: representation": "ent reversal layer, are used to additionally ‘purify’ this attribute-"
        },
        {
          "Figure 1: An illustration of our proposed method for attribute-aligned representation learning. It includes three blocks: representation": "aligned representation."
        },
        {
          "Figure 1: An illustration of our proposed method for attribute-aligned representation learning. It includes three blocks: representation": ""
        },
        {
          "Figure 1: An illustration of our proposed method for attribute-aligned representation learning. It includes three blocks: representation": "Dropout\nis\na well-known regularization method in deep"
        },
        {
          "Figure 1: An illustration of our proposed method for attribute-aligned representation learning. It includes three blocks: representation": "learning to prevent neural networks from overﬁtting[21]. Lay-"
        },
        {
          "Figure 1: An illustration of our proposed method for attribute-aligned representation learning. It includes three blocks: representation": ""
        },
        {
          "Figure 1: An illustration of our proposed method for attribute-aligned representation learning. It includes three blocks: representation": "ered dropout works in a similar manner but with a different pur-"
        },
        {
          "Figure 1: An illustration of our proposed method for attribute-aligned representation learning. It includes three blocks: representation": ""
        },
        {
          "Figure 1: An illustration of our proposed method for attribute-aligned representation learning. It includes three blocks: representation": "pose. We propose to use this as a learning mechanism to make"
        },
        {
          "Figure 1: An illustration of our proposed method for attribute-aligned representation learning. It includes three blocks: representation": ""
        },
        {
          "Figure 1: An illustration of our proposed method for attribute-aligned representation learning. It includes three blocks: representation": "each dimension of\nthe latent codes carry different\nimportance"
        },
        {
          "Figure 1: An illustration of our proposed method for attribute-aligned representation learning. It includes three blocks: representation": ""
        },
        {
          "Figure 1: An illustration of our proposed method for attribute-aligned representation learning. It includes three blocks: representation": "to the designated task.\nIn our work,\nthe two tasks are deﬁned"
        },
        {
          "Figure 1: An illustration of our proposed method for attribute-aligned representation learning. It includes three blocks: representation": ""
        },
        {
          "Figure 1: An illustration of our proposed method for attribute-aligned representation learning. It includes three blocks: representation": "as the emotion recognition and the speaker veriﬁcation. We de-"
        },
        {
          "Figure 1: An illustration of our proposed method for attribute-aligned representation learning. It includes three blocks: representation": ""
        },
        {
          "Figure 1: An illustration of our proposed method for attribute-aligned representation learning. It includes three blocks: representation": "sign a dropout rate function making the probability of dropping"
        },
        {
          "Figure 1: An illustration of our proposed method for attribute-aligned representation learning. It includes three blocks: representation": ""
        },
        {
          "Figure 1: An illustration of our proposed method for attribute-aligned representation learning. It includes three blocks: representation": "decreases (or increases) monotonically for each node of the in-"
        },
        {
          "Figure 1: An illustration of our proposed method for attribute-aligned representation learning. It includes three blocks: representation": ""
        },
        {
          "Figure 1: An illustration of our proposed method for attribute-aligned representation learning. It includes three blocks: representation": "put layer. This effectively forces the target task’s discriminatory"
        },
        {
          "Figure 1: An illustration of our proposed method for attribute-aligned representation learning. It includes three blocks: representation": ""
        },
        {
          "Figure 1: An illustration of our proposed method for attribute-aligned representation learning. It includes three blocks: representation": "information to concentrate on nodes with lower dropout rates."
        },
        {
          "Figure 1: An illustration of our proposed method for attribute-aligned representation learning. It includes three blocks: representation": ""
        },
        {
          "Figure 1: An illustration of our proposed method for attribute-aligned representation learning. It includes three blocks: representation": "Let x denotes the input vector with N dimensions of a layer"
        },
        {
          "Figure 1: An illustration of our proposed method for attribute-aligned representation learning. It includes three blocks: representation": ""
        },
        {
          "Figure 1: An illustration of our proposed method for attribute-aligned representation learning. It includes three blocks: representation": "of a neural network, we deﬁne a vector with decreasing pre-"
        },
        {
          "Figure 1: An illustration of our proposed method for attribute-aligned representation learning. It includes three blocks: representation": ""
        },
        {
          "Figure 1: An illustration of our proposed method for attribute-aligned representation learning. It includes three blocks: representation": "serving rates p for task of emotion recognition (increasing pre-"
        },
        {
          "Figure 1: An illustration of our proposed method for attribute-aligned representation learning. It includes three blocks: representation": "serving rates for speaker veriﬁcation), where 0 ≤ pi ≤ 1 for"
        },
        {
          "Figure 1: An illustration of our proposed method for attribute-aligned representation learning. It includes three blocks: representation": ""
        },
        {
          "Figure 1: An illustration of our proposed method for attribute-aligned representation learning. It includes three blocks: representation": "i ∈ {0, . . . , N − 1}. With layered dropout,\nthe input vector x"
        },
        {
          "Figure 1: An illustration of our proposed method for attribute-aligned representation learning. It includes three blocks: representation": "of the feed-forward operation is replaced by vector (cid:101)x, generated"
        },
        {
          "Figure 1: An illustration of our proposed method for attribute-aligned representation learning. It includes three blocks: representation": "by:"
        },
        {
          "Figure 1: An illustration of our proposed method for attribute-aligned representation learning. It includes three blocks: representation": ""
        },
        {
          "Figure 1: An illustration of our proposed method for attribute-aligned representation learning. It includes three blocks: representation": "(2)\nmi ∼ Bernoulli(pi)"
        },
        {
          "Figure 1: An illustration of our proposed method for attribute-aligned representation learning. It includes three blocks: representation": ""
        },
        {
          "Figure 1: An illustration of our proposed method for attribute-aligned representation learning. It includes three blocks: representation": "x = m ∗ x\n(3)"
        },
        {
          "Figure 1: An illustration of our proposed method for attribute-aligned representation learning. It includes three blocks: representation": "(cid:101)"
        },
        {
          "Figure 1: An illustration of our proposed method for attribute-aligned representation learning. It includes three blocks: representation": "Here, ∗ denotes an element-wise product. m acts as a mask"
        },
        {
          "Figure 1: An illustration of our proposed method for attribute-aligned representation learning. It includes three blocks: representation": "before the vector x is fed into the layer.\nFor a dimension mi"
        },
        {
          "Figure 1: An illustration of our proposed method for attribute-aligned representation learning. It includes three blocks: representation": "in the vector m,\nit’s an independent Bernoulli random variable"
        },
        {
          "Figure 1: An illustration of our proposed method for attribute-aligned representation learning. It includes three blocks: representation": "with probability pi being 1, which means to preserve the ith"
        },
        {
          "Figure 1: An illustration of our proposed method for attribute-aligned representation learning. It includes three blocks: representation": "node, and 0 means\nto drop the ith node. While testing, W"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table 1: , our proposed LR-",
      "data": [
        {
          "Table 1: The experiment results are presented in weighted f-scores (%) and EER (%)": "stands for privacy-preserving, where columns of origin stand for original representation without privacy protection, PP-SER stands for",
          "for SER and SV respectively. Notice that PP": ""
        },
        {
          "Table 1: The experiment results are presented in weighted f-scores (%) and EER (%)": "identity-free SER, and PP-SV stands for emotionless SV.",
          "for SER and SV respectively. Notice that PP": ""
        },
        {
          "Table 1: The experiment results are presented in weighted f-scores (%) and EER (%)": "",
          "for SER and SV respectively. Notice that PP": ""
        },
        {
          "Table 1: The experiment results are presented in weighted f-scores (%) and EER (%)": "Method",
          "for SER and SV respectively. Notice that PP": ""
        },
        {
          "Table 1: The experiment results are presented in weighted f-scores (%) and EER (%)": "",
          "for SER and SV respectively. Notice that PP": "origin"
        },
        {
          "Table 1: The experiment results are presented in weighted f-scores (%) and EER (%)": "",
          "for SER and SV respectively. Notice that PP": "52.86"
        },
        {
          "Table 1: The experiment results are presented in weighted f-scores (%) and EER (%)": "emobase",
          "for SER and SV respectively. Notice that PP": ""
        },
        {
          "Table 1: The experiment results are presented in weighted f-scores (%) and EER (%)": "",
          "for SER and SV respectively. Notice that PP": "11.77"
        },
        {
          "Table 1: The experiment results are presented in weighted f-scores (%) and EER (%)": "",
          "for SER and SV respectively. Notice that PP": "50.01"
        },
        {
          "Table 1: The experiment results are presented in weighted f-scores (%) and EER (%)": "netvlad",
          "for SER and SV respectively. Notice that PP": ""
        },
        {
          "Table 1: The experiment results are presented in weighted f-scores (%) and EER (%)": "",
          "for SER and SV respectively. Notice that PP": "8.53"
        },
        {
          "Table 1: The experiment results are presented in weighted f-scores (%) and EER (%)": "",
          "for SER and SV respectively. Notice that PP": "52.57"
        },
        {
          "Table 1: The experiment results are presented in weighted f-scores (%) and EER (%)": "x-vector",
          "for SER and SV respectively. Notice that PP": ""
        },
        {
          "Table 1: The experiment results are presented in weighted f-scores (%) and EER (%)": "",
          "for SER and SV respectively. Notice that PP": "8.44"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table 1: , our proposed LR-",
      "data": [
        {
          "id\n8.14\n8.37\n40.66\n15.25": "34.13\nemo\n53.24\n52.60\n48.95",
          "49.51\n10.91\n8.20\n32.39\n14.51\n8.53": "51.76\n52.22\n36.05\n52.57\n51.09\n34.23"
        },
        {
          "id\n8.14\n8.37\n40.66\n15.25": "x-vector",
          "49.51\n10.91\n8.20\n32.39\n14.51\n8.53": ""
        },
        {
          "id\n8.14\n8.37\n40.66\n15.25": "id\n10.05\n8.55\n45.93\n13.56",
          "49.51\n10.91\n8.20\n32.39\n14.51\n8.53": "49.48\n9.63\n8.75\n37.61\n10.66\n8.44"
        },
        {
          "id\n8.14\n8.37\n40.66\n15.25": "represents for weights of network and the weights are scaled",
          "49.51\n10.91\n8.20\n32.39\n14.51\n8.53": "uate the performance of speaker veriﬁcation by equal error rate"
        },
        {
          "id\n8.14\n8.37\n40.66\n15.25": "as Wtest = W ∗ p(cid:62) and inference without dropout, which is",
          "49.51\n10.91\n8.20\n32.39\n14.51\n8.53": "(EER). For each feature set, we train a factorized layered repre-"
        },
        {
          "id\n8.14\n8.37\n40.66\n15.25": "same as the vanilla dropout layer.",
          "49.51\n10.91\n8.20\n32.39\n14.51\n8.53": "sentation encoder based on training set, select model using vali-"
        },
        {
          "id\n8.14\n8.37\n40.66\n15.25": "This\nlayered dropout mechanism alters\nthe dropout\nrates",
          "49.51\n10.91\n8.20\n32.39\n14.51\n8.53": "dation set, and test performance on test set. Assuming attackers"
        },
        {
          "id\n8.14\n8.37\n40.66\n15.25": "being applied on both sides of\nthe\nrepresentation before\nan",
          "49.51\n10.91\n8.20\n32.39\n14.51\n8.53": "have access to the training set with encoded representation and"
        },
        {
          "id\n8.14\n8.37\n40.66\n15.25": "emotion (identity) classiﬁer,\nthe latent codes form an aligned",
          "49.51\n10.91\n8.20\n32.39\n14.51\n8.53": "labels of speakers. Our goal is to generate a representation such"
        },
        {
          "id\n8.14\n8.37\n40.66\n15.25": "emotion-to-identity order\nfrom top end to bottom end during",
          "49.51\n10.91\n8.20\n32.39\n14.51\n8.53": "that for the encoded representations with particular sensitive at-"
        },
        {
          "id\n8.14\n8.37\n40.66\n15.25": "the optimization step. Furthermore, we add an auxiliary mech-",
          "49.51\n10.91\n8.20\n32.39\n14.51\n8.53": "tributes masked, neither attackers nor hosts are able to identify"
        },
        {
          "id\n8.14\n8.37\n40.66\n15.25": "anism of adversarial branches with gradient reversal layers [22]",
          "49.51\n10.91\n8.20\n32.39\n14.51\n8.53": "the sensitive attributes while the main task performance is main-"
        },
        {
          "id\n8.14\n8.37\n40.66\n15.25": "during multitask learning. The goal\nis to learn cleaner factor-",
          "49.51\n10.91\n8.20\n32.39\n14.51\n8.53": "tained."
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "(a) emobase\n(b) netvlad": "Figure 2: The performance curves in the masking experiment where the y-axis for SER results are weighted f-scores, and EER for SV.",
          "(c) x-vector": ""
        },
        {
          "(a) emobase\n(b) netvlad": "For the ﬁrst row (protect identity), the masking process starts in a bottom up order, while for the second row (protect emotion), we start",
          "(c) x-vector": ""
        },
        {
          "(a) emobase\n(b) netvlad": "masking from the top group.",
          "(c) x-vector": ""
        },
        {
          "(a) emobase\n(b) netvlad": "tion performance (4.46% EER better on emobase, 4.34% EER",
          "(c) x-vector": "reaches the middle part of the latent code, where it results in a"
        },
        {
          "(a) emobase\n(b) netvlad": "better on netvlad, and 3.93% EER better on x-vector), while",
          "(c) x-vector": "high EER indicating the point where we achieve an identity-free"
        },
        {
          "(a) emobase\n(b) netvlad": "achieving state-of-the-art emotion-related attributes protection",
          "(c) x-vector": "representation. We can also see that EER curves of LR-VAE"
        },
        {
          "(a) emobase\n(b) netvlad": "(0.39% WFS better on emobase, 4.57% WFS better on netvlad,",
          "(c) x-vector": "and A-VAE intersects, which shows that\nthe masked LR-VAE"
        },
        {
          "(a) emobase\n(b) netvlad": "and 0.10% WFS worse on x-vector)",
          "(c) x-vector": "latent can better eliminate the identity-related attributes. On the"
        },
        {
          "(a) emobase\n(b) netvlad": "We ﬁrst study the baseline DNN results shown in the col-",
          "(c) x-vector": "other hand, we also observe that the emotion recognition perfor-"
        },
        {
          "(a) emobase\n(b) netvlad": "umn, DNN,\nin table 1. The promising performance show that",
          "(c) x-vector": "mance slightly decreases toward the ending portion of masking"
        },
        {
          "(a) emobase\n(b) netvlad": "regardless of features, it contains both emotion and identity in-",
          "(c) x-vector": "process due to a signiﬁcant\nreduction in the node dimension,"
        },
        {
          "(a) emobase\n(b) netvlad": "formation.\nIt reinforces the current concerns that speech con-",
          "(c) x-vector": "though A-VAE has an even earlier performance drop."
        },
        {
          "(a) emobase\n(b) netvlad": "tains many personal attributes that users may not want to reveal.",
          "(c) x-vector": ""
        },
        {
          "(a) emobase\n(b) netvlad": "",
          "(c) x-vector": "Next, we study the emotion-protection speaker veriﬁcation"
        },
        {
          "(a) emobase\n(b) netvlad": "Then, we compare the DNN results to VAE results shown in the",
          "(c) x-vector": ""
        },
        {
          "(a) emobase\n(b) netvlad": "",
          "(c) x-vector": "task.\nThe results are shown in the lower\nrow of ﬁgure 2.\nIn"
        },
        {
          "(a) emobase\n(b) netvlad": "column, VAE origin, in table 1. We do see that there is a slight",
          "(c) x-vector": ""
        },
        {
          "(a) emobase\n(b) netvlad": "",
          "(c) x-vector": "this part of experiment, we start masking from the top of\nthe"
        },
        {
          "(a) emobase\n(b) netvlad": "performance drop in emotion recognition potentially due to the",
          "(c) x-vector": ""
        },
        {
          "(a) emobase\n(b) netvlad": "",
          "(c) x-vector": "latent code, similar to the previous procedure, but\nin a reverse"
        },
        {
          "(a) emobase\n(b) netvlad": "information loss caused by kl-divergence loss in VAE training",
          "(c) x-vector": ""
        },
        {
          "(a) emobase\n(b) netvlad": "",
          "(c) x-vector": "order. As the progress moves on,\nthe emotion recognition per-"
        },
        {
          "(a) emobase\n(b) netvlad": "for factorization, which is a trade-off between disentanglement",
          "(c) x-vector": ""
        },
        {
          "(a) emobase\n(b) netvlad": "",
          "(c) x-vector": "formance steadily decreases (weighted f-score decreases), and"
        },
        {
          "(a) emobase\n(b) netvlad": "and reconstruction.\nThis factorization VAE is however a key",
          "(c) x-vector": ""
        },
        {
          "(a) emobase\n(b) netvlad": "",
          "(c) x-vector": "ﬁnally reaches to a similar result comparing to A-VAE. On the"
        },
        {
          "(a) emobase\n(b) netvlad": "backbone in achieving our attribute-aligned representation.",
          "(c) x-vector": ""
        },
        {
          "(a) emobase\n(b) netvlad": "",
          "(c) x-vector": "other hand, we can see that the speaker veriﬁcation performance"
        },
        {
          "(a) emobase\n(b) netvlad": "To study how adversarial branches work in our\nframe-",
          "(c) x-vector": "of LR-VAE is better preserved comparing to A-VAE,\ni.e.,\nthe"
        },
        {
          "(a) emobase\n(b) netvlad": "work, we compare LR-VAE results to LR-VAE(w/o adv).\nIt",
          "(c) x-vector": "EER curve of LR-VAE is lower in the beginning and increases"
        },
        {
          "(a) emobase\n(b) netvlad": "shows\nthat without\nadversarial\nlearning in explicitly purify-",
          "(c) x-vector": "slower comparing to A-VAE."
        },
        {
          "(a) emobase\n(b) netvlad": "ing the emotion-related (identity-related) dimension to identity-",
          "(c) x-vector": ""
        },
        {
          "(a) emobase\n(b) netvlad": "free (emotion-free),\nthe representation learned is not “clean”",
          "(c) x-vector": "4. Conclusions and Future works"
        },
        {
          "(a) emobase\n(b) netvlad": "enough. Hence, while LR-VAE(w/o adv) also achieves com-",
          "(c) x-vector": ""
        },
        {
          "(a) emobase\n(b) netvlad": "petitive results on main tasks,\nthe sensitive attribute-preserving",
          "(c) x-vector": "In this paper, we propose a novel disentangled layered speech"
        },
        {
          "(a) emobase\n(b) netvlad": "results\nare\nusually worse.\nThis\nalso\ndemonstrates\nthat\nthe",
          "(c) x-vector": "representation learning that can ﬂexibly preserve sensitive at-"
        },
        {
          "(a) emobase\n(b) netvlad": "emotion-related (identity-related) attributes may contains iden-",
          "(c) x-vector": "tribute in a uniﬁed single training architecture. Compared with"
        },
        {
          "(a) emobase\n(b) netvlad": "tity (emotion) information if not explicitly cleaned.",
          "(c) x-vector": "other methods, our method achieves a competitive performance"
        },
        {
          "(a) emobase\n(b) netvlad": "",
          "(c) x-vector": "on identity-free SER and an improvement on emotionless SV."
        },
        {
          "(a) emobase\n(b) netvlad": "3.2.2. Analysis of Aligned Attributes",
          "(c) x-vector": ""
        },
        {
          "(a) emobase\n(b) netvlad": "",
          "(c) x-vector": "Also, we show that our proposed method help in pushing the"
        },
        {
          "(a) emobase\n(b) netvlad": "",
          "(c) x-vector": "emotion and identity information toward the both ends of\nthe"
        },
        {
          "(a) emobase\n(b) netvlad": "In this section, we further discuss the effectiveness of layered",
          "(c) x-vector": ""
        },
        {
          "(a) emobase\n(b) netvlad": "",
          "(c) x-vector": "latent codes, and this strategy provides a ﬂexible mechanism to"
        },
        {
          "(a) emobase\n(b) netvlad": "dropout that align attribute-speciﬁc information to both ends of",
          "(c) x-vector": ""
        },
        {
          "(a) emobase\n(b) netvlad": "",
          "(c) x-vector": "select\nthe target sensitive attributes to protect. Moreover, our"
        },
        {
          "(a) emobase\n(b) netvlad": "the latent codes. We conduct an experiment with the follow-",
          "(c) x-vector": ""
        },
        {
          "(a) emobase\n(b) netvlad": "",
          "(c) x-vector": "attribute aligned learning strategy reduce the training and mem-"
        },
        {
          "(a) emobase\n(b) netvlad": "ing procedure: we encode the input features into latent codes;",
          "(c) x-vector": ""
        },
        {
          "(a) emobase\n(b) netvlad": "",
          "(c) x-vector": "ory cost as we require only single process and single model\nto"
        },
        {
          "(a) emobase\n(b) netvlad": "next, we divide the latent dimensions into 32 groups;\nthen, for",
          "(c) x-vector": ""
        },
        {
          "(a) emobase\n(b) netvlad": "",
          "(c) x-vector": "achieve competitive privacy-preserving results on SER and SV"
        },
        {
          "(a) emobase\n(b) netvlad": "each step, we mask one additional group of\nlatent codes, and",
          "(c) x-vector": ""
        },
        {
          "(a) emobase\n(b) netvlad": "",
          "(c) x-vector": "against adversarial\ntraining, which requires training twice and"
        },
        {
          "(a) emobase\n(b) netvlad": "train two models, one for emotion recognition, and the other for",
          "(c) x-vector": ""
        },
        {
          "(a) emobase\n(b) netvlad": "",
          "(c) x-vector": "two models."
        },
        {
          "(a) emobase\n(b) netvlad": "speaker veriﬁcation. We compare the performance curve of LR-",
          "(c) x-vector": ""
        },
        {
          "(a) emobase\n(b) netvlad": "VAE and A-VAE to observe how layered dropout inﬂuence the",
          "(c) x-vector": "In the future, we will generalize our attribute aligned repre-"
        },
        {
          "(a) emobase\n(b) netvlad": "discriminatory power of the chosen latent code dimension.",
          "(c) x-vector": "sentation from two speciﬁc task to general multi-attributes sce-"
        },
        {
          "(a) emobase\n(b) netvlad": "We ﬁrst study the privacy-preserving speech emotion recog-",
          "(c) x-vector": "narios. We could utilize the middle portion of the latent codes"
        },
        {
          "(a) emobase\n(b) netvlad": "nition task. The results are shown in the upper row of ﬁgure 2.",
          "(c) x-vector": "to capture other information about the speaker, e.g., gender, per-"
        },
        {
          "(a) emobase\n(b) netvlad": "In this experiment, we start masking from the bottom of\nthe",
          "(c) x-vector": "sonality,\nsemantics, etc,\nin order\nto provide a more complete"
        },
        {
          "(a) emobase\n(b) netvlad": "latent code, which contains more identity-related attributes,\nto",
          "(c) x-vector": "proﬁle on this factorized speech representation. Moreover, as"
        },
        {
          "(a) emobase\n(b) netvlad": "the top of the latent code (more emotion-related attributes). As",
          "(c) x-vector": "the disentanglement achieved by kl divergence loss causes in-"
        },
        {
          "(a) emobase\n(b) netvlad": "the procedure moves on,\nthe speaker veriﬁcation performance",
          "(c) x-vector": "formation loss, different factorization methods may be applied"
        },
        {
          "(a) emobase\n(b) netvlad": "steadily decreases (EER increases) until\nthe masking process",
          "(c) x-vector": "to enhance our representation capacity."
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "5. References": "",
          "[16] ——,\n“An\nintroduction\nto\nvariational\nautoencoders,”\nFound.": "Trends Mach. Learn., vol. 12, no. 4, pp. 307–392, 2019. [Online]."
        },
        {
          "5. References": "[1] M. B. Akc¸ay and K. O˘guz, “Speech emotion recognition: Emo-",
          "[16] ——,\n“An\nintroduction\nto\nvariational\nautoencoders,”\nFound.": ""
        },
        {
          "5. References": "",
          "[16] ——,\n“An\nintroduction\nto\nvariational\nautoencoders,”\nFound.": "Available: https://doi.org/10.1561/2200000056"
        },
        {
          "5. References": "tional models, databases,\nfeatures, preprocessing methods,\nsup-",
          "[16] ——,\n“An\nintroduction\nto\nvariational\nautoencoders,”\nFound.": ""
        },
        {
          "5. References": "porting modalities, and classiﬁers,” Speech Communication, vol.",
          "[16] ——,\n“An\nintroduction\nto\nvariational\nautoencoders,”\nFound.": "[17] M. Bancroft, R. Lotﬁan,\nJ. Hansen,\nand C. Busso,\n“Explor-"
        },
        {
          "5. References": "116, pp. 56–76, 2020.",
          "[16] ——,\n“An\nintroduction\nto\nvariational\nautoencoders,”\nFound.": "ing the\nintersection between speaker veriﬁcation and emotion"
        },
        {
          "5. References": "",
          "[16] ——,\n“An\nintroduction\nto\nvariational\nautoencoders,”\nFound.": "recognition,” in 2019 8th International Conference on Affective"
        },
        {
          "5. References": "[2]\nF. Eyben, M. W¨ollmer,\nand B.\nSchuller,\n“Opensmile:\nThe",
          "[16] ——,\n“An\nintroduction\nto\nvariational\nautoencoders,”\nFound.": ""
        },
        {
          "5. References": "",
          "[16] ——,\n“An\nintroduction\nto\nvariational\nautoencoders,”\nFound.": "Computing\nand\nIntelligent\nInteraction Workshops\nand Demos"
        },
        {
          "5. References": "munich versatile and fast open-source audio feature extractor,”",
          "[16] ——,\n“An\nintroduction\nto\nvariational\nautoencoders,”\nFound.": ""
        },
        {
          "5. References": "",
          "[16] ——,\n“An\nintroduction\nto\nvariational\nautoencoders,”\nFound.": "(ACIIW), 2019, pp. 337–342."
        },
        {
          "5. References": "in Proceedings of\nthe 18th ACM International Conference on",
          "[16] ——,\n“An\nintroduction\nto\nvariational\nautoencoders,”\nFound.": ""
        },
        {
          "5. References": "Multimedia, ser. MM ’10.\nNew York, NY, USA: Association for",
          "[16] ——,\n“An\nintroduction\nto\nvariational\nautoencoders,”\nFound.": "[18] R.\nPappagari,\nT. Wang,\nJ. Villalba, N. Chen,\nand N. De-"
        },
        {
          "5. References": "Computing Machinery, 2010, p. 1459–1462. [Online]. Available:",
          "[16] ——,\n“An\nintroduction\nto\nvariational\nautoencoders,”\nFound.": "hak,\n“x-vectors meet\nemotions: A study on dependencies be-"
        },
        {
          "5. References": "https://doi.org/10.1145/1873951.1874246",
          "[16] ——,\n“An\nintroduction\nto\nvariational\nautoencoders,”\nFound.": "tween emotion and speaker recognition,” in ICASSP 2020-2020"
        },
        {
          "5. References": "",
          "[16] ——,\n“An\nintroduction\nto\nvariational\nautoencoders,”\nFound.": "IEEE International Conference on Acoustics, Speech and Signal"
        },
        {
          "5. References": "[3] W. Xie, A. Nagrani, J. S. Chung, and A. Zisserman, “Utterance-",
          "[16] ——,\n“An\nintroduction\nto\nvariational\nautoencoders,”\nFound.": ""
        },
        {
          "5. References": "",
          "[16] ——,\n“An\nintroduction\nto\nvariational\nautoencoders,”\nFound.": "Processing (ICASSP).\nIEEE, 2020, pp. 7169–7173."
        },
        {
          "5. References": "level aggregation for speaker recognition in the wild,” in ICASSP",
          "[16] ——,\n“An\nintroduction\nto\nvariational\nautoencoders,”\nFound.": ""
        },
        {
          "5. References": "2019-2019 IEEE International Conference on Acoustics, Speech",
          "[16] ——,\n“An\nintroduction\nto\nvariational\nautoencoders,”\nFound.": "[19] R. Lotﬁan and C. Busso, “Building naturalistic emotionally bal-"
        },
        {
          "5. References": "and Signal Processing (ICASSP).\nIEEE, 2019, pp. 5791–5795.",
          "[16] ——,\n“An\nintroduction\nto\nvariational\nautoencoders,”\nFound.": "anced speech corpus by retrieving emotional speech from existing"
        },
        {
          "5. References": "",
          "[16] ——,\n“An\nintroduction\nto\nvariational\nautoencoders,”\nFound.": "podcast recordings,” IEEE Transactions on Affective Computing,"
        },
        {
          "5. References": "[4] D. Braga, A. M. Madureira, L. Coelho,\nand R. Ajith,\n“Auto-",
          "[16] ——,\n“An\nintroduction\nto\nvariational\nautoencoders,”\nFound.": ""
        },
        {
          "5. References": "",
          "[16] ——,\n“An\nintroduction\nto\nvariational\nautoencoders,”\nFound.": "vol. 10, no. 4, pp. 471–483, 2019."
        },
        {
          "5. References": "matic detection of parkinson’s disease based on acoustic analy-",
          "[16] ——,\n“An\nintroduction\nto\nvariational\nautoencoders,”\nFound.": ""
        },
        {
          "5. References": "sis of speech,” Engineering Applications of Artiﬁcial Intelligence,",
          "[16] ——,\n“An\nintroduction\nto\nvariational\nautoencoders,”\nFound.": "[20]\nJ.\nS. Chung,\nA. Nagrani,\nand A.\nZisserman,\n“Voxceleb2:"
        },
        {
          "5. References": "vol. 77, pp. 148–158, 2019.",
          "[16] ——,\n“An\nintroduction\nto\nvariational\nautoencoders,”\nFound.": "Deep\nspeaker\nrecognition,”\nin Proc.\nInterspeech\n2018,\n2018,"
        },
        {
          "5. References": "",
          "[16] ——,\n“An\nintroduction\nto\nvariational\nautoencoders,”\nFound.": "pp. 1086–1090.\n[Online]. Available:\nhttp://dx.doi.org/10.21437/"
        },
        {
          "5. References": "[5]\nL. T´oth,\nI. Hoffmann, G. Gosztolya, V. Vincze, G. Szatl´oczki,",
          "[16] ——,\n“An\nintroduction\nto\nvariational\nautoencoders,”\nFound.": ""
        },
        {
          "5. References": "",
          "[16] ——,\n“An\nintroduction\nto\nvariational\nautoencoders,”\nFound.": "Interspeech.2018-1929"
        },
        {
          "5. References": "Z. B´anr´eti, M. P´ak´aski, and J. K´alm´an, “A speech recognition-",
          "[16] ——,\n“An\nintroduction\nto\nvariational\nautoencoders,”\nFound.": ""
        },
        {
          "5. References": "based solution for the automatic detection of mild cognitive im-",
          "[16] ——,\n“An\nintroduction\nto\nvariational\nautoencoders,”\nFound.": "[21] N.\nSrivastava, G. Hinton, A. Krizhevsky,\nI.\nSutskever,\nand"
        },
        {
          "5. References": "pairment from spontaneous speech,” Current Alzheimer Research,",
          "[16] ——,\n“An\nintroduction\nto\nvariational\nautoencoders,”\nFound.": "R. Salakhutdinov, “Dropout: A simple way to prevent neural net-"
        },
        {
          "5. References": "vol. 15, no. 2, pp. 130–138, 2018.",
          "[16] ——,\n“An\nintroduction\nto\nvariational\nautoencoders,”\nFound.": "works from overﬁtting,” J. Mach. Learn. Res., vol. 15, no. 1, p."
        },
        {
          "5. References": "",
          "[16] ——,\n“An\nintroduction\nto\nvariational\nautoencoders,”\nFound.": "1929–1958, Jan. 2014."
        },
        {
          "5. References": "[6] R. Tatman, “Gender and dialect bias in youtube’s automatic cap-",
          "[16] ——,\n“An\nintroduction\nto\nvariational\nautoencoders,”\nFound.": ""
        },
        {
          "5. References": "tions,” in Proceedings of\nthe First ACL Workshop on Ethics in",
          "[16] ——,\n“An\nintroduction\nto\nvariational\nautoencoders,”\nFound.": "[22] Y. Ganin and V. Lempitsky,\n“Unsupervised domain adaptation"
        },
        {
          "5. References": "Natural Language Processing, 2017, pp. 53–59.",
          "[16] ——,\n“An\nintroduction\nto\nvariational\nautoencoders,”\nFound.": "by\nbackpropagation,”\nin\nInternational\nconference\non machine"
        },
        {
          "5. References": "",
          "[16] ——,\n“An\nintroduction\nto\nvariational\nautoencoders,”\nFound.": "learning.\nPMLR, 2015, pp. 1180–1189."
        },
        {
          "5. References": "[7] M. Sap, D. Card, S. Gabriel, Y. Choi, and N. A. Smith, “The risk",
          "[16] ——,\n“An\nintroduction\nto\nvariational\nautoencoders,”\nFound.": ""
        },
        {
          "5. References": "of racial bias in hate speech detection,” in Proceedings of the 57th",
          "[16] ——,\n“An\nintroduction\nto\nvariational\nautoencoders,”\nFound.": ""
        },
        {
          "5. References": "annual meeting of\nthe association for computational\nlinguistics,",
          "[16] ——,\n“An\nintroduction\nto\nvariational\nautoencoders,”\nFound.": ""
        },
        {
          "5. References": "2019, pp. 1668–1678.",
          "[16] ——,\n“An\nintroduction\nto\nvariational\nautoencoders,”\nFound.": ""
        },
        {
          "5. References": "[8] B. M. L. Srivastava, A. Bellet, M. Tommasi,\nand E. Vincent,",
          "[16] ——,\n“An\nintroduction\nto\nvariational\nautoencoders,”\nFound.": ""
        },
        {
          "5. References": "“Privacy-Preserving\nAdversarial\nRepresentation\nLearning\nin",
          "[16] ——,\n“An\nintroduction\nto\nvariational\nautoencoders,”\nFound.": ""
        },
        {
          "5. References": "Interspeech 2019,\n2019,\nASR: Reality or\nIllusion?”\nin Proc.",
          "[16] ——,\n“An\nintroduction\nto\nvariational\nautoencoders,”\nFound.": ""
        },
        {
          "5. References": "pp. 3700–3704.\n[Online]. Available:\nhttp://dx.doi.org/10.21437/",
          "[16] ——,\n“An\nintroduction\nto\nvariational\nautoencoders,”\nFound.": ""
        },
        {
          "5. References": "Interspeech.2019-2415",
          "[16] ——,\n“An\nintroduction\nto\nvariational\nautoencoders,”\nFound.": ""
        },
        {
          "5. References": "[9] R. Alouﬁ, H. Haddadi,\nand D. Boyle,\n“Emotionless:\nprivacy-",
          "[16] ——,\n“An\nintroduction\nto\nvariational\nautoencoders,”\nFound.": ""
        },
        {
          "5. References": "preserving speech analysis\nfor voice assistants,” arXiv preprint",
          "[16] ——,\n“An\nintroduction\nto\nvariational\nautoencoders,”\nFound.": ""
        },
        {
          "5. References": "arXiv:1908.03632, 2019.",
          "[16] ——,\n“An\nintroduction\nto\nvariational\nautoencoders,”\nFound.": ""
        },
        {
          "5. References": "[10] M. Jaiswal and E. M. Provost, “Privacy enhanced multimodal neu-",
          "[16] ——,\n“An\nintroduction\nto\nvariational\nautoencoders,”\nFound.": ""
        },
        {
          "5. References": "ral representations for emotion recognition,” in AAAI, 2020.",
          "[16] ——,\n“An\nintroduction\nto\nvariational\nautoencoders,”\nFound.": ""
        },
        {
          "5. References": "[11] M. Xia, A. Field,\nand Y. Tsvetkov,\n“Demoting racial bias\nin",
          "[16] ——,\n“An\nintroduction\nto\nvariational\nautoencoders,”\nFound.": ""
        },
        {
          "5. References": "hate speech detection,” in Proceedings of the Eighth International",
          "[16] ——,\n“An\nintroduction\nto\nvariational\nautoencoders,”\nFound.": ""
        },
        {
          "5. References": "Workshop on Natural Language Processing for Social Media.",
          "[16] ——,\n“An\nintroduction\nto\nvariational\nautoencoders,”\nFound.": ""
        },
        {
          "5. References": "Online: Association for Computational Linguistics, Jul. 2020, pp.",
          "[16] ——,\n“An\nintroduction\nto\nvariational\nautoencoders,”\nFound.": ""
        },
        {
          "5. References": "7–14.\n[Online]. Available:\nhttps://www.aclweb.org/anthology/",
          "[16] ——,\n“An\nintroduction\nto\nvariational\nautoencoders,”\nFound.": ""
        },
        {
          "5. References": "2020.socialnlp-1.2",
          "[16] ——,\n“An\nintroduction\nto\nvariational\nautoencoders,”\nFound.": ""
        },
        {
          "5. References": "[12] W.-N. Hsu, Y. Zhang,\nand\nJ. Glass,\n“Learning\nlatent\nrepre-",
          "[16] ——,\n“An\nintroduction\nto\nvariational\nautoencoders,”\nFound.": ""
        },
        {
          "5. References": "sentations\nfor\nspeech generation and transformation,”\nin Proc.",
          "[16] ——,\n“An\nintroduction\nto\nvariational\nautoencoders,”\nFound.": ""
        },
        {
          "5. References": "Interspeech\n2017,\n2017,\npp.\n1273–1277.\n[Online]. Available:",
          "[16] ——,\n“An\nintroduction\nto\nvariational\nautoencoders,”\nFound.": ""
        },
        {
          "5. References": "http://dx.doi.org/10.21437/Interspeech.2017-349",
          "[16] ——,\n“An\nintroduction\nto\nvariational\nautoencoders,”\nFound.": ""
        },
        {
          "5. References": "[13]\nL.\nLi,\nD. Wang,\nY\n.\nChen,\nY\n.\nShi,\nZ.\nTang,\nand\nT.\nF.",
          "[16] ——,\n“An\nintroduction\nto\nvariational\nautoencoders,”\nFound.": ""
        },
        {
          "5. References": "Zheng,\n“Deep factorization for\nspeech signal,”\nin 2018 IEEE",
          "[16] ——,\n“An\nintroduction\nto\nvariational\nautoencoders,”\nFound.": ""
        },
        {
          "5. References": "International\nConference\non Acoustics,\nSpeech\nand\nSignal",
          "[16] ——,\n“An\nintroduction\nto\nvariational\nautoencoders,”\nFound.": ""
        },
        {
          "5. References": "Processing (ICASSP), 2018, pp. 5094–5098.",
          "[16] ——,\n“An\nintroduction\nto\nvariational\nautoencoders,”\nFound.": ""
        },
        {
          "5. References": "[14]\nE. Creager, D. Madras,\nJ.-H.\nJacobsen, M. Weis, K. Swersky,",
          "[16] ——,\n“An\nintroduction\nto\nvariational\nautoencoders,”\nFound.": ""
        },
        {
          "5. References": "T. Pitassi, and R. Zemel, “Flexibly fair\nrepresentation learning",
          "[16] ——,\n“An\nintroduction\nto\nvariational\nautoencoders,”\nFound.": ""
        },
        {
          "5. References": "by disentanglement,”\nin Proceedings of\nthe 36th International",
          "[16] ——,\n“An\nintroduction\nto\nvariational\nautoencoders,”\nFound.": ""
        },
        {
          "5. References": "Conference on Machine Learning,\nser. Proceedings of Machine",
          "[16] ——,\n“An\nintroduction\nto\nvariational\nautoencoders,”\nFound.": ""
        },
        {
          "5. References": "Learning Research, K. Chaudhuri and R. Salakhutdinov, Eds.,",
          "[16] ——,\n“An\nintroduction\nto\nvariational\nautoencoders,”\nFound.": ""
        },
        {
          "5. References": "vol. 97.\nPMLR, 09–15 Jun 2019, pp. 1436–1445.",
          "[16] ——,\n“An\nintroduction\nto\nvariational\nautoencoders,”\nFound.": ""
        },
        {
          "5. References": "[15] D.\nP.\nKingma\nand M. Welling,\n“Auto-Encoding\nVaria-",
          "[16] ——,\n“An\nintroduction\nto\nvariational\nautoencoders,”\nFound.": ""
        },
        {
          "5. References": "tional Bayes,”\nin\n2nd\nInternational Conference\non Learning",
          "[16] ——,\n“An\nintroduction\nto\nvariational\nautoencoders,”\nFound.": ""
        },
        {
          "5. References": "Representations,\nICLR 2014, Banff, AB, Canada, April 14-16,",
          "[16] ——,\n“An\nintroduction\nto\nvariational\nautoencoders,”\nFound.": ""
        },
        {
          "5. References": "2014, Conference Track Proceedings, 2014.",
          "[16] ——,\n“An\nintroduction\nto\nvariational\nautoencoders,”\nFound.": ""
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Speech emotion recognition: Emotional models, databases, features, preprocessing methods, supporting modalities, and classifiers",
      "authors": [
        "M Akc",
        "K Oguz"
      ],
      "year": "2020",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "2",
      "title": "Opensmile: The munich versatile and fast open-source audio feature extractor",
      "authors": [
        "F Eyben",
        "M Wöllmer",
        "B Schuller"
      ],
      "year": "2010",
      "venue": "Proceedings of the 18th ACM International Conference on Multimedia, ser. MM '10",
      "doi": "10.1145/1873951.1874246"
    },
    {
      "citation_id": "3",
      "title": "Utterancelevel aggregation for speaker recognition in the wild",
      "authors": [
        "W Xie",
        "A Nagrani",
        "J Chung",
        "A Zisserman"
      ],
      "year": "2019",
      "venue": "ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "4",
      "title": "Automatic detection of parkinson's disease based on acoustic analysis of speech",
      "authors": [
        "D Braga",
        "A Madureira",
        "L Coelho",
        "R Ajith"
      ],
      "year": "2019",
      "venue": "Engineering Applications of Artificial Intelligence"
    },
    {
      "citation_id": "5",
      "title": "A speech recognitionbased solution for the automatic detection of mild cognitive impairment from spontaneous speech",
      "authors": [
        "L Tóth",
        "I Hoffmann",
        "G Gosztolya",
        "V Vincze",
        "G Szatlóczki",
        "Z Bánréti",
        "M Pákáski",
        "J Kálmán"
      ],
      "year": "2018",
      "venue": "Current Alzheimer Research"
    },
    {
      "citation_id": "6",
      "title": "Gender and dialect bias in youtube's automatic captions",
      "authors": [
        "R Tatman"
      ],
      "year": "2017",
      "venue": "Proceedings of the First ACL Workshop on Ethics in Natural Language Processing"
    },
    {
      "citation_id": "7",
      "title": "The risk of racial bias in hate speech detection",
      "authors": [
        "M Sap",
        "D Card",
        "S Gabriel",
        "Y Choi",
        "N Smith"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th annual meeting of the association for computational linguistics"
    },
    {
      "citation_id": "8",
      "title": "Privacy-Preserving Adversarial Representation Learning in ASR: Reality or Illusion?",
      "authors": [
        "B Srivastava",
        "A Bellet",
        "M Tommasi",
        "E Vincent"
      ],
      "year": "2019",
      "venue": "Proc. Interspeech",
      "doi": "10.21437/Interspeech.2019-2415"
    },
    {
      "citation_id": "9",
      "title": "Emotionless: privacypreserving speech analysis for voice assistants",
      "authors": [
        "R Aloufi",
        "H Haddadi",
        "D Boyle"
      ],
      "year": "2019",
      "venue": "Emotionless: privacypreserving speech analysis for voice assistants",
      "arxiv": "arXiv:1908.03632"
    },
    {
      "citation_id": "10",
      "title": "Privacy enhanced multimodal neural representations for emotion recognition",
      "authors": [
        "E Provost"
      ],
      "year": "2020",
      "venue": "AAAI"
    },
    {
      "citation_id": "11",
      "title": "Demoting racial bias in hate speech detection",
      "authors": [
        "M Xia",
        "A Field",
        "Y Tsvetkov"
      ],
      "year": "2020",
      "venue": "Proceedings of the Eighth International Workshop on Natural Language Processing for Social Media"
    },
    {
      "citation_id": "12",
      "title": "Learning latent representations for speech generation and transformation",
      "authors": [
        "W.-N Hsu",
        "Y Zhang",
        "J Glass"
      ],
      "year": "2017",
      "venue": "Proc. Interspeech",
      "doi": "10.21437/Interspeech.2017-349"
    },
    {
      "citation_id": "13",
      "title": "Deep factorization for speech signal",
      "authors": [
        "L Li",
        "D Wang",
        "Y Chen",
        "Y Shi",
        "Z Tang",
        "T Zheng"
      ],
      "year": "2018",
      "venue": "2018 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "14",
      "title": "Flexibly fair representation learning by disentanglement",
      "authors": [
        "E Creager",
        "D Madras",
        "J.-H Jacobsen",
        "M Weis",
        "K Swersky",
        "T Pitassi",
        "R Zemel"
      ],
      "year": "2019",
      "venue": "Proceedings of the 36th International Conference on Machine Learning, ser. Proceedings of Machine Learning"
    },
    {
      "citation_id": "15",
      "title": "Auto-Encoding Variational Bayes",
      "authors": [
        "D Kingma",
        "M Welling"
      ],
      "year": "2014",
      "venue": "2nd International Conference on Learning Representations, ICLR 2014"
    },
    {
      "citation_id": "16",
      "title": "An introduction to variational autoencoders",
      "year": "2019",
      "venue": "Found. Trends Mach. Learn",
      "doi": "10.1561/2200000056"
    },
    {
      "citation_id": "17",
      "title": "Exploring the intersection between speaker verification and emotion recognition",
      "authors": [
        "M Bancroft",
        "R Lotfian",
        "J Hansen",
        "C Busso"
      ],
      "year": "2019",
      "venue": "2019 8th International Conference on Affective Computing and Intelligent Interaction Workshops and Demos"
    },
    {
      "citation_id": "18",
      "title": "x-vectors meet emotions: A study on dependencies between emotion and speaker recognition",
      "authors": [
        "R Pappagari",
        "T Wang",
        "J Villalba",
        "N Chen",
        "N Dehak"
      ],
      "year": "2020",
      "venue": "ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "19",
      "title": "Building naturalistic emotionally balanced speech corpus by retrieving emotional speech from existing podcast recordings",
      "authors": [
        "R Lotfian",
        "C Busso"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "20",
      "title": "Voxceleb2: Deep speaker recognition",
      "authors": [
        "J Chung",
        "A Nagrani",
        "A Zisserman"
      ],
      "year": "1929",
      "venue": "Proc. Interspeech",
      "doi": "10.21437/Interspeech.2018-1929"
    },
    {
      "citation_id": "21",
      "title": "Dropout: A simple way to prevent neural networks from overfitting",
      "authors": [
        "N Srivastava",
        "G Hinton",
        "A Krizhevsky",
        "I Sutskever",
        "R Salakhutdinov"
      ],
      "year": "2014",
      "venue": "J. Mach. Learn. Res"
    },
    {
      "citation_id": "22",
      "title": "Unsupervised domain adaptation by backpropagation",
      "authors": [
        "Y Ganin",
        "V Lempitsky"
      ],
      "year": "2015",
      "venue": "International conference on machine learning"
    }
  ]
}