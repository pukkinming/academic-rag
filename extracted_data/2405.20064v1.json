{
  "paper_id": "2405.20064v1",
  "title": "St Place Solution To Odyssey Emotion Recognition Challenge Task1: Tackling Class Imbalance Problem",
  "published": "2024-05-30T13:55:43Z",
  "authors": [
    "Mingjie Chen",
    "Hezhao Zhang",
    "Yuanchao Li",
    "Jiachen Luo",
    "Wen Wu",
    "Ziyang Ma",
    "Peter Bell",
    "Catherine Lai",
    "Joshua Reiss",
    "Lin Wang",
    "Philip C. Woodland",
    "Xie Chen",
    "Huy Phan",
    "Thomas Hain"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Speech emotion recognition is a challenging classification task with natural emotional speech, especially when the distribution of emotion types is imbalanced in the training and test data. In this case, it is more difficult for a model to learn to separate minority classes, resulting in those sometimes being ignored or frequently misclassified. Previous work has utilised class weighted loss for training, but problems remain as it sometimes causes over-fitting for minor classes or under-fitting for major classes. This paper presents the system developed by a multi-site team for the participation in the Odyssey 2024 Emotion Recognition Challenge Track-1. The challenge data has the aforementioned properties and therefore the presented systems aimed to tackle these issues, by introducing focal loss in optimisation when applying class weighted loss. Specifically, the focal loss is further weighted by prior-based class weights. Experimental results show that combining these two approaches brings better overall performance, by sacrificing performance on major classes. The system further employs a majority voting strategy to combine the outputs of an ensemble of 7 models. The models are trained independently, using different acoustic features and loss functions -with the aim to have different properties for different data. Hence these models show different performance preferences on major classes and minor classes. The ensemble system output obtained the best performance in the challenge, ranking top-1 among 68 submissions. It also outperformed all single models in our set. On the Odyssey 2024 Emotion Recognition Challenge Task-1 data the system obtained a Macro-F1 score of 35.69% and an accuracy of 37.32%.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "It is commonly assumed that for progress in conversational AI systems it is essential to enable computers to understand emotions from human speech signals. Speech emotion recognition (SER) is gaining increasing attention due to its wide range of potential application, especially in the context of the recent advancement of large language models  [1] . SER has been a research focus for a long time, however it is still a complex task because of the multitude of factors that affect the task, including context information, speaking environments, the personality and the speaking style of speakers, language, cultural aspect, commonsense knowledge etc.  [2, 3] .\n\nTypically there are two types of SER task due to the annotation style used in emotion labelled datasets, namely classification and regression  [3] . In SER classification tasks, speech segments are typically annotated with labels from a small set (4-8) of emotion classes. The task is to predict the correct (single) emotion class representing the complete speech segment.\n\nMany datasets  [4, 5, 6]  have been created for SER. Most of them  [6]  are created by recording actors portraying the required emotion in their speech. Other type of dataset  [4]  have been created by prompting speakers to express specific emotions. There are a few datasets  [5] , usually referred to as natural datasets, that are directly collected from sources containing spontaneous speech with natural emotional expressions. Previous work  [7, 8]  has shown that the performance of SER models on these three types of datasets differs significantly. SER classification tasks on natural datasets are still challenging, for many reasons  [9] .\n\nOne of the many challenges of SER classification on natural datasets is the imbalanced class distributions. Discriminative machine learning methods typically also choose decision boundaries on the basis of the prevalence of a class. Classes with low occurrence will not only get a poor representation, but also end up being considered of less relevance. Thus the training of machine learning models is difficult for minor classes. Models can be easily trained on major classes but can tend to ignore minor classes. One simple solution is to re-balance (reweight) the loss of a class by class frequencies. Many other solutions have been proposed to solve this problem, such as data augmentation  [10, 11] , new sampling strategies  [12] , and the use of a modified loss function  [13] . However, all of the above methods are likely to cause over-fitting problems on minor classes, thus sacrificing performance on major classes. This issue is particularly pertinent here, as the SER system is designed for participation in the Odyssey 2024 Emotion Recognition Challenge Track-1, which has class imbalanced training data, but a class balanced test set.\n\nWe have therefore developed a system that makes use of an ensemble system of 7 models. Each model takes in multi-modal features, from both audio and text. In order to obtain text representations an automatic speech recognition (ASR) model  [14]  is used to generate transcriptions from speech segments. To enhance the quality of the ASR transcriptions, an error correction model is used in post-processing In the ensemble system all models share the same architecture. They are trained independently with different audio features or different optimisation configurations. The loss function chosen is either the focal loss  [15]  or the cross entropy loss, weighted by prior-based class weights or uniform class weights.\n\nThe prior-based class weights are used to give more preference to minor classes than major classes during training. However, this strategy was found to cause over-fitting for minor classes, and thus a reduction in overall performance. In order to alleviate this issue, the use of focal loss  [15]  was included, which aims to give higher weights to more difficult samples and lower weights to easier samples.\n\nThe model trained with the focal loss together with the prior-based class weights obtained the best overall performance. However, this model performs more poor on major classes than the models trained with uniform class weights. To reach better overall performance the ensemble system is designed to comprise of models with different preferences on major classes and minor classes. Experimental results show that the ensemble system reaches the state-of-the-art performance in the Odyssey 2024 Emotion Recognition Challenge track-1  [16] . The system obtained a Macro-F1 of 35.69%, and has ranked the first among 68 submissions.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Works",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Fusion Techniques For Ser",
      "text": "Over the past decade, research on fusion techniques for SER has made significant progress. Alongside traditional featurelevel fusion (i.e., early fusion) and decision-level fusion (i.e., late fusion), there has been widespread exploration of sophisticated tensor-level fusion methods. For instance,  [17]  com-bined both modality-invariant and modality-specific features and applied various regularisation functions to reduce the distance between the modalities.  [18]  proposed a weighted fusion method based on a cross-attention module for encoding inter-modality relations and selectively capturing effective information.  [19]  developed a dual-branch model, with one timesynchronous branch that combined speech and text modalities, and a time-asynchronous branch integrating sentence text embeddings from context utterances.  [20]  fused ASR hidden states and ASR transcriptions with audio features in a hierarchical manner.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Asr Error Correction",
      "text": "As outlined above, the system makes use of ASR for transcript generation, using an off-the-shelf system. To enhance ASR performance ASR Error Correction (AEC) methods can be helpful, by post-processing using some knowledge about the task or target domain. The standard method for addressing language domain mismatch is to train an in-domain language model for direct integration with ASR systems.  [21, 22] . However, an alternative is to use AEC sequence to sequence models that correct the output. This is particularly useful in scenarios where the ASR is a black box  [23] . More recently, there has been interest in employing generative error correction using large language models  [24] . Furthermore, some studies have explored using both speech and ASR hypotheses as input, instead of relying solely on text data, leading to the development of cross-modal AEC methods  [24] .",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "System Description",
      "text": "As mentioned above, the system yielding the best performance makes use of an ensemble of models. Each model takes as input in frame-level audio features as well as token-level text features. The output of each model are the probabilities for each emotion class. In the majority voting, the prediction of each model is equally used for voting to emotion classes. The most voted emotion class is going to be the final prediction of the ensemble system.\n\nTo extract text features, this work used transcriptions generated by the Whisper-large-v2 model  [14] . As there are no transcriptions available for the test set, an ASR system is needed for transcriptions, but it will inevitably produce erroneous transcripts. Training an emotion classification model on ground truth transcripts would result in a mismatch between training and test conditions, thus all models make use of ASR output. To enhance the quality of the ASR transcriptions, a sequenceto-sequence ASR error correction model 1    [25]  is trained and then used to correct errors in the transcriptions in the test set.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Model Architecture",
      "text": "Figure  1  illustrates the principal architecture of models. Framelevel audio features and token-level text features are encoded by two Multi-Layer Perceptron (MLP) modules. Then transformer layers  [26]  are used to process audio and text features to encode dynamic information in features. In order to avoid over-fitting, the number of heads in the transformer layers is set to 1. The transformer layers are followed by a mean pooling layer, then the utterance-level audio features and text features are concatenated. The concatenated features are processed by the two MLP modules. The softmax output of the final MLP produces class probabilities.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Audio Features And Text Features",
      "text": "The audio and text features used in this work are presented in Table  1 . Three types of audio features are used: the final layer representations of WavLM-large  [27] ; the final layer representations of Hubert-extra-large  [28] ; and the final layer output of the encoder of Whisper-large-v3  [14] . In terms of text features, this work utilises Roberta-large  [29] . In order to enhance the representative capability of the text features, this work uses the average representations of the last 4 layers' from Roberta-large.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Loss Functions And Class Weights",
      "text": "This work considers two options for loss functions, the focal loss and the cross entropy loss. The loss function can be weighted by the prior-based class weights or uniform class weights. Combining loss functions with class weights, four types of optimisation configurations can be used. Prior-based class weights can be written as:\n\nwhere Nj is the number of samples in class j in a training or development set.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Cross-Entropy Loss And Focal Loss",
      "text": "Following the above notations , the cross-entropy loss can be defined as:\n\nwhere pi = P (yi|xi) is the output probability of the corrected class. The focal loss can be written as follows,\n\nwhere γ is a hyper-parameter. By combining with the class weights, the class-weighted focal loss can be written as:\n\nwhere wj is the class weight for class yi.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Ensemble Strategy",
      "text": "Table  2  lists the configurations of the 7 models in the ensemble system. All models use the averaged representations of the last 4 layers of Roberta-large (refer to Table  1 ) as text features.\n\nApart from the optimisation configurations, the values of the focal loss hyper-parameter was explored with γ = 2 and γ = 2.5, as well as three types of audio features.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Experimental Setup",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Dataset",
      "text": "The Odyssey 2024 Emotion Recognition Challenge  [16]  used the MSP-Podcast dataset v1.11  [5] . The dataset is derived from podcasts, and annotated through crowd-sourcing. Different from most datasets containing acted speech  [6]  or elicited speech  [4] , this dataset contains spontaneous speech with natural human emotions. The dataset is composed of five subsets: the training set, the development set, the test-1 set, the test-2 set, and the test-3 set. In this challenge, the test-3 set is used to measure the outcome, and the reference labels have not been made public.   3 , including the subset statistics, before and after the sample removal.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Implementations",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Multi-Modal Classifier Model",
      "text": "The Hubert-extra-large model, the WavLM-large model, the Whisper-large-v3 model and the Roberta-large model are imported from the transformers toolkit  [30] .\n\nIn terms of the model architecture, the hidden size of the transformer layers is 512 and the number of the transformer layers is set to 2. The MLP module before the transformer layers The models are trained with a batch size of 128, an initial learning rate of 1e-4, with a learning scheduler  [31] . The model checkpoint of the epoch with the best Macro-F1 on the development set is chosen for evaluation. The implementations are based on the recipe of the speechbrain toolkit  [32] . All models are implemented and trained with the PyTorch toolkit  [33] . Training takes about 2 hours and uses a maximum of 20 epochs.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Feature Fusion",
      "text": "A number of methods for feature fusion were considered. Due to limited time, however, only the following ones were implemented and compared.\n\n• Early fusion: text and audio features are concatenated at the embedding level.\n\n• Late fusion: text and audio features are learned independently and the final decision is determined based on respective outputs. • Early fusion + late fusion.\n\n• Tensor fusion: unimodal information and bimodal interactions are learned explicitly and aggregated  [34] .\n\n• Low-rank tensor fusion: multimodal fusion with modalityspecific low-rank factors, which scale linearly in the number of modalities  [35] .\n\nWhile sophisticated fusion approaches have often outperformed early fusion in various scenarios, this phenomenon was not observed in the experiments. Therefore, early fusion was used in the model, as it was found to outperform all other methods tested.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Asr Error Correction",
      "text": "A pretrained AEC model was used in this work, which has been trained on the English version of Common Voice 13.0  [36]  and TED Talk corpus  [37]  using a publicly available Sequence-to-Sequence (S2S) encoder-decoder architecture  [38] . This model was trained to convert ASR transcriptions to human-transcribed transcriptions (i.e., ground-truth text). Considering the disparity between the MSP-Podcast dataset and the two AEC pretraining corpora (e.g., out-of-domain words), this model was fine-tuned to enhance its performance. Specifically, the model was trained on the training set of the provided MSP-Podcast corpus for 10 epochs and then validated on the development set. The best checkpoint was saved to correct errors on the test set. The correction quality was evaluated using WER, BLEU, and GLEU scores for a comprehensive assessment. The results of the best checkpoint on the development set are presented in Table  5 , demonstrating the effectiveness of the AEC model. As there is no ground-truth text for the test set, there is no way to further evaluate its effectiveness. Given the same domain of the development and test sets, it is expected an improvement in accuracy on the test set by approximately 1%, as suggested by previous research on the impact of WER on SER performance  [39] .   ) . ↓: the lower the better. ↑: the higher the better.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Evaluation Metrics",
      "text": "According to the challenge evaluation setup, Macro-F1 is used as the primary metric. Macro-F1 is the unweighted average of the F1 score of each class. Apart from Macro-F1, the weighted accuracy (WA) and unweighted accuracy (UA) are used.  2",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Results",
      "text": "Results for the 7 models in the ensemble system are presented in Table  6 . Generally speaking, different configurations of audio features, loss functions and class weights yield differences in WA and UA results. From analysing the performance difference in WA and UA results, it would be easy to understand how the models perform on the major classes and minor classes. For convenience, in the following discussion, the index of the models in Table  6  are used to denote the models (e.g. model-1). Among the 7 models, the best Macro-F1 was obtained by the model-1, applying the focal loss (γ = 2) and prior-based class weights. Comparing model-1 to model-4, increasing γ = 2 causes an improvement in WA but a drop in UA, causing a reduction in Macro-F1.\n\nAmong the 3 types of audio features for model- The results for model-1, model-5, model-6 and model-7 can help to understand the effect of the two loss functions and the two class weights. Generally speaking, different loss functions and class weights yields models that have different preferences for major and minor classes. Specifically, when training with the uniform class weights, model-6 and the model-7 show good performance on UA but poor performance on WA. Comparing model-7 and model-5, the prior-based class weights give more attention to minor classes, causing a significant improvement in WA but a large drop in UA. This means that the prior-based class weights improve the performance on the minor classes, but sacrificing the performance on the major classes. The performance drop may be due to model-5 over-fitting on major classes. The effect of focal loss can be found through comparing model-1 and model-5, which shows that focal loss helps model-1 reach a better balance between the major classes and the minor classes. Hence, the model-1 reaches the highest overall macro-F1 performance.\n\nBased on the diverse performance of the models, an obvious strategy to build an ensemble is to combine the outputs of these models through a majority voting process. The results show that the ensemble system outperforms all of the 7 models on Macro-F1 and WA, reaching the state-of-the-art performance.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Conclusions",
      "text": "This paper introduces an ensemble system that includes 7 multimodal models, constructed for participation in the Odyssey 2024 Emotion Recognition Challenge. The system showed the best performance among a total of 68 submissions to the challenge, in all metrics under consideration. The 7 models were trained independently with different loss functions and class weights. Specifically, the cross entropy loss and the focal loss were used. Uniform class weights and prior-based class weights are studied. The experiment results show that the combinations of loss functions and class weights lead to different preference on the major classes or the minor classes",
      "page_start": 5,
      "page_end": 5
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The architecture of each model in the ensemble sys-",
      "page": 1
    },
    {
      "caption": "Figure 1: illustrates the principal architecture of models. Frame-",
      "page": 2
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Abstract": "Speech emotion recognition is a challenging classification task"
        },
        {
          "Abstract": "with natural emotional speech, especially when the distribution"
        },
        {
          "Abstract": "of emotion types\nis\nimbalanced in the training and test data."
        },
        {
          "Abstract": "In this case,\nit\nis more difficult\nfor a model\nto learn to sep-"
        },
        {
          "Abstract": "arate minority classes,\nresulting in those sometimes being ig-"
        },
        {
          "Abstract": "nored or\nfrequently misclassified.\nPrevious work has utilised"
        },
        {
          "Abstract": "class weighted loss for training, but problems remain as it some-"
        },
        {
          "Abstract": "times causes over-fitting for minor classes or under-fitting for"
        },
        {
          "Abstract": "major classes. This paper presents the system developed by a"
        },
        {
          "Abstract": "multi-site team for the participation in the Odyssey 2024 Emo-"
        },
        {
          "Abstract": "tion Recognition Challenge Track-1. The challenge data has the"
        },
        {
          "Abstract": "aforementioned properties and therefore the presented systems"
        },
        {
          "Abstract": "aimed to tackle these issues, by introducing focal\nloss in opti-"
        },
        {
          "Abstract": "misation when applying class weighted loss.\nSpecifically,\nthe"
        },
        {
          "Abstract": "focal loss is further weighted by prior-based class weights. Ex-"
        },
        {
          "Abstract": "perimental\nresults show that combining these two approaches"
        },
        {
          "Abstract": "brings better overall performance, by sacrificing performance"
        },
        {
          "Abstract": "on major classes. The system further employs a majority vot-"
        },
        {
          "Abstract": "ing strategy to combine the outputs of an ensemble of 7 models."
        },
        {
          "Abstract": "The models are trained independently, using different acoustic"
        },
        {
          "Abstract": "features and loss functions - with the aim to have different prop-"
        },
        {
          "Abstract": "erties for different data. Hence these models show different per-"
        },
        {
          "Abstract": "formance preferences on major classes and minor classes. The"
        },
        {
          "Abstract": "ensemble system output obtained the best performance in the"
        },
        {
          "Abstract": "challenge, ranking top-1 among 68 submissions. It also outper-"
        },
        {
          "Abstract": ""
        },
        {
          "Abstract": "formed all single models in our set. On the Odyssey 2024 Emo-"
        },
        {
          "Abstract": ""
        },
        {
          "Abstract": "tion Recognition Challenge Task-1 data the system obtained a"
        },
        {
          "Abstract": ""
        },
        {
          "Abstract": "Macro-F1 score of 35.69% and an accuracy of 37.32%."
        },
        {
          "Abstract": ""
        },
        {
          "Abstract": ""
        },
        {
          "Abstract": "1.\nIntroduction"
        },
        {
          "Abstract": ""
        },
        {
          "Abstract": ""
        },
        {
          "Abstract": "It\nis commonly assumed that for progress in conversational AI"
        },
        {
          "Abstract": ""
        },
        {
          "Abstract": "systems it\nis essential\nto enable computers to understand emo-"
        },
        {
          "Abstract": ""
        },
        {
          "Abstract": "tions from human speech signals. Speech emotion recognition"
        },
        {
          "Abstract": ""
        },
        {
          "Abstract": "(SER)\nis gaining increasing attention due to its wide range of"
        },
        {
          "Abstract": "potential application, especially in the context of the recent ad-"
        },
        {
          "Abstract": "vancement of\nlarge language models [1].\nSER has been a re-"
        },
        {
          "Abstract": "search focus for a long time, however it\nis still a complex task"
        },
        {
          "Abstract": ""
        },
        {
          "Abstract": "⋆The work does not relate to Huy Phan’s position at Amazon."
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "that are directly collected from sources containing spontaneous": "speech with natural emotional expressions. Previous work [7, 8]",
          "bined both modality-invariant\nand modality-specific\nfeatures": "and applied various regularisation functions to reduce the dis-"
        },
        {
          "that are directly collected from sources containing spontaneous": "has shown that\nthe performance of SER models on these three",
          "bined both modality-invariant\nand modality-specific\nfeatures": "tance between the modalities.\n[18] proposed a weighted fu-"
        },
        {
          "that are directly collected from sources containing spontaneous": "types of datasets differs significantly. SER classification tasks",
          "bined both modality-invariant\nand modality-specific\nfeatures": "sion method based on a cross-attention module for encoding"
        },
        {
          "that are directly collected from sources containing spontaneous": "on natural datasets are still challenging, for many reasons [9].",
          "bined both modality-invariant\nand modality-specific\nfeatures": "inter-modality relations and selectively capturing effective in-"
        },
        {
          "that are directly collected from sources containing spontaneous": "One of\nthe many challenges of SER classification on nat-",
          "bined both modality-invariant\nand modality-specific\nfeatures": "formation. [19] developed a dual-branch model, with one time-"
        },
        {
          "that are directly collected from sources containing spontaneous": "ural datasets is the imbalanced class distributions. Discrimina-",
          "bined both modality-invariant\nand modality-specific\nfeatures": "synchronous branch that combined speech and text modalities,"
        },
        {
          "that are directly collected from sources containing spontaneous": "tive machine learning methods typically also choose decision",
          "bined both modality-invariant\nand modality-specific\nfeatures": "and a time-asynchronous branch integrating sentence text em-"
        },
        {
          "that are directly collected from sources containing spontaneous": "boundaries on the basis of\nthe prevalence of a class. Classes",
          "bined both modality-invariant\nand modality-specific\nfeatures": "beddings from context utterances. [20] fused ASR hidden states"
        },
        {
          "that are directly collected from sources containing spontaneous": "with low occurrence will not only get a poor representation, but",
          "bined both modality-invariant\nand modality-specific\nfeatures": "and ASR transcriptions with audio features\nin a hierarchical"
        },
        {
          "that are directly collected from sources containing spontaneous": "also end up being considered of less relevance. Thus the train-",
          "bined both modality-invariant\nand modality-specific\nfeatures": "manner."
        },
        {
          "that are directly collected from sources containing spontaneous": "ing of machine learning models is difficult\nfor minor classes.",
          "bined both modality-invariant\nand modality-specific\nfeatures": ""
        },
        {
          "that are directly collected from sources containing spontaneous": "Models can be easily trained on major classes but can tend to",
          "bined both modality-invariant\nand modality-specific\nfeatures": "2.2. ASR Error Correction"
        },
        {
          "that are directly collected from sources containing spontaneous": "ignore minor classes. One simple solution is to re-balance (re-",
          "bined both modality-invariant\nand modality-specific\nfeatures": ""
        },
        {
          "that are directly collected from sources containing spontaneous": "",
          "bined both modality-invariant\nand modality-specific\nfeatures": "As outlined above, the system makes use of ASR for transcript"
        },
        {
          "that are directly collected from sources containing spontaneous": "weight)\nthe loss of a class by class frequencies. Many other",
          "bined both modality-invariant\nand modality-specific\nfeatures": ""
        },
        {
          "that are directly collected from sources containing spontaneous": "",
          "bined both modality-invariant\nand modality-specific\nfeatures": "generation, using an off-the-shelf system. To enhance ASR per-"
        },
        {
          "that are directly collected from sources containing spontaneous": "solutions have been proposed to solve this problem,\nsuch as",
          "bined both modality-invariant\nand modality-specific\nfeatures": ""
        },
        {
          "that are directly collected from sources containing spontaneous": "",
          "bined both modality-invariant\nand modality-specific\nfeatures": "formance ASR Error Correction (AEC) methods can be help-"
        },
        {
          "that are directly collected from sources containing spontaneous": "data augmentation [10, 11], new sampling strategies [12], and",
          "bined both modality-invariant\nand modality-specific\nfeatures": ""
        },
        {
          "that are directly collected from sources containing spontaneous": "",
          "bined both modality-invariant\nand modality-specific\nfeatures": "ful, by post-processing using some knowledge about the task or"
        },
        {
          "that are directly collected from sources containing spontaneous": "the use of a modified loss function [13]. However, all of\nthe",
          "bined both modality-invariant\nand modality-specific\nfeatures": ""
        },
        {
          "that are directly collected from sources containing spontaneous": "",
          "bined both modality-invariant\nand modality-specific\nfeatures": "target domain.\nThe standard method for addressing language"
        },
        {
          "that are directly collected from sources containing spontaneous": "above methods are likely to cause over-fitting problems on mi-",
          "bined both modality-invariant\nand modality-specific\nfeatures": ""
        },
        {
          "that are directly collected from sources containing spontaneous": "",
          "bined both modality-invariant\nand modality-specific\nfeatures": "domain mismatch is to train an in-domain language model for"
        },
        {
          "that are directly collected from sources containing spontaneous": "nor classes, thus sacrificing performance on major classes. This",
          "bined both modality-invariant\nand modality-specific\nfeatures": ""
        },
        {
          "that are directly collected from sources containing spontaneous": "",
          "bined both modality-invariant\nand modality-specific\nfeatures": "direct\nintegration with ASR systems.\n[21, 22]. However, an"
        },
        {
          "that are directly collected from sources containing spontaneous": "issue is particularly pertinent here, as the SER system is de-",
          "bined both modality-invariant\nand modality-specific\nfeatures": ""
        },
        {
          "that are directly collected from sources containing spontaneous": "",
          "bined both modality-invariant\nand modality-specific\nfeatures": "alternative is to use AEC sequence to sequence models that cor-"
        },
        {
          "that are directly collected from sources containing spontaneous": "signed for participation in the Odyssey 2024 Emotion Recog-",
          "bined both modality-invariant\nand modality-specific\nfeatures": ""
        },
        {
          "that are directly collected from sources containing spontaneous": "",
          "bined both modality-invariant\nand modality-specific\nfeatures": "rect the output. This is particularly useful in scenarios where the"
        },
        {
          "that are directly collected from sources containing spontaneous": "nition Challenge Track-1, which has class imbalanced training",
          "bined both modality-invariant\nand modality-specific\nfeatures": ""
        },
        {
          "that are directly collected from sources containing spontaneous": "",
          "bined both modality-invariant\nand modality-specific\nfeatures": "ASR is a black box [23]. More recently, there has been interest"
        },
        {
          "that are directly collected from sources containing spontaneous": "data, but a class balanced test set.",
          "bined both modality-invariant\nand modality-specific\nfeatures": ""
        },
        {
          "that are directly collected from sources containing spontaneous": "",
          "bined both modality-invariant\nand modality-specific\nfeatures": "in employing generative error correction using large language"
        },
        {
          "that are directly collected from sources containing spontaneous": "We have therefore developed a system that makes use of an",
          "bined both modality-invariant\nand modality-specific\nfeatures": ""
        },
        {
          "that are directly collected from sources containing spontaneous": "",
          "bined both modality-invariant\nand modality-specific\nfeatures": "models [24].\nFurthermore,\nsome studies have explored using"
        },
        {
          "that are directly collected from sources containing spontaneous": "ensemble system of 7 models. Each model takes in multi-modal",
          "bined both modality-invariant\nand modality-specific\nfeatures": ""
        },
        {
          "that are directly collected from sources containing spontaneous": "",
          "bined both modality-invariant\nand modality-specific\nfeatures": "both speech and ASR hypotheses as input,\ninstead of\nrelying"
        },
        {
          "that are directly collected from sources containing spontaneous": "features, from both audio and text.\nIn order to obtain text rep-",
          "bined both modality-invariant\nand modality-specific\nfeatures": ""
        },
        {
          "that are directly collected from sources containing spontaneous": "",
          "bined both modality-invariant\nand modality-specific\nfeatures": "solely on text data,\nleading to the development of cross-modal"
        },
        {
          "that are directly collected from sources containing spontaneous": "resentations an automatic speech recognition (ASR) model [14]",
          "bined both modality-invariant\nand modality-specific\nfeatures": ""
        },
        {
          "that are directly collected from sources containing spontaneous": "",
          "bined both modality-invariant\nand modality-specific\nfeatures": "AEC methods [24]."
        },
        {
          "that are directly collected from sources containing spontaneous": "is used to generate transcriptions from speech segments. To en-",
          "bined both modality-invariant\nand modality-specific\nfeatures": ""
        },
        {
          "that are directly collected from sources containing spontaneous": "hance the quality of the ASR transcriptions, an error correction",
          "bined both modality-invariant\nand modality-specific\nfeatures": ""
        },
        {
          "that are directly collected from sources containing spontaneous": "model is used in post-processing",
          "bined both modality-invariant\nand modality-specific\nfeatures": "3.\nSystem Description"
        },
        {
          "that are directly collected from sources containing spontaneous": "In the ensemble system all models share the same architec-",
          "bined both modality-invariant\nand modality-specific\nfeatures": ""
        },
        {
          "that are directly collected from sources containing spontaneous": "",
          "bined both modality-invariant\nand modality-specific\nfeatures": "As mentioned above,\nthe system yielding the best performance"
        },
        {
          "that are directly collected from sources containing spontaneous": "ture. They are trained independently with different audio fea-",
          "bined both modality-invariant\nand modality-specific\nfeatures": ""
        },
        {
          "that are directly collected from sources containing spontaneous": "",
          "bined both modality-invariant\nand modality-specific\nfeatures": "makes use of an ensemble of models. Each model takes as input"
        },
        {
          "that are directly collected from sources containing spontaneous": "tures or different optimisation configurations. The loss function",
          "bined both modality-invariant\nand modality-specific\nfeatures": ""
        },
        {
          "that are directly collected from sources containing spontaneous": "",
          "bined both modality-invariant\nand modality-specific\nfeatures": "in frame-level audio features as well as token-level text features."
        },
        {
          "that are directly collected from sources containing spontaneous": "chosen is either\nthe focal\nloss [15] or\nthe cross entropy loss,",
          "bined both modality-invariant\nand modality-specific\nfeatures": ""
        },
        {
          "that are directly collected from sources containing spontaneous": "",
          "bined both modality-invariant\nand modality-specific\nfeatures": "The output of each model are the probabilities for each emo-"
        },
        {
          "that are directly collected from sources containing spontaneous": "weighted by prior-based class weights or uniform class weights.",
          "bined both modality-invariant\nand modality-specific\nfeatures": ""
        },
        {
          "that are directly collected from sources containing spontaneous": "",
          "bined both modality-invariant\nand modality-specific\nfeatures": "tion class.\nIn the majority voting,\nthe prediction of each model"
        },
        {
          "that are directly collected from sources containing spontaneous": "The prior-based class weights are used to give more prefer-",
          "bined both modality-invariant\nand modality-specific\nfeatures": "is equally used for voting to emotion classes. The most voted"
        },
        {
          "that are directly collected from sources containing spontaneous": "ence to minor classes than major classes during training. How-",
          "bined both modality-invariant\nand modality-specific\nfeatures": "emotion class is going to be the final prediction of the ensemble"
        },
        {
          "that are directly collected from sources containing spontaneous": "ever,\nthis\nstrategy was\nfound to cause over-fitting for minor",
          "bined both modality-invariant\nand modality-specific\nfeatures": "system."
        },
        {
          "that are directly collected from sources containing spontaneous": "classes, and thus a reduction in overall performance.\nIn order",
          "bined both modality-invariant\nand modality-specific\nfeatures": ""
        },
        {
          "that are directly collected from sources containing spontaneous": "",
          "bined both modality-invariant\nand modality-specific\nfeatures": "To extract text features, this work used transcriptions gener-"
        },
        {
          "that are directly collected from sources containing spontaneous": "to alleviate this issue,\nthe use of focal\nloss [15] was included,",
          "bined both modality-invariant\nand modality-specific\nfeatures": ""
        },
        {
          "that are directly collected from sources containing spontaneous": "",
          "bined both modality-invariant\nand modality-specific\nfeatures": "ated by the Whisper-large-v2 model [14]. As there are no tran-"
        },
        {
          "that are directly collected from sources containing spontaneous": "which aims to give higher weights to more difficult samples and",
          "bined both modality-invariant\nand modality-specific\nfeatures": ""
        },
        {
          "that are directly collected from sources containing spontaneous": "",
          "bined both modality-invariant\nand modality-specific\nfeatures": "scriptions available for\nthe test set, an ASR system is needed"
        },
        {
          "that are directly collected from sources containing spontaneous": "lower weights to easier samples.",
          "bined both modality-invariant\nand modality-specific\nfeatures": ""
        },
        {
          "that are directly collected from sources containing spontaneous": "",
          "bined both modality-invariant\nand modality-specific\nfeatures": "for transcriptions, but it will inevitably produce erroneous tran-"
        },
        {
          "that are directly collected from sources containing spontaneous": "The model\ntrained with the focal\nloss\ntogether with the",
          "bined both modality-invariant\nand modality-specific\nfeatures": "scripts.\nTraining an emotion classification model on ground"
        },
        {
          "that are directly collected from sources containing spontaneous": "prior-based class weights obtained the best overall performance.",
          "bined both modality-invariant\nand modality-specific\nfeatures": "truth transcripts would result\nin a mismatch between training"
        },
        {
          "that are directly collected from sources containing spontaneous": "However, this model performs more poor on major classes than",
          "bined both modality-invariant\nand modality-specific\nfeatures": "and test conditions,\nthus all models make use of ASR output."
        },
        {
          "that are directly collected from sources containing spontaneous": "the models trained with uniform class weights. To reach better",
          "bined both modality-invariant\nand modality-specific\nfeatures": "To enhance the quality of the ASR transcriptions, a sequence-"
        },
        {
          "that are directly collected from sources containing spontaneous": "overall performance the ensemble system is designed to com-",
          "bined both modality-invariant\nand modality-specific\nfeatures": "to-sequence ASR error correction model1\n[25]\nis\ntrained and"
        },
        {
          "that are directly collected from sources containing spontaneous": "prise of models with different preferences on major classes and",
          "bined both modality-invariant\nand modality-specific\nfeatures": "then used to correct errors in the transcriptions in the test set."
        },
        {
          "that are directly collected from sources containing spontaneous": "minor classes.\nExperimental\nresults\nshow that\nthe ensemble",
          "bined both modality-invariant\nand modality-specific\nfeatures": ""
        },
        {
          "that are directly collected from sources containing spontaneous": "system reaches the state-of-the-art performance in the Odyssey",
          "bined both modality-invariant\nand modality-specific\nfeatures": "3.1. Model Architecture"
        },
        {
          "that are directly collected from sources containing spontaneous": "2024 Emotion Recognition Challenge track-1 [16]. The system",
          "bined both modality-invariant\nand modality-specific\nfeatures": ""
        },
        {
          "that are directly collected from sources containing spontaneous": "",
          "bined both modality-invariant\nand modality-specific\nfeatures": "Figure 1 illustrates the principal architecture of models. Frame-"
        },
        {
          "that are directly collected from sources containing spontaneous": "obtained a Macro-F1 of 35.69%, and has ranked the first among",
          "bined both modality-invariant\nand modality-specific\nfeatures": ""
        },
        {
          "that are directly collected from sources containing spontaneous": "",
          "bined both modality-invariant\nand modality-specific\nfeatures": "level audio features and token-level text features are encoded by"
        },
        {
          "that are directly collected from sources containing spontaneous": "68 submissions.",
          "bined both modality-invariant\nand modality-specific\nfeatures": ""
        },
        {
          "that are directly collected from sources containing spontaneous": "",
          "bined both modality-invariant\nand modality-specific\nfeatures": "two Multi-Layer Perceptron (MLP) modules. Then transformer"
        },
        {
          "that are directly collected from sources containing spontaneous": "",
          "bined both modality-invariant\nand modality-specific\nfeatures": "layers [26] are used to process audio and text features to encode"
        },
        {
          "that are directly collected from sources containing spontaneous": "2. Related Works",
          "bined both modality-invariant\nand modality-specific\nfeatures": ""
        },
        {
          "that are directly collected from sources containing spontaneous": "",
          "bined both modality-invariant\nand modality-specific\nfeatures": "dynamic information in features.\nIn order to avoid over-fitting,"
        },
        {
          "that are directly collected from sources containing spontaneous": "",
          "bined both modality-invariant\nand modality-specific\nfeatures": "the number of heads in the transformer layers is set\nto 1. The"
        },
        {
          "that are directly collected from sources containing spontaneous": "2.1. Fusion Techniques for SER",
          "bined both modality-invariant\nand modality-specific\nfeatures": ""
        },
        {
          "that are directly collected from sources containing spontaneous": "",
          "bined both modality-invariant\nand modality-specific\nfeatures": "transformer layers are followed by a mean pooling layer,\nthen"
        },
        {
          "that are directly collected from sources containing spontaneous": "Over\nthe past decade,\nresearch on fusion techniques for SER",
          "bined both modality-invariant\nand modality-specific\nfeatures": "the utterance-level audio features and text features are concate-"
        },
        {
          "that are directly collected from sources containing spontaneous": "has made significant progress. Alongside traditional\nfeature-",
          "bined both modality-invariant\nand modality-specific\nfeatures": "nated. The concatenated features are processed by the two MLP"
        },
        {
          "that are directly collected from sources containing spontaneous": "level\nfusion (i.e., early fusion) and decision-level\nfusion (i.e.,",
          "bined both modality-invariant\nand modality-specific\nfeatures": ""
        },
        {
          "that are directly collected from sources containing spontaneous": "late fusion),\nthere has been widespread exploration of sophis-",
          "bined both modality-invariant\nand modality-specific\nfeatures": "1https://huggingface.co/YC-Li/Sequence-to-Sequence-ASR-Error-"
        },
        {
          "that are directly collected from sources containing spontaneous": "ticated tensor-level\nfusion methods.\nFor\ninstance,\n[17] com-",
          "bined both modality-invariant\nand modality-specific\nfeatures": "Correction"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table 1: Audio and text features, where ‘Dim’ denotes the wherep i =P(y i |x i )istheoutputprobabilityofthecorrected",
      "data": [
        {
          "Features": "WavLM-large",
          "Modality": "Audio",
          "Dim": "1024",
          "#Params": "300M",
          "Hours": "94K",
          "3.3.2. Cross-Entropy Loss and Focal Loss": "Following the above notations ,\nthe cross-entropy loss can be"
        },
        {
          "Features": "Hubert-extra-large",
          "Modality": "Audio",
          "Dim": "1280",
          "#Params": "1B",
          "Hours": "60K",
          "3.3.2. Cross-Entropy Loss and Focal Loss": "defined as:"
        },
        {
          "Features": "Whisper-large-v3",
          "Modality": "Audio",
          "Dim": "1280",
          "#Params": "1.5B",
          "Hours": "680K",
          "3.3.2. Cross-Entropy Loss and Focal Loss": ""
        },
        {
          "Features": "",
          "Modality": "",
          "Dim": "",
          "#Params": "",
          "Hours": "",
          "3.3.2. Cross-Entropy Loss and Focal Loss": "N(cid:88) i\n− log pi,\nLce = 1/N"
        },
        {
          "Features": "Roberta-large",
          "Modality": "text",
          "Dim": "1024",
          "#Params": "355M",
          "Hours": "-",
          "3.3.2. Cross-Entropy Loss and Focal Loss": ""
        },
        {
          "Features": "",
          "Modality": "",
          "Dim": "",
          "#Params": "",
          "Hours": "",
          "3.3.2. Cross-Entropy Loss and Focal Loss": "=1"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table 1: Audio and text features, where ‘Dim’ denotes the wherep i =P(y i |x i )istheoutputprobabilityofthecorrected",
      "data": [
        {
          "dimensionality of": "number of parameters,",
          "frame-level": "",
          "features,": "‘Hours’ denotes the amount of speech",
          "‘#Params’ denotes the": "",
          "class. The focal loss can be written as follows,": ""
        },
        {
          "dimensionality of": "data used for pretraining.",
          "frame-level": "",
          "features,": "",
          "‘#Params’ denotes the": "",
          "class. The focal loss can be written as follows,": ""
        },
        {
          "dimensionality of": "",
          "frame-level": "",
          "features,": "",
          "‘#Params’ denotes the": "",
          "class. The focal loss can be written as follows,": "Lfocal = 1/N"
        },
        {
          "dimensionality of": "",
          "frame-level": "",
          "features,": "",
          "‘#Params’ denotes the": "",
          "class. The focal loss can be written as follows,": ""
        },
        {
          "dimensionality of": "Index",
          "frame-level": "Loss Function",
          "features,": "Class Weights",
          "‘#Params’ denotes the": "Audio Features",
          "class. The focal loss can be written as follows,": ""
        },
        {
          "dimensionality of": "",
          "frame-level": "",
          "features,": "",
          "‘#Params’ denotes the": "",
          "class. The focal loss can be written as follows,": "where γ is a hyper-parameter."
        },
        {
          "dimensionality of": "1",
          "frame-level": "Focal (γ = 2 )",
          "features,": "Prior-based",
          "‘#Params’ denotes the": "Whisper",
          "class. The focal loss can be written as follows,": ""
        },
        {
          "dimensionality of": "",
          "frame-level": "",
          "features,": "",
          "‘#Params’ denotes the": "",
          "class. The focal loss can be written as follows,": "By combining with the class weights,"
        },
        {
          "dimensionality of": "2",
          "frame-level": "Focal (γ = 2.5 )",
          "features,": "Prior-based",
          "‘#Params’ denotes the": "Whisper",
          "class. The focal loss can be written as follows,": ""
        },
        {
          "dimensionality of": "",
          "frame-level": "",
          "features,": "",
          "‘#Params’ denotes the": "",
          "class. The focal loss can be written as follows,": "focal loss can be written as:"
        },
        {
          "dimensionality of": "3",
          "frame-level": "CE",
          "features,": "Prior-based",
          "‘#Params’ denotes the": "Whisper",
          "class. The focal loss can be written as follows,": ""
        },
        {
          "dimensionality of": "4",
          "frame-level": "Focal (γ = 2)",
          "features,": "Uniform",
          "‘#Params’ denotes the": "Whisper",
          "class. The focal loss can be written as follows,": ""
        },
        {
          "dimensionality of": "5",
          "frame-level": "CE",
          "features,": "Uniform",
          "‘#Params’ denotes the": "Whisper",
          "class. The focal loss can be written as follows,": "L′"
        },
        {
          "dimensionality of": "",
          "frame-level": "",
          "features,": "",
          "‘#Params’ denotes the": "",
          "class. The focal loss can be written as follows,": "N(cid:88) i\nfocal = 1/N"
        },
        {
          "dimensionality of": "6",
          "frame-level": "Focal (γ = 2)",
          "features,": "Prior-based",
          "‘#Params’ denotes the": "WavLM",
          "class. The focal loss can be written as follows,": ""
        },
        {
          "dimensionality of": "7",
          "frame-level": "Focal (γ = 3 )",
          "features,": "Prior-based",
          "‘#Params’ denotes the": "Hubert",
          "class. The focal loss can be written as follows,": ""
        },
        {
          "dimensionality of": "",
          "frame-level": "",
          "features,": "",
          "‘#Params’ denotes the": "",
          "class. The focal loss can be written as follows,": "where wj\nis the class weight for class yi."
        }
      ],
      "page": 3
    },
    {
      "caption": "Table 1: Audio and text features, where ‘Dim’ denotes the wherep i =P(y i |x i )istheoutputprobabilityofthecorrected",
      "data": [
        {
          "Table 2: Configurations of models\nin the ensemble systems,": ""
        },
        {
          "Table 2: Configurations of models\nin the ensemble systems,": "where ‘CE’ denotes cross-entropy."
        },
        {
          "Table 2: Configurations of models\nin the ensemble systems,": ""
        },
        {
          "Table 2: Configurations of models\nin the ensemble systems,": ""
        },
        {
          "Table 2: Configurations of models\nin the ensemble systems,": "modules. The softmax output of the final MLP produces class"
        },
        {
          "Table 2: Configurations of models\nin the ensemble systems,": "probabilities."
        },
        {
          "Table 2: Configurations of models\nin the ensemble systems,": ""
        },
        {
          "Table 2: Configurations of models\nin the ensemble systems,": "3.2. Audio Features and Text Features"
        },
        {
          "Table 2: Configurations of models\nin the ensemble systems,": "The audio and text features used in this work are presented in"
        },
        {
          "Table 2: Configurations of models\nin the ensemble systems,": ""
        },
        {
          "Table 2: Configurations of models\nin the ensemble systems,": "Table 1. Three types of audio features are used:\nthe final\nlayer"
        },
        {
          "Table 2: Configurations of models\nin the ensemble systems,": "representations of WavLM-large [27];\nthe final\nlayer represen-"
        },
        {
          "Table 2: Configurations of models\nin the ensemble systems,": "tations of Hubert-extra-large [28]; and the final\nlayer output of"
        },
        {
          "Table 2: Configurations of models\nin the ensemble systems,": ""
        },
        {
          "Table 2: Configurations of models\nin the ensemble systems,": "the encoder of Whisper-large-v3 [14]. In terms of text features,"
        },
        {
          "Table 2: Configurations of models\nin the ensemble systems,": ""
        },
        {
          "Table 2: Configurations of models\nin the ensemble systems,": "this work utilises Roberta-large [29].\nIn order\nto enhance the"
        },
        {
          "Table 2: Configurations of models\nin the ensemble systems,": ""
        },
        {
          "Table 2: Configurations of models\nin the ensemble systems,": "representative capability of the text features, this work uses the"
        },
        {
          "Table 2: Configurations of models\nin the ensemble systems,": ""
        },
        {
          "Table 2: Configurations of models\nin the ensemble systems,": "average representations of the last 4 layers’ from Roberta-large."
        },
        {
          "Table 2: Configurations of models\nin the ensemble systems,": ""
        }
      ],
      "page": 3
    },
    {
      "caption": "Table 5: Comparison of the quality between the original and",
      "data": [
        {
          "Training\n68119\n1405\n110.2h": "Dev\n19815\n454\n31.7h",
          "53386\n1391\n86.3h": "15341\n446\n24.4h"
        },
        {
          "Training\n68119\n1405\n110.2h": "Test-3\n2437\n187\n3.9h",
          "53386\n1391\n86.3h": "-\n-\n-"
        },
        {
          "Training\n68119\n1405\n110.2h": "Table 3: The detailed information of subsets of the MSP-Podcast v1.11, where ‘All’ denotes subset statistics before the filtering and",
          "53386\n1391\n86.3h": ""
        },
        {
          "Training\n68119\n1405\n110.2h": "‘Used’ denotes subset statistics after the filtering.",
          "53386\n1391\n86.3h": ""
        },
        {
          "Training\n68119\n1405\n110.2h": "Class\n# Training Samples\n# Dev Samples",
          "53386\n1391\n86.3h": "between the MSP-Podcast dataset and the two AEC pretraining"
        },
        {
          "Training\n68119\n1405\n110.2h": "",
          "53386\n1391\n86.3h": "corpora (e.g., out-of-domain words), this model was fine-tuned"
        },
        {
          "Training\n68119\n1405\n110.2h": "Neutral\n25016\n5667",
          "53386\n1391\n86.3h": ""
        },
        {
          "Training\n68119\n1405\n110.2h": "",
          "53386\n1391\n86.3h": "to enhance its performance. Specifically, the model was trained"
        },
        {
          "Training\n68119\n1405\n110.2h": "Happy\n13440\n3340",
          "53386\n1391\n86.3h": ""
        },
        {
          "Training\n68119\n1405\n110.2h": "",
          "53386\n1391\n86.3h": "on the training set of the provided MSP-Podcast corpus for 10"
        },
        {
          "Training\n68119\n1405\n110.2h": "Angry\n3053\n2413",
          "53386\n1391\n86.3h": ""
        },
        {
          "Training\n68119\n1405\n110.2h": "",
          "53386\n1391\n86.3h": "epochs and then validated on the development\nset.\nThe best"
        },
        {
          "Training\n68119\n1405\n110.2h": "Sad\n3882\n1101",
          "53386\n1391\n86.3h": ""
        },
        {
          "Training\n68119\n1405\n110.2h": "",
          "53386\n1391\n86.3h": "checkpoint was saved to correct errors on the test set. The cor-"
        },
        {
          "Training\n68119\n1405\n110.2h": "Disgust\n1426\n486",
          "53386\n1391\n86.3h": ""
        },
        {
          "Training\n68119\n1405\n110.2h": "",
          "53386\n1391\n86.3h": "rection quality was evaluated using WER, BLEU, and GLEU"
        },
        {
          "Training\n68119\n1405\n110.2h": "Contempt\n2443\n1323",
          "53386\n1391\n86.3h": ""
        },
        {
          "Training\n68119\n1405\n110.2h": "",
          "53386\n1391\n86.3h": "scores for a comprehensive assessment. The results of the best"
        },
        {
          "Training\n68119\n1405\n110.2h": "Surprise\n2897\n729",
          "53386\n1391\n86.3h": ""
        },
        {
          "Training\n68119\n1405\n110.2h": "",
          "53386\n1391\n86.3h": "checkpoint on the development\nset are presented in Table 5,"
        },
        {
          "Training\n68119\n1405\n110.2h": "Fear\n1139\n282",
          "53386\n1391\n86.3h": ""
        },
        {
          "Training\n68119\n1405\n110.2h": "",
          "53386\n1391\n86.3h": "demonstrating the effectiveness of\nthe AEC model. As there"
        },
        {
          "Training\n68119\n1405\n110.2h": "",
          "53386\n1391\n86.3h": "is no ground-truth text for the test set, there is no way to further"
        },
        {
          "Training\n68119\n1405\n110.2h": "Table 4: Class distributions in the training set and the develop-",
          "53386\n1391\n86.3h": ""
        },
        {
          "Training\n68119\n1405\n110.2h": "",
          "53386\n1391\n86.3h": "evaluate its effectiveness. Given the same domain of the devel-"
        },
        {
          "Training\n68119\n1405\n110.2h": "ment set after the filtering.",
          "53386\n1391\n86.3h": ""
        },
        {
          "Training\n68119\n1405\n110.2h": "",
          "53386\n1391\n86.3h": "opment and test sets, it is expected an improvement in accuracy"
        },
        {
          "Training\n68119\n1405\n110.2h": "",
          "53386\n1391\n86.3h": "on the test set by approximately 1%, as suggested by previous"
        },
        {
          "Training\n68119\n1405\n110.2h": "",
          "53386\n1391\n86.3h": "research on the impact of WER on SER performance [39]."
        },
        {
          "Training\n68119\n1405\n110.2h": "has a hidden size of 512. The MLP modules after the concate-",
          "53386\n1391\n86.3h": ""
        },
        {
          "Training\n68119\n1405\n110.2h": "nation layer has a hidden size of 512 and the output size of the",
          "53386\n1391\n86.3h": ""
        },
        {
          "Training\n68119\n1405\n110.2h": "",
          "53386\n1391\n86.3h": "Transcription\nWER ↓\nBLEU ↑\nGLEU ↑"
        },
        {
          "Training\n68119\n1405\n110.2h": "final MLP module is 8.",
          "53386\n1391\n86.3h": ""
        },
        {
          "Training\n68119\n1405\n110.2h": "The models are trained with a batch size of 128, an initial",
          "53386\n1391\n86.3h": "Original\n17.65\n81.32\n78.02"
        },
        {
          "Training\n68119\n1405\n110.2h": "learning rate of 1e-4, with a learning scheduler [31]. The model",
          "53386\n1391\n86.3h": "Corrected\n14.51\n83.48\n81.19"
        },
        {
          "Training\n68119\n1405\n110.2h": "checkpoint of the epoch with the best Macro-F1 on the devel-",
          "53386\n1391\n86.3h": ""
        },
        {
          "Training\n68119\n1405\n110.2h": "opment set\nis chosen for evaluation. The implementations are",
          "53386\n1391\n86.3h": "Table 5: Comparison of\nthe quality between the original and"
        },
        {
          "Training\n68119\n1405\n110.2h": "based on the recipe of\nthe speechbrain toolkit\n[32]. All mod-",
          "53386\n1391\n86.3h": "corrected transcriptions on the development set. All values are"
        },
        {
          "Training\n68119\n1405\n110.2h": "els are implemented and trained with the PyTorch toolkit [33].",
          "53386\n1391\n86.3h": "presented in percentage scale (%).\n↓:\nthe lower the better.\n↑:"
        },
        {
          "Training\n68119\n1405\n110.2h": "Training takes about 2 hours and uses a maximum of 20 epochs.",
          "53386\n1391\n86.3h": "the higher the better."
        },
        {
          "Training\n68119\n1405\n110.2h": "4.2.2. Feature Fusion",
          "53386\n1391\n86.3h": ""
        },
        {
          "Training\n68119\n1405\n110.2h": "",
          "53386\n1391\n86.3h": "4.3. Evaluation Metrics"
        },
        {
          "Training\n68119\n1405\n110.2h": "A number of methods for feature fusion were considered. Due",
          "53386\n1391\n86.3h": ""
        },
        {
          "Training\n68119\n1405\n110.2h": "to limited time, however, only the following ones were imple-",
          "53386\n1391\n86.3h": ""
        },
        {
          "Training\n68119\n1405\n110.2h": "",
          "53386\n1391\n86.3h": "According to the challenge evaluation setup, Macro-F1 is used"
        },
        {
          "Training\n68119\n1405\n110.2h": "mented and compared.",
          "53386\n1391\n86.3h": ""
        },
        {
          "Training\n68119\n1405\n110.2h": "",
          "53386\n1391\n86.3h": "as the primary metric. Macro-F1 is the unweighted average of"
        },
        {
          "Training\n68119\n1405\n110.2h": "• Early fusion:\ntext and audio features are concatenated at\nthe",
          "53386\n1391\n86.3h": ""
        },
        {
          "Training\n68119\n1405\n110.2h": "",
          "53386\n1391\n86.3h": "the F1 score of each class. Apart from Macro-F1, the weighted"
        },
        {
          "Training\n68119\n1405\n110.2h": "embedding level.",
          "53386\n1391\n86.3h": ""
        },
        {
          "Training\n68119\n1405\n110.2h": "",
          "53386\n1391\n86.3h": "accuracy (WA) and unweighted accuracy (UA) are used. 2"
        },
        {
          "Training\n68119\n1405\n110.2h": "• Late fusion:\ntext and audio features are learned independently",
          "53386\n1391\n86.3h": ""
        },
        {
          "Training\n68119\n1405\n110.2h": "and the final decision is determined based on respective outputs.",
          "53386\n1391\n86.3h": ""
        },
        {
          "Training\n68119\n1405\n110.2h": "",
          "53386\n1391\n86.3h": "5. Results"
        },
        {
          "Training\n68119\n1405\n110.2h": "• Early fusion + late fusion.",
          "53386\n1391\n86.3h": ""
        },
        {
          "Training\n68119\n1405\n110.2h": "• Tensor fusion: unimodal information and bimodal interactions",
          "53386\n1391\n86.3h": "Results for the 7 models in the ensemble system are presented"
        },
        {
          "Training\n68119\n1405\n110.2h": "are learned explicitly and aggregated [34].",
          "53386\n1391\n86.3h": "in Table 6. Generally speaking, different configurations of au-"
        },
        {
          "Training\n68119\n1405\n110.2h": "• Low-rank tensor\nfusion: multimodal\nfusion with modality-",
          "53386\n1391\n86.3h": "dio features,\nloss functions and class weights yield differences"
        },
        {
          "Training\n68119\n1405\n110.2h": "specific low-rank factors, which scale linearly in the number",
          "53386\n1391\n86.3h": "in WA and UA results.\nFrom analysing the performance dif-"
        },
        {
          "Training\n68119\n1405\n110.2h": "of modalities [35].",
          "53386\n1391\n86.3h": "ference in WA and UA results,\nit would be easy to understand"
        },
        {
          "Training\n68119\n1405\n110.2h": "While sophisticated fusion approaches have often outper-",
          "53386\n1391\n86.3h": "how the models perform on the major classes and minor classes."
        },
        {
          "Training\n68119\n1405\n110.2h": "formed early fusion in various scenarios, this phenomenon was",
          "53386\n1391\n86.3h": "For convenience,\nin the following discussion,\nthe index of the"
        },
        {
          "Training\n68119\n1405\n110.2h": "not observed in the experiments. Therefore, early fusion was",
          "53386\n1391\n86.3h": "models in Table 6 are used to denote the models (e.g. model-1)."
        },
        {
          "Training\n68119\n1405\n110.2h": "used in the model, as it was found to outperform all other meth-",
          "53386\n1391\n86.3h": "Among the 7 models,\nthe best Macro-F1 was obtained by"
        },
        {
          "Training\n68119\n1405\n110.2h": "ods tested.",
          "53386\n1391\n86.3h": "the model-1, applying the focal\nloss (γ = 2) and prior-based"
        },
        {
          "Training\n68119\n1405\n110.2h": "",
          "53386\n1391\n86.3h": "class weights. Comparing model-1 to model-4, increasing γ ="
        },
        {
          "Training\n68119\n1405\n110.2h": "4.2.3. ASR Error Correction",
          "53386\n1391\n86.3h": "2 causes an improvement\nin WA but a drop in UA, causing a"
        },
        {
          "Training\n68119\n1405\n110.2h": "",
          "53386\n1391\n86.3h": "reduction in Macro-F1."
        },
        {
          "Training\n68119\n1405\n110.2h": "A pretrained AEC model was used in this work, which has been",
          "53386\n1391\n86.3h": ""
        },
        {
          "Training\n68119\n1405\n110.2h": "",
          "53386\n1391\n86.3h": "Among the 3 types of audio features for model-1, model-2"
        },
        {
          "Training\n68119\n1405\n110.2h": "trained on the English version of Common Voice 13.0 [36] and",
          "53386\n1391\n86.3h": ""
        },
        {
          "Training\n68119\n1405\n110.2h": "",
          "53386\n1391\n86.3h": "and model-3,\nit\nis clear\nthat\nthe Whisper\nfeatures yield better"
        },
        {
          "Training\n68119\n1405\n110.2h": "TED Talk corpus [37] using a publicly available Sequence-to-",
          "53386\n1391\n86.3h": ""
        },
        {
          "Training\n68119\n1405\n110.2h": "Sequence (S2S) encoder-decoder architecture [38]. This model",
          "53386\n1391\n86.3h": ""
        },
        {
          "Training\n68119\n1405\n110.2h": "",
          "53386\n1391\n86.3h": "2The\naccuracy score\nfunction\nand\nthe\nbalanced accuracy score"
        },
        {
          "Training\n68119\n1405\n110.2h": "was trained to convert ASR transcriptions to human-transcribed",
          "53386\n1391\n86.3h": "function from the scikit-learn toolkit are used for\nimplementing un-"
        },
        {
          "Training\n68119\n1405\n110.2h": "transcriptions (i.e., ground-truth text). Considering the disparity",
          "53386\n1391\n86.3h": "weighted accuracy and weighted accuracy, respectively"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Index": "1",
          "Audio": "Whisper",
          "Text": "Roberta",
          "Loss": "Focal (γ = 2)",
          "Class Weights": "Prior-based",
          "Macro-F1 (%)": "34.2",
          "WA (%)": "35.6",
          "UA (%)": "45.7"
        },
        {
          "Index": "2",
          "Audio": "WavLM",
          "Text": "Roberta",
          "Loss": "Focal (γ = 2)",
          "Class Weights": "Prior-based",
          "Macro-F1 (%)": "32.4",
          "WA (%)": "35.3",
          "UA (%)": "43.6"
        },
        {
          "Index": "3",
          "Audio": "Hubert",
          "Text": "Roberta",
          "Loss": "Focal (γ = 2)",
          "Class Weights": "Prior-based",
          "Macro-F1 (%)": "32.7",
          "WA (%)": "34.3",
          "UA (%)": "45.5"
        },
        {
          "Index": "4",
          "Audio": "Whisper",
          "Text": "Roberta",
          "Loss": "Focal (γ = 2.5)",
          "Class Weights": "Prior-based",
          "Macro-F1 (%)": "33.8",
          "WA (%)": "35.8",
          "UA (%)": "45.1"
        },
        {
          "Index": "5",
          "Audio": "Whisper",
          "Text": "Roberta",
          "Loss": "CE",
          "Class Weights": "Prior-based",
          "Macro-F1 (%)": "33.8",
          "WA (%)": "36.0",
          "UA (%)": "44.0"
        },
        {
          "Index": "6",
          "Audio": "Whisper",
          "Text": "Roberta",
          "Loss": "Focal (γ = 2)",
          "Class Weights": "Uniform",
          "Macro-F1 (%)": "33.3",
          "WA (%)": "32.9",
          "UA (%)": "51.9"
        },
        {
          "Index": "7",
          "Audio": "Whisper",
          "Text": "Roberta",
          "Loss": "CE",
          "Class Weights": "Uniform",
          "Macro-F1 (%)": "32.8",
          "WA (%)": "32.6",
          "UA (%)": "51.1"
        },
        {
          "Index": "Ensemble",
          "Audio": "-",
          "Text": "-",
          "Loss": "-",
          "Class Weights": "-",
          "Macro-F1 (%)": "35.6",
          "WA (%)": "36.6",
          "UA (%)": "49.3"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "6\nWhisper\nRoberta\nFocal (γ = 2)": "7\nWhisper\nRoberta\nCE",
          "51.9\nUniform\n33.3\n32.9": "Uniform\n32.8\n32.6\n51.1"
        },
        {
          "6\nWhisper\nRoberta\nFocal (γ = 2)": "Ensemble\n-\n-\n-",
          "51.9\nUniform\n33.3\n32.9": "35.6\n36.6\n-\n49.3"
        },
        {
          "6\nWhisper\nRoberta\nFocal (γ = 2)": "Table 6: Development set results of the 7 models,",
          "51.9\nUniform\n33.3\n32.9": "they are trained independently with different audio features,\ntext features,\nloss and"
        },
        {
          "6\nWhisper\nRoberta\nFocal (γ = 2)": "class weights.",
          "51.9\nUniform\n33.3\n32.9": ""
        },
        {
          "6\nWhisper\nRoberta\nFocal (γ = 2)": "results than both the WavLM features and the Hubert features.",
          "51.9\nUniform\n33.3\n32.9": "8. References"
        },
        {
          "6\nWhisper\nRoberta\nFocal (γ = 2)": "One possible reason is that the Whisper model were trained su-",
          "51.9\nUniform\n33.3\n32.9": ""
        },
        {
          "6\nWhisper\nRoberta\nFocal (γ = 2)": "",
          "51.9\nUniform\n33.3\n32.9": "[1] OpenAI, “Chatgpt,” 2022."
        },
        {
          "6\nWhisper\nRoberta\nFocal (γ = 2)": "pervisedly with text\ntranscriptions, while the other two models",
          "51.9\nUniform\n33.3\n32.9": ""
        },
        {
          "6\nWhisper\nRoberta\nFocal (γ = 2)": "were trained without supervision. These are also difference due",
          "51.9\nUniform\n33.3\n32.9": "[2] Moataz El Ayadi, Mohamed S Kamel, and Fakhri Karray,"
        },
        {
          "6\nWhisper\nRoberta\nFocal (γ = 2)": "to the model\nsizes and the amount of\ntraining data used bay",
          "51.9\nUniform\n33.3\n32.9": "“Survey on speech emotion recognition: Features, classi-"
        },
        {
          "6\nWhisper\nRoberta\nFocal (γ = 2)": "these pretrained models.",
          "51.9\nUniform\n33.3\n32.9": "fication schemes, and databases,” Pattern recognition, vol."
        },
        {
          "6\nWhisper\nRoberta\nFocal (γ = 2)": "The results for model-1, model-5, model-6 and model-7 can",
          "51.9\nUniform\n33.3\n32.9": "44, no. 3, pp. 572–587, 2011."
        },
        {
          "6\nWhisper\nRoberta\nFocal (γ = 2)": "help to understand the effect of the two loss functions and the",
          "51.9\nUniform\n33.3\n32.9": ""
        },
        {
          "6\nWhisper\nRoberta\nFocal (γ = 2)": "",
          "51.9\nUniform\n33.3\n32.9": "[3]\nSamaneh Madanian,\nTalen Chen,\nOlayinka Adeleye,"
        },
        {
          "6\nWhisper\nRoberta\nFocal (γ = 2)": "two class weights. Generally speaking, different loss functions",
          "51.9\nUniform\n33.3\n32.9": ""
        },
        {
          "6\nWhisper\nRoberta\nFocal (γ = 2)": "",
          "51.9\nUniform\n33.3\n32.9": "John Michael Templeton, Christian Poellabauer, Dave"
        },
        {
          "6\nWhisper\nRoberta\nFocal (γ = 2)": "and class weights yields models that have different preferences",
          "51.9\nUniform\n33.3\n32.9": ""
        },
        {
          "6\nWhisper\nRoberta\nFocal (γ = 2)": "",
          "51.9\nUniform\n33.3\n32.9": "Parry, and Sandra L Schneider, “Speech emotion recogni-"
        },
        {
          "6\nWhisper\nRoberta\nFocal (γ = 2)": "for major and minor classes.\nSpecifically, when training with",
          "51.9\nUniform\n33.3\n32.9": ""
        },
        {
          "6\nWhisper\nRoberta\nFocal (γ = 2)": "",
          "51.9\nUniform\n33.3\n32.9": "tion using machine learning—a systematic review,” Intel-"
        },
        {
          "6\nWhisper\nRoberta\nFocal (γ = 2)": "the uniform class weights, model-6 and the model-7 show good",
          "51.9\nUniform\n33.3\n32.9": ""
        },
        {
          "6\nWhisper\nRoberta\nFocal (γ = 2)": "",
          "51.9\nUniform\n33.3\n32.9": "ligent systems with applications, p. 200266, 2023."
        },
        {
          "6\nWhisper\nRoberta\nFocal (γ = 2)": "performance on UA but poor performance on WA. Comparing",
          "51.9\nUniform\n33.3\n32.9": ""
        },
        {
          "6\nWhisper\nRoberta\nFocal (γ = 2)": "",
          "51.9\nUniform\n33.3\n32.9": "[4] Carlos\nBusso, Murtaza\nBulut,\nChi-Chun\nLee,\nAbe"
        },
        {
          "6\nWhisper\nRoberta\nFocal (γ = 2)": "model-7 and model-5,\nthe prior-based class weights give more",
          "51.9\nUniform\n33.3\n32.9": ""
        },
        {
          "6\nWhisper\nRoberta\nFocal (γ = 2)": "",
          "51.9\nUniform\n33.3\n32.9": "Kazemzadeh, Emily Mower, Samuel Kim,\nJeannette N"
        },
        {
          "6\nWhisper\nRoberta\nFocal (γ = 2)": "attention to minor classes, causing a significant\nimprovement",
          "51.9\nUniform\n33.3\n32.9": ""
        },
        {
          "6\nWhisper\nRoberta\nFocal (γ = 2)": "",
          "51.9\nUniform\n33.3\n32.9": "Chang,\nSungbok\nLee,\nand\nShrikanth\nS Narayanan,"
        },
        {
          "6\nWhisper\nRoberta\nFocal (γ = 2)": "in WA but a large drop in UA. This means that\nthe prior-based",
          "51.9\nUniform\n33.3\n32.9": ""
        },
        {
          "6\nWhisper\nRoberta\nFocal (γ = 2)": "",
          "51.9\nUniform\n33.3\n32.9": "“IEMOCAP: Interactive emotional dyadic motion capture"
        },
        {
          "6\nWhisper\nRoberta\nFocal (γ = 2)": "class weights improve the performance on the minor classes,",
          "51.9\nUniform\n33.3\n32.9": ""
        },
        {
          "6\nWhisper\nRoberta\nFocal (γ = 2)": "",
          "51.9\nUniform\n33.3\n32.9": "database,”\nLanguage resources and evaluation, vol. 42,"
        },
        {
          "6\nWhisper\nRoberta\nFocal (γ = 2)": "but sacrificing the performance on the major classes. The per-",
          "51.9\nUniform\n33.3\n32.9": ""
        },
        {
          "6\nWhisper\nRoberta\nFocal (γ = 2)": "",
          "51.9\nUniform\n33.3\n32.9": "pp. 335–359, 2008."
        },
        {
          "6\nWhisper\nRoberta\nFocal (γ = 2)": "formance drop may be due to model-5 over-fitting on major",
          "51.9\nUniform\n33.3\n32.9": ""
        },
        {
          "6\nWhisper\nRoberta\nFocal (γ = 2)": "classes.\nThe effect of\nfocal\nloss can be found through com-",
          "51.9\nUniform\n33.3\n32.9": ""
        },
        {
          "6\nWhisper\nRoberta\nFocal (γ = 2)": "",
          "51.9\nUniform\n33.3\n32.9": "[5] Reza Lotfian and Carlos Busso,\n“Building naturalistic"
        },
        {
          "6\nWhisper\nRoberta\nFocal (γ = 2)": "paring model-1 and model-5, which shows that focal loss helps",
          "51.9\nUniform\n33.3\n32.9": ""
        },
        {
          "6\nWhisper\nRoberta\nFocal (γ = 2)": "",
          "51.9\nUniform\n33.3\n32.9": "emotionally balanced speech corpus by retrieving emo-"
        },
        {
          "6\nWhisper\nRoberta\nFocal (γ = 2)": "model-1 reach a better balance between the major classes and",
          "51.9\nUniform\n33.3\n32.9": "IEEE"
        },
        {
          "6\nWhisper\nRoberta\nFocal (γ = 2)": "",
          "51.9\nUniform\n33.3\n32.9": "tional\nspeech from existing podcast\nrecordings,”"
        },
        {
          "6\nWhisper\nRoberta\nFocal (γ = 2)": "the minor classes. Hence, the model-1 reaches the highest over-",
          "51.9\nUniform\n33.3\n32.9": ""
        },
        {
          "6\nWhisper\nRoberta\nFocal (γ = 2)": "",
          "51.9\nUniform\n33.3\n32.9": "Transactions on Affective Computing, vol. 10, no. 4, pp."
        },
        {
          "6\nWhisper\nRoberta\nFocal (γ = 2)": "all macro-F1 performance.",
          "51.9\nUniform\n33.3\n32.9": ""
        },
        {
          "6\nWhisper\nRoberta\nFocal (γ = 2)": "",
          "51.9\nUniform\n33.3\n32.9": "471–483, 2017."
        },
        {
          "6\nWhisper\nRoberta\nFocal (γ = 2)": "Based on the diverse performance of the models, an obvious",
          "51.9\nUniform\n33.3\n32.9": ""
        },
        {
          "6\nWhisper\nRoberta\nFocal (γ = 2)": "",
          "51.9\nUniform\n33.3\n32.9": "[6]\nSanaul Haq and Philip JB Jackson, “Multimodal emotion"
        },
        {
          "6\nWhisper\nRoberta\nFocal (γ = 2)": "strategy to build an ensemble is to combine the outputs of these",
          "51.9\nUniform\n33.3\n32.9": ""
        },
        {
          "6\nWhisper\nRoberta\nFocal (γ = 2)": "",
          "51.9\nUniform\n33.3\n32.9": "recognition,” in Machine audition: principles, algorithms"
        },
        {
          "6\nWhisper\nRoberta\nFocal (γ = 2)": "models through a majority voting process.\nThe results show",
          "51.9\nUniform\n33.3\n32.9": ""
        },
        {
          "6\nWhisper\nRoberta\nFocal (γ = 2)": "",
          "51.9\nUniform\n33.3\n32.9": "and systems, pp. 398–423. IGI global, 2011."
        },
        {
          "6\nWhisper\nRoberta\nFocal (γ = 2)": "that\nthe ensemble system outperforms all of\nthe 7 models on",
          "51.9\nUniform\n33.3\n32.9": ""
        },
        {
          "6\nWhisper\nRoberta\nFocal (γ = 2)": "Macro-F1 and WA, reaching the state-of-the-art performance.",
          "51.9\nUniform\n33.3\n32.9": "[7] Rosanna Milner, Md Asif Jalal, Raymond WM Ng, and"
        },
        {
          "6\nWhisper\nRoberta\nFocal (γ = 2)": "",
          "51.9\nUniform\n33.3\n32.9": "Thomas Hain,\n“A cross-corpus study on speech emotion"
        },
        {
          "6\nWhisper\nRoberta\nFocal (γ = 2)": "",
          "51.9\nUniform\n33.3\n32.9": "in Proc. of ASRU 2019."
        },
        {
          "6\nWhisper\nRoberta\nFocal (γ = 2)": "6. Conclusions",
          "51.9\nUniform\n33.3\n32.9": "recognition,”\nIEEE, 2019, pp."
        },
        {
          "6\nWhisper\nRoberta\nFocal (γ = 2)": "",
          "51.9\nUniform\n33.3\n32.9": "304–311."
        },
        {
          "6\nWhisper\nRoberta\nFocal (γ = 2)": "This paper introduces an ensemble system that includes 7 multi-",
          "51.9\nUniform\n33.3\n32.9": ""
        },
        {
          "6\nWhisper\nRoberta\nFocal (γ = 2)": "",
          "51.9\nUniform\n33.3\n32.9": "[8] Ziyang Ma, Zhisheng Zheng, Jiaxin Ye, Jinchao Li, Zhifu"
        },
        {
          "6\nWhisper\nRoberta\nFocal (γ = 2)": "modal models,\nconstructed for participation in the Odyssey",
          "51.9\nUniform\n33.3\n32.9": ""
        },
        {
          "6\nWhisper\nRoberta\nFocal (γ = 2)": "",
          "51.9\nUniform\n33.3\n32.9": "Gao, Shiliang Zhang,\nand Xie Chen,\n“Emotion2vec:"
        },
        {
          "6\nWhisper\nRoberta\nFocal (γ = 2)": "2024 Emotion Recognition Challenge. The system showed the",
          "51.9\nUniform\n33.3\n32.9": ""
        },
        {
          "6\nWhisper\nRoberta\nFocal (γ = 2)": "",
          "51.9\nUniform\n33.3\n32.9": "Self-supervised pre-training for speech emotion represen-"
        },
        {
          "6\nWhisper\nRoberta\nFocal (γ = 2)": "best performance among a total of 68 submissions to the chal-",
          "51.9\nUniform\n33.3\n32.9": ""
        },
        {
          "6\nWhisper\nRoberta\nFocal (γ = 2)": "",
          "51.9\nUniform\n33.3\n32.9": "tation,” arXiv preprint arXiv:2312.15185, 2023."
        },
        {
          "6\nWhisper\nRoberta\nFocal (γ = 2)": "lenge,\nin all metrics under consideration. The 7 models were",
          "51.9\nUniform\n33.3\n32.9": ""
        },
        {
          "6\nWhisper\nRoberta\nFocal (γ = 2)": "trained independently with different\nloss\nfunctions and class",
          "51.9\nUniform\n33.3\n32.9": "´"
        },
        {
          "6\nWhisper\nRoberta\nFocal (γ = 2)": "",
          "51.9\nUniform\n33.3\n32.9": "[9] Luc´ıa G´omez-Zaragoz´a,\nOscar Valls, Roc´ıo del Amor,"
        },
        {
          "6\nWhisper\nRoberta\nFocal (γ = 2)": "weights. Specifically,\nthe cross entropy loss and the focal\nloss",
          "51.9\nUniform\n33.3\n32.9": "Mar´ıa\nJos´e\nCastro-Bleda,\nValery\nNaranjo,\nMari-"
        },
        {
          "6\nWhisper\nRoberta\nFocal (γ = 2)": "were used. Uniform class weights and prior-based class weights",
          "51.9\nUniform\n33.3\n32.9": "ano Alca˜niz Raya, and Javier Mar´ın-Morales,\n“Speech"
        },
        {
          "6\nWhisper\nRoberta\nFocal (γ = 2)": "are studied. The experiment results show that the combinations",
          "51.9\nUniform\n33.3\n32.9": "emotion recognition from voice messages recorded in the"
        },
        {
          "6\nWhisper\nRoberta\nFocal (γ = 2)": "of loss functions and class weights lead to different preference",
          "51.9\nUniform\n33.3\n32.9": "wild,” arXiv preprint arXiv:2403.02167, 2024."
        },
        {
          "6\nWhisper\nRoberta\nFocal (γ = 2)": "on the major classes or the minor classes",
          "51.9\nUniform\n33.3\n32.9": ""
        },
        {
          "6\nWhisper\nRoberta\nFocal (γ = 2)": "",
          "51.9\nUniform\n33.3\n32.9": "[10] Tao Meng, Yuntao Shou, Wei Ai, Nan Yin,\nand Ke-"
        },
        {
          "6\nWhisper\nRoberta\nFocal (γ = 2)": "",
          "51.9\nUniform\n33.3\n32.9": "qin Li,\n“Deep\nimbalanced\nlearning\nfor multimodal"
        },
        {
          "6\nWhisper\nRoberta\nFocal (γ = 2)": "7. Acknowledgements",
          "51.9\nUniform\n33.3\n32.9": ""
        },
        {
          "6\nWhisper\nRoberta\nFocal (γ = 2)": "",
          "51.9\nUniform\n33.3\n32.9": "arXiv preprint\nemotion recognition in conversations,”"
        },
        {
          "6\nWhisper\nRoberta\nFocal (γ = 2)": "",
          "51.9\nUniform\n33.3\n32.9": "arXiv:2312.06337, 2023."
        },
        {
          "6\nWhisper\nRoberta\nFocal (γ = 2)": "This work was primarily conducted at the Voicebase/Liveperson",
          "51.9\nUniform\n33.3\n32.9": ""
        },
        {
          "6\nWhisper\nRoberta\nFocal (γ = 2)": "Centre of Speech and Language Technology at the University of",
          "51.9\nUniform\n33.3\n32.9": ""
        },
        {
          "6\nWhisper\nRoberta\nFocal (γ = 2)": "",
          "51.9\nUniform\n33.3\n32.9": "[11] Ziyang Ma, Wen Wu, Zhisheng Zheng, Yiwei Guo, Qian"
        },
        {
          "6\nWhisper\nRoberta\nFocal (γ = 2)": "Sheffield which is supported by Liveperson, Inc..",
          "51.9\nUniform\n33.3\n32.9": ""
        },
        {
          "6\nWhisper\nRoberta\nFocal (γ = 2)": "",
          "51.9\nUniform\n33.3\n32.9": "Chen, Shiliang Zhang, and Xie Chen, “Leveraging speech"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "PTM,\ntext LLM, and emotional TTS for speech emotion": "recognition,” Proc. ICASSP, 2024.",
          "[25] Yuanchao Li, Pinzhen Chen, Peter Bell,\nand Catherine": "Lai, “Crossmodal asr error correction with discrete speech"
        },
        {
          "PTM,\ntext LLM, and emotional TTS for speech emotion": "",
          "[25] Yuanchao Li, Pinzhen Chen, Peter Bell,\nand Catherine": "units,” arXiv preprint arXiv:2405.16677, 2024."
        },
        {
          "PTM,\ntext LLM, and emotional TTS for speech emotion": "[12] Hemin\nIbrahim, Chu Kiong Loo,\nand Fady Alnajjar,",
          "[25] Yuanchao Li, Pinzhen Chen, Peter Bell,\nand Catherine": ""
        },
        {
          "PTM,\ntext LLM, and emotional TTS for speech emotion": "“Bidirectional parallel echo state network for speech emo-",
          "[25] Yuanchao Li, Pinzhen Chen, Peter Bell,\nand Catherine": "[26] Ashish Vaswani, Noam Shazeer, Niki\nParmar,\nJakob"
        },
        {
          "PTM,\ntext LLM, and emotional TTS for speech emotion": "tion recognition,”\nNeural Computing and Applications,",
          "[25] Yuanchao Li, Pinzhen Chen, Peter Bell,\nand Catherine": "Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser,"
        },
        {
          "PTM,\ntext LLM, and emotional TTS for speech emotion": "vol. 34, no. 20, pp. 17581–17599, 2022.",
          "[25] Yuanchao Li, Pinzhen Chen, Peter Bell,\nand Catherine": "and Illia Polosukhin, “Attention is all you need,” Proc. of"
        },
        {
          "PTM,\ntext LLM, and emotional TTS for speech emotion": "",
          "[25] Yuanchao Li, Pinzhen Chen, Peter Bell,\nand Catherine": "NIPS 2017, vol. 30, 2017."
        },
        {
          "PTM,\ntext LLM, and emotional TTS for speech emotion": "[13]\nSungrack Yun\nand Chang D Yoo,\n“Speech\nemotion",
          "[25] Yuanchao Li, Pinzhen Chen, Peter Bell,\nand Catherine": ""
        },
        {
          "PTM,\ntext LLM, and emotional TTS for speech emotion": "",
          "[25] Yuanchao Li, Pinzhen Chen, Peter Bell,\nand Catherine": "[27]\nSanyuan Chen, Chengyi Wang, Zhengyang Chen, Yu Wu,"
        },
        {
          "PTM,\ntext LLM, and emotional TTS for speech emotion": "recognition via\na max-margin framework incorporating",
          "[25] Yuanchao Li, Pinzhen Chen, Peter Bell,\nand Catherine": ""
        },
        {
          "PTM,\ntext LLM, and emotional TTS for speech emotion": "",
          "[25] Yuanchao Li, Pinzhen Chen, Peter Bell,\nand Catherine": "Shujie Liu, Zhuo Chen, Jinyu Li, Naoyuki Kanda, Takuya"
        },
        {
          "PTM,\ntext LLM, and emotional TTS for speech emotion": "a loss function based on the watson and tellegen’s emo-",
          "[25] Yuanchao Li, Pinzhen Chen, Peter Bell,\nand Catherine": ""
        },
        {
          "PTM,\ntext LLM, and emotional TTS for speech emotion": "",
          "[25] Yuanchao Li, Pinzhen Chen, Peter Bell,\nand Catherine": "Yoshioka, Xiong Xiao, et al., “WavLM: Large-scale self-"
        },
        {
          "PTM,\ntext LLM, and emotional TTS for speech emotion": "tion model,”\nin 2009 IEEE International Conference on",
          "[25] Yuanchao Li, Pinzhen Chen, Peter Bell,\nand Catherine": ""
        },
        {
          "PTM,\ntext LLM, and emotional TTS for speech emotion": "",
          "[25] Yuanchao Li, Pinzhen Chen, Peter Bell,\nand Catherine": "supervised pre-training for full stack speech processing,”"
        },
        {
          "PTM,\ntext LLM, and emotional TTS for speech emotion": "Acoustics, Speech and Signal Processing. IEEE, 2009, pp.",
          "[25] Yuanchao Li, Pinzhen Chen, Peter Bell,\nand Catherine": ""
        },
        {
          "PTM,\ntext LLM, and emotional TTS for speech emotion": "",
          "[25] Yuanchao Li, Pinzhen Chen, Peter Bell,\nand Catherine": "IEEE Journal of Selected Topics in Signal Processing, vol."
        },
        {
          "PTM,\ntext LLM, and emotional TTS for speech emotion": "4169–4172.",
          "[25] Yuanchao Li, Pinzhen Chen, Peter Bell,\nand Catherine": ""
        },
        {
          "PTM,\ntext LLM, and emotional TTS for speech emotion": "",
          "[25] Yuanchao Li, Pinzhen Chen, Peter Bell,\nand Catherine": "16, no. 6, pp. 1505–1518, 2022."
        },
        {
          "PTM,\ntext LLM, and emotional TTS for speech emotion": "[14] Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman,",
          "[25] Yuanchao Li, Pinzhen Chen, Peter Bell,\nand Catherine": ""
        },
        {
          "PTM,\ntext LLM, and emotional TTS for speech emotion": "",
          "[25] Yuanchao Li, Pinzhen Chen, Peter Bell,\nand Catherine": "[28] Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai,"
        },
        {
          "PTM,\ntext LLM, and emotional TTS for speech emotion": "Christine McLeavey, and Ilya Sutskever,\n“Robust speech",
          "[25] Yuanchao Li, Pinzhen Chen, Peter Bell,\nand Catherine": ""
        },
        {
          "PTM,\ntext LLM, and emotional TTS for speech emotion": "",
          "[25] Yuanchao Li, Pinzhen Chen, Peter Bell,\nand Catherine": "Kushal Lakhotia, Ruslan Salakhutdinov,\nand Abdelrah-"
        },
        {
          "PTM,\ntext LLM, and emotional TTS for speech emotion": "recognition via large-scale weak supervision,” in Proc. of",
          "[25] Yuanchao Li, Pinzhen Chen, Peter Bell,\nand Catherine": ""
        },
        {
          "PTM,\ntext LLM, and emotional TTS for speech emotion": "",
          "[25] Yuanchao Li, Pinzhen Chen, Peter Bell,\nand Catherine": "man Mohamed,\n“Hubert: Self-supervised speech repre-"
        },
        {
          "PTM,\ntext LLM, and emotional TTS for speech emotion": "ICML. PMLR, 2023, pp. 28492–28518.",
          "[25] Yuanchao Li, Pinzhen Chen, Peter Bell,\nand Catherine": ""
        },
        {
          "PTM,\ntext LLM, and emotional TTS for speech emotion": "",
          "[25] Yuanchao Li, Pinzhen Chen, Peter Bell,\nand Catherine": "sentation learning by masked prediction of hidden units,”"
        },
        {
          "PTM,\ntext LLM, and emotional TTS for speech emotion": "[15] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He,",
          "[25] Yuanchao Li, Pinzhen Chen, Peter Bell,\nand Catherine": ""
        },
        {
          "PTM,\ntext LLM, and emotional TTS for speech emotion": "",
          "[25] Yuanchao Li, Pinzhen Chen, Peter Bell,\nand Catherine": "IEEE/ACM Transactions on Audio, Speech, and Language"
        },
        {
          "PTM,\ntext LLM, and emotional TTS for speech emotion": "and Piotr Doll´ar,\n“Focal\nloss for dense object detection,”",
          "[25] Yuanchao Li, Pinzhen Chen, Peter Bell,\nand Catherine": ""
        },
        {
          "PTM,\ntext LLM, and emotional TTS for speech emotion": "",
          "[25] Yuanchao Li, Pinzhen Chen, Peter Bell,\nand Catherine": "Processing, vol. 29, pp. 3451–3460, 2021."
        },
        {
          "PTM,\ntext LLM, and emotional TTS for speech emotion": "in Proc. of ICCV, 2017, pp. 2980–2988.",
          "[25] Yuanchao Li, Pinzhen Chen, Peter Bell,\nand Catherine": ""
        },
        {
          "PTM,\ntext LLM, and emotional TTS for speech emotion": "",
          "[25] Yuanchao Li, Pinzhen Chen, Peter Bell,\nand Catherine": "[29] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-"
        },
        {
          "PTM,\ntext LLM, and emotional TTS for speech emotion": "[16] L. Goncalves, A. N. Salman, A. Reddy Naini, L. Moro-",
          "[25] Yuanchao Li, Pinzhen Chen, Peter Bell,\nand Catherine": ""
        },
        {
          "PTM,\ntext LLM, and emotional TTS for speech emotion": "",
          "[25] Yuanchao Li, Pinzhen Chen, Peter Bell,\nand Catherine": "dar\nJoshi, Danqi Chen, Omer Levy, Mike Lewis, Luke"
        },
        {
          "PTM,\ntext LLM, and emotional TTS for speech emotion": "Velazquez, T. Thebaud, L.P. Garcia, N. Dehak, B. Sisman,",
          "[25] Yuanchao Li, Pinzhen Chen, Peter Bell,\nand Catherine": ""
        },
        {
          "PTM,\ntext LLM, and emotional TTS for speech emotion": "",
          "[25] Yuanchao Li, Pinzhen Chen, Peter Bell,\nand Catherine": "Zettlemoyer, and Veselin Stoyanov, “Roberta: A robustly"
        },
        {
          "PTM,\ntext LLM, and emotional TTS for speech emotion": "and C. Busso,\n“Odyssey2024 - speech emotion recogni-",
          "[25] Yuanchao Li, Pinzhen Chen, Peter Bell,\nand Catherine": ""
        },
        {
          "PTM,\ntext LLM, and emotional TTS for speech emotion": "",
          "[25] Yuanchao Li, Pinzhen Chen, Peter Bell,\nand Catherine": "arXiv\npreprint\noptimized\nbert\npretraining\napproach,”"
        },
        {
          "PTM,\ntext LLM, and emotional TTS for speech emotion": "tion challenge: Dataset, baseline framework, and results,”",
          "[25] Yuanchao Li, Pinzhen Chen, Peter Bell,\nand Catherine": ""
        },
        {
          "PTM,\ntext LLM, and emotional TTS for speech emotion": "",
          "[25] Yuanchao Li, Pinzhen Chen, Peter Bell,\nand Catherine": "arXiv:1907.11692, 2019."
        },
        {
          "PTM,\ntext LLM, and emotional TTS for speech emotion": "in Odyssey 2024: The Speaker and Language Recognition",
          "[25] Yuanchao Li, Pinzhen Chen, Peter Bell,\nand Catherine": ""
        },
        {
          "PTM,\ntext LLM, and emotional TTS for speech emotion": "",
          "[25] Yuanchao Li, Pinzhen Chen, Peter Bell,\nand Catherine": "[30] HuggingFace, “Transformers,” 2016."
        },
        {
          "PTM,\ntext LLM, and emotional TTS for speech emotion": "Workshop), Quebec, Canada, June 2024, vol. To appear.",
          "[25] Yuanchao Li, Pinzhen Chen, Peter Bell,\nand Catherine": ""
        },
        {
          "PTM,\ntext LLM, and emotional TTS for speech emotion": "",
          "[25] Yuanchao Li, Pinzhen Chen, Peter Bell,\nand Catherine": "[31] Hang Su and Haoyu Chen, “Experiments on parallel train-"
        },
        {
          "PTM,\ntext LLM, and emotional TTS for speech emotion": "[17] Devamanyu Hazarika, Roger Zimmermann, and Soujanya",
          "[25] Yuanchao Li, Pinzhen Chen, Peter Bell,\nand Catherine": ""
        },
        {
          "PTM,\ntext LLM, and emotional TTS for speech emotion": "",
          "[25] Yuanchao Li, Pinzhen Chen, Peter Bell,\nand Catherine": "ing of deep neural network using model averaging,” arXiv"
        },
        {
          "PTM,\ntext LLM, and emotional TTS for speech emotion": "Poria,\n“MISA: Modality-invariant and-specific represen-",
          "[25] Yuanchao Li, Pinzhen Chen, Peter Bell,\nand Catherine": ""
        },
        {
          "PTM,\ntext LLM, and emotional TTS for speech emotion": "",
          "[25] Yuanchao Li, Pinzhen Chen, Peter Bell,\nand Catherine": "preprint arXiv:1507.01239, 2015."
        },
        {
          "PTM,\ntext LLM, and emotional TTS for speech emotion": "tations for multimodal sentiment analysis,”\nin Proceed-",
          "[25] Yuanchao Li, Pinzhen Chen, Peter Bell,\nand Catherine": ""
        },
        {
          "PTM,\ntext LLM, and emotional TTS for speech emotion": "",
          "[25] Yuanchao Li, Pinzhen Chen, Peter Bell,\nand Catherine": "[32] Mirco Ravanelli, Titouan Parcollet, Peter Plantinga, Aku"
        },
        {
          "PTM,\ntext LLM, and emotional TTS for speech emotion": "ings of\nthe 28th ACM international conference on multi-",
          "[25] Yuanchao Li, Pinzhen Chen, Peter Bell,\nand Catherine": ""
        },
        {
          "PTM,\ntext LLM, and emotional TTS for speech emotion": "",
          "[25] Yuanchao Li, Pinzhen Chen, Peter Bell,\nand Catherine": "Rouhe,\nSamuele Cornell,\nLoren Lugosch, Cem Sub-"
        },
        {
          "PTM,\ntext LLM, and emotional TTS for speech emotion": "media, 2020, pp. 1122–1131.",
          "[25] Yuanchao Li, Pinzhen Chen, Peter Bell,\nand Catherine": ""
        },
        {
          "PTM,\ntext LLM, and emotional TTS for speech emotion": "",
          "[25] Yuanchao Li, Pinzhen Chen, Peter Bell,\nand Catherine": "akan, Nauman Dawalatabad, Abdelwahab Heba, Jianyuan"
        },
        {
          "PTM,\ntext LLM, and emotional TTS for speech emotion": "[18]\nJiachen Luo, Huy Phan, Lin Wang,\nand Joshua Reiss,",
          "[25] Yuanchao Li, Pinzhen Chen, Peter Bell,\nand Catherine": ""
        },
        {
          "PTM,\ntext LLM, and emotional TTS for speech emotion": "",
          "[25] Yuanchao Li, Pinzhen Chen, Peter Bell,\nand Catherine": "Zhong, et al.,\n“Speechbrain: A general-purpose speech"
        },
        {
          "PTM,\ntext LLM, and emotional TTS for speech emotion": "“Mutual\ncross-attention\nin\ndyadic\nfusion\nnetworks\nfor",
          "[25] Yuanchao Li, Pinzhen Chen, Peter Bell,\nand Catherine": ""
        },
        {
          "PTM,\ntext LLM, and emotional TTS for speech emotion": "",
          "[25] Yuanchao Li, Pinzhen Chen, Peter Bell,\nand Catherine": "toolkit,” arXiv preprint arXiv:2106.04624, 2021."
        },
        {
          "PTM,\ntext LLM, and emotional TTS for speech emotion": "audio-video emotion recognition,”\nin Proc. of ACIIW",
          "[25] Yuanchao Li, Pinzhen Chen, Peter Bell,\nand Catherine": ""
        },
        {
          "PTM,\ntext LLM, and emotional TTS for speech emotion": "",
          "[25] Yuanchao Li, Pinzhen Chen, Peter Bell,\nand Catherine": "[33] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer,"
        },
        {
          "PTM,\ntext LLM, and emotional TTS for speech emotion": "2023. IEEE, 2023, pp. 1–7.",
          "[25] Yuanchao Li, Pinzhen Chen, Peter Bell,\nand Catherine": ""
        },
        {
          "PTM,\ntext LLM, and emotional TTS for speech emotion": "",
          "[25] Yuanchao Li, Pinzhen Chen, Peter Bell,\nand Catherine": "James Bradbury, Gregory Chanan, Trevor Killeen, Zem-"
        },
        {
          "PTM,\ntext LLM, and emotional TTS for speech emotion": "[19] Wen Wu, Chao Zhang, and Philip C Woodland, “Emotion",
          "[25] Yuanchao Li, Pinzhen Chen, Peter Bell,\nand Catherine": "ing Lin, Natalia Gimelshein, Luca Antiga, et al.,\n“Py-"
        },
        {
          "PTM,\ntext LLM, and emotional TTS for speech emotion": "recognition by fusing time synchronous and time asyn-",
          "[25] Yuanchao Li, Pinzhen Chen, Peter Bell,\nand Catherine": "torch: An imperative style, high-performance deep learn-"
        },
        {
          "PTM,\ntext LLM, and emotional TTS for speech emotion": "chronous\nrepresentations,”\nin Proc. of\nICASSP 2021.",
          "[25] Yuanchao Li, Pinzhen Chen, Peter Bell,\nand Catherine": "Advances in neural\ninformation processing\ning library,”"
        },
        {
          "PTM,\ntext LLM, and emotional TTS for speech emotion": "IEEE, 2021, pp. 6269–6273.",
          "[25] Yuanchao Li, Pinzhen Chen, Peter Bell,\nand Catherine": "systems, vol. 32, 2019."
        },
        {
          "PTM,\ntext LLM, and emotional TTS for speech emotion": "[20] Yuanchao Li, Peter Bell, and Catherine Lai, “Fusing ASR",
          "[25] Yuanchao Li, Pinzhen Chen, Peter Bell,\nand Catherine": "[34] Amir Zadeh, Minghai Chen, Soujanya Poria, Erik Cam-"
        },
        {
          "PTM,\ntext LLM, and emotional TTS for speech emotion": "outputs in joint\ntraining for speech emotion recognition,”",
          "[25] Yuanchao Li, Pinzhen Chen, Peter Bell,\nand Catherine": "bria, and Louis-Philippe Morency,\n“Tensor\nfusion net-"
        },
        {
          "PTM,\ntext LLM, and emotional TTS for speech emotion": "in Proc. of ICASSP 2022. IEEE, 2022, pp. 7362–7366.",
          "[25] Yuanchao Li, Pinzhen Chen, Peter Bell,\nand Catherine": "work for multimodal\nsentiment\nanalysis,”\nin Proc. of"
        },
        {
          "PTM,\ntext LLM, and emotional TTS for speech emotion": "",
          "[25] Yuanchao Li, Pinzhen Chen, Peter Bell,\nand Catherine": "EMNLP 2017, 2017, pp. 1103–1114."
        },
        {
          "PTM,\ntext LLM, and emotional TTS for speech emotion": "[21] Tomohiro Tanaka, Ryo Masumura, Hirokazu Masataki,",
          "[25] Yuanchao Li, Pinzhen Chen, Peter Bell,\nand Catherine": ""
        },
        {
          "PTM,\ntext LLM, and emotional TTS for speech emotion": "and Yushi Aono,\n“Neural\nerror\ncorrective\nlanguage",
          "[25] Yuanchao Li, Pinzhen Chen, Peter Bell,\nand Catherine": "[35] Zhun Liu and Ying Shen, “Efficient low-rank multimodal"
        },
        {
          "PTM,\ntext LLM, and emotional TTS for speech emotion": "models\nfor automatic speech recognition.,”\nin INTER-",
          "[25] Yuanchao Li, Pinzhen Chen, Peter Bell,\nand Catherine": "fusion with modality-specific factors,”\nin Proc. of ACL"
        },
        {
          "PTM,\ntext LLM, and emotional TTS for speech emotion": "SPEECH, 2018, pp. 401–405.",
          "[25] Yuanchao Li, Pinzhen Chen, Peter Bell,\nand Catherine": "2018, 2018."
        },
        {
          "PTM,\ntext LLM, and emotional TTS for speech emotion": "",
          "[25] Yuanchao Li, Pinzhen Chen, Peter Bell,\nand Catherine": "[36] Rosana Ardila, Megan Branson, Kelly Davis, Michael"
        },
        {
          "PTM,\ntext LLM, and emotional TTS for speech emotion": "[22] Hirofumi Inaguma, Masato Mimura, Shinsuke Sakai, and",
          "[25] Yuanchao Li, Pinzhen Chen, Peter Bell,\nand Catherine": ""
        },
        {
          "PTM,\ntext LLM, and emotional TTS for speech emotion": "",
          "[25] Yuanchao Li, Pinzhen Chen, Peter Bell,\nand Catherine": "Henretty, Michael Kohler,\nJosh Meyer, Reuben Morais,"
        },
        {
          "PTM,\ntext LLM, and emotional TTS for speech emotion": "Tatsuya Kawahara,\n“Improving OOV detection and reso-",
          "[25] Yuanchao Li, Pinzhen Chen, Peter Bell,\nand Catherine": ""
        },
        {
          "PTM,\ntext LLM, and emotional TTS for speech emotion": "",
          "[25] Yuanchao Li, Pinzhen Chen, Peter Bell,\nand Catherine": "Lindsay Saunders, Francis M Tyers, and Gregor Weber,"
        },
        {
          "PTM,\ntext LLM, and emotional TTS for speech emotion": "lution with external\nlanguage models in acoustic-to-word",
          "[25] Yuanchao Li, Pinzhen Chen, Peter Bell,\nand Catherine": ""
        },
        {
          "PTM,\ntext LLM, and emotional TTS for speech emotion": "",
          "[25] Yuanchao Li, Pinzhen Chen, Peter Bell,\nand Catherine": "“Common voice: A massively-multilingual\nspeech cor-"
        },
        {
          "PTM,\ntext LLM, and emotional TTS for speech emotion": "ASR,” in Proc. of SLT 2018. IEEE, 2018, pp. 212–218.",
          "[25] Yuanchao Li, Pinzhen Chen, Peter Bell,\nand Catherine": ""
        },
        {
          "PTM,\ntext LLM, and emotional TTS for speech emotion": "",
          "[25] Yuanchao Li, Pinzhen Chen, Peter Bell,\nand Catherine": "pus,” arXiv preprint arXiv:1912.06670, 2019."
        },
        {
          "PTM,\ntext LLM, and emotional TTS for speech emotion": "[23]\nJunwei Liao, Sefik Eskimez, Liyang Lu, Yu Shi, Ming",
          "[25] Yuanchao Li, Pinzhen Chen, Peter Bell,\nand Catherine": ""
        },
        {
          "PTM,\ntext LLM, and emotional TTS for speech emotion": "",
          "[25] Yuanchao Li, Pinzhen Chen, Peter Bell,\nand Catherine": "[37] Mauro Cettolo, Christian Girardi, and Marcello Federico,"
        },
        {
          "PTM,\ntext LLM, and emotional TTS for speech emotion": "Gong, Linjun Shou, Hong Qu, and Michael Zeng,\n“Im-",
          "[25] Yuanchao Li, Pinzhen Chen, Peter Bell,\nand Catherine": ""
        },
        {
          "PTM,\ntext LLM, and emotional TTS for speech emotion": "",
          "[25] Yuanchao Li, Pinzhen Chen, Peter Bell,\nand Catherine": "“Wit3: Web inventory of transcribed and translated talks,”"
        },
        {
          "PTM,\ntext LLM, and emotional TTS for speech emotion": "proving readability for automatic speech recognition tran-",
          "[25] Yuanchao Li, Pinzhen Chen, Peter Bell,\nand Catherine": ""
        },
        {
          "PTM,\ntext LLM, and emotional TTS for speech emotion": "",
          "[25] Yuanchao Li, Pinzhen Chen, Peter Bell,\nand Catherine": "in Proc. of EAMT 2012, 2012, pp. 261–268."
        },
        {
          "PTM,\ntext LLM, and emotional TTS for speech emotion": "scription,” ACM Transactions on Asian and Low-Resource",
          "[25] Yuanchao Li, Pinzhen Chen, Peter Bell,\nand Catherine": ""
        },
        {
          "PTM,\ntext LLM, and emotional TTS for speech emotion": "Language Information Processing, vol. 22, no. 5, pp. 1–",
          "[25] Yuanchao Li, Pinzhen Chen, Peter Bell,\nand Catherine": "[38]\nPinzhen Chen and Gerasimos Lampouras,\n“Exploring"
        },
        {
          "PTM,\ntext LLM, and emotional TTS for speech emotion": "23, 2023.",
          "[25] Yuanchao Li, Pinzhen Chen, Peter Bell,\nand Catherine": "data augmentation for code generation tasks,”\nin Proc."
        },
        {
          "PTM,\ntext LLM, and emotional TTS for speech emotion": "",
          "[25] Yuanchao Li, Pinzhen Chen, Peter Bell,\nand Catherine": "of EACL 2023 Findings, 2023, pp. 1542–1550."
        },
        {
          "PTM,\ntext LLM, and emotional TTS for speech emotion": "[24]\nSrijith Radhakrishnan, Chao-Han Yang, Sumeer Khan,",
          "[25] Yuanchao Li, Pinzhen Chen, Peter Bell,\nand Catherine": ""
        },
        {
          "PTM,\ntext LLM, and emotional TTS for speech emotion": "",
          "[25] Yuanchao Li, Pinzhen Chen, Peter Bell,\nand Catherine": "[39] Yuanchao Li, Zeyu Zhao, Ondrej Klejch, Peter Bell, and"
        },
        {
          "PTM,\ntext LLM, and emotional TTS for speech emotion": "Rohit Kumar, Narsis Kiani, David Gomez-Cabrero, and",
          "[25] Yuanchao Li, Pinzhen Chen, Peter Bell,\nand Catherine": ""
        },
        {
          "PTM,\ntext LLM, and emotional TTS for speech emotion": "",
          "[25] Yuanchao Li, Pinzhen Chen, Peter Bell,\nand Catherine": "Catherine Lai, “ASR and emotional speech: A word-level"
        },
        {
          "PTM,\ntext LLM, and emotional TTS for speech emotion": "Jesper Tegn´er, “Whispering LLaMA: A cross-modal gen-",
          "[25] Yuanchao Li, Pinzhen Chen, Peter Bell,\nand Catherine": ""
        },
        {
          "PTM,\ntext LLM, and emotional TTS for speech emotion": "",
          "[25] Yuanchao Li, Pinzhen Chen, Peter Bell,\nand Catherine": "investigation of the mutual impact of speech and emotion"
        },
        {
          "PTM,\ntext LLM, and emotional TTS for speech emotion": "erative\nerror\ncorrection framework for\nspeech recogni-",
          "[25] Yuanchao Li, Pinzhen Chen, Peter Bell,\nand Catherine": ""
        },
        {
          "PTM,\ntext LLM, and emotional TTS for speech emotion": "",
          "[25] Yuanchao Li, Pinzhen Chen, Peter Bell,\nand Catherine": "recognition,” in Interspeech 2023, 2023."
        },
        {
          "PTM,\ntext LLM, and emotional TTS for speech emotion": "tion,” in Proc. of EMNLP 2023, 2023, pp. 10007–10016.",
          "[25] Yuanchao Li, Pinzhen Chen, Peter Bell,\nand Catherine": ""
        }
      ],
      "page": 6
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Chatgpt",
      "authors": [
        "Openai"
      ],
      "year": "2022",
      "venue": "Chatgpt"
    },
    {
      "citation_id": "3",
      "title": "Survey on speech emotion recognition: Features, classification schemes, and databases",
      "authors": [
        "Moataz Ayadi",
        "Mohamed Kamel",
        "Fakhri Karray"
      ],
      "year": "2011",
      "venue": "Pattern recognition"
    },
    {
      "citation_id": "4",
      "title": "Speech emotion recognition using machine learning-a systematic review",
      "authors": [
        "Talen Samaneh Madanian",
        "Olayinka Chen",
        "John Adeleye",
        "Christian Michael Templeton",
        "Dave Poellabauer",
        "Sandra Parry",
        "Schneider"
      ],
      "year": "2023",
      "venue": "Intelligent systems with applications"
    },
    {
      "citation_id": "5",
      "title": "IEMOCAP: Interactive emotional dyadic motion capture database",
      "authors": [
        "Carlos Busso",
        "Murtaza Bulut",
        "Chi-Chun Lee",
        "Abe Kazemzadeh",
        "Emily Mower",
        "Samuel Kim",
        "Jeannette Chang",
        "Sungbok Lee",
        "Shrikanth S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "6",
      "title": "Building naturalistic emotionally balanced speech corpus by retrieving emotional speech from existing podcast recordings",
      "authors": [
        "Reza Lotfian",
        "Carlos Busso"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "7",
      "title": "Multimodal emotion recognition",
      "authors": [
        "Sanaul Haq",
        "Jackson Philip"
      ],
      "year": "2011",
      "venue": "Machine audition: principles, algorithms and systems"
    },
    {
      "citation_id": "8",
      "title": "A cross-corpus study on speech emotion recognition",
      "authors": [
        "Rosanna Milner",
        "Md Jalal",
        "Raymond Ng",
        "Thomas Hain"
      ],
      "year": "2019",
      "venue": "Proc. of ASRU"
    },
    {
      "citation_id": "9",
      "title": "Emotion2vec: Self-supervised pre-training for speech emotion representation",
      "authors": [
        "Ziyang Ma",
        "Zhisheng Zheng",
        "Jiaxin Ye",
        "Jinchao Li",
        "Zhifu Gao",
        "Shiliang Zhang",
        "Xie Chen"
      ],
      "year": "2023",
      "venue": "Emotion2vec: Self-supervised pre-training for speech emotion representation",
      "arxiv": "arXiv:2312.15185"
    },
    {
      "citation_id": "10",
      "title": "Speech emotion recognition from voice messages recorded in the wild",
      "authors": [
        "Lucía Gómez-Zaragozá",
        "Óscar Valls",
        "Rocío Del Amor",
        "José Castro-Bleda",
        "Valery Naranjo",
        "Mariano Raya",
        "Javier Marín-Morales"
      ],
      "year": "2024",
      "venue": "Speech emotion recognition from voice messages recorded in the wild",
      "arxiv": "arXiv:2403.02167"
    },
    {
      "citation_id": "11",
      "title": "Deep imbalanced learning for multimodal emotion recognition in conversations",
      "authors": [
        "Tao Meng",
        "Yuntao Shou",
        "Wei Ai",
        "Nan Yin",
        "Keqin Li"
      ],
      "year": "2023",
      "venue": "Deep imbalanced learning for multimodal emotion recognition in conversations",
      "arxiv": "arXiv:2312.06337"
    },
    {
      "citation_id": "12",
      "title": "Leveraging speech PTM, text LLM, and emotional TTS for speech emotion recognition",
      "authors": [
        "Ziyang Ma",
        "Wen Wu",
        "Zhisheng Zheng",
        "Yiwei Guo",
        "Qian Chen",
        "Shiliang Zhang",
        "Xie Chen"
      ],
      "year": "2024",
      "venue": "Proc. ICASSP"
    },
    {
      "citation_id": "13",
      "title": "Bidirectional parallel echo state network for speech emotion recognition",
      "authors": [
        "Hemin Ibrahim",
        "Chu Loo",
        "Fady Alnajjar"
      ],
      "year": "2022",
      "venue": "Neural Computing and Applications"
    },
    {
      "citation_id": "14",
      "title": "Speech emotion recognition via a max-margin framework incorporating a loss function based on the watson and tellegen's emotion model",
      "authors": [
        "Sungrack Yun",
        "Chang Yoo"
      ],
      "year": "2009",
      "venue": "2009 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "15",
      "title": "Robust speech recognition via large-scale weak supervision",
      "authors": [
        "Alec Radford",
        "Jong Kim",
        "Tao Xu",
        "Greg Brockman",
        "Christine Mcleavey",
        "Ilya Sutskever"
      ],
      "year": "2023",
      "venue": "Proc. of ICML"
    },
    {
      "citation_id": "16",
      "title": "Focal loss for dense object detection",
      "authors": [
        "Tsung-Yi Lin",
        "Priya Goyal",
        "Ross Girshick",
        "Kaiming He",
        "Piotr Dollár"
      ],
      "year": "2017",
      "venue": "Proc. of ICCV"
    },
    {
      "citation_id": "17",
      "title": "Odyssey2024 -speech emotion recognition challenge: Dataset, baseline framework, and results",
      "authors": [
        "L Goncalves",
        "A Salman",
        "A Reddy Naini",
        "L Moro-Velazquez",
        "T Thebaud",
        "L Garcia",
        "N Dehak",
        "B Sisman",
        "C Busso"
      ],
      "year": "2024",
      "venue": "Odyssey 2024: The Speaker and Language Recognition Workshop)"
    },
    {
      "citation_id": "18",
      "title": "MISA: Modality-invariant and-specific representations for multimodal sentiment analysis",
      "authors": [
        "Devamanyu Hazarika",
        "Roger Zimmermann",
        "Soujanya Poria"
      ],
      "year": "2020",
      "venue": "Proceedings of the 28th ACM international conference on multimedia"
    },
    {
      "citation_id": "19",
      "title": "Mutual cross-attention in dyadic fusion networks for audio-video emotion recognition",
      "authors": [
        "Jiachen Luo",
        "Huy Phan",
        "Lin Wang",
        "Joshua Reiss"
      ],
      "year": "2023",
      "venue": "Proc. of ACIIW 2023"
    },
    {
      "citation_id": "20",
      "title": "Emotion recognition by fusing time synchronous and time asynchronous representations",
      "authors": [
        "Wen Wu",
        "Chao Zhang",
        "Philip Woodland"
      ],
      "year": "2021",
      "venue": "Proc. of ICASSP 2021"
    },
    {
      "citation_id": "21",
      "title": "Fusing ASR outputs in joint training for speech emotion recognition",
      "authors": [
        "Yuanchao Li",
        "Peter Bell",
        "Catherine Lai"
      ],
      "year": "2022",
      "venue": "Proc. of ICASSP 2022"
    },
    {
      "citation_id": "22",
      "title": "Neural error corrective language models for automatic speech recognition",
      "authors": [
        "Tomohiro Tanaka",
        "Ryo Masumura",
        "Hirokazu Masataki",
        "Yushi Aono"
      ],
      "year": "2018",
      "venue": "INTER-SPEECH"
    },
    {
      "citation_id": "23",
      "title": "Improving OOV detection and resolution with external language models in acoustic-to-word ASR",
      "authors": [
        "Hirofumi Inaguma",
        "Masato Mimura",
        "Shinsuke Sakai",
        "Tatsuya Kawahara"
      ],
      "year": "2018",
      "venue": "Proc. of SLT 2018"
    },
    {
      "citation_id": "24",
      "title": "Improving readability for automatic speech recognition transcription",
      "authors": [
        "Junwei Liao",
        "Sefik Eskimez",
        "Liyang Lu",
        "Yu Shi",
        "Ming Gong",
        "Linjun Shou",
        "Hong Qu",
        "Michael Zeng"
      ],
      "year": "2023",
      "venue": "ACM Transactions on Asian and Low-Resource Language Information Processing"
    },
    {
      "citation_id": "25",
      "title": "Whispering LLaMA: A cross-modal generative error correction framework for speech recognition",
      "authors": [
        "Srijith Radhakrishnan",
        "Chao-Han",
        "Sumeer Yang",
        "Rohit Khan",
        "Narsis Kumar",
        "David Kiani",
        "Jesper Gomez-Cabrero",
        "Tegnér"
      ],
      "year": "2023",
      "venue": "Proc. of EMNLP 2023"
    },
    {
      "citation_id": "26",
      "title": "Crossmodal asr error correction with discrete speech units",
      "authors": [
        "Yuanchao Li",
        "Pinzhen Chen",
        "Peter Bell",
        "Catherine Lai"
      ],
      "year": "2024",
      "venue": "Crossmodal asr error correction with discrete speech units",
      "arxiv": "arXiv:2405.16677"
    },
    {
      "citation_id": "27",
      "title": "Attention is all you need",
      "authors": [
        "Ashish Vaswani",
        "Noam Shazeer",
        "Niki Parmar",
        "Jakob Uszkoreit",
        "Llion Jones",
        "Aidan Gomez",
        "Łukasz Kaiser",
        "Illia Polosukhin"
      ],
      "year": "2017",
      "venue": "Proc. of NIPS 2017"
    },
    {
      "citation_id": "28",
      "title": "WavLM: Large-scale selfsupervised pre-training for full stack speech processing",
      "authors": [
        "Sanyuan Chen",
        "Chengyi Wang",
        "Zhengyang Chen",
        "Yu Wu",
        "Shujie Liu",
        "Zhuo Chen",
        "Jinyu Li",
        "Naoyuki Kanda",
        "Takuya Yoshioka",
        "Xiong Xiao"
      ],
      "year": "2022",
      "venue": "IEEE Journal of Selected Topics in Signal Processing"
    },
    {
      "citation_id": "29",
      "title": "Hubert: Self-supervised speech representation learning by masked prediction of hidden units",
      "authors": [
        "Wei-Ning Hsu",
        "Benjamin Bolte",
        "Hubert Yao-Hung",
        "Kushal Tsai",
        "Ruslan Lakhotia",
        "Abdelrahman Salakhutdinov",
        "Mohamed"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "30",
      "title": "Roberta: A robustly optimized bert pretraining approach",
      "authors": [
        "Yinhan Liu",
        "Myle Ott",
        "Naman Goyal",
        "Jingfei Du",
        "Mandar Joshi",
        "Danqi Chen",
        "Omer Levy",
        "Mike Lewis",
        "Luke Zettlemoyer",
        "Veselin Stoyanov"
      ],
      "year": "2019",
      "venue": "Roberta: A robustly optimized bert pretraining approach",
      "arxiv": "arXiv:1907.11692"
    },
    {
      "citation_id": "31",
      "title": "Transformers",
      "authors": [
        "Huggingface"
      ],
      "year": "2016",
      "venue": "Transformers"
    },
    {
      "citation_id": "32",
      "title": "Experiments on parallel training of deep neural network using model averaging",
      "authors": [
        "Hang Su",
        "Haoyu Chen"
      ],
      "year": "2015",
      "venue": "Experiments on parallel training of deep neural network using model averaging",
      "arxiv": "arXiv:1507.01239"
    },
    {
      "citation_id": "33",
      "title": "Speechbrain: A general-purpose speech toolkit",
      "authors": [
        "Mirco Ravanelli",
        "Titouan Parcollet",
        "Peter Plantinga",
        "Aku Rouhe",
        "Samuele Cornell",
        "Loren Lugosch",
        "Cem Subakan",
        "Nauman Dawalatabad",
        "Abdelwahab Heba",
        "Jianyuan Zhong"
      ],
      "year": "2021",
      "venue": "Speechbrain: A general-purpose speech toolkit",
      "arxiv": "arXiv:2106.04624"
    },
    {
      "citation_id": "34",
      "title": "Pytorch: An imperative style, high-performance deep learning library",
      "authors": [
        "Adam Paszke",
        "Sam Gross",
        "Francisco Massa",
        "Adam Lerer",
        "James Bradbury",
        "Gregory Chanan",
        "Trevor Killeen",
        "Zeming Lin",
        "Natalia Gimelshein",
        "Luca Antiga"
      ],
      "year": "2019",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "35",
      "title": "Tensor fusion network for multimodal sentiment analysis",
      "authors": [
        "Amir Zadeh",
        "Minghai Chen",
        "Soujanya Poria",
        "Erik Cambria",
        "Louis-Philippe Morency"
      ],
      "year": "2017",
      "venue": "Proc. of EMNLP"
    },
    {
      "citation_id": "36",
      "title": "Efficient low-rank multimodal fusion with modality-specific factors",
      "authors": [
        "Zhun Liu",
        "Ying Shen"
      ],
      "year": "2018",
      "venue": "Proc. of ACL 2018"
    },
    {
      "citation_id": "37",
      "title": "Common voice: A massively-multilingual speech corpus",
      "authors": [
        "Rosana Ardila",
        "Megan Branson",
        "Kelly Davis",
        "Michael Henretty",
        "Michael Kohler",
        "Josh Meyer",
        "Reuben Morais",
        "Lindsay Saunders",
        "Francis Tyers",
        "Gregor Weber"
      ],
      "year": "2019",
      "venue": "Common voice: A massively-multilingual speech corpus",
      "arxiv": "arXiv:1912.06670"
    },
    {
      "citation_id": "38",
      "title": "Wit3: Web inventory of transcribed and translated talks",
      "authors": [
        "Mauro Cettolo",
        "Christian Girardi",
        "Marcello Federico"
      ],
      "year": "2012",
      "venue": "Proc. of EAMT 2012"
    },
    {
      "citation_id": "39",
      "title": "Exploring data augmentation for code generation tasks",
      "authors": [
        "Pinzhen Chen",
        "Gerasimos Lampouras"
      ],
      "year": "2023",
      "venue": "Proc. of EACL 2023 Findings"
    },
    {
      "citation_id": "40",
      "title": "ASR and emotional speech: A word-level investigation of the mutual impact of speech and emotion recognition",
      "authors": [
        "Yuanchao Li",
        "Zeyu Zhao",
        "Ondrej Klejch",
        "Peter Bell",
        "Catherine Lai"
      ],
      "venue": "ASR and emotional speech: A word-level investigation of the mutual impact of speech and emotion recognition"
    }
  ]
}