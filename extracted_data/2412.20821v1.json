{
  "paper_id": "2412.20821v1",
  "title": "Enhancing Multimodal Emotion Recognition Through Multi-Granularity Cross-Modal Alignment",
  "published": "2024-12-30T09:30:41Z",
  "authors": [
    "Xuechen Wang",
    "Shiwan Zhao",
    "Haoqin Sun",
    "Hui Wang",
    "Jiaming Zhou",
    "Yong Qin"
  ],
  "keywords": [
    "multimodal emotion recognition",
    "multigranularity alignment",
    "human-computer interaction"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Multimodal emotion recognition (MER), leveraging speech and text, has emerged as a pivotal domain within human-computer interaction, demanding sophisticated methods for effective multimodal integration. The challenge of aligning features across these modalities is significant, with most existing approaches adopting a singular alignment strategy. Such a narrow focus not only limits model performance but also fails to address the complexity and ambiguity inherent in emotional expressions. In response, this paper introduces a Multi-Granularity Cross-Modal Alignment (MGCMA) framework, distinguished by its comprehensive approach encompassing distribution-based, instance-based, and token-based alignment modules. This framework enables a multi-level perception of emotional information across modalities. Our experiments on IEMOCAP demonstrate that our proposed method outperforms current state-of-the-art techniques.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "Emotion recognition is an important aspect of humancomputer interaction. Extensive research has been conducted on unimodal emotion recognition  [1, 2] . However, emotions are conveyed in a variety of ways. Speech and text are both important carriers of emotional information  [3] . Multimodal emotion recognition (MER), integrating speech and text, has gained significant attention  [4] . By fusing information from multiple modalities, such as prosodic features from speech and semantic cues from text, MER aims to capture a more comprehensive understanding of human emotions. In recent years, MER has been widely used in many applications across various domains  [5, 6] .\n\nIn the field of MER, the heterogeneity across modalities makes the alignment of multimodal features become a hot research topic. Effective alignment methods are crucial for leveraging complementary information from different modalities. Sun et al.  [8]  employ a fine-grained alignment component to obtain modality-shared representations and capture modal consistency. Liu et al.  [9]  integrate the alignment module with a silence removal technology, achieving accurate alignment of speech and text. These approaches have yielded favorable results in MER. However, they have not fully addressed the ambiguity inherent in emotional expressions  [10] , which can compromise the quality of alignment. Drawing inspiration ‚Ä†Corresponding author. This work was supported by the National Key R&D Program of China (Grant No.2022ZD0116307) and NSF China (Grant No.62271270). from  [11] , leveraging distribution-level representation as a higher-dimensional tool for coarse-grained alignment offers a promising approach to model such ambiguity.\n\nOn the other hand, to achieve fine-grained alignment and uncover potential mapping relationships between multimodal inputs, various innovative alignment strategies have been proposed. Among these, the cross-modal attention mechanism stands out, having been extensively applied to facilitate tokenlevel alignment in numerous studies  [12, 13] . For instance, Liu et al.  [14]  devise an attention-based bidirectional alignment network to map the alignment between speech and text. To address the challenges of emotional asynchrony and modality misalignment in MER, Fan et al.  [15]  introduce a novel multi-granularity attention mechanism to enhance alignment accuracy. However, token-level alignment predominantly concentrates on local information, frequently overlooking the global context. This oversight can, to some extent, compromise the model's overall performance when token-level alignment is applied in isolation.\n\nConversely, contrastive learning has emerged as an innovative method for achieving instance-level multimodal feature alignment  [16, 17] . This approach, by maximizing the similarity between positive pairs and minimizing it for negative pairs, enables the model to develop more generalized representations. Empirical evidence underscores the effectiveness of contrastive learning in enhancing multimodal alignment. Nonetheless, the scope of existing research is often limited to single-level alignment-either at the token or instance level-thereby restricting the model's ability to fully grasp emotional information across different levels.\n\nIn this paper, to address the aforementioned challenges in MER, we introduce a novel Multi-Granularity Cross-Modal Alignment (MGCMA) framework which comprises distribution-based, token-based, and instance-based alignment modules. The distribution-based alignment module, implemented through distribution-level contrastive learning, acts as a coarse-grained alignment from a higher-dimensional perspective, accommodating the ambiguity of emotions. By aligning distribution representations that encapsulate richer information, it sets the stage for more nuanced alignments and interactions. The token-based alignment module employs selfattention and cross-attention mechanisms to accurately match and align local cross-modal representations between speech and text. This fine-grained alignment facilitates the exchange of information across modalities, which is advantageous for",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Feature Extraction Module",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Distribution-Based Alignment Module",
      "text": "Instance-based Alignment Module the subsequent instance-level alignment. The instance-based alignment module prompts the model to learn improved mapping relationships and bolster the correlation between specific speech-text pairs. Our multi-granularity cross-modal alignment framework enables the model to achieve a multi-level perception of emotional information. It integrates the strengths of various granularity alignment strategies and addresses the previously overlooked issue of ambiguity in emotional expression.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Fc",
      "text": "In experiments conducted on the IEMOCAP dataset, our proposed framework surpasses current state-of-the-art methods, achieving weighted accuracy (WA) of 78.87% and unweighted accuracy (UA) of 80.24%.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Ii. Proposed Method",
      "text": "As shown in Fig.  1 , our proposed method consists of a feature extractor, a distribution-based alignment module, a token-based alignment module, and an instance-based alignment module. The following subsections detail each of these components.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "A. Feature Extractor",
      "text": "To obtain high-level representations of each modality, we utilize self-supervised pre-trained models Wav2vec2.0  [18]  and BERT  [19]  as our speech and text encoders, respectively. For each speech-text pair, the speech input s is encoded into x s ‚àà R Ls√óDs , and the text input t is encoded into x t ‚àà R Lt√óDt , where L represents the sequence length and D represents the embedding dimension.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "B. Distribution-Based Alignment Module",
      "text": "1) Distribution Constructor: In order to overcome the shortcomings of the traditional representations, we apply a distribution constructor to construct a multivariate Gaussian distribution for the features x s and x t . As shown in Fig.  2 , our distribution constructor is implemented by the multi-head self-attention mechanism.\n\nGiven a conventional feature representation x m ‚àà R Lm√óDm , m ‚àà {s, t}, the linear layer projects it to Q, K, V and splits it into k heads: Q i , K i and V i , i = {1, . . . , k}. The multi-head self-attention is performed as follows:\n\nwhere W q i , W k i , W v i and W o are trainable matrices, d is set to D/k. The Concat result is divided into ¬µ and œÉ branches. Linear layers and residual connections are applied to obtain the final output.\n\n2) Distribution-level Contrastive Learning: After constructing distributions of features from two modalities, we implement distribution-based alignment module by conducting a distribution-level contrastive learning. Given two multivariate Gaussian distributions N 1 ‚àº (¬µ 1 , Œ£ 1 ) and N 2 ‚àº (¬µ 2 , Œ£ 2 ), we first calculate the 2-Wasserstein distance between them as follows: where",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Linear",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Concat",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Multi-Head Attention",
      "text": "could be written as Œ£ 1 Œ£ 2 because Œ£ 1 and Œ£ 2 are both diagonal matrices. Based on the above distances, we calculate the similarity of two distributions as follows:\n\nwhere p (p>0) denotes a scaler factor and q denotes the bias. This formula reflects a negative correlation between the distance and similarity of distributions. For a set of 2N instances containing N speech-text pairs {s i , t i }, i = 1, . . . , N, we obtain 2N distributions N si ‚àº (¬µ si , Œ£ si ) and N ti ‚àº (¬µ ti , Œ£ ti ). Instances from the same speech-text pair are treated as positives, while the others are regarded as negatives. The distribution-level contrastive learning loss and the final distribution-based alignment loss L DA are calculated as follows:\n\nwhere œÑ denotes a scalar temperature parameter, N is the total number of speech-text pairs, L s2t i and L t2s i denotes the speechto-text contrastive loss and text-to-speech contrastive loss of the i th input {s i , t i }, respectively.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "C. Token-Based Alignment Module",
      "text": "The distribution-based alignment module facilitates more effective alignment of different components within an utterance. Subsequently, we employ the token-based alignment module to achieve fine-grained alignment and foster extensive interaction between features from different modalities. This token-based alignment module comprises N blocks, each consisting of selfattention and cross-attention mechanisms. For the i-th block, the inputs for self-attention, denoted as Q, K, V , are derived from the (i-1)-th block's cross-attention outputs, while the inputs for cross-attention, denoted as Q, K, V , are sourced from the i-th block's self-attention outputs. Consequently, this iterative process yields text-aware speech representations x s and speech-aware text representations x t .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "D. Instance-Based Alignment Module",
      "text": "To further enhance the association of the local information and achieve better mappings between specific speechtext pairs, we implement an instance-based alignment. It is implemented by a contrastive learning with the representations x s and x t . The instance-level contrastive learning loss and the final instance-based alignment loss L IA are calculated as follows:\n\nwhere (‚Ä¢) symbol denotes the dot product, L s2t i and L t2s i denotes the speech-to-text contrastive loss and text-to-speech contrastive loss of the i th input {s i , t i }. œÑ and N have the same meaning as before. Building upon the above alignment stategies, we additionally incorporate cross-entropy loss L CE for supervised training at the end. The final learning objective is expressed as follows:",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Iii. Experiments A. Dataset",
      "text": "We assess the effectiveness of our proposed method using the IEMOCAP  [20]  dataset. To align with previous studies, we perform experiments on a subset of four emotions: angry, happy, sad, and neutral. We combine the original happy category and excited category into a single happy category. In total, the dataset contains 5,531 utterances, which includes 1,103 angry, 1,636 happy, 1,084 sad, and 1,708 neutral utterances.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "B. Experimental Settings",
      "text": "In this work, we use the publicly available pre-trained models, Wav2Vec 2.0 Base and Bert-base-uncased. The dimension size of feature representations is set to 768. The head number of attention mechanism is set to 12 and the block number of token-based alignment module is set to 6. We apply 5-fold cross-validation to evaluate our results, leaving one session out for testing. The batch size is set to 4 and the max training epoch is set to 100. We choose Adam optimizer with the initial learning rate of 10 -5 . The hyperparameter œÑ is set to 0.07. We use weighted accuracy (WA) and unweighted accuracy (UA) as metrics to evaluate the performance of our proposed method.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "C. Comparison With Sota Approaches",
      "text": "Recent state-of-the-art (SOTA) approaches are presented in Table  I . Compared with these results, our method MGCMA shows a significant improvement on both WA and UA metrics. The outstanding performance can be attributed to the model's multi-level perceptual ability towards emotional information, which demonstrates the effectiveness of our proposed method. TABLE I: Performance comparison of our proposed method with SOTA approaches on IEMOCAP.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Method",
      "text": "Year WA(%) UA(%)\n\nChen et al.  [21]  2022 74.30 75.30 Sun et al.  [22]  2023 78.42 79.71 Zhao et al.  [23]  2023 75.50 77.00 Wang et al.  [24]  2023 75.20 76.40 Zhang et al.  [25]  2023 76.00 77.80 Zhao et al.  [26]  2023",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "D. Ablation Studies And Analysis",
      "text": "We conduct a series of experiments to demonstrate the effectiveness and necessity of each module in our framework. The results are shown in Table  II , where DAM, TAM and IAM denotes the distribution-based, token-based and instancebased alignment module, respectively. More detailed analysis are discussed below.\n\nImportance of Different Granularity Alignment Modules. Upon examining the results in Table  II , it becomes apparent that DAM exerts the most significant influence on the overall framework. Within a multimodal framework, alignment at a high-dimensional level enables the model to facilitate more effective token-based and instance-based alignments subsequently.\n\nThe effect of TAM is demonstrated by comparing the results of S0 and S2. Incorporating TAM allows features from one modality to assimilate information from the other modality, thereby augmenting the interaction among local features pertinent to emotion.\n\nA comparison between the results of S0 and S3 highlights the efficacy of IAM. This module compels the framework to position matched speech-text pairs closer, while distancing unmatched pairs. With better distributions in the latent space, the model can obtain a stronger recognition capacity.\n\nTo intuitively illustrate the distribution of speech and text features under various alignment strategies, we employ the t-SNE technique  [27]   Importance of the sequence of different alignment mod-To further investigate the impact of varying sequences of alignment modules on the results, we shuffle the order of these modules. The performances are documented in Table  III .\n\nIt can be observed that the S0 alignment sequence yields the most favorable outcomes. Serving as a higher-dimensional feature, distribution representation contains more information than conventional methods. The distribution-based alignment  module enhances the consistency of various segments in an utterance, playing a pivotal role in elevating the performance of token-based alignment. Through the token-based alignment module, the model focuses more on local information pertinent to the other modality, extracting features critical for emotion recognition. Subsequently, the instance-based alignment module strengthens the association of multimodal features and fosters accurate correlation between the correct speech-text pairs, which facilitates the instance-based downstream task.\n\nAltering the sequence of the alignment modules disrupts the model's ability to develop a multi-level perception of emotional information, which can be detrimental to performance.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Iv. Conclusions",
      "text": "In this paper, we propose a novel Multi-Granularity Cross-Modal Alignment (MGCMA) framework for multimodal emotion recognition. Our approach encourages the model to exhibit comprehensive processing capabilities for emotional information across different granularities. We conduct a series of experiments to prove the effectiveness and necessity of our proposed method. In comparison to state-of-the-art results, our proposed method shows significant improvements on both WA and UA.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Overview of our proposed Multi-Granularity Cross-Modal Alignment (MGCMA) framework which comprises",
      "page": 2
    },
    {
      "caption": "Figure 1: , our proposed method consists of a",
      "page": 2
    },
    {
      "caption": "Figure 2: The structure of Distribution Constructor. Activation",
      "page": 3
    },
    {
      "caption": "Figure 3: , our proposed MGCMA achieves the",
      "page": 4
    },
    {
      "caption": "Figure 3: Visualization of the representations with different align-",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "‚àóTMCC, College of Computer Science, Nankai Unversity, Tianjin, China": "Email: shirleywxc0103@mail.nankai.edu.cn"
        },
        {
          "‚àóTMCC, College of Computer Science, Nankai Unversity, Tianjin, China": "Abstract‚ÄîMultimodal emotion recognition (MER),\nleveraging"
        },
        {
          "‚àóTMCC, College of Computer Science, Nankai Unversity, Tianjin, China": "speech\nand\ntext,\nhas\nemerged\nas\na\npivotal\ndomain within"
        },
        {
          "‚àóTMCC, College of Computer Science, Nankai Unversity, Tianjin, China": ""
        },
        {
          "‚àóTMCC, College of Computer Science, Nankai Unversity, Tianjin, China": "human-computer interaction, demanding sophisticated methods"
        },
        {
          "‚àóTMCC, College of Computer Science, Nankai Unversity, Tianjin, China": ""
        },
        {
          "‚àóTMCC, College of Computer Science, Nankai Unversity, Tianjin, China": "for\neffective multimodal\nintegration. The\nchallenge of aligning"
        },
        {
          "‚àóTMCC, College of Computer Science, Nankai Unversity, Tianjin, China": ""
        },
        {
          "‚àóTMCC, College of Computer Science, Nankai Unversity, Tianjin, China": "features across these modalities is significant, with most existing"
        },
        {
          "‚àóTMCC, College of Computer Science, Nankai Unversity, Tianjin, China": "approaches\nadopting\na\nsingular\nalignment\nstrategy.\nSuch\na"
        },
        {
          "‚àóTMCC, College of Computer Science, Nankai Unversity, Tianjin, China": "narrow focus not only limits model performance but also fails to"
        },
        {
          "‚àóTMCC, College of Computer Science, Nankai Unversity, Tianjin, China": "address the complexity and ambiguity inherent\nin emotional ex-"
        },
        {
          "‚àóTMCC, College of Computer Science, Nankai Unversity, Tianjin, China": ""
        },
        {
          "‚àóTMCC, College of Computer Science, Nankai Unversity, Tianjin, China": "pressions. In response, this paper introduces a Multi-Granularity"
        },
        {
          "‚àóTMCC, College of Computer Science, Nankai Unversity, Tianjin, China": ""
        },
        {
          "‚àóTMCC, College of Computer Science, Nankai Unversity, Tianjin, China": "Cross-Modal Alignment\n(MGCMA)\nframework,\ndistinguished"
        },
        {
          "‚àóTMCC, College of Computer Science, Nankai Unversity, Tianjin, China": ""
        },
        {
          "‚àóTMCC, College of Computer Science, Nankai Unversity, Tianjin, China": "by its comprehensive approach encompassing distribution-based,"
        },
        {
          "‚àóTMCC, College of Computer Science, Nankai Unversity, Tianjin, China": "instance-based, and token-based alignment modules. This frame-"
        },
        {
          "‚àóTMCC, College of Computer Science, Nankai Unversity, Tianjin, China": "work enables a multi-level perception of emotional\ninformation"
        },
        {
          "‚àóTMCC, College of Computer Science, Nankai Unversity, Tianjin, China": "across modalities. Our experiments on IEMOCAP demonstrate"
        },
        {
          "‚àóTMCC, College of Computer Science, Nankai Unversity, Tianjin, China": ""
        },
        {
          "‚àóTMCC, College of Computer Science, Nankai Unversity, Tianjin, China": "that our proposed method outperforms\ncurrent\nstate-of-the-art"
        },
        {
          "‚àóTMCC, College of Computer Science, Nankai Unversity, Tianjin, China": ""
        },
        {
          "‚àóTMCC, College of Computer Science, Nankai Unversity, Tianjin, China": "techniques."
        },
        {
          "‚àóTMCC, College of Computer Science, Nankai Unversity, Tianjin, China": ""
        },
        {
          "‚àóTMCC, College of Computer Science, Nankai Unversity, Tianjin, China": "Index\nTerms‚Äîmultimodal\nemotion\nrecognition,\nmulti-"
        },
        {
          "‚àóTMCC, College of Computer Science, Nankai Unversity, Tianjin, China": ""
        },
        {
          "‚àóTMCC, College of Computer Science, Nankai Unversity, Tianjin, China": "granularity alignment, human-computer interaction"
        },
        {
          "‚àóTMCC, College of Computer Science, Nankai Unversity, Tianjin, China": ""
        },
        {
          "‚àóTMCC, College of Computer Science, Nankai Unversity, Tianjin, China": ""
        },
        {
          "‚àóTMCC, College of Computer Science, Nankai Unversity, Tianjin, China": "I.\nINTRODUCTION"
        },
        {
          "‚àóTMCC, College of Computer Science, Nankai Unversity, Tianjin, China": ""
        },
        {
          "‚àóTMCC, College of Computer Science, Nankai Unversity, Tianjin, China": "Emotion\nrecognition\nis\nan\nimportant\naspect\nof\nhuman-"
        },
        {
          "‚àóTMCC, College of Computer Science, Nankai Unversity, Tianjin, China": ""
        },
        {
          "‚àóTMCC, College of Computer Science, Nankai Unversity, Tianjin, China": "computer\ninteraction. Extensive research has been conducted"
        },
        {
          "‚àóTMCC, College of Computer Science, Nankai Unversity, Tianjin, China": ""
        },
        {
          "‚àóTMCC, College of Computer Science, Nankai Unversity, Tianjin, China": "on unimodal emotion recognition [1, 2]. However, emotions"
        },
        {
          "‚àóTMCC, College of Computer Science, Nankai Unversity, Tianjin, China": ""
        },
        {
          "‚àóTMCC, College of Computer Science, Nankai Unversity, Tianjin, China": "are conveyed in a variety of ways. Speech and text are both"
        },
        {
          "‚àóTMCC, College of Computer Science, Nankai Unversity, Tianjin, China": ""
        },
        {
          "‚àóTMCC, College of Computer Science, Nankai Unversity, Tianjin, China": "important carriers of emotional\ninformation [3]. Multimodal"
        },
        {
          "‚àóTMCC, College of Computer Science, Nankai Unversity, Tianjin, China": ""
        },
        {
          "‚àóTMCC, College of Computer Science, Nankai Unversity, Tianjin, China": "emotion recognition (MER),\nintegrating speech and text, has"
        },
        {
          "‚àóTMCC, College of Computer Science, Nankai Unversity, Tianjin, China": ""
        },
        {
          "‚àóTMCC, College of Computer Science, Nankai Unversity, Tianjin, China": "gained significant\nattention [4]. By fusing information from"
        },
        {
          "‚àóTMCC, College of Computer Science, Nankai Unversity, Tianjin, China": ""
        },
        {
          "‚àóTMCC, College of Computer Science, Nankai Unversity, Tianjin, China": "multiple modalities,\nsuch\nas\nprosodic\nfeatures\nfrom speech"
        },
        {
          "‚àóTMCC, College of Computer Science, Nankai Unversity, Tianjin, China": ""
        },
        {
          "‚àóTMCC, College of Computer Science, Nankai Unversity, Tianjin, China": "and semantic\ncues\nfrom text, MER aims\nto capture\na more"
        },
        {
          "‚àóTMCC, College of Computer Science, Nankai Unversity, Tianjin, China": ""
        },
        {
          "‚àóTMCC, College of Computer Science, Nankai Unversity, Tianjin, China": "comprehensive understanding of human emotions.\nIn recent"
        },
        {
          "‚àóTMCC, College of Computer Science, Nankai Unversity, Tianjin, China": ""
        },
        {
          "‚àóTMCC, College of Computer Science, Nankai Unversity, Tianjin, China": "years, MER has been widely used in many applications across"
        },
        {
          "‚àóTMCC, College of Computer Science, Nankai Unversity, Tianjin, China": ""
        },
        {
          "‚àóTMCC, College of Computer Science, Nankai Unversity, Tianjin, China": "various domains [5, 6]."
        },
        {
          "‚àóTMCC, College of Computer Science, Nankai Unversity, Tianjin, China": ""
        },
        {
          "‚àóTMCC, College of Computer Science, Nankai Unversity, Tianjin, China": "In the field of MER,\nthe heterogeneity across modalities"
        },
        {
          "‚àóTMCC, College of Computer Science, Nankai Unversity, Tianjin, China": ""
        },
        {
          "‚àóTMCC, College of Computer Science, Nankai Unversity, Tianjin, China": "makes\nthe\nalignment of multimodal\nfeatures become\na hot"
        },
        {
          "‚àóTMCC, College of Computer Science, Nankai Unversity, Tianjin, China": ""
        },
        {
          "‚àóTMCC, College of Computer Science, Nankai Unversity, Tianjin, China": "research\ntopic. Effective\nalignment methods\nare\ncrucial\nfor"
        },
        {
          "‚àóTMCC, College of Computer Science, Nankai Unversity, Tianjin, China": ""
        },
        {
          "‚àóTMCC, College of Computer Science, Nankai Unversity, Tianjin, China": "leveraging complementary information from different modali-"
        },
        {
          "‚àóTMCC, College of Computer Science, Nankai Unversity, Tianjin, China": ""
        },
        {
          "‚àóTMCC, College of Computer Science, Nankai Unversity, Tianjin, China": "ties. Sun et al. [8] employ a fine-grained alignment component"
        },
        {
          "‚àóTMCC, College of Computer Science, Nankai Unversity, Tianjin, China": ""
        },
        {
          "‚àóTMCC, College of Computer Science, Nankai Unversity, Tianjin, China": "to obtain modality-shared representations and capture modal"
        },
        {
          "‚àóTMCC, College of Computer Science, Nankai Unversity, Tianjin, China": ""
        },
        {
          "‚àóTMCC, College of Computer Science, Nankai Unversity, Tianjin, China": "consistency. Liu et al. [9] integrate the alignment module with"
        },
        {
          "‚àóTMCC, College of Computer Science, Nankai Unversity, Tianjin, China": ""
        },
        {
          "‚àóTMCC, College of Computer Science, Nankai Unversity, Tianjin, China": "a\nsilence\nremoval\ntechnology,\nachieving accurate\nalignment"
        },
        {
          "‚àóTMCC, College of Computer Science, Nankai Unversity, Tianjin, China": ""
        },
        {
          "‚àóTMCC, College of Computer Science, Nankai Unversity, Tianjin, China": "of speech and text. These approaches have yielded favorable"
        },
        {
          "‚àóTMCC, College of Computer Science, Nankai Unversity, Tianjin, China": ""
        },
        {
          "‚àóTMCC, College of Computer Science, Nankai Unversity, Tianjin, China": "results\nin MER. However,\nthey have not\nfully addressed the"
        },
        {
          "‚àóTMCC, College of Computer Science, Nankai Unversity, Tianjin, China": ""
        },
        {
          "‚àóTMCC, College of Computer Science, Nankai Unversity, Tianjin, China": "ambiguity inherent\nin emotional expressions [10], which can"
        },
        {
          "‚àóTMCC, College of Computer Science, Nankai Unversity, Tianjin, China": ""
        },
        {
          "‚àóTMCC, College of Computer Science, Nankai Unversity, Tianjin, China": "compromise\nthe\nquality\nof\nalignment. Drawing\ninspiration"
        },
        {
          "‚àóTMCC, College of Computer Science, Nankai Unversity, Tianjin, China": ""
        },
        {
          "‚àóTMCC, College of Computer Science, Nankai Unversity, Tianjin, China": ""
        },
        {
          "‚àóTMCC, College of Computer Science, Nankai Unversity, Tianjin, China": "‚Ä†Corresponding\nauthor. This work was\nsupported\nby\nthe National Key"
        },
        {
          "‚àóTMCC, College of Computer Science, Nankai Unversity, Tianjin, China": ""
        },
        {
          "‚àóTMCC, College of Computer Science, Nankai Unversity, Tianjin, China": "R&D Program of China (Grant No.2022ZD0116307) and NSF China (Grant"
        },
        {
          "‚àóTMCC, College of Computer Science, Nankai Unversity, Tianjin, China": "No.62271270)."
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "ùëÑùë†": "ùêæùë†"
        },
        {
          "ùëÑùë†": "Cross-Attention"
        },
        {
          "ùëÑùë†": ""
        },
        {
          "ùëÑùë†": "ùëâ\nùë†"
        },
        {
          "ùëÑùë†": ""
        },
        {
          "ùëÑùë†": ""
        },
        {
          "ùëÑùë†": ""
        },
        {
          "ùëÑùë†": ""
        },
        {
          "ùëÑùë†": ""
        },
        {
          "ùëÑùë†": ""
        },
        {
          "ùëÑùë†": ""
        },
        {
          "ùëÑùë†": ""
        },
        {
          "ùëÑùë†": ""
        },
        {
          "ùëÑùë†": ""
        },
        {
          "ùëÑùë†": ""
        },
        {
          "ùëÑùë†": "ùëÑùë°"
        },
        {
          "ùëÑùë†": ""
        },
        {
          "ùëÑùë†": "ùêæùë°"
        },
        {
          "ùëÑùë†": "Cross-Attention"
        },
        {
          "ùëÑùë†": ""
        },
        {
          "ùëÑùë†": "ùëâùë°"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "√ó N": "Token-based"
        },
        {
          "√ó N": "Alignment Module"
        },
        {
          "√ó N": "proposed Multi-Granularity Cross-Modal Alignment"
        },
        {
          "√ó N": "instance-based alignment modules, and a feature extractor."
        },
        {
          "√ó N": ""
        },
        {
          "√ó N": ""
        },
        {
          "√ó N": ""
        },
        {
          "√ó N": ""
        },
        {
          "√ó N": ""
        },
        {
          "√ó N": ""
        },
        {
          "√ó N": ""
        },
        {
          "√ó N": ""
        },
        {
          "√ó N": ""
        },
        {
          "√ó N": ""
        },
        {
          "√ó N": ""
        },
        {
          "√ó N": ""
        },
        {
          "√ó N": ""
        },
        {
          "√ó N": ""
        },
        {
          "√ó N": ""
        },
        {
          "√ó N": ""
        },
        {
          "√ó N": ""
        },
        {
          "√ó N": ""
        },
        {
          "√ó N": ""
        },
        {
          "√ó N": ""
        },
        {
          "√ó N": ""
        },
        {
          "√ó N": ""
        },
        {
          "√ó N": ""
        },
        {
          "√ó N": ""
        },
        {
          "√ó N": ""
        },
        {
          "√ó N": ""
        },
        {
          "√ó N": ""
        },
        {
          "√ó N": ""
        },
        {
          "√ó N": ""
        },
        {
          "√ó N": ""
        },
        {
          "√ó N": ""
        },
        {
          "√ó N": ""
        },
        {
          "√ó N": ""
        },
        {
          "√ó N": ""
        },
        {
          "√ó N": ""
        },
        {
          "√ó N": ""
        },
        {
          "√ó N": ""
        },
        {
          "√ó N": ""
        },
        {
          "√ó N": ""
        },
        {
          "√ó N": ""
        },
        {
          "√ó N": ""
        },
        {
          "√ó N": ""
        },
        {
          "√ó N": ""
        },
        {
          "√ó N": ""
        },
        {
          "√ó N": ""
        },
        {
          "√ó N": ""
        },
        {
          "√ó N": ""
        },
        {
          "√ó N": ""
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "D.\nInstance-based Alignment Module": "To\nfurther\nenhance\nthe\nassociation\nof\nthe\nlocal\ninforma-"
        },
        {
          "D.\nInstance-based Alignment Module": ""
        },
        {
          "D.\nInstance-based Alignment Module": "tion\nand\nachieve\nbetter mappings\nbetween\nspecific\nspeech-"
        },
        {
          "D.\nInstance-based Alignment Module": "text pairs, we\nimplement\nan instance-based alignment.\nIt\nis"
        },
        {
          "D.\nInstance-based Alignment Module": "implemented by a contrastive learning with the representations"
        },
        {
          "D.\nInstance-based Alignment Module": ""
        },
        {
          "D.\nInstance-based Alignment Module": ""
        },
        {
          "D.\nInstance-based Alignment Module": "instance-level\ncontrastive\nlearning loss\nand"
        },
        {
          "D.\nInstance-based Alignment Module": "(cid:101)\nand (cid:101)xt. The"
        },
        {
          "D.\nInstance-based Alignment Module": "the final\ninstance-based alignment\nloss LIA are calculated as"
        },
        {
          "D.\nInstance-based Alignment Module": ""
        },
        {
          "D.\nInstance-based Alignment Module": "follows:"
        },
        {
          "D.\nInstance-based Alignment Module": ""
        },
        {
          "D.\nInstance-based Alignment Module": "exp((cid:101)xsi\n¬∑ (cid:101)xti )/œÑ )"
        },
        {
          "D.\nInstance-based Alignment Module": ",\nLs2t\n= ‚àí log\n(9)\ni"
        },
        {
          "D.\nInstance-based Alignment Module": "(cid:80)N"
        },
        {
          "D.\nInstance-based Alignment Module": "n=1 exp((cid:101)xsi\n¬∑ (cid:101)xtn )/œÑ )"
        },
        {
          "D.\nInstance-based Alignment Module": "exp((cid:101)xti\n¬∑ (cid:101)xsi)/œÑ )"
        },
        {
          "D.\nInstance-based Alignment Module": "Lt2s\n= ‚àí log\n,\n(10)"
        },
        {
          "D.\nInstance-based Alignment Module": "i"
        },
        {
          "D.\nInstance-based Alignment Module": "(cid:80)N"
        },
        {
          "D.\nInstance-based Alignment Module": "n=1 exp((cid:101)xti\n¬∑ (cid:101)xsn )/œÑ )"
        },
        {
          "D.\nInstance-based Alignment Module": ""
        },
        {
          "D.\nInstance-based Alignment Module": "1"
        },
        {
          "D.\nInstance-based Alignment Module": "N(cid:88) i\n),\n(11)\nLIA =\n( (cid:101)Ls2t\n+ (cid:101)Lt2s"
        },
        {
          "D.\nInstance-based Alignment Module": "2N"
        },
        {
          "D.\nInstance-based Alignment Module": "=1"
        },
        {
          "D.\nInstance-based Alignment Module": ""
        },
        {
          "D.\nInstance-based Alignment Module": "Lt2s\nLs2t\nand\nwhere\n(¬∑)\nsymbol denotes\nthe dot product,"
        },
        {
          "D.\nInstance-based Alignment Module": "i\ni"
        },
        {
          "D.\nInstance-based Alignment Module": "denotes the speech-to-text contrastive loss and text-to-speech"
        },
        {
          "D.\nInstance-based Alignment Module": ""
        },
        {
          "D.\nInstance-based Alignment Module": "œÑ\ncontrastive loss of\nthe ith\nand N have the\ninput {si,\nti}."
        },
        {
          "D.\nInstance-based Alignment Module": "same meaning as before."
        },
        {
          "D.\nInstance-based Alignment Module": ""
        },
        {
          "D.\nInstance-based Alignment Module": "Building upon the above alignment stategies, we addition-"
        },
        {
          "D.\nInstance-based Alignment Module": ""
        },
        {
          "D.\nInstance-based Alignment Module": "ally incorporate cross-entropy loss LCE for supervised training"
        },
        {
          "D.\nInstance-based Alignment Module": "at the end. The final learning objective is expressed as follows:"
        },
        {
          "D.\nInstance-based Alignment Module": ""
        },
        {
          "D.\nInstance-based Alignment Module": ""
        },
        {
          "D.\nInstance-based Alignment Module": ""
        },
        {
          "D.\nInstance-based Alignment Module": "(12)\nL = LDA + LIA + LCE."
        },
        {
          "D.\nInstance-based Alignment Module": ""
        },
        {
          "D.\nInstance-based Alignment Module": "III. EXPERIMENTS"
        },
        {
          "D.\nInstance-based Alignment Module": "A. Dataset"
        },
        {
          "D.\nInstance-based Alignment Module": ""
        },
        {
          "D.\nInstance-based Alignment Module": "We assess the effectiveness of our proposed method using"
        },
        {
          "D.\nInstance-based Alignment Module": ""
        },
        {
          "D.\nInstance-based Alignment Module": "the IEMOCAP [20] dataset. To align with previous\nstudies,"
        },
        {
          "D.\nInstance-based Alignment Module": ""
        },
        {
          "D.\nInstance-based Alignment Module": ""
        },
        {
          "D.\nInstance-based Alignment Module": "we perform experiments on a subset of\nfour emotions: angry,"
        },
        {
          "D.\nInstance-based Alignment Module": "happy, sad, and neutral. We combine the original happy cate-"
        },
        {
          "D.\nInstance-based Alignment Module": ""
        },
        {
          "D.\nInstance-based Alignment Module": ""
        },
        {
          "D.\nInstance-based Alignment Module": "gory and excited category into a single happy category. In total,"
        },
        {
          "D.\nInstance-based Alignment Module": ""
        },
        {
          "D.\nInstance-based Alignment Module": "the dataset\ncontains 5,531 utterances, which includes 1,103"
        },
        {
          "D.\nInstance-based Alignment Module": "angry, 1,636 happy, 1,084 sad, and 1,708 neutral utterances."
        },
        {
          "D.\nInstance-based Alignment Module": ""
        },
        {
          "D.\nInstance-based Alignment Module": ""
        },
        {
          "D.\nInstance-based Alignment Module": "B. Experimental Settings"
        },
        {
          "D.\nInstance-based Alignment Module": "In this work, we use the publicly available pre-trained mod-"
        },
        {
          "D.\nInstance-based Alignment Module": "els, Wav2Vec 2.0 Base and Bert-base-uncased. The dimension"
        },
        {
          "D.\nInstance-based Alignment Module": "size of feature representations is set\nto 768. The head number"
        },
        {
          "D.\nInstance-based Alignment Module": "of attention mechanism is set\nto 12 and the block number of"
        },
        {
          "D.\nInstance-based Alignment Module": "token-based alignment module\nis\nset\nto 6. We\napply 5-fold"
        },
        {
          "D.\nInstance-based Alignment Module": ""
        },
        {
          "D.\nInstance-based Alignment Module": "cross-validation to evaluate our\nresults,\nleaving one\nsession"
        },
        {
          "D.\nInstance-based Alignment Module": ""
        },
        {
          "D.\nInstance-based Alignment Module": "out for testing. The batch size is set\nto 4 and the max training"
        },
        {
          "D.\nInstance-based Alignment Module": ""
        },
        {
          "D.\nInstance-based Alignment Module": "epoch is set to 100. We choose Adam optimizer with the initial"
        },
        {
          "D.\nInstance-based Alignment Module": ""
        },
        {
          "D.\nInstance-based Alignment Module": "learning rate of 10‚àí5. The hyperparameter œÑ is set to 0.07. We"
        },
        {
          "D.\nInstance-based Alignment Module": ""
        },
        {
          "D.\nInstance-based Alignment Module": "use weighted accuracy (WA) and unweighted accuracy (UA) as"
        },
        {
          "D.\nInstance-based Alignment Module": ""
        },
        {
          "D.\nInstance-based Alignment Module": "metrics to evaluate the performance of our proposed method."
        },
        {
          "D.\nInstance-based Alignment Module": ""
        },
        {
          "D.\nInstance-based Alignment Module": "C. Comparison with SOTA Approaches"
        },
        {
          "D.\nInstance-based Alignment Module": ""
        },
        {
          "D.\nInstance-based Alignment Module": "Recent state-of-the-art\n(SOTA) approaches are presented in"
        },
        {
          "D.\nInstance-based Alignment Module": "Table I. Compared with these results, our method MGCMA"
        },
        {
          "D.\nInstance-based Alignment Module": "shows a significant improvement on both WA and UA metrics."
        },
        {
          "D.\nInstance-based Alignment Module": "The outstanding performance can be attributed to the model‚Äôs"
        },
        {
          "D.\nInstance-based Alignment Module": "multi-level perceptual ability towards emotional\ninformation,"
        },
        {
          "D.\nInstance-based Alignment Module": "which demonstrates the effectiveness of our proposed method."
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "TABLE I: Performance comparison of our proposed method": "with SOTA approaches on IEMOCAP.",
          "1.0": ""
        },
        {
          "TABLE I: Performance comparison of our proposed method": "",
          "1.0": "0.8"
        },
        {
          "TABLE I: Performance comparison of our proposed method": "Method",
          "1.0": "0.6"
        },
        {
          "TABLE I: Performance comparison of our proposed method": "Chen et al.",
          "1.0": "0.4"
        },
        {
          "TABLE I: Performance comparison of our proposed method": "Sun et al.",
          "1.0": ""
        },
        {
          "TABLE I: Performance comparison of our proposed method": "Zhao et al.",
          "1.0": "0.2"
        },
        {
          "TABLE I: Performance comparison of our proposed method": "Wang et al.",
          "1.0": "0.0"
        },
        {
          "TABLE I: Performance comparison of our proposed method": "Zhang et al.",
          "1.0": ""
        },
        {
          "TABLE I: Performance comparison of our proposed method": "Zhao et al.",
          "1.0": ""
        },
        {
          "TABLE I: Performance comparison of our proposed method": "",
          "1.0": ""
        },
        {
          "TABLE I: Performance comparison of our proposed method": "",
          "1.0": ""
        },
        {
          "TABLE I: Performance comparison of our proposed method": "",
          "1.0": "1.0"
        },
        {
          "TABLE I: Performance comparison of our proposed method": "",
          "1.0": "0.8"
        },
        {
          "TABLE I: Performance comparison of our proposed method": "",
          "1.0": ""
        },
        {
          "TABLE I: Performance comparison of our proposed method": "System\nMethod",
          "1.0": "0.6"
        },
        {
          "TABLE I: Performance comparison of our proposed method": "S0",
          "1.0": "0.4"
        },
        {
          "TABLE I: Performance comparison of our proposed method": "S1",
          "1.0": "0.2"
        },
        {
          "TABLE I: Performance comparison of our proposed method": "",
          "1.0": ""
        },
        {
          "TABLE I: Performance comparison of our proposed method": "S2\nw/o TAM",
          "1.0": ""
        },
        {
          "TABLE I: Performance comparison of our proposed method": "",
          "1.0": ""
        },
        {
          "TABLE I: Performance comparison of our proposed method": "S3\nw/o IAM",
          "1.0": "0.0"
        },
        {
          "TABLE I: Performance comparison of our proposed method": "S4",
          "1.0": ""
        },
        {
          "TABLE I: Performance comparison of our proposed method": "",
          "1.0": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "S2\nw/o TAM\n78.00\n79.14": "",
          "Neural": ""
        },
        {
          "S2\nw/o TAM\n78.00\n79.14": "S3\nw/o IAM\n78.30\n79.38",
          "Neural": ""
        },
        {
          "S2\nw/o TAM\n78.00\n79.14": "S4\nw/o (DAM + TAM + IAM)\n76.20\n77.62",
          "Neural": "1.0"
        },
        {
          "S2\nw/o TAM\n78.00\n79.14": "",
          "Neural": ""
        },
        {
          "S2\nw/o TAM\n78.00\n79.14": "",
          "Neural": ""
        },
        {
          "S2\nw/o TAM\n78.00\n79.14": "D. Ablation Studies and Analysis",
          "Neural": ""
        },
        {
          "S2\nw/o TAM\n78.00\n79.14": "",
          "Neural": ""
        },
        {
          "S2\nw/o TAM\n78.00\n79.14": "We\nconduct\na\nseries\nof\nexperiments\nto\ndemonstrate\nthe",
          "Neural": ""
        },
        {
          "S2\nw/o TAM\n78.00\n79.14": "",
          "Neural": ""
        },
        {
          "S2\nw/o TAM\n78.00\n79.14": "effectiveness and necessity of each module in our framework.",
          "Neural": ""
        },
        {
          "S2\nw/o TAM\n78.00\n79.14": "The\nresults\nare\nshown in Table\nII, where DAM, TAM and",
          "Neural": ""
        },
        {
          "S2\nw/o TAM\n78.00\n79.14": "IAM denotes the distribution-based, token-based and instance-",
          "Neural": ""
        },
        {
          "S2\nw/o TAM\n78.00\n79.14": "based alignment module,\nrespectively. More detailed analysis",
          "Neural": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "effectiveness and necessity of each module in our framework.": "The\nresults\nare"
        },
        {
          "effectiveness and necessity of each module in our framework.": "IAM denotes the distribution-based, token-based and instance-"
        },
        {
          "effectiveness and necessity of each module in our framework.": "based alignment module,"
        },
        {
          "effectiveness and necessity of each module in our framework.": ""
        },
        {
          "effectiveness and necessity of each module in our framework.": "are discussed below."
        },
        {
          "effectiveness and necessity of each module in our framework.": ""
        },
        {
          "effectiveness and necessity of each module in our framework.": ""
        },
        {
          "effectiveness and necessity of each module in our framework.": ""
        },
        {
          "effectiveness and necessity of each module in our framework.": "ules. Upon\nexamining"
        },
        {
          "effectiveness and necessity of each module in our framework.": ""
        },
        {
          "effectiveness and necessity of each module in our framework.": "apparent that DAM exerts the most significant influence on the"
        },
        {
          "effectiveness and necessity of each module in our framework.": ""
        },
        {
          "effectiveness and necessity of each module in our framework.": "overall framework. Within a multimodal framework, alignment"
        },
        {
          "effectiveness and necessity of each module in our framework.": ""
        },
        {
          "effectiveness and necessity of each module in our framework.": "at\na\nhigh-dimensional"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "grained Disentangled Representation Learning for Multimodal Emotion": "Recognition,‚Äù arXiv preprint arXiv:2312.13567, 2023."
        },
        {
          "grained Disentangled Representation Learning for Multimodal Emotion": "[9] M. Liu, N. Xue, and M. Huo, ‚ÄúMultimodal speech emotion recognition"
        },
        {
          "grained Disentangled Representation Learning for Multimodal Emotion": "based on aligned attention mechanism,‚Äù\nin 2021 IEEE International"
        },
        {
          "grained Disentangled Representation Learning for Multimodal Emotion": "Conference on Unmanned Systems (ICUS), 2021, pp. 802‚Äì808."
        },
        {
          "grained Disentangled Representation Learning for Multimodal Emotion": "[10] Y. Zhou, X. Liang, Y. Gu, Y. Yin, and L. Yao, ‚ÄúMulti-classifier interac-"
        },
        {
          "grained Disentangled Representation Learning for Multimodal Emotion": "tive learning for ambiguous\nspeech emotion recognition,‚Äù IEEE/ACM"
        },
        {
          "grained Disentangled Representation Learning for Multimodal Emotion": "Trans. Audio, Speech, Lang. Process., vol. 30, pp. 695‚Äì705, 2022."
        },
        {
          "grained Disentangled Representation Learning for Multimodal Emotion": ""
        },
        {
          "grained Disentangled Representation Learning for Multimodal Emotion": "and Y. Yang, ‚ÄúMAP: Multimodal Uncertainty-Aware Vision-Language"
        },
        {
          "grained Disentangled Representation Learning for Multimodal Emotion": "IEEE/CVF Conf. Comput. Vis. Pattern\nPre-Training Model,‚Äù in Proc."
        },
        {
          "grained Disentangled Representation Learning for Multimodal Emotion": "Recognit.\n(CVPR), Jun. 2023, pp. 23262‚Äì23271."
        },
        {
          "grained Disentangled Representation Learning for Multimodal Emotion": "and C. W. Lee,\n‚ÄúConvolutional\nattention"
        },
        {
          "grained Disentangled Representation Learning for Multimodal Emotion": "networks\nfor multimodal\nemotion recognition from speech and text"
        },
        {
          "grained Disentangled Representation Learning for Multimodal Emotion": "data,‚Äù in Proc. Grand Challenge Workshop Human Multimodal Lang."
        },
        {
          "grained Disentangled Representation Learning for Multimodal Emotion": "(Challenge-HML), 2018, pp. 28‚Äì34."
        },
        {
          "grained Disentangled Representation Learning for Multimodal Emotion": ""
        },
        {
          "grained Disentangled Representation Learning for Multimodal Emotion": "Modal Attention and 1D Convolutional Neural Networks,‚Äù\nin Proc."
        },
        {
          "grained Disentangled Representation Learning for Multimodal Emotion": "Interspeech 2020, 2020, pp. 4243‚Äì4247."
        },
        {
          "grained Disentangled Representation Learning for Multimodal Emotion": "P. Liu, K. Li,\nand H. Meng,\n‚ÄúGroup Gated\nFusion\non Attention-"
        },
        {
          "grained Disentangled Representation Learning for Multimodal Emotion": "Based Bidirectional Alignment\nfor Multimodal Emotion Recognition,‚Äù"
        },
        {
          "grained Disentangled Representation Learning for Multimodal Emotion": "in Proc.\nInterspeech 2020, 2020, pp. 379‚Äì383."
        },
        {
          "grained Disentangled Representation Learning for Multimodal Emotion": "Fan, X. Xing, B. Cai,\nand X. Xu,\n‚ÄúMGAT: Multi-Granularity"
        },
        {
          "grained Disentangled Representation Learning for Multimodal Emotion": "Attention Based Transformers for Multi-Modal Emotion Recognition,‚Äù"
        },
        {
          "grained Disentangled Representation Learning for Multimodal Emotion": "ICASSP 2023-2023\nIEEE International Conference\non Acoustics\nin"
        },
        {
          "grained Disentangled Representation Learning for Multimodal Emotion": "Speech and Signal Processing(ICASSP).\nIEEE, 2023, pp. 1‚Äì5."
        },
        {
          "grained Disentangled Representation Learning for Multimodal Emotion": ""
        },
        {
          "grained Disentangled Representation Learning for Multimodal Emotion": "G. Sastry, A. Askell, P. Mishkin, J. Clark, et al., ‚ÄúLearning transferable"
        },
        {
          "grained Disentangled Representation Learning for Multimodal Emotion": "International\nvisual models\nfrom natural\nlanguage\nsupervision,‚Äù\nin"
        },
        {
          "grained Disentangled Representation Learning for Multimodal Emotion": "Conference on machine learning. PMLR, 2021, pp. 8748‚Äì8763."
        },
        {
          "grained Disentangled Representation Learning for Multimodal Emotion": ""
        },
        {
          "grained Disentangled Representation Learning for Multimodal Emotion": "audio concepts\nfrom natural\nlanguage supervision,‚Äù in ICASSP 2023-"
        },
        {
          "grained Disentangled Representation Learning for Multimodal Emotion": "2023 IEEE International Conference on Acoustics Speech and Signal"
        },
        {
          "grained Disentangled Representation Learning for Multimodal Emotion": "Processing (ICASSP), 2023, pp. 1‚Äì5."
        },
        {
          "grained Disentangled Representation Learning for Multimodal Emotion": "and M. Auli,\n‚Äúwav2vec\n2.0:"
        },
        {
          "grained Disentangled Representation Learning for Multimodal Emotion": "A framework for\nself-supervised learning of\nspeech representations,‚Äù"
        },
        {
          "grained Disentangled Representation Learning for Multimodal Emotion": "Advances in neural information processing systems, vol. 33, pp. 12449‚Äì"
        },
        {
          "grained Disentangled Representation Learning for Multimodal Emotion": "12460, 2020."
        },
        {
          "grained Disentangled Representation Learning for Multimodal Emotion": "J. Devlin, M.-W. Chang, K. Lee,\nand K. Toutanova,\n‚ÄúBERT: Pre-"
        },
        {
          "grained Disentangled Representation Learning for Multimodal Emotion": "training of Deep Bidirectional Transformers for Language Understand-"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "REFERENCES": "",
          "J. N. Chang, S. Lee,\nand S. S. Narayanan,\n‚ÄúIEMOCAP:\nInteractive": "emotional dyadic motion capture database,‚Äù Language resources and"
        },
        {
          "REFERENCES": "",
          "J. N. Chang, S. Lee,\nand S. S. Narayanan,\n‚ÄúIEMOCAP:\nInteractive": "evaluation, vol. 42, pp. 335‚Äì359, 2008."
        },
        {
          "REFERENCES": "[1] A. F. Adoma, N.-M. Henry, and W. Chen, ‚ÄúComparative analyses of",
          "J. N. Chang, S. Lee,\nand S. S. Narayanan,\n‚ÄúIEMOCAP:\nInteractive": ""
        },
        {
          "REFERENCES": "",
          "J. N. Chang, S. Lee,\nand S. S. Narayanan,\n‚ÄúIEMOCAP:\nInteractive": "[21] W. Chen, X. Xing, X. Xu,\nJ. Yang, and J. Pang, ‚ÄúKey-sparse trans-"
        },
        {
          "REFERENCES": "bert,\nroberta, distilbert, and xlnet\nfor\ntext-based emotion recognition,‚Äù",
          "J. N. Chang, S. Lee,\nand S. S. Narayanan,\n‚ÄúIEMOCAP:\nInteractive": ""
        },
        {
          "REFERENCES": "",
          "J. N. Chang, S. Lee,\nand S. S. Narayanan,\n‚ÄúIEMOCAP:\nInteractive": "former\nfor multimodal speech emotion recognition,‚Äù in ICASSP 2022-"
        },
        {
          "REFERENCES": "in Proc. 2020 17th Int. Comput. Conf. Wavelet Active Media Technol.",
          "J. N. Chang, S. Lee,\nand S. S. Narayanan,\n‚ÄúIEMOCAP:\nInteractive": ""
        },
        {
          "REFERENCES": "",
          "J. N. Chang, S. Lee,\nand S. S. Narayanan,\n‚ÄúIEMOCAP:\nInteractive": "2022 IEEE International Conference on Acoustics Speech and Signal"
        },
        {
          "REFERENCES": "Inf. Process.\n(ICCWAMTIP), 2020, pp. 117‚Äì121.",
          "J. N. Chang, S. Lee,\nand S. S. Narayanan,\n‚ÄúIEMOCAP:\nInteractive": ""
        },
        {
          "REFERENCES": "",
          "J. N. Chang, S. Lee,\nand S. S. Narayanan,\n‚ÄúIEMOCAP:\nInteractive": "Processing (ICASSP), 2022, pp. 6897‚Äì6901."
        },
        {
          "REFERENCES": "[2] X. Wang, S. Zhao, and Y. Qin, ‚ÄúSupervised Contrastive Learning with",
          "J. N. Chang, S. Lee,\nand S. S. Narayanan,\n‚ÄúIEMOCAP:\nInteractive": ""
        },
        {
          "REFERENCES": "",
          "J. N. Chang, S. Lee,\nand S. S. Narayanan,\n‚ÄúIEMOCAP:\nInteractive": "[22] D. Sun, Y. He, and J. Han, ‚ÄúUsing Auxiliary Tasks In Multimodal Fu-"
        },
        {
          "REFERENCES": "Nearest Neighbor Search for Speech Emotion Recognition,‚Äù in Proc.",
          "J. N. Chang, S. Lee,\nand S. S. Narayanan,\n‚ÄúIEMOCAP:\nInteractive": ""
        },
        {
          "REFERENCES": "",
          "J. N. Chang, S. Lee,\nand S. S. Narayanan,\n‚ÄúIEMOCAP:\nInteractive": "sion of Wav2vec 2.0 And Bert\nfor Multimodal Emotion Recognition,‚Äù"
        },
        {
          "REFERENCES": "Interspeech 2023, 2023, pp. 1913‚Äì1917.",
          "J. N. Chang, S. Lee,\nand S. S. Narayanan,\n‚ÄúIEMOCAP:\nInteractive": ""
        },
        {
          "REFERENCES": "",
          "J. N. Chang, S. Lee,\nand S. S. Narayanan,\n‚ÄúIEMOCAP:\nInteractive": "ICASSP 2023-2023\nIEEE International Conference\non Acoustics\nin"
        },
        {
          "REFERENCES": "Z.\nPeng, Y.\nLu,\nS.\nPan,\nand Y.\nLiu,\n‚ÄúEfficient\nspeech\nemotion",
          "J. N. Chang, S. Lee,\nand S. S. Narayanan,\n‚ÄúIEMOCAP:\nInteractive": ""
        },
        {
          "REFERENCES": "",
          "J. N. Chang, S. Lee,\nand S. S. Narayanan,\n‚ÄúIEMOCAP:\nInteractive": "Speech and Signal Processing (ICASSP), 2023, pp. 1‚Äì5."
        },
        {
          "REFERENCES": "ICASSP 2021-\nrecognition\nusing multi-scale\ncnn\nand\nattention,‚Äù\nin",
          "J. N. Chang, S. Lee,\nand S. S. Narayanan,\n‚ÄúIEMOCAP:\nInteractive": ""
        },
        {
          "REFERENCES": "",
          "J. N. Chang, S. Lee,\nand S. S. Narayanan,\n‚ÄúIEMOCAP:\nInteractive": "[23]\nZ. Zhao, Y. Wang,\nand Y. Wang,\n‚ÄúKnowledge-Aware Bayesian Co-"
        },
        {
          "REFERENCES": "2021 IEEE International Conference on Acoustics Speech and Signal",
          "J. N. Chang, S. Lee,\nand S. S. Narayanan,\n‚ÄúIEMOCAP:\nInteractive": ""
        },
        {
          "REFERENCES": "",
          "J. N. Chang, S. Lee,\nand S. S. Narayanan,\n‚ÄúIEMOCAP:\nInteractive": "ICASSP 2023-\nAttention\nfor Multimodal Emotion Recognition,‚Äù\nin"
        },
        {
          "REFERENCES": "Processing(ICASSP), 2021, pp. 3020‚Äì3024.",
          "J. N. Chang, S. Lee,\nand S. S. Narayanan,\n‚ÄúIEMOCAP:\nInteractive": ""
        },
        {
          "REFERENCES": "",
          "J. N. Chang, S. Lee,\nand S. S. Narayanan,\n‚ÄúIEMOCAP:\nInteractive": "2023 IEEE International Conference on Acoustics Speech and Signal"
        },
        {
          "REFERENCES": "Sun, W. Guan, Y. Xia,\nand Z. Zhao,\n‚ÄúDiscriminative",
          "J. N. Chang, S. Lee,\nand S. S. Narayanan,\n‚ÄúIEMOCAP:\nInteractive": ""
        },
        {
          "REFERENCES": "",
          "J. N. Chang, S. Lee,\nand S. S. Narayanan,\n‚ÄúIEMOCAP:\nInteractive": "Processing (ICASSP), 2023, pp. 1‚Äì5."
        },
        {
          "REFERENCES": "Feature Representation Based on Cascaded Attention Network with",
          "J. N. Chang, S. Lee,\nand S. S. Narayanan,\n‚ÄúIEMOCAP:\nInteractive": ""
        },
        {
          "REFERENCES": "",
          "J. N. Chang, S. Lee,\nand S. S. Narayanan,\n‚ÄúIEMOCAP:\nInteractive": "[24]\nS. Wang, Y. Ma,\nand Y. Ding,\n‚ÄúExploring Complementary Features"
        },
        {
          "REFERENCES": "Adversarial Joint Loss for Speech Emotion Recognition,‚Äù Proceedings",
          "J. N. Chang, S. Lee,\nand S. S. Narayanan,\n‚ÄúIEMOCAP:\nInteractive": ""
        },
        {
          "REFERENCES": "",
          "J. N. Chang, S. Lee,\nand S. S. Narayanan,\n‚ÄúIEMOCAP:\nInteractive": "in Multi-Modal Speech Emotion Recognition,‚Äù in ICASSP 2023-2023"
        },
        {
          "REFERENCES": "of\nInterspeech 2022, pp. 4750‚Äì4754, 2022.",
          "J. N. Chang, S. Lee,\nand S. S. Narayanan,\n‚ÄúIEMOCAP:\nInteractive": ""
        },
        {
          "REFERENCES": "",
          "J. N. Chang, S. Lee,\nand S. S. Narayanan,\n‚ÄúIEMOCAP:\nInteractive": "IEEE International Conference on Acoustics Speech and Signal Pro-"
        },
        {
          "REFERENCES": "T. Hu, A. Xu, Z. Liu, Q. You, Y. Guo, V. Sinha, J. Luo, and R. Akkiraju,",
          "J. N. Chang, S. Lee,\nand S. S. Narayanan,\n‚ÄúIEMOCAP:\nInteractive": ""
        },
        {
          "REFERENCES": "",
          "J. N. Chang, S. Lee,\nand S. S. Narayanan,\n‚ÄúIEMOCAP:\nInteractive": "cessing (ICASSP), 2023, pp. 1‚Äì5."
        },
        {
          "REFERENCES": "‚ÄúTouch your heart: A tone-aware chatbot\nfor customer care on social",
          "J. N. Chang, S. Lee,\nand S. S. Narayanan,\n‚ÄúIEMOCAP:\nInteractive": ""
        },
        {
          "REFERENCES": "",
          "J. N. Chang, S. Lee,\nand S. S. Narayanan,\n‚ÄúIEMOCAP:\nInteractive": "[25] X. Zhang and Y. Li, ‚ÄúA Dual Attention-based Modality-Collaborative"
        },
        {
          "REFERENCES": "the 2018 CHI conference on human factors\nmedia,‚Äù in Proceedings of",
          "J. N. Chang, S. Lee,\nand S. S. Narayanan,\n‚ÄúIEMOCAP:\nInteractive": ""
        },
        {
          "REFERENCES": "",
          "J. N. Chang, S. Lee,\nand S. S. Narayanan,\n‚ÄúIEMOCAP:\nInteractive": "Fusion Network for Emotion Recognition,‚Äù in Proc. Interspeech 2023,"
        },
        {
          "REFERENCES": "in computing systems, 2018, pp. 1‚Äì12.",
          "J. N. Chang, S. Lee,\nand S. S. Narayanan,\n‚ÄúIEMOCAP:\nInteractive": ""
        },
        {
          "REFERENCES": "",
          "J. N. Chang, S. Lee,\nand S. S. Narayanan,\n‚ÄúIEMOCAP:\nInteractive": "2023, pp. 1468‚Äì1472."
        },
        {
          "REFERENCES": "[6] H. Arsikere, E. Shriberg, and U. Ozertem, ‚ÄúComputationally-efficient",
          "J. N. Chang, S. Lee,\nand S. S. Narayanan,\n‚ÄúIEMOCAP:\nInteractive": ""
        },
        {
          "REFERENCES": "",
          "J. N. Chang, S. Lee,\nand S. S. Narayanan,\n‚ÄúIEMOCAP:\nInteractive": "[26]\nZ. Zhao, T. Gao, H. Wang, and B. W. Schuller, ‚ÄúSWRR: Feature Map"
        },
        {
          "REFERENCES": "endpointing\nfeatures\nfor\nnatural\nspoken\ninteraction with\npersonal-",
          "J. N. Chang, S. Lee,\nand S. S. Narayanan,\n‚ÄúIEMOCAP:\nInteractive": ""
        },
        {
          "REFERENCES": "",
          "J. N. Chang, S. Lee,\nand S. S. Narayanan,\n‚ÄúIEMOCAP:\nInteractive": "Classifier Based on Sliding Window Attention and High-Response Fea-"
        },
        {
          "REFERENCES": "IEEE International Conference\non Acoustics\nassistant\nsystems,‚Äù\nin",
          "J. N. Chang, S. Lee,\nand S. S. Narayanan,\n‚ÄúIEMOCAP:\nInteractive": ""
        },
        {
          "REFERENCES": "",
          "J. N. Chang, S. Lee,\nand S. S. Narayanan,\n‚ÄúIEMOCAP:\nInteractive": "ture Reuse for Multimodal Emotion Recognition,‚Äù in Proc. Interspeech"
        },
        {
          "REFERENCES": "Speech and Signal Processing(ICASSP), 2014, pp. 3241‚Äì3245.",
          "J. N. Chang, S. Lee,\nand S. S. Narayanan,\n‚ÄúIEMOCAP:\nInteractive": ""
        },
        {
          "REFERENCES": "",
          "J. N. Chang, S. Lee,\nand S. S. Narayanan,\n‚ÄúIEMOCAP:\nInteractive": "2023, 2023, pp. 2433‚Äì2437."
        },
        {
          "REFERENCES": "and Z. Liu,\n‚ÄúLearning Fine-Grained Cross",
          "J. N. Chang, S. Lee,\nand S. S. Narayanan,\n‚ÄúIEMOCAP:\nInteractive": ""
        },
        {
          "REFERENCES": "",
          "J. N. Chang, S. Lee,\nand S. S. Narayanan,\n‚ÄúIEMOCAP:\nInteractive": "[27]\nL. Van Der Maaten and G. Hinton, ‚ÄúVisualizing Data using t-SNE,‚Äù"
        },
        {
          "REFERENCES": "Modality Excitement for Speech Emotion Recognition,‚Äù in Proc. Inter-",
          "J. N. Chang, S. Lee,\nand S. S. Narayanan,\n‚ÄúIEMOCAP:\nInteractive": ""
        },
        {
          "REFERENCES": "",
          "J. N. Chang, S. Lee,\nand S. S. Narayanan,\n‚ÄúIEMOCAP:\nInteractive": "Journal of Machine Learning Research, vol. 9, no. 2605, pp. 2579‚Äì"
        },
        {
          "REFERENCES": "speech 2021, 2021, pp. 3375‚Äì3379.",
          "J. N. Chang, S. Lee,\nand S. S. Narayanan,\n‚ÄúIEMOCAP:\nInteractive": ""
        },
        {
          "REFERENCES": "",
          "J. N. Chang, S. Lee,\nand S. S. Narayanan,\n‚ÄúIEMOCAP:\nInteractive": "2605, 2008."
        },
        {
          "REFERENCES": "and Y. Qin,\n‚ÄúFine-",
          "J. N. Chang, S. Lee,\nand S. S. Narayanan,\n‚ÄúIEMOCAP:\nInteractive": ""
        },
        {
          "REFERENCES": "grained Disentangled Representation Learning for Multimodal Emotion",
          "J. N. Chang, S. Lee,\nand S. S. Narayanan,\n‚ÄúIEMOCAP:\nInteractive": ""
        },
        {
          "REFERENCES": "Recognition,‚Äù arXiv preprint arXiv:2312.13567, 2023.",
          "J. N. Chang, S. Lee,\nand S. S. Narayanan,\n‚ÄúIEMOCAP:\nInteractive": ""
        },
        {
          "REFERENCES": "[9] M. Liu, N. Xue, and M. Huo, ‚ÄúMultimodal speech emotion recognition",
          "J. N. Chang, S. Lee,\nand S. S. Narayanan,\n‚ÄúIEMOCAP:\nInteractive": ""
        },
        {
          "REFERENCES": "based on aligned attention mechanism,‚Äù\nin 2021 IEEE International",
          "J. N. Chang, S. Lee,\nand S. S. Narayanan,\n‚ÄúIEMOCAP:\nInteractive": ""
        },
        {
          "REFERENCES": "Conference on Unmanned Systems (ICUS), 2021, pp. 802‚Äì808.",
          "J. N. Chang, S. Lee,\nand S. S. Narayanan,\n‚ÄúIEMOCAP:\nInteractive": ""
        },
        {
          "REFERENCES": "",
          "J. N. Chang, S. Lee,\nand S. S. Narayanan,\n‚ÄúIEMOCAP:\nInteractive": ""
        },
        {
          "REFERENCES": "tive learning for ambiguous\nspeech emotion recognition,‚Äù IEEE/ACM",
          "J. N. Chang, S. Lee,\nand S. S. Narayanan,\n‚ÄúIEMOCAP:\nInteractive": ""
        },
        {
          "REFERENCES": "Trans. Audio, Speech, Lang. Process., vol. 30, pp. 695‚Äì705, 2022.",
          "J. N. Chang, S. Lee,\nand S. S. Narayanan,\n‚ÄúIEMOCAP:\nInteractive": ""
        },
        {
          "REFERENCES": "",
          "J. N. Chang, S. Lee,\nand S. S. Narayanan,\n‚ÄúIEMOCAP:\nInteractive": ""
        },
        {
          "REFERENCES": "and Y. Yang, ‚ÄúMAP: Multimodal Uncertainty-Aware Vision-Language",
          "J. N. Chang, S. Lee,\nand S. S. Narayanan,\n‚ÄúIEMOCAP:\nInteractive": ""
        },
        {
          "REFERENCES": "IEEE/CVF Conf. Comput. Vis. Pattern\nPre-Training Model,‚Äù in Proc.",
          "J. N. Chang, S. Lee,\nand S. S. Narayanan,\n‚ÄúIEMOCAP:\nInteractive": ""
        },
        {
          "REFERENCES": "Recognit.\n(CVPR), Jun. 2023, pp. 23262‚Äì23271.",
          "J. N. Chang, S. Lee,\nand S. S. Narayanan,\n‚ÄúIEMOCAP:\nInteractive": ""
        },
        {
          "REFERENCES": "and C. W. Lee,\n‚ÄúConvolutional\nattention",
          "J. N. Chang, S. Lee,\nand S. S. Narayanan,\n‚ÄúIEMOCAP:\nInteractive": ""
        },
        {
          "REFERENCES": "networks\nfor multimodal\nemotion recognition from speech and text",
          "J. N. Chang, S. Lee,\nand S. S. Narayanan,\n‚ÄúIEMOCAP:\nInteractive": ""
        },
        {
          "REFERENCES": "data,‚Äù in Proc. Grand Challenge Workshop Human Multimodal Lang.",
          "J. N. Chang, S. Lee,\nand S. S. Narayanan,\n‚ÄúIEMOCAP:\nInteractive": ""
        },
        {
          "REFERENCES": "(Challenge-HML), 2018, pp. 28‚Äì34.",
          "J. N. Chang, S. Lee,\nand S. S. Narayanan,\n‚ÄúIEMOCAP:\nInteractive": ""
        },
        {
          "REFERENCES": "",
          "J. N. Chang, S. Lee,\nand S. S. Narayanan,\n‚ÄúIEMOCAP:\nInteractive": ""
        },
        {
          "REFERENCES": "Modal Attention and 1D Convolutional Neural Networks,‚Äù\nin Proc.",
          "J. N. Chang, S. Lee,\nand S. S. Narayanan,\n‚ÄúIEMOCAP:\nInteractive": ""
        },
        {
          "REFERENCES": "Interspeech 2020, 2020, pp. 4243‚Äì4247.",
          "J. N. Chang, S. Lee,\nand S. S. Narayanan,\n‚ÄúIEMOCAP:\nInteractive": ""
        },
        {
          "REFERENCES": "P. Liu, K. Li,\nand H. Meng,\n‚ÄúGroup Gated\nFusion\non Attention-",
          "J. N. Chang, S. Lee,\nand S. S. Narayanan,\n‚ÄúIEMOCAP:\nInteractive": ""
        },
        {
          "REFERENCES": "Based Bidirectional Alignment\nfor Multimodal Emotion Recognition,‚Äù",
          "J. N. Chang, S. Lee,\nand S. S. Narayanan,\n‚ÄúIEMOCAP:\nInteractive": ""
        },
        {
          "REFERENCES": "in Proc.\nInterspeech 2020, 2020, pp. 379‚Äì383.",
          "J. N. Chang, S. Lee,\nand S. S. Narayanan,\n‚ÄúIEMOCAP:\nInteractive": ""
        },
        {
          "REFERENCES": "Fan, X. Xing, B. Cai,\nand X. Xu,\n‚ÄúMGAT: Multi-Granularity",
          "J. N. Chang, S. Lee,\nand S. S. Narayanan,\n‚ÄúIEMOCAP:\nInteractive": ""
        },
        {
          "REFERENCES": "Attention Based Transformers for Multi-Modal Emotion Recognition,‚Äù",
          "J. N. Chang, S. Lee,\nand S. S. Narayanan,\n‚ÄúIEMOCAP:\nInteractive": ""
        },
        {
          "REFERENCES": "ICASSP 2023-2023\nIEEE International Conference\non Acoustics\nin",
          "J. N. Chang, S. Lee,\nand S. S. Narayanan,\n‚ÄúIEMOCAP:\nInteractive": ""
        },
        {
          "REFERENCES": "Speech and Signal Processing(ICASSP).\nIEEE, 2023, pp. 1‚Äì5.",
          "J. N. Chang, S. Lee,\nand S. S. Narayanan,\n‚ÄúIEMOCAP:\nInteractive": ""
        },
        {
          "REFERENCES": "",
          "J. N. Chang, S. Lee,\nand S. S. Narayanan,\n‚ÄúIEMOCAP:\nInteractive": ""
        },
        {
          "REFERENCES": "G. Sastry, A. Askell, P. Mishkin, J. Clark, et al., ‚ÄúLearning transferable",
          "J. N. Chang, S. Lee,\nand S. S. Narayanan,\n‚ÄúIEMOCAP:\nInteractive": ""
        },
        {
          "REFERENCES": "International\nvisual models\nfrom natural\nlanguage\nsupervision,‚Äù\nin",
          "J. N. Chang, S. Lee,\nand S. S. Narayanan,\n‚ÄúIEMOCAP:\nInteractive": ""
        },
        {
          "REFERENCES": "Conference on machine learning. PMLR, 2021, pp. 8748‚Äì8763.",
          "J. N. Chang, S. Lee,\nand S. S. Narayanan,\n‚ÄúIEMOCAP:\nInteractive": ""
        },
        {
          "REFERENCES": "",
          "J. N. Chang, S. Lee,\nand S. S. Narayanan,\n‚ÄúIEMOCAP:\nInteractive": ""
        },
        {
          "REFERENCES": "audio concepts\nfrom natural\nlanguage supervision,‚Äù in ICASSP 2023-",
          "J. N. Chang, S. Lee,\nand S. S. Narayanan,\n‚ÄúIEMOCAP:\nInteractive": ""
        },
        {
          "REFERENCES": "2023 IEEE International Conference on Acoustics Speech and Signal",
          "J. N. Chang, S. Lee,\nand S. S. Narayanan,\n‚ÄúIEMOCAP:\nInteractive": ""
        },
        {
          "REFERENCES": "Processing (ICASSP), 2023, pp. 1‚Äì5.",
          "J. N. Chang, S. Lee,\nand S. S. Narayanan,\n‚ÄúIEMOCAP:\nInteractive": ""
        },
        {
          "REFERENCES": "and M. Auli,\n‚Äúwav2vec\n2.0:",
          "J. N. Chang, S. Lee,\nand S. S. Narayanan,\n‚ÄúIEMOCAP:\nInteractive": ""
        },
        {
          "REFERENCES": "A framework for\nself-supervised learning of\nspeech representations,‚Äù",
          "J. N. Chang, S. Lee,\nand S. S. Narayanan,\n‚ÄúIEMOCAP:\nInteractive": ""
        },
        {
          "REFERENCES": "Advances in neural information processing systems, vol. 33, pp. 12449‚Äì",
          "J. N. Chang, S. Lee,\nand S. S. Narayanan,\n‚ÄúIEMOCAP:\nInteractive": ""
        },
        {
          "REFERENCES": "12460, 2020.",
          "J. N. Chang, S. Lee,\nand S. S. Narayanan,\n‚ÄúIEMOCAP:\nInteractive": ""
        },
        {
          "REFERENCES": "J. Devlin, M.-W. Chang, K. Lee,\nand K. Toutanova,\n‚ÄúBERT: Pre-",
          "J. N. Chang, S. Lee,\nand S. S. Narayanan,\n‚ÄúIEMOCAP:\nInteractive": ""
        },
        {
          "REFERENCES": "training of Deep Bidirectional Transformers for Language Understand-",
          "J. N. Chang, S. Lee,\nand S. S. Narayanan,\n‚ÄúIEMOCAP:\nInteractive": ""
        },
        {
          "REFERENCES": "the 2019 Conference of\nthe North American\ning,‚Äù in Proceedings of",
          "J. N. Chang, S. Lee,\nand S. S. Narayanan,\n‚ÄúIEMOCAP:\nInteractive": ""
        },
        {
          "REFERENCES": "Chapter\nof\nthe Association\nfor Computational\nLinguistics: Human",
          "J. N. Chang, S. Lee,\nand S. S. Narayanan,\n‚ÄúIEMOCAP:\nInteractive": ""
        },
        {
          "REFERENCES": "Language Technologies, Volume 1 (Long and Short Papers), Jun. 2019,",
          "J. N. Chang, S. Lee,\nand S. S. Narayanan,\n‚ÄúIEMOCAP:\nInteractive": ""
        },
        {
          "REFERENCES": "pp. 4171‚Äì4186.",
          "J. N. Chang, S. Lee,\nand S. S. Narayanan,\n‚ÄúIEMOCAP:\nInteractive": ""
        },
        {
          "REFERENCES": "",
          "J. N. Chang, S. Lee,\nand S. S. Narayanan,\n‚ÄúIEMOCAP:\nInteractive": ""
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Comparative analyses of bert, roberta, distilbert, and xlnet for text-based emotion recognition",
      "authors": [
        "A Adoma",
        "N.-M Henry",
        "W Chen"
      ],
      "year": "2020",
      "venue": "Proc. 2020 17th Int. Comput. Conf. Wavelet Active Media Technol"
    },
    {
      "citation_id": "2",
      "title": "Supervised Contrastive Learning with Nearest Neighbor Search for Speech Emotion Recognition",
      "authors": [
        "X Wang",
        "S Zhao",
        "Y Qin"
      ],
      "year": "2023",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "3",
      "title": "Efficient speech emotion recognition using multi-scale cnn and attention",
      "authors": [
        "Z Peng",
        "Y Lu",
        "S Pan",
        "Y Liu"
      ],
      "year": "2021",
      "venue": "ICASSP 2021-2021 IEEE International Conference on Acoustics Speech and Signal Processing"
    },
    {
      "citation_id": "4",
      "title": "Discriminative Feature Representation Based on Cascaded Attention Network with Adversarial Joint Loss for Speech Emotion Recognition",
      "authors": [
        "Y Liu",
        "H Sun",
        "W Guan",
        "Y Xia",
        "Z Zhao"
      ],
      "year": "2022",
      "venue": "Proceedings of Interspeech 2022"
    },
    {
      "citation_id": "5",
      "title": "Touch your heart: A tone-aware chatbot for customer care on social media",
      "authors": [
        "T Hu",
        "A Xu",
        "Z Liu",
        "Q You",
        "Y Guo",
        "V Sinha",
        "J Luo",
        "R Akkiraju"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 CHI conference on human factors in computing systems"
    },
    {
      "citation_id": "6",
      "title": "Computationally-efficient endpointing features for natural spoken interaction with personalassistant systems",
      "authors": [
        "H Arsikere",
        "E Shriberg",
        "U Ozertem"
      ],
      "year": "2014",
      "venue": "IEEE International Conference on Acoustics Speech and Signal Processing"
    },
    {
      "citation_id": "7",
      "title": "Learning Fine-Grained Cross Modality Excitement for Speech Emotion Recognition",
      "authors": [
        "H Li",
        "W Ding",
        "Z Wu",
        "Z Liu"
      ],
      "year": "2021",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "8",
      "title": "Finegrained Disentangled Representation Learning for Multimodal Emotion Recognition",
      "authors": [
        "H Sun",
        "S Zhao",
        "X Wang",
        "W Zeng",
        "Y Chen",
        "Y Qin"
      ],
      "year": "2023",
      "venue": "Finegrained Disentangled Representation Learning for Multimodal Emotion Recognition",
      "arxiv": "arXiv:2312.13567"
    },
    {
      "citation_id": "9",
      "title": "Multimodal speech emotion recognition based on aligned attention mechanism",
      "authors": [
        "M Liu",
        "N Xue",
        "M Huo"
      ],
      "year": "2021",
      "venue": "2021 IEEE International Conference on Unmanned Systems (ICUS)"
    },
    {
      "citation_id": "10",
      "title": "Multi-classifier interactive learning for ambiguous speech emotion recognition",
      "authors": [
        "Y Zhou",
        "X Liang",
        "Y Gu",
        "Y Yin",
        "L Yao"
      ],
      "year": "2022",
      "venue": "IEEE/ACM Trans. Audio, Speech, Lang. Process"
    },
    {
      "citation_id": "11",
      "title": "MAP: Multimodal Uncertainty-Aware Vision-Language Pre-Training Model",
      "authors": [
        "Y Ji",
        "J Wang",
        "Y Gong",
        "L Zhang",
        "Y Zhu",
        "H Wang",
        "J Zhang",
        "T Sakai",
        "Y Yang"
      ],
      "year": "2023",
      "venue": "Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR)"
    },
    {
      "citation_id": "12",
      "title": "Convolutional attention networks for multimodal emotion recognition from speech and text data",
      "authors": [
        "W Choi",
        "K Song",
        "C Lee"
      ],
      "year": "2018",
      "venue": "Proc. Grand Challenge Workshop Human Multimodal Lang"
    },
    {
      "citation_id": "13",
      "title": "Multimodal Emotion Recognition Using Cross-Modal Attention and 1D Convolutional Neural Networks",
      "authors": [
        "A Patil"
      ],
      "year": "2020",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "14",
      "title": "Group Gated Fusion on Attention-Based Bidirectional Alignment for Multimodal Emotion Recognition",
      "authors": [
        "P Liu",
        "K Li",
        "H Meng"
      ],
      "year": "2020",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "15",
      "title": "MGAT: Multi-Granularity Attention Based Transformers for Multi-Modal Emotion Recognition",
      "authors": [
        "W Fan",
        "X Xing",
        "B Cai",
        "X Xu"
      ],
      "year": "2023",
      "venue": "ICASSP 2023-2023 IEEE International Conference on Acoustics Speech and Signal Processing"
    },
    {
      "citation_id": "16",
      "title": "Learning transferable visual models from natural language supervision",
      "authors": [
        "A Radford",
        "J Kim",
        "C Hallacy",
        "A Ramesh",
        "G Goh",
        "S Agarwal",
        "G Sastry",
        "A Askell",
        "P Mishkin",
        "J Clark"
      ],
      "year": "2021",
      "venue": "International Conference on machine learning"
    },
    {
      "citation_id": "17",
      "title": "Clap learning audio concepts from natural language supervision",
      "authors": [
        "B Elizalde",
        "S Deshmukh",
        "M Ismail",
        "H Wang"
      ],
      "year": "2023",
      "venue": "ICASSP 2023-2023 IEEE International Conference on Acoustics Speech and Signal Processing"
    },
    {
      "citation_id": "18",
      "title": "wav2vec 2.0: A framework for self-supervised learning of speech representations",
      "authors": [
        "A Baevski",
        "Y Zhou",
        "A Mohamed",
        "M Auli"
      ],
      "year": "2020",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "19",
      "title": "BERT: Pretraining of Deep Bidirectional Transformers for Language Understanding",
      "authors": [
        "J Devlin",
        "M.-W Chang",
        "K Lee",
        "K Toutanova"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference of the North American Chapter"
    },
    {
      "citation_id": "20",
      "title": "IEMOCAP: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "21",
      "title": "Key-sparse transformer for multimodal speech emotion recognition",
      "authors": [
        "W Chen",
        "X Xing",
        "X Xu",
        "J Yang",
        "J Pang"
      ],
      "year": "2022",
      "venue": "ICASSP 2022-2022 IEEE International Conference on Acoustics Speech and Signal Processing"
    },
    {
      "citation_id": "22",
      "title": "Using Auxiliary Tasks In Multimodal Fusion of Wav2vec 2.0 And Bert for Multimodal Recognition",
      "authors": [
        "D Sun",
        "Y He",
        "J Han"
      ],
      "year": "2023",
      "venue": "ICASSP 2023-2023 IEEE International Conference on Acoustics Speech and Signal Processing"
    },
    {
      "citation_id": "23",
      "title": "Knowledge-Aware Bayesian Co-Attention for Multimodal Emotion Recognition",
      "authors": [
        "Z Zhao",
        "Y Wang",
        "Y Wang"
      ],
      "year": "2023",
      "venue": "ICASSP 2023-2023 IEEE International Conference on Acoustics Speech and Signal Processing"
    },
    {
      "citation_id": "24",
      "title": "Exploring Complementary Features in Multi-Modal Speech Emotion Recognition",
      "authors": [
        "S Wang",
        "Y Ma",
        "Y Ding"
      ],
      "year": "2023",
      "venue": "ICASSP 2023-2023 IEEE International Conference on Acoustics Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "25",
      "title": "A Dual Attention-based Modality-Collaborative Fusion Network for Emotion Recognition",
      "authors": [
        "X Zhang",
        "Y Li"
      ],
      "year": "2023",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "26",
      "title": "SWRR: Feature Map Classifier Based on Sliding Window Attention and High-Response Feature Reuse for Multimodal Emotion Recognition",
      "authors": [
        "Z Zhao",
        "T Gao",
        "H Wang",
        "B Schuller"
      ],
      "year": "2023",
      "venue": "Proc. Interspeech 2023"
    },
    {
      "citation_id": "27",
      "title": "Visualizing Data using t-SNE",
      "authors": [
        "L Van Der Maaten",
        "G Hinton"
      ],
      "year": "2008",
      "venue": "Journal of Machine Learning Research"
    }
  ]
}