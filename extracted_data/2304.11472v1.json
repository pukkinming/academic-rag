{
  "paper_id": "2304.11472v1",
  "title": "A Comparative Study Of Pre-Trained Speech And Audio Embeddings For Speech Emotion Recognition",
  "published": "2023-04-22T19:56:35Z",
  "authors": [
    "Orchid Chetia Phukan",
    "Arun Balaji Buduru",
    "Rajesh Sharma"
  ],
  "keywords": [
    "Pre-trained models",
    "Speech Emotion Recognition",
    "Transformers",
    "Convolutional Neural Networks"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Pre-trained models (PTMs) have shown great promise in the speech and audio domain. Embeddings leveraged from these models serve as inputs for learning algorithms with applications in various downstream tasks. One such crucial task is Speech Emotion Recognition (SER) which has a wide range of applications, including dynamic analysis of customer calls, mental health assessment, and personalized language learning. PTM embeddings have helped advance SER, however, a comprehensive comparison of these PTM embeddings that consider multiple facets such as embedding model architecture, data used for pre-training, and the pre-training procedure being followed is missing. A thorough comparison of PTM embeddings will aid in the faster and more efficient development of models and enable their deployment in real-world scenarios. In this work, we exploit this research gap and perform a comparative analysis of embeddings extracted from eight speech and audio PTMs (wav2vec 2.0, data2vec, wavLM, UniSpeech-SAT, wav2clip, YAMNet, x-vector, ECAPA). We perform an extensive empirical analysis with four speech emotion datasets (CREMA-D, TESS, SAVEE, Emo-DB) by training three algorithms (XGBoost, Random Forest, FCN) on the derived embeddings. The results of our study indicate that the best performance is achieved by algorithms trained on embeddings derived from PTMs trained for speaker recognition followed by wav2clip and UniSpeech-SAT. This can relay that the top performance by embeddings from speaker recognition PTMs is most likely due to the model taking up information about numerous speech features such as tone, accent, pitch, and so on during its speaker recognition training. Insights from this work will assist future studies in their selection of embeddings for applications related to SER.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "Pre-trained models (PTMs) are widely available in the speech and audio signal processing domain. Pre-training is carried out on large-scale speech (Librispeech (LS)  [1] ) or nonspeech (AudioSet (AS)  [2] , VGGSound (VS)  [3] ) databases. They find application in various narrow-domain tasks in different ways: from feature extractors for downstream models to the whole model being fine-tuned on task-specific data. Their model architectures can be of varied nature, it can be Convolution Neural Network (CNN) based such as AlexNet, VGG, Inception, ResNet  [4] , etc., and also, attention-based such as AALBERT  [5] , CAV-MAE  [6] , etc. Pre-training is executed using different approaches: supervised  [7]  or selfsupervised fashion  [8] . Embeddings exploited from PTMs are used for different tasks, for example, covid-19 detection  [9] , music emotion recognition  [10] , speech emotion recognition (SER)  [11] .\n\nIn this work, we focus on SER, an important task for humanmachine interaction. It has gained traction in recent times due to its prospective applications in a wide span of different domains, for instance, psychology, healthcare, and fields that often include customer interactions, such as customer service providers, call centers, and so on. A variety of methods have been applied for SER, ranging from fuzzy methods  [12] , Hidden Markov Model (HMM) based methods  [13] , classical machine learning-based approaches  [14] , deep learning-based methods  [15]  to embeddings from PTMs such as wav2vec 2.0  [16] , HuBERT  [17] . The availability of a large number of PTMs has resulted in significant progress in the field of SER. As they were trained on vast amounts of data and learned detailed and nuanced representations of speech, the embeddings extracted from them have proven beneficial for emotion recognition.\n\nHowever, it is not clear which PTM embeddings are best for SER. Keesing et al.  [18]  provided a comparison between acoustic and neural (speech and audio) embeddings by training downstream classifiers such as SVM, RF, MLP, etc. on various speech emotion databases. Atmaja et al.  [19]  assessed representations of PTMs for SER that were pre-trained in a self-supervised manner on speech data by training an FCN classifier on the representations as input features. But a comprehensive comparison of embeddings extracted from a broad variety of PTMs with consideration of their model architectures, pre-training methodologies, and pre-training datasets has not been carried out for SER. We address this research gap by conducting a comparative study of embeddings extracted from eight PTMs by training low-level models with the embeddings as input features.\n\nTo summarize, the following are our main contributions:\n\n• Compiling PTM embeddings that could be useful for performing downstream SER tasks. We consider many diverse PTMs (wav2vec 2.0, data2vec, wavLM, UniSpeech-SAT, wav2clip, YAMNet, x-vector, ECAPA) with varied model architectures, pre-training data, and pre-training procedures.\n\n• Comprehensive comparative analysis of different PTM embeddings through downstream classifiers (XGBoost, Random Forest, Fully Connected Network) which are trained and evaluated on four public datasets (CREMA-D, TESS, SAVEE, Emo-DB).\n\n• Our study has found that embeddings from PTMs trained for speaker recognition tasks perform better than embeddings from other categories of Speech/Audio PTMs.\n\nOur hypothesis is that this could be speaker recognition training procedures enabling models to learn various aspects of speech such as tone, accent, pitch. This paper is divided into six sections. Section II discusses past works on PTMs followed by Section III which elaborates on the different speech emotion databases taken into consideration for carrying out our experiments. In Section IV, we provide brief information on PTM embeddings considered for our analysis and the reason behind the consideration. Section V focuses on the lower-level classifiers, their implementation, training, and results obtained for the comparative analysis. Finally, Section VI concludes the work presented and gives prospective directions for future work.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Related Works",
      "text": "Initially, PTM architectures were mostly CNN-based, for instance, SoundNet  [20] , a 1D CNN trained on a massive amount of unlabeled videos collected from Flickr. It was trained in collaboration with a visual recognition network via discriminative knowledge transfer. Later, the trained model's representations were used as features combined with posterior classifiers to classify acoustic scenes. With the availability of a large-scale labeled audio dataset, AS, various models such as VGGish  [4] , L3-Net  [21] , PANNs  [22] , and etc. were proposed. VGGish is based on VGG architecture and was trained in a supervised manner to classify 527 sound events. L3-Net is also based on the VGG network and was pre-trained in a self-supervised manner for audio-visual correspondence. Gong et al.  [23]  trained EfficientNet for audio tagging on AS that was first trained on ImageNet (IM). They also discussed how pre-training in a different modality boosts performance. Niizumi et al.  [24]  extended Bootstrap your own latent (BYOL) approach initially given for vision to BYOL for audio (BYOL-A). BYOL-A presents a novel generalized self-supervised approach for generating audio representation and employs a CNN as an encoder. It was pre-trained on AS by removing the labels and achieved competitive results on various low-level tasks such as speaker identification, language identification, etc. with baseline models. Schneider et al.  [25]  proposed a novel pre-trained multilayer CNN model wav2vec, trained on unlabeled speech data for speech recognition. wav2vec reported the lowest WER for character-based speech recognition compared to past works.\n\nMockingjay  [26] , a multi-layer bidirectional transformer model was pre-trained on LS using masked modeling, where 15% of the input frames were masked to zero and it outputs the masked frames. They observed that pre-training Mockingjay in this manner resulted in improved performance in downstream supervised activities. Baevski et al. proposed wav2vec 2.0  [27] , where the initial layer is a convolutional layer that acts as a feature encoder followed by transformer layer. It is trained in a self-supervised way where masking of a few parts of the feature encoder outputs is done. Unlabeled LS is used as pre-training data and it improves upon wav2vec for phoneme recognition. HuBERT  [28] , a BERT-like architecture with selfsupervised training was also devised that achieves comparable performance with wav2vec 2.0 for speech recognition in LS. The first fully attention-based convolution-devoid architecture named Audio-Spectrogram transformer (AST) was presented in  [7]  for audio classification tasks. It accepts mel-spectrogram as input. AST uses the advantages of pre-trained ViT for image classification tasks, and it is afterward trained on AS. Over previous investigations, AST reported state-of-the-art (SOTA) performance on the AS, ESC-50, and Speech Commands V2 databases. Gong et al.  [29]  enhanced AST further by training it in a self-supervised pattern through joint discriminative training and masked spectrogram modeling. This kind of training improved performance in lower-level tasks over the supervised version. Various encoder-decoder architectures, such as Audio-MAE  [30]  and MaskSpec  [31] , was also proposed.\n\nEmbeddings from PTMs such as YAMNet and wav2vec trained on audio and speech data, respectively, were used as input features to classifiers for SER  [32] . Using models pre-trained on large databases and exploiting embeddings of them as features and applications of transfer learning by finetuning holds a promising future for SER. However, no comparison of embeddings recovered from a wide range of PTMs has been conducted for SER taking into account their model architectures, pre-training procedures, and pre-training datasets. To fill this knowledge gap, we conduct a comparative investigation of embeddings retrieved from eight diverse PTMs pre-trained on speech and audio data.  timodal Actors Dataset (CREMA-D)  [33] , Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS)  [34] , Toronto Emotional Speech Set (TESS)  [35] , Surrey Audio-Visual Expressed Emotion (SAVEE)  [36] , and German Emotional Speech Database (Emo-DB)  [37] . Essential information and distribution of emotions for each corpus are given in Table  I  and Figure  1  respectviely. Additional information related to the databases can be found below:\n\n• CREMA-D: The audio snippets feature 48 male and 43 female performers from various ethnic origins. They talked from a list of 12 phrases. With a diverse range of ages, genders, and ethnicities, CREMA-D is a highquality data source for SER. • TESS: It is recorded by two female actors. Both actresses were fluent in English and cherry-picked 200 words were spoken by the actresses for the seven emotions. • SAVEE: It comprises recordings of four male actors in British English accents. For each emotion, the actors delivered phrases that were phonetically balanced. • Emo-DB: Recordings are from 5 male and 5 female actors. The actors were given a selection of ten distinct scripts from which to speak.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Iv. Pre-Trained Model Embeddings",
      "text": "Embeddings derived from PTMs capture the semantic and aural information of the input clip. We intend to evaluate how effective these embeddings are at capturing emotional content by comparing embeddings retrieved from various PTMs. For selecting PTMs whose embeddings are to be used in our study, we follow two benchmarks: Speech processing Universal PERformance Benchmark (SUPERB)  [38]  and Holistic Evaluation of Audio Representations (HEAR)  [39] .\n\nSUPERB consists of various speech-related tasks ranging from speaker identification, speech emotion recognition, speech recognition, voice separation, speaker diarization, etc. We select models with top performance in SUPERB and are openly available such as wav2vec 2.0, data2vec, wavLM, and UniSpeech-SAT. For wav2vec 2.0, we choose the base 1 version for our experiments that contains 12 transformer blocks in its architecture. On SUPERB, data2vec delivers slightly lower results than the model with the best performance i.e wavLM. data2vec  [40]  aims for bridging the gap in learning methods by proposing a generalized learning framework for different input modalities. wavLM  [41]  outperforms every other counterpart except UniSpeech-SAT on SUPERB. UniSpeech-SAT is a 1 https://huggingface.co/facebook/wav2vec2-base contrastive loss model with multitask learning. UniSpeech-SAT pre-training is done in a speaker-aware format whereas wavLM learns masked speech prediction and denoising concurrently during pre-training. This assists wavLM in dealing with multidimensional information contained in speech, such as speaker identity, spoken content, and so on. wavLM base+ 2  version is used for carrying out our experiments and it is made of a total of 12 transformer encoder layers and was pre-trained on 94k hours data from various diverse speech databases including LibriLight, VoxPopuli, and GigaSpeech. We choose the base+ version for wavLM as it has achieved slight improvement over the base version on SUPERB with a similar number of parameters. For wav2vec 2.0, data2vec, wavLM, and UniSpeech-SAT the last hidden states are extracted and with the application of pooling average, they are converted to a vector of 768-dimension for each audio file to be used as input features for low-level classifiers. The input audio is sampled to 16KHz for all the self-supervised PTMs. We work with the base versions of wav2vec 2.0, data2vec  3  , and UniSpeech-SAT  4  due to computational constraints and they were pretrained on 960 hours of speech data from LS. wav2vec 2.0 is the lowest-performing model on SUPERB among all the selfsupervised models under consideration, however, it has been applied for SER and proven to be effective in both English and multilingual formats  [42] .\n\nAs SUPERB is primarily concerned with speech processing tasks, PTMs pre-trained on speech data and in self-supervised manner, we chose various other PTMs with presence in HEAR such as wav2clip and YAMNet. Presence of wav2vec 2.0 can also be seen in HEAR leaderboard. wav2clip and YAMNet doesn't achieve SOTA performances on HEAR leaderboard and are mostly dominated by transformer-based architectures pre-trained in a self-supervised fashion. However, we added them in our evaluation as we wanted to access the effectiveness of their embeddings for SER as they were pre-trained using different methodologies and differed in terms of the data used for pre-training. wav2clip 5    [43]  is pre-trained using knowledge distillation from CLIP and employs ResNet-18 as an audio encoder and uses VGGSound, an audio-visual Youtube database as pre-training data. Each audio file is transformed to a 2D sprectrogram for input to ResNet and converted to a vector of 512-dimension by average pooling. Similar to its parent architecture CLIP, wav2clip also transfers the audio embeddings to a joint embedding space. wav2clip embeddings as input features with supervised models have    To broaden our assessment, we also considered PTMs for speaker recognition, as knowledge gained for speaker recognition can be beneficial for SER. Evidence suggests that information gained for speaker recognition can help in SER  [44] . Researchers have also advocated inserting knowledge about the speaker identity to network devoted to the primary job of SER  [45]  to boost performance for SER. So, we select x-vector  [46]  and ECAPA  [47]  to validate the efficacy of speaker recognition system for SER. x-vector, a time delay neural network (TDNN) improves over previous speaker recognition system, i-vector and Emphasized Channel Attention, Propagation and Aggregation (ECAPA) approach inserts several modifications to the x-vector model architecture. We pick off-the-shelf x-vector 7 and ECAPA 8 models. Both were pre-trained on a combination of voxceleb1 and voxceleb2 in a supervised manner. For pre-training of x-vector and ECAPA, all of the input audio files were sampled at 16Khz singlechannel. We extract 512 and 192-dimension embeddings using Speechbrain  [48]  for x-vector and ECAPA respectively.\n\nV. EXPERIMENTS",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "A. Downstream Classifier",
      "text": "We experiment with two classical machine learning approaches XGBoost (XGB), and Random Forest (RF), and a fully connected network (FCN). FCN is a simple neural network with three dense layers, batch normalization and dropout in between. Activation function being used is relu in all the dense layers and followed by softmax in the output layer which outputs the probabilies for different emotions. The same models are trained and evaluated with all the embeddings taken under consideration. 7 https://huggingface.co/speechbrain/spkrec-xvect-voxceleb 8 https://huggingface.co/speechbrain/spkrec-ecapa-voxceleb All four speech emotion corpora are splitted to 85:15 ratio with 15% being used for testing. Out of the remaining 85%, 10% is kept for validation and the rest is used for training the classifiers. Hyperparameters are selected based on the performance of the classifiers on the validation set using GridSearchCV from sklearn library. We train the FCN for 50 epochs with a learning rate of 1e-3 and the optimizer being used is Adam. In addition, learning rate decay and early stopping are also applied while training the FCN.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "B. Experimental Results",
      "text": "We compared the performance of eight PTMs embeddings across four speech emotion databases with two popular metrics accuracy and F1-score (macro). Table  II , Table  III  and Table  IV  shows the results of XGB, RF, and FCN for different PTMs embeddings across different datasets respectively.\n\nAmong self-supervised embeddings (wav2vec 2.0, data2vec, wavLM, UniSpeech-SAT), UniSpeech-SAT performed the best. It achieved the highest performance on CREMA-D, TESS, Emo-DB in Table  II  followed by CREMA-D, TESS, Emo-DB in Table  III , and CREMA-D, TESS, SAVEE, Emo-DB in Table  IV . Speaker-aware pre-training may have contributed to these findings. The second is wavLM embeddings that outperformed UniSpeech-SAT embeddings on SAVEE in Table  II  and III. A diverse dataset and the approach for pretraining where denoising is concurrently involved might adhere to this outcome. Among data2vec and wav2vec 2.0, data2vec embeddings perform better than wav2vec 2.0, however, the data used for pre-training belongs to the same dataset (LS), this can be the result of the architectural difference between data2vec and wav2vec 2.0. wav2clip embeddings perform better than the selfsupervised embeddings excluding UniSpeech-SAT across almost all the databases. This could be resultant of the learned knowledge achieved from CLIP and also during its pre-training in a multi-modal format which aims to push all the modalities to a single embedding space. YAMNet embeddings achieved moreover comparable results w.r.t its self-supervised counterparts, sometimes higher and sometimes lower, for example, in Table  II , YAMNet embeddings proved to be more effective in capturing emotion on TESS and Emo-DB. YAMNet reported lower performance than wav2clip across all the datasets except only in one instance in Table  IV .\n\nEmbeddings from speaker recognition PTMs outperformed all other embeddings from different speech/audio PTMs across all spoken emotion datasets. This might be a manifestation of the information learned to identify speakers, where it is trained to recognize and distinguish between unique speakers. As a result, they learned to recognize distinctive elements of a person's speech patterns, such as rhythm, tone, and pitch, as well as linguistic and behavioral variables. x-vector achieves the top performance in comparison to ECAPA in most instances except on TESS and Emo-DB in Table  III  and IV.\n\nWe also present t-SNE plots of raw embeddings extracted from different PTMs to understand the emotion-wise cluster. Figures 2, 3, 4, 5, 6 7, 8, and 9 illustrates the t-SNE plots for wav2vecv 2.0, data2vec, wavLM, UniSpeech-SAT, wav2clip, YAMNet, x-vector, and ECAPA embeddings respectively. These figures support the results obtained from the tables above, it can be seen the embeddings extracted from PTMs for speaker recognition have far better emotion clusters with the highest distance between them than embeddings from other PTMs, especially for TESS corpus followed by wav2clip, YAMNet, and UniSpeech-SAT embeddings. For CREMA-D and TESS, the clusters formed by all eight PTM embeddings are almost inseparable. The results from the tables as well as the t-SNE plots show that models pre-trained with knowledge of the speaker performs best in SER, as evidenced by the performance of UniSpeech-SAT among self-supervised PTMs and the overall performance of x-vector and ECAPA.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Vi. Conclusion",
      "text": "PTMs have been useful in various speech and audio-related tasks. Pre-train it on vast amount of labeled or unlabeled data and these models or the derived features from it can be highly beneficial for a wide range of tasks. Out of the variety of speech processing tasks, SER is a hard task to recon with, as due to various factors comes into play including difference in voice, tone, and accent. Past literature have shown the usage of different speech/audio PTMs embeddings for SER. However, previous studies haven't presented an extensive comparison of PTMs for SER with inclusion of various perspectives such as architectures of the PTMs, data utilized during pre-training phase, and pre-training technique followed. Our studies tries to narrow down this research gap by comparing embeddings derived from eight PTMs (wav2vec 2.0, data2vec, wavLM, UniSpeech-SAT, wav2clip, YAMNet, x-vector, ECAPA) by training three classifiers (XGB, RF, FCN) on top of these features for four speech emotion datasets (CREMA-D, TESS, SAVEE, Emo-DB). Classifiers trained on embeddings extracted from models pre-trained for speaker recognition attained top performance in all corpora. Our findings suggest that the knowledge acquired for speaker recognition, such as recognition of tone and accent, provides benefits for SER. Embeddings generated from self-supervised PTMs have achieved SOTA performance across a wide range of downstream applications, with architectures such as wavLM and UniSpeech-SAT coming out on top. However, the results of our investigation show that embeddings from simpler CNN PTM like YAMNet still hold solid ground in terms of performance for SER. The outcomes of this study can be used to guide future studies in selecting appropriate embeddings for speech-emotion detection applications. Future Work: We considered eight PTMs, and in the future, we plan to extend our work by incorporating more diverse speech/audio PTM architectures. We investigated four speech emotion corpora in this study, three in English and one in German; in the future, we aim to include more databases not just in English but also in other languages.",
      "page_start": 7,
      "page_end": 8
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Distribution of Emotions across different corpora",
      "page": 2
    },
    {
      "caption": "Figure 1: respectviely.",
      "page": 3
    },
    {
      "caption": "Figure 2: t-SNE plots of wav2vec 2.0 embeddings across different speech emotion corpora",
      "page": 4
    },
    {
      "caption": "Figure 3: t-SNE plots of data2vec embeddings across different speech emotion corpora",
      "page": 4
    },
    {
      "caption": "Figure 4: t-SNE plots of wavLM embeddings across different speech emotion corpora",
      "page": 5
    },
    {
      "caption": "Figure 5: t-SNE plots of UniSpeech-SAT embeddings across different speech emotion corpora",
      "page": 5
    },
    {
      "caption": "Figure 6: t-SNE plots of wav2clip embeddings across different speech emotion corpora",
      "page": 5
    },
    {
      "caption": "Figure 7: t-SNE plots of YAMNet embeddings across different speech emotion corpora",
      "page": 5
    },
    {
      "caption": "Figure 8: t-SNE plots of x-vector embeddings across different speech emotion corpora",
      "page": 6
    },
    {
      "caption": "Figure 9: t-SNE plots of ECAPA embeddings across different speech emotion corpora",
      "page": 6
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Audio PTM": "wav2vec 2.0",
          "CREMA-D\nAccuracy\nF1-score": "40.29\n40.47",
          "TESS\nAccuracy\nF1-score": "69.76\n69.61",
          "SAVEE\nAccuracy\nF1-score": "40.28\n29.44",
          "Emo-DB\nAccuracy\nF1-score": "49.38\n46.90"
        },
        {
          "Audio PTM": "data2vec",
          "CREMA-D\nAccuracy\nF1-score": "49.33\n49.52",
          "TESS\nAccuracy\nF1-score": "76.90\n76.37",
          "SAVEE\nAccuracy\nF1-score": "37.50\n29.29",
          "Emo-DB\nAccuracy\nF1-score": "49.38\n48.22"
        },
        {
          "Audio PTM": "wavLM",
          "CREMA-D\nAccuracy\nF1-score": "45.48\n45.85",
          "TESS\nAccuracy\nF1-score": "83.10\n82.41",
          "SAVEE\nAccuracy\nF1-score": "50.00\n42.73",
          "Emo-DB\nAccuracy\nF1-score": "54.32\n52.06"
        },
        {
          "Audio PTM": "UniSpeech-SAT",
          "CREMA-D\nAccuracy\nF1-score": "56.13\n56.35",
          "TESS\nAccuracy\nF1-score": "83.57\n83.40",
          "SAVEE\nAccuracy\nF1-score": "45.83\n32.86",
          "Emo-DB\nAccuracy\nF1-score": "69.70\n61.42"
        },
        {
          "Audio PTM": "wav2clip",
          "CREMA-D\nAccuracy\nF1-score": "47.45\n46.77",
          "TESS\nAccuracy\nF1-score": "95.00\n94.95",
          "SAVEE\nAccuracy\nF1-score": "55.56\n52.79",
          "Emo-DB\nAccuracy\nF1-score": "72.84\n66.31"
        },
        {
          "Audio PTM": "YAMNet",
          "CREMA-D\nAccuracy\nF1-score": "46.82\n46.49",
          "TESS\nAccuracy\nF1-score": "92.38\n92.35",
          "SAVEE\nAccuracy\nF1-score": "50.00\n41.17",
          "Emo-DB\nAccuracy\nF1-score": "58.02\n51.41"
        },
        {
          "Audio PTM": "x-vector",
          "CREMA-D\nAccuracy\nF1-score": "60.16\n60.09",
          "TESS\nAccuracy\nF1-score": "97.86\n97.77",
          "SAVEE\nAccuracy\nF1-score": "68.06\n62.17",
          "Emo-DB\nAccuracy\nF1-score": "83.95\n80.07"
        },
        {
          "Audio PTM": "ECAPA",
          "CREMA-D\nAccuracy\nF1-score": "54.34\n54.02",
          "TESS\nAccuracy\nF1-score": "97.14\n97.05",
          "SAVEE\nAccuracy\nF1-score": "55.56\n50.09",
          "Emo-DB\nAccuracy\nF1-score": "75.31\n69.70"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Audio PTM": "wav2vec 2.0",
          "CREMA-D\nAccuracy\nF1-score": "37.69\n37.47",
          "TESS\nAccuracy\nF1-score": "57.38\n56.68",
          "SAVEE\nAccuracy\nF1-score": "38.89\n25.50",
          "Emo-DB\nAccuracy\nF1-score": "56.79\n51.11"
        },
        {
          "Audio PTM": "data2vec",
          "CREMA-D\nAccuracy\nF1-score": "44.58\n44.37",
          "TESS\nAccuracy\nF1-score": "68.33\n67.46",
          "SAVEE\nAccuracy\nF1-score": "36.11\n23.45",
          "Emo-DB\nAccuracy\nF1-score": "58.02\n54.28"
        },
        {
          "Audio PTM": "wavLM",
          "CREMA-D\nAccuracy\nF1-score": "40.64\n41.01",
          "TESS\nAccuracy\nF1-score": "76.67\n75.79",
          "SAVEE\nAccuracy\nF1-score": "45.83\n35.78",
          "Emo-DB\nAccuracy\nF1-score": "50.62\n47.99"
        },
        {
          "Audio PTM": "UniSpeech-SAT",
          "CREMA-D\nAccuracy\nF1-score": "49.06\n48.93",
          "TESS\nAccuracy\nF1-score": "78.33\n77.99",
          "SAVEE\nAccuracy\nF1-score": "45.83\n32.35",
          "Emo-DB\nAccuracy\nF1-score": "60.49\n49.07"
        },
        {
          "Audio PTM": "wav2clip",
          "CREMA-D\nAccuracy\nF1-score": "44.94\n44.16",
          "TESS\nAccuracy\nF1-score": "94.52\n94.50",
          "SAVEE\nAccuracy\nF1-score": "59.72\n55.24",
          "Emo-DB\nAccuracy\nF1-score": "67.90\n63.55"
        },
        {
          "Audio PTM": "YAMNet",
          "CREMA-D\nAccuracy\nF1-score": "43.87\n42.49",
          "TESS\nAccuracy\nF1-score": "88.57\n88.54",
          "SAVEE\nAccuracy\nF1-score": "51.39\n39.16",
          "Emo-DB\nAccuracy\nF1-score": "53.09\n50.12"
        },
        {
          "Audio PTM": "x-vector",
          "CREMA-D\nAccuracy\nF1-score": "52.01\n51.64",
          "TESS\nAccuracy\nF1-score": "98.33\n98.28",
          "SAVEE\nAccuracy\nF1-score": "61.11\n49.89",
          "Emo-DB\nAccuracy\nF1-score": "81.48\n78.40"
        },
        {
          "Audio PTM": "ECAPA",
          "CREMA-D\nAccuracy\nF1-score": "44.05\n43.05",
          "TESS\nAccuracy\nF1-score": "98.57\n98.45",
          "SAVEE\nAccuracy\nF1-score": "48.61\n36.09",
          "Emo-DB\nAccuracy\nF1-score": "83.95\n80.98"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Audio PTM": "wav2vec 2.0",
          "CREMA-D\nAccuracy\nF1-Score": "46.02\n45.81",
          "TESS\nAccuracy\nF1-Score": "84.76\n84.40",
          "SAVEE\nAccuracy\nF1-Score": "41.67\n31.98",
          "Emo-DB\nAccuracy\nF1-Score": "60.49\n57.70"
        },
        {
          "Audio PTM": "data2vec",
          "CREMA-D\nAccuracy\nF1-Score": "53.89\n53.76",
          "TESS\nAccuracy\nF1-Score": "86.67\n86.08",
          "SAVEE\nAccuracy\nF1-Score": "43.06\n33.41",
          "Emo-DB\nAccuracy\nF1-Score": "64.20\n63.35"
        },
        {
          "Audio PTM": "wavLM",
          "CREMA-D\nAccuracy\nF1-Score": "55.77\n55.57",
          "TESS\nAccuracy\nF1-Score": "95.00\n94.80",
          "SAVEE\nAccuracy\nF1-Score": "50.00\n32.27",
          "Emo-DB\nAccuracy\nF1-Score": "62.96\n59.63"
        },
        {
          "Audio PTM": "UniSpeech-SAT",
          "CREMA-D\nAccuracy\nF1-Score": "64.28\n64.43",
          "TESS\nAccuracy\nF1-Score": "96.67\n96.65",
          "SAVEE\nAccuracy\nF1-Score": "61.11\n49.71",
          "Emo-DB\nAccuracy\nF1-Score": "82.72\n79.04"
        },
        {
          "Audio PTM": "wav2clip",
          "CREMA-D\nAccuracy\nF1-Score": "47.18\n46.92",
          "TESS\nAccuracy\nF1-Score": "96.90\n96.79",
          "SAVEE\nAccuracy\nF1-Score": "61.11\n51.81",
          "Emo-DB\nAccuracy\nF1-Score": "74.07\n75.42"
        },
        {
          "Audio PTM": "YAMNet",
          "CREMA-D\nAccuracy\nF1-Score": "48.25\n48.22",
          "TESS\nAccuracy\nF1-Score": "96.19\n96.09",
          "SAVEE\nAccuracy\nF1-Score": "55.56\n41.52",
          "Emo-DB\nAccuracy\nF1-Score": "61.73\n59.46"
        },
        {
          "Audio PTM": "x-vector",
          "CREMA-D\nAccuracy\nF1-Score": "65.80\n65.64",
          "TESS\nAccuracy\nF1-Score": "98.81\n98.79",
          "SAVEE\nAccuracy\nF1-Score": "70.83\n64.90",
          "Emo-DB\nAccuracy\nF1-Score": "87.65\n87.01"
        },
        {
          "Audio PTM": "ECAPA",
          "CREMA-D\nAccuracy\nF1-Score": "61.15\n60.95",
          "TESS\nAccuracy\nF1-Score": "99.52\n99.50",
          "SAVEE\nAccuracy\nF1-Score": "61.11\n54.11",
          "Emo-DB\nAccuracy\nF1-Score": "88.89\n87.09"
        }
      ],
      "page": 4
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Librispeech: an asr corpus based on public domain audio books",
      "authors": [
        "V Panayotov",
        "G Chen",
        "D Povey",
        "S Khudanpur"
      ],
      "year": "2015",
      "venue": "2015 IEEE international conference on acoustics, speech and signal processing (ICASSP)"
    },
    {
      "citation_id": "2",
      "title": "Audio set: An ontology and humanlabeled dataset for audio events",
      "authors": [
        "J Gemmeke",
        "D Ellis",
        "D Freedman",
        "A Jansen",
        "W Lawrence",
        "R Moore",
        "M Plakal",
        "M Ritter"
      ],
      "year": "2017",
      "venue": "2017 IEEE international conference on acoustics, speech and signal processing (ICASSP)"
    },
    {
      "citation_id": "3",
      "title": "Vggsound: A largescale audio-visual dataset",
      "authors": [
        "H Chen",
        "W Xie",
        "A Vedaldi",
        "A Zisserman"
      ],
      "year": "2020",
      "venue": "ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "4",
      "title": "Cnn architectures for large-scale audio classification",
      "authors": [
        "S Hershey",
        "S Chaudhuri",
        "D Ellis",
        "J Gemmeke",
        "A Jansen",
        "R Moore",
        "M Plakal",
        "D Platt",
        "R Saurous",
        "B Seybold"
      ],
      "year": "2017",
      "venue": "2017 ieee international conference on acoustics, speech and signal processing"
    },
    {
      "citation_id": "5",
      "title": "Audio albert: A lite bert for self-supervised learning of audio representation",
      "authors": [
        "P.-H Chi",
        "P.-H Chung",
        "T.-H Wu",
        "C.-C Hsieh",
        "Y.-H Chen",
        "S.-W Li",
        "H.-Y Lee"
      ],
      "year": "2021",
      "venue": "2021 IEEE Spoken Language Technology Workshop (SLT)"
    },
    {
      "citation_id": "6",
      "title": "Contrastive audio-visual masked autoencoder",
      "authors": [
        "Y Gong",
        "A Rouditchenko",
        "A Liu",
        "D Harwath",
        "L Karlinsky",
        "H Kuehne",
        "J Glass"
      ],
      "year": "2022",
      "venue": "Contrastive audio-visual masked autoencoder",
      "arxiv": "arXiv:2210.07839"
    },
    {
      "citation_id": "7",
      "title": "AST: audio spectrogram transformer",
      "authors": [
        "Y Gong",
        "Y Chung",
        "J Glass"
      ],
      "year": "2021",
      "venue": "CoRR"
    },
    {
      "citation_id": "8",
      "title": "Tera: Self-supervised learning of transformer encoder representation for speech",
      "authors": [
        "A Liu",
        "S.-W Li",
        "H.-Y Lee"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "9",
      "title": "L 3-net deep audio embeddings to improve covid-19 detection from smartphone data",
      "authors": [
        "M Campana",
        "A Rovati",
        "F Delmastro",
        "E Pagani"
      ],
      "year": "2022",
      "venue": "2022 IEEE International Conference on Smart Computing (SMARTCOMP)"
    },
    {
      "citation_id": "10",
      "title": "Comparison and analysis of deep audio embeddings for music emotion recognition",
      "authors": [
        "E Koh",
        "S Dubnov"
      ],
      "year": "2021",
      "venue": "Comparison and analysis of deep audio embeddings for music emotion recognition",
      "arxiv": "arXiv:2104.06517"
    },
    {
      "citation_id": "11",
      "title": "On the use of selfsupervised pre-trained acoustic and linguistic features for continuous speech emotion recognition",
      "authors": [
        "M Macary",
        "M Tahon",
        "Y Estève",
        "A Rousseau"
      ],
      "year": "2021",
      "venue": "2021 IEEE Spoken Language Technology Workshop (SLT)"
    },
    {
      "citation_id": "12",
      "title": "Comparison between fuzzy and nn method for speech emotion recognition",
      "authors": [
        "A Razak",
        "R Komiya",
        "M Izani",
        "Z Abidin"
      ],
      "year": "2005",
      "venue": "Third International Conference on Information Technology and Applications (ICITA'05)"
    },
    {
      "citation_id": "13",
      "title": "Tuning hidden markov model for speech emotion recognition",
      "authors": [
        "B Vlasenko",
        "A Wendemuth"
      ],
      "year": "2007",
      "venue": "Fortschritte der akustik"
    },
    {
      "citation_id": "14",
      "title": "Comparison of different classifiers for emotion recognition",
      "authors": [
        "T Iliou",
        "C.-N Anagnostopoulos"
      ],
      "year": "2009",
      "venue": "2009 13th Panhellenic Conference on Informatics"
    },
    {
      "citation_id": "15",
      "title": "Adieu features? end-to-end speech emotion recognition using a deep convolutional recurrent network",
      "authors": [
        "G Trigeorgis",
        "F Ringeval",
        "R Brueckner",
        "E Marchi",
        "M Nicolaou",
        "B Schuller",
        "S Zafeiriou"
      ],
      "year": "2016",
      "venue": "2016 IEEE international conference on acoustics, speech and signal processing"
    },
    {
      "citation_id": "16",
      "title": "Emotion recognition from speech using wav2vec 2.0 embeddings",
      "authors": [
        "L Pepino",
        "P Riera",
        "L Ferrer"
      ],
      "year": "2021",
      "venue": "Emotion recognition from speech using wav2vec 2.0 embeddings",
      "arxiv": "arXiv:2104.03502"
    },
    {
      "citation_id": "17",
      "title": "Cross-corpus speech emotion recognition with hubert self-supervised representation",
      "authors": [
        "M Pastor",
        "D Ribas",
        "A Ortega",
        "A Miguel",
        "E Solano"
      ],
      "year": "2022",
      "venue": "Proceedings of the IberSPEECH"
    },
    {
      "citation_id": "18",
      "title": "Acoustic features and neural representations for categorical emotion recognition from speech",
      "authors": [
        "A Keesing",
        "Y Koh",
        "M Witbrock"
      ],
      "year": "2021",
      "venue": "Interspeech"
    },
    {
      "citation_id": "19",
      "title": "Evaluating self-supervised speech representations for speech emotion recognition",
      "authors": [
        "B Atmaja",
        "Sasou"
      ],
      "year": "2022",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "20",
      "title": "Soundnet: Learning sound representations from unlabeled video",
      "authors": [
        "Y Aytar",
        "C Vondrick",
        "A Torralba"
      ],
      "year": "2016",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "21",
      "title": "Look, listen, and learn more: Design choices for deep audio embeddings",
      "authors": [
        "J Cramer",
        "H.-H Wu",
        "J Salamon",
        "J Bello"
      ],
      "year": "2019",
      "venue": "ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "22",
      "title": "Panns: Large-scale pretrained audio neural networks for audio pattern recognition",
      "authors": [
        "Q Kong",
        "Y Cao",
        "T Iqbal",
        "Y Wang",
        "W Wang",
        "M Plumbley"
      ],
      "year": "2020",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "23",
      "title": "Psla: Improving audio tagging with pretraining, sampling, labeling, and aggregation",
      "authors": [
        "Y Gong",
        "Y.-A Chung",
        "J Glass"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "24",
      "title": "Byol for audio: Self-supervised learning for general-purpose audio representation",
      "authors": [
        "D Niizumi",
        "D Takeuchi",
        "Y Ohishi",
        "N Harada",
        "K Kashino"
      ],
      "year": "2021",
      "venue": "Byol for audio: Self-supervised learning for general-purpose audio representation"
    },
    {
      "citation_id": "25",
      "title": "wav2vec: Unsupervised pre-training for speech recognition",
      "authors": [
        "S Schneider",
        "A Baevski",
        "R Collobert",
        "M Auli"
      ],
      "year": "2019",
      "venue": "CoRR"
    },
    {
      "citation_id": "26",
      "title": "Mockingjay: Unsupervised speech representation learning with deep bidirectional transformer encoders",
      "authors": [
        "A Liu",
        "S Yang",
        "P.-H Chi",
        "P Hsu",
        "H Yi Lee"
      ],
      "year": "2020",
      "venue": "ICASSP 2020 -2020 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "27",
      "title": "wav2vec 2.0: A framework for self-supervised learning of speech representations",
      "authors": [
        "A Baevski",
        "H Zhou",
        "A Mohamed",
        "M Auli"
      ],
      "year": "2020",
      "venue": "wav2vec 2.0: A framework for self-supervised learning of speech representations"
    },
    {
      "citation_id": "28",
      "title": "Hubert: Self-supervised speech representation learning by masked prediction of hidden units",
      "authors": [
        "W Hsu",
        "B Bolte",
        "Y Tsai",
        "K Lakhotia",
        "R Salakhutdinov",
        "A Mohamed"
      ],
      "year": "2021",
      "venue": "CoRR"
    },
    {
      "citation_id": "29",
      "title": "SSAST: self-supervised audio spectrogram transformer",
      "authors": [
        "Y Gong",
        "C Lai",
        "Y Chung",
        "J Glass"
      ],
      "year": "2021",
      "venue": "CoRR"
    },
    {
      "citation_id": "30",
      "title": "Masked autoencoders that listen",
      "authors": [
        "P.-Y Huang",
        "H Xu",
        "J Li",
        "A Baevski",
        "M Auli",
        "W Galuba",
        "F Metze",
        "C Feichtenhofer"
      ],
      "year": "2022",
      "venue": "Masked autoencoders that listen"
    },
    {
      "citation_id": "31",
      "title": "Masked spectrogram prediction for self-supervised audio pre-training",
      "authors": [
        "D Chong",
        "H Wang",
        "P Zhou",
        "Q Zeng"
      ],
      "year": "2022",
      "venue": "Masked spectrogram prediction for self-supervised audio pre-training"
    },
    {
      "citation_id": "32",
      "title": "Acoustic Features and Neural Representations for Categorical Emotion Recognition from Speech",
      "authors": [
        "A Keesing",
        "Y Koh",
        "M Witbrock"
      ],
      "year": "2021",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "33",
      "title": "Crema-d: Crowd-sourced emotional multimodal actors dataset",
      "authors": [
        "H Cao",
        "D Cooper",
        "M Keutmann",
        "R Gur",
        "A Nenkova",
        "R Verma"
      ],
      "year": "2014",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "34",
      "title": "The ryerson audio-visual database of emotional speech and song (ravdess): A dynamic, multimodal set of facial and vocal expressions in north american english",
      "authors": [
        "S Livingstone",
        "F Russo"
      ],
      "year": "2018",
      "venue": "PloS one"
    },
    {
      "citation_id": "35",
      "title": "Toronto emotional speech set (TESS) | TSpace Repository -tspace.library.utoronto.ca",
      "authors": [
        "-F. Kate Dupuis"
      ],
      "year": "2010",
      "venue": "Toronto emotional speech set (TESS) | TSpace Repository -tspace.library.utoronto.ca"
    },
    {
      "citation_id": "36",
      "title": "Surrey audio-visual expressed emotion (savee) database",
      "authors": [
        "P Jackson",
        "S Haq"
      ],
      "year": "2014",
      "venue": "Surrey audio-visual expressed emotion (savee) database"
    },
    {
      "citation_id": "37",
      "title": "A database of german emotional speech",
      "authors": [
        "F Burkhardt",
        "A Paeschke",
        "M Rolfes",
        "W Sendlmeier",
        "B Weiss"
      ],
      "year": "2005",
      "venue": "Interspeech"
    },
    {
      "citation_id": "38",
      "title": "Superb: Speech processing universal performance benchmark",
      "authors": [
        "S.-W Yang",
        "P.-H Chi",
        "Y.-S Chuang",
        "C.-I Lai",
        "K Lakhotia",
        "Y Lin",
        "A Liu",
        "J Shi",
        "X Chang",
        "G.-T Lin"
      ],
      "year": "2021",
      "venue": "Superb: Speech processing universal performance benchmark",
      "arxiv": "arXiv:2105.01051"
    },
    {
      "citation_id": "39",
      "title": "Hear: Holistic evaluation of audio representations",
      "authors": [
        "J Turian",
        "J Shier",
        "H Khan",
        "B Raj",
        "B Schuller",
        "C Steinmetz",
        "C Malloy",
        "G Tzanetakis",
        "G Velarde",
        "K Mcnally"
      ],
      "year": "2022",
      "venue": "NeurIPS 2021 Competitions and Demonstrations Track"
    },
    {
      "citation_id": "40",
      "title": "Data2vec: A general framework for self-supervised learning in speech, vision and language",
      "authors": [
        "A Baevski",
        "W.-N Hsu",
        "Q Xu",
        "A Babu",
        "J Gu",
        "M Auli"
      ],
      "year": "2022",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "41",
      "title": "Wavlm: Large-scale self-supervised pretraining for full stack speech processing",
      "authors": [
        "S Chen",
        "C Wang",
        "Z Chen",
        "Y Wu",
        "S Liu",
        "Z Chen",
        "J Li",
        "N Kanda",
        "T Yoshioka",
        "X Xiao"
      ],
      "year": "2022",
      "venue": "IEEE Journal of Selected Topics in Signal Processing"
    },
    {
      "citation_id": "42",
      "title": "Multi-lingual multi-task speech emotion recognition using wav2vec 2.0",
      "authors": [
        "M Sharma"
      ],
      "year": "2022",
      "venue": "ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "43",
      "title": "Wav2clip: Learning robust audio representations from clip",
      "authors": [
        "H.-H Wu",
        "P Seetharaman",
        "K Kumar",
        "J Bello"
      ],
      "year": "2022",
      "venue": "ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "44",
      "title": "x-vectors meet emotions: A study on dependencies between emotion and speaker recognition",
      "authors": [
        "R Pappagari",
        "T Wang",
        "J Villalba",
        "N Chen",
        "N Dehak"
      ],
      "year": "2020",
      "venue": "ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "45",
      "title": "Speaker attentive speech emotion recognition",
      "authors": [
        "C Moine",
        "N Obin",
        "A Roebel"
      ],
      "year": "2021",
      "venue": "Speaker attentive speech emotion recognition",
      "arxiv": "arXiv:2104.07288"
    },
    {
      "citation_id": "46",
      "title": "Xvectors: Robust dnn embeddings for speaker recognition",
      "authors": [
        "D Snyder",
        "D Garcia-Romero",
        "G Sell",
        "D Povey",
        "S Khudanpur"
      ],
      "year": "2018",
      "venue": "2018 IEEE international conference on acoustics, speech and signal processing (ICASSP)"
    },
    {
      "citation_id": "47",
      "title": "Ecapa-tdnn: Emphasized channel attention, propagation and aggregation in tdnn based speaker verification",
      "authors": [
        "B Desplanques",
        "J Thienpondt",
        "K Demuynck"
      ],
      "year": "2020",
      "venue": "Ecapa-tdnn: Emphasized channel attention, propagation and aggregation in tdnn based speaker verification",
      "arxiv": "arXiv:2005.07143"
    },
    {
      "citation_id": "48",
      "title": "SpeechBrain: A generalpurpose speech toolkit",
      "authors": [
        "M Ravanelli",
        "T Parcollet",
        "P Plantinga",
        "A Rouhe",
        "S Cornell",
        "L Lugosch",
        "C Subakan",
        "N Dawalatabad",
        "A Heba",
        "J Zhong",
        "J.-C Chou",
        "S.-L Yeh",
        "S.-W Fu",
        "C.-F Liao",
        "E Rastorgueva",
        "F Grondin",
        "W Aris",
        "H Na",
        "Y Gao",
        "R Mori",
        "Y Bengio"
      ],
      "year": "2021",
      "venue": "SpeechBrain: A generalpurpose speech toolkit",
      "arxiv": "arXiv:2106.04624"
    }
  ]
}