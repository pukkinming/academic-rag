{
  "paper_id": "2108.03064v1",
  "title": "Spatiotemporal Contrastive Learning Of Facial Expressions In Videos",
  "published": "2021-08-06T11:27:06Z",
  "authors": [
    "Shuvendu Roy",
    "Ali Etemad"
  ],
  "keywords": [
    "Self-Supervised Learning",
    "Contrastive Learning",
    "Facial Expressions",
    "Affective Computing"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "We propose a self-supervised contrastive learning approach for facial expression recognition (FER) in videos. We propose a novel temporal sampling-based augmentation scheme to be utilized in addition to standard spatial augmentations used for contrastive learning. Our proposed temporal augmentation scheme randomly picks from one of three temporal sampling techniques: (1) pure random sampling, (2) uniform sampling, and (3) sequential sampling. This is followed by a combination of up to three standard spatial augmentations. We then use a deep R(2+1)D network for FER, which we train in a self-supervised fashion based on the augmentations and subsequently fine-tune. Experiments are performed on the Oulu-CASIA dataset and the performance is compared to other works in FER. The results indicate that our method achieves an accuracy of 89.4%, setting a new state-of-the-art by outperforming other works. Additional experiments and analysis confirm the considerable contribution of the proposed temporal augmentation versus the existing spatial ones.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "The objective of facial expression recognition (FER) is to predict basic expressions such as happy, disgust, neutral, sad, Surprised, and others, from facial images or videos  [1] ,  [2] . Applications of such systems include emotion-aware multimedia and smart devices  [3] , personal mood management  [4] ,  [5] , and others. FER is challenging due to a number of reasons including variations in how different people express different emotions, different lighting or background conditions, and more. Moreover, the intensities with which emotions are expressed throughout the face can vary among subjects or even for the same subject in different contexts. For these reason, there has been an increased effort toward facial expression recognition in recent years from both images  [1] ,  [2]  and videos  [2] ,  [6] ,  [7] .\n\nA variety of different machine learning and computer vision solutions have been proposed for FER  [8] ,  [9] . More recently, however, due to the immense success of deep learning algorithms, deep neural networks (DNN) such as convolutional neural networks (CNN) have been successfully used to perform automatic feature extraction from training data. Nonetheless, deep networks generally need considerable amounts of 'labeled' data to learn effective and robust features.\n\nTo reduce the reliance on output class labels during training, and also to learn better representations, self-supervised learning  [10] -  [12]  has emerged as an effective method to pre-train deep neural networks. Different approaches for self-supervised learning such as contrastive learning techniques like SimCLR  [13]  make use of the input data and create pseudo-labels using augmentations to train the network. Experiments have shown that such approaches can achieve competitive results even without the full datasets.\n\nIn this paper, to take advantage of the desired properties of self-supervised learning, we propose a new video-based FER solution using contrastive learning. Our proposed method, Spatiotemporal Contrastive Learning of Representations (ST-CLR), first uses a novel temporal sampling scheme (temporal augmentation) to pick individual frames from the unlabeled input video to construct a sub-video. This will allow for various sub-videos to be constructed from any given input, resulting in better generalization. It then performs standard spatial augmentations, namely random cropping, color distortion, and random flipping to learn by maximizing the agreement between positive examples (augmentations on the same video) and minimizing the agreement for the negative pairs. A visual representation of our approach is shown in Figure  1 . The figure demonstrates how the sub-video samples are projected onto a normalized embedding space, followed by minimizing of the distance between the positive pairs while 978-1-6654-0019-0/21/$31.00 ©2021 IEEE arXiv:2108.03064v1 [cs.CV] 6 Aug 2021 maximizing the distance between the negative samples vs. the positives. We integrate this technique into a deep neural network that uses a R(2+1)dimensional framework instead of 3D convolutions to learn spatiotemporal representations. We perform extensive experiments and show the strong performance of our method on the Oulu-CASIA dataset  [14] , outperforming all the previous methods.\n\nIn summary, our contributions are as follows.\n\n• We use contrastive self-supervised learning for FER in videos. To the best of our knowledge, this is the first time contrastive learning is used in this context. • We introduce ST-CLR, an extension of the original Sim-CLR that performs temporal augmentations (sampling) followed by spatial augmentations for contrastive learning in videos. • Using the proposed technique, we develop an end-toend R(2+1)D network for video-based FER and perform rigorous experiments on the Oulu-CASIA dataset. The experiments show that our method outperforms existing works and sets a new state-of-the-art on this dataset. The rest of this paper is organized as follows. In the following section we review the related literature on self-supervised learning and video-based FER. This is followed by a detailed description of our proposed solution. Next, the experiment setup and implementation details are described, followed by the experimental results. Lastly in the final section, concluding remarks, limitations, and future work are discussed.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Ii. Related Works",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "A. Self-Supervised Learning",
      "text": "Self-supervised learning has been proposed to reduce the reliance of learning techniques on output labels such as human annotations, by applying specific augmentations  [13]  or transformations  [15] ,  [16]  to input data to generate pseudolabels with which the model is trained. Training of the model using the generated pseudo-labels can be seen as an auxiliary pre-training task, also sometimes referred to as a 'pretext' task. Successive to the pre-training step by means of the pretext tasks, main components of the model, for instance the convolutional blocks in a CNN, are generally frozen and transfer learning is used to develop another model for 'downstream' tasks, i.e. the main aim classification problem  [16] . In this phase, the fully connected layers are often trained from scratch with the frozen convolutional blocks using the original labels in the dataset  [13] . In some cases, fine-tuning of the frozen components can also help achieve a better solution  [17] . This approach has been used on a variety of different problem domains including image analysis  [10] ,  [17] , wearable-based activity recognition  [12] ,  [18] , and affective computing with bio-signals  [19] -  [21] .\n\nIn the context of image analysis, when trained with pretext tasks, the shallower layers of a CNN learn to extract lowlevel general purpose features such as corners, textures, and edges, whereas the deeper layers of the model learn high-level features such as detecting objects and larger parts of scenes  [22] . Recently, a number of different techniques have been proposed for self-supervised learning in images  [17] ,  [23] -  [26] . The novelties in most of these techniques lie in the way with which the pretext tasks are defined. For instance, in one approach, input images were divided into a number of blocks and shuffled, with the pretext task being to find the right order of blocks  [17] ,  [23] . In  [25]  images were distorted with different degrees of rotation, where the pretext task was to predict the amount of rotation that was applied to a distorted image. In  [24]  the pretext task was to colorize an input grayscale image while in  [26]  the task was to fill missing parts of an image that was cropped out.\n\nRecently contrastive learning has emerged as a powerful self-supervised learning technique and has shown great promise and achieved state-of-the-art results for a variety of different tasks  [27] -  [30] . In particular, a more recently proposed contrastive method called SimCLR  [13]  provides a generalized learning framework that does not requires any specialized architecture  [30] ,  [31]  nor any memory bank  [11] ,  [32] ,  [33] . SimCLR uses data augmentation to obtain two representations of the same input which they consider as positive pairs. SimCLR learns the visual representation by maximizing the agreement of positive pairs via a contrastive loss in the latent space.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "B. Self-Supervised Learning In Video",
      "text": "Self-supervised learning with video data often uses the temporal information in the video sequence, for instance tracking the temporal order of the frames of a video  [34] ,  [35] . The intuition here is that the movement of objects in frames can be tracked in a sequence of consecutive video frames, thus the representation of frames that are temporally close in the video must be similar. In  [34] , a self-supervised method was proposed that takes a set of consecutive frames of a video to use as the main input. This was followed by selecting two additional sets from the same video, where the one temporally closest to the main set being designated as the positive set, while the other being designated as the negative set. This approach learned the video representations by enforcing the embeddings obtained from of the two close sub-videos to be similar in the latent space, while forcing the embedding of the negative pair to be far apart in the latent space. In  [35] , the performance of  [34]  was improved by picking the negative sub-video from a completely different video. In  [36] , a contrastive learning method was proposed for videos that, similar to  [34] , uses consecutive frames to form the positive and negative pairs.\n\nAs another approach to performing temporal self-supervised learning, a pretext task was designed to validate whether or not the frames of a video are in the right order successive to performing temporal shuffling  [37] . Using this approach, the performance of video classification on large datasets, for instance for action recognition, was improved. Later, the Odd-One-Out Network  [38]  was proposed with a similar objective.\n\nIn this case, given a few videos, the objective was to identity which video does not have the correct temporal order.\n\nIn order to not explicitly learn sequence information, video colorization  [39]  was proposed for self-supervised learning of videos. In this approach, the objective was to colorize input gray scale videos using a pre-colored reference frame. Unlike colorizing a single image, the temporal coherence in a video needed to be preserved, hence an loss function was designed to keep track of correlated pixels in consecutive frames.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "C. Expression Recognition From Video",
      "text": "For sequence-based  [40] ,  [41]  or video-based  [1] ,  [2]  FER, RNNs such as LSTM networks have been used to learn temporal information along with CNNs that exploit spatial information from the individual video frames  [42] ,  [43] . Alternatively, 3D CNNs have also been explored for videobased FER  [42] ,  [44] . For example, a 3D CNN was used with deformable action parts constraints for dynamic motion encoding in FER called CNN-DAP  [44] . In  [42] , a 3D CNN was used in an ensemble with two 2D CNN models. Unlike 3D CNNs that share weights in the time dimension, in  [45] , a method was proposed that used a sequence of images without weight sharing in the time dimension.\n\nSimilar to the nature of self-supervised training, fully supervised pre-training with additional image or video datasets has been widely explored in the literature. In  [46] , a frame-tosequence method was proposed that used a pre-trained CNN as a feature extractor followed by an RNN to capture the temporal features. In  [1] , a multi-modal framework was proposed that used both image and video -level information with an ensemble of 2D and 3D CNNs. Here the 2D model acts as a pre-trained feature extractor which is followed by a NetVLAD layer  [47]  which aggregates the temporal representation of videos. In  [2] , a generative approach was proposed to pre-train the network with a conditional generative adversarial network  [48] .\n\nFinally, another approach for enhancing performance in video-based FER has been to focus on particular parts of the video sequence where expressions are most prominent  [49] . The difficulty with this method is the lack of knowledge as to when the prominent expressions occur. To tackle this, attention mechanisms can be used to focus on salient parts of sequences, as performed in  [8] ,  [40] ,  [50] .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Iii. Method",
      "text": "The proposed method is a 2 step training process. First, the model is pre-trained by contrastive self-supervised learning. Then the model is fine-tuned for the task of video-based FER. Following we present our method in detail.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "A. Self-Supervised Training",
      "text": "The self-supervised step in our method is inspired by the recent developments in contrastive learning, which aim to maximize the agreement between augmented versions of the same input in latent space using contrastive loss. The proposed method consists of three main components, namely stochastic sampling and augmentation, an encoder, and a projection head.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "1) Sampling And Augmentation Module:",
      "text": "We propose a temporal sampling step prior to spatial augmentation, which can also be viewed as a 'temporal augmentation' operation. This is followed by the standard image-based augmentations used in  [13] . Accordingly, given input video x, we sample two sub-videos x1 and x2 . Next, augmentations are performed on each sub-video to yield xAug Temporal Augmentation. To sample a given input video, we defined 3 sampling strategies: (1) Pure random sampling where n frames are sampled at random from the full length video, then sorted according to the order in the original video;\n\n(2) Uniform sampling where frames are sampled uniformly starting and ending with the first and last frames of the input while maintaining uniform distance in between consecutive sampled frames; (3) Sequential sampling where an initial frame is randomly picked, followed by the following sequence of n-1 frames. To perform the temporal augmentation step in our method, we randomly choose between the above strategies every time. Therefore, as we sample the sub-videos in a different way at each iteration, the positive pairs not only see different spatial augmentations of the same input video, but also sees a completely different sub-video. This results in better generalization and robustness to temporal variations. Figure  2  (top) depicts the temporal augmentation step where two sub-videos are sampled using the first sampling strategy.\n\nSpatial Augmentation. Successive to temporal sampling, we apply the following augmentations to the entire sub-video sequences to obtain xAug i . (1) Random resized crop. First, random cropping is applied to a given sub-video. Here, a frame size and position is picked at random, then all the frames of that sub-video are cropped at the same position. Finally, the video sequence is resized to the desired input shape of the CNN model. (2) Random flip. Finally, we randomly choose whether to horizontally flip all the frames in a sub-video or not. (3) Random color distortion. This augmentation includes modifying brightness, contrast, saturation, and hue of the subvideos. Here, the distortion parameters are picked randomly and applied to all the frames of the sub-video. It should be noted that unlike the temporal augmentation step where only one of the sampling strategies is picked for each sub-video, for spatial augmentation, all three operations are performed together. Nonetheless, as the parameters are chosen at random, zeros could be chosen for the parameters of a certain spatial augmentation, effectively not perform it. Figure  2 (bottom)  shows examples of spatial augmentation operations performed on a sample frame.\n\n2) Encoder Module f (x): We use an encoder block (a simple CNN) to transform the input video into a latent representation. A very common way for modeling 4-dimensional data such as videos is using 3D convolutional neural networks  [42] ,  [44] ,  [51] . This strategy allows for both spatial and tem- poral dimensions to be directly learned. However, 3D CNNs have a large number of parameters, requiring significantly more data and processing power to train. To address this drawback, an approach was proposed to factorize the 3D CNNs  [52] , breaking down the 3D convolutions into a 2D operation followed by a 1D operation. The resulting model with residual connections  [53]  was called R(2+1)D  [52] . There are a couple of advantages to this model. First, the factorized operations contain a non-linear function in between the layers, resulting in more non-linearity in the network, which is a desired property. Second, the factorization helps the optimizer during training by lowering the training loss and training time.\n\nIt was also shown in  [52]  that factorizing the 3D convolution into two distinct spatial and temporal operations improves the performance of learning in large scale video classification tasks. In our paper, we use this concept in the encoder block of the proposed network. This notion is depicted in Figure  3 .  the encoder model using the (2+1)D blocks is laid out in Table  I . Lastly, we represent the output embedding of the encoder block as\n\n3) Projection head g(x): Following the intuition of  [13] , we use a projection head g(x) consisting of 2 dense layers, with a ReLU non-linearity in between, which takes the output of f (x) as input and outputs the final latent embedding. Experiments in  [13]  showed improvements in the learnt representation by including such a non-linear learnable transformation. The final embedding is represented as z i = g(h i ) on which the contrastive loss is applied.\n\n4) Contrastive loss: We use a contrastive loss to train the proposed self-supervised network. The loss is designed to learn visual representation from seeing positive and negative pairs. Let's define the similarity between the embedding z i and z j (the outputs from the encoder) as cosine(z i , z j ) = z T i z j /||z i ||.||z j ||. If i and j are positive examples, then the contrastive loss function is defined as:\n\nwhere\n\nHere τ represents a temperature parameter which scales the cosine similarity. In some previous works such as  [13] , loss function defined above is termed normalized temperature-scaled cross entropy loss. Figure  5  depicts a visual illustration of the proposed ST-CLR method.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "B. Model Fine-Tuning",
      "text": "After pre-training the model with self-supervised learning, we no longer use the projection head at the end of the encoder block. During the supervised fine-tuning phase, we drop the projection head and add a single-layer linear transformation for generating the output probability. This fine-tuning is performed with categorical cross-entropy loss.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Iv. Experiments And Results",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "A. Experiments",
      "text": "In this section we present the experiments and the results for the proposed method on video-based FER. We conduct the experiments on a popular video FER dataset called Oulu-CASIA  [14] .\n\nDataset. The Oulu-CASIA  [14]  is a facial expression recognition dataset that contains 6 expressions: Happy, Sad, Surprised, Angry, Fear, and Disgust. This dataset was collected from 80 people aging from 23 to 58. Among them 73.8% were male and 26.2% were female. The dataset was collected in a lab environment where the subjects were asked to sit about 60 cm away from the camera. The videos were collected in 25 frames per second at a spatial resolution of 320 × 240.\n\nImplementation Details As mentioned in the method section, we use the R(2+1)D architecture for the encoder of our method. At the end of the encoder we have a projection head, which is a 2-layer network. The output of the projection head is a 128 dimensional vector, on which the contrastive loss is calculated. The model is trained with an SGD optimizer for 1000 epochs with a momentum of 0.9, a learning rate of 0.001, and weight decay of 1e-4. Oue solution was implemented in PyTorch and trained using 4 NVIDIA V100 GPUs. The model takes sub-videos with 16 frames, and spatial resolution of 224 × 224 as its inputs.\n\nIn the fine-tuning step, we train the final model for 100 epochs. First, we train the newly initialized linear layer for 30 epoch and then fine-tune the entire model for 70 epochs. The model is trained using Adam optimizer with a learning rate of 1e-4 and plateau learning rate decay with a patience of 3.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "B. Results",
      "text": "We train the proposed method on Oulu-CASIA dataset. To compare our method with previous works on this dataset, we have conducted all the experiments with 10-fold crossvalidation. Table  II  compares the result of our solution with previous methods on the Oulu-CASIA dataset. Our ST-CLR method performs the best among all the methods in this comparison. The confusion matrix depicted in Figure  6  shows that our method performs the best on classifying 'disgust' and also performs well on expressions 'fear' and 'happy'. We observe the least effective performance on 'angry' and 'sad'. The figure shows that 'Angry' is often confused with 'Disgust' while 'Sad' is generally confused with 'Fear' and 'Surprised'. Lastly, 'Surprised' is mostly confused with 'Fear' and 'Angry'. This analysis points to relatively reasonable mistakes by the model.\n\nNext, we conduct ablation studies on the main elements of the proposed method. As discussed earlier the proposed method requires a CNN backbone as the encoder suitable for 4 dimensional input video data, for which we used R(2+1)D. We conduct experiments by replacing this backbone with two popular alternative encoder architectures, namely a CNN with 3D convolutions and a CNN with mixed 2D and 3D convolutions. For the 3D convolutional network, we replace all the R(2+1)D blocks mentioned in Table I with 3D blocks. As for the mixed 3D-2D alternative, 3D blocks are used for all the convolutions in the ConvBlock 1 and 2D blocks are used    for rest of the network. Table  III  shows the accuracy of our model versus its variants when the alternative networks are used as the encoder block. As we can see, R(2+1)D performs better than other networks, confirming our choice of using the R(2+1)D architecture. Another important component of contrastive self-supervised leaning is the use of augmentations. One of the novelties in our proposed method is the use of frame sampling -based temporal augmentations along with popular spatial augmentations. Table  IV  shows the ablation study on spatial and temporal augmentations used in our method, where the impact of each augmentation is investigated by systematic exclusion. We see that the temporal augmentation clearly has the highest impact on the final performance as removing this augmentation module reduces the accuracy of the model by about 4%. Among the spatial augmentations, 'random color distortion' shows the highest impact, which is in conformity with other works on contrastive learning  [13] .\n\nIt was shown in previous works such as  [13]  that selfsupervised learning usually benefits from longer training compared to fully supervised learning. In Table  V , we show the impact of different number of epochs for pre-training our model. We find that the accuracy improves significantly when the number of epochs increases from 100 to 500. However, only slight improvements are achieved when training it for another 500 epochs (for a total of 1000 epochs). So, for our final model, we pre-train the model for 1000 epochs.\n\nFinally, to show the robustness of our model, we conduct experiments with partially labeled data. Table  VI  shows the accuracy of the model with different portions of the dataset being labeled and used for fine-tuning. We find that, using 75% and 50% data drops the accuracy by only 2% and 6% respectively. Even with only 25% of the data being labeled, the accuracy is better than few of the previous works shown in Table  II . This indicates that our model is robust to having fewer labeled data, which can be attributed to the self-supervised training strategy used.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "V. Conclusion And Future Work",
      "text": "This paper presents a self-supervised approach for learning facial expressions in video. Our method uses a R(2+1)D style architecture to avoid using 3D convolutions for faster and better performance. To perform self-supervised training for our model, we propose a novel temporal augmentation scheme along with standard spatial (frame-based) augmentations. We test our solution on the Oulu-CASIA dataset and perform rigorous experiments. Our experiments show that the proposed method outperforms existing work on this dataset to set a new state-of-the-art. Extensive experiments were conducted to show the importance of each of the utilized augmentations toward the final performance of the model, highlighting the importance of our proposed temporal augmentation. Additional experiments were performed which validated our choice of R(2+1)D convolutions versus 3D and hybrid approached. Lastly, our analysis shows the advantage and robustness of our self-supervised method when significant portions of output labels were not used for training.\n\nFor future work, the proposed temporal augmentation scheme can be used for contrastive self-supervised learning in other video-based representation learning tasks such as action recognition, pose estimation, object detection and tracking, and others. Accordingly, additional modifications may need to be made to the network architecture and hyper-parameters in order to obtain optimum performance in other domains and with other datasets.",
      "page_start": 7,
      "page_end": 7
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Contrastive self-supervised learning using various augmentations. Here",
      "page": 1
    },
    {
      "caption": "Figure 1: The ﬁgure demonstrates how the sub-video samples",
      "page": 1
    },
    {
      "caption": "Figure 2: (top) depicts the temporal augmentation step where",
      "page": 3
    },
    {
      "caption": "Figure 2: An example of the temporal and spatial augmentations used in ST-CLR are depicted.",
      "page": 4
    },
    {
      "caption": "Figure 3: Factorization of 3D convolutional block into (2+1)D.",
      "page": 4
    },
    {
      "caption": "Figure 3: Conv block2",
      "page": 4
    },
    {
      "caption": "Figure 4: The R(2+1)D encoder network of the proposed method.",
      "page": 4
    },
    {
      "caption": "Figure 3: The encoder with all the CNNs and residual",
      "page": 4
    },
    {
      "caption": "Figure 4: and the full architecture of",
      "page": 4
    },
    {
      "caption": "Figure 5: Visual illustration of our proposed method, where the agreement between different augmented sub-videos is maximized in the latent space.",
      "page": 5
    },
    {
      "caption": "Figure 5: depicts a visual",
      "page": 5
    },
    {
      "caption": "Figure 6: Confusion matrix of the prediction of the model on Oulu-CASIA",
      "page": 6
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Conv Block1": "Conv Block2",
          "[45, 16, 112, 112]\n[64, 16, 112, 112]\n(cid:111)\n[144, 16, 112, 112]\n×4\n[64, 16, 112, 112]": "[230, 16, 56, 56]\n[128, 8, 56, 56]\n[230, 8, 56, 56]\n[128, 8, 56, 56]",
          "45,\n[1, 7, 7]\n64,\n[3, 1, 1]\n144,\n[1, 3, 3]\n144,\n[3, 1, 1]": "230,\n[1, 3, 3]\n128,\n[3, 1, 1]\n230,\n[1, 3, 3]\n128,\n[3, 1, 1]"
        },
        {
          "Conv Block1": "Residual\nlayer1",
          "[45, 16, 112, 112]\n[64, 16, 112, 112]\n(cid:111)\n[144, 16, 112, 112]\n×4\n[64, 16, 112, 112]": "[128, 8, 56, 56]",
          "45,\n[1, 7, 7]\n64,\n[3, 1, 1]\n144,\n[1, 3, 3]\n144,\n[3, 1, 1]": "128,\n[1, 1, 1]"
        },
        {
          "Conv Block1": "Conv Block3",
          "[45, 16, 112, 112]\n[64, 16, 112, 112]\n(cid:111)\n[144, 16, 112, 112]\n×4\n[64, 16, 112, 112]": "(cid:111)\n[288, 8, 56, 56]\n×2\n[128, 8, 56, 56]",
          "45,\n[1, 7, 7]\n64,\n[3, 1, 1]\n144,\n[1, 3, 3]\n144,\n[3, 1, 1]": "288,\n[1, 3, 3]\n128,\n[3, 1, 1]"
        },
        {
          "Conv Block1": "Conv Block4",
          "[45, 16, 112, 112]\n[64, 16, 112, 112]\n(cid:111)\n[144, 16, 112, 112]\n×4\n[64, 16, 112, 112]": "[460, 8, 28, 28]\n[256, 4, 28, 28]\n[460, 4, 28, 28]\n[256, 4, 28, 28]",
          "45,\n[1, 7, 7]\n64,\n[3, 1, 1]\n144,\n[1, 3, 3]\n144,\n[3, 1, 1]": "460,\n[1, 3, 3]\n256,\n[3, 1, 1]\n460,\n[1, 3, 3]\n256,\n[3, 1, 1]"
        },
        {
          "Conv Block1": "Residual\nlayer2",
          "[45, 16, 112, 112]\n[64, 16, 112, 112]\n(cid:111)\n[144, 16, 112, 112]\n×4\n[64, 16, 112, 112]": "[256, 4, 28, 28]",
          "45,\n[1, 7, 7]\n64,\n[3, 1, 1]\n144,\n[1, 3, 3]\n144,\n[3, 1, 1]": "256,\n[1, 1, 1]"
        },
        {
          "Conv Block1": "Conv Block5",
          "[45, 16, 112, 112]\n[64, 16, 112, 112]\n(cid:111)\n[144, 16, 112, 112]\n×4\n[64, 16, 112, 112]": "(cid:111)\n[576, 4, 28, 28]\n×2\n[256, 4, 28, 28]",
          "45,\n[1, 7, 7]\n64,\n[3, 1, 1]\n144,\n[1, 3, 3]\n144,\n[3, 1, 1]": "576,\n[1, 3, 3]\n256,\n[3, 1, 1]"
        },
        {
          "Conv Block1": "Conv Block6",
          "[45, 16, 112, 112]\n[64, 16, 112, 112]\n(cid:111)\n[144, 16, 112, 112]\n×4\n[64, 16, 112, 112]": "[921, 4, 14, 14]\n[512, 2, 14, 14]\n[921, 2, 14, 14]\n[512, 2, 14, 14]",
          "45,\n[1, 7, 7]\n64,\n[3, 1, 1]\n144,\n[1, 3, 3]\n144,\n[3, 1, 1]": "921,\n[1, 3, 3]\n512,\n[3, 1, 1]\n921,\n[1, 3, 3]\n512,\n[3, 1, 1]"
        },
        {
          "Conv Block1": "Residual\nlayer3",
          "[45, 16, 112, 112]\n[64, 16, 112, 112]\n(cid:111)\n[144, 16, 112, 112]\n×4\n[64, 16, 112, 112]": "[512, 2, 14, 14]",
          "45,\n[1, 7, 7]\n64,\n[3, 1, 1]\n144,\n[1, 3, 3]\n144,\n[3, 1, 1]": "512,\n[1, 1, 1]"
        },
        {
          "Conv Block1": "Conv Block7",
          "[45, 16, 112, 112]\n[64, 16, 112, 112]\n(cid:111)\n[144, 16, 112, 112]\n×4\n[64, 16, 112, 112]": "(cid:111)\n[1152, 2, 14, 14]\n×2\n[512, 2, 14, 14]",
          "45,\n[1, 7, 7]\n64,\n[3, 1, 1]\n144,\n[1, 3, 3]\n144,\n[3, 1, 1]": "1152,\n[1, 3, 3]\n512,\n[3, 1, 1]"
        },
        {
          "Conv Block1": "Ada. Ave. Pool",
          "[45, 16, 112, 112]\n[64, 16, 112, 112]\n(cid:111)\n[144, 16, 112, 112]\n×4\n[64, 16, 112, 112]": "[512, 1, 1, 1]",
          "45,\n[1, 7, 7]\n64,\n[3, 1, 1]\n144,\n[1, 3, 3]\n144,\n[3, 1, 1]": ""
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Conv Block 6",
      "venue": "Conv Block 6"
    },
    {
      "citation_id": "2",
      "title": "Ave. Pool [512, 1, 1, 1] REFERENCES",
      "venue": "Ave. Pool [512, 1, 1, 1] REFERENCES"
    },
    {
      "citation_id": "3",
      "title": "Modeling multimodal cues in a deep learning-based framework for emotion recognition in the wild",
      "authors": [
        "S Pini",
        "O Ahmed",
        "M Cornia",
        "L Baraldi",
        "R Cucchiara",
        "B Huet"
      ],
      "year": "2017",
      "venue": "Proceedings of the 19th ACM International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "4",
      "title": "Facial expression recognition by deexpression residue learning",
      "authors": [
        "H Yang",
        "U Ciftci",
        "L Yin"
      ],
      "year": "2018",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "5",
      "title": "Instant stress: detection of perceived mental stress through smartphone photoplethysmography and thermal imaging",
      "authors": [
        "Y Cho",
        "S Julier",
        "N Bianchi-Berthouze"
      ],
      "year": "2019",
      "venue": "JMIR Mental Health"
    },
    {
      "citation_id": "6",
      "title": "Mood recognition based on upper body posture and movement features",
      "authors": [
        "M Thrasher",
        "M Van Der Zwaag",
        "N Bianchi-Berthouze",
        "J Westerink"
      ],
      "year": "2011",
      "venue": "International Conference on Affective Computing and Intelligent Interaction"
    },
    {
      "citation_id": "7",
      "title": "Inferring mood in ubiquitous conversational video",
      "authors": [
        "D Sanchez-Cortes",
        "J.-I Biel",
        "S Kumano",
        "J Yamato",
        "K Otsuka",
        "D Gatica-Perez"
      ],
      "year": "2013",
      "venue": "Proceedings of the 12th International Conference on Mobile and Ubiquitous Multimedia"
    },
    {
      "citation_id": "8",
      "title": "Facial expression recognition based on deep convolution long short-term memory networks of doublechannel weighted mixture",
      "authors": [
        "H Zhang",
        "B Huang",
        "G Tian"
      ],
      "year": "2020",
      "venue": "Pattern Recognition Letters"
    },
    {
      "citation_id": "9",
      "title": "Facenet2expnet: Regularizing a deep face recognition net for expression recognition",
      "authors": [
        "H Ding",
        "S Zhou",
        "R Chellappa"
      ],
      "year": "2017",
      "venue": "12th IEEE International Conference on Automatic Face & Gesture Recognition"
    },
    {
      "citation_id": "10",
      "title": "Attention mechanismbased cnn for facial expression recognition",
      "authors": [
        "J Li",
        "K Jin",
        "D Zhou",
        "N Kubota",
        "Z Ju"
      ],
      "year": "2020",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "11",
      "title": "Patch attention layer of embedding handcrafted features in cnn for facial expression recognition",
      "authors": [
        "X Liang",
        "L Xu",
        "J Liu",
        "Z Liu",
        "G Cheng",
        "J Xu",
        "L Liu"
      ],
      "year": "2021",
      "venue": "Sensors"
    },
    {
      "citation_id": "12",
      "title": "Self-supervised learning for medical image analysis using image context restoration",
      "authors": [
        "L Chen",
        "P Bentley",
        "K Mori",
        "K Misawa",
        "M Fujiwara",
        "D Rueckert"
      ],
      "year": "2019",
      "venue": "Medical Image Analysis"
    },
    {
      "citation_id": "13",
      "title": "Self-supervised learning of pretextinvariant representations",
      "authors": [
        "I Misra",
        "L Maaten"
      ],
      "year": "2020",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "14",
      "title": "Self-supervised wearable-based activity recognition by learning to forecast motion",
      "authors": [
        "S Taghanaki",
        "A Etemad"
      ],
      "year": "2020",
      "venue": "Self-supervised wearable-based activity recognition by learning to forecast motion"
    },
    {
      "citation_id": "15",
      "title": "A simple framework for contrastive learning of visual representations",
      "authors": [
        "T Chen",
        "S Kornblith",
        "M Norouzi",
        "G Hinton"
      ],
      "year": "2020",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "16",
      "title": "Facial expression recognition from near-infrared videos",
      "authors": [
        "G Zhao",
        "X Huang",
        "M Taini",
        "S Li",
        "M Pietikäinen"
      ],
      "year": "2005",
      "venue": "Image and Vision Computing"
    },
    {
      "citation_id": "17",
      "title": "Representation learning by learning to count",
      "authors": [
        "M Noroozi",
        "H Pirsiavash",
        "P Favaro"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE International Conference on Computer Vision"
    },
    {
      "citation_id": "18",
      "title": "Discriminative unsupervised feature learning with exemplar convolutional neural networks",
      "authors": [
        "A Dosovitskiy",
        "P Fischer",
        "J Springenberg",
        "M Riedmiller",
        "T Brox"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "19",
      "title": "Unsupervised learning of visual representations by solving jigsaw puzzles",
      "authors": [
        "M Noroozi",
        "P Favaro"
      ],
      "year": "2016",
      "venue": "European Conference on Computer Vision"
    },
    {
      "citation_id": "20",
      "title": "Multi-task self-supervised learning for human activity detection",
      "authors": [
        "A Saeed",
        "T Özc",
        "J Lukkien"
      ],
      "year": "2019",
      "venue": "Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies"
    },
    {
      "citation_id": "21",
      "title": "Self-supervised learning for ecg-based emotion recognition",
      "authors": [
        "P Sarkar",
        "A Etemad"
      ],
      "year": "2020",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "22",
      "title": "Detection of maternal and fetal stress from ecg with self-supervised representation learning",
      "authors": [
        "P Sarkar",
        "S Lobmaier",
        "B Fabre",
        "G Berg",
        "A Mueller",
        "M Frasch",
        "M Antonelli",
        "A Etemad"
      ],
      "year": "2020",
      "venue": "Detection of maternal and fetal stress from ecg with self-supervised representation learning",
      "arxiv": "arXiv:2011.02000"
    },
    {
      "citation_id": "23",
      "title": "Self-supervised ecg representation learning for emotion recognition",
      "authors": [
        "P Sarkar",
        "A Etemad"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "24",
      "title": "Network dissection: Quantifying interpretability of deep visual representations",
      "authors": [
        "D Bau",
        "B Zhou",
        "A Khosla",
        "A Oliva",
        "A Torralba"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "25",
      "title": "Unsupervised visual representation learning by context prediction",
      "authors": [
        "C Doersch",
        "A Gupta",
        "A Efros"
      ],
      "year": "2015",
      "venue": "Proceedings of the IEEE International Conference on Computer Vision"
    },
    {
      "citation_id": "26",
      "title": "Colorful image colorization",
      "authors": [
        "R Zhang",
        "P Isola",
        "A Efros"
      ],
      "year": "2016",
      "venue": "European Conference on Computer Vision"
    },
    {
      "citation_id": "27",
      "title": "Unsupervised representation learning by predicting image rotations",
      "authors": [
        "S Gidaris",
        "P Singh",
        "N Komodakis"
      ],
      "year": "2018",
      "venue": "Unsupervised representation learning by predicting image rotations",
      "arxiv": "arXiv:1803.07728"
    },
    {
      "citation_id": "28",
      "title": "Context encoders: Feature learning by inpainting",
      "authors": [
        "D Pathak",
        "P Krahenbuhl",
        "J Donahue",
        "T Darrell",
        "A Efros"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "29",
      "title": "Dimensionality reduction by learning an invariant mapping",
      "authors": [
        "R Hadsell",
        "S Chopra",
        "Y Lecun"
      ],
      "year": "2006",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "30",
      "title": "Discriminative unsupervised feature learning with convolutional neural networks",
      "authors": [
        "A Dosovitskiy",
        "J Springenberg",
        "M Riedmiller",
        "T Brox"
      ],
      "year": "2014",
      "venue": "Proceedings of the 27th International Conference on Neural Information Processing Systems"
    },
    {
      "citation_id": "31",
      "title": "Representation learning with contrastive predictive coding",
      "authors": [
        "A Oord",
        "Y Li",
        "O Vinyals"
      ],
      "year": "2018",
      "venue": "Representation learning with contrastive predictive coding",
      "arxiv": "arXiv:1807.03748"
    },
    {
      "citation_id": "32",
      "title": "Learning representations by maximizing mutual information across views",
      "authors": [
        "P Bachman",
        "R Hjelm",
        "W Buchwalter"
      ],
      "year": "2019",
      "venue": "Learning representations by maximizing mutual information across views",
      "arxiv": "arXiv:1906.00910"
    },
    {
      "citation_id": "33",
      "title": "Data-efficient image recognition with contrastive predictive coding",
      "authors": [
        "O Henaff"
      ],
      "year": "2020",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "34",
      "title": "Unsupervised feature learning via non-parametric instance discrimination",
      "authors": [
        "Z Wu",
        "Y Xiong",
        "S Yu",
        "D Lin"
      ],
      "year": "2018",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "35",
      "title": "Momentum contrast for unsupervised visual representation learning",
      "authors": [
        "K He",
        "H Fan",
        "Y Wu",
        "S Xie",
        "R Girshick"
      ],
      "year": "2020",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "36",
      "title": "Unsupervised learning of visual representations using videos",
      "authors": [
        "X Wang",
        "A Gupta"
      ],
      "year": "2015",
      "venue": "Proceedings of the IEEE International Conference on Computer Vision"
    },
    {
      "citation_id": "37",
      "title": "Facenet: A unified embedding for face recognition and clustering",
      "authors": [
        "F Schroff",
        "D Kalenichenko",
        "J Philbin"
      ],
      "year": "2015",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "38",
      "title": "Spatiotemporal contrastive video representation learning",
      "authors": [
        "R Qian",
        "T Meng",
        "B Gong",
        "M.-H Yang",
        "H Wang",
        "S Belongie",
        "Y Cui"
      ],
      "year": "2020",
      "venue": "Spatiotemporal contrastive video representation learning",
      "arxiv": "arXiv:2008.03800"
    },
    {
      "citation_id": "39",
      "title": "Shuffle and learn: unsupervised learning using temporal order verification",
      "authors": [
        "I Misra",
        "C Zitnick",
        "M Hebert"
      ],
      "year": "2016",
      "venue": "European Conference on Computer Vision"
    },
    {
      "citation_id": "40",
      "title": "Self-supervised video representation learning with odd-one-out networks",
      "authors": [
        "B Fernando",
        "H Bilen",
        "E Gavves",
        "S Gould"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "41",
      "title": "Tracking emerges by colorizing videos",
      "authors": [
        "C Vondrick",
        "A Shrivastava",
        "A Fathi",
        "S Guadarrama",
        "K Murphy"
      ],
      "year": "2018",
      "venue": "Proceedings of the European Conference on Computer Vision"
    },
    {
      "citation_id": "42",
      "title": "Facial emotion recognition using light field images with deep attention-based bidirectional lstm",
      "authors": [
        "A Sepas-Moghaddam",
        "A Etemad",
        "F Pereira",
        "P Correia"
      ],
      "year": "2020",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "43",
      "title": "Capsfield: Light field-based face and expression recognition in the wild using capsule routing",
      "authors": [
        "A Sepas-Moghaddam",
        "A Etemad",
        "F Pereira",
        "P Correia"
      ],
      "year": "2021",
      "venue": "Capsfield: Light field-based face and expression recognition in the wild using capsule routing"
    },
    {
      "citation_id": "44",
      "title": "Multiple spatio-temporal feature learning for video-based emotion recognition in the wild",
      "authors": [
        "C Lu",
        "W Zheng",
        "C Li",
        "C Tang",
        "S Liu",
        "S Yan",
        "Y Zong"
      ],
      "year": "2018",
      "venue": "Proceedings of the 20th ACM International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "45",
      "title": "Lstm for dynamic emotion and group emotion recognition in the wild",
      "authors": [
        "B Sun",
        "Q Wei",
        "L Li",
        "Q Xu",
        "J He",
        "L Yu"
      ],
      "year": "2016",
      "venue": "Proceedings of the 18th ACM International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "46",
      "title": "Deeply learning deformable facial action parts model for dynamic expression analysis",
      "authors": [
        "M Liu",
        "S Li",
        "S Shan",
        "R Wang",
        "X Chen"
      ],
      "year": "2014",
      "venue": "Asian Conference on Computer Vision"
    },
    {
      "citation_id": "47",
      "title": "Joint fine-tuning in deep neural networks for facial expression recognition",
      "authors": [
        "H Jung",
        "S Lee",
        "J Yim",
        "S Park",
        "J Kim"
      ],
      "year": "2015",
      "venue": "Proceedings of the IEEE International Conference on Computer Vision"
    },
    {
      "citation_id": "48",
      "title": "A compact deep learning model for robust facial expression recognition",
      "authors": [
        "C.-M Kuo",
        "S.-H Lai",
        "M Sarkis"
      ],
      "year": "2018",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops"
    },
    {
      "citation_id": "49",
      "title": "Netvlad: Cnn architecture for weakly supervised place recognition",
      "authors": [
        "R Arandjelovic",
        "P Gronat",
        "A Torii",
        "T Pajdla",
        "J Sivic"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "50",
      "title": "Generative adversarial networks",
      "authors": [
        "I Goodfellow",
        "J Pouget-Abadie",
        "M Mirza",
        "B Xu",
        "D Warde-Farley",
        "S Ozair",
        "A Courville",
        "Y Bengio"
      ],
      "year": "2014",
      "venue": "Generative adversarial networks",
      "arxiv": "arXiv:1406.2661"
    },
    {
      "citation_id": "51",
      "title": "Peak-piloted deep network for facial expression recognition",
      "authors": [
        "X Zhao",
        "X Liang",
        "L Liu",
        "T Li",
        "Y Han",
        "N Vasconcelos",
        "S Yan"
      ],
      "year": "2016",
      "venue": "European Conference on Computer Vision"
    },
    {
      "citation_id": "52",
      "title": "Classification of hand movements from eeg using a deep attention-based lstm network",
      "authors": [
        "G Zhang",
        "V Davoodnia",
        "A Sepas-Moghaddam",
        "Y Zhang",
        "A Etemad"
      ],
      "year": "2019",
      "venue": "IEEE Sensors Journal"
    },
    {
      "citation_id": "53",
      "title": "Learning spatiotemporal features with 3d convolutional networks",
      "authors": [
        "D Tran",
        "L Bourdev",
        "R Fergus",
        "L Torresani",
        "M Paluri"
      ],
      "year": "2015",
      "venue": "Proceedings of the IEEE International Conference on Computer Vision"
    },
    {
      "citation_id": "54",
      "title": "A closer look at spatiotemporal convolutions for action recognition",
      "authors": [
        "D Tran",
        "H Wang",
        "L Torresani",
        "J Ray",
        "Y Lecun",
        "M Paluri"
      ],
      "year": "2018",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "55",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "K He",
        "X Zhang",
        "S Ren",
        "J Sun"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "56",
      "title": "Dynamic texture recognition using local binary patterns with an application to facial expressions",
      "authors": [
        "G Zhao",
        "M Pietikainen"
      ],
      "year": "2007",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "57",
      "title": "Learning expressionlets on spatio-temporal manifold for dynamic facial expression recognition",
      "authors": [
        "M Liu",
        "S Shan",
        "R Wang",
        "X Chen"
      ],
      "year": "2014",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "58",
      "title": "Dynamic facial expression recognition using longitudinal facial expression atlases",
      "authors": [
        "Y Guo",
        "G Zhao",
        "M Pietikäinen"
      ],
      "year": "2012",
      "venue": "European Conference on Computer Vision"
    },
    {
      "citation_id": "59",
      "title": "Deeper cascaded peak-piloted network for weak expression recognition",
      "authors": [
        "Z Yu",
        "Q Liu",
        "G Liu"
      ],
      "year": "2018",
      "venue": "The Visual Computer"
    },
    {
      "citation_id": "60",
      "title": "Facial expression recognition based on deep evolutional spatial-temporal networks",
      "authors": [
        "K Zhang",
        "Y Huang",
        "Y Du",
        "L Wang"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Image Processing"
    }
  ]
}