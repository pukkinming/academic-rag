{
  "paper_id": "2501.16341v1",
  "title": "A Data-Driven Approach To Spoken Dialog Segmentation",
  "published": "2025-01-14T11:15:16Z",
  "authors": [
    "D. Griol",
    "A. Sanchis",
    "J. M. Molina",
    "Z. Callejas"
  ],
  "keywords": [
    "Domain Knowledge Acquisition",
    "Dialog Structure Annotation",
    "Conversational Interfaces",
    "Human-machine Interaction",
    "Spoken Interaction"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "In this paper, we present a statistical model for spoken dialog segmentation that decides the current phase of the dialog by means of an automatic classification process. We have applied our proposal to three practical conversational systems acting in different domains. The results of the evaluation show that is possible to attain high accuracy rates in dialog segmentation when using different sources of information to represent the user input. The statistical model developed with human-machine dialog corpora has been applied to human-human conversations and provides a good baseline as well insights in the model limitation.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Speech and natural language technologies allow users to communicate in a flexible and efficient manner, making possible to access applications in which traditional input interfaces cannot be used (e.g. in-car applications, access for disabled persons, etc)  [1] . Also speech-based interfaces work seamlessly with small devices (e.g., smartphones and tablets PCs) and allow users to easily interact with robotic agents (e.g., Pepper 1 ), invoke local applications or access remote information by means of enhanced devices and advanced conversational interfaces (e.g., Amazon Echo 2  and Google Home 3  ).\n\nNatural human-computer interaction is a complex problem that requires work on multiple levels, such as automatic speech recognition (ASR), spoken language understanding (SLU), dialog management, and speech synthesis  [1, 2] . In recent years, the dialog systems community has focused on developing processing algorithms that can profit from the vast quantities of dialog corpus data that are generated every day. In this field, a machine learning techniques reduce human effort in the knowledge engineering process and development of a new conversational system.\n\nDialog segmentation can be defined as the process of dividing a dialog by one into (sub-)tasks or phases identifying discourse boundaries using cues (e.g. speaker's intention, topic flow, coherence structure, cohesive devices, etc.). The objective is to detect sequences of turns that accomplish a specific objective inside the dialog flow. These are processed combining different kinds of features provided by the automatic speech recognition and spoken language understanding modules, such as semantic similarities, inter-sentence similarities, entity repetition, word frequency, linguistic features, and prosodic and acoustic characteristics  [3] .\n\nThe identification of the tasks/phases is useful to develop dynamic and useradapted conversational interfaces. Modeling subdialog structures adapted to each specific task is useful to create extensible interfaces with reusable components that can be plugged to more complex tasks. For example, for excerpts of the dialog that are not strictly related to the application domain (greetings, error recovery, etc.).\n\nThe dialog segmentation phase can be performed before dialog management as shown in Figure  1 . The dialog manager relies on the estimated dialog task to choose the next system actions.\n\nThere is also a wide range of natural language processing applications for which discourse segmentation is relevant. For instance, Angheluta, Busser and Moens adapted a three-step segmentation algorithm for automatic text summarization  [4] . Their algorithm uses generic topical cues for detecting the thematic structure of a text using synonymy to reduce the vocabulary.\n\nDifferent proposals apply discourse segmentation to segment text into different fragments in a preprocessing phase of information retrieval and question-answering systems to improve their operation  [5, 6] . Walker applies this kind of techniques for anaphora resolution  [7] . Finally, different studies show the benefits of using discourse segmentation for question answering tasks in order to take into account Figure  1 : Main modules of a spoken dialog system the context for the interpretation and answer questions  [8] , and also for dialog acts segmentation and classification  [9] .\n\nIn this paper, we describe a machine learning approach for the automatic segmentation of spoken dialogs. Our methodology decides the current phase of the dialog by means of a classification process that considers the complete history of the dialog, which is one of the main advantages regarding the previously described statistical methodologies. Another main characteristic is the inclusion of a data structure that stores the information provided by the user. The main objective of this structure is to easily encode the complete information related to the task provided by the user during the dialog history.\n\nThe remainder of the paper is as follows. Section 2 reviews related approaches to dialog structure modeling and segmentation. Section 3 presents our statistical proposal for automatic segmentation of spoken dialogs. Section 4 describes the practical application of this proposal for three different application domains. In Section 5 we discuss the evaluation results. Finally, in Section 6 we present the conclusions and outline guidelines for future work.",
      "page_start": 1,
      "page_end": 3
    },
    {
      "section_name": "Related Work",
      "text": "A number of theories to model discourse structure have been proposed, which are focused on different aspects of dialogs related to the assumptions they made about human-human conversations influenced by many other fields of study including psychology, sociology, philosophy, and computer science.\n\nMost dialog structure models agree that discourse has a compositional structure (i.e., it can be divided into coherent segments that have relationships with each other)  [3, 10, 11] . These segments and their relationships constitute the structure of discourse. These two elements can be seen from two different perspectives: an informative perspective and an intentional perspective. The former captures the real content transmitted, that can be modeled by its superficial or semantic representations. The latter captures the speaker's intention behind each statement and the general objective of a discourse.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Informational-Oriented Discourse Structures",
      "text": "The Discourse Representation Theory (DRT)  [12]  represents the meaning of a text (Discourse Representation Structure, DRS) by means of a logical language similar to the first-order predicate logic, which is extended to represent the context considering the preceding sentences. However, the proposed representation is not focused on the compositional structure of the discourse.\n\nIn the Linguistic Discourse Model (LDM)  [13] , the semantic representation is based on Discourse Parse Trees (DPT) that consider the structural relations and the relations among the contextual categories to capture both propositional content and discourse contexts. Each node in this tree (Discourse Constituent Units, DCU) represents a semantic unit expressing a single event often marked by discourse operators, which denote the relationships among the DCUs. Discourse grammars are usually employed to construct the DPT from DCUs. As in the case of DRT, the LDMs describe semantic representations in terms of truth conditions. In addition, they are influenced by the sentence syntax instead of the objectives of the discourse and the task structure.\n\nThe Segmented Discourse Representation Theory (SDRT)  [14]  extends the DRT model by using a discourse relationship between the current sentence and the previous one. This relationship is used to determine how to combine these two sentences with the analysis of the previous sentences in the overall structure of the semantic representation. A discourse unit is defined at the propositional level and the set of discourse segments recursively defined and their relationships (structural and non-structural) determine how to segment the structure of the discourse. The main difference between this model and the LDM is that the relations in SDRT affect not only the structure of the discourse, but also its semantic representation.\n\nThe Rhetorical Structure Theory (RST)  [15, 16]  explains the structure of a discourse by means of the coherence relationships among its parts. These relationships are classified into subject matter relations (informational) and presentational relations (intentional). This models have been mainly used for text generation and automatic summarization.\n\nDRT, LDM and SDRT differ on how the structure of a discourse affects its semantic representation. They do not explicitly model users' intentions. Although RST considers both the informational and intentional perspectives, the theory also focuses more on the informational perspective and does not consider discourse relations among sentences to create the discourse-level semantic representation.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Intentional-Oriented Discourse Structures",
      "text": "Intentional-oriented theories model the structure of the dialog describing how the user's intentions captured by speech acts or dialog acts fit together in a dialog.\n\nThe Speech act theory  [17, 18]  analyzes the structure of the discourse considering the intention of the user's utterances and the effects on the listener. The set of basic dialog acts can be classified into illocutionary, commissives, expressives and declarations. Many researchers have modified this basic speech act taxonomy to better suite their tasks by adding more domain-specific acts. In addition, dialog acts are usually a key component in other theories modeling the discourse structure, such as dialog grammars, plan-based models and the theory of conversation acts.\n\nThe Dialog Act Markup in Several Layers (DAMSL)  [19, 20]  annotation scheme extends the speech act theory to annotate task-oriented conversations by using multiple labels defined in three orthogonal layers (Forward Communicative Functions, Backward Communicative Functions, and Utterance Features). These layers are respectively used to define a taxonomy similar to the speech act theory, indicate the relationships between the current utterance and the previous ones, and describe the content and form of an utterance. This annotation scheme can be extended with additional domain-specific acts and layers. The main drawback of the model is that the only relationship that is captured is the one between the current utterance and the previous one.\n\nDialog grammars are based on the detection of the regular patterns that are present in collaborative conversations contains regular patterns, such as question/answer pairs  [21] . These hierarchical regular patterns can be expressed using grammars in which the rules specify how the dialog can be segmented into smaller units (e.g., goals, subgoals and dialog acts). Although dialog grammars can be used to predict the next element of a conversation, it is very difficult to generate the complete set of rules that covers all possible variations of conversations in complex domain.\n\nIn the Plan-based model  [22] , dialogs are perceived as a plan that the participants follow to provide appropriate responses and achieve specific goals. The plan describes how the speech acts that model speaker's intentions relate to the conversation goal. The plan-based dialog approach for dialog management has been applied in many complex dialog systems  [18] . However, these models usually make strong assumptions about the plan and the environment in which it will be executed (e.g., dialog participants' beliefs do not change), which are not practical in real situations. In addition, plan-based models fail for dialogs that do not follow the task structure closely (e.g. topic changes, clarification or correction subdialogs, etc.). Several augmented plan-based models have been proposed to address these issues.\n\nThe theory of conversation acts  [23]  models the dialog as a set of speakerhearer actions instead of single agent actions to make grounding actions more explicit. Four levels of actions are defined to maintain the content and coherence of the dialog: turn-taking acts, grounding acts, core speech acts, and argumentation acts. These levels capture distinct dialog information and can be employed independently from each other in a dialog system.\n\nGrosz and Sidner's Theory of discourse structure (GST)  [24]  models the structure of the dialog using the concepts of discourse unit and discourse coherence. Discourse units, which are defined based on dialog participants' intention, are a set of utterances that fulfills a specific function in the overall goal of the dialog. The intentional structure is used to capture the discourse coherence. Although GST describes an abstract model of discourse structures, the theory does not detail how to solve discourse segmentation, the recognition of discourse segment purposes and the detection of the relationships between them.\n\nThe Information State Theory  [25]  uses the concept of information state to represent the information that can be used to differentiate one dialog from the others. Instead of defining a specific representation for the dialog structure, the theory provides a general framework for dialog modeling that can be implemented in the context of any dialog structure theory.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Data-Driven Approaches To Dialog Structure Modeling",
      "text": "Research on data-driven approaches to dialog structure modeling is relatively new and focuses mainly on recognizing a structure of a dialog as it progresses  [26, 27] . In the literature, there are different methodologies for discourse segmentation and the construction of dialog models including task/subtask information. Unsupervised clustering and segmentation techniques are used in  [3]  to identify concepts and subtasks in task-oriented dialogs.\n\nPassoneau and Litman  [28]  presented an algorithm for identifying topic boundaries that uses decision trees to combine multiple linguistic features extracted from corpora of spoken text. These include prosodic features such as pause duration, lexical features such as the presence of certain cue-phrases near boundary candidates, and deeper semantics questions such as whether two noun phrases on opposite sides of a boundary candidate.\n\nYamron  [29]  presented an approach to segmentation that models an unbroken text stream as an unlabeled sequence of topics using Hidden Markov Models. Ponte presented in  [30]  an approach based on information retrieval methods that map a query text into semantically related words and phrases. The approach presented in  [31]  is based on using adaptive language models and cue-word features for incrementally building an exponential model to extract features that are correlated with the presence of boundaries in labeled training text.\n\nDiverse machine-learning methodologies have been recently proposed for dialog state tracking (DST)  [32, 33, 34] , a similar task whose objective is to use the system outputs, user's utterances, dialog context and other external information sources to track what has happened in a dialog.\n\nBayesian dynamic networks are used in generative methods to model a dialog  [35] . Unobserved random variables are used to model the true user actions and dialog states, and the probability distribution over dialog states given the system's action and a noisy observation of the true user action is updated using Bayesian inference. The main drawback of these methods are that additional dependencies and structures must be learned to consider potentially useful features of the dialog history. In addition, many independence assumptions must be made to make the models tractable  [32] .\n\nThe parameters for discriminative methods are directly tuned using machine learning and labeled dialog corpus  [36] . There are two main categories of discriminative DSTs models: static classifiers and sequence models. The sequence of previous observations is used in static classifiers methods to learn learn a probability distribution for the current state. Different approaches have proposed maximum entropy linear classifiers  [36] , neural networks  [37] , and web-style ranking models  [38] . Recurrent Neural Networks (RNNs) have been recently proposed as to deal with the high dimensional continuous input features involved in sequential models  [39, 40] .",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Our Proposed Methodology For Spoken Dialog Segmentation",
      "text": "The task of detecting the current task of a dialog can be considered as specifying the distribution:\n\nwhere T t is the dialog task at the current turn t, and o 1 , • • • , o t is the sequence of observations from the SLU, ASR and machine actions up to and including the current turn.\n\nSpoken dialog systems that help the user to complete a specific task usually define the information that the user and the system can provide in terms of slots. For this reason, they are usually called slot-based dialog systems. The slots and their possible values specify the system's domain, that is, the scope of what users can talk about and the tasks that they can complete. The slots are also related to the set of possible actions that the system can perform, the semantics of the user's utterances and the possible dialog states.\n\nWe consider a task-oriented dialog to be the result of incremental creation of a shared plan by the participants  [41] . This shared plan consists of several tasks and subtasks. The goal of task segmentation is to predict if the current utterance in the dialog is part of the current task or it starts a new task or subtask. We model this prediction as a maximization problem as the following equation shows:\n\nwhere set T contains all the possible tasks/subtasks defined and U n is the semantic representation of the user utterance at time n in terms of the list of features provided by the Spoken Language Understanding (SLU) module of the conversational agent. The prediction of the current task, that is a local process, takes into account the previous history of the dialog, that is to say, the sequence of user turns and dialog segments preceding time i.\n\nThe lexical, syntactic and semantic information associated to the speaker's ith turn (U i ) is usually represented by means of different information sources:\n\n• the words uttered;\n\n• dialog acts, which represent the meaning of an utterance at the level of the speaker's intention in producing that utterance (e.g., Acceptance, Not-Understood, Tag-Question, or Apology).\n\n• part of speech tags, also called word classes or lexical categories. Common linguistic categories include noun, adjective, and verb, among others;\n\n• predicate-argument structures, used by SLU modules in various contexts to represent relations within a sentence structure. They are usually represented as triples (subject-verb-object).\n\n• named entities: sequences of words that refer to a unique identifier. This identifier may be a proper name (e.g., organization, person or location names), a time identifier (e.g., dates, time expressions or durations), or quantities and numerical expressions (e.g., monetary values, phone numbers, etc.).\n\nThe main problem to resolve Equation 1 is regarding the number of possible sequences of user's utterances preceding the current one, which could be very large in a practical conversational system. To solve this problem, we define a data structure, which we call User Register (U R), and contains the information provided by the user throughout the previous history of the dialog. The prediction of the current phase of the dialog T i is then modeled by means of the following equation:\n\nAs a practical implementation of this equation, we propose the use of a classification process that takes the semantic information of the user's utterances and the sequence of previous dialog tasks as input, and provides the probability of the dialog being at each of the dialog tasks as output. The set of variables in Equation  2 are codified in the following form:\n\n• The previous dialogs tasks and subtasks (T 1 • • • T i-1 ): Each dialog task is modeled using a variable, which has as many bits as possible tasks and subtasks defined for each application domain (C).\n\n• User register (U R i ): As previously stated, the user register includes the information provided by the user until the current moment of the dialog. Each slot related to task-dependent information sources in the U R has been coded in terms of three possible values, {0, 1, 2}, according to the following criteria: (0) The value of the slot is unknown or it is empty; (1) the value of the slot is known with a confidence score that is higher than a given threshold;\n\n(2) the value of the slot has a confidence score that is lower than the given threshold. To decide whether the state of a certain value in the U R is 1 or 2, the dialog system employs confidence measures provided by the ASR and SLU modules  [42] . Thus, each one of the task-dependent user dialog acts can take the values {0, 1, 2} and then be modeled using a variable with three bits in the same way described for the task-dependent slots.\n\nUser's utterances can contain specific values for task-dependent slots, but can also provide other kinds of information, such as task-independent information (for instance, Affirmation, Negation, and Not-Understood dialog acts). These three dialog acts have been coded with the same codification used for the task-dependent information in the U R; that is, each one of these three dialog acts can take the values {0, 1, 2}. This information is modeled using three variables with three bits.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Classification Functions",
      "text": "Different classification functions can be defined to solve Equation  2 . We have evaluated three different definitions of such a function: a multilayer perceptron (MLP)  [43] , a decision tree classifier, and a fuzzy-rule-based (FRB) classifier.\n\n• Decision tree classifier: Decision trees are widely used as algorithms for problem solving, tools for data mining and knowledge representation, and classifiers that predict the value of the decision attribute for a new object given by values of conditional attributes  [44, 45] . The C4.5 decision tree learning algorithm  [46]  has been used to learn this classification model, using the Weka machine learning software for classifying the complete list of features contained in the user register. The information gain (difference in entropy) is used as splitting criterion to select the attribute to make the decision. Whenever a new attribute is being selected, the algorithm calculates the entropy values for each possible selection, and selects the attribute with the highest information gain. This continues at each node (working top-down) until eventually the classes are split definitively.\n\n• FRB classifier: We have recently developed a toolkit to develop dialog managers for spoken dialog systems based on evolving Fuzzy-rule-based (FRB) classifiers  [47] . The toolkit uses the eClass0 (evolving classifier) for the definition of the classification function. These classifiers generate several fuzzy rules per class using an evolving clustering approach to decide when a new rule is created:\n\nwhere i denotes the number of rule; n is the number of input features; the vector F eature stores the observed features, and the vector P stores the values of the features of one of the prototypes of the corresponding class T i ∈ {set of different tasks/subtasks}.\n\nThe potential (Cauchy function of the sum of distances between a certain data sample and all other data samples in the feature space) is used in the partitioning algorithm. The potential of the k th data sample (x k ) is calculated by means of Equation 3  [48] .\n\nwhere distance represents the distance between two samples in the data space.\n\nThe potential can be calculated using the cosine distance to measure the similarity between two samples; as it is described in Equation  4 .\n\nwhere x k and x p represent the two samples to measure its distance and n represents the number of different attributes in both samples.\n\n• MLP classifier: In the multilayer perceptron classifier, the input layer holds the codification of the input\n\nThe values of the output layer can be seen as an approximation of the a posteriori probability of the input belonging to the associated class representing the current task/subtask.\n\nTo train and evaluate the neural networks, we used the April toolkit  [49] . We firstly tested the influence of the topology of the MLP, by training different MLPs of increasing number of weights using the standard backpropagation algorithm (with a sigmoid activation function and a learning rate equal to 0.2), and selecting the best topology according to the mean square error (MSE) of the validation data. The minimum MSE value was achieved using an MLP of one hidden layer of 32 units. We followed our experimentation with MLPs of this topology, training MLPs with several algorithms (the incremental version of the backpropagation algorithm with and without momentum term, and the quickprop algorithm). The best result on the validation data was obtained using the MLP trained with the standard backpropagation algorithm and a value of LR equal to 0.3.",
      "page_start": 8,
      "page_end": 11
    },
    {
      "section_name": "Demonstrations With Practical Dialog Systems",
      "text": "As a proof-of-concept we have implemented our proposal in three application domains. We have used three corpora: SoftHard, Dihana and Let's Go!, which main characteristics are summarized in Table  1  and detailed in the following subsections. Figure  2  shows the scheme follow to test our dialog segmentation technique and the relations of each step with the characteristics of the training corpora explained in the following subsections.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Softhard Corpus",
      "text": "The SoftHard corpus is related to a practical spoken dialog system that acts as a customer support service to help solving simple and routine software/hardware repairing problems, both at the domestic and professional levels.\n\nA corpus of 150 conversations with the system attending the calls of users with a software/hardware problem at the City Council of Leganés (Madrid, Spain) was annotated using a multilevel approach similar to the one proposed in the Luna Project  [50] . The first levels include segmentation of the corpus in dialog turns, transcription of the speech signal, and syntactic preprocessing with POS-tagging and shallow parsing. The next level consists of the annotation of main information using attribute-value pairs. The other levels of the annotation show contextual aspects of the semantic interpretation. These levels include the predicate structure, the relations between referring expressions, and the annotation of dialog acts.\n\nThe attribute-value annotation uses a predefined domain ontology to specify concepts and their relations. The attributes defined for the task include Concept, Computer-Hardware, Action, Person-Name, Location, Code, TelephoneNumber, Problem, etc.\n\nDialog act (DA) annotation was performed manually by three annotators on speech transcriptions previously segmented into turns. The DAs defined to label the corpus can be classified into the following categories: i) Core DAs: Actionrequest, Yes-answer, No-answer, Answer, Offer, ReportOnAction, Inform; ii) Conventional DAs: Greet, Quit, Apology, Thank ; iii) Feedback-Turn management DAs: ClarificationRequest, Ack, Filler ; iv) Non interpretable DAs: Other.\n\nThe original FrameNet 4  description of frame elements was adopted for the predicate-argument structure annotation, introducing new frames and roles related to hardware/software only in case of gaps in the FrameNet ontology. Some of the frames included in this representation are Telling, Greeting, Contacting, Statement, Recording, Communication, Being operational, Change operational state, etc. Table  2  shows the complete set of features used for the labeling of the different corpora and for the experiments that are presented in this paper.\n\nThe basic structure of the dialogs is usually composed by the sequence of the following tasks: Opening, Problem-statement, User-identification, Problem-clarification, Problem-resolution, and Closing. This set of tasks contains a list of subtasks, such as Problem-description, Problem-Request, Problem-Confirmation, Brand-Identification, Model-Identification, Help-Request, Message-Confirmation, Name-Identification, Resolution-Confirmation, etc. The shared plan is represented as a data register that encapsulates the task structure, dialog act structure,  attribute-values and predicate-argument structure of utterances. Figure  3  shows an example of the incremental evolution of dialog structure with the complete set of tasks and subtasks. It can be observed the difficulty of correctly detecting the complete structure of the dialog given the number of possible subtasks associated to each main task.  Once finished the Opening task (P1), the caller explains the problem the reasons why he/she calls the help-desk during the Problem-statement task (P2). In the User-identification task (P3), the operator asks for additional information regarding the identity of the caller. Once the caller has described the problem, the operator can ask for additional information to clarify it during the Problemclarification task (P4).",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Opening",
      "text": "",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Help Desk Task",
      "text": "",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Dihana Corpus",
      "text": "A set of 713 dialogs was acquired in the DIHANA project, whose goal was the development of a dialog system providing railway information using spontaneous speech in Spanish  [51] . Although this corpus was acquired using a Wizard of Oz technique (WOz), real speech recognition and understanding modules were used.\n\nThe dialogs in the corpus were labeled in terms of dialog acts. In the case of user turns, the dialog acts correspond to the use of frames to represent the meaning of the utterance. For the DIHANA task, eight concepts and ten attributes were defined. The eight concepts are divided into two groups:\n\n1. Task-dependent concepts: they represent the concepts the user can ask for (Hour, Price, Train-Type, Trip-Time, and Services). 2. Task-independent concepts: they represent typical interactions in a dialog (Affirmation, Negation, and Not-Understood).\n\nThree levels were defined for the labeling of the 29 system responses. The first level describes the general acts of any dialog, independently of the task. The second level represents the concepts and attributes involved. The third level represents the values of the attributes given in the turn. The following labels were defined for the first level: Opening, Closing, Undefined, Not-Understood, Waiting, New-Query, Acceptance, Rejection, Question, Confirmation, and Answer. The labels defined for the second and third level were the following: Departure-Hour, Arrival-Hour, Price, Train-Type, Origin, Destination, Date, Order-Number, Number-Trains, Services, Class, Trip-Type, Trip-Time, and Nil. Figure  5  shows an example of the semantic interpretation of a user's utterance and a system response.\n\nThe dialogs in the corpus were divided into 19 main tasks, which are summarized in Table  3 .",
      "page_start": 16,
      "page_end": 17
    },
    {
      "section_name": "Let'S Go Corpus",
      "text": "Let's Go is a spoken dialog system developed by the Carnegie Mellon University to automatically provide bus schedule information in Pittsburgh for a subset of 5 routes and 559 bus stops. The system has had many users since it was made available for the general public in 2005, with more than 20,000 calls collected just from March to December of 2005  [52] , so there is a substantial dataset that can be used to train a dialog model. In addition, this large amount of data from spoken interactions has been acquired with real callers, rather than lab testers. A corpus of 10,415 dialogs with 122,025 dialog acts was distributed among the scientific community for the Dialog State Tracking Challenge (DSTC)  5  .",
      "page_start": 18,
      "page_end": 18
    },
    {
      "section_name": "Output Sentence:",
      "text": "[SPANISH] Sí, me gustaría conocer los horarios para esta tarde desde Valencia.\n\n[ENGLISH] Yes, I would like to know the timetables for this evening leaving from Valencia.  Services query provide results, Goodbye Figure  6  shows an example of a dialog extracted from the Let's Go corpus  [53] . Each call to the system starts with a welcome message that prompts the user to make a request. Then, the system waits for the user's response and grabs concepts such as question type (e.g., When is the next bus to X?, How can I go from X to Y? ) or departure and arrival times and places. To be successful, calls require three or four pieces of information from the user: a departure stop, a destination, a travel time, and, optionally a bus route. Stops can be specified in one of three ways:  ), a neighborhood (e.g., Oakland), or a landmark or other point of interest (Pittsburgh International Airport, Waterworks Mall). The system explicitly prompts the user to provide the missing information to complete the query. Once the system has the required information to answer the user's query, it submits a query to the database, presents the results to the user, and prompts for a new query.\n\nWith regard the semantic representation defined for the task, the system uses a set of user dialog acts that has been classified into 16 categories following the criteria described in  [54] . A total of 16 categories of user dialog acts were defined. Four of the dialog acts are used to model where the user is leaving from (monument, pair of road names, neighborhood, or stop). The four dialog acts used for modeling the place of arrival are similar. Six dialog acts are used for describing the user's required time of travel (next bus or specific times). The meth node describes whether the user is asking for a bus with some constraints, is finished or wants to restart. The dialog act disc models how the user issues \"discourse\" actions, which relate to only one turn in a dialog.\n\nA total of 36 system dialog acts were defined. These dialog acts can also be classified into 5 groups: formal (dialog formalities like \"welcome\"), results (presentation of search results), queries (request for values to fill slots), statusreports (when the system reports about its status, e.g. \"looking up database\"), error (error messages), and instructions (instructions to the user how to speak to the system).\n\nThe different objectives of the dialogs for the Spoken Dialog Challenge were labeled in the corpus by considering the different places and times for which the users required information (from one to five), users' requirements about previous and next buses, number of uncovered places, and possible system failures. The different combinations of these parameters in the corpus lead to the definition of 38 different objectives. The dialogs were divided into 14 main tasks, which are shown in Table  4 .",
      "page_start": 19,
      "page_end": 19
    },
    {
      "section_name": "Results And Discussion",
      "text": "The developed methodology for the task detection has been evaluated by means of the dialogs of the corpora described in the previous section. For the three corpora, the evaluation of the statistical dialog segmentation technique was carried out turn by turn using a five-fold cross validation process. Each one of the two corpus was randomly split into five subsets. Each trial used a different subset taken from the five subsets as the test set, and the remaining 80% of the dialogs was used as the training set. The three proposals for the definition of the classification function described in Section 3.1 have been evaluated. Tables 5, 6 and 7 compile the results (precision, recall and F-Measure) obtained in the recognition of the tasks for each corpus. The tables show the average results for the main tasks of each system considering all their subtasks. As can be observed in the tables, the best results for all corpora are obtained with the MLP classifier. Table  8  presents the average precision, recall and Fmeasure obtained for all tasks in each corpus using the MLP classifier. The table shows that, despite of having a higher number of tasks, the homogenous structure of the SoftHard dialogs makes tasks more predictable, and so the accuracy for the segmentation is higher. The main problem for Dihana is the lower number of samples for some tasks (triptimes, typesof trains, and services), this makes the dialogs containing such tasks more difficult to segment and has lowered the average accuracy rates. With respect to the Let's Go! system, the main difficulty detected occurred in the occasions when users wanted to restart the dialog, asked for help or there was an error in the communication. These tasks are very difficult to detect as they are less predictable. Albeit these challenges, the accuracy and precision obtained for all tasks is high.",
      "page_start": 19,
      "page_end": 21
    },
    {
      "section_name": "Avg. Precision",
      "text": "",
      "page_start": 21,
      "page_end": 21
    },
    {
      "section_name": "Influence Of The Use Of Human-Human Vs. Human-Machine Dialogs And Training Features",
      "text": "An additional corpus of 150 human-human dialogs was available for the Soft-Hard system. Figure  7  shows the distribution of dialog segments annotated in each corpus. As can be seen, the tasks distribution is very different to the humanmachine corpus described in Section 4.1, and human-human dialogs present an additional 27.31% percentage of situations that has been labeled as Out of the Task (P15). As human-human dialogs are spontaneous, they present several differences with respect to human-machine dialogs. The main one is the great difference in the average number of turns (16.18 turns in the human-machine corpus and 25.71 for the human-human dialogs). This is because human-human dialogs present other minor topics (like small about other persons, previous problems, holidays, etc), a high frequency of interruptions, cut-off phrases, and overlapped contributions. This makes that the 18.24% of the utterances of the human-human corpus have been labeled as Out of the Task.\n\nAnalyzing the annotation available for the DA level, we measured that in average an HH dialog is composed of 37.9±7.3 (Std. Dev.) DAs, whereas a humanmachine dialog is composed of 21.9±5.4. The difference between average lengths shows how human-human spontaneous speech can be redundant, while humanmachine dialogs are more limited to an exchange of essential information. The standard deviation of a conversation in terms of DAs is considerably higher in the human-human corpus than in the human-machine ones. This can be explained by the fact that the human-machine dialogs follow a unique, previously defined task-solving strategy that does not allow digressions.\n\nThe evaluation of the statistical dialog segmentation technique was also carried out turn by turn using a five-fold cross validation process. Table  9  shows the results, which indicate that the prediction is improved once the different SLU features are incorporated to the model. As can be seen, the proposed methodology successfully adapts to the requirements of the human-machine dialogs, since a 0.95 F-measure is obtained, measuring the dialog segments provided by the developed module that are equal to the segment annotated in the corpus for these dialogs. This value is reduced to 0.79 for the human-human dialogs, since the Out of the Task class is usually confused with the rest of dialog segments related to the task. Therefore, the methodology adapts to the very different nature that has been described for both kind of dialogs.\n\nFinally, we learned a model with the total 150 human-machine dialogs and evaluated it using the total 150 human-human dialogs. This experimentation was designed to evaluate if a model learned with human-machine dialogs can detect the task-related structure of spontaneous human-human conversations. The main challenge of this experiment is that only a maximum of 81.76% can be achieved due to the 18.24% Out of the task that is only present in the human-human corpus. As can be observed, the model successfully adapts to detect the task-related parts in the human-human dialogs, achieving a 0.67 F-measure.",
      "page_start": 21,
      "page_end": 22
    },
    {
      "section_name": "Conclusions And Future Work",
      "text": "In this paper, we have presented a statistical approach for automatic dialog segmentation in conversational interfaces. This approach uses feature selection to collect a set of informative features into a model that includes the information provided by the user during the complete history of the dialog. This model can be used to predict the current task in the dialog, helping the dialog manager in the selection of the next system prompt. The experimental results show that the statistical approach successfully adapts to the requirements of three different applications domains, obtaining high accuracy and precision rates, not only separately for human-machine and human-human dialogs acquired for this task, but also when training the system with human-machine dialogs and testing it with spontaneous human-human dialogs. For future work, we want to perform a more detailed analysis of the situations that have been labeled as Out of the Task, studying if our proposal is able to differentiate them. We also plan to incorporate additional information regarding the user, such as specific user profiles adapted to each specific interaction domain. Finally, we also intend to analyze the impact of the corpus size in the results obtained.",
      "page_start": 23,
      "page_end": 23
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The dialog manager relies on the estimated dialog task to",
      "page": 2
    },
    {
      "caption": "Figure 1: Main modules of a spoken dialog system",
      "page": 3
    },
    {
      "caption": "Figure 2: shows the scheme follow to test our dialog segmentation technique and",
      "page": 12
    },
    {
      "caption": "Figure 2: Graphical scheme of the operation of the proposed dialog segmentation technique",
      "page": 12
    },
    {
      "caption": "Figure 3: Incremental evolution of the dialog structure",
      "page": 14
    },
    {
      "caption": "Figure 4: Example of a Human-Machine dialog (translation from Spanish to English)",
      "page": 15
    },
    {
      "caption": "Figure 5: shows an example of the semantic interpretation of a user’s utterance",
      "page": 16
    },
    {
      "caption": "Figure 5: An example of the labeling of a user and system turns in the DIHANA corpus",
      "page": 17
    },
    {
      "caption": "Figure 6: shows an example of a dialog extracted from the Let’s Go corpus [53].",
      "page": 17
    },
    {
      "caption": "Figure 6: Example of dialog extracted from the Let’s Go corpus",
      "page": 18
    },
    {
      "caption": "Figure 7: shows the distribution of dialog segments annotated in",
      "page": 21
    },
    {
      "caption": "Figure 7: Distribution of dialog segments annotated in the human-machine and human-",
      "page": 21
    }
  ],
  "tables": [
    {
      "caption": "Table 2: List of semantic features used for the labeling of the SoftHard corpus",
      "data": [
        {
          "Dialog Acts": "Attribute-value",
          "Core dialog acts (Info-request, Action-request Yes-answer, No-answer,\nAnswer, Oﬀer, ReportOnAction,\nInform), Conventional dialog acts\n(Greet, Quit, Apology, Thank ), Feedback/Turn management dialog\nacts (Clari(cid:12)cationRequest, Ack, Filler ), Non interpretable/Non clas-\nsi(cid:12)able (Other )": "Concept, Computer-Hardware, Time-PartoftheDay, Negation, Action,\nPerson-Name, Person-Surname,\nLocation-Institution, Code,\nLocation-\nOther, Location-TelephoneNumber, Ordinal-Number, Cardinal-Number,\nTime-RelativeTime, Problem, Person-Position"
        },
        {
          "Dialog Acts": "Predicate information",
          "Core dialog acts (Info-request, Action-request Yes-answer, No-answer,\nAnswer, Oﬀer, ReportOnAction,\nInform), Conventional dialog acts\n(Greet, Quit, Apology, Thank ), Feedback/Turn management dialog\nacts (Clari(cid:12)cationRequest, Ack, Filler ), Non interpretable/Non clas-\nsi(cid:12)able (Other )": "Telling, Greeting, Contacting, Statement, Recording, Communication.\nBeing operational, Change operational state, Operational testing, Being\nin operation."
        }
      ],
      "page": 14
    },
    {
      "caption": "Table 4: Table 4: Main tasks for the Let’s Go! dialog system",
      "data": [
        {
          "Decision tree classi(cid:12)er": "Prec.",
          "FRB-classi(cid:12)er": "Prec.",
          "MLP-classi(cid:12)er": "Prec."
        },
        {
          "Decision tree classi(cid:12)er": "0.98",
          "FRB-classi(cid:12)er": "0.98",
          "MLP-classi(cid:12)er": "0.99"
        },
        {
          "Decision tree classi(cid:12)er": "0.94",
          "FRB-classi(cid:12)er": "0.95",
          "MLP-classi(cid:12)er": "0.97"
        },
        {
          "Decision tree classi(cid:12)er": "0.93",
          "FRB-classi(cid:12)er": "0.94",
          "MLP-classi(cid:12)er": "0.96"
        },
        {
          "Decision tree classi(cid:12)er": "0.91",
          "FRB-classi(cid:12)er": "0.92",
          "MLP-classi(cid:12)er": "0.95"
        },
        {
          "Decision tree classi(cid:12)er": "0.90",
          "FRB-classi(cid:12)er": "0.91",
          "MLP-classi(cid:12)er": "0.94"
        },
        {
          "Decision tree classi(cid:12)er": "0.96",
          "FRB-classi(cid:12)er": "0.97",
          "MLP-classi(cid:12)er": "0.98"
        }
      ],
      "page": 19
    },
    {
      "caption": "Table 6: Task and subtasks prediction for the DIHANA dialog system",
      "data": [
        {
          "Decision tree classi(cid:12)er": "Prec.",
          "FRB-classi(cid:12)er": "Prec.",
          "MLP-classi(cid:12)er": "Prec."
        },
        {
          "Decision tree classi(cid:12)er": "0.97",
          "FRB-classi(cid:12)er": "0.98",
          "MLP-classi(cid:12)er": "0.98"
        },
        {
          "Decision tree classi(cid:12)er": "0.88",
          "FRB-classi(cid:12)er": "0.89",
          "MLP-classi(cid:12)er": "0.91"
        },
        {
          "Decision tree classi(cid:12)er": "0.83",
          "FRB-classi(cid:12)er": "0.85",
          "MLP-classi(cid:12)er": "0.86"
        },
        {
          "Decision tree classi(cid:12)er": "0.81",
          "FRB-classi(cid:12)er": "0.81",
          "MLP-classi(cid:12)er": "0.82"
        },
        {
          "Decision tree classi(cid:12)er": "0.80",
          "FRB-classi(cid:12)er": "0.81",
          "MLP-classi(cid:12)er": "0.82"
        },
        {
          "Decision tree classi(cid:12)er": "0.81",
          "FRB-classi(cid:12)er": "0.81",
          "MLP-classi(cid:12)er": "0.82"
        },
        {
          "Decision tree classi(cid:12)er": "0.83",
          "FRB-classi(cid:12)er": "0.83",
          "MLP-classi(cid:12)er": "0.84"
        },
        {
          "Decision tree classi(cid:12)er": "0.82",
          "FRB-classi(cid:12)er": "0.82",
          "MLP-classi(cid:12)er": "0.84"
        },
        {
          "Decision tree classi(cid:12)er": "0.87",
          "FRB-classi(cid:12)er": "0.89",
          "MLP-classi(cid:12)er": "0.91"
        },
        {
          "Decision tree classi(cid:12)er": "0.94",
          "FRB-classi(cid:12)er": "0.96",
          "MLP-classi(cid:12)er": "0.97"
        }
      ],
      "page": 20
    },
    {
      "caption": "Table 6: Task and subtasks prediction for the DIHANA dialog system",
      "data": [
        {
          "Decision tree classi(cid:12)er": "Prec.",
          "FRB-classi(cid:12)er": "Prec.",
          "MLP-classi(cid:12)er": "Prec."
        },
        {
          "Decision tree classi(cid:12)er": "0.98",
          "FRB-classi(cid:12)er": "0.98",
          "MLP-classi(cid:12)er": "0.98"
        },
        {
          "Decision tree classi(cid:12)er": "0.88",
          "FRB-classi(cid:12)er": "0.85",
          "MLP-classi(cid:12)er": "0.92"
        },
        {
          "Decision tree classi(cid:12)er": "0.87",
          "FRB-classi(cid:12)er": "0.88",
          "MLP-classi(cid:12)er": "0.90"
        },
        {
          "Decision tree classi(cid:12)er": "0.88",
          "FRB-classi(cid:12)er": "0.89",
          "MLP-classi(cid:12)er": "0.91"
        },
        {
          "Decision tree classi(cid:12)er": "0.87",
          "FRB-classi(cid:12)er": "0.88",
          "MLP-classi(cid:12)er": "0.90"
        },
        {
          "Decision tree classi(cid:12)er": "0.92",
          "FRB-classi(cid:12)er": "0.93",
          "MLP-classi(cid:12)er": "0.95"
        },
        {
          "Decision tree classi(cid:12)er": "0.82",
          "FRB-classi(cid:12)er": "0.83",
          "MLP-classi(cid:12)er": "0.86"
        },
        {
          "Decision tree classi(cid:12)er": "0.84",
          "FRB-classi(cid:12)er": "0.85",
          "MLP-classi(cid:12)er": "0.87"
        },
        {
          "Decision tree classi(cid:12)er": "0.96",
          "FRB-classi(cid:12)er": "0.96",
          "MLP-classi(cid:12)er": "0.98"
        }
      ],
      "page": 20
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "The Conversational Interface: Talking to Smart Devices",
      "authors": [
        "M Mctear",
        "Z Callejas",
        "D Griol"
      ],
      "year": "2016",
      "venue": "The Conversational Interface: Talking to Smart Devices"
    },
    {
      "citation_id": "2",
      "title": "Natural Language Dialog Systems and Intelligent Assistants",
      "authors": [
        "G Lee",
        "H Kim",
        "M Jeong",
        "J Kim"
      ],
      "year": "2015",
      "venue": "Natural Language Dialog Systems and Intelligent Assistants"
    },
    {
      "citation_id": "3",
      "title": "Learning the structure of task-oriented conversations from the corpus of in-domain dialogs",
      "authors": [
        "A Chotimongkol"
      ],
      "year": "2008",
      "venue": "Learning the structure of task-oriented conversations from the corpus of in-domain dialogs"
    },
    {
      "citation_id": "4",
      "title": "The use of topic segmentation for automatic summarization",
      "authors": [
        "R Angheluta",
        "R Busser",
        "M Moens"
      ],
      "year": "2002",
      "venue": "Proc. of ACL"
    },
    {
      "citation_id": "5",
      "title": "Passage retrieval revisited",
      "authors": [
        "M Kaszkiel",
        "J Zobel"
      ],
      "year": "1997",
      "venue": "Proc. of ACM SIGIR"
    },
    {
      "citation_id": "6",
      "title": "Efficient retrieval of partial documents",
      "authors": [
        "A Moffat",
        "R Sacks-Davis",
        "R Wilkinson",
        "J Zobel"
      ],
      "year": "1995",
      "venue": "Information Processing and Management"
    },
    {
      "citation_id": "7",
      "title": "Centering, anaphora resolution, and discourse structure",
      "authors": [
        "M Walker"
      ],
      "year": "1998",
      "venue": "Centering, anaphora resolution, and discourse structure"
    },
    {
      "citation_id": "8",
      "title": "Discourse structure for context question answering",
      "authors": [
        "J Chai",
        "R Jin"
      ],
      "year": "2004",
      "venue": "Proc. of HLT-NAACL"
    },
    {
      "citation_id": "9",
      "title": "Automatic dialog act segmentation and classification in multiparty meetings",
      "authors": [
        "J Ang",
        "Y Liu",
        "E Shriberg"
      ],
      "year": "2005",
      "venue": "Proc. of ICASSP"
    },
    {
      "citation_id": "10",
      "title": "",
      "authors": [
        "B Grosz",
        "M Pollack",
        "C Sidner",
        "Discourse"
      ],
      "year": "1989",
      "venue": ""
    },
    {
      "citation_id": "11",
      "title": "Discourse in Computational Linguistics and Articial Intelligence",
      "authors": [
        "J Moore",
        "P Wiemer-Hastings"
      ],
      "year": "1989",
      "venue": "Discourse in Computational Linguistics and Articial Intelligence"
    },
    {
      "citation_id": "12",
      "title": "From Discourse to Logic: Introduction to Modeltheoretic Semantics of Natural Language, Formal Logic and Discourse Representation Theory",
      "authors": [
        "H Kamp",
        "U Reyle"
      ],
      "year": "1993",
      "venue": "From Discourse to Logic: Introduction to Modeltheoretic Semantics of Natural Language, Formal Logic and Discourse Representation Theory"
    },
    {
      "citation_id": "13",
      "title": "Center for the Study of Language and Information",
      "authors": [
        "L Polanyi"
      ],
      "year": "1996",
      "venue": "Center for the Study of Language and Information"
    },
    {
      "citation_id": "14",
      "title": "",
      "authors": [
        "N Asher"
      ],
      "year": "1993",
      "venue": ""
    },
    {
      "citation_id": "15",
      "title": "Rhetorical structure theory: Toward a functional theory of text organization",
      "authors": [
        "W Mann",
        "S Thompson"
      ],
      "year": "1988",
      "venue": "Text"
    },
    {
      "citation_id": "16",
      "title": "Rhetorical structure theory: looking back and moving ahead",
      "authors": [
        "M Taboada",
        "W Mann"
      ],
      "year": "2006",
      "venue": "Discourse Studies"
    },
    {
      "citation_id": "17",
      "title": "Speech acts. An essay on the philosophy of language",
      "authors": [
        "J Searle"
      ],
      "year": "1969",
      "venue": "Speech acts. An essay on the philosophy of language"
    },
    {
      "citation_id": "18",
      "title": "Plan Recognition in Verbmobil",
      "authors": [
        "J Alexandersson"
      ],
      "year": "1995",
      "venue": "Proc. of IJCAI-95 Workshop on the Next Generation of Plan Recognition Systems"
    },
    {
      "citation_id": "19",
      "title": "Coding Dialogs with the DAMSL Annotation Scheme",
      "authors": [
        "M Core",
        "J Allen"
      ],
      "year": "1997",
      "venue": "Proc. of AAAI Fall Symposium on Communicative Action in Humans and Machines"
    },
    {
      "citation_id": "20",
      "title": "Meeting Recorder Project: Dialog Act Labeling Guide",
      "authors": [
        "R Dhillon",
        "S Bhagat",
        "H Carvey",
        "E Shriberg"
      ],
      "year": "1998",
      "venue": "Meeting Recorder Project: Dialog Act Labeling Guide"
    },
    {
      "citation_id": "21",
      "title": "",
      "authors": [
        "J Carletta",
        "A Isard",
        "S Isard",
        "J Kowtko",
        "G Doherty-Sneddon",
        "A Anderson"
      ],
      "year": "1997",
      "venue": ""
    },
    {
      "citation_id": "22",
      "title": "The uses of plans",
      "authors": [
        "M Pollack"
      ],
      "year": "1992",
      "venue": "Artificial Intelligence"
    },
    {
      "citation_id": "23",
      "title": "Conversation acts in task-oriented spoken dialogue",
      "authors": [
        "D Traum",
        "E Hinkelman"
      ],
      "year": "1992",
      "venue": "Computational Intelligence"
    },
    {
      "citation_id": "24",
      "title": "Attention, intentions and the structure of discourse",
      "authors": [
        "B Grosz",
        "C Sidner"
      ],
      "year": "1986",
      "venue": "Computational Linguistics"
    },
    {
      "citation_id": "25",
      "title": "Information state and dialogue management in the trindi dialogue move engine toolkit",
      "authors": [
        "S Larsson",
        "D Traum"
      ],
      "year": "2000",
      "venue": "Natural Language Engineering"
    },
    {
      "citation_id": "26",
      "title": "Learning cooperative persuasive dialogue policies using framing",
      "authors": [
        "T Hiraoka",
        "G Neubig",
        "S Sakti",
        "T Toda",
        "S Nakamura"
      ],
      "year": "2016",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "27",
      "title": "Cascaded model adaptation for dialog act segmentation and tagging",
      "authors": [
        "U Guz",
        "G Tur",
        "D Hakkani-Tur",
        "S Cuendet"
      ],
      "year": "2010",
      "venue": "Computer Speech & Language"
    },
    {
      "citation_id": "28",
      "title": "Discourse segmentation by human and automated means",
      "authors": [
        "R Passoneau",
        "D Litman"
      ],
      "year": "1997",
      "venue": "Computational Linguistics"
    },
    {
      "citation_id": "29",
      "title": "Topic detection and tracking segmentation task",
      "authors": [
        "J Yamron"
      ],
      "year": "1998",
      "venue": "Proc. Broadcast News Transcription and Understanding Workshop"
    },
    {
      "citation_id": "30",
      "title": "Text segmentation by topic",
      "authors": [
        "J Ponte",
        "W Croft"
      ],
      "year": "1997",
      "venue": "Proc. of ECDL"
    },
    {
      "citation_id": "31",
      "title": "Statistical models for text segmentation",
      "authors": [
        "D Beeferman",
        "A Berger",
        "J Lafferty"
      ],
      "year": "1999",
      "venue": "Machine Learning"
    },
    {
      "citation_id": "32",
      "title": "Machine learning for dialog state tracking: A review",
      "authors": [
        "M Henderson"
      ],
      "year": "2015",
      "venue": "Proc. of the First International Workshop on Machine Learning in Spoken Language Processing (MLSLP2015)"
    },
    {
      "citation_id": "33",
      "title": "The dialog state tracking challenge series: A review",
      "authors": [
        "J Williams",
        "A Raux",
        "M Henderson"
      ],
      "year": "2016",
      "venue": "Dialogue & Discourse"
    },
    {
      "citation_id": "34",
      "title": "Dialog state tracking, a machine reading approach using Memory Network",
      "authors": [
        "J Perez",
        "F Liu"
      ],
      "year": "2017",
      "venue": "Proc. of the 15th Conference of the European Chapter"
    },
    {
      "citation_id": "35",
      "title": "Partially observable markov decision processes for spoken dialog systems",
      "authors": [
        "J Williams",
        "S Young"
      ],
      "year": "2007",
      "venue": "Computer Speech & Language"
    },
    {
      "citation_id": "36",
      "title": "Discriminative state tracking for spoken dialog systems",
      "authors": [
        "A Metallinou",
        "D Bohus",
        "J Williams"
      ],
      "year": "2013",
      "venue": "Proc. of the 51st Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "37",
      "title": "Markovian discriminative modeling for dialog state tracking",
      "authors": [
        "H Ren",
        "W Xu",
        "Y Yan"
      ],
      "year": "2014",
      "venue": "Proc. of the SIGDIAL 2014 Conference"
    },
    {
      "citation_id": "38",
      "title": "Web-style ranking and SLU combination for dialog state tracking",
      "authors": [
        "J Williams"
      ],
      "year": "2014",
      "venue": "Proc. of the SIGDIAL 2014 Conference"
    },
    {
      "citation_id": "39",
      "title": "Deep learning for conversational AI",
      "authors": [
        "P Su",
        "N Mrksic",
        "I Casanueva",
        "I Vulic"
      ],
      "year": "2018",
      "venue": "Proc. of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics (NAACL-HLT 2018)"
    },
    {
      "citation_id": "40",
      "title": "A Multichannel Convolutional Neural Network for Cross-language Dialog State Tracking",
      "authors": [
        "H Shi",
        "T Ushio",
        "M Endo",
        "K Yamagami",
        "N Horii"
      ],
      "year": "2016",
      "venue": "Proc. of IEEE Spoken Language Technology Workshop (SLT'16)"
    },
    {
      "citation_id": "41",
      "title": "Learning the Structure of Task-Driven Human-Human Dialogs",
      "authors": [
        "S Bangalore",
        "G Difabbrizio",
        "A Stent"
      ],
      "year": "2008",
      "venue": "IEEE Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "42",
      "title": "Error handling in a stochastic dialog system through confidence measures",
      "authors": [
        "F Torres",
        "L Hurtado",
        "F García",
        "E Sanchis",
        "E Segarra"
      ],
      "year": "2005",
      "venue": "Error handling in a stochastic dialog system through confidence measures"
    },
    {
      "citation_id": "43",
      "title": "PDP: Computational models of cognition and perception, I",
      "authors": [
        "D Rumelhart",
        "G Hinton",
        "R Williams"
      ],
      "year": "1986",
      "venue": "PDP: Computational models of cognition and perception, I"
    },
    {
      "citation_id": "44",
      "title": "Bi-criteria optimization of decision trees with applications to data analysis",
      "authors": [
        "I Chikalov",
        "S Hussain",
        "M Moshkov"
      ],
      "year": "2018",
      "venue": "European Journal of Operational Research"
    },
    {
      "citation_id": "45",
      "title": "Speech emotion recognition based on feature selection and extreme learning machine decision tree",
      "authors": [
        "Z.-T Liu",
        "M Wu",
        "W.-H Cao",
        "J.-W Mao",
        "J.-P Xu",
        "G.-Z Tan"
      ],
      "year": "2018",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "46",
      "title": "C4.5: Programs for Machine Learning",
      "authors": [
        "J Quinlan"
      ],
      "year": "1993",
      "venue": "C4.5: Programs for Machine Learning"
    },
    {
      "citation_id": "47",
      "title": "FRB-Dialog: A Toolkit for Automatic Learning of Fuzzy-Rule Based (FRB) Dialog Managers",
      "authors": [
        "D Griol",
        "A Sanchis",
        "J Molina"
      ],
      "year": "2017",
      "venue": "Proc. of International Conference on Hybrid Artificial Intelligence Systems (HAIS 2017)"
    },
    {
      "citation_id": "48",
      "title": "Evolving fuzzy-rule-based classifiers from data streams",
      "authors": [
        "P Angelov",
        "X Zhou"
      ],
      "year": "2008",
      "venue": "IEEE T. Fuzzy Systems"
    },
    {
      "citation_id": "49",
      "title": "Efficient bp algorithms for general feedforward neural networks",
      "authors": [
        "S Espana",
        "F Zamora",
        "M Castro",
        "J Gorbe"
      ],
      "year": "2007",
      "venue": "Lecture Notes in Computer Science"
    },
    {
      "citation_id": "50",
      "title": "The Development of the Multilingual LUNA Corpus for Spoken Language System Porting",
      "authors": [
        "E Stepanov",
        "G Riccardi",
        "A Bayer"
      ],
      "year": "2014",
      "venue": "Proc. of LREC"
    },
    {
      "citation_id": "51",
      "title": "A Statistical Approach to Spoken Dialog Systems Design and Evaluation",
      "authors": [
        "D Griol",
        "L Hurtado",
        "E Segarra",
        "E Sanchis"
      ],
      "year": "2008",
      "venue": "A Statistical Approach to Spoken Dialog Systems Design and Evaluation"
    },
    {
      "citation_id": "52",
      "title": "Let's go public! taking a spoken dialog system to the real world",
      "authors": [
        "A Raux",
        "B Langner",
        "A Black",
        "M Eskenazi"
      ],
      "year": "2005",
      "venue": "Proc. of Interspeech'05"
    },
    {
      "citation_id": "53",
      "title": "Let's Go, DUDE!\" Using the Spoken Dialogue Challenge to teach Spoken Dialogue development",
      "authors": [
        "H Hastie",
        "N Merigaud",
        "X Liu",
        "O Lemon"
      ],
      "year": "2010",
      "venue": "Proc. of SLT'10"
    },
    {
      "citation_id": "54",
      "title": "Bayesian dialogue system for the Let's Go Spoken Dialogue Challenge",
      "authors": [
        "B Thomson",
        "K Yu",
        "S Keizer",
        "M Gasic",
        "F Jurcicek",
        "F Mairesse",
        "S Young"
      ],
      "year": "2010",
      "venue": "Proc. of SLT'10"
    }
  ]
}